---
ver: rpa2
title: 'Classifying complex documents: comparing bespoke solutions to large language
  models'
arxiv_id: '2312.07182'
source_url: https://arxiv.org/abs/2312.07182
tags:
- classification
- documents
- text
- document
- bespoke
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compared three approaches for classifying complex legal
  documents: a bespoke custom-trained neural network model, a large language model
  (GPT-3.5), and a fine-tuned GPT-3.5. The classification task involved categorizing
  approximately 30,000 courthouse records from 12 states and 267 counties into two
  levels: binary classification (Oil and Gas Document vs.'
---

# Classifying complex documents: comparing bespoke solutions to large language models

## Quick Facts
- arXiv ID: 2312.07182
- Source URL: https://arxiv.org/abs/2312.07182
- Reference count: 7
- Key outcome: A bespoke BOW + CNN ensemble model outperformed fine-tuned GPT-3.5 for complex legal document classification, achieving higher accuracy despite requiring more training data.

## Executive Summary
This study compared three approaches for classifying complex legal documents: a bespoke custom-trained neural network model, a large language model (GPT-3.5), and a fine-tuned GPT-3.5. The classification task involved categorizing approximately 30,000 courthouse records from 12 states and 267 counties into two levels: binary classification (Oil and Gas Document vs. Other) and multi-label classification (9 subcategories). The bespoke model, which combined a bag-of-words model of n-grams with a convolutional neural network, achieved the highest accuracy, outperforming both the standard LLM and the fine-tuned LLM. However, fine-tuning GPT-3.5 with a smaller dataset (2,000 vs. 29,307 documents) significantly improved its performance, though it did not reach the accuracy of the bespoke model. The results suggest that while fine-tuned LLMs offer a cost-effective alternative, bespoke models remain superior for complex classification tasks, particularly in scenarios requiring high precision and control.

## Method Summary
The study evaluated three classification approaches on ~30,000 legal courthouse records. The bespoke model used a bag-of-words n-gram encoding combined with a CNN with attention mechanism, trained on 29,307 documents with 800 tokens for binary and 1500 tokens for multi-label tasks. GPT-3.5 was tested both in raw form and fine-tuned on subsets ranging from 200-2,000 documents. All models were evaluated on a consistent test set of 600 documents using accuracy metrics for both binary and multi-label classification tasks.

## Key Results
- The bespoke BOW + CNN ensemble achieved the highest accuracy for both binary and multi-label classification tasks.
- Fine-tuning GPT-3.5 with 2,000 documents improved performance significantly compared to the raw model but still underperformed the bespoke approach.
- Optimal text length varied by task complexity: 800 tokens for binary classification and 1500 tokens for multi-label classification.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a pre-trained LLM with domain-specific data improves classification accuracy compared to using the raw model.
- Mechanism: The fine-tuning process adjusts the model's internal representations to better align with the specific patterns and terminology of the target domain, enabling more accurate predictions for that domain.
- Core assumption: The base LLM has learned generalizable language patterns that can be adapted to the target domain with relatively small amounts of task-specific data.
- Evidence anchors:
  - [abstract] "fine-tuning GPT-3.5 with a smaller dataset (2,000 vs. 29,307 documents) significantly improved its performance"
  - [section] "Fine-tuned LLM: Chat-GPT-3.5... During fine-tuning we provided examples of document text, the document class and a prompt for the task itself."
- Break condition: If the target domain has very different language patterns from the general pretraining corpus, or if the fine-tuning data is too small or noisy, the improvement may be minimal or nonexistent.

### Mechanism 2
- Claim: A bespoke model using n-gram bag-of-words combined with a CNN can achieve higher accuracy than a fine-tuned LLM for complex document classification tasks.
- Mechanism: The n-gram bag-of-words representation captures specific phrase patterns and co-occurrences that are discriminative for the classification task, while the CNN with attention mechanism can learn to focus on the most relevant parts of the document text.
- Core assumption: The classification task can be effectively solved by identifying and leveraging specific textual patterns and phrases, rather than requiring deep semantic understanding.
- Evidence anchors:
  - [abstract] "Our bespoke approach combined a bag-of-words model of n-grams with a convolutional neural net, and was trained on 29,307 example documents... Our results suggest that while such custom-trained models achieve the highest accuracy"
  - [section] "We used an approach which is an ensemble of two models — a bag-of-words (BOW) model based on n-gram encoding, and a convolutional neural network with an attention mechanism."
- Break condition: If the task requires deep semantic understanding or reasoning that goes beyond surface-level patterns, the bespoke model may not perform as well.

### Mechanism 3
- Claim: The optimal text length for the classification models varies depending on the task complexity, with longer text needed for multi-label classification than binary classification.
- Mechanism: Longer text provides more context and information for the model to make accurate classifications, especially for more complex tasks with more categories.
- Core assumption: The relevant information for classification is distributed throughout the document text, and truncating the text too much may result in loss of important information.
- Evidence anchors:
  - [section] "Since to-be-classified documents could be up to hundreds of pages long, we had to limit the length of the input text to the model... The optimal text length was 800 tokens for binary, and 1500 tokens for the multi-class task."
- Break condition: If the most important information for classification is concentrated in a small portion of the text, longer text may not provide additional benefit and could even introduce noise.

## Foundational Learning

- Concept: Bag-of-words model
  - Why needed here: The bespoke model uses a bag-of-words approach with n-gram encoding to represent the document text. Understanding this concept is crucial for grasping how the model processes and learns from the input data.
  - Quick check question: What are the advantages and disadvantages of using a bag-of-words model compared to other text representation methods like word embeddings?

- Concept: Convolutional neural networks (CNNs)
  - Why needed here: The bespoke model incorporates a CNN with attention mechanism. Familiarity with CNNs and their application to text data is necessary to understand how the model learns to extract relevant features from the document text.
  - Quick check question: How do CNNs differ from traditional fully connected neural networks, and what are their advantages for text classification tasks?

- Concept: Large language models (LLMs) and fine-tuning
  - Why needed here: The study compares a bespoke model to a fine-tuned LLM. Understanding the basics of LLMs, their pretraining process, and how fine-tuning works is essential for interpreting the results and implications of the study.
  - Quick check question: What is the purpose of fine-tuning an LLM, and how does it differ from training a model from scratch?

## Architecture Onboarding

- Component map:
  Input text (tokenized) → BOW layer (n-gram encoding) → CNN with attention → Softmax output (for bespoke)
  Input text (raw) → LLM processing → Softmax output (for fine-tuned LLM)

- Critical path:
  1. Preprocess and encode document text
  2. Feed encoded text into the model (bespoke or fine-tuned LLM)
  3. Obtain model predictions
  4. Evaluate predictions against ground truth labels

- Design tradeoffs:
  - Bespoke model: Higher accuracy but requires more labeled training data and computational resources for training
  - Fine-tuned LLM: Lower accuracy but requires less labeled data and computational resources, and can leverage pre-existing knowledge from pretraining

- Failure signatures:
  - Low accuracy: Insufficient training data, poor feature representation, or inadequate model capacity
  - High false positive rate: Model is too lenient in its classification criteria
  - High false negative rate: Model is too strict in its classification criteria

- First 3 experiments:
  1. Evaluate the performance of the bespoke model and the raw LLM on a small subset of the test data to establish a baseline comparison.
  2. Fine-tune GPT-3.5 on subsets of 200-2000 documents using provided prompts, evaluate performance on the 600-doc test set, and compare with bespoke and baseline GPT-3.5 results.
  3. Experiment with different text lengths for the input to the models to determine the optimal context window for each classification task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of fine-tuned LLMs scale with larger training datasets (e.g., 10,000+ documents) compared to the bespoke model?
- Basis in paper: [explicit] The paper mentions fine-tuning with 200-2,000 documents and notes that performance plateaus around 87%, but does not explore larger datasets.
- Why unresolved: The study only tested fine-tuning with a limited range of dataset sizes (200-2,000 documents), leaving uncertainty about whether larger datasets could bridge the performance gap with bespoke models.
- What evidence would resolve it: Conducting experiments with progressively larger fine-tuning datasets (e.g., 5,000, 10,000, 20,000 documents) and comparing their performance to the bespoke model.

### Open Question 2
- Question: Can hybrid models combining bespoke NLP approaches with LLMs outperform either method alone in complex document classification tasks?
- Basis in paper: [inferred] The paper highlights the strengths of bespoke models (high accuracy) and LLMs (cost-effectiveness) but does not explore their combination.
- Why unresolved: The study focuses on comparing standalone approaches but does not investigate whether integrating the two methods could leverage their respective strengths.
- What evidence would resolve it: Developing and testing hybrid models that use LLMs for initial classification and bespoke models for refinement, then comparing their performance to standalone approaches.

### Open Question 3
- Question: How do domain-specific LLMs (e.g., trained on legal or medical data) compare to general-purpose LLMs in terms of classification accuracy and cost-effectiveness?
- Basis in paper: [inferred] The paper discusses fine-tuning general-purpose LLMs (GPT-3.5) but does not explore the use of domain-specific pre-trained models.
- Why unresolved: The study does not address whether domain-specific LLMs could reduce the need for extensive fine-tuning or improve accuracy in specialized tasks.
- What evidence would resolve it: Comparing the performance of domain-specific LLMs (e.g., legal or medical) to general-purpose LLMs when fine-tuned on the same task and dataset.

## Limitations
- The bespoke model requires substantial labeled training data (29,307 documents) that may not be available for other domains.
- The study focused on legal documents from a specific jurisdiction, limiting generalizability to other document types.
- The optimal text length findings may be specific to this document corpus and classification task complexity.

## Confidence

- High confidence: The comparative performance ranking between the three approaches is robust, as evidenced by consistent results across both binary and multi-label classification tasks using the same test set (600 documents).
- Medium confidence: The superiority of the bespoke model is well-supported within this specific domain, but may not generalize to domains requiring deeper semantic understanding rather than pattern recognition.
- Low confidence: The optimal text length findings (800 tokens for binary, 1500 for multi-label) may be specific to this document corpus and classification task, and could vary significantly for different document types or classification complexity.

## Next Checks
1. Test the fine-tuned GPT-3.5 performance with intermediate dataset sizes (e.g., 400, 800, 1000 documents) to better understand the learning curve and determine the minimum viable dataset size for acceptable performance.
2. Apply the three approaches to a different document domain (e.g., medical records or technical documentation) to assess generalizability of the performance rankings and optimal hyperparameters.
3. Conduct ablation studies on the bespoke model to quantify the individual contributions of the BOW component versus the CNN component to overall accuracy, helping identify whether simpler architectures might suffice for similar tasks.