---
ver: rpa2
title: Benchmarking large language models for biomedical natural language processing
  applications and recommendations
arxiv_id: '2305.16326'
source_url: https://arxiv.org/abs/2305.16326
tags:
- biomedical
- llms
- performance
- gpt-4
- bionlp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study systematically evaluated four LLMs on 12 BioNLP benchmarks
  across six applications and compared their zero-shot, few-shot, and fine-tuning
  performance with traditional fine-tuning of BERT or BART models. The study also
  examined inconsistencies, missing information, hallucinations, and performed cost
  analysis.
---

# Benchmarking large language models for biomedical natural language processing applications and recommendations

## Quick Facts
- arXiv ID: 2305.16326
- Source URL: https://arxiv.org/abs/2305.16326
- Authors: 
- Reference count: 40
- One-line primary result: Traditional fine-tuning outperforms zero/few-shot LLMs in most BioNLP tasks, except for reasoning-related tasks like medical QA where GPT-4 excels.

## Executive Summary
This study systematically evaluates four large language models (GPT-3.5, GPT-4, and fine-tuned PubMedBERT) across 12 BioNLP benchmarks spanning six applications. The research compares zero-shot, one-shot, and fine-tuning performance against traditional fine-tuning of BERT and BART models. The primary finding is that fine-tuned domain-specific models generally outperform LLMs for information extraction and classification tasks, while GPT-4 shows superior performance in reasoning-intensive medical question answering. The study also identifies critical issues including inconsistent outputs, missing information, and hallucinations in LLM-generated results.

## Method Summary
The evaluation employed 12 BioNLP benchmark datasets across six applications, testing GPT-3.5 and GPT-4 in zero-shot and one-shot settings against fine-tuned PubMedBERT. For fine-tuning, 18 hyperparameter combinations were tested (learning rates: 1e-05, 3e-5, 5e-5; sequence lengths: 128, 256, 512; batch sizes: 16, 32; dropout rate: 0.1). Performance was assessed using macro-F1, accuracy, and Pearson correlation metrics, with 200 instances randomly sampled from each dataset for evaluation. Manual examination of LLM outputs identified issues with missing information, inconsistencies, and hallucinations.

## Key Results
- Fine-tuned PubMedBERT achieved highest performance across most tasks, ranking first overall
- GPT-4 one-shot learning obtained second-highest macro-average accuracy, excelling in medical QA reasoning tasks
- Issues identified: 94/200 output samples showed inconsistent formats in protein-protein interaction extraction, along with missing information and hallucinations

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning biomedical language models remains more reliable than zero-shot or few-shot LLM usage for information extraction and classification tasks because biomedical language models (e.g., PubMedBERT) are pre-trained on domain-specific corpora, enabling them to encode biomedical semantic representations that generalize well to extraction and classification tasks without requiring extensive demonstrations in prompts.

### Mechanism 2
Prompt engineering (zero-shot vs one-shot) significantly affects LLM performance, but gains plateau beyond one demonstration because providing even a single in-context example helps LLMs adapt to task format, but additional examples do not yield proportional performance gains.

### Mechanism 3
LLMs generate inconsistent outputs and sometimes produce missing or artificial content, which undermines reliability in biomedical settings because LLMs lack strict adherence to output schemas and can generate paraphrases, new labels, or inconsistent formatting, leading to post-processing needs.

## Foundational Learning

- Concept: Zero-shot vs few-shot learning
  - Why needed here: Determines how to evaluate LLM performance without task-specific fine-tuning
  - Quick check question: What is the difference between zero-shot and one-shot learning in the context of LLMs?

- Concept: Named entity recognition (NER) and relation extraction
  - Why needed here: Core BioNLP tasks where LLMs underperform compared to fine-tuned models
  - Quick check question: Why do biomedical NER tasks require precise entity spans?

- Concept: Evaluation metrics (macro-F1, accuracy, Pearson correlation)
  - Why needed here: To correctly interpret performance comparisons across models and tasks
  - Quick check question: When should macro-F1 be preferred over accuracy in multi-class or multi-label settings?

## Architecture Onboarding

- Component map: Dataset → Prompt design → Model execution (GPT-3.5/4 or PubMedBERT) → Post-processing → Error analysis → Benchmarking
- Critical path: Dataset sampling → Prompt construction → Model inference → Metric calculation → Error categorization
- Design tradeoffs: Cost vs coverage (sampling 200 instances vs full test sets), zero-shot vs fine-tuning accuracy, general vs domain-specific models
- Failure signatures: Missing entities in NER, inconsistent output formats, hallucination of new labels, high variance across runs
- First 3 experiments:
  1. Compare zero-shot vs one-shot performance on a single dataset (e.g., PubMedQA) to measure prompting impact
  2. Re-run NER task with constrained prompts to test if missing entity errors decrease
  3. Evaluate the effect of increasing demonstration count from 1 to 3 on a relation extraction task to confirm plateau behavior

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LLMs in biomedical NER tasks improve with more sophisticated prompt engineering and learning strategies? The paper only evaluated zero-shot and one-shot learning with uniform prompt templates, acknowledging that more sophisticated approaches could yield better results.

### Open Question 2
What are the specific factors contributing to the prevalence of missing information, inconsistencies, and hallucinations in LLM outputs for biomedical applications? While the paper identifies these issues and categorizes them, it does not provide a detailed analysis of their underlying causes.

### Open Question 3
How can the data and evaluation paradigms in BioNLP be adapted to better suit the capabilities of LLMs and provide a fairer assessment of their performance? The paper suggests current evaluation settings are tailored to supervised methods and may not be fair for LLMs, but does not provide specific recommendations for adaptation.

## Limitations
- Evaluation relies on sampling only 200 instances per dataset, which may not fully represent task complexity
- Specific prompt templates used for each task are not provided in detail
- Study lacks statistical significance testing between model performances

## Confidence

- High confidence: Traditional fine-tuning of domain-specific models (PubMedBERT) outperforms zero-shot/few-shot LLMs for information extraction and classification tasks
- Medium confidence: Prompt engineering (zero-shot vs one-shot) significantly affects LLM performance but gains plateau beyond one demonstration
- Medium confidence: LLMs generate inconsistent outputs and hallucinations that undermine reliability

## Next Checks

1. Conduct statistical significance testing (e.g., paired t-tests or bootstrap confidence intervals) on performance differences between fine-tuned PubMedBERT and LLM approaches
2. Expand prompt engineering experiments to test multiple demonstration counts (1, 3, 5, 10) on at least two different task types to validate plateau effect
3. Implement automated consistency checking for LLM outputs across all 12 benchmarks to quantify frequency and types of inconsistencies, missing information, and hallucinations