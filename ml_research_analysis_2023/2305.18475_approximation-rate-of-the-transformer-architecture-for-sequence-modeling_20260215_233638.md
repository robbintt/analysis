---
ver: rpa2
title: Approximation Rate of the Transformer Architecture for Sequence Modeling
arxiv_id: '2305.18475'
source_url: https://arxiv.org/abs/2305.18475
tags:
- approximation
- transformer
- sequence
- such
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how well transformers can approximate sequence-to-sequence
  relationships. It establishes a universal approximation theorem for single-layer
  transformers and derives explicit Jackson-type approximation rates for two-layer
  transformers under a novel regularity measure.
---

# Approximation Rate of the Transformer Architecture for Sequence Modeling

## Quick Facts
- arXiv ID: 2305.18475
- Source URL: https://arxiv.org/abs/2305.18475
- Reference count: 40
- Primary result: Establishes universal approximation theorem and Jackson-type rates for transformers, revealing structural bias toward graph-based operations

## Executive Summary
This paper provides the first comprehensive analysis of transformer approximation capabilities for sequence-to-sequence modeling. It establishes theoretical foundations showing transformers are structurally biased toward approximating functions that construct implicit graphs from input sequences and perform simple mixing operations on them. The analysis reveals why transformers excel in tasks like NLP where temporal ordering is less critical, but struggle with time series forecasting that relies heavily on temporal dependencies. The paper derives explicit Jackson-type approximation rates that depend on the regularity of the attention mechanism, quantified through singular value decay rates.

## Method Summary
The paper analyzes transformer approximation through theoretical derivations of universal approximation theorems and Jackson-type error bounds. For single-layer transformers, it establishes density results showing they can approximate any continuous sequence-to-sequence function. For two-layer transformers, it derives explicit error bounds showing approximation error decreases as k^(-2α-1), where k is model complexity and α measures the regularity of the attention function through its singular value decay. The analysis focuses on simplified transformers without layer normalization and assumes certain regularity conditions on activation functions. Experiments use synthetic datasets generated from linear functionals to validate theoretical predictions about approximation rates and structural bias.

## Key Results
- Transformers can universally approximate any continuous sequence-to-sequence function with two layers
- Jackson-type approximation rates reveal transformers are biased toward graph-construction operations
- Approximation error decreases as k^(-2α-1) where α measures attention function regularity
- Transformers learn the same relationship under permutation by modifying feed-forward layers, not attention
- Theoretical predictions about structural bias are validated on synthetic datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers are structurally biased to approximate functions that construct implicit graphs from input sequences and perform simple mixing operations on them
- Mechanism: The approximation rate estimate shows that transformer performance depends on how well the attention mechanism can approximate the function G(u,v) using low-rank decomposition. When G has rapidly decaying singular values, the approximation error decreases as mh^(-2α-1), where mh controls the rank of the attention approximation
- Core assumption: The target function can be decomposed into F(P_s σ(G(x(t),x(s)))ρ(x(s))), where F and ρ are approximated by feed-forward networks and G is approximated by the attention mechanism
- Evidence anchors:
  - [abstract]: "This rate reveals the structural properties of the Transformer and suggests the types of sequential relationships it is best suited for approximating"
  - [section]: "The key idea to prove the density result is to observe that H(2) contains functions of the form...which is obtained by setting Wo in the first layer to zero"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.385, average citations=0.0. Weak evidence for this specific mechanism.

### Mechanism 2
- Claim: The attention mechanism learns a function that constructs sequence-to-graph relationships
- Mechanism: The attention computes weighted sums over sequence elements, effectively creating a stochastic matrix G(x) where each entry represents edge weights between sequence positions. This matrix can be interpreted as describing a weighted graph
- Core assumption: The attention matrix after softmax normalization can be viewed as edge weights in a graph structure
- Evidence anchors:
  - [abstract]: "These are formed by a graph-construction process from an input sequence, followed by a simple mixing of a element-wise-in-time function on this graph"
  - [section]: "Let G(x) be a τ × τ matrix such that [G(x)]t,s = σ(G(x(t), x(s))). Observe that G(x) is a stochastic matrix, and can be viewed as describing a weighted graph"
  - [corpus]: Weak evidence for this specific graph construction mechanism.

### Mechanism 3
- Claim: Transformers do not capture temporal ordering even with positional encoding
- Mechanism: Despite positional encoding eliminating permutation equivariance, the model still learns the permuted relationship by modifying the feed-forward layer. The graph-construction function G remains the same under permutation, so the temporal order is not encoded in the learned attention patterns
- Core assumption: Positional encoding only ensures distinct inputs but doesn't force the model to learn temporal dependencies
- Evidence anchors:
  - [abstract]: "In particular, it allows us to concretely discuss the structural bias between the transformer and classical sequence modeling methods, such as recurrent neural networks"
  - [section]: "Let p be a fixed permutation...we show that the transformer (with positional encoding) can learn both relationships in the same way by modifying the first pointwise feed-forward layer"
  - [corpus]: Weak evidence for this specific temporal ordering claim.

## Foundational Learning

- Concept: Jackson-type approximation rates
  - Why needed here: Provides quantitative error bounds showing how approximation error decreases with model complexity, revealing structural biases
  - Quick check question: What does the decay rate k^(-2α-1) tell us about the target function's regularity?

- Concept: Singular value decomposition for two-variable functions (POD)
  - Why needed here: Used to decompose the attention function G(u,v) and measure its effective rank, which determines how well transformers can approximate it
  - Quick check question: How does the rank of G(u,v) relate to the required hidden dimension mh for accurate approximation?

- Concept: Permutation equivariance
  - Why needed here: Understanding why transformers need positional encoding and why they still fail to capture temporal order despite it
  - Quick check question: Why does adding positional encoding eliminate permutation equivariance but not ensure temporal order learning?

## Architecture Onboarding

- Component map: Input layer → Position encoding → Multi-head attention (WQ, WK, WV) → Feed-forward network → Output layer
- Critical path: Input → Position encoding → Attention computation (query-key dot products, softmax, weighted sum) → Feed-forward transformation → Output
- Design tradeoffs:
  - Higher mh improves attention approximation but increases computational cost
  - Position encoding must ensure disjoint input domains but doesn't guarantee temporal order learning
  - Two-layer architecture sufficient for universal approximation but may need depth for complex tasks
- Failure signatures:
  - Poor performance on tasks with strong temporal dependencies (RNNs outperform)
  - Good performance on tasks where relationships can be expressed as graph operations (NLP, image classification)
  - Training error decreases slowly when target function has slowly decaying singular values
- First 3 experiments:
  1. Train on sequence-to-sequence task with known graph structure (like gravity simulation) and visualize attention patterns
  2. Compare transformer vs RNN performance on permuted vs original temporal sequences
  3. Train on targets with different singular value decay rates and measure error vs mh relationship

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the approximation rates change when using multi-headed attention in transformers compared to single-head attention?
- Basis in paper: [explicit] The paper discusses Jackson-type approximation rates for single-layer transformers with one head, but does not explore the impact of multi-headed attention.
- Why unresolved: The paper focuses on single-head attention and does not provide analysis or experimental results for multi-headed attention.
- What evidence would resolve it: Conducting experiments and theoretical analysis comparing approximation rates between single-head and multi-headed attention transformers.

### Open Question 2
- Question: What are the effects of adding layer normalization to transformers on their approximation capabilities?
- Basis in paper: [inferred] The paper simplifies transformers by excluding layer normalization, but does not explore how its inclusion might affect approximation rates.
- Why unresolved: The paper explicitly excludes layer normalization from its analysis, leaving its impact on approximation rates unexplored.
- What evidence would resolve it: Experiments comparing approximation rates of transformers with and without layer normalization.

### Open Question 3
- Question: How do transformers perform in approximating sequence-to-sequence relationships with strong temporal dependencies compared to other architectures?
- Basis in paper: [explicit] The paper discusses that transformers struggle with tasks heavily relying on temporal dependencies, unlike RNNs, but does not provide detailed comparative analysis.
- Why unresolved: The paper mentions the limitation of transformers in handling temporal dependencies but does not provide extensive comparative experiments or theoretical analysis.
- What evidence would resolve it: Comparative studies and experiments analyzing the performance of transformers and other architectures on tasks with varying levels of temporal dependency.

### Open Question 4
- Question: What is the impact of increasing the depth of transformers on their approximation rates and structural bias?
- Basis in paper: [inferred] The paper focuses on single-layer and two-layer transformers, but does not explore how deeper architectures might affect approximation rates.
- Why unresolved: The analysis is limited to shallow transformers, leaving the effects of deeper architectures unexplored.
- What evidence would resolve it: Experiments and theoretical analysis on the approximation rates and structural biases of deeper transformers.

## Limitations
- Theoretical analysis relies on abstract regularity measures that may not be computable for real-world tasks
- Jackson-type rates provide asymptotic guarantees but don't predict finite-sample performance
- Structural bias claims are based on theoretical analysis rather than extensive empirical validation
- Comparison with RNNs limited to specific synthetic datasets

## Confidence
- High confidence: Universal approximation theorem for single-layer transformers and density result for two-layer transformers are mathematically rigorous
- Medium confidence: Jackson-type approximation rates are derived rigorously but depend on assumptions about singular value decay
- Low confidence: Specific numerical values of approximation rates in experiments may vary significantly with different implementations

## Next Checks
1. Generate synthetic target functions with varying singular value decay rates and measure actual MSE vs mh relationship during training to validate theoretical k^(-2α-1) rate prediction
2. Create a spectrum of sequence tasks from purely graph-based to purely temporal and measure transformer vs RNN performance to validate structural bias claims
3. For gravity simulation experiment, visualize learned attention patterns and verify they correspond to physical relationships, testing whether attention patterns change meaningfully under permutation