---
ver: rpa2
title: Federated Learning of Gboard Language Models with Differential Privacy
arxiv_id: '2305.18465'
source_url: https://arxiv.org/abs/2305.18465
tags:
- privacy
- learning
- training
- rounds
- report
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes the deployment of differentially private (DP)
  language models in Google Keyboard (Gboard) using federated learning (FL). The authors
  apply DP-FTRL, a state-of-the-art DP algorithm, to train next-word prediction (NWP)
  and on-the-fly rescoring (OTF) models across multiple languages without requiring
  uniform sampling of client devices.
---

# Federated Learning of Gboard Language Models with Differential Privacy

## Quick Facts
- arXiv ID: 2305.18465
- Source URL: https://arxiv.org/abs/2305.18465
- Reference count: 16
- Key outcome: Deployment of DP language models in Gboard using DP-FTRL without uniform client sampling

## Executive Summary
This paper describes the deployment of differentially private (DP) language models in Google Keyboard (Gboard) using federated learning (FL). The authors apply DP-FTRL, a state-of-the-art DP algorithm, to train next-word prediction (NWP) and on-the-fly rescoring (OTF) models across multiple languages without requiring uniform sampling of client devices. They introduce adaptive clipping integrated with DP-FTRL to automatically tune the clipping norm during training and reduce hyperparameter tuning. By combining DP with secure aggregation (SecAgg) for two models, they strengthen data minimization. Pretraining on public data is used to significantly improve utility and reduce the number of training rounds, enhancing privacy guarantees. The system uses large report goals (6500) and controlled client participation (via timer-based min-separation) to achieve strong privacy. Over twenty models are trained and deployed with formal DP guarantees (ρ-zCDP in range (0.2, 2)), and all future Gboard neural network LMs will require DP. The approach demonstrates that practical DP training is achievable at scale when supported by large-scale FL infrastructure.

## Method Summary
The paper implements DP-FTRL with adaptive clipping for training Gboard language models in a federated setting. The method uses tree-based noise accumulation across rounds, allowing privacy amplification without requiring uniform client sampling. Adaptive clipping estimates the norm of model updates at a target quantile privately and adjusts the clipping threshold during training. Large report goals (6500) are used to achieve strong privacy guarantees, with client participation controlled through timer-based min-separation to limit participation frequency. Public pretraining on the C4 dataset is used to improve convergence and reduce the number of training rounds needed. Secure aggregation is applied to two models to further enhance privacy. The system achieves ρ-zCDP guarantees in the range (0.2, 2) across more than twenty deployed models.

## Key Results
- Successfully deployed over twenty differentially private language models in Gboard across multiple languages
- Achieved strong privacy guarantees (ρ-zCDP in range (0.2, 2)) without requiring uniform client sampling
- Demonstrated that pretraining on public data significantly improves utility and reduces privacy cost by decreasing training rounds
- Implemented adaptive clipping that reduces hyperparameter tuning while maintaining comparable or slightly better utility than fixed clipping

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DP-FTRL enables strong privacy without uniform client sampling
- Mechanism: The algorithm accumulates noise across rounds using tree-based summation, allowing privacy amplification even when client participation is non-uniform and unpredictable
- Core assumption: Client participation is limited in frequency and can be bounded by timer-based separation
- Evidence anchors: [abstract] "We apply the DP-Follow-the-Regularized-Leader (DP-FTRL) [Kairouz et al., 2021b] algorithm to achieve meaningfully formal DP guarantees without requiring uniform sampling of client devices."
- Break condition: If clients participate too frequently or in large numbers without enforced separation, the privacy guarantees weaken beyond acceptable bounds

### Mechanism 2
- Claim: Adaptive clipping with quantile estimation reduces hyperparameter tuning and maintains utility
- Mechanism: Private estimation of model update norms at a target quantile adjusts the clipping threshold during training, preventing catastrophic failures from overly aggressive clipping
- Core assumption: The quantile-based estimate stabilizes over training and is insensitive to moderate changes in the server learning rate
- Evidence anchors: [section] "Andrew et al. [2021] introduced an adaptive clipping method that automatically adjusts the clip norm each round by privately estimating the norm of the model delta at a targeted quantile."
- Break condition: If the quantile estimate is unstable or the noise in the norm estimation is too high, the adaptive clipping may underperform compared to fixed clipping

### Mechanism 3
- Claim: Public pretraining reduces the number of training rounds needed, thereby strengthening privacy guarantees
- Mechanism: Models pretrained on public multilingual C4 data converge faster on user data, reducing T and thus the total privacy cost
- Core assumption: The pretrained model's feature space is relevant to the downstream Gboard task and can be fine-tuned effectively
- Evidence anchors: [section] "With the help of pretraining on public data, we train and deploy more than twenty Gboard LMs that achieve high utility and ρ−zCDP privacy guarantees."
- Break condition: If the pretraining corpus is not representative or the fine-tuning phase is unstable, the utility gains—and privacy benefits—may not materialize

## Foundational Learning

- Concept: Differential Privacy (DP) fundamentals
  - Why needed here: The paper's core contribution is deploying DP-FTRL at scale; understanding DP definitions, zCDP, and privacy accounting is essential to interpret results
  - Quick check question: What is the difference between (ε, δ)-DP and ρ-zCDP, and why is zCDP preferred here?

- Concept: Federated Learning (FL) client participation dynamics
  - Why needed here: The algorithm relies on bounding client participation frequency; knowing how FL systems control eligibility (e.g., via timers) is critical
  - Quick check question: How does the "min-separation" parameter affect the maximum number of times a client can participate?

- Concept: Tree-based noise accumulation in DP-FTRL
  - Why needed here: DP-FTRL's privacy accounting depends on accumulating Gaussian noise over rounds; misunderstanding this breaks the privacy analysis
  - Quick check question: Why does tree-based accumulation allow privacy amplification without uniform sampling?

## Architecture Onboarding

- Component map:
  - Client-side: Local training, gradient clipping, participation timer, secure aggregation (optional)
  - Server-side: Model aggregation, noise addition, momentum update, privacy accounting, hyperparameter tuning
  - Data: Public pretraining corpus (C4), user-generated data split by language/task
  - Privacy: DP-FTRL aggregator, adaptive clipping estimator, timer enforcement, zCDP accounting

- Critical path:
  1. Pretrain model on public data
  2. Initialize DP-FTRL with noise multiplier and report goal
  3. Enforce client participation limits via timer
  4. Run adaptive clipping (or fixed clipping after estimation)
  5. Aggregate updates with SecAgg if enabled
  6. Apply privatized update and momentum
  7. Monitor utility and privacy; deploy when thresholds met

- Design tradeoffs:
  - Large report goal → stronger privacy but slower per-round aggregation and higher SecAgg cost
  - High noise multiplier → better privacy but potential utility drop; must be tuned per task
  - Pretraining → fewer rounds but requires public data relevance and stable fine-tuning

- Failure signatures:
  - Accuracy plateaus early → check clipping norm, learning rate, or pretraining quality
  - Privacy cost exceeds target → reduce rounds, increase report goal, or adjust timer period
  - SecAgg too slow → consider disabling for non-sensitive tasks or compressing updates

- First 3 experiments:
  1. Simulate DP-FTRL with small report goal on public data to tune noise multiplier and clip norm
  2. Run adaptive clipping with quantile γ=0.5 to estimate stable clip norm for fixed clipping
  3. Deploy full-scale training with large report goal and timer-based MinS; validate MinS enforcement via post-processing logs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the convergence of federated learning with differential privacy be improved when using large report goals?
- Basis in paper: [explicit] The paper mentions that large report goals are used to achieve strong privacy guarantees, but also notes that secure aggregation is notably slower with large report goals
- Why unresolved: The paper does not provide a detailed analysis of how large report goals affect the convergence of federated learning with differential privacy
- What evidence would resolve it: Experimental results showing the impact of different report goals on the convergence of federated learning with differential privacy

### Open Question 2
- Question: What are the trade-offs between using adaptive clipping and fixed clipping in differential privacy for federated learning?
- Basis in paper: [explicit] The paper introduces an algorithm that enables adaptive clipping in DP-FTRL, but also notes that a fixed clip norm can achieve comparable or slightly better model utility
- Why unresolved: The paper does not provide a detailed comparison of the trade-offs between adaptive clipping and fixed clipping
- What evidence would resolve it: Experimental results comparing the performance of adaptive clipping and fixed clipping in terms of utility and privacy guarantees

### Open Question 3
- Question: How can the privacy guarantees of federated learning with differential privacy be further improved without sacrificing utility?
- Basis in paper: [explicit] The paper discusses various factors that affect privacy guarantees, such as noise multiplier, number of rounds, and client participation
- Why unresolved: The paper does not provide a comprehensive analysis of how to optimize these factors to improve privacy guarantees without sacrificing utility
- What evidence would resolve it: Theoretical analysis or experimental results showing how to optimize the factors mentioned in the paper to improve privacy guarantees without sacrificing utility

## Limitations

- The use of zCDP as the primary privacy accounting framework provides weaker per-example guarantees than pure (ε, δ)-DP formulations
- Strong privacy guarantees are contingent on maintaining stated client participation constraints through timer-based separation, which may be difficult to verify in practice across diverse user populations
- The adaptive clipping mechanism, while effective in practice, relies on private norm estimation that introduces additional noise and potential instability

## Confidence

- High confidence in the deployment methodology and practical implementation details
- Medium confidence in the privacy guarantees given the reliance on zCDP accounting and participation assumptions
- Medium confidence in the utility claims due to limited comparative analysis with non-private baselines

## Next Checks

1. Verify client participation frequency constraints across different language populations to ensure min-separation requirements are consistently met in production
2. Conduct ablation studies comparing adaptive clipping with fixed clipping at various quantile settings to quantify the utility-privacy tradeoff
3. Replicate the privacy accounting using alternative frameworks (e.g., Rényi DP or pure DP) to cross-validate the zCDP-based guarantees