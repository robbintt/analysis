---
ver: rpa2
title: 'Concept backpropagation: An Explainable AI approach for visualising learned
  concepts in neural network models'
arxiv_id: '2307.12601'
source_url: https://arxiv.org/abs/2307.12601
tags:
- concept
- neural
- input
- network
- given
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces concept backpropagation, an explainable AI
  method that extends concept detection to visualize how neural networks internalize
  learned concepts. The approach uses pre-trained concept probes to guide perturbations
  of input data, maximizing the representation of a target concept in the model's
  intermediate layers.
---

# Concept backpropagation: An Explainable AI approach for visualising learned concepts in neural network models

## Quick Facts
- arXiv ID: 2307.12601
- Source URL: https://arxiv.org/abs/2307.12601
- Reference count: 11
- Primary result: Introduces concept backpropagation to visualize how neural networks internalize learned concepts through guided perturbations

## Executive Summary
Concept backpropagation extends concept detection methods to visualize learned concepts in neural networks by using pre-trained concept probes to guide perturbations of input data. The method finds minimal changes to inputs that maximize the representation of target concepts in intermediate layers, providing interpretability for complex models without requiring retraining. Applied across tabular data, images, and chess domains, it reveals both direct concept representations and entangled relationships with other features, though chess presents particular challenges due to its discrete input space.

## Method Summary
The method uses pre-trained concept probes to guide gradient-based optimization that finds input perturbations maximizing concept representation in neural network intermediate layers. For each input sample, perturbations are computed to minimize the distance between the probe's output and a target concept value while keeping input changes small. The approach adapts to different input domains through domain-specific perturbation operators and distance functions - direct addition for tabular data, learned embedding spaces for images, and constrained binary perturbations for chess. The method balances concept maximization against input preservation through tunable parameters λ1 and λ2.

## Key Results
- Successfully applied to tabular data, revealing entangled feature representations like bedrooms per person correlating with rooms per person
- Extended to images using autoencoder embedding spaces to avoid high-dimensional noise issues
- Demonstrated on chess data, though challenges remain in maintaining valid positions while maximizing concepts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concept backpropagation uses trained concept probes as gradient guides to find input perturbations that maximize concept representation in neural networks.
- Mechanism: The method sets up a minimization problem where the objective is to minimize the distance between the probe's output and a target concept value while keeping input perturbations small. This allows the method to find minimal changes to inputs that maximize the desired concept.
- Core assumption: The concept probe has learned a meaningful and accurate mapping from the model's intermediate layer to the concept space.
- Evidence anchors:
  - [abstract] "the model input is perturbed in a manner guided by a trained concept probe for the described model, such that the concept of interest is maximised"
  - [section II-B] "we wish to find a minimal perturbation s* so that P(L(s ⊙ s*)) = o"
  - [corpus] Weak evidence - corpus neighbors focus on concept-based XAI but don't specifically discuss concept backpropagation as a method
- Break condition: If the concept probe does not accurately represent the concept in the model's intermediate layer, the gradients will not effectively guide the search for meaningful perturbations.

### Mechanism 2
- Claim: The method can handle different input spaces by adapting the perturbation operator and distance function.
- Mechanism: For tabular data, perturbations are direct additions to input vectors. For images, perturbations occur in a learned embedding space to avoid high-dimensional noise issues. For chess, perturbations are constrained to maintain legal board positions.
- Core assumption: Appropriate perturbation operators and distance functions can be defined for each input domain.
- Evidence anchors:
  - [section II-C] "For a model M: I → O, with an intermediate layer L(s) and a trained logistic probe P: L → C, an input state s, a concept output o as the desired output of the concept function f(·), and some combination operator ⊙, we wish to find a minimal perturbation s* so that P(L(s ⊙ s*)) = o"
  - [section II-C-1] "For this case, the distance function is defined as dist(s, s*) = ||s*||2, and ⊙ as standard, element-wise addition"
  - [section II-C-2] "This latent space then serves as the de facto input space for concept maximisation"
  - [corpus] Weak evidence - corpus neighbors discuss concept visualization but not the specific adaptation to different input spaces
- Break condition: If the chosen perturbation operator and distance function do not preserve meaningful structure in the input space, the resulting maximizations will not be interpretable.

### Mechanism 3
- Claim: The method reveals concept entanglement by showing how multiple features contribute to concept representation.
- Mechanism: By maximizing a concept and observing which input features change, the method can identify when concepts are represented using multiple entangled features rather than independently.
- Core assumption: Changes to input features that maximize concept representation reveal the model's internal representation strategy.
- Evidence anchors:
  - [section V] "it is observed that the presented method is suitable for uncovering how entangled features can affect how a given model internalise concepts"
  - [section V] "When the model is tasked with maximising the ratio of the average amount of bedrooms per person with regard to its intermediate activation space, it also increases the average amount of rooms per person"
  - [corpus] Weak evidence - corpus neighbors discuss concept entanglement but not through perturbation-based visualization
- Break condition: If the model uses highly non-linear or distributed representations that don't correspond to individual feature changes, the perturbation approach may not reveal the true nature of concept entanglement.

## Foundational Learning

- Concept: Gradient-based optimization and backpropagation
  - Why needed here: The method relies on computing gradients of the concept probe output with respect to input perturbations to guide the search for concept-maximizing inputs.
  - Quick check question: How does backpropagation work in neural networks to compute gradients for optimization?

- Concept: Concept detection and concept probes
  - Why needed here: The method builds on concept detection by using pre-trained concept probes to guide the search for concept-maximizing inputs.
  - Quick check question: What is a concept probe and how is it trained to detect concepts in neural network intermediate layers?

- Concept: Distance metrics and perturbation operators
  - Why needed here: Different input spaces require different ways to measure the "distance" between original and perturbed inputs, and different operators to combine them.
  - Quick check question: Why might element-wise addition be appropriate for tabular data but not for images or chess positions?

## Architecture Onboarding

- Component map: Pre-trained model -> Intermediate layer L -> Concept probe P -> Perturbation operator ⊙ -> Distance function dist(·, ·) -> Optimization loop

- Critical path: 1. Load pre-trained model and concept probe 2. For each input sample, initialize perturbation 3. Compute probe output and gradients 4. Update perturbation using gradient descent 5. Repeat until convergence or max iterations

- Design tradeoffs:
  - λ1 vs λ2 weighting: Higher λ1 emphasizes concept maximization, higher λ2 preserves input structure
  - Embedding space vs direct space: Embedding spaces can simplify perturbations but add complexity
  - Legal constraint vs unconstrained: For chess, legal constraints ensure valid positions but may limit concept maximization

- Failure signatures:
  - Concept probe gradients don't effectively guide search (probe not well-trained)
  - Perturbations become unrealistic or uninterpretable (wrong distance function/operator)
  - Method fails to converge (learning rate too high/low, λ weights poorly chosen)

- First 3 experiments:
  1. Apply to simple tabular regression model with known concept relationships to verify method works
  2. Test different λ1/λ2 weightings on image autoencoder to understand tradeoff between concept maximization and input preservation
  3. Apply to chess model with different legal constraint strengths to find balance between validity and concept maximization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the concept backpropagation method be effectively adapted to handle discrete input spaces like chess without introducing irrelevant perturbations?
- Basis in paper: [explicit] The paper discusses challenges in applying the method to chess due to its discrete input space and the need for perturbations to adhere to chess rules.
- Why unresolved: The paper suggests that the method works for chess but introduces irrelevant pieces, indicating a need for better handling of discrete spaces.
- What evidence would resolve it: Successful application of the method to chess without introducing irrelevant perturbations, demonstrating improved handling of discrete input spaces.

### Open Question 2
- Question: What is the optimal balance between maximizing the target concept and preserving the original structure of input samples in the concept backpropagation method?
- Basis in paper: [explicit] The paper discusses the difficulty in finding the right balance between highlighting a concept and preserving the original input structure, particularly in tuning λ1 and λ2.
- Why unresolved: The paper provides examples of different weightings leading to varying results, but does not establish a clear method for optimal tuning.
- What evidence would resolve it: A systematic study establishing guidelines for tuning λ1 and λ2 to achieve optimal balance in various applications.

### Open Question 3
- Question: How does the concept backpropagation method perform when applied to more complex architectures like large language models (LLMs)?
- Basis in paper: [explicit] The paper mentions that applying the method to problems of higher complexity, such as LLMs, would be an interesting future work.
- Why unresolved: The paper only applies the method to simpler models and does not explore its effectiveness on complex architectures like LLMs.
- What evidence would resolve it: Successful application of the method to LLMs, demonstrating its effectiveness in visualizing concepts in complex architectures.

## Limitations

- Limited validation of concept probe quality across different domains and model architectures
- Challenges in handling discrete input spaces like chess while maintaining valid positions
- Requires careful tuning of λ1 and λ2 weights with unclear generalization across applications

## Confidence

Medium confidence due to uncertainties in probe quality validation, scalability to complex architectures, and optimal parameter tuning.

## Next Checks

1. **Probe Quality Validation**: Systematically evaluate concept probe accuracy across different domains and model architectures to establish reliability baselines for the backpropagation approach.

2. **Cross-Domain Scalability**: Test the method on larger, more complex neural network architectures (e.g., transformers, large language models) to assess scalability and identify domain-specific limitations.

3. **Concept Entanglement Analysis**: Conduct controlled experiments to verify whether the method accurately reveals true concept entanglement versus coincidental feature correlations, particularly in high-dimensional input spaces.