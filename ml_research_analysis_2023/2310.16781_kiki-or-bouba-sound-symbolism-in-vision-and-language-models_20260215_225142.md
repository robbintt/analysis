---
ver: rpa2
title: Kiki or Bouba? Sound Symbolism in Vision-and-Language Models
arxiv_id: '2310.16781'
source_url: https://arxiv.org/abs/2310.16781
tags:
- sound
- these
- kiki
- bouba
- pseudowords
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether sound symbolism\u2014a non-arbitrary\
  \ relationship between speech sounds and meanings\u2014is reflected in vision-and-language\
  \ models such as CLIP and Stable Diffusion. The authors construct pseudowords with\
  \ controlled phonetic properties and evaluate their visual semantics using zero-shot\
  \ probing."
---

# Kiki or Bouba? Sound Symbolism in Vision-and-Language Models

## Quick Facts
- arXiv ID: 2310.16781
- Source URL: https://arxiv.org/abs/2310.16781
- Authors: 
- Reference count: 40
- Primary result: VLMs learn sound symbolic associations, discriminating between "sharp" and "round" pseudowords with AUC scores of 0.77-0.74

## Executive Summary
This paper investigates whether sound symbolism—non-arbitrary relationships between speech sounds and meanings—is reflected in vision-and-language models such as CLIP and Stable Diffusion. The authors construct pseudowords with controlled phonetic properties and evaluate their visual semantics using zero-shot probing. They find that these models associate pseudowords with "sharp" or "round" visual properties, paralleling the well-known kiki-bouba effect in psycholinguistics. Quantitative results show strong discrimination between pseudoword classes using geometric and phonetic scores, with CLIP achieving AUC 0.77 and Stable Diffusion AUC 0.74. A user study confirms that humans can match pseudowords to generated images, with 73% accuracy for kiki-bouba and 55% for random pseudowords.

## Method Summary
The authors construct pseudowords following a (CV)1(CV)2(CV)1 template using graphemes from controlled phonetic classes. They generate images using Stable Diffusion with prompts "a 3D rendering of a ⟨w⟩shaped object" for each pseudoword. Geometric scores are calculated by embedding images and adjectives in CLIP space and projecting onto the semantic direction distinguishing "sharp" vs "round" adjectives. Phonetic scores are computed by comparing pseudoword embeddings to the dimension distinguishing sharp vs round pseudoword classes. The method uses zero-shot knowledge probing through CLIP's multimodal embedding space, evaluating results with ROC-AUC and Kendall correlation. A user study with MTurk participants matches image pairs to pseudoword pairs, analyzed with mixed-effects logistic regression.

## Key Results
- CLIP achieves AUC 0.77 for discriminating between sharp and round pseudowords using geometric scores
- Stable Diffusion achieves AUC 0.74 for the same task using geometric scores
- Human participants match images to pseudowords with 73% accuracy for kiki-bouba and 55% for random pseudowords
- Voiceless consonants consistently score higher for "sharp" properties across all evaluation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs learn sound-symbolic associations from multimodal training data
- Mechanism: During pretraining on captioned images, VLMs implicitly learn statistical correlations between certain phonetic structures in text and visual properties in images
- Core assumption: Captioned image datasets contain enough implicit sound-symbolic mappings to be picked up by the training process
- Evidence anchors: [abstract] "recent years have seen explosive progress in the field of machine learning applied to natural language and vision, mainly powered by transformer neural networks and training on web-scale datasets of captioned images"
- Break Condition: If training data lacks sufficient sound-symbolic correlations, VLMs cannot learn these associations

### Mechanism 2
- Claim: CLIP's shared embedding space allows probing of visual semantics from pseudowords
- Mechanism: Pseudowords are embedded in CLIP's shared text-image space, and their proximity to adjectives indicating visual properties reveals learned associations
- Core assumption: CLIP's embedding space meaningfully represents both textual and visual semantics in a shared manifold
- Evidence anchors: [abstract] "We leverage the ability of CLIP to embed text and image data in a shared semantic space"
- Break Condition: If CLIP's embedding space doesn't capture relevant visual-semantic relationships, probing will fail

### Mechanism 3
- Claim: Stable Diffusion inherits sound-symbolic knowledge through its CLIP text encoder
- Mechanism: Stable Diffusion uses OpenCLIP as its text encoder for classifier-free guidance, so it inherits CLIP's learned associations between pseudowords and visual properties
- Core assumption: The CLIP text encoder sufficiently captures sound-symbolic patterns for them to transfer to the generative model
- Evidence anchors: [abstract] "Large diffusion models such as DALL-E 2 [56] and Imagen [61] set a new standard for general text conditioned image generation"
- Break Condition: If Stable Diffusion's generation process modifies or overrides CLIP's learned associations, the transfer won't occur

## Foundational Learning

- Concept: Cross-modal associations between language and visual domain
  - Why needed here: The paper's core contribution is demonstrating that VLMs learn non-arbitrary mappings between speech sounds and visual properties
  - Quick check question: Can you explain what the kiki-bouba effect demonstrates about human cognition?

- Concept: Zero-shot knowledge probing
  - Why needed here: The methodology relies on evaluating models' inherent knowledge without additional training
  - Quick check question: What's the difference between zero-shot and few-shot evaluation paradigms?

- Concept: Geometric vs. phonetic scoring in embedding spaces
  - Why needed here: The paper introduces two complementary methods for measuring associations in CLIP space
  - Quick check question: How would you distinguish between an embedding's proximity to "round" vs. "sharp" adjectives?

## Architecture Onboarding

- Component map: CLIP (text encoder, image encoder, shared embedding space) → Stable Diffusion (text encoder, UNet, VAE) → Image generations
- Critical path: Pseudoword → CLIP embedding → Geometric/phonetic score → Statistical analysis
- Design tradeoffs: Using CLIP's shared space enables direct comparison but may introduce bias from CLIP's specific training
- Failure signatures: Random performance (AUC ~0.5) indicates lack of learned associations; poor human correlation suggests model-human misalignment
- First 3 experiments:
  1. Replicate kiki-bouba effect using CLIP embeddings only
  2. Generate images with Stable Diffusion and measure geometric scores
  3. Conduct human user study comparing model predictions to human preferences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multimodal models like CLIP and Stable Diffusion infer sound symbolic associations from their training data?
- Basis in paper: [explicit] The authors note that their results are agnostic to whether the models have memorized associations between particular letters and shapes or have a deeper understanding of the phonetic or acoustic properties implied by these letters.
- Why unresolved: The paper focuses on demonstrating the presence of sound symbolic patterns in the models but does not investigate the mechanisms by which these associations are learned.
- What evidence would resolve it: Further research could analyze the internal representations of the models to determine if they encode phonetic or acoustic properties, or if the associations are based on memorization of letter-shape correlations.

### Open Question 2
- Question: Are sound symbolic associations learned by multimodal models culturally universal or specific to the English language?
- Basis in paper: [inferred] The authors use English pseudowords and prompts in their experiments, and while they mention probing a multilingual text-to-image model, they do not directly address the universality of sound symbolism across languages.
- Why unresolved: The paper focuses on English language models and does not investigate sound symbolic patterns in other languages or cultures.
- What evidence would resolve it: Experiments using multilingual models with prompts in diverse languages could reveal if sound symbolic associations are learned consistently across languages or if they are specific to English.

### Open Question 3
- Question: Do unimodal text-only models also learn sound symbolic associations from text data alone, or is visual data necessary for this phenomenon?
- Basis in paper: [explicit] The authors mention that they provide results for unimodal text encoder models, which show mixed results when probed for sound symbolism.
- Why unresolved: The paper does not thoroughly investigate the role of visual data in learning sound symbolic associations and leaves this question for future work.
- What evidence would resolve it: Comparing the performance of unimodal text-only models with multimodal models on sound symbolic tasks could reveal the relative contributions of textual and visual data to this phenomenon.

## Limitations

- The paper cannot distinguish whether VLMs learn genuine phonetic-visual mappings or memorize letter-shape correlations from training data
- Results are based on English pseudowords and may not generalize to other languages or cultures
- The methodology relies on CLIP's specific embedding space, which may introduce biases from its particular training process

## Confidence

*High Confidence (High)*: The methodological framework for measuring sound symbolic associations in VLMs is sound. The use of CLIP's shared embedding space for zero-shot probing is well-established, and the geometric/phonetic scoring methods provide complementary measures of association strength.

*Medium Confidence (Medium)*: The experimental results showing discrimination between pseudoword classes are statistically significant, but the effect sizes vary considerably across methods. The human user study provides convergent evidence but shows substantial variance in participant performance.

*Low Confidence (Low)*: The interpretation that VLMs learn the same type of sound symbolic associations as humans requires stronger evidence. The paper doesn't fully address whether the learned associations reflect genuine phonetic-visual mappings or other confounding factors in the training data.

## Next Checks

1. **Cross-linguistic validation**: Test whether the sound symbolic associations transfer across languages by evaluating models trained on different language corpora. This would help distinguish between universal phonetic-visual mappings and language-specific artifacts.

2. **Ablation study on phonetic features**: Systematically remove specific phonetic features (voicing, place of articulation, etc.) from pseudowords to isolate which aspects drive the observed associations. This would help determine if models capture the same feature-level associations as humans.

3. **Controlled training data analysis**: Analyze the specific captioned image datasets used to train CLIP and Stable Diffusion for the presence of sound symbolic patterns. This would help quantify whether the observed associations could plausibly arise from the training data alone.