---
ver: rpa2
title: A General Theoretical Paradigm to Understand Learning from Human Preferences
arxiv_id: '2310.12036'
source_url: https://arxiv.org/abs/2310.12036
tags:
- policy
- preferences
- objective
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces a new theoretical framework called \u03A8\
  PO for learning from human preferences that unifies existing methods like RLHF and\
  \ DPO. It addresses overfitting issues in these methods by proposing a novel objective\
  \ based on pairwise preferences rather than pointwise rewards."
---

# A General Theoretical Paradigm to Understand Learning from Human Preferences

## Quick Facts
- arXiv ID: 2310.12036
- Source URL: https://arxiv.org/abs/2310.12036
- Reference count: 4
- Primary result: ΨPO unifies RLHF and DPO by expressing both as special cases of a general objective based on pairwise preferences

## Executive Summary
This paper introduces ΨPO, a general theoretical framework for learning from human preferences that unifies existing methods like RLHF and DPO. The key innovation is expressing preference learning as optimizing a Ψ function of pairwise preference probabilities while maintaining KL regularization to a reference policy. The paper demonstrates that IPO (ΨPO with identity mapping) avoids overfitting issues present in DPO, particularly in scenarios with deterministic preferences. Through theoretical analysis and illustrative examples, the authors show that IPO maintains regularization strength even when preferences are certain, while DPO tends to overfit and collapse to deterministic policies.

## Method Summary
The paper proposes ΨPO as a general objective: max_π E[Ψ(p*(y≻y'|x))] - τDKL(π||πref), where Ψ is any non-decreasing function. IPO is a special case with Ψ(q)=q that directly optimizes pairwise preference probabilities without converting to pointwise rewards. The method uses a sampled loss function L(π) = E[hπ(y,y') - τ^-1I(y,y')]² where I(y,y') is the preference indicator. The algorithm initializes a policy as softmax(θ), samples preference pairs from the dataset, computes the IPO loss, and updates parameters using gradient descent. The reference policy provides regularization to prevent policy drift.

## Key Results
- ΨPO unifies RLHF and DPO as special cases of a general pairwise preference objective
- IPO avoids overfitting to deterministic preferences that plague DPO
- Theoretical analysis shows IPO has unique global optimum under certain conditions
- IPO maintains KL regularization strength regardless of preference certainty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ΨPO unifies RLHF and DPO by expressing both as special cases of a general objective based on pairwise preferences
- Mechanism: By defining ΨPO as max_π E[Ψ(p*(y≻y'|x))] - τDKL(π||πref), both RLHF and DPO emerge when Ψ(q) = log(q/(1-q)) and the Bradley-Terry model holds
- Core assumption: The Bradley-Terry model accurately represents human pairwise preferences as σ(r(x,y) - r(x,y'))
- Evidence anchors: Proposition 1 shows that under BT model, optimal policies for ΨPO, RLHF, and DPO are identical

### Mechanism 2
- Claim: IPO (ΨPO with identity mapping) avoids overfitting by maintaining regularization strength even with deterministic preferences
- Mechanism: With Ψ(q) = q, the optimal policy becomes π*(y) ∝ πref(y)exp(τ^-1 E[p*(y≻y')]), which preserves KL regularization regardless of preference certainty
- Core assumption: Pairwise preferences can be optimized directly without converting to pointwise rewards
- Evidence anchors: When p*(y1≻y2)=1, DPO converges to deterministic policy while IPO converges to reference policy as τ→∞

### Mechanism 3
- Claim: IPO's sampled loss function provides unbiased gradient estimates without reward modeling
- Mechanism: The loss L(π) = E[hπ(y,y') - τ^-1I(y,y')]², where I(y,y') is sampled preference indicator, directly optimizes preference probabilities
- Core assumption: Preference samples I(y,y') are unbiased estimates of true p*(y≻y')
- Evidence anchors: Proposition 3 proves the sampled loss equals the population loss up to constant

## Foundational Learning

- Concept: Bradley-Terry model for pairwise comparisons
  - Why needed here: The paper builds its theoretical framework on understanding how pairwise preferences relate to pointwise rewards
  - Quick check question: If p*(A≻B)=0.8 and p*(B≻C)=0.7, what are the implied pairwise probabilities under Bradley-Terry?

- Concept: KL regularization in policy optimization
  - Why needed here: The paper uses KL divergence to keep learned policies close to reference policies, preventing drift
  - Quick check question: What happens to the KL regularization term when one action probability goes to zero in the learned policy?

- Concept: Convex optimization and uniqueness of solutions
  - Why needed here: Theorem 2 proves IPO has unique global optimum under certain conditions, which is crucial for stability
  - Quick check question: Why does the loss function become non-strictly convex when the support of µ doesn't cover the whole action space?

## Architecture Onboarding

- Component map: Preference dataset -> Policy parametrisation -> IPO loss function -> Optimization loop -> Reference policy provider

- Critical path:
  1. Load and preprocess preference dataset
  2. Initialize policy parameters
  3. Sample mini-batches and compute IPO loss
  4. Backpropagate and update parameters
  5. Monitor KL divergence from reference policy

- Design tradeoffs:
  - IPO vs DPO: IPO maintains regularization but may converge slower; DPO can overfit but is simpler to implement
  - Batch size: Larger batches reduce gradient variance but increase memory usage
  - Learning rate: Higher rates speed convergence but risk instability in the non-convex landscape

- Failure signatures:
  - Policy collapsing to deterministic (all probability on one action) suggests overfitting
  - KL divergence exploding indicates optimization instability
  - Loss plateauing early suggests poor initialization or learning rate too low

- First 3 experiments:
  1. Implement IPO loss on toy dataset with known preferences and verify convergence to optimal policy
  2. Compare IPO vs DPO on synthetic preference data with deterministic preferences
  3. Scale to simple language model fine-tuning task with human preference data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of IPO compare to DPO and RLHF in large-scale language model fine-tuning tasks with real human preference data?
- Basis in paper: The paper mentions that IPO is theoretically superior to DPO in simple illustrative examples and scenarios with deterministic preferences, but states that "Future works should scale those experiments to more complex settings such as training language models on human preferences data."
- Why unresolved: The current experiments are limited to simple bandit examples with discrete action spaces. Scaling to high-dimensional continuous action spaces like language model generations would require significant computational resources and careful experimental design to properly evaluate performance.
- What evidence would resolve it: Empirical comparison on standard language model alignment benchmarks (e.g., summarization, dialogue) showing task performance, KL divergence from reference policy, and training stability across different preference dataset sizes and deterministic preference scenarios.

### Open Question 2
- Question: What is the optimal choice of Ψ function for ΨPO in different preference learning scenarios?
- Basis in paper: The paper introduces ΨPO as a general framework and analyzes specific cases (identity Ψ for IPO, logit Ψ for DPO/RLHF), but notes that "small increases in preference probabilities already close to 1 are just as incentivized as larger increases in preference probabilities around 50%, which may be undesirable."
- Why unresolved: The paper only analyzes two specific Ψ functions. The general framework allows for many other choices, but there's no systematic study of how different Ψ functions affect learning dynamics, robustness to deterministic preferences, or empirical performance across different preference distributions.
- What evidence would resolve it: Comparative study of various Ψ functions (e.g., bounded functions, polynomial transformations) on synthetic and real preference data, measuring convergence speed, final policy quality, and sensitivity to preference noise.

### Open Question 3
- Question: How does IPO behave in continuous action spaces or when preferences are intransitive?
- Basis in paper: The paper analyzes IPO in discrete bandit settings and assumes transitive preferences, but real-world preference data (especially for language models) involves continuous action spaces and may contain intransitivities or cycles.
- Why unresolved: The current theoretical analysis relies on the discrete finite action space assumption and doesn't address how to handle intransitive preferences or continuous action spaces where the support assumption (Supp(µ) = Supp(πref)) may not hold.
- What evidence would resolve it: Extension of IPO to continuous action spaces with function approximation, testing on preference datasets with known intransitivities, and analysis of policy stability when the reference and behavior policy supports differ.

## Limitations
- Theoretical analysis assumes deterministic preference models that may not hold for real human preferences
- Empirical validation limited to illustrative examples rather than large-scale benchmarks
- Does not address how to handle intransitive preferences or continuous action spaces

## Confidence
- High confidence: The theoretical unification of RLHF and DPO under ΨPO framework (Proposition 1)
- Medium confidence: IPO's stability advantage over DPO in deterministic preference scenarios (Theorem 2 and section 5.3.1)
- Low confidence: General applicability to real human preference data without reward modeling

## Next Checks
1. Test IPO on actual human preference datasets (e.g., Anthropic's HH-RLHF or similar) to verify stability claims beyond synthetic examples
2. Evaluate IPO's performance and computational efficiency when scaling to thousands of actions (e.g., full vocabulary in language models)
3. Systematically vary preference noise levels to determine IPO's robustness compared to DPO across the spectrum from deterministic to highly uncertain preferences