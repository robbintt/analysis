---
ver: rpa2
title: A Geometric Notion of Causal Probing
arxiv_id: '2307.15054'
source_url: https://arxiv.org/abs/2307.15054
tags:
- concept
- information
- language
- subspace
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a formal, intrinsic definition of how language
  models encode concepts in their representation spaces, moving beyond extrinsic classification-based
  methods. It introduces a counterfactual framework that avoids spurious correlations
  by treating the concept subspace and its orthogonal complement as statistically
  independent, allowing measurement of mutual information in each part.
---

# A Geometric Notion of Causal Probing

## Quick Facts
- arXiv ID: 2307.15054
- Source URL: https://arxiv.org/abs/2307.15054
- Authors: 
- Reference count: 12
- Primary result: Proposes a counterfactual framework for causal probing that avoids spurious correlations and enables controlled generation by treating concept subspace and its complement as statistically independent.

## Executive Summary
This paper introduces a geometric framework for causal probing that moves beyond traditional classification-based methods by defining an intrinsic notion of concept information in representation subspaces. The approach uses a counterfactual independence assumption to avoid spurious correlations and identifies ideal concept subspaces through four key properties: erasure, encapsulation, containment, and stability. Empirically, the authors demonstrate that their method can capture roughly half of the concept information in a one-dimensional subspace for verbal number and grammatical gender in English and French language models, and successfully manipulate generated text through causal interventions.

## Method Summary
The method introduces a counterfactual framework where concept information in a subspace is measured using mutual information under an independence assumption between the concept subspace and its orthogonal complement. R-LACE (Ravfogel et al., 2022a) is used to find projection matrices that optimize for counterfactual erasure of concept information from the non-concept subspace. The framework characterizes ideal concept subspaces through four properties and enables causal controlled generation by intervening on the learned concept subspace. The approach is evaluated on GPT-2 models for verbal number and grammatical gender concepts.

## Key Results
- R-LACE finds a one-dimensional subspace capturing roughly half of concept information for verbal number and grammatical gender
- The concept subspace can manipulate generated word's concept value with precision through causal interventions
- Non-concept information is preserved in the orthogonal complement (low containment and stability metrics)
- Results are consistent across English and French models for verbal number, with differences observed for grammatical gender

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The counterfactual framework enables isolation of concept information by treating the concept subspace and its orthogonal complement as statistically independent, thereby avoiding spurious correlations.
- Mechanism: By constructing a counterfactual unigram distribution qu where h∥ and h⊥ are factorized, the framework breaks the statistical dependence between concept and non-concept components. This allows measurement of mutual information in each part without interference from correlated features.
- Core assumption: The concept subspace S∥ and its orthogonal complement S⊥ can be made statistically independent under the counterfactual distribution, even if they are correlated under the original model distribution.
- Evidence anchors:
  - [abstract]: "We propose a counterfactual approach that avoids the failure mode of spurious correlations (Kumar et al., 2022a) by treating components in the subspace and its orthogonal complement independently."
  - [section]: "We construct a variant of our information-theoretic objective in Eq. (12) that assumes these two variables are statistically independent, i.e., I(H ⊥; H ∥) = 0."
  - [corpus]: "How Reliable are Causal Probing Interventions?" - related work questioning the reliability of causal probing methods.
- Break condition: If the concept subspace and its orthogonal complement share a common cause that cannot be factored out by the intervention, the independence assumption breaks down and the counterfactual framework fails to isolate concept information.

### Mechanism 2
- Claim: The four properties (erasure, encapsulation, containment, stability) provide a complete characterization of an ideal concept subspace partition, ensuring both concept information is preserved and non-concept information is protected.
- Mechanism: Erasure ensures the orthogonal complement contains no concept information (Iq(C; H ⊥) ≈ 0), encapsulation ensures the concept subspace contains all concept information (Iq(C; H) ≈ Iq(C; H ∥)), containment ensures the concept subspace contains no non-concept information (Iq(X; H ∥ | C) ≈ 0), and stability ensures the orthogonal complement preserves non-concept information (Iq(X; H | C) ≈ Iq(X; H ⊥ | C)).
- Core assumption: The representation space can be partitioned into two orthogonal subspaces that satisfy all four properties simultaneously.
- Evidence anchors:
  - [abstract]: "We show that our counterfactual notion of information in a subspace is optimized by an causal concept subspace."
  - [section]: "Erasure and encapsulation together characterize the preservation of information not related to concepts."
  - [corpus]: "The Geometry of Harmfulness in LLMs through Subconcept Probing" - related work on subspace probing for harmful concepts.
- Break condition: If the concept information is encoded non-linearly in the representation space, linear erasure cannot achieve perfect erasure, breaking the containment property.

### Mechanism 3
- Claim: The causal graphical model with a latent concept variable C enables controlled generation by allowing interventions on the concept subspace that propagate causally to the generated output.
- Mechanism: By introducing a latent concept variable C that influences the contextual representation H before word generation, and treating H∥ and H⊥ as independent under intervention, the framework allows do(C = c) interventions that set the concept value and generate text accordingly.
- Core assumption: The language model's generation process can be modeled as a causal graph where the concept variable C causally influences the word generation through the representation H.
- Evidence anchors:
  - [abstract]: "Our causal intervention for controlled generation shows that, for at least one concept across two languages models, the concept subspace can be used to manipulate the concept value of the generated word with precision."
  - [section]: "Our graphical model and our independence assumption enable us to derive a causal controlled generation method by manipulating the conceptual component of a representation."
  - [corpus]: "Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution" - related work on concept modeling in LLMs.
- Break condition: If the concept does not have a causal effect on the generated word (e.g., if it's only a byproduct of generation), then intervening on the concept subspace will not affect the output.

## Foundational Learning

- Concept: Information theory and mutual information
  - Why needed here: The framework relies on measuring and comparing mutual information between concepts and representations under different distributions.
  - Quick check question: What is the difference between mutual information I(X;Y) and conditional mutual information I(X;Y|Z)?

- Concept: Linear algebra and subspace projection
  - Why needed here: The framework operates on linear subspaces of the representation space, using projection matrices to isolate concept and non-concept components.
  - Quick check question: What are the properties of an orthogonal projection matrix P that projects onto a subspace S?

- Concept: Causal inference and do-interventions
  - Why needed here: The controlled generation approach is based on causal interventions on the concept variable in a graphical model.
  - Quick check question: What is the difference between conditioning on a variable and intervening on it in a causal graph?

## Architecture Onboarding

- Component map: Language model -> R-LACE -> Counterfactual unigram distribution -> Mutual information metrics -> Controlled generation
- Critical path:
  1. Sample text from language model (ancestral or nucleus sampling)
  2. Compute contextual representations h(x<t) for each context
  3. Apply R-LACE to find projection matrix P_k for concept subspace
  4. Compute counterfactual mutual information metrics using qu
  5. For controlled generation: intervene on C, sample g(c) from concept subspace, generate next word
- Design tradeoffs:
  - Using linear subspaces is computationally tractable but may not capture non-linear concept encoding
  - Assuming statistical independence of h∥ and h⊥ under intervention simplifies analysis but may not hold exactly
  - Optimizing for counterfactual erasure is more complex than optimizing for linear predictability
- Failure signatures:
  - If erasure metric I(C; H ⊥) remains high after R-LACE optimization, concept information leaks into non-concept subspace
  - If stability metric I(X; H | C) - I(X; H ⊥ | C) is large, non-concept information is lost in non-concept subspace
  - If controlled generation fails to change concept value of generated word, intervention did not work
- First 3 experiments:
  1. Verify that R-LACE finds a 1-dimensional subspace for verbal-number in GPT-2 large that achieves low erasure (I(C; H ⊥) ≈ 0) and high encapsulation (Iq(C; H) ≈ Iq(C; H ∥))
  2. Check that stability and containment metrics are low for k = 1, indicating preservation of non-concept information
  3. Test controlled generation by intervening on C = pl and verifying that generated verb agrees in number with intervention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we optimize the counterfactual mutual information (Iq(C; H⊥)) to identify the causal concept subspace?
- Basis in paper: [explicit] The paper states "Computing Eqs. (18) and (19) in practice allows us to come full circle. The independence assumption means a conditional is computed as an average over components obtained from other h representations as follows qu(x | h⊥) = Σh∈H∥ pu(x | h⊥, h∥)pu(h∥). Our theoretical analysis, combined with the efficacy of linear erasure methods using a correlational objective, suggests a tantalizing prospect: That a counterfactual objective could identify a one-dimensional causal subspace containing all information about the concept empirically."
- Why unresolved: The paper notes that "computing it involves a double nested sum over countably infinite vector spaces" and states "We report counterfactual information-theoretic results for the projection matrices learned by R-LACE, and leave the optimization of this metric for identification of the causal subspace to future work."
- What evidence would resolve it: A practical algorithm that optimizes Iq(C; H⊥) directly, along with empirical results showing its effectiveness in identifying causal subspaces compared to existing methods.

### Open Question 2
- Question: Does the constant concept subspace hypothesis hold in practice for language models?
- Basis in paper: [explicit] The paper presents "Hypothesis 4.2. Let (I d − P k) be an ε-encapsulator of C. The constant concept subspace hypothesis states that, for any x<t ∈ Σ∗ and c ∈ C, the projection of h (x<t, c) onto the H∥ depends only on c as ε → 0. That is (I d − P k) h(x<t, c) = g(c) ∈ Rd for all c ∈ C for some g : C → Rd assigning concepts vector representations."
- Why unresolved: The paper states "We suspect that the constant concept subspace hypothesis will not hold completely in practice" but does not provide empirical evidence to support or refute this hypothesis.
- What evidence would resolve it: Experimental results showing whether the projection of contextual representations onto the concept subspace varies with the textual context or remains constant for each concept value across different language models and concepts.

### Open Question 3
- Question: How does the quality and amount of training data affect the ability of language models to encode concepts in linear subspaces?
- Basis in paper: [inferred] The paper notes that "Results in §6.1 do not explain this difference—the information partition for gpt2-base-french was more lossy, but not to an extent that foreshadowed such a difference." and discusses differences in performance between GPT-2 large and GPT-2 base French models.
- Why unresolved: While the paper mentions differences in model performance, it does not systematically investigate the relationship between training data characteristics and concept encoding.
- What evidence would resolve it: A study comparing concept encoding across models trained on datasets of varying size, quality, and domain, controlling for model architecture and other factors.

## Limitations

- The counterfactual independence assumption between concept subspace and its orthogonal complement may not hold for complex semantic concepts involving non-linear relationships
- R-LACE optimization becomes unstable at higher k values, limiting the framework's ability to capture richer concept representations
- The framework's effectiveness for non-syntactic concepts beyond verbal number and grammatical gender remains unproven
- The computational complexity of counterfactual information-theoretic metrics makes direct optimization challenging

## Confidence

**High Confidence**: The counterfactual information-theoretic framework and the four-property characterization of concept subspaces are mathematically sound and well-defined. The controlled generation experiments successfully demonstrate causal manipulation of the verbal number concept.

**Medium Confidence**: The empirical results showing approximately 50% of concept information captured in a 1-dimensional subspace are reproducible, but the variability in metrics at higher k values and the limited scope to only two concepts across two languages reduce confidence in broader applicability.

**Low Confidence**: The assumption that linear subspaces can adequately capture concept information for all types of linguistic concepts, particularly those requiring non-linear representations or involving complex interactions between multiple concepts.

## Next Checks

1. **Generalization Test**: Apply the framework to more complex semantic concepts (e.g., sentiment, topic, or entity type) and evaluate whether the counterfactual independence assumption holds and whether controlled generation remains effective.

2. **Robustness Analysis**: Systematically vary the R-LACE hyperparameters across a wider range and test the stability of the learned concept subspaces and their information-theoretic properties, particularly for k > 1.

3. **Alternative Models**: Validate the approach on language models beyond GPT-2 (e.g., BERT, RoBERTa, or larger GPT models) to assess whether the observed properties of concept subspaces are model-specific or more general.