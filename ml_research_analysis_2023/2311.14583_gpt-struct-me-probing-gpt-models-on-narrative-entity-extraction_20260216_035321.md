---
ver: rpa2
title: 'GPT Struct Me: Probing GPT Models on Narrative Entity Extraction'
arxiv_id: '2311.14583'
source_url: https://arxiv.org/abs/2311.14583
tags:
- extraction
- narrative
- language
- gpt-3
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models can effectively\
  \ extract structured narrative entities\u2014events, participants, and temporal\
  \ expressions\u2014from text. The authors design a prompt-based approach using GPT-3\
  \ and ChatGPT, constructing prompts with varying levels of detail through an ablation\
  \ study on a subset of the Text2Story Lusa dataset."
---

# GPT Struct Me: Probing GPT Models on Narrative Entity Extraction
## Quick Facts
- arXiv ID: 2311.14583
- Source URL: https://arxiv.org/abs/2311.14583
- Reference count: 34
- Key outcome: GPT models outperform baselines in extracting participants and timexs (up to 20 points higher F1), but lag in event extraction due to task-specific definitions.

## Executive Summary
This paper investigates the effectiveness of large language models (GPT-3 and ChatGPT) in extracting structured narrative entities—events, participants, and temporal expressions—from Portuguese news articles. The authors employ a prompt-based approach, constructing and refining prompts through an ablation study on a subset of the Text2Story Lusa dataset, then evaluating on the remaining documents. Results show GPT models significantly outperform out-of-the-box baseline systems in participant and timex extraction, but struggle with event extraction, likely due to the task-specific nature of event definitions. The study highlights the promise of LLMs as all-in-one tools for entity extraction, especially in resource-limited settings, while identifying opportunities for prompt refinement and broader application.

## Method Summary
The study uses a prompt-based approach to extract events, participants, and timexs from the Text2Story Lusa dataset (119 Portuguese news articles). An ablation study on 20 documents determines the best prompt templates for each model-entity pair, considering components like task, definition, classes, example, format, input, and output. These templates are then evaluated on the remaining 96 documents. Performance is measured using strict and relaxed F1 scores and compared to out-of-the-box baseline systems (SRL, TEFE, HeidelTime, TEI2GO).

## Key Results
- GPT models achieve up to 20 points higher strict F1 score than baselines in participant and timex extraction.
- Both GPT models struggle with event extraction, not surpassing baselines, likely due to the task-specific nature of event definitions.
- Including examples in prompts consistently improves performance across extraction tasks.
- ChatGPT slightly outperforms GPT-3 in participant extraction using the same prompts.

## Why This Works (Mechanism)
### Mechanism 1
- Claim: GPT models outperform baselines in participant and timex extraction due to their pre-training on vast textual data that aligns with common knowledge of these concepts.
- Mechanism: The large-scale pretraining on diverse text corpora allows GPT models to develop rich semantic representations of entities like participants and temporal expressions, which are more common and less domain-specific than events.
- Core assumption: The semantic representations learned during pretraining are sufficiently general to capture the meaning of participants and timexs without additional task-specific fine-tuning.
- Evidence anchors:
  - [abstract]: "GPT models are competitive with out-of-the-box baseline systems, presenting an all-in-one alternative for practitioners with limited resources."
  - [section]: "We hypothesize that since the concept of timexs and participants in our annotation is more aligned with the common knowledge, GPT models have a better chance to outperform the baselines due to the large textual data on which they were trained on."
  - [corpus]: No direct evidence; relies on general understanding of GPT pretraining.
- Break condition: If the annotation scheme for participants or timexs becomes highly specialized or domain-specific, GPT's common knowledge advantage may diminish.

### Mechanism 2
- Claim: Including examples in prompts improves GPT model performance by providing task-specific guidance.
- Mechanism: Examples in prompts act as few-shot demonstrations, helping the model understand the desired output format and task requirements, leading to better alignment with the annotation guidelines.
- Core assumption: GPT models can effectively learn from a small number of examples and generalize this knowledge to new instances within the same task.
- Evidence anchors:
  - [abstract]: "We first select the best prompt template through an ablation study over prompt components that provide varying degrees of information on a subset of documents of the dataset."
  - [section]: "Third, the inclusion of the Example improves, on average, the F1 score in the different extraction tasks."
  - [corpus]: No direct evidence; relies on general understanding of few-shot learning in GPT models.
- Break condition: If the task becomes too complex or the annotation guidelines too nuanced, a small number of examples may be insufficient for the model to capture the required patterns.

### Mechanism 3
- Claim: GPT models struggle with event extraction due to the task-specific nature of event definitions in the annotation scheme.
- Mechanism: Events in the Text2Story Lusa dataset are defined according to a specific annotation framework, which may not align well with the general knowledge acquired during GPT's pretraining. This mismatch leads to lower performance compared to task-specific baselines.
- Core assumption: The concept of events in the annotation scheme is sufficiently different from the general understanding of events in natural language, requiring task-specific knowledge that GPT models lack without fine-tuning.
- Evidence anchors:
  - [abstract]: "However, it is interesting to see that both GPT models cannot surpass the baselines in the events extraction task. We hypothesize that, given that the concept of event in the annotation is very specific, one annotation example is not enough for these models."
  - [section]: "Given that all the baselines are used out-of-the-box while the prompts used in one-shot include an annotation example, the results go according to what was expected in timexs and participants. However, it is interesting to see that both GPT models cannot surpass the baselines in the events extraction task."
  - [corpus]: No direct evidence; relies on general understanding of task-specific vs. general knowledge in GPT models.
- Break condition: If the event definition becomes more aligned with common language usage or if GPT models are fine-tuned on a large dataset of similar events, their performance may improve.

## Foundational Learning
- Concept: Prompt engineering
  - Why needed here: The effectiveness of GPT models heavily depends on the quality of the prompts used to guide their behavior. Understanding how to construct effective prompts is crucial for achieving good results in narrative entity extraction.
  - Quick check question: What are the key components of a well-designed prompt for GPT models, and how do they influence the model's output?
- Concept: Evaluation metrics
  - Why needed here: Accurately measuring the performance of GPT models in narrative entity extraction requires a solid understanding of evaluation metrics like precision, recall, and F1 score. These metrics help assess the model's ability to correctly identify and classify entities.
  - Quick check question: How do precision, recall, and F1 score differ in their assessment of a model's performance, and why is it important to consider all three metrics?
- Concept: Ablation studies
  - Why needed here: Ablation studies are used to determine the impact of different prompt components on the model's performance. Understanding how to design and interpret ablation studies is essential for identifying the most effective prompt templates.
  - Quick check question: What is the purpose of an ablation study, and how can it help in optimizing the prompt components for better model performance?

## Architecture Onboarding
- Component map:
  1. Dataset: Text2Story Lusa (119 Portuguese news articles)
  2. Models: GPT-3, ChatGPT
  3. Baseline systems: SRL, TEFE, HeidelTime, TEI2GO
  4. Prompt templates: Task, Definition, Classes, Example, Format, Input, Output
  5. Evaluation metrics: Precision, Recall, F1 score, Relaxed F1 score
- Critical path:
  1. Define prompt templates and components
  2. Conduct ablation study on a subset of the dataset to identify the best prompt templates for each model-entity pair
  3. Evaluate the selected prompt templates on the remaining documents
  4. Compare the results with baseline systems using evaluation metrics
  5. Analyze the strengths and limitations of GPT models in narrative entity extraction
- Design tradeoffs:
  - Using out-of-the-box baseline systems vs. fine-tuning GPT models on the specific task
  - Including more detailed prompt components vs. risking prompt complexity and potential confusion for the model
  - Focusing on strict F1 score vs. relaxed F1 score to account for partial matches
- Failure signatures:
  - Low performance in event extraction due to task-specific nature of event definitions
  - Over-reliance on prompt examples, leading to poor generalization to new instances
  - Sensitivity to prompt formatting or wording changes, affecting model output consistency
- First 3 experiments:
  1. Evaluate the impact of including the Definition component in the prompt on participant extraction performance
  2. Compare the performance of zero-shot and one-shot prompts for timex extraction using GPT-3 and ChatGPT
  3. Assess the effect of varying the number of examples in the prompt on event extraction performance

## Open Questions the Paper Calls Out
- Question: How do GPT models perform on narrative entity extraction in languages other than Portuguese?
  - Basis in paper: [inferred] The paper evaluates GPT models on a Portuguese news dataset but does not test other languages.
  - Why unresolved: The study focuses on a single language, leaving cross-linguistic generalizability untested.
  - What evidence would resolve it: Conducting the same evaluation on datasets in other languages (e.g., English, Spanish, Mandarin) to compare performance.
- Question: What is the optimal number of examples to include in the prompt for event extraction tasks?
  - Basis in paper: [explicit] The paper suggests extending the number of examples in future work but does not determine the optimal count.
  - Why unresolved: The study uses a single example in its best-performing prompt but does not explore varying numbers of examples.
  - What evidence would resolve it: Testing prompts with different numbers of examples (e.g., 1, 3, 5, 10) and measuring performance on event extraction tasks.
- Question: Can GPT models be effectively fine-tuned for specific narrative entity extraction tasks?
  - Basis in paper: [inferred] The paper uses prompt-based learning without fine-tuning, implying this is an unexplored direction.
  - Why unresolved: The study relies on zero-shot and few-shot prompting, leaving the potential of fine-tuning untested.
  - What evidence would resolve it: Fine-tuning GPT models on labeled narrative entity extraction datasets and comparing results to prompt-based approaches.

## Limitations
- The evaluation is constrained by the small dataset size (119 documents), which limits statistical significance and generalizability.
- The ablation study uses only 20 documents to determine optimal prompts, potentially introducing selection bias.
- The study focuses solely on Portuguese news articles, raising questions about cross-lingual and cross-domain applicability.

## Confidence
- High Confidence: GPT models significantly outperform baselines in participant and timex extraction, as evidenced by consistent F1 score improvements across multiple metrics and models.
- Medium Confidence: The mechanism explaining GPT's success with participants and timexs (leveraging pre-training on common knowledge) is plausible but not empirically verified within the study.
- Low Confidence: The specific prompt components that contribute most to performance improvements, as the ablation study's limited scope and document selection method introduce uncertainty about generalizability.

## Next Checks
1. **Dataset Expansion Validation**: Replicate the study using a larger, multi-domain corpus (e.g., combining Text2Story with other news or narrative datasets) to verify if GPT's performance advantages hold across different text types and sizes.
2. **Prompt Component Isolation**: Conduct a more granular ablation study where each prompt component is tested individually across multiple random document subsets to isolate which specific elements drive performance improvements.
3. **Cross-Lingual Verification**: Apply the same methodology to English and other language datasets to determine whether GPT's advantage in participant and timex extraction is language-dependent or generalizable across linguistic contexts.