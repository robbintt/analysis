---
ver: rpa2
title: 'Cross-Task Attention Network: Improving Multi-Task Learning for Medical Imaging
  Applications'
arxiv_id: '2309.03837'
source_url: https://arxiv.org/abs/2309.03837
tags:
- attention
- tasks
- ctan
- task
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Cross-Task Attention Network (CTAN), a novel
  multi-task learning (MTL) framework that leverages cross-task attention mechanisms
  to improve model performance across various medical imaging tasks. CTAN addresses
  the limitation of existing MTL architectures that share information across tasks
  by introducing cross-task attention modules in the encoder and bottleneck layers
  to better capture inter-task interactions.
---

# Cross-Task Attention Network: Improving Multi-Task Learning for Medical Imaging Applications

## Quick Facts
- **arXiv ID**: 2309.03837
- **Source URL**: https://arxiv.org/abs/2309.03837
- **Reference count**: 40
- **Primary result**: CTAN outperforms standard single-task learning with an average improvement of 4.67% and outperforms two widely used MTL baselines: hard parameter sharing (HPS) with an average improvement of 3.22% and MTAN with a relative decrease of 5.38%

## Executive Summary
This paper proposes Cross-Task Attention Network (CTAN), a novel multi-task learning framework that leverages cross-task attention mechanisms to improve model performance across various medical imaging tasks. CTAN addresses the limitation of existing MTL architectures by introducing cross-task attention modules in the encoder and bottleneck layers to better capture inter-task interactions. The framework was validated on four medical imaging datasets spanning different domains and tasks, including radiation treatment planning prediction, pigmented skin lesion segmentation and diagnosis, and COVID-19 diagnosis and severity prediction. Results demonstrate that CTAN outperforms both single-task learning and two widely used MTL baselines, highlighting its effectiveness in improving medical imaging task accuracy across domains.

## Method Summary
CTAN is a multi-task learning framework that introduces cross-task attention mechanisms at both the encoder and bottleneck layers. The architecture uses a shared ResNet-50 encoder, cross-task attention encoder (CTAE) modules that compute attention maps across tasks, a cross-task attention bottleneck (CTAB) module that refines bottleneck features using cross-attention, and task-specific decoders for each task. The framework employs dynamic weight averaging for loss stabilization and was validated across four medical imaging datasets with different task combinations including segmentation, classification, and regression tasks.

## Key Results
- CTAN outperforms standard single-task learning with an average performance improvement of 4.67%
- CTAN outperforms hard parameter sharing (HPS) with an average improvement of 3.22%
- CTAN shows a relative decrease of 5.38% compared to MTAN, indicating better performance than this widely used MTL baseline

## Why This Works (Mechanism)

### Mechanism 1
Cross-task attention enables the model to capture inter-task interactions by allowing task-specific features to inform each other during encoding. The Cross-Task Attention Encoder (CTAE) multiplies attention maps from one task with feature maps from the other task, enabling information flow across tasks rather than within each task independently. Core assumption: Information relevant to one task can improve feature extraction for another task when appropriately aligned through attention mechanisms. Break condition: If tasks are completely unrelated or adversarial, cross-attention could introduce noise and degrade performance.

### Mechanism 2
Cross-task attention at the bottleneck layer provides task-specific decoders with refined, task-informed features from the other task. The Cross-Task Attention Bottleneck (CTAB) computes attention using a query from one task and keys/values from the other task, then applies this attention to the bottleneck features before passing them to decoders. Core assumption: The bottleneck representation contains sufficiently abstract features that can benefit from cross-task attention weighting. Break condition: If bottleneck features are too task-specific or the attention computation becomes unstable, the cross-task refinement may not provide meaningful improvements.

### Mechanism 3
Cross-task attention mitigates task interference by selectively incorporating useful information while suppressing conflicting gradients. By explicitly modeling task interactions through attention, CTAN can emphasize beneficial shared features while reducing the impact of task-specific noise or conflicting objectives. Core assumption: Not all shared information between tasks is beneficial; selective attention can distinguish helpful from harmful interactions. Break condition: If the attention mechanism cannot effectively distinguish useful from harmful interactions, it may amplify negative transfer rather than reduce it.

## Foundational Learning

- **Concept**: Attention mechanisms and their variants (self-attention, cross-attention)
  - Why needed here: The entire CTAN framework relies on attention mechanisms to model task interactions at both encoder and bottleneck levels.
  - Quick check question: Can you explain the difference between self-attention (within a single feature map) and cross-attention (between two different feature maps)?

- **Concept**: Multi-task learning objectives and loss weighting strategies
  - Why needed here: CTAN uses dynamic weight averaging to balance task-specific losses, which is critical for stable training when tasks have different scales or convergence rates.
  - Quick check question: Why might dynamic weight averaging be preferable to static weights when training CTAN on tasks with different convergence speeds?

- **Concept**: Residual networks and transfer learning with pre-trained backbones
  - Why needed here: CTAN uses a ResNet-50 backbone pre-trained on ImageNet, requiring understanding of how to adapt pre-trained models for medical imaging tasks.
  - Quick check question: What considerations should be made when fine-tuning a pre-trained ResNet-50 for medical imaging tasks with potentially different feature distributions?

## Architecture Onboarding

- **Component map**: Input image → Shared encoder → CTAE modules (one per task) → Shared feature maps → CTAB module → Task-specific decoders → Output predictions

- **Critical path**: The cross-attention operations are the critical innovation that distinguishes CTAN from standard MTL approaches. Each CTAE takes shared encoder features and prior attention masks as input, produces task-specific attention masks, and the CTAB takes task-embedded bottleneck features and produces cross-attended bottleneck features.

- **Design tradeoffs**: Using cross-attention adds computational overhead but enables richer task interactions. Pre-trained ResNet-50 provides strong feature extraction but may need adaptation for medical imaging domain shift. Skip connections help preserve spatial information for segmentation but add complexity to the architecture.

- **Failure signatures**: Degraded performance on one task when paired with another suggests harmful cross-task interference. Training instability or exploding/vanishing gradients may indicate issues with attention mask scaling or loss weighting. Minimal performance difference from baseline MTL suggests cross-attention isn't capturing meaningful task interactions.

- **First 3 experiments**: 
  1. Implement CTAN on a simple toy dataset with two related tasks (e.g., digit classification and digit counting) to verify the cross-attention mechanism works as expected
  2. Compare CTAN against standard HPS and MTAN on a medical imaging dataset with one pixel-level and one image-level task to validate performance improvements
  3. Perform ablation studies removing either CTAE or CTAB to quantify the contribution of each cross-attention component to overall performance

## Open Questions the Paper Calls Out

### Open Question 1
How does CTAN's performance scale with increasing numbers of tasks beyond the two-task scenarios tested in this paper? The paper focuses on two-task scenarios but mentions potential for broader applications. The experiments were limited to three pairs of tasks (six total tasks), all in two-task configurations. Experimental results comparing CTAN across scenarios with three, four, or more tasks simultaneously would resolve this.

### Open Question 2
What specific aspects of the cross-task attention mechanism contribute most to performance improvements - is it the encoder attention, bottleneck attention, or their combination? The paper introduces both CTAE and CTAB modules but doesn't isolate their individual contributions through ablation studies. Ablation studies comparing CTAN variants with only CTAE, only CTAB, both modules, and baseline architectures would resolve this.

### Open Question 3
How does CTAN handle task conflicts where one task's optimization could negatively impact another task's performance? While the paper notes task conflicts exist and that CTAN "proved to be effective in minimizing the performance drop," it doesn't detail the mechanism by which CTAN mitigates these conflicts. Analysis of gradient interactions during training, including visualization of gradient directions and correlation analysis between task gradients, would resolve this.

### Open Question 4
How sensitive is CTAN's performance to the choice of loss weighting strategy beyond the Dynamic Weight Averaging approach used in this paper? The paper commits to one specific loss weighting method without exploring how sensitive results are to this choice. Comparative experiments using alternative loss weighting strategies (fixed weights, uncertainty-based weighting, gradient normalization, etc.) across all datasets would resolve this.

## Limitations

- The architecture details for the task embedding and pooling blocks are not fully specified, requiring assumptions during implementation
- The effectiveness of CTAN depends heavily on task compatibility, though the paper doesn't provide clear guidelines for predicting when cross-task attention will be beneficial versus harmful
- The paper doesn't explore CTAN's performance with more than two tasks simultaneously, limiting understanding of its scalability

## Confidence

- **High confidence**: CTAN outperforms both single-task learning and standard hard parameter sharing baselines across all four datasets
- **Medium confidence**: The relative improvement over MTAN (5.38% average decrease) is notable but requires careful interpretation given architectural differences
- **Medium confidence**: Claims about CTAN's ability to mitigate task interference are supported by COVID-19 classification results but would benefit from more extensive ablation studies

## Next Checks

1. Implement and compare CTAN variants with only CTAE, only CTAB, and no cross-attention modules to quantify the individual contributions of each component to overall performance improvements.

2. Systematically test CTAN across a wider range of task combinations, including adversarial pairs, to establish clear boundaries for when cross-task attention is beneficial versus harmful.

3. Visualize and analyze the cross-task attention maps during training to confirm they are highlighting semantically meaningful features and not merely capturing spurious correlations in the data.