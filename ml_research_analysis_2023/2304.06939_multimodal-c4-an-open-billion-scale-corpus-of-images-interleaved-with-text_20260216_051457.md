---
ver: rpa2
title: 'Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text'
arxiv_id: '2304.06939'
source_url: https://arxiv.org/abs/2304.06939
tags:
- images
- image
- mmc4
- text
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces mmc4, a large-scale corpus of images interleaved
  with text designed to support in-context learning for vision-language models. The
  dataset is constructed by augmenting the text-only C4 corpus with images using a
  linear assignment algorithm based on CLIP features to align images with sentences.
---

# Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text

## Quick Facts
- arXiv ID: 2304.06939
- Source URL: https://arxiv.org/abs/2304.06939
- Reference count: 40
- Introduces mmc4, a billion-scale corpus of 103M documents with 585M images interleaved across 43B English tokens

## Executive Summary
This paper introduces mmc4, a large-scale corpus of images interleaved with text designed to support in-context learning for vision-language models. The dataset is constructed by augmenting the text-only C4 corpus with images using a linear assignment algorithm based on CLIP features to align images with sentences. The corpus contains 103 million documents with 585 million images interleaved across 43 billion English tokens. Manual inspection shows that 90% of images are topically relevant and 78% are well-aligned with their assigned sentences. The paper also presents mmc4-core and mmc4-ff subsets with stricter filtering and fewer faces, respectively.

## Method Summary
The mmc4 corpus is constructed by augmenting the text-only C4 corpus with images using CLIP features and a linear assignment algorithm. The process involves downloading C4 data, filtering to English documents with sufficient text and images, using CLIP to compute image-sentence similarities, applying the Hungarian algorithm for bipartite matching to assign one image per sentence while maximizing total similarity, and filtering for quality. The paper presents three variants: mmc4-full (the complete corpus), mmc4-core (stricter filtering with higher quality standards), and mmc4-ff (fewer faces via logistic regression-based filtering).

## Key Results
- mmc4 contains 103 million documents with 585 million images interleaved across 43 billion English tokens
- Manual inspection shows 90% of images are topically relevant and 78% are well-aligned with their assigned sentences
- Models trained on mmc4-core sequences achieve 20-30 CIDEr point improvements on MSCOCO captioning compared to models trained on image-caption pairs alone

## Why This Works (Mechanism)

### Mechanism 1
Linear assignment with CLIP features aligns images to sentences more evenly than maximum similarity assignment. The Hungarian algorithm distributes images across sentences to maximize total similarity while ensuring each sentence gets at most one image, rather than clustering images on the most similar sentences. The core assumption is that even distribution of images throughout documents is more beneficial for multimodal in-context learning than concentration on highly similar sentences.

### Mechanism 2
Interleaved training data enables few-shot multimodal in-context learning better than paired image-caption data. Models pretrained on sequences where images and text naturally co-occur learn to attend across modalities in context, mimicking how humans process information. The core assumption is that the multimodal in-context learning ability emerges from exposure to naturally interleaved sequences during pretraining, not just from seeing images and text together.

### Mechanism 3
CLIP zero-shot similarity is sufficient for document-level image-text alignment without fine-tuning. CLIP's pretrained vision and language encoders capture enough semantic alignment that cosine similarity between embeddings can identify relevant image-sentence pairs. The core assumption is that CLIP's pretraining on image-text pairs generalizes to the more complex relationship between web documents and their images.

## Foundational Learning

- Concept: Bipartite matching and the assignment problem
  - Why needed here: The core technical contribution relies on solving an image-to-sentence assignment problem for each document
  - Quick check: Given a 3x3 cost matrix for matching 3 images to 3 sentences, can you apply the Hungarian algorithm to find the optimal assignment?

- Concept: CLIP embeddings and cosine similarity
  - Why needed here: The entire alignment pipeline depends on computing CLIP similarity scores between images and sentences
  - Quick check: If two CLIP embeddings have cosine similarity of 0.9, what does this indicate about their semantic relationship?

- Concept: Multimodal pretraining objectives
  - Why needed here: Understanding why interleaved sequences matter requires knowing how different pretraining formats affect model capabilities
  - Quick check: What's the key difference between training on (image, caption) pairs versus interleaved image-text sequences in terms of what the model learns?

## Architecture Onboarding

- Component map: Common Crawl HTML -> WAT files -> extracted text and image URLs -> Sentence tokenization -> NSFW filtering -> Duplicate detection -> Resolution filtering -> CLIP feature extraction -> Bipartite matching -> Document assembly -> Face detection approximation -> Stricter deduplication -> Similarity thresholding -> Dataset release
- Critical path: Data ingestion -> Preprocessing -> CLIP alignment -> Document assembly -> Filtering -> Dataset release
- Design tradeoffs: The linear assignment algorithm adds computational complexity but improves document structure; face removal improves privacy but discards potentially relevant images; stricter filtering creates higher-quality subsets but reduces scale
- Failure signatures: Low average CLIP similarity scores indicate poor alignment quality; high face detection rates suggest the face removal isn't working; mismatched sentence/image counts suggest issues with the bipartite matching
- First 3 experiments:
  1. Run the pipeline on a small sample (100 documents) and manually verify the alignment quality
  2. Compare linear assignment vs. maximum similarity assignment on the same sample to verify the distribution claims
  3. Test the face detection approximation by running the slower RetinaFace on a sample and comparing to the logistic regression predictions

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- The paper doesn't directly prove that interleaving is the causal mechanism versus simply having more multimodal data at scale
- The face detection approximation using logistic regression introduces uncertainty about actual face content
- The paper doesn't explore whether CLIP zero-shot alignment might fail on certain domains or image types

## Confidence

**High confidence**: The dataset construction methodology is sound, the manual inspection results are reliable, and the CLIP-based alignment approach works effectively as demonstrated by benchmark performance.

**Medium confidence**: The claim that interleaved sequences specifically enable multimodal in-context learning better than paired data, while supported by experimental results, lacks direct causal evidence.

**Medium confidence**: The face removal approximation using logistic regression is practical but hasn't been validated against more accurate methods.

## Next Checks

1. Conduct an ablation study comparing models trained on mmc4-core with models trained on an equivalent amount of paired image-caption data to isolate the interleaving effect.

2. Validate the face detection logistic regression approximation by running RetinaFace on a random sample and comparing results to confirm the face removal effectiveness.

3. Test CLIP zero-shot alignment on domain-specific subsets (medical, technical documentation, etc.) to identify potential failure modes not captured in general benchmarks.