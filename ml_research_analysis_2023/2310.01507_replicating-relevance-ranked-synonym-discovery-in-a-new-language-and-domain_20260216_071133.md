---
ver: rpa2
title: Replicating Relevance-Ranked Synonym Discovery in a New Language and Domain
arxiv_id: '2310.01507'
source_url: https://arxiv.org/abs/2310.01507
tags:
- original
- synonym
- term
- synonyms
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper replicates and extends prior work on ranking domain-specific
  synonyms by applying the approach to Swedish language synonyms in the building construction
  domain. The authors use learning to rank with multiple features including FastText
  embeddings, PMI, LinSim, and domain-specific features.
---

# Replicating Relevance-Ranked Synonym Discovery in a New Language and Domain

## Quick Facts
- arXiv ID: 2310.01507
- Source URL: https://arxiv.org/abs/2310.01507
- Reference count: 30
- Key outcome: Achieved 88% recall@50 for Swedish building construction synonyms using LambdaMART learning to rank

## Executive Summary
This paper replicates and extends prior work on ranking domain-specific synonyms by applying the approach to Swedish language synonyms in the building construction domain. The authors demonstrate that synonym discovery is best approached as a learning to rank task, where human editors can efficiently build domain thesauri by viewing ranked synonym candidates. Using LambdaMART learning to rank with multiple features including FastText embeddings, PMI, LinSim, and domain-specific features, the method achieves 88% recall@50 compared to 76% for the original logistic regression approach.

## Method Summary
The authors apply learning to rank to identify Swedish language synonyms in the building construction domain using a corpus from Wikipedia, Trafikverket documents, and web sources obtained via Bing Search API. They train FastText embeddings on the corpus and implement two learning to rank models (LambdaMART and logistic regression) with features including PMI, LinSim, FastText embeddings, and domain-specific features. The approach filters ~867,000 initial candidates down to ~26,000 and evaluates performance using TOEFL-style metrics including recall@50, MAP@150, MRR, and accuracy@n.

## Key Results
- LambdaMART learning to rank achieves 88% recall@50 in identifying synonyms for target terms
- FastText embeddings alone provide strong baseline performance but are outperformed by LambdaMART
- The approach shows significantly improved performance over the original logistic regression method (76% recall@50)
- Domain-specific features contribute to improved ranking performance over general semantic similarity measures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning to rank is the right paradigm for synonym discovery because human editors need ranked lists to efficiently build domain thesauri.
- Mechanism: The system ranks candidate synonyms by learned probability, so editors can scan top results and select correct synonyms, rather than examining all candidates.
- Core assumption: Human editors can effectively identify true synonyms from a ranked list without needing to examine all candidates.
- Evidence anchors: [abstract], [section], [corpus] Weak - no direct corpus evidence for human editor effectiveness; inferred from task framing.
- Break condition: If editors cannot efficiently identify synonyms from top ranks (e.g., too many false positives or ranking errors), the learning to rank paradigm fails.

### Mechanism 2
- Claim: FastText embeddings provide strong baseline performance by capturing distributional semantics through subword information.
- Mechanism: FastText represents words as vectors that encode semantic relationships based on character n-grams, enabling cosine similarity to identify semantically similar terms.
- Core assumption: Subword information in FastText embeddings captures domain-specific semantic relationships better than PMI-based methods.
- Evidence anchors: [abstract], [section], [corpus] Weak - no direct corpus evidence comparing FastText vs PMI performance; inferred from methodology section.
- Break condition: If domain-specific terminology is too specialized or out-of-vocabulary terms dominate, FastText's subword model may fail to capture meaningful relationships.

### Mechanism 3
- Claim: LambdaMART outperforms logistic regression because pairwise ranking captures relative synonym quality better than pointwise probability.
- Mechanism: LambdaMART optimizes pairwise comparisons between candidate synonyms, learning which candidates are more likely to be synonyms relative to others, rather than absolute probabilities.
- Core assumption: Relative synonym quality (pairwise comparisons) is more informative than absolute synonym probability for ranking tasks.
- Evidence anchors: [abstract], [section], [corpus] Weak - no direct corpus evidence for LambdaMART superiority; inferred from experimental results section.
- Break condition: If the ranking task benefits more from absolute probabilities (e.g., threshold-based synonym selection), LambdaMART's pairwise optimization may not provide advantages.

## Foundational Learning

- Concept: Learning to rank
  - Why needed here: The synonym discovery task requires ordering candidate synonyms by likelihood, not just classifying them as synonyms or non-synonyms.
  - Quick check question: What is the difference between pointwise, pairwise, and listwise learning to rank approaches?

- Concept: Distributional semantics
  - Why needed here: Understanding how word embeddings capture meaning through context is crucial for interpreting FastText performance.
  - Quick check question: How do FastText embeddings differ from traditional word2vec embeddings in handling rare and compound words?

- Concept: Domain adaptation
  - Why needed here: The paper demonstrates that synonym discovery methods must adapt to different domains and languages, not just work universally.
  - Quick check question: What features in the paper specifically address domain-specific characteristics like compound nouns?

## Architecture Onboarding

- Component map: Corpus creation -> Preprocessing -> Candidate generation -> Feature extraction (Windows, LinSim, EmbeddingSim, etc.) -> Learning to rank model (LogReg/LambdaMART) -> Evaluation (TOEFL-style, ranking metrics)
- Critical path: Corpus creation -> Preprocessing -> Candidate generation -> Feature computation -> Model training -> Ranking evaluation
- Design tradeoffs: Larger corpus improves coverage but increases computational cost; more features improve accuracy but increase complexity and risk of overfitting.
- Failure signatures: Poor MAP scores indicate feature extraction issues; low recall suggests corpus coverage problems; inconsistent cross-validation results indicate model instability.
- First 3 experiments:
  1. Run PMI baseline on filtered candidate set to establish minimum performance threshold
  2. Evaluate FastText embeddings alone to measure subword information contribution
  3. Compare LogReg vs LambdaMART on the same feature set to validate pairwise ranking benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of synonym discovery methods change when applied to other language pairs (e.g., English to German, or French to Swedish)?
- Basis in paper: [inferred] The paper only evaluates the approach on Swedish, but the original study was in English. The authors note differences in language and domain may account for performance variations.
- Why unresolved: The paper does not test the method on additional language pairs or provide a systematic analysis of how language-specific factors affect performance.
- What evidence would resolve it: Evaluating the approach on multiple language pairs with varying linguistic characteristics (e.g., compounding, inflection) and comparing performance across these languages.

### Open Question 2
- Question: What is the optimal balance between precision and recall for synonym discovery in different domain-specific applications?
- Basis in paper: [explicit] The authors note that "for use cases where recall is important, such as ours, the task is best approached as a ranking problem" but do not explore the precision-recall tradeoff.
- Why unresolved: The paper focuses on recall@50 but does not systematically explore how different precision-recall tradeoffs might be optimal for different use cases or domains.
- What evidence would resolve it: Conducting a detailed analysis of precision-recall curves across different domains and use cases, and developing guidelines for selecting appropriate tradeoffs.

### Open Question 3
- Question: How can the synonym discovery approach be extended to handle multi-word expressions and idiomatic phrases more effectively?
- Basis in paper: [explicit] The authors note that "many of our target terms are compound nouns" and introduce a decompounding feature, but the evaluation is limited to single-word synonyms.
- Why unresolved: The paper does not evaluate the approach's ability to handle multi-word expressions or idiomatic phrases, which are common in domain-specific terminology.
- What evidence would resolve it: Evaluating the approach on datasets containing multi-word expressions and idiomatic phrases, and developing specialized features or techniques for handling such cases.

## Limitations

- The exact filtering criteria for reducing candidates from ~867,000 to ~26,000 is unspecified, potentially impacting recall performance
- Bing Search API parameters for corpus construction are not specified, making exact replication challenging
- Evaluation relies heavily on TOEFL-style metrics that may not fully capture practical utility for domain thesaurus construction

## Confidence

- **High confidence**: FastText embeddings provide strong baseline performance and LambdaMART outperforms logistic regression in pairwise ranking tasks
- **Medium confidence**: Learning to rank is the optimal paradigm for synonym discovery given human editorial workflows
- **Medium confidence**: Domain-specific features improve ranking performance over general semantic similarity measures

## Next Checks

1. Replicate the candidate filtering process using different thresholds to assess sensitivity of recall@50 to filtering aggressiveness
2. Compare LambdaMART performance against a simple threshold-based classifier using absolute FastText similarity scores to validate pairwise ranking benefits
3. Test the approach on a different domain (e.g., medical terminology) to evaluate generalizability beyond building construction domain