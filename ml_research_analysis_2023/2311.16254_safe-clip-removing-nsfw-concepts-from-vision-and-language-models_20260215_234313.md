---
ver: rpa2
title: 'Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models'
arxiv_id: '2311.16254'
source_url: https://arxiv.org/abs/2311.16254
tags:
- nsfw
- unsafe
- safe
- content
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Safe-CLIP fine-tunes CLIP to remove NSFW concepts, enabling safer
  text-to-image retrieval and generation. It creates a toxic LLM to generate paired
  safe/unsafe captions from images, then fine-tunes CLIP with a novel loss combination
  that redirects unsafe content while preserving safe embedding structure.
---

# Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models

## Quick Facts
- arXiv ID: 2311.16254
- Source URL: https://arxiv.org/abs/2311.16254
- Reference count: 40
- Removes NSFW concepts from CLIP-like vision-and-language models through fine-tuning

## Executive Summary
Safe-CLIP presents a method to remove NSFW concepts from vision-and-language models like CLIP by fine-tuning the text encoder to redirect unsafe embeddings while preserving safe content structure. The approach uses a fine-tuned LLM to generate paired safe/unsafe captions from images, then trains CLIP with a novel multi-loss combination that separates inappropriate content from the embedding space. Experiments demonstrate 10-20 percentage point reductions in NSFW content during both retrieval and generation tasks, with minimal impact on safe content quality.

## Method Summary
Safe-CLIP works by first fine-tuning Llama 2 on manually-curated safe/unsafe sentence pairs to create an NSFW text generator. This generator is used to create paired safe/unsafe captions from COCO Captions images, forming the ViSU dataset. The core innovation involves fine-tuning CLIP's text encoder with four losses: unsafe-to-image (LU→V), unsafe-to-safe (LU→S*), safe-to-safe (LS→S*), and image-to-safe (LV→S) embeddings. This combination redirects unsafe content embeddings toward safe regions while preserving the original safe embedding structure. The approach is validated on both retrieval tasks and text-to-image generation using Stable Diffusion.

## Key Results
- Reduces NSFW content retrieval by 10-20 percentage points compared to baseline CLIP models
- Maintains high retrieval performance on safe content (Recall@1: 68.76, Recall@10: 94.88)
- Significantly decreases NSFW image generation while preserving image quality (minimal FID score impact)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safe-CLIP redirects unsafe text embeddings toward safe regions while preserving the safe embedding space structure.
- Mechanism: Fine-tuning CLIP's text encoder with a combination of losses that pull unsafe text embeddings toward corresponding safe embeddings (LU→S*), push them toward matching image embeddings (LU→V), and preserve the original safe embedding structure (LS→S*, LV→S).
- Core assumption: CLIP's embedding space has compositional properties where unsafe content can be redirected without destroying safe semantic relationships.
- Evidence anchors:
  - [abstract]: "our methodology seeks to sever 'toxic' linguistic and visual concepts, unlearning the linkage between unsafe linguistic or visual items and unsafe regions of the embedding space"
  - [section 3.2]: "we adopt a multi-modal training scheme with four loss functions... two inappropriate content redirection losses... two structure preservation losses"
  - [corpus]: SafeR-CLIP uses similar redirection approach but Safe-CLIP preserves pre-trained knowledge better
- Break condition: If CLIP's embedding space lacks compositional properties or safe/unsafe concepts are too entangled to separate cleanly.

### Mechanism 2
- Claim: A fine-tuned LLM can generate paired safe/unsafe captions that effectively capture the semantic relationships needed for training.
- Mechanism: Fine-tuning Llama 2 with supervised fine-tuning (SFT) on curated safe/unsafe pairs, followed by direct preference optimization (DPO) to improve alignment with safety preferences.
- Core assumption: The LLM can generalize from limited curated examples to generate diverse paired captions that maintain semantic relationships while varying safety levels.
- Evidence anchors:
  - [section 3.1]: "we curate 100 paired sentences... convert safe sentences to unsafe ones... fine-tune Llama 2 with SFT... further fine-tune with DPO"
  - [corpus]: ViSU dataset generation relies on LLM's ability to maintain semantic relationships while changing safety levels
- Break condition: If the LLM fails to maintain semantic relationships between paired captions or generates implausible unsafe content.

## Foundational Learning

### Vision-Language Embeddings
- Why needed: Understanding how CLIP represents the relationship between images and text in a shared embedding space is crucial for understanding Safe-CLIP's redirection mechanism.
- Quick check: Verify CLIP's image-text matching capability on simple paired examples before and after Safe-CLIP fine-tuning.

### Multi-Task Loss Optimization
- Why needed: Safe-CLIP combines four different loss functions during training, requiring understanding of how to balance competing objectives.
- Quick check: Monitor individual loss values during training to ensure none dominate or vanish.

### Text-to-Image Generation Pipelines
- Why needed: Safe-CLIP's effectiveness is evaluated on Stable Diffusion, requiring understanding of how text embeddings influence generated images.
- Quick check: Test Stable Diffusion with various prompts to establish baseline NSFW generation rates.

## Architecture Onboarding

### Component Map
- Curated sentence pairs -> Llama 2 fine-tuning -> NSFW text generator
- COCO Captions images + generated unsafe captions -> ViSU dataset
- ViSU dataset + CLIP model -> Multi-loss fine-tuning -> Safe-CLIP
- Safe-CLIP + Stable Diffusion -> NSFW-reduced image generation

### Critical Path
COCO Captions images → ViSU dataset generation → Safe-CLIP fine-tuning → NSFW content reduction

### Design Tradeoffs
- Using Llama 2 for caption generation trades control for scalability, potentially introducing variability in the quality of paired captions
- The four-loss combination balances content redirection with structure preservation but requires careful hyperparameter tuning

### Failure Signatures
- High NSFW retrieval/generation rates despite fine-tuning
- Degraded performance on safe-only retrieval tasks
- Unstable training indicated by oscillating loss values

### First 3 Experiments
1. Test Safe-CLIP retrieval performance on COCO Captions with and without NSFW filtering
2. Generate images with Stable Diffusion using Safe-CLIP embeddings and measure NSFW content reduction
3. Compare Safe-CLIP's embedding similarity to original CLIP on safe-only content

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Safe-CLIP's performance scale with larger training datasets and more diverse NSFW categories?
- Basis in paper: [explicit] The paper notes that Safe-CLIP was trained on a dataset of 165k paired sentences, but acknowledges that further improvements could be achieved by enlarging the training dataset in both quantity and variance.
- Why unresolved: The paper demonstrates effectiveness with the current dataset but does not explore how performance changes with significantly larger or more diverse training data.
- What evidence would resolve it: Experiments showing Safe-CLIP's performance metrics (NSFW content reduction, safe content preservation) across datasets of varying sizes and with different NSFW category distributions.

### Open Question 2
- Question: Can Safe-CLIP's approach be generalized to remove other types of harmful content beyond NSFW, such as misinformation or hate speech?
- Basis in paper: [inferred] The methodology is described as a general framework for cleaning generative and discriminative models, suggesting potential applicability to other harmful content types.
- Why unresolved: The paper focuses specifically on NSFW content and does not test the approach on other harmful content categories.
- What evidence would resolve it: Experiments applying Safe-CLIP's methodology to other harmful content types (e.g., hate speech, misinformation) with measurable outcomes on content reduction and preservation of legitimate content.

### Open Question 3
- Question: How robust is Safe-CLIP against adversarial attacks that attempt to bypass the NSFW filtering?
- Basis in paper: [explicit] The paper acknowledges that Safe-CLIP might fail to remove inappropriate content under certain conditions and provides some failure case examples.
- Why unresolved: The paper demonstrates effectiveness in controlled experiments but does not systematically test resistance to adversarial prompts or creative bypass attempts.
- What evidence would resolve it: Comprehensive testing using adversarial prompt generation techniques to identify vulnerabilities in Safe-CLIP's filtering, followed by proposed mitigation strategies.

## Limitations

- Training Data Scope: Effectiveness may be limited to NSFW concepts present in the curated dataset and COCO Captions, potentially missing domain-specific or emerging unsafe content.
- Embedding Space Assumptions: Assumes CLIP's embedding space has sufficient compositional properties to cleanly separate safe and unsafe concepts, which may not hold for all variants.
- Generalization Beyond Evaluation Domains: Performance on datasets beyond COCO-based collections or with different generative models remains untested.

## Confidence

- **High Confidence** in retrieval task improvements: Well-supported by multiple experiments with different classifiers and baseline comparisons.
- **Medium Confidence** in generative model improvements: Limited evaluation scope with specific prompts and datasets, unclear impact on diverse generation tasks.
- **Low Confidence** in long-term robustness: No systematic testing against adversarial attacks or concept drift scenarios.

## Next Checks

1. **Cross-dataset Generalization Test**: Evaluate Safe-CLIP's performance on LAION-400M or other large-scale image-text datasets to verify effectiveness beyond COCO Captions.

2. **Adversarial Robustness Assessment**: Design and test adversarial prompts that attempt to bypass Safe-CLIP's filtering mechanisms, measuring vulnerability to prompt engineering attacks.

3. **Long-term Stability Evaluation**: Conduct a longitudinal study by fine-tuning Safe-CLIP on periodically updated NSFW datasets over several months, measuring retention of safe content quality while maintaining NSFW reduction as new unsafe concepts emerge.