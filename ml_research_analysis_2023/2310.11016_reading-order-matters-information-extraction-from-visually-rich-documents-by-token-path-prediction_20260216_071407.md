---
ver: rpa2
title: 'Reading Order Matters: Information Extraction from Visually-rich Documents
  by Token Path Prediction'
arxiv_id: '2310.11016'
source_url: https://arxiv.org/abs/2310.11016
tags:
- entity
- token
- document
- order
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Token Path Prediction (TPP) is introduced to address the reading
  order issue in information extraction from visually-rich documents (VrDs). TPP models
  VrD-NER as predicting token paths within a complete directed graph of tokens, rather
  than relying on token classification with BIO-tags.
---

# Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction

## Quick Facts
- arXiv ID: 2310.11016
- Source URL: https://arxiv.org/abs/2310.11016
- Reference count: 31
- Key outcome: Token Path Prediction (TPP) achieves up to 9.13 F1 score improvement over sequence-labeling methods for VrD-NER on revised FUNSD-r and CORD-r benchmarks.

## Executive Summary
Token Path Prediction (TPP) introduces a novel approach to Named Entity Recognition (NER) in visually-rich documents (VrDs) by modeling entities as token paths in a complete directed graph rather than relying on sequential input order. This method effectively addresses the reading order problem that arises when OCR systems produce disordered text inputs, achieving state-of-the-art performance on revised VrD-NER benchmarks. TPP demonstrates both independent effectiveness as a VrD-NER model and utility as a pre-processing mechanism to improve sequence-labeling methods.

## Method Summary
TPP represents VrD-NER as predicting token paths within a complete directed graph of all document tokens, where each entity corresponds to a directed path. For each entity type, TPP generates an n×n grid where each cell indicates whether two tokens should be linked (1) or not (0), transforming the problem into binary classification tasks. The method uses document transformers (like LayoutLMv3 or LayoutMask) with Global Pointer as the classification model to predict grid labels. During inference, greedy search decodes the predicted links into coherent entity paths. TPP can function independently or as a pre-processing step to reorder inputs for sequence-labeling methods.

## Key Results
- Achieves state-of-the-art VrD-NER performance with up to 9.13 F1 score improvement over sequence-labeling methods
- Effectively handles disordered inputs from OCR systems while maintaining high entity extraction accuracy
- Demonstrates dual utility as both an independent VrD-NER model and pre-processing mechanism for sequence-labeling methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TPP addresses reading order issues by modeling entities as token paths in a complete directed graph, independent of input order.
- Core assumption: Correct entity boundaries can be identified regardless of token appearance order.
- Evidence: TPP constructs complete directed graphs where entities are represented as directed paths between tokens.

### Mechanism 2
- Claim: Grid label representation enables effective binary classification for entity path prediction.
- Core assumption: Binary classification of token pairs can capture entity structure without modeling full sequence order.
- Evidence: n×n grids with binary values for token linking, using class-imbalance loss for training.

### Mechanism 3
- Claim: TPP serves as both independent model and pre-processing mechanism.
- Core assumption: TPP-predicted reading order creates continuous entity spans for sequence-labeling methods.
- Evidence: TPP improves sequence-labeling performance when used for input reordering.

## Foundational Learning

- Concept: Graph-based representation of document structure
  - Why needed: VrD-NER requires modeling relationships between tokens that may not be adjacent due to layout complexity.
  - Quick check: Can you explain how representing a document as a complete directed graph helps address the reading order problem in VrD-NER?

- Concept: Binary classification with class imbalance handling
  - Why needed: TPP's grid approach creates highly imbalanced classification tasks requiring specialized loss functions.
  - Quick check: Why is class-imbalance loss important for TPP's grid label training, and what would happen without it?

- Concept: Sequence decoding from predicted links
  - Why needed: TPP must convert predicted binary token links into coherent entity paths.
  - Quick check: How does TPP convert the predicted binary grid of token links into actual entity mentions?

## Architecture Onboarding

- Component map: Document Transformer Backbone -> Token Path Prediction Head -> Grid Label Representation -> Decoding Module

- Critical path: 1. Document tokens + layout features → Document Transformer, 2. Transformer outputs → TPP Head, 3. TPP Head → Grid label predictions, 4. Grid labels → Filtered positive token pairs, 5. Positive pairs → Greedy path decoding → Entity mentions

- Design tradeoffs: TPP vs sequence-labeling (handles disordered inputs but requires more computation), grid label size (larger documents increase memory/computation), decoding strategy (greedy vs beam search speed/accuracy tradeoff)

- Failure signatures: Poor performance on short entities (class imbalance), incorrect entity type predictions (over-reliance on layout), missing entities in dense layouts (path decoding failure)

- First 3 experiments: 1. Implement TPP with synthetic dataset to verify handling of disordered inputs, 2. Compare TPP's entity boundary detection against sequence-labeling on datasets with reading order issues, 3. Test TPP's pre-processing capability by using predicted reading order to reorder inputs for sequence-labeling models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does computational overhead of TPP scale with document length and complexity?
- Basis: Paper states TPP requires 2.45x/2.75x longer training time and 1.56x/1.47x higher peak memory than vanilla token classification on FUNSD-r, but doesn't explore scaling with document length.
- What would resolve it: Experiments comparing computational costs across documents of varying lengths and complexity.

### Open Question 2
- Question: What is the impact of document language on TPP performance?
- Basis: Paper focuses on English datasets (FUNSD-r and CORD-r) without exploring multilingual performance.
- What would resolve it: Experiments on multilingual datasets demonstrating consistent performance across different languages.

### Open Question 3
- Question: How does TPP compare to sequence-labeling on ordered documents?
- Basis: Paper demonstrates TPP's effectiveness on disordered documents but doesn't compare performance on ordered documents.
- What would resolve it: Experiments comparing TPP and sequence-labeling methods on datasets with correct reading order.

## Limitations
- Computational overhead increases quadratically with document length due to n×n grid label approach
- Limited evaluation on diverse document types beyond FUNSD and CORD datasets
- Claims about universal applicability to other VrD tasks lack comprehensive experimental validation

## Confidence

**High Confidence**: TPP effectively addresses reading order problems for VrD-NER with significant F1 score improvements on revised benchmarks.

**Medium Confidence**: TPP serves as both independent model and pre-processing mechanism, though more ablation studies would strengthen dual-role claims.

**Low Confidence**: TPP as universal solution for other VrD tasks like entity linking and reading order prediction lacks sufficient experimental evidence.

## Next Checks

1. **Scalability and Efficiency Analysis**: Implement TPP on progressively larger documents (100, 500, 1000+ tokens) to measure computational scaling and compare with sequence-labeling methods.

2. **Robustness to OCR Error Types**: Create controlled experiments with systematic reading order errors (random shuffling, local inversions, long-range displacements) to quantify TPP's actual benefit over sequence-labeling methods.

3. **Cross-Domain Generalization**: Test TPP on datasets from different domains (medical forms, invoices, scientific articles) to validate generalization beyond FUNSD and CORD layout patterns.