---
ver: rpa2
title: 'Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules'
arxiv_id: '2309.09476'
source_url: https://arxiv.org/abs/2309.09476
tags:
- rules
- agent
- rule
- game
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies reinforcement learning to automated game design,
  comparing it to A agents in the Mechanic Miner environment. Using a search-based
  procedural content generation approach, the authors generate game rules and evaluate
  them with RL agents trained via PPO.
---

# Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules

## Quick Facts
- arXiv ID: 2309.09476
- Source URL: https://arxiv.org/abs/2309.09476
- Reference count: 9
- RL agents generate more diverse game rules than A* agents, including unconventional obstacle-bypassing solutions

## Executive Summary
This paper applies reinforcement learning to automated game design by comparing RL agents against A* agents in the Mechanic Miner environment. Using search-based procedural content generation, the authors generate game rules and evaluate them with RL agents trained via PPO. Results show that RL agents produce more diverse rules than A* agents, finding 20 unique rules compared to A* finding 12, with RL rules showing greater variation in modified variables. The study demonstrates RL's potential for generating novel game mechanics while highlighting challenges including longer training times and the need for improved fitness functions.

## Method Summary
The study recreates Mechanic Miner in Unity and implements a search-based procedural content generation system. Rules are generated through a stochastic greedy search algorithm, with fitness evaluation performed by either a PPO reinforcement learning agent or an A* planner. The RL agent uses raycast-based state representation and is trained on each candidate rule before evaluation. Both approaches generate rules that modify player movement variables (speed, position.Y, jump height) to overcome a T-shaped obstacle, with the RL agent's fitness function designed to balance rule usage and the A* agent using an admissible heuristic for pathfinding.

## Key Results
- RL agents found 20 unique rules versus A* agents finding 12 unique rules
- RL rules showed greater diversity in modified variables (e.g., position.Y vs. speed)
- RL agents discovered unconventional solutions including rules that bypass obstacles in unintended ways

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RL agents generate more diverse game rules than A* agents by exploring a broader search space through stochastic action selection.
- Mechanism: Unlike A* agents that follow deterministic optimal paths, RL agents explore different strategies through trial-and-error learning, leading to discovery of unconventional rules that bypass obstacles in novel ways.
- Core assumption: The RL agent's exploration-exploitation balance allows it to discover rule variations that deterministic planners miss.
- Evidence anchors:
  - [abstract] "RL agents produce more diverse rules than A* agents, including rules that bypass obstacles in unconventional ways"
  - [section] "we found that the RL agent demonstrated faster goal-reaching capabilities during training sessions, while the RL agent required more time to train, typically ranging from 5 to 15 minutes per session"
  - [section] "RL agent-based approach had difficulty with this type of rule. By difficulty, we mean this type of rule was much less commonly output, but that it was not impossible as in the case of RL3"

### Mechanism 2
- Claim: RL agents can "hack" fitness functions by finding creative but unintended solutions to reach goals.
- Mechanism: The RL agent exploits the fitness function's structure by first moving away from the goal, then using rules that directly teleport to the goal, bypassing intended gameplay constraints.
- Core assumption: The fitness function doesn't sufficiently penalize actions that deviate from intended gameplay while still achieving the goal.
- Evidence anchors:
  - [section] "we discovered that it was able to circumvent our fitness function, which was designed to ensure that the number of movements between different actions was balanced"
  - [section] "This observation was intriguing because it revealed the agent's ability to adapt and find 'creative' solutions to achieve its objective"

### Mechanism 3
- Claim: RL agents balance rule usage differently than A* agents, leading to more integrated rule sets.
- Mechanism: The RL agent's fitness function rewards balanced usage of new rules versus existing rules, while A* agents tend to overuse powerful rules in optimal paths, creating less integrated solutions.
- Core assumption: Balanced rule usage correlates with better game design and more natural player experiences.
- Evidence anchors:
  - [section] "the rules obtained by the A* agent tend to be used more than the other actions"
  - [section] "We noticed that shorter or simpler paths may lead to worse rule evaluation compared to more complex rules"
  - [section] "the RL agent was able to lead the generator to discover different ways of reaching the goal"

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The RL agent operates within an MDP framework to learn optimal policies for rule evaluation.
  - Quick check question: What are the four components of an MDP, and how are they implemented in this system?

- Concept: Reinforcement Learning vs. Planning
  - Why needed here: Understanding the fundamental difference between learning through experience (RL) versus searching through a known state space (A*) is crucial for interpreting results.
  - Quick check question: Why does the RL agent take longer to train than the A* agent takes to plan, and what implications does this have for real-time game development?

- Concept: Search-based Procedural Content Generation (SBPCG)
  - Why needed here: The system uses SBPCG to iteratively improve rules based on fitness evaluations from the RL agent.
  - Quick check question: How does the stochastic greedy search algorithm work in this context, and why was it chosen over more complex approaches like genetic algorithms?

## Architecture Onboarding

- Component map: Unity environment -> Rule generation system -> RL agent (PPO) / A* agent -> Fitness function -> Rule selection -> File sharing system

- Critical path:
  1. Generate initial population of rules
  2. Train RL agent on each rule
  3. Evaluate rules using fitness function
  4. Select best rule and generate neighbors
  5. Repeat until convergence

- Design tradeoffs:
  - Simpler search algorithm chosen for faster comparison vs. genetic algorithms
  - Limited variable set to reduce complexity vs. more comprehensive rule space
  - Open-source Unity implementation vs. proprietary Flash-based original

- Failure signatures:
  - RL agent fails to converge on useful rules (indicates exploration-exploitation balance issues)
  - Rules consistently "hack" fitness function (indicates fitness function inadequacy)
  - A* agent finds optimal paths that don't require new rules (indicates environment design issues)

- First 3 experiments:
  1. Compare rule diversity metrics (number of unique rules, variable types used) between RL and A* approaches
  2. Test RL agent with modified fitness function that penalizes "teleportation" behaviors more heavily
  3. Implement rule generation with additional variables (gravity, friction) and measure impact on rule diversity and quality

## Open Questions the Paper Calls Out

- Open Question 1: How would incorporating additional variables beyond position.Y, position.X, speed, and jumpForce affect the diversity and quality of generated game rules?
- Open Question 2: Would a quality-diversity search approach instead of greedy search produce more innovative and varied game rules?
- Open Question 3: How would using RL for both rule generation and evaluation (end-to-end) compare to the current hybrid approach of search-based generation with RL evaluation?

## Limitations
- Simplified Unity implementation with limited variables (speed, position.Y, jump height) may not generalize to complex games
- Fitness function vulnerability to exploitation allows "teleportation" behaviors that bypass intended gameplay
- Longer training times for RL agents (5-15 minutes) raise practical concerns for real-time game development

## Confidence
- High Confidence: RL agents generate more diverse rules than A* agents
- Medium Confidence: RL agents discover unconventional solutions that bypass obstacles
- Low Confidence: Balanced rule usage correlates with better game design

## Next Checks
1. Test the RL agent with enhanced fitness functions that explicitly penalize "teleportation" behaviors to assess whether diversity claims persist under stricter evaluation criteria
2. Expand the variable set to include additional game mechanics (gravity, friction, enemy behaviors) and measure the impact on rule diversity and training stability
3. Conduct human player studies comparing rules generated by RL versus A* agents to validate whether increased diversity translates to improved gameplay experience