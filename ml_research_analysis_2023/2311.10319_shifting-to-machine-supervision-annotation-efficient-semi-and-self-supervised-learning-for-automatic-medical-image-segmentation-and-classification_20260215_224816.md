---
ver: rpa2
title: 'Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised
  Learning for Automatic Medical Image Segmentation and Classification'
arxiv_id: '2311.10319'
source_url: https://arxiv.org/abs/2311.10319
tags:
- image
- learning
- segmentation
- medical
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces S4MI, a pipeline leveraging self-supervised
  and semi-supervised learning to address the challenge of limited annotated data
  in medical imaging. The authors propose using self-supervised techniques (DINO and
  CASS) for classification and semi-supervised approaches for segmentation, reducing
  reliance on human annotations.
---

# Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification

## Quick Facts
- arXiv ID: 2311.10319
- Source URL: https://arxiv.org/abs/2311.10319
- Reference count: 25
- Self-supervised pretraining with 10% labels matches or exceeds supervised learning with 100% labels for classification

## Executive Summary
This paper introduces S4MI, a pipeline that leverages self-supervised and semi-supervised learning to address the challenge of limited annotated data in medical imaging. The authors propose using self-supervised techniques (DINO and CASS) for classification and semi-supervised approaches for segmentation, significantly reducing reliance on human annotations. Experiments across three medical imaging datasets demonstrate that self-supervised methods outperform traditional supervised learning for classification, while semi-supervised approaches surpass fully-supervised methods using 50% fewer labels for segmentation.

## Method Summary
The paper proposes a two-pronged approach: self-supervised pretraining for classification and semi-supervised learning for segmentation. For classification, DINO and CASS methods perform auxiliary tasks without labels to learn discriminative features, which are then fine-tuned on labeled data. For segmentation, the cross-teaching approach uses two architectures (CNN and Transformer) that alternately predict pseudo-labels for each other on unlabeled data. The pipeline is evaluated on Dermatomyositis, Dermofit, and ISIC-2017 datasets, demonstrating significant performance gains with reduced labeling requirements.

## Key Results
- Self-supervised pretraining with 10% labels approaches or exceeds supervised learning with 100% labels for classification across all datasets
- Semi-supervised segmentation with 50% labeled data outperforms fully-supervised methods with 100% labels across all three datasets
- Cross-architectural self-supervision (CASS) shows superior performance compared to augmentation-based approaches

## Why This Works (Mechanism)

### Mechanism 1
Self-supervised pretraining creates richer feature representations that improve downstream classification accuracy even with limited labels. Self-supervised methods perform auxiliary tasks without labels to learn discriminative visual features, which are then fine-tuned on labeled data. Core assumption: Unlabeled data contains sufficient structural information to learn useful features that transfer to supervised tasks. Break condition: If pretraining data distribution differs significantly from target task data.

### Mechanism 2
Semi-supervised learning leverages both labeled and unlabeled data to improve segmentation performance while using fewer labeled examples. The cross-teaching approach uses two architectures that alternately predict pseudo-labels for each other on unlabeled data, creating a feedback loop that improves both models simultaneously. Core assumption: Predictions from one architecture can serve as reliable pseudo-labels for the other architecture. Break condition: If pseudo-labels become too noisy or if the two architectures make correlated errors.

### Mechanism 3
Architectural diversity in CASS creates more robust feature representations than augmentation-based approaches alone. CASS uses a CNN and Transformer on the same image, maximizing similarity between their embeddings. Core assumption: Different architectures extract complementary information from the same image, and forcing them to produce similar embeddings creates more robust features. Break condition: If architectures are too similar and make correlated errors.

## Foundational Learning

- **Concept: Transfer learning limitations in medical imaging**
  - Why needed here: The paper states transfer learning works best when source and target datasets are similar, which is not always possible in medical imaging
  - Quick check question: Why might ImageNet-pretrained models perform poorly on specialized medical imaging tasks?

- **Concept: Self-supervised learning objectives**
  - Why needed here: The paper uses self-supervised pretraining as a key mechanism for improving classification with limited labels
  - Quick check question: What is the difference between DINO's augmentation-based approach and CASS's architecture-based approach?

- **Concept: Semi-supervised learning strategies**
  - Why needed here: The segmentation results show that semi-supervised approaches outperform fully-supervised methods using 50% fewer labels
  - Quick check question: How does the cross-teaching mechanism in semi-supervised learning differ from traditional self-training approaches?

## Architecture Onboarding

- **Component map**: Data preprocessing pipeline (resizing, normalization, tiling) -> Pretraining modules (DINO/CASS for classification, PiCIE for unsupervised segmentation) -> Fine-tuning modules (supervised training with reduced labels) -> Evaluation modules (F1 score for classification, IoU for segmentation) -> Cross-architecture training loop (for semi-supervised segmentation)

- **Critical path**: 1. Load and preprocess medical imaging dataset 2. Apply self-supervised pretraining (DINO/CASS) on unlabeled data 3. Fine-tune on labeled subset (10% or 100% for classification, various fractions for segmentation) 4. Evaluate performance against supervised baselines 5. For segmentation: implement cross-teaching between CNN and Transformer architectures

- **Design tradeoffs**: Using ViT vs ResNet: ViT shows better performance with CASS but requires more computational resources; Label fraction vs performance: 10% labels with self-supervised pretraining approaches 100% labels with transfer learning; Architectural complexity: Adding cross-architecture training increases training time but improves semi-supervised segmentation

- **Failure signatures**: Poor pretraining convergence: Check learning rates and augmentation strategies; Overfitting with few labels: Implement stronger regularization or data augmentation; Semi-supervised instability: Monitor pseudo-label quality and adjust loss weighting

- **First 3 experiments**: 1. Reproduce baseline transfer learning results on Dermatomyositis dataset with 100% labels 2. Implement DINO pretraining on Dermatomyositis dataset, then fine-tune with 10% labels 3. Set up semi-supervised segmentation pipeline with 50% labeled data on ISIC-2017 dataset

## Open Questions the Paper Calls Out

- **Open Question 1**: How do self-supervised learning methods like DINO and CASS perform across different types of medical imaging modalities (e.g., histopathology, dermatology, radiology)? Basis: The paper evaluates DINO and CASS on three distinct medical imaging datasets but focuses on histopathology and dermatology. Unresolved: Limited to specific datasets without exploring broader range of modalities. Evidence needed: Comparative experiments across wider variety of medical imaging modalities.

- **Open Question 2**: What is the impact of varying the amount of labeled data on the performance of semi-supervised segmentation methods in medical imaging? Basis: The paper evaluates semi-supervised methods across four label fractions (10%, 50%, 70%, 100%). Unresolved: Does not explore label fractions below 10% or above 100%. Evidence needed: Experiments with label fractions below 10% and above 100%.

- **Open Question 3**: How do unsupervised segmentation methods like PiCIE perform in datasets with multiple foreground objects or more complex structures? Basis: The paper mentions PiCIE performs well in datasets with a single foreground object. Unresolved: Focuses on datasets with single dominant object. Evidence needed: Testing PiCIE on datasets with multiple foreground objects.

## Limitations
- Lacks specific hyperparameter details for self-supervised pretraining and semi-supervised segmentation methods
- No comparison with other state-of-the-art semi-supervised approaches like FixMatch or Mean Teacher
- Limited ablation studies on architectural choices (e.g., ViT vs ResNet impact on performance)

## Confidence

- **High confidence**: Self-supervised pretraining improves classification with limited labels
- **Medium confidence**: Semi-supervised segmentation outperforms fully-supervised methods with 50% fewer labels
- **Low confidence**: Cross-architectural approaches (CASS) are universally superior to augmentation-based methods

## Next Checks
1. Perform ablation study: Compare DINO vs CASS pretraining effectiveness on Dermatomyositis classification with 10% labels
2. Conduct statistical validation: Perform paired t-tests on segmentation results across all three datasets to confirm significance of semi-supervised improvements
3. Execute architecture analysis: Evaluate whether ViT-based CASS consistently outperforms ResNet-based DINO across different label fractions and datasets