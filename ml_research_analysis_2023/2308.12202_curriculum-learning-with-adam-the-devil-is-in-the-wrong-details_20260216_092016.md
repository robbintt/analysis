---
ver: rpa2
title: 'Curriculum Learning with Adam: The Devil Is in the Wrong Details'
arxiv_id: '2308.12202'
source_url: https://arxiv.org/abs/2308.12202
tags:
- learning
- curriculum
- adam
- curricula
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the brittleness of curriculum learning
  (CL) when applied with the Adam optimizer in natural language processing (NLP) tasks.
  The authors show that the apparent learning benefits of CL methods, such as the
  commentaries approach, are primarily due to interactions with Adam's momentum terms
  rather than a beneficial data ordering.
---

# Curriculum Learning with Adam: The Devil Is in the Wrong Details

## Quick Facts
- arXiv ID: 2308.12202
- Source URL: https://arxiv.org/abs/2308.12202
- Reference count: 32
- One-line primary result: Curriculum learning with Adam shows apparent benefits only under suboptimal hyperparameters, not due to genuine data ordering advantages

## Executive Summary
This paper investigates why curriculum learning (CL) appears to improve performance when used with the Adam optimizer in NLP tasks. Through extensive experimentation, the authors demonstrate that CL's apparent benefits are not due to beneficial data ordering, but rather artifacts of interactions between the curriculum structure and Adam's momentum terms. Specifically, when difficulty measures correlate with gradient norms, curriculum scheduling scales gradients over time, which interacts with Adam's asymmetric momentum decay to artificially inflate parameter updates, mimicking higher effective learning rates.

The study reveals that these interactions only manifest under suboptimal hyperparameter choices. When learning rates are properly tuned, plain Adam without any curriculum consistently outperforms all tested CL approaches. This finding challenges the conventional wisdom that curriculum learning inherently benefits optimization, suggesting instead that CL methods are brittle and only effective when hyperparameters are not well-optimized.

## Method Summary
The researchers implemented both automated (commentaries) and hand-crafted curricula across vision (CIFAR10/100) and language (GLUE benchmark) tasks. For commentaries, they trained a teacher model to optimize practice students' validation loss, then used the teacher to guide curriculum ordering. Hand-crafted curricula used difficulty measures like sequence length and loss with discrete scheduling functions. All experiments used Adam optimizer with varying β1, β2, and learning rate values to test interaction effects.

## Key Results
- Curriculum learning shows benefits only under suboptimal learning rates (γ = 4e-6), not at optimal rates (γ = 2e-5)
- When β1 = β2 (symmetric momentum), curriculum-Adam interactions vanish and performance drops to baseline
- Plain Adam without curriculum outperforms all CL approaches when hyperparameters are properly tuned

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Curriculum learning benefits in Adam are artifacts of optimizer-momentum interactions, not data ordering.
- Mechanism: When curriculum weight w_i increases over time, it scales the gradient |g_i|, causing Adam's momentum term m_i to grow faster than v_i due to asymmetric decay rates β1 < β2. This leads to artificially inflated parameter updates |∆θ_i| that mimic a higher effective learning rate.
- Core assumption: Difficulty measures (e.g., sequence length, loss) are correlated with gradient norms |g|, and Adam's asymmetric momentum decay is the key interaction point.
- Evidence anchors:
  - [abstract] "the apparent learning benefits...are primarily due to interactions with Adam's momentum terms"
  - [section 4] "the only components sensitive to changes with time are the two momentum terms m_i and v_i"
  - [corpus] "Toward Understanding Why Adam Converges Faster Than SGD for Transformers" (directly relevant)
- Break condition: If β1 = β2 (symmetric momentum), the curriculum-Adam interaction vanishes and performance drops to baseline.

### Mechanism 2
- Claim: Hand-crafted curricula also trigger Adam-curriculum interactions when difficulty measures correlate with gradient norms.
- Mechanism: Ordering examples by sequence length or loss implicitly orders them by |g|. This creates a gradient size schedule that, when combined with Adam's asymmetric momentum, scales |∆θ_i| similarly to adaptive learning rates.
- Core assumption: Longer sequences or higher loss examples systematically produce smaller gradients, and this correlation is preserved in the curriculum ordering.
- Evidence anchors:
  - [section 4.2] "difficulty measures like sequence lengths...are oftentimes correlated with the size of the gradients |g|"
  - [section 4.2] "we expect that they might be affected by interactions with Adam nevertheless"
  - [corpus] "A Theoretical and Empirical Study on the Convergence of Adam with an 'Exact' Constant Step Size" (related but not direct evidence)
- Break condition: If the difficulty measure is uncorrelated with |g|, the interaction effect disappears.

### Mechanism 3
- Claim: Properly tuned hyperparameters eliminate the need for curriculum-induced learning rate scaling.
- Mechanism: When learning rate γ is optimal, vanilla Adam already provides sufficient optimization dynamics. Curriculum-induced scaling of |∆θ_i| becomes redundant or harmful, leading to instability or no improvement.
- Core assumption: There exists an optimal γ for each task/model combination, and curriculum effects only manifest under suboptimal γ.
- Evidence anchors:
  - [abstract] "when hyperparameters are properly tuned, plain Adam without any curriculum outperforms all tested CL approaches"
  - [section 3.2] "Changes in hyperparameters...erase any curriculum advantage"
  - [corpus] "Automatic Gradient Descent: Deep Learning without Hyperparameters" (suggests hyperparameter tuning can replace heuristics)
- Break condition: If the task/model combination is sensitive to learning rate, curriculum scaling may still provide marginal benefits.

## Foundational Learning

- Concept: Adam optimizer mechanics (momentum terms m_i, v_i, decay rates β1, β2)
  - Why needed here: The entire paper's mechanism hinges on how Adam's asymmetric momentum interacts with curriculum-induced gradient scaling.
  - Quick check question: What happens to Adam's update step when β1 = β2 = 0.99? (Answer: It becomes equivalent to SGD with momentum)

- Concept: Curriculum learning structure (difficulty measures, scheduling functions)
  - Why needed here: Understanding the universal "curriculum structure" shift is key to generalizing the interaction findings beyond specific implementations.
  - Quick check question: What are the two main components every CL approach must define? (Answer: Difficulty measure and scheduling function)

- Concept: Gradient norm correlation with difficulty measures
  - Why needed here: The interaction mechanism requires that difficulty measures be correlated with gradient sizes |g|.
  - Quick check question: In the paper's experiments, which two difficulty measures were shown to correlate with |g|? (Answer: Sequence length and loss)

## Architecture Onboarding

- Component map: Curriculum learning module (difficulty measure + scheduler) → Adam optimizer (β1, β2, γ) → Model parameters θ
- Critical path: Data ordering → Gradient scaling → Momentum interaction → Parameter update → Performance
- Design tradeoffs: Curriculum learning adds complexity but only helps under suboptimal hyperparameters; removing it simplifies training without loss of performance when γ is well-tuned.
- Failure signatures: Curriculum learning shows benefits only at specific γ values; performance becomes unstable across runs; benefits disappear when β1 = β2.
- First 3 experiments:
  1. Replicate the CIFAR10 2-layer CNN experiment with and without commentaries teacher at different γ values to observe the interaction threshold.
  2. Implement β1 = β2 = 0.99 Adam and train with commentaries to verify the interaction mechanism.
  3. Create a hand-crafted curriculum based on sequence length and test it with both low and optimal γ to confirm the interaction generalizes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different curriculum learning methods interact with Adam optimizer variants like AdamW or AdaBelief, and do these interactions similarly affect parameter update scaling?
- Basis in paper: [inferred] The paper extensively analyzes Adam-curriculum interactions but doesn't explore other Adam variants
- Why unresolved: The study focused specifically on standard Adam, leaving open whether other variants have similar brittleness issues
- What evidence would resolve it: Experiments comparing curriculum learning performance across different Adam variants with optimal hyperparameter tuning

### Open Question 2
- Question: What are the theoretical foundations explaining why curriculum structure interacts with Adam's momentum terms to scale parameter updates?
- Basis in paper: [explicit] "Theoretical explanations of the efficiency of curriculum learning remain relatively sparse" - the paper provides empirical observations but no theoretical framework
- Why unresolved: The paper identifies the interaction mechanism but doesn't provide mathematical or theoretical justification for why this occurs
- What evidence would resolve it: Mathematical analysis connecting curriculum structure, gradient scaling, and momentum term dynamics

### Open Question 3
- Question: How does the Adam-curriculum interaction phenomenon manifest in larger language models beyond RoBERTa BASE, such as BERT-large or GPT-style models?
- Basis in paper: [inferred] Experiments were limited to small transformer models and CNNs, suggesting scalability questions
- Why unresolved: The study used relatively small models, leaving uncertainty about whether the phenomenon scales to larger architectures
- What evidence would resolve it: Comparative experiments with larger pre-trained models showing whether the interaction persists at scale

### Open Question 4
- Question: Are there curriculum learning methods that are inherently robust to optimizer interactions, or is some form of interaction inevitable with momentum-based optimizers?
- Basis in paper: [explicit] "We can summarise the results of our first set of experiments as follows..." suggesting the interaction is a general phenomenon
- Why unresolved: The paper shows interactions exist but doesn't explore whether any curriculum design can avoid them entirely
- What evidence would resolve it: Testing curriculum methods specifically designed to minimize gradient magnitude variations or using alternative optimization strategies

## Limitations
- The mechanism is empirically demonstrated but not theoretically proven
- Findings primarily validated on NLP tasks and one vision task, limiting generalizability
- The study focuses on Adam optimizer without exploring other variants

## Confidence
- Core claim that curriculum learning benefits are artifacts of suboptimal hyperparameter tuning: Medium-High
- Specific mechanism involving Adam's momentum terms: Medium
- Assertion that properly tuned Adam always outperforms curriculum learning: Low-Confidence

## Next Checks
1. **Cross-domain replication**: Test the curriculum-Adam interaction on non-NLP tasks (e.g., vision transformers, reinforcement learning) to verify if the mechanism generalizes beyond the current experimental scope.

2. **Theoretical formalization**: Derive mathematical bounds on the curriculum-Adam interaction by analyzing the convergence properties of Adam with time-varying gradient schedules, building on existing theoretical work on Adam's convergence.

3. **Curriculum-agnostic validation**: Design a synthetic curriculum that decouples gradient scaling from data ordering (e.g., random permutations with magnitude weighting) to isolate whether the interaction is truly about ordering or just gradient size manipulation.