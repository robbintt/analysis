---
ver: rpa2
title: A Novel Framework for Policy Mirror Descent with General Parameterization and
  Linear Convergence
arxiv_id: '2301.13139'
source_url: https://arxiv.org/abs/2301.13139
tags:
- policy
- mirror
- convergence
- have
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel framework for policy optimization
  in reinforcement learning, specifically focusing on mirror descent methods that
  can handle general parameterizations beyond tabular or log-linear policies. The
  authors define a Bregman projected policy class that recovers known policy classes
  and accommodates any parametrization function.
---

# A Novel Framework for Policy Mirror Descent with General Parameterization and Linear Convergence

## Quick Facts
- **arXiv ID**: 2301.13139
- **Source URL**: https://arxiv.org/abs/2301.13139
- **Reference count**: 40
- **Primary result**: Introduces AMPO algorithm achieving linear convergence for general policy parameterizations without explicit regularization

## Executive Summary
This paper presents a novel framework for policy optimization in reinforcement learning that extends mirror descent methods to handle general parameterizations beyond tabular or log-linear policies. The authors define a Bregman projected policy class that recovers known policy classes (softmax, log-linear, neural) and generates new ones based on the choice of mirror map. They propose the Approximate Mirror Policy Optimization (AMPO) algorithm, which extends actor-critic methods to this new policy class and establishes both sublinear and linear convergence rates without requiring explicit regularization or bounded updates. The framework also demonstrates that actor error can be made arbitrarily small when using shallow neural networks, improving upon previous sample complexity results.

## Method Summary
The AMPO algorithm operates through a two-step procedure: first updating dual parameters via mirror descent in the dual space, then projecting back to the probability simplex using Bregman divergence. The policy class is defined as the Bregman projection of the dual gradient step, allowing any parameterization function to be used within the same optimization structure. The algorithm uses function approximation for both actor and critic components, with the critic estimating the Q-function and the actor solving a function approximation problem to minimize expected actor error. Theoretical analysis establishes convergence rates under different step-size schedules, while empirical validation demonstrates performance on classic control tasks.

## Key Results
- AMPO achieves linear convergence for general parameterizations without explicit regularization or bounded updates
- The framework recovers known policy classes (softmax, log-linear, neural) and generates new ones depending on mirror map choice
- Actor error with shallow neural networks can be made arbitrarily small as a function of network width
- Convergence analysis establishes sublinear and linear rates under different step-size schedules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework recovers known policy classes (softmax, log-linear, neural) and generates new ones depending on the choice of mirror map.
- Mechanism: By defining the policy class via Bregman projection of the dual gradient step, the framework decouples the policy update from the parametrization, allowing any parameterization function to be used within the same optimization structure.
- Core assumption: The mirror map is strictly convex, continuously differentiable, and essentially smooth, ensuring the existence of the Bregman projection and its non-expansivity property.
- Evidence anchors:
  - [abstract]: "The policy class induced by our scheme recovers known classes, e.g., softmax, log-linear, and neural policies. It also generates new ones, depending on the choice of the mirror map."
  - [section]: Example 4.2 shows how the negative entropy mirror map recovers the softmax policy class, while Example 4.4 shows recovery of natural policy gradient methods.
  - [corpus]: The corpus mentions "Dual Approximation Policy Optimization" and "Convergence of Policy Mirror Descent Beyond Compatible Function Approximation" as related works, suggesting that general function approximation in policy mirror descent is an active research area, supporting the novelty of this claim.
- Break condition: If the mirror map is not strictly convex or does not satisfy the essential smoothness condition, the Bregman projection may not be well-defined or may not have the required properties for the analysis.

### Mechanism 2
- Claim: The Approximate Mirror Policy Optimization (AMPO) algorithm achieves linear convergence for general parameterizations without requiring explicit regularization or bounded updates.
- Mechanism: The algorithm uses a two-step procedure (dual update followed by Bregman projection) that allows the use of function approximation while maintaining theoretical guarantees. The key is Lemma 5.1, which extends the three-point descent lemma to account for general parameterization functions.
- Core assumption: The actor and critic errors are bounded, and the concentrability coefficients C1 and C2 are finite.
- Evidence anchors:
  - [abstract]: "we obtain the first result that guarantees linear convergence for a policy-gradient-based method involving general parameterization."
  - [section]: Theorem 5.3 states linear convergence rates for AMPO under different step-size schedules, and Lemma 5.1 is described as the key technical ingredient that enables this result.
  - [corpus]: The corpus mentions "Policy Mirror Descent with Temporal Difference Learning: Sample Complexity under Online Markov Data" as a related work, indicating that temporal difference learning is used for critic evaluation in the AMPO algorithm.
- Break condition: If the actor or critic errors are too large, or if the concentrability coefficients are infinite, the convergence rates may degrade to sublinear or the algorithm may fail to converge.

### Mechanism 3
- Claim: The error from using a general parameterization function (e.g., shallow neural networks) can be made arbitrarily small as a function of the width of the network.
- Mechanism: By extending the function approximation framework of Ji et al. (2019) to the policy optimization setting, the actor error can be bounded in terms of the modulus of continuity of the target function and the width of the neural network.
- Core assumption: The target function belongs to a function class that can be approximated by shallow ReLU networks, and the sampling scheme satisfies certain conditions.
- Evidence anchors:
  - [abstract]: "we provide its sample complexity when using shallow neural networks, show that it is an improvement upon the previous best results."
  - [section]: Theorem 5.5 gives an explicit bound on the actor error in terms of the width of the shallow ReLU network and the modulus of continuity of the target function.
  - [corpus]: The corpus does not provide specific evidence for this mechanism, but the existence of related works on neural network function approximation in RL suggests that this is a plausible claim.
- Break condition: If the target function cannot be well-approximated by shallow ReLU networks, or if the sampling scheme does not satisfy the required conditions, the actor error bound may not hold.

## Foundational Learning

- Concept: Bregman divergence and its properties
  - Why needed here: The Bregman divergence is the key tool used to define the policy class and to analyze the convergence of the AMPO algorithm. It provides a way to measure the distance between policies and to ensure that the policy updates are well-behaved.
  - Quick check question: What is the relationship between the Bregman divergence and the mirror map? How does the non-expansivity property of the Bregman projection ensure that the policy updates are stable?

- Concept: Mirror descent and its application to reinforcement learning
  - Why needed here: The AMPO algorithm is based on the mirror descent framework, which provides a principled way to update the policy in the dual space and then project back to the probability simplex. This allows the algorithm to handle general parameterizations and to achieve good convergence properties.
  - Quick check question: How does the mirror descent update differ from a standard gradient descent update? What are the benefits of using mirror descent in the context of policy optimization?

- Concept: Function approximation and its role in reinforcement learning
  - Why needed here: The AMPO algorithm allows for the use of general function approximation schemes, such as neural networks, to represent the policy. This is important for handling large or continuous state and action spaces, where tabular methods are not feasible.
  - Quick check question: What are the challenges of using function approximation in reinforcement learning? How does the AMPO algorithm address these challenges?

## Architecture Onboarding

- Component map: Dual parameters -> Mirror descent update -> Bregman projection -> Policy parameters -> Function approximation -> Actor and Critic components

- Critical path:
  1. Initialize the policy parameters.
  2. Evaluate the current policy using the critic to obtain an estimate of the Q-function.
  3. Update the policy parameters using the actor to minimize the expected actor error.
  4. Project the updated parameters back to the policy space using the Bregman projection.
  5. Repeat steps 2-4 until convergence or a stopping criterion is met.

- Design tradeoffs:
  - Choice of mirror map: Different mirror maps can lead to different policy classes and convergence properties. The negative entropy mirror map recovers the softmax policy class, while other mirror maps can generate new policy classes.
  - Function approximation scheme: The choice of function approximation scheme (e.g., neural networks) affects the actor error and the sample complexity of the algorithm.
  - Step-size schedule: The choice of step-size schedule (e.g., non-decreasing vs. geometrically increasing) affects the convergence rate of the algorithm.

- Failure signatures:
  - Large actor or critic errors: If the actor or critic errors are too large, the convergence rates may degrade or the algorithm may fail to converge.
  - Infinite concentrability coefficients: If the concentrability coefficients C1 and C2 are infinite, the convergence analysis breaks down.
  - Poor function approximation: If the function approximation scheme cannot well-approximate the target function, the actor error may be large.

- First 3 experiments:
  1. Implement the AMPO algorithm with a simple mirror map (e.g., negative entropy) and a tabular parameterization on a small MDP to verify the convergence properties.
  2. Experiment with different mirror maps and parameterizations to see how they affect the policy class and convergence rates.
  3. Test the algorithm on a larger MDP with a neural network parameterization to verify the scalability and sample complexity claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal mirror map for reinforcement learning problems with specific Markov Decision Process (MDP) structures?
- Basis in paper: [explicit] The paper states: "Given the extensive literature on mirror maps (Orabona, 2020; Vaškevičius et al., 2020; Ghai et al., 2020), we expect our approach to pave the way for the use of mirror maps tailored for MDP structures."
- Why unresolved: The paper introduces a framework that can accommodate any mirror map but does not provide guidance on which mirror maps are optimal for specific MDP structures.
- What evidence would resolve it: Experimental comparisons of AMPO performance using different mirror maps on MDPs with known optimal policies or well-studied structures (e.g., sparse rewards, delayed rewards, etc.) would identify which mirror maps are most effective for specific problem types.

### Open Question 2
- Question: How does the choice of step-size schedule affect the performance and convergence of AMPO in practice?
- Basis in paper: [explicit] The paper discusses two step-size schedules: non-decreasing (for sublinear convergence) and geometrically increasing (for linear convergence). It states: "When η = c/T, for some constant c > 0, the total Bregman divergence of the path of the algorithm is always upper bounded by the expected Bregman divergence between the optimal and the initial policy plus a constant error term proportional to c."
- Why unresolved: While the paper provides theoretical guarantees for different step-size schedules, it does not explore the practical implications of these choices on algorithm performance, such as speed of convergence, stability, or sensitivity to hyperparameters.
- What evidence would resolve it: Empirical studies comparing the performance of AMPO using different step-size schedules (including non-monotonic schedules) on a variety of reinforcement learning tasks would reveal the practical trade-offs and guide the selection of step-size schedules for different problem types.

### Open Question 3
- Question: What are the sample complexity and computational complexity of AMPO compared to other policy optimization methods?
- Basis in paper: [explicit] The paper states: "To showcase the ability of our framework to accommodate general parametrization schemes, we present a case study involving shallow neural networks." However, it does not provide a comprehensive comparison of AMPO's sample complexity or computational complexity with other policy optimization methods.
- Why unresolved: While the paper establishes theoretical convergence rates, it does not quantify the number of samples or computational resources required for AMPO to achieve a certain level of performance compared to existing methods like PPO, TRPO, or NPG.
- What evidence would resolve it: Experimental studies comparing the sample efficiency and computational cost of AMPO with other policy optimization methods on a range of reinforcement learning tasks would provide insights into the practical advantages and limitations of AMPO in terms of data and computational requirements.

## Limitations
- The analysis relies on strong assumptions about function approximation error and concentrability coefficients
- Empirical validation is limited to classic control tasks, which may not capture performance in complex domains
- Extension to deep neural networks beyond shallow architectures remains an open question
- Scalability to high-dimensional problems and continuous action spaces is not fully explored

## Confidence
**High Confidence**: The convergence analysis for AMPO under different step-size schedules (Theorem 5.3) is well-established, with the key technical contribution (Lemma 5.1) providing a clear mechanism for extending three-point descent to general parameterizations. The recovery of known policy classes through different mirror maps is also well-supported by the mathematical framework.

**Medium Confidence**: The actor error bounds for shallow neural networks (Theorem 5.5) build on existing function approximation results but require careful verification of sampling conditions and network architecture choices. The empirical validation on classic control tasks provides supporting evidence but with limited scope.

**Low Confidence**: The generalizability of the results to deep neural networks, continuous action spaces, and complex environments remains uncertain without further theoretical and empirical investigation.

## Next Checks
1. **Theoretical validation**: Rigorously verify the concentrability coefficient assumptions by conducting a systematic analysis of the distribution mismatch between target and behavior policies across different environments and mirror maps.

2. **Empirical scalability test**: Implement AMPO with deep neural networks (multiple hidden layers) on benchmark continuous control tasks (e.g., MuJoCo environments) to assess the algorithm's performance beyond shallow architectures and classic control settings.

3. **Function approximation robustness**: Conduct experiments varying the neural network width and depth to empirically validate the actor error bounds and identify the practical limitations of the function approximation scheme in the AMPO framework.