---
ver: rpa2
title: A Topical Approach to Capturing Customer Insight In Social Media
arxiv_id: '2307.11775'
source_url: https://arxiv.org/abs/2307.11775
tags:
- topic
- topics
- page
- data
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis addresses the challenge of unsupervised topic extraction
  from massive, heterogeneous, and noisy social media data in an industrial context.
  The author proposes three nonparametric models built on the Variational Autoencoder
  framework: the Embedded Dirichlet Process (EDP), the Embedded Hierarchical Dirichlet
  Process (EHDP), and the Dynamic Embedded Dirichlet Process (D-EDP).'
---

# A Topical Approach to Capturing Customer Insight In Social Media

## Quick Facts
- arXiv ID: 2307.11775
- Source URL: https://arxiv.org/abs/2307.11775
- Reference count: 40
- One-line primary result: Three nonparametric VAE-based topic models achieve equal or better performance than state-of-the-art methods on benchmark and industrial datasets.

## Executive Summary
This thesis addresses the challenge of unsupervised topic extraction from massive, heterogeneous, and noisy social media data in an industrial context. The author proposes three nonparametric models built on the Variational Autoencoder framework: the Embedded Dirichlet Process (EDP), the Embedded Hierarchical Dirichlet Process (EHDP), and the Dynamic Embedded Dirichlet Process (D-EDP). These models automatically determine the number of topics, compute word and topic embeddings, and capture temporal dynamics. Evaluated on benchmark and industrial datasets, the models achieve equal or better performance than state-of-the-art methods in terms of perplexity and topic quality. The D-EDP achieves a perplexity of 1251.9 and a topic quality of 0.1660 on an English industrial dataset. The work demonstrates the effectiveness of these models for customer insight extraction and highlights the need for improved evaluation metrics in topic modeling.

## Method Summary
The thesis proposes three nonparametric topic models based on the Variational Autoencoder framework: Embedded Dirichlet Process (EDP), Embedded Hierarchical Dirichlet Process (EHDP), and Dynamic Embedded Dirichlet Process (D-EDP). These models use Dirichlet Processes with stick-breaking construction to automatically determine the number of topics, compute word and topic embeddings in a shared semantic space, and capture temporal dynamics through Markovian topic embedding transitions. The models are trained using online stochastic variational inference with the Adam optimizer and evaluated on benchmark datasets (20 Newsgroups, HADR) and industrial datasets (English and French social media data) using perplexity and topic quality metrics.

## Key Results
- D-EDP achieves a perplexity of 1251.9 and a topic quality of 0.1660 on an English industrial dataset.
- Models demonstrate equal or better performance than state-of-the-art methods on benchmark datasets.
- Nonparametric approach automatically determines the number of topics without prior specification.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The nonparametric nature of the EDP and EHDP allows automatic determination of the number of topics without prior specification.
- Mechanism: These models use Dirichlet Processes with stick-breaking construction to sample topic weights, enabling dynamic topic emergence based on data complexity.
- Core assumption: The data contains latent topic structure that can be discovered without pre-specifying the number of topics.
- Evidence anchors:
  - [abstract] "These nonparametric approaches concerning topics present the particularity of determining word embeddings and topic embeddings."
  - [section] "These models can automatically detect the number of topics in a corpus and compute word embeddings and topic embeddings for better text exploration."
- Break condition: If the data lacks clear topic structure or if the concentration parameter is poorly chosen, the model may fail to identify meaningful topics or overfit noise.

### Mechanism 2
- Claim: Word and topic embeddings in the same space enable semantic exploration and disambiguation.
- Mechanism: The log-linear decomposition of word probabilities using dot products between word and topic embeddings allows direct comparison of semantic similarity between words and topics.
- Core assumption: Words and topics can be meaningfully represented as vectors in a shared semantic space where proximity indicates similarity.
- Evidence anchors:
  - [abstract] "These embeddings do not require transfer learning, but knowledge transfer remains possible."
  - [section] "The EDP decomposes the word level in a dot product between the (transposed) word embeddings ρ and the context embeddings ϕ."
- Break condition: If the embedding dimension is too low or the training data is insufficient, the semantic relationships may not be captured accurately.

### Mechanism 3
- Claim: The Dynamic extension captures temporal evolution of topics through Markovian topic embedding transitions.
- Mechanism: Topic embeddings evolve over time slices with Gaussian noise, allowing the model to track how topics change in semantic content across time periods.
- Core assumption: Topics exhibit temporal dynamics that can be modeled as Markovian transitions in embedding space.
- Evidence anchors:
  - [abstract] "We present three approaches we built on the Variational Autoencoder framework: the Embedded Dirichlet Process, the Embedded Hierarchical Dirichlet Process, and the time-aware Dynamic Embedded Dirichlet Process."
  - [section] "The D-EDP uses a Markov Chain over the topic embeddings such that the topic representations evolve with Gaussian noise with variance σ²."
- Break condition: If the time granularity is too coarse or too fine, the temporal dependencies may not be properly captured, leading to poor topic evolution modeling.

## Foundational Learning

- Concept: Dirichlet Processes and stick-breaking construction
  - Why needed here: To enable nonparametric topic modeling where the number of topics is determined by the data rather than pre-specified.
  - Quick check question: What is the role of the concentration parameter in a Dirichlet Process and how does it affect the number of topics generated?

- Concept: Variational Autoencoders and reparameterization trick
  - Why needed here: To enable scalable inference for complex topic models with continuous latent variables.
  - Quick check question: Why can't we use the reparameterization trick directly with Beta distributions, and what alternative does the paper propose?

- Concept: Word embeddings and semantic similarity
  - Why needed here: To enable the model to capture semantic relationships between words and topics in a shared vector space.
  - Quick check question: How does the log-linear decomposition of word probabilities using dot products enable semantic exploration in the model?

## Architecture Onboarding

- Component map: Data → Encoder → Stochastic Layer → Decoder → ELBO computation → Parameter update via backpropagation
- Critical path: Data → Encoder → Stochastic Layer → Decoder → ELBO computation → Parameter update via backpropagation
- Design tradeoffs: Nonparametric vs parametric (flexibility vs computational efficiency), shared embedding space vs separate spaces (semantic exploration vs model complexity), temporal dynamics vs static models (realistic but more complex)
- Failure signatures: Posterior collapse (model generates NaNs), poor perplexity on held-out data (overfitting), low topic coherence (model not capturing meaningful topics), embedding collapse (topics and words not distinguishable)
- First 3 experiments:
  1. Run EDP on 20 Newsgroups with varying concentration parameters to observe topic number sensitivity.
  2. Compare perplexity and topic quality of EDP vs ETM on a small benchmark dataset.
  3. Implement D-EDP on UN dataset and visualize topic evolution over time slices.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of topics to use for industrial datasets when using nonparametric topic models like the Embedded Dirichlet Process?
- Basis in paper: [explicit] The authors state that "We determined the number of K= 10 components with cross-validation in a held-out perplexity (or log-likelihood) task" for the D-EDP model, but note that this is a truncation level and the actual number of topics is theoretically infinite.
- Why unresolved: The optimal number of topics depends on the specific dataset and use case, and the authors did not explore this in detail. The truncation level chosen may not be optimal for all industrial datasets.
- What evidence would resolve it: Conducting experiments with different truncation levels on a variety of industrial datasets and evaluating the topic quality and interpretability would provide evidence for the optimal number of topics.

### Open Question 2
- Question: How do the current topic modeling metrics, such as perplexity and topic coherence, align with the practical needs of practitioners for evaluating topic models?
- Basis in paper: [explicit] The authors state that "we must find a way to address the deficiencies of current metrics for topic model evaluation" and that "there is evidence of automatic evaluation being 'broken' [Hoy+21]".
- Why unresolved: The current metrics do not always align with practitioners' expectations for topic models' evaluation, as demonstrated by the authors' findings that the mean NPMI underestimates coherence in document models with word embeddings.
- What evidence would resolve it: Conducting user studies with practitioners to evaluate their preferences for topic models and comparing the results with the current metrics would provide evidence for the adequacy of the metrics.

### Open Question 3
- Question: How can we improve the scalability of nonparametric topic models like the Embedded Dirichlet Process to handle massive datasets in industrial settings?
- Basis in paper: [inferred] The authors mention that "Gibbs sampling in general is too slow for document stream processing, thus making variational techniques preferable for this task" and that "the bottleneck of inference, however, hinders moving smoothly between the necessary steps to performance improvement".
- Why unresolved: The scalability of nonparametric topic models is crucial for industrial applications, but the authors do not provide a detailed solution for improving it.
- What evidence would resolve it: Developing and testing new inference algorithms or techniques that can scale to massive datasets while maintaining the quality of the topic models would provide evidence for improved scalability.

## Limitations
- The evaluation relies primarily on perplexity and topic quality metrics, which have known limitations in capturing true semantic quality.
- Industrial dataset analysis lacks direct comparison to established industrial baselines.
- The topic quality metric used (diversity × mean NPMI) doesn't account for topic specificity or coverage.

## Confidence
- Confidence in core contributions: **Medium** - the theoretical framework is sound and benchmark results are promising, but the industrial validation could be more rigorous.
- Confidence in technical implementation: **High** - the variational inference framework and nonparametric modeling approach are well-established.
- Confidence in generalization: **Low** - the limited diversity of evaluation datasets raises questions about generalization to vastly different domains or languages.

## Next Checks
1. Conduct ablation studies removing either the word embeddings or topic embeddings to quantify their individual contributions to model performance.
2. Compare D-EDP against a parametric baseline (fixed number of topics) on the industrial dataset to demonstrate the value of nonparametric approaches in real-world settings.
3. Implement an alternative evaluation using human annotators to assess topic interpretability and relevance, complementing the automated metrics.