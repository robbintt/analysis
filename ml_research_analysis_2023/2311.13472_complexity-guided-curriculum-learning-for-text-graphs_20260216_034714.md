---
ver: rpa2
title: Complexity-Guided Curriculum Learning for Text Graphs
arxiv_id: '2311.13472'
source_url: https://arxiv.org/abs/2311.13472
tags:
- training
- indices
- graph
- degree
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TGCL, a curriculum learning approach that
  uses graph and text complexity formalisms to guide training of graph neural networks
  (GNNs) on text graph data. The method employs a data scheduler that uses "spaced
  repetition" and complexity indices to order training samples.
---

# Complexity-Guided Curriculum Learning for Text Graphs

## Quick Facts
- **arXiv ID**: 2311.13472
- **Source URL**: https://arxiv.org/abs/2311.13472
- **Reference count**: 32
- **Key outcome**: TGCL improves performance over baselines, gaining 5.1 absolute points in average score while using 39.2% less data

## Executive Summary
This paper introduces TGCL, a curriculum learning approach that uses graph and text complexity formalisms to guide training of graph neural networks (GNNs) on text graph data. The method employs a data scheduler that uses "spaced repetition" and complexity indices to order training samples. Experiments on node classification and link prediction tasks show that TGCL improves performance over baselines while using less data, with the model consistently prioritizing text complexity indices over graph indices during training. The approach also learns transferable curricula across GNN models and datasets.

## Method Summary
TGCL uses graph and text complexity indices to create a curriculum for training GNNs on text graph data. The method computes 26 graph complexity indices (centrality, connectivity, degree) and 14 text complexity indices (readability scores, sentence length) for all training samples. A data scheduler with "spaced repetition" gradually includes samples based on a competence function c(t) that controls the fraction of training data used. The scheduler assigns delays to complexity indices based on their difficulty, with text indices being prioritized over graph indices. The approach is evaluated using four GNN models (GraphSAGE, GCN, GAT, GTNN) on node classification and link prediction tasks.

## Key Results
- TGCL achieves 5.1 absolute points improvement in average score over baselines
- Uses 39.2% less data compared to standard training approaches
- Consistently prioritizes text complexity indices over graph indices during training
- Learns transferable curricula across different GNN models and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TGCL improves training efficiency by delaying exposure to harder samples based on model competence
- Mechanism: The competence function c(t) gradually increases the fraction of training data used, allowing the model to focus on easier samples first. This spacing reduces redundant training on already-learned samples
- Core assumption: Model competence correlates with sample difficulty; easier samples can be learned earlier, freeing resources for harder ones later
- Evidence anchors:
  - [abstract] "gains more and uses less data; consistently prefers text over graph complexity indices throughout training"
  - [section] "the proposed model gains more and uses less data; consistently prefers text over graph complexity indices throughout training"
  - [corpus] Weak correlation: neighbor papers focus on curriculum learning but not specifically on spaced repetition for efficiency gains
- Break condition: If competence estimation fails (e.g., incorrect loss thresholds), harder samples may be delayed too long, slowing convergence

### Mechanism 2
- Claim: TGCL prioritizes text complexity indices over graph indices during training
- Mechanism: Scheduler assigns higher delays to graph indices, reducing their frequency in training batches while text indices are used more often
- Core assumption: Text complexity features are more predictive of sample difficulty for the specific tasks (node classification, link prediction) studied
- Evidence anchors:
  - [abstract] "consistently prefers text over graph complexity indices throughout training"
  - [section] "the model consistently prefers text over graph complexity indices throughout all stages of training"
  - [corpus] Missing: no corpus evidence directly supporting this preference
- Break condition: If graph features become more informative for a new task, this bias could degrade performance

### Mechanism 3
- Claim: TGCL learns transferable curricula across GNN models and datasets
- Mechanism: The scheduler's delay and competence parameters adapt to model behavior, capturing generalizable difficulty patterns
- Core assumption: Difficulty ordering learned on one dataset/model transfers to others because underlying complexity features remain relevant
- Evidence anchors:
  - [abstract] "learns transferable curricula across GNN models and datasets"
  - [section] "curricula learned by TGCL can be transferred across GNN models, and in some cases improves the performance"
  - [corpus] Weak: neighbor papers discuss curriculum learning transferability but not specifically for graph/text complexity formalisms
- Break condition: If target dataset has very different data distribution, transferred curricula may mislead the scheduler

## Foundational Learning

- **Concept**: Graph complexity indices (centrality, connectivity, degree)
  - Why needed here: These quantify structural difficulty of subgraphs, informing curriculum ordering
  - Quick check question: How does node degree centrality differ from eigenvector centrality in measuring node importance?

- **Concept**: Text complexity indices (readability scores, sentence length)
  - Why needed here: These capture linguistic difficulty, complementing graph metrics for text graph data
  - Quick check question: What is the difference between traditional readability formulas (e.g., Flesch-Kincaid) and shallow features (e.g., average token length)?

- **Concept**: Spaced repetition scheduling
  - Why needed here: Delays exposure to harder samples, improving training efficiency by avoiding redundancy
  - Quick check question: How does the scheduler's delay parameter evolve as model competence increases?

## Architecture Onboarding

- **Component map**: Complexity index computation -> Competence function c(t) -> Scheduler with delay estimation -> Base GNN model -> Validation loop
- **Critical path**:
  1. Compute complexity indices for all training samples
  2. Initialize delays (δi = 1 for all indices)
  3. At each iteration: select current batch (δi ≤ 1), train on top c(t) fraction
  4. Update delays based on loss and validation performance
  5. Repeat until convergence
- **Design tradeoffs**:
  - More indices → finer-grained curriculum but higher compute cost
  - Larger α in competence function → faster inclusion of harder samples but risk of overwhelming model
  - Fixed vs. adaptive η (recall threshold) affects delay aggressiveness
- **Failure signatures**:
  - If all delays stay low → scheduler not learning difficulty, treat as no-curriculum baseline
  - If all delays grow quickly → model never sees hard samples, underfitting
  - If validation loss plateaus early → competence function too conservative
- **First 3 experiments**:
  1. Run TGCL with only graph indices on Ogbn-Arxiv; compare accuracy vs. No-CL
  2. Swap GTNN for GCN; verify transfer of learned curriculum
  3. Enable text indices; confirm scheduler prioritizes them over graph indices

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do additional linguistic complexity indices beyond those tested affect the performance of TGCL?
- Basis in paper: [inferred] The authors note that incorporating additional linguistic indices "have the potential to further enhance performance" after observing consistent prioritization of linguistic indices over graph indices during training
- Why unresolved: The current study only tested a limited set of 14 linguistic indices, leaving the impact of a broader range of linguistic complexity measures unexplored
- What evidence would resolve it: Systematic experiments testing TGCL with an expanded set of linguistic complexity indices across multiple datasets and tasks to quantify performance improvements

### Open Question 2
- Question: What is the optimal combination of local and global graph complexity indices for different types of graph learning tasks?
- Basis in paper: [explicit] The authors observe that TGCL "consistently prefers text over graph complexity indices throughout all stages of training" despite finding "both node-level (local) and graph-level (global) graph complexity indices play a crucial role in effective curriculum learning"
- Why unresolved: The study identifies the importance of both local and global indices but doesn't determine the optimal balance or combination for specific tasks like node classification versus link prediction
- What evidence would resolve it: Comparative experiments systematically varying the ratio and types of local versus global graph indices in TGCL across different graph learning tasks to identify optimal combinations

### Open Question 3
- Question: How does the transferability of learned curricula vary with dataset similarity and size differences?
- Basis in paper: [explicit] The authors demonstrate that "curricula learned by TGCL can be transferred across GNN models and datasets" with performance reductions described as "negligible considering the significant efficiency gained"
- Why unresolved: While transferability is shown, the relationship between source-target dataset similarity, size differences, and transfer performance remains unexplored
- What evidence would resolve it: Systematic analysis of TGCL performance when transferring curricula between datasets with varying degrees of structural similarity and size differences, potentially developing a similarity metric for predicting transfer success

## Limitations

- The corpus evidence provides weak support for the mechanism where TGCL prioritizes text complexity indices over graph indices during training
- Transferability claims across GNN models and datasets lack quantitative validation for edge cases and failure modes
- The spaced repetition mechanism's efficiency gains rely on competence function calibration without sensitivity analysis

## Confidence

- **High**: TGCL improves performance over baselines (5.1 absolute points gain)
- **Medium**: Text complexity indices are consistently preferred over graph indices
- **Medium**: Learned curricula transfer across models and datasets

## Next Checks

1. Test TGCL with only graph indices on Ogbn-Arxiv; verify that performance degrades if text indices are truly more informative, confirming the stated preference mechanism
2. Implement ablation studies varying the competence function parameter α to quantify efficiency sensitivity and identify optimal spacing for different dataset sizes
3. Cross-validate curriculum transferability by training TGCL on one dataset (e.g., Cora), transferring to another (e.g., Citeseer), and measuring performance degradation relative to training-from-scratch