---
ver: rpa2
title: 'Bigger, Better, Faster: Human-level Atari with human-level efficiency'
arxiv_id: '2305.19452'
source_url: https://arxiv.org/abs/2305.19452
tags:
- learning
- atari
- performance
- human-level
- sr-spr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BBF, a model-free deep RL agent, achieves super-human performance
  on the Atari 100K benchmark by scaling neural networks and incorporating design
  choices like higher replay ratios, harder network resets, receding update horizons,
  increasing discount factors, and weight decay. BBF outperforms prior methods, including
  EfficientZero and SR-SPR, achieving an IQM human-normalized score of 1.045 and reducing
  the optimality gap to 0.344 on 26 Atari games.
---

# Bigger, Better, Faster: Human-level Atari with human-level efficiency

## Quick Facts
- arXiv ID: 2305.19452
- Source URL: https://arxiv.org/abs/2305.19452
- Authors: Anonymous
- Reference count: 38
- Primary result: BBF achieves super-human performance on Atari 100K benchmark with IQM human-normalized score of 1.045 and 0.344 optimality gap, requiring only 6 hours on single GPU

## Executive Summary
BBF is a model-free deep RL agent that achieves super-human performance on the challenging Atari 100K benchmark by scaling neural networks and incorporating several key design choices. The agent combines a 4× wider Impala-CNN ResNet with higher replay ratios (RR=8), harder network resets, receding update horizons, increasing discount factors, and weight decay. BBF outperforms previous state-of-the-art methods including EfficientZero and SR-SPR, achieving an IQM human-normalized score of 1.045 and reducing the optimality gap to 0.344 on 26 Atari games. The approach demonstrates that network scaling, when combined with appropriate regularization and architectural modifications, can dramatically improve sample efficiency in deep RL.

## Method Summary
BBF is a value-based RL agent that builds on the Rainbow DQN architecture with several key modifications. The agent uses a 15-layer Impala-CNN ResNet architecture scaled by 4× in width for improved value function expressiveness. Training employs a higher replay ratio (RR=8) with periodic network resets that apply 50% perturbation to convolutional layers, enabling the agent to scale replay ratios without catastrophic forgetting. The receding update horizon exponentially anneals n-step returns from n=10 to n=3, while the discount factor schedule increases from γ=0.97 to γ=0.997. Weight decay regularization (AdamW with 0.1) is added to prevent overfitting given the increased network capacity. The agent also incorporates the SPR self-supervised objective for auxiliary learning and uses exponential moving average target networks.

## Key Results
- BBF achieves super-human performance on Atari 100K benchmark with IQM human-normalized score of 1.045
- Reduces optimality gap to 0.344 on 26 Atari games, outperforming previous state-of-the-art methods
- Requires only 6 hours of training on a single GPU, demonstrating high computational efficiency
- Ablation studies show individual components contribute significantly: 4× network scaling provides 0.19 IQM gain, RR=8 provides 0.22 IQM gain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger neural networks improve value estimation expressiveness in sample-efficient RL.
- Mechanism: Increasing network width allows the agent to capture more complex value function representations from limited data, compensating for the reduced number of environment interactions.
- Core assumption: The Atari 100K environment complexity requires expressive value functions that cannot be captured by smaller networks like the original 3-layer CNN.
- Evidence anchors:
  - [abstract] "BBF relies on scaling the neural networks used for value estimation"
  - [section] "Scaling network widths for both ResNet and CNN architectures... BBF's performance continues to grow as width is increased, whereas SR-SPR seems to peak"
  - [corpus] Weak evidence - related work focuses on scaling in different contexts but doesn't directly validate this claim for sample-efficient RL
- Break condition: Performance degrades due to overfitting when network capacity exceeds what limited data can support, or when other architectural components cannot leverage the increased expressiveness.

### Mechanism 2
- Claim: Periodic network resets enable scaling replay ratios without catastrophic forgetting.
- Mechanism: Resetting network parameters at regular intervals refreshes the learning dynamics, preventing the network from overfitting to early experiences while maintaining plasticity to learn from new data.
- Core assumption: Deep RL networks tend to overfit to early experiences in the replay buffer, and this overfitting becomes more severe as replay ratio increases.
- Evidence anchors:
  - [abstract] "incorporating design choices like higher replay ratios, harder network resets"
  - [section] "Periodic resetting, as suggested by Nikishin et al. (2022) and D'Oro et al. (2023), has proven effective to enable scaling to larger replay ratios"
  - [corpus] Moderate evidence - the neighbor paper "Bigger, Regularized, Optimistic" discusses scaling but focuses on continuous control rather than Atari
- Break condition: Resets become too frequent, preventing the network from stabilizing and learning coherent value functions, or too infrequent, allowing catastrophic forgetting to dominate.

### Mechanism 3
- Claim: Receding update horizon (annealing n-step returns) optimizes the bias-variance tradeoff during learning.
- Mechanism: Starting with larger n-step returns accelerates learning by incorporating more future rewards, then gradually reducing n to minimize asymptotic error as the agent approaches optimal performance.
- Core assumption: There exists an optimal schedule for n-step returns that balances fast initial learning with low asymptotic error, as derived by Kearns & Singh (2000).
- Evidence anchors:
  - [abstract] "receding update horizons"
  - [section] "Our n-step schedule is motivated by the theoretical results of Kearns & Singh (2000)"
  - [corpus] Weak evidence - no direct corpus support for this specific annealing schedule in Atari RL
- Break condition: The annealing schedule is too aggressive, causing instability, or too conservative, failing to capture the benefits of larger n-step returns during early learning.

## Foundational Learning

- Concept: Bellman equation and temporal difference learning
  - Why needed here: BBF is a value-based RL agent that learns Q-values through temporal difference updates
  - Quick check question: What is the difference between the Bellman target and the temporal difference error in Q-learning?

- Concept: Experience replay and replay ratio
  - Why needed here: BBF uses high replay ratios (8) and periodic resets to enable sample-efficient learning from limited data
  - Quick check question: How does increasing the replay ratio affect sample efficiency and computational requirements?

- Concept: Network architecture scaling and expressivity
  - Why needed here: BBF scales the Impala-CNN architecture by 4× width to improve value function representation
  - Quick check question: What are the tradeoffs between network width, parameter count, and computational cost in deep RL?

## Architecture Onboarding

- Component map: Impala-CNN (15-layer ResNet) scaled by 4× width -> SPR self-supervised loss -> Exponential moving average target network -> AdamW optimizer with weight decay (0.1) -> Periodic network resets (50% perturbation) -> Annealing n-step schedule (10→3) and discount factor schedule (0.97→0.997)

- Critical path: Data collection → Buffer storage → Mini-batch sampling → Forward pass through scaled Impala-CNN → SPR loss + TD loss → Backpropagation → Parameter update → Target network sync

- Design tradeoffs: Higher replay ratio improves sample efficiency but increases computation; larger networks improve expressivity but require more regularization; periodic resets prevent overfitting but may disrupt learning stability

- Failure signatures: Performance plateaus despite high replay ratios (insufficient network capacity); training becomes unstable after resets (reset frequency too high); n-step annealing causes divergence (schedule too aggressive)

- First 3 experiments:
  1. Train BBF at RR=2 with width scale 1× to verify baseline performance
  2. Increase width scale to 4× while keeping RR=2 to measure scaling benefits
  3. Enable RR=8 with 4× width to test full BBF configuration and identify computational bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific self-supervised losses, beyond SPR, could further improve sample-efficient RL performance?
- Basis in paper: [explicit] The paper discusses the importance of self-supervision but suggests other losses like (Mazoure et al., 2020; Castro et al., 2021; Agarwal et al., 2021a) could be explored.
- Why unresolved: The paper does not test these alternative self-supervised losses, leaving their potential impact on sample efficiency unknown.
- What evidence would resolve it: Empirical comparisons of BBF with various self-supervised losses on Atari 100K or similar benchmarks would clarify their relative effectiveness.

### Open Question 2
- Question: How does the balance between catastrophic forgetting and network plasticity affect the optimal frequency and intensity of network resets in large-scale RL?
- Basis in paper: [inferred] The paper highlights the role of periodic network resets in enabling large replay ratios and discusses the trade-off between forgetting and plasticity, but does not provide a definitive answer on optimal reset strategies.
- Why unresolved: The optimal balance likely depends on task complexity, network architecture, and replay ratio, making it difficult to establish general guidelines.
- What evidence would resolve it: Systematic ablation studies varying reset frequency and intensity across diverse tasks and network scales would reveal their impact on performance and learning dynamics.

### Open Question 3
- Question: Does the linearity between BBF's performance and SR-SPR across replay ratios hold for other RL algorithms and network architectures?
- Basis in paper: [explicit] The paper observes a linear relationship between BBF and SR-SPR performance across replay ratios, but notes this is unexpected and requires further investigation.
- Why unresolved: The observed linearity might be specific to the BBF and SR-SPR design choices, and may not generalize to other algorithms or architectures.
- What evidence would resolve it: Testing the relationship between performance and replay ratio for other RL algorithms with varying network architectures would determine the generality of the observed linearity.

## Limitations

- The ablation studies do not fully isolate the contribution of individual components, particularly the interaction between network scaling and the receding update horizon schedule
- Computational efficiency claims are based on a single GPU implementation, and scaling to multi-GPU setups may introduce additional complexities
- Claims about preventing catastrophic forgetting through harder resets are primarily supported by the success of the full BBF system rather than targeted experiments isolating this mechanism

## Confidence

**High Confidence**: The core claim that scaling network width improves performance on Atari 100K is well-supported by ablation results and comparison with prior methods. The computational efficiency improvements are clearly demonstrated.

**Medium Confidence**: The effectiveness of the receding update horizon schedule and the specific hyperparameters for periodic resets are supported by empirical results, but lack extensive ablation or theoretical grounding beyond the referenced Kearns & Singh (2000) work.

**Low Confidence**: The paper's claims about preventing catastrophic forgetting through harder resets are primarily supported by the success of the full BBF system rather than targeted experiments isolating this mechanism.

## Next Checks

1. Isolate the receding update horizon contribution: Run ablations with fixed n-step returns (e.g., n=3 and n=10) across different network widths to quantify the interaction between scaling and update horizon.

2. Explore reset frequency sensitivity: Systematically vary the reset frequency and perturbation magnitude to identify optimal schedules and understand the mechanism's sensitivity to hyperparameters.

3. Test generalization to other domains: Evaluate BBF on continuous control tasks from the DM Control suite or other sample-efficient RL benchmarks to assess the approach's broader applicability.