---
ver: rpa2
title: Continual Learning for Generative Retrieval over Dynamic Corpora
arxiv_id: '2308.14968'
source_url: https://arxiv.org/abs/2308.14968
tags:
- documents
- clever
- document
- retrieval
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes CLEVER, a novel continual learning model for
  generative retrieval over dynamic corpora. The key challenges addressed are: (1)
  efficiently encoding new documents into docids with low computational cost, and
  (2) preventing catastrophic forgetting for previously indexed documents while maintaining
  retrieval ability.'
---

# Continual Learning for Generative Retrieval over Dynamic Corpora

## Quick Facts
- **arXiv ID**: 2308.14968
- **Source URL**: https://arxiv.org/abs/2308.14968
- **Reference count**: 40
- **Key outcome**: CLEVER achieves significant improvements in MRR@10 and Hits@10 metrics while requiring less memory and training time compared to state-of-the-art methods

## Executive Summary
This paper addresses the challenge of continually indexing new documents into a dynamic corpus for generative retrieval while preventing catastrophic forgetting of previously indexed documents. The proposed CLEVER model introduces incremental product quantization (IPQ) for efficient encoding of new documents with low computational cost, and a memory-augmented learning mechanism to maintain retrieval ability across document updates. The model outperforms existing generative retrieval approaches in both incremental and non-incremental scenarios on two benchmark datasets, demonstrating superior effectiveness and efficiency.

## Method Summary
CLEVER implements a two-component approach to continual learning for generative retrieval. First, it uses Incremental Product Quantization (IPQ) to encode new documents into docids without full re-clustering, updating only a subset of quantization centroids based on adaptive thresholds. Second, it employs a memory-augmented learning mechanism that maintains a dynamic memory bank of exemplar documents and uses pseudo-queries generated by a T5 model to prevent catastrophic forgetting during indexing updates. The model trains through a two-step iterative process combining contrastive loss for span-level discrimination and clustering loss for quantization quality, with an overall objective that includes indexing, memory rehearsal, pseudo-query, and regularization losses.

## Key Results
- CLEVER significantly outperforms baselines and existing generative retrieval models in both incremental and non-incremental scenarios
- The model achieves substantial improvements in MRR@10 and Hits@10 metrics while requiring less memory and training time
- CLEVER demonstrates effectiveness in preventing catastrophic forgetting, maintaining strong performance on previously indexed documents while incorporating new ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incremental Product Quantization (IPQ) enables efficient indexing of new documents without full re-clustering
- Mechanism: IPQ divides document representations into M sub-vectors and applies K-means clustering to each group separately, updating only a subset of centroids based on adaptive thresholds (average distance ùëéùëë and maximum distance ùëöùëë)
- Core assumption: Document representations can be effectively partitioned into sub-vectors where each group can be clustered independently without losing retrieval quality
- Evidence anchors:
  - [abstract] "we present Incremental Product Quantization, which updates a partial quantization codebook according to two adaptive thresholds"
  - [section 3.1.1] "we devise two adaptive thresholds, i.e., ùëéùëë and ùëöùëë, according to this distance, to achieve three types of update"
- Break condition: If document distribution changes dramatically between sessions, the fixed sub-vector partitioning may no longer capture meaningful clusters, causing retrieval degradation

### Mechanism 2
- Claim: Memory-augmented learning prevents catastrophic forgetting during continual indexing
- Mechanism: Maintains a dynamic memory bank of exemplar documents similar to new documents and uses pseudo-queries generated by a query generator model to maintain retrieval ability during indexing updates
- Core assumption: Similar documents in PQ space share relevant information that can be leveraged to prevent forgetting
- Evidence anchors:
  - [abstract] "To memorize new documents for querying without forgetting previous knowledge, we propose a memory-augmented learning mechanism"
  - [section 3.2] "we propose a memory-augmented learning mechanism to build meaningful connections between new and old documents"
- Break condition: If the memory bank becomes too large or if pseudo-query generation quality degrades, the learning mechanism may become inefficient or introduce noise

### Mechanism 3
- Claim: Two-step iterative process with contrastive and clustering losses learns discriminative document representations
- Mechanism: Alternates between clustering centroids and training document encoder with contrastive loss (pulling spans of same document together) and clustering loss (minimizing distance between original and quantized representations)
- Core assumption: Contrastive learning of document spans improves the quality of representations used for clustering
- Evidence anchors:
  - [section 3.1.1] "we propose a bootstrapped training process based on BERT to learn discriminative document representations"
  - [section 3.1.1] "The contrastive loss helps to generate the document representation close to its own random spans while being far away from others"
- Break condition: If the contrastive loss hyperparameters are poorly tuned, the representation learning may fail to capture document semantics effectively

## Foundational Learning

- Concept: Product quantization for document representation
  - Why needed here: Enables efficient storage and comparison of large document collections by representing documents as combinations of quantized centroids
  - Quick check question: Why does dividing document representations into sub-vectors help with scalability?

- Concept: Contrastive learning for representation quality
  - Why needed here: Improves document encoder's ability to distinguish between different documents by pulling together different spans of the same document
  - Quick check question: How does pulling together different spans of the same document help with retrieval?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Explains why directly fine-tuning on new documents without protection mechanisms leads to degraded performance on previously seen documents
  - Quick check question: What happens to a model's performance on old documents when it's fine-tuned only on new data?

## Architecture Onboarding

- Component map: Document encoder ‚Üí Incremental Product Quantization ‚Üí Dynamic Memory Bank ‚Üí Query Generator ‚Üí Generative Retrieval Model
- Critical path: New document ‚Üí IPQ encoding ‚Üí Memory bank update ‚Üí Pseudo-query generation ‚Üí Combined training objective
- Design tradeoffs: Memory efficiency vs retrieval accuracy (IPQ vs atomic docids), computational cost vs representation quality (adaptive updates vs full re-clustering)
- Failure signatures: Performance degradation on old documents, increased memory usage over time, slow indexing of new documents
- First 3 experiments:
  1. Test IPQ with fixed thresholds on a small document collection to verify the three update types work correctly
  2. Evaluate memory bank construction with different similarity thresholds to find optimal balance
  3. Measure catastrophic forgetting when removing the memory-augmented learning component

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CLEVER compare to traditional IR models (BM25 and DPR) when scaling to extremely large document collections (e.g., billions of documents)?
- Basis in paper: [explicit] The paper mentions that CLEVER outperforms traditional IR models in non-incremental scenarios and demonstrates effectiveness and efficiency in incremental scenarios with datasets containing hundreds of thousands of documents
- Why unresolved: The experiments were conducted on relatively small datasets (MS MARCO and Natural Questions), which may not reflect the performance on truly large-scale document collections
- What evidence would resolve it: Experiments evaluating CLEVER's performance on benchmark datasets with billions of documents or synthetic large-scale datasets, comparing it to traditional IR models in terms of retrieval effectiveness, memory usage, and training time

### Open Question 2
- Question: What is the impact of the choice of document encoder (e.g., BERT vs. other transformer-based models) on the performance of CLEVER's incremental product quantization (IPQ) component?
- Basis in paper: [explicit] The paper uses BERT as the initial document encoder for IPQ but does not explore the impact of using other encoder architectures
- Why unresolved: Different document encoders may capture semantic information differently, which could affect the quality of document representations and the effectiveness of the clustering and quantization process in IPQ
- What evidence would resolve it: Experiments comparing CLEVER's performance with different document encoders (e.g., RoBERTa, DistilBERT, or other transformer-based models) while keeping other components constant, measuring retrieval effectiveness and efficiency

### Open Question 3
- Question: How does CLEVER handle document updates or removals in dynamic corpora, and what is the impact on retrieval performance and model efficiency?
- Basis in paper: [inferred] The paper focuses on the scenario of continuously adding new documents but does not address document updates or removals, which are also common in dynamic corpora
- Why unresolved: The proposed method is designed for incremental document addition, but real-world scenarios often involve document updates or removals, which may require different strategies to maintain retrieval effectiveness and efficiency
- What evidence would resolve it: Experiments evaluating CLEVER's performance when documents are updated or removed from the corpus, comparing it to baselines and analyzing the impact on retrieval effectiveness, memory usage, and training time

## Limitations

- The paper lacks detailed implementation specifics for the two-step iterative process and adaptive threshold calculation methods, which may affect reproducibility
- Performance evaluation is limited to relatively small datasets (MS MARCO and Natural Questions), leaving questions about scalability to truly large document collections
- The quality of pseudo-queries generated for memory-augmented learning is not directly evaluated, relying only on indirect end-to-end metrics

## Confidence

- **High confidence**: The general approach of combining incremental product quantization with memory-augmented learning for continual generative retrieval is sound and addresses well-known challenges in the field
- **Medium confidence**: The specific adaptive threshold mechanisms and their claimed efficiency improvements, as the exact implementation details and sensitivity to hyperparameters are not fully specified
- **Medium confidence**: The memory-augmented learning mechanism's effectiveness, as it depends critically on the quality of pseudo-query generation and the similarity-based memory bank construction

## Next Checks

1. **Threshold sensitivity analysis**: Systematically vary the adaptive threshold parameters (average distance and max distance with noise) across a range of values to identify optimal settings and understand robustness to hyperparameter choices
2. **Memory bank quality evaluation**: Evaluate the quality of the pseudo-queries generated by the T5 model and measure the impact of different memory bank sizes and similarity thresholds on preventing catastrophic forgetting
3. **Scalability testing**: Test CLEVER's performance and memory usage on larger document collections (beyond the 1M documents in current experiments) to verify the claimed computational efficiency advantages over full re-clustering approaches