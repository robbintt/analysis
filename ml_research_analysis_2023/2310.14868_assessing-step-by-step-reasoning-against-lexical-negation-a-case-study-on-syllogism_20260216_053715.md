---
ver: rpa2
title: 'Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on
  Syllogism'
arxiv_id: '2310.14868'
source_url: https://arxiv.org/abs/2310.14868
tags:
- task
- negation
- reasoning
- answer
- setting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how well large language models (LLMs) perform
  chain-of-thought reasoning when confronted with lexical negation (e.g., plausible
  vs. implausible).
---

# Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism

## Quick Facts
- **arXiv ID**: 2310.14868
- **Source URL**: https://arxiv.org/abs/2310.14868
- **Reference count**: 30
- **Primary result**: LLMs exhibit substantial accuracy drops on syllogism tasks when lexical negation is present, with no reliable scaling with model size and distinct failure patterns across different model families.

## Executive Summary
This study investigates how well large language models (LLMs) perform chain-of-thought reasoning when confronted with lexical negation (e.g., plausible vs. implausible). Using controlled experiments with dozens of modern LLMs on syllogism tasks, we find that most models exhibit substantial accuracy drops when negation is present, even though the underlying reasoning chains are valid. Different LLM families display distinct failure patterns—some consistently answer "no," others fail more variably—suggesting unique biases. Performance does not reliably scale with model size, and no family fully overcomes this weakness. Overall, the results highlight that current LLMs struggle with robust logical reasoning in the presence of negation, a critical limitation for their reasoning capabilities.

## Method Summary
The study evaluates LLM reasoning against lexical negation using controlled syllogism tasks (SPORTS, OCCUPATION, WEIGHT TRANSITION) with BASE, FIC, FICNEG, and FICNEG-O settings. Researchers collected 1,000 instances for SPORTS from BIG-Bench and manually created instances for other tasks, using fictional entities generated by GPT-4 to isolate reasoning from factual knowledge. They applied few-shot prompting with chain-of-thought instructions ("Let's think step by step"), temperature=0.0, max_tokens=1, and logit bias for yes/no outputs. The study tested 30+ modern LLMs including OpenAI, Anthropic, Google, and open-source models, measuring binary accuracy and no-ratio metrics across different task settings.

## Key Results
- Most models show substantial accuracy drops when negation is introduced, even though reasoning chains remain valid
- Different LLM families exhibit distinct failure patterns, with some consistently answering "no" and others showing variable failures
- Performance does not reliably improve with model size, breaking typical scaling law expectations
- No LLM family fully overcomes the weakness against lexical negation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexical negation disrupts LLMs' chain-of-thought reasoning even when reasoning chains are valid.
- Mechanism: The models fail to properly handle the semantic flip induced by negative prefixes (e.g., plausible → implausible), leading to systematic errors.
- Core assumption: LLMs rely on lexical cues and surface patterns in their reasoning process, rather than deeply understanding logical structure.
- Evidence anchors:
  - [abstract]: "most models exhibit substantial accuracy drops when negation is present, even though the underlying reasoning chains are valid."
  - [section]: "we find that most models exhibit substantial accuracy drops when negation is present, even though the underlying reasoning chains are valid."
  - [corpus]: Corpus signals show related work on negation benchmarks, but no direct evidence of step-by-step reasoning with lexical negation.
- Break condition: If LLMs are explicitly trained on lexical negation patterns or if they develop a deeper understanding of logical structure beyond surface cues.

### Mechanism 2
- Claim: Different LLM families have distinct biases against lexical negation, suggesting model-dependent weaknesses.
- Mechanism: The training data and objectives of different LLM families lead to varying degrees of robustness against lexical negation.
- Core assumption: The training process and data distribution significantly influence how models handle negation.
- Evidence anchors:
  - [abstract]: "Different LLM families display distinct failure patterns—some consistently answer 'no,' others fail more variably—suggesting unique biases."
  - [section]: "the results show that each LLM family has its unique biases against lexical negation, which suggests that different LLM training settings produce substantial differences under certain conditions."
  - [corpus]: Related work on negation benchmarks does not specifically address family-specific biases.
- Break condition: If all LLM families are trained on a standardized curriculum that addresses lexical negation consistently.

### Mechanism 3
- Claim: Performance does not reliably scale with model size for handling lexical negation in reasoning tasks.
- Mechanism: Increasing model size does not necessarily improve the ability to handle lexical negation, indicating that this is not a simple parameter scaling issue.
- Core assumption: The weakness against lexical negation is not simply due to insufficient model capacity.
- Evidence anchors:
  - [abstract]: "Performance does not reliably scale with model size, and no family fully overcomes this weakness."
  - [section]: "Scaling law breaks: Scaling law in LLMs has generally been reported; however, the improvement over the model scale broke, specifically in the FICNEG-O setting, which confirms that our introduced task is challenging."
  - [corpus]: No direct evidence in the corpus about scaling laws for lexical negation.
- Break condition: If future models demonstrate a clear correlation between size and performance on negation tasks.

## Foundational Learning

- Concept: Chain-of-thought (CoT) prompting
  - Why needed here: The study investigates how well LLMs perform step-by-step reasoning when confronted with lexical negation, building on the success of CoT prompting.
  - Quick check question: What is the primary benefit of using CoT prompting in LLMs?

- Concept: Lexical negation
  - Why needed here: The study focuses on how LLMs handle negation at the lexical level (e.g., plausible vs. implausible) during reasoning tasks.
  - Quick check question: How does lexical negation differ from syntactic negation in terms of difficulty for neural models?

- Concept: Syllogistic reasoning
  - Why needed here: The study uses syllogism tasks to evaluate the logical reasoning abilities of LLMs in the presence of lexical negation.
  - Quick check question: What is the basic structure of a syllogism, and how is it used in this study?

## Architecture Onboarding

- Component map: Input prompt → LLM reasoning → output answer
- Critical path: The reasoning process is disrupted by lexical negation, leading to systematic errors
- Design tradeoffs: Balancing task complexity with model capability; using fictional entities to isolate reasoning from factual knowledge
- Failure signatures: Consistent accuracy drops when negation is introduced, model-dependent failure patterns, lack of scaling with model size
- First 3 experiments:
  1. Test LLMs on syllogism tasks with and without lexical negation to confirm accuracy drops
  2. Compare performance across different LLM families to identify unique biases
  3. Evaluate the impact of model size on performance in negation tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do different LLM families exhibit distinct failure patterns when processing lexical negation?
- Basis in paper: [explicit] The paper notes that each LLM family has unique biases against lexical negation, with some models consistently answering "no" and others showing more variable behavior.
- Why unresolved: The paper identifies these patterns but does not investigate the underlying causes, such as differences in training data, model architecture, or optimization objectives.
- What evidence would resolve it: Detailed analysis of model architectures, training datasets, and internal representations during negation tasks could reveal the source of these biases.

### Open Question 2
- Question: Can fine-tuning or specialized training improve LLMs' ability to handle lexical negation in step-by-step reasoning?
- Basis in paper: [inferred] The paper demonstrates that current LLMs struggle with negation but does not explore whether this weakness can be mitigated through targeted training or architectural modifications.
- Why unresolved: The study focuses on evaluating existing models rather than testing interventions to improve their negation-handling capabilities.
- What evidence would resolve it: Experiments comparing the performance of fine-tuned models against baseline models on negation tasks would demonstrate whether targeted training is effective.

### Open Question 3
- Question: How does lexical negation affect LLMs' performance on other types of reasoning tasks beyond syllogisms?
- Basis in paper: [explicit] The study uses syllogism tasks to probe reasoning abilities but acknowledges that results may not generalize to other reasoning formats.
- Why unresolved: The experiments are limited to specific task formats, leaving open the question of whether negation poses similar challenges in other reasoning domains.
- What evidence would resolve it: Testing LLMs on a broader range of reasoning tasks (e.g., mathematical reasoning, causal inference) with lexical negation would reveal the generalizability of the observed weaknesses.

## Limitations
- Results are based on specific syllogism tasks and may not generalize to all logical reasoning domains
- The study uses synthetic datasets with fictional entities, which may not fully represent real-world reasoning challenges
- Binary yes/no outputs may oversimplify the complexity of logical reasoning and mask nuanced failure modes

## Confidence
- **High Confidence**: Different LLM families exhibit distinct failure patterns against lexical negation
- **Medium Confidence**: Performance does not reliably scale with model size for handling lexical negation
- **Medium Confidence**: LLMs struggle with robust logical reasoning in the presence of lexical negation

## Next Checks
1. Test the same LLMs on a variety of logical reasoning tasks beyond syllogisms (e.g., Wason selection tasks, logical deductions with different structures) to assess generalizability of the lexical negation weakness.
2. Conduct a detailed analysis of model outputs to identify specific patterns of errors beyond simple yes/no failures, such as partial reasoning, inconsistent logic, or reliance on superficial cues.
3. Experiment with targeted fine-tuning or prompt engineering strategies designed to specifically address lexical negation, to determine if this weakness can be mitigated and to understand the underlying mechanisms better.