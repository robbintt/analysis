---
ver: rpa2
title: Exploiting Multiple Abstractions in Episodic RL via Reward Shaping
arxiv_id: '2303.00516'
source_url: https://arxiv.org/abs/2303.00516
tags:
- learning
- abstract
- value
- each
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of sample inefficiency in reinforcement
  learning, particularly in scenarios with sparse rewards. The core method introduces
  a novel reward shaping technique that leverages a hierarchy of abstract Markov Decision
  Processes (MDPs) to guide learning in the ground MDP.
---

# Exploiting Multiple Abstractions in Episodic RL via Reward Shaping

## Quick Facts
- arXiv ID: 2303.00516
- Source URL: https://arxiv.org/abs/2303.00516
- Reference count: 28
- Key outcome: Novel reward shaping technique using hierarchical abstract MDPs to improve sample efficiency in episodic RL with sparse rewards.

## Executive Summary
This paper introduces a method to improve sample efficiency in episodic reinforcement learning by leveraging a hierarchy of abstract Markov Decision Processes (MDPs). The approach uses the optimal value function of an abstract MDP as a potential function to shape rewards in the lower-level MDP, guiding exploration toward states that correspond to desirable abstract states. The method is shown to improve sample efficiency and guarantees optimal convergence when combined with off-policy algorithms, demonstrating significant performance gains across various navigation tasks.

## Method Summary
The method builds a hierarchy of MDPs from most abstract (Mn) to concrete (M0), with mapping functions φi projecting states between adjacent levels. Each level learns on both a biased MDP (using reward shaping from the next level up) and the original MDP. The optimal value function of each abstract MDP is used as a potential function to shape rewards, biasing exploration toward states that are likely to be on good paths in the abstract problem. The approach combines biased MDP learning with off-policy algorithms to guarantee optimal convergence.

## Key Results
- The method significantly improves sample efficiency compared to naive exploration in sparse reward environments
- Performance gains are demonstrated across various navigation tasks, including continuous state spaces
- The approach shows robustness to modeling errors in the abstraction
- The method is effective for tasks with temporally-extended behaviors

## Why This Works (Mechanism)

### Mechanism 1
Using the optimal value function of an abstract MDP as a potential in reward shaping guides exploration in the lower-level MDP toward states that are useful for solving the overall task. The potential function Φ(s) = V*_i+1(φ_i(s)) assigns higher potentials to states whose corresponding abstract states have higher optimal values, biasing the agent to explore regions of the state space that are likely to be on good paths in the abstract problem.

### Mechanism 2
The method guarantees optimal convergence by combining biased MDP learning with off-policy algorithms. Each level of the hierarchy learns on a biased MDP (using reward shaping from the next level up) but also performs updates on the original MDP. The off-policy algorithm ensures that learning on the biased MDP converges to the optimal policy for that level, and the knowledge is propagated downward through the hierarchy.

### Mechanism 3
The quality of the abstraction can be quantified and bounded, allowing assessment of when the method will be effective. The exploration loss L(M, ⟨M̄,φ⟩) measures how similar the optimal policy of the biased MDP is to some optimal policy of the original MDP. This is bounded by the abstract similarity (ϵ) and abstract value approximation (ν), which measure how well the abstraction captures the dynamics and value structure of the lower-level MDP.

## Foundational Learning

- **Markov Decision Processes (MDPs)**: The entire framework is built on MDP theory, including definitions of optimal policies, value functions, and the mechanics of reward shaping. Quick check: Can you explain how the optimal value function V*(s) is defined in terms of expected discounted returns?

- **Reward Shaping and Potential-Based Reward Shaping**: The method uses a novel form of reward shaping where the potential function is derived from the optimal value function of an abstract MDP. Quick check: What is the mathematical form of the potential-based reward shaping function F(s,a,s')?

- **Hierarchical Reinforcement Learning and Abstraction**: The method builds a hierarchy of abstractions where each level guides learning at the level below. Quick check: How does the mapping function φ_i relate states in M_i to states in M_i+1, and what does this induce over the state space?

## Architecture Onboarding

- **Component map**: Mn (most abstract) -> Mn-1 -> ... -> M0 (ground MDP), with mapping functions φi between each level and reward shaping modules using optimal value functions

- **Critical path**: 1) Start with the most abstract MDP (Mn), 2) Solve Mn to obtain optimal policy and value function, 3) Use this value function to shape rewards in Mn-1, 4) Learn on both the biased and original Mn-1, 5) Repeat down to M0

- **Design tradeoffs**: Abstraction quality vs. computational efficiency (coarser abstractions are faster but less accurate), return-invariance vs. exploration bias (non-return-invariant shaping provides stronger bias but may alter optimal policies), hierarchy depth (more levels provide more guidance but increase computational overhead)

- **Failure signatures**: Slow convergence (poor abstraction quality or inappropriate hyper-parameters), suboptimal final policy (off-policy algorithm didn't satisfy required conditions or abstraction is misleading), instability during training (issues with reward shaping parameters or network architecture)

- **First 3 experiments**: 1) Implement the 4-rooms environment to verify basic functionality with Q-learning, 2) Test the method on the 8-rooms environment to observe performance gains over naive exploration, 3) Introduce modeling errors in the abstraction to verify robustness claims

## Open Questions the Paper Calls Out

### Open Question 1
What is the impact of the hierarchy depth (number of abstraction layers) on sample efficiency? The paper discusses a linear hierarchy of abstractions but does not explore the effects of varying the number of layers.

### Open Question 2
How does the proposed method perform in continuous action spaces? The paper mentions that the method can be applied to continuous state spaces but does not explicitly address continuous action spaces.

### Open Question 3
What is the sensitivity of the method to the choice of the mapping function φ? The paper mentions that the method has few requirements for the design of the abstract models and is tolerant to modeling errors, but does not quantify the sensitivity to the choice of φ.

## Limitations
- Claims about optimal convergence rely on assumptions about the off-policy algorithm that are not fully specified in implementation details
- Experimental results lack comparison to other hierarchical RL methods, limiting assessment of relative effectiveness
- Robustness to modeling errors is demonstrated in only one experiment, requiring more extensive testing across different error types

## Confidence
- **High Confidence**: The core mechanism of using abstract MDP value functions for reward shaping and the theoretical framework for exploration loss bounds are well-supported by the proofs and experiments
- **Medium Confidence**: The experimental results demonstrating performance gains are convincing, but the lack of comparison to other hierarchical RL methods limits the assessment of relative effectiveness
- **Medium Confidence**: The claim about robustness to modeling errors is supported by one experiment, but more extensive testing across different types of errors would strengthen this claim

## Next Checks
1. Reproduce the 8-rooms experiment to verify the claimed performance improvements over naive exploration methods
2. Test abstraction sensitivity by systematically varying the quality of the abstraction to assess the method's robustness and validate the theoretical bounds
3. Compare the approach with other state-of-the-art hierarchical RL methods on similar sparse-reward tasks to evaluate its relative effectiveness