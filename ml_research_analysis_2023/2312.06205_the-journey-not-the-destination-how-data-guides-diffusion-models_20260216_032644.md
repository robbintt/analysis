---
ver: rpa2
title: 'The Journey, Not the Destination: How Data Guides Diffusion Models'
arxiv_id: '2312.06205'
source_url: https://arxiv.org/abs/2312.06205
tags:
- diffusion
- training
- attribution
- data
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for attributing diffusion-model-generated
  images back to their training data by analyzing how each training example influences
  the distribution of generated images at each step of the diffusion process. The
  method computes data attribution scores at individual steps using TRAK, enabling
  fine-grained analysis of how training data guides the generative process.
---

# The Journey, Not the Destination: How Data Guides Diffusion Models

## Quick Facts
- **arXiv ID**: 2312.06205
- **Source URL**: https://arxiv.org/abs/2312.06205
- **Reference count**: 35
- **Primary result**: Introduces a framework for attributing diffusion-model-generated images back to their training data by analyzing how each training example influences the distribution of generated images at each step of the diffusion process

## Executive Summary
This paper presents a framework for attributing diffusion-model-generated images back to their training data by analyzing how each training example influences the distribution of generated images at each step of the diffusion process. The method computes data attribution scores at individual steps using TRAK (Training data Reconstruction using Average Kernel), enabling fine-grained analysis of how training data guides the generative process. The authors evaluate their attributions using two counterfactual metrics: linear datamodeling score and retraining without the most influential images. Results show that their method outperforms baselines in predicting model behavior and significantly impacts the distribution of generated images when the most influential training examples are removed.

## Method Summary
The framework uses TRAK to compute data attribution scores at individual diffusion steps by approximating the original model with a linear model over small learning-rate updates. The method evaluates attributions using linear datamodeling score (rank correlation between predicted and true model outputs) and retraining experiments where top influencers are removed from the training set. The framework is evaluated on CIFAR-10 and MS COCO datasets using denoising diffusion probabilistic models (DDPMs) and latent diffusion models (LDMs) respectively.

## Key Results
- Outperforms baselines (CLIP similarity, pixel similarity) in predicting model behavior
- Successfully attributes specific features to particular steps in the diffusion trajectory
- Demonstrates significant impact on the distribution of generated images when the most influential training examples are removed
- Scales to both small (CIFAR-10) and large (MS COCO) datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Diffusion model sampling is a sequential denoising process where each step t conditions the distribution p(·|xt) of possible final images.
- **Mechanism**: By attributing training data at each step t using TRAK, we isolate the marginal effect of each training example on the conditional distribution, which evolves gradually as the latent becomes less noisy.
- **Core assumption**: The denoising network's output at step t can be approximated as linear in parameters over small learning-rate updates, enabling TRAK to compute per-example gradients without retraining.
- **Evidence anchors**:
  - [abstract]: "we attribute each individual step along the (denoising) 'journey' taken by diffusion model"
  - [section 2.1]: "approximating the original model with a model that is linear in its parameters"
  - [corpus]: Weak - no direct citation; the claim relies on TRAK's original assumption about linear approximations in deep nets.
- **Break condition**: If the diffusion model uses large learning rates or the noise schedule is highly non-linear, the linear approximation breaks and TRAK attributions become inaccurate.

### Mechanism 2
- **Claim**: Features in the final image appear within a narrow window of steps, so step-level attribution localizes influence to the relevant time interval.
- **Mechanism**: By sampling from p(·|xt) and measuring the likelihood of a feature, we can identify the step interval where that feature rapidly increases in probability; positive influencers appear once the feature is "decided", negative influencers emerge afterward to steer away from other aspects.
- **Core assumption**: The conditional distribution p(·|xt) concentrates rapidly around the final image's features over a small number of steps.
- **Evidence anchors**:
  - [section 3.1]: "we are able to tie the presence of a given feature in the final image back to a small interval of steps t"
  - [section 5.2]: empirical observation that feature likelihood rises sharply within short step intervals
  - [corpus]: Weak - the corpus lists unrelated papers; no direct evidence for the step-concentration assumption.
- **Break condition**: If features form gradually over many steps or multiple features co-evolve, the narrow-window assumption fails and attribution becomes less precise.

### Mechanism 3
- **Claim**: Removing top influencers and retraining changes the conditional distribution p(·|xt) as measured by FID, validating counterfactual impact.
- **Mechanism**: By retraining without the most influential training images for a specific step t, the altered model's conditional distribution at that step shifts measurably; this is quantified by the increase in FID between the original and retrained model's conditional samples.
- **Core assumption**: The influence scores computed at step t are causally linked to the model's ability to generate the target conditional distribution; removing them should change the distribution.
- **Evidence anchors**:
  - [section 3.2]: "remove from the training set the most influential images...measure the change in the conditional distribution"
  - [section 5.3]: "removing the top influencers identified by our attribution method has a greater impact than removing the most similar images"
  - [corpus]: Weak - no corpus citations support this counterfactual retraining claim directly.
- **Break condition**: If the model is overparameterized or other training examples compensate, removing influencers may have negligible effect on the conditional distribution.

## Foundational Learning

- **Concept**: Influence functions and their limitations in non-convex, high-dimensional settings.
  - Why needed here: TRAK builds on influence-function ideas but addresses their inaccuracy for deep nets by linearizing the model.
  - Quick check question: Why can't we directly compute the change in model parameters θ when removing a training example in deep nets?

- **Concept**: Denoising diffusion probabilistic models and their training objective.
  - Why needed here: Understanding the noise schedule and how the model predicts noise is essential to define the step-specific model output function.
  - Quick check question: In DDPM training, what is the role of the noise schedule αt in defining the denoising target?

- **Concept**: Conditional distributions p(·|xt) and one-step denoising approximations.
  - Why needed here: Attribution is performed on the conditional distribution at each step, and the one-step estimate ˆx0 serves as a proxy for sampling from that distribution.
  - Quick check question: How does the one-step estimate ˆx0 approximate the expectation of p(·|xt)?

## Architecture Onboarding

- **Component map**: TRAK feature extractor -> Step-specific model output function -> Attribution aggregation -> Evaluation modules
- **Critical path**: 1. Load model checkpoints 2. Compute per-example gradients for training loss and model output 3. Project gradients, solve for attribution scores 4. Sample from p(·|xt) or use ˆx0 for evaluation 5. Compute evaluation metrics
- **Design tradeoffs**:
  - Using ˆx0 instead of full sampling trades accuracy for speed
  - Random projections reduce memory but add variance
  - Step-by-step attribution increases granularity but multiplies computation
- **Failure signatures**:
  - Low LDS scores despite high visual similarity → attribution method not counterfactual
  - Unstable FID after removing influencers → attribution scores not robust
  - No correlation between positive/negative influencers over steps → feature evolution misestimated
- **First 3 experiments**:
  1. Verify that TRAK attribution scores on CIFAR-10 DDPM correlate with pixel similarity at early steps.
  2. Confirm that removing top influencers increases FID at step t=400 compared to baseline.
  3. Test that the one-step estimate ˆx0 approximates samples from p(·|xt) by comparing feature likelihoods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do attribution scores behave when computed for latent diffusion models (LDMs) trained on datasets other than MS COCO, such as LAION-5B or ImageNet?
- Basis in paper: [explicit] The paper only evaluates attributions for LDMs trained on MS COCO, and mentions that scaling to larger datasets is an important future direction.
- Why unresolved: The paper does not provide evidence of attribution behavior on other datasets.
- What evidence would resolve it: Computing and comparing attribution scores for LDMs trained on different datasets, and evaluating their counterfactual significance using the proposed metrics.

### Open Question 2
- Question: Can the proposed attribution framework be extended to attribute specific regions of an image to particular training examples, rather than attributing the entire image?
- Basis in paper: [explicit] The paper briefly mentions the possibility of restricting the model output function to a user-specified patch, but does not explore this direction in depth.
- Why unresolved: The paper only demonstrates patch-based attribution for a few examples, and does not provide a systematic evaluation of this approach.
- What evidence would resolve it: Developing a method for computing region-specific attribution scores, and evaluating their counterfactual significance and interpretability.

### Open Question 3
- Question: How do the attribution scores change when computed for diffusion models with different architectures, such as denoising diffusion implicit models (DDIMs) or score-based generative models (SGMs)?
- Basis in paper: [inferred] The paper focuses on DDPMs and LDMs, but does not explore other diffusion model architectures.
- Why unresolved: The paper does not provide evidence of attribution behavior for other diffusion model architectures.
- What evidence would resolve it: Computing and comparing attribution scores for different diffusion model architectures, and evaluating their counterfactual significance using the proposed metrics.

## Limitations

- Linear approximation assumption in TRAK may break for large learning rates or highly non-linear noise schedules
- Step-level attribution precision relies on unproven assumption that features concentrate within narrow step windows
- Retraining experiments demonstrate causal impact but don't quantify attribution robustness across multiple random seeds

## Confidence

- **High**: Linear datamodeling score (LDS) shows strong rank correlation between predicted and true model outputs
- **Medium**: Step-level attribution successfully localizes influence to specific time intervals in controlled experiments
- **Low**: Generalizability of narrow-window feature concentration assumption across diverse image domains

## Next Checks

1. Test TRAK attribution accuracy when using only 10 vs 100 model checkpoints to verify the linear approximation scales properly
2. Measure feature likelihood evolution over steps for complex multi-feature images to validate the narrow-window concentration assumption
3. Quantify attribution robustness by comparing scores across 5 different random seeds for the same diffusion model