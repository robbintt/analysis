---
ver: rpa2
title: Drilling Down into the Discourse Structure with LLMs for Long Document Question
  Answering
arxiv_id: '2311.13565'
source_url: https://arxiv.org/abs/2311.13565
tags:
- document
- question
- retrieval
- performance
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of evidence retrieval for long
  document question answering (LDQA), which involves locating relevant paragraphs
  within a lengthy document to answer a specific question. The authors propose a method
  called D3 (Drilling Down into the Discourse) that exploits the discourse structure
  commonly found in long documents.
---

# Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering

## Quick Facts
- arXiv ID: 2311.13565
- Source URL: https://arxiv.org/abs/2311.13565
- Reference count: 22
- Primary result: D3 retains 99.6% of the best zero-shot approach's performance while processing only 26% of the total tokens

## Executive Summary
This paper addresses the challenge of evidence retrieval for long document question answering (LDQA) by proposing D3 (Drilling Down into the Discourse), a method that exploits the inherent discourse structure of long documents. By creating a condensed representation of the document through section summaries, D3 efficiently filters relevant sections before retrieving specific paragraphs. The approach demonstrates significant token efficiency gains while maintaining competitive performance compared to baseline methods, achieving 99.6% of the best zero-shot performance while processing only 26% of the tokens. The method is evaluated in both information-seeking and multi-hop reasoning scenarios, showing its versatility across different question-answering contexts.

## Method Summary
D3 leverages the hierarchical discourse structure of long documents by first creating a condensed representation where each section is replaced with its name and a content summary generated by a summarization model. The method operates in two stages: first identifying relevant sections based on the question using an LLM, then retrieving relevant paragraphs from those sections. This hierarchical approach reduces the total tokens processed while maintaining retrieval performance. The method is implemented using off-the-shelf LLMs in a zero-shot setting, making it practical and accessible without requiring fine-tuning. The approach can be combined with reasoning agents like self-ask for complex multi-hop question answering scenarios.

## Key Results
- D3 retains 99.6% of the best zero-shot approach's performance while processing only 26% of the total tokens
- In multi-hop reasoning, D3 combined with self-ask achieves performance just 4% below the gold evidence baseline
- The method demonstrates effective zero-shot performance without requiring supervised fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using discourse structure as a first-stage filter improves retrieval efficiency without significant performance loss
- Mechanism: The document's inherent discourse structure (sections, headings, sub-headings) is leveraged to create a condensed representation where each section is replaced with its name and content summary. This condensed representation is then used by an LLM to identify relevant sections before retrieving paragraphs from those sections
- Core assumption: Section headings and summaries contain sufficient information to judge section relevance to a question
- Evidence anchors:
  - [abstract] "we create a condensed representation of the document, enabling a more comprehensive understanding and analysis of relationships between different parts"
  - [section] "The cognitive strategy employed by humans to search for relevant information from a document entails a systematic approach of first categorizing the information within the document to determine relevant coarse segments and then conducting a deeper analysis of the relevant categories to extract fine-grained segments"
  - [corpus] Weak - corpus contains related papers on discourse-aware approaches but lacks direct evidence about the effectiveness of first-stage discourse filtering specifically
- Break condition: When section summaries lose critical information needed to judge relevance, or when discourse structure is absent or poorly defined in the document

### Mechanism 2
- Claim: Chaining LLM calls in a hierarchical manner reduces total tokens processed while maintaining performance
- Mechanism: The approach first processes the condensed document (sections with summaries) to identify relevant sections, then processes only the paragraphs within those relevant sections. This two-stage approach reduces the total number of tokens processed compared to processing all paragraphs directly
- Core assumption: The token savings from filtering at the section level outweighs any additional overhead from the multi-stage process
- Evidence anchors:
  - [abstract] "We retain 99.6% of the best zero-shot approach's performance, while processing only 26% of the total tokens used by the best approach"
  - [section] "By efficiently filtering out irrelevant sections in the initial stage, our method reduces the number of tokens processed"
  - [corpus] Missing - no direct corpus evidence for the specific token reduction claim, though related work exists on hierarchical approaches
- Break condition: When the overhead of multiple LLM calls exceeds the token savings, or when most sections are relevant making the first filtering stage unnecessary

### Mechanism 3
- Claim: Zero-shot LLM performance can match or approach supervised fine-tuned models for evidence retrieval
- Mechanism: Instruction-tuned LLMs (like gpt-3.5-turbo) can perform evidence retrieval without fine-tuning by using appropriate prompts, achieving competitive performance to models that were fine-tuned on supervised data
- Core assumption: Instruction tuning provides sufficient task understanding for evidence retrieval without domain-specific fine-tuning
- Evidence anchors:
  - [abstract] "We aim to assess the applicability of large language models (LLMs) in the task of zero-shot long document evidence retrieval, owing to their unprecedented performance across various NLP tasks"
  - [section] "Given the remarkable few-shot/zero-shot performance and enhanced generalization capabilities demonstrated by Large Language Models (LLMs) across various Natural Language Generation and Understanding tasks... we explore the feasibility of utilizing LLMs for zero-shot evidence retrieval"
  - [corpus] Weak - corpus contains related papers but lacks direct comparison of zero-shot LLM performance to supervised approaches for this specific task
- Break condition: When the question domain is too specialized or the document domain too different from the LLM's training data, causing performance degradation

## Foundational Learning

- Concept: Discourse structure and its role in information organization
  - Why needed here: The entire approach relies on leveraging the document's discourse structure to create a condensed representation and filter relevant sections
  - Quick check question: Can you explain how section headings and summaries help in determining a section's relevance to a question?

- Concept: Prompt engineering for LLMs
  - Why needed here: The method uses carefully crafted prompts to instruct the LLM to identify relevant sections and paragraphs without fine-tuning
  - Quick check question: What are the key elements of an effective prompt for asking an LLM to identify relevant document sections?

- Concept: Token efficiency and context window limitations
  - Why needed here: The approach is designed to work within LLM context window constraints while minimizing token usage
  - Quick check question: How does processing a condensed document representation help address LLM context window limitations?

## Architecture Onboarding

- Component map: Document → Summarize sections → Identify relevant sections → Retrieve relevant paragraphs → Answer question
- Critical path: Document → Summarize sections → Identify relevant sections → Retrieve relevant paragraphs → Answer question
- Design tradeoffs:
  - Performance vs. cost: Using smaller models for summarization reduces cost but may affect performance
  - Granularity vs. efficiency: More detailed section breakdowns improve accuracy but increase processing
  - Single-stage vs. multi-stage: Multi-stage approach reduces tokens but adds complexity and potential latency
- Failure signatures:
  - Poor section relevance identification: Check if section summaries are too generic or headings are uninformative
  - Missing relevant paragraphs: Verify if relevant sections were correctly identified in first stage
  - Hallucinations in answers: May indicate insufficient evidence was retrieved or LLM was prompted incorrectly
- First 3 experiments:
  1. Test section relevance identification: Use a simple document with clear sections, ask for relevant sections, verify correctness
  2. Test paragraph retrieval: With a known relevant section, test if the paragraph retrieval correctly identifies relevant paragraphs
  3. End-to-end with known answer: Use a document with a clear answer, verify the complete pipeline retrieves correct evidence and answers accurately

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of D3 vary when using different types of discourse structures beyond section-based organization (e.g., argument structures, narrative arcs)?
- Basis in paper: [inferred] The paper focuses on section-based discourse structure but mentions that documents often have hierarchical discourse structures. It evaluates the role of section headings but does not explore alternative discourse structures
- Why unresolved: The paper only experiments with section-based discourse structures and their summaries. It does not investigate how other discourse organizational patterns might affect performance
- What evidence would resolve it: Comparative experiments testing D3 with different discourse structure types (argument, narrative, etc.) on the same datasets, measuring retrieval performance and token efficiency

### Open Question 2
- Question: What is the optimal balance between summary length and information retention for maximizing D3's performance across different document types?
- Basis in paper: [explicit] The paper uses a fine-tuned BART model for summarization but notes that representing sections with summaries leads to information loss that affects performance. It mentions exploring different configurations but doesn't systematically study summary length optimization
- Why unresolved: The paper uses a fixed summarization approach without investigating how varying summary length affects downstream performance or token efficiency
- What evidence would resolve it: Controlled experiments varying summary length (e.g., percentage of original tokens) across document types while measuring evidence retrieval F1, token usage, and question-answering accuracy

### Open Question 3
- Question: How does D3's performance degrade as the number of relevant sections increases, and what strategies could mitigate this limitation?
- Basis in paper: [inferred] The method processes all paragraphs from relevant sections in the fine-grained retrieval step, which could become inefficient as relevant section count grows. The paper shows competitive performance but doesn't analyze scalability with increasing relevant sections
- Why unresolved: The paper demonstrates effectiveness but doesn't characterize performance degradation or provide solutions for scenarios with many relevant sections
- What evidence would resolve it: Experiments varying the number of relevant sections per query and testing hierarchical filtering strategies or dynamic summarization approaches to maintain efficiency

## Limitations
- The approach relies heavily on the presence of well-defined discourse structure in documents, which may not be available in all domains or document types
- The token efficiency claims (26% of baseline tokens while retaining 99.6% performance) are specific to the experimental conditions and datasets used
- The method assumes that section summaries adequately capture the information needed to judge relevance, which may not hold for highly technical or specialized content

## Confidence
- High Confidence: The general framework of using discourse structure for hierarchical evidence retrieval is well-supported by the experimental results
- Medium Confidence: The extension to multi-hop reasoning scenarios using the self-ask agent shows promise but is less thoroughly validated
- Medium Confidence: The zero-shot performance claims are demonstrated against specific baselines but lack comprehensive comparison to supervised approaches

## Next Checks
1. **Cross-domain robustness test**: Apply D3 to documents from domains with less explicit discourse structure (scientific papers, legal documents, or technical manuals) to assess whether the method maintains its effectiveness when discourse organization is implicit rather than explicit

2. **Ablation study on summarization quality**: Systematically vary the quality and detail level of section summaries to determine the threshold at which performance degrades, helping identify the minimum summarization requirements for the approach to work effectively

3. **Cost-benefit analysis across document lengths**: Test the method across a wider range of document lengths (shorter than QASPER, longer than HOTPOTQA-Doc) to determine the break-even point where multi-stage processing becomes less efficient than direct processing, and identify optimal document length ranges for the approach