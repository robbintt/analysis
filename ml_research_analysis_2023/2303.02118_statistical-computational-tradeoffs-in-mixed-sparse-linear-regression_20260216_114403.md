---
ver: rpa2
title: Statistical-Computational Tradeoffs in Mixed Sparse Linear Regression
arxiv_id: '2303.02118'
source_url: https://arxiv.org/abs/2303.02118
tags:
- theorem
- sb-mslr
- have
- mslr
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the computational hardness of mixed sparse\
  \ linear regression (MSLR) with two k-sparse signals, a problem that exhibits a\
  \ statistical-to-computational gap. The authors use the low-degree polynomial method\
  \ to establish a smooth trade-off between sample complexity and runtime, proving\
  \ that polynomial-time algorithms require at least n = \u03A9(k\xB2(SNR+1)\xB2/SNR\xB2\
  ) samples in a narrow symmetric regime (SB-MSLR), and provide reductions showing\
  \ hardness for broader regimes (PSB-MSLR and SPR)."
---

# Statistical-Computational Tradeoffs in Mixed Sparse Linear Regression

## Quick Facts
- **arXiv ID**: 2303.02118
- **Source URL**: https://arxiv.org/abs/2303.02118
- **Reference count**: 40
- **Key outcome**: Establishes computational hardness for mixed sparse linear regression (MSLR) with two k-sparse signals, proving a statistical-to-computational gap that only exists in a narrow symmetric parameter regime.

## Executive Summary
This paper studies the computational hardness of mixed sparse linear regression (MSLR) with two k-sparse signals, revealing a statistical-to-computational gap. Using the low-degree polynomial method, the authors prove that polynomial-time algorithms require at least n = Ω(k²(SNR+1)²/SNR²) samples in a narrow symmetric regime (SB-MSLR). Outside this hard regime, they analyze a simple thresholding algorithm (CORR) that achieves optimal sample complexity n = O(k(SNR+1)log p/SNR) and exactly recovers the joint support of both signals. These results reconcile contradictory findings in the literature and clarify the role of signal symmetries in computational hardness.

## Method Summary
The paper employs the low-degree polynomial method to establish computational lower bounds for MSLR detection by bounding the chi-squared divergence between planted and null distributions. This divergence is expanded in terms of normalized Hermite polynomials, with moment bounds on the overlap between signal copies proving convergence. Polynomial-time reductions transfer hardness from detection to recovery variants and to broader regimes. For algorithmic upper bounds, the CORR algorithm uses correlation thresholding to achieve joint support recovery with optimal sample complexity outside the hard regime. The analysis combines concentration inequalities, high-probability events, and careful moment computations.

## Key Results
- Proves computational hardness for MSLR detection in narrow symmetric regime (SB-MSLR) requiring n = Ω(k²(SNR+1)²/SNR²) samples
- Shows CORR algorithm achieves joint support recovery with optimal sample complexity n = O(k(SNR+1)log p/SNR) outside hard regime
- Establishes polynomial-time reductions transferring hardness from detection to recovery and to more general regimes (PSB-MSLR, SPR)
- Demonstrates CORR is order-optimal among low-degree polynomial algorithms for sparse linear regression

## Why This Works (Mechanism)

### Mechanism 1
The low-degree polynomial method proves computational hardness by showing the chi-squared divergence between planted and null distributions remains bounded (O(1)) for all polynomial-time algorithms when sample complexity is below a threshold. The proof expands the chi-squared divergence in terms of normalized Hermite polynomials of input data. For SB-MSLR detection, this expansion involves moments of the overlap between two independent copies of signal β1. By bounding these moments and showing the resulting weighted sum converges, the authors prove no low-degree polynomial algorithm can distinguish between hypotheses. Core assumption: Conjecture 1.2 holds, stating that if low-degree chi-squared divergence is O(1), then strong detection requires super-polynomial time.

### Mechanism 2
The CORR algorithm solves detection outside the narrow symmetric regime by leveraging that for most parameter settings, the weighted sum φβ1 + (1-φ)β2 is non-zero, allowing signal separation through simple thresholding. CORR computes correlation between each feature and response, normalized by response norm. For features in joint support, this correlation has larger mean than features outside support, enabling separation. Key insight: outside WSB-MSLR regime, this mean separation is sufficient for detection with optimal sample complexity. Core assumption: Signal prior ensures φβ1 + (1-φ)β2 ≠ 0 almost surely, which holds for absolutely continuous priors.

### Mechanism 3
Polynomial-time reductions transfer hardness from detection variant SB-MSLR-D to recovery variant SB-MSLR, and further to more general regimes like PSB-MSLR and SPR. The reduction constructs average-case reduction by showing if exact recovery were possible in harder regime (e.g., PSB-MSLR), it would imply ability to solve detection problem in SB-MSLR-D, contradicting low-degree lower bound. Done through careful construction of problem instances and preprocessing steps. Core assumption: Reductions preserve computational hardness, i.e., solving target problem implies solving source problem in polynomial time.

## Foundational Learning

- **Concept**: Low-degree polynomial method for proving computational lower bounds
  - Why needed here: This is the main technique used to establish computational hardness of MSLR problem. Understanding how to bound chi-squared divergence in terms of Hermite polynomials is crucial for following proofs.
  - Quick check question: What is the key quantity that needs to be bounded to prove computational hardness using the low-degree method?

- **Concept**: Hermite polynomials and their properties
  - Why needed here: Proofs rely heavily on properties of Hermite polynomials, such as orthonormality with respect to Gaussian measures and behavior under integration by parts. Familiarity with these properties is essential for understanding derivations.
  - Quick check question: What is the recursion relation for univariate Hermite polynomials?

- **Concept**: Concentration inequalities and tail bounds
  - Why needed here: Analysis of CORR algorithm relies on concentration inequalities to bound probability of false positives and false negatives. Understanding these inequalities is crucial for following proof of correctness.
  - Quick check question: What is the Chernoff bound for probability that sum of independent Bernoulli random variables deviates from its mean?

## Architecture Onboarding

- **Component map**: Low-degree method analysis (Section B) -> Polynomial-time reductions (Section C) -> CORR algorithm analysis (Section D.1-D.4) -> Supporting lemmas and technical results

- **Critical path**: 
  1. Define detection and recovery variants of MSLR problem
  2. Apply low-degree method to prove hardness for detection variant in SB-MSLR regime
  3. Construct polynomial-time reductions to transfer hardness to recovery variant and more general regimes
  4. Analyze CORR algorithm to establish algorithmic upper bounds outside hard regime
  5. Combine results to characterize statistical-computational tradeoffs

- **Design tradeoffs**: 
  - Tightness of lower bounds vs. generality of result
  - Simplicity of CORR algorithm vs. optimality of sample complexity
  - Generality of signal prior vs. tightness of bounds

- **Failure signatures**:
  - If low-degree chi-squared divergence is not O(1), lower bounds may not hold
  - If signal prior places significant mass on WSB-MSLR regime, CORR algorithm may fail
  - If polynomial-time reductions are not computationally efficient, hardness results may not transfer

- **First 3 experiments**:
  1. Implement low-degree polynomial framework to compute chi-squared divergence between planted and null distributions for MSLR and SLR, following analysis in Appendices B.1, B.2, and B.3. Use Hermite polynomial expansions and moment bounds as outlined.
  2. Implement CORR algorithm and verify its success in recovering joint support in MSLR and signed support in SLR, as per Theorems D.1, D.3, and D.21. Simulate data under model and apply CORR, checking thresholding conditions and error probabilities.
  3. Implement reductions from detection to recovery (Lemma C.1 and Lemma C.2) and from SB-MSLR to SPR (Theorem C.10) to transfer hardness results. Construct preprocessing and postprocessing steps and verify polynomial-time complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can computational lower bounds for MSLR be extended beyond regime where k = o(√p)?
- Basis in paper: [explicit] Paper notes k = o(√p) assumption is often standard for detection lower bounds but mentions it can sometimes be lifted by conditioning away certain bad event.
- Why unresolved: Current proof technique relies on bounding D moments of overlap ⟨β(1)₁, β(2)₁⟩, which becomes challenging when k is not sublinear in √p.
- What evidence would resolve it: Proof showing low-degree method can be adapted to handle k = Ω(√p) through alternative techniques, or experimental evidence demonstrating current lower bounds still hold in this regime.

### Open Question 2
- Question: Is there polynomial-time algorithm that can solve exact support recovery in PSB-MSLR regime?
- Basis in paper: [explicit] Paper establishes PSB-MSLR is computationally hard through reductions from SB-MSLR, but does not provide algorithm for this regime.
- Why unresolved: Paper proves any polynomial-time algorithm for PSB-MSLR would contradict computational hardness of SB-MSLR, but does not explore whether alternative algorithmic approaches could circumvent this reduction.
- What evidence would resolve it: Polynomial-time algorithm for exact support recovery in PSB-MSLR, or proof that no such algorithm exists (perhaps by extending low-degree method or finding reduction from known hard problem).

### Open Question 3
- Question: Can sample complexity requirements for CORR algorithm be improved for specific signal priors, such as sparse phase retrieval with binary signals?
- Basis in paper: [explicit] Paper shows CORR achieves order-optimal performance for SLR and MSLR outside narrow hard regime, but notes existing algorithms for sparse phase retrieval require n = Ω(k²) samples.
- Why unresolved: Paper's analysis of CORR is general and does not exploit specific structure of sparse phase retrieval or other signal priors.
- What evidence would resolve it: Analysis of CORR for sparse phase retrieval with binary signals showing improved sample complexity, or proof that such improvement is impossible.

## Limitations
- Computational lower bounds established only under Conjecture 1.2, which remains unproven
- Bounds are tight only in symmetric Bernoulli-φ prior regime (SB-MSLR), with less tight results for more general priors (PSB-MSLR, SPR)
- Dependence on signal amplitude ∥β∥∞ in lower bounds stated to be artifact of proof technique, suggesting potential for improvement

## Confidence

- **High**: The statistical-computational tradeoff characterization for SB-MSLR (Theorems 2.1, 2.2, 2.3)
- **Medium**: The algorithmic upper bounds for CORR outside the hard regime (Theorems 2.4, 2.5, 2.6, 2.7)
- **Medium**: The polynomial-time reductions transferring hardness (Theorems C.6, C.7, C.10)

## Next Checks

1. Implement the low-degree polynomial analysis to compute chi-squared divergence for various signal priors and parameter regimes, verifying the convergence bounds and computational hardness claims.

2. Conduct experiments with the CORR algorithm on synthetic data across different SNR levels and sample sizes to empirically validate the sample complexity bounds and detection performance outside the hard regime.

3. Verify the polynomial-time reductions by implementing the preprocessing and postprocessing steps, checking that they correctly transform instances while preserving computational hardness.