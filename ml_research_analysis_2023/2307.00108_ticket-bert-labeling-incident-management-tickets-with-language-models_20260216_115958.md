---
ver: rpa2
title: 'Ticket-BERT: Labeling Incident Management Tickets with Language Models'
arxiv_id: '2307.00108'
source_url: https://arxiv.org/abs/2307.00108
tags:
- ticket
- tickets
- ticket-bert
- data
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Ticket-BERT, a language model for labeling
  incident management tickets. The key contributions include: (1) creating three large-scale
  datasets (D-Human, D-Machine, D-Mixture) with over 76K tickets and 10 fine-grained
  labels; (2) developing Ticket-BERT, a BERT-based model that outperforms strong baselines
  on all datasets; (3) proposing a prompt-prefix strategy using ticket titles/summaries
  to boost performance, especially on the challenging D-Human dataset; and (4) designing
  an active learning pipeline for real-time model adaptation.'
---

# Ticket-BERT: Labeling Incident Management Tickets with Language Models

## Quick Facts
- arXiv ID: 2307.00108
- Source URL: https://arxiv.org/abs/2307.00108
- Reference count: 18
- Key outcome: Ticket-BERT achieves F1-scores above 85% on all datasets and over 98% on D-Machine and D-Mixture.

## Executive Summary
This paper presents Ticket-BERT, a language model for labeling incident management tickets. The authors create three large-scale datasets (D-Human, D-Machine, D-Mixture) with over 76K tickets and 10 fine-grained labels. They develop a BERT-based model that outperforms strong baselines on all datasets. The key innovations include a prompt-prefix strategy using ticket titles/summaries to boost performance, especially on the challenging D-Human dataset, and an active learning pipeline for real-time model adaptation. Ticket-BERT demonstrates state-of-the-art results, with F1-scores above 85% on all datasets and over 98% on D-Machine and D-Mixture.

## Method Summary
The method involves preprocessing ticket data by cleaning URLs, code, HTML tags, tables, braces, and metadata, converting to lowercase, and removing stopwords. Three datasets (D-Human, D-Machine, D-Mixture) are constructed by drawing long ticket descriptions (n > 50 characters) from the first few updates (T1, T2, T3). BERT-base is fine-tuned with a multi-layer perceptron and softmax for multi-class classification, using a prompt-prefix strategy that concatenates ticket titles and summaries with descriptions. The model is evaluated using macro Precision, Recall, F1-Score, and AUC of ROC.

## Key Results
- Ticket-BERT outperforms strong baselines on all datasets, achieving F1-scores above 85%.
- The prompt-prefix strategy significantly boosts performance, especially on the challenging D-Human dataset.
- An active learning pipeline enables real-time model adaptation and handles diverse incidents over time.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ticket-BERT achieves superior performance by leveraging auxiliary prompt-prefixes (title and summary) to enrich the context of ticket descriptions.
- Mechanism: Concatenating ticket titles and summaries with descriptions before feeding them into the BERT-base model helps the model capture more issue-specific information, especially for human-generated tickets which have high variability and diverse vocabularies.
- Core assumption: Ticket titles and summaries contain salient information that, when combined with descriptions, improves the model's ability to classify tickets correctly.
- Evidence anchors:
  - [abstract] "proposing a prompt-prefix strategy using ticket titles/summaries to boost performance, especially on the challenging D-Human dataset"
  - [section] "Table 3 shows the model performance after concatenating prompt-prefix... We observe strong boosts for all the models on the D-Human dataset"
  - [corpus] Weak evidence - corpus does not contain relevant papers on prompt-prefix strategies for ticket classification.
- Break condition: If the auxiliary text (title/summary) does not contain relevant or salient information, or if it introduces noise, the performance boost may not occur.

### Mechanism 2
- Claim: The active learning pipeline enables Ticket-BERT to adapt to new data distributions and handle diverse incidents over time.
- Mechanism: The pool-based active learning cycle queries the most uncertain tickets, annotates them, and fine-tunes the model iteratively, allowing it to learn from newly collected tickets and newly developed ticket labels.
- Core assumption: The model can improve its performance by learning from newly labeled data that it previously found uncertain or difficult to classify.
- Evidence anchors:
  - [abstract] "designing an active learning pipeline for real-time model adaptation... enables the model to quickly finetune on newly-collected tickets with a few annotations"
  - [section] "To bridge this gap, we build a pool-based active learning cycle... we hope to achieve near-perfect accuracy as we continue to iterate with our customers"
  - [corpus] Weak evidence - corpus does not contain relevant papers on active learning for ticket classification.
- Break condition: If the newly collected data is not diverse enough or if the annotation process is not efficient, the active learning cycle may not provide significant improvements.

### Mechanism 3
- Claim: Ticket-BERT outperforms traditional text classification methods (e.g., Naive Bayes, Logistic Regression) due to its ability to learn context-aware text information from diverse environments.
- Mechanism: The BERT-base model, pretrained on large corpora, can capture complex semantic relationships in ticket descriptions, making it more effective at classifying tickets into fine-grained categories compared to methods that rely on BoW or TF-IDF features.
- Core assumption: Pretrained language models like BERT are better suited for handling the variability and complexity of ticket descriptions than traditional methods.
- Evidence anchors:
  - [abstract] "Experiments demonstrate the superiority of Ticket-BERT over baselines and state-of-the-art text classifiers on Azure Cognitive Services"
  - [section] "Table 2 shows the baselines and Ticket-BERT model performance... the Ticket-BERT shows competitive results on D-Human in terms of precision, recall, and F1 scores"
  - [corpus] Weak evidence - corpus does not contain relevant papers comparing BERT-based models to traditional methods for ticket classification.
- Break condition: If the ticket descriptions are too short or lack sufficient context, the advantages of BERT-based models may be diminished.

## Foundational Learning

- Concept: Text preprocessing and cleaning
  - Why needed here: To remove noise (URLs, code, HTML tags, etc.) and standardize the text (lowercase, remove stopwords) for better model performance.
  - Quick check question: What are the steps involved in preprocessing the ticket descriptions, and why are they important?

- Concept: Fine-tuning pre-trained language models
  - Why needed here: To adapt the BERT-base model to the specific task of ticket labeling by training it on the ticket datasets.
  - Quick check question: How does fine-tuning a pre-trained model differ from training a model from scratch, and why is it beneficial in this context?

- Concept: Active learning
  - Why needed here: To iteratively improve the model's performance by incorporating newly labeled data and adapting to changes in the ticket data distribution over time.
  - Quick check question: What is the purpose of active learning in the context of Ticket-BERT, and how does it help address the challenges of real-world ticket labeling?

## Architecture Onboarding

- Component map:
  - Data preprocessing pipeline: cleans and standardizes ticket descriptions, titles, and summaries.
  - BERT-base model: pretrained language model fine-tuned for ticket labeling.
  - Multi-layer perceptron (MLP): classifies ticket descriptions into fine-grained categories.
  - Active learning pipeline: iteratively improves the model by incorporating newly labeled data.

- Critical path: Ticket descriptions → preprocessing → BERT-base + MLP → classification → active learning (if enabled) → updated model

- Design tradeoffs:
  - Using BERT-base instead of training a model from scratch: faster training and better performance, but requires more computational resources.
  - Implementing active learning: continuous improvement, but requires efficient annotation processes and diverse data collection.

- Failure signatures:
  - Poor performance on specific ticket categories: may indicate insufficient training data or model bias.
  - Slow convergence during fine-tuning: may indicate suboptimal hyperparameters or data quality issues.
  - Active learning cycle not providing improvements: may indicate insufficient diversity in newly collected data or inefficient annotation processes.

- First 3 experiments:
  1. Compare Ticket-BERT's performance to baseline models (Naive Bayes, Logistic Regression) on the D-Human dataset.
  2. Evaluate the impact of the prompt-prefix strategy by comparing Ticket-BERT's performance with and without auxiliary text (title/summary).
  3. Test the active learning pipeline by simulating the addition of newly labeled data and measuring the model's performance improvement over time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Ticket-BERT's performance compare to human experts on labeling tickets?
- Basis in paper: [inferred] The paper mentions Ticket-BERT achieves 90% accuracy on "hard-to-identify tickets" that are difficult for human annotators, but does not provide direct comparison data.
- Why unresolved: The paper lacks a systematic comparison of Ticket-BERT's performance against human experts on a standard benchmark dataset.
- What evidence would resolve it: A head-to-head comparison of Ticket-BERT and human experts on a standardized dataset of incident tickets with known labels.

### Open Question 2
- Question: How well does Ticket-BERT generalize to incident tickets from other companies or domains?
- Basis in paper: [inferred] The paper notes that Ticket-BERT was trained and tested on Microsoft Azure hardware tickets, and explicitly states it may not generalize to all tickets within the Microsoft IcM system or to other companies.
- Why unresolved: The paper lacks experiments or evidence of Ticket-BERT's performance on datasets from other companies or domains.
- What evidence would resolve it: Evaluation of Ticket-BERT on a diverse set of incident ticket datasets from multiple companies and domains, demonstrating its generalizability.

### Open Question 3
- Question: How does Ticket-BERT's performance change over time as the ticket labeling system is updated with new data and labels?
- Basis in paper: [explicit] The paper mentions that Ticket-BERT is deployed with an active learning pipeline for continuous updates, but does not provide long-term performance data.
- Why unresolved: The paper lacks long-term performance data or studies on how Ticket-BERT's performance evolves as it is continuously updated with new data and labels.
- What evidence would resolve it: Longitudinal studies tracking Ticket-BERT's performance over time as it is updated with new data and labels, demonstrating its adaptability and improvement.

## Limitations

- The exact implementation details of the prompt-prefix strategy are not fully specified, which may hinder reproducibility.
- The effectiveness of the active learning pipeline is mentioned but not thoroughly validated through experiments.
- The datasets used for training and evaluation are limited to Microsoft Azure hardware tickets, which may not generalize well to other companies or domains.

## Confidence

- **High Confidence**: The general approach of using BERT for ticket classification and the reported performance metrics (F1-scores above 85% on all datasets) are well-supported by the experimental results presented in the paper.
- **Medium Confidence**: The claims about the prompt-prefix strategy boosting performance, especially on the D-Human dataset, are supported by the reported results but lack detailed implementation specifics.
- **Low Confidence**: The effectiveness of the active learning pipeline and the reproducibility of the exact dataset construction and preprocessing steps are not well-supported due to limited experimental validation and unspecified details.

## Next Checks

1. **Replicate Prompt-Prefix Strategy**: Implement the prompt-prefix strategy using the described approach (concatenating ticket titles and summaries with descriptions) and evaluate its impact on performance, particularly for the D-Human dataset.

2. **Test Active Learning Pipeline**: Simulate the active learning cycle by iteratively adding newly labeled data and measure the model's performance improvement over time. Assess the efficiency of the annotation process and the diversity of the newly collected data.

3. **Validate Dataset Construction**: Reconstruct the three datasets (D-Human, D-Machine, D-Mixture) using the described method (drawing long ticket descriptions from the first few updates) and verify the reported dataset sizes and label distributions.