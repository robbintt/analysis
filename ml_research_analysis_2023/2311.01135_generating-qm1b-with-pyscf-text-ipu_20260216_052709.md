---
ver: rpa2
title: Generating QM1B with PySCF$_{\text{IPU}}$
arxiv_id: '2311.01135'
source_url: https://arxiv.org/abs/2311.01135
tags:
- training
- learning
- qm1b
- datasets
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PySCFIPU, a hardware-accelerated DFT data
  generator using Intelligence Processing Units (IPUs) to create QM1B, a dataset with
  one billion training examples containing 9-11 heavy atoms. The authors demonstrate
  that a simple neural network (SchNet 9M) improves its performance by increasing
  the amount of training data without additional inductive biases.
---

# Generating QM1B with PySCF$_{\text{IPU}}$

## Quick Facts
- arXiv ID: 2311.01135
- Source URL: https://arxiv.org/abs/2311.01135
- Reference count: 40
- Key outcome: Hardware-accelerated DFT dataset generation using IPUs created QM1B (1B molecules), showing that simple neural networks improve performance with larger training data alone.

## Executive Summary
This paper introduces PySCFIPU, a hardware-accelerated DFT data generator that leverages Intelligence Processing Units (IPUs) to create QM1B, a dataset containing one billion training examples with 9-11 heavy atoms. The authors demonstrate that a simple neural network (SchNet 9M) improves its performance by increasing the amount of training data without additional inductive biases. The validation Mean Absolute Error (MAE) improved from 285meV to 32meV as the number of training samples approached 500M. The paper highlights the low-resolution of their DFT options and encourages future researchers to use QM1B responsibly.

## Method Summary
The authors developed PySCFIPU, a hardware-accelerated DFT library built on JAX with IPU TensorFlow XLA backend, which uses float32 precision and restricted Kohn-Sham DFT with B3LYP functional. They generated QM1B by processing SMILES strings through RDKit conformer generation, then computing molecular properties (energy, HOMO, LUMO) using PySCFIPU. A SchNet model was trained on progressively larger subsets of QM1B, showing improved validation MAE from 285meV to 32meV as training data increased to 500M examples.

## Key Results
- Validation MAE improved from 285meV to 32meV as training data increased from 100k to 500M examples
- Training and validation performance overlapped after approximately 100M samples, suggesting underfitting
- Dataset generation completed in 40,000 IPU hours versus approximately 2 years on CPU supercomputers
- PySCFIPU uses float32 precision with numerical errors of 6meV for energies and 0.2meV for HL gap compared to float64

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PySCFIPU accelerates DFT dataset generation by leveraging IPU hardware advantages over traditional CPU implementations.
- Mechanism: IPUs provide 940MB on-chip memory with 12-65TB/s bandwidth, enabling small DFT computations without relying on slower off-chip RAM. Additionally, IPUs support MIMD parallelism, which simplifies parallel computation of electron repulsion integrals compared to SIMD parallelism.
- Core assumption: The hardware-specific advantages of IPUs (on-chip memory bandwidth and MIMD parallelism) translate directly to performance improvements for DFT calculations.
- Evidence anchors:
  - [abstract] "This allowed us to create the dataset QM1B with one billion training examples within 40000 IPU hours, substantially less than the two years it took to create PCQ on CPU supercomputers."
  - [section 3.2] "IPUs have 940MB on-chip memory with 12-65TB/s bandwidth, enough to perform small DFT computations without relying on off-chip RAM with < 3TB/s bandwidth."
  - [section 3.2] "IPUs support Multiple Instruction Multiple Data (MIMD) parallelism which simplifies parallel computation of the notoriously difficult computation of Electron Repulsion Integrals."
- Break condition: When molecular size exceeds IPU memory capacity (N > 70 atomic orbitals), or when the computational workload doesn't benefit from MIMD parallelism.

### Mechanism 2
- Claim: Using lower precision float32 instead of float64 in PySCFIPU significantly speeds up DFT computations while maintaining sufficient accuracy for training neural networks.
- Mechanism: Float32 computations are approximately 6x faster than float64 on IPUs, and the numerical error introduced (6meV for energies, 0.2meV for HL gap) is much smaller than neural network prediction errors (20-70meV).
- Core assumption: The numerical precision requirements for generating training data are less stringent than for production DFT calculations, allowing float32 to be used without compromising neural network performance.
- Evidence anchors:
  - [section 3.2] "We use float32 instead of float64. JAX-XC (66) recently machine translated the libxc Maple files to JAX. This may allow us support more functionals by extending JAX-XC to float32."
  - [section 3.2] "While float32 has yet to be thoroughly demonstrated in DFT libraries, our goal is to generate data to train NNs, therefore, our requirements for numerical precision is less stringent compared to mainstream DFT libraries, our errors just have to be below that of NNs (20-70meV for HL gap)."
  - [section 3.2] "For converged molecules our MAE was 6meV for energies and 0.2meV for HL gap, see Figure 2."
- Break condition: When the numerical error from float32 becomes comparable to or larger than the neural network's prediction errors, or when downstream tasks require higher precision.

### Mechanism 3
- Claim: Scaling training data size improves neural network performance on quantum chemistry tasks without requiring architectural changes.
- Mechanism: As training data increases from 100k to 500M examples, validation MAE improves from 285meV to 32meV, with training and validation performance overlapping at larger scales, indicating underfitting that can be addressed by increasing model capacity.
- Core assumption: The relationship between dataset size and model performance observed in other domains (CV, NLP) also applies to quantum chemistry, and current neural network architectures are not yet data-saturated.
- Evidence anchors:
  - [abstract] "We demonstrate that a simple baseline neural network (SchNet 9M) improves its performance by simply increasing the amount of training data without additional inductive biases."
  - [section 4] "We found the validation Mean Absolute Error (MAE) improved from 285meV to 32meV, see Figure 1."
  - [section 4] "Interestingly we see that the training and validation performance plateaus after approximately 100M samples are in the dataset, this suggests that the model is underfitting, and increasing the model size would further improve performance."
- Break condition: When model performance plateaus despite further increases in training data, indicating that additional architectural innovations or data quality improvements are needed.

## Foundational Learning

- Concept: Density Functional Theory (DFT) and its computational requirements
  - Why needed here: Understanding DFT is essential to grasp why hardware acceleration and precision choices matter for dataset generation
  - Quick check question: What are the two main computational bottlenecks in DFT calculations that PySCFIPU addresses?

- Concept: Graph Neural Networks (GNNs) for molecular property prediction
  - Why needed here: The paper uses SchNet as a baseline model, so understanding GNN architectures is crucial for interpreting results
  - Quick check question: How does SchNet represent molecular structures, and what type of molecular properties does it predict?

- Concept: Hardware accelerators (IPUs) and their programming models
  - Why needed here: The paper's main contribution is hardware-accelerated dataset generation, so understanding IPU architecture is essential
  - Quick check question: What are the key differences between IPU and GPU architectures that make IPUs suitable for DFT computations?

## Architecture Onboarding

- Component map: PySCFIPU (hardware-accelerated DFT generator) -> QM1B dataset -> SchNet baseline model
- Critical path: Data generation pipeline - SMILES string → RDKit conformer generation → PySCFIPU DFT computation → QM1B dataset. Model training pipeline - QM1B sampling → SchNet training → validation on held-out data.
- Design tradeoffs: Lower DFT accuracy (STO-3G basis set) vs. larger dataset size, float32 vs. float64 precision, MIMD parallelism vs. SIMD efficiency, on-chip memory limits vs. computational throughput.
- Failure signatures: Memory overflow for molecules with N > 70 atomic orbitals, numerical instability in float32 computations, poor neural network performance despite large dataset size, compilation bottlenecks during data generation.
- First 3 experiments:
  1. Profile PySCFIPU on a small molecule (N < 12) to measure computational throughput and memory usage
  2. Compare float32 vs. float64 DFT results on a subset of QM1B to quantify numerical error
  3. Train SchNet on progressively larger subsets of QM1B to verify the scaling relationship between data size and model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does pretraining on low-resolution DFT data (QM1B) affect the performance of models fine-tuned on higher-resolution datasets like PCQ?
- Basis in paper: [explicit] The authors mention that the HL gap disagreement between STO-3G and 6-31G* basis sets is larger than chemical accuracy and state it remains an open problem how pretraining on 1B low-resolution conformers may bias subsequent models fine-tuned on downstream tasks.
- Why unresolved: The paper only provides preliminary evidence by showing pretraining on QM1B improves performance when fine-tuning on QM9, but doesn't explore the effects on higher-resolution datasets like PCQ.
- What evidence would resolve it: Comparative experiments showing model performance when fine-tuned on PCQ after pretraining on QM1B vs training from scratch on PCQ.

### Open Question 2
- Question: What is the optimal balance between dataset size and DFT accuracy for training foundation models in quantum chemistry?
- Basis in paper: [explicit] The authors discuss trading off dataset size for data quality (DFT accuracy) and created QM1B with lower resolution DFT to enable larger datasets.
- Why unresolved: The paper demonstrates that larger datasets improve model performance but doesn't investigate the relationship between DFT accuracy and model performance, or determine an optimal point.
- What evidence would resolve it: Systematic experiments varying both dataset size and DFT accuracy to identify the point of diminishing returns.

### Open Question 3
- Question: How would using double precision (float64) instead of single precision (float32) in DFT calculations affect the quality of the QM1B dataset and subsequent model performance?
- Basis in paper: [explicit] The authors implemented PySCFIPU using float32 instead of the standard float64 used in DFT libraries, reporting numerical errors of 6meV for energies and 0.2meV for HL gap compared to float64.
- Why unresolved: While the authors show these errors are small compared to neural network errors, they don't investigate whether using float64 would significantly improve model performance or dataset quality.
- What evidence would resolve it: Comparative experiments generating a subset of QM1B using float64 and training models on both versions to measure performance differences.

## Limitations

- The paper lacks direct performance comparisons between PySCFIPU and CPU implementations on identical hardware setups
- Float32 numerical precision validation is limited to converged molecules without testing edge cases or non-converged systems
- The scaling relationship between dataset size and model performance remains unproven beyond 500M examples

## Confidence

- **High Confidence**: The hardware-acceleration mechanism for small molecules (N < 70 atomic orbitals) and the float32 precision error bounds (6meV for energies, 0.2meV for HL gap) are well-supported by direct measurements and comparisons.
- **Medium Confidence**: The claim that scaling training data improves neural network performance without architectural changes is demonstrated empirically but may not hold at larger scales or with different model architectures.
- **Low Confidence**: The assertion that PySCFIPU's performance is "substantially less" than CPU supercomputers lacks quantitative benchmarks and specific comparison metrics.

## Next Checks

1. Conduct direct performance comparisons between PySCFIPU and CPU implementations on identical hardware setups, measuring both computational throughput and energy efficiency across different molecular sizes.
2. Expand float32 numerical precision testing to include non-converged molecules and edge cases (e.g., near-degeneracies, transition states) to verify error bounds hold across diverse chemical systems.
3. Train SchNet on QM1B subsets larger than 500M examples to determine whether the observed scaling relationship continues, plateaus, or reverses, and identify the optimal dataset size for model performance.