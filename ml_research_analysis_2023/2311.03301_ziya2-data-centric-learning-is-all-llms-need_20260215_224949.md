---
ver: rpa2
title: 'Ziya2: Data-centric Learning is All LLMs Need'
arxiv_id: '2311.03301'
source_url: https://arxiv.org/abs/2311.03301
tags:
- data
- training
- ziya2
- arxiv
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Ziya2, a 13-billion-parameter language model
  based on LLaMA2 and further pre-trained on 700 billion tokens. The authors address
  the challenges of high pre-training costs and catastrophic forgetting in continual
  learning by focusing on data-centric optimization.
---

# Ziya2: Data-centric Learning is All LLMs Need

## Quick Facts
- arXiv ID: 2311.03301
- Source URL: https://arxiv.org/abs/2311.03301
- Reference count: 13
- Ziya2 is a 13-billion-parameter language model that significantly outperforms LLaMA2 on multiple benchmarks through data-centric optimization

## Executive Summary
Ziya2 is a 13-billion-parameter language model built on LLaMA2 and further pre-trained on 700 billion tokens. The authors address challenges of high pre-training costs and catastrophic forgetting by focusing on data-centric optimization through a proprietary data factory. The model employs a three-stage continual pre-training strategy and achieves significant performance improvements across multiple benchmarks including MMLU, CMMLU, C-Eval, GSM8K, MATH, and HumanEval.

## Method Summary
Ziya2 builds on LLaMA2-13B with several key modifications: tokenizer vocabulary expansion for Chinese characters, rotary position encoding in FP32 precision, and a three-stage continual pre-training strategy. The model uses a proprietary data factory to process and filter high-quality web data, with the pre-training divided into unsupervised, supervised, and domain-specific stages. The approach emphasizes data quality over architectural complexity, claiming that data-centric optimization is sufficient to achieve state-of-the-art performance.

## Key Results
- Improves LLaMA2's MMLU performance by 10%
- Achieves 61% improvement on CMMLU and 68% on C-Eval
- Shows dramatic gains on specialized tasks: 138% on GSM8K, 120% on MATH, and 89% on HumanEval

## Why This Works (Mechanism)

### Mechanism 1
Continual pre-training on high-quality curated data preserves and enhances model capabilities without catastrophic forgetting. The three-stage training strategy uses large-scale unsupervised data first to refresh general knowledge, then supervised instruction data to adapt to downstream tasks, and finally domain-specific data (e.g., mathematics) to specialize without losing earlier capabilities. Core assumption: High-quality data filtering ensures that only relevant, coherent, and non-toxic examples are used for training.

### Mechanism 2
Extending tokenizer vocabulary with additional Chinese characters improves Chinese language representation efficiency. The BPE tokenizer is augmented by adding 7,400 commonly used Chinese characters to the original LLaMA2 vocabulary, reducing the average token count per Chinese character from 2-4 to approximately 1. Core assumption: Chinese characters are semantically rich and can be tokenized as single tokens without loss of meaning.

### Mechanism 3
Positional embedding adaptation to new data length distributions prevents overflow and improves stability during mixed-precision training. Rotary position encoding is implemented in FP32 precision to handle longer sequences and divergent length distributions between LLaMA2 and the new dataset. Core assumption: LLaMA2's original positional encoding is insufficient for the length distribution shift in continual pre-training data.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper explicitly addresses catastrophic forgetting as a key challenge when pre-training on new data after LLaMA2
  - Quick check question: What training strategy does Ziya2 use to mitigate catastrophic forgetting?

- Concept: Data quality filtering for LLMs
  - Why needed here: The paper's entire approach hinges on high-quality data curation via perplexity scoring, rule-based filtering, and de-duplication
  - Quick check question: Which metrics are used in the data evaluation stage?

- Concept: Mixed-precision training and overflow handling
  - Why needed here: The paper modifies LLaMA2's normalization and attention layers to prevent overflow in BF16 training
  - Quick check question: Why does Ziya2 use BF16 instead of FP16 during training?

## Architecture Onboarding

- Component map: Data Factory → Tokenizer + Positional Embedding + Normalization/Attention → Three-stage Continual Pre-training → Benchmark Evaluation
- Critical path: Data preprocessing → High-quality data selection → Continual pre-training (3 stages) → Model evaluation
- Design tradeoffs: Larger tokenizer vocabulary improves Chinese efficiency but increases model size; FP32 positional encoding prevents overflow but adds computation; three-stage training reduces forgetting but increases total training time
- Failure signatures: Overfitting to low-quality data (high perplexity scores), training instability (loss spikes), degraded performance on source language tasks (English in this case)
- First 3 experiments:
  1. Run the data factory pipeline on a small sample of CCNet data and verify perplexity thresholds
  2. Test tokenizer vocabulary expansion on a held-out Chinese corpus and measure token count per character
  3. Conduct a single-stage continual pre-training on unsupervised data and compare perplexity to LLaMA2 baseline

## Open Questions the Paper Calls Out

- How does the performance of Ziya2 compare to other models when evaluated on domain-specific tasks beyond mathematics and programming? The paper focuses on mathematics and programming benchmarks but does not extensively compare performance on other domain-specific tasks.

- What are the long-term effects of the three-stage pre-training strategy on the model's ability to generalize to unseen tasks or domains? While the paper highlights improvements in specific tasks, it does not explore how the model's pre-training strategy impacts its ability to adapt to new and unseen tasks over time.

- How does the data-centric approach impact the model's robustness to adversarial attacks or out-of-distribution inputs? The paper focuses on data quality and pre-training strategies but does not address the model's robustness to adversarial scenarios.

## Limitations

- The proprietary data factory and its specific filtering rules are not fully disclosed, limiting reproducibility
- The claimed performance improvements cannot be independently verified without access to the exact training configurations
- The non-commercial license restricts independent validation by the broader research community

## Confidence

- **High confidence**: The three-stage continual pre-training strategy is technically sound and aligns with established practices in LLM training
- **Medium confidence**: The data quality filtering mechanisms are described but not fully detailed, making it difficult to assess their effectiveness independently
- **Low confidence**: The claimed performance improvements on benchmarks are impressive but cannot be independently verified without access to the proprietary data factory

## Next Checks

1. Replicate the data factory pipeline on a small sample of publicly available web data and verify that the perplexity-based filtering produces consistent results with claimed high-quality dataset
2. Conduct a controlled experiment comparing token counts per Chinese character between original LLaMA2 tokenizer and expanded Ziya2 tokenizer on held-out Chinese corpus
3. Implement simplified three-stage continual pre-training on public dataset and measure performance degradation on English benchmarks to validate catastrophic forgetting prevention claim