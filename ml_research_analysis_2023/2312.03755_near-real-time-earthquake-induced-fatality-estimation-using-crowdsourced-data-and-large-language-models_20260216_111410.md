---
ver: rpa2
title: Near-real-time Earthquake-induced Fatality Estimation using Crowdsourced Data
  and Large-Language Models
arxiv_id: '2312.03755'
source_url: https://arxiv.org/abs/2312.03755
tags:
- data
- earthquake
- information
- human
- media
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents an end-to-end framework to automatically retrieve
  accurate earthquake-induced human loss statistics from multi-sourced social media
  data. It addresses challenges such as complex semantics, noisy and conflicting information,
  and dynamic changes in reported losses.
---

# Near-real-time Earthquake-induced Fatality Estimation using Crowdsourced Data and Large-Language Models

## Quick Facts
- **arXiv ID**: 2312.03755
- **Source URL**: https://arxiv.org/abs/2312.03755
- **Reference count**: 37
- **Key outcome**: End-to-end framework automatically retrieves accurate earthquake-induced human loss statistics from multi-sourced social media data, achieving comparable timeliness and accuracy to USGS manual methods.

## Executive Summary
This paper presents an end-to-end framework for automatically retrieving accurate earthquake-induced human loss statistics from multi-sourced social media data. The framework addresses challenges such as complex semantics, noisy and conflicting information, and dynamic changes in reported losses. Tested on three real-world earthquakes in 2021 and 2022, the framework achieves comparable timeliness and accuracy to manual methods by USGS. The hierarchical event classifier demonstrates high accuracy (97.4% for earthquake classification, 96.1% for casualty statistics classification) and low false positive rates.

## Method Summary
The framework integrates four key components: a hierarchical event classifier using XLM-RoBERTa for filtering irrelevant multilingual data, a large language model-based casualty extraction module using GPT-J with few-shot learning to extract precise casualty numbers and locations, a physical constraint-aware dynamic truth discovery model to filter noise and resolve conflicts among conflicting casualty reports, and a Bayesian updating loss projection model to integrate new reports while respecting physical reality that death tolls cannot decrease.

## Key Results
- Hierarchical event classifier achieves 97.4% accuracy for earthquake classification and 96.1% for casualty statistics classification
- Large language model-based casualty extraction effectively extracts exact numbers of fatalities, injuries, and locations from complex multilingual text
- Physical constraint-aware dynamic truth discovery successfully uncovers truthful human loss from noisy and conflicting claims
- Framework significantly improves timeliness and accuracy of global earthquake-induced human loss forecasting using multi-lingual, crowdsourced social media

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical event classifier reduces false positives and improves efficiency by filtering irrelevant text before costly casualty extraction. Two-stage RoBERTa-based classification (earthquake event relevance + casualty statistics presence) culls irrelevant multilingual data early, reducing computational load on LLMs. Core assumption: majority of retrieved social media posts are irrelevant to target earthquake event or do not contain casualty statistics.

### Mechanism 2
Large language models with few-shot learning extract precise casualty numbers and locations from complex multilingual text. GPT-J processes structured prompts with examples, extracting exact fatality/injury counts and locations even with abbreviations, ambiguous syntax, or non-English text. Core assumption: LLMs retain sufficient world knowledge to disambiguate local place names and interpret informal text patterns.

### Mechanism 3
Physical constraint-aware dynamic truth discovery filters noise and resolves conflicts among conflicting casualty reports. Information scores weighted by confidence, relevance, and independence are aggregated into probability distributions constrained so that fatality numbers never decrease over time. Core assumption: casualty numbers reported by different sources follow monotonic increase patterns over time due to unfolding disaster information.

## Foundational Learning

- **Concept**: Multilingual text classification with cross-lingual transfer
  - Why needed here: Earthquake events involve global social media posts in many languages; model must generalize from English-labeled data to other languages
  - Quick check question: Why does XLM-RoBERTa enable multilingual classification without training data in every language?

- **Concept**: Few-shot learning with structured prompts
  - Why needed here: Real-time deployment cannot afford full fine-tuning; structured examples in prompts allow LLMs to adapt to casualty extraction task instantly
  - Quick check question: How does providing example tweets with query-response pairs in the prompt guide GPT-J to extract casualty statistics?

- **Concept**: Bayesian updating with physical constraints
  - Why needed here: Casualty reports evolve over time; Bayesian updating allows integration of new reports while respecting physical reality that death tolls cannot decrease
  - Quick check question: Why must the transition matrix for casualty estimates be upper triangular?

## Architecture Onboarding

- **Component map**: Data pipeline → Hierarchical event classifier (earthquake + casualty filters) → LLM-based casualty extractor → Dynamic truth discovery (weighted aggregation + physical constraints) → Bayesian updating → PAGER loss projection
- **Critical path**: Data pipeline → Hierarchical event classifier → LLM casualty extraction → Truth discovery → Final fatality estimate
- **Design tradeoffs**: XLM-RoBERTa for multilingual coverage vs. slightly lower accuracy than monolingual RoBERTa; GPT-J for speed/cost vs. GPT-3 for potential accuracy gains
- **Failure signatures**: High false positive rate in event classifier → noisy LLM input; LLM confidence scores consistently low → prompt design issues; Truth discovery outputs unstable casualty numbers → source reliability estimation problems
- **First 3 experiments**:
  1. Evaluate event classifier on held-out multilingual disaster tweets to measure FPR and accuracy
  2. Test LLM casualty extraction on sample tweets with known answers to calibrate prompt examples
  3. Run truth discovery on synthetic conflicting reports to verify physical constraint enforcement

## Open Questions the Paper Calls Out

### Open Question 1
How can the accuracy of the dynamic truth discovery model be improved when dealing with rapidly evolving casualty reports in real-time? The paper acknowledges the challenge of finding truth when many posts are disingenuous and ground truth dynamically changes, but does not provide specific strategies or results on how to enhance accuracy in real-time scenarios.

### Open Question 2
What is the impact of using larger language models, such as GPT-3, on the performance of the hierarchical event classifier and human loss extraction module? The paper discusses using GPT-J but mentions that larger models like GPT-3 may further substantially improve information extraction performance without exploring these benefits.

### Open Question 3
How does the framework handle multilingual text data in languages other than English, and what is the impact on its performance? The paper mentions using XLM-RoBERTa and evaluates performance on languages other than English, but does not provide detailed results on handling multilingual text data, particularly in languages with fewer resources.

## Limitations
- Framework's performance on earthquakes outside Latin America and the Caribbean regions is untested
- Computational costs and latency of processing large volumes of social media data in real-time are not quantified
- Framework assumes casualty reports follow predictable patterns which may not hold across different cultural contexts or disaster types

## Confidence
- **High Confidence**: Hierarchical event classifier accuracy (97.4% for earthquake classification, 96.1% for casualty statistics classification)
- **Medium Confidence**: LLM-based casualty extraction effectiveness and framework's comparable timeliness to USGS manual methods
- **Low Confidence**: Claims about computational efficiency and scalability; assumption that social media casualty reports follow predictable patterns

## Next Checks
1. **Cross-linguistic robustness test**: Evaluate the hierarchical event classifier and LLM-based extraction on earthquakes in regions with non-Latin script languages (e.g., East Asia, Middle East) to assess cross-lingual generalization beyond tested Caribbean and South American cases.

2. **Temporal accuracy validation**: Conduct systematic comparison of casualty extraction timeliness against ground truth over extended time periods (24-48 hours post-event) to quantify trade-off between early reporting and accuracy, including false alarm rates.

3. **Source reliability estimation validation**: Test the dynamic truth discovery model's ability to correctly weight and aggregate casualty reports from sources with known reliability profiles, including simulated scenarios with coordinated misinformation campaigns or initial over-reporting.