---
ver: rpa2
title: Accelerating Split Federated Learning over Wireless Communication Networks
arxiv_id: '2310.15584'
source_url: https://arxiv.org/abs/2310.15584
tags:
- latency
- split
- device
- learning
- devices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of communication and computation
  efficiency in split federated learning (SFL) over wireless networks. The authors
  propose a joint optimization framework that selects optimal split points for deep
  neural networks (DNNs) and allocates bandwidth among heterogeneous devices to minimize
  total system latency.
---

# Accelerating Split Federated Learning over Wireless Communication Networks

## Quick Facts
- arXiv ID: 2310.15584
- Source URL: https://arxiv.org/abs/2310.15584
- Reference count: 38
- This paper proposes a joint optimization framework that minimizes total system latency in split federated learning by optimizing split points and bandwidth allocation.

## Executive Summary
This paper addresses the communication and computation efficiency challenges in split federated learning (SFL) over wireless networks. The authors propose a joint optimization framework that selects optimal split points for deep neural networks and allocates bandwidth among heterogeneous devices to minimize total system latency. The method decomposes the problem into two subproblems: split point optimization solved by backward induction and bandwidth allocation solved by convex optimization, which are solved iteratively through alternating optimization. Experimental results demonstrate significant latency reduction compared to traditional federated learning, achieving over 75% latency savings while maintaining competitive accuracy.

## Method Summary
The proposed method jointly optimizes DNN split points and bandwidth allocation in wireless SFL systems. It decomposes the problem into two subproblems: (1) split point optimization solved via backward induction algorithm that selects optimal layers for each device based on latency minimization, and (2) bandwidth allocation solved via convex optimization using binary search to equalize device latencies. These subproblems are solved iteratively through alternating optimization until convergence. The framework considers heterogeneous devices with individual split points, computation latency models based on shifted exponential distributions, and communication latency based on Shannon capacity with fading channels.

## Key Results
- The proposed SFL approach achieves over 75% latency reduction compared to traditional federated learning
- On AlexNet and VGG16, SFL maintains competitive accuracy while significantly improving communication efficiency
- The method shows particular effectiveness in scenarios with better computing or communication conditions
- Experimental results demonstrate superiority in latency reduction across varying SNR and computing conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Alternating optimization between split point selection and bandwidth allocation minimizes total system latency.
- **Mechanism:** The problem is decomposed into two subproblems: (1) split point optimization solved via backward induction, and (2) bandwidth allocation solved via convex optimization. These are solved iteratively, with each subproblem's solution serving as input to the other, until convergence.
- **Core assumption:** The optimal split point and bandwidth allocation are interdependent but can be approximated by solving each independently in alternation.
- **Evidence anchors:**
  - [abstract] "By using alternating optimization, we decompose the problem into two sub-problems and solve them optimally."
  - [section] "Therefore, P2 and P3 are solved by alternating optimization as shown in Alg. 2, in which the output of P2 (P3) is the input of P3 (P2), and the process is repeated until converge or maximum number of alternation NIter is reached."
  - [corpus] "Weak or missing - no directly comparable alternating optimization schemes found in corpus neighbors."

### Mechanism 2
- **Claim:** Backward induction optimally selects split points for heterogeneous devices.
- **Mechanism:** Starting from the deepest allowable layer, the algorithm computes expected latency for each layer and propagates backward, choosing the layer that minimizes expected total latency while respecting computational constraints.
- **Core assumption:** Latency minimization can be achieved by sequentially evaluating and selecting optimal split points layer-by-layer in reverse order.
- **Evidence anchors:**
  - [section] "we use the method of backward induction to decide the optimal splitting strategy for each layer... The splitting strategy is that the devices execute the calculation of each layer in sequence and records τ cp ℓk. If τ cp ℓk < ˆτ cp ℓk, it will be split at this layer..."
  - [abstract] "Experiment results demonstrate the superiority of our work in latency reduction..."
  - [corpus] "Weak or missing - no directly comparable backward induction approaches found in corpus neighbors."

### Mechanism 3
- **Claim:** Equalizing device latency through bandwidth allocation optimizes total system performance.
- **Mechanism:** Bandwidth is allocated such that all devices complete their tasks simultaneously, as the system's total latency is determined by the slowest device. This is achieved through a binary search to find the optimal common latency.
- **Core assumption:** System latency is bottlenecked by the slowest device, so optimal allocation equalizes all device latencies.
- **Evidence anchors:**
  - [section] "Note that the optimal solution of problem P3 can be established if and only if the total delay of all devices is equal, because the total latency of the system is limited by the worst device due to the parallel computing and communication among devices..."
  - [abstract] "We consider a practical scenario of heterogeneous devices with individual split points of DNN."
  - [corpus] "Weak or missing - no directly comparable latency equalization approaches found in corpus neighbors."

## Foundational Learning

- **Concept:** Split Federated Learning (SFL)
  - Why needed here: SFL combines the parallel training of FL with the model splitting of SL to reduce communication overhead while maintaining privacy.
  - Quick check question: What is the key difference between SFL and traditional FL in terms of data transmission?

- **Concept:** Backward Induction
  - Why needed here: This method is used to optimally select split points by evaluating latency from the deepest layer backward to the input.
  - Quick check question: Why does the algorithm start evaluating split points from the deepest layer rather than the first layer?

- **Concept:** Convex Optimization
  - Why needed here: Bandwidth allocation is formulated as a convex problem, allowing for efficient solution using binary search.
  - Quick check question: What property of the bandwidth allocation problem makes it suitable for convex optimization?

## Architecture Onboarding

- **Component map:** Devices -> Server -> Communication network -> Optimization engine
- **Critical path:**
  1. Device computes front-end submodel up to split point.
  2. Device transmits intermediate result to server.
  3. Server computes back-end submodel and aggregates.
  4. Server transmits gradients back to devices.
  5. Devices update local parameters.
- **Design tradeoffs:**
  - Split point vs. latency: Deeper splits reduce computation but increase communication.
  - Bandwidth allocation vs. fairness: Equalizing latency may starve some devices of bandwidth.
  - Accuracy vs. efficiency: Shallower splits preserve more model parameters for aggregation but increase computation.
- **Failure signatures:**
  - If split points are too shallow, devices may become computational bottlenecks.
  - If bandwidth allocation is uneven, stragglers will dominate total latency.
  - If alternating optimization doesn't converge, split points and bandwidth may oscillate.
- **First 3 experiments:**
  1. Test latency reduction on AlexNet with varying device-server distances to validate split point selection.
  2. Compare SFL vs. FL accuracy and latency on MNIST with IID and non-IID data distributions.
  3. Measure convergence behavior of alternating optimization with different numbers of devices and SNR conditions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of SFL compare to FL under realistic non-IID data distributions across heterogeneous devices?
- Basis in paper: [explicit] The paper mentions non-IID settings in experimental evaluation but doesn't analyze convergence rates systematically
- Why unresolved: The paper focuses on latency optimization but doesn't provide convergence rate analysis or comparison under varying data heterogeneity
- What evidence would resolve it: Rigorous theoretical analysis or comprehensive experiments comparing convergence speed under different levels of data heterogeneity

### Open Question 2
- Question: What is the impact of communication latency variability (due to channel fading) on the effectiveness of the alternating optimization approach?
- Basis in paper: [inferred] The communication model includes fading channels, but the optimization assumes fixed channel conditions
- Why unresolved: The paper doesn't analyze how rapidly changing channel conditions affect the stability and performance of the alternating optimization algorithm
- What evidence would resolve it: Analysis of algorithm performance under dynamic channel conditions or adaptive mechanisms for handling channel variability

### Open Question 3
- Question: How does the proposed method scale with the number of devices when the split point constraint (12b) becomes increasingly restrictive?
- Basis in paper: [explicit] The constraint (12b) shows the split point is inversely related to the number of devices (ℓk(1 − 1/K)Φ2 ≤ ˆΦ2)
- Why unresolved: The paper doesn't explore scenarios with very large numbers of devices where the split point constraint becomes severe
- What evidence would resolve it: Experimental results showing performance degradation with increasing K and potential solutions to relax the constraint

### Open Question 4
- Question: Can the optimization framework be extended to support multiple split points per device (multi-split architecture)?
- Basis in paper: [inferred] The paper focuses on single split points, while related work mentions multi-split architectures
- Why unresolved: The decomposition approach and backward induction method may not directly extend to multiple split points
- What evidence would resolve it: Modified optimization formulation and solution approach for multi-split scenarios, with performance comparison to single-split

## Limitations
- The alternating optimization framework lacks rigorous theoretical convergence guarantees
- Performance depends heavily on accurate estimation of latency distributions in dynamic wireless environments
- The claimed 75% latency reduction may not generalize to different SNR regimes or device heterogeneity patterns
- The backward induction method may become computationally expensive with very deep neural networks

## Confidence

- **High Confidence**: The latency decomposition model combining computation and communication components follows established formulations in wireless edge computing literature. The convexity of the bandwidth allocation subproblem is mathematically sound given the Shannon capacity model used.
- **Medium Confidence**: The backward induction algorithm for split point selection is logically consistent but lacks rigorous convergence analysis. The alternating optimization approach is a heuristic that works well in practice but without formal convergence guarantees.
- **Low Confidence**: The comparison results showing 75% latency reduction depend heavily on the specific parameter choices and baseline implementation details that are not fully specified in the paper.

## Next Checks

1. **Convergence Validation**: Implement the alternating optimization algorithm and measure objective value changes across iterations to verify convergence behavior under different device counts and SNR conditions.

2. **Sensitivity Analysis**: Test the method with varying computational latency distributions (changing ak values) and communication conditions to assess robustness beyond the nominal parameters.

3. **Baseline Comparison**: Implement a detailed FedAvg baseline with identical computing and communication assumptions to verify the claimed latency improvements across different network scenarios.