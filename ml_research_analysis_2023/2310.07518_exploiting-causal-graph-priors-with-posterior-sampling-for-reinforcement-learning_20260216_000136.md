---
ver: rpa2
title: Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning
arxiv_id: '2310.07518'
source_url: https://arxiv.org/abs/2310.07518
tags:
- causal
- prior
- learning
- graph
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel approach to improve the sample efficiency
  of posterior sampling in reinforcement learning by exploiting prior knowledge expressed
  as a (partial) causal graph over the environment's variables. The key idea is to
  extend posterior sampling to a hierarchical Bayesian procedure, called C-PSRL, which
  simultaneously learns the full causal graph at a higher level and the parameters
  of the resulting factored dynamics at a lower level.
---

# Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning

## Quick Facts
- arXiv ID: 2310.07518
- Source URL: https://arxiv.org/abs/2310.07518
- Reference count: 40
- Key outcome: C-PSRL improves sample efficiency of posterior sampling by exploiting causal graph priors, achieving regret bounds that scale with degree of prior knowledge

## Executive Summary
This paper introduces Causal Posterior Sampling Reinforcement Learning (C-PSRL), a novel approach that improves sample efficiency by incorporating partial causal graph knowledge into the posterior sampling framework. The method extends PSRL to a hierarchical Bayesian setting where both the causal graph structure and the factored dynamics parameters are learned simultaneously. The key innovation is that C-PSRL can leverage incomplete causal graph priors without requiring the full graph, unlike previous approaches that needed complete oracle knowledge. The authors provide theoretical regret bounds showing explicit dependence on the degree of prior knowledge and demonstrate empirically that C-PSRL significantly outperforms uninformative PSRL while approaching the performance of methods with complete prior knowledge.

## Method Summary
C-PSRL is a hierarchical Bayesian algorithm that simultaneously learns the causal graph structure and the parameters of the factored dynamics model. It operates by first restricting the hypothesis space to factored MDPs consistent with the partial causal graph prior, then performing hierarchical posterior sampling where the upper level samples causal graph structures and the lower level samples transition and reward parameters for each structure. The algorithm maintains posteriors over both levels and updates them based on observed transitions. Planning is performed exactly in the sampled factored MDP using standard methods, though this is computationally expensive. The regret analysis shows that C-PSRL achieves sublinear regret with explicit dependence on the degree of prior knowledge (η) and the sparsity of the causal graph (Z).

## Key Results
- C-PSRL achieves Bayesian regret scaling as O(H^(5/2) N^(1+Z/2) dY^(3/2) sqrt(K) + H^2 dX^η sqrt(K))
- Experiments show C-PSRL strongly improves efficiency over uninformative PSRL while approaching full prior performance
- As a byproduct, C-PSRL can extract a sparse super-graph of the true causal graph under causal minimality assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: C-PSRL reduces the size of the latent hypothesis space by exploiting the causal graph prior.
- Mechanism: The algorithm restricts the space of consistent factorizations to those that include all edges in the prior graph and are sparse, thereby avoiding the exponential blow-up in the general case.
- Core assumption: The prior graph is a subset of the true causal graph, and the true graph is sparse.
- Evidence anchors:
  - [abstract] "Specifically, we propose a hierarchical Bayesian procedure, calledCausal PSRL (C-PSRL), simultaneously learning the full causal graph at the higher level and the parameters of the resulting factored dynamics at a lower level."
  - [section] "Taking inspiration from [Hong et al., 2020, 2022b,a, Kveton et al., 2021], we design a hierarchical Bayesian procedure, calledCausal PSRL (C-PSRL), extending PSRL to the setting where the true model lies within a set of FMDPs (induced by the causal graph prior)."
- Break condition: If the prior graph is too sparse or contains incorrect edges, the restriction may exclude the true factorization, leading to poor performance.

### Mechanism 2
- Claim: C-PSRL achieves sublinear regret that depends on the degree of prior knowledge.
- Mechanism: The regret bound has a term that scales with the inverse of the degree of prior knowledge (η), so richer priors lead to lower regret.
- Core assumption: The degree of prior knowledge is bounded by the size of the minimal factorization.
- Evidence anchors:
  - [abstract] "For this procedure, we provide an analysis of its Bayesian regret, which explicitly connects the regret rate with the degree of prior knowledge."
  - [section] "Our analysis shows that C-PSRL takes the best of both worlds by avoiding a direct dependence on the number of states in the regret (as in FMDPs) and without requiring a full causal graph prior (as in hierarchical posterior sampling)."
- Break condition: If η is very small (i.e., prior knowledge is minimal), the benefit over uninformative priors diminishes.

### Mechanism 3
- Claim: C-PSRL provides a weak form of causal discovery as a byproduct.
- Mechanism: By running C-PSRL, one can extract a Z-sparse super-graph of the true causal graph with high probability, under a mild causal minimality assumption.
- Core assumption: The transition model fulfills causal minimality with respect to the true causal graph.
- Evidence anchors:
  - [abstract] "An ancillary result on causal discovery that shows how a (sparse) super-graph of the true causal graph can be extracted from a run of C-PSRL as a byproduct."
  - [section] "First, we need to assume that any misspecification inGFk negatively affects the value function of πk. Thus, we extend the traditional notion of causal minimality [Spirtes et al., 2000] to value functions."
- Break condition: If the causal minimality assumption is violated, the extracted graph may not be a super-graph of the true causal graph.

## Foundational Learning

- Concept: Factored MDPs and their factorization
  - Why needed here: C-PSRL operates on FMDPs induced by causal graphs, so understanding how factorizations work is crucial.
  - Quick check question: Given a causal graph with edges (X1,Y1) and (X2,Y2), what is the factorization of the transition model?

- Concept: Hierarchical Bayesian methods
  - Why needed here: C-PSRL is a hierarchical Bayesian algorithm that samples from a hyper-prior and then from a lower-level prior.
  - Quick check question: In C-PSRL, what are the two levels of the hierarchy, and what do they represent?

- Concept: Causal minimality assumption
  - Why needed here: The weak causal discovery result relies on this assumption, so understanding its implications is important.
  - Quick check question: How does causal minimality differ from the faithfulness assumption in causal discovery?

## Architecture Onboarding

- Component map: Input causal graph G0 -> Consistent factorizations Z -> Hyper-prior P0 -> Lower-level priors P0(·|z) -> Hierarchical posterior sampling loop -> Policy πk and estimated graph

- Critical path:
  1. Parse input causal graph G0
  2. Compute set of consistent factorizations Z
  3. Initialize hyper-prior P0 and lower-level priors P0(·|z)
  4. For each episode:
     - Sample z ~ Pk and p ~ Pk(·|z)
     - Build FMDP Fk
     - Compute optimal policy πk
     - Execute πk in true environment
     - Update posteriors Pk+1 and Pk+1(·|z)

- Design tradeoffs:
  - Sparsity vs. expressiveness: More edges in G0 lead to smaller hypothesis space but may exclude true graph
  - Degree of prior knowledge η: Higher η reduces regret but requires more accurate prior knowledge
  - Computational cost: Exact planning in FMDPs is intractable; approximation methods needed

- Failure signatures:
  - Linear regret growth: Prior knowledge is too weak or incorrect
  - Slow convergence: Hypothesis space is still too large
  - Poor causal discovery: Causal minimality assumption is violated

- First 3 experiments:
  1. Random FMDP domain with varying prior graph density
  2. Taxi domain with known factorization (compare to F-PSRL)
  3. Varying degree of prior knowledge η on a fixed problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between the degree of prior knowledge (η) and the degree of sparseness (Z) in the causal graph prior to achieve the best regret rate in C-PSRL?
- Basis in paper: The paper explicitly discusses the impact of η and Z on the regret rate, stating that a richer causal graph prior (higher η) will benefit the efficiency of PSRL.
- Why unresolved: The paper does not provide a specific analysis or experiment to determine the optimal balance between η and Z. It only states that a higher η leads to a better regret rate.
- What evidence would resolve it: An experiment comparing the performance of C-PSRL with different combinations of η and Z, or a theoretical analysis deriving the optimal balance between η and Z.

### Open Question 2
- Question: How does the choice of the hyper-prior and prior distributions in C-PSRL affect the algorithm's performance?
- Basis in paper: The paper mentions that C-PSRL requires specifying a hyper-prior and a prior for each latent factorization, but does not discuss the impact of different choices of these distributions on the algorithm's performance.
- Why unresolved: The paper does not provide any analysis or experiments comparing the performance of C-PSRL with different choices of hyper-prior and prior distributions.
- What evidence would resolve it: An experiment comparing the performance of C-PSRL with different choices of hyper-prior and prior distributions, or a theoretical analysis deriving the optimal choice of these distributions.

### Open Question 3
- Question: How does C-PSRL perform in domains with a large number of state variables and actions compared to other RL algorithms?
- Basis in paper: The paper only evaluates C-PSRL on simple domains like Random FMDP and Taxi, and does not provide any analysis or experiments on its performance in domains with a large number of state variables and actions.
- Why unresolved: The paper does not provide any theoretical or empirical evidence on how C-PSRL scales with the number of state variables and actions.
- What evidence would resolve it: An experiment comparing the performance of C-PSRL with other RL algorithms in domains with a large number of state variables and actions, or a theoretical analysis deriving the scaling properties of C-PSRL.

### Open Question 4
- Question: Can C-PSRL be extended to handle continuous state and action spaces?
- Basis in paper: The paper does not discuss the possibility of extending C-PSRL to handle continuous state and action spaces.
- Why unresolved: The paper does not provide any theoretical or empirical evidence on the feasibility of extending C-PSRL to handle continuous state and action spaces.
- What evidence would resolve it: A theoretical analysis or experiment demonstrating the extension of C-PSRL to handle continuous state and action spaces.

### Open Question 5
- Question: How does the performance of C-PSRL compare to other causal RL methods that do not rely on a causal graph prior?
- Basis in paper: The paper only compares C-PSRL to PSRL with an uninformative prior and F-PSRL with an oracle prior, but does not compare it to other causal RL methods that do not rely on a causal graph prior.
- Why unresolved: The paper does not provide any theoretical or empirical evidence on how C-PSRL compares to other causal RL methods that do not rely on a causal graph prior.
- What evidence would resolve it: An experiment comparing the performance of C-PSRL to other causal RL methods that do not rely on a causal graph prior, or a theoretical analysis deriving the advantages and disadvantages of using a causal graph prior compared to other causal RL methods.

## Limitations
- Strong assumptions on causal minimality and sparsity may not hold in real-world environments
- Computational intractability of exact planning in factored MDPs requires approximation methods
- Performance heavily depends on quality of prior causal graph knowledge

## Confidence
- High Confidence: The core mechanism of C-PSRL (hierarchical posterior sampling with causal graph priors) is theoretically sound and well-established
- Medium Confidence: The practical effectiveness of C-PSRL depends heavily on the quality of the prior causal graph
- Low Confidence: The causal discovery result is the most speculative component, relying on the novel extension of causal minimality to value functions

## Next Checks
1. **Empirical validation of causal discovery:** Implement the weak causal discovery mechanism on synthetic environments where the true causal graph is known, and measure the accuracy of the extracted graph across varying levels of prior knowledge completeness.

2. **Robustness to prior misspecification:** Systematically test C-PSRL's performance when the prior graph contains incorrect edges or is too sparse, quantifying the degradation in both regret and causal discovery accuracy.

3. **Scalability analysis:** Evaluate C-PSRL on environments with larger state spaces and more complex causal structures than the illustrative domains presented, measuring computational costs and checking whether the theoretical scaling bounds hold empirically.