---
ver: rpa2
title: Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based
  ASR
arxiv_id: '2311.04534'
source_url: https://arxiv.org/abs/2311.04534
tags:
- loss
- speech
- tokens
- discrete
- masking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the effectiveness of modeling discrete speech
  tokens in a decoder-only transformer for automatic speech recognition (ASR). The
  authors propose a novel approach called Smoothed Label Distillation (SLD), which
  combines cross-entropy loss and Kullback-Leibler (KL) divergence loss with smoothed
  labels on input speech tokens.
---

# Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR

## Quick Facts
- arXiv ID: 2311.04534
- Source URL: https://arxiv.org/abs/2311.04534
- Reference count: 0
- Primary result: Smoothed Label Distillation (SLD) outperforms Loss Masking for discrete-token ASR, achieving significant WER improvements on LibriSpeech

## Executive Summary
This paper challenges the common practice of using Loss Masking in decoder-only transformer models for discrete-token-based automatic speech recognition (ASR). The authors propose a novel approach called Smoothed Label Distillation (SLD) that combines cross-entropy loss with KL divergence loss using smoothed labels on speech tokens. Experiments on the LibriSpeech dataset demonstrate that SLD consistently outperforms Loss Masking across different speech discretization methods, achieving significant improvements in word error rates on both development and test sets.

## Method Summary
The paper investigates three loss strategies for training decoder-only transformers on discrete speech tokens: Loss Masking (cross-entropy only on text), Multimodal Cross-Entropy Loss (cross-entropy on both speech and text), and the proposed Smoothed Label Distillation (SLD) that combines cross-entropy with KL divergence using smoothed labels. The SLD approach is motivated by the observation that conventional cross-entropy loss on speech tokens doesn't consistently improve ASR performance, possibly due to noise introduced during discretization. The authors implement this on a GPT2-medium backbone with expanded vocabulary to handle both speech and text tokens, using WavLM-large and HuBERT-large for speech discretization followed by Sentencepiece tokenization.

## Key Results
- SLD achieves the best ASR performance, surpassing both Loss Masking and Multimodal CE Loss on LibriSpeech
- SLD effectively mitigates limitations of conventional cross-entropy loss on speech tokens
- The approach consistently outperforms Loss Masking across different speech discretization methods (WavLM-large and HuBERT-large)

## Why This Works (Mechanism)

### Mechanism 1
The KL divergence loss in SLD encourages the model to match the output distribution to a smoothed version of true labels, making it more robust to discretization noise compared to hard label matching. This addresses the core assumption that discretization of continuous speech into discrete tokens introduces noise that negatively impacts ASR performance when using standard cross-entropy loss.

### Mechanism 2
SLD provides a more informative training signal than hard labels alone by using smoothed labels in the KL divergence component. This richer target distribution captures uncertainty and relationships between tokens, allowing the model to learn more nuanced patterns in speech token sequences rather than treating them as independent entities.

### Mechanism 3
The combination of cross-entropy loss and KL divergence loss provides complementary training signals - label matching for text tokens through cross-entropy and logit matching for speech tokens through KL divergence with smoothed labels. This modality-specific approach addresses the different characteristics of speech and text tokens rather than applying the same training strategy to both.

## Foundational Learning

- **Discrete speech tokenization and its impact on ASR performance**: Understanding how continuous speech is converted to discrete tokens and how this affects model training is crucial for grasping the motivation behind SLD. *Quick check: What are the main challenges of using discrete speech tokens compared to continuous speech features in ASR systems?*

- **Knowledge distillation and label smoothing techniques**: SLD is inspired by knowledge distillation, and understanding these concepts is essential for comprehending how SLD works. *Quick check: How does label smoothing help in making the model more robust to noise in the training data?*

- **KL divergence and its role in training neural networks**: KL divergence is a key component of SLD, and understanding its properties is crucial for implementing and debugging the method. *Quick check: What is the difference between using KL divergence loss and cross-entropy loss in training neural networks?*

## Architecture Onboarding

- **Component map**: GPT2-medium backbone -> Expanded token embedding matrix (speech subwords + text) -> Modified softmax layer -> WavLM-large/HuBERT-large for discretization -> Sentencepiece tokenization

- **Critical path**: 1) Speech signal preprocessing and discretization, 2) Tokenization of both speech and text inputs, 3) Model forward pass with combined tokens, 4) Calculation of LCE_text, LCE_speech, and LKL_speech, 5) Backpropagation and parameter updates

- **Design tradeoffs**: Using decoder-only architecture vs. encoder-decoder for ASR; Expanding vocabulary vs. using separate models for speech and text; Implementing SLD vs. using simpler approaches like Loss Masking

- **Failure signatures**: Inconsistent performance across different speech discretization methods; Degraded performance when using speech tokens compared to Loss Masking; Sensitivity to temperature and weight parameters in SLD

- **First 3 experiments**: 1) Compare SLD performance against Loss Masking and Multimodal CE Loss on small LibriSpeech subset using WavLM-large discretization, 2) Analyze impact of different temperature values (T) and weight (Î±) on SLD performance, 3) Evaluate SLD on different speech discretization method (HuBERT-large) to verify consistency

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of different discretization methods vary when used as continuous features versus discrete tokens for ASR? The paper notes that WavLM-large and HuBERT-large achieve comparable or better performance as continuous features compared to HuBERT-large, but HuBERT-large discrete tokens perform better than WavLM-large discrete tokens for GPT2 models. This discrepancy is not fully explained.

### Open Question 2
Can the proposed SLD approach be effectively applied to other speech understanding and generation tasks using unified speech-text models? The authors state they believe SLD is applicable to other speech understanding and generation tasks using unified speech-text models and plan to investigate it in future work.

### Open Question 3
How does the combination of CE loss and KL divergence loss in SLD contribute to improved performance and generalization compared to using either loss alone? The authors mention that this combination allows for label matching on text and logit matching on speech, but the specific mechanisms are not fully explained.

## Limitations

- Experimental scope is limited to a single dataset (LibriSpeech), which may not capture performance across diverse acoustic conditions, speaker populations, or languages
- The paper lacks detailed ablation studies to isolate the specific contribution of each component in SLD
- Implementation details for critical components like k-means clustering and exact temperature parameter settings are unclear or missing

## Confidence

**High Confidence**: The claim that SLD outperforms Loss Masking on LibriSpeech with the specific experimental setup described, with clearly presented results and sound methodology within stated constraints.

**Medium Confidence**: The broader claim that SLD is a superior approach for discrete-token ASR in general, based on limited experiments with two speech discretization methods and one dataset.

**Low Confidence**: The specific mechanisms proposed for why SLD works (noise mitigation, dependency modeling, modality-specific training) as the paper presents these as hypotheses without strong empirical validation.

## Next Checks

1. **Cross-dataset validation**: Evaluate SLD performance on different speech corpora (Common Voice, TED-LIUM, or multilingual datasets) to assess generalizability beyond LibriSpeech and test robustness to different acoustic conditions.

2. **Ablation study of SLD components**: Systematically disable or modify components of SLD (remove KL divergence, change smoothing distribution, vary temperature) to quantify the contribution of each element and validate claimed mechanisms.

3. **Comparative analysis with modern baselines**: Benchmark SLD against recent strong approaches in discrete token ASR, including Conformer-based models, CTC systems, and other decoder-only architectures with different loss strategies to establish relative performance in current state of the field.