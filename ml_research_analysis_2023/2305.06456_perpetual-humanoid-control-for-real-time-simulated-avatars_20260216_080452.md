---
ver: rpa2
title: Perpetual Humanoid Control for Real-time Simulated Avatars
arxiv_id: '2305.06456'
source_url: https://arxiv.org/abs/2305.06456
tags:
- motion
- sequences
- humanoid
- imitation
- pose
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Perpetual Humanoid Controller (PHC),
  a physics-based motion imitation system that can handle noisy input and unexpected
  falls without requiring resets. The key innovation is the Progressive Multiplicative
  Control Policy (PMCP), which dynamically allocates new network capacity to learn
  increasingly difficult motion sequences while avoiding catastrophic forgetting.
---

# Perpetual Humanoid Control for Real-time Simulated Avatars

## Quick Facts
- arXiv ID: 2305.06456
- Source URL: https://arxiv.org/abs/2305.06456
- Reference count: 40
- This paper introduces PHC, achieving 98% success rate on AMASS motion imitation without external stabilizing forces and 90% success with noisy video input.

## Executive Summary
This paper presents the Perpetual Humanoid Controller (PHC), a physics-based motion imitation system capable of handling noisy input and unexpected falls without requiring simulation resets. The key innovation is the Progressive Multiplicative Control Policy (PMCP), which dynamically allocates new network capacity to learn increasingly difficult motion sequences while avoiding catastrophic forgetting. This enables scaling to learn ten thousand motion clips from the AMASS dataset while maintaining fail-state recovery capabilities, achieving state-of-the-art performance in both motion imitation and robustness.

## Method Summary
PHC uses PMCP, a multi-primitive control policy that trains subnetworks (primitives) progressively on increasingly difficult motion sequences. When learning harder sequences, new primitives are created while previous ones are frozen, allowing the system to retain knowledge of easier motions. A composer combines all primitives during inference using multiplicative control. For fail-state recovery, a dedicated primitive P(F) is trained on locomotion data to handle falls, replacing reference motion with simulated values for non-root joints. The system incorporates Adversarial Motion Prior (AMP) to ensure natural motion through discriminator-based rewards, and uses PPO with relaxed early termination for stable training.

## Key Results
- Achieves 98% success rate on AMASS motion imitation without external stabilizing forces
- Demonstrates 90% success rate on noisy poses estimated from video
- Successfully controls simulated avatars in real-time using webcam input
- Handles unexpected falls through dedicated fail-state recovery primitive

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PMCP avoids catastrophic forgetting by dynamically allocating new network capacity to learn harder motion sequences while retaining ability to imitate easier ones.
- Mechanism: When learning new, harder motion sequences, PMCP creates new primitives (subnetworks) that are responsible for these sequences. Previous primitives are frozen and remain capable of imitating their learned sequences. A composer combines all primitives during inference.
- Core assumption: Motion imitation tasks are highly correlated, so learning harder sequences can draw from experiences in previous primitives.
- Evidence anchors: [abstract]: "Progressive Multiplicative Control Policy (PMCP), which dynamically allocates new network capacity to learn harder and harder motion sequences." [section]: "By treating harder and harder motion sequences as a different 'task' and gradually allocating new network capacity to learn, PMCP retains its ability to imitate easier motion clips when learning harder ones."

### Mechanism 2
- Claim: PMCP enables learning fail-state recovery without compromising motion imitation capabilities.
- Mechanism: A dedicated primitive P(F) is trained specifically for fail-state recovery using locomotion data. This primitive replaces the reference motion with simulated values for non-root joints, turning the problem into a point-goal navigation task. The composer combines this with other primitives during inference.
- Core assumption: Fail-state recovery can be learned as a separate task from motion imitation, and the primitive can switch between tasks based on distance to reference motion.
- Evidence anchors: [abstract]: "Our controller scales up to learning ten thousand motion clips... and learns to naturally recover from fail-state." [section]: "For this new task, we initialize a primitive P(F) at the end of the primitive stack... Since the reference motion does not provide useful information about fail-state recovery... we modify the state space during fail-state recovery to remove all information about the reference motion except the root."

### Mechanism 3
- Claim: Adversarial Motion Prior (AMP) provides natural and human-like behavior during both normal motion imitation and fail-state recovery.
- Mechanism: AMP uses a discriminator that takes recent proprioceptive observations as input and provides a reward signal encouraging natural motion. This reward is combined with task-specific rewards (motion imitation or fail-state recovery) and an energy penalty.
- Core assumption: Natural human motion has learnable statistical patterns that can be captured by a discriminator trained on motion data.
- Evidence anchors: [abstract]: "We adopt Adversarial Motion Prior (AMP)[34] throughout our pipeline and ensure natural and human-like behavior during fail-state recovery." [section]: "Unlike prior motion tracking policies that only use a motion imitation reward, we use the recently proposed Adversarial Motion Prior [34] and include a discriminator reward term throughout our framework."

## Foundational Learning

- Concept: Reinforcement Learning with Proximal Policy Optimization (PPO)
  - Why needed here: The physics simulation is not differentiable, requiring RL to learn the control policy. PPO provides stable learning by limiting policy updates.
  - Quick check question: Why is PPO preferred over other RL algorithms for this application?

- Concept: Catastrophic Forgetting in Neural Networks
  - Why needed here: Learning from large motion datasets with diverse patterns can cause the policy to forget previously learned sequences when learning new ones.
  - Quick check question: What are the two main approaches to combat catastrophic forgetting in neural networks?

- Concept: Mixture of Experts and Multiplicative Control Policies
  - Why needed here: Combining multiple pretrained primitives allows the controller to handle diverse motion sequences by activating appropriate experts based on the current state.
  - Quick check question: How does multiplicative control differ from top-1 mixture of experts in terms of expert activation?

## Architecture Onboarding

- Component map: State (Proprioception + Goal) → Primitives (K for motion imitation + 1 for fail-state recovery) → Composer (learned weights) → Policy (Gaussian distribution) → Actions (PD controller targets) → Simulation → New State
- Critical path: State → Primitives → Composer → Policy → Actions → Simulation → New State
- Design tradeoffs: Motion imitation vs. fail-state recovery: Separate primitives allow dedicated learning but increase complexity; Rotation-based vs. keypoint-based input: Keypoints are easier to estimate but provide less information; Energy penalty: Prevents high-frequency jitter but may limit dynamic motions
- Failure signatures: Policy fails on previously learned sequences: Catastrophic forgetting, needs more primitives or better training procedure; Humanoid falls frequently: Insufficient stability, may need better reward shaping or more training data; Unnatural motion: AMP discriminator not working properly, check discriminator training
- First 3 experiments: 1) Train a single primitive on a small subset of AMASS data to verify basic motion imitation works; 2) Test the single primitive on held-out motion sequences to measure generalization; 3) Add a second primitive for harder sequences and evaluate if performance improves on both easy and hard sequences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PHC scale with the number of primitives when learning extremely complex motion sequences (e.g., gymnastics or martial arts routines)?
- Basis in paper: [explicit] The paper discusses scaling to ten thousand motion clips and mentions that highly dynamic motions like backflips and cartwheels are challenging, but does not provide quantitative results for scaling beyond four primitives.
- Why unresolved: The paper only tests up to four primitives and does not explore the upper limits of the progressive training framework or how performance degrades when learning increasingly complex sequences.
- What evidence would resolve it: Empirical results showing success rates and imitation quality when using 5+ primitives to learn highly complex motion datasets, along with analysis of the diminishing returns or bottlenecks in the progressive training process.

### Open Question 2
- Question: Can PHC be adapted to learn from motion sequences that require long-term planning or intent, such as sports plays or choreographed routines?
- Basis in paper: [inferred] The paper notes that highly dynamic motions require specific setups (e.g., running up for a high jump) and that a per-frame policy without sequence-level information struggles with these tasks, suggesting a limitation in handling motions requiring planning.
- Why unresolved: The current PHC framework does not incorporate sequence-level information or intent, and the paper does not explore modifications to address this limitation.
- What evidence would resolve it: Experiments demonstrating improved performance on complex sequences by incorporating sequence-level context, trajectory prediction, or intent modeling into the PHC framework.

### Open Question 3
- Question: How does the integration of PHC with downstream tasks like pose estimation or motion generation affect the overall system performance and realism?
- Basis in paper: [explicit] The paper mentions that tighter integration with pose estimation and motion generation is needed for better downstream tasks, but does not provide experimental results or analysis of such integration.
- Why unresolved: The paper treats PHC as a task-agnostic controller and only uses off-the-shelf pose estimators and motion generators, without exploring how end-to-end training or joint optimization could improve results.
- What evidence would resolve it: Comparative studies showing the performance differences between using PHC as a standalone controller versus integrating it with learned pose estimators or motion generators, including metrics for realism and task success.

### Open Question 4
- Question: What are the limitations of using 3D keypoints versus joint rotations for motion imitation, and under what conditions does one approach outperform the other?
- Basis in paper: [explicit] The paper shows that the keypoint-based controller performs comparably or better than the rotation-based controller on noisy input, but does not provide a detailed analysis of the trade-offs or failure modes of each approach.
- Why unresolved: The paper does not explore scenarios where joint rotations might be necessary (e.g., fine-grained control or specific joint articulations) or the limitations of keypoint-only methods in capturing complex poses.
- What evidence would resolve it: Systematic experiments comparing keypoint-based and rotation-based controllers across diverse motion types, including analysis of failure cases and conditions where one method clearly outperforms the other.

## Limitations
- Dataset curation limits real-world applicability (excludes chairs, treadmills, ground contacts)
- PMCP assumes motion tasks are sufficiently correlated for effective knowledge transfer
- Real-time performance lacks quantitative baseline comparison under identical noisy conditions

## Confidence
- Perpetual motion imitation without resets: Medium
- PMCP's ability to avoid catastrophic forgetting: Medium
- Real-time performance with noisy video input: Medium
- Natural motion through AMP: High for normal motion, Medium for recovery scenarios

## Next Checks
1. **Ablation study on PMCP composition**: Test whether multiplicative control outperforms simple averaging or gating-only approaches, and measure performance degradation when primitives are trained independently without shared architecture.

2. **Cross-dataset generalization test**: Evaluate the trained controller on motion capture datasets not seen during training (e.g., CMU Motion Capture Database) to assess true generalization beyond AMASS.

3. **Noise robustness quantification**: Systematically vary noise levels in input poses (keypoint position error, joint angle noise) and measure success rate degradation curves to establish performance bounds under different noise conditions.