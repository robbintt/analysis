---
ver: rpa2
title: 'NP$^2$L: Negative Pseudo Partial Labels Extraction for Graph Neural Networks'
arxiv_id: '2310.01098'
source_url: https://arxiv.org/abs/2310.01098
tags:
- labels
- partial
- graph
- pseudo
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve the accuracy of pseudo
  labels for graph neural networks (GNNs) by extracting negative pseudo partial labels.
  The key idea is to construct a signed graph from the original graph, where negative
  edges are added between nodes that do not share any partial labels.
---

# NP$^2$L: Negative Pseudo Partial Labels Extraction for Graph Neural Networks

## Quick Facts
- arXiv ID: 2310.01098
- Source URL: https://arxiv.org/abs/2310.01098
- Reference count: 36
- Primary result: NP$^2$L achieves state-of-the-art performance on link prediction and node classification, with up to 17.60% improvement on node classification tasks.

## Executive Summary
This paper proposes NP$^2$L, a method to improve pseudo label accuracy for graph neural networks by extracting negative pseudo partial labels. The approach constructs a signed graph from the original graph by adding negative edges between nodes that don't share partial labels, then trains GNNs on this signed graph. The method significantly outperforms baseline models on multiple benchmark datasets for both link prediction and node classification tasks.

## Method Summary
NP$^2$L extracts negative pseudo partial labels through three main steps: first, node embeddings are learned via graph reconstruction using GAE/VGAE with GCN encoder; second, partial labels are obtained by clustering these embeddings using K-means; third, negative edges are constructed between nodes with non-overlapping partial labels to form a signed graph. This signed graph is then used with modified GNN architectures (SGCN, GNCN variants) to improve learning through both positive and negative relations.

## Key Results
- Achieves state-of-the-art performance on link prediction and node classification tasks
- Improves GCN and GNCN models by up to 17.60% on node classification tasks
- Demonstrates effectiveness across multiple benchmark datasets including Amazon, Coauthor, Cora, CiteSeer, and others
- Shows that negative pseudo partial labels bridge training and testing sets, benefiting transductive learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Negative pseudo partial labels extracted by NP²E module are more accurate than regular pseudo labels.
- Mechanism: By selecting node pairs that don't share overlapping partial labels, the method creates negative edges that bridge different classes with high probability. This is because when partial labels are accurate (high recall), disparate partial labels strongly indicate different ground truth classes.
- Core assumption: Ground truth class labels fall into the top-o partial labels with high probability when o is sufficiently large (between k/3 and k/2).
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If clustering produces partial labels with low recall, negative edges may connect nodes of the same class, reducing effectiveness.

### Mechanism 2
- Claim: Signed graphs improve GNN performance by providing explicit negative relations for message passing.
- Mechanism: Traditional GNNs only use positive edges for message passing. By converting the graph to a signed graph with negative edges, the model can learn from both positive and negative relations, improving node embeddings and generalization.
- Core assumption: GNNs can effectively utilize negative edges through modified architectures (like SGCN or two-stream GNCN).
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If the GNN architecture cannot properly handle negative edges, performance may degrade or show no improvement.

### Mechanism 3
- Claim: NP²E module bridges training and testing sets, improving generalization in transductive learning.
- Mechanism: Negative edges connect nodes across different splits (training and test sets), allowing information to flow between them during training. This provides supervision for test nodes without direct access.
- Core assumption: Negative edges connecting training and test nodes are beneficial for generalization, not just noise.
- Evidence anchors: [section], [corpus]
- Break condition: If negative edges connecting training and test nodes introduce too much noise, they may hurt generalization.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: The paper builds on GNN architectures and their limitations with only positive edges.
  - Quick check question: How does a standard GCN aggregate information from neighbors during message passing?

- Concept: Pseudo labeling and semi-supervised learning
  - Why needed here: The method uses pseudo labels to create partial labels, which are then used to extract negative pseudo partial labels.
  - Quick check question: What is the difference between hard and soft pseudo labels in semi-supervised learning?

- Concept: Signed graphs and their applications
  - Why needed here: The core contribution is converting an unsigned graph to a signed graph for improved learning.
  - Quick check question: How do signed graphs differ from unsigned graphs in terms of edge representation and interpretation?

## Architecture Onboarding

- Component map: Node embedding learning (GAE/VGAE/GCN encoder) -> Clustering (K-means) -> Partial labels -> Negative pseudo partial labels extraction -> Signed graph construction -> Signed GNN training (SGCN, GNCN variants)

- Critical path: 1) Learn node embeddings from original graph, 2) Cluster embeddings and extract partial labels, 3) Construct signed graph with negative edges, 4) Train signed GNN on the new graph

- Design tradeoffs: Number of clusters (k) vs. partial label accuracy, Number of partial labels per node (o) vs. computational cost, GNN architecture choice (SGCN vs. two-stream vs. others) vs. performance, Clustering method choice vs. partial label quality

- Failure signatures: Low recall of partial labels (ground truth not in top-o clusters), Too many negative edges connecting same-class nodes, GNN unable to handle negative edges properly, Over-smoothing or over-squashing in signed GNN

- First 3 experiments: 1) Verify partial label recall increases with o on a small dataset, 2) Test signed graph construction with different o values, 3) Compare baseline GNN vs. signed GNN on link prediction task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of clustering method affect the quality of negative pseudo partial labels and the overall performance of the NP²L method?
- Basis in paper: [inferred] The paper mentions that different clustering methods like spectral clustering and hierarchical clustering can be applied to generate consensus pseudo partial labels, but does not provide a detailed comparison or analysis of their effects.
- Why unresolved: The paper only briefly mentions the possibility of using different clustering methods without exploring their impact on the method's performance.
- What evidence would resolve it: Conducting experiments using different clustering methods and comparing their performance in terms of the quality of negative pseudo partial labels and the overall performance of the NP²L method on various benchmark datasets.

### Open Question 2
- Question: How does the NP²L method perform on dynamic graphs where the structure and node features change over time?
- Basis in paper: [inferred] The paper focuses on static graphs and does not discuss the applicability or performance of the NP²L method on dynamic graphs.
- Why unresolved: The method's ability to handle dynamic graphs is not explored, and it is unclear how well it would adapt to changes in the graph structure and node features over time.
- What evidence would resolve it: Extending the NP²L method to handle dynamic graphs and evaluating its performance on benchmark datasets with temporal components, such as citation networks or social networks with time-stamped interactions.

### Open Question 3
- Question: Can the NP²L method be effectively applied to graphs with a large number of classes, and how does its performance scale with the number of classes?
- Basis in paper: [inferred] The paper demonstrates the method's effectiveness on datasets with a relatively small number of classes (up to 10 classes), but does not discuss its performance on graphs with a larger number of classes.
- Why unresolved: The scalability of the method to graphs with a large number of classes is not investigated, and it is unclear how the quality of negative pseudo partial labels and the overall performance would be affected.
- What evidence would resolve it: Evaluating the NP²L method on benchmark datasets with a larger number of classes and analyzing its performance in terms of accuracy, scalability, and the quality of negative pseudo partial labels as the number of classes increases.

## Limitations

- The core assumption about partial label recall thresholds (o between k/3 and k/2) lacks systematic experimental validation across datasets
- The effectiveness of signed graph message passing depends heavily on specific GNN architecture modifications whose implementation details are not fully specified
- The method's performance on dynamic graphs and graphs with large numbers of classes remains unexplored

## Confidence

- Mechanism 1 (partial label accuracy): Low confidence - relies on single experimental observation across datasets
- Mechanism 2 (signed graph message passing): Medium confidence - theoretically sound but requires specific GNN architectures
- Mechanism 3 (cross-split generalization): Low confidence - no ablation studies isolating this effect

## Next Checks

1. Systematically vary o (partial label count) and k (cluster count) on a single dataset to verify the claimed recall threshold and identify optimal values
2. Implement a controlled experiment comparing signed vs unsigned GNNs with identical architectures to isolate the effect of negative edges
3. Create synthetic graphs where ground truth classes are known to test whether negative pseudo partial labels actually connect different classes as claimed