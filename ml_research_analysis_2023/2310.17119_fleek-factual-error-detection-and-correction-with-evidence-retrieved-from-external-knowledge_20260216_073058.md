---
ver: rpa2
title: 'FLEEK: Factual Error Detection and Correction with Evidence Retrieved from
  External Knowledge'
arxiv_id: '2310.17119'
source_url: https://arxiv.org/abs/2310.17119
tags:
- fact
- fleek
- evidence
- triple
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FLEEK is a prototype tool that automatically extracts factual claims
  from text, gathers evidence from external knowledge sources, evaluates the factuality
  of each claim, and suggests revisions for identified errors using the collected
  evidence. It breaks down sentences into fine-grained verifiable facts represented
  as triples and uses LLMs to generate questions for evidence retrieval from knowledge
  graphs and the web.
---

# FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge

## Quick Facts
- arXiv ID: 2310.17119
- Source URL: https://arxiv.org/abs/2310.17119
- Authors: 
- Reference count: 4
- Key outcome: FLEEK achieves 77-85% F1 score on fact error detection and 72.7% accuracy on fact correction

## Executive Summary
FLEEK is a prototype tool that automatically extracts factual claims from text, gathers evidence from external knowledge sources, evaluates the factuality of each claim, and suggests revisions for identified errors. The system breaks down sentences into fine-grained verifiable facts represented as triples and uses LLMs to generate questions for evidence retrieval from knowledge graphs and the web. FLEEK achieves strong performance on fact error detection (77-85% F1) and fact correction (72.7% accuracy), with the GPT-3 based version outperforming the Vicuna version by 12 F1 points.

## Method Summary
FLEEK operates through a pipeline of components: fact extraction converts sentences to flat and extended triples, question generation creates type-aware and context-driven questions for each triple, evidence retrieval queries knowledge graphs and web sources, verification classifies evidence as supporting/non-supporting using LLMs, and fact revision proposes corrections based on verified evidence. The system is designed to be model-agnostic, with evaluation showing strong performance using both Vicuna and GPT-3 LLMs.

## Key Results
- FLEEK achieves 77-85% F1 score for fact error detection across two evaluation datasets
- The fact correction component achieves 72.7% accuracy, rising to 87.5% when given correct verification results
- FLEEK GPT-3 outperforms FLEEK Vicuna by 12 F1 points on fact detection tasks

## Why This Works (Mechanism)

### Mechanism 1
Breaking sentences into fine-grained verifiable facts as triples enables more accurate error detection than classifying entire claims. Decomposing complex sentences into individual subject-predicate-object triples allows the system to verify each atomic fact independently against external evidence sources. Core assumption: Most factual errors can be isolated to individual components within a sentence rather than requiring holistic evaluation. Break condition: When a sentence contains interdependent facts where the truth value of one fact depends on another, breaking them into triples may lose necessary context for accurate verification.

### Mechanism 2
Using type-aware and context-driven question generation improves the precision of evidence retrieval for fact verification. By first determining the expected type of the answer (e.g., year, city) and generating questions that target specific answer formats, the system retrieves more relevant evidence. Context-driven questions also ensure extended triples retrieve evidence matching the specific situation described. Core assumption: The type and context of a fact directly influence the format and relevance of evidence that should be retrieved. Break condition: When the expected answer type is ambiguous or when context information is insufficient to generate meaningful questions, leading to irrelevant evidence retrieval.

### Mechanism 3
Attribution of evidence to individual facts enables interpretable and actionable verification results. By associating each fact with the specific evidence that supports or refutes it, the system provides transparent reasoning that users can examine and understand, rather than just binary correctness labels. Core assumption: Users need to understand not just whether a claim is correct, but why it is correct or incorrect based on specific evidence sources. Break condition: When evidence sources are inconsistent or when multiple pieces of evidence conflict, making it difficult to attribute a single definitive reason for the verification outcome.

## Foundational Learning

- Concept: Information extraction and triple representation
  - Why needed here: The system relies on converting natural language sentences into structured triples for verification, requiring understanding of subject-predicate-object relationships.
  - Quick check question: Given the sentence "Albert Einstein was born in Germany in 1879", what would be the flat and extended triple representations?

- Concept: Question generation techniques
  - Why needed here: The system generates specific questions for each fact to retrieve targeted evidence, requiring knowledge of how to formulate effective queries.
  - Quick check question: How would you generate a type-aware question for the triple (Taylor Swift; birthdate; 1989) versus a context-driven question for (Taylor Swift; moved; move_ID; place; Nashville)?

- Concept: Evidence retrieval from multiple sources
  - Why needed here: The system queries both knowledge graphs and web search, requiring understanding of different retrieval strategies and evidence evaluation.
  - Quick check question: What are the key differences in how you would evaluate evidence from a knowledge graph versus web search results?

## Architecture Onboarding

- Component map: Fact Extraction -> Question Generation -> Evidence Retrieval -> Verification -> Fact Revision
- Critical path: Fact Extraction → Question Generation → Evidence Retrieval → Verification → Fact Revision
- Design tradeoffs:
  - Breaking sentences into triples improves granularity but may lose contextual dependencies
  - Using multiple LLMs (Vicuna and GPT-3) provides options but increases complexity
  - Supporting both KG and web evidence increases coverage but requires handling different evidence formats
- Failure signatures:
  - High error rate in Fact Extraction: Likely issues with triple format or missing triples
  - Low precision in Evidence Retrieval: Problems with question generation or evidence evaluation
  - Poor Fact Revision accuracy: Errors propagating from earlier components or inadequate revision prompting
- First 3 experiments:
  1. Test Fact Extraction component independently with sentences containing both flat and extended triples to verify correct triple formats are produced
  2. Evaluate Question Generation by checking if generated questions produce relevant evidence when used with actual KG and web search
  3. Test end-to-end verification on a small dataset with known facts to verify the complete pipeline produces correct fact-level decisions

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of FLEEK compare when using different LLMs beyond GPT-3 and Vicuna, such as GPT-4 or open-source alternatives like LLaMA? Basis in paper: [explicit] The paper mentions FLEEK's model-agnostic design and evaluates two LLM instances (FLEEK Vicuna and FLEEK GPT-3), but notes this as future work to test with various LLMs. Why unresolved: The current evaluation is limited to only two LLMs, leaving performance comparisons with other models unexplored. What evidence would resolve it: Empirical results showing F1 scores, precision, and recall of FLEEK using different LLMs on the same evaluation datasets.

### Open Question 2
What is the impact of increasing the size and diversity of the evaluation datasets on FLEEK's performance? Basis in paper: [explicit] The paper acknowledges the current datasets are small (50 sentences each) and from the same source (Wikipedia), and plans to create larger, more diverse benchmarks. Why unresolved: The limited size and homogeneity of the datasets may not reflect real-world performance across varied text sources and error types. What evidence would resolve it: Performance metrics (F1, precision, recall) on larger datasets with diverse sources, error types, and difficulty levels.

### Open Question 3
How effective are multi-round prompting and majority voting strategies in improving the accuracy of individual components (e.g., Fact Extraction, Question Generation) in FLEEK? Basis in paper: [explicit] The paper mentions this as a limitation, noting that the system depends on initial LLM responses and suggesting multi-round prompting with majority voting as a potential improvement. Why unresolved: The current implementation does not explore these strategies, leaving their effectiveness unknown. What evidence would resolve it: Comparative results showing accuracy improvements in fact extraction, question generation, and verification when using multi-round prompting and majority voting versus single-shot prompting.

## Limitations
- Evaluation conducted on relatively small datasets (50 sentences each) from a single source (Wikipedia)
- Performance highly dependent on the quality of evidence retrieved from external sources
- Significant performance variation between different LLM providers (12 F1 point difference between GPT-3 and Vicuna)

## Confidence

- Fact extraction and triple decomposition: Medium
- Question generation effectiveness: Medium  
- Evidence attribution and interpretability: High
- Overall system performance metrics: Medium

## Next Checks

1. Conduct ablation studies comparing the triple decomposition approach against end-to-end fact verification methods to quantify the claimed accuracy improvements
2. Test the system across multiple domains (scientific, news, technical writing) to evaluate domain generalizability and identify performance variations
3. Perform a user study to assess whether the evidence attribution feature actually improves user understanding and trust compared to systems that only provide correctness labels