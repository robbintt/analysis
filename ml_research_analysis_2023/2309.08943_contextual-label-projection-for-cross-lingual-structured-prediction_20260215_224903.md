---
ver: rpa2
title: Contextual Label Projection for Cross-Lingual Structured Prediction
arxiv_id: '2309.08943'
source_url: https://arxiv.org/abs/2309.08943
tags:
- translation
- translated
- label
- linguistics
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of label projection for cross-lingual
  structured prediction, which is essential for leveraging machine translation to
  facilitate cross-lingual transfer in tasks like event argument extraction and named
  entity recognition. Prior label projection methods compromise translation accuracy
  by either facilitating easy identification of translated labels from translated
  text or using word-level alignments.
---

# Contextual Label Projection for Cross-Lingual Structured Prediction

## Quick Facts
- arXiv ID: 2309.08943
- Source URL: https://arxiv.org/abs/2309.08943
- Authors: 
- Reference count: 32
- Key outcome: CLAP achieves over 2.4 F1 improvement for EAE and 1.4 F1 improvement for NER in zero-shot cross-lingual transfer across 39 languages.

## Executive Summary
This paper introduces CLAP (Contextual Label Projection), a novel approach for label projection in cross-lingual structured prediction tasks like event argument extraction and named entity recognition. CLAP addresses the limitations of existing projection methods by translating text to the target language and then performing contextual translation of labels using the translated text as context. The method leverages instruction-tuned multilingual language models to ensure accurate and contextually consistent label translations while enforcing faithfulness constraints. Extensive experiments across 39 languages demonstrate significant improvements over baseline projection techniques, with over 2.4 F1 improvement for EAE and 1.4 F1 improvement for NER.

## Method Summary
CLAP performs label projection by first translating the input sentence using a standard translation model T, then using the resulting translated sentence as context to translate each label via a multilingual instruction-tuned language model M. The instruction prompt explicitly requires the translated label to appear in the translated sentence. The approach separates sentence translation and label translation into two specialized models, improving robustness and quality compared to marker-based methods. CLAP is benchmarked on zero-shot cross-lingual transfer for event argument extraction and named entity recognition across 39 languages.

## Key Results
- CLAP achieves over 2.4 F1 improvement for event argument extraction (EAE) compared to baseline projection methods
- CLAP achieves over 1.4 F1 improvement for named entity recognition (NER) compared to baseline projection methods
- Across 39 languages, CLAP consistently outperforms other translate-train techniques by 2-2.5 F1 points on average

## Why This Works (Mechanism)

### Mechanism 1
Using translated input text as context for label translation ensures label accuracy and presence. CLAP first translates the input sentence using a standard translation model T, then uses the resulting translated sentence as context to translate each label via a multilingual instruction-tuned language model M. The instruction prompt explicitly requires the translated label to appear in the translated sentence. Core assumption: The translation model T produces high-quality translations, and the language model M can reliably incorporate the context to produce accurate and contextually consistent label translations.

### Mechanism 2
Instruction-tuned language models can understand and execute label projection instructions. The language model M is prompted with the translated sentence and a natural language instruction that both requests the label translation and enforces the constraint that the translated label must appear in the translated sentence. In-context examples are provided to improve instruction understanding. Core assumption: The instruction-tuned model M has sufficient multilingual capability and instruction-following ability to perform the task.

### Mechanism 3
Separating sentence translation and label translation improves robustness and quality. CLAP uses two separate models: T for sentence translation and M for label translation. This separation allows each model to specialize, improving overall translation quality compared to marker-based methods that combine the tasks. Core assumption: The independence of T and M for translating x and y respectively assures that CLAP has better translation quality on xtgt and is more robust than the marker-based baselines.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: CLAP is designed to improve zero-shot cross-lingual transfer for structured prediction tasks by generating high-quality pseudo-training data in target languages.
  - Quick check question: What is the difference between zero-shot cross-lingual transfer and few-shot cross-lingual transfer?

- Concept: Label projection
  - Why needed here: Label projection is the core task that CLAP addresses, involving the translation of labels while ensuring they appear in the translated text.
  - Quick check question: What are the two main challenges in label projection, and how does CLAP address them?

- Concept: Contextual machine translation
  - Why needed here: CLAP leverages contextual machine translation to improve the accuracy and faithfulness of label translations by using the translated sentence as context.
  - Quick check question: How does contextual machine translation differ from standard machine translation, and why is it beneficial for label projection?

## Architecture Onboarding

- Component map: Translation model T -> Language model M -> Instruction prompt with in-context examples
- Critical path: 1) Translate input sentence x using T to get xtgt, 2) For each label ym in y, use M to translate ym to ytgt m given xtgt as context and the instruction prompt, 3) Ensure ytgt m appears in xtgt
- Design tradeoffs: Using a separate model M for label translation adds complexity but improves accuracy and faithfulness compared to marker-based methods. The quality of T directly impacts the quality of the context for M, so T must be high-quality.
- Failure signatures: Poor-quality translations from T lead to poor label translations from M. M fails to follow instructions or lacks multilingual capability, leading to inaccurate label translations. The faithfulness constraint is not met, indicating a failure in the label translation process.
- First 3 experiments: 1) Evaluate the accuracy and faithfulness of label translations using different translation models T, 2) Compare the performance of CLAP with marker-based and word alignment baselines on a downstream structured prediction task, 3) Analyze the impact of the number of in-context examples on the performance of M.

## Open Questions the Paper Calls Out

### Open Question 1
How does CLAP's performance scale when applied to structure extraction tasks beyond event argument extraction, such as relation extraction or named entity recognition? The paper evaluates CLAP primarily on event argument extraction (EAE) but mentions its potential applicability to other structure extraction tasks. Empirical results showing CLAP's performance on tasks like relation extraction or named entity recognition, with comparisons to baseline methods, would resolve this question.

### Open Question 2
What is the impact of the number of in-context examples (n) on CLAP's performance, and is there an optimal value for different languages or tasks? The paper mentions using n=2 in-context examples but does not explore the effect of varying n on performance. Systematic experiments varying n and analyzing its impact on translation quality and downstream task performance across different languages and tasks would resolve this question.

### Open Question 3
How does CLAP handle extremely low-resource languages where even parallel corpora for training translation models are scarce? The paper mentions exploring CLAP's applicability on ten extremely low-resource languages but does not provide detailed results or analysis. Experimental results and analysis of CLAP's performance on extremely low-resource languages, including comparisons with baseline methods and insights into challenges and potential solutions, would resolve this question.

## Limitations

- The evaluation primarily relies on proxy metrics rather than direct human evaluation of label translation quality
- The paper lacks ablation studies on critical components of CLAP, including instruction formats and in-context examples
- The evaluation scope is limited to only two structured prediction tasks and two target languages, raising questions about generalizability

## Confidence

- High Confidence: The core mechanism of using translated text as context for label translation is well-specified and theoretically sound
- Medium Confidence: The claim that instruction-tuned models can reliably follow label projection instructions across multiple languages
- Low Confidence: The scalability claims to extremely low-resource languages are not empirically validated

## Next Checks

1. Conduct human evaluation studies where bilingual annotators assess the quality of translated labels independently of downstream task performance to validate whether FMR score improvements correspond to meaningful improvements in label translation quality.

2. Systematically vary the instruction prompt format (different phrasings, constraints, and in-context examples) to determine which components most strongly impact performance and help optimize the approach.

3. Apply CLAP to at least three additional structured prediction tasks (e.g., relation extraction, semantic role labeling, and part-of-speech tagging) across a diverse set of target languages including both high-resource and low-resource languages to test generalizability and identify any task-specific limitations.