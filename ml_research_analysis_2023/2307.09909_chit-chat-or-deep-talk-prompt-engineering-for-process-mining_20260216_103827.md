---
ver: rpa2
title: 'Chit-Chat or Deep Talk: Prompt Engineering for Process Mining'
arxiv_id: '2307.09909'
source_url: https://arxiv.org/abs/2307.09909
tags:
- process
- mining
- data
- question
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a novel framework for integrating Large Language
  Models (LLMs) into conversational agents for process mining, addressing the complexity
  and diverse skill requirements of process mining tasks. The framework emulates the
  tasks of process analysts, domain experts, and data engineers by constructing nuanced
  prompts that guide the LLM to generate SQL queries for answering user questions.
---

# Chit-Chat or Deep Talk: Prompt Engineering for Process Mining

## Quick Facts
- arXiv ID: 2307.09909
- Source URL: https://arxiv.org/abs/2307.09909
- Reference count: 17
- Primary result: 77% comprehension rate and 68% accuracy rate for LLM-based process mining conversational agent

## Executive Summary
This study presents a novel framework for integrating Large Language Models (LLMs) into conversational agents for process mining, addressing the complexity and diverse skill requirements of process mining tasks. The framework emulates the tasks of process analysts, domain experts, and data engineers by constructing nuanced prompts that guide the LLM to generate SQL queries for answering user questions. Evaluated on the BPI Challenge 2019 dataset and a corpus of 795 process mining questions, the framework achieved 77% comprehension rate (fully or partially understood) and 68% accuracy rate (fully or partially correct answers). The results demonstrate the potential of LLMs in enhancing accessibility and performance of process mining conversational agents, though further improvements in LLM memory and prompt engineering are suggested for future research.

## Method Summary
The framework employs an orchestrator architecture that receives user questions, performs similarity matching against previously answered questions, and if no match is found, creates specialized prompts for three roles: data engineer, domain expert, and process analyst. The LLM generates SQL queries based on these prompts, which are then executed against the process mining database. If SQL execution fails, the system provides iterative feedback with error messages until a valid query is produced or iteration limits are reached. The framework compares GPT-3.5-turbo and GPT-4 models with zero-shot and few-shot learning approaches to optimize the balance between accuracy and computational cost.

## Key Results
- Achieved 77% comprehension rate (fully or partially understood questions)
- Achieved 68% accuracy rate (fully or partially correct answers)
- GPT-4 with few-shot learning outperformed GPT-3.5 but at significantly higher cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task decomposition via role-specific prompts (Data Engineer, Domain Expert, Process Analyst) improves LLM comprehension of process mining queries.
- Mechanism: Assigning specific roles to LLMs through tailored prompts narrows the scope of each interaction, allowing the model to focus on relevant domain knowledge and data context. The orchestrator coordinates these specialized prompts to build a comprehensive SQL query.
- Core assumption: LLMs perform better when given explicit role-based instructions rather than generic prompts for complex, multi-domain tasks.
- Evidence anchors:
  - [abstract] "Our framework emulates the tasks of process analysts, domain experts, and data engineers by constructing nuanced prompts that guide the LLM to generate SQL queries"
  - [section 3.3] "By establishing a process and assigning roles to segregate duties and context, enabling the LLM to focus on its specific task, the outcomes of multiple queries present a greater specificity"

### Mechanism 2
- Claim: Chain-of-thought reasoning and iterative feedback loops improve SQL query accuracy.
- Mechanism: After generating an initial SQL query, the system executes it and provides error feedback or semantic validation back to the LLM. This iterative process allows the model to correct mistakes and refine its reasoning until a valid, correct query is produced.
- Core assumption: LLMs can self-correct when provided with execution feedback and error messages, improving over multiple iterations.
- Evidence anchors:
  - [section 3.2] "If the execution is successful, the user is provided with a response; if it fails, the LLM is given feedback—previous results, the SQL error, and instructions for correction. This iterative cycle continues until a satisfactory answer is achieved"
  - [section 4.4] "In 61 instances, the GPT 3.5-Turbo Model produced the correct answer without additional shots (Zero Shot) and in 49 the answer was partially correct. If the code failed to execute, we asked the model to correct the error, supplying it with the error message"

### Mechanism 3
- Claim: Contextual embeddings and similarity matching reduce redundant LLM queries.
- Mechanism: The system stores embeddings of previously answered questions. When a new question arrives, it checks for high-similarity matches (>0.9) in the database. If found and the prior answer was successful, it reuses the SQL query instead of querying the LLM again.
- Core assumption: Similar questions will require similar SQL queries, making caching via embeddings an effective optimization.
- Evidence anchors:
  - [section 3.3] "Upon receiving a user's question, the application (orchestrator) first checks for similar questions in the database. These questions have previously been submitted to an LLM to generate embeddings for each of them. If any of these vectors show a similarity greater than 0.9 with the new question, the orchestrator verifies the success of the previous answer"

## Foundational Learning

- Concept: Role-based prompt engineering
  - Why needed here: Process mining requires integrating knowledge from multiple domains (data engineering, process analysis, domain expertise). Without role separation, LLMs may produce incomplete or incorrect SQL queries.
  - Quick check question: What are the three primary roles emulated in this framework, and why is each necessary?

- Concept: Chain-of-thought reasoning
  - Why needed here: Complex process mining questions often require multi-step reasoning. Without explicit reasoning steps, LLMs may skip critical intermediate logic, leading to incorrect answers.
  - Quick check question: How does the framework ensure the LLM demonstrates its reasoning before generating SQL?

- Concept: Iterative error correction
  - Why needed here: Generated SQL may fail due to syntax errors, incorrect table/column names, or logical flaws. Without feedback loops, these errors would result in user-facing failures.
  - Quick check question: What happens when the initial SQL query fails execution, and how does the system respond?

## Architecture Onboarding

- Component map: User Interface → Orchestrator → Prompt Creation → LLM (GPT-3.5/GPT-4) → SQL Execution → Database → Response
- Critical path: User Question → Similarity Check → Prompt Creation → LLM Query → SQL Execution → Response Delivery
- Design tradeoffs:
  - GPT-4 vs GPT-3.5: Higher accuracy with GPT-4 but significantly higher cost; framework uses GPT-3.5 as default with GPT-4 fallback
  - Zero-shot vs Few-shot: Zero-shot is cheaper but less accurate; few-shot improves accuracy but increases cost and latency
  - Iteration limits: More iterations improve accuracy but increase latency and cost
- Failure signatures:
  - High similarity matches with failed previous answers → potential cache poisoning
  - Consistent SQL syntax errors across iterations → prompt structure issues
  - LLM consistently misunderstands domain terms → insufficient context ontology coverage
- First 3 experiments:
  1. Test similarity matching with a small set of manually crafted similar and dissimilar questions to validate embedding quality
  2. Run a single complex process mining question through the full pipeline to verify all components interact correctly
  3. Measure cost and accuracy trade-offs by comparing GPT-3.5 vs GPT-4 on a subset of questions with few-shot examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively measure and quantify LLM understanding in process mining queries?
- Basis in paper: [inferred] The paper mentions "For future research, an important objective would be to explore ways to effectively measure the understanding of LLMs to enhance the results."
- Why unresolved: Current evaluation criteria (Understood, Partially Understood) rely on manual assessment and may not capture nuanced comprehension levels. The paper suggests this is a key area for improvement.
- What evidence would resolve it: A validated quantitative metric for LLM understanding that correlates with actual query performance across diverse process mining scenarios.

### Open Question 2
- Question: What is the optimal balance between prompt complexity and cost-efficiency in LLM-based process mining?
- Basis in paper: [explicit] "It's evident from these results that GPT-4 performs best with few-shot learning. However, the significant cost associated with this model should not be overlooked."
- Why unresolved: The paper demonstrates cost-performance trade-offs but doesn't provide a systematic framework for optimizing this balance across different use cases.
- What evidence would resolve it: Empirical studies mapping cost-performance curves across different prompt engineering techniques and LLM models in real-world process mining applications.

### Open Question 3
- Question: How can LLMs be equipped with persistent memory to maintain context across extended process mining sessions?
- Basis in paper: [explicit] "We suggest future research could explore the idea of external memory for LLMs to retain context over extended interactions."
- Why unresolved: Current LLMs process queries independently without maintaining historical context, limiting their ability to handle complex, multi-step process mining tasks.
- What evidence would resolve it: Demonstrated architecture for integrating external memory systems with LLMs that improves performance on multi-query process mining tasks while maintaining accuracy.

### Open Question 4
- Question: What methods can detect and prevent LLM hallucinations in process mining query generation?
- Basis in paper: [explicit] "Detecting answers that are technically correct but semantically incorrect is another challenge, referred to as 'hallucination', that needs to be addressed."
- Why unresolved: The paper acknowledges hallucination as a challenge but doesn't propose specific solutions for identifying or preventing it in the process mining context.
- What evidence would resolve it: Implementation of hallucination detection mechanisms that can identify and correct semantically incorrect answers while maintaining the LLM's ability to handle complex queries.

## Limitations

- Limited generalizability to datasets and question types beyond BPI Challenge 2019 and the 795-question corpus
- Unclear prompt templates and Chain-of-Thought examples make exact replication difficult
- Iterative error correction may lead to unpredictable latency and cost in production scenarios

## Confidence

- Framework Architecture: High
- Performance Metrics: Medium
- Generalizability: Low-Medium

## Next Checks

1. Conduct ablation studies to determine the specific contribution of each prompt engineering component (role separation, Chain-of-Thought, few-shot examples) to overall performance.
2. Evaluate the framework on at least two additional process mining datasets from different domains to assess robustness and identify dataset-specific limitations.
3. Perform a detailed analysis of the trade-offs between GPT-3.5 and GPT-4 usage, including comprehensive latency measurements, iteration counts, and cost calculations for different query types.