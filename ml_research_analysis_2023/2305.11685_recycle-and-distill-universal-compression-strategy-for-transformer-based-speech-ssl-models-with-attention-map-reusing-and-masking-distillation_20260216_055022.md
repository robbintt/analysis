---
ver: rpa2
title: 'Recycle-and-Distill: Universal Compression Strategy for Transformer-based
  Speech SSL Models with Attention Map Reusing and Masking Distillation'
arxiv_id: '2305.11685'
source_url: https://arxiv.org/abs/2305.11685
tags:
- speech
- distillation
- attention
- masking
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a universal compression strategy for Transformer-based
  speech SSL models that combines attention map reusing and masking distillation.
  Attention map reusing eliminates key and query parameters in certain Transformer
  layers by substituting attention maps from previous layers, while masking distillation
  extends the distillation loss to both masked and unmasked speech frames to fully
  leverage the teacher model's representation quality.
---

# Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation

## Quick Facts
- arXiv ID: 2305.11685
- Source URL: https://arxiv.org/abs/2305.11685
- Authors: 
- Reference count: 0
- Primary result: ARMHuBERT achieves 7.72% PER and 9.96% WER on SUPERB benchmark with 28% parameters and 30% MACs of original model

## Executive Summary
This work introduces a universal compression strategy for Transformer-based speech SSL models that combines attention map reusing and masking distillation. Attention map reusing eliminates key and query parameters in certain Transformer layers by substituting attention maps from previous layers, while masking distillation extends the distillation loss to both masked and unmasked speech frames to fully leverage the teacher model's representation quality. The resulting ARMHuBERT model achieves state-of-the-art performance in universal compression of HuBERT, with 7.72% phoneme error rate (PER) and 9.96% word error rate (WER) on the SUPERB benchmark, while reducing parameters to 28% and MACs to 30% of the original model. The method demonstrates consistent effectiveness across different teacher models including wavLM, establishing a universal compression strategy for speech SSL models.

## Method Summary
The proposed method compresses Transformer-based speech SSL models through two complementary techniques. First, attention map reusing eliminates key and query parameters in certain Transformer layers by substituting attention maps from previous layers, reducing both parameters and computation. Second, masking distillation extends the distillation loss to both masked and unmasked speech frames, leveraging the teacher model's high-quality representation across all input frames. The method is applied to create ARMHuBERT by compressing a pretrained HuBERT BASE model using a student model with attention map reusing and masking distillation. Parameters saved from attention map reusing are reinvested into increasing the width of feed-forward networks (FFN) in each layer, maintaining model capacity while reducing computation. The approach is validated across different teacher models including wavLM, demonstrating its universal applicability.

## Key Results
- ARMHuBERT achieves 7.72% PER and 9.96% WER on SUPERB benchmark, significantly outperforming existing compression methods
- Model reduces parameters to 28% and MACs to 30% of the original HuBERT BASE model while maintaining competitive performance
- ARMwavLM-S model achieves 9.61% PER and 12.66% WER, demonstrating the method's effectiveness across different teacher architectures

## Why This Works (Mechanism)

### Mechanism 1
Attention map reusing reduces parameters and MACs by eliminating redundant key and query computations in Transformer layers. The method reuses the attention map from a previous layer (e.g., even-numbered layer uses odd-numbered layer's attention map) instead of computing new key and query projections, cutting computation in half for those layers. This works because attention maps across layers and heads are sufficiently similar in pretrained models that reusing them does not significantly degrade representation quality. The core assumption is that attention map similarity across layers is sufficient to maintain performance when reusing. Break condition: If attention maps across layers diverge significantly in the student model, performance will degrade and the parameter savings will not compensate.

### Mechanism 2
Masking distillation extends distillation loss to both masked and unmasked frames to fully leverage teacher model's representation quality. During distillation, both the masked prediction loss (from masked frames) and an unmasked loss (from unmasked frames, but with the teacher also receiving masked input) are combined, preventing bias and improving general representation. This works because the teacher model's representation remains high quality even when both models receive masked input, and this unmasked loss improves student learning without leaking essential information. The core assumption is that unmasked loss from masked input provides useful learning signals without overfitting. Break condition: If masking ratio is too high or too low, or if the unmasked loss leaks too much information, the student may overfit or underperform.

### Mechanism 3
Reinvesting saved parameters from attention map reusing into FFN width improves speech representation quality without increasing total parameter count. Parameters saved by removing key/query in certain layers are reassigned to increase the width of the FFN in each layer, maintaining model capacity while reducing computation. This works because increasing FFN width can compensate for reduced attention complexity and even improve downstream task performance. The core assumption is that FFN capacity can effectively substitute for reduced attention capacity. Break condition: If FFN width increase does not compensate for reduced attention complexity, or if the capacity distribution is suboptimal, performance will suffer.

## Foundational Learning

- Concept: Self-supervised learning (SSL) in speech using masked prediction objectives
  - Why needed here: The teacher models (HuBERT, wavLM) are pretrained using SSL with masked prediction, so the student must learn from these representations
  - Quick check question: What is the main objective used to pretrain HuBERT and wavLM?

- Concept: Knowledge distillation and its application to compressing large models
  - Why needed here: The core compression strategy relies on distilling knowledge from a large teacher model to a smaller student model
  - Quick check question: What is the difference between layer-to-layer (L2L) distillation and standard KD?

- Concept: Transformer architecture and multi-head self-attention (MHSA) mechanics
  - Why needed here: Understanding how attention maps are computed and reused is central to the compression strategy
  - Quick check question: How are queries, keys, and values computed in MHSA, and what is their dimensionality relationship?

## Architecture Onboarding

- Component map:
  Input -> Masking -> Teacher (HuBERT/wavLM) -> Student (ARMHuBERT) -> Distillation Loss (Masked + Unmasked) -> Output

- Critical path:
  1. Apply masking to input frames
  2. Forward pass through teacher (with masked input for unmasked loss)
  3. Forward pass through student (with ReuseMHSA and masking)
  4. Compute combined masked and unmasked distillation losses
  5. Backpropagate and update student parameters

- Design tradeoffs:
  - Reuse pattern selection (e.g., 2by6 vs 3by4) vs parameter/MAC savings vs performance
  - Masking ratio selection (e.g., 0.4 vs 0.8) vs learning difficulty vs representation quality
  - FFN width increase vs attention capacity reduction

- Failure signatures:
  - Performance drop in content/semantics tasks indicates attention reuse is too aggressive
  - Overfitting or unstable training suggests masking ratio or loss balance is incorrect
  - High MACs or parameters indicate reuse pattern or reinvestment is suboptimal

- First 3 experiments:
  1. Compare 2by6 vs 3by4 reuse patterns on LibriSpeech 100h with fixed masking ratio
  2. Test masking ratio sweep (0.4, 0.6, 0.8) with fixed reuse pattern on LibriSpeech 100h
  3. Evaluate FFN width reinvestment impact by comparing MaskHuBERT vs ARMHuBERT on SUPERB benchmark

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of attention map reusing vary across different speech SSL architectures (e.g., wav2vec 2.0, HuBERT, wavLM) and why? The authors demonstrate that their ARMwavLM-S model achieves better results than ARMHuBERT-S, suggesting architecture-specific effectiveness. This remains unresolved because the paper only compares two architectures with one compression strategy, and the underlying reasons for performance differences across architectures remain unexplored.

### Open Question 2
What is the optimal masking ratio schedule during training for different dataset sizes and why does it vary? The authors find that 0.4 masking ratio works best for 100h dataset while 0.4 and 0.8 both work for 960h dataset, but don't explore dynamic scheduling strategies beyond a simple linear schedule. This remains unresolved because the paper only tests static masking ratios and one linear scheduling approach, with the impact of more sophisticated scheduling on different dataset sizes unexplored.

### Open Question 3
How does attention map reusing affect the interpretability and explainability of compressed speech SSL models? The paper focuses on performance metrics but doesn't examine how removing key and query parameters through attention map reusing impacts the model's ability to provide interpretable attention patterns. This remains unresolved because the compression technique fundamentally changes how attention is computed, potentially affecting interpretability, but this aspect is not investigated.

## Limitations
- Limited empirical validation of core mechanisms - no direct ablation showing individual component contributions
- Evaluation scope restricted to LibriSpeech-derived benchmarks and SUPERB, potentially limiting generalizability
- Theoretical claims about attention map similarity lack direct quantitative analysis or corpus evidence

## Confidence
- **High Confidence**: The empirical results showing ARMHuBERT achieving 7.72% PER and 9.96% WER on SUPERB benchmark with 28% parameters and 30% MACs of the original model
- **Medium Confidence**: The general effectiveness of the attention map reusing and masking distillation approach across different teacher models (HuBERT and wavLM)
- **Low Confidence**: The theoretical claims about why attention map reusing works and why masking distillation is superior to standard KD approaches

## Next Checks
1. **Attention Map Correlation Analysis**: Measure the similarity (e.g., cosine similarity, KL divergence) between attention maps across layers in pretrained teacher models to empirically validate the core assumption of attention map reusing
2. **Ablation Study of Individual Components**: Create controlled experiments isolating the contributions of attention map reusing, FFN reinvestment, and masking distillation by testing variants with only one or two components active
3. **Cross-Domain Robustness Testing**: Evaluate ARMHuBERT on speech datasets from different domains (e.g., conversational speech, accented speech, noisy environments) beyond LibriSpeech to assess whether the compression strategy maintains performance across diverse real-world conditions