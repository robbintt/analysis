---
ver: rpa2
title: 'AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search
  Engine Marketing Optimization'
arxiv_id: '2306.11971'
source_url: https://arxiv.org/abs/2306.11971
tags:
- each
- keyword
- environment
- keywords
- conversion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdCraft is a novel benchmark environment for evaluating reinforcement
  learning algorithms in Search Engine Marketing (SEM), addressing the need for realistic,
  complex, and cost-effective simulation environments. It simulates bidding and budgeting
  dynamics in SEM, incorporating stochastic and non-stationary properties to mimic
  real-world challenges.
---

# AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization

## Quick Facts
- arXiv ID: 2306.11971
- Source URL: https://arxiv.org/abs/2306.11971
- Reference count: 40
- Key outcome: AdCraft is a novel benchmark environment for evaluating reinforcement learning algorithms in Search Engine Marketing (SEM), addressing the need for realistic, complex, and cost-effective simulation environments.

## Executive Summary
AdCraft is a novel benchmark environment designed to evaluate reinforcement learning algorithms in Search Engine Marketing (SEM) optimization. The environment simulates bidding and budgeting dynamics within SEM, incorporating stochastic and non-stationary properties to mimic real-world challenges. AdCraft allows users to customize keyword parameters, traffic volumes, and conversion rates, enabling tailored experimentation. Experiments demonstrate that Deep RL algorithms struggle with sparse and non-stationary data, with performance dropping significantly under non-stationary conditions. The environment provides built-in metrics like Normalized Cumulative Profit (NCP) and Average per-Keyword Normalized Cumulative Profit (AKNCP) for evaluation. AdCraft serves as a valuable tool for advancing RL research in SEM optimization and addressing real-world uncertainties.

## Method Summary
AdCraft simulates SEM bidding environments using an MDP framework where agents submit bids and budgets for keywords, competing in second-price auctions with stochastic CTR and CVR values. The environment features three types of sparsity (volume, CTR, CVR) that create hierarchical reward structures, making credit assignment difficult. Non-stationarity is introduced through multiplicative random walks in CTR and CVR values. The benchmark includes both implicit keywords (sampled from Laplace distributions) and explicit keywords (deterministic cost functions). Evaluation uses NCP and AKNCP metrics across dense (high volume/CTR/CVR) and sparse (low volume/CTR/CVR) regimes, both stationary and non-stationary. Deep RL algorithms (PPO, A2C, TD3) are compared against a built-in baseline algorithm using RLlib wrappers.

## Key Results
- Deep RL algorithms struggle with sparse and non-stationary data, showing significant performance degradation in non-stationary conditions
- Performance drops more severely in sparse regimes with low conversion rates (e.g., mean volume 16, CVR 0.1)
- The baseline algorithm outperforms all three Deep RL algorithms in both dense and sparse non-stationary environments
- AdCraft successfully demonstrates the challenges of credit assignment in hierarchical sparse reward structures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The non-stationary random walk in CTR and CVR creates dynamic profit landscapes that expose weaknesses in static bidding strategies.
- **Mechanism:** By multiplying CTR and CVR by a random step in [1-η, 1+η] each day, the expected profit for a given bid fluctuates over time. This forces the agent to adapt bids dynamically rather than learning a fixed optimal bid.
- **Core assumption:** The daily fluctuation magnitude η is small enough that the profit landscape remains continuous but large enough to cause meaningful shifts in optimal bidding.
- **Evidence anchors:**
  - [abstract] "we demonstrate the challenges imposed on agent convergence and performance by sparsity and non-stationarity."
  - [section] "the AdCraft environment seeks to emulate the dynamic and uncertain nature of live auction scenarios... By default, AdCraft utilizes a multiplicative random walk for click-through rate and conversion rate"
- **Break condition:** If η is set to 0, the environment becomes stationary and the mechanism no longer applies.

### Mechanism 2
- **Claim:** The hierarchical sparsity structure (volume → CTR → CVR) creates long-tail reward distributions that make learning difficult.
- **Mechanism:** Keyword volume determines the number of auctions observed, CTR determines the fraction of auctions that generate clicks (and thus costs), and CVR determines the fraction of clicks that generate revenue. Low values in any level drastically reduce the number of reward signals, making credit assignment difficult.
- **Core assumption:** The joint distribution of volume, CTR, and CVR in the initialization quantiles matches real-world sparsity patterns.
- **Evidence anchors:**
  - [abstract] "demonstrate the challenges imposed on agent convergence and performance by sparsity"
  - [section] "The AdCraft environment features three notions of sparsity: keyword volume, click-through rate (CTR), and conversion rate (CVR). These factors hierarchically influence the sparsity of rewards in SEM bidding."
- **Break condition:** If all keywords have high volume, CTR, and CVR, the sparsity mechanism is effectively disabled.

### Mechanism 3
- **Claim:** The second-price auction simulation with competing bid sampling creates a realistic cost structure that depends on both bid level and competition.
- **Mechanism:** For ImplicitKeywords, the agent's bid competes against bids sampled from a Laplace distribution. The cost paid when winning is the second-highest bid, creating a non-linear relationship between bid level and cost that agents must learn.
- **Core assumption:** The Laplace distribution parameters (location and scale) adequately model real competitor bid distributions.
- **Evidence anchors:**
  - [section] "Implicit keywords incorporate internal distributions for the number of bidders and the distributions from which their bids are drawn... For each auction, the distribution used to sample the second price bid for a given keyword follows a Laplace distribution."
- **Break condition:** If the cost is set to be a fixed percentage of the bid regardless of competition, this mechanism no longer applies.

## Foundational Learning

- **Concept:** Markov Decision Process (MDP) formulation of SEM bidding
  - Why needed here: The paper frames SEM bidding as an MDP where states are aggregated daily observations, actions are bids, and rewards are profits. Understanding MDPs is crucial for grasping how RL algorithms are applied.
  - Quick check question: In the AdCraft MDP, what constitutes the state transition function given that the environment is non-stationary?

- **Concept:** Exploration-exploitation tradeoff in sparse reward environments
  - Why needed here: The paper shows that deep RL algorithms struggle with sparse keywords where conversions are rare. Understanding exploration strategies (like those in PPO, A2C, TD3) is key to why some algorithms perform better than others.
  - Quick check question: Why might a sparse keyword with mean volume 16 and CVR 0.1 be particularly challenging for credit assignment?

- **Concept:** Non-stationary reinforcement learning
  - Why needed here: The paper's main contribution includes demonstrating how non-stationarity affects RL performance. Understanding techniques for handling non-stationary environments (like tracking, meta-learning, or adaptive exploration) is essential.
  - Quick check question: How does the multiplicative random walk for CTR differ from an additive random walk in terms of the impact on optimal bidding strategy?

## Architecture Onboarding

- **Component map:** Environment core -> Keyword class (ImplicitKeyword/ExplicitKeyword) -> RL interface -> Rust acceleration layer -> Evaluation metrics module
- **Critical path:** Initialize environment with keywords → Agent submits bid and budget → Environment processes auctions → Generate impressions, clicks, conversions, revenue → Return aggregated observations and profit reward → Agent updates policy
- **Design tradeoffs:** 
  - Implicit vs Explicit keywords: Implicit provides more realistic auction dynamics but requires sampling from distributions; Explicit allows deterministic cost functions but may be less realistic
  - Rust integration: Provides 3-110x speedup but increases packaging complexity
  - Daily aggregation: Reduces computational load but loses intra-day auction detail
- **Failure signatures:**
  - Agent consistently bids zero: Likely due to poor exploration or misunderstanding of reward structure
  - Agent exhausts budget immediately: Possibly overfitting to high-volume keywords or misunderstanding budget constraints
  - Agent performance drops in non-stationary regime: Indicates lack of adaptation mechanisms
- **First 3 experiments:**
  1. Run baseline algorithm with fixed CTR=0.8, CVR=0.8, volume=128 to establish dense regime performance
  2. Run baseline with fixed CTR=0.1, CVR=0.1, volume=16 to establish sparse regime performance
  3. Run PPO with default hyperparameters on stationary dense environment to compare against baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Deep RL algorithms in AdCraft scale with campaign size beyond 100 keywords?
- Basis in paper: [explicit] The paper mentions that while the experiments use a campaign size of 100 keywords, real-world scenarios often involve hundreds to thousands of keywords per campaign.
- Why unresolved: The experiments conducted in the paper were limited to a campaign size of 100 keywords due to computational constraints and research considerations.
- What evidence would resolve it: Conducting experiments with larger campaign sizes (e.g., 500, 1000, or more keywords) and comparing the performance of Deep RL algorithms to the baseline algorithm would provide insights into the scalability of these algorithms in AdCraft.

### Open Question 2
- Question: How do different exploration strategies (e.g., ϵ-greedy, upper confidence bound, Thompson sampling) affect the performance of Deep RL algorithms in AdCraft?
- Basis in paper: [explicit] The paper mentions that RL algorithms tackle the exploration-exploitation tradeoff using strategies such as ϵ-greedy, upper confidence bound (UCB), and Thompson sampling.
- Why unresolved: The paper does not compare the performance of different exploration strategies within the AdCraft environment.
- What evidence would resolve it: Conducting experiments with different exploration strategies and comparing their performance in terms of NCP and AKNCP would provide insights into the impact of exploration strategies on Deep RL algorithms in AdCraft.

### Open Question 3
- Question: How does the performance of Deep RL algorithms in AdCraft change when incorporating real-world data for keyword parameter initialization?
- Basis in paper: [explicit] The paper mentions that AdCraft allows for the incorporation of offline data to create realistic advertising scenarios by initializing keyword parameters based on empirical observations.
- Why unresolved: The experiments conducted in the paper use randomly sampled keyword parameters, and the impact of using real-world data for initialization is not explored.
- What evidence would resolve it: Conducting experiments with keyword parameters initialized based on real-world data and comparing the performance of Deep RL algorithms to the baseline algorithm would provide insights into the benefits of using real-world data in AdCraft.

## Limitations

- The paper's baseline algorithm implementation details are not fully specified, making it difficult to verify whether performance differences with RL algorithms are consistent across implementations
- The experiments were limited to a campaign size of 100 keywords due to computational constraints, preventing analysis of scalability to real-world campaign sizes
- The broader claims about AdCraft being a valuable tool for advancing RL research are not yet substantiated by published results from external research groups

## Confidence

- **High Confidence**: The technical implementation of AdCraft's environment mechanics (second-price auction simulation, non-stationary CTR/CVR dynamics, hierarchical sparsity structure) is well-documented and reproducible based on the provided code repository
- **Medium Confidence**: The empirical demonstration that RL algorithms struggle with sparsity and non-stationarity is supported by experimental results, though the baseline comparison could be strengthened with more detailed implementation specifications
- **Low Confidence**: The paper's broader claims about AdCraft being a "valuable tool for advancing RL research in SEM optimization" are not yet substantiated by published results from external research groups using the benchmark

## Next Checks

1. **Replicate the baseline algorithm**: Implement and test the exact baseline algorithm used in the paper to verify whether performance differences with RL algorithms are consistent across implementations

2. **Hyperparameter sensitivity analysis**: Systematically vary RL algorithm hyperparameters (learning rates, network architectures, exploration strategies) to determine if the observed performance issues can be mitigated through tuning

3. **External validation study**: Have an independent research group use AdCraft to evaluate at least one novel RL algorithm, verifying that the benchmark can indeed identify improvements over existing approaches