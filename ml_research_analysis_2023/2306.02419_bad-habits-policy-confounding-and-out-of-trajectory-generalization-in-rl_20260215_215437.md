---
ver: rpa2
title: 'Bad Habits: Policy Confounding and Out-of-Trajectory Generalization in RL'
arxiv_id: '2306.02419'
source_url: https://arxiv.org/abs/2306.02419
tags:
- policy
- agent
- history
- when
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of policy confounding, where
  reinforcement learning agents develop habits based on spurious correlations between
  observations and rewards induced by their own policies. These habits may fail when
  agents deviate from their typical trajectories, leading to poor out-of-trajectory
  generalization.
---

# Bad Habits: Policy Confounding and Out-of-Trajectory Generalization in RL

## Quick Facts
- **arXiv ID**: 2306.02419
- **Source URL**: https://arxiv.org/abs/2306.02419
- **Reference count**: 29
- **Primary result**: RL agents develop habits based on spurious correlations induced by their own policies, leading to poor out-of-trajectory generalization.

## Executive Summary
This paper introduces the concept of policy confounding in reinforcement learning, where agents develop state representations that rely on spurious correlations between observations and rewards induced by their own policies. These habits may fail when agents deviate from their typical trajectories. The authors provide a mathematical framework using POMDPs and causal inference tools to characterize policy confounding and demonstrate it through examples in three environments: Frozen T-Maze, Key2Door, and Diversion. They show that on-policy methods like PPO suffer from this issue, while off-policy methods like DQN with large replay buffers are more robust. The paper highlights the need for representations that generalize beyond an agent's typical trajectories.

## Method Summary
The authors implement three grid-world environments with varying levels of partial observability and policy-induced correlation structures. They train DQN and PPO agents using standard RL algorithms with observation stacks as input. Training occurs for 100K timesteps with interleaved evaluation on both original and variant environments. The Frozen T-Maze environment introduces binary signals with optional ice dynamics, Key2Door requires memory for key collection with varying start positions, and Diversion has full observability but trajectory-dependent variable irrelevance. Performance is measured as average return over 10 random seeds.

## Key Results
- On-policy methods (PPO) show significant performance drops when evaluated on environments that deviate from their typical trajectories due to policy confounding
- Off-policy methods (DQN) with large replay buffers (100K experiences) are more robust to policy confounding than those with small buffers (10K experiences)
- Adding stochasticity through exploration or domain randomization mitigates policy confounding by broadening the trajectory distribution
- Agents learn to ignore variables that appear irrelevant under their policy-induced trajectory distribution, even when those variables are necessary for generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policy confounding occurs when agents develop history representations that omit variables needed for generalization outside their typical trajectories.
- Mechanism: As agents commit to a specific policy, they repeatedly experience the same state-action trajectories. Variables correlated with rewards under this policy but not causally related become superfluous from the agent's perspective. The agent learns to ignore these variables to simplify its representation, causing failures when the environment or policy changes slightly.
- Core assumption: The agent's policy induces a distribution over trajectories that makes certain state variables appear irrelevant, even though they are needed for out-of-trajectory generalization.
- Evidence anchors: Abstract states agents may pick up on correlations and learn state representations that do not generalize beyond the agent's trajectory distribution. Section explains agents can exploit spurious correlations to build simple habits that require little effort to carry out.

### Mechanism 2
- Claim: Off-policy methods with large replay buffers are more robust to policy confounding than on-policy methods.
- Mechanism: Off-policy methods store experiences from multiple policies in a replay buffer. When training, the agent samples from this mixture of trajectories, which dilutes the influence of spurious correlations from any single policy. Large buffers ensure the mixture is diverse enough to prevent overfitting to specific trajectories.
- Core assumption: The replay buffer contains a sufficiently diverse set of experiences from different policies that spurious correlations present in any one policy's trajectory distribution are diluted.
- Evidence anchors: Section results confirm this claim showing DQN performance in Frozen T-Maze with large vs small replay buffers. Corpus papers don't directly address replay buffer size effects on confounding.

### Mechanism 3
- Claim: Exploration and domain randomization can mitigate policy confounding by broadening the trajectory distribution.
- Mechanism: Adding stochasticity to the policy or environment forces the agent to experience a wider variety of trajectories. This prevents the agent from overfitting to correlations present in a narrow set of trajectories and encourages learning representations that generalize across different paths.
- Core assumption: The added stochasticity is sufficient to expose the agent to trajectories that would break the spurious correlations without making the task too difficult or changing the optimal policy.
- Evidence anchors: Section states added noise prevents samples containing spurious correlations from dominating training batches. Corpus papers don't directly address exploration effects on confounding.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The paper models the RL problem as a POMDP where agents must track history to make decisions, essential for understanding how history representations can be confounded.
  - Quick check question: What is the difference between a POMDP and an MDP in terms of what the agent observes?

- Concept: State abstraction and history representations
  - Why needed here: The paper builds on state abstraction theory to define history representations and explain how policy confounding occurs when agents abstract away necessary information.
  - Quick check question: How does a Markov history representation differ from a minimal history representation?

- Concept: Causal inference and the do-calculus
  - Why needed here: The paper uses causal inference concepts like the do-operator to formally characterize when a history representation is confounded by a policy.
  - Quick check question: What does the do-operator represent in causal inference, and how is it used to distinguish correlation from causation?

## Architecture Onboarding

- Component map: Environment simulator -> Agent policy/value function approximators -> Experience replay buffer -> Training loop
- Critical path: Environment step → Collect experience (state, action, reward, next state) → Store in replay buffer → Sample mini-batch → Compute loss (policy and/or value) → Update network parameters → Repeat
- Design tradeoffs: On-policy vs off-policy (sample efficiency vs bias), exploration strategy (pure exploitation vs robustness), replay buffer size (memory vs diversity), network architecture (capacity vs overfitting)
- Failure signatures: On-policy methods show performance drop on evaluation environments after training on original environment; off-policy methods fail when replay buffer is too small; both methods can fail when stochasticity is insufficient
- First 3 experiments:
  1. Replicate Frozen T-Maze experiment comparing PPO and DQN performance on train vs evaluation environments
  2. Test effect of replay buffer size on DQN's out-of-trajectory generalization in Key2Door environment
  3. Evaluate impact of exploration noise on PPO's generalization to diversion environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a principled method to identify and quantify the degree of policy confounding in reinforcement learning agents?
- Basis in paper: The paper introduces policy confounding and provides a mathematical framework to characterize it, but acknowledges that a complete answer for how to prevent policy confounding is not yet available.
- Why unresolved: While the paper provides a theoretical foundation for understanding policy confounding, it does not offer a practical method to measure the extent to which an agent's policy is confounded. This is a crucial step in addressing the problem, as it would allow researchers to identify when and to what degree policy confounding is occurring.
- What evidence would resolve it: Developing a metric or set of metrics that can quantify the degree of policy confounding in an RL agent's policy would provide a concrete way to measure the problem.

### Open Question 2
- Question: What are the specific mechanisms by which exploration and domain randomization mitigate the effects of policy confounding?
- Basis in paper: The paper suggests that exploration and domain randomization can help mitigate policy confounding by broadening the distribution of trajectories, but it does not provide a detailed explanation of how these techniques achieve this.
- Why unresolved: While the paper proposes exploration and domain randomization as potential solutions, it does not delve into the specific ways in which these techniques help agents overcome policy confounding. Understanding these mechanisms would provide insights into how to design more effective exploration strategies or domain randomization techniques.
- What evidence would resolve it: Conducting experiments that systematically vary the level of exploration or domain randomization and analyzing the resulting agent behavior would shed light on the specific mechanisms by which these techniques mitigate policy confounding.

### Open Question 3
- Question: How can we design RL algorithms that inherently prevent policy confounding, rather than relying on external techniques like exploration or domain randomization?
- Basis in paper: The paper suggests that policy confounding arises from the way RL agents learn representations based on the trajectories they experience under a specific policy. This implies that designing algorithms that encourage agents to learn more robust representations could prevent policy confounding.
- Why unresolved: While the paper proposes exploration and domain randomization as potential solutions, these techniques are external to the RL algorithm itself. Designing algorithms that inherently prevent policy confounding would be a more fundamental and potentially more effective approach.
- What evidence would resolve it: Developing and testing new RL algorithms that incorporate mechanisms to encourage robust representation learning would provide evidence for the effectiveness of this approach.

## Limitations

- The causal framing, while intuitive, relies heavily on parallels to causal inference concepts that require further validation in the RL context
- The mechanism explanations rely on intuition about "spurious correlations" without extensive quantitative analysis of what specific correlations agents learn
- The proposed solutions (larger replay buffers, exploration, domain randomization) are well-established RL techniques, and their benefits here may be attributed to more than just policy confounding mitigation

## Confidence

- **High confidence**: The empirical demonstrations of policy confounding across multiple environments and methods (PPO vs DQN performance differences, replay buffer effects)
- **Medium confidence**: The theoretical characterization using POMDPs and history representations as a framework for understanding the problem
- **Medium confidence**: The proposed solutions work empirically but the causal mechanism connecting them to confounding mitigation could be more rigorously established

## Next Checks

1. **Quantitative correlation analysis**: Measure and visualize the specific state-action-reward correlations that develop during training and disappear during evaluation to directly demonstrate spurious correlation formation.

2. **Cross-environment transfer**: Train agents on one environment variant and test on another to determine whether learned representations transfer across different policy-induced correlation structures.

3. **Representation ablation study**: Compare learned representations (via probing classifiers) when agents are trained with and without the confounding-inducing policies to identify which features are dropped and why.