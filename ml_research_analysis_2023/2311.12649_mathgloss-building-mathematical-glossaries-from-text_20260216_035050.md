---
ver: rpa2
title: 'MathGloss: Building mathematical glossaries from text'
arxiv_id: '2311.12649'
source_url: https://arxiv.org/abs/2311.12649
tags:
- mathematics
- terms
- mathematical
- mathgloss
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MathGloss is an automated knowledge graph construction project
  that builds a comprehensive database of undergraduate mathematical concepts by mapping
  terms from multiple sources (Wikidata, university courses, theorem provers, multilingual
  dictionaries) to a unified framework. The system addresses the challenge of inconsistent
  mathematical terminology across different resources and languages.
---

# MathGloss: Building mathematical glossaries from text

## Quick Facts
- arXiv ID: 2311.12649
- Source URL: https://arxiv.org/abs/2311.12649
- Reference count: 2
- Key outcome: Automated knowledge graph construction mapping mathematical terms from multiple sources to Wikidata for unified access

## Executive Summary
MathGloss addresses the challenge of inconsistent mathematical terminology across different resources by creating a unified knowledge graph that maps terms from university courses, theorem provers, and multilingual dictionaries to Wikidata identifiers. The system uses wikimapper for automated mapping combined with manual curation to resolve disambiguation issues, processing approximately 700+ terms from Chicago course notes, 369 terms from French undergraduate curriculum, and 5377 terms from nLab. By integrating natural language processing with a custom detextor pipeline component for handling mathematical notation, MathGloss enables users to explore mathematical concepts across formal theorem provers, natural language definitions, and translations, facilitating better communication between mathematicians and formal verification tools.

## Method Summary
The system employs a multi-stage approach to build mathematical knowledge graphs: first, collecting mathematical terms from diverse sources including university course notes, French undergraduate curriculum with Lean 4 links, nLab category theory wiki, and multilingual dictionaries; second, mapping these terms to Wikidata identifiers using the wikimapper library with manual curation to resolve disambiguation issues; third, processing mathematical text using spaCy's syntactic parsing enhanced with a custom detextor pipeline component that treats mathematical notation as single tokens; and finally, organizing the mapped terms into a database that enables cross-referencing between natural language definitions, formal theorem prover representations, and translations. The approach balances automated processing with human oversight to ensure mapping accuracy while maintaining scalability.

## Key Results
- Successfully mapped approximately 700+ terms from Chicago course notes to Wikidata identifiers
- Processed 369 terms from French undergraduate curriculum with Lean 4 integration
- Mapped 5377 terms from nLab category theory wiki to unified identifiers
- Created functional database enabling navigation between formal theorem provers, natural language definitions, and translations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system maps mathematical terms from multiple sources to Wikidata identifiers to create a unified knowledge graph.
- Mechanism: The approach uses wikimapper to automatically map terms from resources like Chicago course notes, French undergraduate curriculum, and nLab to Wikidata QIDs, with manual curation to resolve disambiguation issues.
- Core assumption: Mathematical terms from different sources referring to the same concept should map to the same Wikidata identifier.
- Evidence anchors:
  - [abstract] "Using wikimapper and manual curation, the project maps approximately 700+ terms from Chicago course notes, 369 terms from French undergraduate curriculum, and 5377 terms from nLab to Wikidata identifiers."
  - [section] "The library wikimapper takes in the name of an 'item' and produces a Wikidata ID whose page has a title matching that name."
- Break condition: If mathematical terms are used inconsistently across sources or if Wikidata lacks entries for certain mathematical concepts, the mapping will fail.

### Mechanism 2
- Claim: Natural language processing techniques can extract mathematical terms from text corpora.
- Mechanism: The system uses spaCy's syntactic parsing and a custom "detextor" pipeline component to handle mathematical notation, extracting terms from LaTeX code by treating everything between dollar signs as single tokens.
- Core assumption: Mathematical expressions in text follow predictable patterns that can be captured by NLP preprocessing.
- Evidence anchors:
  - [section] "This new pipeline component, called 'detextor,' is implemented after the 'tagger' component, which assigns parts of speech to tokens."
  - [section] "The implementation of this pipeline component is done using the 'retokenizer' context manager and requires that dollar signs (and for best results, hyphens) be padded with spaces."
- Break condition: If mathematical notation becomes too complex or unconventional, the tokenization approach may fail to capture meaningful terms.

### Mechanism 3
- Claim: Organizing mathematical resources by concept enables better cross-referencing between natural and formal mathematics.
- Mechanism: The system creates a database where users can click on a term and see its definition across multiple resources (Wikipedia, course notes, theorem provers, translations), facilitating understanding across different mathematical contexts.
- Core assumption: Mathematicians need multiple perspectives on concepts to fully understand them, and formal theorem provers represent a distinct but complementary perspective.
- Evidence anchors:
  - [abstract] "The resulting database enables users to explore mathematical concepts across multiple resources including formal theorem provers, natural language definitions, and translations."
  - [section] "By putting them together, we can ensure common understanding across all levels of mathematics."
- Break condition: If the user interface doesn't effectively present the cross-referenced information or if the mappings between resources are incomplete, the benefit is diminished.

## Foundational Learning

- Concept: Knowledge Graph Construction
  - Why needed here: Understanding how to build and query knowledge graphs is essential for implementing the Wikidata mapping system.
  - Quick check question: What is the difference between a knowledge graph and a traditional relational database in terms of data representation?

- Concept: Natural Language Processing for Mathematical Text
  - Why needed here: The system relies on NLP techniques to extract terms from mathematical documents written in LaTeX and natural language.
  - Quick check question: How does spaCy's dependency parsing help identify mathematical terms in a sentence?

- Concept: Entity Linking and Disambiguation
  - Why needed here: The system must resolve when different terms refer to the same mathematical concept across multiple sources.
  - Quick check question: What challenges arise when mapping the term "group" to a Wikidata identifier given its multiple meanings in mathematics and everyday language?

## Architecture Onboarding

- Component map: Data collection -> Wikidata mapping (wikimapper + manual curation) -> NLP processing (spaCy + detextor) -> Database construction -> Web interface
- Critical path: The Wikidata mapping process, as it requires both automated processing and manual curation to ensure accuracy. Delays in mapping will bottleneck the entire system.
- Design tradeoffs: The system trades computational efficiency for accuracy by using manual curation to resolve disambiguation issues rather than relying solely on automated methods. This ensures higher quality mappings but requires more human effort.
- Failure signatures: Common failures include incorrect Wikidata mappings due to ambiguous terms, NLP processing errors when handling complex mathematical notation, and incomplete data when sources don't provide sufficient context for mapping.
- First 3 experiments:
  1. Test the wikimapper accuracy by running a small set of mathematical terms through it and manually verifying the mappings against known Wikidata entries.
  2. Validate the detextor pipeline component by processing sample LaTeX documents and checking if mathematical expressions are correctly tokenized.
  3. Test the end-to-end workflow by mapping a small corpus of terms and verifying that the resulting database allows users to navigate between different resource representations of the same concept.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for automatically mapping mathematical terms from diverse sources to Wikidata identifiers while minimizing disambiguation errors?
- Basis in paper: [explicit] The paper discusses challenges with wikimapper producing disambiguation pages and mentions using parenthetical subject names to reduce errors
- Why unresolved: The current approach of adding parenthetical subject names helps but still allows some disambiguation pages to slip through, and the paper notes this verification process needs improvement
- What evidence would resolve it: A comprehensive evaluation comparing different mapping strategies (e.g., contextual disambiguation, machine learning classifiers, or improved parsing of mathematical notation) against human-curated gold standards across multiple mathematical domains

### Open Question 2
- Question: How can natural language processing techniques be improved to extract and define mathematical concepts from unstructured mathematical text?
- Basis in paper: [explicit] The paper discusses using spaCy for term extraction and mentions that results "do not look impressive" and that the detextor pipeline only provides a partial solution
- Why unresolved: Current NLP approaches struggle with mathematical notation, produce incomplete definitions, and cannot reliably link extracted terms to their definitions in the source text
- What evidence would resolve it: Development and evaluation of NLP models specifically trained on mathematical corpora that can accurately identify definitions, extract precise mathematical concepts, and link terms to their formal definitions

### Open Question 3
- Question: What is the most effective way to integrate multiple theorem provers (Lean, Agda, Coq, Isabelle) into a unified knowledge graph of mathematical concepts?
- Basis in paper: [explicit] The paper mentions wanting to include more theorem provers and discusses the need for better cross-prover mappings at the Dagstuhl seminar
- Why unresolved: Different theorem provers use different formal languages and naming conventions, making direct mapping difficult without losing semantic precision
- What evidence would resolve it: A comparative study showing which integration approach (translation layers, common intermediate formal language, or machine learning-based alignment) best preserves mathematical meaning while enabling cross-prover navigation and concept discovery

## Limitations
- Manual curation requirements limit scalability of the Wikidata mapping process
- Reliance on Wikidata's coverage of mathematical concepts may miss specialized undergraduate topics
- NLP processing struggles with complex or unconventional mathematical notation

## Confidence
- Wikidata mapping accuracy: Medium (depends on Wikidata coverage and manual curation)
- NLP processing effectiveness: High (for standard notation), Medium (for complex expressions)
- User benefit realization: Medium (theoretical benefits not empirically validated)

## Next Checks
1. Conduct systematic evaluation of wikimapper accuracy for mathematical terminology by comparing automated mappings against expert-verified mappings across diverse mathematical domains.
2. Test detextor pipeline robustness by processing mathematical documents with varying notation complexity and measuring tokenization accuracy against ground truth annotations.
3. Implement small-scale user study where mathematicians use the system to navigate between natural language definitions and formal theorem prover representations, measuring both usability and comprehension improvement.