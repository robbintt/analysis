---
ver: rpa2
title: Wasserstein Diversity-Enriched Regularizer for Hierarchical Reinforcement Learning
arxiv_id: '2308.00989'
source_url: https://arxiv.org/abs/2308.00989
tags:
- uni00000013
- subpolicies
- learning
- uni00000048
- wasserstein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel regularizer called the Wasserstein Diversity-Enriched
  Regularizer (WDER) for hierarchical reinforcement learning. The key idea is to maximize
  the Wasserstein distances among action distributions to increase the diversity of
  subpolicies in HRL.
---

# Wasserstein Diversity-Enriched Regularizer for Hierarchical Reinforcement Learning

## Quick Facts
- arXiv ID: 2308.00989
- Source URL: https://arxiv.org/abs/2308.00989
- Reference count: 34
- The paper proposes WDER, a regularizer that maximizes Wasserstein distances among action distributions to increase subpolicy diversity in HRL.

## Executive Summary
This paper addresses the degradation problem in hierarchical reinforcement learning by proposing the Wasserstein Diversity-Enriched Regularizer (WDER). WDER maximizes the Wasserstein distances among subpolicy action distributions to increase their diversity, making it a task-agnostic solution that can be easily integrated into existing HRL methods. Experimental results demonstrate that WDER improves performance and sample efficiency compared to prior work without modifying hyperparameters.

## Method Summary
The method involves integrating WDER as a regularization term in the loss function of existing HRL frameworks like MLSH and OC. WDER computes the minimum Wasserstein distance between each subpolicy and all others, then adds this distance (scaled by hyperparameter α) to the loss function. This encourages each subpolicy to explore distinct action regions. The algorithm uses PPO as the backend RL algorithm and requires only the addition of α to control the strength of diversity regularization.

## Key Results
- On MovementBandits, WDER-MLSH outperforms the MLSH baseline by approximately 25% in average return using only 30% of all samples
- On complex robot tasks, WDER-OC achieves higher average returns than baselines like PPO, OC, and DEOC in three out of four Mujoco tasks
- The proposed WDER effectively increases subpolicy diversity and improves the performance of HRL methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Maximizing Wasserstein distances between subpolicy action distributions increases subpolicy diversity
- Mechanism: WDER computes the minimum Wasserstein distance between each subpolicy and all others, then adds this distance (scaled by hyperparameter α) as a regularization term in the loss function
- Core assumption: Action distributions fully capture policy behavior for diversity purposes
- Evidence anchors: Abstract mentions "enlarges the diversity of subpolicies by maximizing the Wasserstein distances among action distributions"
- Break condition: If action distributions do not adequately represent policy behavior

### Mechanism 2
- Claim: WDER provides smooth gradients for policy updates even when action distributions don't overlap
- Mechanism: Wasserstein distance provides geometry-aware topology with informative gradients regardless of distribution overlap
- Core assumption: Smooth gradients are essential for stable policy learning
- Evidence anchors: Abstract states "WD can provide smooth and informative gradients for updating parameters, regardless of whether the distributions of the two subpolicies overlap or not"
- Break condition: If the gradient smoothing property doesn't translate to actual policy learning stability

### Mechanism 3
- Claim: WDER can be easily integrated into existing HRL methods without hyperparameter modification
- Mechanism: WDER adds a single regularization term to the existing loss function
- Core assumption: The existing HRL method's training pipeline can accommodate an additional regularization term
- Evidence anchors: Abstract mentions "The proposed WDER can be easily incorporated into the loss function of existing methods"
- Break condition: If the existing method's training dynamics are too sensitive to the added regularization term

## Foundational Learning

- Concept: Wasserstein distance
  - Why needed here: It's the core metric used to measure and maximize diversity between subpolicy action distributions
  - Quick check question: What is the key advantage of Wasserstein distance over KL divergence when measuring distribution differences?

- Concept: Hierarchical reinforcement learning
  - Why needed here: The paper builds upon existing HRL frameworks (MLSH and option framework) and aims to improve them
  - Quick check question: What is the main challenge that WDER addresses in hierarchical RL?

- Concept: Policy gradient methods
  - Why needed here: WDER modifies the loss function used in policy gradient methods like PPO
  - Quick check question: How does adding a regularization term to the policy loss affect the gradient updates?

## Architecture Onboarding

- Component map: Master policy (selects subpolicies) -> Multiple subpolicy networks (each with actor and critic networks) -> WDER regularization module (computes Wasserstein distances)
- Critical path: Master policy selects subpolicy → Subpolicy executes actions and collects rewards → Experience stored → All subpolicies updated with PPO + WDER regularization
- Design tradeoffs: WDER adds computational overhead for distance computation but improves diversity; hyperparameter α controls diversity vs. performance tradeoff
- Failure signatures: Degraded subpolicy diversity (all policies converge to similar behavior), unstable training (due to gradient conflicts), computational bottleneck (distance computation)
- First 3 experiments:
  1. Verify WDER can be integrated into MLSH by reproducing the MovementBandits experiment
  2. Test WDER-OC on HalfCheetah to verify improved performance over baseline OC
  3. Vary α hyperparameter to find optimal diversity vs. performance balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise computational complexity of the WDER algorithm as the number of subpolicies increases?
- Basis in paper: The paper states that the distance computing time grows in the order of O(N^2) as the number of subpolicies N increases
- Why unresolved: The paper mentions that when N is large, the WD can be approximated by sliced or projected WD, but it does not provide specific computational complexity for these approximations
- What evidence would resolve it: Empirical studies comparing the computational complexity of WDER with different numbers of subpolicies, and theoretical analysis of the complexity of sliced or projected WD approximations

### Open Question 2
- Question: How does the choice of the hyperparameter α in the WDER affect the diversity and performance of the subpolicies?
- Basis in paper: The paper mentions that the value of α is set to 0.5 for MovementBandits and 0.2 for Mujoco tasks, but does not provide a systematic analysis of its impact
- Why unresolved: The paper does not provide a comprehensive study on the sensitivity of the algorithm to the choice of α
- What evidence would resolve it: A thorough ablation study varying α over a wide range of values and analyzing its effect on subpolicy diversity and overall performance

### Open Question 3
- Question: Can the WDER be extended to hierarchical reinforcement learning scenarios with more than two levels of hierarchy?
- Basis in paper: The paper focuses on a two-level hierarchical structure, but does not explicitly discuss the possibility of extending the method to deeper hierarchies
- Why unresolved: The paper does not provide any theoretical or empirical evidence regarding the applicability of WDER to multi-level hierarchies
- What evidence would resolve it: Experimental results demonstrating the effectiveness of WDER in scenarios with three or more levels of hierarchy, and theoretical analysis of how the algorithm would need to be adapted for such cases

## Limitations

- The paper's claims about Wasserstein distance providing superior gradient information compared to KL divergence lack direct empirical validation within this work
- The computational complexity of computing pairwise Wasserstein distances between all subpolicies is not discussed
- The ablation study focuses on comparing WDER with JS divergence but doesn't examine the impact of removing the diversity regularizer entirely

## Confidence

- **High**: The core mechanism of using Wasserstein distance as a diversity regularizer is well-defined and theoretically sound
- **Medium**: The experimental results showing improved performance over baselines are promising but limited in scope
- **Low**: Claims about smooth gradient properties and superior performance relative to all alternative diversity methods require more rigorous validation

## Next Checks

1. Conduct a computational complexity analysis comparing WDER with alternative diversity regularizers as the number of subpolicies scales
2. Perform an ablation study measuring the impact of the diversity regularizer when removed entirely, not just when replaced with JS divergence
3. Test WDER on additional HRL benchmarks beyond the two frameworks (MLSH and OC) to verify general applicability across different HRL architectures