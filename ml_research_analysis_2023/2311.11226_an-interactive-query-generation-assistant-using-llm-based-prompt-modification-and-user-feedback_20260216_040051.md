---
ver: rpa2
title: An Interactive Query Generation Assistant using LLM-based Prompt Modification
  and User Feedback
arxiv_id: '2311.11226'
source_url: https://arxiv.org/abs/2311.11226
tags:
- query
- search
- generation
- interface
- better
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The Query Generation Assistant interface addresses the challenge
  of formulating effective queries for information retrieval, particularly in multilingual
  and complex search scenarios. The interface offers three main features: a manual
  search tab that displays documents with translations and event annotations, an auto
  query generator that uses a fine-tuned T5 model to generate queries from example
  documents, and an interactive query generation tab that allows users to refine queries
  using LLM-based prompt modification and user feedback.'
---

# An Interactive Query Generation Assistant using LLM-based Prompt Modification and User Feedback

## Quick Facts
- arXiv ID: 2311.11226
- Source URL: https://arxiv.org/abs/2311.11226
- Authors:
- Reference count: 25
- Key outcome: Interactive query generation interface with LLM-based prompt modification and user feedback for multilingual search

## Executive Summary
The Query Generation Assistant interface addresses the challenge of formulating effective queries for information retrieval, particularly in multilingual and complex search scenarios. The interface offers three main features: a manual search tab that displays documents with translations and event annotations, an auto query generator that uses a fine-tuned T5 model to generate queries from example documents, and an interactive query generation tab that allows users to refine queries using LLM-based prompt modification and user feedback. The system was demonstrated on the IARPA BETTER dataset, which contains multilingual documents for event retrieval tasks.

## Method Summary
The Query Generation Assistant uses HuggingFace's Gradio platform to create an interactive interface for query generation. The system implements three main components: manual search using cross-lingual dense retrieval, auto query generation using a fine-tuned docT5query model, and interactive query generation using FlanT5 with prompt editing capabilities. Documents are preprocessed with Google Translate for translation and span-finder for event annotation. The interface allows users to provide feedback on retrieved documents and incorporate this feedback into subsequent query generations.

## Key Results
- docT5query model outperforms original T5 model for auto query generation
- Interface enables Human-in-the-Loop studies through interactive query refinement
- Supports multilingual document collections across Arabic, Persian, Chinese, Korean, and Russian

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interactive prompt modification improves query generation quality by incorporating user feedback and retrieval results.
- Mechanism: The system allows users to edit prompts and add retrieved documents to the prompt, which provides additional context and relevance feedback to the LLM. This iterative process helps refine the query to better match the user's information need.
- Core assumption: The LLM can effectively incorporate new examples and feedback into the prompt to generate more relevant queries.
- Evidence anchors:
  - [abstract]: "the proposed assistive interface enables the users to refine the queries generated by different LLMs, to provide feedback on the retrieved documents or passages, and is able to incorporate the users' feedback as prompts to generate more effective queries."
  - [section 3.3]: "We use this feature to our advantage for query generation by letting people edit their prompts either directly or through user relevance feedback so as to improve subsequent query generations and corresponding retrieval."
- Break condition: If the LLM is unable to effectively incorporate the additional context or if user feedback is inconsistent or noisy, the quality of generated queries may not improve.

### Mechanism 2
- Claim: Fine-tuning a T5 model on (document, query) pairs improves query generation compared to the original T5 model.
- Mechanism: By training the T5 model on a dataset of documents and their corresponding queries, the model learns to generate queries that are more likely to retrieve relevant documents for a given example document.
- Core assumption: The fine-tuning dataset contains representative examples of effective query-document pairs.
- Evidence anchors:
  - [section 3.2]: "We fine-tune a T5 [15] model on (document, query) pairs. To evaluate the performance of our approach, we compare the original T5 model with a docT5query [14] model... Our results indicate that the docT5query model outperforms the original T5 model, and thus we utilize it for our demonstration."
- Break condition: If the fine-tuning dataset is not representative or if the model overfits to the training data, the performance gains may not generalize to new examples.

### Mechanism 3
- Claim: Providing document translations and event annotations helps users navigate and analyze search results, especially in multilingual contexts.
- Mechanism: The system displays documents in their original language along with English translations, and highlights event annotations. This allows users to understand the content and context of the documents, even if they are not proficient in the document's language.
- Core assumption: Users benefit from having access to both the original document and its translation, as well as event annotations.
- Evidence anchors:
  - [abstract]: "The interface provides a simple document search interface that displays documents in their original language along with their translations, making it simple for researchers to navigate and analyze search results."
  - [section 3.1]: "The search is conducted on each index built using their corresponding language's documents... All the documents are translated offline using Google Translate for faster look-up during query time. To highlight the event annotations... the collection is parsed to a SOTA event annotator (span-finder) offline and then looked-up during the query time."
- Break condition: If the translations are inaccurate or if the event annotations are not relevant to the user's information need, the added context may not be helpful.

## Foundational Learning

- Concept: Query-by-example (QBE) information retrieval
  - Why needed here: The system is designed to help users find relevant documents by providing example documents instead of explicit queries, which is a key aspect of QBE.
  - Quick check question: What is the main advantage of using QBE over traditional keyword-based search?
- Concept: Cross-lingual information retrieval
  - Why needed here: The system supports multilingual document collections and uses cross-lingual dense retrieval models to find relevant documents across languages.
  - Quick check question: How does cross-lingual dense retrieval differ from traditional cross-lingual retrieval methods?
- Concept: Prompt engineering and few-shot learning with LLMs
  - Why needed here: The system uses LLM-based prompt modification to improve query generation, which requires understanding how to effectively engineer prompts and leverage few-shot learning capabilities of LLMs.
  - Quick check question: What are some strategies for designing effective prompts for few-shot learning with LLMs?

## Architecture Onboarding

- Component map: User Interface (Gradio) -> Query Generation (docT5query/FlanT5) -> Document Retrieval (ColBERT-x) -> Document Preprocessing (Translation + Annotation)
- Critical path: User provides example document → System generates query using fine-tuned T5 → Queries are executed on document indices → Retrieved documents are displayed with translations and annotations → User can refine queries through prompt editing and feedback incorporation
- Design tradeoffs:
  - Using offline translation and annotation to improve runtime performance vs. potential staleness of translations
  - Balancing the number of examples in the prompt for few-shot learning vs. risk of overfitting or prompt dilution
  - Providing a simple interface for users vs. offering more advanced customization options
- Failure signatures:
  - Low precision in retrieved documents despite high recall
  - User feedback not leading to improved query generations
  - System performance degradation with large document collections
- First 3 experiments:
  1. Test the end-to-end query generation and retrieval pipeline with a small, controlled dataset to verify basic functionality.
  2. Evaluate the impact of prompt editing and feedback incorporation on query generation quality using a diverse set of example documents.
  3. Measure the system's performance and user satisfaction with the interface on a real-world multilingual document collection, comparing it to traditional search methods.

## Open Questions the Paper Calls Out

- How does the performance of the Query Generation Assistant compare to human query formulation in complex multilingual search tasks?
- What is the impact of incorporating user feedback on the quality of generated queries over multiple iterations?
- How does the Query Generation Assistant perform in terms of query interpretability and user satisfaction compared to traditional query-by-example methods?

## Limitations

- Evaluation is primarily qualitative through demonstrations rather than quantitative metrics
- System performance depends on quality of translation services and event annotation tools
- Effectiveness of LLM-based prompt modification and feedback mechanisms not rigorously validated

## Confidence

- High Confidence: Technical implementation using Gradio platform, integration of fine-tuned docT5query model, and use of FlanT5 for interactive prompt modification
- Medium Confidence: Claim that docT5query outperforms original T5 model, though specific evaluation metrics not detailed
- Low Confidence: Effectiveness of LLM-based prompt modification and user feedback mechanisms in improving query generation quality

## Next Checks

1. Design and conduct a controlled experiment to measure the impact of prompt editing and feedback incorporation on query generation quality, using standard IR evaluation metrics such as precision, recall, and mean average precision (MAP).

2. Evaluate the system's performance and user satisfaction on a diverse set of multilingual document collections, testing its ability to handle different language pairs and document types beyond the BETTER dataset.

3. Conduct a user study comparing the Query Generation Assistant to traditional keyword-based search and other state-of-the-art query generation methods, measuring factors such as task completion time, user satisfaction, and retrieval effectiveness.