---
ver: rpa2
title: Soft Frequency Capping for Improved Ad Click Prediction in Yahoo Gemini Native
arxiv_id: '2312.05052'
source_url: https://arxiv.org/abs/2312.05052
tags:
- frequency
- user
- feature
- offset
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Problem addressed: User ad fatigue causes dramatic drop in click-through
  rate (CTR) as users repeatedly view the same ads. Core method idea: Incorporated
  frequency feature into OFFSET click prediction model as a user-ad feature, learned
  via logistic regression as part of OFFSET training, replacing rule-based hard frequency
  capping.'
---

# Soft Frequency Capping for Improved Ad Click Prediction in Yahoo Gemini Native

## Quick Facts
- arXiv ID: 2312.05052
- Source URL: https://arxiv.org/abs/2312.05052
- Reference count: 23
- Primary result: 7.3% revenue lift from bucket testing, translating to millions in additional revenue yearly

## Executive Summary
This paper addresses user ad fatigue in native advertising by incorporating frequency features into the OFFSET click prediction model. Rather than using rule-based hard frequency capping that removes ads after a certain number of views, the authors propose learning frequency weights as part of the OFFSET training process via logistic regression. The approach captures how click-through rates (CTR) decay with repeated ad exposures and adjusts predictions accordingly.

The method was evaluated through online bucket testing at Yahoo's scale, showing a 7.3% revenue lift compared to the production model. By replacing explicit filtering rules with learned frequency weights, the model naturally deprioritizes fatigued impressions in the auction while preserving potentially valuable first-view opportunities. The work demonstrates how incorporating behavioral signals like ad frequency can significantly improve both prediction accuracy and business outcomes.

## Method Summary
The paper proposes incorporating frequency features into the OFFSET click prediction model as a user-ad feature learned via logistic regression during OFFSET training. The frequency feature counts how many times a user has seen a specific ad (creative, campaign, or advertiser) within a predefined time window. This frequency count is binned into discrete categories, and each bin has an associated weight vector learned during training. During serving, the appropriate weight is selected based on the frequency bin and added to the OFFSET score, replacing rule-based hard frequency capping with learned frequency decay patterns.

## Key Results
- 7.3% revenue lift in online bucket testing, translating to millions in additional revenue yearly
- 1.02% LogLoss lift and 0.83% sAUC lift in offline evaluation
- Normalized CTR drops by ~20% after first repeat view and ~50% after 7 views
- Replaces rule-based hard frequency capping with learned frequency weights

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The frequency feature reduces predicted CTR for repeated ad views by user.
- Mechanism: During training, the model learns a weight vector where each entry corresponds to a frequency bin. The weight for higher frequency bins is negative, directly lowering the score $s'_{u,a} = s_{u,a} + w_i$ where $i$ is the bin index for frequency $f_{u,a}$.
- Core assumption: The logistic loss surface allows the SGD to assign negative weights to high frequency bins because repeated views correlate with lower CTR.
- Evidence anchors:
  - [abstract] "the frequency weight vector and therefore the resulting pCTR decrease with frequency, expressing the user fatigue of viewing the same ads repeatedly."
  - [section 5] Shows normalized CTR drops by ~20% after first repeat view and ~50% after 7 views.
  - [corpus] Weak or missing corpus evidence; no direct neighbor papers mention frequency-based CTR decay in native ads.
- Break condition: If frequency bins are too coarse, the model may not capture subtle CTR changes; if data is sparse for high frequency bins, weights may be unreliable.

### Mechanism 2
- Claim: Learning frequency weights within OFFSET removes the need for rule-based hard frequency capping.
- Mechanism: Instead of filtering out high-frequency ads during serving, the model outputs lower CTR scores for them, allowing the auction to naturally deprioritize these impressions without explicit rules.
- Core assumption: The auction ranking function $bid \cdot pCTR$ is monotonic in pCTR, so lower predicted CTR leads to lower ranking and fewer impressions.
- Evidence anchors:
  - [section 2.3] Explains hard frequency capping removes ads seen >5 times/week or >2 times/day.
  - [abstract] States SFC replaces rule-based hard frequency capping.
  - [corpus] No neighbor paper explicitly discusses auction ranking changes from learned frequency decay.
- Break condition: If bid values dominate pCTR differences, frequency-based score drops may not sufficiently reduce impressions.

### Mechanism 3
- Claim: Frequency weights improve first-view CTR predictions by correcting systematic under-prediction.
- Mechanism: Without frequency modeling, OFFSET must average CTR across all views, depressing scores for first views. Adding frequency weights allows first-view pCTR to rise toward true first-view CTR.
- Core assumption: First-view CTR is higher than average CTR across all views, so adding a negative frequency bias increases first-view scores relative to repeated views.
- Evidence anchors:
  - [section 7] Explains that models ignoring frequency "tend towards an average of the CTR on first and repeated impressions."
  - [section 8.1] Reports 1.02% LogLoss lift and 0.83% sAUC lift after adding frequency weights.
  - [corpus] No neighbor papers explicitly discuss first-view vs. repeated-view CTR bias correction.
- Break condition: If frequency distribution is heavily skewed toward high counts, first-view samples may be too few to learn accurate weights.

## Foundational Learning

- Concept: Stochastic Gradient Descent with adaptive step sizes (AdaGrad variant)
  - Why needed here: OFFSET updates model parameters online per mini-batch; adaptive step sizes help convergence when gradients vary across features.
  - Quick check question: What happens to the learning rate for a parameter if its gradient magnitudes are consistently large across updates?

- Concept: Feature-enhanced collaborative filtering with user-less representation
  - Why needed here: Users are represented by feature vectors (age, gender, geo) rather than individual IDs to handle sparsity in ad click data.
  - Quick check question: How does the model compute a user's latent factor vector from their feature vectors?

- Concept: Binning continuous frequency values into discrete categories
  - Why needed here: Allows learning separate weights per frequency range without assuming a functional form; handles non-linear CTR decay.
  - Quick check question: Why might a linear regression on raw frequency values perform worse than binned weights?

## Architecture Onboarding

- Component map: Data pipeline (logs → feature extraction → OFFSET training) -> OFFSET core (logistic loss minimization with SGD, frequency weight vector update) -> Serving layer (computes pCTR including frequency weight lookup during auction) -> Evaluation (offline LogLoss/sAUC, online CPM lift metrics)

- Critical path: 1. Impression logged with user features, ad features, click label 2. Frequency feature computed from historical activity within time window 3. Frequency bin index determined, corresponding weight added to score 4. Model updates via SGD, weight vector adjusted 5. At serving, same frequency calculation used to adjust real-time score

- Design tradeoffs:
  - Bin granularity vs. data sparsity: more bins → finer CTR modeling but fewer samples per bin
  - Time window length: longer windows → more stable frequency counts but slower adaptation to user fatigue changes
  - Global vs. per-campaign weights: global reduces sparsity but may miss vertical-specific patterns

- Failure signatures:
  - Sudden drop in offline LogLoss lift after deployment → frequency weight learning diverged
  - No online CPM improvement despite offline gains → serving layer not applying weights correctly
  - High variance in frequency weight estimates → insufficient data in certain bins

- First 3 experiments:
  1. Train OFFSET with frequency binning on a small historical dataset, inspect learned weight vector shape.
  2. Deploy to shadow traffic, compare predicted CTR distribution for first vs. repeated views.
  3. Run A/B test with small traffic split, measure CPM lift and frequency distribution changes.

## Open Questions the Paper Calls Out

- Does user ad fatigue increase with ad frequency indefinitely, or does it plateau at some point? The paper shows CTR decreases with frequency but doesn't analyze long-term trends beyond 50 views. Longitudinal study tracking CTR across many more ad views (100+) would resolve this.

- How does the frequency weight vector behave for extremely high frequency values (v >> 25)? The paper mentions under-prediction for frequencies ≥25 due to binning limitations. Analysis of CTR patterns for extremely frequent ad viewers (v>50) would provide answers.

- Would a hierarchical structure for frequency weight vectors improve prediction accuracy compared to the current global approach? The authors mention considering hierarchical structures to address sparsity issues. A/B testing comparing hierarchical vs. global frequency weight structures would resolve this.

- How do recency effects (time since last ad view) interact with frequency in determining CTR? The authors mention incorporating recency features as future work. Analysis of CTR patterns controlling for both frequency and recency would provide insights.

## Limitations
- Evaluation relies entirely on bucket testing at Yahoo's scale, limiting external validation
- No ablation studies to isolate frequency feature's contribution from other OFFSET improvements
- Binning approach assumes CTR decay follows discrete patterns without sensitivity analysis on bin granularity
- Global frequency weights may not capture campaign-specific fatigue patterns

## Confidence
- **High Confidence**: The core observation that repeated ad views reduce CTR (20% drop after first repeat, 50% after 7 views)
- **Medium Confidence**: The 7.3% revenue lift from bucket testing, as it's the primary quantitative result but lacks independent replication
- **Medium Confidence**: The mechanism that learning frequency weights within OFFSET replaces hard frequency capping
- **Low Confidence**: That frequency weights improve first-view CTR predictions by correcting systematic under-prediction

## Next Checks
1. **Ablation study on frequency feature importance**: Remove the frequency weight component from the trained model and measure the degradation in LogLoss and offline CTR prediction accuracy to quantify the feature's marginal contribution.

2. **Cross-vertical performance analysis**: Segment the evaluation data by ad vertical (e.g., retail vs. finance) and measure whether the frequency weight patterns and revenue lift vary significantly across different campaign types.

3. **Bin granularity sensitivity test**: Re-train the model with different binning strategies (fewer, wider bins vs. more, narrower bins) and measure the impact on both offline metrics and simulated online performance to identify optimal frequency representation.