---
ver: rpa2
title: Read, Look or Listen? What's Needed for Solving a Multimodal Dataset
arxiv_id: '2307.04532'
source_url: https://arxiv.org/abs/2307.04532
tags:
- modality
- modalities
- questions
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method to analyze multimodal datasets
  by mapping each instance to the required modalities for processing. The approach
  uses a small seed of human annotations and expands them to the full dataset using
  classifiers.
---

# Read, Look or Listen? What's Needed for Solving a Multimodal Dataset

## Quick Facts
- **arXiv ID**: 2307.04532
- **Source URL**: https://arxiv.org/abs/2307.04532
- **Reference count**: 40
- **Primary result**: TVQA questions can be solved using a single modality 99% of the time, but over 70% are solvable using two or more modalities separately

## Executive Summary
This paper introduces a novel method to analyze multimodal datasets by mapping each instance to the required modalities for processing. The approach uses a small seed of human annotations and expands them to the full dataset using classifiers. Applied to the TVQA dataset, the method reveals that 99% of questions can be solved using a single modality, with no substantial bias towards any specific modality. However, over 70% of questions are solvable using two or more modalities separately. Analysis of the MERLOT Reserve model shows it struggles with image-based questions and auditory speaker identification. Based on these findings, a new test set requiring multiple modalities is introduced, demonstrating a dramatic drop in model performance. This methodology provides valuable insights into multimodal datasets and highlights the need for more robust models.

## Method Summary
The paper proposes a two-step annotation methodology to analyze multimodal datasets. First, human annotators label a subset of instances (seed annotation) with the modalities required to answer each question. Second, random forest classifiers are trained using features extracted from a fine-tuned MERLOT Reserve model, which processes each modality subset separately and outputs softmax probabilities. These classifiers are then applied to the full dataset to map each instance to its required modalities. The method is validated on the TVQA dataset, revealing that questions are predominantly solvable using single modalities despite being multimodal in nature.

## Key Results
- 99% of TVQA questions can be solved using a single modality
- Over 70% of questions are solvable using two or more modalities separately
- MERLOT Reserve shows poor performance on image-based questions and auditory speaker identification
- Performance drops dramatically (41% vs 83% accuracy) on a new test set requiring multiple modalities
- No substantial bias towards any specific modality across the dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Human annotations serve as a high-precision anchor that classifiers can extrapolate to the full dataset
- **Mechanism**: Annotators mark which modality subsets solve each question. Classifiers trained on these silver labels then label the full dataset, creating a scalable mapping without re-annotating everything
- **Core assumption**: A small seed (e.g., ~500 instances) is representative enough for classifier generalization to the entire set
- **Evidence anchors**:
  - [abstract] "We propose a two-step method to analyze multimodal datasets, which leverages a small seed of human annotation to map each multimodal instance to the modalities required to process it."
  - [section] "We sample a subset of the dataset, to be used for our seed annotation. We present human annotators with different subsets of modalities for each instance, recording their responses."
  - [corpus] FMR = 0.55, 0 cited-by; limited corroboration
- **Break condition**: If seed annotation is not representative (e.g., biased toward certain question types), classifiers will propagate that bias to full dataset

### Mechanism 2
- **Claim**: Model softmax probabilities from modality-masked inputs provide discriminative features for modality importance
- **Mechanism**: A fine-tuned MERLOT Reserve generates softmax outputs for each possible modality subset. Concatenating these probability vectors yields a high-dimensional signature distinguishing which modalities a question depends on
- **Core assumption**: Softmax outputs from a strong pretrained model carry modality-specific signal beyond raw features
- **Evidence anchors**:
  - [abstract] "We apply the trained model on the masked instances and extract the softmax layers’ outputs obtained from each modality subset."
  - [section] "We then train a random forest [Ho, 1995] classifier for each modality, using these input features, and the annotated labels collected above."
  - [corpus] FMR = 0.55, no direct citations; weak corroboration
- **Break condition**: If MERLOT Reserve is weak on some modality, softmax scores will be unreliable for that modality's classifier

### Mechanism 3
- **Claim**: Performance drops on multi-modal test sets reveal model weaknesses in modality integration
- **Mechanism**: A new test set of questions requiring multiple modalities is constructed. MERLOT Reserve's performance on this set is dramatically lower than on the original validation set, indicating poor integration across modalities
- **Core assumption**: Single-modality performance being high masks inability to integrate multiple modalities
- **Evidence anchors**:
  - [abstract] "Based on our observations, we introduce a new test set that necessitates multiple modalities, observing a dramatic drop in model performance."
  - [section] "We find that MERLOT Reserve performs dramatically worse on these questions compared to the original validation set (41% vs. 83% accuracy), suggesting that it struggles with questions that require multiple modalities."
  - [corpus] FMR = 0.55, 0 citations; weak corroboration
- **Break condition**: If new test set questions are not truly multimodal (e.g., still solvable with one modality), the drop is invalid

## Foundational Learning

- **Concept**: Multimodal dataset structure and the role of each modality
  - Why needed here: Understanding how TVQA combines video, audio, and text is essential to designing correct masking experiments and interpreting classifier results
  - Quick check question: In TVQA, which modality contains character dialogue text?

- **Concept**: Random Forest classifier behavior and hyperparameter sensitivity
  - Why needed here: The paper trains three separate RF classifiers, one per modality; understanding their training dynamics is key to interpreting accuracy differences
  - Quick check question: What is the effect of using majority baseline vs. trained classifier in Table 1?

- **Concept**: Training dynamics (variance of softmax outputs) for identifying hard examples
  - Why needed here: The paper uses training dynamics to highlight that image-based questions are more ambiguous for MERLOT Reserve
  - Quick check question: How does high variance in softmax probabilities relate to question difficulty?

## Architecture Onboarding

- **Component map**: Human annotation pipeline → RF classifiers per modality → MERLOT Reserve inference with masking → Multi-modal test set creation
- **Critical path**: Annotate seed → Train RF classifiers → Apply to full dataset → Evaluate MERLOT Reserve under modality masks → Construct multi-modal test set
- **Design tradeoffs**: Using MERLOT Reserve both for feature extraction and as the base model creates tight coupling but ensures feature consistency; alternatively, a separate model could be used for features
- **Failure signatures**: Low classifier accuracy on out-of-domain data indicates seed annotation bias; large performance gaps between modalities suggest modality-specific model weaknesses; poor multi-modal test performance indicates integration issues
- **First 3 experiments**:
  1. Run the three classifiers on the annotated validation set to verify accuracy meets the 74-81% range
  2. Mask each modality in MERLOT Reserve and measure performance on questions answerable by each single modality
  3. Create a small pilot multi-modal test set (e.g., 20 questions) and evaluate MERLOT Reserve performance compared to the original validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MERLOT Reserve on multimodal questions compare to other state-of-the-art multimodal models?
- Basis in paper: [inferred] The paper shows that MERLOT Reserve struggles with multimodal questions, but does not compare its performance to other models.
- Why unresolved: The paper only analyzes MERLOT Reserve and does not provide a comparison with other models.
- What evidence would resolve it: Benchmarking MERLOT Reserve against other state-of-the-art multimodal models on the new test set of multimodal questions.

### Open Question 2
- Question: What are the specific challenges that models face when answering multimodal questions that require multiple modalities?
- Basis in paper: [explicit] The paper mentions that MERLOT Reserve struggles with multimodal questions but does not delve into the specific challenges.
- Why unresolved: The paper does not provide a detailed analysis of the challenges models face when answering multimodal questions.
- What evidence would resolve it: A detailed error analysis of model predictions on multimodal questions to identify specific failure modes.

### Open Question 3
- Question: How can the dataset creation process be improved to ensure a more balanced representation of modalities in the questions?
- Basis in paper: [explicit] The paper notes that TVQA has a limited integration of multiple modalities in its questions.
- Why unresolved: The paper does not provide specific strategies for improving the dataset creation process.
- What evidence would resolve it: Experiments with different data collection methods or guidelines to create a more balanced dataset with questions requiring multiple modalities.

## Limitations

- The methodology relies heavily on a small human-annotated seed (unspecified size) to extrapolate modality mappings to the full dataset
- The dramatic performance drop on the new multi-modal test set is compelling but the construction methodology is not fully detailed
- Lack of inter-annotator agreement statistics makes it difficult to assess annotation quality

## Confidence

**High Confidence**: The finding that 99% of TVQA questions can be solved using a single modality is well-supported by the experimental design and classifier results.

**Medium Confidence**: The claim that over 70% of questions are solvable using two or more modalities separately is supported but depends on the accuracy of the extrapolation from the seed annotations.

**Medium Confidence**: The observation that MERLOT Reserve struggles with image-based questions and auditory speaker identification is based on training dynamics analysis, which is indirect evidence.

## Next Checks

1. **Inter-annotator agreement analysis**: Calculate and report the agreement between human annotators on the seed dataset to establish annotation reliability as a foundation for classifier training.

2. **Classifier calibration validation**: Test the three RF classifiers on a held-out subset of the human-annotated data to verify the 74-81% accuracy range and assess whether this is sufficient for reliable full-dataset extrapolation.

3. **Multi-modal test set verification**: Manually verify a sample of questions from the new multi-modal test set to confirm they genuinely require multiple modalities for solution, rather than being artificially difficult or still solvable with single modalities.