---
ver: rpa2
title: GPT-4 Vision on Medical Image Classification -- A Case Study on COVID-19 Dataset
arxiv_id: '2310.18498'
source_url: https://arxiv.org/abs/2310.18498
tags:
- image
- learning
- images
- medical
- classi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the application of GPT-4V for COVID-19
  image classification using in-context learning. The authors explore various prompting
  strategies, including naive zero-shot prompting, in-context learning with examples,
  and in-context learning with reasoning explanations.
---

# GPT-4 Vision on Medical Image Classification -- A Case Study on COVID-19 Dataset

## Quick Facts
- arXiv ID: 2310.18498
- Source URL: https://arxiv.org/abs/2310.18498
- Reference count: 30
- One-line primary result: GPT-4V achieves comparable accuracy to traditional models when provided with equivalent examples for COVID-19 image classification

## Executive Summary
This paper investigates GPT-4V's capability for medical image classification using COVID-19 lung X-ray images through in-context learning. The authors compare six prompting strategies, ranging from naive zero-shot prompting to in-context learning with reasoning explanations, against traditional ResNet and VGG models. Results demonstrate that GPT-4V can achieve classification accuracy comparable to few-shot baselines when provided with an equivalent number of examples, with optimal performance at 9 images per prompt. However, adding reasoning explanations did not improve results, highlighting the importance of prompt engineering in this application domain.

## Method Summary
The study uses a Kaggle COVID-19 lung X-ray dataset with 181 training images (111 COVID, 70 normal) and 46 test images (26 COVID, 20 normal). Six prompting strategies are tested: naive zero-shot, ICL1 (3 images), ICL2 (3 images in one figure), ICL3 (9 images in one figure), ICL4 (9 images in one figure), and ICL-R1/2 (reasoning-enhanced versions). Performance is evaluated using precision, recall, F1-score, and accuracy metrics for both COVID and normal classes, with results compared against traditional ResNet and VGG baselines trained on equivalent amounts of data.

## Key Results
- GPT-4V achieves comparable accuracy to few-shot baselines when provided with equivalent examples
- Best performance (85% accuracy) achieved with ICL4 using 9 images per prompt
- Adding reasoning explanations did not improve classification results
- GPT-4V's performance approaches but does not match fully-trained baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4V achieves comparable accuracy to traditional models when provided with equivalent examples
- Mechanism: In-context learning allows GPT-4V to leverage its pre-trained knowledge base without requiring task-specific training
- Core assumption: The model's pre-existing knowledge contains sufficient patterns to recognize COVID-19 features in X-ray images
- Evidence anchors: [abstract] "Results show that GPT-4V achieves comparable accuracy to the few-shot baselines when provided with an equivalent number of examples"; [section 3] "The effectiveness of ICL is rooted in a phenomenon known as 'few-shot learning'"

### Mechanism 2
- Claim: Consolidating multiple images into a single figure improves performance
- Mechanism: GPT-4V's attention mechanism can better process comparative information when images are presented together rather than individually
- Core assumption: The model can effectively distribute attention across multiple images when presented in a single context
- Evidence anchors: [section 4] "Consolidating all images into a single figure has demonstrated enhanced performance compared to uploading them individually"

### Mechanism 3
- Claim: Adding reasoning explanations does not improve results
- Mechanism: The reasoning prompts may be misaligned with how GPT-4V processes visual information
- Core assumption: GPT-4V's decision-making process does not benefit from explicit reasoning instructions
- Evidence anchors: [section 4] "Contrary to expectations, supplementing the GPT-4V prompts with reasons underlying the classifications does not yield an improvement in results"

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: Allows leveraging GPT-4V's pre-trained knowledge without extensive fine-tuning on medical data
  - Quick check question: How does in-context learning differ from traditional fine-tuning in terms of data requirements?

- Concept: Few-shot learning
  - Why needed here: Medical datasets are often limited in size, making few-shot approaches essential
  - Quick check question: What is the minimum number of examples needed for GPT-4V to achieve reasonable performance?

- Concept: Prompt engineering
  - Why needed here: The quality and structure of prompts directly impact GPT-4V's performance in medical image classification
  - Quick check question: How do different prompt formats (zero-shot, ICL1, ICL2, ICL4) affect classification accuracy?

## Architecture Onboarding

- Component map: Input (medical images) -> GPT-4V Vision model with in-context learning -> Classification label (COVID-19 or normal) -> Evaluation metrics (accuracy, precision, recall, F1-score)

- Critical path: 1. Prepare image dataset with COVID-19 and normal cases; 2. Design prompt templates for different ICL strategies; 3. Upload images and run inference with GPT-4V; 4. Collect and analyze classification results; 5. Compare performance against baseline models

- Design tradeoffs: Number of examples per prompt vs. computational cost; Image resolution vs. prompt size limitations; Simple prompts vs. complex reasoning instructions; Single image upload vs. combined figure approach

- Failure signatures: Random or inconsistent classifications; Over-reliance on certain visual features; Poor performance compared to baselines despite equivalent examples; No improvement from reasoning explanations

- First 3 experiments: 1. Test zero-shot performance to establish baseline; 2. Implement ICL1 (3 images, 2 for context) to validate in-context learning; 3. Try ICL4 (9 images, 6 for context) to determine optimal example count

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does adding reasoning explanations to prompts not improve GPT-4V's classification performance?
- Basis in paper: [explicit] The paper states that "supplementing the GPT-4V prompts with reasons underlying the classifications does not yield an improvement in results" and suggests this may be due to "a misalignment between the provided reasoning and the model's processing capabilities"
- Why unresolved: The exact nature of this misalignment is unclear - whether it's due to the reasoning being poorly formulated for GPT-4V's comprehension, or the model lacking capacity to incorporate such reasoning effectively into its decision-making process.
- What evidence would resolve it: Systematic testing of different reasoning formats and depths, along with ablation studies isolating reasoning components from other prompt elements.

### Open Question 2
- Question: What is the optimal number of examples for in-context learning with GPT-4V on medical image classification?
- Basis in paper: [explicit] The paper tested ICL1 (3 images), ICL2 (3 images in one figure), ICL3 (9 images in one figure), and ICL4 (9 images in one figure), but notes that "certain optimizations might be necessary to fully realize its learning potential"
- Why unresolved: The study only tested a limited range of example quantities (3-9 images), leaving uncertainty about whether performance could be improved with more or fewer examples.
- What evidence would resolve it: Testing with varying numbers of examples (1, 2, 4, 6, 12, etc.) while keeping other factors constant.

### Open Question 3
- Question: Can GPT-4V achieve performance comparable to fully-trained models when given the same amount of labeled data?
- Basis in paper: [explicit] The paper shows GPT-4V achieves "comparable accuracy to established baseline models" when provided with an equivalent number of examples, but "does not yet match the efficacy of the comprehensive baseline model that benefits from training on the complete set of examples"
- Why unresolved: The study only tested 6-shot learning for baselines but didn't compare GPT-4V's few-shot performance to other few-shot learning methods on the same dataset.
- What evidence would resolve it: Direct comparison of GPT-4V's few-shot performance against few-shot versions of ResNet and VGG trained with the same number of labeled examples.

## Limitations

- Small dataset size (227 total images) limits generalizability of findings
- Focus on single disease (COVID-19) restricts applicability to other medical imaging domains
- Lack of corpus evidence supporting key mechanisms, particularly regarding attention behavior

## Confidence

- High confidence: GPT-4V can perform medical image classification through in-context learning with accuracy comparable to traditional models when provided with equivalent examples
- Medium confidence: Specific performance metrics and relative effectiveness of different prompting strategies
- Low confidence: Theoretical explanations for why certain approaches work or fail, as these are primarily based on observed behavior rather than validated mechanisms

## Next Checks

1. Validate the image consolidation mechanism by testing GPT-4V's attention distribution across individual vs. combined images using attention visualization tools
2. Conduct experiments across multiple medical imaging domains (not just COVID-19) to assess generalizability of findings
3. Implement ablation studies on reasoning explanations using different formats and instruction styles to better understand the mechanism behind their lack of effectiveness