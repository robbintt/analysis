---
ver: rpa2
title: 'When Meta-Learning Meets Online and Continual Learning: A Survey'
arxiv_id: '2311.05241'
source_url: https://arxiv.org/abs/2311.05241
tags:
- learning
- meta-learning
- continual
- training
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of the intersections
  among online learning, continual learning, and meta-learning. It proposes a unified
  taxonomy for learning frameworks, categorizing them into eight major branches based
  on problem settings: offline learning, online learning, continual learning, meta-learning,
  meta-online learning, meta-continual learning, online meta-learning, and continual
  meta-learning.'
---

# When Meta-Learning Meets Online and Continual Learning: A Survey

## Quick Facts
- arXiv ID: 2311.05241
- Source URL: https://arxiv.org/abs/2311.05241
- Reference count: 40
- One-line primary result: Comprehensive survey categorizing learning frameworks into eight branches based on their problem settings

## Executive Summary
This paper provides a comprehensive survey of the intersections among online learning, continual learning, and meta-learning frameworks. It proposes a unified taxonomy that categorizes learning paradigms into eight major branches: offline learning, online learning, continual learning, meta-learning, meta-online learning, meta-continual learning, online meta-learning, and continual meta-learning. For each framework, the paper offers formal definitions and surveys relevant papers, organizing them based on their methodological characteristics. The survey highlights synergies and challenges of combining these learning paradigms, providing insights into their potential applications and future research directions.

## Method Summary
The survey employs a systematic approach to unify the notation and terminology across different learning frameworks. It establishes formal definitions for each of the eight learning branches, distinguishing them based on problem settings, data availability, and learning objectives. The paper surveys existing literature by categorizing papers according to their methodological characteristics, including stochastic gradient descent, sequential Bayesian updates, sequence modeling, and various initialization strategies. It also identifies key challenges such as scalability, memory efficiency, and the trade-off between task-awareness and generality.

## Key Results
- Unified taxonomy categorizing learning frameworks into eight branches based on problem settings
- Comprehensive survey of methodological approaches including unitary, mixture, and compositional initialization strategies
- Identification of key challenges in combining meta-learning with online and continual learning paradigms
- Analysis of failure modes including catastrophic forgetting, meta-overfitting, and scalability issues
- Proposed future research directions for improving scalability and task-agnostic learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unitary initialization in MAML-based OML/CML provides a single meta-learned starting point that can adapt quickly to any episode type
- Mechanism: A single initial parameter vector θ0 is meta-learned across all episodes. During each episode, θ0 is updated via SGD to produce an episode-specific parameter θu
- Core assumption: A single initialization point can capture sufficient generality to adapt to diverse task distributions
- Evidence anchors:
  - [abstract] "While these learning frameworks have undergone extensive research individually, there has been a recent surge of interest in their synergistic combination"
  - [section] "FTML [90] applies the same fundamental principle within the framework of MAML"
  - [corpus] Weak - no direct mention of unitary initialization in neighbor papers
- Break condition: When task distributions are too diverse for a single initialization to capture the necessary adaptation dynamics, leading to poor meta-generalization

### Mechanism 2
- Claim: Mixture of initializations provides specialized starting points for different clusters of episodes, improving adaptation efficiency
- Mechanism: Multiple initialization vectors {θ0_l} are maintained, with each episode selecting the best-matching initialization based on similarity to its task characteristics
- Core assumption: Episodes can be meaningfully clustered into distinct groups with shared characteristics
- Evidence anchors:
  - [section] "In contrast to the single initialization point in the unitary initialization θ0, there is another approach that maintains a set of initializations {θ0_l}"
  - [corpus] Missing - no direct mention of mixture approaches in neighbor papers
- Break condition: When the number of distinct task types exceeds the number of mixture components, or when task boundaries are unclear

### Mechanism 3
- Claim: Compositional initialization enables combinatorial diversity of initializations while maintaining memory efficiency
- Mechanism: A set of initialization components is maintained, with each episode selecting a subset of components to compose its initial parameter vector
- Core assumption: Knowledge can be effectively shared across different task types through component reuse
- Evidence anchors:
  - [section] "In a mixture of initializations, an episode is handled by a single mixture component. On the other hand, compositional initialization combines a subset of components for each episode"
  - [corpus] Weak - no direct mention of compositional approaches in neighbor papers
- Break condition: When component interactions become too complex to learn effective combinations, or when task-specific information cannot be adequately captured through component composition

## Foundational Learning

- Concept: Stochastic Gradient Descent
  - Why needed here: SGD is the primary optimization mechanism used across all frameworks discussed, from basic offline learning to complex meta-learning scenarios
  - Quick check question: Can you explain how mini-batch sampling affects the variance of gradient estimates in SGD?

- Concept: Bayesian Sequential Updating
  - Why needed here: Bayesian approaches provide a theoretically grounded framework for online and continual learning, particularly when combined with meta-learning
  - Quick check question: How does the choice of prior affect the speed of convergence in sequential Bayesian updating?

- Concept: Sequence Modeling
  - Why needed here: Treating episodes as sequences enables flexible learning dynamics that can capture complex dependencies between examples
  - Quick check question: What are the trade-offs between RNN and Transformer architectures for long sequence modeling?

## Architecture Onboarding

- Component map:
  - Meta-learner: Parameterized by ω, responsible for producing episode-specific models
  - Episode processor: Handles individual training and test sets
  - Memory buffer: Stores past examples for replay-based methods
  - Hypernetwork (optional): Generates task-specific model parameters

- Critical path:
  1. Initialize meta-learner parameters
  2. Sample episode from meta-training stream
  3. Generate episode-specific model via inner loop
  4. Evaluate on test set and compute meta-loss
  5. Update meta-learner parameters
  6. Repeat until convergence

- Design tradeoffs:
  - Memory vs. performance: Larger memory buffers improve performance but increase computational cost
  - Flexibility vs. efficiency: More complex architectures can capture better representations but require more training data
  - Task-awareness vs. generality: Task-specific approaches may perform better but are less applicable to task-agnostic scenarios

- Failure signatures:
  - Catastrophic forgetting: Performance degradation on previously learned tasks
  - Meta-overfitting: Poor generalization to novel episodes
  - Vanishing/exploding gradients: Numerical instability during training
  - Memory overflow: Insufficient capacity to store necessary historical data

- First 3 experiments:
  1. Implement unitary initialization with MAML on a simple few-shot classification benchmark
  2. Compare mixture vs. compositional initialization strategies on a split benchmark
  3. Evaluate sequence modeling approaches against traditional optimization-based methods on a continual learning task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can meta-learning methods be made more scalable for large-scale continual learning scenarios?
- Basis in paper: [explicit] Section 7 discusses scalability challenges of bi-level optimization methods in meta-learning and their application to continual learning
- Why unresolved: Current bi-level optimization methods require second-order gradient computations and storing entire computation graphs, which becomes computationally prohibitive for large models and datasets
- What evidence would resolve it: Development of efficient first-order approximations or novel optimization techniques that maintain performance while significantly reducing computational complexity

### Open Question 2
- Question: What is the optimal way to combine generative replay and meta-learning in continual learning?
- Basis in paper: [explicit] Section 6.2 discusses various approaches that combine generative replay with meta-learning, but highlights the challenge of training hypernetworks while avoiding catastrophic forgetting
- Why unresolved: Existing methods show promise but struggle with memory usage, computational efficiency, and maintaining performance across many tasks
- What evidence would resolve it: A method that demonstrates superior performance on long task sequences while using significantly less memory than current approaches

### Open Question 3
- Question: How can task-agnostic continual learning be achieved without sacrificing performance compared to task-aware methods?
- Basis in paper: [explicit] Section 6 discusses the trade-off between task-awareness and practical applicability in continual learning methods
- Why unresolved: Current task-agnostic methods often underperform task-aware methods, but task-aware methods are impractical for many real-world scenarios
- What evidence would resolve it: A task-agnostic method that matches or exceeds the performance of state-of-the-art task-aware methods on standard continual learning benchmarks

## Limitations

- The taxonomy may not capture all emerging hybrid approaches combining multiple paradigms simultaneously
- Several frameworks lack well-established benchmarks and standardized evaluation protocols, making direct comparisons difficult
- Scalability claims for some methods remain theoretical or tested on limited benchmarks

## Confidence

- High confidence in the unified taxonomy structure and its logical organization
- Medium confidence in the coverage of all relevant papers due to the rapidly evolving nature of this field
- Medium confidence in the evaluation metrics for novel frameworks with limited empirical validation
- Low confidence in the practical scalability claims for some methods, as many remain theoretical or tested on limited benchmarks

## Next Checks

1. Implement and benchmark at least two representative methods from each of the eight framework categories on standardized datasets to verify the claimed performance characteristics and identify any implementation-specific challenges.

2. Conduct a systematic ablation study on the initialization strategies (unitary, mixture, compositional) across different task distributions to quantify their relative advantages and identify failure conditions.

3. Evaluate the proposed taxonomy's completeness by attempting to classify recent papers published after the survey's completion, documenting any frameworks that resist classification or suggest modifications to the taxonomy.