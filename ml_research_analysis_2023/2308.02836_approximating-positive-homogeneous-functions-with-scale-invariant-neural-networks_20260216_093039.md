---
ver: rpa2
title: Approximating Positive Homogeneous Functions with Scale Invariant Neural Networks
arxiv_id: '2308.02836'
source_url: https://arxiv.org/abs/2308.02836
tags:
- function
- such
- theorem
- then
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the capability of ReLU neural networks to solve
  linear inverse problems, with a focus on sparse recovery from underdetermined linear
  measurements. It proves that ReLU networks with one hidden layer cannot approximate
  1-sparse vectors, even with arbitrary width.
---

# Approximating Positive Homogeneous Functions with Scale Invariant Neural Networks

## Quick Facts
- arXiv ID: 2308.02836
- Source URL: https://arxiv.org/abs/2308.02836
- Reference count: 27
- This paper studies the capability of ReLU neural networks to solve linear inverse problems, with a focus on sparse recovery from underdetermined linear measurements.

## Executive Summary
This paper establishes theoretical foundations for using ReLU neural networks to solve inverse problems, particularly sparse recovery from underdetermined linear measurements. The key insight is that positive homogeneity properties of inverse problems can be leveraged by neural network architectures. The work proves that while one-hidden-layer ReLU networks cannot solve even simple sparse recovery problems, two-hidden-layer networks are both necessary and sufficient for approximating continuous positive homogeneous functions. The paper provides a universal approximation theorem for this function class and demonstrates robustness to measurement noise.

## Method Summary
The paper uses theoretical analysis to study ReLU neural networks with and without bias terms for solving inverse problems. The core approach involves constructing networks that preserve positive homogeneity properties, where f(λx) = λf(x) for λ ≥ 0. For sparse recovery, the networks take linear measurements y = Ax as input and attempt to recover the sparse signal x. The analysis focuses on unbiased networks (zero biases) that maintain scale invariance, with two hidden layers shown to be necessary and sufficient for the class of continuous positive homogeneous functions.

## Key Results
- ReLU networks with one hidden layer cannot approximate 1-sparse vectors from underdetermined measurements, regardless of network width.
- Two-hidden-layer ReLU networks can recover s-sparse vectors with arbitrary sparsity s and precision, and are robust to noisy measurements.
- ReLU is the only activation function that allows unbiased networks to approximate continuous positive homogeneous functions.
- The work establishes a universal approximation theorem for continuous positive homogeneous functions, showing two hidden layers are both necessary and sufficient.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReLU networks with two hidden layers can approximate any continuous positive homogeneous function to arbitrary precision.
- Mechanism: The network leverages the positive homogeneity property by constructing each layer to maintain scale invariance. The first hidden layer approximates a continuous function on the unit sphere, and the second layer extends this approximation to the entire space while preserving homogeneity through scaling.
- Core assumption: The activation function must be a linear combination of ReLU and ReLU(-x) with coefficients of different magnitudes.
- Evidence anchors:
  - [abstract] "The work also establishes a universal approximation theorem for continuous positive homogeneous functions, showing that ReLU networks with two hidden layers are necessary and sufficient"
  - [section] "By equivalence of norms, there exists a number M > 0 (that possibly depends on the dimension n), such that ∥ · ∥II ≤ M ∥ · ∥2. f is positive homogeneous and continuous, so by Theorem 2.5, for each component fj of f, there exists an unbiased ReLu network with 2 hidden layers that approximates fj up to a relative error of 1/√n min{1/τ, δ/(ρM)} > 0."
- Break condition: If the activation function cannot be expressed as a linear combination of ReLU and ReLU(-x) with |α| ≠ |β|, the positive homogeneity cannot be maintained through the network layers.

### Mechanism 2
- Claim: One hidden layer ReLU networks cannot recover even 1-sparse vectors from underdetermined linear measurements.
- Mechanism: The reconstruction function f must satisfy f(λy) = λf(y) for all λ ≥ 0 due to the linearity of the measurement process. With only one hidden layer, the network cannot maintain this positive homogeneity property while also satisfying the constraints imposed by the measurement matrix, leading to reconstruction errors bounded away from zero.
- Core assumption: The measurement matrix has rank less than the dimension of the signal space.
- Evidence anchors:
  - [abstract] "ReLU networks with only one hidden layer cannot even recover 1-sparse vectors, not even approximately, and regardless of the width of the network"
  - [section] "Corollary 2.1 follows from Theorem 2.3 by choosing x1 = e1, ..., xn = en. Then X = (x1 ... xn) = Id n and U = Σ1. The lower bound simplifies to √1/n(n - m) = √1 - m/n."
- Break condition: If the number of measurements equals or exceeds the signal dimension (m ≥ n), the reconstruction problem becomes well-posed and one hidden layer may suffice.

### Mechanism 3
- Claim: Two hidden layer ReLU networks provide stable recovery of s-sparse vectors from noisy measurements.
- Mechanism: The network first maps the measurement space to an intermediate representation that preserves sparsity structure, then maps to the signal space while being robust to noise. The positive homogeneity ensures that scaling the input measurement scales the output proportionally, maintaining stability across different signal magnitudes.
- Core assumption: The measurement matrix satisfies a restricted isometry property (RIP) with parameters related to the sparsity level.
- Evidence anchors:
  - [abstract] "networks with two hidden layers can recover s-sparse vectors with arbitrary sparsity s and precision, and are robust to noisy measurements"
  - [section] "Let A ∈ Rm×n be a matrix satisfying the (2s, δ)-RIP for a δ ∈ (0, 1). Then for each δ′ ∈ (0, 1), there exists a function ˜f : Rm → Rn, represented by an unbiased ReLu network with two hidden layers such that for all x ∈ Rn, e ∈ Rm, ∥ ˜f (Ax + e) − x∥2 ≤ δ′∥x∥2 + Cσs(x)1 + D∥e∥2"
- Break condition: If the noise level exceeds the threshold determined by the RIP parameters, the stability guarantee breaks down.

## Foundational Learning

- Concept: Positive homogeneous functions
  - Why needed here: The paper relies on the property that f(λx) = λf(x) for all λ ≥ 0 to characterize the structure of inverse problems and guide network design.
  - Quick check question: What is the defining property of a positive homogeneous function, and how does this relate to scale-invariant neural networks?

- Concept: Restricted Isometry Property (RIP)
  - Why needed here: The RIP condition on the measurement matrix is crucial for establishing recovery guarantees for sparse signals using ReLU networks.
  - Quick check question: What does it mean for a matrix to satisfy the (s, δ)-RIP, and why is this important for sparse recovery?

- Concept: Universal Approximation Theorem
  - Why needed here: The paper extends the classical universal approximation theorem to the class of positive homogeneous functions, requiring understanding of how function approximation works with neural networks.
  - Quick check question: How does the universal approximation theorem for general continuous functions differ from the version for positive homogeneous functions?

## Architecture Onboarding

- Component map: Input (m-dimensional measurements) → Hidden Layer 1 (ReLU) → Hidden Layer 2 (ReLU) → Output (n-dimensional signal estimate)

- Critical path: Input → Hidden Layer 1 → Hidden Layer 2 → Output. The two hidden layers are essential for maintaining positive homogeneity while achieving approximation capability.

- Design tradeoffs: Depth (2 layers) is fixed by theory, but width can vary. Wider networks may provide better approximation but at increased computational cost. The unbiased nature (zero biases) is required for positive homogeneity.

- Failure signatures: Poor recovery performance on sparse signals, large reconstruction errors that don't decrease with increased network width, instability to small measurement noise.

- First 3 experiments:
  1. Verify that a one-hidden-layer ReLU network cannot recover 1-sparse vectors from random Gaussian measurements by testing reconstruction error across different sparsity patterns.
  2. Implement a two-hidden-layer unbiased ReLU network for sparse recovery and measure reconstruction error versus sparsity level s and measurement noise magnitude.
  3. Test the robustness of the two-layer network by varying the restricted isometry constant δ of the measurement matrix and observing the effect on recovery error.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the width of the neural network affect the ability to solve sparse recovery problems, or is depth the only critical factor?
- Basis in paper: [explicit] The paper establishes that two hidden layers are necessary and sufficient for solving sparse recovery problems, but does not specify the required width of the network.
- Why unresolved: The authors focus on proving the existence of a network with two hidden layers that can solve the problem, but do not provide a construction or specify the width.
- What evidence would resolve it: A construction of a network with two hidden layers and a specified width that can solve the sparse recovery problem, or a proof that the width does not affect the ability to solve the problem.

### Open Question 2
- Question: Can the robustness of neural networks for inverse problems be improved by incorporating additional knowledge about the problem structure?
- Basis in paper: [inferred] The paper discusses the robustness of neural networks for inverse problems, but does not explore the possibility of incorporating additional problem structure to improve robustness.
- Why unresolved: The authors focus on proving the existence of robust networks, but do not investigate the impact of incorporating additional problem structure on robustness.
- What evidence would resolve it: An analysis of the impact of incorporating additional problem structure on the robustness of neural networks for inverse problems, or a construction of a network that incorporates problem structure and demonstrates improved robustness.

### Open Question 3
- Question: How does the choice of activation function affect the ability of neural networks to solve inverse problems?
- Basis in paper: [explicit] The paper shows that ReLU is the only activation function that allows unbiased networks to approximate continuous positive homogeneous functions, but does not explore the impact of other activation functions on solving inverse problems.
- Why unresolved: The authors focus on proving the unique role of ReLU in approximating positive homogeneous functions, but do not investigate the impact of other activation functions on solving inverse problems.
- What evidence would resolve it: An analysis of the impact of different activation functions on the ability of neural networks to solve inverse problems, or a construction of a network with a non-ReLU activation function that can solve an inverse problem.

## Limitations

- The paper lacks empirical validation of the theoretical results on real-world sparse recovery problems.
- The analysis assumes idealized measurement matrices (restricted isometry property) that may not hold in practical scenarios.
- The focus on positive homogeneous functions limits the applicability of results to a specific class of inverse problems.

## Confidence

- Theoretical results on the necessity of two hidden layers for positive homogeneous functions: High
- Robustness guarantees for sparse recovery under noise: Medium (due to reliance on idealized assumptions)
- Universal approximation theorem for positive homogeneous functions: High

## Next Checks

1. Conduct empirical experiments comparing the performance of one-layer versus two-layer ReLU networks on synthetic and real-world sparse recovery problems to validate the theoretical claims.
2. Test the robustness of the proposed networks under more realistic noise models and measurement matrices that do not strictly satisfy the restricted isometry property.
3. Investigate the extension of the universal approximation theorem to broader function classes beyond positive homogeneous functions to assess the generalizability of the findings.