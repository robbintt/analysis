---
ver: rpa2
title: Empathy and Distress Detection using Ensembles of Transformer Models
arxiv_id: '2312.02578'
source_url: https://arxiv.org/abs/2312.02578
tags:
- empathy
- distress
- task
- dataset
- also
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an approach for empathy and distress detection
  in natural language using ensembles of transformer models for the WASSA 2023 shared
  task. The method experiments with several BERT-based models and ensemble techniques
  to predict empathy and distress scores from essays.
---

# Empathy and Distress Detection using Ensembles of Transformer Models

## Quick Facts
- arXiv ID: 2312.02578
- Source URL: https://arxiv.org/abs/2312.02578
- Reference count: 3
- The team achieved third place in the WASSA 2023 shared task with Pearson's r score of 0.346

## Executive Summary
This paper presents an approach for empathy and distress detection in natural language using ensembles of transformer models for the WASSA 2023 shared task. The method experiments with several BERT-based models and ensemble techniques to predict empathy and distress scores from essays. The best individual model is an unsupervised SimCSE RoBERTa variant, and the best ensemble method is simple averaging of predictions. The approach achieves a Pearson's r score of 0.346, placing the team third in the empathy and distress detection subtask.

## Method Summary
The approach fine-tunes several BERT-based models (RoBERTa-base, Twitter-RoBERTa-emotion, Twitter-RoBERTa-sentiment, and SimCSE RoBERTa) on a dataset of essays responding to news stories about people facing hardships. These models predict continuous empathy and distress scores, which are then combined using ensemble methods including simple averaging, linear regression, SVR, and XGBoost. The best individual model is the unsupervised SimCSE RoBERTa variant using sentence embeddings, and the optimal ensemble method is simple averaging of predictions.

## Key Results
- Best individual model: SimCSE RoBERTa with Pearson's r of 0.346
- Best ensemble method: Simple averaging of predictions
- Twitter-RoBERTa models did not significantly outperform RoBERTa-base
- Team placed third in the WASSA 2023 empathy and distress detection subtask

## Why This Works (Mechanism)

### Mechanism 1
- Sentence embeddings via SimCSE capture contextual meaning better than token embeddings for long essays.
- SimCSE learns contrastive representations by maximizing similarity between augmented versions of the same sentence while pushing apart different sentences.
- Core assumption: The contrastive objective aligns with the empathy/distress prediction task.
- Evidence: The unsupervised SimCSE model uses sentence embeddings instead of token embeddings and performs best.

### Mechanism 2
- Averaging ensemble predictions reduces variance and stabilizes output across different model biases.
- Simple mean of model outputs combines their strengths while canceling out individual weaknesses.
- Core assumption: Model errors are uncorrelated, so averaging improves overall performance.
- Evidence: Simple averaging generates the best result amongst ensemble approaches.

### Mechanism 3
- BERT-based models pre-trained on domain-relevant data (Twitter corpora) should perform better on informal text.
- Domain-specific pre-training aligns the model's understanding with the informal language style and vocabulary.
- Core assumption: The essays contain informal language similar to tweets.
- Evidence: Twitter-RoBERTa models do not exhibit significantly better performance than RoBERTA-base.

## Foundational Learning

- Concept: Pearson correlation as evaluation metric
  - Why needed: The task requires predicting continuous empathy/distress scores, making correlation a natural fit to measure prediction accuracy.
  - Quick check: If predictions perfectly match ground truth but are scaled by a constant factor, what happens to Pearson correlation?

- Concept: Contrastive learning for sentence embeddings
  - Why needed: SimCSE's contrastive objective helps capture semantic similarity in long essays, which is crucial for empathy detection.
  - Quick check: In SimCSE, what happens to the embedding space when positive pairs are created through dropout?

- Concept: Ensemble methods for regression
  - Why needed: Combining multiple model predictions can reduce variance and improve robustness in the final output.
  - Quick check: If three models have correlations of 0.3, 0.4, and 0.5 with ground truth, what's the theoretical maximum correlation achievable through averaging?

## Architecture Onboarding

- Component map: Input essays → BERT models (RoBERTa-base, Twitter-RoBERTa variants, SimCSE) → Individual predictions → Ensemble layer (averaging) → Final output
- Critical path: Model inference → Ensemble aggregation → Correlation calculation
- Design tradeoffs: Simpler averaging vs. complex regression-based ensembling; broader vs. domain-specific pre-training
- Failure signatures: Low correlation despite high individual model performance; ensemble performing worse than best single model
- First 3 experiments:
  1. Test each BERT variant individually on validation set to establish baseline correlations
  2. Implement simple averaging ensemble and measure improvement over individual models
  3. Try weighted averaging based on individual model validation performance

## Open Questions the Paper Calls Out

- How do transformer-based models perform on datasets with different demographic distributions?
- What is the impact of using sentence embeddings versus token embeddings for long-text documents?
- How do different ensembling techniques compare in improving model performance?
- What are the computational requirements and scalability challenges of deploying these models?

## Limitations
- Limited theoretical justification for mechanism selection
- Restricted exploration of ensemble methodology beyond simple averaging
- Narrow model diversity (only BERT-based variants tested)

## Confidence
- High confidence: Empirical finding that simple averaging outperforms individual models
- Medium confidence: SimCSE RoBERTa as best individual model
- Low confidence: Hypothesized mechanisms linking architecture to performance

## Next Checks
1. Conduct statistical significance testing on Pearson correlations across all model variants
2. Perform ablation study on SimCSE components to isolate performance drivers
3. Validate Twitter-RoBERTa models on actual Twitter data to explain performance discrepancy