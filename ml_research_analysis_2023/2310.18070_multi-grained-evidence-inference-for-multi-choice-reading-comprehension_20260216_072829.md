---
ver: rpa2
title: Multi-grained Evidence Inference for Multi-choice Reading Comprehension
arxiv_id: '2310.18070'
source_url: https://arxiv.org/abs/2310.18070
tags:
- evidence
- mugen
- phrase
- sentence
- fragment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach called Multi-grained Evidence
  Inference (Mugen) for multi-choice reading comprehension tasks. Mugen addresses
  the challenge of extracting and integrating evidence at different granularities
  (coarse, middle, and fine) to enhance the model's ability to answer questions based
  on given passages and options.
---

# Multi-grained Evidence Inference for Multi-choice Reading Comprehension

## Quick Facts
- arXiv ID: 2310.18070
- Source URL: https://arxiv.org/abs/2310.18070
- Reference count: 40
- Key outcome: Mugen achieves significant performance improvements on four multi-choice MRC benchmarks (RACE, DREAM, Cosmos QA, and MCTest) compared to strong baselines, with statistically significant results.

## Executive Summary
This paper introduces Multi-grained Evidence Inference (Mugen), a novel approach for multi-choice reading comprehension that extracts and integrates evidence at different linguistic granularities (coarse, middle, and fine) to enhance model performance. The core idea is to first extract multi-grained evidence using specialized extractors and then integrate it with the original passage for improved prediction. Mugen demonstrates statistically significant performance improvements on four multi-choice MRC benchmarks compared to strong baselines, validating the effectiveness of incorporating multi-grained evidence in reading comprehension tasks.

## Method Summary
Mugen extracts evidence at three granularities - sentence (coarse), fragment (middle), and phrase (fine) - using specialized extractors, with the sentence evidence extractor pre-trained on SQuAD 2.0 for accuracy. These evidence embeddings are combined with the original passage embeddings using learnable weights (α, β, γ, σ) in a weighted sum approach, creating an enriched representation that balances context and critical information. The integrated representation is then passed through a softmax classifier to predict the correct answer from multiple choices. The model is evaluated on RACE, DREAM, Cosmos QA, and MCTest datasets, showing significant improvements over baseline models.

## Key Results
- Mugen achieves statistically significant accuracy improvements on RACE, DREAM, Cosmos QA, and MCTest benchmarks compared to strong baselines
- Performance gains are consistent across different model sizes (ALBERT base and xxlarge)
- The approach demonstrates the effectiveness of multi-grained evidence integration in reading comprehension tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-grained evidence integration enhances model performance by capturing critical information at different linguistic levels.
- Mechanism: The model extracts evidence at three granularities (coarse, middle, fine) and integrates them with the original passage to create an enriched representation for prediction.
- Core assumption: Different granularities of evidence contain complementary information necessary for accurate answer prediction.
- Evidence anchors:
  - [abstract] "Mugen extracts three different granularities of evidence: coarse-, middle- and fine-grained evidence, and integrates evidence with the original passages, achieving significant and consistent performance improvement"
  - [section III.A] "Three different grains of evidence are proposed for multi-grained evidence integration and modeling enhancement"
- Break condition: If evidence at one granularity consistently misleads the model or introduces too much noise, integration may degrade performance.

### Mechanism 2
- Claim: Evidence extractors pre-trained on relevant tasks improve the quality of extracted evidence.
- Mechanism: Sentence Evidence Extractor is pre-trained on SQuAD 2.0 to ensure accurate fragment extraction with complete linguistic structures.
- Core assumption: Pre-training on a related task (span extraction) transfers to better evidence extraction in the target task.
- Evidence anchors:
  - [section III.C] "Mugen uses Sentence Evidence Extractor to extract both Sentence and Fragment Evidence. To ensure Sentence Evidence Extractor can extract precise evidence, we implement a contextual encoder (we employ ALBERT base in Mugen) which is pre-trained on SQuAD 2.0 individually"
- Break condition: If the pre-training task is too dissimilar from the target task, transfer learning may not occur effectively.

### Mechanism 3
- Claim: Balanced integration of evidence with the original passage provides context while highlighting critical information.
- Mechanism: Evidence embeddings are combined with passage embeddings using learnable weights, allowing the model to focus on both context and evidence.
- Core assumption: The original passage provides necessary context that evidence alone may lack, and their balanced integration is optimal.
- Evidence anchors:
  - [section III.B] "Ei = dropout(αei_pas + βei_sen + γei_fra + σei_phr) ∈ RH, where H is the hidden size of Encoder, and α, β, γ, σ are learnable parameters"
- Break condition: If one type of evidence consistently dominates or is irrelevant, the fixed integration weights may prevent optimal adaptation.

## Foundational Learning

- Concept: Pre-trained language models and their fine-tuning
  - Why needed here: The model builds upon ALBERT as a baseline encoder and fine-tunes it for the specific multi-choice MRC task
  - Quick check question: What is the difference between pre-training and fine-tuning in the context of transformer-based models?

- Concept: Evidence extraction and integration in reading comprehension
  - Why needed here: The core innovation involves extracting evidence at multiple granularities and integrating it with the original passage
  - Quick check question: How does extracting evidence at different linguistic levels (phrase, sentence, passage) potentially benefit reading comprehension?

- Concept: Statistical significance testing in NLP experiments
  - Why needed here: The paper reports that improvements are statistically significant using McNemar's test
  - Quick check question: What does it mean for a model improvement to be statistically significant in NLP experiments, and why is this important?

## Architecture Onboarding

- Component map: Input Passage, Question, Options -> Evidence Extraction (Sentence, Fragment, Phrase) -> ALBERT Encoder -> Weighted Integration (α, β, γ, σ) -> Softmax Classifier -> Predicted Answer

- Critical path: Input Passage, Question, Options → Evidence Extraction → Encoding → Integration → Classification → Output

- Design tradeoffs:
  - Multiple evidence extraction vs. computational cost
  - Fixed integration weights vs. learned weights
  - Pre-training evidence extractor vs. end-to-end training
  - Complexity of multi-grained approach vs. potential gains

- Failure signatures:
  - Performance degrades when evidence extraction is inaccurate
  - Model overfits to specific evidence patterns
  - Integration weights become unbalanced, favoring one evidence type
  - Computational cost becomes prohibitive for large-scale deployment

- First 3 experiments:
  1. Implement and test the simplified version (Mugensimp) to verify the core hypothesis without additional computational cost
  2. Perform ablation studies by removing each type of evidence to understand their individual contributions
  3. Test different integration strategies (voting vs. weighted embedding) to find the optimal approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Mugen scale with the size of the pre-trained encoder used as the baseline model?
- Basis in paper: [explicit] The paper compares Mugen's performance on ALBERT base and ALBERT xxlarge, showing significant improvements over the baseline models at both sizes.
- Why unresolved: The paper only tests two sizes of ALBERT. It is unclear how Mugen would perform with other pre-trained models of varying sizes, such as BERT, RoBERTa, or T5.
- What evidence would resolve it: Experiments comparing Mugen's performance on various multi-choice MRC benchmarks using different pre-trained models as baselines, including models of different sizes and architectures.

### Open Question 2
- Question: What is the impact of the evidence threshold θ on the performance of Mugen, and how sensitive is the model to changes in this hyperparameter?
- Basis in paper: [explicit] The paper mentions that θ is set to 0.8 for Phrase Evidence Extractor and provides some sensitivity analysis by varying θ to 0.7, 0.9, and 1.0, resulting in minor performance changes.
- Why unresolved: The paper only tests a limited range of θ values and does not explore the full impact of this hyperparameter on Mugen's performance. It is unclear if there is an optimal θ value or if the model is robust to changes in this parameter.
- What evidence would resolve it: Extensive experiments varying the evidence threshold θ across a wider range of values and analyzing its impact on Mugen's performance on multiple multi-choice MRC benchmarks.

### Open Question 3
- Question: How does Mugen perform on multi-choice MRC tasks that involve multi-hop reasoning, where the answer requires evidence from multiple parts of the passage?
- Basis in paper: [inferred] The paper focuses on multi-choice MRC tasks where the answer can be derived from a single piece of evidence or a chain of evidence within the passage. However, it does not explicitly test Mugen's performance on tasks that require multi-hop reasoning.
- Why unresolved: The paper does not provide evidence of Mugen's ability to handle multi-hop reasoning tasks, which are common in many real-world multi-choice MRC scenarios.
- What evidence would resolve it: Experiments evaluating Mugen's performance on multi-choice MRC benchmarks that specifically require multi-hop reasoning, such as MultiRC or HotpotQA, and comparing its results to strong baselines.

## Limitations
- The approach relies heavily on pre-trained models that may not generalize well to domains with different linguistic patterns
- Fixed integration weights assume optimal balance of evidence types is static across all examples, which may not hold for diverse question types
- Computational overhead of extracting evidence at three granularities could limit scalability

## Confidence

- High confidence: The reported performance improvements on the four benchmark datasets are significant and well-supported by the experimental results.
- Medium confidence: The mechanism by which multi-grained evidence integration improves performance is plausible but relies on several assumptions about complementary information across granularities.
- Low confidence: The claim that pre-training evidence extractors on SQuAD 2.0 ensures accurate fragment extraction is weakly supported, as the paper provides limited analysis of extraction quality or transfer effectiveness.

## Next Checks

1. **Ablation study validation**: Systematically remove each type of evidence (coarse, middle, fine) individually and in combination to quantify their individual contributions and test the assumption of complementary information.
2. **Generalization testing**: Evaluate Mugen on out-of-domain datasets or datasets with different linguistic characteristics to assess whether the multi-grained approach generalizes beyond the tested benchmarks.
3. **Integration weight analysis**: Conduct a sensitivity analysis by varying the integration weights (α, β, γ, σ) dynamically based on question type or evidence quality scores to test whether fixed weights are optimal.