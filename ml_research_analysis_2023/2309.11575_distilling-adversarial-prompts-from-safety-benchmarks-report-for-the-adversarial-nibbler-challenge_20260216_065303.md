---
ver: rpa2
title: 'Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial
  Nibbler Challenge'
arxiv_id: '2309.11575'
source_url: https://arxiv.org/abs/2309.11575
tags:
- prompts
- adversarial
- safety
- images
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report analyzes over 1,000 adversarial prompts distilled from
  existing safety benchmarks to evaluate the fragility of input filters in text-to-image
  models. The prompts were extracted from the I2P dataset by filtering out those containing
  explicitly banned terms.
---

# Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge

## Quick Facts
- arXiv ID: 2309.11575
- Source URL: https://arxiv.org/abs/2309.11575
- Authors: 
- Reference count: 2
- Primary result: Over 1,000 adversarial prompts extracted from safety benchmarks demonstrate the fragility of input filters in text-to-image models

## Executive Summary
This report analyzes over 1,000 adversarial prompts distilled from existing safety benchmarks to evaluate the fragility of input filters in text-to-image models. The prompts were extracted from the I2P dataset by filtering out those containing explicitly banned terms. The resulting set of adversarial inputs demonstrates that current input filtering approaches are highly brittle, as benign-sounding prompts can still produce inappropriate content. Key findings include the ineffectiveness of keyword-based filters due to semantic gaps and misspellings, the subjective nature of what constitutes inappropriate content, and systematic issues like the ease of generating sexualized imagery of women. The distilled prompts provide a valuable resource for stress-testing input filtering systems and highlight the need for more robust safety guardrails in generative models.

## Method Summary
The study collected prompts from the I2P dataset and filtered them against Midjourney's banned word list to identify potentially adversarial prompts. The remaining prompts were analyzed for their likelihood to generate inappropriate content despite passing input filters. The analysis focused on identifying systematic safety issues and evaluating the effectiveness of keyword-based filtering approaches. The researchers examined correlations between benign-sounding prompts and inappropriate content generation, particularly focusing on gender-based patterns in sexualized imagery.

## Key Results
- Over 1,100 adversarial prompts were distilled from safety benchmarks that bypass keyword-based filters
- Keyword-based input filters fail against misspellings and semantically similar terms
- Systematic issues identified, including the ease of generating sexualized imagery of women
- Subjectivity in defining inappropriate content makes universal safety guardrails challenging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Keyword-based input filters fail because semantic gaps allow adversarial prompts to bypass detection while still producing inappropriate content.
- Mechanism: The system uses a static list of banned words (e.g., Midjourney's 800-word list). Prompts containing banned words are blocked, but semantically similar or misspelled terms are not detected, allowing adversarial inputs to pass through.
- Core assumption: Semantic understanding and context are required to accurately detect inappropriate content, which static keyword lists cannot provide.
- Evidence anchors:
  - [abstract]: "demonstrating the fragility of input filters and provides further insights into systematic safety issues"
  - [section]: "The remaining prompts clearly demonstrate the severe limitations of ban-list based input filters. We identified several simple misspellings of prohibited words bypassing filters while still being able to produce unsafe material."
  - [corpus]: Weak evidence - no direct corpus support found for this specific mechanism.
- Break condition: The system could be broken by adversarial actors who intentionally use misspellings, synonyms, or related terms not included in the ban list.

### Mechanism 2
- Claim: The subjectivity of what constitutes inappropriate content makes it difficult to create effective universal safety guardrails.
- Mechanism: Different individuals and contexts have varying interpretations of what is considered safe or unsafe. Prompts that appear benign in isolation may be inappropriate in certain contexts, making it challenging to create a one-size-fits-all safety solution.
- Core assumption: Safety is not an absolute concept but rather a subjective judgment that depends on cultural, social, and individual factors.
- Evidence anchors:
  - [abstract]: "Our analysis of the gathered prompts and corresponding images demonstrates the fragility of input filters and provides further insights into systematic safety issues"
  - [section]: "A closer look at the collected prompts and generated images highlights the subjectivity of what is considered inappropriate or unsafe. The definition of safety can differ based on context, setting, cultural and social predisposition, and individual factors."
  - [corpus]: Weak evidence - no direct corpus support found for this specific mechanism.
- Break condition: The system could be broken by adversarial actors who exploit the subjective nature of safety by creating prompts that appear benign but are inappropriate in certain contexts.

### Mechanism 3
- Claim: The correlation between certain benign-sounding prompts and inappropriate content can be exploited by adversarial actors to bypass safety filters.
- Mechanism: The system has learned associations between certain terms, artists, or prompt structures and inappropriate content. By using these correlated terms in seemingly benign prompts, adversarial actors can trigger the generation of inappropriate images.
- Core assumption: The generative model has learned biased associations between certain concepts and inappropriate content, which can be exploited by adversarial actors.
- Evidence anchors:
  - [abstract]: "The resulting set of adversarial inputs demonstrates that current input filtering approaches are highly brittle, as benign-sounding prompts can still produce inappropriate content."
  - [section]: "We discovered multiple systematic issues leading to unsafe imagery. Unfortunately, our results confirm observations of previous work that sexually explicit imagery of women is remarkably easy to produce with seemingly safe prompts."
  - [corpus]: Weak evidence - no direct corpus support found for this specific mechanism.
- Break condition: The system could be broken by adversarial actors who intentionally use correlated terms, artists, or prompt structures to generate inappropriate content.

## Foundational Learning

- Concept: Text-to-image generation models and their vulnerabilities
  - Why needed here: Understanding how text-to-image models work and their potential safety issues is crucial for designing effective adversarial testing methods.
  - Quick check question: What are the main components of a text-to-image generation model, and how can they be exploited to generate inappropriate content?

- Concept: Safety benchmarks and their limitations
  - Why needed here: Evaluating the effectiveness of safety guardrails requires understanding the strengths and weaknesses of existing safety benchmarks and their methodologies.
  - Quick check question: What are the key limitations of current safety benchmarks for text-to-image models, and how can they be improved?

- Concept: Adversarial testing and red teaming
  - Why needed here: Adversarial testing is essential for identifying vulnerabilities in safety guardrails and improving their robustness against malicious attacks.
  - Quick check question: What are the key principles of effective adversarial testing, and how can they be applied to evaluate the safety of text-to-image models?

## Architecture Onboarding

- Component map: Text-to-image model -> Input filter -> Safety benchmark dataset -> Adversarial prompt distillation pipeline -> Evaluation metrics
- Critical path: 1. Collect and preprocess safety benchmark dataset 2. Apply input filter to identify benign prompts 3. Generate images using text-to-image model 4. Evaluate generated images for inappropriateness 5. Distill adversarial prompts based on evaluation results
- Design tradeoffs: False positives vs. false negatives in input filtering, Specificity vs. generalizability of safety guardrails, Computational cost vs. evaluation thoroughness
- Failure signatures: High false positive rate in input filtering (blocking benign prompts), High false negative rate in input filtering (allowing inappropriate prompts), Inability to detect semantically similar or misspelled terms, Inconsistent evaluation of inappropriateness across different contexts
- First 3 experiments: 1. Evaluate the effectiveness of the input filter on a held-out test set of prompts. 2. Analyze the correlation between benign-sounding prompts and inappropriate content generation. 3. Test the robustness of safety guardrails against adversarial attacks using distilled prompts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are current input filtering systems at preventing the generation of inappropriate content when using adversarial prompts that bypass ban lists through semantic gaps or misspellings?
- Basis in paper: [explicit] The paper demonstrates that over 1,000 adversarial prompts were extracted from safety benchmarks, showing that benign-sounding prompts can produce inappropriate content despite existing filters.
- Why unresolved: The paper identifies the problem but doesn't systematically test how many of these adversarial prompts actually bypass current production filters across different models.
- What evidence would resolve it: Systematic testing of the distilled 1,100 prompts against multiple deployed text-to-image models' input filters to measure bypass rates.

### Open Question 2
- Question: What are the most effective alternative approaches to ban-list-based filtering for preventing inappropriate content generation in text-to-image models?
- Basis in paper: [explicit] The paper highlights the ineffectiveness of ban-list based input filters and states that "designing holistic filters proves virtually impossible."
- Why unresolved: While the paper identifies limitations of current approaches, it doesn't propose or evaluate alternative filtering methodologies.
- What evidence would resolve it: Comparative evaluation of alternative filtering approaches (semantic analysis, context-aware filtering, multi-modal verification) against the distilled adversarial prompt set.

### Open Question 3
- Question: How can the subjectivity of what constitutes "inappropriate" or "unsafe" content be systematically addressed in safety benchmarks for generative models?
- Basis in paper: [explicit] The paper notes that "The definition of safety can differ based on context, setting, cultural and social predisposition, and individual factors" and provides examples of disturbing but not clearly unsafe content.
- Why unresolved: The paper identifies the subjectivity problem but doesn't propose methodologies for creating more objective or context-aware safety standards.
- What evidence would resolve it: Development and validation of context-aware safety frameworks that account for different cultural and situational interpretations of inappropriate content.

## Limitations

- Scope limitations: Relies on existing safety benchmarks rather than actively generated adversarial prompts
- Classifier reliability: Uses automated classifiers with unspecified accuracy metrics
- Single-model evaluation: Testing conducted primarily with Stable Diffusion, limiting generalizability

## Confidence

**High Confidence:** The brittleness of keyword-based input filters against misspellings and semantic variations is well-demonstrated through direct evidence in the filtered prompt analysis.

**Medium Confidence:** The claim about the subjectivity of safety definitions relies on observational evidence rather than systematic user studies across diverse populations.

**Low Confidence:** The generalizability of findings to real-world adversarial scenarios without additional validation across multiple models and attack strategies.

## Next Checks

1. **Classifier Validation Study:** Conduct a human evaluation study comparing classifier assessments with expert judgments across a representative sample of the distilled prompts to establish accuracy rates and identify systematic biases in automated safety detection.

2. **Cross-Model Testing:** Test the same distilled prompt set across at least three different text-to-image models (e.g., DALL-E, Midjourney, Stable Diffusion) to evaluate whether vulnerabilities are model-specific or represent broader architectural weaknesses.

3. **Adversarial Actor Simulation:** Engage professional red-teamers to attempt generating prompts not present in the I2P dataset using the insights from this study, measuring whether the distilled prompts provide novel attack vectors or merely document known vulnerabilities.