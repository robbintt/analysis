---
ver: rpa2
title: 'Model Leeching: An Extraction Attack Targeting LLMs'
arxiv_id: '2309.10544'
source_url: https://arxiv.org/abs/2309.10544
tags:
- attack
- extracted
- target
- chatgpt-3
- turbo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Model Leeching, a novel extraction attack
  against Large Language Models (LLMs) that distills task-specific knowledge into
  a smaller model. The attack leverages automated prompt generation to extract task-specific
  data characteristics from a target LLM, which are then used to train an extracted
  model.
---

# Model Leeching: An Extraction Attack Targeting LLMs

## Quick Facts
- arXiv ID: 2309.10544
- Source URL: https://arxiv.org/abs/2309.10544
- Reference count: 33
- Primary result: Model Leeching extracts task-specific knowledge from LLMs at $50 cost vs $3,600 for human labeling, achieving 75% EM and 87% F1 accuracy on SQuAD

## Executive Summary
Model Leeching introduces a novel extraction attack that distills task-specific knowledge from large language models into smaller, more deployable models. The attack uses automated prompt generation to extract responses from a target LLM, which are then used to train an extracted model that can perform the same task with comparable accuracy. Experiments demonstrate successful extraction from ChatGPT-3.5-Turbo for question-answering tasks, achieving significant performance metrics at a fraction of traditional data labeling costs. Critically, the extracted model can be used to stage adversarial attacks against the original LLM, increasing attack success rates by 11%.

## Method Summary
The attack methodology involves designing automated prompts targeting specific tasks within a target LLM, using the model's API to generate labeled datasets, training a smaller base model (such as Roberta-Large, Bert, or Albert) on this data, and then using the extracted model to develop and transfer adversarial attacks to the original LLM. The process was evaluated using the SQuAD dataset with ChatGPT-3.5-Turbo as the target, costing approximately $50 and requiring 48 hours to complete. The extracted model was then used to stage adversarial attacks, demonstrating transferability from the extracted model to the target LLM.

## Key Results
- Achieved 73% Exact Match similarity between extracted and target models
- Extracted model achieved 75% EM and 87% F1 accuracy on SQuAD benchmark
- Attack staging increased success rate by 11% when using the extracted model
- Total attack cost was $50 versus $3,600 for human labeling services

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model Leeching can distill task-specific knowledge from a target LLM into a smaller model with comparable performance.
- Mechanism: Automated prompt generation extracts responses from the target LLM, which are then used to train a smaller base model, effectively distilling task-specific capabilities.
- Core assumption: The target LLM contains task-specific knowledge that can be effectively extracted through well-designed prompts and transferred to a smaller model.
- Evidence anchors:
  - [abstract] "capable of distilling task-specific knowledge from a target LLM into a reduced parameter model"
  - [section] "Our attack is performed by designing an automated prompt generation system targeting specific tasks within LLMs"
- Break condition: If the target LLM uses advanced training methods like reinforcement learning from human feedback that are not captured in the extracted data, or if the base model architecture is fundamentally incompatible with the target's knowledge representation.

### Mechanism 2
- Claim: Extracted models can be used to perform ML attack staging against the target LLM, increasing attack success rates.
- Mechanism: The extracted model serves as a sandbox environment where adversarial attacks can be developed and optimized, then transferred to the target LLM with improved effectiveness.
- Core assumption: Adversarial vulnerabilities learned on the extracted model are transferable to the target LLM due to shared task-specific knowledge.
- Evidence anchors:
  - [abstract] "demonstrating the feasibility of adversarial attack transferability from an extracted model extracted via Model Leeching to perform ML attack staging against a target LLM"
  - [section] "By having access to this 'sandbox' model, adversaries can refine or innovate their attack strategies"
- Break condition: If the target LLM has robust adversarial defenses that are not present in the extracted model, or if the attack surface differs significantly between models.

### Mechanism 3
- Claim: Model Leeching is economically viable compared to traditional data labeling services.
- Mechanism: Using the target LLM's API to generate labeled data is significantly cheaper than hiring human labelers for the same task.
- Core assumption: The cost per API query is lower than the cost per labeled example from human labelers, and the error rate is acceptable.
- Evidence anchors:
  - [section] "Compared to using labelling services such as Amazon SageMaker Data Labeling...the estimated cost of labelling would be $0.036 per example of data, totalling $3,600, demonstrating a significant reduction in cost when using generative LLMs to label datasets"
  - [section] "In total, this process cost $50 and required 48 hours to complete"
- Break condition: If API costs increase significantly, if rate limiting becomes more restrictive, or if the error rate requires extensive manual correction.

## Foundational Learning

- Concept: Deep Learning Model Extraction Attacks
  - Why needed here: Understanding the threat model and techniques for extracting model characteristics is crucial for comprehending how Model Leeching works.
  - Quick check question: What are the key differences between model extraction attacks on traditional ML models versus LLMs?

- Concept: Prompt Engineering and Automated Prompt Generation
  - Why needed here: The effectiveness of Model Leeching depends heavily on the ability to design prompts that elicit the desired task-specific knowledge from the target LLM.
  - Quick check question: What are the key components of effective prompt design for extracting task-specific knowledge?

- Concept: Adversarial Attack Transferability
  - Why needed here: The ability to transfer attacks from the extracted model to the target LLM is a critical aspect of the attack's effectiveness.
  - Quick check question: What factors influence the transferability of adversarial attacks between different machine learning models?

## Architecture Onboarding

- Component map:
  - Target LLM (e.g., ChatGPT-3.5-Turbo) -> Automated prompt generation system -> Data collection and preprocessing pipeline -> Base model architectures (e.g., Roberta-Large, Bert, Albert) -> Extracted model training pipeline -> Adversarial attack staging environment

- Critical path:
  1. Design task-specific prompts
  2. Generate labeled dataset using target LLM
  3. Train extracted model on the generated dataset
  4. Evaluate extracted model's similarity and task performance
  5. Develop and optimize adversarial attacks on the extracted model
  6. Transfer and execute attacks on the target LLM

- Design tradeoffs:
  - Model size vs. task performance: Smaller models are cheaper to deploy but may capture less nuanced knowledge
  - Query volume vs. cost: More queries improve dataset quality but increase cost
  - Prompt complexity vs. success rate: More complex prompts may yield better results but are harder to design

- Failure signatures:
  - Low similarity scores between extracted and target models
  - Poor task performance of the extracted model compared to baselines
  - Low attack success rates when transferring attacks to the target LLM

- First 3 experiments:
  1. Test prompt effectiveness: Design and evaluate a set of prompts for a simple task (e.g., sentiment analysis) to assess their ability to extract consistent responses from the target LLM.
  2. Compare base model architectures: Train extracted models using different base architectures on a small dataset to determine which architectures capture the target LLM's knowledge most effectively.
  3. Evaluate attack transferability: Develop a simple adversarial attack on the extracted model and measure its success rate when transferred to the target LLM.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would Model Leeching be against other state-of-the-art LLMs like GPT-4 or PaLM, and would the attack transferability rates differ significantly?
- Basis in paper: [inferred] The paper mentions "Further work includes conducting Model Leeching against a larger array of LLM(s) such as BARD, LLaMA and available variations of GPT models from OpenAI" but does not provide results for these models.
- Why unresolved: The paper only tested Model Leeching on ChatGPT-3.5-Turbo, leaving the effectiveness on other LLMs unknown.
- What evidence would resolve it: Conducting the same experiments on other LLMs and comparing the extraction similarity, task performance, and attack transferability rates.

### Open Question 2
- Question: What is the minimum dataset size required for Model Leeching to achieve a certain level of extraction similarity and task performance?
- Basis in paper: [inferred] The paper used 83,335 labeled examples but does not explore the impact of dataset size on extraction effectiveness.
- Why unresolved: The paper does not provide a systematic analysis of how dataset size affects the attack's success.
- What evidence would resolve it: Conducting experiments with varying dataset sizes and measuring the resulting extraction similarity and task performance to determine the minimum effective dataset size.

### Open Question 3
- Question: How effective are current defense mechanisms against Model Leeching and subsequent attack staging?
- Basis in paper: [explicit] The paper mentions "LLM Defenses" as a future work area, stating "There has been limited work to defend against attacks on LLMs."
- Why unresolved: The paper does not evaluate the effectiveness of existing defense mechanisms against Model Leeching.
- What evidence would resolve it: Testing known defense mechanisms (e.g., Membership Classification, Model Watermarking) against Model Leeching and measuring their ability to prevent or detect the attack.

## Limitations
- The attack was only tested on a single task (SQuAD QA) and single target model (ChatGPT-3.5-Turbo), limiting generalizability claims
- The economic analysis doesn't account for potential API rate limits, error rates requiring manual correction, or increased costs with more complex tasks
- The 11% improvement in attack success rates needs clearer baseline comparison and scalability assessment with more sophisticated attacks

## Confidence
- High Confidence: The core mechanism of using automated prompts to extract task-specific knowledge from LLMs is well-supported by the experimental results. The cost comparison between API-based labeling and human labeling is straightforward and verifiable.
- Medium Confidence: The transferability of adversarial attacks from extracted models to target LLMs shows promise but would benefit from testing across multiple attack types and target models to establish robustness.
- Low Confidence: Generalizability claims beyond the specific SQuAD task and ChatGPT-3.5-Turbo are not adequately supported. The paper doesn't explore how the attack performs against models with different architectures or training methodologies.

## Next Checks
1. Cross-task Validation: Test Model Leeching on at least three additional tasks (e.g., sentiment analysis, summarization, code generation) using the same methodology to assess whether the attack's effectiveness generalizes beyond QA tasks.

2. Target Model Diversity: Apply the extraction attack to alternative LLM architectures (e.g., Claude, LLaMA, PaLM) to determine whether the success rate depends on the specific characteristics of ChatGPT-3.5-Turbo or applies broadly across different model families.

3. Defense Robustness Testing: Implement and test common defense mechanisms (adversarial training, gradient masking, query limiting) against Model Leeching to identify which defenses are most effective at mitigating this attack vector.