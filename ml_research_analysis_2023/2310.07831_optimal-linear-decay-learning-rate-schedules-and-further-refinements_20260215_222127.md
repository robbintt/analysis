---
ver: rpa2
title: Optimal Linear Decay Learning Rate Schedules and Further Refinements
arxiv_id: '2310.07831'
source_url: https://arxiv.org/abs/2310.07831
tags:
- learning
- rate
- schedule
- page
- schedules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the theory-practice gap in learning rate scheduling
  for optimization algorithms like SGD. The core method involves a refined analysis
  that studies the last iterate (rather than the average) and uses observed gradient
  norms to derive problem-adaptive schedules.
---

# Optimal Linear Decay Learning Rate Schedules and Further Refinements

## Quick Facts
- arXiv ID: 2310.07831
- Source URL: https://arxiv.org/abs/2310.07831
- Reference count: 40
- Primary result: Linear decay schedules match or outperform other schedules across 10 diverse deep learning problems, with adaptive refinements providing further improvements

## Executive Summary
This paper addresses the gap between theoretical learning rate schedules (like 1/t decay) and practical choices (like linear decay and cosine annealing) by developing a new analysis framework focused on last-iterate convergence rather than averaged iterates. The key insight is that linear decay schedules naturally emerge from optimizing a novel bound on the final iterate's function value. The authors then extend this to create adaptive "refined" schedules that automatically generate both warm-up and rapid annealing properties by optimizing based on observed gradient norms during training.

## Method Summary
The method involves two phases: first, training with a linear decay schedule to collect gradient norm sequences; second, computing refined schedules using inverse squared gradient norms with median smoothing, then retraining with these schedules. The approach works with any optimizer that provides regret bounds and produces schedules that exhibit both warm-up (when gradient norms are large early in training) and rapid annealing (when gradient norms become small near convergence).

## Key Results
- Linear decay schedule (1 - t/T) matches or outperforms cosine annealing across 10 diverse deep learning problems
- Refined schedules based on observed gradient norms provide additional improvements over linear decay
- The refined schedules automatically exhibit both warm-up and rapid annealing properties without heuristic tuning
- Performance degradation occurs on ViT and RCNN tasks when gradient norms drop to near-zero

## Why This Works (Mechanism)

### Mechanism 1
Linear decay schedule provides optimal last-iterate convergence for convex G-Lipschitz problems by matching the contribution pattern of iterate averaging while directly optimizing the final iterate bound through the all-tail summation identity.

### Mechanism 2
Refined schedules automatically generate warm-up and rapid annealing by optimizing the bound with observed gradient norms (wt ∝ ∥gt∥−2), increasing when gradient norms are large (early training) and decreasing rapidly when gradient norms become small (late training).

### Mechanism 3
The refinement method generalizes to any optimization algorithm with bounded regret by converting regret bounds into last-iterate convergence guarantees through the all-tail summation identity.

## Foundational Learning

- **Subgradient calculus and convex optimization theory**: Needed to understand the subgradient bounds and convex function properties that form the basis of convergence guarantees. Quick check: What is the difference between a gradient and a subgradient for non-differentiable convex functions?

- **Online learning and regret bounds**: Required to understand how regret bounds relate to convergence rates of the final iterate through the all-tail summation identity. Quick check: How does the regret bound PTt=1⟨gt, zt − u⟩ relate to the convergence rate of the final iterate?

- **Stochastic approximation and Robbins-Monro conditions**: Important for understanding why traditional schedules like 1/t fail in practice due to anytime convergence requirements. Quick check: What are the Robbins-Monro conditions and why do they lead to poor practical performance?

## Architecture Onboarding

- **Component map**: Base optimizer (SGD/Adam) → Initial training run → Gradient norm collection → Schedule refinement → Final training run
- **Critical path**: Initial training → Gradient norm logging → Refinement calculation → Schedule application
- **Design tradeoffs**: Memory vs accuracy (storing full gradient norm sequence vs sampling), adaptivity vs stability (aggressive refinement vs conservative smoothing), generality vs specificity (algorithm-agnostic vs optimizer-specific optimizations)
- **Failure signatures**: Gradient norm sequence drops to near-zero causing degenerate schedules with exploding learning rates, refined schedules performing worse than linear decay on ViT and RCNN tasks
- **First 3 experiments**: 1) Simple logistic regression on LIBSVM dataset with Adam comparing linear decay vs cosine schedule, 2) CIFAR-10 with Wide ResNet using SGD testing linear decay vs step-wise schedule, 3) ImageNet with ResNet-50 comparing refined schedule vs linear decay baseline

## Open Questions the Paper Calls Out

### Open Question 1
Does the refinement technique work effectively when gradient norms drop to zero at the end of training due to overfitting? The paper acknowledges this as a limitation but doesn't provide a solution or further analysis on how to handle such cases.

### Open Question 2
How sensitive is the refinement technique to the choice of smoothing parameter τ, and can an adaptive method be developed to choose this parameter automatically? The paper suggests setting τ by eye rather than by grid search, implying potential sensitivity to this choice.

### Open Question 3
Can the refinement technique be extended to work with other optimizers like LAMB or Yogi, which have different update rules from Adam and SGD? The paper only demonstrates results for Adam and SGD, leaving open the question of applicability to other popular optimizers.

## Limitations

- Theoretical analysis assumes convex G-Lipschitz problems with known constants D and G, which may not hold for deep learning landscapes
- Refined schedule performance degrades on ViT and RCNN tasks when gradient norms drop to near-zero
- Method depends critically on gradient norm sequence from initial training being representative
- Focus on last-iterate convergence rather than averaged iterates may not align with practical generalization metrics

## Confidence

**High confidence**: Theoretical analysis of linear decay schedules for convex problems with bounded gradients - rigorous analysis through all-tail summation identity
**Medium confidence**: Refined schedule approach for convex problems where gradient norms remain stable - theoretically sound but practical implementation details matter
**Low confidence**: Refined schedule's effectiveness across all deep learning tasks - empirical results show clear degradation on some problems

## Next Checks

1. **Gradient norm stability analysis**: Plot gradient norm sequences throughout training for each of the 10 problems and identify epochs where norms drop below 10% of initial values; compare refined schedule performance on problems with stable vs unstable gradient norm sequences.

2. **Sensitivity to smoothing parameter**: Systematically vary median smoothing filter parameter τ from 0.05 to 0.5 and measure how refined schedule performance changes across different model architectures to identify whether aggressive smoothing mitigates zero-gradient issues.

3. **Hybrid schedule implementation**: Implement a hybrid approach that uses linear decay when gradient norms drop below a threshold (e.g., 0.1·∥g1∥) and refined scheduling otherwise; test this on problems showing degraded refined schedule performance to determine if it recovers lost accuracy.