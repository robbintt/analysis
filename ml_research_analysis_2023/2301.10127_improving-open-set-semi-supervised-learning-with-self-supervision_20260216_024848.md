---
ver: rpa2
title: Improving Open-Set Semi-Supervised Learning with Self-Supervision
arxiv_id: '2301.10127'
source_url: https://arxiv.org/abs/2301.10127
tags:
- data
- learning
- unlabeled
- training
- semi-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the problem of open-set semi-supervised learning,
  where a model must learn from a labeled dataset and an unlabeled dataset that may
  contain classes not present in the labeled data. The paper challenges the common
  assumption that out-of-distribution (OOD) data in the unlabeled set harms performance,
  proposing instead to leverage all available data through self-supervision.
---

# Improving Open-Set Semi-Supervised Learning with Self-Supervision

## Quick Facts
- arXiv ID: 2301.10127
- Source URL: https://arxiv.org/abs/2301.10127
- Reference count: 40
- Key outcome: This work challenges the assumption that OOD data in unlabeled sets harms SSL performance, proposing instead to leverage all data through self-supervision, achieving state-of-the-art results in closed-set accuracy and open-set recognition compared to existing methods.

## Executive Summary
This paper addresses open-set semi-supervised learning (OSSL) where models must learn from labeled data and unlabeled data that may contain classes not present in the labeled set. The authors challenge the conventional wisdom that OOD data in the unlabeled set harms performance, proposing instead to leverage all available data through self-supervision. They introduce SeFOSS, a framework that uses self-supervised consistency loss on all unlabeled data and an energy-based score to distinguish known classes from unknown ones. Extensive experiments demonstrate that SeFOSS achieves state-of-the-art results in both closed-set accuracy and open-set recognition compared to existing methods.

## Method Summary
SeFOSS is an OSSL framework that combines self-supervision on all unlabeled data with pseudo-labeling for in-distribution (ID) samples and energy regularization for out-of-distribution (OOD) samples. The method uses a cosine similarity loss between feature representations of weakly and strongly augmented versions of all unlabeled data as a self-supervised proxy for learning. For OSR, it employs a free-energy score based on treating logits as negative energies, which is theoretically aligned with the marginal distribution for ID data. The framework also uses adaptive confidence thresholds computed from the median and IQR of energy scores from labeled data, rather than fixed thresholds. The overall loss combines supervised cross-entropy, self-supervision cosine similarity, pseudo-labeling cross-entropy, and energy regularization terms.

## Key Results
- SeFOSS achieves state-of-the-art performance in both closed-set accuracy and open-set recognition compared to existing OSSL methods like FixMatch, MTCF, OpenMatch, and T2T.
- The method demonstrates that traditional SSL methods like FixMatch can perform competitively in closed-set accuracy even with OOD data present in the unlabeled set.
- Results show that using self-supervision on all unlabeled data provides a robust learning signal that is less sensitive to incorrect pseudo-labeling of OOD data compared to traditional SSL methods.

## Why This Works (Mechanism)

### Mechanism 1
Self-supervision on all unlabeled data provides a robust learning signal that is less sensitive to incorrect pseudo-labeling of OOD data compared to traditional SSL methods. The cosine similarity loss between feature representations of weakly and strongly augmented versions of all unlabeled data acts as a proxy for learning without requiring class predictions. This makes the learning signal less dependent on accurate outlier detection. The core assumption is that feature representations learned through self-supervision are meaningful for both ID and OOD data, and maintaining consistency across augmentations provides useful gradients regardless of class membership.

### Mechanism 2
Using free-energy score as a discriminant for OSR provides better separation between ID and OOD data than softmax confidence. The free-energy score F(x) = -1/β log(∑e^(-βE(x,y))) is theoretically aligned with the marginal distribution for ID data, p(x), making it more sensitive to distributional differences between ID and OOD data. The core assumption is that the free-energy score captures the uncertainty of a sample being from the ID distribution more effectively than maximum softmax probability.

### Mechanism 3
Adaptive confidence thresholds based on the median and IQR of energy scores from labeled data provide better OSR performance than fixed thresholds. By computing τid and τood as Sm ± Siqr · ζ where Sm is the median and Siqr is the interquartile range of energy scores on labeled data, the thresholds adapt to the specific distribution of the dataset rather than using arbitrary fixed values. The core assumption is that the distribution of energy scores on labeled ID data provides a meaningful baseline for distinguishing ID from OOD data in the unlabeled set.

## Foundational Learning

- **Concept: Semi-supervised learning fundamentals**
  - Why needed here: Understanding how SSL methods like FixMatch and UDA work is crucial for seeing how SeFOSS differs by using self-supervision on all data
  - Quick check question: What are the two main components of FixMatch's training objective?

- **Concept: Out-of-distribution detection**
  - Why needed here: The method needs to distinguish ID from OOD data effectively, and understanding existing OSR techniques helps appreciate why free-energy score is chosen
  - Quick check question: What is the key difference between softmax confidence and free-energy score for OSR?

- **Concept: Energy-based models**
  - Why needed here: The free-energy score is based on treating logits as negative energies, so understanding this framework is important
  - Quick check question: How does the free-energy score relate to the marginal distribution p(x)?

## Architecture Onboarding

- **Component map**: Input image -> Backbone f -> Feature representation -> Classification head g -> Class logits; Feature representation -> Projection head h -> Self-supervised space
- **Critical path**: Unlabeled data → weak and strong augmentations → feature extraction → self-supervision loss computation → gradient update
- **Design tradeoffs**: Using self-supervision on all data trades potential noise from OOD data for better utilization of available information, versus traditional methods that only use ID predictions
- **Failure signatures**: 
  - If closed-set accuracy is high but OSR performance is poor: likely issue with free-energy score or threshold adaptation
  - If both metrics are poor: likely issue with self-supervision or backbone architecture
  - If training is unstable: likely issue with energy regularization hyperparameters
- **First 3 experiments**:
  1. Implement just the self-supervision component on CIFAR-10 with CIFAR-100 as OOD, compare to FixMatch
  2. Add free-energy based OSR with fixed thresholds to verify improvement over softmax confidence
  3. Implement adaptive threshold calculation and compare to fixed thresholds on the same dataset

## Open Questions the Paper Calls Out
- The paper does not evaluate their method for very low-label regimes with extremely few labeled samples per class.
- The study only considers OSSL problems where the OOD data in training and testing follow the same distributions.
- The paper only uses ID sets that are balanced in terms of classes, not investigating how SeFOSS handles class-imbalanced ID sets.

## Limitations
- Limited dataset diversity: Experiments primarily focus on CIFAR datasets with limited domain variation
- Unspecified hyperparameters: Exact values of ζid, ζood, ξood for threshold computation are not fully specified
- Restricted scenarios: Only evaluates balanced ID sets and same-distribution OOD in training/testing

## Confidence
- Self-supervision mechanism effectiveness: Medium - supported by experimental results but limited dataset diversity
- Free-energy score superiority: Medium - theoretically sound but empirical comparison could be more comprehensive
- Adaptive threshold effectiveness: Medium - novel approach but sensitivity to hyperparameters not thoroughly explored

## Next Checks
1. Test SeFOSS on non-CIFAR datasets (e.g., ImageNet, real-world OOD scenarios) to verify generalizability beyond current experimental scope
2. Systematically vary ζid, ζood, and ξood parameters to understand their impact on OSR performance and determine optimal ranges for different dataset combinations
3. Include more recent OSSL methods and domain adaptation techniques in comparison to establish relative performance advantage in current research landscape