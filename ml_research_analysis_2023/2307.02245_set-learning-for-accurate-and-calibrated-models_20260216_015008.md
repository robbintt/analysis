---
ver: rpa2
title: Set Learning for Accurate and Calibrated Models
arxiv_id: '2307.02245'
source_url: https://arxiv.org/abs/2307.02245
tags:
- training
- data
- class
- calibration
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Odd-k-out Learning (OKO), a method that improves
  both calibration and accuracy of classifiers by training on sets of examples rather
  than single instances. OKO works by selecting sets containing a pair from one class
  and k examples each from different classes, then minimizing cross-entropy over the
  entire set.
---

# Set Learning for Accurate and Calibrated Models

## Quick Facts
- arXiv ID: 2307.02245
- Source URL: https://arxiv.org/abs/2307.02245
- Reference count: 40
- One-line primary result: OKO improves both calibration and accuracy by training on sets rather than single instances, achieving lower ECE and higher accuracy than standard cross-entropy, especially in low-data and class-imbalanced regimes.

## Executive Summary
This paper introduces Odd-k-out Learning (OKO), a novel method that improves both calibration and accuracy of classifiers by training on sets of examples rather than single instances. OKO works by selecting sets containing a pair from one class and k examples each from different classes, then minimizing cross-entropy over the entire set. This approach inherently smooths the loss landscape, reducing overconfidence and improving calibration—especially in low-data and class-imbalanced regimes—without requiring label smoothing or post-training temperature scaling. Experimental results across MNIST, FashionMNIST, CIFAR-10, and CIFAR-100 show OKO achieves lower Expected Calibration Error (ECE) and higher test accuracy than standard cross-entropy, weighted CE, batch-balancing, and label smoothing baselines.

## Method Summary
OKO modifies standard cross-entropy by training on sets rather than individual examples. The method samples sets containing a pair from one class and k examples each from different classes, then computes cross-entropy over the entire set. This set-based objective naturally smooths logits and prevents the extreme divergence seen in standard cross-entropy, where true class logits are pushed toward +∞ and others toward -∞. OKO can be implemented by modifying the forward pass to sum logits across sets and computing a single loss, requiring no architectural changes beyond an optional additional head for odd-class prediction.

## Key Results
- OKO improves 10-shot MNIST accuracy by 8.59% over prior work and consistently achieves lower calibration error across heavy-tailed class distributions.
- Across MNIST, FashionMNIST, CIFAR-10, and CIFAR-100, OKO achieves lower Expected Calibration Error (ECE) and higher test accuracy than standard cross-entropy, weighted CE, batch-balancing, and label smoothing baselines.
- OKO attains better calibration without requiring label smoothing or post-training temperature scaling, working effectively with hard labels alone.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OKO smooths logits by forcing a model to jointly consider pairs from one class and odd examples from other classes, reducing divergence of logit values.
- Mechanism: Standard cross-entropy strongly pushes true class logits toward +∞ and others toward -∞. OKO's set-based loss averages across all examples in a set, so each logit is constrained by the others. This implicit regularization keeps logits from diverging as strongly.
- Core assumption: Training on sets where at least two examples share a label, and the remaining are from different classes, creates natural smoothing of the loss landscape.
- Evidence anchors:
  - [abstract]: "OKO works by selecting sets containing a pair from one class and k examples each from different classes, then minimizing cross-entropy over the entire set."
  - [section]: "One issue with standard cross entropy is that it strongly encourages the entries of F to diverge... In other words: standard cross-entropy loss always encourages the logits of the true class to move towards ∞ and the logits of the wrong class towards −∞."
  - [corpus]: Weak signal. Only 1/8 neighbor papers mention calibration, none discuss set-based training or logit smoothing.

### Mechanism 2
- Claim: OKO implicitly performs entropic regularization by preventing overfitting to low-entropy regions, improving calibration.
- Mechanism: In datasets with noisy low-entropy regions (e.g., many samples from one feature vector but different labels), standard training overfits by assigning high confidence to such clusters. OKO's set sampling ensures singleton sets are rare, so the model learns to keep predictions high-entropy in those regions, spreading uncertainty to all samples of that class.
- Core assumption: Class-imbalanced or noisy datasets contain regions where many samples share features but differ in labels; set sampling breaks this overfitting pattern.
- Evidence anchors:
  - [abstract]: "This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes."
  - [section]: "In such a high-noise dataset it is likely that the low entropy regions are mislabeled... With OKO the learned model parameters extend the uncertainty in the noisy majority to all data points in the classes."
  - [corpus]: Weak. No corpus neighbor directly supports this mechanism; it is derived from the paper's theory section.

### Mechanism 3
- Claim: OKO achieves calibration without requiring label smoothing or temperature scaling because it trains on hard labels while still smoothing logits.
- Mechanism: Unlike label smoothing, which directly alters target distributions, OKO changes the training objective to sets. This changes how gradients flow and prevents logit divergence without changing the hard label targets, avoiding the need for post-hoc temperature scaling.
- Core assumption: Logit smoothing can be achieved by modifying the learning objective (set-based CE) rather than altering label distributions.
- Evidence anchors:
  - [abstract]: "Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling."
  - [section]: "Optimizing the (hard) OKO risk still allows F to diverge if that's advantageous... the OKO risk strikes a balance between the excessive overconfidence caused by standard cross-entropy and the inflexible calibration of fixed minima in label smoothing."
  - [corpus]: No corpus evidence; the claim is internal to the paper.

## Foundational Learning

- Concept: Cross-entropy as a proper scoring rule
  - Why needed here: OKO is a modification of cross-entropy on sets; understanding why standard CE causes overconfidence is essential.
  - Quick check question: What happens to logits in standard CE when a model memorizes training data perfectly?

- Concept: Class imbalance and resampling strategies
  - Why needed here: OKO is compared against batch-balancing and reweighting; knowing their mechanics helps interpret results.
  - Quick check question: How does batch-balancing differ from error reweighting in expectation?

- Concept: Entropy and calibration measures
  - Why needed here: OKO's calibration is measured via ECE and relative cross-entropy; understanding these metrics is necessary to interpret results.
  - Quick check question: Why is relative cross-entropy (RC) defined as H(P,Q) - H(Q), and what does it capture beyond KL divergence?

## Architecture Onboarding

- Component map: Data sampler -> OKOHead -> Loss function -> Backpropagation
- Critical path:
  1. Sample set S = (Sx, Sy) via OKO sampler
  2. Forward pass: OKOHead.set_sum(Sx) → summed logits
  3. Compute hard OKO loss: -log(softmax(summed_logits)[pair_class])
  4. (Optional) Compute odd-class loss with separate head
  5. Backpropagate combined loss
- Design tradeoffs:
  - k=1 is computationally cheapest and performs well, but larger k may increase calibration stability
  - Adding odd-class head slightly improves accuracy but increases compute
  - OKOHead introduces no new parameters beyond the extra odd-class head
- Failure signatures:
  - If set sampling fails to ensure class diversity, calibration degrades to standard CE behavior
  - If batch size is too small, set sampling may fail to produce valid sets
  - If k=0, method collapses to pair-only training with poor calibration
- First 3 experiments:
  1. Train OKO with k=1 on MNIST, compare ECE vs vanilla CE
  2. Compare OKO vs batch-balancing + label smoothing on CIFAR-10 with heavy-tailed distribution
  3. Measure impact of adding odd-class head on test accuracy and calibration on FashionMNIST

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does OKO's calibration benefit extend to other architectures beyond CNNs and ResNets, such as transformers or vision transformers?
- Basis in paper: [explicit] The paper notes that OKO "can be easily adapted to many settings" and "does not modify the model architecture," but experiments only used CNNs and ResNets.
- Why unresolved: The experiments were limited to standard image classification architectures, leaving uncertainty about performance with modern architectures like transformers.
- What evidence would resolve it: Experiments applying OKO to transformer-based models (ViT, Swin, etc.) across various vision and language tasks with calibration metrics.

### Open Question 2
- Question: How does OKO's performance scale with dataset size and diversity, particularly in the infinite data limit?
- Basis in paper: [inferred] The paper shows strong performance in low-data regimes and mentions OKO's benefits are "especially in limited training data," but doesn't explore very large-scale datasets.
- Why unresolved: All experiments used moderate-sized datasets, and the theoretical analysis focuses on the low-data regime without addressing the asymptotic behavior.
- What evidence would resolve it: Comparative studies on web-scale datasets (ImageNet-21k, JFT-300M) measuring both accuracy and calibration as training set size increases.

### Open Question 3
- Question: What is the theoretical relationship between OKO's set-based learning and other regularization methods like dropout or batch normalization?
- Basis in paper: [explicit] The paper establishes that OKO provides implicit smoothing but doesn't compare this to other regularization mechanisms or explore potential synergies.
- Why unresolved: While OKO is shown to be theoretically distinct from label smoothing, the relationship to other regularization methods remains unexplored.
- What evidence would resolve it: Theoretical analysis connecting OKO's set-based objective to established regularization frameworks, possibly through empirical studies combining OKO with other methods.

## Limitations

- The method's effectiveness in extremely low-data regimes (e.g., few-shot learning) beyond the 10-shot experiments is not thoroughly explored.
- The computational overhead of set sampling and processing could become prohibitive for very large batch sizes or models with expensive forward passes.
- The theoretical analysis focuses on binary classification and extends to multi-class via reduction, which may not fully capture complex decision boundaries in high-dimensional spaces.

## Confidence

- **High Confidence**: Claims about OKO improving calibration (ECE reduction) are well-supported by consistent experimental results across multiple datasets and comparison methods.
- **Medium Confidence**: Claims about OKO's effectiveness in class-imbalanced and low-data regimes are supported but could benefit from more extensive ablations.
- **Low Confidence**: The claim that OKO inherently achieves better calibration than label smoothing without requiring temperature scaling is supported empirically but lacks rigorous theoretical justification.

## Next Checks

1. **Ablation on Set Size k**: Systematically vary k values beyond k=1 to determine the optimal set size for different dataset sizes and complexity levels.

2. **Transfer to Other Model Architectures**: Test OKO on transformer-based architectures and larger vision models to verify that calibration improvements generalize beyond small CNNs.

3. **Robustness to Noisy Labels**: Conduct experiments with varying levels of label noise to test OKO's effectiveness in handling noisy training data, particularly focusing on whether the entropic regularization mechanism holds under different noise distributions.