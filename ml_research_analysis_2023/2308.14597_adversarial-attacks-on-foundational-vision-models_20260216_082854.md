---
ver: rpa2
title: Adversarial Attacks on Foundational Vision Models
arxiv_id: '2308.14597'
source_url: https://arxiv.org/abs/2308.14597
tags:
- attack
- adversarial
- target
- learning
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates how adversarial attacks can exploit vulnerabilities\
  \ in foundational vision models like CLIP, DINOv2, and SWAG. The authors develop\
  \ two attacks: ID\u2192OOD, which makes in-distribution images appear out-of-distribution,\
  \ and OOD\u2192ID, which makes random noise appear in-distribution."
---

# Adversarial Attacks on Foundational Vision Models

## Quick Facts
- arXiv ID: 2308.14597
- Source URL: https://arxiv.org/abs/2308.14597
- Authors: 
- Reference count: 40
- Primary result: Adversarial attacks can reliably exploit vulnerabilities in foundational vision models like CLIP, DINOv2, and SWAG, causing in-distribution images to appear out-of-distribution and vice versa.

## Executive Summary
This paper demonstrates how adversarial attacks can exploit vulnerabilities in foundational vision models like CLIP, DINOv2, and SWAG. The authors develop two attacks: ID→OOD, which makes in-distribution images appear out-of-distribution, and OOD→ID, which makes random noise appear in-distribution. They show these attacks are effective in whitebox and blackbox settings, and can transfer across different foundational model architectures. The attacks work by manipulating deep feature representations to fool OOD detectors, which are critical for using these models in closed-set downstream tasks. The authors provide concrete suggestions for improving the robustness of future foundational models.

## Method Summary
The paper presents two adversarial attacks targeting foundational vision models. The first attack (ID→OOD) uses the Away From Start (AFS) method to push in-distribution images into out-of-distribution regions in feature space, measured by minimizing cosine similarity with the starting representation. The second attack (OOD→ID) combines Towards Target and Away From Start methods to make random noise appear as in-distribution by maximizing similarity to target representations while maintaining distance from the starting point. Both attacks employ ℓ∞ constraints, diverse input transformations, ensemble methods, and translation invariance to enhance effectiveness and transferability.

## Key Results
- Adversarial examples crafted on CLIP models transfer reliably to DINOv2 and SWAG models despite different training algorithms
- Both attacks achieve high success rates in whitebox and blackbox settings with low perturbation budgets (ℓ∞ ϵ=8/255)
- Attacks significantly degrade OOD detector performance, causing high false positive rates while maintaining reasonable image quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial examples crafted on CLIP models can transfer across different foundational model training algorithms (from CLIP to DINOv2 and SWAG).
- Mechanism: The attacks manipulate deep feature representations that are aligned across different foundational models, despite their different learning mechanisms (contrastive, self-distillation, weak-supervision).
- Core assumption: There exists a shared semantic space or feature alignment between different foundational models that can be exploited.
- Evidence anchors:
  - [abstract]: "We show that our attacks can reliably transfer across foundational model training algorithms (from CLIP to DINOv2 and SWAG)"
  - [section 4.1.4]: "We believe that this transferability suggests some amount of feature 'alignment' between the different foundational models that is worth a future study."
- Break condition: If foundational models diverge significantly in their learned feature representations or if the semantic alignment between them is disrupted.

### Mechanism 2
- Claim: Adversarial perturbations can cause in-distribution images to be predicted as out-of-distribution and vice versa.
- Mechanism: The attacks perturb images in feature space to move them into regions that are misclassified by OOD detectors, exploiting the reliance of these detectors on feature space boundaries.
- Core assumption: OOD detectors are based on feature space representations and can be fooled by perturbations that move examples across these boundaries.
- Evidence anchors:
  - [abstract]: "Our methods reliably make in-distribution (ID) images (w.r.t. the downstream task) be predicted as OOD and vice versa"
  - [section 3.1]: "Our first attack perturbs an ID input such that it's predicted by the target model as OOD"
- Break condition: If OOD detectors become more robust to feature space perturbations or if the feature space becomes less sensitive to adversarial perturbations.

### Mechanism 3
- Claim: The centralized distribution of foundational models on platforms like HuggingFace and torch.hub creates a predictable attack surface.
- Mechanism: The limited number of organizations capable of training these models at scale results in a small set of popular models, making it easier for attackers to guess which model a target is using.
- Core assumption: The distribution of foundational models is concentrated among a few popular choices, creating a bottleneck that can be exploited.
- Evidence anchors:
  - [abstract]: "given the complexity of working at this scale, there is a bottleneck where relatively few organizations in the world are executing the training then sharing the models on centralized platforms"
  - [section 1]: "Given the centralized distribution of these models on HuggingFace and torch.hub and low diversity of organizations capable training at this scale, it's feasible that an adversary can make an accurate guess at the exact model their target is using"
- Break condition: If the diversity of foundational models increases significantly or if the distribution becomes more decentralized.

## Foundational Learning

- Concept: Contrastive learning and self-supervised learning
  - Why needed here: These are the primary training methods for foundational vision models, which the attacks target
  - Quick check question: What is the key difference between supervised learning and self-supervised learning in terms of data requirements?

- Concept: Feature space representations and embeddings
  - Why needed here: The attacks manipulate deep feature representations, so understanding how models represent data in feature space is crucial
  - Quick check question: How do feature space representations differ between supervised models and foundational models?

- Concept: Out-of-distribution detection
  - Why needed here: The attacks specifically target OOD detectors, so understanding how they work is essential
  - Quick check question: What are the main approaches to OOD detection and how do they differ in their assumptions about the feature space?

## Architecture Onboarding

- Component map: Input image → Vision encoder → Projector → Feature space → OOD detector/Classifier head
- Critical path: Input image → Vision encoder → Projector → Feature space → OOD detector/Classifier head
- Design tradeoffs:
  - Model size vs. computational efficiency
  - Zero-shot flexibility vs. task-specific performance
  - OOD detection robustness vs. ID classification accuracy
  - Centralized distribution vs. model diversity
- Failure signatures:
  - High AUROC and low FPR95 for clean data but poor performance on adversarial examples
  - Cross-architecture transfer of adversarial examples
  - Sensitivity to ℓ∞ perturbations
- First 3 experiments:
  1. Generate adversarial examples using AFS attack on a CLIP model and evaluate OOD detection performance
  2. Test transferability of adversarial examples to DINOv2 and SWAG models
  3. Vary perturbation strength (ℓ∞ ϵ) and measure impact on attack success metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can foundational vision models be made more robust to adversarial attacks while maintaining their task-agnostic properties?
- Basis in paper: [explicit] The authors conclude by suggesting future work on "adversarially robust representation learning at the foundational model scale" and "improved attacking methods for current-generation foundational vision models."
- Why unresolved: While the paper identifies vulnerabilities, it does not propose concrete solutions for improving robustness without compromising the models' flexibility.
- What evidence would resolve it: Development and evaluation of new training methods or architectures that improve adversarial robustness while preserving or enhancing task-agnostic performance.

### Open Question 2
- Question: What is the underlying reason for the high transferability of adversarial attacks across different foundational model architectures (e.g., CLIP to DINOv2 to SWAG)?
- Basis in paper: [explicit] The authors observe that "adversarial samples generated on CLIP models transferred to DINOv2 and SWAG models despite the learning algorithms being so different" and suggest this "could potentially expand the convergent learning theories from efforts focused on supervised learning."
- Why unresolved: The paper notes this phenomenon but does not investigate the mechanism behind it or its implications for model design.
- What evidence would resolve it: Detailed analysis of feature space alignments across different foundational models and theoretical or empirical explanations for why certain perturbations transfer effectively.

### Open Question 3
- Question: How do different starting images (e.g., natural images vs. random noise) impact the effectiveness of OOD→ID attacks?
- Basis in paper: [explicit] The authors note that "it would be interesting to explore the power of our Towards Target + Away From Start attack when the starting image is from the natural imagery domain" and suggest future work to "tweak the threat model s.t. the attacker is granted slightly more information about the downstream task."
- Why unresolved: The current study uses random noise as the starting point for OOD→ID attacks, leaving open the question of whether more semantically related starting images would yield stronger attacks.
- What evidence would resolve it: Systematic comparison of attack success rates using different types of starting images (e.g., random noise, natural images from related categories, natural images from unrelated categories) while controlling for other variables.

## Limitations

- The transferability mechanism across different foundational model architectures remains speculative without rigorous empirical validation
- Attack effectiveness may not generalize to alternative OOD detection methods beyond the tested MCM and MSP approaches
- The practical feasibility of attacks in real-world scenarios depends on maintaining perceptual quality of adversarial examples

## Confidence

- High Confidence: The basic attack methodology and its effectiveness against OOD detectors in controlled settings
- Medium Confidence: Cross-architecture transferability claims and the proposed feature alignment mechanism
- Low Confidence: Real-world attack feasibility and robustness against alternative OOD detection methods

## Next Checks

1. **Ablation Study on Feature Alignment**: Conduct experiments comparing feature similarity metrics (cosine similarity, mutual information) between attacked examples and their targets across different model pairs to quantify the alignment mechanism.

2. **OOD Detector Robustness Testing**: Evaluate attack effectiveness against alternative OOD detection methods (energy-based, density-based) and with different feature space transformations to assess vulnerability scope.

3. **Perceptual Quality Analysis**: Systematically measure perturbation visibility using established metrics (SSIM, LPIPS) across different perturbation strengths and downstream tasks to establish practical attack limits.