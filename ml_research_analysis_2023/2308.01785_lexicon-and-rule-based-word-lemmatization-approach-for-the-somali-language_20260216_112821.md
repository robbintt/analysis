---
ver: rpa2
title: Lexicon and Rule-based Word Lemmatization Approach for the Somali Language
arxiv_id: '2308.01785'
source_url: https://arxiv.org/abs/2308.01785
tags:
- words
- lexicon
- lemmatization
- somali
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first lemmatization system for the Somali
  language, a low-resource language with limited NLP resources. The proposed approach
  combines a lexicon-based dictionary look-up method with rule-based lemmatization,
  considering Somali language morphology.
---

# Lexicon and Rule-based Word Lemmatization Approach for the Somali Language

## Quick Facts
- arXiv ID: 2308.01785
- Source URL: https://arxiv.org/abs/2308.01785
- Reference count: 13
- Primary result: First lemmatization system for Somali achieving 95.87% accuracy on short texts, 60.57% on news extracts, and 57% on full news articles

## Executive Summary
This paper introduces the first lemmatization system for the Somali language, addressing the challenge of processing this morphologically rich, low-resource language. The authors develop a combined lexicon and rule-based approach, manually constructing a lexicon of 1247 root words and 7173 derivational forms. The system demonstrates promising results, particularly for short texts like social media messages, though performance decreases significantly for longer, more diverse documents. This work establishes a foundational resource for Somali language processing and outlines paths for future development.

## Method Summary
The proposed approach combines lexicon-based dictionary lookup with rule-based lemmatization for Somali. The algorithm first attempts to find a word in the constructed lexicon (1247 roots + 7173 derivations). If not found, it applies morphological rules based on word beginnings and suffixes. The lexicon is organized as key-value pairs with roots as keys and derivational forms as values. Rules handle words not present in the lexicon by analyzing possible suffixes and word beginnings. The system was evaluated on 120 documents from news articles and social media, showing varying performance across document types.

## Key Results
- 95.87% accuracy for short texts like social media messages
- 60.57% accuracy for news article extracts
- 57% accuracy for full news articles
- Lexicon contains 1247 root words and 7173 derivational forms
- First published lemmatization system for Somali language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The combined lexicon and rule-based approach increases lemmatization accuracy by covering both known and unknown morphological forms.
- Mechanism: Words are first searched in the lexicon (1247 roots + 7173 derivations). If not found, rules based on word beginnings are applied. This two-stage process captures both memorized forms and generalizable patterns.
- Core assumption: Somali morphology follows predictable suffix and prefix patterns that can be encoded as rules.
- Evidence anchors:
  - [abstract] "We have developed an initial lexicon of 1247 root words and 7173 derivationally related terms enriched with rules for lemmatizing words not present in the lexicon."
  - [section] "Because of the limited size of the lexicon created in this initial work, we have also developed simple rules for handling words that are not found in the lexicon to improve the lemmatizer accuracy."
  - [corpus] Weak - corpus only shows neighbor papers on lemmatization, not specific validation of Somali rules.
- Break condition: If Somali morphology has irregular or unpredictable forms that cannot be captured by suffix/prefix rules, accuracy will degrade for those cases.

### Mechanism 2
- Claim: Higher accuracy on short texts (social media) vs long texts (news articles) is due to lexicon coverage bias toward common, frequent words.
- Mechanism: Short texts contain more frequently used words that are likely in the lexicon. Long texts contain more diverse vocabulary, including rare words and domain-specific terms outside the lexicon.
- Core assumption: The lexicon was constructed from "most frequently used words in several domains such as news, sport, and social media."
- Evidence anchors:
  - [section] "The authors, who are native Somali speakers, compiled an initial collection of the most frequently used words in several domains such as news, sport, and social media."
  - [section] "Results show promising performance with 95.87% accuracy for short texts like social media messages, 60.57% for news article extracts, and 57% for full news articles."
  - [corpus] Weak - corpus doesn't provide frequency analysis of Somali words.
- Break condition: If lexicon coverage improves for rare words, the accuracy gap between short and long texts should decrease.

### Mechanism 3
- Claim: The key-value lexicon structure (roots as keys, derivations as values) enables efficient O(1) average lookup time for known words.
- Mechanism: The lexicon is stored as a dictionary where searching for a word returns its root immediately if present, avoiding exhaustive search through all entries.
- Core assumption: The underlying data structure is a hash map or similar constant-time lookup structure.
- Evidence anchors:
  - [section] "The lexicon is organized into key-value pairs with the keys representing root words and values constituting the derivational forms of the roots."
  - [section] "Constructed in those pairs of root and derived words, the lexicon dataset is stored in a machine-readable dictionary, which is easily searchable..."
  - [corpus] Weak - corpus doesn't address implementation efficiency.
- Break condition: If the lexicon grows very large or hash collisions become frequent, lookup performance may degrade.

## Foundational Learning

- Concept: Morphological analysis
  - Why needed here: Understanding how Somali words change form through affixation is essential for both lexicon construction and rule design
  - Quick check question: How does Somali mark tense, person, and number in verbs compared to English?

- Concept: Lexicon-based vs rule-based approaches
  - Why needed here: The system combines both methods, so understanding their tradeoffs (coverage vs precision) is crucial
  - Quick check question: When would a lexicon-based approach outperform a rule-based one for a morphologically rich language?

- Concept: Evaluation metrics for NLP preprocessing
  - Why needed here: Accuracy is used here, but understanding precision, recall, and their relevance to lemmatization is important for proper assessment
  - Quick check question: Why might high lemmatization accuracy not guarantee high performance in downstream NLP tasks?

## Architecture Onboarding

- Component map: Input → Preprocessor (tokenization, stopword/punctuation removal) → Lexicon Lookup → Rule-based fallback → Output (lemmatized words)
- Critical path: The path where a word is found in the lexicon (most common case for short texts)
- Design tradeoffs: Large lexicon = better coverage but more memory; complex rules = better generalization but potential for errors
- Failure signatures: High unresolved word count indicates either lexicon gaps or rule mismatches; low accuracy on long texts indicates coverage issues
- First 3 experiments:
  1. Test lexicon-only lookup on a corpus of common Somali words to establish baseline coverage
  2. Apply rule-based component to words not in lexicon to measure rule effectiveness
  3. Compare accuracy across text lengths with a balanced test set to quantify short-text bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed lemmatizer scale with increasing lexicon size?
- Basis in paper: [explicit] The paper mentions future plans to increase lexicon size and improve accuracy
- Why unresolved: The paper only presents initial results with a lexicon of 1247 root words and 7173 derivations
- What evidence would resolve it: Systematic evaluation of lemmatizer performance with incrementally larger lexicon sizes

### Open Question 2
- Question: Can the lemmatization accuracy be improved for long documents through hybrid approaches combining rule-based and machine learning methods?
- Basis in paper: [inferred] The paper notes lower accuracy (57%) for full news articles compared to short texts (95.87%)
- Why unresolved: The paper only presents lexicon and rule-based approaches without exploring hybrid methods
- What evidence would resolve it: Comparative evaluation of lexicon/rule-based vs hybrid machine learning approaches on long documents

### Open Question 3
- Question: What is the minimum corpus size required to automatically generate a comprehensive Somali lemmatization lexicon?
- Basis in paper: [explicit] The paper mentions investigating automatic lexicon construction from corpus data as future work
- Why unresolved: The paper only presents manually constructed lexicon without exploring automatic methods
- What evidence would resolve it: Experiments varying corpus sizes to determine minimum threshold for comprehensive lexicon generation

## Limitations

- Significant performance gap between short (95.87%) and long texts (57%), suggesting limited coverage for domain-specific vocabulary
- No precision and recall metrics provided, making it difficult to assess false positive vs false negative rates
- Lemmatization rules are not fully specified, limiting reproducibility and evaluation of their completeness

## Confidence

- **High Confidence**: The fundamental approach of combining lexicon lookup with rule-based fallback is sound and well-documented. The accuracy measurement methodology (correct lemmatizations divided by total input words) is clearly specified.
- **Medium Confidence**: The reported accuracy figures are likely valid for the specific test corpus used, but their generalizability to other Somali text domains is uncertain due to the significant performance variation across text types.
- **Low Confidence**: The effectiveness and completeness of the lemmatization rules cannot be fully evaluated without more detailed documentation of their design and coverage.

## Next Checks

1. **Rule Coverage Analysis**: Manually test the rule-based component on a diverse sample of 100 Somali words not in the lexicon to determine what percentage can be correctly lemmatized, identifying specific morphological patterns the rules handle or miss.

2. **Cross-Domain Evaluation**: Evaluate the system on a balanced corpus containing equal proportions of social media, news, and technical Somali text to quantify performance variance across domains and identify coverage gaps.

3. **Precision-Recall Assessment**: For a subset of test cases, have native Somali speakers judge not just whether lemmatization is "correct" but also identify false positives (incorrect lemmatizations) and false negatives (unresolved words) to compute precision and recall metrics.