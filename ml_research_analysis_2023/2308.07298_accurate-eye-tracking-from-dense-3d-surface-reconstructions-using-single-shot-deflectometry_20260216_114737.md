---
ver: rpa2
title: Accurate Eye Tracking from Dense 3D Surface Reconstructions using Single-Shot
  Deflectometry
arxiv_id: '2308.07298'
source_url: https://arxiv.org/abs/2308.07298
tags:
- surface
- camera
- gaze
- deflectometry
- display
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel reflection-based eye tracking method
  that leverages dense 3D surface reconstructions from single-shot deflectometry measurements.
  Unlike traditional glint tracking methods that rely on sparse 3D data from the cornea,
  the proposed approach acquires dense 3D information of both cornea and sclera within
  a single camera frame.
---

# Accurate Eye Tracking from Dense 3D Surface Reconstructions using Single-Shot Deflectometry

## Quick Facts
- arXiv ID: 2308.07298
- Source URL: https://arxiv.org/abs/2308.07298
- Authors: 
- Reference count: 40
- Primary result: Gaze errors below 0.12° on model eye and 0.46°-0.97° on human eyes in vivo

## Executive Summary
This paper introduces a novel reflection-based eye tracking method that leverages dense 3D surface reconstructions from single-shot deflectometry measurements. Unlike traditional glint tracking methods that rely on sparse 3D data from the cornea, the proposed approach acquires dense 3D information of both cornea and sclera within a single camera frame. The method uses phase-measuring deflectometry (PMD) to capture over 3,000 times more surface reflection points than conventional techniques, achieving gaze errors below 0.12° on a model eye and between 0.46° and 0.97° on real human eyes in vivo.

## Method Summary
The method employs single-shot stereo deflectometry with crossed sinusoidal patterns displayed on a mobile screen. Two cameras capture the reflected patterns from the eye surface, and 2D continuous wavelet transforms extract phase information from each frame. This phase data maps to surface normals through ray tracing, and stereo reconstruction resolves the depth-normal ambiguity. An iterative surface integration algorithm combines local slope measurements into a complete 3D surface reconstruction. Gaze direction is calculated by back-tracing surface normals to find cornea and sclera centers, then determining the optical axis connecting these points.

## Key Results
- Gaze errors below 0.12° on model eye
- Gaze errors between 0.46° and 0.97° on real human eyes in vivo
- >3000× more surface reflection points than conventional methods
- Single-shot capability enables motion-robust measurements regardless of eye movement speed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense surface sampling increases gaze estimation accuracy.
- Mechanism: Traditional glint tracking uses ~12 sparse corneal reflection points. This method captures >40,000 points from both cornea and sclera using PMD, providing much richer 3D surface data.
- Core assumption: More surface points improve gaze direction estimation through better characterization of eye geometry.
- Evidence anchors:
  - [abstract] "For a typical measurement, we acquire >3000 × more surface reflection points ('glints') than conventional methods."
  - [section] "Compared to glint tracking with 12 points light sources, this is an improvement in acquired surface points of 3300×"
- Break condition: If surface normals become unreliable (e.g., from limbus regions or occlusion), the benefit diminishes.

### Mechanism 2
- Claim: Single-shot deflectometry enables motion-robust measurements.
- Mechanism: Uses 2D continuous wavelet transforms on cross-sinusoid patterns to extract phase information from one camera frame, avoiding motion artifacts from temporal phase-shifting.
- Core assumption: The wavelet-based phase extraction is robust to local frequency variations and object motion within a single frame.
- Evidence anchors:
  - [abstract] "Due to its single-shot character, it allows for fully motion-robust measurements, regardless of the moving speed of the eye."
  - [section] "To obtain the display-camera correspondence information in single-shot, the unidirectional sinusoidal pattern on the display is replaced with a crossed sinusoidal pattern."
- Break condition: If eye motion exceeds camera exposure time, causing severe motion blur.

### Mechanism 3
- Claim: Stereo deflectometry resolves the depth-normal ambiguity.
- Mechanism: Two cameras observe the same specular reflection from different angles. Each camera has its own set of possible surface normal-height combinations, but only one combination matches both views.
- Core assumption: Accurate calibration and sufficient overlap between cameras' views.
- Evidence anchors:
  - [section] "A common solution to this 'depth-normal ambiguity problem' [36] is to add a second camera and turn the system into a 'stereo deflectometry' system [24]."
  - [section] "Each camera has its own set of possible surface normal-height combinations, but both sets only match for one combination, which is the true height and normal of the surface."
- Break condition: Poor calibration or limited overlap between camera views.

## Foundational Learning

- Concept: Phase-shifting deflectometry
  - Why needed here: Provides the theoretical foundation for PMD and understanding why single-shot methods are needed for moving eyes.
  - Quick check question: What is the fundamental limitation of classical phase-shifting deflectometry for eye tracking?

- Concept: Continuous wavelet transform
  - Why needed here: Enables single-shot phase extraction from crossed fringe patterns, critical for motion-robust measurements.
  - Quick check question: How does the wavelet-based method handle local frequency variations compared to Fourier-based approaches?

- Concept: Ray tracing for surface normal calculation
  - Why needed here: Essential for converting correspondence information between display and camera pixels into 3D surface normals.
  - Quick check question: Why can't a single pair of corresponding points uniquely determine both surface normal and height?

## Architecture Onboarding

- Component map:
  iPhone 12 Pro display (2532 × 1170px) showing crossed sinusoidal pattern -> Two Flea3 machine vision cameras with 9mm objectives -> Novel switchable specular screen calibration -> Wavelet transform processing -> Correspondence mapping -> Stereo reconstruction -> Iterative surface integration -> Gaze estimation

- Critical path: Display pattern generation -> Camera capture -> Wavelet phase extraction -> Correspondence mapping -> Stereo reconstruction -> Surface integration -> Gaze estimation

- Design tradeoffs:
  - Coverage vs. resolution: Larger display provides more coverage but lower effective resolution per eye surface point
  - Pattern complexity: Crossed sinusoids enable single-shot capture but require more complex processing than simple glints
  - Camera placement: Wider baseline improves depth resolution but may reduce coverage of peripheral eye regions

- Failure signatures:
  - Pattern visibility degradation: Low contrast or occlusion from eyelids/lashes
  - Calibration drift: Misalignment between cameras and display over time
  - Normal outliers: Limbus regions or blood vessels creating invalid surface normals
  - Coverage gaps: Insufficient pattern reflection from sclera due to diffuse scattering

- First 3 experiments:
  1. Verify wavelet-based phase extraction works on static eye model with known geometry
  2. Test stereo reconstruction accuracy using bearing ball with known radius (12mm ± 0.00125mm)
  3. Validate gaze estimation on rotation stage with controlled angular positions (-4° to +4°)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of surface points needed to achieve maximum gaze tracking accuracy without introducing computational complexity?
- Basis in paper: [explicit] The paper mentions acquiring >40,000 surface points but doesn't determine the point at which accuracy gains plateau
- Why unresolved: The authors demonstrate significant improvements with dense surface reconstruction but don't explore the trade-off between point density and accuracy gains
- What evidence would resolve it: Systematic experiments varying the number of acquired surface points and measuring corresponding accuracy improvements would establish an optimal point

### Open Question 2
- Question: How does the method perform across different eye shapes and conditions (e.g., wearing glasses, contact lenses, different ethnicities)?
- Basis in paper: [inferred] The method is tested on a model eye and limited human subjects, but no systematic evaluation across diverse eye conditions
- Why unresolved: The paper demonstrates feasibility but doesn't address robustness across the full range of real-world variations
- What evidence would resolve it: Large-scale testing across diverse populations with varying eye characteristics and conditions would establish robustness

### Open Question 3
- Question: Can the computational efficiency be improved to enable real-time gaze tracking in resource-constrained devices like VR headsets?
- Basis in paper: [explicit] The authors mention computational expense of stereo deflectometry and potential for real-time implementation
- Why unresolved: While the method shows high accuracy, no implementation details or optimization strategies for real-time performance are provided
- What evidence would resolve it: Benchmark studies comparing processing times with various optimization techniques and hardware configurations would demonstrate feasibility for real-time applications

### Open Question 4
- Question: What is the impact of eye movement speed on measurement accuracy, particularly for saccadic movements?
- Basis in paper: [explicit] The paper emphasizes single-shot capability for motion-robust measurements but doesn't quantify performance at different speeds
- Why unresolved: The authors claim motion-robustness but don't provide empirical data on how accuracy degrades with increasing eye movement speed
- What evidence would resolve it: Controlled experiments measuring accuracy across different eye movement speeds, including rapid saccades, would establish performance limits

## Limitations

- Computational expense of dense 3D reconstruction may limit real-time performance on mobile devices
- Accuracy heavily depends on precise calibration and alignment between display and cameras
- Performance with partial occlusions from eyelids or lashes remains unclear

## Confidence

**High Confidence** (Mechanistic Understanding): The core mechanisms of dense surface reconstruction and stereo deflectometry are well-established in the literature, with clear mathematical foundations. The phase-measuring deflectometry approach is theoretically sound and the experimental results demonstrate consistent improvements over sparse glint methods.

**Medium Confidence** (Implementation Details): While the method's framework is clearly described, several implementation specifics remain unclear. The exact wavelet transform parameters, calibration procedure details, and specific handling of surface integration boundaries are not fully specified. The error metrics comparison with state-of-the-art methods, though promising, requires independent verification.

**Low Confidence** (Generalizability): The method's performance across diverse populations (different eye shapes, skin tones, ages) and real-world conditions (varying lighting, glasses, contact lenses) has not been thoroughly demonstrated. The in vivo human experiments, while showing promising results, involved a limited sample size that may not represent broader population variability.

## Next Checks

1. **Robustness to Partial Occlusion**: Systematically test the method's performance with varying degrees of eyelid coverage (0% to 75%) and eyelash interference, measuring how reconstruction quality degrades with occlusion levels.

2. **Calibration Stability Over Time**: Evaluate the switchable specular screen calibration accuracy over extended periods (24+ hours) under different temperature and humidity conditions to assess drift and recalibration frequency requirements.

3. **Real-time Performance Benchmarking**: Implement the complete pipeline on target hardware (e.g., mobile VR processor) and measure end-to-end latency, frame rate stability, and power consumption to verify practical deployment feasibility.