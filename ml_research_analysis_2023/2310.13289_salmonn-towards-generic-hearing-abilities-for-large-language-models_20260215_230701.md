---
ver: rpa2
title: 'SALMONN: Towards Generic Hearing Abilities for Large Language Models'
arxiv_id: '2310.13289'
source_url: https://arxiv.org/abs/2310.13289
tags:
- audio
- speech
- salmonn
- tasks
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SALMONN, the first multimodal large language
  model capable of processing and understanding general audio inputs consisting of
  speech, audio events, and music. The model uses a dual encoder structure with a
  Whisper speech encoder and a BEATs audio encoder, connected via a window-level query
  Transformer to a pre-trained Vicuna LLM with LoRA adaptors.
---

# SALMONN: Towards Generic Hearing Abilities for Large Language Models

## Quick Facts
- arXiv ID: 2310.13289
- Source URL: https://arxiv.org/abs/2310.13289
- Reference count: 30
- Key outcome: First multimodal LLM capable of processing general audio inputs (speech, events, music) with emergent cross-modal abilities

## Executive Summary
SALMONN introduces the first multimodal large language model capable of processing general audio inputs through a dual encoder architecture combining Whisper (speech) and BEATs (non-speech audio) encoders with a window-level query Transformer that connects to a Vicuna LLM. The model is trained through three stages: pre-training, instruction tuning on multiple audio tasks, and activation tuning to restore emergent abilities lost during task-specific fine-tuning. SALMONN achieves competitive performance on trained tasks and demonstrates emergent abilities for untrained cross-modal tasks including speech translation to unseen languages and speech-based reasoning.

## Method Summary
SALMONN uses a dual encoder structure with Whisper for speech and BEATs for non-speech audio, synchronized at 50Hz frame rate. A window-level query Transformer segments the concatenated encoder outputs into fixed-size windows (0.33 seconds) to efficiently process variable-length audio while maintaining temporal resolution. The processed features connect to a Vicuna LLM with LoRA adaptors through text-like tokens. The model undergoes three training stages: pre-training to learn basic audio-text alignment, instruction tuning on specific speech/audio/music tasks, and activation tuning with few-shot examples to restore emergent abilities that were lost due to task over-fitting during instruction tuning.

## Key Results
- Achieves competitive performance on trained tasks including ASR (WER), speech translation (BLEU), emotion recognition (accuracy), and audio captioning (SPIDEr)
- Demonstrates emergent abilities for untrained tasks: speech translation to untrained languages, speech-based slot filling, spoken-query question answering, audio-based storytelling
- Activation tuning with few-shot examples effectively restores emergent abilities while maintaining performance on trained tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SALMONN's dual encoder structure allows effective processing of both speech and non-speech audio components
- Mechanism: Whisper captures speech content and background noise while BEATs extracts high-level non-speech audio semantics; concatenated features provide complementary information
- Core assumption: Both encoders output features at same frame rate (50Hz), enabling straightforward synchronization
- Evidence anchors: Abstract states dual encoder structure; section shows Z = Concat(Encoderwhisper(X), Encoderbeats(X))

### Mechanism 2
- Claim: Window-level Q-Former enables efficient processing of variable-length audio sequences while maintaining temporal resolution
- Mechanism: Segments encoder output into fixed-size windows (0.33s) and processes each independently, reducing computational complexity while preserving local temporal patterns
- Core assumption: 0.33 seconds per window is sufficient to capture meaningful audio patterns while being computationally efficient
- Evidence anchors: Abstract describes Q-Former converting variable-length sequences to tokens; section shows segmentation into L-sized windows

### Mechanism 3
- Claim: Activation tuning with few-shot examples effectively restores cross-modal emergent abilities lost during instruction tuning
- Mechanism: Reducing LoRA scaling factor encourages model to rely more on pre-trained LLM capabilities, helping recover zero-shot reasoning abilities while maintaining task-specific performance
- Core assumption: Intrinsic conditional language model embedded in LoRA can be regularized by reducing scaling factor to enable better generalization
- Evidence anchors: Abstract states activation tuning restores emergent abilities; section describes using reduced LoRA scaling factor for fine-tuning

## Foundational Learning

- Concept: Cross-modal alignment
  - Why needed here: SALMONN needs to align auditory features with textual representations so LLM can understand audio inputs
  - Quick check question: What would happen if auditory features were not properly aligned with LLM's input space?

- Concept: Emergent abilities in LLMs
  - Why needed here: SALMONN leverages emergent zero-shot capabilities of LLMs, which can be lost during task-specific fine-tuning
  - Quick check question: Why do some abilities emerge in LLMs during training while others are explicitly taught?

- Concept: Catastrophic forgetting
  - Why needed here: During instruction tuning, SALMONN may lose previously learned abilities, requiring activation tuning to restore them
  - Quick check question: How does fine-tuning on specific tasks affect a model's ability to perform other tasks it was previously capable of?

## Architecture Onboarding

- Component map: Audio input -> Dual encoders (Whisper + BEATs) -> Synchronized features -> Window-level Q-Former -> Text-like tokens -> Vicuna LLM (with LoRA) -> Text response

- Critical path:
  1. Audio input → Dual encoders → Synchronized features
  2. Features → Window-level Q-Former → Text-like tokens
  3. Tokens + prompt → Vicuna LLM (with LoRA) → Text response

- Design tradeoffs:
  - Dual encoders vs single encoder: Better coverage of audio types vs increased complexity
  - Window-level vs sequence-level Q-Former: Better efficiency for variable-length audio vs potential loss of global context
  - LoRA fine-tuning vs full fine-tuning: Parameter efficiency vs potentially limited adaptation capability

- Failure signatures:
  - Task over-fitting: Model performs well on trained tasks but fails on untrained cross-modal tasks
  - Temporal misalignment: Poor performance on tasks requiring precise timing (e.g., speech recognition)
  - Modality bias: Model performs well on one audio type (speech vs non-speech) but poorly on others

- First 3 experiments:
  1. Test with pure speech input to verify Whisper encoder integration
  2. Test with pure non-speech audio to verify BEATs encoder integration
  3. Test with mixed audio to verify dual encoder synergy and window-level Q-Former performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does window-level Q-Former's temporal resolution impact model's performance on tasks requiring fine-grained temporal understanding?
- Basis in paper: [explicit] Paper mentions window-level Q-Former has better temporal resolution important for speech recognition
- Why unresolved: Paper doesn't provide detailed analysis of how window size affects performance on different tasks
- What evidence would resolve it: Experiments with different window sizes comparing performance on tasks requiring fine-grained temporal understanding

### Open Question 2
- Question: What specific factors contribute to task over-fitting in SALMONN, and how can they be addressed?
- Basis in paper: [explicit] Paper attributes over-fitting to simpler instruction prompts and deterministic nature of some tasks
- Why unresolved: Paper doesn't provide detailed analysis of specific contributing factors or comprehensive solutions
- What evidence would resolve it: Experiments identifying specific factors and proposing targeted solutions

### Open Question 3
- Question: How does activation tuning affect performance on trained tasks, and what's optimal balance?
- Basis in paper: [explicit] Paper mentions activation tuning may degrade trained task performance
- Why unresolved: Paper doesn't analyze impact on trained tasks or propose method to find optimal balance
- What evidence would resolve it: Experiments analyzing activation tuning impact and proposing balance-finding method

## Limitations
- Implementation details for window-level Q-Former interaction with dual encoders are not fully specified
- Hyperparameter configurations for training stages, especially activation tuning, are not provided in detail
- Limited systematic analysis of when emergent abilities might fail or their boundaries

## Confidence

- High confidence in dual encoder architecture and basic functionality
- Medium confidence in effectiveness of window-level Q-Former approach
- Medium confidence in activation tuning methodology for restoring emergent abilities
- Low confidence in exact implementation details needed for perfect replication

## Next Checks
1. Perform systematic ablation studies to isolate contribution of each component (dual encoders, window-level Q-Former, LoRA adaptation) to overall performance
2. Test model's performance on cross-modal tasks with varying degrees of temporal precision to understand limitations in temporal alignment
3. Evaluate model's robustness to different audio quality levels and noise conditions to determine practical deployment boundaries