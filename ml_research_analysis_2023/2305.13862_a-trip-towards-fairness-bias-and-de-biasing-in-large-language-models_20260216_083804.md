---
ver: rpa2
title: 'A Trip Towards Fairness: Bias and De-Biasing in Large Language Models'
arxiv_id: '2305.13862'
source_url: https://arxiv.org/abs/2305.13862
tags:
- language
- bias
- llama
- association
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates bias in large language models, specifically
  LLaMA and OPT families. The authors analyze the presence of bias in different model
  sizes and domains, and propose a debiasing technique using LoRA on attention matrices
  with anti-stereotypical sentences from the PANDA dataset.
---

# A Trip Towards Fairness: Bias and De-Biasing in Large Language Models

## Quick Facts
- arXiv ID: 2305.13862
- Source URL: https://arxiv.org/abs/2305.13862
- Reference count: 11
- Primary result: Debiasing method reduces bias by up to 4.12 points in normalized stereotype score, with average decrease of 1.20 points across models and domains

## Executive Summary
This paper investigates bias in large language models (LLaMA and OPT families) and proposes a LoRA-based debiasing technique. The authors analyze bias across different model sizes and domains, finding that bias correlates with perplexity rather than model size. They demonstrate that fine-tuning attention matrices using LoRA on anti-stereotypical sentences from PANDA effectively reduces bias while maintaining language modeling performance.

## Method Summary
The authors employ LoRA (Low-Rank Adaptation) to fine-tune attention matrices on anti-stereotypical sentences from the PANDA dataset. This approach freezes most model parameters while adapting the low-rank components of attention weights to reduce stereotypical associations. The debiased models are evaluated using StereoSet for bias measurement and GLUE benchmark for downstream performance validation.

## Key Results
- LoRA-based debiasing reduces bias by up to 4.12 points in normalized stereotype score
- Average bias reduction of 1.20 points across all models and domains
- Bias correlates with perplexity, not model size
- Language modeling performance maintained as measured by GLUE benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Debiasing via LoRA on attention matrices reduces bias by up to 4.12 points in normalized stereotype score
- Mechanism: LoRA trains only the low-rank adaptation matrices of attention weights, freezing the rest of the model parameters. Fine-tuning these attention matrices on anti-stereotypical sentences from PANDA helps the model learn to de-emphasize stereotypical associations while retaining most learned representations
- Core assumption: Attention matrices are low-rank and capture primary directions of bias in the model
- Break condition: If attention matrices are not low-rank or do not capture bias directions, LoRA adaptation will not effectively reduce bias

### Mechanism 2
- Claim: Bias in large language models correlates with perplexity, not model size
- Mechanism: Higher perplexity indicates the model is more uncertain when processing certain demographic contexts, which correlates with higher stereotypical bias scores
- Core assumption: Perplexity serves as a proxy for model uncertainty when encountering biased or stereotypical patterns
- Break condition: If perplexity does not reliably reflect model uncertainty in demographic contexts, the correlation may not hold across different datasets or model families

### Mechanism 3
- Claim: Using anti-stereotypical sentences from PANDA for fine-tuning maintains language modeling performance while reducing bias
- Mechanism: PANDA provides human-annotated rewrites that flip stereotypical demographic references, creating a training signal that encourages the model to assign similar probabilities to both stereotypical and anti-stereotypical contexts
- Core assumption: The model can learn to balance stereotypical and anti-stereotypical associations without losing general language modeling capabilities
- Break condition: If anti-stereotypical sentences do not adequately represent demographic context diversity, the model may overfit to specific patterns in PANDA

## Foundational Learning

- **Concept: Attention mechanisms and low-rank adaptation**
  - Why needed here: Understanding how LoRA modifies attention matrices is critical to grasping the debiasing mechanism
  - Quick check question: How does LoRA's low-rank update differ from standard fine-tuning in terms of parameter efficiency and adaptation scope?

- **Concept: Perplexity as a measure of language model uncertainty**
  - Why needed here: Perplexity is used as a proxy for bias correlation; understanding its computation and interpretation is essential
  - Quick check question: What does a lower perplexity score indicate about a model's confidence in predicting the next token?

- **Concept: Bias measurement metrics (StereoSet, SS, LMS, ICAT)**
  - Why needed here: These metrics are used to quantify bias reduction and must be understood to evaluate debiasing results
  - Quick check question: How does the Stereotype Score (SS) differ from the Idealized CAT Score (ICAT) in measuring bias?

## Architecture Onboarding

- **Component map**: LLaMA/OPT model → LoRA adapter on attention matrices → PANDA anti-stereotypical sentences for fine-tuning → StereoSet for bias evaluation → GLUE for downstream performance
- **Critical path**: Load pre-trained model → Apply LoRA adapter → Fine-tune on PANDA → Evaluate bias reduction → Validate performance on GLUE
- **Design tradeoffs**: Parameter freezing for efficiency vs. potential loss of fine-grained adaptation; anti-stereotypical sentence quality vs. generalization to unseen bias patterns
- **Failure signatures**: No bias reduction despite LoRA training (attention matrices not capturing bias), performance degradation on GLUE (overfitting to PANDA), or increased perplexity (LoRA interfering with language modeling)
- **First 3 experiments**:
  1. Run bias evaluation on pre-trained LLaMA 7B using StereoSet to establish baseline SS scores
  2. Apply LoRA adapter and fine-tune on a small subset of PANDA; re-evaluate bias and perplexity
  3. Test GLUE performance before and after debiasing to ensure no degradation

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the exact relationship between model perplexity and bias across different model architectures beyond LLaMA and OPT families?
  - Basis: The paper observes a negative correlation between perplexity and stereotype score but notes statistical significance cannot be established due to limited model samples
  - Why unresolved: Study only analyzed LLaMA and OPT models, leaving open whether this correlation generalizes to other architectures like GPT-3, BLOOM, or smaller models
  - What evidence would resolve it: A comprehensive study testing multiple model families with varying perplexity scores and bias metrics would establish if this relationship holds across architectures

- **Open Question 2**: Does the LoRA-based debiasing approach transfer effectively to larger models beyond those tested (7B, 13B, 33B, 65B parameters)?
  - Basis: The authors successfully applied LoRA debiasing to LLaMA and OPT models but note hardware limitations prevented testing larger models
  - Why unresolved: The scalability of this lightweight debiasing technique to truly massive models remains untested, particularly regarding effectiveness and computational efficiency
  - What evidence would resolve it: Empirical testing of the LoRA debiasing method on models larger than 65B parameters, measuring both bias reduction and performance maintenance

- **Open Question 3**: How does the proposed debiasing method affect model performance on specialized downstream tasks versus general benchmarks like GLUE?
  - Basis: The authors demonstrate GLUE performance is maintained but note this doesn't guarantee effectiveness on all downstream tasks
  - Why unresolved: GLUE represents general language understanding, but the impact on domain-specific tasks (medical, legal, technical) where bias could be particularly harmful remains unknown
  - What evidence would resolve it: Systematic evaluation of debiased models across diverse specialized benchmarks measuring both performance and bias in task-specific contexts

## Limitations

- The study examines a limited scope of bias domains (gender, race, religion, profession) without addressing intersectional bias patterns
- The effectiveness of LoRA-based debiasing lacks comparison with alternative debiasing approaches or ablation studies isolating contribution of different attention layers
- The correlation between perplexity and bias may not generalize to other model architectures or datasets

## Confidence

- **High confidence**: The experimental methodology for measuring bias using established metrics (StereoSet, SS, ICAT) is sound and reproducible
- **Medium confidence**: The LoRA-based debiasing approach effectively reduces bias scores, though the mechanism's generalizability beyond tested models remains uncertain
- **Medium confidence**: The perplexity-bias correlation finding, while supported by the data, requires validation across different model families and evaluation datasets

## Next Checks

1. Conduct ablation studies to determine which attention layers contribute most significantly to bias reduction when fine-tuned with LoRA

2. Test the debiasing method on additional model families (e.g., GPT, BLOOM) to validate whether the perplexity-bias correlation holds across architectures

3. Evaluate intersectional bias scenarios by creating compound demographic prompts not covered in the PANDA dataset to test generalization of the debiasing approach