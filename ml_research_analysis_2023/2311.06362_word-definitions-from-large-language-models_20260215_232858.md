---
ver: rpa2
title: Word Definitions from Large Language Models
arxiv_id: '2311.06362'
source_url: https://arxiv.org/abs/2311.06362
tags:
- definitions
- word
- dictionaries
- words
- dictionary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares dictionary definitions from traditional sources
  with those generated by large language models, particularly ChatGPT. The authors
  analyze over 2,500 words across three frequency tiers, collecting definitions from
  WordNet, Merriam-Webster, Dictionary.com, and two ChatGPT versions (3.5 and 4.0)
  using two prompts.
---

# Word Definitions from Large Language Models

## Quick Facts
- arXiv ID: 2311.06362
- Source URL: https://arxiv.org/abs/2311.06362
- Reference count: 12
- Key outcome: ChatGPT-generated definitions differ significantly in surface form from traditional dictionaries but maintain high accuracy, with GPT-4 definitions closer to Dictionary.com than other sources, and definition quality consistency across word frequencies outperforms traditional embeddings for rare words.

## Executive Summary
This study compares dictionary definitions from traditional sources with those generated by large language models, particularly ChatGPT. The authors analyze over 2,500 words across three frequency tiers, collecting definitions from WordNet, Merriam-Webster, Dictionary.com, and two ChatGPT versions (3.5 and 4.0) using two prompts. Key findings show that (i) ChatGPT-generated definitions differ significantly in surface form from traditional dictionaries but are highly accurate, (ii) GPT-4 definitions are closer to Dictionary.com than other sources, (iii) definition quality is relatively consistent across word frequencies, unlike word embeddings, and (iv) SBERT embeddings of ChatGPT definitions outperform GloVe and FastText for low-frequency words. The study concludes that LLM-generated definitions are comparable to traditional dictionaries in accuracy and may provide superior representations for less common words.

## Method Summary
The study uses over 2,500 carefully selected English words distributed across three frequency tiers (high, medium, low), with definitions collected from three traditional dictionary sources (WordNet, Merriam-Webster, Dictionary.com) and two ChatGPT versions (3.5 and 4.0) using two prompts. The authors calculate distance metrics including BERT embeddings, MPNet embeddings, and edit distance between all definition pairs across sources. They analyze definition similarity, consistency across frequency tiers and parts of speech, and compare word embedding consistency between traditional embeddings and definition-based embeddings using pre-trained models for embedding comparison and manual evaluation for accuracy assessment.

## Key Results
- ChatGPT-generated definitions show high accuracy while differing significantly in surface form from traditional dictionaries
- GPT-4 definitions are closer to Dictionary.com than other sources and generally longer than GPT-3.5 definitions
- Definition quality consistency across word frequencies outperforms traditional embeddings like GloVe and FastText for rare words

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT definitions maintain accuracy across word frequencies unlike traditional embeddings
- Mechanism: SBERT embeddings of generated definitions create more stable semantic representations regardless of word frequency
- Core assumption: LLMs capture word semantics consistently even for rare words due to training on diverse text corpora
- Evidence anchors:
  - [abstract]: "ChatGPT-based embedding definitions retain their accuracy even on low frequency words, much better than GloVE and FastText word embeddings"
  - [section]: "The consistency of LLM-generated definitions is relatively independent of frequency, unlike word embeddings"
- Break condition: If LLM training data disproportionately lacks rare word contexts or if SBERT embedding quality degrades for low-frequency definitions

### Mechanism 2
- Claim: Generated definitions show minimal plagiarism from published dictionaries despite surface differences
- Mechanism: LLMs synthesize definitions through semantic understanding rather than text copying, creating novel surface forms while preserving meaning
- Core assumption: LLMs can generate semantically equivalent but textually distinct definitions through learned representations
- Evidence anchors:
  - [abstract]: "definitions from different traditional dictionaries exhibit more surface form similarity than do model-generated definitions"
  - [section]: "Little-to-no unexpected trace of these texts remain in the generated text"
- Break condition: If LLM training involves direct text memorization or if definition generation relies heavily on template-based approaches

### Mechanism 3
- Claim: GPT-4 produces more dictionary-like definitions than GPT-3.5
- Mechanism: GPT-4's larger training corpus and improved architecture better capture traditional lexicographic patterns
- Core assumption: Model architecture improvements directly translate to more traditional definition styles
- Evidence anchors:
  - [abstract]: "GPT-4 definitions are closer to Dictionary.com than other sources"
  - [section]: "GPT4 definitions are generally a little longer than those produced by GPT3.5"
- Break condition: If definition style differences result from prompt engineering rather than model capabilities

## Foundational Learning

- Concept: Cosine similarity for text embedding comparison
  - Why needed here: Primary metric for measuring definition alignment between sources
  - Quick check question: If two definitions have cosine similarity of 0.9, are they considered highly similar?

- Concept: Edit distance for surface form comparison
  - Why needed here: Complements embedding-based measures by capturing textual differences
  - Quick check question: If definition A requires 5 edits to become definition B, and both are 20 words long, what's the normalized edit distance?

- Concept: Frequency tier analysis
  - Why needed here: Essential for understanding how definition quality varies with word usage patterns
  - Quick check question: Why might rare words present unique challenges for both dictionary definitions and embeddings?

## Architecture Onboarding

- Component map: Word selection -> Definition collection -> Text processing -> Embedding generation -> Distance calculation -> Statistical analysis -> Manual validation
- Critical path: Embedding comparison pipeline (word selection -> definition collection -> SBERT encoding -> distance computation -> frequency analysis)
- Design tradeoffs: SBERT vs MPNet embeddings (trade accuracy for speed), automated vs manual validation (trade coverage for precision)
- Failure signatures: High variance in frequency-based results suggests embedding quality issues; inconsistent POS results suggest model bias
- First 3 experiments:
  1. Verify frequency correlation stability by testing with different random word samples
  2. Test alternative embedding models (e.g., RoBERTa, DeBERTa) to confirm SBERT results
  3. Implement cross-validation with different prompt formulations to test robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLM-generated definitions maintain their accuracy and consistency across different languages and translation tasks?
- Basis in paper: [inferred] from the limitation section stating the study focused only on English words and definitions, and suggesting future work on evaluating model fidelity in other languages or translation tasks.
- Why unresolved: The paper only examines English words and definitions, leaving the performance of LLMs in other languages or translation tasks unexplored.
- What evidence would resolve it: Conducting similar studies comparing LLM-generated definitions with traditional dictionaries in multiple languages, and evaluating translation accuracy using LLM-generated definitions.

### Open Question 2
- Question: How do LLM-generated definitions perform in capturing the completeness of multiple senses or alternate usages of words?
- Basis in paper: [inferred] from the limitation section mentioning the need to validate the completeness of multiple senses or alternate usages.
- Why unresolved: The study establishes basic agreement between publishing and generated definitions but does not address the comprehensiveness of LLM-generated definitions in capturing all possible meanings of a word.
- What evidence would resolve it: Evaluating LLM-generated definitions against a comprehensive set of word senses and alternate usages, comparing them with traditional dictionaries.

### Open Question 3
- Question: How does the accuracy of LLM-generated definitions change with the advancement of generative models?
- Basis in paper: [explicit] from the question posed in the introduction about how quickly LLM-generated definitions are improving as technology advances.
- Why unresolved: The study uses specific versions of GPT models (3.5 and 4.0) and does not track the evolution of definition quality across newer model versions.
- What evidence would resolve it: Conducting longitudinal studies comparing definition quality from successive generations of LLM models, using consistent evaluation metrics.

## Limitations
- The study relies on a single SBERT embedding model for comparing definition quality, which may not capture all semantic nuances
- The manual validation sample size (approximately 2,500 words across all sources) may not fully represent edge cases or domain-specific terminology
- The study focuses on English words only, limiting generalizability to other languages

## Confidence
- **High Confidence**: ChatGPT definitions show minimal plagiarism from traditional dictionaries (based on surface form analysis and manual inspection)
- **Medium Confidence**: GPT-4 produces more dictionary-like definitions than GPT-3.5 (supported by comparative analysis but sensitive to prompt variations)
- **Medium Confidence**: Definition quality consistency across word frequencies (based on embedding analysis, but dependent on SBERT model performance)

## Next Checks
1. Replicate key findings using alternative embedding models (RoBERTa, DeBERTa) to verify SBERT results and assess robustness
2. Implement a larger manual validation sample with domain experts to verify definition accuracy claims, particularly for rare words
3. Test the frequency consistency hypothesis with different random word samples and varying frequency tier boundaries to assess result stability