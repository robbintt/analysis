---
ver: rpa2
title: 'T5 meets Tybalt: Author Attribution in Early Modern English Drama Using Large
  Language Models'
arxiv_id: '2310.18454'
source_url: https://arxiv.org/abs/2310.18454
tags:
- plays
- samples
- author
- authors
- thomas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of large language models (LLMs)
  for authorship attribution in Early Modern English drama. The authors fine-tune
  a T5-large model on short speaker utterances from plays, and compare its performance
  to several baselines including logistic regression, SVM, and cosine delta.
---

# T5 meets Tybalt: Author Attribution in Early Modern English Drama Using Large Language Models

## Quick Facts
- arXiv ID: 2310.18454
- Source URL: https://arxiv.org/abs/2310.18454
- Reference count: 36
- Primary result: T5-large fine-tuned on short Early Modern English utterances achieves 52.7% accuracy on authorship attribution, outperforming traditional baselines

## Executive Summary
This paper investigates the use of large language models for authorship attribution in Early Modern English drama, fine-tuning a T5-large model on short speaker utterances from plays. The authors compare the T5 model's performance to several baselines including logistic regression, SVM, and cosine delta, finding that the T5-large model significantly outperforms all tested baselines at attributing short text passages. However, the model exhibits a problematic tendency to confidently misattribute texts to specific "scapegoat" authors, often those with large vocabularies and word use similar to the corpus average. The results suggest that while LLMs show promise for stylometry, their pre-training data can introduce biases that are difficult to assess and may affect predictions in unexpected ways.

## Method Summary
The study fine-tunes a T5-large model using a masked language modeling objective on speaker utterances from Early Modern English drama, with input format "AUTHOR: <extra_id_0> | [speaker utterance]" and output format "AUTHOR: [author name] | [speaker utterance]". The corpus consists of 253 plays by 23 authors from the Folger Digital Anthology and Shakespeare His Contemporaries, with utterances segmented into 5-450 word samples. The model is trained with batch size 4, 10 epochs, learning rate 2x10^-5, and weight decay 0.01, using a train/validation/test split with 15 validation samples per play. Performance is evaluated against baselines (logistic regression, SVM, cosine delta) using per-sample and play-level accuracy metrics, including analysis of attribution patterns to identify "scapegoat" authors.

## Key Results
- T5-large achieves 52.7% accuracy on held-out samples from plays in the training set, outperforming all baselines
- Model scale correlates with performance: T5-large outperforms T5-base and T5-small
- Attribution accuracy on full-play excerpts (up to 3,000 words) is significantly higher than on short utterances
- The model consistently misattributes to specific "scapegoat" authors (Heywood, Shirley), often with high confidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: T5-large fine-tuning captures subtle authorship style cues in short utterances better than simpler models
- Mechanism: The generative nature of T5 allows the model to learn word co-occurrence patterns and subtle linguistic variations from relatively short text segments, which are lost in bag-of-words or frequency-based baselines
- Core assumption: Short Early Modern English utterances contain enough stylistic signal to differentiate authors when modeled by a large, pre-trained language model
- Evidence anchors: [abstract] "LLMs are able to accurately predict the author of surprisingly short passages"; [section] "T5 is a generative large language model and the pre-trained T5 models are optimized with a masked language modeling objective"
- Break condition: If the training corpus is too small or too homogeneous, the model will overfit or fail to learn meaningful distinctions

### Mechanism 2
- Claim: Model scale (T5-large vs. T5-base vs. T5-small) improves attribution accuracy due to capacity for nuanced style representation
- Mechanism: Larger models have more parameters to capture fine-grained differences in word usage, phrase structure, and author-specific idiosyncrasies that smaller models cannot represent
- Core assumption: The relationship between model size and attribution accuracy is monotonic for this task
- Evidence anchors: [abstract] "A fine-tuned t5-large model outperforms all tested baselines"; [section] "Model scale also effects accuracy. The t5-large model performed better than the smaller models we compare it to, t5-base and t5-small"
- Break condition: Beyond a certain scale, performance gains plateau or overfitting dominates

### Mechanism 3
- Claim: Pre-training data exposure to prominent authors (e.g., Shakespeare, Jonson) improves correct attribution for those authors even with limited fine-tuning data
- Mechanism: The model's prior exposure to certain authors' styles in pre-training creates a bias that helps correctly identify them in attribution tasks, independent of fine-tuning corpus size
- Core assumption: T5's pre-training corpus includes works by Shakespeare and Jonson, and their style is distinctive enough to be recognized
- Evidence anchors: [abstract] "trained LLMs may be able to quantify 'style'"; [section] "It is possible that Shakespeare and Shirley's uniqueness comes from using words that the other authors do not, instead of using common words uniquely"
- Break condition: If pre-training corpus is unknown or if other authors are equally represented in pre-training, this effect may not hold

## Foundational Learning

- Concept: Stylometry and authorship attribution fundamentals
  - Why needed here: Understanding the task requires familiarity with how linguistic features map to authorial style and how different models capture these features
  - Quick check question: What are the main differences between frequency-based stylometry (e.g., cosine delta) and representation-based approaches (e.g., LLMs)?

- Concept: Fine-tuning large language models
  - Why needed here: The paper's core contribution relies on adapting a pre-trained model to a new domain (Early Modern drama), so knowing how fine-tuning works is essential
  - Quick check question: How does fine-tuning differ from training a model from scratch, and what are the risks of overfitting in this context?

- Concept: Early Modern English linguistic features
  - Why needed here: The corpus's historical language variation adds complexity; understanding typical stylistic markers of the period helps interpret model performance
  - Quick check question: What are some common linguistic features of Early Modern English that could aid or hinder authorship attribution?

## Architecture Onboarding

- Component map: XML parsing -> text regularization -> utterance splitting -> sample length control -> T5-large fine-tuning -> accuracy evaluation -> attribution analysis
- Critical path: Data → Preprocessing → Fine-tuning → Evaluation → Attribution analysis
- Design tradeoffs:
  - Fine-tuning a large generative model vs. simpler, faster baselines
  - Using regularized text vs. preserving original spelling
  - Short utterance segmentation vs. longer continuous passages
- Failure signatures:
  - Over-attribution to "scapegoat" authors (Heywood, Shirley)
  - Poor performance on authors with few plays or unique style
  - Confusion in disputed/co-authored works
- First 3 experiments:
  1. Compare fine-tuned T5-large vs. T5-base on same train/test split to confirm scale effect
  2. Test attribution accuracy on full-play excerpts to establish upper bound vs. short utterances
  3. Run cosine delta with different vocabulary sizes (e.g., 1k, 5k, 10k) to optimize baseline performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions beyond acknowledging limitations in its methodology and results.

## Limitations
- Relatively small corpus of only 23 authors and 253 plays limits generalizability to broader authorship attribution tasks
- Model's tendency to "scapegoat" specific authors (Heywood, Shirley) suggests potential learning of spurious correlations rather than genuine authorial style
- Uncertainty about T5 model's pre-training corpus makes it difficult to assess how prior exposure to certain authors might influence attribution accuracy

## Confidence
- High confidence: T5-large outperforms simpler baselines on this specific dataset
- Medium confidence: Model scale correlates with attribution accuracy
- Low confidence: Attribution accuracy generalizes to broader authorship contexts

## Next Checks
1. Test the fine-tuned model on a larger, more diverse corpus of Early Modern drama (50+ authors, 500+ plays) to assess scalability and identify whether scapegoating patterns persist
2. Compare T5's attribution accuracy to human expert judgments on the same disputed authorship cases to validate model reliability
3. Conduct ablation studies by removing high-frequency "scapegoat" authors from training data to determine whether attribution patterns are artifactual or reflect genuine stylistic similarities