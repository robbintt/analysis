---
ver: rpa2
title: How to Configure Good In-Context Sequence for Visual Question Answering
arxiv_id: '2312.01571'
source_url: https://arxiv.org/abs/2312.01571
tags:
- answer
- shot
- question
- image
- demonstrations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how to configure in-context sequences
  for Visual Question Answering (VQA) using Large Vision-Language Models (LVLMs).
  It explores diverse retrieval and manipulation strategies to improve ICL performance
  while uncovering three key properties of the model: limited task learning ability,
  a short-cut inference effect, and partial compatibility between vision and language
  modules.'
---

# How to Configure Good In-Context Sequence for Visual Question Answering

## Quick Facts
- **arXiv ID**: 2312.01571
- **Source URL**: https://arxiv.org/abs/2312.01571
- **Reference count**: 40
- **Primary result**: Demonstrations with similar images and questions consistently improve VQA performance, while instructions and pseudo answers enhance results under certain conditions.

## Executive Summary
This paper investigates how to configure in-context sequences for Visual Question Answering (VQA) using Large Vision-Language Models (LVLMs). The study explores diverse retrieval and manipulation strategies to improve In-Context Learning (ICL) performance while uncovering three key properties of the model: limited task learning ability, a short-cut inference effect, and partial compatibility between vision and language modules. Through extensive experiments on VQAv2, VizWiz, and OK-VQA datasets, the research demonstrates that using demonstrations with both similar images and questions consistently improves results, while instructions and pseudo answers also enhance performance under certain conditions.

## Method Summary
The paper employs Open-Flamingo v1/v2 LVLMs with CLIP-based retrieval systems to construct in-context sequences for VQA tasks. The method uses training sets as supporting demonstrations and validation sets as queries, applying various retrieval strategies (Random Sampling, Similar Image, Similar Question, Similar Question&Answer, Similar Question&Pseudo Answer) and manipulation techniques (mismatching, reordering, instructions, declarative sentences). Experiments are conducted with 4, 8, and 16-shot demonstrations across three VQA datasets, measuring accuracy with a specific formula that accounts for answer matching.

## Key Results
- Using demonstrations with similar images and similar questions simultaneously provides the most consistent performance improvements across all three VQA datasets.
- Instructions significantly enhance the model's format recognition and task learning capabilities, especially in linguistically advanced models.
- The model exhibits a short-cut inference effect where it copies answers from similar questions rather than processing visual information, particularly in OK-VQA tasks requiring external knowledge.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL performance is primarily driven by Task Recognition (TR) rather than Task Learning (TL).
- Mechanism: TR uses demonstrations to identify task format, label space, and input distribution, while TL learns the mapping between inputs and outputs. The model leverages pre-trained knowledge from demonstrations to narrow down the answer space before applying learned mappings.
- Core assumption: The vision and language modules are not fully aligned, making TR more efficient than TL.
- Evidence anchors:
  - [abstract] "uncover three important inner properties of the applied LVLM and demonstrate which strategies can consistently improve the ICL VQA performance"
  - [section 4.2.1] "When the disturbed triplets are used, anti-intuitively, the VQA performance does not significantly degrade"
  - [corpus] Weak - no direct mention of TR/TL distinction in related papers
- Break condition: When demonstrations contain sufficient diverse examples to enable effective TL learning.

### Mechanism 2
- Claim: Short-cut inference occurs when the model copies answers from demonstrations with similar questions rather than processing visual information.
- Mechanism: The model identifies questions similar to the query and copies corresponding answers, bypassing visual reasoning. This creates a shortcut that works when similar questions have similar answers but fails when visual context matters.
- Core assumption: The language module is stronger than the vision module, leading the model to rely on linguistic similarity over visual analysis.
- Evidence anchors:
  - [section 4.2.1] "when the demonstration has a question similar to the query, OF often copies the answer from the demonstration with the similar question instead of using visual information"
  - [section 3.3] "SQ approach also brings improvements, although these enhancements are not consistently stable due to the presence of the short-cut"
  - [corpus] Weak - related papers mention retrieval but not specific shortcut behavior
- Break condition: When visual information is critical and cannot be inferred from question similarity alone.

### Mechanism 3
- Claim: Instructions enhance performance by improving format TR and TL capabilities, especially in linguistically advanced models.
- Mechanism: Instructions provide explicit guidance on task format and reasoning process, reducing ambiguity in the label space and improving the mapping between inputs and outputs.
- Core assumption: The model can parse and utilize natural language instructions effectively when the language module is sufficiently advanced.
- Evidence anchors:
  - [section 4.2.2] "Providing instructions notably enhances the format TR and TL capabilities of the LVLM"
  - [section 3.3] "we add an instruction at the beginning of the in-context sequence"
  - [corpus] Moderate - related papers mention instruction tuning but not specific VQA context
- Break condition: When the language module cannot effectively process natural language instructions.

## Foundational Learning

- Concept: In-Context Learning (ICL) and Few-Shot Learning
  - Why needed here: The paper investigates how LVLMs can learn from demonstrations without explicit fine-tuning
  - Quick check question: What distinguishes zero-shot, few-shot, and ICL approaches in terms of demonstration requirements?

- Concept: Visual Question Answering (VQA) Task Structure
  - Why needed here: Understanding VQA is essential for analyzing how LVLMs process image-question-answer triplets
  - Quick check question: What are the key components of a VQA sample and how do they interact during model inference?

- Concept: Task Recognition vs Task Learning
  - Why needed here: The paper's central hypothesis distinguishes between recognizing task format and learning input-output mappings
  - Quick check question: How do TR and TL differ in their dependence on demonstration quality and quantity?

## Architecture Onboarding

- Component map:
  LVLM Core -> Retrieval System -> Demonstration Configurator -> Output Generator

- Critical path:
  1. Query processing (image and question)
  2. Demonstration retrieval using similarity metrics
  3. In-context sequence construction with potential modifications
  4. Model inference using TR and TL mechanisms
  5. Answer generation and post-processing

- Design tradeoffs:
  - Similarity-based retrieval vs diversity-based retrieval
  - Random sampling vs targeted retrieval strategies
  - Demonstration order (visual vs linguistic similarity)
  - Instruction inclusion vs demonstration quantity

- Failure signatures:
  - Short-cut inference when similar questions yield incorrect answers
  - Performance degradation when visual information is critical but ignored
  - Limited improvement with increased shot numbers due to weak TL

- First 3 experiments:
  1. Random sampling baseline to establish performance floor
  2. Similar image retrieval to test visual compensation hypothesis
  3. Similar question retrieval to observe short-cut effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we further improve the task learning (TL) capability of large vision-language models (LVLMs) like Open-Flamingo?
- Basis in paper: [explicit] The paper discusses the limited TL capabilities of Open-Flamingo and how it affects the model's ability to learn input-output mappings from demonstrations.
- Why unresolved: While the paper identifies the limited TL capability as a key limitation, it does not provide concrete solutions for enhancing this ability beyond general suggestions like increasing the amount of pre-training data.
- What evidence would resolve it: Experimental results demonstrating significant improvements in TL capability through specific architectural changes, training strategies, or novel in-context learning techniques.

### Open Question 2
- Question: What are the underlying reasons for the short-cut inference effect observed in LVLMs, and how can we mitigate it?
- Basis in paper: [explicit] The paper identifies the short-cut inference effect, where the model tends to copy answers from similar questions rather than using visual information, as a key limitation.
- Why unresolved: The paper provides some qualitative observations and quantitative measurements of the short-cut effect but does not delve into the root causes or propose effective mitigation strategies.
- What evidence would resolve it: Analysis of model internals to identify the specific mechanisms leading to short-cut inference, along with experimental results showing successful mitigation techniques.

### Open Question 3
- Question: How can we achieve better alignment between the vision and language modules in LVLMs to leverage the full potential of language reasoning?
- Basis in paper: [explicit] The paper discusses the partial compatibility between vision and language modules in Open-Flamingo and how it affects the model's performance.
- Why unresolved: While the paper identifies the compatibility issue and suggests that better alignment could improve performance, it does not provide concrete methods for achieving this alignment.
- What evidence would resolve it: Experimental results demonstrating significant performance improvements through specific techniques for aligning the vision and language modules, such as joint training or cross-modal attention mechanisms.

## Limitations
- The TR-TL distinction may not generalize to other LVLM architectures beyond OpenFlamingo
- The short-cut inference mechanism may be task-specific to VQA rather than broadly applicable
- Pseudo-answer generation introduces variability depending on which model is used for initial inference

## Confidence
- TR vs TL distinction mechanism: **High** - supported by consistent experimental evidence across multiple manipulation strategies
- Short-cut inference mechanism: **Medium** - well-demonstrated but may be task-specific
- Instruction effectiveness: **Medium** - clear results but potentially model-dependent
- Vision-language compatibility limitations: **High** - evidenced by multiple failure patterns

## Next Checks
1. Test whether the TR-TL distinction holds when demonstrations are replaced with synthetic or generated examples, which would validate the core mechanism
2. Evaluate the short-cut inference mechanism on non-VQA multimodal tasks (e.g., image captioning or visual reasoning) to assess generalizability
3. Compare instruction effectiveness across LVLMs with different instruction-tuning histories to quantify model dependency