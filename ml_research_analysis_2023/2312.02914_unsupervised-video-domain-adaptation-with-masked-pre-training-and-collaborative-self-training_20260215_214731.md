---
ver: rpa2
title: Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative
  Self-Training
arxiv_id: '2312.02914'
source_url: https://arxiv.org/abs/2312.02914
tags:
- domain
- video
- target
- masked
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses unsupervised domain adaptation (UDA) for video
  action recognition. The proposed method, UNITE, adapts a video student model to
  a target domain using an image teacher model (CLIP).
---

# Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training

## Quick Facts
- arXiv ID: 2312.02914
- Source URL: https://arxiv.org/abs/2312.02914
- Authors: [Not specified]
- Reference count: 40
- One-line primary result: Proposed method UNITE achieves state-of-the-art performance on multiple video domain adaptation benchmarks by combining masked pre-training and collaborative self-training with a CLIP teacher model.

## Executive Summary
This paper presents UNITE, a novel approach for unsupervised domain adaptation (UDA) in video action recognition. The method addresses the challenge of adapting a video student model to a target domain using an image teacher model (CLIP). UNITE employs a three-stage pipeline: self-supervised pre-training on target domain videos using masked distillation, supervised fine-tuning on source domain videos, and collaborative self-training with improved pseudolabels. The approach leverages the strengths of both models to achieve strong transfer performance, significantly outperforming previously reported results on multiple video domain adaptation benchmarks.

## Method Summary
UNITE adapts a video student model to a target domain using an image teacher model (CLIP). The method employs a three-stage pipeline: (1) unsupervised pre-training on target domain videos using masked distillation with a CLIP teacher, (2) supervised fine-tuning on source domain videos, and (3) collaborative self-training using both source and target videos with pseudolabel refinement via CLIP. The approach aims to improve feature learning and adaptation by leveraging the complementary strengths of the video student and image teacher models.

## Key Results
- UNITE achieves state-of-the-art performance on multiple video domain adaptation benchmarks (Daily-DA, Sports-DA, UCF-HMDB)
- Masked pre-training and collaborative self-training significantly improve upon previously reported results
- The method demonstrates the effectiveness of combining self-supervised pre-training and collaborative self-training for video UDA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masked video modeling with teacher-guided distillation improves feature invariance to masking for target domain adaptation.
- Mechanism: By training a spatiotemporal student model on heavily masked target videos while matching features from an unmasked spatial teacher, the student learns to extract discriminative features regardless of which patches are visible.
- Core assumption: Spatial features from CLIP contain sufficient semantic information to guide spatiotemporal feature learning, even when the student only sees masked video patches.
- Evidence anchors:
  - [abstract]: "UNITE first employs self-supervised pre-training to promote discriminative feature learning on target domain videos using a teacher-guided masked distillation objective."
  - [section 4.1]: "UMT training uses a pre-trained and frozen spatial teacher model g∗ to train a spatiotemporal student model ga by enforcing alignment between feature representations of the two networks at multiple layers."
  - [corpus]: Weak evidence - no direct citations about masked distillation for video UDA found in neighbor papers.
- Break condition: If spatial teacher features lack semantic richness for the target domain actions, the distillation objective fails to produce discriminative spatiotemporal features.

### Mechanism 2
- Claim: Collaborative self-training with teacher-student agreement improves pseudolabel quality for target domain adaptation.
- Mechanism: By combining predictions from the video student model and image teacher model using the MatchOrConf scheme, UNITE generates more reliable pseudolabels for self-training, reducing the impact of incorrect predictions.
- Core assumption: The image teacher (CLIP) and video student model have complementary strengths in recognizing target domain actions, and their agreement indicates higher confidence in pseudolabels.
- Evidence anchors:
  - [abstract]: "We then perform self-training on masked target data, using the video student model and image teacher model together to generate improved pseudolabels for unlabeled target videos."
  - [section 4.1]: "We employ the MatchOrConf scheme proposed in [69] to combine the outputs of the two models as shown in Eq. (3)."
  - [corpus]: Weak evidence - neighbor papers focus on self-training but not collaborative approaches with teacher models.
- Break condition: If either the student or teacher model has systematically poor performance on the target domain, the agreement-based pseudolabeling becomes unreliable.

### Mechanism 3
- Claim: Masked self-training on target domain videos enhances context learning compared to unmasked training.
- Mechanism: By applying the cross-entropy loss on masked target videos during self-training, the model is forced to use different visual cues from the same video to predict the pseudolabel, resulting in more robust recognition.
- Core assumption: Masking forces the model to rely on a broader range of contextual information rather than specific visual cues that may not generalize across domains.
- Evidence anchors:
  - [section 4.1]: "we compute the target domain loss on masked videos, where we employ the same teacher-guided attention masking strategy (with r = 0.8) used during UMT pre-training in Stage 1."
  - [section 6]: "we find that training on masked videos in this stage results in substantial performance boosts compared to training on unmasked videos."
  - [corpus]: Weak evidence - neighbor papers do not discuss masked self-training for domain adaptation.
- Break condition: If the masking ratio is too high or the attention-guided masking fails to preserve essential action information, the self-training process may degrade performance.

## Foundational Learning

- Concept: Masked image modeling and masked language modeling as self-supervised learning paradigms
  - Why needed here: UNITE builds on the success of masked modeling techniques by applying them to video domain adaptation, leveraging the principle of learning invariant representations.
  - Quick check question: How does masked modeling differ from contrastive learning approaches in self-supervised learning?

- Concept: Knowledge distillation and teacher-student learning frameworks
  - Why needed here: UNITE uses a CLIP image encoder as a teacher to guide the learning of a video student model through masked distillation and collaborative pseudolabeling.
  - Quick check question: What are the advantages of using a pre-trained teacher model versus training a student model from scratch in domain adaptation?

- Concept: Domain adaptation and unsupervised domain adaptation (UDA) problem formulation
  - Why needed here: UNITE specifically addresses the UDA problem for video action recognition, where labeled source data and unlabeled target data come from different distributions.
  - Quick check question: How does unsupervised domain adaptation differ from supervised domain adaptation and source-free domain adaptation?

## Architecture Onboarding

- Component map:
  - CLIP image encoder (ViT-B/16) as spatial teacher model
  - Video transformer network (ViT-B/16) as spatiotemporal student model
  - Linear classification head for source and target domain adaptation
  - Mask generation module using teacher attention maps
  - Pseudolabel refinement module implementing MatchOrConf scheme

- Critical path: UMT pre-training → Source domain fine-tuning → Collaborative self-training
  - Each stage builds upon the previous one, with UMT pre-training enhancing target domain feature extraction, fine-tuning adapting to source labels, and self-training refining target domain performance

- Design tradeoffs:
  - Using CLIP as teacher provides strong spatial features but may limit temporal modeling capabilities
  - High masking ratio (0.8) improves efficiency but risks losing essential action information
  - Collaborative self-training with teacher agreement improves pseudolabel quality but may reduce target sample utilization

- Failure signatures:
  - Poor performance on domain shifts where CLIP has weak zero-shot classification accuracy
  - Degradation in target accuracy when applying collaborative self-training (as seen on M→A in Daily-DA)
  - Instability during collaborative self-training when source classification loss is omitted

- First 3 experiments:
  1. Verify UMT pre-training improves target domain feature extraction by comparing Stage 1 vs. Source Only baseline on a single domain shift
  2. Test the impact of masking ratio on UMT pre-training performance by varying r from 0.5 to 0.9
  3. Evaluate different pseudolabeling strategies (MatchOrConf vs. masked consistency) on collaborative self-training performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of UNITE scale with larger video models and more extensive pre-training on target domains?
- Basis in paper: [explicit] The paper mentions using a standard ViT-B/16 architecture and evaluates the effectiveness of self-supervised pre-training on target domain videos. It also discusses the potential for further improvements by combining UMT pre-training and collaborative self-training.
- Why unresolved: The paper uses a specific architecture (ViT-B/16) and pre-training duration (50 epochs). It does not explore the impact of scaling the model size or extending the pre-training phase on target domains.
- What evidence would resolve it: Experiments comparing UNITE's performance using larger video models (e.g., ViT-L/16) and varying pre-training durations on target domains, demonstrating the scalability of the approach.

### Open Question 2
- Question: Can the collaborative self-training process be further improved by incorporating additional teacher models or ensemble methods?
- Basis in paper: [explicit] The paper discusses the use of a CLIP image encoder as a teacher model in combination with the video student model for improved pseudolabeling. It also mentions the potential for leveraging the strengths of both models.
- Why unresolved: The paper focuses on using a single CLIP image encoder as the teacher model. It does not explore the possibility of incorporating multiple teacher models or ensemble methods to enhance the collaborative self-training process.
- What evidence would resolve it: Experiments comparing the performance of UNITE with different combinations of teacher models (e.g., CLIP + additional image or video encoders) or ensemble methods, showing the impact on target domain accuracy.

### Open Question 3
- Question: How robust is UNITE to variations in the source domain data distribution, and can it adapt to scenarios where the source and target domains have limited or no class overlap?
- Basis in paper: [explicit] The paper discusses the challenges of domain adaptation when the source and target data distributions differ. It mentions the UDA problem formulation and the potential issues with supervised pre-training when there is category overlap between the pre-training dataset and the DA dataset.
- Why unresolved: The paper evaluates UNITE on benchmarks with overlapping classes between source and target domains. It does not investigate the performance of UNITE when the source and target domains have limited or no class overlap, which is a more challenging scenario.
- What evidence would resolve it: Experiments evaluating UNITE's performance on benchmarks with minimal or no class overlap between source and target domains, demonstrating the robustness of the approach to variations in the source domain data distribution.

## Limitations

- The effectiveness of the UMT pre-training stage relies heavily on the quality of CLIP features for the target domain actions, which may not generalize well to domains with different action vocabularies or visual styles.
- The collaborative self-training approach shows performance degradation on certain domain shifts (e.g., M→A in Daily-DA), suggesting the method may be sensitive to the specific characteristics of the source and target domains.
- The paper lacks ablations on the impact of the masking ratio and attention-guided masking strategy, making it unclear how robust these design choices are.

## Confidence

- High confidence: The overall three-stage pipeline structure and the use of CLIP as a teacher model are well-established approaches in domain adaptation
- Medium confidence: The specific implementation of masked distillation and collaborative self-training with MatchOrConf scheme shows promise but requires further validation on diverse domain shifts
- Low confidence: The paper claims substantial performance boosts from masked self-training, but lacks ablations to isolate the contribution of this component from other factors

## Next Checks

1. Perform extensive ablations on the masking ratio (r) in UMT pre-training to determine the optimal value across different domain shifts
2. Evaluate the performance of UNITE on domain shifts where CLIP has poor zero-shot classification accuracy to assess the limitations of using CLIP as a teacher
3. Compare the MatchOrConf scheme against alternative pseudolabel refinement strategies (e.g., mean teacher, temporal ensemble) to validate the benefits of collaborative self-training