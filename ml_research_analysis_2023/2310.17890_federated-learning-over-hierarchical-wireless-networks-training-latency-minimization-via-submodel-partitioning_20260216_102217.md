---
ver: rpa2
title: 'Federated Learning over Hierarchical Wireless Networks: Training Latency Minimization
  via Submodel Partitioning'
arxiv_id: '2310.17890'
source_url: https://arxiv.org/abs/2310.17890
tags:
- hist
- edge
- training
- clients
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach for hierarchical federated
  learning (HFL) that partitions the global model into disjoint submodels distributed
  across different cells. This enables each client to train only one submodel, reducing
  computation, communication, and storage costs.
---

# Federated Learning over Hierarchical Wireless Networks: Training Latency Minimization via Submodel Partitioning

## Quick Facts
- arXiv ID: 2310.17890
- Source URL: https://arxiv.org/abs/2310.17890
- Reference count: 40
- Key outcome: Submodel partitioning in hierarchical FL reduces per-client computation, communication, and storage costs while maintaining O(1/√T) convergence to a neighborhood of a stationary point.

## Executive Summary
This paper proposes HIST (Hierarchical submodel partitioning Federated learning), a novel approach for hierarchical federated learning that partitions the global model into disjoint submodels distributed across different cells. By training only one submodel per client, the method significantly reduces computation, communication, and storage costs. The authors provide theoretical convergence analysis showing O(1/√T) convergence to a neighborhood of a stationary point under non-convex loss functions and non-i.i.d. data settings. Experiments on Fashion-MNIST demonstrate that HIST achieves comparable accuracy to standard HFL while reducing communication costs by factors of 2-5x depending on the partitioning scheme.

## Method Summary
HIST partitions the global model into N disjoint submodels using Hadamard product masks, distributing one submodel per cell. Each client trains only their assigned submodel for H local steps, then edge servers aggregate locally every H steps. The cloud aggregates edge models every E steps and redistributes new random masks. The method uses a two-layer fully connected network (784→300→10) on Fashion-MNIST with 60 clients across N cells (N ∈ {2,3,4,5}). Key hyperparameters are H=40 and E=200, with sensitivity analysis varying these values. The approach maintains convergence guarantees through careful theoretical analysis of the partitioning scheme's impact on stationarity and convergence rate.

## Key Results
- HIST achieves comparable test accuracy to standard HFL on Fashion-MNIST while reducing communication costs by 2-5×
- Theoretical analysis shows O(1/√T) convergence to a neighborhood of a stationary point under non-convex loss functions
- Communication cost per client scales as (1/N) × (E/H) × full-model transmission load, with significant savings for larger N
- Convergence rate and stationarity gap depend on data heterogeneity parameters δ1, δ2 and submodel size

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Submodel partitioning reduces per-client computation and communication load by distributing disjoint partitions across cells.
- **Mechanism**: The global model is split into N disjoint submodels, one per cell, so each client only trains and communicates a fraction (1/N) of the full model parameters.
- **Core assumption**: The masks pt_j can be uniformly and randomly generated such that each cell gets a unique partition and all partitions sum to the full model.
- **Evidence anchors**:
  - [abstract] "The key idea behind HIST is to divide the global model into disjoint partitions (or submodels) per round so that each group of clients (i.e., cells) is responsible for training only one partition of the model."
  - [section] "Specifically, in the beginning of t/E-th global round... the cloud server initiates the training process by partitioning the current global model ¯xt into N disjoint submodels: ¯xt_j = pt_j ⊙ ¯xt, ∀j ∈ {1, 2, ..., N}."
- **Break condition**: If the masks cannot be generated disjointly (pt_j ⊙ pt_j' ≠ 0), then submodels overlap and the per-client load reduction fails.

### Mechanism 2
- **Claim**: Hierarchical aggregation reduces global communication rounds by aggregating at the edge before cloud.
- **Mechanism**: Edge servers aggregate client updates locally, then upload compressed edge models to the cloud every E iterations instead of each client communicating with the cloud directly.
- **Core assumption**: The edge can aggregate locally without significant accuracy loss; non-i.i.d. data is handled by local updates before edge aggregation.
- **Evidence anchors**:
  - [abstract] "The introduction of edge servers in HFL reduces communication and scheduling complexity, as the cloud server now only needs to communicate with the edge servers."
  - [section] "Edge Servers: After every H steps of local submodel updates, each edge server aggregates the local models within its coverage as ¯xt+1_j ← 1/nj Σ_{i∈Cj} xt+1_i."
- **Break condition**: If E is too large, stale edge models degrade convergence; if H is too large, local drift overwhelms edge correction.

### Mechanism 3
- **Claim**: Convergence to a neighborhood of a stationary point is preserved despite submodel partitioning.
- **Mechanism**: Theoretical analysis shows that HIST achieves O(1/√T) convergence rate to a neighborhood, with error terms depending on data heterogeneity (δ1, δ2) and submodel size.
- **Core assumption**: Assumptions 1-6 (smoothness, unbiased gradients, bounded variance, bounded heterogeneity) hold; masks are uniformly random.
- **Evidence anchors**:
  - [abstract] "We characterize the convergence behavior of HIST for non-convex loss functions under mild assumptions, showing the impacts of several key attributes... on the rate and stationarity gap."
  - [section] "Theorem 1... HIST achieves the following convergence behavior for non-convex loss functions: ... ∥∇f(ˆxt)∥² ≤ O(1/√T) ... to a neighborhood of a stationary point."
- **Break condition**: If data heterogeneity is extreme (large δ1, δ2) or submodel size too small, the non-diminishing bound dominates and accuracy suffers.

## Foundational Learning

- **Concept**: Hadamard (element-wise) product for model partitioning
  - Why needed here: To mask and distribute disjoint submodel partitions efficiently.
  - Quick check question: Given a mask pt_j with 1s in positions [1, 3, 5] and 0s elsewhere, what does pt_j ⊙ x produce?

- **Concept**: Hierarchical aggregation (edge-cloud separation)
  - Why needed here: To reduce global communication load and enable scalability in wireless networks.
  - Quick check question: If each edge has 10 clients and E=5, how many global rounds are needed to aggregate 50 client updates?

- **Concept**: Stochastic gradient descent with partitioned models
  - Why needed here: To update only the submodel relevant to each client while preserving overall model quality.
  - Quick check question: In the update xt+1_i = xt_i - γ pt_j ⊙ ∇l(xt_i, ξ_i), which parameters are actually updated?

## Architecture Onboarding

- **Component map**: Cloud server -> Edge servers -> Clients
- **Critical path**:
  1. Cloud partitions model → distributes to edges
  2. Edges distribute submodels → clients train for H steps
  3. Clients upload to edges → edges aggregate
  4. Edges upload to cloud every E iterations → cloud aggregates globally
  5. Cloud repartitions and redistributes → repeat
- **Design tradeoffs**:
  - Smaller submodels (larger N) → less per-client load but higher heterogeneity penalty
  - Larger E → fewer global rounds but more stale aggregation
  - Larger H → less edge communication but more local drift
- **Failure signatures**:
  - Slow convergence → E or H too large; check δ1, δ2
  - High communication cost → N too small; check partitioning logic
  - Low accuracy → submodel size too small or data heterogeneity too high
- **First 3 experiments**:
  1. Vary N from 2 to 5 on Fashion-MNIST; measure accuracy vs communication cost.
  2. Fix N=3; sweep H and E; plot convergence curves for fully non-i.i.d. and cell-i.i.d. data.
  3. Measure actual per-iteration communication volume with HIST vs standard HFL.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of non-uniform submodel partitioning on convergence rate and resource efficiency?
- Basis in paper: [explicit] The paper assumes uniform submodel partitioning but notes that the choice of partition affects convergence
- Why unresolved: The theoretical analysis only covers uniform partitioning, leaving the impact of optimal non-uniform partitioning unexplored
- What evidence would resolve it: Experiments comparing convergence and resource usage across different partitioning strategies (e.g., balanced vs. imbalanced submodel sizes)

### Open Question 2
- Question: How does HIST perform with heterogeneous client computation capabilities across cells?
- Basis in paper: [inferred] The convergence analysis assumes homogeneous computation, but real-world scenarios often involve varying client capabilities
- Why unresolved: The theoretical framework doesn't account for different local update counts (H) per cell
- What evidence would resolve it: Simulations with heterogeneous client capabilities and analysis of convergence under adaptive local update strategies

### Open Question 3
- Question: What is the impact of stochastic gradient quantization on HIST's convergence and communication efficiency?
- Basis in paper: [explicit] The paper discusses communication complexity but doesn't explore quantization techniques
- Why unresolved: The theoretical bounds are derived without considering gradient compression, which is common in practice
- What evidence would resolve it: Experiments measuring convergence rates and communication savings with different quantization schemes (e.g., QSGD, 1-bit SGD) integrated with HIST

## Limitations
- Theoretical bounds depend on strict assumptions that may not hold in extreme non-i.i.d. conditions
- Partitioning strategy sensitivity requires perfect mask generation and balanced submodel sizes
- Hyperparameter tuning requirements make method sensitive to specific deployment scenarios

## Confidence
- **High Confidence**: The core mechanism of submodel partitioning reducing per-client load is well-supported by clear mathematical formulation
- **Medium Confidence**: The hierarchical aggregation mechanism is logically sound but requires empirical validation for optimal H/E values
- **Medium Confidence**: The convergence guarantee is mathematically rigorous under stated assumptions but needs more extensive validation for practical stationarity gap

## Next Checks
1. **Mask orthogonality verification**: Implement automated checks that pt_j ⊙ pt_j' = 0 for all j≠j' and Σ pt_j = 1 at each global round. Measure the impact of mask generation errors on convergence speed and accuracy.
2. **Edge aggregation frequency sensitivity**: Systematically sweep H and E values (e.g., H ∈ [10, 20, 40, 80], E ∈ [50, 100, 200, 400]) and plot convergence curves for both fully non-i.i.d. and cell-i.i.d. data distributions. Identify the regimes where local drift dominates or aggregation becomes too stale.
3. **Communication cost breakdown**: Instrument the implementation to measure actual bytes transmitted per client per iteration. Verify that communication cost per client = (1/N) × (E/H) × full-model transmission load, and compare this theoretical prediction with measured values across different N, H, E configurations.