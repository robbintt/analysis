---
ver: rpa2
title: 'Vector Search with OpenAI Embeddings: Lucene Is All You Need'
arxiv_id: '2308.14963'
source_url: https://arxiv.org/abs/2308.14963
tags:
- search
- lucene
- vector
- retrieval
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper argues that existing search infrastructure, particularly
  the Lucene ecosystem (Elasticsearch, OpenSearch, Solr), is sufficient for implementing
  vector search with OpenAI embeddings, negating the need for dedicated vector stores.
  The authors demonstrate this by indexing OpenAI''s ada2 embeddings of the MS MARCO
  passage corpus using Lucene and achieving competitive effectiveness scores (RR@10:
  0.343, R@1k: 0.984, AP: 0.479) compared to state-of-the-art models.'
---

# Vector Search with OpenAI Embeddings: Lucene Is All You Need

## Quick Facts
- arXiv ID: 2308.14963
- Source URL: https://arxiv.org/abs/2308.14963
- Authors: 
- Reference count: 6
- Key outcome: Lucene with HNSW indexes is sufficient for vector search using OpenAI embeddings, negating need for dedicated vector stores

## Executive Summary
This paper challenges the prevailing narrative that dedicated vector databases are necessary for modern AI-driven search by demonstrating that existing Lucene infrastructure can effectively handle vector search tasks. The authors show that Lucene's HNSW implementation, when combined with OpenAI's ada2 embeddings, achieves competitive retrieval effectiveness on the MS MARCO dataset while leveraging widely-adopted enterprise search infrastructure. This approach potentially simplifies enterprise architectures by eliminating the need for separate vector storage systems.

## Method Summary
The researchers indexed OpenAI's ada2 embeddings of the MS MARCO passage corpus using Lucene's HNSW implementation and evaluated retrieval effectiveness using standard IR metrics. They used Anserini as a research interface to Lucene 9.5.0, encoding MS MARCO passages with the OpenAI API and configuring HNSW with parameters M=16 and efC=100. The system was tested against MS MARCO development queries and TREC Deep Learning Track queries, with performance benchmarks conducted on dated hardware (2x Intel Xeon Platinum 8160, 1TB RAM).

## Key Results
- Achieved competitive effectiveness scores: RR@10 of 0.343, R@1k of 0.984, and AP of 0.479 on MS MARCO
- Demonstrated reasonable query throughput of 9.8 QPS on dated hardware
- Showed Lucene's HNSW implementation scales well with multiple threads despite lower single-thread performance compared to Faiss
- Validated approach across multiple query sets including TREC Deep Learning 2019/2020 tracks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HNSW indexes in Lucene provide adequate vector search capabilities for OpenAI embeddings
- Mechanism: Lucene's HNSW implementation enables approximate nearest neighbor search with performance comparable to dedicated vector stores
- Core assumption: Lucene's HNSW implementation is sufficiently mature and performant for production vector search
- Evidence anchors:
  - [abstract] "we show that hierarchical navigable small-world network (HNSW) indexes in Lucene are adequate to provide vector search capabilities"
  - [section 2] "The most recent major release of Lucene (version 9), dating back to December 2021, includes HNSW indexing and vector search capabilities, which have steadily improved over the past couple of years"
  - [corpus] Weak evidence - related papers focus on Lucene's HNSW integration but lack performance comparisons
- Break condition: Performance degrades significantly below state-of-the-art dedicated vector stores

### Mechanism 2
- Claim: OpenAI embeddings work effectively with Lucene's vector search capabilities
- Mechanism: OpenAI's ada2 embeddings maintain semantic relationships that Lucene's HNSW can exploit for effective retrieval
- Core assumption: OpenAI embeddings preserve sufficient semantic information for retrieval tasks
- Evidence anchors:
  - [section 3] "Evaluation on the MS MARCO development set queries and queries from the TREC Deep Learning Tracks show that OpenAI embeddings are able to achieve a respectable level of effectiveness"
  - [table 1] "OpenAIada2 0.343 0.984 0.479 0.704 0.863 0.477 0.676 0.871" - effectiveness metrics comparable to state-of-the-art
  - [corpus] Weak evidence - limited corpus data on OpenAI embedding effectiveness
- Break condition: Embedding quality degrades or Lucene's vector search fails to preserve semantic relationships

### Mechanism 3
- Claim: Existing Lucene ecosystem adoption makes this approach practical for enterprises
- Mechanism: Enterprises already invested in Lucene infrastructure can adopt vector search without new architectural components
- Core assumption: Lucene ecosystem has sufficient enterprise adoption and maturity
- Evidence anchors:
  - [section 1] "Modern enterprise architectures are already exceedingly complex, and the addition of another software component (i.e., a dedicated vector store) requires carefully weighing costs as well as benefits"
  - [section 1] "Elastic, the publicly traded company behind Elasticsearch, reports approximately 20,000 subscriptions to its cloud service as of Q4 FY2023"
  - [section 1] "it would not be an exaggeration to say that Lucene has an immense install base"
- Break condition: Enterprise adoption shifts away from Lucene ecosystem or vector search requirements exceed Lucene capabilities

## Foundational Learning

- Concept: Hierarchical Navigable Small World (HNSW) graphs
  - Why needed here: Core mechanism for efficient vector similarity search in Lucene
  - Quick check question: What property of HNSW graphs enables logarithmic time complexity for nearest neighbor search?

- Concept: Dense vector embeddings and semantic search
  - Why needed here: Foundation for understanding how OpenAI embeddings represent semantic relationships
  - Quick check question: How do dense vector embeddings differ from traditional sparse representations in capturing semantic relationships?

- Concept: Bi-encoder architecture
  - Why needed here: Framework for understanding how queries and passages are encoded into vectors
  - Quick check question: What is the key advantage of bi-encoder architecture over cross-encoder approaches in terms of retrieval efficiency?

## Architecture Onboarding

- Component map:
  - OpenAI API endpoint -> Embedding generation service
  - Lucene 9.5.0 -> Vector indexing and search engine
  - Anserini toolkit -> Research interface for Lucene
  - MS MARCO corpus -> Test dataset
  - Evaluation metrics -> RR@10, AP, nDCG@10, R@1k

- Critical path:
  1. Generate embeddings for corpus using OpenAI API
  2. Index embeddings in Lucene using HNSW
  3. Perform vector search queries
  4. Evaluate effectiveness metrics

- Design tradeoffs:
  - Performance vs. accuracy: HNSW provides approximate search
  - Memory usage: 1536-dimensional vectors require significant storage
  - Latency vs. throughput: Trade-off between query speed and concurrent requests

- Failure signatures:
  - Poor effectiveness scores indicate embedding or indexing issues
  - High memory usage suggests inefficient vector storage
  - Low query throughput indicates indexing or search configuration problems

- First 3 experiments:
  1. Verify embedding generation: Generate embeddings for small subset and validate dimensions
  2. Test basic indexing: Index small set of vectors and perform nearest neighbor search
  3. Benchmark performance: Measure indexing time and query throughput on sample data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do performance differences between Lucene and dedicated vector stores like Faiss translate to real-world cost-benefit analyses for enterprise deployments?
- Basis in paper: [explicit] The paper explicitly states that Lucene achieves only around half the query throughput of Faiss under comparable settings, but appears to scale better when using multiple threads.
- Why unresolved: While the paper provides a performance comparison, it doesn't quantify how these differences impact total cost of ownership or operational complexity in enterprise settings where both systems would need to coexist with existing infrastructure.
- What evidence would resolve it: Comparative studies measuring total operational costs (hardware, maintenance, integration overhead) across different deployment scales and use cases would provide the necessary evidence.

### Open Question 2
- Question: What is the actual adoption rate and user experience of vector search capabilities in existing search platforms like Elasticsearch and OpenSearch compared to dedicated vector databases?
- Basis in paper: [explicit] The paper mentions that Elasticsearch and OpenSearch have announced vector search capabilities but doesn't provide data on actual adoption or user satisfaction.
- Why unresolved: The paper makes assertions about the sufficiency of existing search platforms but lacks empirical data on how organizations are actually using these features versus dedicated vector stores.
- What evidence would resolve it: Surveys of enterprise users, case studies comparing implementations, and usage statistics from major search platform providers would provide concrete evidence.

### Open Question 3
- Question: How do hybrid retrieval systems combining dense and sparse representations perform when implemented using only Lucene versus systems that use separate vector stores?
- Basis in paper: [inferred] The paper discusses hybrid approaches and mentions that Lucene can handle both dense and sparse retrieval, but doesn't provide direct performance comparisons between unified versus separate implementations.
- Why unresolved: While the paper argues for simplicity, it doesn't demonstrate whether a unified Lucene-based approach can match or exceed the performance of systems using separate vector stores for hybrid retrieval.
- What evidence would resolve it: Head-to-head benchmark comparisons of hybrid retrieval systems using different architectural approaches across various query workloads and dataset sizes would provide definitive answers.

## Limitations
- Performance benchmarks conducted on dated hardware (2017-era Xeon processors) may not reflect modern enterprise capabilities
- Limited evaluation to 1536-dimensional vectors without testing higher-dimensional embeddings
- No analysis of real-time update performance or dynamic dataset handling

## Confidence
- **High Confidence**: The fundamental claim that Lucene's HNSW implementation works for vector search with OpenAI embeddings - empirically demonstrated and reproducible
- **Medium Confidence**: The assertion that dedicated vector stores are unnecessary for most enterprise use cases - depends heavily on specific workload requirements and scale
- **Low Confidence**: The broader claim that this approach eliminates the need for vector databases entirely - paper doesn't adequately address edge cases requiring real-time updates or extreme scale

## Next Checks
1. Scale Test: Reproduce the experiment on a dataset 10x larger than MS MARCO (100M+ passages) to evaluate indexing and query performance at scale
2. Update Benchmark: Test the system's performance with frequent index updates to assess suitability for dynamic datasets requiring real-time vector search
3. Cross-Embedding Comparison: Evaluate the approach with different embedding models (e.g., Cohere, sentence-BERT) to determine if results are specific to OpenAI's ada2 or generalize to other embedding types