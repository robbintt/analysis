---
ver: rpa2
title: 'SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of
  Large Language Models'
arxiv_id: '2307.10635'
source_url: https://arxiv.org/abs/2307.10635
tags:
- problem
- arxiv
- problems
- language
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SciBench introduces a benchmark suite for evaluating large language
  models (LLMs) on college-level scientific problem-solving. It includes two datasets:
  an open set of 695 problems from textbooks across physics, chemistry, and mathematics,
  and a closed set of 104 problems from undergraduate exams.'
---

# SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models

## Quick Facts
- arXiv ID: 2307.10635
- Source URL: https://arxiv.org/abs/2307.10635
- Reference count: 40
- Key outcome: Current LLMs achieve only 35.80% accuracy on open-ended scientific problems, revealing significant reasoning limitations

## Executive Summary
SciBench introduces a benchmark suite for evaluating large language models on college-level scientific problem-solving. The benchmark includes two datasets: 695 open problems from textbooks and 104 closed problems from undergraduate exams, all featuring open-ended, free-response questions requiring complex reasoning and advanced computations. Experiments with GPT-3.5 and GPT-4 under various prompting strategies reveal that current LLMs struggle with scientific problem-solving, achieving only modest accuracy rates. Error analysis identifies ten essential problem-solving skills and shows that no single strategy consistently outperforms others, with some strategies even weakening specific skills.

## Method Summary
SciBench evaluates LLMs using two datasets: an open set of 695 problems from physics, chemistry, and mathematics textbooks, and a closed set of 104 problems from undergraduate exams. The evaluation uses GPT-3.5 and GPT-4 under seven prompting strategies including zero-shot, few-shot, chain-of-thought, and tool-augmented approaches with Python and Wolfram. Answers are extracted using boxed notation and compared against ground truth with tolerance thresholds. Error analysis employs a self-critic LLM verifier combined with human validation to classify deficient problem-solving skills.

## Key Results
- GPT-3.5 and GPT-4 achieve average accuracy of 35.80% on the open textbook dataset and 51.57% on the closed exam dataset
- Chain-of-thought prompting improves calculation accuracy but may weaken logical reasoning skills
- External tool integration with Python significantly boosts calculation accuracy (7.92% improvement) but introduces code conversion errors
- No single prompting strategy consistently outperforms others across all problem-solving skills

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The SciBench dataset improves LLM evaluation by focusing on open-ended, free-response questions requiring complex reasoning and advanced computations, unlike prior benchmarks that use multiple-choice formats
- Mechanism: By requiring free-response answers with detailed solutions, SciBench forces LLMs to generate full reasoning chains rather than guessing from options, exposing weaknesses in calculation and domain knowledge
- Core assumption: LLMs cannot rely on pattern matching or answer selection heuristics when answers are free-form numeric values
- Evidence anchors:
  - [abstract]: "SciBench introduces a benchmark suite for evaluating large language models (LLMs) on college-level scientific problem-solving. It includes two datasets... The benchmark focuses on open-ended, free-response questions requiring complex reasoning and advanced computations."
  - [section]: "Distinct from existing benchmarks, all of the problems in SciBench are open-ended, free-response questions. They require multiple steps of reasoning and the computation therein involve complex arithmetic operations such as differentiation and integration."
  - [corpus]: Weak. The corpus lists related benchmarks but does not provide evidence that free-response format specifically improves evaluation quality
- Break condition: If LLMs develop robust methods to generate plausible numeric answers without true understanding, the format advantage diminishes

### Mechanism 2
- Claim: Chain-of-thought prompting improves LLM calculation accuracy but does not universally enhance other problem-solving skills
- Mechanism: CoT encourages LLMs to generate intermediate reasoning steps, which helps catch calculation errors but may introduce logical missteps or misinterpretation of problem conditions
- Core assumption: Step-by-step reasoning exposes calculation pathways while preserving domain knowledge application
- Evidence anchors:
  - [abstract]: "Error analysis identifies ten essential problem-solving skills, showing that no single strategy significantly outperforms others, and some strategies may weaken specific skills."
  - [section]: "The CoT prompting yields average improvements of 2.58% and 2.39% under zero-shot and few-shot learning for GPT-3.5... However, in certain textbooks such as Quantum Chemistry... CoT prompting sometimes brings adverse effects."
  - [corpus]: Weak. The corpus lists benchmarks but lacks direct evidence about CoT's mixed impact on specific skills
- Break condition: If CoT consistently improves all skill categories or consistently degrades them, the mechanism fails

### Mechanism 3
- Claim: External tool integration (Python) significantly boosts calculation accuracy but may introduce code conversion errors and weaken logical reasoning
- Mechanism: By offloading numeric computation to Python, LLMs avoid arithmetic mistakes but must correctly translate problem-solving steps into executable code, which can fail due to syntax or semantic errors
- Core assumption: LLMs can accurately map natural language reasoning to correct programming constructs
- Evidence anchors:
  - [abstract]: "Some strategies that demonstrate improvements in certain problem-solving skills result in declines in other skills."
  - [section]: "Utilizing Python as an external tool results in an improvement of 7.92%... However, utilizing Wolfram Language does not help... and even results in a deteriorated performance."
  - [corpus]: Weak. The corpus lists related work on tool-augmented LLMs but lacks specific evidence about Python vs Wolfram performance trade-offs
- Break condition: If LLMs consistently generate correct code without errors, or if tool integration never improves accuracy, the mechanism breaks

## Foundational Learning

- Concept: Differentiation and integration in symbolic mathematics
  - Why needed here: Many SciBench problems involve calculus operations like integration and differentiation, which require precise symbolic manipulation beyond basic arithmetic
  - Quick check question: What is the derivative of f(x) = x³ with respect to x?

- Concept: Domain-specific scientific knowledge (physics, chemistry, mathematics)
  - Why needed here: Problems span college-level physics, chemistry, and mathematics, requiring understanding of concepts like Planck distribution, molecular orbitals, and differential equations
  - Quick check question: What is the Planck distribution formula for spectral radiance?

- Concept: Code translation from natural language to programming syntax
  - Why needed here: External tool settings require LLMs to convert reasoning steps into Python or Wolfram Language code, demanding accurate syntax and semantic mapping
  - Quick check question: How would you express "calculate the square root of x" in Python?

## Architecture Onboarding

- Component map: Dataset ingestion pipeline (PDF → LaTeX conversion) -> LLM evaluation harness (OpenAI API integration) -> Prompt engineering modules (zero-shot, few-shot, CoT, tool-augmented) -> Error classification system (self-critic LLM verifier + human validation) -> Performance metrics aggregator (accuracy scoring, skill deficiency profiling)
- Critical path: 1. Load problem dataset and verify LaTeX compilation 2. Generate LLM responses under each prompting strategy 3. Extract answers using boxed notation 4. Compare against ground truth with tolerance thresholds 5. Run self-critic classification for incorrectly answered problems 6. Aggregate skill deficiency statistics
- Design tradeoffs: Open-ended vs multiple-choice: More diagnostic power but harder evaluation automation; Tool-augmented vs pure LLM: Better calculation accuracy but added code generation complexity; Self-critic vs human annotation: Scalable but less accurate error classification
- Failure signatures: High variance in accuracy across similar problem types indicates prompting strategy issues; Consistent code generation failures suggest translation model weakness; Uniform skill deficiencies across all strategies indicate dataset bias or LLM capability limits
- First 3 experiments: 1. Run zero-shot evaluation on textbook dataset to establish baseline accuracy 2. Apply CoT prompting and compare calculation skill improvement vs logical reasoning decline 3. Enable Python tool integration and measure accuracy gains alongside code conversion error rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed self-refinement method for error analysis be further improved to achieve higher accuracy in classifying deficient skills?
- Basis in paper: [explicit] The paper mentions an accuracy rate of 81.45% for the self-refinement method in classifying error reasons into deficient skills
- Why unresolved: The accuracy rate of 81.45% indicates that there is still room for improvement in the self-refinement method. It is unclear whether the remaining 18.55% of misclassifications can be attributed to limitations in the method itself or other factors
- What evidence would resolve it: Further experiments and analysis could be conducted to identify the specific types of errors that are consistently misclassified by the self-refinement method. This would help in understanding the limitations of the method and potential areas for improvement

### Open Question 2
- Question: How do the performance of LLMs vary across different scientific domains (e.g., Physics, Chemistry, Mathematics) within the SciBench dataset?
- Basis in paper: [explicit] The paper evaluates the performance of GPT-3.5 and GPT-4 on the SciBench dataset, which includes problems from Physics, Chemistry, and Mathematics domains
- Why unresolved: While the paper provides an overall average score for the LLMs, it does not delve into the specific performance of LLMs in each scientific domain. This could provide insights into the strengths and weaknesses of LLMs in different domains
- What evidence would resolve it: A detailed analysis of the performance of LLMs in each scientific domain within the SciBench dataset would help in understanding the domain-specific capabilities and limitations of LLMs

### Open Question 3
- Question: How do the performance of LLMs on the closed exam dataset compare to their performance on the open textbook dataset?
- Basis in paper: [explicit] The paper presents experimental results on both the open textbook dataset and the closed exam dataset
- Why unresolved: While the paper provides the average scores for LLMs on both datasets, it does not directly compare the performance of LLMs on the two datasets. This comparison could provide insights into the generalization ability of LLMs across different problem types and difficulty levels
- What evidence would resolve it: A direct comparison of the performance of LLMs on the closed exam dataset and the open textbook dataset would help in understanding the generalizability of LLMs across different problem types and difficulty levels

## Limitations

- The error classification system relies on a self-critic LLM verifier combined with human validation, but the exact accuracy of this hybrid approach is not quantified, creating uncertainty about the reliability of skill deficiency profiling
- The study compares only GPT-3.5 and GPT-4 models, leaving questions about how other state-of-the-art LLMs would perform on the same benchmark
- The paper notes that some prompting strategies improve certain skills while weakening others, but the underlying reasons for these trade-offs are not fully explained or analyzed

## Confidence

- High confidence: The benchmark construction methodology and dataset composition are well-documented and reproducible
- Medium confidence: The experimental results showing average accuracy rates of 35.80% (open) and 51.57% (closed) are likely reliable, but the error analysis conclusions require more validation
- Low confidence: The comparative effectiveness of different prompting strategies is uncertain due to limited model diversity and unexplained performance variations

## Next Checks

1. Replicate the experiments with additional LLM models (Claude, Gemini, Llama) to determine if performance patterns are model-specific or universal across architectures
2. Conduct a systematic ablation study on the self-critic error classification system to quantify its accuracy compared to pure human annotation
3. Design controlled experiments isolating individual problem-solving skills to better understand why certain prompting strategies strengthen some abilities while weakening others