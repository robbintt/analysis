---
ver: rpa2
title: 'Grounding Visual Illusions in Language: Do Vision-Language Models Perceive
  Illusions Like Humans?'
arxiv_id: '2311.00047'
source_url: https://arxiv.org/abs/2311.00047
tags:
- illusions
- visual
- illusion
- human
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first systematic investigation of visual
  illusions in vision-language models (VLMs), aiming to understand whether VLMs align
  with human visual perception under illusions. The authors create a new dataset covering
  five types of visual illusions and design four benchmark tasks: SameDiffQA, RefQA,
  AttrQA, and RefLoc.'
---

# Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?

## Quick Facts
- arXiv ID: 2311.00047
- Source URL: https://arxiv.org/abs/2311.00047
- Authors: 
- Reference count: 15
- Primary result: VLMs show low alignment with human visual illusions in QA tasks but better alignment in localization tasks, with larger models showing stronger alignment.

## Executive Summary
This paper presents the first systematic investigation of visual illusions in vision-language models (VLMs), examining whether these models align with human visual perception under illusions. The authors create a new dataset covering five types of visual illusions and design four benchmark tasks to evaluate state-of-the-art VLMs. Key findings reveal that VLMs generally do not align with human visual illusions in question-answering tasks, with humanlike rates typically below 15%, but perform better in object localization tasks under illusions (up to 44.5% humanlike rate). The study also finds that larger models are closer to human perception and more susceptible to visual illusions, with a positive correlation between model size and human-machine alignment across different tasks.

## Method Summary
The study evaluates four state-of-the-art VLMs (Unified-IO, OFA, LLaVA, InstructBLIP) across different model sizes using a carefully curated dataset of 150 illusion instances spanning five categories. Four benchmark tasks are designed: SameDiffQA (comparing illusion-free vs illusion settings), RefQA (reference-based question answering), AttrQA (attribute-based questions), and RefLoc (object localization). The evaluation measures humanlike rates (alignment with human perception), no-illusion rates (consistency across settings), and N/A rates (failure to identify objects). Attention visualization is used to understand model decision-making processes.

## Key Results
- VLMs show low humanlike rates (typically below 15%) in QA-based tasks under visual illusions
- Larger VLMs demonstrate better alignment with human perception and are more susceptible to visual illusions
- VLMs perform significantly better in object localization tasks (up to 44.5% humanlike rate) compared to QA tasks
- A positive correlation exists between model size and human-machine alignment across different tasks

## Why This Works (Mechanism)

### Mechanism 1
VLMs trained on human-generated data inherit human-like perceptual biases, including susceptibility to visual illusions. Large vision-language models are trained on vast amounts of human-annotated data that inherently reflect human perceptual errors and cognitive shortcuts. When VLMs learn from this data, they can internalize these biases and reproduce them during inference, leading to illusion-like behavior similar to humans.

### Mechanism 2
Larger VLMs demonstrate better alignment with human visual perception under illusions due to their enhanced pattern recognition capabilities. As models scale up in parameter count, they become more capable of capturing subtle patterns and contextual cues in the training data. Human perception of illusions often depends on recognizing contextual relationships and integrating prior knowledge, which larger models can better represent.

### Mechanism 3
VLMs show better alignment with human perception in localization tasks under illusions compared to question-answering tasks because object localization relies more on direct visual processing while QA relies more on language understanding. The RefLoc task requires the model to directly identify and locate objects in the image, which is more closely tied to the visual processing pathway.

## Foundational Learning

- **Visual illusions as perceptual discrepancies**: Understanding the definition and nature of visual illusions is crucial for designing appropriate benchmarks and interpreting results when evaluating VLMs.
  - Quick check: Can you explain why the checker shadow illusion demonstrates a discrepancy between physical reality and perception?

- **Grounding in situated language communication**: The paper's novel contribution is examining illusions through the lens of language communication, requiring understanding of how language references and describes visual perceptions.
  - Quick check: How does the concept of "the darker square" demonstrate the grounding of language in visual perception under illusions?

- **Statistical significance testing in machine learning evaluation**: The paper uses Ï‡2-tests to validate that models perform better than chance, requiring understanding of hypothesis testing and p-values.
  - Quick check: What does it mean when the paper states that 9 out of 12 models rejected the null hypothesis with p < 0.005?

## Architecture Onboarding

- **Component map**: Dataset creation with five illusion categories -> Benchmark task formulation (SameDiffQA, RefQA, AttrQA, RefLoc) -> VLM model selection and inference -> Result collection and analysis -> Attention visualization for interpretability
- **Critical path**: Understanding the illusion dataset creation and annotation process, as this forms the foundation for all subsequent evaluation tasks
- **Design tradeoffs**: The paper chose quality over quantity in dataset creation, selecting a modest number of carefully curated instances rather than a large dataset
- **Failure signatures**: Models failing to identify identical objects in illusion-free images (high N/A rates), models answering based on text alone without image analysis, and models not showing significant differences between illusion and non-illusion conditions
- **First 3 experiments**:
  1. Run SameDiffQA task on a simple VLM to verify basic functionality and understand the illusion-free vs illusion-induced comparison
  2. Test RefLoc task to observe the contrast between localization and QA performance under illusions
  3. Perform attention visualization analysis on a specific illusion category to understand model decision-making processes

## Open Questions the Paper Calls Out

### Open Question 1
Why do larger vision-language models show stronger alignment with human perception under visual illusions?
- Basis: The paper states "our results have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions."
- Why unresolved: The paper does not provide a clear explanation for why model size correlates with alignment to human perception under illusions.
- What evidence would resolve it: Detailed analysis of internal representations and attention mechanisms in larger vs smaller models when processing illusion images.

### Open Question 2
What causes the stark contrast in performance between QA-based tasks and object localization tasks under visual illusions?
- Basis: The paper notes "models exhibit much stronger alignment in the localization task, with the highest alignment of 44.5% achieved by Unified-IO XL" compared to much lower performance on QA tasks.
- Why unresolved: The paper does not explain why localization tasks show better alignment than QA tasks.
- What evidence would resolve it: Comparative analysis of how different model architectures handle spatial vs linguistic reasoning under illusion conditions.

### Open Question 3
Why do different categories of visual illusions show varying degrees of alignment between machines and humans?
- Basis: The paper observes that "perspective category demonstrates the highest degree of alignment between machines and humans" while "color constancy illusions emerge as the category with the least congruity in relation to human responses."
- Why unresolved: The paper does not explain why certain illusion categories are easier or harder for models to align with human perception.
- What evidence would resolve it: Systematic study of how different illusion mechanisms (color vs geometric) interact with model architectures and training data.

## Limitations

- Dataset Generalization: The study uses a carefully curated dataset with 150 illusion instances across five categories, which may not fully represent the diversity of visual illusions encountered in real-world scenarios.
- Model Architecture Variability: Results may not generalize to newer architectures or different training paradigms that emerged after the study was conducted.
- Prompt Engineering Effects: The impact of prompt engineering on illusion perception is not thoroughly explored, representing an uncontrolled variable in the current analysis.

## Confidence

- **High Confidence**: VLMs perform better on localization tasks under illusions compared to question-answering tasks
- **Medium Confidence**: Larger models show better alignment with human perception under illusions
- **Low Confidence**: VLMs "mostly do not align with human visual illusions" in QA-based tasks (15% humanlike rate threshold may be too strict)

## Next Checks

1. Test the illusion perception benchmarks on newer VLM architectures released after this study to verify if observed patterns hold across a broader range of models.

2. Systematically vary the prompt templates for each task to determine how sensitive the humanlike rates are to prompt engineering.

3. Apply the illusion benchmarks to images from real-world datasets to assess whether laboratory conditions translate to naturalistic visual perception scenarios.