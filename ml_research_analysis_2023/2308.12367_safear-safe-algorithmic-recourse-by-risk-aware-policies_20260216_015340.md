---
ver: rpa2
title: 'SafeAR: Safe Algorithmic Recourse by Risk-Aware Policies'
arxiv_id: '2308.12367'
source_url: https://arxiv.org/abs/2308.12367
tags:
- recourse
- cost
- policy
- risk
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Safe Algorithmic Recourse (SafeAR) to address
  the lack of risk considerations in existing algorithmic recourse methods. SafeAR
  computes recourse policies using risk-sensitive reinforcement learning, explicitly
  accounting for the uncertainty and potential higher costs associated with feature
  changes.
---

# SafeAR: Safe Algorithmic Recourse by Risk-Aware Policies

## Quick Facts
- arXiv ID: 2308.12367
- Source URL: https://arxiv.org/abs/2308.12367
- Reference count: 32
- Key outcome: Introduces SafeAR to compute risk-aware recourse policies using risk-sensitive reinforcement learning, achieving lower variance in costs compared to risk-neutral baselines while revealing potential demographic disparities in risk exposure.

## Executive Summary
This paper addresses the critical gap in existing algorithmic recourse methods that ignore uncertainty and risk in feature change costs. SafeAR introduces a risk-sensitive reinforcement learning approach that computes recourse policies explicitly accounting for the variability and potential higher costs associated with different actions. The authors propose a greedy risk-sensitive value iteration algorithm that modifies standard value iteration by incorporating variance into the optimization objective. Experiments on real-world datasets demonstrate that SafeAR policies can achieve better risk measures (lower variance, improved VaR and CVaR) while maintaining reasonable expected costs, and reveal potential disparities in risk exposure across demographic groups that warrant further investigation.

## Method Summary
SafeAR formulates algorithmic recourse as a finite horizon Markov Decision Process where states represent feature combinations, actions represent feature changes, and rewards represent costs. The method computes risk-aware policies using greedy risk-sensitive value iteration (G-RSVI), which maximizes expected cost minus β times the standard deviation of cost at each step. This approach encourages policies that prefer actions with more predictable outcomes even if their expected cost is slightly higher. The method evaluates policies using statistical risk measures including Value at Risk (VaR) and Conditional Value at Risk (CVaR) to provide interpretable summaries of the risk profile. Experiments use two real-world datasets (Adult Income and German Credit) with discretized features and assumed transition probabilities based on domain knowledge.

## Key Results
- SafeAR policies achieve lower variance in recourse costs compared to risk-neutral baselines while maintaining comparable expected costs
- Risk measures (VaR and CVaR) effectively capture and communicate the risk profile of different recourse policies
- Experiments reveal potential disparities in risk exposure across demographic groups, with some groups facing higher risk despite using the same risk-aversion level
- The greedy risk-sensitive value iteration algorithm successfully computes policies that balance expected cost against cost variability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Risk-aware policies can provide lower variance in recourse costs while maintaining reasonable expected costs.
- Mechanism: The G-RSVI algorithm modifies standard value iteration by subtracting β times the standard deviation of total cost from the expected value when selecting actions. This encourages the policy to prefer actions with more predictable outcomes even if their expected cost is slightly higher.
- Core assumption: The cost distribution for recourse actions is known and can be estimated through transition probabilities and action costs.
- Evidence anchors:
  - [abstract] "Experiments on real-world datasets demonstrate that SafeAR policies can achieve lower variance in cost and better risk measures compared to risk-neutral baselines"
  - [section 5.1] "At each step h, the action is selected to maximize the risk-sensitive value using: Vh = max μ[r(·) + Vh+1] − βσ[r(·) + Vh+1]"
- Break condition: If transition probabilities are unknown or the cost distribution is heavily skewed, the variance-based risk measure may not accurately capture true risk.

### Mechanism 2
- Claim: Risk measures like VaR and CVaR provide more interpretable summaries of recourse risk than variance alone.
- Mechanism: VaR gives the maximum cost at a given confidence level, while CVaR gives the expected cost in the worst cases exceeding VaR. These measures help individuals understand both typical and worst-case scenarios.
- Core assumption: The recourse cost distribution is stable enough that these statistical measures are meaningful for decision-making.
- Evidence anchors:
  - [section 5.2] "Value at Risk (VaR) is to provide a succinct probabilistic guarantee on the recourse policy cost...CVaR is a complementary measure to VaR, and tells us the expected worst case cost when the cost exceeds the threshold given by VaRα value"
  - [section 6] "The policies with different risk-aversion levels using risk measures and recourse desiderata"
- Break condition: If recourse outcomes have extreme outliers or non-stationary distributions, VaR and CVaR may not provide reliable guidance.

### Mechanism 3
- Claim: Finite horizon MDP formulation allows incorporation of action costs, transition uncertainties, and causal constraints into recourse computation.
- Mechanism: The MDP framework models states as feature combinations, actions as feature changes, transitions as probabilistic outcomes of actions, and rewards as costs. This allows the G-RSVI algorithm to compute optimal risk-aware policies.
- Core assumption: The recourse problem can be adequately modeled as a finite horizon MDP with discrete states and actions.
- Evidence anchors:
  - [section 4.2] "To compute SafeAR recommendations, we frame the problem as solving a finite horizon Markov Decision Process (MDP), defined as tuple of ⟨S, A, T, R, H⟩"
  - [section 5.1] "In risk-neutral settings, the recourse policy π maximizes the expected total cost E[R̂πh(s)]"
- Break condition: If the recourse problem involves continuous or high-dimensional state spaces that cannot be discretized effectively, the MDP formulation becomes intractable.

## Foundational Learning

- Concept: Reinforcement learning and Markov Decision Processes
  - Why needed here: The recourse problem is formulated as an MDP where states represent feature combinations, actions represent feature changes, and the goal is to find a policy that leads to favorable outcomes while managing risk.
  - Quick check question: What are the key components of an MDP and how do they map to the algorithmic recourse problem?

- Concept: Risk-sensitive optimization and variance-aware decision making
  - Why needed here: Standard RL optimizes expected return, but algorithmic recourse needs to consider the variability of costs to avoid policies that might lead to very high costs with some probability.
  - Quick check question: How does the G-RSVI algorithm incorporate risk sensitivity into value iteration?

- Concept: Statistical risk measures (VaR and CVaR)
  - Why needed here: These measures provide interpretable summaries of the risk profile of recourse policies, allowing individuals to choose policies aligned with their risk tolerance.
  - Quick check question: What is the difference between VaR and CVaR, and when might each be more informative?

## Architecture Onboarding

- Component map: Dataset preprocessor -> MDP constructor -> G-RSVI algorithm -> Risk evaluation -> Visualization
- Critical path: Load and preprocess dataset → construct MDP with state space, actions, transitions, and costs → run G-RSVI to compute policy → evaluate policy using risk measures → visualize and compare policies
- Design tradeoffs: Discrete state/action spaces enable exact computation but limit scalability; risk sensitivity improves safety but may increase computational cost and reduce success rates; using domain knowledge for transition models improves realism but may introduce bias.
- Failure signatures: Policies with high variance in cost despite low expected cost; failure to reach favorable states within the horizon; large disparities in risk measures across demographic groups; computational intractability for large state spaces.
- First 3 experiments:
  1. Run G-RSVI with β=0 (risk-neutral) on a small discretized dataset and verify that it finds a policy with low expected cost but potentially high variance.
  2. Run G-RSVI with β=0.5 and compare the variance in cost to the risk-neutral case, expecting lower variance but higher expected cost.
  3. Compute VaR95 and CVaR95 for policies with different β values and verify that more risk-averse policies have lower values for these measures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of risk measure (VaR vs CVaR) impact the recommended recourse policies and their perceived safety by end users?
- Basis in paper: [explicit] The paper uses both Value at Risk (VaR) and Conditional Value at Risk (CVaR) to evaluate and communicate risk, but doesn't explore the implications of choosing one over the other for policy recommendations.
- Why unresolved: The paper presents both measures but doesn't analyze how different risk measures might lead to different policy recommendations or affect user perception of safety.
- What evidence would resolve it: Comparative analysis of policies generated using VaR-only vs CVaR-only optimization, including user studies on perceived safety and policy selection.

### Open Question 2
- Question: What is the long-term impact of implementing risk-aware algorithmic recourse on systemic fairness and equity across different demographic groups?
- Basis in paper: [explicit] The paper observes disparities in risk exposure between gender groups but doesn't explore the broader societal implications of these disparities.
- Why unresolved: The paper identifies disparities but doesn't investigate how widespread adoption of risk-aware recourse might affect systemic fairness over time or how to mitigate potential negative impacts.
- What evidence would resolve it: Longitudinal studies tracking fairness metrics across multiple domains and demographic groups as risk-aware recourse systems are implemented.

### Open Question 3
- Question: How can the computational complexity of G-RSVI be reduced for large-scale deployment in real-world systems with high-dimensional state spaces?
- Basis in paper: [inferred] The paper presents G-RSVI and G-RSEVI algorithms but acknowledges the computational complexity challenges, particularly for large state spaces.
- Why unresolved: The paper demonstrates the algorithms on datasets with manageable state spaces but doesn't address scaling challenges for real-world applications with many features and states.
- What evidence would resolve it: Empirical comparisons of approximation methods, function approximation techniques, or distributed implementations showing scalability to high-dimensional problems.

## Limitations

- The transition probabilities and action costs are based on qualitative domain knowledge rather than learned from data, which may not accurately reflect real-world uncertainty.
- The MDP formulation requires discretization of continuous features, potentially losing important information and limiting scalability to high-dimensional problems.
- The experiments only consider two datasets, and the disparity analysis across demographic groups is limited to a binary gender classification without exploring intersectional factors.

## Confidence

**High confidence**: The theoretical framework connecting MDPs to algorithmic recourse and the mechanism of risk-sensitive value iteration are well-established and clearly explained. The relationship between β values and risk-aversion levels is straightforward and verifiable.

**Medium confidence**: The experimental results showing reduced variance and improved risk measures are convincing, but the reliance on assumed transition probabilities rather than learned models introduces uncertainty. The observed demographic disparities are concerning but require more comprehensive analysis to draw strong conclusions.

**Low confidence**: Claims about the practical applicability to real-world systems are not fully supported, as the paper doesn't address how to estimate transition probabilities and action costs from observational data or how to handle continuous, high-dimensional feature spaces.

## Next Checks

1. **Transition Model Validation**: Replace the qualitative assumptions about transition probabilities with a learned model (e.g., using causal inference or reinforcement learning from observational data) and verify that the core findings about risk reduction persist.

2. **Scalability Assessment**: Test SafeAR on a dataset with higher dimensionality (e.g., 20+ features) to evaluate how the discretization approach and computational requirements scale, and assess whether the MDP framework remains tractable.

3. **Extended Disparity Analysis**: Expand the demographic disparity analysis to include intersectional factors (e.g., gender × race) and additional protected attributes, using statistical tests to determine whether observed disparities are significant and exploring whether different risk-aversion levels can mitigate these disparities without sacrificing overall performance.