---
ver: rpa2
title: Robust Knowledge Extraction from Large Language Models using Social Choice
  Theory
arxiv_id: '2312.14877'
source_url: https://arxiv.org/abs/2312.14877
tags:
- query
- uncertainty
- queries
- answers
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of improving robustness of Large
  Language Models (LLMs) for high-stakes query answering domains like medicine by
  reducing uncertainty in repeated queries. The core method aggregates multiple rankings
  from repeated LLM queries using the Partial Borda Choice (PBW) function from social
  choice theory, normalizing and scoring diagnoses based on their frequency and ranking
  positions.
---

# Robust Knowledge Extraction from Large Language Models using Social Choice Theory

## Quick Facts
- arXiv ID: 2312.14877
- Source URL: https://arxiv.org/abs/2312.14877
- Reference count: 19
- Key outcome: PBW aggregation improves LLM ranking robustness across domains, achieving 0.63-0.78 Kendall's tau vs. 0.29-0.57 for baselines in high-uncertainty settings.

## Executive Summary
This paper addresses the problem of improving robustness in LLM query answering for high-stakes domains by reducing uncertainty in repeated queries. The core method aggregates multiple rankings from repeated LLM queries using the Partial Borda Choice (PBW) function from social choice theory. Experiments show PBW significantly outperforms baselines across manufacturing, finance, and medical domains, demonstrating improved robustness against both query and syntax uncertainty.

## Method Summary
The method generates symptom-cause matrices for each domain, samples symptom sets with controlled uncertainty levels, converts them to ranking queries, and executes these queries multiple times with an LLM. Rankings are normalized and aggregated using PBW scoring, with results evaluated using Kendall's and Spearman's rank correlation coefficients. The approach leverages social choice theory to combine multiple partial rankings into a single consensus ranking with uncertainty quantification.

## Key Results
- PBW aggregation achieves 0.63-0.78 Kendall's tau vs. 0.29-0.57 for baselines in high-uncertainty settings
- Even aggregating just two answers yields notable robustness gains
- PBW demonstrates improved robustness against both query and syntax uncertainty across manufacturing, finance, and medical domains

## Why This Works (Mechanism)

### Mechanism 1
Repeated LLM queries followed by PBW aggregation reduce query uncertainty by exploiting the tendency of LLMs to converge toward correct diagnoses when information is present in training data. Multiple rankings are collected and PBW scoring assigns higher weights to outcomes appearing in higher ranks, amplifying consensus while diminishing random variations. If the LLM has learned the correct information, the same diagnosis will appear frequently and in top positions across queries.

### Mechanism 2
PBW satisfies desirable axiomatic properties (Consistency, Faithfulness, Neutrality, Cancellation) that make it suitable for aggregating LLM rankings. The aggregation process ensures that outcomes consistently preferred across rankings dominate the final score, while random or contradictory rankings cancel out. Faithfulness ensures that a single high-ranked outcome is scored maximally; Consistency ensures combining independent sets of rankings preserves the dominant outcome.

### Mechanism 3
Normalizing PBW scores to [0,1] yields interpretable confidence values and preserves ranking order, enabling direct comparison across queries. Raw PBW scores are divided by their sum across all outcomes, producing a probability-like scale without changing the argmax outcome. The relative magnitude of PBW scores reflects true uncertainty, and rescaling does not distort decision-relevant information.

## Foundational Learning

- **Partial Borda Choice (PBW) weighting and scoring**: Provides a principled method to aggregate multiple partial rankings into a single consensus ranking with uncertainty quantification. *Quick check*: Given two rankings [A>B>C] and [B>A>C], what are the Down and Inc counts for A, B, C and their PBW scores?

- **Jaccard similarity and entropy for symptom set uncertainty**: Quantifies how uniquely a symptom set identifies a diagnosis, enabling controlled sampling of low/high uncertainty queries. *Quick check*: If symptom set S1 overlaps with diagnosis D1 in 2 symptoms out of 5 total, and with D2 in 1 out of 5, what are the normalized Jaccard similarities?

- **Kendall's tau and Spearman's rho for rank correlation**: Measures robustness of aggregated rankings against repeated queries and syntactic variants. *Quick check*: For two rankings A>B>C>D and A>C>B>D, compute the number of concordant and discordant pairs and Kendall's tau.

## Architecture Onboarding

- **Component map**: Symptom-Cause Matrix Generation -> Symptom Set Sampling -> Ranking Query Generation -> LLM Query Execution -> Ranking Normalization -> PBW Aggregation -> Robustness Evaluation

- **Critical path**: 1) Generate symptom-cause matrices for domain. 2) Sample symptom sets with desired uncertainty. 3) Convert to ranking queries. 4) Run LLM queries repeatedly. 5) Normalize and aggregate rankings via PBW. 6) Evaluate robustness.

- **Design tradeoffs**: Using 5 repeats balances robustness gains against cost; fewer may under-sample randomness, more may be wasteful. Sentence-BERT similarity threshold of 0.5 may discard borderline symptoms; lowering it risks noise. PBW weighting constants (α=2, β=1, γ=0) are fixed; tuning could adapt to domain-specific ranking quality.

- **Failure signatures**: Low correlation scores across repeats indicate high query uncertainty or hallucination. High variance in PBW scores across syntax variants indicates sensitivity to wording. Many symptoms mapped below similarity threshold suggest poor base-outcome coverage.

- **First 3 experiments**: 1) Run PBW on a single symptom set with N=2 repeats; compare Kendall's tau to N=5. 2) Generate two syntax variants of the same query; aggregate each separately and compute correlation. 3) Vary the Sentence-BERT similarity threshold (0.4, 0.5, 0.6) and measure impact on final ranking quality.

## Open Questions the Paper Calls Out

- **Performance comparison to other social choice functions**: How does PBW compare to other established social choice functions (e.g., majority voting, Borda count) in the context of LLM ranking query aggregation? The paper only compares PBW to simple baselines and does not explore other social choice functions.

- **Impact of number of ranking outcomes**: What is the impact of the number of ranking outcomes (diagnoses) on the effectiveness of PBW in aggregating LLM responses? The paper does not analyze how this affects PBW's performance.

- **Complex symptom-diagnosis relationships**: How does PBW perform when aggregating LLM responses in domains with more complex relationships between symptoms and diagnoses (e.g., overlapping symptoms, causal relationships)? The experiments use relatively simple symptom-diagnosis relationships, and the paper does not explore more complex scenarios.

## Limitations
- PBW's axiomatic properties from social choice theory have not been independently validated for LLM rankings
- Experiments rely on controlled synthetic symptom sets rather than real-world query distributions
- Fixed aggregation parameters (5 repeats, α=2, β=1, γ=0, similarity threshold=0.5) may not generalize across domains or LLM models

## Confidence
- **Core claim validation**: Medium - experimental results are internally consistent but lack independent validation
- **Transfer of social choice guarantees**: Low - axiomatic properties assumed to transfer without empirical verification
- **Generalization across domains**: Medium - fixed parameters may not adapt well to different domains or LLM models

## Next Checks
1. Validate PBW's axiomatic properties (Consistency, Faithfulness, Neutrality, Cancellation) on LLM-generated rankings using synthetic preference profiles
2. Conduct ablation studies varying the number of repeats (N=2, 5, 10) and PBW parameters to quantify sensitivity and diminishing returns
3. Compare PBW against alternative aggregation methods (Borda count, plurality, average rank) on the same dataset to isolate the contribution of PBW specifically