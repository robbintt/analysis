---
ver: rpa2
title: 'DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services'
arxiv_id: '2309.11325'
source_url: https://arxiv.org/abs/2309.11325
tags:
- legal
- disc-lawllm
- knowledge
- llms
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DISC-LawLLM is an intelligent legal system built on large language
  models, designed to provide comprehensive legal services including consultation,
  document analysis, and examination assistance. The system uses legal syllogism prompting
  strategies to construct supervised fine-tuning datasets in the Chinese judicial
  domain, training LLMs with legal reasoning capabilities.
---

# DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services

## Quick Facts
- arXiv ID: 2309.11325
- Source URL: https://arxiv.org/abs/2309.11325
- Reference count: 8
- Key outcome: DISC-LawLLM outperforms existing legal LLMs and GPT-3.5-turbo by an average of 7% on legal reasoning tasks

## Executive Summary
DISC-LawLLM is an intelligent legal system built on large language models, designed to provide comprehensive legal services including consultation, document analysis, and examination assistance. The system uses legal syllogism prompting strategies to construct supervised fine-tuning datasets in the Chinese judicial domain, training LLMs with legal reasoning capabilities. A retrieval module is integrated to access and utilize external legal knowledge, enhancing response reliability. The system is evaluated using DISC-Law-Eval, a comprehensive benchmark assessing both objective and subjective dimensions. Experimental results show DISC-LawLLM significantly outperforms existing legal LLMs and even GPT-3.5-turbo on most test subjects, with accuracy improvements averaging 7%. The system demonstrates strong legal reasoning capabilities, particularly for multi-answer questions requiring nuanced judgment.

## Method Summary
DISC-LawLLM employs a two-step approach: first, supervised fine-tuning on a base LLM (Baichuan-13B) using a large legal dataset (DISC-Law-SFT with 403K samples) constructed with legal syllogism prompting strategies; second, integration of a retrieval module that accesses a knowledge base of Chinese laws and regulations. The legal syllogism prompting transforms raw legal text into instruction pairs following the major premise-minor premise-conclusion format, while the retrieval module provides external legal references to ground responses and reduce hallucinations. The system is evaluated using DISC-Law-Eval, a comprehensive benchmark combining objective multiple-choice questions from standardized exams with subjective assessments scored by GPT-3.5 on accuracy, completeness, and clarity.

## Key Results
- DISC-LawLLM achieves an average accuracy improvement of 7% over GPT-3.5-turbo on legal reasoning tasks
- The system demonstrates superior performance on multi-answer questions requiring nuanced legal judgment
- Integration of retrieval augmentation significantly reduces hallucinations and improves response reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Legal syllogism prompting improves reasoning consistency by enforcing structured legal argumentation
- Mechanism: The system transforms raw legal text into instruction pairs that follow the major premise (law), minor premise (facts), and conclusion format, which aligns with how judges reason
- Core assumption: Legal reasoning is inherently syllogistic and benefits from explicit structure
- Evidence anchors: [abstract] "We adopt legal syllogism prompting strategies to construct supervised fine-tuning datasets in the Chinese Judicial domain"; [section 3.2] "In the syllogism of legal judgment, the major premise is the applicable law, while the minor premise is pertinent facts, and the conclusion is the final judgment"

### Mechanism 2
- Claim: Retrieval augmentation reduces hallucinations by grounding responses in verified legal sources
- Mechanism: A knowledge base of statutes is encoded and retrieved based on semantic similarity to user queries, providing external references that the LLM uses to generate more reliable answers
- Core assumption: Access to current legal knowledge improves answer accuracy and reduces fabrication
- Evidence anchors: [section 4.2] "We augment the DISC-LawLLM with a retrieval module based on an open-source retrieval framework" and "our knowledge base is designed for dynamic updates"; [abstract] "We augment LLMs with a retrieval module to enhance models' ability to access and utilize external legal knowledge"

### Mechanism 3
- Claim: Comprehensive evaluation benchmark enables reliable system assessment across multiple dimensions
- Mechanism: DISC-Law-Eval combines objective multiple-choice questions from standardized exams (Easy/Normal/Hard) with subjective assessments using GPT-3.5 as an arbitrator scoring accuracy, completeness, and clarity
- Core assumption: Multi-dimensional evaluation provides better insight into system capabilities than single metrics
- Evidence anchors: [section 5] "We develop a fair evaluation framework, DISC-Law-Eval Benchmark, assessing systems from both the objective perspective and subjective perspective"; [abstract] "A comprehensive legal benchmark, DISC-Law-Eval, is presented to evaluate intelligent legal systems from both objective and subjective dimensions"

## Foundational Learning

- Concept: Legal syllogism structure (major premise, minor premise, conclusion)
  - Why needed here: This forms the foundation for legal reasoning capability in the LLM
  - Quick check question: Can you identify the major premise, minor premise, and conclusion in this legal statement: "Article 123 of the Civil Code states that contracts must be honored. The defendant signed a contract but failed to deliver goods. Therefore, the defendant breached the contract."

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: This enables the system to access up-to-date legal knowledge beyond what was in the original training data
  - Quick check question: What happens if the retrieval module finds no relevant statutes for a user query?

- Concept: Multi-dimensional evaluation metrics
  - Why needed here: Legal systems require assessment beyond simple accuracy to capture completeness and clarity
  - Quick check question: Why might a system score high on accuracy but low on completeness in legal Q&A?

## Architecture Onboarding

- Component map: Baichuan-13B-Base model → Supervised Fine-Tuning → Retrieval Module → DISC-Law-Eval Benchmark
- Critical path: User query → Retrieval module (statute lookup) → LLM with retrieved references → Structured legal response
- Design tradeoffs: Larger base model vs. computational cost; more retrieval documents vs. response latency
- Failure signatures: Incorrect legal conclusions (reasoning failure), irrelevant statute retrieval (knowledge failure), verbose/unclear responses (communication failure)
- First 3 experiments:
  1. Test syllogism prompting on simple contract law cases to verify reasoning structure
  2. Evaluate retrieval accuracy by measuring relevance of top-K statutes for diverse legal queries
  3. Compare subjective evaluation scores with and without retrieval augmentation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DISC-LawLLM compare to other legal LLMs when tested on a multilingual dataset including both Chinese and English legal texts?
- Basis in paper: [inferred] The paper focuses on Chinese legal texts and DISC-LawLLM's performance on Chinese legal datasets, but does not explore its capabilities in multilingual settings
- Why unresolved: The paper does not provide any comparative analysis or experimental results for multilingual legal datasets
- What evidence would resolve it: Comparative studies and performance metrics of DISC-LawLLM on multilingual legal datasets, including both Chinese and English texts

### Open Question 2
- Question: What are the long-term implications of using DISC-LawLLM in legal practice, particularly in terms of its impact on the legal profession and access to justice?
- Basis in paper: [explicit] The paper discusses the potential applications of DISC-LawLLM in legal practice but does not delve into the long-term implications or broader societal impact
- Why unresolved: The paper focuses on the technical aspects and performance of DISC-LawLLM without addressing its broader implications for the legal profession and society
- What evidence would resolve it: Studies and analyses on the long-term effects of implementing DISC-LawLLM in legal practice, including changes in legal job roles, access to legal services, and public trust in legal AI systems

### Open Question 3
- Question: How does the integration of external knowledge retrieval impact the model's ability to handle novel legal scenarios that are not well-represented in the training data?
- Basis in paper: [inferred] The paper mentions the use of a retrieval module to enhance the model's ability to access external legal knowledge, but does not explore its effectiveness in handling novel legal scenarios
- Why unresolved: The paper does not provide experimental results or case studies demonstrating the model's performance in novel legal scenarios
- What evidence would resolve it: Experimental results and case studies showing DISC-LawLLM's performance in novel legal scenarios, comparing its accuracy and reliability with and without the retrieval module

## Limitations

- Evaluation relies on a proprietary Chinese legal benchmark that may not generalize to other legal systems or jurisdictions
- Claims of 7% average accuracy improvement depend entirely on the quality and representativeness of the evaluation questions, which are not publicly available
- System's performance on novel legal scenarios outside the training distribution remains unverified

## Confidence

- **High Confidence**: The technical architecture (SFT followed by retrieval augmentation) is sound and well-documented. The retrieval mechanism and evaluation framework structure are clearly specified
- **Medium Confidence**: The reported performance improvements are plausible given the methodology, but verification is limited by lack of public benchmark access. The legal reasoning improvements through syllogism prompting are theoretically justified but not empirically isolated from other factors
- **Low Confidence**: Claims about handling multi-answer questions requiring nuanced judgment are difficult to verify without seeing the specific test cases. The system's performance in real-world deployment scenarios is unknown

## Next Checks

1. **Cross-Jurisdictional Testing**: Apply DISC-LawLLM to legal questions from non-Chinese legal systems to assess generalization beyond the Chinese judicial domain. This would test whether the legal reasoning capabilities are domain-specific or transferable

2. **Ablation Study on Reasoning Components**: Conduct controlled experiments comparing performance with and without the legal syllogism prompting versus standard instruction tuning, and with and without the retrieval module, to isolate the contribution of each component to overall performance

3. **Real-World Deployment Trial**: Deploy the system in a controlled legal consultation setting with human legal professionals evaluating the quality, accuracy, and usefulness of responses in actual practice, rather than relying solely on exam-style questions