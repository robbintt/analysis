---
ver: rpa2
title: 'VideolandGPT: A User Study on a Conversational Recommender System'
arxiv_id: '2309.03645'
source_url: https://arxiv.org/abs/2309.03645
tags:
- recommender
- user
- system
- personalised
- recommendations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VideolandGPT, a conversational recommender
  system that integrates ChatGPT with a personalized ranking model for the Videoland
  video-on-demand platform. The system uses user interactions and preferences to generate
  personalized recommendations from a candidate list of content.
---

# VideolandGPT: A User Study on a Conversational Recommender System

## Quick Facts
- arXiv ID: 2309.03645
- Source URL: https://arxiv.org/abs/2309.03645
- Reference count: 25
- Primary result: Personalized LLM-based recommendations achieve higher accuracy (nDCG@9 of 0.427 vs 0.388) and user satisfaction compared to non-personalized versions.

## Executive Summary
This paper presents VideolandGPT, a conversational recommender system that integrates ChatGPT with a personalized ranking model for the Videoland video-on-demand platform. The system uses user interactions and preferences to generate personalized recommendations from a candidate list of content. A user study compared personalized and non-personalized versions, finding that the personalized version achieved higher accuracy (nDCG@9 of 0.427 vs 0.388) and user satisfaction. However, both versions sometimes recommended titles not available on the platform, raising fairness concerns. The study demonstrates the potential of LLMs to enhance recommender systems through natural language interaction, while highlighting the need for safeguards to ensure recommendation consistency and platform alignment.

## Method Summary
The study compared a personalized conversational recommender system against a non-personalized baseline using a between-subject user study design. Participants interacted with the system across five tasks, with the personalized version using a matrix factorization and neural ranking model to generate top 300 personalized candidates per user, while the non-personalized version used fixed candidates. ChatGPT (gpt-3.5-turbo) was used to generate recommendations and explanations from the candidate lists based on conversational context. User interactions and satisfaction were logged and measured using accuracy metrics (nDCG@9, HR@9) and Likert-type questionnaires.

## Key Results
- Personalized version achieved 10% relative improvement in accuracy over non-personalized version (nDCG@9: 0.427 vs 0.388)
- Both personalized and non-personalized versions recommended titles not available on Videoland platform in over 22% of tasks
- User satisfaction was higher for personalized version across all tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Personalized LLM-based recommendations outperform non-personalized ones in both accuracy and user satisfaction.
- Mechanism: Personalization enables the system to leverage individual user profiles and interaction histories, which the LLM uses to filter and rank candidate content, resulting in more relevant recommendations.
- Core assumption: The ranking model provides sufficiently accurate candidate lists for personalization to be effective.
- Evidence anchors:
  - [abstract] "personalized version achieved higher accuracy (nDCG@9 of 0.427 vs 0.388) and user satisfaction"
  - [section] "The personalised framework demonstrated a 10% relative improvement over the non-personalised version in all tasks"
  - [corpus] Found related work on CRS personalization but no direct replication of these specific accuracy gains.
- Break condition: If the ranking model fails to produce quality candidates, personalization may degrade rather than improve recommendations.

### Mechanism 2
- Claim: Conversational interaction allows the LLM to dynamically refine recommendations based on user feedback.
- Mechanism: Users can request refinements or provide explicit preferences during conversation, and the LLM uses this context to adjust recommendations within the candidate pool.
- Core assumption: The LLM can effectively incorporate conversational context into its reasoning about recommendations.
- Evidence anchors:
  - [section] "As the conversation progresses, the user can either accept a recommendation or give feedback to the system to refine their discovery preferences"
  - [abstract] "uses user interactions and preferences to generate personalized recommendations"
  - [corpus] Limited direct evidence in corpus about LLM refinement effectiveness in CRS settings.
- Break condition: If user feedback is ambiguous or the LLM cannot properly interpret conversational intent, refinement may produce worse recommendations.

### Mechanism 3
- Claim: Integrating LLM reasoning with existing ranking models creates a hybrid system that balances personalization with broad content discovery.
- Mechanism: The ranking model provides a narrowed candidate set (top 300 items), while the LLM adds natural language interaction and reasoning capabilities to select from this set based on user conversation.
- Core assumption: The candidate pool size (300 items) provides sufficient diversity while maintaining relevance.
- Evidence anchors:
  - [section] "Our Ranking Model retrieves the top 300 titles for each user, reducing the catalog by approximately 90%. We believe this number achieves a balance between relevance and discoverability"
  - [abstract] "uses ChatGPT to select from a predetermined set of contents"
  - [corpus] No corpus evidence about optimal candidate pool sizes for CRS systems.
- Break condition: If the candidate pool is too small, discoverability suffers; if too large, the LLM may struggle to maintain relevance.

## Foundational Learning

- Concept: Matrix Factorization and Collaborative Filtering
  - Why needed here: The system uses a matrix factorization component as part of its ranking model to generate personalized candidate lists.
  - Quick check question: What is the fundamental assumption behind matrix factorization for collaborative filtering?

- Concept: Attention Mechanisms and Transformer Architecture
  - Why needed here: The neural component of the ranking model utilizes attention mechanisms, and the LLM itself is based on transformer architecture.
  - Quick check question: How does the attention mechanism enable the model to weigh different items in the user's interaction history?

- Concept: Fairness in Recommender Systems
  - Why needed here: The paper explicitly evaluates fairness concerns, noting that recommendations sometimes include unavailable titles.
  - Quick check question: What are the key dimensions along which recommender system fairness is typically evaluated?

## Architecture Onboarding

- Component map: User Interface -> Chat Interface (LLM) <- Prompt Generator <- Ranking Model (Matrix Factorization + Neural Component) <- User Interaction History; Post-processor adds metadata and filters recommendations.
- Critical path: User query -> Prompt generation -> LLM recommendation -> Post-processing -> User presentation
- Design tradeoffs: Balance between personalization (requires user data) and privacy; trade-off between candidate pool size (300 items) and recommendation quality; LLM capabilities vs. platform content constraints.
- Failure signatures: Recommendations include unavailable titles; LLM generates irrelevant recommendations; system becomes slow due to LLM processing; user engagement drops despite personalization.
- First 3 experiments:
  1. Test LLM with fixed candidate list to isolate reasoning quality from ranking model performance.
  2. Compare recommendation quality with varying candidate pool sizes (100, 300, 500 items).
  3. A/B test with and without conversational refinement to measure the value of interactive feedback.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design a conversational recommender system that consistently avoids recommending titles not available on the platform while maintaining high recommendation quality?
- Basis in paper: [explicit] The study found that both personalized and non-personalized versions recommended titles not available on Videoland in over 22% of tasks, despite attempts to control this through candidate filtering
- Why unresolved: The paper intentionally omitted the post-processing safeguard to examine fairness impacts, and current methods don't prevent LLM hallucinations of unavailable content
- What evidence would resolve it: Development and evaluation of a system that completely eliminates unavailable recommendations while maintaining or improving nDCG scores

### Open Question 2
- Question: What specific mechanisms can ensure that LLM-based conversational recommender systems respect user privacy preferences when handling explicit personal information shared during conversations?
- Basis in paper: [inferred] The discussion mentions the need for safeguards to ensure safety and prevent users from exploiting the system, particularly when users explicitly share personal details
- Why unresolved: The paper identifies this as a concern but doesn't propose or test specific privacy-preserving mechanisms for conversational contexts
- What evidence would resolve it: Implementation and evaluation of privacy-preserving techniques that maintain recommendation quality while protecting user-shared personal information

### Open Question 3
- Question: How do different language backgrounds of users affect the quality and fairness of LLM-based conversational recommendations, particularly when the platform primarily serves content in one language?
- Basis in paper: [explicit] The study notes that 65% of participants were Dutch speakers and asks about potential correlations between recommendation quality and language background
- Why unresolved: The paper collected language information but didn't analyze its impact on recommendation performance or fairness metrics
- What evidence would resolve it: Comparative analysis of recommendation quality and fairness across different language groups using the same conversational system

## Limitations

- The fairness evaluation reveals a significant limitation: recommendations sometimes include titles not available on the platform, suggesting potential misalignment between LLM reasoning and platform constraints.
- The system's performance depends heavily on the quality of the underlying ranking model's candidate generation, which is not fully detailed.
- The user study sample size and demographic distribution are not specified, which may affect generalizability of satisfaction results.

## Confidence

- **High confidence**: The personalized vs non-personalized accuracy comparison (nDCG@9: 0.427 vs 0.388) is well-supported by study data and demonstrates clear performance differences.
- **Medium confidence**: The mechanism of conversational refinement improving recommendations is supported by the study design but lacks detailed quantitative analysis of refinement effectiveness.
- **Low confidence**: The optimal candidate pool size (300 items) is presented as a design choice without systematic exploration of alternatives or sensitivity analysis.

## Next Checks

1. Conduct a sensitivity analysis varying candidate pool sizes (100-500 items) to determine optimal balance between discoverability and recommendation quality.
2. Implement and test additional filtering mechanisms to prevent LLM recommendations of unavailable titles, then measure impact on both accuracy and fairness metrics.
3. Replicate the user study with a more diverse participant pool and include longitudinal tracking to assess whether satisfaction and accuracy gains persist over extended use.