---
ver: rpa2
title: 'MEGA: Multilingual Evaluation of Generative AI'
arxiv_id: '2303.12528'
source_url: https://arxiv.org/abs/2303.12528
tags:
- languages
- language
- performance
- dv003
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents MEGA (Multilingual Evaluation of Generative
  AI), the first comprehensive benchmarking study of generative large language models
  (LLMs) like ChatGPT and GPT-4 across 33 typologically diverse languages and 8 NLP
  tasks including natural language inference, paraphrase detection, commonsense reasoning,
  and question answering. The authors compare the performance of GPT-3.5 text-davinci-003
  against BLOOMZ and state-of-the-art non-autoregressive models like TULRv6 and MuRIL
  across three prompting strategies: monolingual, zero-shot cross-lingual, and translate-test.'
---

# MEGA: Multilingual Evaluation of Generative AI

## Quick Facts
- arXiv ID: 2303.12528
- Source URL: https://arxiv.org/abs/2303.12528
- Authors: 
- Reference count: 18
- Key outcome: First comprehensive benchmarking of generative LLMs across 33 languages and 8 NLP tasks, finding translate-test prompting best but generative models still lag specialized models for low-resource and non-Latin script languages.

## Executive Summary
This paper presents MEGA (Multilingual Evaluation of Generative AI), the first comprehensive benchmarking study of generative large language models like ChatGPT and GPT-4 across 33 typologically diverse languages and 8 NLP tasks. The authors compare GPT-3.5 text-davinci-003 against BLOOMZ and state-of-the-art non-autoregressive models across three prompting strategies: monolingual, zero-shot cross-lingual, and translate-test. They find that while translate-test (translating input to English, querying the model, and translating back) generally performs best, generative LLMs still lag significantly behind specialized models, particularly for low-resource and non-Latin script languages.

## Method Summary
The study evaluates DV003 on 16 NLP datasets covering 70 typologically diverse languages using prompt-based few-shot learning with three prompting strategies. The authors compare DV003 performance against BLOOMZ and state-of-the-art models like TULRv6 and MuRIL, using classification accuracy for classification tasks and Exact Match and F1 scores for QA tasks. Experiments are conducted without fine-tuning or hyperparameter tuning, relying on automatic translation services for prompt and test data translation.

## Key Results
- Translate-test prompting strategy performs best overall across all languages and tasks
- Generative LLMs significantly underperform specialized models, especially for low-resource and non-Latin script languages
- Tokenization quality and prompt translation issues significantly impact performance, with automatic translation often producing semantically incorrect prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative models lag significantly behind specialized models, especially for low-resource and non-Latin script languages, because they are English-centric in tokenization and pre-training.
- Mechanism: Tokenization affects how well the model can represent and process input. Poor tokenization in non-Latin scripts leads to worse semantic representation, causing performance drops in downstream tasks.
- Core assumption: Tokenization quality is a primary bottleneck for multilingual performance.
- Evidence anchors:
  - "Tokenization is a key component that influences the performance of multilingual models... Figure 26 highlights the disparity in DV003's tokenization to the tokenizer in mBERT... This points to a clear direction for improvement..."
  - "Since tokenization is crucial to generating a meaningful representation of the input, its differential behaviour across languages can help explain the model's poor performance..."

### Mechanism 2
- Claim: Automatic translation of prompts introduces semantic errors that degrade performance, especially for languages with different linguistic structures.
- Mechanism: Prompt translation often fails to preserve meaning due to syntactic differences, leading to incorrect task instructions. Native speaker verification is needed.
- Core assumption: Automatic translation cannot preserve semantic meaning across all language pairs.
- Evidence anchors:
  - "One key observation of translating prompts... is that the resulting prompts are often not semantically meaningful... This is typically due to the difference in the linguistic structure of the pivot language (English) and the target language."
  - "For example: For IndicWNLI... Translating the prompt automatically changes the meaning of the instructions, in this case, swapping the order of the hypothesis and premise..."

### Mechanism 3
- Claim: Translate-test is the best prompting strategy because it leverages English's superior tokenization and pre-training data, but at the cost of losing language-specific knowledge.
- Mechanism: Translating to English allows the model to use its strongest representation capabilities, then translating back. However, this process loses cultural and linguistic nuances.
- Core assumption: English representation is consistently better than other languages in the model.
- Evidence anchors:
  - "Our experiments with DV003 show that the translate-test prompt setting works best overall for all languages and tasks... Translating into English and querying the system (followed by translating back) is feasible for the languages supported by translators available today."
  - "However, any holistic evaluation would consider such metrics in conjunction with human-evaluations by users... Any real world deployment of the technology... would not be productive and successful in meeting user-requirements without a thorough evaluation..."

## Foundational Learning

- Concept: Tokenization and subword units
  - Why needed here: Tokenization quality directly affects how well the model can represent and process input in different languages, which is a core factor in the performance gaps observed.
  - Quick check question: What is the average number of tokens per word for Hindi in GPT vs mBERT, and how might this affect context window usage?

- Concept: Cross-lingual transfer and zero-shot learning
  - Why needed here: Understanding how models generalize across languages is crucial for interpreting why zero-shot cross-lingual prompting performs worse than translate-test.
  - Quick check question: In zero-shot cross-lingual prompting, what is the pivot language used, and why might this choice affect performance?

- Concept: Evaluation metrics for multilingual tasks
  - Why needed here: The paper uses accuracy, exact match, and F1 scores, but understanding their limitations and differences is important for interpreting results across languages.
  - Quick check question: Why might exact match be a harsher metric than F1 for QA tasks, and how could this affect low-resource language evaluation?

## Architecture Onboarding

- Component map: Multilingual datasets (33 typologically diverse languages, 8 NLP tasks) -> Three prompting strategies (monolingual, zero-shot cross-lingual, translate-test) -> Generative models (DV003, BLOOMZ) and specialized models (TULRv6, MuRIL) -> Evaluation framework (automatic metrics) -> Translation services
- Critical path: (1) Prepare datasets in target languages, (2) Design and translate prompts, (3) Generate predictions using chosen model and prompting strategy, (4) Evaluate using appropriate metrics, (5) Compare against specialized models and analyze gaps
- Design tradeoffs: Translate-test gives best performance but loses language-specific knowledge; monolingual prompting preserves language but suffers from tokenization and prompt translation issues; zero-shot cross-lingual is most scalable but performs worst
- Failure signatures: Large performance gaps for low-resource and non-Latin script languages; poor tokenization leading to context truncation; semantic errors in translated prompts
- First 3 experiments:
  1. Replicate tokenization analysis: Compare token counts for a set of languages between DV003 and mBERT to quantify tokenization disparities
  2. Manual prompt verification: Have native speakers verify and correct translated prompts for 2-3 languages, then re-run experiments to measure impact
  3. Controlled translate-test ablation: For a high-resource language, compare translate-test vs. direct prompting with manually crafted prompts to isolate the effect of translation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of generative LLMs vary across different language families and typological features beyond what is captured in current multilingual benchmarks?
- Basis in paper: The authors note that current datasets do not cover many typologically diverse and under-resourced languages, particularly African languages and Indigenous languages of the Americas.
- Why unresolved: The study's evaluation is limited to 33 languages and does not systematically explore typological diversity or language family effects.
- What evidence would resolve it: A comprehensive evaluation across a wider range of language families with systematic variation in typological features (e.g., word order, morphology, script type).

### Open Question 2
- Question: What is the impact of different tokenization strategies on the performance of generative LLMs across languages, and how can tokenization be improved for low-resource languages?
- Basis in paper: The authors highlight significant disparities in tokenization quality between languages, particularly for lower-resource languages, and suggest this affects context encapsulation and semantic representation.
- Why unresolved: While the paper identifies tokenization as a key issue, it does not explore alternative tokenization strategies or their effects on model performance.
- What evidence would resolve it: Comparative studies of different tokenization approaches (e.g., BPE, SentencePiece, language-specific tokenizers) across a diverse set of languages and tasks.

### Open Question 3
- Question: How can we develop more effective prompt generation strategies for multilingual generative LLMs that do not rely on translation and preserve cultural and linguistic nuances?
- Basis in paper: The authors discuss the limitations of automatic translation for prompts, noting that translated prompts often lack semantic meaning and cultural context, particularly for languages with different linguistic structures.
- Why unresolved: The paper suggests human supervision is needed but does not provide a scalable solution for multilingual prompt generation.
- What evidence would resolve it: Development and evaluation of multilingual prompt generation techniques that incorporate native speaker input and cultural context, potentially through human-in-the-loop approaches.

## Limitations

- The study relies on automatic translation services, which may introduce semantic errors in prompts that aren't fully quantified
- Specialized models are compared using zero-shot cross-lingual prompting, which the paper itself shows performs worst, potentially understating the true performance gap
- The analysis identifies tokenization as a key issue but doesn't provide quantitative metrics to prove its impact on performance gaps

## Confidence

**High Confidence**: The overall finding that translate-test performs best among the three prompting strategies is well-supported by experimental results across multiple datasets and languages. The performance ranking (translate-test > monolingual > zero-shot) appears consistent and robust.

**Medium Confidence**: The claim that tokenization quality is a primary bottleneck for low-resource languages is plausible but not definitively proven. The evidence shows correlation between poor tokenization and performance gaps, but causation is not established through controlled experiments.

**Low Confidence**: The assertion that automatic translation frequently produces semantically incorrect prompts is based on anecdotal examples rather than systematic measurement. Without knowing the frequency and severity of these errors, their overall impact on results is uncertain.

## Next Checks

1. **Quantitative Tokenization Analysis**: Measure average token counts per word for 10-15 representative languages across DV003 and mBERT, then correlate these differences with task performance to establish statistical significance of the tokenization-performance relationship.

2. **Prompt Translation Error Audit**: For 3-5 languages showing poor performance, manually audit 50-100 automatically translated prompts to quantify the percentage containing semantic errors, then measure performance difference between corrected and uncorrected prompts.

3. **Controlled SOTA Comparison**: Re-run specialized models (TULRv6, MuRIL) using translate-test prompting rather than zero-shot cross-lingual to establish a fairer baseline for comparison with generative models.