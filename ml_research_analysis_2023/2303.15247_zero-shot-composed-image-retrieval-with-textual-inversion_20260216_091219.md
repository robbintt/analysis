---
ver: rpa2
title: Zero-Shot Composed Image Retrieval with Textual Inversion
arxiv_id: '2303.15247'
source_url: https://arxiv.org/abs/2303.15247
tags:
- image
- images
- dataset
- textual
- cirr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the zero-shot composed image retrieval (ZS-CIR)
  task to remove the need for expensive labeled training data. The authors propose
  SEARLE, a method that maps reference images into pseudo-words using a textual inversion
  network, which are then combined with relative captions for text-to-image retrieval.
---

# Zero-Shot Composed Image Retrieval with Textual Inversion

## Quick Facts
- **arXiv ID**: 2303.15247
- **Source URL**: https://arxiv.org/abs/2303.15247
- **Reference count**: 40
- **Primary result**: Introduces SEARLE, a zero-shot method for composed image retrieval that maps reference images to pseudo-words and combines them with relative captions for text-to-image retrieval, achieving state-of-the-art performance on FashionIQ, CIRR, and CIRCO datasets.

## Executive Summary
This paper introduces the zero-shot composed image retrieval (ZS-CIR) task to eliminate the need for expensive labeled training data. The authors propose SEARLE, a method that maps reference images into pseudo-words using a textual inversion network, which are then combined with relative captions for text-to-image retrieval. SEARLE employs a two-stage training process involving optimization-based textual inversion with GPT-powered regularization and knowledge distillation to a lightweight network. The authors also introduce CIRCO, a new open-domain dataset with multiple ground truths per query. Experiments show SEARLE significantly outperforms baselines on FashionIQ, CIRR, and CIRCO datasets, achieving state-of-the-art performance.

## Method Summary
SEARLE tackles ZS-CIR by mapping images into pseudo-words through a two-stage process. First, optimization-based textual inversion (OTI) generates pseudo-word tokens by minimizing cosine distance between CLIP image and text features, enhanced with GPT-powered regularization that ensures pseudo-words interact well with human-like text. Second, a lightweight textual inversion network φ is trained via knowledge distillation to predict these tokens from CLIP image features, enabling efficient inference. The method is trained on unlabeled datasets and evaluated on three benchmarks: FashionIQ, CIRR, and the newly introduced CIRCO dataset.

## Key Results
- SEARLE achieves state-of-the-art performance on FashionIQ, CIRR, and CIRCO datasets
- Outperforms existing baselines in both zero-shot and fine-tuned settings
- Introduces CIRCO, the first composed image retrieval dataset with multiple ground truths per query
- Demonstrates effective zero-shot transfer without requiring labeled training data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The pseudo-word token generated via optimization-based textual inversion can effectively encode both visual content and interact with relative captions in CLIP's embedding space.
- **Core assumption**: GPT-generated phrases are sufficiently similar in structure to relative captions, so regularization generalizes to real relative captions.
- **Evidence anchors**: Abstract and section claims about pseudo-word integration with relative captions; weak corpus support.
- **Break condition**: If relative captions have very different structure or vocabulary from GPT-generated phrases, regularization may not generalize.

### Mechanism 2
- **Claim**: Distilling OTI knowledge into φ enables efficient zero-shot inference while retaining expressive power.
- **Core assumption**: OTI-generated pseudo-words capture essential visual information that can be transferred to a feed-forward network via distillation.
- **Evidence anchors**: Abstract and section claims about distillation; weak corpus support.
- **Break condition**: If target dataset distribution differs significantly from pre-training data, φ may not generalize well.

### Mechanism 3
- **Claim**: CIRCO's multiple ground truths per query enable more fine-grained evaluation by reducing false negatives.
- **Core assumption**: SEARLE-XL-annotated ground truths are valid matches, and the false negative rate is <10%.
- **Evidence anchors**: Abstract and section claims about multiple ground truths; no corpus support.
- **Break condition**: If SEARLE-XL has biases or blind spots, it may miss valid ground truths, underestimating true positives.

## Foundational Learning

- **Concept: CLIP (Contrastive Language-Image Pre-training)**
  - **Why needed here**: SEARLE relies on frozen pre-trained CLIP to map images and text into common embedding space for retrieval.
  - **Quick check question**: What is the purpose of the contrastive loss in CLIP's training objective, and how does it relate to the cosine similarity used in SEARLE's losses?

- **Concept: Textual Inversion**
  - **Why needed here**: SEARLE uses textual inversion to map visual features of reference images into pseudo-word tokens in CLIP's token embedding space.
  - **Quick check question**: How does the textual inversion process in SEARLE differ from that used in personalized text-to-image synthesis, and why is GPT-powered regularization important for CIR?

- **Concept: Knowledge Distillation**
  - **Why needed here**: SEARLE uses knowledge distillation to transfer knowledge from OTI-generated pseudo-word tokens to a lightweight neural network φ for efficient inference.
  - **Quick check question**: What is the difference between the distillation loss used in SEARLE (contrastive) and a simple mean squared error loss, and why is contrastive loss more effective?

## Architecture Onboarding

- **Component map**: CLIP image encoder → φ network → CLIP text encoder → retrieval
- **Critical path**: At inference, the critical path is: CLIP image encoder → φ network → CLIP text encoder → retrieval. The bottleneck is the φ network, which is optimized to be a single forward pass.
- **Design tradeoffs**: Using frozen CLIP avoids task-specific training data but limits flexibility; two-stage training (OTI + distillation) is more complex but enables leveraging unlabeled data and achieving efficient inference; GPT-powered regularization adds complexity but improves pseudo-word interaction with relative captions.
- **Failure signatures**: If φ fails to generalize to new domains, retrieved images may not match relative captions well; if GPT-generated phrases differ too much from relative captions, pseudo-words may not interact effectively; if pre-training dataset is too small or unrepresentative, pseudo-words may not capture visual content well.
- **First 3 experiments**:
  1. **Ablation of GPT-powered regularization**: Remove Lgpt from both OTI and φ training, compare retrieval performance on validation set.
  2. **Ablation of distillation**: Use OTI directly at inference time (SEARLE-OTI) and compare with φ-based method (SEARLE) on validation set.
  3. **Domain generalization**: Train φ on different dataset (e.g., ImageNet) and test on target CIR dataset (e.g., FashionIQ), compare with domain-specific OTI at inference.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does SEARLE's performance change when trained on datasets with different domain distributions?
- **Basis in paper**: [explicit] Mentions training on ImageNet1K, FashionIQ, and CIRR with performance variations based on domain similarity.
- **Why unresolved**: Paper provides some comparisons but doesn't deeply explore domain shift effects.
- **What evidence would resolve it**: Systematic study training SEARLE on various datasets with different domain distributions and comparing retrieval performance.

### Open Question 2
- **Question**: Can GPT-powered regularization be further optimized or replaced with more advanced language models?
- **Basis in paper**: [explicit] Uses GPT-Neo-2.7B model and notes its effectiveness without exploring alternatives.
- **Why unresolved**: Paper uses specific GPT model but doesn't explore more advanced language models.
- **What evidence would resolve it**: Experiments comparing SEARLE's performance using different language models for regularization.

### Open Question 3
- **Question**: How does pre-training dataset size and diversity impact φ's generalization ability?
- **Basis in paper**: [explicit] Mentions using ImageNet1K test split and discusses dataset size impact.
- **Why unresolved**: Paper touches on dataset size but doesn't thoroughly investigate diversity and size effects.
- **What evidence would resolve it**: Experiments varying pre-training dataset size and diversity and measuring impact on SEARLE's performance.

### Open Question 4
- **Question**: What is the impact of using different CLIP model architectures on SEARLE's performance?
- **Basis in paper**: [explicit] Uses CLIP ViT-B/32 and ViT-L/14 backbones and notes performance differences.
- **Why unresolved**: Paper doesn't explore other CLIP architectures or their potential impact.
- **What evidence would resolve it**: Comparative studies using SEARLE with various CLIP model architectures.

## Limitations
- GPT-powered regularization effectiveness depends on assumption that GPT-generated phrases generalize to real relative captions, which is not empirically validated across diverse caption styles.
- Knowledge distillation approach assumes pre-training dataset distribution adequately represents target domains, but only tested on fashion and object-centric datasets.
- Method's reliance on CLIP's frozen embeddings means it cannot adapt to domains where CLIP's visual understanding is limited.

## Confidence

- **Performance claims**: High confidence - well-supported quantitative results with established evaluation protocols
- **Mechanism explanations**: Medium confidence - general approach is clear but specific component contributions are not isolated through proper ablations
- **Dataset quality claims**: Low confidence - multiple ground truth assertion based on SEARLE-XL's own performance without independent verification

## Next Checks
1. **Ablation of GPT-powered regularization**: Remove GPT-based regularization loss from both OTI and φ training, then compare retrieval performance on FashionIQ and CIRR to isolate regularization's contribution.
2. **Domain generalization stress test**: Train φ on a dataset with visual characteristics very different from FashionIQ (e.g., medical imaging), then evaluate zero-shot transfer performance to reveal generalization limits.
3. **Alternative annotation validation**: Re-annotate a subset of CIRCO queries using human annotators or different retrieval model, then measure agreement with SEARLE-XL based annotations to validate multiple ground truth approach.