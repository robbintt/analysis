---
ver: rpa2
title: Leveraging Large Language Models for Topic Classification in the Domain of
  Public Affairs
arxiv_id: '2306.02864'
source_url: https://arxiv.org/abs/2306.02864
tags:
- classi
- public
- topic
- samples
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of classifying public affairs
  documents, which is crucial for transparency and informed decision-making. The authors
  collected a Spanish text corpus of over 33K documents annotated with 30 topics using
  a regex-powered tool.
---

# Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs

## Quick Facts
- arXiv ID: 2306.02864
- Source URL: https://arxiv.org/abs/2306.02864
- Reference count: 24
- Key outcome: RoBERTa-base with SVM classifiers achieved TPR over 80% for most topics in Spanish public affairs document classification.

## Executive Summary
This paper explores the use of Large Language Models (LLMs) for multi-label topic classification in Spanish public affairs documents. The authors collected a corpus of over 33K annotated documents with 30 topics and tested transformer-based models as feature extractors combined with different classifiers. Their experiments demonstrated that RoBERTa-base with SVM classifiers achieved the best performance, with True Positive Rates exceeding 80% for most topics, even with limited training data per class. The approach proved effective for domain-specific document processing while maintaining high accuracy across multiple topics.

## Method Summary
The authors employed transformer-based models (RoBERTa variants and GPT-2) as feature extractors, followed by classifiers (SVM, Random Forest, or Neural Network) to perform multi-label topic classification. They used a one-vs-all binary classification approach for each topic, where each classifier independently determines whether a document belongs to a specific topic. The study evaluated various transformer models and classifier combinations using K-fold cross-validation (5 folds) on a Spanish public affairs corpus of over 33K documents.

## Key Results
- RoBERTa-base with SVM classifiers achieved the highest performance with TPR over 80% for most topics
- SVM classifiers consistently outperformed neural networks and random forests, especially for topics with fewer than 2K samples
- The one-vs-all binary classification approach proved effective for handling multi-label classification with imbalanced classes
- Domain-specific fine-tuning (RoBERTalex) did not improve performance compared to the base RoBERTa model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RoBERTa-base with SVM classifiers provides strong multi-label classification accuracy despite class imbalance and limited samples
- Mechanism: The transformer backbone learns robust semantic embeddings while the SVM classifier maps these embeddings into topic-specific decision boundaries, compensating for limited samples
- Core assumption: Transformer embeddings preserve discriminative features for topic detection even with scarce training data
- Evidence anchors:
  - [abstract] states that RoBERTa-base with SVM classifiers achieved TPR over 80% for most topics
  - [section 4.1] reports that SVM performance consistently surpasses RoBERTa with NN or RF classifiers, even for topics with fewer than 2K samples

### Mechanism 2
- Claim: One-vs-all binary classification for each topic avoids complexity of multi-label classification while maintaining accuracy
- Mechanism: Decomposing multi-label classification into multiple independent binary detectors reduces model complexity and allows specialized classifiers per topic
- Core assumption: Topic labels are sufficiently independent to avoid significant label correlation interference
- Evidence anchors:
  - [abstract] describes the approach as breaking multi-label into binary detection tasks
  - [section 3] explains that individual topic detectors are trained in a one-vs-all setup, and their predictions are aggregated

### Mechanism 3
- Claim: Using domain-specific transformer models (e.g., RoBERTalex fine-tuned on legal text) improves performance on public affairs documents
- Mechanism: Fine-tuning transformer models on domain-specific corpora aligns embedding spaces with domain vocabulary and structure
- Core assumption: Public affairs documents share lexical and semantic characteristics with legal texts
- Evidence anchors:
  - [section 3] mentions RoBERTalex is a RoBERTa-base fine-tuned for the Spanish legal domain
  - [section 4.1] shows RoBERTalex underperforms RoBERTa-base in experiments, contradicting the assumption

## Foundational Learning

- Concept: Transformer architectures and attention mechanisms
  - Why needed here: Understanding how encoder-based models like RoBERTa process text and generate embeddings is essential for tuning backbone choices
  - Quick check question: What is the key difference between encoder-only and decoder-only transformer models in how they handle input text?

- Concept: Multi-label vs. multi-class classification
  - Why needed here: The dataset allows multiple topic labels per document, so the engineer must understand why one-vs-all binary classifiers are preferred over multi-class approaches
  - Quick check question: Why might one-vs-all binary classification be more robust than a single multi-label classifier in highly imbalanced datasets?

- Concept: Class imbalance and its impact on classifier performance
  - Why needed here: The dataset has many topics with few samples; knowing how imbalance affects TPR/TNR helps in model selection and evaluation
  - Quick check question: How does class imbalance typically affect True Positive Rate and True Negative Rate in binary classifiers?

## Architecture Onboarding

- Component map: Raw text → cleaning (remove short samples, non-Spanish, noise) → annotated corpus → RoBERTa-base encoder → [CLS] token embedding (768-dim) → SVM with RBF kernel (complexity=1) → binary prediction per topic → aggregation → multi-label document topic set

- Critical path: Text cleaning → embedding extraction → SVM training per topic → inference aggregation

- Design tradeoffs:
  - One-vs-all vs. multi-label: Simplicity and robustness vs. potential label correlation loss
  - SVM vs. NN/RF: Higher TPR with fewer samples vs. possibly better generalization with more data
  - [CLS] embedding vs. mean pooling: Semantic focus vs. holistic representation

- Failure signatures:
  - Consistently low TPR for low-sample topics → insufficient embedding discriminative power
  - Near-100% TNR with low TPR → classifier defaulting to negative class
  - High variance in cross-validation → overfitting or data leakage

- First 3 experiments:
  1. Train RoBERTa-base + SVM on top 5 most frequent topics, evaluate TPR/TNR with 5-fold CV
  2. Compare RoBERTa-base + SVM vs. RoBERTa-base + NN for mid-frequency topics (2K-5K samples)
  3. Test [CLS] token embedding vs. mean pooling for a subset of topics to assess embedding impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the bias towards healthcare-related topics observed in the dataset be mitigated to improve classification accuracy for non-healthcare topics?
- Basis in paper: [explicit] The authors note a significant imbalance in the dataset, with healthcare topics dominating due to the COVID-19 pandemic, which affects the classification performance for other topics
- Why unresolved: The paper identifies the issue but does not explore specific strategies for mitigating this bias in the context of topic classification
- What evidence would resolve it: Implementing and testing various techniques such as resampling, data augmentation, or class weighting, and measuring their impact on classification performance for non-healthcare topics

### Open Question 2
- Question: How does the performance of the proposed topic classification system generalize to other domains beyond public affairs, such as finance or healthcare?
- Basis in paper: [inferred] The paper focuses on public affairs documents, and while the methodology is described, its applicability to other domains is not explored
- Why unresolved: The authors do not provide experiments or results demonstrating the system's effectiveness on datasets from other domains
- What evidence would resolve it: Conducting experiments using the same methodology on datasets from other domains and comparing the results to those obtained in the public affairs domain

### Open Question 3
- Question: What is the impact of using more recent LLMs, such as GPT-4 or instruction-tuned models, on the performance of topic classification in the public affairs domain?
- Basis in paper: [explicit] The authors mention the potential of testing more recent LLMs, including multilingual and instruction-based models, as future work
- Why unresolved: The paper does not include experiments with these more recent models, leaving their potential impact on performance unknown
- What evidence would resolve it: Experimenting with newer LLM architectures and comparing their classification performance to the models used in the study

## Limitations

- The results depend heavily on the quality and representativeness of the Spanish public affairs corpus, which contains domain-specific vocabulary and potential annotation biases
- The observed superiority of RoBERTa-base with SVM classifiers is established only within this specific dataset and may not generalize to other languages or document types
- The failure of domain-specific fine-tuning (RoBERTalex) to outperform the base model suggests potential mismatches between assumed domain relevance and actual model performance

## Confidence

- High Confidence: The experimental methodology (K-fold cross-validation, one-vs-all binary classification approach) is sound and well-documented
- Medium Confidence: The claim that transformer embeddings preserve discriminative features for topic detection despite limited training data is supported but not extensively validated
- Low Confidence: The assumption that domain-specific fine-tuning (RoBERTalex) would improve performance on public affairs documents is contradicted by experimental results

## Next Checks

1. Test the RoBERTa-base + SVM approach on a held-out test set of public affairs documents to verify that the cross-validation performance generalizes to unseen data

2. Evaluate alternative embedding strategies (mean pooling vs. [CLS] token) across all topics to determine if embedding selection explains performance differences between classifier types

3. Conduct ablation studies removing the most frequent topics to assess whether the high TPR scores are driven by a few dominant classes rather than robust multi-topic detection