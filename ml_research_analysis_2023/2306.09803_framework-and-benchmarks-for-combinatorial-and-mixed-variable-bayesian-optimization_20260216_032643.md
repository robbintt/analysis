---
ver: rpa2
title: Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization
arxiv_id: '2306.09803'
source_url: https://arxiv.org/abs/2306.09803
tags:
- optimization
- acquisition
- function
- mcbo
- combinatorial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a modular framework for Mixed-variable and
  Combinatorial Bayesian Optimization (MCBO) to address the lack of systematic benchmarking
  and standardized evaluation in the field. Current MCBO papers often introduce non-diverse
  or non-standard benchmarks to evaluate their methods, impeding the proper assessment
  of different MCBO primitives and their combinations.
---

# Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization

## Quick Facts
- arXiv ID: 2306.09803
- Source URL: https://arxiv.org/abs/2306.09803
- Reference count: 40
- Key outcome: Introduces modular framework for MCBO with 47 novel algorithms benchmarked against existing solvers and non-BO methods across 10 tasks

## Executive Summary
This paper addresses the critical need for standardized benchmarking in Mixed-variable and Combinatorial Bayesian Optimization (MCBO) by introducing a modular framework that enables effortless combination of optimization primitives. The authors implement 47 novel MCBO algorithms and conduct over 4000 experiments across ten diverse tasks, revealing superior combinations of primitives that outperform existing approaches. The framework includes implementations of surrogate models, acquisition functions, optimizers, and trust region managers, along with a comprehensive set of synthetic and real-world benchmarks.

## Method Summary
The framework provides a high-level API for constructing MCBO algorithms by combining surrogate models, acquisition functions, and acquisition optimizers. It implements seven surrogate models including Gaussian Processes with various kernels, five acquisition functions (Expected Improvement, Probability of Improvement, Lower Confidence Bound, Thompson Sampling), and three acquisition optimizers (Local Search, Genetic Algorithm, Simulated Annealing). The framework also includes a trust region manager for constraining acquisition optimization in high-dimensional spaces. Experiments were conducted with 200 iterations per algorithm, 20 initial random points, 180 BO steps, and 10 random seeds per task across six combinatorial and four mixed-variable tasks.

## Key Results
- Trust region constraints consistently improve performance by constraining acquisition optimization in high-dimensional spaces
- Certain combinations of surrogate models and acquisition functions significantly outperform existing MCBO solvers
- Model fit quality shows strong correlation with BO performance across different surrogate models
- On two real-world tasks, a non-BO algorithm outperformed all BO solvers, highlighting areas for future development

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The modular framework enables effortless combination of Bayesian Optimization components, which directly addresses the lack of standardized evaluation and benchmarking in MCBO.
- Mechanism: By providing a high-level API that allows users to mix-and-match surrogate models, acquisition functions, and acquisition optimizers using a single line of code, the framework eliminates the significant implementation overhead that has previously prevented controlled assessments of different MCBO primitives and their combinations.
- Core assumption: The primitives are sufficiently decoupled that they can be combined in any valid configuration without causing conflicts or requiring extensive re-implementation.
- Evidence anchors: [abstract] "our proposed framework enables an effortless combination of Bayesian Optimization components" [section 4.3] "By specifying the IDs of the surrogate model, acquisition function, acquisition optimizer, and TR manager in the BoBuilder class constructor, the corresponding BO primitives are automatically retrieved"
- Break condition: If the primitives are not truly modular and require significant code changes when combined, the framework would not achieve the promised ease of implementation.

### Mechanism 2
- Claim: The trust region (TR) constraint improves performance of MCBO algorithms by constraining the acquisition optimization procedure in high-dimensional spaces.
- Mechanism: The TR creates a local search region around the current best solution, allowing the surrogate model to focus on a smaller, more relevant portion of the search space. This addresses the challenge that surrogates may struggle to accurately model the black box over the entire high-dimensional search space.
- Core assumption: The global optimum is likely to be found near previously identified good solutions, making local search around the current best solution an effective strategy.
- Evidence anchors: [section 4.3] "Trust region (TR) Finally, Fig. 2 (right) pleads for the use of a TR in combinatorial BO. Methods working with a dynamic TR to fit local models and constrain acquisition optimization provide consistently better suggestions compared to global approaches." [section 5.2] "This confirms and extends the findings of Wan et al. [2]."
- Break condition: If the true optimum is far from the initial good solutions, the TR approach would miss it by focusing too narrowly on the local region.

### Mechanism 3
- Claim: The comprehensive benchmarking suite enables systematic evaluation of MCBO methods across various optimization domains, dimensionalities, and difficulties.
- Mechanism: By providing a diverse set of synthetic and real-world benchmarks covering a broad spectrum of domain dimensionalities and optimization difficulties, the framework enables controlled comparisons between different MCBO algorithms and their components.
- Core assumption: The benchmarks are representative of real-world optimization challenges and provide sufficient diversity to evaluate the strengths and weaknesses of different approaches.
- Evidence anchors: [abstract] "Our library also includes implementations of a wide range of synthetic and real-world mixed-variable and combinatorial benchmarks, covering a broad spectrum of domain dimensionalities and optimization difficulties." [section 4.5] "The benchmarks encompass both synthetic and real-world tasks... These benchmarks include well-known optimization problems, such as the Ackley function [33] and the Pest Control problem [1], and optimization problems that extend the current application venues of BO"
- Break condition: If the benchmarks do not adequately represent the diversity of real-world problems or if they are too easy/hard compared to practical applications, the evaluation results may not generalize.

## Foundational Learning

- Concept: Gaussian Processes (GPs) as surrogate models
  - Why needed here: GPs are the most widely adopted surrogate models in MCBO due to their traceability, sample efficiency, and capacity to maintain calibrated uncertainties. Understanding GPs is essential for implementing and modifying the surrogate model components.
  - Quick check question: What are the two key functions that fully define a GP, and what does each capture about the modeled function?

- Concept: Acquisition functions and their optimization
  - Why needed here: Acquisition functions balance exploration and exploitation in the optimization process. Different acquisition functions (EI, PI, LCB, TS) have different properties and are suited to different optimization scenarios. Understanding these functions is crucial for implementing and comparing different MCBO algorithms.
  - Quick check question: How does the Expected Improvement (EI) acquisition function differ from the Probability of Improvement (PI) in terms of what they measure about potential query points?

- Concept: Trust regions and their implementation
  - Why needed here: Trust regions constrain the acquisition optimization to a local region around the current best solution, which is particularly important in high-dimensional combinatorial spaces. Understanding the trust region mechanism is essential for implementing and modifying acquisition optimizers.
  - Quick check question: How is the trust region defined for combinatorial variables versus numeric variables in the framework?

## Architecture Onboarding

- Component map: TaskBase -> SearchSpace -> ModelBase -> AcqBase -> AcqOptimizerBase -> BoBuilder -> OptimizerBase
- Critical path:
  1. Define a TaskBase subclass with get_search_space() and evaluate() methods
  2. Create a SearchSpace instance specifying the problem domain
  3. Use BoBuilder to combine surrogate model, acquisition function, and optimizer
  4. Run optimization loop with suggest() and observe() methods
- Design tradeoffs:
  - Modularity vs. performance: The framework prioritizes modularity, which may introduce some overhead compared to monolithic implementations
  - Generality vs. specialization: The framework supports many combinations but may not be optimized for specific problem types
  - Ease of use vs. flexibility: The high-level API makes common tasks easy but may obscure some implementation details
- Failure signatures:
  - Surrogate model fails to converge: Check kernel choice, hyperparameter optimization, and input normalization
  - Acquisition optimization gets stuck: Verify trust region settings and consider alternative optimizers
  - Poor performance across benchmarks: Evaluate model fit quality and consider domain-specific kernel choices
- First 3 experiments:
  1. Implement and run the Ackley-20D benchmark with a simple GP (overlap kernel) + EI + GA configuration to verify basic functionality
  2. Compare different surrogate models (GP (overlap), GP (transformed-overlap), GP (SSK)) on the same benchmark to understand their relative performance
  3. Test the trust region mechanism by running with and without TR constraints on a high-dimensional benchmark to observe its impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different surrogate models perform across diverse optimization tasks?
- Basis in paper: [explicit] The paper states that certain models are better suited for specific types of black-box functions and suggests investigating this in Section 5.3.
- Why unresolved: The paper mentions the potential variability in surrogate model performance but does not provide a comprehensive analysis of their performance across different tasks.
- What evidence would resolve it: A detailed comparative analysis of surrogate model performance on a wide range of optimization tasks, including both synthetic and real-world problems, would provide insights into their suitability for different types of functions.

### Open Question 2
- Question: What is the impact of trust region constraints on the performance of MCBO algorithms?
- Basis in paper: [explicit] The paper mentions the use of trust region constraints to constrain acquisition optimization and states that methods working with a dynamic trust region provide consistently better suggestions compared to global approaches.
- Why unresolved: While the paper highlights the positive impact of trust region constraints, it does not provide a comprehensive analysis of their effect on the performance of MCBO algorithms across different tasks.
- What evidence would resolve it: A systematic evaluation of MCBO algorithms with and without trust region constraints on a diverse set of optimization tasks would provide insights into the effectiveness of trust region constraints in improving performance.

### Open Question 3
- Question: How do the implemented MCBO algorithms compare to non-BO optimization methods on real-world tasks?
- Basis in paper: [explicit] The paper mentions that on two real-world tasks, a non-BO algorithm outperforms all BO solvers, highlighting the need for further development of combinatorial BO techniques for real-world tasks.
- Why unresolved: The paper provides limited information on the performance of MCBO algorithms compared to non-BO methods on real-world tasks, making it difficult to assess their relative effectiveness.
- What evidence would resolve it: A comprehensive comparison of MCBO algorithms and non-BO methods on a diverse set of real-world optimization tasks would provide insights into their relative performance and identify areas for improvement in MCBO algorithms.

## Limitations
- Framework modularity assumes primitives can be freely combined without conflicts, which may not hold for all combinations
- Benchmark suite, while diverse, may not fully represent all real-world optimization challenges
- Study focuses on sequential BO settings, potentially limiting applicability to batch BO scenarios

## Confidence

**Framework modularity and implementation: High** - Well-documented with clear API design and extensive code examples
**Benchmark representativeness: Medium** - Diverse but may not cover all practical optimization scenarios
**Trust region effectiveness: High** - Strong empirical evidence across multiple tasks and consistent with prior work
**Comparative analysis of MCBO primitives: Medium** - Comprehensive but limited by the specific implementations chosen

## Next Checks
1. Test the framework with user-defined MCBO primitives beyond those provided to validate true modularity
2. Apply the benchmarking suite to a real-world industrial optimization problem outside the tested domains
3. Compare performance with batch BO variants to assess the sequential BO focus limitation