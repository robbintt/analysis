---
ver: rpa2
title: 'Findings of Factify 2: Multimodal Fake News Detection'
arxiv_id: '2307.10475'
source_url: https://arxiv.org/abs/2307.10475
tags:
- news
- fake
- text
- multimodal
- factify
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper reports results from the Factify 2 shared task on multimodal
  fake news detection, which used a dataset pairing social media claims with supporting
  documents across five categories: support, no-evidence, refute (text and multimodal),
  and satirical. The task attracted over 60 participants, with 9 teams submitting
  final predictions.'
---

# Findings of Factify 2: Multimodal Fake News Detection

## Quick Facts
- arXiv ID: 2307.10475
- Source URL: https://arxiv.org/abs/2307.10475
- Reference count: 40
- Best model achieved 81.82% weighted F1 on multimodal fake news detection.

## Executive Summary
Factify 2 was a shared task on multimodal fake news detection using claim-document pairs with text and image inputs. Nine teams participated, achieving up to 81.82% weighted F1, significantly outperforming the baseline of 64.99%. Top models combined DeBERTa text encoders with SwinV2 or CLIP image encoders, using co-attention fusion. Despite strong performance on refute detection, the support-text category remained challenging for all systems.

## Method Summary
The task used 50,000 claim-document pairs from Twitter, fact-checking sites, and satirical news, each with text and image for claim and document. Five categories were defined: support (text and multimodal), insufficient evidence (text and multimodal), and refute. Models extracted features using pre-trained encoders (DeBERTa for text, SwinV2/CLIP for images), fused via co-attention, and sometimes enriched with auxiliary features (text length, OCR). Classification was performed using MLPs or ensembles, with weighted F1 as the primary metric.

## Key Results
- Top model (Triple-Check) achieved 81.82% weighted F1 using DeBERTa + SwinV2 with co-attention fusion.
- All participating teams outperformed the baseline (64.99% F1) by at least 6.3%.
- Support-text category was most challenging, with multimodal models sometimes underperforming unimodal baselines.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeBERTa-based text encoders combined with SwinV2/CLIP image encoders, fused via co-attention, enable the strongest multimodal representations for fake news detection.
- Mechanism: DeBERTa provides contextualized text embeddings that capture nuanced linguistic cues in claims and documents, while SwinV2 and CLIP encode hierarchical visual features. The co-attention layer aligns and weights cross-modal interactions, allowing the model to focus on the most discriminative joint cues (e.g., textual claims paired with supporting or refuting imagery).
- Core assumption: The strongest performance comes from combining high-capacity, pre-trained uni-modal encoders with a learnable fusion strategy rather than relying on single encoders or simple concatenation.
- Evidence anchors:
  - [abstract]: "The best performances came from the use of DeBERTa for text and Swinv2 and CLIP for image."
  - [section]: "Triple-Check propose a model with pre-trained DeBERTa for text and Swinv2 for image embeddings, that are combined using a co-attention fusion block."
  - [corpus]: Found 25 related papers; average neighbor FMR=0.32, average citations=0.0. Evidence is weak for citations, suggesting the dataset is relatively new but the community has active follow-up work.
- Break condition: If the claim-document pairs lack strong cross-modal alignment (e.g., unrelated images/text), co-attention cannot learn meaningful correlations and performance degrades sharply.

### Mechanism 2
- Claim: Multi-task feature engineering (e.g., text length, OCR, semantic similarity metrics) improves classification accuracy beyond raw embeddings alone.
- Mechanism: Auxiliary features capture metadata and surface-level cues that raw embeddings may miss, such as document length indicating depth of evidence or OCR output revealing image text content. These are concatenated or fed into the classifier to enrich decision boundaries.
- Core assumption: Domain-specific metadata is predictive of fake news class even when unimodal embeddings are strong.
- Evidence anchors:
  - [section]: "Features such as text length, OCR etc. are also used for the final classification."
  - [abstract]: "Top-performing approaches used DeBERTa for text and Swinv2 or CLIP for images..."
  - [corpus]: Weak citation evidence; assume the approach is under-explored in literature.
- Break condition: If the metadata features are noisy or irrelevant to the domain, they may introduce overfitting or distract the model.

### Mechanism 3
- Claim: Ensembling multiple model pipelines yields more robust predictions than any single architecture.
- Mechanism: Different models (e.g., DeBERTa+CLIP, DeBERTa+SwinV2, CLIP+DeiT) capture complementary representations; averaging or weighted voting reduces variance and mitigates individual model biases.
- Core assumption: Diversity in encoder architectures and training data leads to complementary error patterns that cancel out in ensemble settings.
- Evidence anchors:
  - [section]: "coco use an ensemble of two model pipelines."
  - [section]: "Some teams opted to use multiple embeddings to capture features."
  - [corpus]: No direct citation evidence, but ensemble methods are standard in shared tasks.
- Break condition: If all models are trained on similar data and have correlated errors, ensembling yields diminishing returns.

## Foundational Learning

- Concept: Multi-modal feature extraction
  - Why needed here: Fake news detection requires joint understanding of text and image cues; unimodal models miss cross-modal deception patterns.
  - Quick check question: How does CLIP differ from standard image encoders in handling text-image pairs?

- Concept: Co-attention fusion
  - Why needed here: Claims and documents may align at different semantic granularities; co-attention dynamically weights these interactions.
  - Quick check question: What happens to co-attention if the text and image are completely unrelated?

- Concept: Weighted average F1 evaluation
  - Why needed here: Imbalanced class distribution (e.g., more supports than refutes) necessitates class-aware scoring.
  - Quick check question: How does weighted F1 differ from macro F1 in this context?

## Architecture Onboarding

- Component map:
  1. Text encoder (DeBERTa, SBERT, Word2Vec, CLIP text branch)
  2. Image encoder (SwinV2, CLIP image branch, ResNet, ViT, DeiT)
  3. Co-attention fusion layer
  4. Feature engineering block (text length, OCR, semantic similarity)
  5. Classifier (MLP, Random Forest, ensemble voting)
  6. Evaluation pipeline (weighted average F1)

- Critical path:
  Pre-trained encoder → Co-attention fusion → Feature engineering → Classifier → Weighted F1 scoring

- Design tradeoffs:
  - Using large encoders (DeBERTa, SwinV2) boosts accuracy but increases memory/time; smaller encoders reduce compute but may miss subtle cues.
  - Co-attention vs. simple concatenation: co-attention is more expressive but needs more data to train; concatenation is faster but may underfit.
  - Ensembling improves robustness but multiplies inference cost.

- Failure signatures:
  - Overfitting to training domain if model assumes text-image pairs are always semantically aligned.
  - Performance collapse on satire category if model overfits to fact-checking style text.
  - Misclassification spikes when images are generic but claims are specific.

- First 3 experiments:
  1. Baseline: Concatenate DeBERTa text + CLIP image embeddings → MLP → weighted F1 (matches reported 64.99%).
  2. Co-attention fusion: DeBERTa + SwinV2 → co-attention → MLP → compare F1 gain.
  3. Feature enrichment: Add text length and OCR features to experiment 2; test if weighted F1 improves.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do multimodal approaches still struggle to outperform unimodal baselines in certain categories like Support_Text despite the potential for richer information fusion?
- Basis in paper: [explicit] The paper notes that despite multimodal approaches being used, Support_Text had the lowest scores among categories and all systems failed on certain samples
- Why unresolved: The paper observes this performance gap but doesn't deeply investigate the specific reasons why multimodal fusion fails to help in text-only support cases
- What evidence would resolve it: Detailed error analysis comparing multimodal vs unimodal performance on Support_Text samples, identifying whether fusion introduces noise or if certain textual patterns are being missed

### Open Question 2
- Question: What specific architectural improvements could help distinguish between claims and documents that share similar images but have no textual relationship (like the drone example in Figure 4)?
- Basis in paper: [explicit] The paper shows examples where all systems failed when claims and documents had similar images but were unrelated in content
- Why unresolved: The paper identifies this failure mode but doesn't propose or test specific architectural solutions to handle this ambiguity
- What evidence would resolve it: Comparative testing of models with different fusion strategies (e.g., stronger text-image disentanglement, cross-attention mechanisms) on similar image-but-unrelated-text cases

### Open Question 3
- Question: How can synthetic fake news data generation be optimized to better match real-world distributions and improve model generalization to harder cases?
- Basis in paper: [explicit] The conclusion suggests using synthetic fake news data that matches the general data distribution to add complexity to the refute category
- Why unresolved: While proposed as future work, the paper doesn't explore what characteristics make synthetic data effective or how to generate it
- What evidence would resolve it: Experiments comparing models trained with different synthetic data generation strategies against those trained only on real data, measuring performance on challenging cases

## Limitations
- Specific implementation details of top-performing co-attention fusion and ensemble strategies are not fully disclosed.
- Dataset is relatively new, so long-term robustness and generalizability are untested.
- Evaluation only reports weighted F1, without precision-recall breakdowns per class.

## Confidence
- High confidence: DeBERTa + SwinV2/CLIP + co-attention is effective for multimodal fake news detection; ensemble methods improve robustness; support-text category is hardest.
- Medium confidence: Auxiliary features (text length, OCR) meaningfully boost performance; the satire category is handled adequately but not optimally.
- Low confidence: Long-term generalizability of these models to other multimodal fake news datasets; whether the specific fusion or ensembling approaches are optimal or just effective in this domain.

## Next Checks
1. Replicate the top Triple-Check pipeline (DeBERTa + SwinV2 + co-attention) and compare weighted F1 to the reported 81.82%.
2. Conduct ablation studies removing co-attention or auxiliary features to quantify their individual contributions.
3. Test model performance on a held-out satire subset to measure robustness and identify overfitting risks.