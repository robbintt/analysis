---
ver: rpa2
title: 'CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No'
arxiv_id: '2308.12213'
source_url: https://arxiv.org/abs/2308.12213
tags:
- clip
- text
- clipn
- detection
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses zero-shot out-of-distribution (OOD) detection
  using CLIP-based models. It introduces CLIPN, a novel method that equips CLIP with
  "no" logic to distinguish OOD samples using negation-semantic prompts.
---

# CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No

## Quick Facts
- arXiv ID: 2308.12213
- Source URL: https://arxiv.org/abs/2308.12213
- Reference count: 40
- Achieves at least 2.34% and 11.64% improvements in AUROC and FPR95, respectively, for zero-shot OOD detection on ImageNet-1K

## Executive Summary
This paper addresses zero-shot out-of-distribution (OOD) detection using CLIP-based models by introducing CLIPN, a method that equips CLIP with "no" logic to distinguish OOD samples using negation-semantic prompts. The key innovation is the design of learnable "no" prompts and a dedicated "no" text encoder, trained with two novel loss functions that create semantically opposite feature spaces for standard and negation prompts. The proposed approach outperforms existing algorithms on 9 benchmark datasets, achieving significant improvements in OOD detection metrics.

## Method Summary
CLIPN modifies pre-trained CLIP by adding a learnable "no" text encoder and token embedding layer for negation-semantic prompts. The model is trained on vision-language pairs using two loss functions: image-text binary-opposite loss (which associates images with negation-semantic prompts) and text semantic-opposite loss (which ensures standard and negation prompts are embedded far apart). After training, two threshold-free inference algorithms—competing-to-win and agreeing-to-differ—are used for OOD detection. The method is evaluated on 9 benchmark datasets using pre-trained CLIP models (ViT-B-16 and ViT-L) and shows substantial improvements over existing OOD detection approaches.

## Key Results
- CLIPN achieves at least 2.34% and 11.64% improvements in AUROC and FPR95, respectively, for zero-shot OOD detection on ImageNet-1K
- Outperforms existing algorithms on 9 benchmark datasets including ImageNet-1K, CIFAR-100, and various OOD datasets (Texture, iNaturalist, SUN, Places365, CIFAR-10, LSUN, ImageNet-R)
- Demonstrates effectiveness of negation-semantic prompts in creating distinct feature spaces for OOD detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIPN's "no" text encoder learns to distinguish OOD samples by creating semantically opposite feature spaces for standard and negation prompts.
- Mechanism: By training with image-text binary-opposite loss, CLIPN associates images with negation-semantic prompts, creating a new feature space that separates OOD samples more clearly.
- Core assumption: Negation semantics in prompts can be effectively captured by a dedicated text encoder and used to identify OOD samples.
- Evidence anchors:
  - [abstract]: "Our key motivation is to equip CLIP with the capability of distinguishing OOD and ID samples using positive-semantic prompts and negation-semantic prompts."
  - [section]: "We further propose two loss functions. The first is image-text binary-opposite loss, which makes an image feature match with correct 'no' prompt features."
  - [corpus]: Weak evidence - only 25 related papers found, average FMR 0.463, no citations.
- Break condition: If negation semantics cannot be effectively captured by the "no" text encoder, or if the learned feature space does not provide clear separation between OOD and ID samples.

### Mechanism 2
- Claim: The text semantic-opposite loss ensures that standard and negation prompts are embedded far apart in the feature space, enhancing the distinction between ID and OOD samples.
- Mechanism: By minimizing the L2 distance between standard and negation prompt features, CLIPN ensures that the feature spaces for ID and OOD samples are distinct.
- Core assumption: Embedding standard and negation prompts far apart in the feature space improves OOD detection performance.
- Evidence anchors:
  - [abstract]: "we introduce two loss functions: the image-text binary-opposite loss and the text semantic-opposite loss, which we use to teach CLIPN to associate images with 'no' prompts, thereby enabling it to identify unknown samples."
  - [section]: "the text semantic-opposite loss which makes the standard prompt and 'no' prompts be embedded far away from each other."
  - [corpus]: Weak evidence - only 25 related papers found, average FMR 0.463, no citations.
- Break condition: If the distance between standard and negation prompt features does not correlate with improved OOD detection performance.

### Mechanism 3
- Claim: The threshold-free inference algorithms (competing-to-win and agreeing-to-differ) enable CLIPN to detect OOD samples without requiring pre-defined thresholds.
- Mechanism: Competing-to-win selects the most confident probability from standard and "no" text encoders, while agreeing-to-differ generates an additional probability for the OOD class by considering predictions from both encoders.
- Core assumption: Threshold-free inference algorithms can effectively detect OOD samples without requiring pre-defined thresholds.
- Evidence anchors:
  - [abstract]: "Furthermore, we propose two threshold-free inference algorithms to perform OOD detection by utilizing negation semantics from 'no' prompts and the text encoder."
  - [section]: "After the training of CLIPN, we design two threshold-free algorithms: competing-to-win and agreeing-to-differ."
  - [corpus]: Weak evidence - only 25 related papers found, average FMR 0.463, no citations.
- Break condition: If threshold-free inference algorithms fail to detect OOD samples accurately, or if pre-defined thresholds are required for reliable OOD detection.

## Foundational Learning

- Concept: Contrastive learning and its application in vision-language models
  - Why needed here: CLIPN is built upon CLIP, which uses contrastive learning to learn informative features from vision-language pairs.
  - Quick check question: How does contrastive learning in CLIP enable zero-shot learning and generalization to downstream tasks?

- Concept: Negation semantics and their representation in text
  - Why needed here: CLIPN uses negation-semantic prompts to create a new feature space for OOD detection.
  - Quick check question: How can negation semantics be effectively captured by a text encoder and used to identify OOD samples?

- Concept: Out-of-distribution detection and its challenges
  - Why needed here: CLIPN aims to improve OOD detection by addressing the challenge of hard-to-distinguish OOD samples.
  - Quick check question: What are the main challenges in OOD detection, and how does CLIPN address them?

## Architecture Onboarding

- Component map: Image → ϕ → f → ψ/ψno → g/gno → p/no_p → OOD detection
- Critical path: Image → frozen image encoder (ϕ) → feature extraction (f) → text encoders (ψ/ψno) → probability calculation (g/gno) → OOD detection
- Design tradeoffs:
  - Using learnable "no" prompts vs. handcrafted prompts
  - Choosing between competing-to-win and agreeing-to-differ inference algorithms
  - Balancing the contributions of ITBO and TSO losses
- Failure signatures:
  - Poor OOD detection performance
  - High false positive rate for ID samples
  - Inability to distinguish between hard-to-distinguish OOD samples
- First 3 experiments:
  1. Evaluate CLIPN's performance on a small-scale OOD detection task (e.g., CIFAR-100 as ID dataset, CIFAR-10 as OOD dataset)
  2. Compare CLIPN's performance with baseline methods (e.g., MSP, MaxLogit, Energy, ReAct, ODIN, ZOC, MCM) on a large-scale OOD detection task (e.g., ImageNet-1K as ID dataset)
  3. Analyze the impact of learnable "no" prompts vs. handcrafted prompts on OOD detection performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does CLIPN generalize to specialized domains like medical imaging or satellite imagery for OOD detection?
- Basis in paper: [explicit] The paper mentions that CLIPN's effectiveness in specialized datasets like medical and satellite images is underexplored.
- Why unresolved: The paper only tests CLIPN on general datasets (ImageNet, CIFAR, etc.) and acknowledges that CLIP's effectiveness in specialized domains is not well-studied.
- What evidence would resolve it: Experiments applying CLIPN to OOD detection in medical imaging datasets (e.g., ChestX-ray) or satellite imagery datasets (e.g., SAT-6) would demonstrate its generalizability to specialized domains.

### Open Question 2
- Question: Can CLIPN be extended to handle OOD detection tasks beyond image classification, such as object detection or semantic segmentation?
- Basis in paper: [explicit] The paper mentions that one limitation is the lack of clear demonstrations of CLIPN's extension to OOD segmentation or detection tasks.
- Why unresolved: The paper focuses solely on image classification OOD detection and does not explore how the CLIPN framework could be adapted for other vision tasks.
- What evidence would resolve it: Developing and evaluating CLIPN variants for OOD object detection (e.g., on COCO dataset) or semantic segmentation (e.g., on Cityscapes dataset) would show its potential for broader applications.

### Open Question 3
- Question: How does the performance of CLIPN compare to other state-of-the-art OOD detection methods when applied to very large-scale datasets?
- Basis in paper: [explicit] The paper demonstrates CLIPN's performance on ImageNet-1K, but does not test it on larger datasets like JFT-300M or Laion-5B.
- Why unresolved: While CLIPN shows strong performance on ImageNet-1K, it's unclear if it maintains this advantage when scaling up to even larger datasets.
- What evidence would resolve it: Conducting experiments comparing CLIPN to other methods on very large-scale datasets (e.g., JFT-300M, Laion-5B) would reveal its scalability and relative performance at scale.

## Limitations
- Weak evidence from related work corpus (only 25 related papers found, no citations)
- Limited evaluation to standard benchmark datasets without testing specialized domains
- Lack of ablation studies on the relative importance of the two loss functions

## Confidence

- **High Confidence**: The basic architecture and training framework of CLIPN are clearly described and reproducible.
- **Medium Confidence**: The empirical improvements over baseline methods are demonstrated, but the robustness of these improvements across different datasets and domains needs further validation.
- **Low Confidence**: The theoretical justification for why negation semantics specifically enable better OOD detection remains incomplete.

## Next Checks

1. **Ablation Study on Prompt Design**: Compare performance using only handcrafted prompts, only learnable prompts, and various combinations to quantify the contribution of learnable prompt embeddings to overall performance.

2. **Loss Function Analysis**: Conduct controlled experiments where each loss function (ITBO and TSO) is trained independently and in combination to determine their individual and synergistic effects on OOD detection performance.

3. **Cross-Domain Generalization**: Test CLIPN on OOD detection tasks outside the standard benchmark datasets (e.g., medical imaging, satellite imagery) to evaluate whether the negation-semantic approach generalizes to domains with different visual characteristics and semantic structures.