---
ver: rpa2
title: Image retrieval outperforms diffusion models on data augmentation
arxiv_id: '2304.10253'
source_url: https://arxiv.org/abs/2304.10253
tags:
- images
- diffusion
- data
- augmentation
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diffusion models show promise for data augmentation but have not
  yet surpassed simpler retrieval methods. While personalization techniques improve
  photorealism and reduce domain mismatch, retrieving images from the diffusion model's
  training data (via CLIP nearest neighbors) still outperforms all generative approaches
  on ImageNet classification tasks.
---

# Image retrieval outperforms diffusion models on data augmentation
## Quick Facts
- arXiv ID: 2304.10253
- Source URL: https://arxiv.org/abs/2304.10253
- Reference count: 40
- Key outcome: Retrieval-based augmentation from diffusion model's pre-training data outperforms generative diffusion methods on ImageNet classification tasks.

## Executive Summary
This paper evaluates diffusion models for data augmentation in image classification, finding that simple retrieval methods using the pre-training dataset (Laion) outperform generative approaches. While personalization techniques like fine-tuned conditioning vectors improve photorealism and domain alignment, retrieving nearest neighbors to CLIP-encoded class prompts still yields better classification accuracy. The results suggest diffusion models have not yet leveraged their generative potential for augmentation, as real images from pre-training data provide more effective diversity than synthesized samples.

## Method Summary
The study evaluates data augmentation methods on a 10% ImageNet subset using ResNet-50 classifiers. Augmentation techniques include simple prompt-based diffusion generation, CLIP-encoded prompts, learned conditioning vectors (FT CONDITIONING), and retrieval from the Laion dataset used to train Stable Diffusion v1.4. All methods aim to triple the training dataset size, with retrieval finding nearest neighbors in CLIP embedding space. Performance is measured by top-1 classification accuracy on ImageNet validation, comparing against baseline 10% subset performance.

## Key Results
- Retrieval-based augmentation from Laion dataset outperforms all generative diffusion methods on ImageNet classification
- Personalization techniques (FT CONDITIONING, IMAGIC, PSEUDOWORD +DM) improve over simple prompting but don't match retrieval performance
- Generated images show domain shift and artistic style issues compared to real training images

## Why This Works (Mechanism)
### Mechanism 1
Retrieval-based augmentation outperforms generative methods because retrieved images already contain real-world visual diversity that diffusion models struggle to replicate. The Laion dataset contains billions of filtered image-text pairs, and when retrieving nearest neighbors to simple CLIP-encoded prompts, the results are real-world examples matching ImageNet's visual domain without requiring synthesis of novel combinations. This works when pre-training data distribution overlaps sufficiently with downstream task distribution and CLIP embeddings provide meaningful semantic similarity for retrieval. Break condition: significant domain shift between pre-training and target data, or poor CLIP embedding performance for target classes.

### Mechanism 2
Personalization techniques improve performance by learning class-specific conditioning vectors that better reconstruct target domain images. Fine-tuning either conditioning vectors alone or both conditioning and denoising components enables the model to generate images matching the visual characteristics of the target dataset while maintaining photorealism. This assumes the diffusion model has sufficient capacity to adapt to new domains without catastrophic forgetting, and that reconstruction objectives provide useful adaptation gradients. Break condition: overfitting to small target datasets, or personalization degrading generation diversity.

### Mechanism 3
Simple prompt-based conditioning improves performance by providing diffusion models with clear semantic guidance. Conditioning with straightforward prompts like "A photo of [class name]" generates images capturing the general concept of each class, providing additional training samples that increase dataset size and diversity. This works when the diffusion model's pre-trained text encoder can interpret simple prompts effectively and generated images maintain sufficient quality and relevance. Break condition: significant class ambiguity in prompts (e.g., "papillon" generating butterflies instead of dogs), or domain shift between generated and target images.

## Foundational Learning
- **CLIP embeddings and semantic similarity**: Why needed - retrieval method relies on CLIP embeddings to find semantically similar images in pre-training dataset. Quick check - How does CLIP compute similarity between text prompts and images?
- **Diffusion model conditioning mechanisms**: Why needed - augmentation methods manipulate conditioning vectors in various ways (simple prompts, learned pseudo-words, fine-tuned vectors). Quick check - What are the components of a diffusion model's conditioning mechanism?
- **Domain adaptation and transfer learning**: Why needed - personalization techniques adapt diffusion model from pre-training domain to ImageNet domain. Quick check - What are key considerations when adapting pre-trained model to new domain?

## Architecture Onboarding
- **Component map**: Data preparation (subsampling ImageNet, generating prompts) → Augmentation generation (diffusion models, retrieval) → Classifier training (ResNet-50 on augmented data). Augmentation methods interface with diffusion model backbone and CLIP embedding space.
- **Critical path**: Retrieval: CLIP prompt encoding → nearest neighbor search in Laion dataset → filtering NSFW and low-aesthetic images → classifier training. Generative: prompt/template generation → diffusion model sampling → optional personalization fine-tuning → classifier training.
- **Design tradeoffs**: Retrieval is computationally cheaper but limited by pre-training data distribution; generative methods offer more flexibility but require more computation and may have domain shift issues. Personalization improves domain alignment but risks overfitting.
- **Failure signatures**: Poor performance may indicate class ambiguity in prompts, insufficient diversity in generated samples, domain shift between pre-training and target data, or overfitting during personalization. Retrieval failures may show up as missing classes or irrelevant images.
- **First 3 experiments**: 1) Verify CLIP embedding similarity by retrieving nearest neighbors for test prompts and visually inspecting relevance. 2) Test simple prompt-based generation with small subset of classes to check for class ambiguity issues. 3) Compare classifier performance with and without augmentation using single method to establish baseline improvement.

## Open Questions the Paper Calls Out
### Open Question 1
Can diffusion models outperform retrieval methods when generating out-of-distribution samples for data augmentation? The paper notes diffusion models could potentially improve over retrieval by generating more diverse and compositionally novel images, particularly out-of-distribution samples (e.g., "a photograph of an astronaut riding a horse"). This remains untested as the current study only evaluates in-distribution augmentation on ImageNet. Experiments comparing retrieval and diffusion-based augmentation on out-of-distribution datasets would provide clarity.

### Open Question 2
What are specific failure cases where data augmentation decreases classifier performance, and can these be systematically addressed? The paper observes that while both retrieval and diffusion-based augmentation improve performance for most classes, some classes still see decreases of up to 10% in accuracy. The authors suggest investigating these failure cases as a fruitful avenue for improvement, but don't analyze characteristics of harmed classes or propose solutions. Detailed analysis of failure cases to identify common patterns, followed by targeted improvements, would help resolve this.

### Open Question 3
How does performance of diffusion model-based augmentation scale with augmentation ratio? The authors note that while they compared all methods with same augmentation budget, diffusion models can theoretically generate arbitrarily many samples, and exploring their scaling behavior for higher augmentation ratios would be interesting. The study only tripled dataset size for all methods, so performance curve as more synthetic data is added remains unknown. Experiments varying augmented data amount (3x, 5x, 10x original dataset) and measuring classifier performance would resolve this.

## Limitations
- Study focuses exclusively on ImageNet classification, limiting generalizability to other vision tasks or datasets with different domain characteristics
- Performance comparisons assume access to same pre-training data (Laion) as Stable Diffusion, which may not hold for other diffusion models or tasks
- 10% subset evaluation may not capture full potential of augmentation methods when applied to smaller or more challenging datasets

## Confidence
- High confidence: Retrieval-based augmentation outperforms simple prompt conditioning and personalization techniques on specific ImageNet 10% subset task
- Medium confidence: Mechanism explanation (domain alignment via CLIP embeddings) adequately explains performance differences
- Medium confidence: Personalization techniques provide consistent improvements over simple prompting, though not reaching retrieval performance

## Next Checks
1. Test retrieval vs. generative augmentation performance on out-of-distribution datasets where Laion pre-training data may have less overlap with target classes
2. Evaluate computational efficiency trade-offs between retrieval (fast, limited by pre-training data) and generative methods (slower, more flexible) across different dataset sizes
3. Investigate whether combining retrieval and generative approaches (using retrieved images to guide generation) can surpass either method alone