---
ver: rpa2
title: Training Priors Predict Text-To-Image Model Performance
arxiv_id: '2306.01755'
source_url: https://arxiv.org/abs/2306.01755
tags:
- training
- examples
- alignment
- these
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether text-to-image models rely on training
  priors rather than compositional reasoning when generating novel relations. The
  authors analyze the Stable Diffusion 2.1 model by examining subject-verb-object
  (SVO) triads in prompts and measuring how well generated images align with these
  relations.
---

# Training Priors Predict Text-To-Image Model Performance

## Quick Facts
- arXiv ID: 2306.01755
- Source URL: https://arxiv.org/abs/2306.01755
- Authors: 
- Reference count: 33
- Key outcome: This paper investigates whether text-to-image models rely on training priors rather than compositional reasoning when generating novel relations. The authors analyze the Stable Diffusion 2.1 model by examining subject-verb-object (SVO) triads in prompts and measuring how well generated images align with these relations. They find that the frequency of an SVO triad in the training data positively correlates with alignment for that relation, but negatively correlates with alignment for the flipped (OVS) relation. This suggests the model is biased towards generating relations seen during training rather than constructing novel compositions. The results indicate that current text-to-image models primarily interpolate between seen relations rather than employing abstract compositional structure.

## Executive Summary
This study investigates whether text-to-image models like Stable Diffusion 2.1 rely on training data priors rather than compositional reasoning when generating novel relationships. The authors analyze how well the model can generate images aligned with subject-verb-object (SVO) triads by examining the frequency of these triads in the LAION training dataset. Their findings show that higher frequency of an SVO triad in training data improves alignment for that relation, while increased frequency of the flipped relation (OVS) actually decreases alignment for the original prompt. This suggests the model primarily interpolates between seen relations rather than constructing novel compositions from abstract structure.

## Method Summary
The researchers parsed the LAION dataset using SpaCy to extract SVO triads and estimate frequency counts for each triad and individual term roles (Sxx, xVx, xxO, Oxx, xxS). They then generated one image per prompt using Stable Diffusion 2.1 with a standardized prompt format ("A photograph of a {subject} {verb} a {object}"). Human raters evaluated alignment between each generated image and its corresponding prompt on a 0-1 scale, with 5 ratings per pair. The analysis used regression models controlling for SVO frequency, OVS frequency, and individual term frequencies to isolate the effects of training priors on generation quality.

## Key Results
- SVO triad frequency in training data positively correlates with alignment for that relation (coefficient 0.31, p < 0.05)
- OVS (flipped) triad frequency negatively correlates with alignment for the original SVO relation (coefficient -0.40, p < 0.05)
- Individual term role frequencies significantly impact alignment, with typical roles showing better performance than atypical assignments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increased frequency of a subject-verb-object (SVO) triad in training data improves the model's ability to generate images aligned with that relation.
- Mechanism: The model learns to map text prompts to image regions via frequency-based associations; higher training frequency strengthens these associations.
- Core assumption: Training data contains sufficient examples of the SVO triad for the model to form a reliable mapping.
- Evidence anchors:
  - [abstract] "the more often an SVO triad appears in the training data, the better the model can generate an image aligned with that triad"
  - [section 4.1.1] Regression shows SVO has a significant positive coefficient (0.31, p < 0.05) for alignment
  - [corpus] Weak: No corpus neighbor directly addresses frequency-based alignment; closest is "InfSplign" on spatial alignment
- Break condition: If the triad is too rare or the model has no clear association in training data.

### Mechanism 2
- Claim: High frequency of the flipped relation (OVS) degrades alignment for the original SVO relation.
- Mechanism: The model's learned mapping for SVO is overridden by the stronger, more frequent OVS mapping, causing confusion or misassociation.
- Core assumption: The flipped relation appears frequently enough in training data to interfere with the original relation.
- Evidence anchors:
  - [abstract] "this increased frequency also diminishes how well the model can generate an image aligned with the flipped triad"
  - [section 4.3.1] OVS coefficient is negative (-0.40, p < 0.05) for default examples
  - [corpus] Weak: No corpus neighbor directly addresses interference from flipped relations
- Break condition: If the flipped relation is infrequent or semantically very distinct.

### Mechanism 3
- Claim: Individual term roles (agent vs. patient) in training data influence how well the model can generate images for atypical role assignments.
- Mechanism: The model learns typical semantic roles for words (e.g., "horse" often as patient), and struggles when asked to generate them in atypical roles (e.g., as agent).
- Core assumption: Words have consistent role distributions across training data.
- Evidence anchors:
  - [section 4.1.1] Oxx and xxS have negative coefficients (-0.18, -0.23, p < 0.05), showing individual role frequency impacts alignment
  - [section 4.3.1] Oxx and xxS also negatively impact alignment for default examples
  - [corpus] Weak: No corpus neighbor directly addresses role-based frequency effects
- Break condition: If role distributions are balanced or atypical assignments are common in training data.

## Foundational Learning

- Concept: Subject-verb-object (SVO) structure
  - Why needed here: The paper analyzes text-to-image alignment by breaking prompts into SVO triads.
  - Quick check question: What are the subject, verb, and object in the sentence "dog chasing ball"?

- Concept: Frequency counts in training data
  - Why needed here: The study uses estimated counts of SVO triads to test hypotheses about model performance.
  - Quick check question: How does the frequency of a triad in training data relate to the model's ability to generate it?

- Concept: Regression analysis with control variables
  - Why needed here: The paper uses regression to isolate the effect of triad frequency while controlling for individual term frequencies.
  - Quick check question: Why do we include Sxx, xVx, and xxO in the regression model alongside SVO and OVS?

## Architecture Onboarding

- Component map:
  LAION dataset parsing -> SVO triad extraction -> Frequency counting -> Prompt formatting -> Image generation (Stable Diffusion 2.1) -> Human alignment rating

- Critical path:
  Parse LAION -> Extract and count SVO triads -> Filter for drawable prompts -> Generate images -> Collect alignment ratings -> Analyze regression results
  Bottleneck: Manual filtering for drawable prompts and parsing efficiency

- Design tradeoffs:
  Use of parsed SVO triads vs. raw text: parsing is noisy but allows structured analysis
  Single prompt per relation vs. multiple prompts: simpler but may miss variations in alignment
  Log-transformed frequency counts: stabilizes regression but may mask low-frequency effects

- Failure signatures:
  High SVO but low alignment: model may not have learned the mapping or the prompt is ambiguous
  High OVS but low alignment for flipped prompt: interference from flipped relation or role confusion
  Individual term role effects: model has strong priors for typical roles, struggles with atypical assignments

- First 3 experiments:
  1. Generate images for unseen SVO triads with varying OVS frequencies to test backward hypothesis
  2. Swap prompts and images in default/flipped pairs to see if frequency still drives alignment
  3. Analyze alignment for atypical role assignments (e.g., patient as subject) to test role-based frequency effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed training priors generalize across different text-to-image models or are they specific to Stable Diffusion 2.1?
- Basis in paper: [explicit] The authors acknowledge that their results "only truly weigh on the second part of this question" and note that "our results only reflect the behavior of one model (both DALLE2 and ImaGen appear to be more efficacious than stablediffusion 2.1)"
- Why unresolved: The study only tested Stable Diffusion 2.1, and while other models are mentioned, no comparative analysis was conducted.
- What evidence would resolve it: Systematic testing of the same experimental design across multiple text-to-image models to compare the strength and direction of training priors.

### Open Question 2
- Question: How do individual term frequencies interact with triad frequencies to influence generation quality?
- Basis in paper: [explicit] The authors note that "Individual effectsâ€“how common a term took a given role significantly impacted the results throughout our experiments" and found that "some words tend to have typical roles"
- Why unresolved: While individual term effects were observed, the paper didn't fully explore the interaction between individual term frequencies and triad frequencies.
- What evidence would resolve it: Controlled experiments varying individual term frequencies while holding triad frequencies constant, or vice versa.

### Open Question 3
- Question: Are there specific semantic or syntactic properties of relations that make them more or less susceptible to training priors?
- Basis in paper: [inferred] The authors mention that "Semantic similarity between infrequent and frequent relations may further explain some of the results" but don't explore this systematically.
- Why unresolved: The paper focused on frequency effects but didn't analyze the semantic or syntactic characteristics of the relations tested.
- What evidence would resolve it: Clustering relations by semantic/syntactic properties and testing whether certain clusters show stronger or weaker frequency effects.

### Open Question 4
- Question: Does the order of terms in a prompt (e.g., subject-verb-object vs. object-verb-subject) affect generation quality beyond just frequency effects?
- Basis in paper: [explicit] The authors tested flipped triads (OVS) and found that increased OVS count decreased alignment for the original prompt, but didn't systematically vary prompt structure.
- Why unresolved: The study only tested one prompt structure ("A photograph of a {subject} {verb} a {object}") and didn't explore how different orderings might affect results.
- What evidence would resolve it: Testing the same relations with different prompt structures (e.g., passive voice, different word orderings) to isolate the effect of prompt structure from frequency effects.

## Limitations
- The study relies on noisy SpaCy parsing of training data, which may miss or incorrectly identify SVO triads and bias frequency estimates
- Results are based on a single text-to-image model (Stable Diffusion 2.1), limiting generalizability across architectures
- Only one prompt variation per relation was tested, potentially missing effects of different prompting strategies

## Confidence
Confidence in the main claims is Medium:
- The positive correlation between SVO frequency and alignment is well-supported by regression analysis (coefficient 0.31, p < 0.05)
- The negative correlation with OVS frequency for flipped relations is statistically significant but weaker (coefficient -0.40, p < 0.05)
- Individual term role effects are present but show smaller effect sizes (coefficients -0.18 to -0.23)

## Next Checks
1. **Cross-model validation**: Test whether similar frequency priors appear in other text-to-image models (e.g., DALL-E 2, Midjourney) using the same evaluation protocol.

2. **Prompt variation study**: Generate multiple prompt variations for each relation (e.g., "A photo of a {subject} {verb} a {object}", "An image of a {subject} {verb} a {object}") to determine if prompting strategy affects the observed frequency biases.

3. **Temporal analysis**: Analyze how alignment scores change across different frequency ranges (low, medium, high) to determine if there are thresholds where training frequency effects plateau or reverse.