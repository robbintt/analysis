---
ver: rpa2
title: 'Curve Your Attention: Mixed-Curvature Transformers for Graph Representation
  Learning'
arxiv_id: '2309.04082'
source_url: https://arxiv.org/abs/2309.04082
tags:
- graph
- curvature
- fps-t
- attention
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper generalizes the Transformer architecture to non-Euclidean
  geometry by introducing a Fully Product-Stereographic Transformer (FPS-T) that operates
  on the product of constant-curvature spaces. The key innovation is using the stereographic
  model to represent Euclidean, hyperbolic, and spherical spaces uniformly, allowing
  learnable curvatures per attention head.
---

# Curve Your Attention: Mixed-Curvature Transformers for Graph Representation Learning

## Quick Facts
- arXiv ID: 2309.04082
- Source URL: https://arxiv.org/abs/2309.04082
- Reference count: 40
- Key outcome: Introduces FPS-T, a mixed-curvature Transformer that learns graph geometry through product-stereographic spaces with kernelized attention for linear complexity.

## Executive Summary
This paper proposes a novel Transformer architecture that operates on non-Euclidean geometry by learning mixed-curvature spaces for graph representation learning. The Fully Product-Stereographic Transformer (FPS-T) unifies Euclidean, hyperbolic, and spherical geometries through the stereographic model, allowing each attention head to adapt its curvature to the underlying graph structure. By combining this with tokenized graph Transformers and kernelized attention approximation, the model achieves both superior performance and computational efficiency on graph reconstruction and node classification tasks.

## Method Summary
The method tokenizes graphs into node and edge sequences with positional and type encodings, then processes them through FPS-T layers operating in product-stereographic spaces. Each attention head learns its own curvature (κh) from initialization at zero, enabling adaptation to hyperbolic (κh < 0), Euclidean (κh = 0), or spherical (κh > 0) geometries. The stereographic model provides differentiable geometric operations, while kernelization approximates attention scores as ϕ(Q)ϕ(K) for linear complexity. The architecture integrates with TokenGT's tokenization approach and uses Adam optimization with separate learning rates for curvatures in node classification tasks.

## Key Results
- FPS-T outperforms TokenGT baselines on graph reconstruction with 5.3-22.6% higher mAP across datasets
- Achieves comparable or better node classification F1 scores with fewer parameters than TokenGT
- Learns interpretable curvatures that align with expected graph geometries (negative for hierarchical, positive for cyclical structures)
- Kernelized approximation maintains performance while reducing complexity from O(n²) to O(n)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learnable curvatures per attention head allow the model to adapt to the underlying geometry of the input graph.
- Mechanism: Each attention head operates on a distinct stereographic model with curvature κh. By initializing all curvatures to zero (Euclidean) and allowing them to update during training, the model can move towards hyperbolic (κh < 0) or spherical (κh > 0) geometry depending on the graph structure.
- Core assumption: The stereographic model's geometric operations (distance, exponential map, etc.) are differentiable with respect to curvature κ, enabling gradient-based learning.
- Evidence anchors:
  - [abstract]: "Each layer chooses a set of curvatures that fits the input graph by changing the sign of the curvature κ in a differentiable manner."
  - [section 4.2]: "Because the metric tensor of the origin of the stereographic model is simply 4I... the Riemannian inner product becomes equivalent to the Euclidean inner product at the origin."
  - [corpus]: Weak evidence; no direct citations found. The idea of differentiable curvature learning is novel in this context.
- Break condition: If the curvature updates are too small or vanish due to vanishing gradients, the model cannot adapt its geometry effectively.

### Mechanism 2
- Claim: Linearizing the stereographic attention using kernelization reduces computational complexity from O(n²) to O(n) while preserving the geometric properties.
- Mechanism: The attention score ⟨Q_i, K_j⟩ is approximated as a product of kernel functions ϕ(Q_i)ϕ(K_j), where ϕ(X) = ELU(X) + 1. This approximation allows aggregation to be computed in linear time.
- Core assumption: The kernel approximation is sufficiently accurate for the model to maintain performance while gaining efficiency.
- Evidence anchors:
  - [section 4.5]: "By applying the kernelization to stereographic attention, we can rewrite the stereographic aggregation... as: ... where ϕ′(K)i = ϕ(K)i(λκVi − 1) and ˜Vi = λκVi / (λκVi − 1) Vi."
  - [corpus]: No direct evidence; this is a novel application of kernel linearization to non-Euclidean attention.
- Break condition: If the kernel approximation error is too large, the model's ability to capture long-range dependencies may degrade.

### Mechanism 3
- Claim: Combining FPS-T with tokenized graph Transformers allows the model to handle both node and edge tokens while leveraging global attention.
- Mechanism: The input graph is tokenized into a sequence X = [X_V, X_E] ∈ R(N+M)×d by treating each node and edge as an independent token, and augment the tokens with 1) node identifiers... and 2) type identifiers...
- Core assumption: Tokenization preserves the graph structure sufficiently for the attention mechanism to learn meaningful representations.
- Evidence anchors:
  - [section 4.4]: "We tokenize the graph into a sequence X = [X_V, X_E] ∈ R(N+M)×d by treating each node and edge as an independent token, and augment the tokens with 1) node identifiers... and 2) type identifiers..."
  - [section 5.1]: "TokenGT feeds this sequence into a pure Euclidean Transformer, an approach proven to pass the 2-dimensional Weisfeiler-Lehman (2-WL) graph isomorphism test..."
  - [corpus]: Weak evidence; the specific combination of tokenization with non-Euclidean geometry is novel.
- Break condition: If the tokenization process loses critical structural information, the model may fail to learn accurate graph representations.

## Foundational Learning

- Concept: Product-stereographic model
  - Why needed here: Provides a unified framework to represent Euclidean, hyperbolic, and spherical spaces with learnable curvatures, enabling the model to adapt to different graph geometries.
  - Quick check question: How does the stereographic model handle the transition between different curvature regimes (e.g., from Euclidean to hyperbolic)?

- Concept: Riemannian operations (distance, exponential map, logarithmic map, parallel transport)
  - Why needed here: These operations define how geometric computations are performed in non-Euclidean spaces, which is essential for implementing attention mechanisms and other neural network components.
  - Quick check question: What is the role of the conformal factor λκ_x in the stereographic model's metric tensor?

- Concept: Graph tokenization and positional encoding
  - Why needed here: Allows the model to process graphs using a sequence-based Transformer architecture while preserving structural information through positional encodings derived from Laplacian eigenvectors.
  - Quick check question: How do the node and edge tokens differ in their positional and type identifiers?

## Architecture Onboarding

- Component map: Input tokenization layer -> FPS-T layers (multiple) -> Output decoder
- Critical path:
  1. Tokenize input graph into sequence.
  2. Apply exponential map to place tokens in product-stereographic space of first layer.
  3. For each FPS-T layer:
     - Compute stereographic multi-head attention with kernelized approximation.
     - Apply feed-forward network.
     - Use residual connections and layer normalization.
     - Translate representations to next layer's stereographic space.
  4. Decode final representations for task-specific output.
- Design tradeoffs:
  - Learnable curvatures vs. fixed curvatures: Learnable curvatures allow adaptation but may increase training instability.
  - Kernelized attention vs. exact attention: Kernelization reduces complexity but may introduce approximation error.
  - Tokenization vs. direct graph processing: Tokenization enables use of Transformer architectures but may lose some structural information.
- Failure signatures:
  - Curvatures not updating: May indicate optimization issues or vanishing gradients.
  - Performance similar to TokenGT: May suggest that the model is not effectively leveraging non-Euclidean geometry.
  - High computational cost: May indicate that kernel approximation is not effective or that model is too large.
- First 3 experiments:
  1. Verify that curvatures learn appropriate values on synthetic graphs with known geometry (e.g., trees for hyperbolic, cycles for spherical).
  2. Compare performance of FPS-T with and without kernelized attention on small graphs to assess approximation quality.
  3. Test tokenization process on simple graphs to ensure structural information is preserved.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the product-stereographic model be extended to handle heterogeneous manifolds with input-dependent sectional curvatures?
- Basis in paper: The authors mention this as a potential future work direction, stating "For future work, we plan to extend towards heterogeneous manifolds [21] with input-dependent sectional curvatures as well as optimize Stereographic operations towards better stability and efficiency under machine precision."
- Why unresolved: This extension would require significant modifications to the current FPS-T architecture and training procedures to allow for dynamic curvature adjustments based on input features.
- What evidence would resolve it: A demonstration of improved performance on graph reconstruction and node classification tasks when using input-dependent curvatures compared to fixed curvatures.

### Open Question 2
- Question: Can the stereographic operations be optimized for better stability and efficiency under machine precision?
- Basis in paper: The authors mention this as a potential future work direction, stating "For future work, we plan to extend towards heterogeneous manifolds [21] with input-dependent sectional curvatures as well as optimize Stereographic operations towards better stability and efficiency under machine precision."
- Why unresolved: While the authors show that their model works well in practice, there may be numerical instabilities or inefficiencies when dealing with very large or very small curvatures, or when using high-dimensional representations.
- What evidence would resolve it: A comparison of the numerical stability and computational efficiency of FPS-T with and without optimized stereographic operations, using benchmarks with large graphs and high-dimensional embeddings.

### Open Question 3
- Question: How does the performance of FPS-T scale with graph size and density?
- Basis in paper: The authors provide runtime and memory usage comparisons for a few small to medium-sized graphs, but do not analyze how the model's performance (in terms of accuracy and efficiency) scales with graph size and density.
- Why unresolved: While the authors show that their kernelized attention approximation allows FPS-T to run in linear time with respect to the number of nodes and edges, it is unclear how this translates to practical performance gains on large-scale graphs.
- What evidence would resolve it: A thorough analysis of FPS-T's performance on graphs of varying sizes and densities, comparing runtime, memory usage, and accuracy against baseline models.

## Limitations
- Kernel approximation may introduce accuracy trade-offs that aren't fully quantified
- Connection between learned curvatures and actual graph structure needs more rigorous validation
- Tokenization approach with Laplacian positional encodings may not scale well to irregular graph structures

## Confidence
- Learnable curvatures effectively capture graph geometry: Medium
- Kernelized attention maintains performance while reducing complexity: Low
- Tokenization preserves sufficient structural information: Low

## Next Checks
1. Generate synthetic graphs with controlled curvature properties and systematically analyze whether learned curvatures converge to expected values, measuring correlation between ground-truth and learned curvature across different graph families.
2. Implement both exact stereographic attention and kernelized attention, then measure the discrepancy in attention weights and downstream task performance on small graphs where exact computation is feasible, quantifying how approximation error scales with graph size.
3. Design experiments that systematically vary graph properties (diameter, clustering coefficient, degree distribution) and measure how these changes affect tokenization quality and positional encoding effectiveness, comparing FPS-T performance when gradually removing structural information from the tokenization process.