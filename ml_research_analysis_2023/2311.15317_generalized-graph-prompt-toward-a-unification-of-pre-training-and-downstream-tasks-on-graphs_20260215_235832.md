---
ver: rpa2
title: 'Generalized Graph Prompt: Toward a Unification of Pre-Training and Downstream
  Tasks on Graphs'
arxiv_id: '2311.15317'
source_url: https://arxiv.org/abs/2311.15317
tags:
- graph
- node
- pre-training
- graphprompt
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GraphPrompt, a novel pre-training and prompting
  framework on graphs. GraphPrompt unifies pre-training and downstream tasks into
  a common task template, based on subgraph similarity, and employs a learnable prompt
  to assist a downstream task in locating the most relevant knowledge from the pre-trained
  model in a task-specific manner.
---

# Generalized Graph Prompt: Toward a Unification of Pre-Training and Downstream Tasks on Graphs

## Quick Facts
- arXiv ID: 2311.15317
- Source URL: https://arxiv.org/abs/2311.15317
- Reference count: 40
- This paper proposes GraphPrompt, a novel pre-training and prompting framework on graphs that unifies pre-training and downstream tasks into a common task template based on subgraph similarity.

## Executive Summary
GraphPrompt introduces a novel framework that unifies graph pre-training and downstream tasks by converting them to subgraph similarity learning. The framework employs learnable prompts to guide downstream tasks in exploiting pre-trained knowledge in a task-specific manner. GraphPrompt+ extends this with two major enhancements: generalizing pre-training tasks beyond link prediction and incorporating layer-wise prompt vectors to capitalize on hierarchical information. Extensive experiments on five public datasets demonstrate state-of-the-art performance across various downstream tasks.

## Method Summary
GraphPrompt converts both pre-training and downstream tasks into a common template based on subgraph similarity. It uses a pre-trained graph encoder (typically 3-layer GIN) to extract contextual subgraph embeddings, then applies a learnable prompt vector to the ReadOut layer for task-specific adaptation. The framework supports link prediction, node classification, and graph classification through this unified approach. GraphPrompt+ enhances this by supporting multiple contrastive pre-training tasks (like DGI and GraphCL) and applying a series of prompt vectors across all encoder layers to exploit hierarchical knowledge. Both frameworks use prompt tuning rather than fine-tuning, freezing the pre-trained encoder while updating only the prompts.

## Key Results
- GraphPrompt and GraphPrompt+ achieve state-of-the-art performance on five benchmark datasets
- The unified subgraph similarity framework successfully handles link prediction, node classification, and graph classification tasks
- GraphPrompt+ with layer-wise prompting shows improved performance over single-prompt GraphPrompt
- Few-shot learning capabilities demonstrated with 1-shot, 5-shot, 10-shot, and 30-shot experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GraphPrompt unifies pre-training and downstream tasks by converting them to subgraph similarity learning
- Mechanism: Both nodes and graphs are represented as subgraphs, with all tasks cast as computing cosine similarity between subgraph embeddings
- Core assumption: Subgraph similarity is a meaningful abstraction that preserves task-specific objectives
- Break condition: If subgraph embeddings fail to capture task-relevant information

### Mechanism 2
- Claim: Learnable prompts guide downstream tasks to exploit pre-trained knowledge in a task-specific manner
- Mechanism: A prompt vector is element-wise multiplied with node embeddings before ReadOut aggregation, reweighting features for task-specific relevance
- Core assumption: The pre-trained encoder contains generalizable hierarchical knowledge that can be selectively activated
- Break condition: If the prompt space is too constrained or pre-trained knowledge is not hierarchical

### Mechanism 3
- Claim: GraphPrompt+ generalizes the framework by supporting multiple contrastive pre-training tasks and layer-wise prompting
- Mechanism: Extends loss function to fit any contrastive task via subgraph similarity, and applies prompt vectors to each encoder layer
- Core assumption: Different layers encode distinct levels of abstraction that can be selectively emphasized per task
- Break condition: If layer-wise prompts overfit to limited supervision or encoder lacks meaningful hierarchical structure

## Foundational Learning

- Concept: Subgraph extraction and representation
  - Why needed here: The entire framework relies on representing nodes and graphs as subgraphs for similarity tasks
  - Quick check question: Given a node v and hop threshold δ=1, what nodes and edges are included in its contextual subgraph?

- Concept: Contrastive learning on graphs
  - Why needed here: GraphPrompt+ supports contrastive tasks like DGI and GraphCL, requiring understanding of positive/negative pair sampling
  - Quick check question: In DGI, how are positive and negative pairs defined for a target graph G?

- Concept: Prompt tuning vs. fine-tuning
  - Why needed here: GraphPrompt freezes the pre-trained encoder and only updates prompts, requiring understanding of parameter efficiency
  - Quick check question: Why does prompt tuning require fewer labeled samples than fine-tuning in few-shot settings?

## Architecture Onboarding

- Component map: Pre-trained graph encoder (GIN layers) -> Subgraph extraction module -> ReadOut layer (sum pooling + prompt multiplication) -> Loss computation -> Prompt vectors
- Critical path: 1) Pre-train encoder on link prediction or contrastive task, 2) For downstream task: extract subgraphs → apply layer-wise prompts → compute subgraph embeddings → calculate similarity to class prototypes → optimize prompts
- Design tradeoffs: Single prompt vs. layer-wise prompts (fewer parameters vs. richer hierarchical exploitation), link prediction vs. contrastive pre-training (simplicity vs. richer structural knowledge), prompt vector vs. prompt matrix (parameter efficiency vs. expressiveness)
- Failure signatures: Over-smoothing (deeper layer prompts degrade performance on small graphs), overfitting (layer-wise prompts require more supervision), subgraph irrelevance (contextual subgraphs fail to capture discriminative features)
- First 3 experiments: 1) Verify subgraph extraction produces expected node/edge sets for varying δ, 2) Test prompt tuning on simple node classification with 1-shot supervision, 3) Compare GraphPrompt vs. GraphPrompt+ on small dataset (e.g., ENZYMES) to observe layer-wise prompt impact

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- The framework's reliance on contextual subgraphs assumes local neighborhood structures sufficiently capture task-relevant information across diverse graph datasets
- Computational overhead of maintaining separate prompt vectors per layer may limit scalability to large graphs or deep architectures
- Performance in extreme few-shot scenarios (1-5 shots) remains unproven across diverse graph types

## Confidence

| Claim | Confidence |
|-------|------------|
| Using learnable prompts to adapt pre-trained models is well-established | High |
| Subgraph similarity unification is theoretically plausible | Medium |
| Layer-wise prompting assumes meaningful hierarchical knowledge | Low |

## Next Checks
1. Test GraphPrompt on datasets with known non-local dependencies (e.g., citation networks) to assess subgraph similarity limitations
2. Evaluate few-shot performance across a broader range of shot counts (1, 2, 3, 5, 10) to establish the framework's breaking point
3. Benchmark against established fine-tuning approaches to quantify the actual parameter efficiency gains of prompt tuning in this graph context