---
ver: rpa2
title: 'ADT: Agent-based Dynamic Thresholding for Anomaly Detection'
arxiv_id: '2312.01488'
source_url: https://arxiv.org/abs/2312.01488
tags:
- anomaly
- detection
- thresholding
- time
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of thresholding in anomaly detection,
  a critical yet understudied factor in detection effectiveness. The authors propose
  ADT, an agent-based dynamic thresholding framework that models thresholding as a
  Markov Decision Process and uses Deep Q-network to adaptively adjust thresholds
  between passive and active modes.
---

# ADT: Agent-based Dynamic Thresholding for Anomaly Detection

## Quick Facts
- arXiv ID: 2312.01488
- Source URL: https://arxiv.org/abs/2312.01488
- Reference count: 36
- Key outcome: ADT achieves F1 scores of 0.945-0.999 on three real-world datasets, significantly outperforming static thresholding methods

## Executive Summary
This paper addresses the critical yet understudied problem of dynamic thresholding in anomaly detection. The authors propose ADT, an agent-based framework that uses reinforcement learning to adaptively adjust detection thresholds between passive and active modes. ADT models thresholding as a Markov Decision Process and employs Deep Q-networks to learn optimal policies. Experiments demonstrate ADT's superior performance across three benchmark datasets, with F1 scores reaching up to 0.999 while maintaining data efficiency by requiring less than 1% of labeled data for training.

## Method Summary
ADT formulates anomaly detection thresholding as a Markov Decision Process where an agent learns to adjust binary thresholds based on anomaly scores generated by an autoencoder. The framework uses Deep Q-networks to learn policies mapping state representations (anomaly score statistics and recent detection outcomes) to threshold actions. ADT trains on a small fraction (<1%) of labeled data and employs experience replay and action stability mechanisms to ensure efficient learning. The method can be integrated with various anomaly detection systems that produce anomaly scores.

## Key Results
- ADT achieves F1 scores ranging from 0.945 to 0.999 across Yahoo, SWaT, and WADI datasets
- The method demonstrates high precision and recall while maintaining stability across different data subsets
- ADT significantly outperforms static thresholding baselines and requires minimal training data (<1% of benchmark datasets)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic thresholding based on passive/active mode adaptation outperforms static thresholds
- Mechanism: ADT uses a binary threshold that switches between passive mode (δ=1) and active mode (δ=0) based on learned policy
- Core assumption: Binary thresholding captures sufficient adaptive behavior across diverse anomaly types
- Evidence anchors: Abstract states ADT "can adjust thresholds adaptively by utilizing the anomaly scores" and significantly improves performance

### Mechanism 2
- Claim: Reinforcement learning optimizes thresholding through MDP modeling
- Mechanism: ADT uses Deep Q-network to learn optimal policy mapping states (μ, σ, TP/FP/FN percentages) to threshold actions
- Core assumption: State representation captures sufficient information for effective thresholding
- Evidence anchors: Abstract mentions MDP formulation; section 3.3.1 defines state as {μ_t, σ_t, ρTP_t, ρTN_t, ρFP_t, ρFN_t}

### Mechanism 3
- Claim: Data-efficient training enables effective adaptation with minimal labeled data
- Mechanism: ADT trains using <1% of dataset with experience replay and action stability every l steps
- Core assumption: Small fraction of labeled data sufficient to learn robust thresholding policies
- Evidence anchors: Abstract states ADT is "lightweight and data-efficient" requiring "less than 1% of the benchmarked dataset"

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: Enables sequential decision-making where agent learns to choose thresholds based on historical detection outcomes
  - Quick check question: What are the four components of an MDP tuple (S, A, T, R) used in ADT?

- Concept: Deep Q-network (DQN)
  - Why needed here: Approximates Q-function to handle high-dimensional continuous inputs and learn optimal threshold policies
  - Quick check question: How does DQN use experience replay and a target network to stabilize training?

- Concept: Autoencoder for anomaly scoring
  - Why needed here: Generates reconstruction errors that serve as anomaly scores for threshold adjustment decisions
  - Quick check question: Why does training autoencoder only on normal data help in anomaly detection?

## Architecture Onboarding

- Component map: Data preprocessing → Sliding window extraction → Autoencoder training → ADT training → Online detection
- Critical path: 1) Normalize and window time series data 2) Train AE on normal data only 3) Use AE to generate anomaly scores 4) Train ADT on small labeled subset 5) Deploy ADT + AE for online detection
- Design tradeoffs: Binary vs. continuous thresholding (simplicity vs. granularity), state size k (history vs. information dilution), action stability l (efficiency vs. adaptation speed)
- Failure signatures: High variance in precision/recall across subsets, ADT performance close to static thresholding, ADT overfitting to training subset
- First 3 experiments: 1) Compare ADT F1 vs. static thresholding on Yahoo dataset with varying k values 2) Evaluate ADT training stability with different l values on SWaT 3) Test ADT robustness by measuring precision/recall variance across 10 random data splits

## Open Questions the Paper Calls Out
1. How does ADT performance change with isolated point anomalies vs. contiguous segments? (Paper notes detection may be compromised for isolated point anomalies but lacks experimental analysis)
2. How does anomaly ratio in training set affect ADT performance? (Paper states this effect is "beyond the scope" but "worth studying")
3. How would ADT perform with continuous thresholding instead of binary approach? (Paper plans to explore this as future work)

## Limitations
- Binary thresholding may be too restrictive for complex anomaly scenarios where continuous thresholds could provide better discrimination
- State representation may not fully account for temporal dependencies or contextual information that could improve detection accuracy
- Implementation details for autoencoder and Q-network architectures are not specified, limiting reproducibility

## Confidence
- High Confidence: ADT significantly outperforms static thresholding methods on benchmark datasets (F1 scores 0.945-0.999)
- Medium Confidence: Data efficiency claim (<1% training data) based on experimental setup, but generalizability to other domains uncertain
- Medium Confidence: MDP formulation appropriately models thresholding problem, though alternative formulations might yield better results

## Next Checks
1. Test ADT on datasets with varying anomaly types and severities to verify binary threshold sufficiency across diverse scenarios
2. Compare ADT performance when using continuous vs. binary thresholding to quantify tradeoff between simplicity and discrimination power
3. Evaluate ADT robustness to concept drift by simulating gradual changes in normal behavior patterns and measuring adaptation speed