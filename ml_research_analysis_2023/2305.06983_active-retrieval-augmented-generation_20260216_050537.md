---
ver: rpa2
title: Active Retrieval Augmented Generation
arxiv_id: '2305.06983'
source_url: https://arxiv.org/abs/2305.06983
tags:
- question
- answer
- retrieval
- generation
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FLARE, a forward-looking active retrieval
  augmented generation method that iteratively generates temporary next sentences,
  uses them as queries to retrieve relevant documents when low-confidence tokens are
  detected, and regenerates the sentence with retrieved information. Tested on four
  long-form knowledge-intensive tasks (multihop QA, commonsense reasoning, long-form
  QA, and open-domain summarization), FLARE outperforms single-time retrieval and
  multi-time retrieval baselines, achieving superior or competitive performance across
  all tasks.
---

# Active Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2305.06983
- Source URL: https://arxiv.org/abs/2305.06983
- Reference count: 28
- Key outcome: FLARE outperforms single-time and multi-time retrieval baselines across four long-form knowledge-intensive tasks by actively deciding when and what to retrieve based on anticipated future content

## Executive Summary
FLARE introduces a forward-looking active retrieval augmented generation method that iteratively generates temporary next sentences, uses them as queries to retrieve relevant documents when low-confidence tokens are detected, and regenerates the sentence with retrieved information. Tested on multihop QA, commonsense reasoning, long-form QA, and open-domain summarization tasks, FLARE demonstrates superior or competitive performance compared to passive retrieval approaches. The method shows that anticipating future content needs through temporary sentence generation is more effective than relying on past context or fixed retrieval intervals.

## Method Summary
FLARE is a forward-looking active retrieval augmented generation approach that generates a temporary next sentence, uses it as a query to retrieve relevant documents when low-confidence tokens are detected, and regenerates the sentence with retrieved information. The method operates by splitting generated output into sentences, checking token confidence scores against a threshold θ, and triggering retrieval when low-confidence tokens are present. Three query formulation methods are employed: direct use of the temporary sentence, masking low-confidence tokens with a threshold β, or generating explicit questions targeting uncertain spans. This iterative process continues until the entire output is generated, with retrieval decisions made dynamically based on the LM's confidence in its temporary predictions.

## Key Results
- FLARE outperforms single-time retrieval and multi-time retrieval baselines across all four tested tasks
- Forward-looking retrieval using next sentences is more effective than backward-looking approaches using previous context
- Confidence-based active retrieval reduces unnecessary retrievals and improves efficiency compared to fixed-interval approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FLARE's forward-looking retrieval outperforms passive multi-time retrieval baselines
- Mechanism: By generating a temporary next sentence without retrieval and using it as a query, FLARE anticipates future content needs rather than relying on past context
- Core assumption: The next sentence generated by the LM contains sufficient semantic content to guide effective retrieval
- Evidence anchors:
  - [abstract]: "FLARE outperforms single-time retrieval and multi-time retrieval baselines, achieving superior or competitive performance across all tasks"
  - [section]: "on both datasets, using the next sentence to retrieve is clearly better than using the previous sentence"
  - [corpus]: Weak evidence - corpus contains related papers but no direct experimental comparison of forward-looking vs backward-looking retrieval

### Mechanism 2
- Claim: Confidence-based active retrieval reduces unnecessary retrievals and improves efficiency
- Mechanism: Only retrieves when the temporary next sentence contains tokens with probability below threshold θ, avoiding retrieval when LM is confident
- Core assumption: Low probability tokens in LM output indicate knowledge gaps that require external retrieval
- Evidence anchors:
  - [abstract]: "iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens"
  - [section]: "Since LMs tend to be well-calibrated that low probability/conﬁdence often indicates a lack of knowledge"
  - [corpus]: Weak evidence - while the claim is stated, corpus doesn't provide direct experimental validation of confidence calibration

### Mechanism 3
- Claim: Masked and question-generated queries reduce error propagation in retrieval
- Mechanism: Masks low-confidence tokens or generates explicit questions targeting uncertain spans to avoid using erroneous temporary sentences directly as queries
- Core assumption: Low-confidence tokens in temporary sentences are likely to be errors that would mislead retrievers
- Evidence anchors:
  - [section]: "If the LM produces the sentence 'Joe Biden attended the University of Pennsylvania' instead of the correct fact that he attended the University of Delaware, using this erroneous sentence as a query could prompt the retriever to retrieve irrelevant information"
  - [section]: "We propose two simple methods to overcome this issue"
  - [corpus]: No direct evidence in corpus - this appears to be a novel methodological contribution not yet validated in related work

## Foundational Learning

- Concept: Retrieval-augmented generation pipeline
  - Why needed here: Understanding how FLARE fits into the broader RAG ecosystem and differs from passive approaches
  - Quick check question: What distinguishes active retrieval from passive multi-time retrieval in terms of when/what to retrieve?

- Concept: Language model confidence calibration
  - Why needed here: FLARE's core mechanism relies on using LM probability scores to decide when to retrieve
  - Quick check question: How does FLARE determine whether a token has "low confidence" and what threshold is used?

- Concept: Query formulation strategies
  - Why needed here: FLARE offers multiple ways to formulate queries (direct, masked, question generation)
  - Quick check question: What are the three query formulation methods in FLARE and how do they differ in handling uncertainty?

## Architecture Onboarding

- Component map:
  - User input → Initial retrieval → Temporary sentence generation → Confidence check → Query formulation → Document retrieval → Sentence regeneration → Output
  - Key components: Retriever (BM25/Bing), Language model (text-davinci-003), Sentence tokenizer, Confidence thresholder

- Critical path: User input → Temporary sentence generation → Confidence check → Query formulation → Retrieval → Regeneration
  - This is the active retrieval loop that distinguishes FLARE from passive approaches

- Design tradeoffs:
  - Retrieval frequency vs. generation efficiency: More retrievals improve accuracy but increase cost and latency
  - Query quality vs. error propagation: Direct queries are simple but may propagate errors; masked/question queries are safer but more complex
  - Threshold selection: θ and β parameters require task-specific tuning

- Failure signatures:
  - Over-retrieval: Low θ threshold causing retrieval on confident sentences, introducing noise
  - Under-retrieval: High θ threshold missing knowledge gaps, producing hallucinations
  - Query drift: Temporary sentences that don't accurately represent next content, leading to irrelevant retrievals
  - Error propagation: Using erroneous temporary sentences directly as queries without masking/question generation

- First 3 experiments:
  1. Baseline comparison: Run FLARE vs single-time retrieval on 2WikiMultihopQA with identical exemplars and retriever
  2. Threshold sensitivity: Vary θ from 0.1 to 0.9 on StrategyQA to find optimal retrieval percentage
  3. Query formulation ablation: Compare direct, masked (β=0.2), and question generation methods on ASQA-hint using same confidence threshold

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FLARE perform on knowledge-intensive dialogue generation tasks like Wizard of Wikipedia where the output is relatively short?
- Basis in paper: explicit
- Why unresolved: The paper mentions preliminary experiments on Wizard of Wikipedia showed no significant gains, but does not provide detailed results or analysis.
- What evidence would resolve it: Comprehensive experiments comparing FLARE performance to baselines on Wizard of Wikipedia dataset with detailed metrics and analysis.

### Open Question 2
- Question: What architectural modifications to language models could enable more efficient active retrieval augmentation?
- Basis in paper: explicit
- Why unresolved: The paper mentions this as a limitation but does not explore or propose specific architectural solutions.
- What evidence would resolve it: Proposed model architectures that encode retrieved documents and generation context independently, along with experimental validation of their efficiency.

### Open Question 3
- Question: How does the performance of FLARE vary with different masking thresholds (β) across different types of tasks?
- Basis in paper: explicit
- Why unresolved: The paper shows results for one dataset but doesn't systematically analyze how optimal β varies across task types.
- What evidence would resolve it: A study showing FLARE performance across multiple tasks with varying β values to identify patterns in optimal threshold selection.

### Open Question 4
- Question: What is the relationship between the frequency of retrieval triggers and generation quality across different domains?
- Basis in paper: explicit
- Why unresolved: The paper shows some analysis on 2WikiMultihopQA and StrategyQA but doesn't provide a comprehensive analysis across all tested domains.
- What evidence would resolve it: A systematic study measuring generation quality metrics at different retrieval frequencies for each task/domain in the evaluation suite.

### Open Question 5
- Question: How does FLARE compare to single-time retrieval when the initial retrieval already contains most of the necessary information?
- Basis in paper: inferred
- Why unresolved: The paper focuses on scenarios where additional retrieval is beneficial but doesn't explore edge cases where initial retrieval is sufficient.
- What evidence would resolve it: Controlled experiments comparing FLARE and single-time retrieval on tasks where initial retrieval is intentionally made comprehensive.

## Limitations

- Limited empirical validation of error propagation claims - while masked and question generation methods are proposed to reduce error propagation, the paper provides limited experimental evidence for their effectiveness
- Task-specific threshold tuning - FLARE's performance depends on optimal selection of confidence threshold θ and masking threshold β, which may require task-specific tuning rather than generalizing across domains
- Lack of analysis on initial retrieval sufficiency - the paper doesn't explore scenarios where initial retrieval already contains sufficient information, potentially limiting FLARE's advantage in well-retrieved domains

## Confidence

**High Confidence (3 claims)**:
1. FLARE's forward-looking retrieval approach outperforms passive single-time retrieval baselines across all four tested tasks
2. The active retrieval mechanism, which uses temporary next sentences as queries, is fundamentally different from passive approaches that use past context or fixed intervals
3. FLARE demonstrates competitive or superior performance compared to multiple strong baselines including previous-window, previous-sentence, and question decomposition approaches

**Medium Confidence (2 claims)**:
1. The confidence threshold θ effectively identifies knowledge gaps requiring retrieval (based on LM calibration claims but limited experimental validation)
2. Masked and question generation query formulation methods reduce error propagation compared to direct queries (conceptually sound but empirically limited evidence)

**Low Confidence (1 claim)**:
1. FLARE's threshold settings (θ and β) generalize across all four task types without task-specific tuning (insufficient sensitivity analysis provided)

## Next Checks

**Validation Check 1: Threshold Sensitivity Analysis**
Run FLARE on StrategyQA with θ values ranging from 0.1 to 0.9 in increments of 0.1, measuring both performance metrics and percentage of sentences requiring retrieval. This will reveal whether there's a narrow optimal range or robust performance across thresholds, and whether the default settings are truly generalizable or task-specific.

**Validation Check 2: Error Propagation Comparison**
Implement an ablation study comparing direct query, masked query (β=0.2), and question generation methods on ASQA-hint, measuring not just end-task performance but also intermediate metrics like query quality (e.g., semantic similarity between temporary sentence and retrieved documents) and generation accuracy when retrieval is triggered versus when it's not.

**Validation Check 3: Corpus Integration and Citation Validation**
Systematically search for prior work on forward-looking retrieval, confidence-based active retrieval, and error-aware query formulation in the RAG literature. Verify whether the paper's claims of novelty are accurate and whether the methodological innovations have been previously explored, even if not under the same terminology or framework.