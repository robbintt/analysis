---
ver: rpa2
title: The Sample Complexity Of ERMs In Stochastic Convex Optimization
arxiv_id: '2311.05398'
source_url: https://arxiv.org/abs/2311.05398
tags:
- convex
- complexity
- then
- learning
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper resolves a fundamental open question in stochastic convex
  optimization by determining the exact sample complexity of empirical risk minimizers
  (ERMs). The authors prove that ERMs require only $\tilde{O}(\frac{d}{\varepsilon}
  + \frac{1}{\varepsilon^2})$ data points to achieve $\varepsilon$-accuracy, matching
  the known lower bound and closing the gap that had remained open since Feldman's
  2016 work.
---

# The Sample Complexity Of ERMs In Stochastic Convex Optimization

## Quick Facts
- arXiv ID: 2311.05398
- Source URL: https://arxiv.org/abs/2311.05398
- Reference count: 21
- Primary result: Proves ERMs require only $\tilde{O}(\frac{d}{\varepsilon} + \frac{1}{\varepsilon^2})$ samples to achieve $\varepsilon$-accuracy in stochastic convex optimization, closing the gap since Feldman's 2016 work

## Executive Summary
This paper resolves a fundamental open question in stochastic convex optimization by determining the exact sample complexity of empirical risk minimizers (ERMs). The authors prove that ERMs require only $\tilde{O}(\frac{d}{\varepsilon} + \frac{1}{\varepsilon^2})$ data points to achieve $\varepsilon$-accuracy, matching the known lower bound and closing the gap that had remained open since Feldman's 2016 work. This establishes a new separation between ERMs and uniform convergence, showing that learning in SCO is strictly easier than uniform convergence would suggest.

The key technical innovation involves using the Bregman divergence to control the behavior of stochastic convex optimization problems, particularly through concentration bounds on the empirical Bregman divergence. The proof introduces a mechanism for handling both cases where the optimal point is in the interior or on the boundary of the domain, using the non-negativity and boundedness of the Bregman divergence to achieve the improved sample complexity.

## Method Summary
The method uses Bregman divergence to create a bridge between empirical and population losses that can be controlled through concentration inequalities. For any point x, the Bregman divergence D(x,x*) between empirical loss F̂ and population loss F is non-negative and bounded, allowing Bernstein's inequality to show that if D(x,x*) is large (meaning x is far from optimal), then the empirical divergence D̂(x,x*) is likely to be small with high probability. This creates a contradiction for spurious ERMs - if x is an empirical minimizer but far from optimal, the divergence behavior makes this unlikely. The proof combines this with first-order optimality conditions and covering arguments to establish the sample complexity bound.

## Key Results
- Establishes exact sample complexity of ERMs as $\tilde{O}(\frac{d}{\varepsilon} + \frac{1}{\varepsilon^2})$
- Shows separation between ERMs and uniform convergence, proving SCO is strictly easier than uniform convergence suggests
- Generalizes result to arbitrary symmetric convex bodies using truncated Bregman divergence
- Introduces new mechanism for controlling stochastic convex optimization problems using Bregman divergences

## Why This Works (Mechanism)

### Mechanism 1: Bregman Divergence Concentration
- **Claim**: The Bregman divergence provides a non-negative bridge between empirical and population losses that can be controlled through concentration inequalities.
- **Mechanism**: The proof uses the fact that for any point x, the Bregman divergence D(x,x*) between the empirical loss F̂ and population loss F is non-negative and bounded. This allows applying Bernstein's inequality to show that if D(x,x*) is large (meaning x is a bad point far from optimal), then the empirical divergence D̂(x,x*) is likely to be small with high probability. This creates a contradiction for spurious ERMs - if x is an empirical minimizer but far from optimal, the divergence behavior makes this unlikely.
- **Core assumption**: The functions f_z are convex, L-Lipschitz, and bounded, which ensures the Bregman divergence is well-behaved and concentrated.
- **Evidence anchors**:
  - [abstract] "The key technical innovation involves using the Bregman divergence to control the behavior of stochastic convex optimization problems, particularly through concentration bounds on the empirical Bregman divergence."
  - [section] "The Bregman divergence measures the difference between a convex function and its first-order approximation... Two key observations are that by the convexity of all the fz's, both D(x, x*) and D̂(x, x*) are non-negative, and E[D̂(x, x*)] = D(x, x*)."
- **Break condition**: If the functions f_z are not Lipschitz or bounded, the Bregman divergence may not be concentrated enough for the argument to work.

### Mechanism 2: ERM vs Uniform Convergence Separation
- **Claim**: The sample complexity separation between ERMs and uniform convergence arises because ERMs only need to control the behavior at actual minimizers, not everywhere in the parameter space.
- **Mechanism**: Uniform convergence requires that for all x in the parameter space, the empirical loss approximates the population loss. This typically requires O(d/ε²) samples. However, ERMs only need to ensure that points that are empirical minimizers are also close to population minimizers. By using the first-order optimality condition and the structure of convex optimization, the proof shows that only O(d/ε + 1/ε²) samples are needed. The d/ε term comes from covering arguments, while the 1/ε² term handles the stochastic fluctuations.
- **Core assumption**: The optimization landscape has structure that allows focusing only on minimizers rather than the entire space.
- **Evidence anchors**:
  - [abstract] "This establishes a new separation between ERMs and uniform convergence, showing that learning in SCO is strictly easier than uniform convergence would suggest."
- **Break condition**: If the loss functions have pathological geometry or if the domain K is highly irregular, the structure that enables this separation might not hold.

### Mechanism 3: First-Order Optimality Condition
- **Claim**: The first-order optimality condition for stochastic functions allows controlling the behavior of empirical risk minimizers through gradient concentration.
- **Mechanism**: The proof leverages that for stochastic convex functions, the subgradient at the optimum can be expressed as the expectation of subgradients. This means that the gradient of the population loss at the optimum (which should be zero) can be approximated by the average of gradients of individual functions. By showing that this average concentrates around zero, the proof controls how far empirical minimizers can deviate from population minimizers.
- **Core assumption**: The subgradient sum property holds, allowing decomposition of population gradients into expectations over individual function gradients.
- **Evidence anchors**:
  - [section] "Proposition 3 (Stochastic first-order condition)... For a stochastic convex function, the subgradient can be thought as the expectation of the subgradients... This leads to the following stochastic first-order optimality condition."
- **Break condition**: If the functions f_z don't have well-defined subgradients or if the expectation relationship breaks down, this mechanism fails.

## Foundational Learning

- **Concept: Bregman divergence**
  - Why needed here: The Bregman divergence provides the crucial non-negative measure that connects empirical and population losses, enabling concentration arguments specific to convex optimization.
  - Quick check question: What property of the Bregman divergence makes it particularly useful for proving sample complexity bounds in stochastic convex optimization?

- **Concept: Subgradient sum property for stochastic functions**
  - Why needed here: This property allows expressing population gradients as expectations of individual function gradients, which is essential for controlling the behavior of empirical risk minimizers.
  - Quick check question: How does the subgradient sum property differ from standard gradient descent analysis in non-stochastic settings?

- **Concept: Covering numbers and ε-nets**
  - Why needed here: Covering arguments are used to extend local concentration results to the entire parameter space, particularly for handling the dimension-dependent term in the sample complexity.
  - Quick check question: Why does the proof use an ε/3-net rather than a standard ε-net when applying the union bound?

## Architecture Onboarding

- **Component map**: First-order optimality conditions -> Bregman divergence concentration -> Covering arguments -> Sample complexity bound
- **Critical path**: The most critical sequence is: establish first-order conditions → use Bregman divergence to relate empirical and population losses → apply concentration inequalities → use covering arguments for union bound → conclude sample complexity bound.
- **Design tradeoffs**: The proof trades off between tightness of concentration bounds and the complexity of the covering argument. Using Bregman divergence allows tighter bounds than uniform convergence, but requires careful handling of the boundary cases where the optimum lies on the boundary of the domain.
- **Failure signatures**: If the Bregman divergence concentration fails (e.g., due to non-Lipschitz functions), the proof breaks down. Similarly, if the first-order optimality condition doesn't hold (e.g., for non-convex losses), the entire approach fails.
- **First 3 experiments**:
  1. Verify the Bregman divergence concentration empirically on synthetic convex functions with known Lipschitz constants.
  2. Test the first-order optimality condition numerically by checking if the gradient of the population loss at the empirical minimizer concentrates around zero.
  3. Experiment with different covering strategies to see how the net size affects the final sample complexity bound empirically.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sample complexity bound be further improved for specific classes of loss functions beyond bounded convex Lipschitz functions?
- Basis in paper: The paper focuses on bounded convex Lipschitz functions over unit balls and shows a bound of O(d/ε + 1/ε²). It mentions generalizations to other norms but doesn't explore specific function classes.
- Why unresolved: The authors prove a general bound that applies to all convex Lipschitz functions, but don't investigate whether certain restricted function classes might admit tighter bounds.
- What evidence would resolve it: Analysis of sample complexity for restricted function classes (e.g., strongly convex, smooth, or with specific structure) showing improved bounds compared to the general O(d/ε + 1/ε²) result.

### Open Question 2
- Question: How does the sample complexity change when the optimal solution lies on the boundary of the domain versus in the interior?
- Basis in paper: The paper notes that when x⋆ is on the boundary, they don't know that ∇F(x⋆) = 0, and mentions needing additional arguments for this case (Claim 5).
- Why unresolved: The authors acknowledge this case requires separate treatment but don't provide a complete characterization of how boundary optima affect the sample complexity.
- What evidence would resolve it: A detailed analysis showing whether boundary optima lead to the same sample complexity, better bounds, or require additional assumptions/constraints.

### Open Question 3
- Question: Can the techniques developed for controlling stochastic convex optimization problems be extended to non-convex settings?
- Basis in paper: The paper builds a "mechanism for controlling the behavior of stochastic convex optimization problems" using Bregman divergences and concentration bounds.
- Why unresolved: The paper focuses exclusively on convex problems and doesn't explore whether the core techniques could be adapted to non-convex optimization, which is more challenging but practically important.
- What evidence would resolve it: Extension of the Bregman divergence-based analysis to specific classes of non-convex functions, showing either similar guarantees or proving limitations of the approach.

## Limitations
- The proof's reliance on Bregman divergence concentration requires Lipschitz and bounded functions, which may not hold in all practical settings.
- The mechanism for handling boundary cases (where the optimal point lies on the domain boundary) appears more complex and potentially less robust than the interior case.
- The extension to general symmetric convex bodies involves additional technical machinery (truncated Bregman divergence) that introduces new approximation terms requiring careful control.

## Confidence

**High Confidence**: The core separation between ERMs and uniform convergence (O(d/ε + 1/ε²) vs O(d/ε²)) - this follows directly from the concentration arguments and is well-supported by the theoretical framework.

**Medium Confidence**: The exact constant factors in the sample complexity bound - while the asymptotic scaling is established, the precise constants depend on technical details in the covering arguments and truncation mechanisms.

**Medium Confidence**: The generalization to arbitrary symmetric convex bodies - this extends beyond the main result and requires additional machinery whose tightness is harder to verify.

## Next Checks

1. Empirical validation of Bregman divergence concentration: Generate synthetic convex functions with controlled Lipschitz constants and verify that the empirical Bregman divergence concentrates around its expectation as predicted by the theory.

2. Boundary case stress testing: Construct optimization problems where the optimal point lies near or on the boundary of the domain, and measure whether the sample complexity bound degrades as expected when transitioning from interior to boundary cases.

3. Comparison with uniform convergence baselines: Implement both the ERM approach and uniform convergence methods on identical problem instances, measuring the actual sample complexity required to achieve ε-accuracy in each case to verify the predicted separation.