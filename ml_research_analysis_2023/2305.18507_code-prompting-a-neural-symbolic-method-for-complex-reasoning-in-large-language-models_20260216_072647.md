---
ver: rpa2
title: 'Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language
  Models'
arxiv_id: '2305.18507'
source_url: https://arxiv.org/abs/2305.18507
tags:
- coin
- code
- prompting
- heads
- last
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Code prompting is a neural-symbolic prompting method that guides\
  \ large language models to first generate executable code as intermediate steps,\
  \ then either use an external interpreter or the model itself to derive final answers.\
  \ This approach addresses limitations of natural language reasoning chains by leveraging\
  \ code\u2019s unambiguous, structured format to break down complex problems into\
  \ simpler sub-tasks."
---

# Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2305.18507
- Source URL: https://arxiv.org/abs/2305.18507
- Reference count: 40
- Zero-shot code prompting significantly outperforms zero-shot chain-of-thought prompting—by at least 26.2% in symbolic tasks

## Executive Summary
Code prompting is a neural-symbolic prompting method that guides large language models to first generate executable code as intermediate steps, then either use an external interpreter or the model itself to derive final answers. This approach addresses limitations of natural language reasoning chains by leveraging code's unambiguous, structured format to break down complex problems into simpler sub-tasks. Experiments on 7 benchmarks show that zero-shot code prompting significantly outperforms zero-shot chain-of-thought prompting—by at least 26.2% in symbolic tasks and remains competitive in arithmetic tasks.

## Method Summary
Code prompting is a two-stage neural-symbolic prompting method where LLMs first generate Python code to represent reasoning steps, then either execute the code using an external interpreter or use it as a template to guide LLM reasoning. The approach transforms complex reasoning tasks into code generation problems, with each line of code representing a discrete sub-task. The method includes both interpreter-based and LLM-based answer generation approaches, along with optional modules for self-debugging, irrelevant information filtering, and equation solving.

## Key Results
- Zero-shot code prompting achieves 97.2% accuracy on last letter concatenation task vs 71.0% for zero-shot chain-of-thought
- On coin flip tasks, code prompting reaches 87.0% average accuracy vs 75.3% for chain-of-thought
- Ensemble methods combining code prompting and chain-of-thought achieve 87.95% accuracy on GSM8K vs 81.58% for chain-of-thought alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code prompting reduces complex reasoning tasks into simpler, LLM-manageable sub-tasks through explicit task reduction
- Mechanism: By requiring the model to generate code first, each line of code represents a discrete sub-task. The model breaks down the problem into these manageable units before attempting to solve it
- Core assumption: LLMs can reliably generate code that accurately represents the decomposition of the reasoning task
- Evidence anchors:
  - [abstract]: "code prompting generally outperforms chain-of-thought prompting" and "code works as a mind map for the LLM and reduces the task into sub-tasks represented by separate operations in the code"
  - [section 4.3]: "Code prompting transforms a question into a program. Each line of the code can be seen as a sub-task. Sub-tasks that can be represented as a line of code are usually easy for the LLM to complete, and thus the question transformation actually generates friendly instructions for LLM reasoning"

### Mechanism 2
- Claim: Code prompting eliminates ambiguity present in natural language prompts, leading to more robust reasoning
- Mechanism: Code is a formal language without ambiguity, unlike natural language. By formalizing the reasoning steps in code, confusion caused by ambiguous natural language is removed
- Core assumption: The ambiguity in natural language prompts is a significant source of error for LLMs
- Evidence anchors:
  - [abstract]: "code is a formal language without ambiguity, which eliminates confusion caused by ambiguity in natural language and makes the reasoning process more robust"
  - [section 6]: "Code prompting has the potential to discover ambiguity in a question" and shows experiments where code prompting is more sensitive to ambiguity than CoT

### Mechanism 3
- Claim: Code prompting provides an explicit template or mindmap for the reasoning process, guiding the LLM step-by-step
- Mechanism: The generated code serves as a structured plan that the LLM follows to complete the reasoning. This is more effective than CoT's implicit guidance
- Core assumption: LLMs benefit from having a clear, structured plan to follow rather than generating reasoning steps organically
- Evidence anchors:
  - [abstract]: "code generated by the LLM can be seen as a formalization of the question that eliminates the ambiguity in the original natural language questions"
  - [section 4.3]: "Through case-by-case observation, we discover that code generated in the first stage works as an explicit template, a mindmap for the LLM to conduct the reasoning process. LLM follows the code to complete the task. In comparison, CoT prompting fails to provide such an explicit template before reasoning"

## Foundational Learning

- Concept: Neural-symbolic integration
  - Why needed here: The method combines neural methods (LLMs) with symbolic methods (code) to leverage the strengths of both
  - Quick check question: Can you explain how neural-symbolic methods differ from purely neural or purely symbolic approaches?

- Concept: Prompt engineering techniques
  - Why needed here: The paper builds upon and improves existing prompting methods like chain-of-thought prompting
  - Quick check question: What are the key differences between zero-shot and few-shot prompting, and when might each be preferred?

- Concept: Code generation by LLMs
  - Why needed here: The method relies on the LLM's ability to generate executable code as an intermediate step
  - Quick check question: How does the quality of code generated by LLMs impact the effectiveness of code prompting?

## Architecture Onboarding

- Component map: LLM -> Prompt templates -> Code generation -> (Optional: External Python interpreter) -> Answer generation -> Final output
- Critical path: 1. Generate code from question using prompt 2a. (Interpreter approach) Execute code and handle errors with self-debugging 2b. (LLM approach) Use code as template to guide LLM reasoning 3. Output final answer
- Design tradeoffs:
  - Interpreter vs. LLM-based answer generation: Interpreter is more reliable but adds complexity; LLM approach is simpler but may inherit LLM errors
  - Zero-shot vs. few-shot prompting: Few-shot may improve code quality but requires manual effort
  - Code annotations: Improve readability and performance but add to prompt length
- Failure signatures:
  - Code generation fails to properly decompose the problem
  - Generated code contains logical errors or is unexecutable
  - LLM fails to follow the code template correctly
  - External interpreter introduces errors not caught by self-debugging
- First 3 experiments:
  1. Replicate the last letter concatenation benchmark to verify basic functionality
  2. Test on a simple arithmetic reasoning task to compare interpreter vs. LLM answer generation
  3. Implement and test the self-debugging module on a task known to occasionally generate buggy code

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of code prompting vary across different types of symbolic reasoning tasks beyond last letter concatenation and coin flip?
- Basis in paper: The paper shows code prompting significantly outperforms CoT prompting on last letter concatenation (71.0% → 97.2%) and coin flip tasks (average 75.3% → 87.0%), with performance gains increasing with question complexity. However, these are limited to specific tasks
- Why unresolved: The experiments only tested two symbolic reasoning tasks. The paper does not explore whether code prompting's advantages generalize to other types of symbolic reasoning problems like pattern recognition, sequence prediction, or logical inference
- What evidence would resolve it: Testing code prompting on a broader range of symbolic reasoning benchmarks (e.g., Raven's Progressive Matrices, pattern completion tasks, or logical puzzles) would reveal its generalizability

### Open Question 2
- Question: What are the fundamental differences in reasoning patterns between code prompting and chain-of-thought prompting that lead to complementary strengths?
- Basis in paper: The authors note that code prompting and CoT lead LLMs to "think from different angles" and demonstrate through ensemble methods that combining both approaches achieves better results (87.95% accuracy on GSM8K vs 81.58% for CoT alone)
- Why unresolved: While the paper shows that the two methods produce different error patterns and benefit from ensembling, it does not analyze what specific reasoning differences cause this complementarity. The error analysis shows sensitivity to ambiguity differs between methods, but the underlying cognitive mechanisms remain unclear
- What evidence would resolve it: Systematic analysis of intermediate steps generated by both methods on the same problems, perhaps using cognitive science techniques like protocol analysis or comparing with human problem-solving strategies

### Open Question 3
- Question: Why does zero-shot code prompting perform competitively with few-shot CoT and PAL on arithmetic reasoning despite lacking task-specific demonstrations?
- Basis in paper: The authors note that "the gap between zero-shot and few-shot prompting methods seems to narrow" as LLMs grow larger, and demonstrate that zero-shot code prompting achieves performance close to few-shot methods like PAL on arithmetic tasks
- Why unresolved: The paper does not explain the mechanisms behind this surprising finding. It mentions that code prompting's "explicit template" and "task reduction" properties help, but does not investigate why these advantages allow zero-shot code prompting to match few-shot performance without any demonstrations
- What evidence would resolve it: Comparative analysis of what code generation captures that exemplars provide in few-shot methods, possibly through ablation studies testing different aspects of code generation (abstraction, disambiguation, task reduction) in isolation

## Limitations
- The method relies heavily on LLM's ability to generate executable code, which may not scale well to domains where code generation is inherently difficult
- Experiments focus primarily on symbolic and arithmetic reasoning tasks, leaving uncertainty about performance on more complex reasoning domains
- Few-shot code prompting requires manual effort to construct high-quality exemplars, limiting practical deployment

## Confidence

**High Confidence:** The core finding that code prompting outperforms zero-shot chain-of-thought prompting on symbolic tasks (by 26.2%+) is well-supported by experimental results across multiple benchmarks. The mechanism that code's unambiguous nature reduces ambiguity in reasoning is consistently demonstrated.

**Medium Confidence:** The claim that code prompting provides better task reduction and explicit templating is supported by case studies but could benefit from more systematic analysis across diverse problem types. The ensemble method improvements are promising but tested on a limited set of tasks.

**Low Confidence:** Generalization claims to non-arithmetic/symbolic domains are largely speculative, as experiments are confined to the 7 benchmarks tested. The long-term robustness of the method as LLMs evolve is unknown.

## Next Checks
1. **Cross-Domain Generalization Test:** Evaluate code prompting on commonsense reasoning benchmarks like HellaSwag or strategyQA to assess performance beyond symbolic/arithmetic tasks
2. **Code Quality Impact Analysis:** Systematically measure how code generation quality (syntax errors, logical completeness) correlates with final answer accuracy across different problem types
3. **Resource Efficiency Benchmark:** Compare the computational overhead of code prompting with ensemble methods against simpler prompting approaches to quantify practical deployment costs