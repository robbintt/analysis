---
ver: rpa2
title: How Robust is Google's Bard to Adversarial Image Attacks?
arxiv_id: '2309.11751'
source_url: https://arxiv.org/abs/2309.11751
tags:
- adversarial
- image
- attack
- bard
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the adversarial robustness of Google\u2019\
  s Bard, a commercial multimodal large language model (MLLM) that integrates vision\
  \ and language. The authors propose two black-box attack methods: image embedding\
  \ attack and text description attack."
---

# How Robust is Google's Bard to Adversarial Image Attacks?

## Quick Facts
- arXiv ID: 2309.11751
- Source URL: https://arxiv.org/abs/2309.11751
- Authors: 
- Reference count: 40
- This paper demonstrates that adversarial examples transfer from surrogate MLLMs to Google's Bard with 22% success rate

## Executive Summary
This paper investigates the adversarial robustness of Google's Bard, a commercial multimodal large language model, through black-box attack methods. The authors propose image embedding and text description attacks that leverage transferability from white-box surrogate models to generate adversarial examples. These attacks achieve a 22% success rate in misleading Bard's image description task, while also identifying vulnerabilities in Bard's face detection and toxicity detection defenses. The results highlight significant security risks for commercial MLLMs and emphasize the need for more robust multimodal foundation models.

## Method Summary
The authors employ two black-box attack methods targeting Google's Bard and other commercial MLLMs. Image embedding attacks maximize the distance between natural and adversarial image embeddings, while text description attacks maximize the likelihood of predicting target sentences. Both attacks use state-of-the-art transfer-based methods (SSA-CW A) optimized on surrogate vision encoders and MLLMs. The study evaluates attack success rates on 100 images from the NIPS17 dataset and tests transferability to Bing Chat (26% success) and ERNIE Bot (86% success). Defense mechanisms are targeted using surrogate face detectors (PyramidBox, S3FD, DSFD) and CLIP-based toxicity detectors.

## Key Results
- Transfer-based adversarial examples achieve 22% success rate against Bard's image description task
- Adversarial examples transfer to other MLLMs: 26% success on Bing Chat, 86% on ERNIE Bot
- Defense evasion attacks successfully bypass Bard's face detection and toxicity detection mechanisms
- Ensemble attacks outperform single-model attacks by promoting flatness of loss landscapes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer-based adversarial examples generated on surrogate MLLMs can mislead Bard
- Mechanism: Adversarial perturbations shift image embeddings in surrogate models, which also shift embeddings in Bard due to shared training data distributions and architectural similarities
- Core assumption: The transferability gap between surrogate and target models is small enough that perturbed embeddings still mislead the downstream text generation pipeline
- Evidence anchors:
  - [abstract] "By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability"
  - [section 3.2] "We adopt the state-of-the-art transfer-based attack methods [8, 30] in this paper... The spectrum simulation attack (SSA) [30] performs a spectrum transformation to the input to improve the adversarial transferability"
  - [corpus] Found 25 related papers with average neighbor FMR=0.443, indicating moderate density of adversarial research

### Mechanism 2
- Claim: Evasion of face and toxicity detection defenses is possible via targeted perturbation
- Mechanism: Adversarial examples that reduce detection confidence scores in surrogate face/toxicity detectors also reduce confidence in Bard's internal detectors, causing them to fail silently
- Core assumption: Bard's defense modules are functionally similar to the surrogate detectors used for optimization
- Evidence anchors:
  - [section 4.1] "Let {Di}K i=1 denote the set of surrogate face detectors... Our face attack minimizes the confidence score such that the model cannot detect the face"
  - [section 4.2] "To target these surrogate models, we only need to perturb the features of these pre-trained models... we employ the exact same objective function as given in Eq. (1)"
  - [corpus] No direct corpus evidence of face/toxicity defense evasion; this is a novel contribution

### Mechanism 3
- Claim: Ensemble attacks outperform single-model attacks due to reduced variance in gradient directions
- Mechanism: Aggregating adversarial gradients from multiple surrogate models smooths the loss landscape and finds a direction that is more likely to transfer across models
- Core assumption: Surrogate models share enough inductive biases that their combined gradient directions capture a common vulnerability subspace
- Evidence anchors:
  - [section 3.2] "Tab. 2 shows the results. The attack success rate increases with the number of surrogate models"
  - [section 3.2] "We adopt the state-of-the-art transfer-based attack methods [8, 30]... CWA proposes to find the common weakness of an ensemble of surrogate models by promoting the flatness of loss landscapes and closeness between local optima"
  - [corpus] Moderate corpus support (FMR=0.443) for transferability and ensemble-based attack research

## Foundational Learning

- Concept: Adversarial transferability in deep neural networks
  - Why needed here: Understanding how perturbations optimized on one model affect another is the core of the attack strategy
  - Quick check question: What property of neural networks allows an adversarial example to fool a different model?

- Concept: Vision-language model pipeline (encoder → embedding → LLM → text)
  - Why needed here: The attack targets the embedding stage, so knowing how vision features feed into text generation is essential
  - Quick check question: At which stage does the attack inject perturbations, and how does this affect the final description?

- Concept: Gradient-based optimization under ℓ∞ constraints
  - Why needed here: The attack optimizes perturbations while bounding maximum change per pixel to remain imperceptible
  - Quick check question: Why is the ℓ∞ norm commonly used in adversarial attacks instead of ℓ2 or ℓ1?

## Architecture Onboarding

- Component map: Input image → Vision encoder → Embedding perturbation → LLM → Text output
- Critical path: Image → Vision encoder → Embedding perturbation → LLM → Text output
- Design tradeoffs:
  - More surrogate models → higher success rate but longer runtime
  - Higher perturbation budget (ϵ) → higher success but more visible artifacts
  - Targeted vs. untargeted attack → targeted may be harder to succeed but more controllable
- Failure signatures:
  - Low success rate despite high surrogate accuracy → poor transferability
  - High rejection rate → defenses catching perturbations
  - No change in embedding distance → attack optimization stuck
- First 3 experiments:
  1. Run image embedding attack with a single surrogate (ViT-B/16) and measure Bard's success rate
  2. Vary perturbation budget ϵ (8/255, 16/255, 32/255) to find the sweet spot between stealth and effectiveness
  3. Compare ensemble vs. single-surrogate transferability on a held-out validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different architectures and training datasets of commercial MLLMs affect their adversarial robustness compared to open-source models?
- Basis in paper: [explicit] The paper demonstrates that commercial MLLMs like Bard, Bing Chat, and ERNIE Bot have varying levels of robustness to adversarial attacks, with ERNIE Bot being the least robust. The paper notes that commercial MLLMs have larger architectures and unknown training datasets, making them more challenging to attack
- Why unresolved: The paper only evaluates a few commercial MLLMs and does not provide a comprehensive comparison across different architectures and training datasets. The varying robustness levels suggest that architectural differences and training data play a role, but the specific impact is unclear
- What evidence would resolve it: Systematic evaluation of multiple commercial MLLMs with different architectures (e.g., transformer-based, recurrent neural networks) and training datasets (e.g., diverse vs. specialized) to identify patterns in adversarial robustness

### Open Question 2
- Question: How effective are preprocessing-based defenses (e.g., diffusion purification) against adversarial attacks on MLLMs compared to adversarial training?
- Basis in paper: [inferred] The paper discusses the limitations of adversarial training for large-scale foundation models and suggests that preprocessing-based defenses may be more suitable. It mentions diffusion purification as a promising strategy but does not evaluate its effectiveness against MLLMs
- Why unresolved: The paper does not provide empirical evidence on the effectiveness of preprocessing-based defenses against adversarial attacks on MLLMs. The potential of these defenses is suggested but not validated
- What evidence would resolve it: Experimental comparison of preprocessing-based defenses (e.g., diffusion purification, likelihood maximization) and adversarial training on MLLMs, measuring their robustness against various types of adversarial attacks

### Open Question 3
- Question: How do adversarial attacks on MLLMs affect their alignment and safety, particularly in generating harmful or biased content?
- Basis in paper: [explicit] The paper mentions that adversarial attacks can be used to break the alignment of LLMs and MLLMs, leading to the generation of objectionable responses. It also notes that MLLMs are more vulnerable to adversarial image attacks due to the continuous space of images
- Why unresolved: The paper does not explore the specific impacts of adversarial attacks on the alignment and safety of MLLMs. It highlights the potential for harm but does not provide concrete examples or measurements
- What evidence would resolve it: Case studies and quantitative analysis of MLLMs' responses to adversarial inputs, measuring the generation of harmful or biased content and comparing it to their responses to benign inputs

## Limitations

- The study relies on surrogate models to approximate the behavior of commercial MLLMs, which may not perfectly represent their actual implementations
- Commercial MLLMs continuously evolve through updates, meaning the attack success rates may change over time
- The specific model configurations and training datasets of commercial MLLMs remain unknown, limiting the ability to understand architectural vulnerabilities

## Confidence

- High Confidence: The fundamental claim that adversarial examples transfer between models in the vision-language domain is well-supported by existing literature and experimental results
- Medium Confidence: The specific success rates (22% for Bard, 26% for Bing Chat, 86% for ERNIE Bot) are reliable within the experimental setup but may vary across different model versions
- Medium Confidence: The effectiveness of defense evasion techniques is demonstrated but may be model-specific, as surrogate detectors might not perfectly represent Bard's actual defense implementations

## Next Checks

1. **Cross-Model Consistency Test:** Conduct the same attack methodology on additional commercial MLLMs (e.g., GPT-4V, Claude 3) to verify whether the transferability patterns hold across a broader range of vision-language models with different architectures and training approaches

2. **Defense Mechanism Verification:** Implement the same face and toxicity detection attacks directly on the surrogate models used by Bard (if accessible) to measure the exact gap between surrogate and target model defenses, and quantify how much the transferability assumption contributes to the observed success rates

3. **Temporal Robustness Assessment:** Repeat the experiments after 3-6 months to account for potential model updates and improvements in commercial MLLMs, measuring how the attack success rates change over time as models evolve their defense mechanisms