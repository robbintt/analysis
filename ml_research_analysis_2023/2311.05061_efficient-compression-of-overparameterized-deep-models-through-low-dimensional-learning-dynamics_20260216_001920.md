---
ver: rpa2
title: Efficient Compression of Overparameterized Deep Models through Low-Dimensional
  Learning Dynamics
arxiv_id: '2311.05061'
source_url: https://arxiv.org/abs/2311.05061
tags:
- matrix
- deep
- learning
- network
- compressed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel compression method for overparameterized
  deep networks based on low-dimensional learning dynamics. The key insight is that
  updates to weight matrices occur within a low-dimensional invariant subspace, allowing
  for significant parameter reduction.
---

# Efficient Compression of Overparameterized Deep Models through Low-Dimensional Learning Dynamics

## Quick Facts
- **arXiv ID**: 2311.05061
- **Source URL**: https://arxiv.org/abs/2311.05061
- **Reference count**: 40
- **Primary result**: Proposed method achieves >2x acceleration in training time without compromising model quality through low-dimensional learning dynamics

## Executive Summary
This paper introduces a novel compression technique for overparameterized deep networks that exploits low-dimensional learning dynamics. The key insight is that weight updates during training occur within low-dimensional invariant subspaces, allowing for significant parameter reduction. By reducing intermediate layer widths and using spectral initialization based on top singular subspaces, the method achieves faster convergence and smaller recovery errors than original overparameterized networks. The approach is theoretically grounded for deep linear networks and empirically validated on both linear and nonlinear architectures.

## Method Summary
The compression method works by first computing a surrogate matrix from available measurements, then extracting its top-ˆr singular subspaces. The compressed network is initialized with these subspaces and trained using gradient descent with scaled learning rates (αη for outer layers, η for inner layers). For deep linear networks, the approach provably achieves lower recovery error throughout training. The method extends to nonlinear networks by overparameterizing the penultimate layer with a deep linear network, which is then compressed using the same spectral initialization technique.

## Key Results
- Deep linear networks achieve faster convergence and lower recovery error with compression vs. original network
- Matrix completion experiments show superior performance on MovieLens 100K dataset
- Deep nonlinear networks (MLP, ViT) achieve >2x training acceleration without accuracy loss
- Recovery error consistently below original network throughout all training iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Network weight updates occur within low-dimensional invariant subspaces
- Mechanism: Gradient descent updates only affect a small portion of singular values, with most remaining close to initialization
- Core assumption: Training dynamics preserve low-rank structure
- Evidence anchors:
  - [abstract] "updates to the weight matrices occur within a low-dimensional invariant subspace"
  - [section] "gradient descent only updates a small portion of the singular values"
  - [corpus] Weak - related works discuss low-rank structures but don't confirm invariant subspace persistence
- Break condition: If training dynamics change neglected singular values significantly

### Mechanism 2
- Claim: Spectral initialization leads to faster convergence and lower recovery error
- Mechanism: Initializing with top singular subspaces of surrogate matrix starts network closer to solution
- Core assumption: Surrogate matrix's singular subspaces approximate target matrix's structure
- Evidence anchors:
  - [abstract] "using an initialization that exploits the structure of the problem"
  - [section] "left and rightmost factors of the original DLN align with the left and right singular vectors"
  - [corpus] Missing - no direct confirmation from related works
- Break condition: If surrogate matrix poorly approximates target structure

### Mechanism 3
- Claim: Incremental learning enables efficient compression
- Mechanism: Principal components fitted sequentially within small subspace
- Core assumption: Incremental learning pattern holds consistently
- Evidence anchors:
  - [abstract] "principal components of deep linear models are fitted incrementally"
  - [section] "singular subspaces of the DLN are fitted one at a time"
  - [corpus] Weak - related works mention incremental learning but don't confirm subspace constraint
- Break condition: If multiple components need simultaneous fitting

## Foundational Learning

- Concept: Deep Linear Networks and optimization dynamics
  - Why needed here: Compression theory builds on understanding DLN learning of low-rank structures
  - Quick check question: What distinguishes DLN optimization from nonlinear networks, and why is this important for compression?

- Concept: Singular Value Decomposition and low-rank matrix structure
  - Why needed here: Compression relies on identifying and exploiting low-rank structure through singular subspaces
  - Quick check question: How does SVD relate to low-rank approximation, and why relevant for initializing compressed networks?

- Concept: Gradient flow and implicit regularization in overparameterized models
  - Why needed here: Leverages implicit bias toward low-rank solutions in overparameterized networks
  - Quick check question: How does gradient descent implicitly regularize toward low-rank solutions, and what theory supports this?

## Architecture Onboarding

- Component map:
  Original DLN (Wl ∈ Rd×d) -> Compressed DLN (reduced width ˆr < d) -> Surrogate matrix -> Spectral initialization -> Gradient descent training

- Critical path:
  1. Compute surrogate matrix from measurements
  2. Extract top-ˆr singular subspaces
  3. Initialize compressed network factors with these subspaces
  4. Train using gradient descent with appropriate learning rate scaling
  5. Monitor recovery error throughout training

- Design tradeoffs:
  - Width reduction vs. accuracy: Smaller ˆr reduces parameters but may hurt performance
  - Initialization quality vs. computation: Better spectral initialization improves convergence but requires SVD computation
  - Depth vs. implicit regularization: Deeper networks provide stronger low-rank bias but increase training time

- Failure signatures:
  - Recovery error plateaus above acceptable threshold
  - Training instability with large learning rate scaling α
  - No improvement over original network despite compression

- First 3 experiments:
  1. Matrix factorization with synthetic data (rank 10, d=100) to verify faster convergence claim
  2. Matrix completion on MovieLens dataset to test real-world applicability
  3. Overparameterized MLP with DLN penultimate layer on FashionMNIST to validate nonlinear extension

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incremental learning extend to deeper architectures with more than three layers?
- Basis in paper: [explicit] Paper demonstrates incremental learning in 3-layer DLNs but doesn't explore deeper architectures
- Why unresolved: Theoretical analysis focuses on L=3, experiments with deeper networks not reported
- What evidence would resolve it: Experimental results showing singular value fitting patterns in DLNs with L>3 layers

### Open Question 2
- Question: How does performance change when true rank r is unknown and chosen rank upper bound ˆr is not a multiple of r?
- Basis in paper: [explicit] Paper assumes ˆr ≥ r but chooses ˆr = 2r in experiments
- Why unresolved: Impact of different ˆr choices on recovery error and training time not systematically explored
- What evidence would resolve it: Comprehensive study varying ˆr for different r values

### Open Question 3
- Question: Can compression technique extend to train deep nonlinear networks with architectures beyond MLPs and vision transformers?
- Basis in paper: [explicit] Demonstrates compression for MLPs and ViTs but doesn't explore other architectures
- Why unresolved: Applicability to other network types not investigated
- What evidence would resolve it: Experimental results for CNNs, GNNs, or other architectures

## Limitations

- Theoretical guarantees primarily validated for deep linear networks with specific initialization schemes
- Extension to nonlinear networks has weaker theoretical foundation and may be architecture-sensitive
- Method's effectiveness for networks without natural low-rank structures not thoroughly examined

## Confidence

- **High Confidence**: Empirical observation of weight updates within low-dimensional subspaces for DLNs, supported by theoretical analysis and synthetic experiments
- **Medium Confidence**: Extension to nonlinear networks shows promising results but theoretical foundation is weaker
- **Low Confidence**: Claims about generalization to arbitrary overparameterized networks without low-rank structures

## Next Checks

1. Test compression method on ResNet architectures where relationship between initialization and low-rank structure is less clear
2. Systematically vary learning rate scaling factor α in nonlinear networks to identify optimal values and failure modes
3. Evaluate method's performance on datasets with noisy measurements to assess robustness beyond clean matrix recovery problems