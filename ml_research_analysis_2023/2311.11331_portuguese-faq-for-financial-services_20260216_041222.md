---
ver: rpa2
title: Portuguese FAQ for Financial Services
arxiv_id: '2311.11331'
source_url: https://arxiv.org/abs/2311.11331
tags:
- data
- dataset
- question
- similarity
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the scarcity of domain-specific data in the
  Portuguese financial domain, which has hindered the development of Natural Language
  Processing (NLP) applications. To overcome this limitation, the authors propose
  utilizing synthetic data generated through data augmentation techniques.
---

# Portuguese FAQ for Financial Services

## Quick Facts
- arXiv ID: 2311.11331
- Source URL: https://arxiv.org/abs/2311.11331
- Reference count: 24
- Primary result: Data augmentation improves Portuguese financial NLP model performance by up to 72% F1 in text classification.

## Executive Summary
This paper addresses the scarcity of Portuguese domain-specific data for financial services NLP by proposing data augmentation techniques. Using the Central Bank of Brazil FAQ dataset, the authors apply paraphrasing methods to generate synthetic training data with varying semantic similarity levels. They evaluate the impact of these augmented datasets on supervised text classification and unsupervised semantic search tasks, demonstrating significant performance improvements when using modern transformer architectures.

## Method Summary
The authors augment the BACEN FAQ dataset (approximately 2,000 question-answer pairs) using three data augmentation techniques: synonym replacement (DASYNONYM), maximum similarity paraphrasing (DAMAX_SIM), and minimum similarity paraphrasing (DAMIN_SIM). They evaluate these augmented datasets on text classification using mBERT, BERTaú, BERTimbau, and DPR models, measuring F1 score. For semantic search, they compare BM25+ with transformer-based models (mBERT, BERTaú, BERTimbau) using MRR@k. They also implement a two-stage FAQ retrieval system combining BM25+ first-pass retrieval with ColBERT re-ranking.

## Key Results
- 72% enhancement in F1 score for text classification when using BERT models with augmented data
- Transformer-based models outperform traditional BM25 for semantic search when semantic similarity is low
- ColBERT re-ranking improves BM25+ performance, especially for low semantic similarity queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentation improves supervised classification performance for low-resource Portuguese financial text.
- Mechanism: Generating synthetic paraphrases of questions increases the diversity of training examples, allowing the model to learn more robust representations of semantically similar inputs.
- Core assumption: The augmented examples retain high semantic similarity to the originals, and the model can generalize across them.
- Evidence anchors:
  - [abstract] "data augmentation techniques, such as paraphrasing, noise injection, and sampling, can improve the performance of NLP models in tasks like text classification and semantic search."
  - [section 4.1] "a notable 72% enhancement in performance attributed to data augmentation (DA) when the BERT model was employed."
  - [corpus] Weak: no directly related Portuguese DA papers in corpus.
- Break condition: If augmented sentences introduce too much semantic drift or grammatical noise, model performance degrades.

### Mechanism 2
- Claim: Low semantic similarity between queries and documents favors modern transformer-based models over BM25.
- Mechanism: Transformers capture contextual and semantic nuances better than term-matching heuristics, especially when lexical overlap is low.
- Core assumption: Modern architectures encode richer semantic relationships than sparse term-matching models.
- Evidence anchors:
  - [abstract] "models with contemporary architectures become more effective than traditional BM25 for semantic search tasks."
  - [section 4.2] "BERT's context-dependent embeddings introduce flexibility for the same word to possess distinct dense representations."
  - [corpus] Weak: no direct semantic shift detection paper in corpus, but related financial STS paper exists.
- Break condition: When queries and documents are highly similar lexically, BM25 may still outperform transformers.

### Mechanism 3
- Claim: In unsupervised FAQ retrieval, re-ranking improves retrieval quality more when semantic similarity is lower.
- Mechanism: First-pass BM25 retrieves a broad candidate set; ColBERT fine-tunes ranking by capturing contextual relevance, especially beneficial for harder (low-similarity) cases.
- Core assumption: The re-ranker can correct false positives from the first-pass retrieval, with larger gains where initial matches are weaker.
- Evidence anchors:
  - [section 4.3] "as the semantic similarity of the data decreases, i.e, when the data becomes tricky, the ColBERT gain improves."
  - [section 4.3] "re-ranking improves BM25+ performance... when the data becomes tricky, the ColBERT gain improves."
  - [corpus] Weak: no FAQ retrieval re-ranking paper in corpus.
- Break condition: If candidate set is too small or too large, or if semantic similarity is high, gains from re-ranking diminish.

## Foundational Learning

- Concept: Semantic similarity metrics (cosine similarity on embeddings)
  - Why needed here: Used to evaluate both augmented data quality and retrieval effectiveness.
  - Quick check question: What range does cosine similarity take, and what does a value near 0.8 mean in this context?

- Concept: Data augmentation techniques for NLP (paraphrasing, back-translation, noise injection)
  - Why needed here: The core experimental intervention to expand the dataset.
  - Quick check question: How does back-translation preserve meaning while varying surface form?

- Concept: Dense vs sparse retrieval (BERT embeddings vs BM25)
  - Why needed here: The paper contrasts these approaches across tasks and data conditions.
  - Quick check question: Why might BM25 outperform dense retrieval on highly lexical matches?

## Architecture Onboarding

- Component map:
  Data source (BACEN FAQ) -> Preprocessing -> Augmentation pipeline (DASYNONYM, DAMAX_SIM, DAMIN_SIM) -> Model training (mBERT, BERTaú, BERTimbau, DPR) -> Evaluation (F1, MRR@k)

- Critical path:
  1. Load original FAQ dataset → augment via paraphrasing → split into train/test → train classification model → evaluate.
  2. For retrieval: embed augmented data → build index (BM25+ or dense) → retrieve → optionally re-rank with ColBERT.

- Design tradeoffs:
  - High semantic similarity augmentation (DASYNONYM) may improve classification but add little diversity.
  - Low similarity augmentation (DAMIN_SIM) may challenge models but better simulate real-world variance.
  - Dense retrieval needs large memory for index; BM25+ is lightweight but less semantic.

- Failure signatures:
  - Classification F1 drops after augmentation → semantic drift in augmented data.
  - Retrieval MRR drops with dense models → embedding quality or training mismatch.
  - Re-ranking degrades performance → candidate set too small or model overfits.

- First 3 experiments:
  1. Train mBERT classifier on original data vs augmented (DASYNONYM); compare F1.
  2. Run semantic search using BM25+ vs mBERT embeddings on augmented test set; compare MRR.
  3. Run FAQ retrieval with BM25+ first-pass, then ColBERT re-rank on DAMIN_SIM; measure gain over BM25 alone.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the BACEN FAQ dataset compare when using different data augmentation techniques (paraphrasing, noise injection, sampling) beyond just paraphrasing?
- Basis in paper: [explicit] The paper mentions three categories of data augmentation techniques: paraphrasing, noise injection, and sampling. However, the study only employed paraphrasing techniques for data augmentation.
- Why unresolved: The paper only explored paraphrasing techniques for data augmentation, leaving the potential impact of other techniques unexplored.
- What evidence would resolve it: Conducting experiments using noise injection and sampling techniques on the BACEN FAQ dataset and comparing the results with the paraphrasing approach would provide insights into the effectiveness of different data augmentation methods.

### Open Question 2
- Question: How does the performance of the BACEN FAQ dataset vary with different levels of semantic similarity between the original and augmented data?
- Basis in paper: [explicit] The paper mentions that they created three DA objects (DASYNONYM, DAMAX_SIM, DAMIN_SIM) with varying levels of semantic similarity. However, the impact of these different similarity levels on model performance is not explicitly discussed.
- Why unresolved: The paper does not provide a detailed analysis of how the performance of the models varies with different levels of semantic similarity between the original and augmented data.
- What evidence would resolve it: Conducting experiments with different levels of semantic similarity and analyzing the impact on model performance would provide insights into the optimal level of similarity for data augmentation.

### Open Question 3
- Question: How does the performance of the BACEN FAQ dataset compare when using different pre-trained models (e.g., multilingual models, domain-specific models) for text classification and semantic search tasks?
- Basis in paper: [explicit] The paper mentions the use of mBERT, BERTaú, and BERTimbau models for text classification and semantic search tasks. However, it does not explore the performance of other pre-trained models.
- Why unresolved: The paper only explores the performance of a limited set of pre-trained models, leaving the potential impact of other models unexplored.
- What evidence would resolve it: Conducting experiments using different pre-trained models, including multilingual models and domain-specific models, and comparing the results with the current models would provide insights into the effectiveness of different model choices.

## Limitations
- The study focuses on a single domain (Brazilian financial FAQ) with a relatively small dataset (~2k examples), limiting generalizability.
- The paper lacks detailed implementation specifics for the augmentation techniques, particularly back-translation.
- Evaluation metrics are standard but narrow, with no ablation studies examining the relative contributions of different augmentation techniques.

## Confidence
- Text classification improvements (72% F1 gain): Medium confidence
- Semantic search comparisons between transformers and BM25: Medium confidence  
- Re-ranking effectiveness claims: Medium confidence

## Next Checks
1. Re-run the classification experiments with a controlled comparison: augment only half the training data and measure F1 gain versus baseline.
2. Test BM25 and dense retrieval head-to-head on a held-out test set with varied semantic similarity ranges to confirm the crossover point where transformers outperform.
3. Perform an ablation on the re-ranking stage by varying the candidate set size and measuring the corresponding MRR gain to establish optimal parameters.