---
ver: rpa2
title: 'Choir Transformer: Generating Polyphonic Music with Relative Attention on
  Transformer'
arxiv_id: '2308.02531'
source_url: https://arxiv.org/abs/2308.02531
tags:
- music
- polyphonic
- transformer
- attention
- chord
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Choir Transformer, a Transformer-based model
  for polyphonic music generation. It introduces relative positional attention to
  better model musical structures and a data representation method combining chord
  and note information.
---

# Choir Transformer: Generating Polyphonic Music with Relative Attention on Transformer

## Quick Facts
- arXiv ID: 2308.02531
- Source URL: https://arxiv.org/abs/2308.02531
- Reference count: 25
- Outperforms state-of-the-art models with 4.06% improvement in accuracy

## Executive Summary
This paper introduces Choir Transformer, a novel Transformer-based architecture for polyphonic music generation that leverages relative positional attention to better capture long-range musical dependencies. The model combines chord and note information through a chord-first data representation method, enabling it to generate harmonically coherent music across various styles. The proposed architecture achieves state-of-the-art performance on polyphonic music generation tasks, with harmony metrics approaching the quality of Bach's original compositions.

## Method Summary
The Choir Transformer modifies the standard Transformer decoder by replacing absolute positional encoding with relative positional attention, which computes attention weights based on relative distances between tokens rather than absolute positions. The model uses a chord-first data representation that arranges input data to prioritize chord information before individual note events for each voice part. Data augmentation through transposition and inversion expands the training dataset to 12,000 pieces. The architecture employs a multi-head encoder-decoder structure with cross-entropy loss during training.

## Key Results
- Achieves 4.06% improvement in token error rate compared to previous state-of-the-art models
- Harmony metrics (CTnCTR, PCS, MCTD) approach the quality of Bach's original compositions
- Supports adjustable melody and rhythm generation across various musical styles
- Outperforms baseline models on polyphonic music generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relative positional attention captures long-range musical dependencies better than absolute positional encoding
- Mechanism: By computing attention weights based on relative distances between tokens rather than absolute positions, the model can learn relationships between notes regardless of where they appear in the sequence
- Core assumption: Musical structure depends more on intervals and relative positions than absolute timing
- Evidence anchors: [abstract] "with relative positional attention to better model the structure of music"; [section] "The positional encoding in Transformer is absolute. However, the relative position plays an essential role in music structure."
- Break condition: If musical structure is primarily determined by absolute timing rather than relative intervals, the benefit would diminish

### Mechanism 2
- Claim: Chord-first data representation improves harmonic coherence
- Mechanism: By arranging input data to prioritize chord information before individual note events, the model learns to generate notes that are more harmonically consistent with the underlying chord progression
- Core assumption: Harmonic structure is the primary organizing principle for polyphonic music
- Evidence anchors: [section] "We arrange the polyphonic music data to combine information in a chord-first manner"; [section] "This paper give the sequence of input x = {Ccls1, Snote1, Anote1, Tnote1, Bnote1, ...}"
- Break condition: If melodic coherence is more important than harmonic coherence for the target musical style

### Mechanism 3
- Claim: Data augmentation through transposition and inversion expands the model's understanding of interval relationships
- Mechanism: By training on transposed and inverted versions of the music, the model learns invariant relationships between notes that hold across different keys and melodic orientations
- Core assumption: Musical relationships are largely key-independent and can be learned through augmentation
- Evidence anchors: [section] "We propose two data enhancements, the transposition and inversion methods"; [section] "Through the two enhancements, we expand the dataset to 12000 chorus music"
- Break condition: If musical context is too strongly tied to specific keys or inversions for augmentation to be beneficial

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The Choir Transformer builds directly on Transformer architecture, modifying the attention mechanism for musical applications
  - Quick check question: What is the difference between self-attention and cross-attention in the Transformer decoder?

- Concept: Musical harmony and chord structures
  - Why needed here: Understanding how chords and harmonies work is essential for interpreting the data representation and evaluation metrics
  - Quick check question: What is the difference between chord tones and non-chord tones in harmony analysis?

- Concept: Positional encoding in sequence models
  - Why needed here: The relative positional attention is a modification of standard positional encoding that's central to the Choir Transformer's innovation
  - Quick check question: How does absolute positional encoding differ from relative positional encoding in terms of what relationships they capture?

## Architecture Onboarding

- Component map: Input representation → chord-first tokenization → relative positional encoding → multi-head attention with relative distances → feed-forward layers → output softmax over note vocabulary
- Critical path: Input representation → chord-first tokenization → relative positional encoding → multi-head attention with relative distances → feed-forward layers → output softmax over note vocabulary
- Design tradeoffs: Chord-first representation prioritizes harmonic coherence over melodic independence; relative attention adds complexity but improves long-range dependency modeling
- Failure signatures: Poor harmonic coherence (high CTnCTR scores), inability to maintain melodic consistency over long sequences, generation of unrealistic note durations
- First 3 experiments:
  1. Compare validation accuracy with and without relative positional attention on a small dataset
  2. Test different chord-to-note ratios in the input representation
  3. Evaluate the impact of transposition augmentation on model generalization across keys

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the relative positional attention mechanism in Choir Transformer compare to absolute positional encoding in terms of modeling long-range musical dependencies?
- Basis in paper: [explicit] The paper proposes relative positional attention to better model musical structures and compares it to Transformer's absolute positional encoding.
- Why unresolved: The paper shows that relative positional attention improves performance but does not provide a detailed quantitative comparison of how well it captures long-range dependencies versus absolute positional encoding.
- What evidence would resolve it: A controlled experiment comparing the attention distance distributions and generated music quality using both relative and absolute positional encoding would clarify their relative effectiveness.

### Open Question 2
- Question: Can the Choir Transformer model be extended to generate vocal textures that more closely resemble human composition, beyond the four-part choral music?
- Basis in paper: [explicit] The paper mentions exploring how to generate vocal textures that more closely resemble human composition in future work.
- Why unresolved: The paper focuses on four-part choral music generation and does not explore more complex vocal textures or human-like composition.
- What evidence would resolve it: Experiments generating vocal music with varied textures and comparing it to human compositions would demonstrate the model's ability to generate more human-like vocal textures.

### Open Question 3
- Question: How does the use of different dataset enhancement methods (transposition and inversion) affect the model's ability to learn interval relationships and generalize to new musical styles?
- Basis in paper: [explicit] The paper proposes transposition and inversion methods for data enhancement and discusses their impact on the model's performance.
- Why unresolved: While the paper shows that these methods improve performance, it does not explore how they specifically affect the model's understanding of interval relationships or its ability to generalize to new musical styles.
- What evidence would resolve it: Experiments comparing the model's performance with and without dataset enhancement methods, as well as testing its ability to generate music in new styles, would clarify their impact on interval learning and generalization.

## Limitations

- Lack of hyperparameter specifications and implementation details prevents faithful reproduction of reported results
- Evaluation methodology doesn't provide statistical significance testing or confidence intervals for reported improvements
- Harmony metrics comparison to Bach's compositions lacks validation that these metrics correlate with musical quality as perceived by listeners

## Confidence

- **Medium Confidence**: The claim that relative positional attention improves long-range musical dependency modeling. Theoretically plausible but lacks direct ablation evidence.
- **Low Confidence**: The claim of achieving state-of-the-art performance with a 4.06% improvement. Without detailed experimental methodology or comparison to more recent models.
- **Medium Confidence**: The assertion that chord-first data representation improves harmonic coherence. Reasoning is sound but lacks ablation study.

## Next Checks

1. Implement an ablation study comparing Choir Transformer with and without relative positional attention, using the same hyperparameters and training procedure, to isolate the specific contribution of this architectural modification.

2. Conduct statistical significance testing on all reported improvements, including confidence intervals for TER scores and hypothesis tests for differences in harmony metrics, to verify that improvements are not due to random variation.

3. Extend the evaluation to include more recent polyphonic music generation models from the corpus (such as CoCoFormer and Polyffusion), using the same evaluation metrics and dataset, to verify that Choir Transformer maintains its state-of-the-art status.