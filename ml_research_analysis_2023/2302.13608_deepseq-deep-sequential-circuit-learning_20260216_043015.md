---
ver: rpa2
title: 'DeepSeq: Deep Sequential Circuit Learning'
arxiv_id: '2302.13608'
source_url: https://arxiv.org/abs/2302.13608
tags:
- deepseq
- circuit
- power
- sequential
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepSeq introduces the first graph neural network framework for
  learning effective representations of sequential circuits by exploiting temporal
  correlations between gates and flip-flops. The core idea involves a novel DAG-GNN
  architecture with customized propagation and dual attention aggregation to capture
  transition and logic probabilities simultaneously through multi-task learning.
---

# DeepSeq: Deep Sequential Circuit Learning

## Quick Facts
- **arXiv ID**: 2302.13608
- **Source URL**: https://arxiv.org/abs/2302.13608
- **Reference count**: 26
- **Primary result**: Introduces first GNN framework for sequential circuits achieving 20% and 15.8% relative error reductions for transition and logic probability prediction

## Executive Summary
DeepSeq introduces the first graph neural network framework for learning effective representations of sequential circuits by exploiting temporal correlations between gates and flip-flops. The core innovation is a novel DAG-GNN architecture with customized propagation and dual attention aggregation to capture transition and logic probabilities simultaneously through multi-task learning. When applied to power estimation on larger circuits after fine-tuning, DeepSeq reduces estimation error by 80.7% compared to baseline methods, demonstrating strong generalization and practical utility.

## Method Summary
DeepSeq is a DAG-GNN with customized propagation (removing FF cycles, forward/backward message passing, updating FFs), dual attention aggregation for transition and logic probability prediction, GRU-based combine function for hidden state updates, and multi-layer perceptrons for final prediction. The model is trained with ADAM optimizer for 50 epochs using multi-task learning with L1 losses for both transition and logic probability prediction tasks.

## Key Results
- Achieves 20% relative error reduction for transition probability prediction compared to existing GNN models
- Achieves 15.8% relative error reduction for logic probability prediction
- Reduces power estimation error by 80.7% on larger circuits after fine-tuning compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The dual attention aggregation mimics logic computation and transition probability calculation simultaneously, allowing DeepSeq to capture both probabilities effectively.
- **Mechanism**: First computes logic probability via attention between predecessors and node states, then computes transition probability via attention between current and previous state representations, and concatenates both results.
- **Core assumption**: Logic probabilities depend on predecessor gate states while transition probabilities depend on relationships between current and previous node states.
- **Evidence anchors**: Abstract mentions "dual attention aggregation mechanism to facilitate learning both tasks efficiently" and section describes how it "mimics and learns the computational behavior of logic and transition probabilities at the same time."
- **Break condition**: If transition probabilities don't depend on previous states or logic computation doesn't follow predecessor-based attention patterns.

### Mechanism 2
- **Claim**: The customized propagation scheme removes cycles by treating flip-flops as pseudo-primary inputs and processes information in forward and reverse topological orders to capture temporal correlations.
- **Mechanism**: Flip-flops are temporarily moved to logic level 1 by removing incoming edges, creating a DAG. Forward propagation passes information from primary inputs through combinational logic, reverse propagation passes information backward, and flip-flops are updated last to mimic clock-cycle behavior.
- **Core assumption**: Sequential circuit behavior can be effectively captured by temporarily removing flip-flop cycles and processing in topological order.
- **Evidence anchors**: Section describes moving FFs to logic level 1, propagating information using forward and reverse layers, and copying updated representations of FFs' predecessors to FFs.
- **Break condition**: If flip-flops have complex timing behaviors or circuit structure cannot be simplified to D-FFs.

### Mechanism 3
- **Claim**: Multi-task learning with strongly related supervision (transition and logic probabilities) directs DeepSeq to learn representations that reflect true logical behavior and computational structure.
- **Mechanism**: The model jointly predicts transition probabilities (2D vector for 0→1 and 1→0 transitions) and logic probabilities (1D vector for probability of being logic 1) by minimizing the sum of L1 losses for both tasks.
- **Core assumption**: Transition and logic probabilities are strongly related and their joint supervision helps learn more informative representations.
- **Evidence anchors**: Abstract mentions "multi-task training objective with two sets of strongly related supervision" and section explains that using logic probability as supervision "helps to learn a more informative and accurate sequential circuit representation."
- **Break condition**: If transition and logic probabilities are not sufficiently correlated or one task dominates the learning process.

## Foundational Learning

- **Graph Neural Networks (GNNs)**
  - Why needed here: Circuits are naturally represented as graphs where gates and connections form nodes and edges, and GNNs can effectively learn representations from such structured data.
  - Quick check question: What is the difference between convolution sum and attention aggregation in GNNs, and when would each be preferred?

- **Directed Acyclic Graphs (DAGs) and Topological Ordering**
  - Why needed here: Sequential circuits with flip-flops form cyclic graphs, but by removing flip-flop cycles and processing in topological order, DeepSeq can effectively propagate information while maintaining the temporal structure.
  - Quick check question: How does topological ordering ensure that all predecessors of a node are processed before the node itself?

- **Multi-task Learning**
  - Why needed here: Jointly learning transition and logic probabilities provides richer supervision than single-task learning, helping the model capture both structural and temporal aspects of sequential circuits.
  - Quick check question: What are the advantages and potential drawbacks of multi-task learning compared to single-task learning?

## Architecture Onboarding

- **Component map**: Circuit in AIG format -> Custom DAG-GNN with forward/reverse layers and dual attention aggregation -> GRU-based combine function for hidden state updates -> Multi-task MLP regressor for transition and logic probability prediction

- **Critical path**:
  1. Convert sequential circuit to AIG format with AND gates, NOT gates, PIs, and FFs
  2. Remove FF cycles and initialize node embeddings
  3. Perform forward propagation (topological order) through combinational logic
  4. Perform reverse propagation (reverse topological order)
  5. Update FF states to mimic clock-cycle behavior
  6. Predict transition and logic probabilities via separate MLPs
  7. Compute multi-task loss and update model parameters

- **Design tradeoffs**:
  - Single propagation vs recursive architecture: Single propagation cannot capture complex circuit information, so recursive architecture is needed but increases computation
  - Simple vs dual attention: Dual attention captures both logic and transition probabilities but is more complex than single attention
  - Fixed vs dynamic feature initialization: Using workload information for PIs provides useful initialization but requires additional preprocessing

- **Failure signatures**:
  - Poor performance on transition probability prediction: May indicate issues with temporal correlation capture or dual attention mechanism
  - Poor performance on logic probability prediction: May indicate issues with combinational logic modeling or basic GNN propagation
  - Slow training or inference: May indicate inefficiencies in the customized propagation scheme or model architecture

- **First 3 experiments**:
  1. Test basic GNN performance on combinational circuits (without FFs) to establish baseline
  2. Test custom propagation scheme with simple attention on sequential circuits to isolate the effect of the propagation scheme
  3. Test dual attention aggregation with the custom propagation scheme to evaluate the combined effect on both tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can DeepSeq's architecture be adapted to handle sequential circuits containing FFs other than D-FFs (e.g., JK-FF, T-FF) without significant loss of accuracy?
- **Basis in paper**: Explicit claim that DeepSeq "is also applicable to the circuits containing other kinds of FFs" since other FF types can be converted into a combination of DFF and combinational logic.
- **Why unresolved**: No experimental validation provided for this claim. Unclear how accurately the conversion preserves temporal behavior captured by DeepSeq's specialized propagation scheme.
- **What evidence would resolve it**: Experiments comparing DeepSeq's performance on benchmark circuits with various FF types, both before and after conversion to DFF-based equivalents.

### Open Question 2
- **Question**: How does the accuracy of DeepSeq's power estimation scale with circuit size beyond the 1-2 order-of-magnitude larger circuits tested in the paper?
- **Basis in paper**: Inferred from demonstration on circuits 10-100x larger than pre-training data but not tested on circuits 3x or larger.
- **Why unresolved**: Experiments limited to circuits up to ~18k nodes. Unclear whether performance gains (80.67% error reduction) would persist or degrade for much larger industrial-scale designs.
- **What evidence would resolve it**: Testing DeepSeq on state-of-the-art industrial circuits with hundreds of thousands or millions of nodes.

### Open Question 3
- **Question**: Can the slow inference speed of DeepSeq (3-4x slower than commercial tools) be sufficiently improved to enable practical deployment without sacrificing accuracy?
- **Basis in paper**: Explicit identification of levelized, sequential propagation as the bottleneck and suggestion of PACE as potential solution.
- **Why unresolved**: No experimental results showing effectiveness of parallelization techniques like PACE. Trade-off between speed and accuracy remains unquantified.
- **What evidence would resolve it**: Implementing and benchmarking DeepSeq with parallelizable architectures (e.g., PACE) on the same test circuits.

## Limitations

- The custom propagation scheme's effectiveness depends on the assumption that sequential circuits can be adequately modeled by treating flip-flops as pseudo-primary inputs, which may not hold for circuits with complex timing behaviors.
- The dual attention mechanism's design choices (weight sharing, attention function specifics) are not fully detailed, making it difficult to assess whether reported improvements are due to architectural advantages or implementation specifics.
- The evaluation focuses on relatively small benchmark circuits (150-300 nodes), raising questions about scalability to industrial-scale designs.

## Confidence

- **High Confidence**: The core architecture of DeepSeq as a DAG-GNN with customized propagation and dual attention is well-defined and technically sound. The multi-task learning approach for transition and logic probability prediction is theoretically justified.
- **Medium Confidence**: The reported performance improvements (20% and 15.8% error reductions) are based on benchmark evaluations, but exact architectural details needed for reproduction are partially unspecified. The power estimation results show strong improvements but rely on fine-tuning that may not generalize.
- **Low Confidence**: The scalability claims to larger circuits (1k-10k nodes) are demonstrated through power estimation fine-tuning, but the model's ability to learn effective representations for truly large-scale industrial circuits remains unverified.

## Next Checks

1. **Architectural Reproducibility Check**: Implement DeepSeq with the specified dual attention mechanism and multi-task learning objective on a small sequential circuit benchmark. Compare the learned representations and prediction accuracy against reported results to verify core architectural claims.

2. **Scalability Validation**: Test DeepSeq on sequential circuits larger than 1,000 nodes (e.g., 2,000-5,000 node circuits) to evaluate whether performance gains observed in the paper scale proportionally. Measure both prediction accuracy and computational efficiency.

3. **Generalization Assessment**: Apply the learned representations from DeepSeq to a downstream EDA task not used during training (e.g., timing analysis or fault detection) on the same benchmark circuits. This would validate whether representations capture circuit semantics beyond specific tasks they were trained on.