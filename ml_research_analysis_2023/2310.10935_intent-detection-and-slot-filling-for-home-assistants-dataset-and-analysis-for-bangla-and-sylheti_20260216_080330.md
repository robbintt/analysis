---
ver: rpa2
title: 'Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis
  for Bangla and Sylheti'
arxiv_id: '2310.10935'
source_url: https://arxiv.org/abs/2310.10935
tags:
- intent
- bangla
- slot
- filling
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first comprehensive dataset for intent
  detection and slot filling in formal Bangla, colloquial Bangla, and Sylheti, totaling
  984 samples across 10 intents. The dataset was created by translating and manually
  correcting the English SNIPS dataset into the three languages, with a focus on capturing
  colloquial forms.
---

# Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis for Bangla and Sylheti

## Quick Facts
- arXiv ID: 2310.10935
- Source URL: https://arxiv.org/abs/2310.10935
- Reference count: 13
- First comprehensive dataset for intent detection and slot filling in Bangla and Sylheti languages, totaling 984 samples across 10 intents

## Executive Summary
This paper introduces the first comprehensive dataset for intent detection and slot filling in formal Bangla, colloquial Bangla, and Sylheti, totaling 984 samples across 10 intents. The dataset was created by translating and manually correcting the English SNIPS dataset into the three languages, with a focus on capturing colloquial forms. Inter-annotator agreement was ensured through rigorous annotation and review processes. The authors evaluate the dataset using GPT-3.5 and JointBERT models for intent detection and slot filling tasks. GPT-3.5 significantly outperforms JointBERT, achieving F1 scores of 0.94 for intent detection and 0.51 for slot filling in colloquial Bangla. The results highlight the effectiveness of large language models for low-resource languages, even with limited training data.

## Method Summary
The study translates the English SNIPS dataset into formal Bangla, colloquial Bangla, and Sylheti, creating a total of 984 samples across 10 intents. Four annotators perform the translation and annotation process, ensuring inter-annotator agreement through Cohen's Kappa and BLEU metrics. The dataset is split into 80-10-10 train-dev-test sets. Two models are evaluated: JointBERT (a BERT-based model) and GPT-3.5 (using few-shot learning with 5 examples). The evaluation focuses on intent detection (accuracy and F1) and slot filling (F1) tasks.

## Key Results
- GPT-3.5 significantly outperforms JointBERT on intent detection, achieving F1 scores of 0.94 for colloquial Bangla versus 0.68 for JointBERT
- For slot filling, GPT-3.5 achieves F1 scores of 0.51 for colloquial Bangla versus 0.14 for JointBERT
- GPT-3.5's performance benefits from pre-training on diverse linguistic data including Bangla, which JointBERT lacks
- The slot-filling task performance depends on the quality of intent detection, as slot categories are intent-specific

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3.5's superior performance stems from its pre-training on diverse linguistic data including Bangla, which JointBERT lacks.
- Mechanism: The extensive pre-training corpus of GPT-3.5 includes multilingual data that exposes the model to Bangla language patterns, colloquial expressions, and syntactic structures during the foundational training phase. This prior exposure allows GPT-3.5 to better understand and process Bangla text without requiring extensive task-specific fine-tuning on Bangla data.
- Core assumption: GPT-3.5's pre-training corpus genuinely includes substantial Bangla language data that covers colloquial forms.
- Evidence anchors:
  - [abstract] "GPT-3.5 significantly outperforms JointBERT, achieving F1 scores of 0.94 for intent detection and 0.51 for slot filling in colloquial Bangla"
  - [section] "A significant reason behind GPT-3.5's superior performance is its broader exposure to diverse languages during training, including Bangla"
  - [corpus] Weak - no direct corpus evidence that GPT-3.5 was pre-trained on Bangla data
- Break condition: If GPT-3.5's pre-training corpus did not include Bangla data, or if the Bangla data was limited to formal/standard forms that don't capture colloquial nuances.

### Mechanism 2
- Claim: The few-shot prompting approach with 5 training samples provides sufficient context for GPT-3.5 to understand the intent detection and slot filling tasks.
- Mechanism: GPT-3.5 uses the few-shot examples in the prompt to learn the input-output pattern mapping. By seeing 5 examples of how sentences map to intents and how words map to slots, the model can generalize this pattern to new, unseen sentences without requiring full fine-tuning on the training set.
- Core assumption: Five examples are sufficient for GPT-3.5 to learn the task patterns and generalize effectively.
- Evidence anchors:
  - [section] "We used GPT in a few-shot setting, passing 5 training samples along with the prompt"
  - [section] "Rigorous prompt engineering was performed before settling on the two prompts"
  - [corpus] Weak - no quantitative comparison of different numbers of shots
- Break condition: If the task complexity or domain specificity requires more than 5 examples for effective pattern learning, or if the 5 examples happen to be unrepresentative of the overall data distribution.

### Mechanism 3
- Claim: The slot-filling task performance depends on the quality of intent detection, as slot categories are intent-specific.
- Mechanism: Slot-filling accuracy is enhanced when the correct intent is provided because slot definitions and categories are tied to specific intents. When GPT-3.5 receives the correct intent from GPT-3.5 itself or the original dataset, it can narrow down the relevant slot categories and focus on filling those specific slots rather than considering all possible slots across all intents.
- Core assumption: Slot definitions are truly intent-specific and not overlapping across intents in ways that would confuse the model.
- Evidence anchors:
  - [section] "A significant reason behind GPT-3.5's superior performance is its broader exposure to diverse languages during training, including Bangla"
  - [section] "The slot-filling task is separate from but dependent on the intent detection task"
  - [table] GPT-3.5 with original intent (0.54 F1) outperforms GPT-3.5 with JointBERT intent (0.45 F1)
- Break condition: If slot categories overlap significantly across intents, or if the slot-filling task can be solved without intent context.

## Foundational Learning

- Concept: Inter-annotator agreement metrics (Cohen's Kappa and BLEU)
  - Why needed here: To ensure dataset quality and reliability across multiple annotators working on different language variants
  - Quick check question: What does a Cohen's Kappa score of 0.55 indicate about annotator agreement compared to 0.42?

- Concept: Few-shot learning principles
  - Why needed here: The study relies on GPT-3.5's ability to learn from just 5 examples per task, which is central to the experimental design
  - Quick check question: Why might few-shot learning work better for GPT-3.5 than for smaller models on low-resource languages?

- Concept: Slot-filling dependency on intent detection
  - Why needed here: The experimental results show that slot-filling performance varies based on the source of intent information
  - Quick check question: How does providing the correct intent to the slot-filling model improve its performance compared to letting it predict the intent independently?

## Architecture Onboarding

- Component map: Dataset creation (translation and annotation pipeline) -> Model training/evaluation (JointBERT and GPT-3.5) -> Performance analysis. The dataset creation involves four annotators with quality control through inter-annotator agreement, blind overlap, and independent adjudication.

- Critical path: Data generation → Model training/evaluation → Performance analysis. The data generation phase is critical because dataset quality directly impacts model performance. The evaluation phase must compare models across both tasks using consistent train/dev/test splits.

- Design tradeoffs: Using GPT-3.5 in few-shot mode trades computational cost (API calls) for data efficiency, avoiding the need for large training datasets. JointBERT requires more data but provides local deployment capability. The choice of 5-shot examples represents a balance between prompt length limits and learning effectiveness.

- Failure signatures: JointBERT performs poorly on slot-filling (F1 scores of 0.11-0.14) indicating the BERT-based approach struggles with this task on low-resource languages. Large performance gaps between GPT-3.5 with original vs. predicted intents (0.54 vs 0.45 F1) indicate sensitivity to intent quality.

- First 3 experiments:
  1. Run JointBERT on the full dataset for both intent detection and slot filling to establish baseline performance
  2. Test GPT-3.5 for intent detection with 5-shot prompts while using JointBERT for slot filling to isolate performance differences
  3. Test GPT-3.5 for both tasks with different intent sources (GPT-3.5, JointBERT, original) to measure dependency effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would GPT-3.5 and JointBERT models perform on intent detection and slot filling for Bangla and Sylheti if trained on larger datasets?
- Basis in paper: [inferred] The authors mention that their dataset is smaller compared to established benchmarks, and they prioritize precise and robust data generation over volume. They also note that the performance difference between GPT-3.5 and JointBERT could be due to GPT-3.5's broader exposure to diverse languages during training.
- Why unresolved: The paper does not explore the impact of training data size on model performance, focusing instead on the quality of the dataset.
- What evidence would resolve it: Experiments comparing model performance on datasets of varying sizes, particularly with larger datasets for Bangla and Sylheti, would provide insights into the relationship between training data size and model effectiveness.

### Open Question 2
- Question: How would the performance of GPT-3.5 and JointBERT models change if optimized models for Bangla were used in the experiments?
- Basis in paper: [explicit] The authors mention that the dearth of optimized Bangla models for specific tasks posed challenges, and an attempt with a Bangla BERT tokenizer didn't yield satisfactory outcomes, affecting the JointBERT's efficacy.
- Why unresolved: The paper does not explore the impact of using optimized models for Bangla on the performance of intent detection and slot filling tasks.
- What evidence would resolve it: Experiments using optimized Bangla models for both GPT-3.5 and JointBERT, and comparing their performance to the results presented in the paper, would provide insights into the importance of language-specific optimization.

### Open Question 3
- Question: How would the results of intent detection and slot filling for Bangla and Sylheti change if evaluated on diverse settings or other languages?
- Basis in paper: [inferred] The authors note that their results are tied to their specific dataset and context, and extending their findings to diverse settings or other languages requires further exploration.
- Why unresolved: The paper focuses on Bangla and Sylheti, and does not explore the generalizability of their findings to other languages or settings.
- What evidence would resolve it: Experiments evaluating the performance of GPT-3.5 and JointBERT models on intent detection and slot filling tasks for other low-resource languages, or in different contexts, would provide insights into the generalizability of the results presented in the paper.

## Limitations

- The dataset size (984 total samples) is relatively small for comprehensive evaluation, potentially limiting the robustness of the results
- The translation process from English SNIPS may introduce artifacts that don't reflect natural Bangla usage
- The study doesn't explore the impact of training data size on model performance, focusing instead on dataset quality
- The performance gap between GPT-3.5 with original intents (0.54 F1) versus predicted intents (0.45 F1) for slot filling suggests significant dependency on intent quality

## Confidence

- **High Confidence**: GPT-3.5 outperforms JointBERT on intent detection (F1 scores of 0.94 vs 0.61-0.68)
- **Medium Confidence**: GPT-3.5's few-shot learning approach works for low-resource languages, but the specific mechanism (pre-training vs prompt engineering) is unclear
- **Low Confidence**: The results generalize to other low-resource languages beyond Bangla and Sylheti, or to different intent detection/slot filling domains

## Next Checks

1. **Cross-language validation**: Test the same experimental setup (JointBERT and GPT-3.5 few-shot) on another low-resource language dataset to determine if the performance pattern holds or if it's specific to Bangla/Sylheti.

2. **Prompt engineering ablation**: Systematically vary the number of few-shot examples (1, 3, 5, 10) and prompt formats to quantify how much performance depends on the specific prompting strategy versus model capability.

3. **Annotation quality verification**: Re-annotate a random 10% sample of the dataset using independent annotators to verify the reported Cohen's Kappa scores and ensure the dataset quality supports the claimed model performance.