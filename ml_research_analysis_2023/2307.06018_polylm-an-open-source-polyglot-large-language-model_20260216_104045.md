---
ver: rpa2
title: 'PolyLM: An Open Source Polyglot Large Language Model'
arxiv_id: '2307.06018'
source_url: https://arxiv.org/abs/2307.06018
tags:
- tasks
- language
- multilingual
- languages
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces POLYLM, a multilingual large language model
  trained on 640B tokens across 18 languages, available in 1.7B and 13B parameter
  versions. To improve multilingual performance, the authors use bilingual data and
  a curriculum learning strategy that increases non-English data from 30% to 60% during
  training.
---

# PolyLM: An Open Source Polyglot Large Language Model

## Quick Facts
- arXiv ID: 2307.06018
- Source URL: https://arxiv.org/abs/2307.06018
- Reference count: 40
- Primary result: POLYLM outperforms LLaMA and BLOOM on multilingual tasks while maintaining English performance

## Executive Summary
POLYLM is a multilingual large language model available in 1.7B and 13B parameter versions, trained on 640B tokens across 18 languages. The model employs a curriculum learning strategy that progressively increases non-English data from 30% to 60% during training, and is fine-tuned on a self-generated multilingual instruction dataset (MULTIALPACA) of 132.7K samples. Experiments demonstrate that POLYLM achieves superior multilingual performance compared to open-source baselines like LLaMA and BLOOM while maintaining strong English capabilities.

## Method Summary
POLYLM uses a decoder-only transformer architecture with 256K token vocabulary and is trained using a two-stage curriculum learning approach. The first stage trains on the full pre-training dataset, while the second stage focuses on higher-quality data with increased multilingual content (60% non-English). The model is fine-tuned on MULTIALPACA, a multilingual instruction dataset generated through self-instruct using ChatGPT API queries. Training was conducted using Megatron-LM on 32 A100 GPUs with bfloat16 mixed-precision, gradient clipping at 1.0, and a learning rate schedule with warmup to 6e-5 followed by cosine decay.

## Key Results
- POLYLM-13B significantly outperforms LLaMA-13B and BLOOM-176B on multilingual benchmarks
- The curriculum learning strategy improves multilingual performance by gradually increasing non-English data from 30% to 60%
- Multilingual instruction tuning with self-generated data enhances zero-shot task performance across 15 languages

## Why This Works (Mechanism)

### Mechanism 1: Curriculum Learning
- Claim: Progressive increase in non-English data improves multilingual performance
- Core assumption: English linguistic knowledge transfers effectively to other languages when they become more prominent in training
- Evidence: Training transitions from 30% to 60% non-English data across two stages
- Break condition: Initial English focus creates too strong a language-specific bias

### Mechanism 2: Multilingual Self-Instruct
- Claim: Self-generated multilingual instructions improve zero-shot multilingual task performance
- Core assumption: Self-generated instructions capture necessary diversity for instruction following
- Evidence: 132.7K multilingual instruction samples generated iteratively through ChatGPT API
- Break condition: Self-generated instructions lack sufficient quality or diversity

### Mechanism 3: Large Vocabulary Design
- Claim: 256K token vocabulary improves compression and representation of low-resource languages
- Core assumption: Larger vocabulary with more tokens for low-resource languages improves their representation
- Evidence: Superior compression rates achieved for most covered languages while maintaining English performance
- Break condition: Computational inefficiency outweighs vocabulary benefits

## Foundational Learning

- **Autoregressive language modeling**: Why needed - POLYLM predicts next token given previous context; Quick check - What is the fundamental task during training?
- **Curriculum learning**: Why needed - Gradually increases non-English data to improve multilingual performance; Quick check - How does the two-stage training process work?
- **Byte-Pair Encoding (BPE) tokenization**: Why needed - Creates 256K vocabulary for better low-resource language representation; Quick check - Why split all numbers into individual digits?

## Architecture Onboarding

- **Component map**: Tokenized text → Embedding → Transformer layers → Attention → Feed-forward → Output distribution
- **Critical path**: Tokenization → Embedding → Transformer layers → Attention → Feed-forward → Output distribution
- **Design tradeoffs**: Large vocabulary (256K) vs computational efficiency; Curriculum learning vs straightforward multilingual training; Self-instruct data vs human-annotated data
- **Failure signatures**: Loss spikes during training; Poor multilingual performance despite English proficiency; Slow convergence
- **First 3 experiments**: 
  1. Verify tokenization and vocabulary coverage across all 18 target languages
  2. Test curriculum learning schedule with varying language proportions
  3. Evaluate multilingual instruction following on small self-generated task sample

## Open Questions the Paper Calls Out

1. **Performance scaling with model size**: How does POLYLM performance scale for low-resource languages across different model sizes? The paper only compares 1.7B and 13B models without comprehensive scaling analysis.

2. **Zero-shot cross-lingual transfer**: What is the impact of curriculum learning on zero-shot performance for languages not included in training? The paper focuses on included languages without addressing generalization to unseen languages.

3. **Self-instruct vs human data quality**: How does MULTIALPACA quality compare to human-annotated instruction data in terms of model performance and task diversity? The paper demonstrates MULTIALPACA effectiveness but lacks direct comparison to human data.

## Limitations

- Limited ablation studies to isolate curriculum learning contributions
- Self-instruct methodology lacks detailed quality control specifications
- Vocabulary improvements demonstrated through compression rather than downstream task performance
- No comprehensive analysis of performance scaling across model sizes

## Confidence

**High Confidence**: Architecture specifications, pre-training dataset composition, overall multilingual performance improvements
**Medium Confidence**: Curriculum learning strategy effectiveness, multilingual instruction tuning benefits, specific benchmark performance numbers
**Low Confidence**: Detailed data filtering methodology, exact self-instruct implementation details, comparative analysis with very recent models

## Next Checks

1. **Curriculum Learning Validation**: Replicate two-stage training with controlled experiments varying non-English data proportions (30%, 45%, 60, 75%) to isolate optimal schedule and test English-to-multilingual knowledge transfer.

2. **Self-Instruct Data Quality Assessment**: Generate small multilingual instruction dataset using described method with specific seed tasks, then conduct human evaluation on instruction diversity, quality, and task complexity across 18 target languages.

3. **Vocabulary Design Impact Study**: Conduct ablation experiments comparing 256K vocabulary with smaller vocabularies (128K, 64K) on multilingual downstream tasks, focusing on low-resource language performance to verify practical task improvements.