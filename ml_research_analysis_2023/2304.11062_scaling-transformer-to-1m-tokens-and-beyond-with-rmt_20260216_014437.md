---
ver: rpa2
title: Scaling Transformer to 1M tokens and beyond with RMT
arxiv_id: '2304.11062'
source_url: https://arxiv.org/abs/2304.11062
tags:
- memory
- tokens
- transformer
- input
- recurrent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the challenge of scaling transformers to handle
  extremely long input sequences by proposing a recurrent memory mechanism that enables
  linear computational scaling with sequence length. The core method involves augmenting
  pre-trained transformer models like BERT with a token-based recurrent memory system,
  where memory tokens are passed between segments of the input sequence.
---

# Scaling Transformer to 1M tokens and beyond with RMT

## Quick Facts
- arXiv ID: 2304.11062
- Source URL: https://arxiv.org/abs/2304.11062
- Reference count: 10
- Authors: 
- Key outcome: Linear computational scaling for transformers handling sequences up to 2 million tokens using recurrent memory tokens

## Executive Summary
This paper addresses the challenge of scaling transformers to handle extremely long input sequences by proposing a recurrent memory mechanism that enables linear computational scaling with sequence length. The core method involves augmenting pre-trained transformer models like BERT with a token-based recurrent memory system, where memory tokens are passed between segments of the input sequence. Through curriculum learning on progressively longer sequences, the model demonstrates the ability to extrapolate to sequences up to 2 million tokens long while maintaining high memory retrieval accuracy. The approach achieves linear computational scaling compared to the quadratic scaling of standard transformers, with memory requirements remaining constant regardless of input length.

## Method Summary
The authors propose augmenting pre-trained transformers with a token-based recurrent memory system that enables linear scaling for long sequences. The model processes input in fixed-size segments (512 tokens) with trainable memory vectors prepended to each segment. These memory vectors capture and maintain context across segments, allowing the transformer to handle arbitrarily long sequences without quadratic memory growth. The model is trained using curriculum learning, starting with short sequences and progressively increasing length. Experiments use synthetic datasets requiring memorization of facts and basic reasoning, with the RMT model demonstrating successful extrapolation to sequences up to 2 million tokens while maintaining high memory retrieval accuracy.

## Key Results
- Achieved linear computational scaling with sequence length while maintaining constant memory requirements
- Successfully extrapolated to sequences up to 2 million tokens beyond training lengths
- Maintained high memory retrieval accuracy across extended sequence lengths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-based memory augmentation enables transformers to maintain context across segments without quadratic memory growth.
- Mechanism: The model prepends trainable memory vectors to each input segment, allowing the backbone transformer to process long sequences in fixed-size chunks while passing updated memory between segments.
- Core assumption: Memory vectors can learn to store and retrieve relevant information across segments through backpropagation.
- Evidence anchors:
  - [abstract] "augmenting pre-trained transformer models like BERT with a token-based recurrent memory system, where memory tokens are passed between segments of the input sequence"
  - [section] "memory, composed of m real-valued trainable vectors... are prepended to the first segment embeddings and processed alongside the segment tokens"
  - [corpus] Weak evidence - no direct corpus papers found on token-based memory augmentation
- Break condition: If memory vectors fail to capture task-relevant information or become unstable during training, the recurrent connection breaks down and context is lost between segments.

### Mechanism 2
- Claim: Curriculum learning on progressively longer sequences enables effective extrapolation to extremely long inputs.
- Mechanism: The model starts training on short sequences that fit within a single segment, then gradually increases sequence length by adding segments as training progresses.
- Core assumption: Learning to solve shorter versions of the task provides a foundation for solving longer versions through generalization.
- Evidence anchors:
  - [section] "using a training schedule greatly improves solution accuracy and stability... Initially, RMT is trained on shorter versions of the task, and upon training convergence, the task length is increased by adding one more segment"
  - [section] "after training on shorter tasks, it is easier for RMT to solve longer versions as it converges to the perfect solution using fewer training steps"
  - [corpus] Weak evidence - no direct corpus papers found on curriculum learning for long-sequence transformers
- Break condition: If the model overfits to specific sequence lengths during training, it may fail to generalize to longer or shorter sequences during inference.

### Mechanism 3
- Claim: Recurrent memory enables linear computational scaling while maintaining quadratic attention within segments.
- Mechanism: By dividing input into fixed-size segments and computing full attention only within segment boundaries, the model achieves linear scaling with sequence length.
- Core assumption: Segment size can be fixed while sequence length varies, allowing linear scaling of computations.
- Evidence anchors:
  - [section] "RMT scales linearly for any model size if the segment length is fixed... We achieve linear scaling by dividing an input sequence into segments and computing the full attention matrix only within segment boundaries"
  - [section] "RMT can run OPT-175B with ×29 fewer FLOPs and with ×295 fewer FLOPs than OPT-135M"
  - [corpus] Weak evidence - no direct corpus papers found on computational complexity analysis of recurrent memory transformers
- Break condition: If segment size becomes too small relative to sequence length, the number of segments grows large enough that linear scaling advantage diminishes.

## Foundational Learning

- Concept: Transformer attention mechanism
  - Why needed here: Understanding how attention works is crucial for grasping why quadratic complexity becomes problematic for long sequences and how segment-level attention provides a solution
  - Quick check question: What is the computational complexity of standard self-attention and why does it become prohibitive for long sequences?

- Concept: Curriculum learning strategy
  - Why needed here: The training approach used here differs from standard end-to-end training and requires understanding how gradual progression helps model learn complex tasks
  - Quick check question: How does starting with shorter sequences and gradually increasing length help the model learn to handle extremely long sequences?

- Concept: Recurrent neural network principles
  - Why needed here: The recurrent memory mechanism builds on concepts from RNNs, where information flows between time steps or segments
  - Quick check question: How does the recurrent memory mechanism in RMT differ from traditional RNN recurrence, and what advantages does it offer?

## Architecture Onboarding

- Component map: Input -> Segment division -> Memory augmentation -> Segment processing -> Memory update -> Next segment -> Output prediction
- Critical path: Input → Segment division → Memory augmentation → Segment processing → Memory update → Next segment → Output prediction
- Design tradeoffs:
  - Fixed segment size vs. adaptive sizing for different sequence lengths
  - Number of memory vectors vs. memory capacity and computational overhead
  - Curriculum learning schedule speed vs. training stability
  - Memory update frequency vs. information retention
- Failure signatures:
  - Memory vectors become uninformative (uniform or random values)
  - Model fails to improve with additional training segments
  - Attention patterns show no meaningful interaction between memory and input tokens
  - Extrapolation to longer sequences shows sharp performance degradation
- First 3 experiments:
  1. Implement basic RMT with fixed segment size and minimal memory vectors, train on single-segment memorization task to verify core mechanism works
  2. Add curriculum learning by gradually increasing sequence length, verify performance improvement on longer sequences
  3. Test extrapolation by evaluating trained model on sequences longer than those seen during training, measure memory retrieval accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental limits of sequence length that RMT can handle before performance degradation becomes unacceptable?
- Basis in paper: [explicit] The paper mentions extrapolation to 2 million tokens but notes reasoning tasks become most complex, suggesting there may be practical limits.
- Why unresolved: The paper only tested up to 2 million tokens and did not systematically explore where performance breaks down or what the theoretical limits might be.
- What evidence would resolve it: Systematic experiments testing RMT on progressively longer sequences (10M, 100M tokens) while measuring accuracy and computational efficiency to identify the point where performance degrades beyond usability thresholds.

### Open Question 2
- Question: How does the optimal memory size scale with sequence length and task complexity?
- Basis in paper: [inferred] The paper uses a fixed memory size of 10 but does not explore how memory requirements change with different sequence lengths or task types.
- Why unresolved: The paper does not investigate whether larger memory sizes improve performance on longer sequences or whether there's a point of diminishing returns for memory allocation.
- What evidence would resolve it: Experiments varying memory sizes (5, 10, 20, 50 tokens) across different sequence lengths and task complexities to determine the relationship between memory allocation and performance.

### Open Question 3
- Question: What are the specific attention mechanisms that RMT uses to handle long-range dependencies, and can they be further optimized?
- Basis in paper: [explicit] The paper analyzes attention patterns showing specific operations like "Detect fact 1", "write to memory", "retain memory", but does not explore whether these patterns could be optimized.
- Why unresolved: While the paper identifies attention patterns, it does not investigate whether these patterns are optimal or if alternative attention mechanisms could improve performance.
- What evidence would resolve it: Comparative studies testing different attention mechanisms (sparse attention, adaptive attention heads, hierarchical attention) on the same tasks to determine if alternative patterns yield better performance.

## Limitations

- Memory mechanism generalizability remains unproven on real-world heterogeneous data beyond controlled synthetic datasets
- Computational overhead scaling effects are not fully characterized for larger models and different hardware configurations
- Curriculum learning dependency makes it unclear whether benefits stem from memory architecture itself versus training methodology

## Confidence

**High Confidence Claims**:
- The RMT architecture can handle sequences up to 2 million tokens when properly trained with curriculum learning
- Linear computational scaling is achieved through segment-based processing with fixed-size segments
- Memory retrieval accuracy improves as more segments are processed when using the proposed training approach

**Medium Confidence Claims**:
- The memory vectors effectively capture and maintain context across segments
- Extrapolation to sequences longer than training length is robust and reliable
- The approach generalizes to reasoning tasks beyond simple memorization

**Low Confidence Claims**:
- RMT will perform similarly well on real-world NLP tasks with complex dependencies
- The memory mechanism provides advantages over other long-sequence transformer variants in practical settings
- The computational benefits remain significant when accounting for all implementation overhead

## Next Checks

1. **Cross-Task Generalization Test**: Evaluate RMT on established long-sequence benchmarks like PG-19, Arxiv, and real-world document processing tasks to assess performance beyond synthetic datasets. This would validate whether the memory mechanism generalizes to natural language patterns and varying information densities.

2. **Memory Stability Analysis**: Track memory vector evolution across training and inference, measuring cosine similarity and information entropy of memory states over time. This would reveal whether memory tokens maintain stable, task-relevant representations or degrade during extended processing.

3. **Ablation Study on Curriculum vs Memory**: Train identical models with and without the memory mechanism but identical curriculum learning schedules, measuring relative contributions to performance improvements. This would isolate whether observed benefits stem from the memory architecture itself or primarily from the training methodology.