---
ver: rpa2
title: Benchmarking the Generation of Fact Checking Explanations
arxiv_id: '2308.15202'
source_url: https://arxiv.org/abs/2308.15202
tags:
- claim
- article
- verdict
- extractive
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks the generation of fact-checking explanations
  using summarization techniques on two novel datasets (LIAR++ and FullFact). It evaluates
  extractive, abstractive, and hybrid approaches for producing justifications from
  news articles given a claim.
---

# Benchmarking the Generation of Fact Checking Explanations

## Quick Facts
- arXiv ID: 2308.15202
- Source URL: https://arxiv.org/abs/2308.15202
- Reference count: 23
- Primary result: Combining claim-driven extractive and abstractive steps outperforms either approach alone for generating fact-checking justifications

## Executive Summary
This paper benchmarks the generation of fact-checking explanations using summarization techniques on two novel datasets (LIAR++ and FullFact). The study evaluates extractive, abstractive, and hybrid approaches for producing justifications from news articles given a claim. Key findings include that claim-driven extractive summarization improves results over relevance-based methods, combining extractive and abstractive steps outperforms either alone, and models benefit from claim information in input. The optimal model choice depends on article length, with single models trained on both datasets performing comparably to separate models, indicating style retention.

## Method Summary
The study benchmarks fact-checking explanation generation by comparing multiple summarization approaches on two datasets containing claim-verdict-article triplets. Extractive methods include text truncation, LexRank, and SBERT-based sentence ranking by similarity to claims. Abstractive methods use pre-trained language models (T5, Pegasus variants, DistilBart) fine-tuned on datasets with different input configurations (article-only vs claim+article). Hybrid approaches combine extractive selection with abstractive generation. Models are evaluated using ROUGE-1, ROUGE-2, and ROUGE-L scores comparing generated justifications to gold verdicts, with experiments testing different input lengths (512 vs 1024 tokens) and decoding strategies.

## Key Results
- Claim-driven extractive summarization using SBERT to rank sentences by similarity to claims improves results over relevance-based methods
- Combining extractive and abstractive steps in a pipeline outperforms either approach alone
- Language models benefit from claim information in input during fine-tuning
- Optimal model choice depends on article length (512 vs 1024 tokens)
- Single models trained on both datasets perform comparably to separate models, indicating style retention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Claim-driven extractive summarization improves results over relevance-based methods
- Mechanism: Using SBERT to rank sentences by similarity to the claim selects more relevant content for justification generation
- Core assumption: Sentences most similar to the claim contain the most relevant information for generating justifications
- Evidence anchors:
  - [abstract] "claim-driven extractive summarization improves results over relevance-based methods"
  - [section] "Results show that in justification production summarization benefits from the claim information"
  - [corpus] Weak - only 5 related papers found, none specifically address claim-driven extraction
- Break condition: If claims contain misleading information or are poorly worded, the extracted sentences may not be the most relevant for justification

### Mechanism 2
- Claim: Combining extractive and abstractive steps outperforms either alone
- Mechanism: Extractive step reduces input length while preserving key information, allowing abstractive model to generate more coherent justifications
- Core assumption: Abstractive models can generate better summaries when given a reduced but still informative input
- Evidence anchors:
  - [abstract] "combining extractive and abstractive steps in a unique pipeline outperforms either alone"
  - [section] "Results show that in justification production summarization benefits from the claim information, and, in particular, that a claim-driven extractive step improves abstractive summarization performances"
  - [corpus] Weak - no direct evidence found in corpus neighbors
- Break condition: If extractive step removes too much context, abstractive model may generate hallucinated or irrelevant content

### Mechanism 3
- Claim: Single model trained on both datasets performs comparably to separate models
- Mechanism: Large language models can learn to retain style information from multiple datasets when fine-tuned together
- Core assumption: Language models have sufficient capacity to distinguish and reproduce different writing styles
- Evidence anchors:
  - [abstract] "single model trained on both datasets performs comparably to separate models, indicating style retention"
  - [section] "LM-based models are able to retain different verdict styles: fine-tuning a single LM on the union of datasets with different stylistic characteristics leads to performance similar to those obtained by fine-tuning a model for every single dataset"
  - [corpus] Weak - only 5 related papers found, none specifically address multi-dataset style retention
- Break condition: If datasets have very different structures or vocabulary, single model may not learn to distinguish styles effectively

## Foundational Learning

- Concept: Text summarization techniques (extractive vs abstractive)
  - Why needed here: Core task is generating justifications through summarization
  - Quick check question: What's the difference between extractive and abstractive summarization?

- Concept: Sentence similarity and ranking methods
  - Why needed here: SBERT is used to rank sentences by similarity to claims
  - Quick check question: How does SBERT calculate sentence similarity?

- Concept: Language model fine-tuning strategies
  - Why needed here: Models are fine-tuned with different configurations (article vs claim+article)
  - Quick check question: What's the difference between unsupervised, article, and claim+article fine-tuning?

## Architecture Onboarding

- Component map: Input (Claim + Article) → Extractive selection (SBERT ranking) → Reduction (top N sentences) → Abstractive generation (fine-tuned LM) → Output (Justification)

- Critical path: Claim → Extractive selection → Abstractive generation → Justification

- Design tradeoffs:
  - Model size vs performance: 512 vs 1024 token input models
  - Extractive vs abstractive: Tradeoff between faithfulness and coherence
  - Fine-tuning data: Article-only vs claim+article configuration

- Failure signatures:
  - Low ROUGE scores: Likely issues with extractive selection or model capacity
  - Hallucinations: Abstractive model generating unsupported content
  - Style mismatch: Model not adapting to dataset characteristics

- First 3 experiments:
  1. Compare SBERT vs LexRank for extractive selection on a small sample
  2. Test claim+article vs article-only fine-tuning on a validation set
  3. Evaluate 512 vs 1024 token models on short vs long articles

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do hybrid extractive-abstractive models compare to end-to-end abstractive models in terms of hallucination rates and factuality preservation?
- Basis in paper: [explicit] The paper mentions that abstractive models suffer from hallucination issues, and hybrid approaches combining extractive and abstractive steps are proposed as a solution.
- Why unresolved: The paper focuses on ROUGE-based evaluation of output quality but does not specifically measure or compare hallucination rates or factuality preservation between hybrid and end-to-end approaches.
- What evidence would resolve it: Quantitative analysis comparing factual consistency metrics (e.g., FEVER score) and hallucination detection between hybrid and end-to-end models.

### Open Question 2
- Question: How does the optimal summarization approach vary with different claim types (e.g., numerical claims vs. subjective claims)?
- Basis in paper: [inferred] The paper uses datasets with various claim types but does not analyze whether different summarization strategies are optimal for different claim categories.
- Why unresolved: The experimental results are aggregated across all claim types without examining potential variations in optimal approaches for different claim categories.
- What evidence would resolve it: Comparative analysis of model performance stratified by claim type (numerical, subjective, policy-related, etc.) to identify if certain approaches work better for specific claim categories.

### Open Question 3
- Question: What is the impact of including external knowledge sources (beyond the article) on the quality of generated justifications?
- Basis in paper: [explicit] The paper focuses on summarization approaches over unstructured knowledge (news articles) but does not explore the potential benefits of incorporating external knowledge sources.
- Why unresolved: The experimental design is limited to using only the provided article as the knowledge source, without exploring how additional knowledge might improve justification quality.
- What evidence would resolve it: Experiments comparing justification quality when using articles alone versus articles augmented with external knowledge sources (e.g., knowledge graphs, fact databases).

## Limitations
- Study relies entirely on ROUGE metrics without human evaluation, which may not capture factual accuracy
- Sentence extraction methods evaluated only in combination with abstractive models, not as standalone approaches
- Different dataset characteristics (LIAR++ vs FullFact) make direct performance comparisons potentially misleading

## Confidence
- **High Confidence**: Claim-driven extractive summarization outperforms relevance-based methods; hybrid approach combining extractive and abstractive steps shows consistent improvements
- **Medium Confidence**: Single models trained on both datasets perform comparably to separate models; 512-token models work better for shorter articles while 1024-token models work better for longer articles
- **Low Confidence**: Optimal model selection depending on article length is based on limited experimental conditions and doesn't account for other factors like computational efficiency

## Next Checks
1. Conduct human evaluation studies comparing generated justifications against gold verdicts, measuring factual accuracy, coherence, and relevance to assess whether ROUGE scores correlate with human judgment quality

2. Implement hallucination detection metrics to quantify the factual consistency of abstractive generations, particularly focusing on cases where extractive selection may have removed critical context needed for accurate justification

3. Design ablation studies testing the impact of different sentence ordering strategies (article order vs ranking order) on both extractive and abstractive performance, and evaluate whether the claim+article input configuration provides benefits beyond the extractive step alone