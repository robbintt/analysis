---
ver: rpa2
title: Gradient Descent Fails to Learn High-frequency Functions and Modular Arithmetic
arxiv_id: '2310.12660'
source_url: https://arxiv.org/abs/2310.12660
tags:
- gradient
- where
- function
- theorem
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the limitations of gradient-based optimization
  methods in learning high-frequency periodic functions and modular arithmetic operations,
  particularly modular multiplication. The authors focus on the "barren plateau" phenomenon,
  where the variance of the gradient with respect to a randomly chosen target function
  becomes negligibly small, preventing successful learning.
---

# Gradient Descent Fails to Learn High-frequency Functions and Modular Arithmetic

## Quick Facts
- arXiv ID: 2310.12660
- Source URL: https://arxiv.org/abs/2310.12660
- Reference count: 22
- Key outcome: Gradient-based methods cannot learn high-frequency periodic functions and modular multiplication when frequency or prime base grows large due to vanishing gradient variance

## Executive Summary
This paper analyzes why gradient-based optimization methods fail to learn high-frequency periodic functions and modular multiplication operations. The authors introduce a framework that measures gradient variance with respect to hypothesis spaces of target functions, showing that when this variance becomes negligibly small, gradient descent cannot effectively learn. The theoretical analysis proves that both high-frequency periodic functions (with variance O(1/√A)) and modular multiplication (with variance O(1/√p)) exhibit this barren plateau phenomenon as their parameters grow large. The paper also establishes connections to the Statistical Query model, showing these functions are hard to learn by any SQ algorithm, not just gradient-based methods.

## Method Summary
The authors analyze the variance of the gradient with respect to a hypothesis space of target functions, showing that low gradient variance prevents effective learning. For high-frequency periodic functions of the form h_a(x) = ψ(ax) where ψ is 1-periodic, they prove the variance is bounded by O(1/√A) as frequency A increases. For modular multiplication modulo a prime p, they show the variance is bounded by O(1/√p). The theoretical framework uses tools like the Boas-Bellman inequality and orthogonality properties of functions in the hypothesis spaces. Experimental results with neural networks validate the theoretical findings, demonstrating inability to learn these functions for large parameters.

## Key Results
- Gradient variance for high-frequency periodic functions is bounded by O(1/√A), making learning impossible as frequency A grows
- Gradient variance for modular multiplication is bounded by O(1/√p), preventing learning as prime p increases
- These functions are hard not only for gradient methods but also for the entire Statistical Query model, with low SQ dimension bounds
- Experimental results confirm theoretical predictions, showing neural networks fail to learn these functions with high frequencies or large primes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-frequency periodic functions on the real line become unlearnable by gradient descent when the frequency grows large.
- Mechanism: As the frequency parameter A increases, the variance of the gradient with respect to a randomly chosen target function becomes negligibly small. This variance is bounded by O(1/√A), meaning the gradient provides almost no information about the target function's frequency parameter, leading to the barren plateau phenomenon.
- Core assumption: The target function is of the form h_a(x) = ψ(ax) where ψ is 1-periodic with bounded variation.
- Evidence anchors:
  - [abstract] "we highlight that the variance of the gradient is negligibly small in both cases when either a frequency or the prime base p is large"
  - [section] "Theorem 1. There exists a universal constant C > 0 such that Var(HA, w) ≤ C||g(w)||2Ls2([0,1])||ψ||2BV A−1/2(log A + 1)5/2"
  - [corpus] Weak - no direct citations about high-frequency periodic functions in neighbors

### Mechanism 2
- Claim: Modular multiplication modulo a prime p becomes unlearnable by gradient descent when the prime grows large.
- Mechanism: The variance of the gradient with respect to a randomly chosen multiplier a from Z*p is bounded by O(1/√p). This low variance means the gradient carries insufficient information to distinguish between different multipliers, preventing effective learning.
- Core assumption: The target function is of the form h_a(x) = ψ(ax) where ψ is p-periodic and we're working over Z.
- Evidence anchors:
  - [abstract] "we highlight that the variance of the gradient is negligibly small in both cases when either a frequency or the prime base p is large"
  - [section] "Theorem 15. For standardized t(x) = (a·x)− p2q p2 12 − p6 we have Vara∼Z∗p[∇wEx∼Z∗p[(t(a · x) − p(w, x))2]] ≪ ||g(w)||2∗s log p p1/2"
  - [corpus] Weak - neighbors mention modular arithmetic but don't discuss gradient variance bounds

### Mechanism 3
- Claim: The hardness of learning these functions extends beyond gradient-based methods to the entire Statistical Query (SQ) model.
- Mechanism: The low variance of the gradient implies a low SQ dimension for these hypothesis classes. The SQ dimension lower bounds the number of queries needed to learn the function, showing these problems are hard for any SQ algorithm, not just gradient-based methods.
- Core assumption: The variance bounds established for gradient methods translate directly to SQ dimension bounds.
- Evidence anchors:
  - [abstract] "The authors derive lower bounds on the SQ dimension of the hypothesis spaces, indicating that these functions are not only hard to learn by gradient-based methods but also in the general SQ model"
  - [section] "Theorem 21. SQ − dim(H, D) + 1 ≥ min[ |H|2/3/3BB(H, D)1/3 , |H|1/2]"
  - [corpus] Missing - no neighbors discuss SQ dimension or its relationship to gradient methods

## Foundational Learning

- Concept: Variance of gradient as information measure
  - Why needed here: The paper uses gradient variance to quantify how much information the gradient contains about the target function's parameters. Low variance means the gradient is uninformative, preventing learning.
  - Quick check question: If Var(HA, w) = 0, what does this imply about the gradient's ability to distinguish between different target functions in HA?

- Concept: Statistical Query (SQ) model
  - Why needed here: The SQ model provides a framework for understanding the limitations of gradient-based learning. Functions that are hard in the SQ model are also hard for gradient methods.
  - Quick check question: How does the SQ dimension relate to the number of queries needed to learn a function class?

- Concept: Orthogonality and approximate orthogonality
  - Why needed here: The functions in the hypothesis spaces are approximately pairwise orthogonal, which contributes to the low gradient variance. Understanding orthogonality helps explain why gradient methods fail.
  - Quick check question: Why would perfectly orthogonal functions lead to a vanishing gradient variance?

## Architecture Onboarding

- Component map: Parameterized model p(w,x) (neural network) -> Loss function L -> Gradient computation ∇w L -> Parameter update w <- Gradient descent optimizer

- Critical path: 1) Sample target function h_a from hypothesis space 2) Compute gradient of loss with respect to parameters w 3) Check gradient variance 4) Update parameters using gradient descent 5) Repeat until convergence or plateau

- Design tradeoffs: Using higher-capacity models may help with representation but won't solve the fundamental variance problem. Alternative optimization methods might bypass gradient limitations but could be computationally expensive. Restricting hypothesis space to lower frequencies or smaller primes makes learning possible but reduces applicability.

- Failure signatures: Training shows immediate overfitting (gradient vanishing from start) or two-phase pattern with initial fluctuations followed by sudden convergence (grokking). Test loss plateaus at variance of target function distribution rather than decreasing toward zero.

- First 3 experiments:
  1. Train neural network to learn h_a(x) = ψ(ax) for increasing A, measuring final MSE loss and checking if it approaches 1/12 (variance of uniform distribution)
  2. Train neural network to learn parity bit of modular multiplication for increasing prime sizes p, measuring test accuracy and checking if it plateaus below 50%
  3. Compute correlation between multiplications by different numbers modulo p for increasing p, verifying if it decreases as O(1/√p) as predicted

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the variance of the gradient decay even faster than 1/√A or 1/√p, as suggested by experimental results showing a decay rate of 1/A or 1/p?
- Basis in paper: [inferred] The paper notes that experimental results suggest the variance may decay like 1/A or 1/p, rather than the theoretically proven 1/√A or 1/√p.
- Why unresolved: The paper's theoretical bounds are based on the Boas-Bellman inequality and other techniques, which may not capture the full complexity of the problem. The experimental results hint at a potentially tighter bound that remains unproven.
- What evidence would resolve it: Proving a tighter theoretical bound on the variance of the gradient that matches the experimental observations would resolve this question.

### Open Question 2
- Question: Is there a more general theory that unifies the results for high-frequency periodic functions on R and Z, as hinted by the authors?
- Basis in paper: [explicit] The authors state, "We believe that both bounds are edges of a more general theory (which, probably, can be formulated for periodic functions on general abelian groups), but this falls out of the scope of our paper."
- Why unresolved: The authors only provide specific results for the cases of high-frequency periodic functions on R and Z, but suggest a more general theory may exist. Developing such a theory would require a deeper understanding of the underlying mathematical structures.
- What evidence would resolve it: Proving a general theorem that encompasses both the results for R and Z, and potentially extends to other abelian groups, would resolve this question.

### Open Question 3
- Question: What is the relationship between the barren plateau phenomenon in training modular multiplication and aspects of grokking, such as the "Goldilocks" zone and the "LU mechanism"?
- Basis in paper: [explicit] The authors mention that "it is an open problem to study the relationship between the verified barren plateau phenomenon in training modular multiplication and certain aspect of grokking, such as the reported existence of the 'Goldilocks' zone in the weight space of training models and the 'LU mechanism'."
- Why unresolved: While the paper establishes the hardness of learning modular multiplication due to the barren plateau phenomenon, the connection to grokking and its specific aspects remains unexplored.
- What evidence would resolve it: Conducting experiments or developing theoretical models that explicitly link the barren plateau phenomenon to the characteristics of grokking, such as the "Goldilocks" zone and the "LU mechanism," would resolve this question.

## Limitations

- The theoretical variance bounds rely on specific assumptions about function spaces and bounded variation properties that may not extend to all periodic functions
- Experimental validation is limited to specific neural network architectures and training regimes without exploring alternative optimization methods
- The analysis focuses on gradient-based methods and doesn't investigate whether non-gradient approaches could circumvent the barren plateau phenomenon

## Confidence

High confidence: The variance bounds for gradient information (Mechanism 1 and 2) are mathematically rigorous and well-supported by the proofs provided.

Medium confidence: The SQ dimension bounds and their implications for general learnability (Mechanism 3) are less directly established, relying on the translation from gradient variance to SQ complexity which, while plausible, requires more explicit justification.

Low confidence: The experimental results are limited in scope and don't fully explore the parameter space or alternative architectures that might show different behavior.

## Next Checks

1. Test alternative architectures: Evaluate whether deeper or recurrent neural networks can circumvent the barren plateau phenomenon for high-frequency functions and modular multiplication, or if the variance bound is truly architecture-independent.

2. Measure correlation structure: Systematically compute and verify the predicted correlation decay O(1/√p) between different modular multipliers for various prime sizes, and determine if this correlation structure directly causes the gradient variance reduction.

3. Explore non-gradient methods: Implement and evaluate alternative optimization approaches (e.g., evolutionary strategies, random search) on the same problems to determine if the hardness is specific to gradient-based methods or represents a more fundamental computational barrier.