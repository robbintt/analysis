---
ver: rpa2
title: Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning
arxiv_id: '2310.11594'
source_url: https://arxiv.org/abs/2310.11594
tags:
- attacks
- learning
- federated
- adversarial
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Adversarial Robustness Unhardening (ARU),
  a novel train-time attack that undermines federated adversarial training by embedding
  backdoors that increase model vulnerability to evasion attacks. ARU exploits model
  replacement attacks, where a small group of adversarial clients collaboratively
  degrade model robustness during federated learning.
---

# Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning

## Quick Facts
- arXiv ID: 2310.11594
- Source URL: https://arxiv.org/abs/2310.11594
- Reference count: 27
- Key outcome: ARU significantly reduces adversarial accuracy in federated learning while maintaining high test accuracy through backdoor attacks

## Executive Summary
This paper introduces Adversarial Robustness Unhardening (ARU), a novel train-time attack that undermines federated adversarial training by embedding backdoors that increase model vulnerability to evasion attacks. ARU exploits model replacement attacks, where a small group of adversarial clients collaboratively degrade model robustness during federated learning. The authors propose ARU-Extract (ARU-E), a practical method for adversaries to extract a non-robust model from the robust global model during training by inducing catastrophic forgetting. Experiments on CIFAR-10 and CIFAR-100 datasets show that ARU significantly reduces adversarial accuracy compared to standard federated adversarial training.

## Method Summary
The ARU attack consists of two main components: ARU-E (extraction phase) and model replacement. In ARU-E, adversarial clients jointly update the FAT model on data points perturbed via PGD with incorrect labels, inducing catastrophic forgetting of adversarial robustness. The model replacement phase involves strategically boosting model updates based on aggregation weights to ensure the adversarial model influences the global model. The attack maintains high test accuracy while degrading adversarial robustness, making it difficult to detect through standard accuracy metrics.

## Key Results
- CIFAR-10 test accuracy remains high (0.667) but adversarial accuracy drops sharply (0.068) under ARU-E
- ARU effectively undermines federated adversarial training by embedding backdoors that increase model vulnerability
- Robust aggregation defenses like trimmed mean and median reduce ARU effectiveness but don't eliminate the threat

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ARU-Extract induces catastrophic forgetting of adversarial robustness by forcing overfitting on perturbed data with incorrect labels.
- Mechanism: Adversarial clients jointly update the FAT model on data points perturbed via PGD and assigned incorrect labels based on the new model's classification. This corrupts the model's learned robustness.
- Core assumption: A small number of colluding clients can induce catastrophic forgetting through joint updates on perturbed data.
- Evidence anchors:
  - [section] "The inclusion of perturbed data for the ARU-E pushes the decision boundary to be even more feeble than FedAvg. Figure 3a shows that the catastrophic forgetting of robustness occurs very quickly with a limited number of training rounds amongst the relatively small group of adversaries, making ARU-E highly practical."
  - [abstract] "We present extensive experiments evaluating ARU's impact on adversarial training and existing robust aggregation defenses against poisoning and backdoor attacks."
- Break condition: If the number of ARU clients is too small relative to total clients, or if robust aggregation defenses effectively filter out their updates.

### Mechanism 2
- Claim: Model replacement attacks allow ARU clients to inject non-robust models into the global model despite being a small fraction of total clients.
- Mechanism: ARU clients strategically boost their model differences from the global model based on their aggregation weights, enabling successful model replacement even with limited participation.
- Core assumption: ARU clients can estimate their aggregation weights and boost their updates accordingly to survive aggregation.
- Evidence anchors:
  - [section] "To accomplish this, the attacker strategically ensures that its uploaded model U t+1 j survive the aggregation across all clients by boosting the difference between desired model R and global model Gt based on γj, as seen in Equation 4."
  - [corpus] "The work in [6] discusses how adversary j can estimate γj during the training procedure, if not known ahead of time."
- Break condition: If robust aggregation defenses become too effective at identifying and discarding outlier updates.

### Mechanism 3
- Claim: Spreading ARU attacks across multiple rounds can overcome robust aggregation defenses by making adversarial updates less distinguishable as outliers.
- Mechanism: Instead of replacing the model in a single round, ARU clients distribute their attack across multiple rounds, reducing per-round deviation and making updates harder to filter.
- Core assumption: Benign clients' updates will deviate more from zero as the model moves away from convergence, making ARU updates relatively less extreme.
- Evidence anchors:
  - [section] "Also, as the FAT and ARU-E models have similar performance regarding benign test accuracy, the two models are comparatively not too far off compared to label swapping or boosting poisoning attacks [26], leading to higher inclusion rates against trimmed mean."
  - [section] "However, spreading the model replacement attack across multiple rounds of training induces benign clients to begin to contribute more to training, as the model is further from convergence."
- Break condition: If the defense adapts to track multi-round attack patterns or if benign clients' updates remain small enough to maintain contrast.

## Foundational Learning

- Concept: Federated Adversarial Training (FAT)
  - Why needed here: ARU specifically targets FAT by undermining its robustness mechanisms.
  - Quick check question: How does FAT differ from standard federated learning in its training objective?

- Concept: Projected Gradient Descent (PGD) attacks
  - Why needed here: ARU-E uses PGD to create perturbed data for inducing catastrophic forgetting.
  - Quick check question: What is the purpose of the perturbation budget in PGD attacks?

- Concept: Model replacement attacks in federated learning
  - Why needed here: ARU uses model replacement as its primary attack vector to inject non-robust models.
  - Quick check question: How does the model replacement attack mathematically ensure the attacker's model influences the global model?

## Architecture Onboarding

- Component map: Global model → ARU-E clients (joint updates on perturbed data with incorrect labels) → catastrophic forgetting → model replacement phase (strategic boosting) → compromised global model
- Critical path: ARU-E clients receive global model → jointly update on perturbed data → induce catastrophic forgetting → perform model replacement → undermine robustness while maintaining test accuracy
- Design tradeoffs: ARU balances between effective robustness degradation and maintaining high test accuracy to evade detection. Spreading attacks across rounds reduces per-round deviation but requires tracking multi-round patterns.
- Failure signatures: Test accuracy remains high while adversarial accuracy drops sharply; robust aggregation defenses start filtering out more adversarial updates; benign clients' updates deviate significantly from zero.
- First 3 experiments:
  1. Implement ARU-E with 5 clients on CIFAR-10 to verify catastrophic forgetting occurs within 50 rounds.
  2. Test ARU against trimmed mean defense with varying β values to find the breaking point.
  3. Implement multi-round ARU attack to evaluate effectiveness against median defense compared to single-round attack.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is ARU against robust aggregation schemes beyond trimmed-mean and median, such as Krum, Bulyan, or MixTailor?
- Basis in paper: [explicit] The paper evaluates ARU against trimmed-mean and median defenses but suggests exploring other robust aggregation schemes in the discussion section.
- Why unresolved: The paper only tests two common robust aggregation schemes and acknowledges the need to evaluate ARU against a broader range of defenses.
- What evidence would resolve it: Empirical results showing ARU's performance against Krum, Bulyan, MixTailor, and other robust aggregation schemes on standard datasets like CIFAR-10 and CIFAR-100.

### Open Question 2
- Question: Can ARU be adapted to work in settings where data is highly non-i.i.d. across clients, and how would this affect its effectiveness?
- Basis in paper: [inferred] The paper mentions that robust aggregation schemes may be less effective in non-i.i.d. settings, and ARU's effectiveness could vary depending on data distribution.
- Why unresolved: The paper does not explore ARU's performance under different data distribution scenarios, such as highly skewed or non-i.i.d. data.
- What evidence would resolve it: Experimental results comparing ARU's effectiveness under i.i.d. and non-i.i.d. data distributions, along with analysis of how data heterogeneity impacts ARU's success.

### Open Question 3
- Question: What are the theoretical limits of ARU in terms of the number of colluding clients required to successfully undermine adversarial training?
- Basis in paper: [explicit] The paper discusses the practicality of ARU with a small number of colluding clients but does not provide theoretical bounds on the minimum number needed.
- Why unresolved: The paper focuses on empirical results with specific numbers of colluding clients but does not derive theoretical guarantees or bounds.
- What evidence would resolve it: Theoretical analysis or proofs establishing the minimum number of colluding clients required for ARU to succeed, along with empirical validation of these bounds.

## Limitations

- Scalability concerns as the number of clients increases, making it harder to maintain consistent joint updates across adversarial clients
- Assumes perfect knowledge of aggregation weights for model replacement, though estimation methods exist
- Effectiveness against more sophisticated aggregation defenses beyond trimmed mean and median remains unexplored

## Confidence

- **High confidence**: The core mechanism of ARU-E inducing catastrophic forgetting through joint updates on perturbed data with incorrect labels is well-supported by experimental evidence showing rapid degradation of adversarial accuracy while maintaining test accuracy.
- **Medium confidence**: The effectiveness of spreading ARU attacks across multiple rounds to overcome robust aggregation defenses is demonstrated, but the paper doesn't explore the optimal distribution strategy or provide theoretical guarantees about the trade-off between detection risk and attack effectiveness.
- **Medium confidence**: The claim that ARU can maintain high test accuracy while significantly reducing adversarial accuracy is supported by experiments, but the generalization to other datasets and model architectures beyond CIFAR-10/CIFAR-100 and MobilenetV2 requires further validation.

## Next Checks

1. **Scale Test**: Implement ARU-E with 100+ clients on CIFAR-10 to evaluate how attack effectiveness scales with increased client heterogeneity and whether the catastrophic forgetting mechanism remains practical.

2. **Defense Generalization**: Test ARU against additional robust aggregation defenses including Krum, Bulyan, and coordinate-wise median with varying client participation rates to establish a more comprehensive robustness profile.

3. **Cross-Dataset Validation**: Apply ARU to different dataset pairs (e.g., CIFAR-10/CIFAR-100 and SVHN/CIFAR-100) to verify the attack's effectiveness across varying data distributions and to test the claim about catastrophic forgetting's generalizability.