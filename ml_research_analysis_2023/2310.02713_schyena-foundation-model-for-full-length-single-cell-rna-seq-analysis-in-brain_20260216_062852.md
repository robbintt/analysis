---
ver: rpa2
title: 'scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in
  Brain'
arxiv_id: '2310.02713'
source_url: https://arxiv.org/abs/2310.02713
tags:
- cell
- data
- schyena
- scrna-seq
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces scHyena, a foundation model for analyzing
  single-cell RNA sequencing (scRNA-seq) data in brain tissue. The key innovation
  is adapting the Hyena operator for full-length scRNA-seq data processing without
  information loss.
---

# scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in Brain

## Quick Facts
- arXiv ID: 2310.02713
- Source URL: https://arxiv.org/abs/2310.02713
- Reference count: 19
- scHyena achieves F1-scores of 0.982-0.998 for cell type classification across four datasets, excelling in doublet detection and outperforming MAGIC and DCA in imputation tasks.

## Executive Summary
scHyena introduces a foundation model for single-cell RNA sequencing (scRNA-seq) analysis in brain tissue by adapting the Hyena operator to process full-length gene expression data without information loss. The model uses a linear adaptor layer to preserve continuous expression values, incorporates gene embeddings instead of positional encoding, and employs a bidirectional Hyena operator to capture relationships between all genes. Pre-trained using masked expression modeling, scHyena demonstrates superior performance in downstream tasks including cell type classification and imputation, with F1-scores of 0.982-0.998 across four datasets and better MSE and Pearson correlation metrics than baseline methods.

## Method Summary
scHyena is a foundation model for scRNA-seq analysis that adapts the Hyena operator for full-length gene expression data. It uses a linear adaptor layer to map continuous expression values to embeddings, incorporates gene-specific embeddings instead of positional encoding, and employs a bidirectional Hyena operator to capture relationships between all genes. The model is pre-trained using masked expression modeling on brain tissue datasets and fine-tuned for downstream tasks like cell type classification (with a prepended [CLS] token and classification head) and imputation (by masking and predicting expression values). This architecture preserves continuous information and enables efficient processing of long gene sequences.

## Key Results
- Cell type classification: F1-scores of 0.982-0.998 across four datasets, with superior doublet detection performance
- Imputation: Outperforms MAGIC and DCA in both MSE and Pearson correlation metrics
- Imputed values show biologically meaningful patterns in UMAP visualizations
- Effective across different masking ratios in pre-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bidirectional Hyena enables full-length scRNA-seq analysis without information loss.
- Mechanism: By extending the Hyena operator to be bidirectional (using filters of length 2L-1 instead of L), the model can capture relationships between all genes regardless of their positions. This overcomes the causality limitation of the original Hyena operator which only considers past tokens.
- Core assumption: Gene expression relationships are non-directional and all genes can potentially have relationships with each other.
- Evidence anchors:
  - [abstract]: "we design a novel Transformer architecture called singe-cell Hyena (scHyena) that is equipped with a linear adaptor layer, the positional encoding via gene-embedding, and a {bidirectional} Hyena operator"
  - [section]: "we require a non-causal, bidirectional operator. To address this requirement, we design our convolution filters with a length of 2L − 1, defining them for t = −L + 1, . . . , L − 1"
- Break condition: If gene expression relationships were directional or temporal in nature, the bidirectional approach would capture spurious correlations.

### Mechanism 2
- Claim: Linear adaptor layer preserves continuous gene expression information better than discretization.
- Mechanism: The linear adaptor layer directly maps continuous expression values to embeddings without quantization, avoiding information loss that occurs when discretizing continuous values into bins.
- Core assumption: Continuous gene expression data contains fine-grained information that would be lost through discretization.
- Evidence anchors:
  - [abstract]: "we introduce a linear layer as the adapter layer instead of discretizing and tokenizing input values"
  - [section]: "gene expression levels are continuous values and cannot be discretized... we encode the expression levels into expression embeddings ( E1, E2, . . . , EL) using a linear adapter layer, in contrast to traditional tokenization approaches"
- Break condition: If the continuous nature of gene expression data were not critical for downstream tasks, discretization might provide computational benefits without significant performance loss.

### Mechanism 3
- Claim: Gene embeddings provide explicit gene identity information that positional encoding cannot capture.
- Mechanism: Since gene order in scRNA-seq data carries no inherent meaning, gene-specific embeddings are used instead of positional encoding to inform the model which gene's expression level each position represents.
- Core assumption: Gene identity is crucial contextual information that the model needs to properly interpret expression patterns.
- Evidence anchors:
  - [abstract]: "we incorporate gene encoding to provide gene-related information to the model"
  - [section]: "we incorporate gene embeddings into the scHyena model, rather than using positional encoding employed in the original Transformers... Each gene is encoded with its own embedding (G1, G2, . . . , GL), which is then added to the expression embeddings"
- Break condition: If gene position had inherent meaning or if gene identity could be inferred from expression patterns alone, explicit gene embeddings might be redundant.

## Foundational Learning

- Concept: Hyena operator and its advantages over self-attention
  - Why needed here: Understanding why Hyena was chosen over traditional Transformers is crucial for appreciating the architecture's efficiency in handling long sequences
  - Quick check question: What is the computational complexity advantage of Hyena over standard self-attention, and how does this enable full-length scRNA-seq analysis?

- Concept: Masked expression modeling as a self-supervised learning strategy
  - Why needed here: This pre-training approach is fundamental to how scHyena learns generalizable features from unlabeled data
  - Quick check question: How does masked expression modeling differ from masked language modeling in NLP, and why is it appropriate for scRNA-seq data?

- Concept: The distinction between true and false zeros in scRNA-seq data
  - Why needed here: Understanding dropout events and their impact is essential for appreciating the imputation task and evaluation metrics
  - Quick check question: What is the difference between technical zeros (dropout) and true zeros in scRNA-seq data, and why is this distinction important for imputation?

## Architecture Onboarding

- Component map:
  Input layer: Continuous gene expression values (L genes per cell) -> Linear adaptor layer: Maps continuous values to expression embeddings -> Gene embedding layer: Adds gene-specific embeddings -> Bidirectional Hyena blocks: Stacked bidirectional Hyena operators for sequence processing -> [CLS] token (for classification): Prepended learnable embedding -> Classification head: Linear layer for cell type prediction -> Masking mechanism: For MEM pre-training and imputation fine-tuning

- Critical path: Gene expression values → Linear adaptor → Gene embeddings → Bidirectional Hyena blocks → [CLS] embedding (classification) or masked expression prediction (imputation)

- Design tradeoffs:
  - Bidirectional vs. causal Hyena: Bidirectional captures all gene relationships but may introduce spurious correlations; causal would be more efficient but miss important non-directional relationships
  - Linear adaptor vs. tokenization: Linear adaptor preserves information but may require more parameters; tokenization is more efficient but loses granularity
  - Gene embeddings vs. positional encoding: Gene embeddings provide explicit gene identity but require learning gene-specific representations; positional encoding is more general but doesn't capture gene-specific information

- Failure signatures:
  - Poor performance on long sequences might indicate Hyena parameters need tuning or insufficient model capacity
  - Inability to distinguish cell types might suggest gene embeddings aren't capturing sufficient gene-specific information
  - Over-smoothing in imputation could indicate the bidirectional Hyena is over-smoothing gene expression patterns

- First 3 experiments:
  1. Ablation study: Compare scHyena with and without gene embeddings on a small dataset to quantify their contribution
  2. Masking ratio sensitivity: Test different masking probabilities during pre-training to find the optimal balance for learning
  3. Length scalability: Test on progressively longer sequences to verify the Hyena operator's efficiency advantage holds in practice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would scHyena perform when pre-trained on single-cell RNA-seq data from diverse tissue types beyond brain tissue?
- Basis in paper: [inferred] The authors mention that pre-training scHyena with scRNA-seq data from various tissue types could expand its applicability to a wider array of downstream tasks.
- Why unresolved: The paper only demonstrates scHyena's performance on brain tissue data, and the authors do not explore its performance on other tissue types.
- What evidence would resolve it: Pre-training scHyena on single-cell RNA-seq data from diverse tissue types and evaluating its performance on downstream tasks for those tissues would provide evidence for its generalizability beyond brain tissue.

### Open Question 2
- Question: How does the choice of masking probability in the pre-training stage affect scHyena's performance on downstream tasks?
- Basis in paper: [explicit] The authors mention that they choose a masking probability from a range of [0.05, 0.4] during pre-training, but they do not explore the impact of different masking probabilities on downstream performance.
- Why unresolved: The paper does not investigate the sensitivity of scHyena's performance to the choice of masking probability in the pre-training stage.
- What evidence would resolve it: Conducting experiments with different masking probabilities during pre-training and comparing scHyena's performance on downstream tasks would reveal the impact of this hyperparameter on its effectiveness.

### Open Question 3
- Question: How does scHyena's performance compare to other state-of-the-art methods when dealing with highly imbalanced cell type distributions in single-cell RNA-seq data?
- Basis in paper: [explicit] The authors mention that scBERT, one of the baseline methods, performs poorly on imbalanced datasets, suggesting that scHyena might be more robust to such imbalances.
- Why unresolved: The paper does not directly compare scHyena's performance to other methods on datasets with highly imbalanced cell type distributions.
- What evidence would resolve it: Evaluating scHyena and other state-of-the-art methods on single-cell RNA-seq datasets with highly imbalanced cell type distributions and comparing their performance metrics would provide insights into scHyena's robustness in such scenarios.

## Limitations

- Limited to brain tissue data despite being a "foundation model" - generalizability to other tissues remains unproven
- Computational requirements for full-length analysis not thoroughly benchmarked against alternatives
- No explicit handling of batch effects or technical confounders common in scRNA-seq data
- Gene embeddings require learning L×embedding_dim parameters, which could be substantial for datasets with many genes

## Confidence

- **High confidence**: The bidirectional Hyena mechanism and its superiority over causal approaches for non-directional gene relationships; the linear adaptor's preservation of continuous information
- **Medium confidence**: The claim of superior performance across all datasets - while results show strong performance, the ablation studies are limited and some baselines appear to be relatively simple methods
- **Low confidence**: The assertion that this is truly a "foundation model" given the tissue-specific training and limited evaluation of cross-tissue transfer

## Next Checks

1. **Cross-tissue transfer evaluation**: Test scHyena's pre-trained weights on scRNA-seq datasets from non-brain tissues to assess true foundation model capabilities
2. **Scalability benchmarking**: Systematically evaluate computational efficiency and memory usage across varying gene set sizes and cell numbers, comparing against self-attention alternatives
3. **Ablation study expansion**: Conduct comprehensive ablation of all architectural components (bidirectional Hyena, linear adaptor, gene embeddings) on a held-out test set to quantify individual contributions to performance