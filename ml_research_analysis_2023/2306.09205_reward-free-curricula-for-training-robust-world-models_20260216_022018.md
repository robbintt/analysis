---
ver: rpa2
title: Reward-Free Curricula for Training Robust World Models
arxiv_id: '2306.09205'
source_url: https://arxiv.org/abs/2306.09205
tags:
- world
- environments
- policy
- environment
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of training robust world models
  in a reward-free setting, where agents must learn from diverse environments without
  specific task rewards. The key idea is to connect the minimax regret objective,
  commonly used for robust policy optimization, to minimizing the maximum error of
  the world model's latent dynamics across environments.
---

# Reward-Free Curricula for Training Robust World Models

## Quick Facts
- arXiv ID: 2306.09205
- Source URL: https://arxiv.org/abs/2306.09205
- Reference count: 40
- One-line primary result: WAKER algorithm significantly improves robustness and out-of-distribution performance over domain randomization baselines in pixel-based continuous control domains.

## Executive Summary
This paper addresses the challenge of training robust world models in a reward-free setting, where agents must learn from diverse environments without specific task rewards. The key insight is connecting minimax regret optimization to minimizing maximum world model error across environments. This leads to WAKER (Weighted Acquisition of Knowledge across Environments for Robustness), an algorithm that actively samples environments based on estimated model errors. Experiments on novel pixel-based continuous control domains demonstrate that WAKER significantly outperforms domain randomization baselines, achieving improved robustness and better performance on out-of-distribution environments.

## Method Summary
WAKER trains robust world models by actively sampling environments where the model has highest uncertainty. It uses ensemble disagreement as a proxy for model error and employs Boltzmann sampling to bias environment selection toward uncertain regions. The method trains a DreamerV2 world model with an ensemble of latent dynamics models, and derives task policies via zero-shot adaptation. Environment parameters are sampled using a mix of domain randomization and error-based selection, with the exploration policy trained to maximize model error across all environments.

## Key Results
- WAKER-M improves CVaR0.1 from 748.9 to 805.3 when paired with Plan2Explore in the Terrain Walker domain
- WAKER achieves better average performance while also improving robustness metrics
- Outperforms domain randomization baselines significantly on out-of-distribution environments
- Demonstrates effective zero-shot adaptation for specific tasks after reward-free training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimax regret in the latent space upper bounds regret in the true environment
- Mechanism: By Assumption 2, the latent dynamics T exactly model the true environment when combined with q. Proposition 1 shows that the regret of the optimal world model policy is bounded by the total variation error between the learned latent dynamics bT and T under the state-action distributions of both the optimal policy and the world model policy.
- Core assumption: Assumption 2 holds (representation model q creates Markovian latent states with exact latent dynamics T)
- Evidence anchors: [section] "Proposition 1 Let bT be the learnt latent dynamics in the world model..." and [section] "Assumption 2 Consider the representation model learnt by the world model..."
- Break condition: If the representation model q fails to create Markovian latent states, the bound no longer holds and world model error no longer guarantees policy regret bounds.

### Mechanism 2
- Claim: Active environment selection based on ensemble disagreement reduces maximum latent dynamics error
- Mechanism: WAKER estimates world model error using ensemble disagreement. By sampling environments with highest estimated error via Boltzmann sampling, it collects more data where the model is uncertain, reducing the maximum error across environments.
- Core assumption: Ensemble disagreement approximates total variation distance between learned and true latent dynamics
- Evidence anchors: [section] "Following works on offline RL [37, 75, 34], we use the disagreement between an ensemble of neural networks..." and [section] "WORLD MODEL ERROR (bT , θ) ≈ Ez,a∼d(πexplθ, cMθ) [Var [E[bTi(·|z, a)]Ni=1]]"
- Break condition: If ensemble disagreement poorly approximates TV distance, WAKER may not effectively target the environments with highest true error.

### Mechanism 3
- Claim: Reward-free minimax regret objective connects to minimizing maximum world model error across environments
- Mechanism: Problem 1 defines reward-free minimax regret over all environments and reward functions. Proposition 1 bounds this regret by the maximum expected TV error in the latent dynamics across environments under an exploration policy. WAKER operationalizes this by minimizing this upper bound through targeted data collection.
- Core assumption: Exploration policy πexpl approximately maximizes the expected error
- Evidence anchors: [section] "This allows us to write an upper bound on the regret that has no dependence on the reward function..." and [section] "Therefore, we can upper bound the objective of Problem 1..."
- Break condition: If the exploration policy fails to maximize the expected error, the connection between minimax regret and world model error minimization breaks down.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The paper operates in both MDP and POMDP settings, with underspecified POMDPs (UPOMDPs) modeling environments with free parameters. Understanding these frameworks is essential for grasping the world model formulation and regret analysis.
  - Quick check question: What is the key difference between an MDP and a POMDP, and why does this distinction matter for world model learning?

- Concept: Total Variation Distance and its use in bounding model errors
  - Why needed here: Total variation distance is used throughout the paper to quantify the difference between learned and true latent dynamics, and to bound the regret of policies derived from the world model. Understanding this metric is crucial for following the theoretical analysis.
  - Quick check question: How does total variation distance between two distributions relate to their maximum difference in probability over any event?

- Concept: Minimax Regret Optimization
  - Why needed here: The paper's robustness objective is based on minimax regret, which seeks policies with low regret across all possible environments. This concept is fundamental to understanding why WAKER's approach to environment selection improves robustness.
  - Quick check question: What is the difference between maximizing worst-case performance and minimizing worst-case regret, and why might the latter be preferable in this context?

## Architecture Onboarding

- Component map: World Model (W = {q, bT}) -> Environment Parameter Space Θ -> Error Buffer Derror -> Data Buffer D -> Exploration Policy πexpl -> Task Policy bπ*R

- Critical path: 1. Initialize world model W, exploration policy πexpl, empty data buffer D and error buffer Derror 2. Select environment θ using pDR probability of domain randomization or Boltzmann sampling from Derror 3. Collect trajectory τθ by rolling out πexpl in environment Pθ 4. Add τθ to D and train world model W on D 5. Generate imagined trajectories from D using W, estimate errors via ensemble disagreement, update Derror and πexpl 6. Repeat until convergence, then derive task policies bπ*R from final world model

- Design tradeoffs:
  - Ensemble size vs. computational cost: Larger ensembles provide better error estimates but increase training time
  - pDR vs. exploration efficiency: Higher pDR ensures broader coverage but reduces targeted sampling of uncertain environments
  - Error estimate update method (WAKER-M vs WAKER-R): WAKER-M prioritizes highest error environments, WAKER-R prioritizes fastest error reduction rate

- Failure signatures:
  - World model predictions consistently poor across all environments: Indicates fundamental issues with representation learning or dynamics modeling
  - Error estimates show no reduction over training: Suggests exploration policy not effectively maximizing error or ensemble disagreement not capturing true model uncertainty
  - Task policies perform well on training environments but poorly on OOD environments: Indicates overfitting to training distribution or insufficient diversity in sampled environments

- First 3 experiments:
  1. Run WAKER with pDR=1.0 (pure domain randomization) to establish baseline performance and verify world model training pipeline works
  2. Run WAKER with pDR=0.0 and random exploration policy to test environment selection mechanism in isolation from exploration policy sophistication
  3. Run WAKER with Plan2Explore and varying η values on Clean Up domain to tune temperature hyperparameter and verify improvement over domain randomization

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical analysis relies heavily on Assumption 2 which may not hold in practice for complex pixel-based environments
- Method's performance on environments with continuous parameter spaces beyond terrain roughness and object numbers remains untested
- The connection between ensemble disagreement and true model error is empirically validated but lacks rigorous theoretical guarantees

## Confidence

**High Confidence**: The core algorithmic framework of WAKER and its integration with DreamerV2 world models is well-specified and implementable. The empirical improvements over domain randomization baselines are clearly demonstrated.

**Medium Confidence**: The theoretical connection between minimax regret and maximum world model error is sound under stated assumptions, but practical violations of these assumptions could significantly impact performance. The effectiveness of ensemble disagreement as an error proxy is empirically supported but theoretically incomplete.

**Low Confidence**: The generalizability of WAKER to completely different problem domains (e.g., robotic manipulation, autonomous driving) without modification is uncertain, as the current evaluation focuses on locomotion and cleaning tasks.

## Next Checks

1. **Assumption Stress Test**: Systematically evaluate WAKER's performance as the quality of the representation model q degrades (e.g., by reducing latent state dimensionality or adding noise), to quantify the impact of Assumption 2 violations.

2. **Error Proxy Validation**: Conduct ablation studies comparing ensemble disagreement against alternative error estimation methods (e.g., Bayesian neural networks, bootstrap ensembles) to confirm that the specific choice of ensemble disagreement is optimal for this application.

3. **Domain Transfer Experiment**: Apply WAKER to a new domain type (e.g., robotic arm reaching with varying object positions) to assess its effectiveness beyond locomotion and cleaning tasks, measuring both adaptation speed and final performance.