---
ver: rpa2
title: Deep Prompt Tuning for Graph Transformers
arxiv_id: '2309.10131'
source_url: https://arxiv.org/abs/2309.10131
tags:
- graph
- prompt
- transformer
- tuning
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of applying large graph transformer
  models to downstream graph prediction tasks. Graph transformers have quadratic complexity
  and require extensive layering, making them resource-intensive and prone to overfitting
  on small datasets.
---

# Deep Prompt Tuning for Graph Transformers

## Quick Facts
- arXiv ID: 2309.10131
- Source URL: https://arxiv.org/abs/2309.10131
- Authors: 
- Reference count: 28
- One-line primary result: DeepGPT achieves comparable or superior performance to fine-tuning on graph datasets using less than 0.5% of parameters

## Executive Summary
Deep Prompt Tuning for Graph Transformers (DeepGPT) addresses the challenge of applying large pre-trained graph transformer models to downstream tasks without the computational burden of fine-tuning. The method introduces trainable task-specific tokens at both the input graph level and each transformer layer while keeping pre-trained parameters frozen. This approach significantly reduces the number of trainable parameters (to less than 0.5% of total) and demonstrates strong performance across various graph datasets, including molecular property prediction tasks from OGB and Moleculenet.

## Method Summary
DeepGPT modifies graph transformer models by adding trainable prompt tokens to input graph nodes and pre-pending prefix tokens to each transformer layer's embeddings. The pre-trained model parameters remain frozen while only the prompt tokens are updated during training. This contrasts with traditional fine-tuning approaches that update all model parameters. The method is evaluated on molecular graph datasets using 5-fold cross-validation, comparing performance against fine-tuning and lightweight tuning baselines across multiple graph transformer architectures.

## Key Results
- DeepGPT achieves comparable or superior performance to fine-tuning across multiple molecular graph datasets
- Uses less than 0.5% of total parameters compared to full fine-tuning (approximately 6.7k vs 1.4M parameters)
- Demonstrates scalability to large graphs while reducing computational overhead
- Effective across different graph transformer architectures including Graphormer, GraphGPS, and LiGhT

## Why This Works (Mechanism)

### Mechanism 1
Adding task-specific continuous prompt tokens at the input graph level allows the model to approximate various node-level, edge-level, and graph-level transformations without altering pre-trained parameters. The prompt tokens act as trainable feature nodes that propagate through transformer layers via self-attention, influencing the final graph representation. The core assumption is that prompt tokens can effectively approximate complex transformations. Break condition: If prompt tokens cannot capture necessary task-specific information or are too few to represent required transformations.

### Mechanism 2
Pre-pending prefix tokens to each transformer layer embedding allows self-attention to attend to task-specific information as if it were originally part of node embeddings. Prefix tokens are added to embeddings after each layer, enabling the self-attention mechanism to incorporate task-specific information at multiple levels. The core assumption is that prefix tokens can propagate influence through self-attention to affect final output. Break condition: If prefix tokens aren't effectively incorporated into attention or their influence diminishes too quickly.

### Mechanism 3
Freezing pre-trained parameters while updating only prompt tokens reduces free parameters, making the method suitable for small datasets and scalable to large graphs. By keeping the pre-trained model fixed, the approach avoids overfitting on small datasets and reduces computational overhead for large graphs. The core assumption is that pre-trained representations are transferable with minimal additional training. Break condition: If pre-trained representations aren't transferable or prompt tokens cannot capture necessary information.

## Foundational Learning

- **Graph Neural Networks (GNNs) and message-passing**: Understanding GNN message-passing is crucial for grasping differences between GNNs and graph transformers. Quick check: How does message-passing in GNNs differ from self-attention in graph transformers?

- **Transformer architecture and self-attention**: The paper builds upon transformer architecture, so understanding self-attention is essential. Quick check: What is the role of Query, Key, and Value matrices in self-attention?

- **Pre-training and fine-tuning paradigms**: The paper proposes an alternative to fine-tuning, so understanding traditional approaches is important. Quick check: What are main challenges of fine-tuning large models on small downstream datasets?

## Architecture Onboarding

- **Component map**: Input graph with prompt tokens -> Frozen pre-trained graph transformer -> Prefix tokens at each layer -> Classification head -> Loss function

- **Critical path**: 
  1. Prepare input graph by adding prompt tokens to each node
  2. Pass modified input through frozen pre-trained transformer
  3. Add prefix tokens to embeddings after each transformer layer
  4. Apply classification head to final output
  5. Compute loss and update only prompt tokens

- **Design tradeoffs**: 
  - Number of prompt tokens vs. model expressiveness
  - Depth of prefix token insertion vs. influence on final output
  - Size of pre-trained model vs. computational requirements

- **Failure signatures**: 
  - Poor performance despite successful training
  - Unstable training due to insufficient prompt tokens
  - Overfitting on small datasets from excessive prompt token complexity

- **First 3 experiments**:
  1. Compare DeepGPT with fine-tuning on small molecular graph classification dataset
  2. Ablation study: Test impact of graph prompt tokens vs. only prefix tokens
  3. Investigate prefix token depth effect by varying layers they're inserted into

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implicit questions arise from the work:

1. How does DeepGPT perform on graph types beyond molecular property prediction, such as social networks or citation networks?

2. What is the impact of different pre-training strategies on DeepGPT's downstream performance?

3. How does DeepGPT scale to extremely large graphs that exceed current benchmark sizes?

## Limitations

- No empirical validation on truly massive graphs with millions of nodes
- Mechanism of prefix token propagation through layers remains underspecified
- Limited ablation studies on the trade-off between prompt tokens and prefix token depth

## Confidence

**High Confidence**: The claim that prompt tuning reduces trainable parameters compared to fine-tuning (0.5% vs 100%) is well-supported by parameter counting.

**Medium Confidence**: Claims about performance parity or superiority over fine-tuning are supported by experimental results but lack thorough variance analysis and statistical significance testing.

**Low Confidence**: The assertion that DeepGPT is "scalable to large graphs" is based on theoretical arguments rather than empirical validation on truly large-scale graphs.

## Next Checks

1. Test DeepGPT on a graph dataset with graphs containing millions of nodes to empirically validate scalability claims beyond current OGB/Moleculenet datasets.

2. Conduct systematic ablation studies varying prefix token depth (1, 2, 4, 8 layers) and number of prompt tokens per node (1 to 64) to understand hyperparameter relationships.

3. Evaluate DeepGPT's transfer learning robustness when pre-training domain differs significantly from downstream tasks, such as pre-training on small molecules but testing on protein graphs or social networks.