---
ver: rpa2
title: Cultural Adaptation of Recipes
arxiv_id: '2310.17353'
source_url: https://arxiv.org/abs/2310.17353
tags:
- recipe
- recipes
- chinese
- english
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce CulturalRecipes, a bidirectional Chinese-English
  dataset of cooking recipes, to explore cross-cultural recipe adaptation. They experiment
  with sequence-to-sequence models, including machine translation, multilingual language
  models, and large language models (LLMs), as well as automatic recipe matching.
---

# Cultural Adaptation of Recipes

## Quick Facts
- arXiv ID: 2310.17353
- Source URL: https://arxiv.org/abs/2310.17353
- Reference count: 19
- The authors introduce CulturalRecipes, a bidirectional Chinese-English dataset of cooking recipes, to explore cross-cultural recipe adaptation. They experiment with sequence-to-sequence models, including machine translation, multilingual language models, and large language models (LLMs), as well as automatic recipe matching. Automatic and human evaluations show that GPT-4 excels at adapting Chinese recipes into English but struggles with English-to-Chinese translation, highlighting the challenges of cultural adaptation. Finetuned models improve upon zero-shot machine translation, but LLMs generally outperform them. The study underscores the need for culturally-aware language models and the potential of large-scale pre-training for complex adaptation tasks.

## Executive Summary
This paper introduces CulturalRecipes, a bidirectional Chinese-English dataset for cross-cultural recipe adaptation. The authors explore various sequence-to-sequence approaches, including machine translation models, multilingual language models, and large language models (LLMs), to adapt recipes between cultures. They find that while finetuned models improve upon zero-shot machine translation, LLMs generally outperform them, particularly GPT-4 in adapting Chinese recipes into English. The study highlights the challenges of cultural adaptation and the potential of large-scale pre-training for complex adaptation tasks.

## Method Summary
The authors create the CulturalRecipes dataset by matching recipes from monolingual Chinese and English corpora based on title similarity using MPNet embeddings. They experiment with various sequence-to-sequence models including machine translation, multilingual language models (mT5, mBART50), and LLMs (BLOOM, GPT-4, ChatGLM2) for recipe adaptation. The models are evaluated using automatic metrics (BLEU, ChrF, ROUGE-L, BERTScore, Smatch) and human evaluation on a gold-standard test set. The study focuses on Chinese-English recipe adaptation, going beyond simple translation to account for ingredients, culinary techniques, and cultural differences.

## Key Results
- GPT-4 excels at adapting Chinese recipes into English but struggles with English-to-Chinese translation
- Finetuned models improve upon zero-shot machine translation, but LLMs generally outperform them
- Automatic and human evaluations show that cultural adaptation is more challenging than direct translation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequence-to-sequence models learn cultural adaptation patterns when finetuned on matched recipe pairs.
- Mechanism: The training data provides paired examples where one recipe is culturally adapted from the other, allowing the model to learn mappings between culturally specific terms, measurement units, and cooking methods.
- Core assumption: Recipe titles can reliably indicate semantic equivalence across cultures, and the paired corpus captures sufficient cultural adaptation patterns for learning.
- Evidence anchors:
  - [abstract]: "We experiment with various sequence-to-sequence approaches to adapt the recipes, including machine translation models and multilingual language models."
  - [section 3.2]: "Our recipe matching procedure relies on the following assumption: if two recipes have the same title, they describe the same dish."
  - [corpus]: Weak - the matching procedure relies on title similarity but cultural adaptation requires deeper semantic understanding beyond titles.
- Break condition: If the matched pairs contain too much noise or the cultural differences are too subtle to be captured by surface patterns in the paired data.

### Mechanism 2
- Claim: Large language models can perform zero-shot cultural adaptation without task-specific finetuning.
- Mechanism: LLMs leverage their broad pretraining on diverse multilingual and cultural data to generalize cultural adaptation capabilities, recognizing patterns in ingredients, tools, and cooking methods across cultures.
- Core assumption: LLMs have been exposed to sufficient cross-cultural cooking knowledge during pretraining to enable zero-shot adaptation without task-specific training data.
- Evidence anchors:
  - [abstract]: "GPT-4 exhibits impressive abilities in adapting Chinese recipes into English...LLMs generally outperform them [finetuned models]."
  - [section 6.1]: "Building on the remarkable performance of Multilingual LLMs in zero-shot translation without additional finetuning or in-context learning (Wang et al., 2021), we explore their recipe translation and adaptation capabilities."
  - [corpus]: Weak - no evidence that the pretraining corpora included sufficient recipe or cooking domain data for effective cultural adaptation.
- Break condition: If the LLM pretraining data lacks sufficient coverage of the target cultures' cooking practices or if the cultural differences are too nuanced for pattern recognition.

### Mechanism 3
- Claim: Automatic recipe matching based on title similarity provides sufficient supervision signal for cultural adaptation learning.
- Mechanism: The retrieval process creates many-to-many recipe pairs that expose the model to diverse examples of how the same dish is prepared differently across cultures, enabling learning of cultural adaptation patterns.
- Core assumption: Recipes with similar titles but different cultural origins contain sufficient structural similarity to enable learning of cultural adaptation patterns despite differences in ingredients and methods.
- Evidence anchors:
  - [section 3.2]: "Our recipe matching procedure relies on the following assumption: if two recipes have the same title, they describe the same dish."
  - [section 6.2]: "The significant performance gap between MT-zs and MT-ft emphasizes that the recipe pairs in our dataset are not merely translations of each other."
  - [corpus]: Moderate - the matching procedure uses MPNet embeddings and cosine similarity threshold, suggesting some attempt to ensure semantic similarity beyond just titles.
- Break condition: If the title-based matching creates too many false positives or if the cultural differences between matched recipes are too extreme to provide coherent learning signals.

## Foundational Learning

- Concept: Cross-cultural semantic alignment
  - Why needed here: Cultural adaptation requires understanding that words or concepts in one culture may not have direct equivalents in another, requiring semantic rather than literal translation.
  - Quick check question: Why might the Chinese term "dòufu" (tofu) align with Western protein sources like "ham" or "sausage" in embedding space?

- Concept: Compositional generalization in recipe generation
  - Why needed here: Models must learn to generate coherent recipes with correct ingredient lists, measurements, and sequential steps while adapting cultural elements.
  - Quick check question: What challenges arise when a model must simultaneously adapt ingredients and modify cooking steps while maintaining recipe coherence?

- Concept: Multimodal cultural knowledge representation
  - Why needed here: Effective cultural adaptation may require understanding cultural context beyond text, such as common ingredients, cooking tools, and preparation methods in different cultures.
  - Quick check question: How might knowledge of common cooking tools in Chinese vs. Western kitchens impact the adaptation of cooking instructions?

## Architecture Onboarding

- Component map: Monolingual recipe corpora → Title translation → MPNet encoding → Cosine similarity retrieval → Paired dataset → Model finetuning → Automatic metrics + Human evaluation
- Critical path: Recipe title matching → Paired dataset creation → Model finetuning → Evaluation on gold test set
- Design tradeoffs:
  - Title-based matching is simple but may introduce noise; more sophisticated semantic matching could improve quality but increase complexity
  - Finetuning requires task-specific data but enables targeted learning; zero-shot LLM approaches avoid data requirements but may lack cultural nuance
  - Automatic evaluation is scalable but may miss cultural appropriateness; human evaluation is more accurate but resource-intensive
- Failure signatures:
  - High BLEU scores but poor human ratings indicate automatic metrics may not capture cultural appropriateness
  - Model predictions that are fluent but culturally inappropriate suggest pretraining data bias
  - Low Smatch scores with high BERTScore indicate structural differences that automatic metrics miss
- First 3 experiments:
  1. Compare MT-zs (zero-shot) vs MT-ft (finetuned) on silver test set to validate learning from paired data
  2. Evaluate LLM zero-shot performance on gold test set to assess pretraining effectiveness for cultural adaptation
  3. Analyze literal translation rates for culturally specific terms to understand model adaptation strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more effective evaluation metrics for cultural adaptation that better capture the nuances of cross-cultural recipe translation?
- Basis in paper: [explicit] The paper discusses the limitations of current automatic metrics (BLEU, ChrF, ROUGE-L, BERTScore, Smatch) in evaluating cultural adaptation and suggests that human evaluation is essential.
- Why unresolved: The paper acknowledges that current automatic metrics have limitations and that human evaluation is necessary, but it does not propose specific solutions for developing more effective evaluation metrics.
- What evidence would resolve it: Development and testing of new evaluation metrics specifically designed for cultural adaptation tasks, with empirical comparisons to human judgments.

### Open Question 2
- Question: What are the specific strategies employed by LLMs like GPT-4 to achieve superior performance in cross-cultural recipe adaptation compared to finetuned models?
- Basis in paper: [explicit] The paper notes that GPT-4 exhibits impressive abilities in adapting Chinese recipes into English but lags behind in the opposite direction, and suggests that LLMs employ different strategies than just substituting ingredients and methods.
- Why unresolved: The paper observes differences in performance but does not provide a detailed analysis of the specific strategies employed by LLMs.
- What evidence would resolve it: In-depth analysis of LLM outputs, comparing them to finetuned model outputs and human adaptations, to identify the specific strategies used.

### Open Question 3
- Question: How can we expand the CulturalRecipes dataset to include more languages and cultures, and how will this impact the performance of cross-cultural recipe adaptation models?
- Basis in paper: [inferred] The paper acknowledges the limitations of its cultural categories and suggests expanding the dataset to include more languages and cultures as future work.
- Why unresolved: The paper only focuses on Chinese-English adaptation and does not explore the impact of expanding to other languages and cultures.
- What evidence would resolve it: Creation and evaluation of cross-cultural recipe adaptation models on expanded datasets including more languages and cultures.

## Limitations
- The dataset construction relies heavily on title-based matching, which may introduce noise and semantic mismatches in the training pairs.
- The human evaluation sample size (20 recipes per direction) may be insufficient to capture the full complexity of cultural adaptation quality.
- The study focuses on Chinese-English pairs, limiting generalizability to other language pairs and cultural contexts.

## Confidence
- **High confidence**: GPT-4's superior performance on Chinese-to-English recipe adaptation (supported by both automatic and human evaluation metrics)
- **Medium confidence**: Finetuned models' improvement over zero-shot baselines (consistent across metrics but limited by dataset quality)
- **Low confidence**: Generalizability of findings to other language pairs and cultural adaptation tasks beyond Chinese-English recipes

## Next Checks
1. Conduct a more extensive human evaluation with 100+ recipes per direction and include domain experts in culinary translation to validate cultural appropriateness judgments
2. Test the proposed methods on additional language pairs (e.g., Japanese-English, French-English) to assess generalizability of the cultural adaptation framework
3. Implement and evaluate more sophisticated semantic matching algorithms beyond title similarity to improve dataset quality and assess impact on model performance