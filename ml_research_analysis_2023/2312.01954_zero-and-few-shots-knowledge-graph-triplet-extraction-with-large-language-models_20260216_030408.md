---
ver: rpa2
title: Zero- and Few-Shots Knowledge Graph Triplet Extraction with Large Language
  Models
arxiv_id: '2312.01954'
source_url: https://arxiv.org/abs/2312.01954
tags:
- triplets
- llms
- context
- figure
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the zero- and few-shot capabilities of large
  language models (LLMs) for knowledge graph triplet extraction (TE). The authors
  propose a pipeline that dynamically retrieves contextual information from a knowledge
  base (KB) and provides it to the LLM through a prompt.
---

# Zero- and Few-Shots Knowledge Graph Triplet Extraction with Large Language Models

## Quick Facts
- arXiv ID: 2312.01954
- Source URL: https://arxiv.org/abs/2312.01954
- Reference count: 9
- Large language models augmented with KB context can match or exceed fully supervised BiLSTM baselines in zero/few-shot triplet extraction

## Executive Summary
This paper explores zero- and few-shot knowledge graph triplet extraction using large language models (LLMs) by dynamically retrieving contextual information from a knowledge base (KB) and incorporating it into prompts. The authors propose a pipeline that retrieves relevant KB triplets or sentence-triplet pairs via vector similarity search, then augments LLM prompts with this context to improve extraction performance. Experiments on WebNLG and NYT datasets show that KB-augmented prompts significantly boost LLM performance, making them competitive with fully trained baselines. The study finds that KB quality scales linearly with performance while model size scales logarithmically, suggesting retrieval quality matters more than model size for this task.

## Method Summary
The method involves no model training - instead, it uses a knowledge base built from training and validation data, indexed via MiniLM embeddings. For each test sentence, the pipeline encodes the sentence with MiniLM, retrieves top NKB similar context (either triplets or sentence-triplet pairs) from the KB vector store, and constructs a prompt by adding this context to a base prompt. The augmented prompt is fed to an LLM with temperature 0.1, and output triplets are parsed and evaluated against ground truth using micro-averaged F1 score. The approach is tested across multiple LLM sizes including GPT-2, Falcon, LLaMA, GPT-3.5, and GPT-4 on WebNLG and NYT datasets.

## Key Results
- KB-augmented prompts improve LLM triplet extraction F1 scores from near-zero to competitive with BiLSTM baselines
- Sentence-triplet pairs in prompts outperform raw triplets, making LLMs competitive with classical baselines
- KB quality scaling shows linear improvement while model size scaling shows logarithmic improvement in performance
- Retrieval quality strongly correlates with final TE performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Dynamically retrieved KB context improves LLM TE performance by providing entity-relation examples.
- **Mechanism**: The pipeline retrieves KB triplets or sentence-triplet pairs via a vector store and embeds them with MiniLM. These are appended to the prompt, giving the LLM explicit examples of (subject, predicate, object) structures that match the input text.
- **Core assumption**: The KB contains relevant examples that overlap sufficiently with the test data, so retrieval will supply useful context.
- **Evidence anchors**:
  - [abstract] "augmenting LLMs with KB information, i.e., dynamically gathering contextual information from the KB, largely improves their TE capabilities"
  - [section 4.5] "the KB-augmented prompt has a large probability of containing the correct triplets to extract already"
  - [corpus] Weak: No corpus paper explicitly verifies this mechanism.
- **Break condition**: If KB overlap with test data is low, retrieved context becomes irrelevant and performance degrades.

### Mechanism 2
- **Claim**: Sentence-triplet pair examples improve TE more than raw triplets because they provide structural patterns.
- **Mechanism**: Instead of only providing entity-relation triples, the prompt includes full sentences with their corresponding triplets. This gives the LLM both the semantic relations and the syntactic patterns linking entities in context.
- **Core assumption**: The LLM can generalize from sentence patterns to new sentences with similar structure but different entities.
- **Evidence anchors**:
  - [section 4.4] "including 5 of these examples in the prompt makes the LLM competitive with most of the classical baselines"
  - [section 4.5] "performance increases with NKB in this case, providing further evidence that the examples composed of sentence-triplets pairs are much more informative"
  - [corpus] Weak: No corpus study explicitly compares these two forms.
- **Break condition**: If retrieved sentence-triplet pairs are too dissimilar from input, the pattern transfer fails.

### Mechanism 3
- **Claim**: LLM performance scales logarithmically with model size, while KB quality scales linearly.
- **Mechanism**: Larger models benefit from more parameters but with diminishing returns, whereas better KB retrieval consistently improves results.
- **Core assumption**: TE accuracy is bottlenecked more by KB coverage than by model size.
- **Evidence anchors**:
  - [section 4.6] "the F1 score and thus the TE accuracy appears to scale linearly with the size of the KB... but only logarithmically with the size of the model"
  - [section 4.6] "Improving the quality of the KB and the information retriever might be more effective than increasing the modeling power of the LLM"
  - [corpus] Weak: No corpus evidence directly measuring KB scaling vs model scaling.
- **Break condition**: If KB retrieval is saturated, further model scaling becomes comparatively more valuable.

## Foundational Learning

- **Concept**: Knowledge graph triplet extraction (TE)
  - Why needed here: TE is the core task; understanding its formulation (subject, predicate, object) is prerequisite to grasping the pipeline.
  - Quick check question: In a sentence like "Paris is the capital of France," what are the subject, predicate, and object?

- **Concept**: Vector store indexing and similarity search
  - Why needed here: KB context retrieval relies on encoding sentences and triplets into vectors and comparing them via cosine similarity.
  - Quick check question: If you embed a sentence and a triplet into the same vector space, what similarity measure would you use to retrieve relevant context?

- **Concept**: Prompt engineering for LLMs
  - Why needed here: The pipeline's performance depends on prompt structure; understanding how examples and context are integrated is essential.
  - Quick check question: What is the difference between zero-shot and few-shot prompting in LLM usage?

## Architecture Onboarding

- **Component map**: Input sentence -> MiniLM encoder -> vector similarity search -> KB retriever -> prompt augmentation -> LLM -> output triplets
- **Critical path**: 1. Encode input sentence with MiniLM. 2. Retrieve top NKB relevant context (triplets or sentence-triplet pairs). 3. Format retrieved context into prompt. 4. Feed prompt to LLM and parse output.
- **Design tradeoffs**: Larger NKB increases chance of relevant context but risks dilution. Sentence-triplet pairs give richer context but are costlier to retrieve and embed. Smaller models with good KB retrieval can match larger models without KB.
- **Failure signatures**: Low TE F1: KB retrieval irrelevant (low overlap) or LLM fails to parse prompt. Inconsistent outputs: Temperature too high or prompt formatting error. High latency: NKB too large or inefficient vector store.
- **First 3 experiments**: 1. Run TE on WebNLG with zero-shot base prompt, record F1. 2. Repeat with NKB=5 triplets added, compare F1. 3. Repeat with NKB=5 sentence-triplet pairs, compare F1 and check if F1 > experiment 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs in triplet extraction scale with increasing model size and context window?
- Basis in paper: [explicit] The paper states that "the size of the model appeared to only logarithmically improve the TE capabilities of the LLMs."
- Why unresolved: The paper only provides a qualitative observation of the scaling behavior. A more detailed quantitative analysis, including a plot of performance against model size, would provide a clearer understanding of the relationship.
- What evidence would resolve it: A comprehensive study with a wider range of model sizes and context window lengths, along with a quantitative analysis of the scaling behavior, would provide a more definitive answer.

### Open Question 2
- Question: What is the impact of the quality of the retrieved knowledge base context on the performance of LLMs in triplet extraction?
- Basis in paper: [explicit] The paper states that "the quality of the gathered KB context is strongly correlated with the final TE performance of the model."
- Why unresolved: The paper does not provide a detailed analysis of the relationship between the quality of the retrieved context and the performance of the LLMs. A more in-depth study, including different methods of retrieving and evaluating the quality of the context, would be needed.
- What evidence would resolve it: A study that systematically varies the quality of the retrieved context and measures its impact on the performance of the LLMs would provide a clearer understanding of the relationship.

### Open Question 3
- Question: How does the performance of LLMs in triplet extraction compare to that of fully supervised models on datasets with reduced training-validation-test overlap?
- Basis in paper: [inferred] The paper suggests that the performance of LLMs is strongly influenced by the quality of the retrieved context, which is affected by the overlap between the training, validation, and test splits of the datasets.
- Why unresolved: The paper only evaluates the performance of LLMs on datasets with substantial overlap. A comparison with fully supervised models on datasets with reduced overlap would provide a more realistic evaluation of the generalization capabilities of LLMs.
- What evidence would resolve it: A study that compares the performance of LLMs and fully supervised models on datasets with varying degrees of overlap would provide a clearer understanding of the generalization capabilities of LLMs.

## Limitations
- KB quality is critical - performance degrades substantially when KB coverage is sparse or retrieval fails
- Experimental scope limited to two datasets (WebNLG and NYT) with specific relation distributions
- Logarithmic scaling of model size suggests diminishing returns beyond certain parameter counts

## Confidence
- **High confidence**: KB context improves LLM TE performance when relevant context is available
- **Medium confidence**: Sentence-triplet pairs provide richer context than raw triplets
- **Medium confidence**: KB quality scaling matters more than model size scaling

## Next Checks
1. **KB coverage sensitivity test**: Systematically vary KB retrieval relevance by holding out portions of test data from the KB to quantify performance degradation as a function of KB overlap with test instances.
2. **Cross-domain generalization**: Evaluate the pipeline on a dataset with different entity types (e.g., biomedical or scientific literature) to assess whether the KB augmentation approach generalizes beyond WebNLG and NYT domains.
3. **Retrieval mechanism ablation**: Compare MiniLM-based retrieval against alternative encoders (e.g., sentence-transformers) and different similarity measures to determine if retrieval quality is the bottleneck in the current approach.