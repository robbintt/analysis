---
ver: rpa2
title: 'CLIMAX: An exploration of Classifier-Based Contrastive Explanations'
arxiv_id: '2307.00680'
source_url: https://arxiv.org/abs/2307.00680
tags:
- climax
- explanations
- lime
- data
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CLIMAX addresses the lack of contrastive, model-agnostic explanations\
  \ in XAI by proposing a local classifier-based explainer that contrasts why a predicted\
  \ class is chosen versus why another is not. The method employs label-aware surrogate\
  \ sampling\u2014using random oversampling and GMMs\u2014to generate a balanced dataset,\
  \ and leverages influence functions for sample efficiency."
---

# CLIMAX: An exploration of Classifier-Based Contrastive Explanations

## Quick Facts
- arXiv ID: 2307.00680
- Source URL: https://arxiv.org/abs/2307.00680
- Reference count: 36
- Primary result: CLIMAX provides more stable and contrastive explanations than LIME and variants across tabular, textual, and image datasets

## Executive Summary
CLIMAX addresses the lack of contrastive, model-agnostic explanations in XAI by proposing a local classifier-based explainer that contrasts why a predicted class is chosen versus why another is not. The method employs label-aware surrogate sampling—using random oversampling and GMMs—to generate a balanced dataset, and leverages influence functions for sample efficiency. Two variants are presented: L-CLIMAX (logistic regression) and CE-CLIMAX (cross-entropy loss). Empirically, CLIMAX outperforms LIME, BayLIME, and S-LIME in stability (Jaccard scores) across tabular, textual, and image datasets, and delivers more contrastive explanations by highlighting uncertain regions and class-specific features.

## Method Summary
CLIMAX is a model-agnostic XAI method that generates contrastive explanations by training a local logistic regression classifier on a balanced surrogate dataset. The method perturbs the index sample, applies random oversampling and GMM sampling to ensure class balance, then uses influence functions to identify and retain only the most influential samples. The resulting logistic regression model provides feature importance scores that directly contrast the predicted class against alternatives. The approach is demonstrated on tabular, textual, and image datasets, with two variants: L-CLIMAX using logistic loss and CE-CLIMAX using cross-entropy loss.

## Key Results
- CLIMAX achieves higher Jaccard consistency scores than LIME, BayLIME, and S-LIME across all tested datasets
- The method demonstrates sample efficiency through influence function-based subsampling while maintaining explanation quality
- CLIMAX provides more contrastive explanations by highlighting features that distinguish the predicted class from alternatives, particularly in uncertain regions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIMAX uses logistic regression to directly model class probabilities, ensuring contrastive explanations by incorporating both positive and negative class information in a single model.
- Mechanism: Instead of training separate regression models for each class as in LIME, CLIMAX trains a local logistic regression model that outputs probabilities for both classes simultaneously. This forces the explainer to distinguish between features that support the predicted class versus those that would support alternative classes.
- Core assumption: Logistic regression is a better local surrogate for classification tasks than linear regression because it directly models the decision boundary rather than just class-specific scores.
- Evidence anchors:
  - [abstract]: "Our method, which we refer to as CLIMAX which is short for Contrastive Label-aware Influence-based Model Agnostic XAI, is based on local classifiers"
  - [section]: "Alternately, we propose a contrastive explainer which is model-agnostic and perturbation-based... we focus on a local logistic regression model"
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If the black-box model's decision boundary is highly non-linear in the local region, the linear logistic approximation may fail to capture important contrastive features.

### Mechanism 2
- Claim: CLIMAX ensures label-aware surrogate sampling through random oversampling and GMM-based generation to create balanced datasets that include samples from all relevant classes.
- Mechanism: The method generates perturbations in the neighborhood of the index sample, then applies random oversampling of minority classes and GMM sampling to ensure the surrogate dataset contains examples from both the predicted class and alternative classes. This balanced dataset allows the logistic regression to learn meaningful contrastive boundaries.
- Core assumption: A balanced surrogate dataset is necessary for the logistic regression explainer to provide contrastive explanations that distinguish between the predicted class and alternatives.
- Evidence anchors:
  - [abstract]: "In order to ensure model fidelity of the explainer, we require the perturbations to be such that it leads to a class-balanced surrogate dataset"
  - [section]: "A classifier based explanation model necessitates that the surrogate samples form a balanced dataset, i.e., there are approximately equal number of samples from different classes"
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If the black-box model's decision boundary is very sharp or if classes are highly imbalanced in the original data, achieving a truly balanced surrogate dataset may be impossible or may introduce unrealistic samples.

### Mechanism 3
- Claim: CLIMAX uses influence functions for sample efficiency, identifying and retaining only the most influential perturbations for training the local explainer.
- Mechanism: After generating the balanced surrogate dataset, CLIMAX applies influence functions to compute how much each surrogate sample affects the explainer's predictions. Samples with low influence are removed, reducing computational cost while maintaining explanation quality.
- Core assumption: Not all perturbations are equally important for understanding the local decision boundary; influential samples can be identified and used to reduce computational complexity.
- Evidence anchors:
  - [abstract]: "Further, we propose influence subsampling in order to retaining effective samples and hence ensure sample complexity"
  - [section]: "Influence functions are a classic technique from robust statistics which trace a model's prediction through the learning algorithm and back to its training data thereby identifying training points most responible for a given prediction"
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If influence function estimates are noisy or if the local region is too complex, subsampling may remove important samples and degrade explanation quality.

## Foundational Learning

- Concept: Logistic regression and sigmoid functions
  - Why needed here: CLIMAX uses logistic regression as its local explainer model, requiring understanding of how logistic regression models probabilities and decision boundaries
  - Quick check question: What is the relationship between the logistic regression coefficients and the log-odds of class membership?

- Concept: Gaussian Mixture Models (GMMs) and sampling
  - Why needed here: CLIMAX uses GMMs to generate balanced surrogate samples when random oversampling is insufficient
  - Quick check question: How does a GMM model the data distribution as a weighted combination of Gaussian components?

- Concept: Influence functions and their computation
  - Why needed here: CLIMAX uses influence functions to identify the most important samples in the surrogate dataset
  - Quick check question: What does an influence function measure in the context of model parameters and data points?

## Architecture Onboarding

- Component map: Black-box classifier interface -> Surrogate data generator -> Influence function module -> Local logistic regression explainer -> Explanation post-processor

- Critical path:
  1. Generate perturbations around index sample
  2. Apply balancing techniques to ensure multi-class representation
  3. Compute influence scores and subsample
  4. Train local logistic regression on remaining samples
  5. Extract feature importance scores as explanations

- Design tradeoffs:
  - More surrogate samples improve stability but increase computation time
  - GMM sampling provides better balance but may generate unrealistic samples
  - Influence subsampling reduces computation but may remove important samples
  - L-CLIMAX (closed-form solution) vs CE-CLIMAX (iterative optimization)

- Failure signatures:
  - Poor stability (high Jaccard variance across runs) indicates issues with surrogate sampling or model fitting
  - Explanations that don't distinguish between classes suggests insufficient contrast in the surrogate data
  - Extremely high or low feature importance scores may indicate numerical instability in the logistic regression

- First 3 experiments:
  1. Run CLIMAX with varying numbers of surrogate samples (100, 500, 1000) on a simple binary classification dataset and measure stability via Jaccard scores
  2. Compare explanations from CLIMAX vs LIME on a text dataset, focusing on contrastive features that appear in one class but not the other
  3. Apply influence subsampling to a dataset with known influential samples and verify that explanation quality is maintained with fewer samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CLIMAX's performance compare to contrastive explainers like CEM in terms of interpretability and user trust?
- Basis in paper: [explicit] The paper compares CLIMAX with LIME and CEM on image datasets, highlighting CLIMAX's ability to show ambiguity and provide more contrastive explanations.
- Why unresolved: While the paper shows CLIMAX's superiority over LIME, a direct comparison with CEM in terms of interpretability and user trust is not fully explored.
- What evidence would resolve it: User studies or surveys comparing the interpretability and trust levels of explanations generated by CLIMAX and CEM.

### Open Question 2
- Question: Can CLIMAX be extended to handle multi-class classification problems beyond binary classification?
- Basis in paper: [inferred] The paper focuses on binary classification tasks and mentions the potential for contrastive explanations. However, it does not explicitly address multi-class scenarios.
- Why unresolved: The methodology and algorithms presented are primarily designed for binary classification, and extending them to multi-class problems would require additional considerations.
- What evidence would resolve it: Implementation and evaluation of CLIMAX on multi-class datasets, comparing its performance with existing multi-class explainers.

### Open Question 3
- Question: How does the choice of kernel width in CLIMAX affect the quality and stability of explanations?
- Basis in paper: [explicit] The paper mentions that CLIMAX, like LIME, uses hyperparameters such as kernel width that need to be tuned.
- Why unresolved: The paper does not provide a detailed analysis of how different kernel width values impact the quality and stability of explanations generated by CLIMAX.
- What evidence would resolve it: A systematic study varying the kernel width parameter and evaluating its impact on explanation quality and stability across different datasets and tasks.

### Open Question 4
- Question: What is the computational complexity of CLIMAX compared to other post-hoc explainers like LIME and SHAP?
- Basis in paper: [inferred] The paper discusses the sample efficiency of CLIMAX and mentions the use of influence functions for subsampling. However, a direct comparison of computational complexity with other methods is not provided.
- Why unresolved: The paper focuses on the sample efficiency aspect but does not explicitly compare the computational complexity of CLIMAX with other explainers.
- What evidence would resolve it: Benchmarking experiments measuring the runtime and computational resources required by CLIMAX, LIME, and SHAP for generating explanations on various datasets.

## Limitations
- The effectiveness of GMM-based sampling for maintaining realistic surrogate data remains unproven, particularly for high-dimensional datasets where generated samples may fall outside the data manifold
- Influence function computation for logistic regression in high dimensions may be computationally expensive and numerically unstable
- The paper lacks ablation studies to isolate the contribution of each component (balancing, influence subsampling, logistic regression) to overall performance

## Confidence

**High confidence:** CLIMAX provides more stable explanations than LIME variants across multiple datasets (Jaccard consistency scores show significant improvement)

**Medium confidence:** The contrastive nature of explanations is genuinely captured by the logistic regression formulation, though the mechanism could benefit from clearer mathematical proof

**Low confidence:** The sample efficiency claims via influence functions, as the paper does not provide runtime comparisons or computational complexity analysis

## Next Checks
1. Implement ablation study comparing CLIMAX variants with and without influence subsampling and GMM sampling to isolate their individual contributions to stability
2. Conduct runtime analysis measuring wall-clock time for surrogate sampling, influence computation, and model training across different dataset sizes
3. Perform qualitative evaluation of generated surrogate samples using t-SNE visualization to verify that GMM sampling produces realistic data points within the data manifold