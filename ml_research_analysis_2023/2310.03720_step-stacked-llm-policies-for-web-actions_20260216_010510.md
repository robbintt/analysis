---
ver: rpa2
title: 'SteP: Stacked LLM Policies for Web Actions'
arxiv_id: '2310.03720'
source_url: https://arxiv.org/abs/2310.03720
tags:
- text
- click
- actions
- button
- browser
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hierarchical LLM prompting framework, SteP,
  for solving complex web tasks. SteP defines a Markov Decision Process with a stack
  of LLM policies representing the control state.
---

# SteP: Stacked LLM Policies for Web Actions

## Quick Facts
- **arXiv ID**: 2310.03720
- **Source URL**: https://arxiv.org/abs/2310.03720
- **Reference count**: 40
- **Primary result**: Hierarchical LLM prompting framework that improves web task success rates by 14.9-33.5% over GPT-4 baselines

## Executive Summary
SteP introduces a hierarchical LLM prompting framework for solving complex web tasks through dynamic policy stacking. The system breaks down tasks into specialized low-level policies (e.g., FILL_TEXT, CHOOSE_DATE) that are composed by a high-level task planner. Unlike static hierarchies, SteP's control state is represented as a stack of policies that adapts to task complexity. Experiments on WebArena, MiniWoB++, and a CRM demonstrate significant performance improvements over state-of-the-art methods while using far less training data.

## Method Summary
SteP models web tasks as a Markov Decision Process where the state is a stack of LLM policies. The framework hierarchically composes policies by first generating a high-level task plan, then invoking sequences of low-level web policies. Demonstrations from human users performing web tasks are auto-labeled with low-level policies to generate prompts. The system uses in-context learning with few-shot examples rather than extensive fine-tuning, enabling efficient adaptation to new tasks. SteP's dynamic control stack adapts its depth based on task complexity, allowing it to handle both simple and complex tasks efficiently.

## Key Results
- SteP improves success rates by 14.9-33.5% on WebArena compared to state-of-the-art GPT-4 policy methods
- Matches or outperforms prior work on MiniWoB++ while using 15 examples versus 12k-2.2M examples required by other approaches
- Achieves competitive performance on a CRM task while demonstrating superior data efficiency
- Dynamic control stack adapts effectively to varying task complexities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SteP's hierarchical decomposition improves generalization across tasks by specializing low-level policies for reusable sub-tasks
- Mechanism: The system breaks complex tasks into sequences of specialized low-level policies (e.g., FILL_TEXT, CHOOSE_DATE). Each policy is trained on specific sub-tasks, allowing better generalization than monolithic approaches. The task planner dynamically composes these specialized policies based on task context.
- Core assumption: Low-level policies can be specialized for sub-tasks and reused across different high-level tasks without significant performance loss
- Evidence anchors: Abstract mentions SteP enables "dynamic control that adapts to the complexity of the task" and section discusses how few-shot learning breaks down complex tasks into reusable low-level policy calls

### Mechanism 2
- Claim: SteP's dynamic control stack adapts to task complexity better than static hierarchies
- Mechanism: Unlike fixed hierarchical structures, SteP represents control state as a stack of policies that can grow or shrink dynamically. This allows efficient handling of both simple tasks (shallow stack) and complex tasks (deeper stack) by adjusting the depth based on task requirements.
- Core assumption: Task complexity can be accurately reflected by policy stack depth, and the LLM can effectively manage this dynamic structure
- Evidence anchors: Abstract states SteP enables "dynamic control that adapts to the complexity of the task" while section notes how HeaP Few-shot achieves higher success rates through breaking down complex tasks

### Mechanism 3
- Claim: SteP's few-shot learning capability allows generalization to new tasks with minimal demonstrations
- Mechanism: By leveraging in-context learning with few-shot examples, SteP learns new tasks without extensive retraining. The hierarchical structure reduces the number of examples needed per policy, making the overall system efficient in learning new tasks.
- Core assumption: In-context learning is effective for web tasks, and hierarchical decomposition reduces the number of examples needed per policy
- Evidence anchors: Abstract reports significant performance improvements while using much less data, and section shows HeaP matches priors with 15 examples versus 12k-2.2M examples in other approaches

## Foundational Learning

- **Concept: Markov Decision Process (MDP)**
  - Why needed here: SteP models the web task as an MDP where the state is a stack of policies, allowing for dynamic control and decision-making
  - Quick check question: What are the components of an MDP, and how does SteP's representation of the state as a policy stack fit into this framework?

- **Concept: In-context learning**
  - Why needed here: SteP leverages in-context learning to perform tasks with few demonstrations, avoiding the need for extensive retraining
  - Quick check question: How does in-context learning differ from traditional fine-tuning, and what are the advantages and limitations of using it for web tasks?

- **Concept: Hierarchical decomposition**
  - Why needed here: SteP decomposes complex tasks into a hierarchy of sub-tasks, each handled by a specialized policy, enabling better generalization and adaptability
  - Quick check question: What are the benefits and challenges of hierarchical decomposition in reinforcement learning, and how does SteP address these challenges?

## Architecture Onboarding

- **Component map**: Task Planner -> Low-Level Policies (FILL_TEXT, CHOOSE_DATE, etc.) -> LLM Execution -> Browser Interface
- **Critical path**: 1) Receive task context and initial state, 2) Task planner generates sequence of low-level policy calls, 3) Execute each low-level policy in sequence updating state and previous actions, 4) Repeat until task completion
- **Design tradeoffs**: Hierarchical vs. monolithic (specialization vs. complexity), dynamic vs. static control (adaptability vs. decision complexity), few-shot vs. fine-tuning (efficiency vs. robustness)
- **Failure signatures**: Task planner generates incorrect policy sequences, low-level policies fail due to ambiguous instructions or unexpected webpage states, LLM cannot manage dynamic policy stack effectively
- **First 3 experiments**: 1) Test task planner's ability to generate correct policy sequences for simple tasks, 2) Evaluate individual low-level policies on their specific sub-tasks, 3) Assess system's ability to handle tasks of increasing complexity

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored including the optimal granularity of low-level policies, the limits of task complexity that can be handled, and the scalability of the approach to more diverse web interfaces.

## Limitations
- Exact prompt structures and few-shot examples are not fully specified, making replication challenging
- Limited ablation studies on optimal number and specialization boundaries of low-level policies
- No direct comparison against single LLM policy baseline that handles all web actions directly
- Generalization claims across different web interfaces need more systematic validation

## Confidence
- **High Confidence**: The core hierarchical decomposition approach and its implementation are well-described with sound experimental methodology
- **Medium Confidence**: Few-shot learning efficiency claims are supported by data but comparison to other few-shot methods could be more comprehensive
- **Medium Confidence**: Dynamic control stack mechanism is theoretically sound but needs more detailed analysis of stack depth changes

## Next Checks
1. **Prompt Structure Validation**: Conduct controlled experiments varying prompt formats and few-shot example selection for both high-level and low-level policies to identify optimal configurations
2. **Cross-Interface Generalization Test**: Systematically test SteP's performance across a wider variety of web interfaces beyond the three mentioned, measuring how well specialized low-level policies transfer
3. **Dynamic Stack Analysis**: Implement detailed logging of stack depth changes during task execution to empirically validate that the dynamic control mechanism is actually adapting to task complexity as claimed