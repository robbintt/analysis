---
ver: rpa2
title: 'Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning'
arxiv_id: '2311.11551'
source_url: https://arxiv.org/abs/2311.11551
tags:
- domain
- target
- sentence
- entity
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies unsupervised domain adaptation (UDA) for large
  language models under an in-context learning (ICL) setting, where labeled source
  data and unlabeled target data are available but target labels are not. The authors
  propose a retrieval-augmented ICL framework that retrieves semantically similar
  target examples to enrich source input contexts, and trains models to jointly learn
  task discrimination and target domain distribution via language modeling.
---

# Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning

## Quick Facts
- arXiv ID: 2311.11551
- Source URL: https://arxiv.org/abs/2311.11551
- Authors: 
- Reference count: 16
- Key outcome: Retrieval-augmented ICL significantly outperforms baselines for UDA, with fine-tuning still beneficial for LLMs

## Executive Summary
This paper addresses unsupervised domain adaptation (UDA) for large language models under an in-context learning setting. The proposed method retrieves semantically similar target examples to enrich source input contexts, enabling models to learn both task discrimination and target domain distribution simultaneously. Experiments on NER and sentiment analysis demonstrate significant performance improvements over baseline approaches, while also revealing that retrieval of out-of-distribution demonstrations fails for large language models, suggesting fine-tuning remains beneficial for UDA.

## Method Summary
The method involves a retrieval-augmented in-context learning framework that retrieves top-k target domain examples semantically similar to source inputs. These retrieved contexts are concatenated with source inputs to form augmented contexts. The model is trained on two objectives: task discrimination using source labels and language modeling on target contexts. For encoder-only models (XLM-RoBERTa-large), masked language modeling predicts target context tokens, while decoder-only models (LLaMA-7B) use causal language modeling. CRF layers are added for NER tasks, and LoRA enables parameter-efficient fine-tuning for decoder-only models.

## Key Results
- Retrieval-augmented ICL significantly outperforms baseline methods on NER and sentiment analysis tasks
- Fine-tuning smaller models (RoBERTa) achieves better results than inference-only approaches for LLMs
- Adaptive ICL outperforms adaptive pre-training by simultaneously learning task and language modeling objectives
- Retrieval of out-of-distribution demonstrations fails for large language models, highlighting the importance of fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieval-augmented ICL bridges the domain gap by providing semantically similar target contexts that help the model learn target distribution while maintaining task discrimination.
- **Mechanism:** The model learns two objectives simultaneously: (1) task discrimination using source labels and (2) language modeling on target contexts. The target contexts serve as demonstrations that reduce surface form differences between domains.
- **Core assumption:** Retrieved target contexts are semantically similar enough to the source input to provide useful context without introducing contradictory information.
- **Evidence anchors:**
  - [abstract]: "The core idea is to retrieve a subset of cross-domain elements that are the most similar to the query, and elicit language model to adapt in an in-context manner by learning both target domain distribution and the discriminative task signal simultaneously with the augmented cross-domain in-context examples."
  - [section 3.1.2]: "The first objective (1) aims to learn task discrimination with the help of context... The second objective (2) encourages the model to learn the target distribution by predicting tokens in the target context xT."
  - [corpus]: Found 25 related papers, FMR=0.42, but none directly support this specific mechanism.
- **Break condition:** If retrieved contexts are semantically dissimilar or contain contradictory information, the model may learn incorrect patterns or fail to bridge the domain gap.

### Mechanism 2
- **Claim:** Fine-tuning smaller LMs is more effective than inference-only approaches for UDA because it allows updating domain-specific knowledge.
- **Mechanism:** Fine-tuning updates model parameters to adapt to target domain distribution, while inference-only approaches rely on pre-trained knowledge that may not generalize well to new domains.
- **Core assumption:** The domain shift is significant enough that pre-trained knowledge is insufficient for good performance.
- **Evidence anchors:**
  - [abstract]: "Experiments on NER and sentiment analysis show that the proposed method significantly outperforms baselines, and surprisingly, retrieving out-of-distribution demonstrations fails for large language models, suggesting fine-tuning is still beneficial for UDA."
  - [section 4.4]: "In NER experiments, ChatGPT achieves very low performances, but fine-tuning a much smaller RoBERTa model achieves state-of-the-art scores in most adaptation scenarios."
  - [corpus]: No direct evidence found in corpus about this specific claim.
- **Break condition:** If the domain shift is minimal or if the pre-trained model already has sufficient knowledge of the target domain, fine-tuning may not provide significant benefits.

### Mechanism 3
- **Claim:** Adaptive ICL outperforms adaptive pre-training because it allows simultaneous learning of task and language modeling objectives.
- **Mechanism:** By mixing source input with target contexts and learning both objectives simultaneously, the model can better fuse distributions from different domains.
- **Core assumption:** Simultaneous learning of both objectives is more effective than sequential learning.
- **Evidence anchors:**
  - [section 4.5]: "No ICL is identical to the second stage in adaptive pre-training... We could observe that pre-training only gains marginal benefits for SA tasks compared with No-ICL."
  - [section 3.1.2]: "By mixing with a source input, the model learns to fuse the distributions from two different domains in order to bridge the domain gap."
  - [corpus]: No direct evidence found in corpus about this specific claim.
- **Break condition:** If the task and language modeling objectives conflict or if one dominates the other, simultaneous learning may be less effective than sequential approaches.

## Foundational Learning

- **Concept: Unsupervised Domain Adaptation**
  - Why needed here: The paper addresses the challenge of adapting models from labeled source domains to unlabeled target domains without access to target labels.
  - Quick check question: What is the key difference between supervised and unsupervised domain adaptation?

- **Concept: In-Context Learning**
  - Why needed here: The paper leverages ICL to adapt language models using retrieved target contexts as demonstrations without fine-tuning.
  - Quick check question: How does in-context learning differ from traditional fine-tuning approaches?

- **Concept: Language Modeling Mechanisms**
  - Why needed here: The paper uses different language modeling mechanisms (MLM for encoder-only, CLM for decoder-only) to learn target distribution.
  - Quick check question: What are the key differences between masked language modeling and causal language modeling?

## Architecture Onboarding

- **Component map:**
  Retriever (SimCSE/BERTScore) -> Encoder-only LM (XLM-RoBERTa-large) with CRF or Decoder-only LM (LLaMA-7B) with LoRA -> Evaluation on target domain

- **Critical path:**
  1. Retrieve top-k target contexts using semantic similarity
  2. Concatenate source input with retrieved contexts
  3. Train model on combined objective (task discrimination + language modeling)
  4. Evaluate on target domain test set

- **Design tradeoffs:**
  - Encoder-only vs decoder-only: Different language modeling mechanisms and prompting strategies
  - Retrieval vs random sampling: Semantic similarity vs. diversity of contexts
  - Fine-tuning vs inference-only: Parameter updates vs. relying on pre-trained knowledge

- **Failure signatures:**
  - Low performance despite high semantic similarity: Retrieved contexts may be irrelevant or contradictory
  - No improvement over baseline: Domain shift may be too small or retrieval system may be ineffective
  - Degradation in performance: Objectives may conflict or retrieved contexts may introduce noise

- **First 3 experiments:**
  1. Compare DAICL with random context sampling to validate the importance of semantic similarity
  2. Test encoder-only vs decoder-only models on the same task to understand architectural differences
  3. Evaluate the impact of different k values (number of retrieved contexts) on adaptation performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different retrieval methods (e.g., SimCSE vs BERTScore) impact the effectiveness of in-context learning across various NLP tasks?
- Basis in paper: The paper mentions using SimCSE for sentiment analysis and BERTScore for named entity recognition, but does not extensively compare these retrieval methods.
- Why unresolved: The paper focuses on the overall effectiveness of retrieval-augmented in-context learning but does not delve into the nuances of how different retrieval methods might perform across tasks.
- What evidence would resolve it: Comparative experiments using multiple retrieval methods on a variety of NLP tasks, analyzing the impact on in-context learning performance.

### Open Question 2
- Question: Can the proposed domain-adaptive in-context learning framework be extended to other NLP tasks beyond named entity recognition and sentiment analysis?
- Basis in paper: The paper demonstrates the framework's effectiveness on NER and SA tasks, suggesting potential applicability to other tasks.
- Why unresolved: The paper does not explore the framework's applicability to a broader range of NLP tasks.
- What evidence would resolve it: Experiments applying the framework to tasks such as machine translation, question answering, or summarization, evaluating performance improvements.

### Open Question 3
- Question: How does the choice of the number of retrieved examples (k) affect the performance of in-context learning in domain adaptation?
- Basis in paper: The paper sets k=5 for top-k retrieval but does not investigate the impact of varying this parameter.
- Why unresolved: The optimal number of retrieved examples for effective in-context learning is not determined.
- What evidence would resolve it: Experiments with different values of k, analyzing the trade-off between performance and computational cost.

### Open Question 4
- Question: What are the long-term effects of in-context learning on the generalization capabilities of language models?
- Basis in paper: The paper focuses on immediate performance improvements but does not address the potential long-term impact on model generalization.
- Why unresolved: The paper does not provide insights into how in-context learning might affect a model's ability to generalize to new, unseen data over time.
- What evidence would resolve it: Longitudinal studies tracking model performance on diverse datasets after repeated exposure to in-context learning, assessing changes in generalization capabilities.

## Limitations

- Data Scale Constraints: The study uses relatively small datasets for domain adaptation, and the effectiveness on larger-scale tasks remains unproven.
- Retrieval Quality Dependencies: The framework's success heavily depends on the quality of retrieved contexts, which is not thoroughly explored.
- Architectural Constraints: The method is evaluated primarily on encoder-only and decoder-only architectures, with limited exploration of other model types.

## Confidence

**High Confidence (8/10):** The core retrieval-augmented ICL mechanism is well-supported by experimental results. The ablation studies provide strong evidence that combining source task signals with target domain contexts improves adaptation performance compared to baselines.

**Medium Confidence (6/10):** The claim that fine-tuning outperforms inference-only approaches for large models is supported but could benefit from more extensive testing across different model sizes and domain pairs.

**Low Confidence (4/10):** The assertion that adaptive ICL outperforms adaptive pre-training requires more rigorous comparison, as the experimental setup doesn't fully control for differences in training objectives, data usage, and computational budgets.

## Next Checks

1. **Retrieval Robustness Test:** Systematically evaluate the framework's performance when retrieval quality degrades (e.g., by adding noise to target contexts or using weaker retrievers). This would quantify how sensitive the approach is to retrieval quality.

2. **Cross-Task Generalization:** Apply the framework to at least two additional NLP tasks (e.g., question answering and text classification) with different domain pairs to test generalizability beyond NER and sentiment analysis.

3. **Scale-Up Experiment:** Test the framework on a larger-scale domain adaptation problem (e.g., adapting from Wikipedia to scientific literature or from news to social media) to evaluate scalability and identify potential bottlenecks.