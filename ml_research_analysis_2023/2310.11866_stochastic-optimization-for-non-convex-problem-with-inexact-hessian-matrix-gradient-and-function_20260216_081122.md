---
ver: rpa2
title: Stochastic Optimization for Non-convex Problem with Inexact Hessian Matrix,
  Gradient, and Function
arxiv_id: '2310.11866'
source_url: https://arxiv.org/abs/2310.11866
tags:
- hessian
- gradient
- function
- lemma
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes stochastic versions of trust-region (STR) and
  adaptive regularization using cubics (SARC) methods that can handle inexact Hessian,
  gradient, and function value computations. The algorithms achieve the same iteration
  complexity order as their exact counterparts while reducing computational overhead
  by leveraging subsampling techniques.
---

# Stochastic Optimization for Non-convex Problem with Inexact Hessian Matrix, Gradient, and Function

## Quick Facts
- arXiv ID: 2310.11866
- Source URL: https://arxiv.org/abs/2310.11866
- Reference count: 40
- One-line primary result: Proposes stochastic trust-region and adaptive regularization using cubics methods with inexact computations achieving same iteration complexity as exact methods.

## Executive Summary
This paper introduces stochastic versions of trust-region (STR) and adaptive regularization using cubics (SARC) methods that can handle inexact Hessian, gradient, and function value computations. The algorithms leverage subsampling techniques to reduce computational overhead per iteration while maintaining the same iteration complexity order as their exact counterparts. The key insight is that error bounds for these estimates can be controlled probabilistically using operator Bernstein inequalities, allowing convergence analysis to proceed similarly to the exact case. The methods achieve iteration complexity of O(max{(εH − εB)−1(ε∇f − εg)−2, (εH − εB)−3}) for STR and O(max{(ε∇f − εg)−3/2, (εH − εB)−3}) for SARC under certain conditions.

## Method Summary
The paper proposes stochastic trust-region (STR) and adaptive regularization using cubics (SARC) methods for non-convex optimization problems where function values, gradients, and Hessian matrices can only be computed inexactly. The methods use subsampling to estimate these quantities, with sample sizes determined by operator Bernstein inequalities to ensure probabilistic error bounds. The STR algorithm adjusts trust-region radii based on a modified parameter ρ̃ that measures approximation quality, while SARC adjusts cubic regularization parameters similarly. Both algorithms achieve (ε∇f, εH)-optimality with iteration complexity matching their exact counterparts, reducing computational overhead by requiring fewer propagations per iteration.

## Key Results
- STR achieves iteration complexity O(max{(εH − εB)−1(ε∇f − εg)−2, (εH − εB)−3})
- SARC achieves iteration complexity O(max{(ε∇f − εg)−3/2, (εH − εB)−3})
- Experimental results show improved performance compared to existing second-order methods on non-convex least squares and deep learning tasks
- Algorithms reduce computational overhead while maintaining same iteration complexity as exact methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stochastic STR and SARC achieve same iteration complexity as exact methods using subsampling
- Mechanism: Subsampling reduces computational overhead while probabilistic error bounds from operator Bernstein inequalities ensure convergence analysis proceeds similarly to exact case
- Core assumption: Error bounds for Hessian and gradient approximations can be controlled probabilistically
- Evidence anchors: Abstract states algorithms achieve same iteration complexity order as exact computations; section confirms theoretical results

### Mechanism 2
- Claim: Modified parameter ρ̃ determines quality of approximation and enables adaptive adjustment
- Mechanism: Compares approximate function values with quadratic model to measure approximation quality and adjust parameters
- Core assumption: Approximate function h(x) satisfies |f(xk) − h(xk)| ≤ εh‖sk‖²
- Evidence anchors: Abstract mentions modified parameter within STR and SARC; section describes its use in determining parameter changes

### Mechanism 3
- Claim: New sub-sampling technique enables automatic adjustment of trust-region and regularization parameters
- Mechanism: Uses upper bound for subsampled function value from operator Bernstein inequality to adjust parameters
- Core assumption: Subsampled function value can be bounded using operator Bernstein inequality
- Evidence anchors: Abstract mentions random sampling technology; section introduces upper bound technique

## Foundational Learning

- Concept: Lipschitz continuity of gradient and Hessian matrix
  - Why needed here: Establishes error bounds for gradient and Hessian approximations crucial for convergence analysis
  - Quick check question: What is the Lipschitz constant for a function f(x) satisfying ‖∇f(x) − ∇f(y)‖ ≤ L∇f‖x − y‖ for all x, y ∈ Rd?

- Concept: Operator Bernstein inequality
  - Why needed here: Provides error bounds for subsampled function values crucial for convergence analysis and parameter adjustment
  - Quick check question: What is the operator Bernstein inequality and how does it bound subsampled function value errors?

- Concept: Second-order optimality conditions
  - Why needed here: Algorithms aim for ε-approximate second-order optimality requiring understanding gradient-Hessian relationships
  - Quick check question: What are second-order optimality conditions for non-convex f(x) and how do they relate to gradient and Hessian?

## Architecture Onboarding

- Component map: Sub-sampling technique -> Function/gradient/Hessian estimation -> Search direction computation -> Parameter adjustment -> Convergence check

- Critical path: 1) Estimate function, gradient, Hessian using sub-sampling; 2) Compute search direction and update solution; 3) Adjust trust-region radius or regularization parameter; 4) Repeat until convergence

- Design tradeoffs: Computational cost vs. accuracy (more samples improve accuracy but increase cost); convergence rate vs. robustness (tighter bounds improve rate but reduce noise robustness)

- Failure signatures: Sub-sampling failing to meet error bounds causes slow convergence or divergence; inability to establish function-derivative relationships breaks convergence analysis

- First 3 experiments: 1) Verify sub-sampling provides accurate estimates with high probability; 2) Test STR/SARC convergence on simple non-convex problem with known solution; 3) Compare STR/SARC with exact methods on large-scale non-convex problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does STR/SARC performance scale with problem dimension d when using same subsampling rate?
- Basis in paper: [inferred] Paper mentions log(2d/δ0) terms in sample size bounds but doesn't analyze dimension scaling
- Why unresolved: Theoretical analysis uses fixed approximation errors rather than analyzing scaling with d
- What evidence would resolve it: Experiments varying dimension while holding other parameters constant, or theoretical analysis of approximation error scaling

### Open Question 2
- Question: What is optimal relationship between sample sizes |Sh|, |Sg|, and |SB|?
- Basis in paper: [inferred] Different sample sizes used but optimal allocation not analyzed
- Why unresolved: Theoretical analysis focuses on worst-case bounds without exploring practical trade-offs
- What evidence would resolve it: Systematic experiments varying relative sizes, or theoretical analysis of optimal allocation

### Open Question 3
- Question: How robust are convergence guarantees to violations of Lipschitz continuity assumptions?
- Basis in paper: [explicit] Analysis relies heavily on Lipschitz continuity but doesn't explore violations
- Why unresolved: No analysis or experimental results for non-Lipschitz cases
- What evidence would resolve it: Experiments on non-Lipschitz problems, or analysis of convergence rate degradation

### Open Question 4
- Question: What is impact of trust-region/ARC parameter update rules on practical performance?
- Basis in paper: [explicit] Specific update rules used but impact not analyzed
- Why unresolved: Analysis proves convergence for given rules without exploring alternatives
- What evidence would resolve it: Experiments comparing different update strategies, or analysis of convergence rate impact

## Limitations
- Assumes access to stochastic oracles that may require careful tuning in practice
- Convergence analysis relies on specific error bounds that may not hold in all scenarios
- Limited experimental validation on diverse non-convex optimization problems

## Confidence
- High: Theoretical iteration complexity bounds for STR and SARC methods
- Medium: Practical performance on non-convex least squares and deep learning tasks
- Medium: Implementation details for stochastic oracles and subsampling techniques

## Next Checks
1. Implement and test stochastic oracles for function values, gradients, and Hessian matrices on various non-convex problems to assess accuracy and overhead
2. Conduct extensive experiments comparing STR/SARC with exact counterparts and state-of-the-art methods on large-scale non-convex problems including deep learning tasks
3. Analyze algorithm sensitivity to parameter choices and explore automatic parameter tuning strategies