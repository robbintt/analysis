---
ver: rpa2
title: 'FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning
  with Hypergradient Descent'
arxiv_id: '2310.03156'
source_url: https://arxiv.org/abs/2310.03156
tags:
- learning
- local
- rate
- edhyper
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents FedHyper, a universal and robust learning
  rate scheduler for federated learning (FL) that uses hypergradient descent to adaptively
  adjust both global and local learning rates during training. FedHyper addresses
  the challenges of scheduling learning rates in FL, which involves two types of learning
  rates: global rates on the server and local rates on clients.'
---

# FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent

## Quick Facts
- arXiv ID: 2310.03156
- Source URL: https://arxiv.org/abs/2310.03156
- Reference count: 36
- Universal learning rate scheduler that improves FL convergence 1.1-3x faster than FedAvg and increases accuracy by up to 15%

## Executive Summary
This paper presents FedHyper, a novel learning rate scheduler for federated learning that uses hypergradient descent to adaptively adjust both global and local learning rates during training. FedHyper addresses the challenge of scheduling learning rates in FL by treating the problem as a bilevel optimization task and deriving hypergradients from the relationship between optimization objectives and learning rates. The approach consists of three schedulers - global, server-side local, and client-side local - that work together to ensure robust and efficient convergence. Extensive experiments demonstrate that FedHyper consistently outperforms existing methods across vision and language benchmark datasets, particularly under suboptimal initial learning rate configurations.

## Method Summary
FedHyper implements a three-level learning rate scheduling framework for federated learning. The global scheduler adjusts the server-side learning rate based on aggregated pseudo-gradients from client updates. The server-side local scheduler modifies the global learning rate used for aggregating client updates. The client-side local scheduler independently adjusts each client's local learning rate during their local optimization steps. All schedulers use hypergradient descent to compute learning rate updates, with bound clipping mechanisms to ensure convergence stability. The algorithm requires no additional hyper-parameters beyond the clipping bounds and can be integrated with existing FL optimization algorithms.

## Key Results
- FedHyper converges 1.1-3x faster than FedAvg across all tested datasets
- Achieves up to 15% higher accuracy under suboptimal initial learning rate settings
- Maintains robustness to random initial learning rate configurations
- Can be integrated with existing FL optimization algorithms for further performance improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FedHyper adjusts global learning rate based on pseudo gradients derived from aggregated local updates
- Mechanism: The algorithm treats the average of client updates as a proxy for the global gradient, then computes hypergradients as the inner product between current and previous pseudo gradients
- Core assumption: Aggregated client updates can approximate the global gradient direction for learning rate scheduling
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If client updates become uncorrelated with global gradient due to extreme data heterogeneity or client drift

### Mechanism 2
- Claim: Client-side local scheduler prevents learning rate divergence by incorporating global update direction
- Mechanism: Each client's local learning rate is adjusted using both local gradient products and global update alignment
- Core assumption: Local and global optimization objectives maintain directional correlation during training
- Evidence anchors: [section], [section], [corpus]
- Break condition: When local objectives become completely misaligned with global objective

### Mechanism 3
- Claim: Bound clipping on learning rates ensures convergence while maintaining adaptability
- Mechanism: Learning rates are clipped between 1/γα and γα (global) or 1/γβ and γβ (local)
- Core assumption: Convergence guarantees hold when learning rates stay within bounded ranges
- Evidence anchors: [section], [section], [corpus]
- Break condition: If optimal learning rates fall outside the clipping bounds for extended periods

## Foundational Learning

- Concept: Hypergradient descent
  - Why needed here: Provides theoretical foundation for adaptive learning rate scheduling based on gradient dynamics
  - Quick check question: What is the mathematical relationship between learning rate hypergradient and consecutive gradients?

- Concept: Federated optimization challenges
  - Why needed here: Explains why standard hypergradient methods fail in FL and require modification
  - Quick check question: What are the two types of learning rates in FL and why do they need separate scheduling?

- Concept: Convergence analysis in non-iid settings
  - Why needed here: Validates that FedHyper maintains convergence guarantees despite heterogeneous data
  - Quick check question: How do smoothness and bounded dissimilarity assumptions affect convergence bounds?

## Architecture Onboarding

- Component map: Server-side (global scheduler + server-side local scheduler) ↔ Clients (client-side local scheduler) with bound clipping mechanisms at each level
- Critical path: Server aggregates updates → computes global hypergradient → updates global/local rates → sends rates to clients → clients compute local hypergradients → update local rates → send local updates → repeat
- Design tradeoffs: Client-side scheduler provides finer control but higher computational overhead vs server-side scheduler being lighter but less adaptive
- Failure signatures: Oscillating learning rates (hypergradient computation errors), premature convergence (over-aggressive clipping), accuracy plateaus (insufficient adaptation)
- First 3 experiments:
  1. Test global scheduler alone with synthetic FL setup to verify pseudo-gradient approximation
  2. Validate client-side scheduler on iid data to confirm local-global synchronization
  3. Run full FedHyper on small-scale non-iid benchmark to observe convergence behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FedHyper's performance scale with the number of clients in federated learning, particularly in scenarios with very large numbers of clients (e.g., 10,000+)?
- Basis in paper: [inferred] The paper does not explore the performance of FedHyper with a large number of clients. It only considers 100 clients in the experiments.
- Why unresolved: The paper focuses on a fixed number of clients (100) and does not investigate how FedHyper performs as the number of clients increases significantly.
- What evidence would resolve it: Experiments comparing FedHyper's performance with varying numbers of clients, especially in large-scale scenarios, would provide insights into its scalability and potential limitations.

### Open Question 2
- Question: How does FedHyper handle heterogeneous data distributions across clients, particularly in scenarios where some clients have significantly different data distributions compared to others?
- Basis in paper: [explicit] The paper mentions that data heterogeneity is a challenge in federated learning, but it does not provide detailed analysis of how FedHyper performs in highly heterogeneous scenarios.
- Why unresolved: The experiments in the paper use a Dirichlet distribution with α = 0.5 to partition data, which may not fully capture the complexity of real-world heterogeneous data distributions.
- What evidence would resolve it: Experiments with various levels of data heterogeneity, including scenarios with extreme differences in data distributions across clients, would help understand FedHyper's robustness and adaptability.

### Open Question 3
- Question: How does FedHyper perform in federated learning scenarios with non-convex loss functions, such as those encountered in deep learning tasks?
- Basis in paper: [inferred] The paper focuses on the theoretical convergence analysis of FedHyper, which is typically applicable to convex optimization problems. However, deep learning tasks often involve non-convex loss functions.
- Why unresolved: The convergence analysis in the paper assumes convexity, which may not hold for deep learning tasks. The paper does not provide empirical evidence of FedHyper's performance on non-convex problems.
- What evidence would resolve it: Experiments evaluating FedHyper's performance on deep learning tasks with non-convex loss functions, such as image classification with convolutional neural networks or natural language processing with recurrent neural networks, would help assess its effectiveness in practical scenarios.

## Limitations
- Performance relies heavily on specific hyperparameter settings (γα=3, γβ=10) that may not generalize across different FL scenarios
- Convergence proof assumes bounded dissimilarity and smoothness conditions that may not hold in practice with highly heterogeneous client data
- Server-side scheduler's reliance on aggregated pseudo-gradients as proxies for true gradients introduces approximation error that isn't fully characterized

## Confidence

- **High confidence:** The three-scheduler architecture and bound clipping mechanisms are clearly specified and theoretically justified
- **Medium confidence:** Convergence guarantees under stated assumptions, though practical violations are likely
- **Low confidence:** Claims about superior performance across all datasets and learning rate configurations without systematic ablation studies

## Next Checks

1. **Ablation study:** Test each scheduler independently on synthetic FL data to isolate their individual contributions to convergence
2. **Robustness analysis:** Systematically vary the clipping bounds (γα, γβ) across orders of magnitude to identify sensitivity thresholds
3. **Heterogeneity stress test:** Evaluate performance under extreme non-iid conditions (ρ2 → 1) to validate theoretical bounds in practice