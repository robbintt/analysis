---
ver: rpa2
title: 'ParroT: Translating during Chat using Large Language Models tuned with Human
  Translation and Feedback'
arxiv_id: '2304.02426'
source_url: https://arxiv.org/abs/2304.02426
tags:
- translation
- instruction
- llms
- which
- translations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ParroT, a framework for enhancing and regulating
  translation abilities of open-source large language models (LLMs) like LLaMA-7b.
  The core idea is to reformulate translation data into instruction-following format
  and introduce a "Hint" field for extra requirements.
---

# ParroT: Translating during Chat using Large Language Models tuned with Human Translation and Feedback

## Quick Facts
- arXiv ID: 2304.02426
- Source URL: https://arxiv.org/abs/2304.02426
- Reference count: 4
- Primary result: ParroT framework improves translation performance of LLaMA-7b through instruction tuning, especially for English-to-other-languages directions

## Executive Summary
This paper introduces ParroT, a framework for enhancing and regulating translation abilities of open-source large language models like LLaMA-7b. The core idea is to reformulate translation data into instruction-following format with a "Hint" field for extra requirements. Three instruction types are proposed: translation, contrastive, and error-guided. Experiments show that translation instruction significantly improves performance, especially for English-to-other-languages directions. Error-guided instruction further improves results by learning from low-quality human-annotated translations. However, contrastive instruction did not work as expected due to subtle quality differences between competitive WMT systems. ParroT models can also preserve general task abilities when including multi-task data in fine-tuning.

## Method Summary
The ParroT framework reformulates translation data into instruction-following style with a "Hint" field for incorporating extra requirements to regulate the translation process. Three instruction types are proposed: translation, contrastive, and error-guided. The framework fine-tunes LLaMA-7b models using HuggingFace Transformers with DeepSpeed ZeRO stage 3. The training procedure involves reformulating translation data, creating three instruction types, and fine-tuning for 3 epochs on the combined dataset with Alpaca multi-task data to preserve general task abilities.

## Key Results
- Translation instruction significantly improves translation performance, especially for English-to-other-languages directions
- Error-guided instruction leads to further improvement by learning from low-quality human-annotated translations
- ParroT models can preserve general task abilities when including multi-task data in fine-tuning
- Contrastive instruction did not work as expected due to subtle quality differences between competitive WMT systems

## Why This Works (Mechanism)

### Mechanism 1
Reformulating translation data into instruction-following format with "Hint" fields enables LLMs to better understand translation tasks by explicitly separating task context, input, and response. The "Hint" field allows for additional constraints or quality indicators that guide the model toward desired behaviors. Core assumption: LLMs benefit from structured task specification and can effectively incorporate hint-based guidance during fine-tuning. Evidence: Abstract and section describing instruction format with preface, instruction, input, hint, and response fields. Break condition: If hints become too complex or contradictory, the model may struggle to reconcile multiple requirements or ignore hint information entirely.

### Mechanism 2
Error-guided instruction improves translation quality by teaching models to avoid specific error types. By exposing the model to low-quality translations annotated with specific error types (accuracy, mistranslation, fluency, grammar), the model learns to recognize and avoid these patterns when generating new translations. Core assumption: Models can generalize from error examples to avoid similar mistakes in novel translations. Evidence: Abstract mentioning learning from low-quality translations annotated by human, and section describing use of translations with errors annotated by <v></v> span. Break condition: If error annotations are inconsistent or error types are too numerous or overlapping, the model may become confused about which errors to prioritize.

### Mechanism 3
Maintaining general task abilities through multi-task fine-tuning prevents catastrophic forgetting. Including the Alpaca multi-task dataset during fine-tuning ensures the model retains capabilities beyond translation, preserving the general instruction-following abilities of the base LLM. Core assumption: Fine-tuning on diverse tasks with proper balancing can maintain multiple capabilities without significant degradation in any single task. Evidence: Abstract mentioning preservation of general tasks with Alpaca dataset, and section describing preservation of capabilities like question answering and code generation. Break condition: If multi-task data is too small relative to translation data, or tasks are too dissimilar, the model may still experience catastrophic forgetting of certain abilities.

## Foundational Learning

- Concept: Instruction Tuning
  - Why needed here: Base LLaMA model needs adaptation from general language modeling to following specific instructions for translation tasks
  - Quick check question: What is the difference between pre-training and instruction tuning in the context of LLMs?

- Concept: Human Evaluation Metrics
  - Why needed here: Quality assessment of translations requires understanding of human-centric evaluation frameworks like MQM (Multidimensional Quality Metrics)
  - Quick check question: How does MQM differ from automatic metrics like BLEU or COMET in evaluating translation quality?

- Concept: Beam Search vs Sampling
  - Why needed here: Different search strategies impact translation quality and computational efficiency during inference
  - Quick check question: When might sampling be preferred over beam search despite potentially lower BLEU scores?

## Architecture Onboarding

- Component map: Base LLaMA model -> Instruction reformulation pipeline -> Hint integration module -> Fine-tuning trainer with multi-task support -> Inference engine with search strategy selection
- Critical path: Data preparation -> Instruction reformulation -> Fine-tuning with balanced task mixing -> Evaluation on test sets -> Inference with appropriate search strategy
- Design tradeoffs: Beam search vs sampling balances quality against computational cost; including more error types in hints increases regulation but may confuse the model; multi-task fine-tuning preserves general abilities but may slow translation-specific learning
- Failure signatures: Degraded general task performance suggests imbalance in multi-task fine-tuning; poor translation quality despite instruction tuning may indicate issues with hint quality or search strategy; OOM errors suggest batch size or sequence length issues
- First 3 experiments:
  1. Fine-tune LLaMA-7B on translation instructions only, evaluate on Flores subset with beam search
  2. Add error-guided instructions with varying error types, compare performance on WMT22 test sets
  3. Test different search strategies (beam size 4 vs sampling) on the same fine-tuned model to measure quality/compute tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
How effective would the error-guided instruction be on larger LLMs (e.g., LLaMA-30b or LLaMA-65b) compared to LLaMA-7b? Basis: Paper only tests on LLaMA-7b but mentions potential for extending to larger models. Why unresolved: No experimental results on larger models provided. What evidence would resolve it: Experiments comparing translation performance of ParroT models with different sizes of LLaMA models, particularly focusing on error-guided instruction.

### Open Question 2
Can the ParroT framework be extended to incorporate entity alignments and other hints to further improve translation quality? Basis: Paper mentions potential for extending ParroT to other hints like entity alignments. Why unresolved: No experimental results on additional hints provided. What evidence would resolve it: Experiments comparing translation performance of ParroT models with and without additional hints like entity alignments.

### Open Question 3
How does the ParroT framework perform on low-resource language pairs compared to high-resource language pairs? Basis: Paper only tests on Chinese-English and German-English but mentions potential for extending to more translation directions. Why unresolved: No experimental results on low-resource language pairs provided. What evidence would resolve it: Experiments comparing translation performance of ParroT models on low-resource and high-resource language pairs.

## Limitations
- Contrastive instruction effectiveness is questionable due to subtle quality differences between competitive WMT systems
- Exact mechanism of error-guided instruction improvement is not fully explained
- Claims about preserving general task abilities lack quantitative evidence beyond qualitative statements

## Confidence

- **High confidence**: Instruction tuning significantly improves translation performance, especially for English-to-other-languages directions (supported by quantitative metrics and straightforward mechanism)
- **Medium confidence**: Error-guided instruction benefits (some evidence but limited analysis of error types and their impact)
- **Low confidence**: Contrastive instruction results and preservation of general task abilities (contradicted by results or lack sufficient quantitative validation)

## Next Checks
1. Re-run contrastive instruction experiments with deliberately varied quality levels to test whether the approach can learn from more distinguishable differences
2. Conduct ablation studies on error-guided instruction to determine which specific error types contribute most to performance improvements
3. Quantitatively evaluate general task performance before and after fine-tuning using standardized benchmarks to verify claims about preserving general abilities