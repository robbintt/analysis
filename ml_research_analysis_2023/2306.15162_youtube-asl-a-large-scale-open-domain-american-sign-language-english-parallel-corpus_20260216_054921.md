---
ver: rpa2
title: 'YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel
  Corpus'
arxiv_id: '2306.15162'
source_url: https://arxiv.org/abs/2306.15162
tags:
- sign
- language
- translation
- video
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces YouTube-ASL, a large-scale American Sign
  Language (ASL) corpus with over 1000 hours of video and more than 2500 unique signers.
  The dataset is constructed by automatically retrieving videos tagged with sign language
  content from YouTube, followed by human filtering to ensure quality.
---

# YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus

## Quick Facts
- arXiv ID: 2306.15162
- Source URL: https://arxiv.org/abs/2306.15162
- Reference count: 40
- This paper introduces YouTube-ASL, a large-scale American Sign Language (ASL) corpus with over 1000 hours of video and more than 2500 unique signers.

## Executive Summary
This paper introduces YouTube-ASL, a large-scale American Sign Language (ASL) corpus with over 1000 hours of video and more than 2500 unique signers. The dataset is constructed by automatically retrieving videos tagged with sign language content from YouTube, followed by human filtering to ensure quality. The authors train baseline models for ASL to English translation using MediaPipe Holistic landmarks and T5, achieving state-of-the-art results on the How2Sign benchmark with 12.39 BLEU score, and report the first zero-shot results at 3.95 BLEU. This work significantly advances the field by providing a much larger and more diverse dataset than previous ASL corpora, enabling more robust evaluation and potentially improving accessibility for the Deaf/Hard of Hearing community.

## Method Summary
The YouTube-ASL corpus is constructed through a two-step process: automatic content-based annotations identify potentially relevant captioned videos from YouTube, followed by skilled human annotators filtering out videos with poor quality or misaligned captions. The translation model uses MediaPipe Holistic landmarks as input, reduced to 85 relevant points and normalized, fed into a T5.1.1-Base encoder-decoder Transformer pretrained on English text. The model is trained on a mixture of How2Sign and YouTube-ASL data, with evaluation on the How2Sign benchmark using BLEU and BLEURT metrics.

## Key Results
- YouTube-ASL contains 984 hours of video with over 2500 unique signers, ~10x larger than previous ASL datasets
- T5 model with landmark embeddings achieves 12.39 BLEU score on How2Sign benchmark (state-of-the-art)
- First zero-shot results on How2Sign benchmark: 3.95 BLEU score
- T5 pretraining on English text provides better initialization than training from scratch

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open-domain mining increases data diversity and signer variety beyond manually curated datasets.
- Mechanism: Automatic tagging with YouTube's Knowledge Graph entities retrieves a large candidate pool, followed by human filtering to ensure quality, enabling coverage of ~3x more hours and ~10x more signers than prior datasets.
- Core assumption: Automatic tags sufficiently cover relevant sign language content, and human filtering can maintain quality at scale.
- Evidence anchors:
  - [abstract] "We mined these videos from YouTube using a two-step process: first, we used automatic content-based annotations to identify potentially relevant captioned videos; and second, we used skilled human annotators to filter out videos with poor quality or misaligned captions."
  - [section] "YouTube-ASL is constructed with open-ended mining from automatic tags, rather than manual channel curation."
  - [corpus] "Even this likely underestimate is ~10x the count of any individual sign language dataset to date."
- Break condition: Automatic tags miss relevant videos due to lack of sign language content awareness, or human filtering cannot scale to maintain quality.

### Mechanism 2
- Claim: Pretraining T5 on English text provides a strong initialization for ASL-to-English translation.
- Mechanism: The T5 model's encoder-decoder architecture, pretrained on web-crawled English text, learns general language understanding that transfers to the low-resource ASL-to-English translation task.
- Core assumption: Linguistic structures and patterns learned from English text are transferable to ASL-to-English translation.
- Evidence anchors:
  - [section] "Results are substantially worse when training from scratch, which suggests that T5's English pretraining gives the model a better initialization, as De Coster et al.[11] found for frozen pretrained language models."
  - [corpus] "Our model is a slightly modified version of T5 [32], which is an encoder-decoder Transformer [39] that has been trained on web-crawled English text."
- Break condition: Pretrained knowledge is not transferable due to significant linguistic differences between English and ASL, or the pretraining data is not representative of the translation task.

### Mechanism 3
- Claim: Using MediaPipe Holistic landmarks as input provides a computationally efficient and privacy-preserving representation for sign language translation.
- Mechanism: MediaPipe predicts 3D landmarks for the face, hands, and body, which are then reduced to relevant points and normalized, providing a compact input representation for the T5 model.
- Core assumption: The reduced set of 85 landmarks captures sufficient information for ASL translation, and the normalization process preserves the necessary spatial relationships.
- Evidence anchors:
  - [section] "Sign language models that use pose-based inputs have a history of underperforming those that operate on learned video embeddings [ 20, 26]; it is unclear to what extent this is due to the information bottleneck in the (imperfectly predicted) pose representation, vs. availability of higher quality pretrained video encoders than pretrained pose encoders."
  - [corpus] "We use MediaPipe Holistic landmarks [25, 16], rather than raw video."
- Break condition: The reduced landmark representation loses critical information for translation, or the MediaPipe predictions are inaccurate.

## Foundational Learning

- Concept: American Sign Language (ASL) and its unique linguistic features.
  - Why needed here: Understanding the linguistic structure of ASL is crucial for designing effective translation models and interpreting results.
  - Quick check question: What are some key differences between ASL and spoken languages like English?

- Concept: Machine learning model architectures for sequence-to-sequence tasks.
  - Why needed here: The T5 encoder-decoder Transformer architecture is central to the translation approach, and understanding its components is essential for model development and analysis.
  - Quick check question: What are the main components of a Transformer model, and how do they contribute to its performance?

- Concept: Data preprocessing and feature engineering techniques.
  - Why needed here: The preprocessing steps, such as landmark reduction and normalization, significantly impact the model's input representation and performance.
  - Quick check question: How do different preprocessing techniques affect the quality and quantity of features extracted from raw data?

## Architecture Onboarding

- Component map:
  Data collection -> Automatic tagging + human filtering -> Preprocessing -> MediaPipe Holistic landmarks -> Landmark reduction and normalization -> Model -> T5 encoder-decoder Transformer -> Training -> Mixture of How2Sign and YouTube-ASL -> Evaluation -> BLEU and BLEURT on How2Sign

- Critical path:
  1. Data collection and filtering
  2. Preprocessing and feature extraction
  3. Model training and evaluation

- Design tradeoffs:
  - Open-domain mining vs. manual curation: Increased diversity and scale vs. potential quality issues
  - Landmark-based input vs. raw video: Computational efficiency and privacy vs. potential information loss
  - Pretrained T5 vs. training from scratch: Faster convergence and better performance vs. potential bias from pretraining data

- Failure signatures:
  - Low BLEU scores: Poor data quality, insufficient model capacity, or ineffective training
  - High variance in translations: Overfitting to specific signers or domains, or lack of model generalization
  - Slow convergence: Inadequate learning rate, batch size, or training duration

- First 3 experiments:
  1. Train and evaluate the baseline model on How2Sign only to establish a performance baseline.
  2. Train and evaluate the model on YouTube-ASL only to assess the impact of the larger, more diverse dataset.
  3. Train and evaluate the model on a mixture of How2Sign and YouTube-ASL to determine the optimal data balance.

## Open Questions the Paper Calls Out
- Open Question 1: How does the inclusion of videos across all skill levels and signing styles affect the corpus's usefulness for generation tasks versus recognition tasks?
- Open Question 2: What is the impact of the 12-15 fps landmark extraction rate on model performance compared to using higher frame rates?
- Open Question 3: How do the model's translation errors in zero-shot mode relate to the sign language input, and can these errors be predicted or explained by linguistic features?

## Limitations
- The exact YouTube search queries and filtering criteria for automatic video retrieval are unspecified, making dataset construction difficult to reproduce
- The scalability of human filtering to maintain quality across the entire dataset is asserted but not quantified
- The impact of open-domain mining on translation quality versus curated datasets remains an open question without controlled comparisons

## Confidence
**High Confidence**: The reported BLEU scores on the How2Sign benchmark (12.39 BLEU with finetuning, 3.95 BLEU zero-shot) are well-supported by the methodology and consistent with the described model architecture.

**Medium Confidence**: The assertion that T5's English pretraining significantly improves performance relies on comparative results but lacks ablation studies isolating the pretraining effect.

**Low Confidence**: The scalability of human filtering to maintain quality across the entire dataset is asserted but not quantified.

## Next Checks
1. Reproduce the landmark preprocessing pipeline: Implement the exact MediaPipe landmark selection and normalization procedure using the specified 85 landmarks, then verify that the reduced representation maintains translation performance within 5% of reported scores.

2. Validate dataset construction methodology: Using the authors' described automatic tagging approach, collect a small validation set of 100 videos and compare the automatic filtering accuracy against human annotators to quantify quality maintenance at scale.

3. Conduct controlled ablation on pretraining: Train identical T5 models with and without English pretraining on the same YouTube-ASL training data, measuring both convergence speed and final BLEU scores to isolate the pretraining contribution.