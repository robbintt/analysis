---
ver: rpa2
title: Evaluating the Generation Capabilities of Large Chinese Language Models
arxiv_id: '2308.04823'
source_url: https://arxiv.org/abs/2308.04823
tags:
- questions
- evaluation
- chinese
- score
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CG-Eval, the first comprehensive evaluation
  framework for assessing the generative capabilities of large Chinese language models
  across diverse academic disciplines. The framework evaluates models'' ability to
  generate precise and contextually relevant responses in six key domains: Science
  and Engineering, Humanities and Social Sciences, Mathematical Calculations, Medical
  Practitioner Qualification Examination, Judicial Examination, and Certified Public
  Accountant Examination.'
---

# Evaluating the Generation Capabilities of Large Chinese Language Models

## Quick Facts
- arXiv ID: 2308.04823
- Source URL: https://arxiv.org/abs/2308.04823
- Reference count: 3
- GPT-4 achieved the highest Gscore of 41.12 across six academic disciplines

## Executive Summary
This paper introduces CG-Eval, the first comprehensive evaluation framework for assessing the generative capabilities of large Chinese language models across diverse academic disciplines. The framework evaluates models' ability to generate precise and contextually relevant responses in six key domains: Science and Engineering, Humanities and Social Sciences, Mathematical Calculations, Medical Practitioner Qualification Examination, Judicial Examination, and Certified Public Accountant Examination. The paper proposes Gscore, a composite index derived from the weighted sum of multiple metrics (BLEU, ROUGE, CHRF, and semantic similarity) to automate the quality measurement of a model's text generation against reference standards. Experimental results show that GPT-4 achieved the highest Gscore of 41.12 across the six disciplines, significantly outperforming other evaluated models.

## Method Summary
The evaluation framework uses a zero-shot approach where models generate responses to 11,000 questions across six subject categories. Dynamic prompt generation with length constraints guides model responses, while automated scoring employs a sliding window encoding module to handle long text sequences. The Gscore composite index combines BLEU4 (weight 0.2), ROUGE2 (weight 0.25), CHRF (weight 0.25), and semantic similarity (weight 0.3) to evaluate generation quality. The framework is accessible through a web interface at http://cgeval.besteasy.com/ and requires reference answers that are only accessible through the evaluation website.

## Key Results
- GPT-4 achieved the highest Gscore of 41.12 across all six evaluated disciplines
- The framework successfully differentiates between models of varying sizes (6B to 130B parameters)
- Mathematical calculations showed particularly strong performance from GPT-4 with a Gscore of 44.57

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The weighted sum of multiple evaluation metrics (BLEU, ROUGE, CHRF, and semantic similarity) produces a composite Gscore that captures both surface form and semantic similarity of generated text.
- Mechanism: By combining n-gram overlap metrics with semantic similarity, the Gscore balances lexical matching with deeper meaning representation.
- Core assumption: Each metric captures complementary aspects of text quality, and their weighted sum better represents overall generation quality than any single metric alone.
- Evidence anchors:
  - [abstract]: "Gscore, an innovative composite index developed from a weighted sum of multiple metrics"
  - [section 2.3]: "we have amalgamated them to design a composite metric termed Gscore"
- Break condition: If the weight distribution is suboptimal, the composite score may overweight certain aspects of generation quality while underweighting others.

### Mechanism 2
- Claim: Dynamic prompt generation with length constraints improves model response relevance and consistency across different question types.
- Mechanism: By specifying the expected answer length in prompts, the framework guides models to generate responses of appropriate scope and detail.
- Core assumption: Providing explicit length constraints reduces variance in response quality and helps models understand expected output format.
- Evidence anchors:
  - [section 2.2]: "We have adopted a dynamic and flexible method for generating prompt words, ensuring that each question is paired with a unique prompt"
  - [section 2.2]: "For non-computational questions, we've imposed constraints on the length of the answer"
- Break condition: If models ignore length constraints or if the reference answer lengths are not representative, the constraint-based approach may not improve response quality.

### Mechanism 3
- Claim: Automated evaluation with a sliding window encoding module enables processing of long text sequences that exceed model input limits.
- Mechanism: The sliding window approach breaks long text into manageable chunks, encodes each chunk, and then aggregates the results to represent the entire text.
- Core assumption: Aggregating encoded vectors from sliding windows preserves semantic information about the full text.
- Evidence anchors:
  - [section 2.3.4]: "Due to the potential of model answers and reference answers exceeding the maximum processing length of the model, we designed a sliding window encoding module"
  - [section 2.3.4]: "Upon completion of processing all windows, we aggregate the encoding vectors, either by taking an average or by concatenation, to represent the entire text"
- Break condition: If semantic information is lost during windowing and aggregation, or if window boundaries cut through important semantic units, the encoded representation may be inaccurate.

## Foundational Learning

- Concept: Text generation evaluation metrics (BLEU, ROUGE, CHRF)
  - Why needed here: Understanding the strengths and limitations of each metric is crucial for designing the composite Gscore
  - Quick check question: What is the main difference between BLEU and ROUGE in terms of what they measure?

- Concept: Chinese language processing challenges
  - Why needed here: The framework specifically evaluates Chinese language models, requiring awareness of Chinese linguistic features
  - Quick check question: Why might character-level metrics like CHRF be particularly important for Chinese text evaluation?

- Concept: Large language model prompt engineering
  - Why needed here: The framework uses dynamic prompts with specific constraints to guide model responses
  - Quick check question: How might specifying answer length in prompts affect the quality of generated responses?

## Architecture Onboarding

- Component map: Question → Prompt Generation → Model Response → Sliding Window Encoding → Metric Calculation → Gscore Aggregation → Result Storage
- Critical path: Question → Prompt Generation → Model Response → Sliding Window Encoding → Metric Calculation → Gscore Aggregation → Result Storage
- Design tradeoffs: Using multiple metrics increases evaluation comprehensiveness but also computational complexity; dynamic prompts improve consistency but require careful template design
- Failure signatures: Poor Gscore differentiation between models may indicate metric saturation or insufficient discrimination power; inconsistent results across runs may suggest implementation bugs
- First 3 experiments:
  1. Test Gscore calculation with simple, known input/output pairs to verify metric implementation
  2. Evaluate a small set of models on a subset of questions to validate the overall pipeline
  3. Compare Gscore results with manual evaluations on a sample to assess correlation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the Gscore metric correlate with human judgment in evaluating the generation capabilities of large Chinese language models across diverse academic disciplines?
- Basis in paper: [explicit] The paper introduces Gscore as a composite index derived from a weighted sum of multiple metrics (BLEU, ROUGE, CHRF, and semantic similarity) to automate the quality measurement of a model's text generation against reference standards. It mentions that current metrics have limitations and that Gscore aims to provide a more comprehensive assessment.
- Why unresolved: The paper does not provide a direct comparison between Gscore results and human evaluations. It would be valuable to assess the correlation between the automated Gscore and subjective human judgments to validate the effectiveness of Gscore in capturing the nuances of text generation quality.
- What evidence would resolve it: A study comparing Gscore results with human evaluations on a subset of the test data would provide evidence of the correlation between the automated metric and human judgment.

### Open Question 2
- Question: How does the performance of large Chinese language models on CG-Eval translate to their performance on real-world tasks in the evaluated domains?
- Basis in paper: [inferred] The paper evaluates models' performance on a comprehensive set of questions across six key domains. However, it does not explore how well the models perform on practical, real-world tasks within these domains.
- Why unresolved: The paper focuses on the models' ability to generate accurate and relevant responses to test questions but does not assess their practical application in real-world scenarios. Understanding the transferability of the models' performance to practical tasks would provide insights into their real-world utility.
- What evidence would resolve it: Conducting experiments where the models are applied to real-world tasks within the evaluated domains and comparing their performance with the CG-Eval results would provide evidence of the models' practical effectiveness.

### Open Question 3
- Question: How does the size of the language model impact its performance on the CG-Eval benchmark, and is there a point of diminishing returns in terms of model size and performance?
- Basis in paper: [inferred] The paper evaluates models with varying parameter counts, ranging from 6B to 130B parameters. It would be interesting to analyze the relationship between model size and performance on the CG-Eval benchmark.
- Why unresolved: While the paper presents the performance of different models, it does not explicitly investigate the impact of model size on performance. Understanding the correlation between model size and performance would provide insights into the scalability and efficiency of large language models.
- What evidence would resolve it: Conducting a systematic study comparing the performance of models with different parameter counts on the CG-Eval benchmark would provide evidence of the relationship between model size and performance. Additionally, analyzing the performance gains as the model size increases would help identify the point of diminishing returns.

## Limitations

- The reference answers for the 11,000 questions are not directly accessible through the paper, requiring reliance on the external evaluation website
- The weight distribution in the Gscore formula lacks detailed justification for the specific values chosen
- The paper does not provide statistical significance testing to determine whether performance differences between models are meaningful

## Confidence

- **High confidence**: The framework's basic architecture (dynamic prompt generation, multi-metric scoring, sliding window processing) is well-specified and reproducible
- **Medium confidence**: The overall superiority of GPT-4 (Gscore of 41.12) is demonstrated, though the absolute meaning of this score is unclear without benchmarking against human performance or alternative evaluation methods
- **Low confidence**: The optimal weight distribution in the Gscore formula and the specific implementation details of the sliding window encoding mechanism

## Next Checks

1. **Metric Correlation Analysis**: Calculate pairwise correlations between BLEU4, ROUGE2, CHRF, and semantic similarity scores across all model responses to determine whether these metrics capture independent aspects of generation quality or are highly redundant, which would question the validity of their weighted combination.

2. **Human Evaluation Validation**: Conduct a small-scale human evaluation comparing model responses for a sample of questions across the six domains, using blinded scoring to assess whether Gscore rankings align with human judgments of answer quality, particularly focusing on cases where automated metrics may disagree with human assessment.

3. **Weight Sensitivity Analysis**: Systematically vary the weights in the Gscore formula (e.g., testing combinations like equal weighting, reversing priority, or emphasizing semantic similarity) and measure how rank orderings of models change across the six subject categories to determine whether the current weighting scheme significantly impacts conclusions about model performance.