---
ver: rpa2
title: 'Generative Models as a Complex Systems Science: How can we make sense of large
  language model behavior?'
arxiv_id: '2308.00189'
source_url: https://arxiv.org/abs/2308.00189
tags:
- behavior
- language
- arxiv
- what
- behaviors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that large language models (LMs) should be studied
  as complex systems exhibiting emergent behaviors. The authors propose shifting focus
  from benchmarks to identifying and characterizing LM behaviors that explain cross-task
  performance.
---

# Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?

## Quick Facts
- arXiv ID: 2308.00189
- Source URL: https://arxiv.org/abs/2308.00189
- Reference count: 40
- Key outcome: Large language models should be studied as complex systems exhibiting emergent behaviors, with focus on identifying and characterizing behaviors that explain cross-task performance

## Executive Summary
This paper argues that large language models (LMs) should be studied as complex systems exhibiting emergent behaviors rather than solely through benchmark performance metrics. The authors propose formalizing "behavior" as functions mapping inputs to features that reveal information about LM outputs, and advocate for studying these behaviors to guide mechanistic explanations of model capabilities. They highlight the unique advantages of studying LMs as complex systems, including perfect state encoding, complete low-level dynamics theory, exact repeatability, ease of perturbation, and human-understandable outputs. The paper emphasizes the critical need for open-source models to leverage these advantages and calls for increased focus on understanding "what LMs are doing" alongside classic interpretability questions about "why LMs are doing that."

## Method Summary
The paper proposes a formal framework for studying large language models as complex systems by decomposing their behavior into categories that explain cross-task performance. The method involves defining behaviors as functions that map inputs to features revealing information about model outputs, using mutual information to quantify how much behaviors explain outputs, and applying Minimum Description Length principles to identify parsimonious behavioral explanations. The approach emphasizes empirical discovery of emergent behaviors through analysis of model outputs, training data, and behavioral patterns, while advocating for open-source models to enable the full range of scientific investigation advantages that LMs uniquely offer compared to natural complex systems.

## Key Results
- LMs exhibit emergent behaviors that cannot be fully explained by existing benchmarks
- The computational nature of LMs provides unique research advantages over natural complex systems
- Open-source models are essential for leveraging these advantages in scientific study
- Behavioral decomposition offers a promising framework for understanding cross-task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Studying language models as complex systems allows researchers to leverage advantages like perfect state encoding and exact repeatability that are impossible with natural systems.
- Mechanism: Because LMs are computational models implemented in code, we can capture their complete state, understand their low-level dynamics perfectly, and repeat experiments exactly - advantages not available when studying biological or physical complex systems.
- Core assumption: The mathematical/computational nature of LMs provides unique research advantages over naturally occurring complex systems.
- Evidence anchors: [abstract] "we can take full advantage of the complete simulability of generative models"; [section 4.2] "Perfect fidelity state encoding Because neural networks are formal mathematical models, represented by code and parameters, there is zero necessary ambiguity in our representations"
- Break condition: If LMs incorporate stochastic elements that cannot be perfectly captured or if model access becomes restricted, these advantages diminish.

### Mechanism 2
- Claim: Focusing on behavioral decomposition rather than benchmark performance enables transferable understanding across model architectures.
- Mechanism: By formalizing behaviors as functions mapping inputs to features that explain model outputs, researchers can develop a vocabulary of behaviors that applies across different model architectures, guiding mechanistic explanations even when architectures change dramatically.
- Core assumption: Behaviors exist as distinct, identifiable patterns that can be formalized and that explain cross-task performance.
- Evidence anchors: [abstract] "we lack explanations of what behaviors language models exhibit that allow them to complete these tasks in the first place"; [section 2.1] "We think of a behavior as an explanation of limited aspects of a model, a concept we briefly formalize"
- Break condition: If behaviors are too model-specific or if the formalization cannot capture the complexity of real model outputs.

### Mechanism 3
- Claim: Open-source models are necessary to leverage the advantages of studying LMs as complex systems.
- Mechanism: Without access to model weights, parameters, and architecture, researchers cannot achieve perfect state encoding, cannot perturb models to test hypotheses, and cannot guarantee repeatability of experiments.
- Core assumption: Proprietary APIs fundamentally limit the scientific study of LMs compared to open-source alternatives.
- Evidence anchors: [section 4.3] "Most of these advantages rely on stable access to a consistent representation of a model, which is difficult to guarantee via a proprietary API"; [section 4.3] "With only imperfect knowledge of an underlying model, researchers must make assumptions about low-level dynamics"
- Break condition: If API providers offer sufficient transparency and control, or if alternative methods for studying proprietary models emerge.

## Foundational Learning

- Concept: Complex systems theory
  - Why needed here: The paper frames LMs as complex systems exhibiting emergent behaviors, requiring understanding of how macro-level patterns arise from micro-level interactions
  - Quick check question: What distinguishes a complex system from a simple system in terms of predictability of macro-level behaviors from micro-level dynamics?

- Concept: Information theory and mutual information
  - Why needed here: The paper formalizes behaviors using mutual information to quantify how much behaviors reveal about model outputs
  - Quick check question: How does mutual information measure the relationship between model outputs and behavioral predictions?

- Concept: Minimum Description Length (MDL) principle
  - Why needed here: The paper uses MDL as a criterion for finding good behavioral hypotheses that balance simplicity and explanatory power
  - Quick check question: Why would we prefer behaviors that both explain model outputs well and are simple to describe?

## Architecture Onboarding

- Component map: Generative models M -> Behaviors B (functions mapping inputs to features) -> Metamodels (predict behaviors to understand M)
- Critical path: 1) Define behavior formalization using mutual information, 2) Identify candidate behaviors through qualitative analysis of model outputs, 3) Quantify behavior explanatory power using MDL, 4) Use behaviors to guide mechanistic investigation
- Design tradeoffs: Balancing between highly explanatory behaviors (which may be model-specific) versus generalizable behaviors (which may be less explanatory); between behaviors that predict many aspects of outputs versus behaviors that are simple to describe
- Failure signatures: 1) Behaviors that don't transfer across model architectures, 2) Inability to formalize behaviors mathematically, 3) Behaviors that explain too little or too much of model output variance
- First 3 experiments:
  1. Analyze copying behavior across multiple LM architectures to identify common behavioral patterns
  2. Test whether formalizing behaviors using mutual information improves prediction of model performance on unseen tasks
  3. Compare explanatory power of different behavioral formalizations using MDL criteria on the same model outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively discover and characterize new emergent behaviors in large language models beyond current benchmarks?
- Basis in paper: [explicit] The paper emphasizes that benchmarks alone cannot discover new behaviors and argues for a systematic effort to decompose LM behavior into explanatory categories
- Why unresolved: Current benchmarks are limited in scope and cannot capture the full range of model behaviors, especially those that emerge unexpectedly
- What evidence would resolve it: Development of systematic methods to identify, categorize, and characterize model behaviors through data analysis and black-box testing approaches

### Open Question 2
- Question: What is the relationship between training data distribution and the emergent behaviors exhibited by large language models?
- Basis in paper: [explicit] The paper discusses how aspects of training data (e.g., number of repeated documents, presence of multiple languages) can be explanatory of model behaviors like zero-shot translation abilities and tendency to leak training data
- Why unresolved: While correlations have been observed, the precise mechanisms linking training data properties to emergent behaviors remain unclear
- What evidence would resolve it: Systematic studies mapping specific training data characteristics to corresponding model behaviors across different model architectures and scales

### Open Question 3
- Question: How can we develop a parsimonious metamodel that explains cross-task performance in large language models?
- Basis in paper: [explicit] The paper proposes finding behaviors that are both explanatory and simple to describe, using concepts like mutual information and Minimum Description Length to evaluate behavioral explanations
- Why unresolved: Current approaches either focus on bottom-up mechanistic explanations or black-box behavior characterization, but lack a unified framework for explaining model performance across tasks
- What evidence would resolve it: Development of formal methods to identify behaviors that maximize explanatory power while minimizing complexity, validated across diverse model architectures and tasks

## Limitations

- The behavioral framework remains largely theoretical with limited empirical validation
- Open-source model requirement may limit practical applicability given current proprietary model dominance
- The claim that LM outputs are "human-understandable" may not hold for all behavioral patterns
- Behavioral formalization using mutual information and MDL needs more concrete implementation details

## Confidence

**High confidence**: LMs exhibit emergent behaviors that cannot be fully explained by existing benchmarks.
**Medium confidence**: LMs offer unique advantages for studying complex systems due to their computational nature.
**Medium confidence**: The proposed behavioral framework using mutual information and MDL.

## Next Checks

1. **Empirical validation of the behavioral framework**: Apply the mutual information and MDL-based behavioral analysis to a set of well-studied LMs (e.g., GPT-2 variants) and compare the discovered behaviors against known capabilities and failure modes. This would test whether the formalization captures meaningful patterns.

2. **Cross-architecture behavioral transfer**: Test whether behaviors identified in one LM architecture (e.g., transformer-based) transfer to substantially different architectures (e.g., state-space models or recurrent architectures). This would validate the claim that behaviors provide architecture-independent insights.

3. **Open-source vs. proprietary comparison**: Conduct parallel behavioral studies using both open-source and API-accessed models on identical tasks. Measure the degree to which API limitations constrain the ability to identify, validate, and perturb behaviors as proposed in the framework.