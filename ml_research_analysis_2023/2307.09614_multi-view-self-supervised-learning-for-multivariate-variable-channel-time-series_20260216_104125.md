---
ver: rpa2
title: Multi-view self-supervised learning for multivariate variable-channel time
  series
arxiv_id: '2307.09614'
source_url: https://arxiv.org/abs/2307.09614
tags:
- mpnn
- channels
- loss
- pretraining
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose a channel agnostic network that generalizes between
  datasets with varying sets of input channels with no further preprocessing. We learn
  a single encoder that acts on each individual channel separately.
---

# Multi-view self-supervised learning for multivariate variable-channel time series

## Quick Facts
- arXiv ID: 2307.09614
- Source URL: https://arxiv.org/abs/2307.09614
- Reference count: 0
- We propose a channel agnostic network that generalizes between datasets with varying sets of input channels with no further preprocessing. We learn a single encoder that acts on each individual channel separately. To account for the inter-channel information, we add a message passing neural network (MPNN) after the encoder which learns the optimal combination of the representation for each channel. We demonstrate the use of the MPNN by pretraining on an EEG datasets with six channels and finetuning on an EEG dataset with two different channels and compare different loss functions in the pretraining phase. Our results show that when combined with the TS2Vec loss, our method outperforms all other methods on most sample sizes.

## Executive Summary
This paper introduces a channel-agnostic self-supervised learning approach for multivariate time series where the model can generalize across datasets with different numbers of input channels without manual preprocessing. The method uses a shared convolutional encoder applied to each channel independently, followed by a message passing neural network (MPNN) to learn optimal combinations of channel representations. The approach is demonstrated on EEG sleep staging, pretraining on a 6-channel dataset and finetuning on a 2-channel dataset, showing superior performance when combined with the TS2Vec loss function.

## Method Summary
The method processes multivariate time series by applying a shared convolutional encoder to each channel independently, producing channel-wise representations. These representations are then aggregated using a message passing neural network (MPNN) that learns optimal combinations through random channel grouping. The model is pretrained using contrastive learning with three different loss functions (NT-Xent, TS2Vec, COCOA) on a 6-channel EEG dataset, then finetuned on a 2-channel EEG dataset for sleep staging. The channel-agnostic design allows direct application to datasets with varying channel counts without preprocessing modifications.

## Key Results
- Channel-agnostic encoder successfully generalizes from 6-channel pretraining to 2-channel finetuning without performance degradation
- MPNN improves performance, particularly at smaller sample sizes (10-100 per class)
- TS2Vec loss outperforms NT-Xent and COCOA across most sample sizes
- Balanced accuracy on 5-class sleep staging reaches 68.5% at 1000 samples per class with full network finetuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The channel-agnostic encoder allows the model to generalize across datasets with different channel sets without manual preprocessing.
- Mechanism: A single convolutional encoder is applied to each channel independently, producing representations hc ∈ RN×L×Tout. The model treats each channel as a separate input view, enabling direct application to datasets with any number of channels.
- Core assumption: Channel order and identity are not critical; the encoder can extract meaningful features from any single channel.
- Evidence anchors:
  - [abstract] "We propose a channel agnostic network that generalizes between datasets with varying sets of input channels with no further preprocessing."
  - [section 2.1] "We learn a single encoder that acts on all input channels individually."
- Break condition: If channel identity carries essential semantic meaning that the encoder cannot capture independently, or if channels are not independent (e.g., spatial arrangement matters).

### Mechanism 2
- Claim: The MPNN learns optimal combinations of channel representations for downstream tasks.
- Mechanism: After channel-wise encoding, an MPNN aggregates representations by forming fully connected graphs within randomly partitioned channel groups. The message passing layers learn context-aware combinations, producing a single fused representation.
- Core assumption: The optimal combination of channel information is not simply additive or fixed but depends on task-specific interactions.
- Evidence anchors:
  - [abstract] "We use a message passing neural network (MPNN) after the encoder which learns the optimal combination of the representation for each channel."
  - [section 2.2] "The MPNN consists of two phases, the message passing phase and the readout phase."
- Break condition: If the optimal combination is trivial (e.g., simple averaging suffices) or if the task does not benefit from inter-channel interactions.

### Mechanism 3
- Claim: TS2Vec loss improves performance by explicitly modeling temporal relationships between positive pairs.
- Mechanism: Unlike NT-Xent, TS2Vec uses temporal and instance-level negative examples. Temporal negatives are different time stamps within the same sequence; instance negatives are different sequences at the same time stamp. This dual loss captures richer temporal structure.
- Core assumption: The temporal ordering and within-sequence consistency are more informative than purely cross-view similarity.
- Evidence anchors:
  - [section 2.3] "the TS2Vec loss... takes the temporal relations in the representations into account... constructing two different versions of the negative examples and using these to compute a temporal loss and an instance loss respectively."
  - [abstract] "Our results show that when combined with the TS2Vec loss, our method outperforms all other methods on most sample sizes."
- Break condition: If temporal structure is irrelevant to the task or if computational cost outweighs the performance gain.

## Foundational Learning

- Concept: Contrastive learning fundamentals
  - Why needed here: The method relies on forming positive pairs from different views (channels) and pushing them closer in representation space.
  - Quick check question: What defines a positive pair in this multi-view setup, and how are negatives sampled?

- Concept: Message passing neural networks
  - Why needed here: The MPNN is used to aggregate channel-wise representations into a unified embedding by passing messages across a fully connected graph.
  - Quick check question: How does the MPNN handle graphs of different sizes during pretraining and finetuning?

- Concept: Temporal contrastive losses (TS2Vec)
  - Why needed here: TS2Vec explicitly models temporal dynamics by constructing dual losses (temporal and instance) rather than treating all negatives equally.
  - Quick check question: How does the temporal loss differ from the instance loss in terms of negative sampling strategy?

## Architecture Onboarding

- Component map:
  - Input (N samples × C channels × T time steps) -> Channel encoder (shared conv per channel) -> MPNN (optional) -> Loss computation (TS2Vec/NT-Xent/COCOA) -> Downstream classifier

- Critical path:
  1. Channel-wise encoding → 2. MPNN aggregation (if used) → 3. Loss computation → 4. Gradient backprop through encoder and MPNN

- Design tradeoffs:
  - Fixed vs. random channel grouping: Random grouping during pretraining improves generalization to variable channel counts but adds stochasticity.
  - Shared vs. separate encoders: Shared encoder enforces channel-agnosticism but may limit specialized channel modeling.
  - TS2Vec vs. NT-Xent: TS2Vec better captures temporal dynamics but is more computationally expensive.

- Failure signatures:
  - Overfitting to channel order: Model performs well on same-channel pretrain/finetune but poorly on different channels.
  - MPNN collapse: Representations become too uniform, losing channel-specific information.
  - Loss saturation: Contrastive loss plateaus early, indicating poor negative sampling or model capacity issues.

- First 3 experiments:
  1. Verify channel-agnosticism: Pretrain on 6-channel EEG, finetune on 2-channel EEG without MPNN, measure performance drop.
  2. MPNN ablations: Compare TS2Vec with/without MPNN across small sample sizes to confirm benefit.
  3. Loss comparison: Run NT-Xent, TS2Vec, and COCOA on same pretraining setup to quantify performance differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the MPNN approach generalize to datasets with more than two channels, and how does its performance scale with the number of input channels?
- Basis in paper: [inferred] The authors hypothesize that the usefulness of the MPNN might be even more clear when finetuning on datasets with more than two channels, but this was not tested in the current work.
- Why unresolved: The experimental validation was limited to comparing datasets with six channels (pretraining) and two channels (finetuning). Scaling to more channels was not explored.
- What evidence would resolve it: Testing the MPNN on downstream tasks with three or more channels and comparing performance against the non-MPNN baseline across varying channel counts.

### Open Question 2
- Question: Can the proposed channel-agnostic pretraining scheme be effectively combined with other self-supervised learning strategies beyond the multi-view contrastive approach?
- Basis in paper: [explicit] The authors state that future work includes testing the MPNN when using other methods than the multi-view strategy for choosing positive pairs.
- Why unresolved: Only the multi-view strategy was evaluated; other positive pair selection methods (e.g., augmentations, temporal contrasting) were not tested with the MPNN architecture.
- What evidence would resolve it: Applying the MPNN to other self-supervised learning frameworks and comparing performance across different positive pair generation methods.

### Open Question 3
- Question: What is the impact of different MPNN architectures (e.g., number of message passing layers, readout functions) on the performance of the channel-agnostic pretraining scheme?
- Basis in paper: [inferred] The authors used a single message passing layer and a mean readout, but did not explore architectural variations or their effects on downstream task performance.
- Why unresolved: Only one MPNN configuration was evaluated, leaving the sensitivity to architectural choices unexplored.
- What evidence would resolve it: Systematic ablation studies varying MPNN depth, message passing mechanisms, and readout strategies, and measuring their impact on finetuning accuracy.

## Limitations
- The paper lacks detailed architectural specifications for the MPNN, making exact reproduction challenging
- Implementation details for data augmentations during pretraining are not fully specified
- The ablation study comparing TS2Vec against NT-Xent and COCOA is limited to a single dataset pair

## Confidence
- **High Confidence:** The channel-agnostic encoder mechanism is well-supported by the experimental results showing successful transfer from 6-channel to 2-channel EEG
- **Medium Confidence:** The MPNN's contribution to performance is demonstrated but the ablation study shows mixed results, with benefits more pronounced at smaller sample sizes
- **Medium Confidence:** TS2Vec's superiority over NT-Xent is shown empirically, but the computational overhead and its benefits across different downstream tasks remain unclear

## Next Checks
1. **Architectural Reproducibility:** Implement the MPNN with the described random channel grouping strategy and verify that the model can handle varying numbers of channels during finetuning
2. **Loss Function Comparison:** Conduct a comprehensive comparison of TS2Vec against NT-Xent and COCOA across multiple pretraining datasets to validate the claimed performance improvements
3. **Generalization Test:** Evaluate the pretrained model on a different multivariate time series dataset (e.g., accelerometer data) to assess cross-domain generalization capabilities