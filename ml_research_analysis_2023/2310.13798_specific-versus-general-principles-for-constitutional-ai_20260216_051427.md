---
ver: rpa2
title: Specific versus General Principles for Constitutional AI
arxiv_id: '2310.13798'
source_url: https://arxiv.org/abs/2310.13798
tags:
- human
- trait
- traits
- helpful
- would
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the generalization abilities of AI feedback
  in constitutional AI (CAI) by testing whether a model trained with a single high-level
  principle like "do what's best for humanity" can learn general ethical behaviors
  without targeting specific problematic traits. The authors compare preference models
  trained on specific AI traits versus those trained only on a good-for-humanity principle,
  finding that the largest 175B models can generalize effectively from the single
  principle to discourage harmful behaviors including power-seeking and survival instincts.
---

# Specific versus General Principles for Constitutional AI

## Quick Facts
- arXiv ID: 2310.13798
- Source URL: https://arxiv.org/abs/2310.13798
- Authors: 
- Reference count: 40
- Key outcome: 175B models can generalize ethical behaviors from a single "do what's best for humanity" principle without requiring detailed constitutional rules for each specific harmful trait.

## Executive Summary
This work explores whether AI feedback in constitutional AI can learn general ethical behaviors from a single high-level principle versus specific detailed constitutions. The authors compare preference models trained on specific AI traits (power-seeking, self-preservation, etc.) against those trained only on a good-for-humanity principle. The results show that 175B models can effectively generalize from the single principle to discourage harmful behaviors including power-seeking and survival instincts, though more detailed constitutions still provide better fine-grained control over specific harms.

## Method Summary
The study uses constitutional AI framework to generate AI feedback by having pre-trained feedback models evaluate prompt-response pairs according to written principles. They train two 175B preference models: one on five specific problematic traits and another on a general "do what's best for humanity" principle. Both models are then used to fine-tune language models via reinforcement learning. The evaluation includes held-out datasets testing for problematic traits and general harmlessness, using both automatic LM-generated persona evaluations and crowdsourced comparisons.

## Key Results
- 175B Good-for-Humanity (GfH) preference models can generalize effectively to discourage power-seeking and survival instincts without explicit training on these traits
- The GfH PMs achieved higher harmlessness Elo scores than specific trait PMs on general harmfulness detection tasks
- Larger models show a "phase transition" where 175B parameters enables effective learning from general principles, while smaller models (52B and below) fail to generalize effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can generalize ethical behaviors from a single high-level principle ("do what's best for humanity") without requiring detailed constitutional rules for each specific harmful trait.
- Mechanism: The 175B parameter models develop the ability to identify nuanced patterns of harmful language and extrapolate from the general principle to specific cases through their pre-trained knowledge and scaling effects.
- Core assumption: Larger models (175B+) have sufficient capacity to learn abstract representations of "what's best for humanity" and map them to concrete behavioral prohibitions.
- Evidence anchors:
  - [abstract] "The results show that large language models can learn general ethical behaviors directly from simple principles"
  - [section] "the 175B GfH PM even surpassed the 175B HH-RLHF PM at detecting harmfulness"
- Break condition: The generalization fails when the model cannot establish clear connections between the abstract principle and specific harmful behaviors, particularly for traits with tenuous links to humanity's wellbeing (e.g., self-identity).

### Mechanism 2
- Claim: AI feedback using constitutional principles can effectively identify and discourage problematic AI traits without human supervision.
- Mechanism: The constitutional approach generates comparison data by having a pre-trained feedback model evaluate pairs of responses according to constitutional principles, creating a training signal for preference models that learn to distinguish harmful from harmless outputs.
- Core assumption: Pre-trained language models can reliably judge which responses better align with constitutional principles when presented in a multiple-choice format.
- Evidence anchors:
  - [abstract] "Constitutional AI offers an alternative, replacing human feedback with feedback from AI models conditioned only on a list of written principles"
- Break condition: The feedback model fails to provide consistent judgments across similar prompts, leading to noisy training data that prevents the preference model from learning meaningful distinctions.

### Mechanism 3
- Claim: Scaling trends show a phase transition around 175B parameters where models gain the ability to identify and discourage subtle problematic behaviors.
- Mechanism: As model size increases, the ability to detect nuanced patterns in language improves dramatically, with a threshold effect where 175B models can identify behaviors that smaller models cannot detect reliably.
- Core assumption: Model performance on detecting problematic traits follows a non-linear scaling law with a threshold effect rather than smooth improvement.
- Evidence anchors:
  - [abstract] "the largest 175B models can generalize effectively from the single principle"
  - [section] "a 'phase transition' somewhere between the 52B and 175B parameters"
- Break condition: The phase transition effect disappears if the task requires reasoning capabilities beyond what language models can provide, or if the constitutional principles are too ambiguous for even large models to interpret consistently.

## Foundational Learning

- Concept: Constitutional AI (CAI) framework
  - Why needed here: This work builds on CAI as the foundational approach for replacing human feedback with AI feedback conditioned on written principles
  - Quick check question: What are the five key steps in the constitutional AI approach for training preference models?

- Concept: Preference modeling and reinforcement learning from AI feedback (RLAIF)
  - Why needed here: The work uses preference models trained on AI-generated comparisons to fine-tune language models via reinforcement learning
  - Quick check question: How does the preference model assign scores to prompt-response pairs, and what loss function is used during training?

- Concept: Model-generated evaluations and LM-generated persona assessments
  - Why needed here: These evaluation methods are used to test whether models learn to avoid problematic behaviors without requiring human-labeled data
  - Quick check question: What is the format of the evaluation datasets used to test for problematic behavioral traits, and how is performance measured?

## Architecture Onboarding

- Component map: Response generation models -> AI feedback models -> Preference models -> RL fine-tuning
- Critical path: Generate prompts → Generate response pairs → Get AI feedback using constitutional principles → Train preference model → Use preference model for RL fine-tuning
- Design tradeoffs: Using a single general principle ("do what's best for humanity") provides broader applicability but less precise control compared to trait-specific constitutions; larger models provide better generalization but at higher computational cost
- Failure signatures: Preference models show degraded performance on traits with weak connections to the general principle; RL fine-tuning can lead to over-training and evasive responses; smaller models fail to detect subtle problematic behaviors
- First 3 experiments:
  1. Train a trait preference model using the constitutional approach on five specific problematic traits and evaluate on held-out trait datasets
  2. Train a good-for-humanity preference model using the same response pairs but with general constitutional principles, and compare performance on both the original traits and general harmfulness
  3. Fine-tune a language model via RL using the good-for-humanity preference model and evaluate for both helpfulness/harmlessness and problematic trait avoidance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Good-for-Humanity approach scale to larger models (beyond 175B parameters) while maintaining its effectiveness at preventing problematic behavioral traits?
- Basis in paper: Explicit - The paper discusses that the GfH PMs only work effectively at 175B parameters, with a significant performance jump at this scale.
- Why unresolved: The paper only tests up to 175B parameters and notes that smaller models (6.4B, 13B, 22B, 52B) struggle to learn the good-for-humanity principle effectively.
- What evidence would resolve it: Testing GfH PMs on models larger than 175B parameters (e.g., 500B, 1T parameters) to see if performance continues to improve or plateaus, and comparing their effectiveness at preventing problematic traits against specific trait PMs.

### Open Question 2
- Question: How can we ensure that Good-for-Humanity constitutions remain culturally neutral and fair across different societies and time periods?
- Basis in paper: Explicit - The discussion section explicitly raises this concern, stating "it simply leaves the interpretation of the GfH idea to AI systems themselves. This interpretation will necessarily be culture-bound, and is likely to vary by language and era."
- Why unresolved: The paper acknowledges this limitation but does not propose technical methods to address cultural bias in GfH interpretations.
- What evidence would resolve it: Developing and testing methods to measure and control for cultural bias in GfH PMs, potentially through diverse training data, explicit fairness constraints, or cross-cultural evaluation frameworks.

### Open Question 3
- Question: Can we optimize the constitutional principles themselves to improve the performance of Good-for-Humanity preference models?
- Basis in paper: Explicit - The paper states "It is possible that better preference modeling data can be generated by taking the average over all constitutional principles for each comparison label" but notes this is computationally intensive and left for future work.
- Why unresolved: The current GfH PMs use a fixed set of 9 constitutional principles that were "selected somewhat arbitrarily."
- What evidence would resolve it: Systematic testing of different constitutional principle formulations (varying wording, specificity, ethical frameworks) and comparing their effectiveness at training PMs that prevent problematic behaviors while maintaining helpfulness.

## Limitations
- The observed "phase transition" at 175B parameters needs more extensive validation across different model families and tasks
- General principles provide sufficient control for most practical applications, but may underestimate the value of specific principles for fine-grained behavior steering
- The effectiveness of Good-for-Humanity constitutions may vary across different cultures and time periods

## Confidence
- High confidence: The core finding that 175B models can learn general ethical behaviors from simple principles is well-supported by experimental results
- Medium confidence: The claim about a phase transition around 175B parameters requires more extensive validation
- Medium confidence: The assertion that general principles provide sufficient control for most practical applications, while technically supported, may underestimate the value of specific principles for fine-grained behavior steering

## Next Checks
1. Test the scaling hypothesis by evaluating models at intermediate sizes (e.g., 70B, 100B) to determine if the phase transition is truly discrete or represents a gradual improvement
2. Evaluate the general GfH principle on a broader set of problematic behaviors beyond the five tested traits to assess true generalization capabilities
3. Conduct human preference studies comparing outputs from trait-specific versus general constitutions to validate that the automatic evaluations capture human judgments of harmfulness and helpfulness