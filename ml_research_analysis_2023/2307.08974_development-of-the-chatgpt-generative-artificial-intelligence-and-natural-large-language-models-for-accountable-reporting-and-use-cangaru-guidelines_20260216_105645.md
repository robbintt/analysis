---
ver: rpa2
title: Development of the ChatGPT, Generative Artificial Intelligence and Natural
  Large Language Models for Accountable Reporting and Use (CANGARU) Guidelines
arxiv_id: '2307.08974'
source_url: https://arxiv.org/abs/2307.08974
tags:
- will
- guidelines
- reporting
- delphi
- gpts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This protocol outlines the development of CANGARU guidelines for
  ethical use of generative AI/GPT/LLM tools in academic research. The approach combines
  systematic review, bibliometric analysis of existing journal guidelines, and Delphi
  consensus methodology to establish three lists: what not to do, disclosure requirements,
  and reporting standards.'
---

# Development of the ChatGPT, Generative Artificial Intelligence and Natural Large Language Models for Accountable Reporting and Use (CANGARU) Guidelines

## Quick Facts
- arXiv ID: 2307.08974
- Source URL: https://arxiv.org/abs/2307.08974
- Reference count: 36
- Primary result: Protocol for developing standardized guidelines for ethical use, disclosure, and reporting of generative AI tools in academic research

## Executive Summary
This protocol outlines the development of CANGARU guidelines to address the growing need for standardized approaches to generative AI/GPT/LLM use in scholarly research. The approach combines systematic review, bibliometric analysis of existing journal guidelines, and Delphi consensus methodology to establish three lists: what not to do, disclosure requirements, and reporting standards. The Delphi panel will include 200+ global stakeholders. Expected outcome is standardized, cross-disciplinary guidance to prevent confusion from disparate publisher policies and ensure accountable use of AI tools in scholarly work.

## Method Summary
The CANGARU protocol employs a four-phase approach: a living systematic review of GAI/GPT/LLM applications in scholarly research, bibliometric analysis of existing journal instructions to authors, Delphi survey with 200+ global stakeholders across three rounds, and a consensus meeting to develop three distinct guideline lists. The systematic review will be updated prospectively at 6, 12, and 24 months, while bibliometric analysis will examine top publishers and journals to map current policy heterogeneity. The Delphi process uses iterative rounds with >80% agreement threshold to establish consensus on guideline items.

## Key Results
- Development of three distinct guideline lists: "DON'T" Criteria, Disclosure Criteria, and Reporting Criteria
- Systematic review will capture evolving GAI/GPT/LLM practices and reporting standards
- Delphi consensus methodology with 200+ global stakeholders to ensure cross-disciplinary applicability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A living systematic review combined with bibliometric analysis ensures comprehensive coverage of evolving GAI/GPT/LLM practices.
- Mechanism: Prospective continuous literature screening at 6, 12, and 24 months captures new publications and emerging trends, while bibliometric analysis of top publishers/journals maps current policy heterogeneity.
- Core assumption: The GAI/GPT/LLM field evolves rapidly enough that periodic, structured reviews will detect meaningful changes in practice and reporting standards.

### Mechanism 2
- Claim: Delphi consensus with 200+ global stakeholders produces guidelines that reflect diverse disciplinary norms.
- Mechanism: Structured anonymity and iterative rounds allow minority views to surface while majority agreement (>80%) is required for inclusion.
- Core assumption: Stakeholder diversity (academic, clinical, regulatory, publishing) ensures that resulting guidelines are globally and cross-disciplinarily applicable.

### Mechanism 3
- Claim: Three separate guideline lists (Don't, Disclosure, Reporting) address distinct ethical and practical dimensions without conflation.
- Mechanism: By splitting criteria into "what not to do," "what/how to disclose," and "how to report," each list targets a specific stage of the research workflow and avoids overlap.
- Core assumption: Different GAI/GPT/LLM risks (misuse, lack of transparency, reproducibility) map cleanly to separate checklist categories.

## Foundational Learning

- Concept: Systematic review methodology (PRISMA, PROSPERO registration)
  - Why needed here: Ensures reproducible capture of GAI/GPT/LLM literature and transparent inclusion/exclusion decisions.
  - Quick check question: What is the minimum number of independent reviewers required for abstract screening in this protocol?

- Concept: Delphi consensus technique
  - Why needed here: Enables anonymous, iterative agreement-building across a geographically and disciplinarily diverse panel.
  - Quick check question: What minimum agreement threshold is used to move items to the consensus meeting?

- Concept: Bibliometric analysis of journal instructions
  - Why needed here: Identifies existing heterogeneity in publisher policies to inform consensus items.
  - Quick check question: Which two data sources are used to define the "top" publishers and journals?

## Architecture Onboarding

- Component map: Core Team (CT) → Systematic Review → Bibliometric Analysis → Delphi Rounds (1-3) → Consensus Meeting → Piloting → Explanation Document → Dissemination
- Critical path: Bibliometric Analysis → Delphi Round 1 → Delphi Round 2 → Consensus Meeting (final statements)
- Design tradeoffs: Broader stakeholder pool (more diversity) vs. logistical complexity (scheduling, response rates); three separate lists (clarity) vs. potential redundancy.
- Failure signatures: Low Delphi response rates (<50%), >30% items failing to reach ≥80% agreement after 3 rounds, major publisher policy shifts during guideline development.
- First 3 experiments:
  1. Pilot abstract screening with 20 pre-2022 GAI/GPT/LLM papers to calibrate inclusion criteria.
  2. Test Delphi survey usability with 10 volunteer CT members before full rollout.
  3. Conduct a mock consensus meeting with 5 stakeholders to refine agenda and breakout structure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific threshold values for disclosure of AI tool usage will be considered acceptable across different academic disciplines, and how will these thresholds be determined and validated?
- Basis in paper: [explicit] The paper mentions developing a "Disclosure Criteria List" but doesn't specify threshold values for disclosure requirements.
- Why unresolved: The paper acknowledges the need for disclosure guidelines but doesn't address how to determine appropriate thresholds for different disciplines or research contexts.
- What evidence would resolve it: Empirical studies comparing current disclosure practices across disciplines, analysis of stakeholder perspectives on acceptable thresholds, and validation studies testing different threshold approaches.

### Open Question 2
- Question: How will the CANGARU guidelines address potential conflicts between AI tool usage restrictions and academic freedom, particularly in fields where AI assistance is becoming standard practice?
- Basis in paper: [inferred] The paper mentions developing a "DON'T Criteria List" but doesn't address how to balance restrictions with academic freedom.
- Why unresolved: The paper establishes the need for restrictions but doesn't explore the tension between these restrictions and the evolving role of AI tools in academic work.
- What evidence would resolve it: Case studies of existing conflicts, surveys of academic perspectives on AI tool usage, and analysis of how other fields have navigated similar tensions.

### Open Question 3
- Question: What mechanisms will be established to ensure the long-term maintenance and updating of the CANGARU guidelines as AI technology continues to evolve?
- Basis in paper: [explicit] The paper notes that "These guidelines pertain to a snapshot of the current state of this technology and will be updated as validation studies address the concerns or issues mentioned in this document."
- Why unresolved: While the need for updates is acknowledged, the paper doesn't specify how or when updates will be triggered or who will be responsible for maintaining the guidelines.
- What evidence would resolve it: Analysis of successful long-term guideline maintenance models, stakeholder input on update mechanisms, and a clear framework for version control and update triggers.

## Limitations

- The protocol assumes meaningful new GAI/GPT/LLM publications will emerge at predictable intervals for the "living" systematic review aspect
- Bibliometric analysis lacks explicit selection criteria beyond vague size metrics for identifying "top" publishers and journals
- The cross-disciplinary applicability claim assumes AI use patterns are sufficiently similar across fields

## Confidence

**High Confidence**: The systematic review methodology (PRISMA compliance, PROSPERO registration) is well-established and appropriate for capturing GAI/GPT/LLM literature. The Delphi consensus approach is a validated method for developing expert guidelines when used with appropriate sample sizes and iterative rounds.

**Medium Confidence**: The bibliometric analysis approach will identify meaningful heterogeneity in publisher policies. The three-list structure will provide practical utility without excessive overlap. The stakeholder diversity target of 200+ participants is achievable.

**Low Confidence**: The timeline for guideline development (6-24 months) will capture sufficient evolution in the field. The consensus threshold of >80% agreement will be achievable for most items without excessive attrition. The resulting guidelines will achieve meaningful adoption across diverse academic disciplines.

## Next Checks

1. **Bibliometric Sampling Validation**: Test the journal/publisher selection criteria by randomly sampling 20 journals from different ranking systems (Scopus, Web of Science, field-specific rankings) to verify that the "top 100" approach captures policy heterogeneity rather than just size.

2. **Delphi Recruitment Feasibility**: Conduct a pilot recruitment with 20 potential participants across 4 disciplines to validate recruitment messaging, response rates, and time burden estimates before full-scale rollout.

3. **List Separation Clarity**: Administer a cognitive walkthrough with 10 researchers from different fields, asking them to categorize 15 hypothetical AI use cases into the three proposed lists to identify where boundaries blur or overlap occurs.