---
ver: rpa2
title: Dynamic Demonstrations Controller for In-Context Learning
arxiv_id: '2310.00385'
source_url: https://arxiv.org/abs/2310.00385
tags:
- demonstrations
- examples
- performance
- number
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how the number of demonstrations in in-context
  learning (ICL) affects model performance. Contrary to the common belief that more
  demonstrations always improve performance, the authors show through pilot experiments
  that increasing demonstration numbers does not necessarily lead to better accuracy.
---

# Dynamic Demonstrations Controller for In-Context Learning

## Quick Facts
- arXiv ID: 2310.00385
- Source URL: https://arxiv.org/abs/2310.00385
- Reference count: 40
- One-line primary result: Dynamic selection of optimal demonstration count improves ICL performance by 5.4% on average across 10 datasets and 8 LLM sizes

## Executive Summary
This paper challenges the conventional wisdom that more demonstrations always improve in-context learning (ICL) performance. Through pilot experiments, the authors demonstrate that increasing demonstration numbers can yield inconsistent performance gains or even degradation across different datasets and models. To address this, they propose a Dynamic Demonstrations Controller (D²Controller) that automatically selects the optimal number of demonstrations for each dataset and model combination. The method uses an Intra-Inter-Class Score (IICScore) metric to evaluate and select the most suitable demonstrations, achieving an average 5.4% relative improvement over baseline methods while using fewer resources.

## Method Summary
The Dynamic Demonstrations Controller (D²Controller) dynamically selects the optimal number of demonstrations for each dataset and model combination in in-context learning. The method works by first determining the maximum demonstration count (kmax) based on the model's input length constraint. It then samples Ns groups of in-context examples and evaluates each k-shot setting (from 1 to kmax) using the IICScore metric. For each candidate evaluation example, IICScore computes the KL divergence between its distribution and the average distributions of same-class and different-class demonstrations. The k-shot setting with the highest accuracy (determined through IICScore-guided evaluation) is selected as optimal. This approach achieves better performance with fewer demonstrations compared to using the maximum number, while maintaining compatibility with other ICL methods like demonstration selection and calibration techniques.

## Key Results
- Achieves 5.4% average relative improvement over baseline k-shot settings across 10 datasets and 8 LLM sizes
- Outperforms using maximum demonstrations while maintaining near-optimal performance with fewer resources
- Demonstrates consistent effectiveness across diverse LLM architectures including GPT-2, Cerebras-GPT, OPT, and GPT-3 variants
- Maintains compatibility with other ICL enhancement methods like demonstration selection and calibration techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimal number of demonstrations varies depending on the dataset and model combination, rather than being universally optimal across all datasets and models.
- Mechanism: Different datasets and models have varying characteristics that affect how they benefit from different numbers of demonstrations. The proposed D2Controller method dynamically selects the appropriate demonstration count based on these variations.
- Core assumption: The relationship between demonstration count and performance is non-linear and dataset/model-specific.
- Evidence anchors:
  - [abstract]: "Through pilot experiments, we discover that increasing the number of demonstrations does not necessarily lead to improved performance."
  - [section]: "The optimal k-shot setting differs depending on specific datasets and models."
  - [corpus]: Weak. Related papers discuss demonstration selection but don't address the core claim about non-universal optimality.
- Break condition: If a single demonstration count consistently performed best across all datasets and models, this mechanism would be invalidated.

### Mechanism 2
- Claim: The Intra-Inter-Class Score (IICScore) effectively identifies representative evaluation examples for different demonstration numbers.
- Mechanism: IICScore combines intra-class similarity (closeness to same-class demonstrations) and inter-class distance (distance from other-class demonstrations) to select examples that provide accurate performance assessment.
- Core assumption: KL divergence is an appropriate measure of similarity between language model distributions.
- Evidence anchors:
  - [section]: "IICScore is defined as: IICScore(ecj, Ek i) = −KL(xcj, ¯xcIEk i) + Xc′∈C,c′̸=c |D′c′||D′| KL(xcj, ¯xc′IEk i)"
  - [section]: "The higher the IICScore is, the more similar that candidate example ecj is to class-c in-context examples."
  - [corpus]: Missing. No direct corpus evidence about the effectiveness of IICScore.
- Break condition: If alternative distance metrics (Euclidean distance, cosine similarity) consistently outperformed IICScore in example selection.

### Mechanism 3
- Claim: Dynamic demonstration selection saves computational resources while maintaining near-optimal performance.
- Mechanism: By selecting the optimal demonstration count for each dataset and model, D2Controller avoids using excessive demonstrations that don't improve performance, reducing inference costs.
- Core assumption: The performance degradation from using fewer demonstrations is offset by the computational savings.
- Evidence anchors:
  - [abstract]: "Our approach achieves better performance with fewer demonstrations compared to utilizing the maximum number of demonstrations."
  - [section]: "Our approach underscores the practical feasibility of striking a balance between performance and resource consumption."
  - [corpus]: Weak. Related papers discuss efficiency but not specifically in the context of dynamic demonstration count selection.
- Break condition: If the performance gains from optimal demonstration count selection were consistently outweighed by the computational overhead of the selection process.

## Foundational Learning

- Concept: In-Context Learning (ICL) and k-shot setting
  - Why needed here: The paper's core contribution is about optimizing demonstration count within ICL framework
  - Quick check question: What is the relationship between k-shot setting and total demonstration count in class-balanced ICL?

- Concept: KL divergence as a similarity measure
  - Why needed here: IICScore relies on KL divergence to measure distances between language model distributions
  - Quick check question: Why might KL divergence be preferred over Euclidean distance for comparing language model distributions?

- Concept: Text classification evaluation metrics
  - Why needed here: The paper uses accuracy as the primary evaluation metric across multiple datasets
  - Quick check question: How does accuracy handle class imbalance in multi-class classification tasks?

## Architecture Onboarding

- Component map: Data preprocessing -> Template-based transformation -> Evaluation example selection (IICScore) -> Performance evaluation -> Optimal k selection
- Critical path: Sampling in-context examples → Selecting evaluation examples using IICScore → Evaluating accuracy for each k → Selecting optimal k
- Design tradeoffs: The choice of Ns (number of in-context example groups) balances computational cost against evaluation accuracy
- Failure signatures:
  - Inconsistent performance improvements across different k-shot settings
  - Minimal differences in accuracy between different k-shot settings
  - Degradation in performance when using dynamically selected k compared to fixed k
- First 3 experiments:
  1. Implement IICScore with different distance metrics (KL divergence, Euclidean distance, cosine similarity) and compare evaluation example quality
  2. Vary Ns (number of in-context example groups) and measure impact on final performance
  3. Test D2Controller on a single dataset and model pair, comparing against baseline k-shot settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the number of demonstrations and model performance in ICL, and can this relationship be modeled mathematically?
- Basis in paper: [explicit] The paper states that "increasing the number of demonstrations does not necessarily lead to improved performance" and that the optimal number of demonstrations varies across datasets and models.
- Why unresolved: The paper provides empirical evidence of this phenomenon but does not offer a theoretical explanation or mathematical model to describe the relationship.
- What evidence would resolve it: A mathematical model that predicts the optimal number of demonstrations based on dataset characteristics and model properties, validated through extensive experiments across diverse datasets and model architectures.

### Open Question 2
- Question: How does the proposed Intra-Inter-Class Score (IICScore) metric compare to other distance metrics or similarity measures in selecting evaluation examples?
- Basis in paper: [explicit] The paper proposes the IICScore metric but also explores other methods like Random, Euclidean distance, and negative cosine similarity in the analysis section.
- Why unresolved: While the paper shows that IICScore performs better than these alternatives, it does not provide a comprehensive comparison with a wide range of other metrics or explain why IICScore is particularly effective.
- What evidence would resolve it: A thorough comparison of IICScore with various other distance metrics and similarity measures across multiple datasets and models, along with an analysis of the strengths and weaknesses of each approach.

### Open Question 3
- Question: Can the Dynamic Demonstrations Controller (D²Controller) be extended to handle more complex tasks beyond text classification, such as question answering or natural language inference?
- Basis in paper: [explicit] The paper focuses on text classification tasks and mentions that the method is model-agnostic, but does not explore its applicability to other NLP tasks.
- Why unresolved: The paper does not provide evidence or experiments demonstrating the effectiveness of D²Controller on tasks other than text classification.
- What evidence would resolve it: Successful application and evaluation of D²Controller on a variety of NLP tasks, including question answering, natural language inference, and text summarization, with comparisons to existing methods tailored for those specific tasks.

## Limitations
- The optimal k-shot setting discovery relies on pilot experiments that are not fully detailed in methodology
- The IICScore metric lacks comprehensive validation against a broad range of alternative distance metrics
- The paper focuses exclusively on text classification tasks without exploring generalizability to other NLP domains
- The computational efficiency claims would benefit from explicit resource usage comparisons with baseline methods

## Confidence

- **High confidence**: The empirical observation that increasing demonstrations doesn't always improve performance, supported by pilot experiments across multiple datasets and models.
- **Medium confidence**: The effectiveness of IICScore as a selection metric, given the theoretical foundation but limited comparative validation against alternative metrics.
- **Medium confidence**: The computational efficiency claims, as the paper demonstrates reduced resource usage but doesn't provide detailed cost-benefit analysis.
- **Medium confidence**: The generalizability of findings across different LLM sizes, though the consistent pattern across eight model variants provides supporting evidence.

## Next Checks

1. **IICScore validation**: Implement alternative distance metrics (cosine similarity, Euclidean distance) for evaluation example selection and compare performance against the KL divergence-based IICScore across the same dataset-model pairs.

2. **Resource efficiency quantification**: Measure and compare the actual computational costs (inference time, memory usage) of D²Controller versus baseline fixed-k methods, including the overhead of the dynamic selection process itself.

3. **Cross-task generalization**: Test D²Controller on non-text classification tasks (e.g., question answering, commonsense reasoning) to evaluate whether the optimal k-shot variability pattern holds across different types of in-context learning problems.