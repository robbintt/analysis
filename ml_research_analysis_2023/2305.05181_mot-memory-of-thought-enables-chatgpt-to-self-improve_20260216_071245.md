---
ver: rpa2
title: 'MoT: Memory-of-Thought Enables ChatGPT to Self-Improve'
arxiv_id: '2305.05181'
source_url: https://arxiv.org/abs/2305.05181
tags:
- answer
- memory
- reasoning
- language
- corr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MoT, a framework that enables Large Language
  Models (LLMs) to self-improve through pre-thinking and recalling with Memory-of-Thoughts,
  without the need for annotated datasets or parameter updates. The approach involves
  two stages: first, the LLM generates high-confidence reasoning paths (thoughts)
  from unlabeled data and stores them as external memory; second, during inference,
  the LLM retrieves relevant memories to assist in answering new questions.'
---

# MoT: Memory-of-Thought Enables ChatGPT to Self-Improve

## Quick Facts
- arXiv ID: 2305.05181
- Source URL: https://arxiv.org/abs/2305.05181
- Authors: 
- Reference count: 33
- Key outcome: MoT significantly improves ChatGPT's performance on reasoning tasks without annotated data or parameter updates

## Executive Summary
This paper introduces MoT (Memory-of-Thought), a framework enabling Large Language Models to self-improve through pre-thinking and recalling. The approach generates high-confidence reasoning paths from unlabeled data, stores them as external memory, and retrieves relevant memories during inference to assist reasoning. Experiments demonstrate significant performance improvements across arithmetic reasoning, commonsense reasoning, factual reasoning, and natural language inference tasks compared to zero-shot and few-shot baselines.

## Method Summary
MoT operates in two stages: first, an LLM generates reasoning paths (thoughts) from unlabeled data, uses majority voting to select consistent answers, and filters high-confidence paths using entropy-based uncertainty measures. Second, during inference, the LLM retrieves relevant memories through semantic clustering and LLM-based selection, then uses these memories with few-shot Chain-of-Thought prompting to answer new questions. The method requires no annotated datasets or parameter updates, relying solely on the LLM's reasoning capabilities.

## Key Results
- MoT significantly improves ChatGPT's performance on arithmetic, commonsense, factual reasoning, and NLI tasks
- Improvements are consistent across various decoding strategies and LLMs
- Each component (pre-thinking, memory filtering, retrieval) contributes critically to performance gains
- MoT outperforms both zero-shot and few-shot baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-thinking generates high-confidence reasoning paths that serve as a memory bank for future inference
- Mechanism: LLM generates multiple reasoning paths per unlabeled example, uses majority voting to select the most consistent answer, then filters by entropy to retain only high-confidence paths
- Core assumption: LLM's internal uncertainty estimation via entropy correlates with correctness of generated reasoning paths
- Evidence anchors: [abstract] "LLM generates high-confidence reasoning paths (thoughts) from unlabeled data and stores them as external memory"; [section] "We propose the following answer-entropy to filter out high-uncertainty thoughts"

### Mechanism 2
- Claim: During inference, LLM retrieves memory that complements its own reasoning by capturing deep logical connections beyond semantic similarity
- Mechanism: LLM clusters memory, semantically filters candidates with SBERT, then selects the most helpful example using LLM-based reasoning about relevance
- Core assumption: LLM has sufficient meta-reasoning capacity to judge which memory is helpful for a given test question
- Evidence anchors: [abstract] "LLM recalls relevant memory to help itself reason and answer it"; [section] "Since the LLM, e.g., ChatGPT, has shown impressively powerful and general natural language understanding capacity... we propose to let the LLM retrieve helpful memory for itself"

### Mechanism 3
- Claim: Combining retrieved memory with few-shot prompting improves reasoning over standalone CoT baselines
- Mechanism: Test question + retrieved memories are fed into few-shot CoT prompting, enabling the LLM to leverage both external reasoning paths and its own reasoning
- Core assumption: Relevant memory provides complementary reasoning cues that the LLM can integrate effectively
- Evidence anchors: [abstract] "Experimental results show that MoT can help ChatGPT significantly improve its abilities... compared to zero-shot and few-shot baselines"; [section] "s = LLM(m(1),m (2),··· ,m (k),q test)"

## Foundational Learning

- **Concept**: Entropy-based uncertainty filtering
  - Why needed here: Removes incorrect or uncertain reasoning paths that could degrade memory quality
  - Quick check question: What threshold value of entropy is used to filter memory paths in the paper?

- **Concept**: Semantic clustering for memory retrieval
  - Why needed here: Enables efficient retrieval from large memory pools by grouping similar reasoning paths
  - Quick check question: How many clusters are used for memory retrieval in the paper?

- **Concept**: Chain-of-Thought prompting
  - Why needed here: Provides the reasoning framework that both generates memory and processes retrieved memory
  - Quick check question: What format is used to parse the final answer from LLM output?

## Architecture Onboarding

- **Component map**: Unlabeled data → Pre-thinking → Memory storage → Retrieval → Inference → Final answer
- **Critical path**: Unlabeled data → Pre-thinking → Memory storage → Retrieval → Inference → Final answer
- **Design tradeoffs**:
  - Memory size vs. quality: Higher entropy threshold reduces memory but improves quality
  - Retrieval diversity vs. relevance: More clusters increase diversity but may reduce immediate relevance
  - Computational cost: Multiple LLM calls (pre-thinking, retrieval, inference) vs. single baseline call
- **Failure signatures**:
  - Low recall performance: Likely due to poor memory quality or retrieval errors
  - High variance across datasets: Indicates sensitivity to dataset characteristics or memory filtering thresholds
  - Degradation below baseline: Suggests memory interference or retrieval of misleading examples
- **First 3 experiments**:
  1. Run pre-thinking on a small unlabeled subset and verify memory filtering by checking entropy scores
  2. Test retrieval with a fixed query and inspect selected memory candidates for relevance
  3. Compare MoT inference output with and without retrieved memory on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MoT perform on multilingual reasoning tasks compared to monolingual ones?
- Basis in paper: [inferred] The paper focuses on English datasets and does not explore multilingual reasoning capabilities
- Why unresolved: The authors only tested MoT on English datasets, leaving multilingual reasoning performance unexplored
- What evidence would resolve it: Experiments on multilingual reasoning datasets across different language families would reveal MoT's cross-lingual generalization capabilities

### Open Question 2
- Question: What is the impact of memory size on MoT's performance and efficiency?
- Basis in paper: [explicit] The authors mention memory filtering but don't systematically study how different memory sizes affect performance
- Why unresolved: The paper doesn't explore the trade-off between memory size, performance, and computational cost
- What evidence would resolve it: Systematic experiments varying memory size while measuring performance and inference time would reveal optimal memory scaling

### Open Question 3
- Question: How does MoT handle ambiguous or underspecified questions?
- Basis in paper: [inferred] The paper focuses on well-defined questions but doesn't address handling of ambiguous queries
- Why unresolved: The authors don't discuss MoT's behavior when faced with questions lacking clear context or multiple valid interpretations
- What evidence would resolve it: Experiments with deliberately ambiguous questions and analysis of MoT's handling strategies would reveal its robustness to unclear inputs

### Open Question 4
- Question: Can MoT be extended to non-textual reasoning tasks like visual reasoning?
- Basis in paper: [explicit] The authors focus exclusively on text-based reasoning tasks
- Why unresolved: The paper doesn't explore whether MoT's framework can be adapted for multimodal reasoning
- What evidence would resolve it: Experiments applying MoT to visual reasoning datasets like VQA would demonstrate its potential for multimodal reasoning tasks

## Limitations
- Entropy-based filtering as a proxy for reasoning path quality lacks empirical validation
- LLM's capacity for reliable self-assessment of memory relevance during retrieval is asserted but not demonstrated
- Claim that retrieved memories "complement" rather than interfere with reasoning remains largely theoretical

## Confidence

- **High Confidence**: The architectural framework combining pre-thinking, memory storage, and retrieval is clearly specified and logically coherent
- **Medium Confidence**: The reported performance improvements over zero-shot and few-shot baselines are measurable, though the extent of contribution from each component is unclear
- **Low Confidence**: The core mechanism that entropy correlates with reasoning path quality, and that LLM can reliably self-assess memory relevance, lacks direct empirical support

## Next Checks

1. **Correlation Validation**: Measure the actual correlation between entropy scores of pre-thinking outputs and their correctness rates to validate the filtering mechanism's reliability
2. **Retrieval Ablation**: Test performance with manually inserted irrelevant memories to quantify how much memory interference affects results
3. **Component Contribution**: Run ablation studies isolating each MoT component (pre-thinking, entropy filtering, clustering, retrieval) to determine individual impact on performance gains