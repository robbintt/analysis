---
ver: rpa2
title: Unsupervised Sentiment Analysis of Plastic Surgery Social Media Posts
arxiv_id: '2307.02640'
source_url: https://arxiv.org/abs/2307.02640
tags:
- sentiment
- surgery
- social
- plastic
- media
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether unsupervised learning can effectively
  label social media posts about plastic surgery for sentiment analysis. The authors
  scraped Reddit and Twitter posts, applied TF-IDF vectorization, and used t-SNE,
  k-means clustering, and LDA to generate sentiment labels.
---

# Unsupervised Sentiment Analysis of Plastic Surgery Social Media Posts

## Quick Facts
- arXiv ID: 2307.02640
- Source URL: https://arxiv.org/abs/2307.02640
- Reference count: 27
- Primary result: Unsupervised methods achieved nearly 90% accuracy on sentiment classification of plastic surgery social media posts

## Executive Summary
This study investigates whether unsupervised learning can effectively label social media posts about plastic surgery for sentiment analysis. The authors scraped Reddit and Twitter posts, applied TF-IDF vectorization, and used t-SNE, k-means clustering, and LDA to generate sentiment labels. They then trained simple neural networks to classify sentiment as positive, negative, or neutral. The unsupervised approach achieved nearly 90% accuracy, outperforming a supervised document classification task. The models struggled most with distinguishing positive from negative sentiment, suggesting vernacular usage of terms confounds polarity detection.

## Method Summary
The authors scraped Reddit and Twitter posts about plastic surgery, concatenated them into a combined corpus, and applied preprocessing (lowercase, remove punctuation/emojis, tokenize, remove stop words, stem). They fit a TF-IDF vectorizer (unigrams) to the corpus, then applied t-SNE, k-means clustering, and LDA topic modeling to generate sentiment labels through analyst judgment. The TF-IDF matrix was split 80/20 for training and testing, and two neural network architectures were trained: a dense network (ReLU→Dropout→Softmax) and a 1D-CNN (conv→pool→dropout→dense→softmax), both using Adam optimizer and categorical cross-entropy loss for 15 epochs.

## Key Results
- Unsupervised sentiment analysis achieved nearly 90% accuracy on test set
- Outperformed supervised document classification (77.78% accuracy)
- Models struggled most with distinguishing positive from negative sentiment
- Dropout regularization improved validation and test accuracies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unsupervised methods can effectively generate sentiment labels by exploiting semantic similarity patterns in social media corpora.
- Mechanism: TF-IDF vectorization creates weighted term features that, when combined with clustering (k-means) and dimensionality reduction (t-SNE), reveal natural groupings of documents with similar sentiment orientations. LDA topic modeling supports analyst judgment by surfacing key terms that map to sentiment clusters.
- Core assumption: Document similarity in high-dimensional TF-IDF space correlates with sentiment polarity, even without manual labeling.
- Evidence anchors:
  - [abstract] "using extremely simple deep learning models, this study demonstrates that the applied results of unsupervised analysis allow a computer to predict either negative, positive, or neutral user sentiment... with almost 90% accuracy."
  - [section] "the applied results of unsupervised analysis allow a computer to predict either negative, positive, or neutral user sentiment towards plastic surgery... with almost 90% accuracy."
  - [corpus] Weak: no direct corpus-level validation that semantic similarity equals sentiment polarity; relies on analyst judgment mapping clusters to sentiment.
- Break condition: If term weighting fails to capture sentiment-bearing words (e.g., sarcasm, negation), or if clusters do not correspond to coherent sentiment themes.

### Mechanism 2
- Claim: Simple dense neural networks can generalize sentiment prediction from unsupervised labels better than supervised document classification.
- Mechanism: A shallow architecture with ReLU and dropout regularization captures decision boundaries defined by unsupervised clustering, achieving higher test accuracy (87.12%) than supervised classification (77.78%).
- Core assumption: Unsupervised labels encode richer, more generalizable patterns than simple category labels for this domain.
- Evidence anchors:
  - [abstract] "the model is capable of achieving higher accuracy on the unsupervised sentiment task than on a rudimentary supervised document classification task."
  - [section] "the model was able to achieve better performance on unsupervised sentiment analysis versus supervised document classification."
  - [corpus] Weak: no external validation that unsupervised labeling is more effective across other domains; performance tied to specific corpus and feature set.
- Break condition: If unsupervised labels are noisy or inconsistent, or if domain-specific vernacular confuses polarity detection.

### Mechanism 3
- Claim: Dropout regularization mitigates overfitting in both dense and temporal convolutional networks, stabilizing validation performance.
- Mechanism: Dropout rates of 0.3 and 0.6 improve test accuracy and reduce loss compared to no dropout, while excessive dropout (0.6) in CNNs causes erratic validation loss.
- Core assumption: The models are prone to overfitting given small dataset size relative to feature dimensionality.
- Evidence anchors:
  - [section] "Increasing dropout rate in both model cases increases both validation and test accuracies overall."
  - [section] "Increasing dropout rate for the 1D-CNN does not improve validation or test loss compared to using no dropout regularization, and instead caused the validation loss to behave erratically."
  - [corpus] Weak: no evidence provided about optimal dropout rate selection or generalization beyond these experiments.
- Break condition: If dropout rate is too high relative to model capacity, causing underfitting or unstable training.

## Foundational Learning

- Concept: Term Frequency-Inverse Document Frequency (TF-IDF)
  - Why needed here: Converts raw text into weighted features that emphasize distinctive terms across documents, enabling semantic clustering.
  - Quick check question: How does TF-IDF balance term importance within a document versus across the corpus?

- Concept: t-Distributed Stochastic Neighbor Embedding (t-SNE)
  - Why needed here: Reduces high-dimensional TF-IDF space to 2D for visual inspection of document clusters and outlier detection.
  - Quick check question: Why is perplexity set to 50.0 in this study, and what does it control?

- Concept: Latent Dirichlet Allocation (LDA)
  - Why needed here: Generates interpretable topics by modeling term distributions, supporting analyst judgment for labeling clusters.
  - Quick check question: How do the top 20 terms per topic help in assigning sentiment labels to k-means clusters?

## Architecture Onboarding

- Component map: Text scraping → TF-IDF vectorization → t-SNE + k-means + LDA (unsupervised labeling) → Train/Test split → Dense or 1D-CNN → Evaluate accuracy/loss → Confusion matrix analysis
- Critical path: Unsupervised labeling must complete before training sentiment models; label quality directly impacts downstream accuracy
- Design tradeoffs: Unigrams vs. n-grams (unigrams simpler but may miss context); dropout rates (balance overfitting vs. underfitting); model complexity (dense vs. CNN, minimal gain observed)
- Failure signatures: High training accuracy but low validation accuracy (overfitting); class imbalance skewing metrics; clusters not aligning with intuitive sentiment themes
- First 3 experiments:
  1. Re-run t-SNE with different perplexity values (e.g., 30, 50, 70) and observe cluster stability
  2. Test bigram TF-IDF features and compare clustering quality and model accuracy
  3. Apply early stopping callback during training to see if validation loss improves

## Open Questions the Paper Calls Out

- Question: How does the performance of the unsupervised sentiment analysis approach compare to a fully supervised approach using the same neural network architecture but with manually labeled training data?
- Basis in paper: [inferred] The paper shows that the unsupervised approach achieved nearly 90% accuracy, outperforming a supervised document classification task. However, it does not directly compare the unsupervised approach to a supervised approach using manually labeled sentiment data.
- Why unresolved: The paper does not conduct this direct comparison, instead comparing the unsupervised approach to a supervised document classification task which is not the same as sentiment analysis.
- What evidence would resolve it: Conduct an experiment training the same neural network architecture on manually labeled sentiment data and compare its performance to the unsupervised approach on the same test set.

- Question: How sensitive is the unsupervised sentiment labeling approach to the choice of hyperparameters such as the number of topics in LDA and the number of clusters in k-means?
- Basis in paper: [explicit] The paper uses k-means with k=8, 3, and 2, and LDA with 8, 3, and 2 topics. However, it does not explore the sensitivity of the results to these choices.
- Why unresolved: The paper does not systematically vary these hyperparameters or analyze the impact on the quality of the sentiment labels generated.
- What evidence would resolve it: Conduct experiments varying the number of topics and clusters, and evaluate the impact on the accuracy of the sentiment labels and the performance of the neural network trained on those labels.

- Question: How well does the unsupervised sentiment analysis approach generalize to other domains beyond plastic surgery?
- Basis in paper: [inferred] The paper demonstrates the approach on a specific domain (plastic surgery social media posts) but does not test its applicability to other domains.
- Why unresolved: The paper does not explore the generalizability of the approach to other types of social media posts or textual data.
- What evidence would resolve it: Apply the unsupervised sentiment analysis approach to social media posts from other domains (e.g., politics, sports, entertainment) and evaluate its performance compared to supervised approaches in those domains.

## Limitations
- Sentiment labels were assigned through analyst judgment mapping of unsupervised clusters, with no ground truth validation or inter-annotator agreement reported
- Relatively small corpus of 4,156 documents, performance may not generalize to larger or more diverse datasets
- Models' difficulty distinguishing positive from negative sentiment suggests vernacular usage confounds polarity detection, but mitigation strategies are not explored

## Confidence
- Medium: The technical implementation is sound and accuracy results are impressive, but lack of external validation and potential subjectivity in label assignment reduce confidence

## Next Checks
1. Conduct inter-annotator agreement study on the unsupervised label assignments to assess reliability of the sentiment mapping process
2. Test the unsupervised labeling approach on a held-out subset of posts with manual sentiment annotations to validate accuracy claims against ground truth
3. Apply the methodology to a different domain (e.g., product reviews or political discourse) to assess generalizability beyond plastic surgery