---
ver: rpa2
title: 'Inter-case Predictive Process Monitoring: A candidate for Quantum Machine
  Learning?'
arxiv_id: '2307.00080'
source_url: https://arxiv.org/abs/2307.00080
tags:
- quantum
- process
- inter-case
- which
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores applying quantum machine learning (QML) to
  predictive process monitoring (PPM) problems. The core idea is to use quantum kernel
  methods to capture complex patterns between process instances (inter-case dependencies).
---

# Inter-case Predictive Process Monitoring: A candidate for Quantum Machine Learning?

## Quick Facts
- arXiv ID: 2307.00080
- Source URL: https://arxiv.org/abs/2307.00080
- Reference count: 40
- Primary result: Quantum kernels significantly outperformed classical ones in accuracy when using inter-case features for predictive process monitoring

## Executive Summary
This paper explores applying quantum machine learning (QML) to predictive process monitoring (PPM) problems, specifically using quantum kernel methods to capture complex patterns between process instances (inter-case dependencies). The authors benchmarked classical and quantum classifiers on real-world event log data, finding that quantum kernels significantly outperformed classical ones in accuracy when using inter-case features. For example, a quantum kernel with a zz feature map achieved over 4% higher accuracy than a classical SVM with an RBF kernel on one dataset. However, quantum simulations were much slower than classical methods. The authors also implemented a prototype plugin connecting a workflow engine to IBM quantum cloud services. Overall, the results suggest QML is a promising direction for PPM, but quantum hardware needs further development to be practical.

## Method Summary
The study applied quantum kernel methods to predictive process monitoring using real-world event logs from BPI challenges (BPIC17 loan applications and RTFM road traffic fines). Event logs were preprocessed to remove rare variants and shortened by timeframe. The authors used index-based intra-case features augmented with inter-case features (peer cases, peer activities, resources, delays) extracted using a moving time window set to 50% of median case duration. Quantum kernel estimators with zz and angle feature maps were trained and evaluated against classical SVM with RBF kernels and XGBoost baselines using 3-fold cross-validation. Stratification sampling was employed to reduce quantum kernel training time by 85% while maintaining accuracy.

## Key Results
- Quantum kernel with zz feature map achieved over 4% higher accuracy than classical SVM with RBF kernel on one dataset
- Stratification sampling reduced quantum kernel training time by 85% while maintaining the same accuracy
- Quantum kernels inherently captured PPM-specific patterns between cases, significantly outperforming classical kernel-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantum kernels inherently capture inter-case dependencies better than classical kernels.
- Mechanism: The quantum feature map embeds high-dimensional data into a Hilbert space where complex, non-linear correlations between cases are linearly separable.
- Core assumption: Quantum Hilbert space representation allows for richer feature interactions than classical kernel methods.
- Evidence anchors:
  - [abstract] "quantum kernels significantly outperformed classical ones in accuracy when using inter-case features"
  - [section] "the quantum kernels were able to inherently capture the PPM-specific patterns between cases"
  - [corpus] Weak evidence; no direct citations in corpus papers about quantum kernels for process mining
- Break Condition: If the quantum feature map cannot encode the relevant features from the event log, or if the Hilbert space dimension is insufficient.

### Mechanism 2
- Claim: Quantum kernels provide a practical accuracy improvement for inter-case PPM problems.
- Mechanism: By leveraging quantum superposition and entanglement in the feature map, quantum kernels can model complex inter-case relationships (e.g., resource contention, temporal dependencies) more effectively.
- Core assumption: The problem structure of inter-case PPM benefits from the quantum advantage in modeling correlations.
- Evidence anchors:
  - [abstract] "a quantum kernel with a zz feature map achieved over 4% higher accuracy than a classical SVM with an RBF kernel"
  - [section] "the quantum kernels were able to inherently capture the PPM-specific patterns between cases, thus significantly outperforming classical kernel-based approaches"
  - [corpus] Weak evidence; no direct citations in corpus papers about quantum kernels for process mining
- Break Condition: If the inter-case features are not the primary drivers of prediction accuracy, or if classical methods can adequately model the relationships.

### Mechanism 3
- Claim: Stratification sampling can significantly reduce quantum kernel training time without sacrificing accuracy.
- Mechanism: By training on a subset of the data that maintains the class distribution, the quantum kernel estimator can learn the decision boundary more efficiently.
- Core assumption: The quantum kernel estimator is robust to reduced training data when class distribution is preserved.
- Evidence anchors:
  - [section] "training time can be reduced by 85% while maintaining the same accuracy by considering only 50% of the features"
  - [corpus] Weak evidence; no direct citations in corpus papers about quantum kernels for process mining
- Break Condition: If the reduced dataset no longer captures the full variability of the inter-case features, or if the quantum kernel estimator is sensitive to the size of the training data.

## Foundational Learning

- Concept: Quantum computing basics (qubits, superposition, entanglement)
  - Why needed here: To understand how quantum kernels work and why they might provide an advantage over classical methods.
  - Quick check question: What is the difference between a classical bit and a qubit?
- Concept: Quantum kernel methods (feature maps, Hilbert space, inner products)
  - Why needed here: To understand how quantum kernels are implemented and how they capture complex patterns in the data.
  - Quick check question: How does a quantum feature map differ from a classical kernel function?
- Concept: Predictive process monitoring (event logs, inter-case dependencies, feature extraction)
  - Why needed here: To understand the problem domain and why inter-case features are important for accurate predictions.
  - Quick check question: What is an inter-case dependency in the context of process mining?

## Architecture Onboarding

- Component map: Event log preprocessing -> Feature extraction -> Quantum kernel estimator -> Classical SVM -> Workflow engine integration
- Critical path:
  1. Load and preprocess event log data
  2. Extract inter-case features and augment intra-case features
  3. Encode features into quantum circuit
  4. Estimate kernel matrix on quantum simulator or hardware
  5. Train SVM with estimated kernel matrix
  6. Deploy model in workflow engine via plugin
- Design tradeoffs:
  - Quantum simulator vs. real quantum hardware (accuracy vs. runtime)
  - Number of qubits and circuit depth (expressiveness vs. noise)
  - Feature extraction methods (inter-case feature selection and window size)
- Failure signatures:
  - Low accuracy despite high-dimensional feature space (quantum feature map not capturing relevant patterns)
  - High variance in accuracy across cross-validation folds (overfitting or noisy data)
  - Extremely long training times (inefficient quantum kernel estimation or large dataset)
- First 3 experiments:
  1. Compare accuracy of quantum kernel vs. classical RBF kernel on a small dataset with inter-case features
  2. Vary the number of qubits and circuit depth in the quantum feature map to assess impact on accuracy and runtime
  3. Test stratification sampling with different percentages of the training data to find the optimal balance between accuracy and training time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can quantum kernel methods be optimized to reduce runtime while maintaining accuracy for large-scale event logs?
- Basis in paper: [explicit] The paper discusses runtime as a bottleneck for quantum kernel simulations and shows that undersampling reduces training time without accuracy loss.
- Why unresolved: Current NISQ hardware limitations and the trade-off between simulation accuracy and computational efficiency remain unclear for large-scale real-world applications.
- What evidence would resolve it: Empirical studies comparing runtime and accuracy of quantum kernel methods on progressively larger event logs using both simulators and real quantum hardware.

### Open Question 2
- Question: What is the optimal configuration of inter-case features and quantum kernels for different types of business processes?
- Basis in paper: [explicit] The paper shows that accuracy varies significantly with inter-case feature combinations and quantum kernel types, but does not provide a systematic method for selecting optimal configurations.
- Why unresolved: The interplay between process characteristics, feature selection, and quantum kernel performance is not fully understood, making it difficult to generalize findings.
- What evidence would resolve it: A comprehensive benchmark across diverse process types with automated feature and kernel selection methods.

### Open Question 3
- Question: Can quantum machine learning models outperform classical models in terms of both accuracy and runtime for predictive process monitoring?
- Basis in paper: [explicit] The paper finds quantum kernels achieve higher accuracy but are slower than classical methods in simulations, questioning their practical advantage.
- Why unresolved: Current NISQ hardware limitations prevent definitive conclusions about quantum advantage, and the gap between simulation and real hardware performance is unclear.
- What evidence would resolve it: Comparative studies using real quantum hardware on large-scale process monitoring tasks, measuring both accuracy and runtime against state-of-the-art classical methods.

## Limitations
- Quantum advantage is currently demonstrated only in simulation, not on actual quantum hardware
- The 4% accuracy improvement figure is based on a single inter-case feature combination
- Quantum kernels' ability to capture complex patterns may be limited by current noisy quantum hardware and simulator constraints

## Confidence

- **High confidence**: The superiority of quantum kernels over classical methods for accuracy in inter-case PPM scenarios, supported by cross-validation results on two real datasets
- **Medium confidence**: The potential of stratification sampling to reduce training time by 85% without accuracy loss, as this requires careful implementation and may depend on dataset characteristics
- **Low confidence**: The practical viability of quantum kernel methods for real-time PPM deployment, given current quantum hardware limitations and simulation runtime costs

## Next Checks

1. **Hardware Validation**: Replicate the experiments on actual quantum hardware (IBM Quantum) to verify if the accuracy gains observed in simulation persist under hardware noise and gate errors
2. **Feature Sensitivity Analysis**: Systematically test the impact of different inter-case feature combinations and window sizes on quantum kernel performance to identify optimal configurations
3. **Runtime Benchmarking**: Compare the wall-clock training times of quantum kernels (on both simulator and hardware) against classical methods on datasets of increasing size to establish practical scalability limits