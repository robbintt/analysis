---
ver: rpa2
title: On the Interplay Between Stepsize Tuning and Progressive Sharpening
arxiv_id: '2312.00209'
source_url: https://arxiv.org/abs/2312.00209
tags:
- stepsize
- sharpness
- learning
- epoch
- polyak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the interplay between stepsize tuning and
  progressive sharpening in deep learning. The authors empirically study how sharpness
  evolves when using stepsize-tuners like Armijo linesearch and Polyak stepsizes.
---

# On the Interplay Between Stepsize Tuning and Progressive Sharpening

## Quick Facts
- arXiv ID: 2312.00209
- Source URL: https://arxiv.org/abs/2312.00209
- Reference count: 40
- Key outcome: This paper investigates the interplay between stepsize tuning and progressive sharpening in deep learning. The authors empirically study how sharpness evolves when using stepsize-tuners like Armijo linesearch and Polyak stepsizes. They find that Armijo linesearch in the deterministic setting performs worse than constant stepsize due to its tendency to ever-increase sharpness. In contrast, Polyak stepsizes operate at or above the edge of stability and outperform constant stepsize counterparts. In the stochastic setting, Armijo's performance depends highly on batch size, while Polyak performs reasonably well across settings. The authors conclude that designing effective stepsize tuners requires understanding the joint dynamics of stepsize and sharpness, as simple models fail to capture the observed behavior.

## Executive Summary
This paper explores how different stepsize tuning methods interact with the progressive sharpening phenomenon in deep learning optimization. The authors conduct empirical studies comparing Armijo linesearch and Polyak stepsizes against constant stepsize baselines across deterministic and stochastic settings. They discover that Armijo linesearch, while guaranteeing monotonic loss decrease, tends to increase sharpness in deterministic settings, leading to poor performance. In contrast, Polyak stepsizes naturally operate at the edge of stability and achieve better results. The study reveals that stepsize tuners must account for the joint dynamics of stepsize and sharpness, as simple theoretical models fail to predict their behavior.

## Method Summary
The authors investigate stepsize tuning methods (Armijo linesearch and Polyak stepsizes) in both deterministic (full-batch) and stochastic (mini-batch) settings. They train three architectures (MLP, VGG11, ResNet34) on CIFAR10 using squared loss with weight decay. The experiments compare constant stepsize GD/SGD with Armijo and Polyak tuners, monitoring both train loss and sharpness (largest Hessian eigenvalue) computed via power iteration. Sharpness is measured periodically (every 50 epochs) to track the progressive sharpening phenomenon during training.

## Key Results
- Armijo linesearch in deterministic settings guarantees monotonic loss decrease but performs worse than constant stepsize due to progressive sharpness increase
- Polyak stepsizes operate at or above the edge of stability and outperform constant stepsize counterparts
- In stochastic settings, Armijo's performance depends heavily on batch size, while Polyak performs reasonably well across settings
- Sharpness stabilizes around 2/γ for gradient descent when operating at the edge of stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Armijo line-search tends to increase sharpness because it accepts steps that improve the objective on the current mini-batch, even when those steps worsen the true objective's sharpness.
- Mechanism: In the deterministic setting, Armijo finds the largest step satisfying the Armijo condition. This often corresponds to taking aggressive steps that reduce the objective but increase the Hessian's largest eigenvalue. The sharpness increase occurs because the line-search criterion prioritizes immediate loss reduction over long-term stability.
- Core assumption: The Armijo condition with its fixed constant c=10^-4 allows steps that are too large from a sharpness-stability perspective.
- Evidence anchors:
  - [abstract] "we find that the surprisingly poor performance of a classical Armijo linesearch in the deterministic setting may be well explained by its tendency to ever-increase the sharpness of the objective"
  - [section] "Armijo-GD (linesearch, deterministic setting) gives monotonically decreasing loss for all architectures... However, Armijo-GD greatly underperforms constant stepsize GD"
- Break condition: If c is set very large, the line-search becomes too conservative and loses its practical benefits.

### Mechanism 2
- Claim: Polyak stepsize operates at or above the edge of stability because it sets the step size based on the ratio of loss gap to gradient norm, which naturally selects large steps when the loss is far from optimal.
- Mechanism: The Polyak formula γ = min{(ϕ(wt) - ϕ⋆)/||∇ϕ(wt)||², γmax} tends to select γ ≈ γmax = 1 when the loss is not near zero. This pushes the system to operate near the edge of stability (λ_max ≈ 2/γ) or slightly beyond it, creating oscillations around the critical value.
- Core assumption: The loss gap ϕ(wt) - ϕ⋆ is large enough throughout training that the formula consistently selects large steps.
- Evidence anchors:
  - [abstract] "Polyak stepsizes operate generally at the edge of stability or even slightly beyond"
  - [section] "Polyak-GD gives non-monotonic loss trajectories but with a faster decrease than Armijo, and leads to low final values of the sharpness"
- Break condition: If the loss gap becomes very small, the formula may select much smaller steps, losing the edge-of-stability behavior.

### Mechanism 3
- Claim: In the stochastic setting, Armijo's performance depends on batch size because the maximal acceptable step size on a mini-batch can be larger than the optimal step size for the full loss.
- Mechanism: The Armijo condition is evaluated on a mini-batch, which has higher variance than the full loss. This means the line-search may accept steps that would be too large if evaluated on the full batch, leading to inconsistent sharpness behavior across different batch sizes.
- Core assumption: The variance between mini-batch gradients and full-batch gradients is significant enough to affect the line-search outcome.
- Evidence anchors:
  - [section] "In the stochastic (minibatch) setting, the performance of Armijo depends highly on the batch size, while Polyak performs slightly less well than in the full batch setting"
  - [section] "For the largest batch size of 4096... we observe essentially the same behavior as in the deterministic (full batch) setting"
- Break condition: If batch size approaches the full dataset size, the stochastic effects diminish and Armijo behaves more like its deterministic counterpart.

## Foundational Learning

- Concept: Edge of Stability (EOS)
  - Why needed here: The paper's core finding is that sharpness stabilizes around 2/γ for gradient descent, and understanding this helps explain why certain stepsize tuners fail or succeed.
  - Quick check question: What value does the largest Hessian eigenvalue stabilize at for gradient descent with step size γ at the edge of stability?

- Concept: Progressive sharpening
  - Why needed here: This phenomenon describes how sharpness increases during early training, which is crucial for understanding why stepsize tuners that don't account for it may fail.
  - Quick check question: Does sharpness typically increase or decrease during the early stages of training deep networks with large learning rates?

- Concept: Stochastic vs deterministic optimization
  - Why needed here: The paper distinguishes between full-batch (deterministic) and mini-batch (stochastic) settings, showing different behaviors for stepsize tuners in each regime.
  - Quick check question: How does the variance in mini-batch gradients affect the behavior of line-search methods compared to full-batch optimization?

## Architecture Onboarding

- Component map: The system consists of a deep learning training loop with three main components: (1) the base optimizer (gradient descent variants), (2) stepsize tuners (Armijo line-search and Polyak stepsize), and (3) monitoring infrastructure (sharpness computation via power iteration, loss tracking). The sharpness computation runs periodically (every 50 epochs in the paper) to measure the largest Hessian eigenvalue.

- Critical path: The training loop iterates over epochs, computing gradients, selecting stepsizes via the tuner, updating weights, and periodically computing sharpness. The stepsize tuner is the critical component whose behavior determines convergence speed and sharpness dynamics.

- Design tradeoffs: Armijo prioritizes monotonic loss decrease but may increase sharpness; Polyak prioritizes large steps that maintain sharpness low but allows non-monotonic loss; fixed stepsize provides stable sharpness but requires manual tuning. The choice depends on whether sharpness stability or loss monotonicity is more important for the application.

- Failure signatures: Armijo fails when it keeps increasing sharpness while decreasing stepsize (deterministic setting); Polyak fails when the loss gap becomes too small and it selects tiny steps; both may fail when batch size is poorly matched to the algorithm's assumptions.

- First 3 experiments:
  1. Run Armijo-GD on a small MLP with full batch and monitor sharpness vs fixed stepsize GD to observe the progressive sharpening phenomenon.
  2. Run Polyak-GD with varying γmax values on the same architecture to see how the maximal step size affects sharpness and convergence.
  3. Run Armijo-SGD with different batch sizes (256, 1024, 4096) on a small network to observe the batch-size dependency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the tendency of Armijo to increase sharpness in the deterministic setting also occur with other line-search methods like backtracking line-search?
- Basis in paper: [explicit] The paper shows that Armijo line-search performs worse than constant stepsize due to its tendency to ever-increase sharpness in the deterministic setting.
- Why unresolved: The paper only investigates Armijo line-search, not other line-search methods.
- What evidence would resolve it: Compare the sharpness evolution and performance of Armijo with other line-search methods like backtracking line-search in the deterministic setting.

### Open Question 2
- Question: How does the batch size affect the performance of Polyak stepsizes in the stochastic setting compared to fixed stepsizes?
- Basis in paper: [inferred] The paper shows that Polyak stepsizes perform well across different batch sizes in the stochastic setting, but the performance of Armijo depends highly on the batch size.
- Why unresolved: The paper does not directly compare the performance of Polyak stepsizes with fixed stepsizes across different batch sizes.
- What evidence would resolve it: Conduct experiments comparing the performance of Polyak stepsizes and fixed stepsizes across various batch sizes in the stochastic setting.

### Open Question 3
- Question: What is the underlying mechanism that causes Polyak stepsizes to operate at or above the edge of stability?
- Basis in paper: [explicit] The paper observes that Polyak stepsizes operate generally at the edge of stability or even slightly beyond.
- Why unresolved: The paper does not provide a detailed explanation of why Polyak stepsizes tend to operate at or above the edge of stability.
- What evidence would resolve it: Investigate the theoretical properties of Polyak stepsizes and their relationship with the edge of stability.

## Limitations

- The paper focuses on specific architectures (MLP, VGG11, ResNet34) without batch normalization, leaving uncertainty about generalizability to other architectures
- The analysis uses squared loss with weight decay, and it's unclear whether conclusions extend to cross-entropy loss or other regularization schemes
- The sharpness computation via power iteration may have computational noise that affects the precision of edge-of-stability measurements

## Confidence

- **High Confidence**: The observation that Armijo linesearch in deterministic settings leads to monotonically decreasing loss but worse final performance compared to constant stepsize, explained by progressive sharpness increase.
- **Medium Confidence**: The claim that Polyak stepsizes operate at or above the edge of stability, as this requires precise sharpness measurements that may have computational noise.
- **Medium Confidence**: The batch-size dependency of Armijo in stochastic settings, as the effect is empirically observed but the theoretical explanation could benefit from more rigorous analysis.

## Next Checks

1. **Architecture Sensitivity Test**: Run the same experiments on architectures with batch normalization and dropout to verify whether the stepsize tuner behaviors generalize beyond the specific architectures studied.

2. **Loss Function Variation**: Repeat the experiments using cross-entropy loss instead of squared loss to test whether the sharpness dynamics and stepsize tuner performance are loss-function dependent.

3. **Theoretical Consistency Check**: Analyze whether the observed sharpness dynamics under different stepsize tuners can be predicted by the existing edge-of-stability theory, or whether new theoretical frameworks are needed to explain the empirical observations.