---
ver: rpa2
title: Universal Adversarial Framework to Improve Adversarial Robustness for Diabetic
  Retinopathy Detection
arxiv_id: '2312.08193'
source_url: https://arxiv.org/abs/2312.08193
tags:
- adversarial
- performance
- arxiv
- https
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the vulnerability of deep learning models for
  diabetic retinopathy (DR) detection to adversarial attacks. The authors propose
  a method to improve model robustness by fine-tuning with universal adversarial perturbations
  (UAPs).
---

# Universal Adversarial Framework to Improve Adversarial Robustness for Diabetic Retinopathy Detection

## Quick Facts
- arXiv ID: 2312.08193
- Source URL: https://arxiv.org/abs/2312.08193
- Reference count: 40
- Key outcome: Adversarial fine-tuning with universal adversarial perturbations (UAPs) significantly improves DR detection model robustness, with average Cohen-kappa increase of 3.41 and maximum increase of 31.92

## Executive Summary
This paper addresses the vulnerability of deep learning models for diabetic retinopathy (DR) detection to adversarial attacks by proposing a universal adversarial framework. The authors fine-tune state-of-the-art image classifiers with universal adversarial perturbations (UAPs) to enhance model robustness against unseen adversarial attacks. The framework demonstrates significant improvements in Cohen-kappa metric performance, particularly when using ensemble methods, while maintaining statistical insignificance in performance degradation on normal data.

## Method Summary
The method involves pre-training and fine-tuning seven state-of-the-art image classifiers on the APTOS2019 DR dataset, then performing adversarial fine-tuning using UAPs generated by each model. The models are evaluated using Cohen's Kappa metric to measure agreement between predictions and human annotations. An ensemble voting system combines predictions from multiple fine-tuned models to further improve robustness while maintaining performance on normal data.

## Key Results
- Adversarially fine-tuned models show average Cohen-kappa increase of 3.41 against unseen attacks
- Maximum Cohen-kappa improvement reaches 31.92 for certain model-attack combinations
- Ensemble of fine-tuned models maintains performance on normal data with statistically insignificant degradation (p-value > 0.05)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Universal Adversarial Perturbations (UAPs) are effective for adversarial fine-tuning because they exploit geometric correlations in the classifier's decision boundary.
- Mechanism: UAPs generate input-agnostic perturbations that generalize across different models and attack directions, unlike image-specific perturbations.
- Core assumption: The geometric structure of decision boundaries is shared across models, allowing UAPs to transfer effectively.
- Evidence anchors:
  - [abstract] "Our hypothesis is based on the observation that universal perturbations exploit geometric correlations in different parts of the classifier's decision boundary [5]."
  - [section] "UAPs generate perturbations that are agnostic to specific images and have demonstrated generalization capabilities across different models."
  - [corpus] Weak evidence - no direct citations found in corpus supporting geometric correlation claim.
- Break condition: If UAPs fail to achieve >90% fooling ratio during generation, indicating poor geometric exploitation.

### Mechanism 2
- Claim: Adversarial fine-tuning with self-generated UAPs improves robustness against unseen attacks.
- Mechanism: By training on perturbations generated by the model itself, the model learns to resist perturbations it will encounter during inference.
- Core assumption: Self-generated perturbations are representative of the adversarial space the model will face.
- Evidence anchors:
  - [section] "We perform adversarial fine-tuning of the models with the dataset perturbed using perturbation vectors of the model itself."
  - [section] "This ensures that the model has been trained on only one type of adversarial perturbation."
  - [corpus] No direct evidence in corpus supporting self-generated perturbation effectiveness.
- Break condition: If performance on unseen perturbations doesn't improve after fine-tuning, indicating limited transferability.

### Mechanism 3
- Claim: Ensemble voting improves robustness while maintaining performance on normal data.
- Mechanism: Combining predictions from multiple fine-tuned models reduces variance and increases resistance to individual model failures.
- Core assumption: Different models will fail on different perturbations, so ensemble averaging provides better coverage.
- Evidence anchors:
  - [section] "The performance degradation on normal data upon ensembling the fine-tuned models was found to be statistically insignificant using t-test."
  - [section] "The performance upon ensembling improves for some of the datasets."
  - [corpus] Weak evidence - no direct citations supporting ensemble voting for adversarial robustness.
- Break condition: If ensemble performance drops below individual model performance on normal data.

## Foundational Learning

- Concept: Universal Adversarial Perturbations (UAPs)
  - Why needed here: UAPs provide a model-agnostic way to generate adversarial examples that generalize across different models and attack types.
  - Quick check question: What makes UAPs "universal" compared to standard adversarial examples?

- Concept: Transfer learning in medical imaging
  - Why needed here: Pre-training on ImageNet and fine-tuning on medical datasets addresses data scarcity while leveraging learned feature representations.
  - Quick check question: Why is fine-tuning on APTOS2019 dataset critical for this DR detection task?

- Concept: Cohen's Kappa metric
  - Why needed here: Cohen's Kappa measures agreement between model predictions and human annotations, accounting for chance agreement.
  - Quick check question: Why is Cohen's Kappa preferred over accuracy for medical diagnosis tasks?

## Architecture Onboarding

- Component map: Pre-trained models (ConvNextTiny, DenseNet121, EfficientNet variants, MobileNetV3, RegNet, ResNet18) -> UAP generation module -> Adversarial fine-tuning pipeline -> Ensemble voting system -> Evaluation framework

- Critical path: UAP generation → Adversarial fine-tuning → Ensemble prediction → Evaluation

- Design tradeoffs:
  - High fooling ratio (>90%) vs. computational cost during UAP generation
  - Model-specific fine-tuning vs. generalization to unseen perturbations
  - Ensemble size vs. inference latency and resource usage

- Failure signatures:
  - Low fooling ratio during UAP generation (<90%)
  - Performance degradation on normal data after fine-tuning
  - Statistical significance test fails (p-value < 0.05)

- First 3 experiments:
  1. Verify UAP generation achieves >90% fooling ratio on validation set
  2. Compare Cohen's Kappa before and after adversarial fine-tuning on model's own perturbations
  3. Test ensemble performance on adversarial examples from all perturbation sources

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are UAPs at generating robust adversarial perturbations for multi-class DR classification across different severity levels?
- Basis in paper: [explicit] The authors mention using UAPs to quantify vulnerability and improve robustness, noting that universal perturbations exploit geometric correlations in classifier decision boundaries.
- Why unresolved: The paper doesn't provide detailed analysis of UAP effectiveness across different DR severity levels or compare it to other perturbation methods.
- What evidence would resolve it: Systematic evaluation of UAP robustness across all five DR severity classes and comparison with other adversarial attack methods.

### Open Question 2
- Question: How do ensemble methods affect the biological significance of DR classification results after adversarial fine-tuning?
- Basis in paper: [explicit] The authors note that statistical insignificance doesn't guarantee biological insignificance, highlighting the need for further investigation.
- Why unresolved: The paper focuses on statistical significance but doesn't explore clinical relevance or impact on patient care.
- What evidence would resolve it: Clinical validation studies showing improved diagnostic accuracy and patient outcomes with adversarially fine-tuned ensemble models.

### Open Question 3
- Question: What is the impact of adversarial training on model performance when encountering previously unseen DR datasets?
- Basis in paper: [explicit] The authors emphasize the need for models to be robust to unseen adversarial attacks in real-world scenarios.
- Why unresolved: The paper doesn't evaluate model performance on external, unseen DR datasets after adversarial fine-tuning.
- What evidence would resolve it: Cross-institutional validation studies using different DR datasets to assess generalizability and robustness of adversarially fine-tuned models.

## Limitations
- Lack of direct evidence supporting the geometric correlation hypothesis underlying UAP effectiveness
- Limited evaluation of model performance on external, unseen DR datasets
- No analysis of UAP effectiveness across different DR severity levels

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Ensemble voting methodology for improving robustness | High |
| Overall improvement in Cohen-kappa values for adversarially fine-tuned models | Medium |
| Theoretical foundation of why UAPs work for adversarial fine-tuning | Low |

## Next Checks

1. **Mechanism validation test**: Compare Cohen's kappa improvements between models fine-tuned with UAPs versus models fine-tuned with image-specific adversarial perturbations to isolate the "universal" advantage.

2. **Cross-model perturbation transferability**: Test whether UAPs generated for one model architecture can effectively improve robustness when used for fine-tuning a different architecture.

3. **Long-term stability evaluation**: Assess whether the robustness gains from adversarial fine-tuning persist after retraining on normal data for multiple epochs, measuring potential catastrophic forgetting of adversarial resistance.