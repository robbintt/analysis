---
ver: rpa2
title: 'Scientific Opinion Summarization: Paper Meta-review Generation Dataset, Methods,
  and Evaluation'
arxiv_id: '2305.14647'
source_url: https://arxiv.org/abs/2305.14647
tags:
- meta-review
- opinion
- reviews
- summarization
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of scientific opinion summarization,
  where research paper reviews are synthesized into meta-reviews. The authors propose
  the ORSUM dataset, containing 15,062 paper meta-reviews and 57,536 paper reviews
  from 47 conferences.
---

# Scientific Opinion Summarization: Paper Meta-review Generation Dataset, Methods, and Evaluation

## Quick Facts
- arXiv ID: 2305.14647
- Source URL: https://arxiv.org/abs/2305.14647
- Reference count: 31
- Key outcome: Introduces ORSUM dataset and CGI2 method for scientific opinion summarization, showing strong performance in meta-review generation tasks.

## Executive Summary
This paper introduces the task of scientific opinion summarization, where research paper reviews are synthesized into meta-reviews. The authors propose the ORSUM dataset, containing 15,062 paper meta-reviews and 57,536 paper reviews from 47 conferences. To address this task, they present the Checklist-guided Iterative Introspection (CGI2) approach, which breaks down the task into stages and iteratively refines the summary under the guidance of questions from a checklist. Experiments show that human-written summaries do not always satisfy all necessary criteria, and the combination of task decomposition and iterative self-refinement shows strong potential for enhancing opinions.

## Method Summary
The method introduces CGI2, a prompting-based approach that decomposes meta-review generation into four stages: opinion extraction, strength/weakness synthesis, consensus/controversy analysis, and final draft. The approach uses a predefined checklist to guide iterative refinement through self-feedback from GPT-3.5-turbo, eliminating the need for training data or human annotations. The ORSUM dataset was created by scraping OpenReview for paper reviews and meta-reviews across 47 conferences, containing 15,062 paper meta-reviews and 57,536 paper reviews.

## Key Results
- CGI2 outperforms existing methods in terms of discussion involvement, opinion faithfulness, and decision consistency
- Human-written meta-reviews only meet fundamental criteria for advantages/discussion in 47.7% and 20.7% of cases respectively
- The combination of task decomposition and iterative self-refinement shows strong potential for enhancing opinions in complex text generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task decomposition + iterative refinement improves scientific opinion summarization quality
- Mechanism: CGI2 breaks down the complex task into smaller steps (opinion extraction, strength/weakness synthesis, consensus/controversy analysis, final draft) and iteratively refines output based on self-generated feedback from a checklist
- Core assumption: LLMs struggle with complex instructions and produce hallucinations; breaking down tasks and providing iterative feedback mitigates these issues
- Evidence anchors: "combination of task decomposition and iterative self-refinement shows strong potential for enhancing the opinions" and "Motivated by the unreliability of human-written meta-reviews"

### Mechanism 2
- Claim: CGI2 produces more comprehensive meta-reviews by explicitly addressing pros/cons and consensus/controversy
- Mechanism: The checklist specifically prompts LLM to discuss paper's strengths/weaknesses and reviewer agreements/disagreements
- Core assumption: Human-written meta-reviews are unreliable because they don't always follow guidelines
- Evidence anchors: "human-written summaries do not always satisfy all necessary criteria such as depth of discussion" and findings that only 20.7% encompass detailed discussions

### Mechanism 3
- Claim: CGI2 approach can be applied to other complex text generation tasks using black-box LLMs
- Mechanism: Breaking down complex tasks into smaller steps with iterative refinement offers general solution for employing GPT as black box
- Core assumption: Principles of task decomposition and iterative refinement are applicable to other complex text generation tasks
- Evidence anchors: "can be applied to other complex text generation using black-box LLMs" and "provides a general solution for employing GPT as black box in complex text generation tasks"

## Foundational Learning

- Concept: Task decomposition
  - Why needed here: Complex tasks like scientific opinion summarization are difficult for LLMs to handle directly. Breaking them down into smaller, more manageable steps improves performance.
  - Quick check question: Can you identify the four steps in the CGI2 approach for meta-review generation?

- Concept: Iterative refinement
  - Why needed here: LLMs often require multiple attempts to generate high-quality output. Iterative refinement allows for gradual improvement based on feedback.
  - Quick check question: How does CGI2 use self-feedback to iteratively refine the meta-review?

- Concept: Checklist-based evaluation
  - Why needed here: Standard evaluation metrics may not capture all aspects of meta-review quality. A checklist provides a more comprehensive and task-specific evaluation framework.
  - Quick check question: What are the four key aspects of meta-review quality assessed by the CGI2 checklist?

## Architecture Onboarding

- Component map: Data Collection -> Dataset Preprocessing -> Task Decomposition -> Checklist-guided Refinement -> Evaluation
- Critical path: Data collection and preprocessing → Task decomposition → Initial meta-review generation → Iterative refinement → Evaluation
- Design tradeoffs: Using black-box LLM provides flexibility but limits control; self-feedback may introduce biases; checklist-based evaluation is comprehensive but potentially subjective
- Failure signatures: Generated meta-reviews contain hallucinations or are inconsistent with input reviews; iterative refinement doesn't lead to improvements
- First 3 experiments:
  1. Evaluate CGI2 performance on small ORSUM subset using standard metrics (ROUGE-L, BERTScore)
  2. Compare CGI2 with other prompting methods (Vanilla, 3Sent, TCG, ICL) on same subset
  3. Analyze generated meta-reviews qualitatively for coherence, comprehensiveness, and checklist adherence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of author rebuttals in the input data affect the quality and comprehensiveness of generated meta-reviews?
- Basis in paper: [explicit] Current dataset does not include author rebuttals; noted as direct extension of current work
- Why unresolved: Authors haven't conducted experiments with author rebuttals included
- What evidence would resolve it: Experiments comparing meta-review generation with and without author rebuttals as input

### Open Question 2
- Question: How well do the proposed evaluation metrics generalize to other domains beyond Machine Learning?
- Basis in paper: [explicit] Dataset mainly focuses on Machine Learning area, which might introduce biases
- Why unresolved: Evaluation framework was developed and tested primarily on ML papers
- What evidence would resolve it: Applying evaluation framework to meta-review datasets from diverse domains

### Open Question 3
- Question: How does CGI2 perform compared to other prompting strategies for complex text generation tasks beyond meta-review generation?
- Basis in paper: [explicit] Authors suggest CGI2 can be applied to other complex text generation tasks
- Why unresolved: Paper only demonstrates effectiveness for meta-review generation
- What evidence would resolve it: Applying CGI2 to other complex text generation tasks and comparing with other prompting strategies

## Limitations
- Evaluation relies heavily on automated metrics and LLM-based assessments, which may not fully capture human judgment nuances
- Study focuses on peer review data from specific conferences, limiting generalizability to other domains or review formats
- Performance improvements are limited to specific metrics and evaluation setup

## Confidence
- High confidence: Task decomposition and iterative refinement framework (well-established methodology with clear implementation)
- Medium confidence: Performance improvements over baselines (limited to specific metrics and evaluation setup)
- Medium confidence: Claims about human meta-review inadequacy (based on automated analysis rather than comprehensive human studies)

## Next Checks
1. Conduct human evaluation study comparing CGI2-generated meta-reviews against human-written ones across multiple conferences and domains
2. Test CGI2's performance on scientific opinion summarization tasks outside peer review context (e.g., product reviews, policy discussions)
3. Analyze CGI2's robustness across different LLM variants and parameter settings to assess sensitivity to model choice