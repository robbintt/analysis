---
ver: rpa2
title: 'Perturbation-based QE: An Explainable, Unsupervised Word-level Quality Estimation
  Method for Blackbox Machine Translation'
arxiv_id: '2305.07457'
source_url: https://arxiv.org/abs/2305.07457
tags:
- translation
- system
- data
- source
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Perturbation-based QE, a simple word-level
  Quality Estimation approach that works by analyzing MT system output on perturbed
  input source sentences. Our approach is unsupervised, explainable, and can evaluate
  any type of blackbox MT systems, including the currently prominent large language
  models (LLMs) with opaque internal processes.
---

# Perturbation-based QE: An Explainable, Unsupervised Word-level Quality Estimation Method for Blackbox Machine Translation

## Quick Facts
- **arXiv ID**: 2305.07457
- **Source URL**: https://arxiv.org/abs/2305.07457
- **Reference count**: 3
- **Primary result**: Perturbation-based QE achieves similar or better performance than zero-shot supervised approaches on WMT21 shared task, with added explainability for blackbox MT systems

## Executive Summary
This paper introduces Perturbation-based QE, an unsupervised word-level quality estimation method for blackbox machine translation systems. The approach works by systematically perturbing source sentences and analyzing how these perturbations affect the MT output, without requiring labeled data or access to MT system internals. The method is particularly effective at detecting gender bias and word-sense-disambiguation errors, and provides explainability by showing which source words influence each output word.

## Method Summary
Perturbation-based QE generates multiple perturbed versions of source sentences by masking and replacing individual words using language models. These perturbed sentences are then translated by the target MT system. The method aligns original and perturbed translations, identifies inconsistent translations, and counts how many source words influence each output token. Words influenced by more than a threshold number of source words are classified as BAD quality, providing both quality predictions and interpretability through source-to-target influence mapping.

## Key Results
- Achieves similar or better MCC scores compared to zero-shot supervised methods on WMT21 shared task
- Shows superior performance on detecting gender bias errors (WinoMT) and word-sense-disambiguation errors (MuCoW)
- Demonstrates better generalizability to nontraditional MT systems like LLMs
- Provides explicit interpretability showing which source words affect each output word

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Word-level QE can be performed without labeled data by using perturbations to detect unreliable translations
- **Mechanism**: Perturbing individual source words and observing how translations change reveals which source words influence output tokens. If too many source words influence a single output token, it indicates the MT system is relying on spurious correlations, flagging that token as unreliable
- **Core assumption**: MT systems that are uncertain about a translation tend to rely on multiple unrelated source words, leading to unreliable output
- **Evidence anchors**: [abstract]: "Our motivation is inspired by a known problem of MT systems: when uncertain, MT systems rely on spurious correlations learnt from the training data in order to generate translation"
- **Break condition**: If perturbations do not meaningfully change the translation output (e.g., due to over-smoothing in the MT model), the method will fail to detect unreliable tokens

### Mechanism 2
- **Claim**: The number of source words influencing an output token can be used as a proxy for translation quality
- **Mechanism**: For each output token, count how many perturbed source words cause changes in its translation. If this count exceeds a threshold, classify the token as BAD; otherwise, classify it as OK
- **Core assumption**: A translation token that depends on many source words is more likely to be erroneous than one depending on few
- **Evidence anchors**: [section 3]: "If an output word hj is influenced by more than t source words (excluding the source word directly translated to hj), then hj is predicted as a BAD translation"
- **Break condition**: If the threshold t is set too low or too high, the method will produce too many false positives or miss actual errors

### Mechanism 3
- **Claim**: Perturbation-based QE is explainable because it shows which source words influence each output token
- **Mechanism**: By tracking which perturbations cause changes in each output token, the method can generate an explicit mapping of source-to-target word influence, providing interpretability
- **Core assumption**: Showing the source words that affect an output token provides meaningful insight into potential translation errors
- **Evidence anchors**: [abstract]: "our QE approach comes with explainability power: it shows which source words affect each output word in the translation"
- **Break condition**: If the alignment between perturbed and original translations is noisy or inaccurate, the influence mapping will be unreliable

## Foundational Learning

- **Concept**: Word-level Quality Estimation (QE)
  - **Why needed here**: The paper focuses on predicting the quality of individual words in machine translation output, which is more granular than sentence-level QE and useful for post-editing
  - **Quick check question**: What is the difference between word-level and sentence-level QE in terms of output granularity?

- **Concept**: Perturbation methods in NLP
  - **Why needed here**: The core innovation relies on systematically perturbing input text to understand model behavior, a technique also used in adversarial testing and interpretability research
  - **Quick check question**: How does masking a source word and generating replacements help understand which words influence translation?

- **Concept**: Influence counting as a quality metric
  - **Why needed here**: The method quantifies translation reliability by counting how many source words affect each output token, which is a novel approach compared to traditional feature-based QE
  - **Quick check question**: Why might an output token influenced by many source words be considered less reliable than one influenced by few?

## Architecture Onboarding

- **Component map**: Perturbation Generator -> Translation Engine -> Alignment Module -> Consistency Evaluator -> Influence Counter -> Quality Predictor

- **Critical path**: Perturbation Generation → Translation → Alignment → Consistency Evaluation → Influence Counting → Quality Prediction

- **Design tradeoffs**:
  - Perturbation scope (content words vs all words vs all tokens): More perturbations increase coverage but also computational cost
  - Number of replacements per perturbation (n=30 in paper): More replacements improve robustness but increase runtime
  - Alignment method choice: Levenshtein is faster but Tercom handles shifts better
  - Threshold selection (c, p, t): Affects precision-recall tradeoff; method claims robustness to hyperparameter choices

- **Failure signatures**:
  - Low MCC scores despite high perturbation coverage: Alignment errors or threshold misconfiguration
  - High computational cost with diminishing returns: Too many perturbations or replacements
  - Inconsistent influence counts across similar sentences: Noise in translation or alignment

- **First 3 experiments**:
  1. Run on a small bilingual corpus with known errors to verify perturbation detection works
  2. Test different perturbation scopes (content words vs all words) on the same data to measure impact
  3. Compare alignment methods (Levenshtein vs Tercom) on a sample to check alignment quality

## Open Questions the Paper Calls Out

- **Open Question 1**: How does Perturbation-based QE compare to supervised QE methods on detecting other types of translation errors beyond gender bias and word-sense-disambiguation, such as tense errors, singular/plural forms, or idiomatic expressions?
- **Open Question 2**: Can Perturbation-based QE be effectively adapted for use in other natural language processing tasks, such as question answering or summarization, by minimally perturbing the input and analyzing changes in the output?
- **Open Question 3**: How does the performance of Perturbation-based QE vary with different language pairs, especially those with significantly different linguistic structures, such as languages with agglutinative morphology or logographic writing systems?

## Limitations

- The method's effectiveness depends on the assumption that MT systems produce inconsistent translations when influenced by multiple spurious source words, which may not hold for all MT architectures
- Computational cost of generating multiple perturbed versions may limit scalability for real-world deployment
- Alignment quality between original and perturbed translations is critical but not extensively validated in the paper

## Confidence

- **High Confidence**: The method's core explainability mechanism (showing which source words influence each output token) is well-demonstrated and technically sound
- **Medium Confidence**: The unsupervised quality estimation performance claims are supported by WMT21 results, but the comparison with supervised baselines could be more comprehensive
- **Medium Confidence**: The generalizability claims to different MT systems (especially LLMs) are demonstrated but with limited scope - more diverse MT architectures should be tested

## Next Checks

1. **Alignment Quality Validation**: Implement detailed alignment accuracy metrics (e.g., alignment error rate) and visualize sample alignments between original and perturbed translations to quantify the impact of alignment errors on QE performance

2. **Cross-Architecture Robustness Test**: Apply perturbation-based QE to a broader range of MT architectures including non-transformer models and smaller language pairs to test generalizability claims beyond the reported experiments

3. **Computational Efficiency Analysis**: Measure the wall-clock time and memory usage for perturbation generation and translation across different scales (sentence length, number of perturbations) to establish practical deployment limits