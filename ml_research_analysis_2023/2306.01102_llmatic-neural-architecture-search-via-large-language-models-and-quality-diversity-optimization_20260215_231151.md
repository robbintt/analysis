---
ver: rpa2
title: 'LLMatic: Neural Architecture Search via Large Language Models and Quality
  Diversity Optimization'
arxiv_id: '2306.01102'
source_url: https://arxiv.org/abs/2306.01102
tags:
- self
- network
- prompt
- archive
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LLMatic addresses the problem of neural architecture search (NAS)
  by leveraging large language models (LLMs) for generating network variations combined
  with quality-diversity (QD) optimization to maintain a diverse set of high-performing
  architectures. The core method uses two cooperative QD archives: one for neural
  networks (indexed by depth-to-width ratio and FLOPS) and one for prompts with temperatures
  (for LLM generation).'
---

# LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization

## Quick Facts
- arXiv ID: 2306.01102
- Source URL: https://arxiv.org/abs/2306.01102
- Authors: Not specified in input
- Reference count: 40
- Primary result: LLMatic achieves competitive results on CIFAR-10 without prior domain knowledge by combining LLM-generated code with quality-diversity optimization

## Executive Summary
LLMatic is a neural architecture search method that leverages large language models (LLMs) to generate network variations, combined with quality-diversity (QD) optimization to maintain a diverse set of high-performing architectures. The approach uses two cooperative QD archives—one for neural networks and one for prompts with temperatures—to balance exploration and exploitation while avoiding domain-specific prior knowledge. Experiments on CIFAR-10 demonstrate that LLMatic finds competitive networks while maintaining architectural diversity across 2,000 searches.

## Method Summary
LLMatic uses a two-archive QD system where one archive stores neural networks (indexed by depth-to-width ratio and FLOPS) and another stores prompts with temperatures for LLM generation. The method starts with a simple network and generates new candidates through mutation or crossover operations guided by LLM-generated code. Network fitness is measured by training performance, while prompt fitness is determined by curiosity scores based on whether generated networks are trainable and added to the network archive. Temperature is adjusted dynamically—increased for exploration when fitness improves, decreased for exploitation when fitness worsens.

## Key Results
- Achieves better loss than ablation variants that remove QD optimization or temperature control
- Fills more niches in the archives, demonstrating better diversity maintenance
- Finds competitive networks without prior knowledge of the benchmark domain or exposure to previous top-performing models
- Demonstrates effectiveness on CIFAR-10 with 2,000 searches across 20 generations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMatic combines LLM code generation with QD optimization to produce diverse, high-performing architectures without domain-specific prior knowledge.
- Mechanism: The two-archive QD system maintains diversity while optimizing fitness through network loss (for networks) and curiosity score (for prompts).
- Core assumption: LLMs can generate meaningful network variations when guided by prompts, and QD optimization can maintain diversity while finding high-performing solutions.
- Evidence anchors: Abstract states merging code-generating abilities of LLMs with QD solutions; section describes procedural approach leveraging QD for prompts and architecture; related work provides weak evidence.
- Break condition: If LLMs fail to generate meaningful network variations or QD optimization cannot maintain diversity while finding high-performing solutions.

### Mechanism 2
- Claim: Temperature-controlled LLM generation allows balancing exploration and exploitation in architecture search.
- Mechanism: Temperature is increased when fitness improves (exploration) and decreased when fitness worsens (exploitation).
- Core assumption: Temperature adjustment in LLM generation meaningfully influences the quality and diversity of generated architectures.
- Evidence anchors: Section explains temperature increase for exploration and decrease for exploitation; no direct evidence found for temperature-controlled LLM generation in NAS.
- Break condition: If temperature adjustment does not lead to meaningful changes in architecture quality or diversity.

### Mechanism 3
- Claim: The two-archive cooperative QD optimization allows LLMatic to maintain both network diversity and prompt effectiveness.
- Mechanism: Network archive stores diverse architectures while prompt archive stores effective prompts, with crossover and mutation operators generating new candidates from both archives.
- Core assumption: Maintaining separate archives for networks and prompts, with cooperative evolution, improves search efficiency and solution quality.
- Evidence anchors: Section describes procedural approach leveraging QD for prompts and network architecture; related work provides weak evidence.
- Break condition: If cooperative evolution between network and prompt archives does not lead to improved search performance.

## Foundational Learning

- Concept: Quality-Diversity (QD) optimization
  - Why needed here: QD optimization is crucial for maintaining diversity in generated architectures while optimizing for performance, allowing LLMatic to find a range of high-performing networks rather than a single optimal architecture.
  - Quick check question: What is the main difference between QD optimization and traditional optimization methods in neural architecture search?

- Concept: Large Language Models (LLMs) for code generation
  - Why needed here: LLMs are used to generate meaningful variations in neural network architectures, providing a source of domain knowledge that can guide the search process without requiring prior knowledge of the specific benchmark domain.
  - Quick check question: How do LLMs contribute to the neural architecture search process in LLMatic?

- Concept: Behavioral descriptors in QD optimization
  - Why needed here: Behavioral descriptors (depth-to-width ratio and FLOPS for networks; encoded prompt and temperature for prompts) are used to index the archives and maintain diversity in the search space.
  - Quick check question: What are the behavioral descriptors used for the network and prompt archives in LLMatic, and why were these specific descriptors chosen?

## Architecture Onboarding

- Component map: LLM code generation with temperature control -> Two-archive QD optimization (network archive and prompt archive) -> Mutation and crossover operators -> Training pipeline for generated networks -> Fitness evaluation (network loss and curiosity score)

- Critical path: 1. Initialize with simple network and random prompts 2. Generate initial batch of networks using LLM 3. Train networks and add to archives 4. Perform evolutionary operations (mutation/crossover) based on archive contents 5. Repeat steps 2-4 for specified number of generations

- Design tradeoffs: Using a smaller LLM (6.1B parameters) vs. larger models for computational efficiency; balancing exploration and exploitation through temperature control; maintaining diversity vs. focusing on high-performing architectures

- Failure signatures: Network archive fails to fill with diverse architectures; prompt archive becomes stagnant with low curiosity scores; generated networks are consistently untrainable or perform poorly; temperature control leads to oscillations rather than stable improvements

- First 3 experiments: 1. Run LLMatic with default settings on CIFAR-10 to establish baseline performance 2. Modify temperature control parameters to observe effects on exploration vs. exploitation 3. Replace QD optimization with random search to quantify the contribution of the QD component

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLMatic's performance scale with larger language models beyond the 6.1B parameter CodeGen variant used in the experiments?
- Basis in paper: Authors explicitly state desire to test larger models and identify this as a constraint in current work.
- Why unresolved: Experiments were computationally constrained to CodeGen-6.1B, with authors acknowledging this limitation without testing larger models.
- What evidence would resolve it: Running LLMatic with larger language models (e.g., 175B+ parameter models) on CIFAR-10 benchmark and comparing performance metrics.

### Open Question 2
- Question: What is the optimal balance between mutation and crossover operators for different neural architecture search tasks and domains?
- Basis in paper: Authors conducted experiments varying mutation and crossover probabilities (0.3/0.7, 0.5/0.5, 0.7/0.3) and found mutation should be given more probability, but this was specifically for CIFAR-10.
- Why unresolved: Experiments only tested one dataset and one set of architectural characteristics; different tasks or domains might benefit from different operator ratios.
- What evidence would resolve it: Systematic experiments across multiple datasets (different image classification tasks, NLP tasks) and benchmarks.

### Open Question 3
- Question: How does LLMatic compare to traditional reinforcement learning and evolutionary computation approaches to NAS in terms of sample efficiency and final architecture quality?
- Basis in paper: Authors state LLMatic should be compared to other NAS methods on other computer vision and natural language processing tasks, and note comparison to RL benchmarks is needed.
- Why unresolved: Paper only compares LLMatic to ablation variants of itself and does not benchmark against standard RL or evolutionary NAS approaches.
- What evidence would resolve it: Direct comparison of LLMatic against state-of-the-art RL-based NAS (like DARTS) and evolutionary NAS methods on identical tasks.

## Limitations
- Limited to single benchmark (CIFAR-10) with relatively small search budget (2,000 searches)
- Uses 6.1B parameter LLM due to computational constraints, potentially limiting performance
- Temperature-controlled LLM generation mechanism lacks direct supporting evidence from related work

## Confidence
- Medium confidence due to lack of detailed implementation specifications for critical components and single benchmark evaluation

## Next Checks
1. Test on multiple benchmark datasets (CIFAR-100, ImageNet) to assess domain generalizability
2. Compare against traditional NAS methods like DARTS and random search with equal computational budgets
3. Conduct ablation studies to isolate contributions of temperature control, two-archive QD system, and LLM component