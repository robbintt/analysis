---
ver: rpa2
title: Why Does Sharpness-Aware Minimization Generalize Better Than SGD?
arxiv_id: '2310.07269'
source_url: https://arxiv.org/abs/2310.07269
tags:
- have
- inequality
- lemma
- where
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical analysis of Sharpness-Aware Minimization
  (SAM) and its advantages over Stochastic Gradient Descent (SGD) in training two-layer
  convolutional ReLU networks. The authors prove that SAM can prevent early-stage
  noise memorization, leading to better generalization, especially in high-dimensional
  settings where SGD fails.
---

# Why Does Sharpness-Aware Minimization Generalize Better Than SGD?

## Quick Facts
- **arXiv ID**: 2310.07269
- **Source URL**: https://arxiv.org/abs/2310.07269
- **Reference count**: 40
- **Primary result**: SAM achieves benign overfitting under much milder conditions than SGD, which requires signal strength to scale with data dimension.

## Executive Summary
This paper provides a theoretical analysis of Sharpness-Aware Minimization (SAM) and its advantages over Stochastic Gradient Descent (SGD) in training two-layer convolutional ReLU networks. The authors prove that SAM can prevent early-stage noise memorization, leading to better generalization, especially in high-dimensional settings where SGD fails. The key result is a phase transition between benign and harmful overfitting: SAM achieves benign overfitting under much milder conditions than SGD, which requires the signal strength to scale with data dimension. The analysis shows that SAM's perturbation mechanism deactivates neurons that would otherwise memorize noise, enabling more effective feature learning.

## Method Summary
The paper analyzes two-layer convolutional ReLU networks trained on synthetic data following a sparse coding model with signal patches and noise patches. The method compares mini-batch SGD with Sharpness-Aware Minimization (SAM) using cross-entropy loss. SAM introduces an adversarial perturbation during training that deactivates neurons that would otherwise memorize noise. The analysis tracks signal coefficients γ(t,b) and noise coefficients ρ(t,b) throughout training to characterize learning progress and demonstrate how SAM prevents noise memorization while SGD doesn't.

## Key Results
- SAM achieves benign overfitting when signal strength ∥μ∥2 ≥ eΩ(1), while SGD requires ∥μ∥2 ≥ Ω(d1/4)
- The perturbation mechanism in SAM deactivates neurons that would otherwise memorize noise, preventing early-stage noise memorization
- SAM creates a phase transition between benign and harmful overfitting that occurs at much lower signal strengths than required for SGD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAM prevents noise memorization in early training stages by deactivating neurons that would otherwise learn noise.
- Mechanism: The adversarial perturbation in SAM changes activation patterns. When a neuron is activated by SGD, SAM's perturbation deactivates it for noise features while preserving signal learning.
- Core assumption: The perturbation magnitude τ is appropriately sized to shift activations away from noise features but not signal features.
- Evidence anchors:
  - [abstract] "SAM's perturbation mechanism deactivates neurons that would otherwise memorize noise"
  - [section 4.1] "SAM introduces a perturbation during the model parameter update process, which effectively prevents the early-stage memorization of noise by deactivating the corresponding neurons"
- Break condition: If τ is too small, SAM behaves like SGD; if too large, it disrupts signal learning.

### Mechanism 2
- Claim: SAM enables benign overfitting under much milder conditions than SGD by maintaining better feature learning.
- Mechanism: SAM's perturbation prevents early-stage noise learning, allowing the model to focus on signal features. This creates a phase transition where SAM achieves benign overfitting at signal strengths ∥μ∥2 ≥ eΩ(1) while SGD requires ∥μ∥2 ≥ Ω(d1/4).
- Core assumption: The perturbation effectively discriminates between signal and noise features based on their activation patterns.
- Evidence anchors:
  - [abstract] "SAM achieves benign overfitting under much milder conditions than SGD, which requires the signal strength to scale with data dimension"
  - [section 3] "When the condition n∥μ∥4 ≥ C3dP 4σ4p holds, the test error fails to approach the Bayes risk"
- Break condition: If noise features are too similar to signal features, SAM's perturbation may not effectively discriminate.

### Mechanism 3
- Claim: SAM's perturbation creates a more balanced learning dynamic between signal and noise features.
- Mechanism: By preventing neurons from memorizing noise early on, SAM ensures that signal coefficients γ(t,b) grow faster relative to noise coefficients ρ(t,b), maintaining a favorable signal-to-noise ratio throughout training.
- Core assumption: The initial stage of training is critical for determining which features dominate learning.
- Evidence anchors:
  - [section 4.1] "This perturbation to the weight update process at each iteration gives SAM an intriguing denoising property"
  - [section 3.1] "Each mini-batch update utilizes only a portion of the samples, meaning that some ρ(t,b)yi,r,i can increase or decrease much more than the others"
- Break condition: If the signal-to-noise ratio is extremely unfavorable, even SAM may struggle to maintain balanced learning.

## Foundational Learning

- Concept: Phase transition between benign and harmful overfitting
  - Why needed here: The paper demonstrates a sharp transition where SGD requires signal strength to scale with dimension d, while SAM achieves benign overfitting at much lower signal strengths. Understanding this phase transition is crucial for interpreting the theoretical results.
  - Quick check question: What condition must hold for SGD to achieve benign overfitting versus harmful overfitting?

- Concept: Signal-noise decomposition in neural networks
  - Why needed here: The analysis relies on decomposing network weights into signal and noise components to track learning progress. This decomposition is essential for understanding how SAM prevents noise memorization while SGD doesn't.
  - Quick check question: How does the signal-noise decomposition help characterize the learning progress of signal μ versus noise?

- Concept: Adversarial perturbation in optimization
  - Why needed here: SAM uses adversarial perturbation to find flatter minima. Understanding how this perturbation affects activation patterns is key to explaining why SAM prevents noise memorization while SGD doesn't.
  - Quick check question: How does the adversarial perturbation in SAM differ from standard gradient descent updates?

## Architecture Onboarding

- Component map: Data distribution -> Two-layer convolutional ReLU networks -> Training algorithms (SGD vs SAM) -> Analysis framework (signal-noise decomposition)
- Critical path:
  1. Initialize weights with Gaussian distribution
  2. For each batch, compute standard gradient (SGD) or adversarial gradient (SAM)
  3. Update weights using computed gradients
  4. Track signal coefficients γ(t,b) and noise coefficients ρ(t,b)
  5. Monitor training loss and test error throughout training
- Design tradeoffs:
  - SAM perturbation magnitude τ: Larger τ better prevents noise memorization but may disrupt signal learning
  - Mini-batch size B: Larger batches provide more stable gradients but reduce computational efficiency
  - Learning rate η: Must be small enough for stable convergence but large enough for efficient learning
- Failure signatures:
  - SGD fails: Test error doesn't approach Bayes risk when signal strength is low relative to dimension
  - SAM fails: If perturbation is too large or signal-to-noise ratio is extremely unfavorable
  - Both fail: If training loss doesn't converge within specified iterations
- First 3 experiments:
  1. Compare SGD vs SAM on synthetic data with varying signal strength and dimension to verify phase transition
  2. Measure coefficient growth rates γ(t,b) vs ρ(t,b) for both algorithms to verify noise memorization prevention
  3. Test SAM with different perturbation magnitudes τ to find optimal balance between noise prevention and signal learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SAM's perturbation mechanism scale with data dimension and signal strength compared to SGD?
- Basis in paper: [explicit] The paper proves SAM achieves benign overfitting under much milder conditions than SGD, which requires signal strength to scale with data dimension
- Why unresolved: The theoretical analysis focuses on specific phase transitions but doesn't provide a complete scaling relationship between perturbation radius, data dimension, and signal strength
- What evidence would resolve it: Empirical studies varying both dimension and signal strength while measuring required perturbation radius for SAM to achieve benign overfitting

### Open Question 2
- Question: Can the noise memorization prevention property of SAM be extended to deeper neural networks beyond two-layer CNNs?
- Basis in paper: [explicit] The paper analyzes SAM's denoising property for two-layer convolutional ReLU networks but doesn't extend to deeper architectures
- Why unresolved: The analysis relies heavily on specific properties of two-layer networks that may not generalize to deeper networks with more complex architectures
- What evidence would resolve it: Theoretical analysis or empirical experiments demonstrating SAM's effectiveness in preventing noise memorization in deeper networks

### Open Question 3
- Question: What is the optimal learning rate schedule for SAM to maximize its noise prevention benefits?
- Basis in paper: [inferred] The paper uses fixed learning rates in analysis and experiments, but mentions that larger learning rates can improve SGD's generalization
- Why unresolved: The theoretical analysis doesn't explore how learning rate schedules interact with SAM's perturbation mechanism
- What evidence would resolve it: Systematic experiments comparing different learning rate schedules (warmup, cosine decay, etc.) with SAM across various noise levels and architectures

## Limitations
- The theoretical analysis is limited to two-layer convolutional ReLU networks with specific data generation processes
- The analysis relies on idealized assumptions about data distribution (sparse coding model) that may not hold in practical scenarios
- The perturbation mechanism's effectiveness depends critically on proper tuning of τ, and the paper does not provide comprehensive guidance on parameter selection for different problem settings

## Confidence
- **High**: The mathematical framework and proofs are rigorous and internally consistent within the defined theoretical setting
- **Medium**: The phase transition claims between benign and harmful overfitting are well-supported theoretically but may be sensitive to the specific data generation assumptions
- **Low**: The claim that SAM prevents noise memorization through neuron deactivation is based on theoretical arguments but lacks empirical validation or broader corpus support

## Next Checks
1. **Empirical Validation**: Test the phase transition phenomenon on real-world datasets (e.g., CIFAR-10) to verify if SAM maintains its advantages outside the theoretical sparse coding model.

2. **Perturbation Sensitivity Analysis**: Systematically vary the perturbation magnitude τ across multiple orders of magnitude to identify the optimal range and understand failure modes when τ is too large or too small.

3. **Generalization to Other Architectures**: Extend the analysis to deeper convolutional networks and fully-connected networks to assess whether the noise memorization prevention mechanism generalizes beyond two-layer networks.