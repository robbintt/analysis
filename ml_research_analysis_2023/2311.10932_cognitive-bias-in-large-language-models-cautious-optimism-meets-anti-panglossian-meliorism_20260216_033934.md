---
ver: rpa2
title: 'Cognitive bias in large language models: Cautious optimism meets anti-Panglossian
  meliorism'
arxiv_id: '2311.10932'
source_url: https://arxiv.org/abs/2311.10932
tags:
- biases
- bias
- language
- cognitive
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper evaluates claims of cognitive biases in large language
  models (LLMs), distinguishing between biases in reasoning (cognitive bias) and biases
  in fairness. It examines several alleged biases: knowledge effects, availability
  bias, anchoring bias, and framing effects.'
---

# Cognitive bias in large language models: Cautious optimism meets anti-Panglossian meliorism

## Quick Facts
- arXiv ID: 2311.10932
- Source URL: https://arxiv.org/abs/2311.10932
- Reference count: 19
- Large language models show some cognitive biases, but framing effects are identified as a genuine concern warranting mitigation

## Executive Summary
This paper examines cognitive biases in large language models (LLMs) by testing them on classic reasoning tasks where human biases are well-documented. The study finds that while LLMs exhibit some biases, many of these may be desirable features rather than bugs. Knowledge effects and availability bias appear to reflect rational probabilistic reasoning rather than problematic shortcuts. Anchoring effects, while present, may represent justified incorporation of relevant information. However, framing effects emerge as a genuine concern where superficial changes in problem presentation lead to substantive changes in reasoning outcomes. The paper advocates for cautious optimism about LLM performance on cognitive biases while emphasizing the need to address framing effects through better training and architectural approaches.

## Method Summary
The study evaluates LLM cognitive biases using a combination of established psychological tasks and custom-designed experiments. Key methods include testing models on Wason selection tasks to measure knowledge effects, training models on Drug-Drug Interaction datasets to examine availability bias, and using HumanEval datasets to test anchoring effects. The researchers also employed classic framing problems like the Asian disease problem to assess sensitivity to problem presentation. Models were evaluated based on accuracy rates, classification patterns, and sensitivity to irrelevant framing changes across multiple reasoning domains.

## Key Results
- Knowledge effects in LLMs reflect rational probabilistic reasoning rather than problematic bias
- Availability bias appears to be a desirable feature that helps models prioritize important information
- Anchoring bias may represent justified incorporation of relevant information rather than irrational influence
- Framing effects are identified as a genuine and undesirable bias where superficial wording changes lead to different reasoning outcomes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models exhibit knowledge effects because they learn probabilistic dependencies between concepts during training.
- Mechanism: Models trained on large corpora develop conditional probability distributions over concepts. When reasoning about conditional rules, they naturally compute P(consequent|antecedent) using these learned distributions rather than applying formal logical rules.
- Core assumption: The model's internal representations capture statistical regularities in the training data that correspond to real-world probabilistic relationships.
- Evidence anchors:
  - [abstract] "Many studies find biases which have standard rationalizing explanations when produced by humans"
  - [section 3.3] "Large language models often retain the ability to reason either logically or probabilistically"
  - [corpus] Weak - no direct evidence about knowledge effects from related papers
- Break condition: The model fails to learn meaningful probabilistic relationships from training data, or the training data lacks sufficient examples of conditional relationships to establish reliable probability estimates.

### Mechanism 2
- Claim: Framing effects emerge because language models treat linguistic surface features as evidence for underlying conceptual differences.
- Mechanism: When presented with positively vs negatively framed versions of the same problem, the model processes the different surface forms as distinct inputs. It then generates outputs based on the entire input string, including framing cues, because these cues correlate with different outcomes in the training data.
- Core assumption: The training corpus contains examples where framing correlates with different outcomes, and the model learns to treat framing as predictive.
- Evidence anchors:
  - [section 6] "Framing effects occur when irrelevant changes in the framing of a reasoning problem lead to substantive changes in the judgments that result"
  - [abstract] "Framing effects are identified as a genuine and undesirable bias in LLMs"
  - [corpus] Weak - no direct evidence about framing effects from related papers
- Break condition: The model learns to ignore surface linguistic features and focus only on underlying conceptual content, or the training data contains insufficient examples of framing influencing outcomes.

### Mechanism 3
- Claim: Anchoring effects arise because language models treat all input information as relevant evidence for predicting continuations.
- Mechanism: When given prompts containing anchor information (e.g., partial solutions), the model updates its probability distribution over possible continuations to give higher weight to solutions that incorporate the anchor information. This occurs because the anchor is both relevant and justifiably presumed relevant based on training.
- Core assumption: The model's architecture and training procedure lead it to treat all input features as potentially relevant to the prediction task.
- Evidence anchors:
  - [section 5.3] "The anchor information is arguably both relevant, and justifiably presumed to be relevant, to the problem"
  - [abstract] "knowledge effects, availability bias, and anchoring bias are either desirable features of reasoning or not clearly problematic"
  - [corpus] Weak - no direct evidence about anchoring from related papers
- Break condition: The model develops explicit mechanisms to filter out irrelevant input information, or training data includes sufficient examples of manipulative anchors to teach the model to be skeptical of all anchors.

## Foundational Learning

- Concept: Bayesian inference and probabilistic reasoning
  - Why needed here: Understanding how models might learn and apply probabilistic relationships rather than logical rules is crucial for interpreting knowledge effects and availability bias
  - Quick check question: If a model learns that P(rain|cloudy) = 0.8 from training data, how should it update its beliefs when presented with new evidence of cloudy conditions?

- Concept: Ecological rationality and environmental context
  - Why needed here: The paper argues that biases should be evaluated based on how well they perform in the environments where models are actually used, not in all possible environments
  - Quick check question: Why might a bias that seems irrational in laboratory settings be rational or even desirable in real-world deployment scenarios?

- Concept: Cognitive architecture differences between humans and AI
  - Why needed here: The paper emphasizes that similar biases in humans and LLMs may arise from different mechanisms and have different normative implications
  - Quick check question: What key architectural differences between human cognition and LLMs might lead to similar biases emerging through different causal pathways?

## Architecture Onboarding

- Component map:
  Input processing layer -> Tokenization -> Embedding -> Attention mechanism -> Prediction head -> Output generation

- Critical path:
  1. Prompt → tokenization → embedding
  2. Context encoding via attention layers
  3. Probability distribution over next tokens
  4. Sampling/greedy decoding to generate output

- Design tradeoffs:
  - Size vs. efficiency: Larger models capture more patterns but are more resource-intensive
  - Training data diversity vs. focus: Broad data captures more scenarios but may introduce noise
  - Inference speed vs. accuracy: More sophisticated decoding increases quality but reduces speed

- Failure signatures:
  - Knowledge effects: Model consistently favors conclusions with higher prior probability regardless of logical validity
  - Framing effects: Model outputs change significantly with superficial wording changes
  - Anchoring effects: Model outputs incorporate irrelevant anchor information

- First 3 experiments:
  1. Test Wason selection task variants to measure knowledge effects across different prior probabilities
  2. Run framing effect experiments using classic problems like Asian disease to measure sensitivity to surface features
  3. Test anchoring effects using code generation tasks with varying anchor relevance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do large language models show framing effects across a wider range of reasoning tasks beyond the Asian disease problem and gain/loss framing scenarios?
- Basis in paper: [explicit] The paper identifies framing effects as a genuine bias warranting mitigation efforts, citing Talboy and Fuller (2023) and Binz and Schulz (2023) who found framing effects in specific tasks.
- Why unresolved: The evidence is limited to a few specific framing scenarios. It's unclear if framing effects generalize across diverse reasoning domains.
- What evidence would resolve it: Systematic testing of LLMs on a broader range of framing scenarios across different reasoning domains (e.g., moral reasoning, logical inference, numerical estimation) would clarify the prevalence and scope of framing effects.

### Open Question 2
- Question: Are cognitive biases in LLMs reducible to unrepresentative training data, or do they emerge from fundamental architectural or algorithmic features?
- Basis in paper: [inferred] The paper contrasts traditional algorithmic bias (linked to unrepresentative data) with cognitive biases in LLMs, noting that cognitive biases are not attributed to unrepresentative data in humans and may persist across model architectures.
- Why unresolved: The paper raises the question but doesn't provide a definitive answer. It's unclear whether cognitive biases are artifacts of training data or emergent properties of LLM architectures.
- What evidence would resolve it: Comparative studies of cognitive biases across LLMs trained on different datasets, as well as models with varying architectures, would help determine the role of data versus architecture in bias emergence.

### Open Question 3
- Question: How do cognitive biases in LLMs compare to human biases in terms of both prevalence and severity across different reasoning tasks?
- Basis in paper: [explicit] The paper discusses how some biases in LLMs (e.g., knowledge effects, availability bias) may have rationalizing explanations similar to those proposed for human biases, and how the emergence of similar biases across different architectures might support vindicatory explanations of human biases.
- Why unresolved: While the paper identifies some similarities between LLM and human biases, it doesn't provide a comprehensive comparison of their prevalence and severity across a wide range of tasks.
- What evidence would resolve it: Systematic comparison of LLM and human performance on standardized cognitive bias tasks, measuring both the frequency and magnitude of biases, would clarify the similarities and differences between human and machine reasoning.

## Limitations

- The study's conclusions about which biases are desirable versus problematic rely heavily on philosophical arguments about ecological rationality that may not hold in all deployment scenarios
- The mechanistic explanations for how LLMs develop biases differ from human cognitive processes, making direct comparisons potentially misleading
- Findings are based on specific model architectures and training procedures that may not generalize to other LLM variants or future developments

## Confidence

- Knowledge effects and availability bias are desirable features: Medium Confidence
- Anchoring bias is not clearly problematic: Low-Medium Confidence
- Framing effects represent genuine, undesirable bias: High Confidence
- Cautious optimism about LLM cognitive biases: Medium Confidence

## Next Checks

1. **Cross-Model Validation**: Test the same bias experiments across multiple LLM architectures (GPT, Claude, LLaMA variants) to determine whether findings are architecture-specific or general properties of LLMs.

2. **Ecological Deployment Study**: Conduct a longitudinal study of LLM performance in actual deployment scenarios where framing effects and knowledge effects occur, measuring real-world impact rather than laboratory performance.

3. **Mechanistic Investigation**: Use interpretability tools to examine internal representations during biased reasoning tasks, distinguishing between surface-level pattern matching and deeper conceptual understanding of the problems being solved.