---
ver: rpa2
title: 'CompoundPiece: Evaluating and Improving Decompounding Performance of Language
  Models'
arxiv_id: '2305.14214'
source_url: https://arxiv.org/abs/2305.14214
tags:
- compound
- words
- segmentation
- language
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a comprehensive study of compound word segmentation
  and normalization across 56 languages. It addresses the lack of multilingual datasets
  by creating a 255k-word Wiktionary-based corpus annotated with compound and non-compound
  examples.
---

# CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models

## Quick Facts
- arXiv ID: 2305.14214
- Source URL: https://arxiv.org/abs/2305.14214
- Reference count: 17
- Key outcome: CompoundPiece tokenizer improves decompounding performance by 5.5% over SentencePiece baselines

## Executive Summary
This work addresses the challenge of compound word segmentation and normalization across 56 languages, where current large language models struggle particularly with "hard" compounds whose subword boundaries do not align with constituent boundaries. The authors create a comprehensive 255k-word Wiktionary-based corpus to enable systematic evaluation of decompounding performance. They introduce a two-stage self-supervised training framework using byte-level ByT5 models that first predicts hyphenation patterns, then fine-tunes on annotated data. Additionally, they develop CompoundPiece, a tokenizer that leverages compound segmentation during training to reduce hard compounds, achieving 5.5% improvement over standard SentencePiece tokenizers.

## Method Summary
The method employs a two-stage self-supervised training framework using byte-level ByT5 models. Stage 1 trains on 25M hyphenated words from mC4 corpus, predicting hyphenation patterns through a masking objective to learn compound boundaries without requiring labeled data. Stage 2 fine-tunes the Stage 1 model on the 255k-word Wiktionary corpus annotated with compound and non-compound examples across 56 languages. The authors also propose CompoundPiece, which modifies pretokenization by applying compound segmentation before SentencePiece tokenization, reducing hard compounds from 27.1% to 9.7% on average in monolingual cases.

## Key Results
- Stage 1 self-supervised models outperform prior unsupervised decompounding methods by 13.9% accuracy
- Fine-tuned models exceed all previous language-specific decompounding tools
- CompoundPiece reduces hard compounds from 27.1% to 9.7% on average in monolingual cases
- CompoundPiece improves overall model performance by 5.5% over standard SentencePiece tokenizers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised hyphenation prediction enables effective compound segmentation without annotated data
- Mechanism: Hyphens act as high-precision, low-recall indicators of compound boundaries. By masking hyphens and training a model to restore them, the model learns to recognize constituent boundaries
- Core assumption: Hyphenated forms in text predominantly indicate compound boundaries rather than other uses like line breaks
- Evidence anchors:
  - [abstract]: "The proposed two-stage procedure relies on a fully self-supervised objective in the first stage"
  - [section 3.2]: "We use this natural segmentation into compound constituents to create a compound segmentation model without requiring any labeled data."
  - [corpus]: Evidence shows hyphens are "high-precision, low-recall indicator of compound constituent boundaries" in Web corpus data
- Break condition: If hyphen usage in the target language/text domain is primarily for purposes other than compound boundaries (e.g., en-dashes, line breaks), model performance degrades

### Mechanism 2
- Claim: Byte-level tokenization eliminates hard compounds by avoiding subword boundary mismatches
- Mechanism: ByT5 tokenizes at the byte level, ensuring that no subword token boundary can cut across a compound constituent boundary
- Core assumption: Compound constituents are always composed of contiguous bytes that can be segmented independently
- Evidence anchors:
  - [abstract]: "Since it directly ingests Unicode bytes instead of using subword tokenization, leading to zero hard compounds"
  - [section 4.2]: "Our Stage 1 models outperform the prior best unsupervised decompounding models by 13.9% accuracy on average"
- Break condition: If compound constituents include multi-byte characters where constituent boundaries fall within multi-byte sequences

### Mechanism 3
- Claim: CompoundPiece tokenizer reduces hard compounds during training, improving downstream decompounding performance
- Mechanism: Pretokenization with compound segmentation before applying SentencePiece ensures more token boundaries align with compound constituent boundaries
- Core assumption: Aligning token boundaries with compound constituents during tokenizer creation improves both tokenization quality and subsequent decompounding accuracy
- Evidence anchors:
  - [abstract]: "CompoundPiece tokenizes compound words more favorably on average, leading to improved performance on decompounding"
  - [section 3.4]: "We propose modifying pretokenization by applying compound segmentation in addition to splitting on whitespace"
  - [corpus]: "CompoundPiece reduces the number of hard compounds from 27.1% → 9.7% on average in the monolingual case"
- Break condition: If compound segmentation accuracy during pretokenization is low, the benefit diminishes or reverses

## Foundational Learning

- Concept: Sequence-to-sequence learning for text generation tasks
  - Why needed here: The decompounding task is formulated as predicting normalized constituents (or hyphenated forms) from compound words
  - Quick check question: What is the difference between sequence-to-sequence and sequence labeling approaches in NLP?

- Concept: Byte-level tokenization vs. subword tokenization
  - Why needed here: Understanding why ByT5 avoids hard compounds while subword models like mT5 and T5 do not
  - Quick check question: How does the vocabulary size and granularity differ between byte-level and subword tokenizers?

- Concept: Edit distance optimization for segmentation
  - Why needed here: The method to convert compound normalization predictions into segmentation using minimal edit distance
  - Quick check question: What is the computational complexity of finding optimal segmentation via edit distance, and how does the proposed algorithm improve efficiency?

## Architecture Onboarding

- Component map:
  - Data pipeline: Wiktionary scraping → dataset creation → split into train/validation
  - Model training: ByT5/Flan-T5/mT5 → Stage 1 (self-supervised) → Stage 2 (supervised fine-tuning)
  - Tokenizer creation: SentencePiece/CompoundPiece with pretokenization
  - Evaluation: Accuracy metrics on compound/non-compound words across languages

- Critical path:
  1. Data collection and preprocessing
  2. Stage 1 training on self-supervised objective
  3. Stage 2 fine-tuning on annotated data
  4. Tokenizer creation with CompoundPiece
  5. Evaluation on held-out test sets

- Design tradeoffs:
  - Byte-level vs. subword: ByT5 avoids hard compounds but has larger sequence lengths; subword models are more efficient but suffer from hard compounds
  - Multilingual vs. monolingual tokenizers: Multilingual covers more languages but increases hard compounds due to token interference
  - Stage 1 only vs. Stage 1 + Stage 2: Stage 1 alone works without annotations but Stage 2 improves accuracy significantly

- Failure signatures:
  - Poor performance on negatives indicates over-segmentation
  - Low accuracy on hard compounds indicates tokenizer boundary issues
  - Stage 1 performance close to random suggests hyphen filtering failed

- First 3 experiments:
  1. Train ByT5 with Stage 1 only on English and evaluate on validation set to establish baseline
  2. Apply Stage 2 fine-tuning and compare accuracy improvement
  3. Create CompoundPiece tokenizer for English and measure reduction in hard compounds vs. SentencePiece

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do larger language models trained with CompoundPiece tokenization compare to those using SentencePiece on downstream tasks beyond decompounding?
- Basis in paper: [inferred] The paper demonstrates CompoundPiece improves decompounding by 5.5% over SentencePiece but does not test larger models or other downstream tasks
- Why unresolved: The authors explicitly state they were unable to report on benefits at larger scales due to computational constraints
- What evidence would resolve it: Training and evaluating larger models (e.g., 1B+ parameters) with CompoundPiece on multiple downstream tasks like text classification, question answering, and machine translation would provide comparative performance data

### Open Question 2
- Question: Can the logit bias strategy used in self-supervised Stage 1 training be optimized for each language to further improve performance?
- Basis in paper: [explicit] The authors use a fixed logit bias of 3 for all languages in Stage 1 training but note it was chosen based on English validation data
- Why unresolved: The paper only evaluates one fixed bias value rather than exploring language-specific optimization
- What evidence would resolve it: Systematically testing different logit bias values (e.g., {0, 1, 2, 3, 4}) for each language and measuring the impact on Stage 1 and overall performance would identify optimal bias settings

### Open Question 3
- Question: How does the performance of the proposed two-stage training framework compare when using other byte-level or subword tokenizers as starting points?
- Basis in paper: [explicit] The authors use ByT5 as the main starting point and compare against subword-based T5, Flan-T5, and mT5 models but do not explore other tokenizer architectures
- Why unresolved: The paper focuses on ByT5 vs. subword tokenizers without testing alternative approaches like character-level or hybrid tokenizers
- What evidence would resolve it: Training the two-stage framework starting from models with different tokenization strategies (e.g., character-level, word-piece, or hybrid approaches) and comparing performance would reveal the impact of starting architecture

### Open Question 4
- Question: Can the hard compound reduction technique be extended to work effectively in multilingual tokenizers without the interference issues observed?
- Basis in paper: [explicit] The authors note that multilingual CompoundPiece tokenizers show less improvement (23.2% → 16.5% hard compounds) compared to monolingual ones (27.1% → 9.7%) due to token interference across languages
- Why unresolved: The paper identifies the problem but does not propose or test solutions for multilingual tokenizers
- What evidence would resolve it: Developing and testing methods to adjust token probability based on input language, or using language-specific sub-vocabularies within multilingual tokenizers, would demonstrate whether the interference issue can be mitigated

## Limitations
- Limited linguistic diversity in annotated data with only 4.6 words per language on average
- Unverified assumption about hyphen usage across all 56 target languages
- Byte-level tokenization computational trade-offs not fully characterized

## Confidence
- High confidence: Current LLMs perform poorly on hard compounds; CompoundPiece improves performance by 5.5%
- Medium confidence: Two-stage self-supervised training framework effectiveness depends on hyphenation patterns quality
- Low confidence: CompoundPiece reduces hard compounds from 27.1% to 9.7% requires more granular language-specific analysis

## Next Checks
1. Apply Stage 1 self-supervised model trained on English data to morphologically rich compounding languages (German, Dutch, Finnish) without fine-tuning to quantify cross-linguistic generalization
2. Implement runtime and memory profiling comparisons between ByT5 byte-level and mT5 subword tokenization on identical decompounding tasks
3. For each language, categorize the 9.7% remaining hard compounds after CompoundPiece by linguistic properties to identify resistant compound types