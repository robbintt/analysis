---
ver: rpa2
title: A Deep RL Approach on Task Placement and Scaling of Edge Resources for Cellular
  Vehicle-to-Network Service Provisioning
arxiv_id: '2305.09832'
source_url: https://arxiv.org/abs/2305.09832
tags:
- scaling
- placement
- delay
- network
- vehicles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of joint task placement and
  scaling of edge resources for Cellular Vehicle-to-Network (C-V2N) services. The
  authors propose a decentralized approach that combines a greedy task placement strategy
  with a Deep Deterministic Policy Gradient (DDPG) based scaling algorithm.
---

# A Deep RL Approach on Task Placement and Scaling of Edge Resources for Cellular Vehicle-to-Network Service Provisioning

## Quick Facts
- arXiv ID: 2305.09832
- Source URL: https://arxiv.org/abs/2305.09832
- Reference count: 40
- Key outcome: DDPG-based solutions achieve 99th percentile service delay below target while using fewer resources than state-of-the-art approaches

## Executive Summary
This paper addresses the joint task placement and scaling of edge resources for Cellular Vehicle-to-Network (C-V2N) services, which is proven to be NP-hard due to the coupling of placement and scaling decisions. The authors propose a decentralized approach combining greedy task placement with Deep Deterministic Policy Gradient (DDPG) based scaling algorithms. Evaluation using real-world traffic data from Turin, Italy shows the proposed DDPG-based solutions outperform state-of-the-art approaches, achieving sub-millisecond runtime while meeting strict latency requirements.

## Method Summary
The approach uses a decentralized architecture where greedy algorithms handle task placement while DDPG agents learn scaling policies for CPU allocation across Points of Presence (PoPs). Two DDPG variants are implemented: DDPG-1 with per-PoP agents and DDPG-5 with centralized scaling decisions. The agents are trained using PyTorch with experience replay and target networks, optimizing for minimum service delay and resource utilization. The system is evaluated against baseline methods (CNST, PI, TES) using real-world traffic data.

## Key Results
- Proposed DDPG-based solutions achieve 99th percentile service delay below target while using fewer resources than CNST, PI, and TES baselines
- Decentralized DDPG-1 agent slightly outperforms centralized DDPG-5 agent
- Solutions run in sub-millisecond time, meeting strict C-V2N latency requirements
- DDPG agents implicitly learn traffic seasonality patterns without explicit seasonal inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint task placement and scaling is NP-hard due to coupling of decisions.
- Mechanism: The reward function depends nonlinearly on both placement (p'_v) and scaling (C_p,v) variables through the delay term, preventing linearization or convex optimization.
- Core assumption: Vehicle arrivals are known in advance (oracle setting) to prove NP-hardness.
- Evidence anchors:
  - [abstract] "prove that it is not computationally tractable"
  - [section] "the reward function (9) is nonlinear in the decision variables Cp,v and p′v"
  - [corpus] Weak - no direct citations about NP-hardness proofs
- Break condition: If future arrivals become known, optimal solution requires exhaustive search.

### Mechanism 2
- Claim: DDPG learns implicit traffic seasonality without explicit input.
- Mechanism: The DDPG agent receives only current state (number of vehicles and CPUs at each PoP) and learns through experience replay to anticipate traffic patterns.
- Core assumption: Road traffic patterns follow seasonal trends (daily repetition).
- Evidence anchors:
  - [abstract] "DHPG, a new Deep Reinforcement Learning (DRL) approach that operates in hybrid action spaces"
  - [section] "DDPG-1 has (implicitly) learned to foresee how the traffic patterns will evolve"
  - [corpus] Weak - no direct citations about seasonality learning in DDPG
- Break condition: If traffic patterns become completely random without seasonality.

### Mechanism 3
- Claim: Decentralized DDPG-1 outperforms centralized DDPG-5 due to local information sufficiency.
- Mechanism: Each PoP's DDPG agent makes decisions based only on local state, avoiding redundant information from distant PoPs.
- Core assumption: PoPs are geographically dispersed with uncorrelated traffic patterns.
- Evidence anchors:
  - [abstract] "The decentralized DDPG-1 agent slightly outperforms the centralized DDPG-5 agent"
  - [section] "the information from other PoPs becomes redundant"
  - [corpus] Weak - no direct citations about geographic dispersion effects
- Break condition: If PoPs become geographically close with correlated traffic.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The joint placement and scaling problem is formulated as an MDP to handle uncertainty in future vehicle arrivals
  - Quick check question: What are the state, action, and reward components in this MDP formulation?

- Concept: Deep Deterministic Policy Gradient (DDPG)
  - Why needed here: DDPG is used to learn scaling policies that map continuous state spaces to discrete action spaces for CPU allocation
  - Quick check question: How does DDPG handle the hybrid action space of discrete CPU scaling decisions?

- Concept: M/G/1-PS Queue Model
  - Why needed here: Used to estimate processing delay at each PoP based on arrival rate and CPU capacity
  - Quick check question: What is the formula for average processing delay in an M/G/1-PS queue?

## Architecture Onboarding

- Component map: Vehicle arrival → State update → Greedy placement decision → DDPG scaling decision → Resource allocation → Reward calculation
- Critical path: Vehicle arrival → State update → Placement decision → Scaling decision → Resource allocation → Reward calculation
- Design tradeoffs: Decentralized (DDPG-1) vs Centralized (DDPG-5) scaling decisions - local vs global information
- Failure signatures: High delay violations indicate under-provisioning; low reward indicates poor placement decisions
- First 3 experiments:
  1. Test DDPG-1 vs DDPG-5 on a single PoP scenario to verify basic functionality
  2. Compare greedy placement vs random placement with fixed scaling to validate placement strategy
  3. Run full simulation with varying traffic patterns to test robustness of learned policies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed DDPG-based approach perform in a larger-scale deployment with hundreds of PoPs across a city?
- Basis in paper: [explicit] The authors mention that future work should investigate solutions working on a larger number of PoPs involving varying degrees of correlation between them.
- Why unresolved: The paper only evaluates the approach on a small-scale scenario with 5 PoPs, so its scalability and performance in a larger deployment are unknown.
- What evidence would resolve it: Running simulations or real-world tests with hundreds of PoPs to compare the DDPG-based approach's performance against state-of-the-art methods in terms of reward, delay violations, and resource utilization.

### Open Question 2
- Question: How would adjusting the reward function shape when the experienced delay exceeds the target (d > d_target) impact the DDPG-based agents' performance?
- Basis in paper: [explicit] The authors suggest that a faster decay in the reward for d > d_target may reduce the chances of overloading and delay violations, and provide an example reward function for future work.
- Why unresolved: The paper uses a fixed reward function that does not decay fast when the experienced delay exceeds the target, which may lead to delay violations.
- What evidence would resolve it: Implementing and comparing the proposed reward function with the original one in the paper, measuring the impact on delay violations, resource utilization, and overall reward.

### Open Question 3
- Question: How would incorporating task placement decisions into the DDPG-based agents affect the overall performance compared to solving placement and scaling separately?
- Basis in paper: [explicit] The authors mention that coupling task placement and scaling decisions leads to sub-optimal solutions and suggest incorporating placement decisions into the DDPG-based agents in future work.
- Why unresolved: The paper addresses task placement and scaling as separate problems, which may result in sub-optimal solutions due to the inter-dependency of the two decisions.
- What evidence would resolve it: Developing a DDPG-based approach that jointly optimizes task placement and scaling, and comparing its performance against the separate approaches in terms of reward, delay violations, and resource utilization.

## Limitations
- NP-hardness proof relies on oracle access to future vehicle arrivals, which is unrealistic in practical deployment scenarios
- Decentralized approach assumes geographically dispersed PoPs with uncorrelated traffic patterns
- Solution achieves sub-millisecond runtime only after training, initial training costs not addressed

## Confidence

**High confidence**: The claim that DDPG-based solutions outperform state-of-the-art approaches (CNST, PI, TES) is supported by empirical results showing 99th percentile delay below target with lower resource usage.

**Medium confidence**: The mechanism by which DDPG learns traffic seasonality implicitly is plausible but lacks direct empirical validation or citations to similar work.

**Medium confidence**: The claim that DDPG-1 slightly outperforms DDPG-5 is supported by results but the margin of improvement is small, suggesting the benefit may be context-dependent.

## Next Checks

1. Test the approach under stochastic vehicle arrivals without oracle knowledge to validate robustness to uncertainty in the NP-hardness setting.
2. Evaluate performance when PoPs experience correlated traffic patterns to verify the assumptions underlying the decentralized approach's advantage.
3. Measure initial training time and resource requirements to assess practical deployment feasibility for dynamic traffic conditions.