---
ver: rpa2
title: Prompt Sketching for Large Language Models
arxiv_id: '2311.04954'
source_url: https://arxiv.org/abs/2311.04954
tags:
- answer
- beam
- sketching
- decoding
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces prompt sketching, a framework for template-guided
  LLM inference that frames multi-step prompting as a segmented sequence decoding
  problem. Instead of generating unstructured responses, the model fills multiple
  variables in a template, enabling more structured and controllable reasoning.
---

# Prompt Sketching for Large Language Models

## Quick Facts
- arXiv ID: 2311.04954
- Source URL: https://arxiv.org/abs/2311.04954
- Reference count: 40
- One-line primary result: Prompt sketching framework achieves up to 10% accuracy gains on 7 of 8 reasoning tasks by framing multi-step prompting as segmented sequence decoding

## Executive Summary
This paper introduces prompt sketching, a framework that treats multi-step prompting as a segmented sequence decoding problem. Instead of generating unstructured responses, the model fills multiple variables in a template, enabling more structured and controllable reasoning. The key innovation is adapting decoding strategies to score follow-up instructions during generation, optimizing the overall template likelihood. Experiments on 8 reasoning tasks demonstrate that prompt sketching outperforms standard methods like direct asking or chain-of-thought on 7 tasks, with up to 10% accuracy gains. The authors also enable novel applications such as causal reordering, Sudoku solving, and interactive graph traversal.

## Method Summary
The method frames multi-step prompting as segmented sequence decoding by structuring prompts as templates with variables and deterministic chunks. The framework introduces sketch-aware decoding procedures (VAR and BEAM VAR) that optimize over variable assignments jointly rather than sequentially. These methods extend beam search by allocating beam slots dynamically per variable or sampling values for the current variable. The approach is implemented using LMQL (Language Model Query Language) and tested across 8 reasoning tasks using text-davinci-003 and Llama-2 Chat models.

## Key Results
- Prompt sketching outperforms standard methods on 7 of 8 reasoning tasks
- Up to 10% accuracy improvements compared to answer-only and chain-of-thought approaches
- Sketch-aware decoders (VAR and BEAM VAR) show significant improvements over simple stop-and-go templating on 5/8 tasks
- Enables novel applications like forward referencing, Sudoku solving, and interactive graph traversal

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Adapting decoding procedures to score follow-up instructions during text generation optimizes overall template likelihood.
- **Mechanism**: By phrasing multi-step prompting as segmented sequence decoding, the model jointly optimizes the probability of filling all template variables, not just the final answer. This allows the decoder to anticipate future template structure and avoid undesirable intermediate outputs.
- **Core assumption**: The model's probability distribution over tokens can be leveraged not just for next-token prediction, but for joint optimization over a segmented sequence defined by a template.
- **Evidence anchors**:
  - [abstract] "The key idea enabling sketching with existing, autoregressive models is to adapt the decoding procedure to also score follow-up instructions during text generation, thus optimizing overall template likelihood in inference."
  - [section 3] "Crucially, we derive the values of all chunks from a single sequence of tokens y, which can be predicted sequentially using an autoregressive model."
  - [corpus] Weak/no direct evidence; only related work on constrained decoding, not joint template optimization.
- **Break condition**: If the template is too restrictive or the model cannot anticipate future information (e.g., in causal reordering tasks), the joint optimization may not improve or could degrade performance.

### Mechanism 2
- **Claim**: Sketch-aware decoding (VAR and BEAM VAR) outperforms sequential decoding by exploring a wider hypothesis space over variables.
- **Mechanism**: Instead of extending each hypothesis by the n most likely next tokens, VAR extends by n sampled values for the current variable, and BEAM VAR allocates beam slots dynamically per variable. This allows the decoder to optimize over variable assignments jointly, rather than greedily.
- **Core assumption**: Variable-level beam search and dynamic beam allocation can effectively explore the space of template completions, and the model's distribution over variables is meaningful for joint optimization.
- **Evidence anchors**:
  - [section 3.1] "We consider constrained variables as a special case of non-deterministic variables, whose values are predicted by the model, but can only be chosen from a restricted set of sequences."
  - [section 4.1] "For 5/8 tasks, we even observe significant improvements over simple stop-and-go templating, demonstrating that sketch-aware decoding and joint optimization of multiple variables are crucial components of effective template-guided LLM inference."
  - [corpus] No direct evidence; related work on constrained decoding but not variable-level beam search.
- **Break condition**: If the model's variable-level probabilities are unreliable (e.g., for smaller models like Llama-2), or if the template structure is too simple, the added complexity of sketch-aware decoding may not yield benefits.

### Mechanism 3
- **Claim**: Templated generation enables novel applications like forward referencing, Sudoku solving, and interactive graph traversal.
- **Mechanism**: By structuring the prompt as a template with variables and deterministic chunks, the model can be guided to reason about future information before it is revealed. This is enabled by the joint optimization over variables and the ability to inject deterministic phrases.
- **Core assumption**: The model can reason about future information to some degree, and this capability can be leveraged by structuring the prompt appropriately.
- **Evidence anchors**:
  - [section 4.2] "For this custom task (cf. Table 1), we indeed observe that ARGMAX is incapable of producing any meaningful results (0.01 accuracy), whereas, BEAM VAR and VAR achieve an improved accuracy of 0.25 and 0.06 respectively, by exploring a wider hypotheses space."
  - [section 4.2] "BEAM VAR and VAR solve 6/10 and 7/10 puzzles respectively, demonstrating again that they explore a wider hypotheses space."
  - [corpus] Weak/no direct evidence; only related work on forward reasoning, not Sudoku or graph traversal.
- **Break condition**: If the task requires reasoning about information that is too far in the future, or if the model's reasoning capabilities are insufficient (e.g., for smaller models), forward referencing may fail.

## Foundational Learning

- **Concept**: Autoregressive sequence generation and beam search
  - Why needed here: Understanding how LLMs generate text token-by-token and how beam search explores the space of possible continuations is crucial for understanding how sketching adapts these mechanisms for template-guided inference.
  - Quick check question: How does beam search differ from greedy decoding, and what is the trade-off between exploration and computation?

- **Concept**: Template-guided generation and constrained decoding
  - Why needed here: Sketching relies on templates with variables and constraints, and adapting decoding procedures to optimize over these templates. Understanding how templates can guide generation and how constraints can be enforced is key.
  - Quick check question: What is the difference between stop-and-go inference and sketching, and how does sketching enable joint optimization over variables?

- **Concept**: Joint optimization and variable-level beam search
  - Why needed here: Sketch-aware decoding methods like VAR and BEAM VAR rely on optimizing over variable assignments jointly, rather than sequentially. Understanding how joint optimization can improve performance is important.
  - Quick check question: How does variable-level beam search differ from standard beam search, and what is the intuition behind dynamic beam allocation?

## Architecture Onboarding

- **Component map**: Template/skeleton -> Variable extraction -> Decoding procedure (VAR/BEAM VAR) -> LLM generation -> Variable filling -> Final output
- **Critical path**: The critical path is: template → variable extraction → decoding procedure → LLM generation → variable filling → final output. The decoding procedure is the key component that enables joint optimization over variables.
- **Design tradeoffs**: The main tradeoff is between the complexity of the template/skeleton and the model's ability to reason about it. More complex templates enable more control and novel applications, but may be harder for the model to handle. Another tradeoff is between the granularity of variables and the effectiveness of joint optimization.
- **Failure signatures**: If the model struggles to fill variables correctly, the template may be too complex or the decoding procedure may not be effective. If the model's output does not adhere to the template structure, the stopping phrases or constraints may not be working properly. If performance does not improve with sketch-aware decoding, the model's variable-level probabilities may not be reliable.
- **First 3 experiments**:
  1. Implement a simple template with one variable (e.g., answer-only) and compare standard decoding vs. sketching with VAR/BEAM VAR on a simple task (e.g., date understanding).
  2. Implement a multi-variable template (e.g., chain-of-thought) and compare performance on a reasoning task (e.g., AQuA) with different decoding procedures.
  3. Implement a template that requires forward referencing (e.g., information essentiality) and test if sketch-aware decoding can improve performance over sequential decoding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does prompt sketching scale to tasks with more than 8-10 intermediate reasoning steps?
- Basis in paper: [inferred] The paper evaluates sketching on tasks with relatively simple reasoning flows, but doesn't test scalability to more complex, multi-step problems.
- Why unresolved: The experiments only cover 8 reasoning tasks with moderate complexity. No evaluation is provided for tasks requiring many reasoning steps.
- What evidence would resolve it: Experiments on tasks with 10+ reasoning steps, measuring performance degradation and comparing against sequential prompting baselines.

### Open Question 2
- Question: How sensitive are sketch-aware decoders to the exact wording of stopping phrases and deterministic chunks?
- Basis in paper: [explicit] The paper mentions using stopping phrases but doesn't systematically study how different phrasings affect performance.
- Why unresolved: No ablation study is provided on the impact of stopping phrase wording or deterministic chunk phrasing on task accuracy.
- What evidence would resolve it: Experiments varying stopping phrase wording and deterministic chunk phrasing, measuring impact on task accuracy across multiple tasks.

### Open Question 3
- Question: What is the computational complexity trade-off between sketch-aware decoding and traditional sequential prompting as task complexity increases?
- Basis in paper: [explicit] The paper mentions computational overhead but doesn't provide detailed complexity analysis or scaling experiments.
- Why unresolved: Only qualitative discussion of computational overhead is provided, without quantitative analysis of how costs scale with task complexity.
- What evidence would resolve it: Detailed complexity analysis showing how computational costs scale with number of variables and reasoning steps, compared to traditional sequential prompting.

## Limitations

- The framework's effectiveness depends on the model's ability to reliably predict variable-level probabilities, which may not hold for smaller models or more complex templates
- Computational overhead of sketch-aware decoding increases with template complexity, potentially limiting scalability for tasks requiring many variables
- The framework's generalizability beyond reasoning tasks to domains like creative generation or code synthesis remains untested

## Confidence

- **High confidence**: The framework's ability to enable novel applications like forward referencing and Sudoku solving is well-supported by experimental results showing clear improvements over baseline methods.
- **Medium confidence**: The claim that joint optimization of template variables improves performance over sequential approaches is supported by 5/8 tasks showing significant improvements, but the magnitude of gains varies substantially across tasks.
- **Low confidence**: The generalizability of sketch-aware decoding benefits across different model architectures and task types requires further validation, as results show mixed performance between text-davinci-003 and Llama-2.

## Next Checks

1. Test prompt sketching framework on non-reasoning tasks (e.g., creative writing, code generation) to evaluate cross-domain applicability and identify task characteristics that benefit most from template-guided generation.
2. Conduct ablation studies isolating the impact of joint optimization versus template structure by comparing sketch-aware decoding against sequential approaches using identical templates.
3. Evaluate the framework's sensitivity to model size by testing sketch-aware decoding across a spectrum of model scales (7B, 13B, 70B parameters) to determine the minimum model size required for effective joint optimization.