---
ver: rpa2
title: Towards Robust Text Retrieval with Progressive Learning
arxiv_id: '2311.11691'
source_url: https://arxiv.org/abs/2311.11691
tags:
- arxiv
- retrieval
- preprint
- text
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitations of existing embedding models
  for text retrieval, which suffer from insufficient batch diversity, high noise levels,
  and suboptimal convergence due to equal treatment of easy and difficult samples.
  To overcome these challenges, the authors propose PEG, a progressively learned embedding
  model.
---

# Towards Robust Text Retrieval with Progressive Learning

## Quick Facts
- **arXiv ID**: 2311.11691
- **Source URL**: https://arxiv.org/abs/2311.11691
- **Reference count**: 10
- **Primary result**: State-of-the-art text retrieval performance on C-MTEB and DuReader benchmarks using progressive learning with 80,000 batch size

## Executive Summary
This paper addresses key limitations in existing embedding models for text retrieval, including insufficient batch diversity, high noise levels, and suboptimal convergence from equal treatment of easy and difficult samples. The authors propose PEG, a progressively learned embedding model trained on over 110 million diverse samples across multiple domains and tasks. PEG employs a progressive learning mechanism that dynamically adjusts sample weights based on learning difficulty, uses large batch sizes up to 80,000 samples, and incorporates hard negative mining. Extensive experiments demonstrate PEG achieves state-of-the-art performance on Chinese retrieval benchmarks, outperforming existing methods in NDCG@10, MAP, and other metrics.

## Method Summary
PEG is a text retrieval model that combines masked auto-encoding pretraining with contrastive fine-tuning using progressive learning. The model is trained on over 110 million samples spanning domains like finance, medicine, and tourism, covering tasks including QA, MRC, and similarity matching. The progressive learning mechanism dynamically adjusts sample weights during training based on difficulty, implemented through weight scaling in the InfoNCE loss. The model uses large batch sizes (up to 80,000 in-batch negatives) and incorporates hard negative mining with LLM filtering to remove false negatives. The training pipeline involves data preprocessing into passages, MAE-style pretraining on Wudao Corpora, and contrastive fine-tuning with progressive weighting.

## Key Results
- Achieves state-of-the-art performance on C-MTEB and DuReader Chinese benchmarks
- Outperforms existing methods in NDCG@10, MAP, MRR, and Recall@K metrics
- Demonstrates improved robustness against noise and better discrimination of textual nuances through large-scale contrastive learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Progressive learning adjusts sample weights based on difficulty to improve convergence
- **Mechanism**: The model starts by focusing on easier samples (higher similarity positive pairs and lower similarity negatives), then progressively shifts attention to harder samples as training advances via dynamic weight scaling in InfoNCE loss
- **Core assumption**: Sample difficulty correlates with similarity scores, and focusing on easier samples first leads to better initial representation learning
- **Evidence anchors**: Abstract states progressive mechanism enables dynamic modulation of sample attention; section describes assigning varying weights based on learning difficulties
- **Break condition**: If similarity scores don't reliably indicate difficulty, or if progressive weighting destabilizes training

### Mechanism 2
- **Claim**: Large batch sizes with hard negative mining improve discrimination of textual nuances
- **Mechanism**: Scaling to 80,000 in-batch negatives and incorporating 5 carefully selected hard negatives per query forces the model to learn finer-grained distinctions between semantically similar texts
- **Core assumption**: Increasing quantity and quality of negative samples directly improves discrimination between similar texts
- **Evidence anchors**: Abstract mentions increasing in-batch negatives to 80,000; section discusses large-scale contrastive learning for improved discriminability
- **Break condition**: If computational constraints prevent effective training at this scale, or if hard negatives don't improve over random negatives

### Mechanism 3
- **Claim**: Domain and task diversity in training data improves generalization
- **Mechanism**: Training on 110 million samples across diverse domains (finance, medicine, tourism) and tasks (QA, MRC, similarity matching) exposes the model to varied linguistic patterns and semantic structures
- **Core assumption**: Exposure to diverse data during training translates to better performance on unseen domains and tasks
- **Evidence anchors**: Abstract mentions training on more than 100 million data spanning wide range of domains; section describes amassing extensive collection across various fields
- **Break condition**: If domain-specific fine-tuning outperforms this general approach, or if certain domains dominate training

## Foundational Learning

- **Concept**: Contrastive Learning
  - **Why needed here**: The core training objective relies on pulling similar samples together while pushing dissimilar ones apart in embedding space
  - **Quick check question**: What is the difference between the standard InfoNCE loss and the progressive version proposed here?

- **Concept**: Curriculum Learning
  - **Why needed here**: The progressive learning mechanism is essentially a curriculum learning approach where easier samples are learned first
  - **Quick check question**: How does the weight calculation in Equation 4 implement curriculum learning principles?

- **Concept**: Hard Negative Mining
  - **Why needed here**: The model's performance depends on having high-quality negatives that are challenging but not false positives
  - **Quick check question**: Why is LLM filtering used after initial hard negative retrieval, and what problem does it solve?

## Architecture Onboarding

- **Component map**: BERT-large backbone → Masked Autoencoding pretraining → Contrastive fine-tuning with progressive weighting → Hard negative mining pipeline
- **Critical path**: Data preprocessing → Pretraining (MAE) → Fine-tuning (contrastive with progressive weights) → Evaluation
- **Design tradeoffs**: Large batch size improves discrimination but requires significant computational resources; domain diversity improves generalization but may slow convergence on specific tasks
- **Failure signatures**: Poor MRR/Recall@1 on DuReader suggests hard negative mining issues; low NDCG@10 on C-MTEB suggests batch size or progressive weighting problems
- **First 3 experiments**:
  1. Compare baseline InfoNCE vs progressive InfoNCE with small batch size to isolate progressive learning effect
  2. Test hard negative mining with and without LLM filtering to measure false negative reduction
  3. Evaluate domain-specific vs general training on C-MTEB subsets to measure diversity impact

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed PEG model's performance compare to other state-of-the-art models when evaluated on text retrieval tasks in languages other than Chinese?
- **Basis in paper**: [inferred] The paper focuses on Chinese text retrieval benchmarks and does not mention evaluations on other languages
- **Why unresolved**: The paper only provides experimental results on Chinese benchmarks and does not discuss the model's performance on other languages
- **What evidence would resolve it**: Evaluating PEG on text retrieval benchmarks in other languages and comparing its performance to other state-of-the-art models

### Open Question 2
- **Question**: What is the impact of the progressive learning mechanism on the model's ability to handle noise in the training data?
- **Basis in paper**: [explicit] The paper mentions that the progressive learning mechanism dynamically adjusts the weights of samples based on their learning difficulty and aims to improve the model's robustness against noise
- **Why unresolved**: The paper does not provide a detailed analysis of how the progressive learning mechanism specifically handles noise in the training data
- **What evidence would resolve it**: Conducting experiments that explicitly measure the model's performance on noisy training data with and without the progressive learning mechanism

### Open Question 3
- **Question**: How does the proposed PEG model's performance scale with the size of the training dataset?
- **Basis in paper**: [explicit] The paper mentions that PEG is trained on a large-scale dataset of over 110 million samples and achieves state-of-the-art performance
- **Why unresolved**: The paper does not provide a detailed analysis of how the model's performance scales with the size of the training dataset
- **What evidence would resolve it**: Conducting experiments that train PEG on datasets of varying sizes and evaluate its performance on downstream benchmarks

## Limitations

- Evaluation scope limited to Chinese benchmarks (C-MTEB, DuReader), leaving unclear how well the model generalizes to other retrieval scenarios or languages
- Computational requirements for training with 80,000 batch size may limit practical adoption in resource-constrained environments
- Paper lacks quantitative validation of domain diversity benefits and doesn't show whether general training outperforms focused domain-specific training

## Confidence

- **High Confidence**: Claims about large batch sizes improving discrimination are well-supported by existing literature on contrastive learning
- **Medium Confidence**: Effectiveness of progressive learning depends heavily on specific weight calculation details not fully provided in the paper
- **Low Confidence**: Claims about domain diversity improving generalization lack quantitative validation and comparative analysis

## Next Checks

1. **Ablation Study**: Conduct controlled experiments comparing PEG with and without progressive learning, holding constant batch size and hard negative mining parameters, to isolate the contribution of the progressive mechanism to overall performance

2. **Cross-Domain Transfer**: Evaluate PEG on a completely different retrieval task (e.g., e-commerce product search or legal document retrieval) not represented in the training data to assess true generalization capabilities beyond the tested benchmarks

3. **Inference Efficiency Analysis**: Measure the actual latency and memory usage of PEG at inference time compared to baseline models, particularly examining how the large embedding dimensionality (1024) impacts real-world deployment scenarios