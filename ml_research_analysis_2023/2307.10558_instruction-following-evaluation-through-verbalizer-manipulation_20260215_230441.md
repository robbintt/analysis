---
ver: rpa2
title: Instruction-following Evaluation through Verbalizer Manipulation
arxiv_id: '2307.10558'
source_url: https://arxiv.org/abs/2307.10558
tags:
- instructions
- language
- natural
- output
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces verbalizer manipulation, a novel protocol
  to evaluate instruction-following capabilities of large language models by systematically
  varying the alignment between task instructions and model priors. The method maps
  task labels to verbalizers ranging from highly aligned to completely misaligned
  (e.g., mapping positive sentiment to "negative") and tests whether models can override
  their priors to follow instructions accurately.
---

# Instruction-following Evaluation through Verbalizer Manipulation

## Quick Facts
- arXiv ID: 2307.10558
- Source URL: https://arxiv.org/abs/2307.10558
- Reference count: 17
- Primary result: Scaling improves instruction-following on aligned instructions, but even GPT-4 performs near chance on highly misaligned verbalizers

## Executive Summary
This paper introduces verbalizer manipulation as a novel protocol to evaluate instruction-following capabilities of large language models. The method systematically varies the alignment between task instructions and model priors by mapping task labels to verbalizers ranging from highly aligned to completely misaligned. Across nine datasets and four model families, the evaluation reveals that while scaling improves performance on aligned and neutral instructions, even the strongest models struggle significantly when verbalizers contradict learned priors. The findings highlight fundamental limitations in current models' ability to override their learned associations to follow instructions accurately.

## Method Summary
The evaluation protocol maps task labels to verbalizers ranging from highly aligned to completely misaligned, then tests whether models can override their priors to follow instructions accurately. The method uses nine binary classification datasets with 100 examples each per verbalizer mapping, creating instruction sets that range from natural (highly aligned) to unnatural (completely misaligned). Models are evaluated in an instruction-only setting using direct prompting and zero-shot chain-of-thought prompting with temperature set to 0 during decoding. Performance is measured by accuracy percentages across different instruction types and model families.

## Key Results
- Scaling improves performance on aligned and neutral instructions but not on highly misaligned verbalizers
- Even GPT-4 performs near chance on highly misaligned verbalizers
- Different model families show varied sensitivities to verbalizer choice
- Zero-shot chain-of-thought prompting offers limited improvement on misaligned instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-following ability can be evaluated by systematically varying alignment between task instructions and model priors through verbalizer manipulation
- Mechanism: The protocol maps task labels to verbalizers ranging from highly aligned to completely misaligned, then tests whether models can override their priors to follow instructions accurately
- Core assumption: Models trained on standard datasets have strong priors about which words correspond to which labels, and these priors can be systematically manipulated to test instruction-following capabilities
- Evidence anchors:
  - [abstract] "It instructs the model to verbalize the task label with words aligning with model priors to different extents, adopting verbalizers from highly aligned (e.g., outputting 'postive' for positive sentiment), to minimally aligned (e.g., outputting 'negative' for positive sentiment)"
  - [section 3.2] "For example, for SST-2, golden label names are 'positive'|'negative' and in natural instructions, they will be mapped to 'positive'|'negative', '1'|'0', 'yes|no'"
  - [corpus] Weak evidence - corpus mentions related work on verbalizer manipulation but doesn't directly support this specific mechanism
- Break condition: If models lack the capability to override their learned priors, they would perform at chance levels on misaligned verbalizers regardless of instruction clarity

### Mechanism 2
- Claim: Scaling model size improves instruction-following ability, but this improvement is limited when instructions contradict learned priors
- Mechanism: Larger models have better instruction-following capabilities even on neutral instructions that don't align with priors, but struggle significantly when verbalizers are flipped to contradict priors
- Core assumption: Model scale correlates with instruction-following capability, but this correlation breaks down when instructions require overriding strong learned priors
- Evidence anchors:
  - [abstract] "scaling improves performance on aligned and neutral instructions, but even the strongest models (including GPT-4) perform near chance on highly misaligned verbalizers"
  - [section 4.1] "Larger models generally perform better on both natural and neutral instructions" but "Different model families diverge significantly on unnatural instructions"
  - [corpus] Moderate evidence - corpus contains related work on scaling effects but doesn't directly address the contradiction case
- Break condition: If a model family shows consistent improvement across all instruction types including misaligned ones, this mechanism would be invalid

### Mechanism 3
- Claim: Zero-shot chain-of-thought prompting can improve performance on misaligned instructions but cannot fully bridge the gap to aligned instructions
- Mechanism: Allowing models to output intermediate reasoning steps helps them overcome some difficulties with misaligned verbalizers, but performance still lags behind aligned instructions
- Core assumption: Chain-of-thought prompting provides additional reasoning capacity that helps models navigate conflicting priors, but the fundamental conflict remains challenging
- Evidence anchors:
  - [abstract] "zero-shot chain-of-thought prompting offers limited improvement" and results show "large performance gaps compared to corresponding results in natural instructions"
  - [section 4.3] "zero-shot CoT prompting can make models better instruction-followers when instructions contradict prior knowledge, but the models still have a large performance gap with instructions that align with prior knowledge"
  - [corpus] Weak evidence - corpus doesn't contain direct evidence about chain-of-thought effectiveness on misaligned instructions
- Break condition: If zero-shot CoT prompting eliminated the performance gap between aligned and misaligned instructions, this mechanism would be invalid

## Foundational Learning

- Concept: Prior knowledge and model biases in language models
  - Why needed here: Understanding how models develop strong associations between specific words and labels is crucial for interpreting why verbalizer manipulation reveals instruction-following limitations
  - Quick check question: What would happen if you asked a sentiment model trained on standard data to output "negative" for positive reviews?

- Concept: Instruction-tuning and few-shot learning paradigms
  - Why needed here: The evaluation protocol builds on instruction-tuning foundations, where models learn to follow instructions rather than just complete tasks
  - Quick check question: How does instruction-following differ from standard supervised learning in terms of model behavior?

- Concept: Scaling laws and their limitations
  - Why needed here: The results show that scaling improves performance on some instruction types but not others, highlighting the nuanced relationship between model size and capability
  - Quick check question: Why might larger models perform better on neutral instructions but still struggle with misaligned ones?

## Architecture Onboarding

- Component map: Dataset preparation → Verbalizer mapping → Prompt template generation → Model inference → Result aggregation → Analysis of performance patterns across instruction types
- Critical path: Dataset preparation → Verbalizer mapping → Prompt template generation → Model inference → Result aggregation → Analysis of performance patterns across instruction types
- Design tradeoffs: Using binary classification simplifies the protocol but limits generalizability; fixed 100 examples per dataset balances evaluation thoroughness with computational cost; focusing on instruction-only setting isolates instruction-following from demonstration effects
- Failure signatures: If all models perform similarly across all verbalizer types, the evaluation may not be sensitive enough; if performance gaps don't scale with model size, the protocol may not capture instruction-following differences; if results vary significantly across dataset domains, the findings may not generalize
- First 3 experiments:
  1. Run evaluation on a single dataset (e.g., SST-2) across all model families to verify the protocol works and produces expected patterns
  2. Compare performance on natural vs. flipped verbalizers for a mid-sized model to confirm the misalignment effect
  3. Test zero-shot CoT prompting on a challenging misaligned verbalizer to verify it provides measurable improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications could enable models to better follow instructions that contradict their priors, beyond simple scaling?
- Basis in paper: Explicit - The paper concludes that "scaling is not the only factor influencing instruction following" and that even GPT-4 struggles with highly misaligned verbalizers, suggesting fundamental limitations in current architectures.
- Why unresolved: The paper identifies the problem but doesn't propose or test specific architectural solutions. It only evaluates existing models and prompting techniques.
- What evidence would resolve it: Experimental results comparing models with different architectural modifications (attention mechanisms, memory components, meta-learning approaches) on the verbalizer manipulation task would show which modifications specifically help with contradictory instructions.

### Open Question 2
- Question: Are there task-specific characteristics that make some datasets (like sentiment classification) more resistant to verbalizer manipulation than others (like NLI), and can this be predicted?
- Basis in paper: Explicit - The paper notes that "ChatGPT and GPT-4 retain performance on sentiment classification task in unnatural direct prompting compared to natural counterpart but drop significantly on natural language inference task."
- Why unresolved: While the paper observes this difference, it only hypothesizes about reasoning requirements without systematically analyzing what task features predict vulnerability to verbalizer manipulation.
- What evidence would resolve it: A comprehensive analysis correlating task characteristics (reasoning depth, linguistic complexity, domain specificity) with performance degradation under verbalizer manipulation across many tasks would identify predictive features.

### Open Question 3
- Question: How does the effectiveness of zero-shot chain-of-thought prompting vary across different model families and task types when dealing with misaligned verbalizers?
- Basis in paper: Explicit - The paper shows that "zero-shot CoT prompting can make models better instruction-followers when instructions contradict prior knowledge" but notes "large performance gaps" remain, with varying effectiveness across models.
- Why unresolved: The paper provides limited comparison of CoT effectiveness across model families and doesn't explore task-specific variations in CoT benefits for misaligned instructions.
- What evidence would resolve it: Systematic experiments testing zero-shot CoT across all model families on all task types with detailed analysis of when and why CoT succeeds or fails with misaligned verbalizers.

## Limitations

- The evaluation protocol focuses on binary classification tasks, limiting generalizability to more complex tasks
- The choice of verbalizer mappings, while systematic, may not capture all possible misalignments in real-world applications
- The protocol assumes model priors are consistent across different model families and scales
- The reliance on accuracy as the sole metric misses other important aspects of instruction-following

## Confidence

**High confidence**: The core finding that instruction-following ability varies significantly with verbalizer alignment and that scaling provides limited benefit on misaligned instructions is well-supported by systematic evaluation across multiple datasets and model families.

**Medium confidence**: The relative performance differences between model families on misaligned instructions are well-documented, but underlying reasons for these differences remain speculative without deeper analysis.

**Low confidence**: The effectiveness of zero-shot chain-of-thought prompting is presented as "limited improvement" but the quantitative extent of this improvement across different model families and verbalizer types is not fully explored.

## Next Checks

1. Apply the verbalizer manipulation protocol to a multi-class classification dataset (e.g., AG News or SST-5) to validate whether instruction-following limitations extend beyond binary classification tasks.

2. Conduct an ablation study by fine-tuning a subset of models on custom datasets with specific verbalizer-label associations, then test whether targeted training improves performance on misaligned instructions.

3. Implement a practical task where models must follow instructions that contradict typical priors (e.g., medical diagnosis where "negative" means positive outcome) and compare performance against controlled verbalizer manipulation results to assess ecological validity.