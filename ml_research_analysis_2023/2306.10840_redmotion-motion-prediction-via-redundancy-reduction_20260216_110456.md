---
ver: rpa2
title: 'RedMotion: Motion Prediction via Redundancy Reduction'
arxiv_id: '2306.10840'
source_url: https://arxiv.org/abs/2306.10840
tags:
- motion
- prediction
- road
- agent
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a transformer-based model for motion prediction
  in self-driving vehicles, leveraging redundancy reduction for both local road environment
  tokens and embeddings from augmented views. The model, called REDMotion, employs
  a novel approach to generate road environment descriptors by reducing a variable-sized
  set of local road environment tokens to a fixed-sized global embedding.
---

# RedMotion: Motion Prediction via Redundancy Reduction

## Quick Facts
- arXiv ID: 2306.10840
- Source URL: https://arxiv.org/abs/2306.10840
- Reference count: 40
- Primary result: Achieves 20% lower minFDE and 29% lower minADE than baseline without pre-training on Waymo Open Motion dataset

## Executive Summary
This paper introduces REDMotion, a transformer-based model for motion prediction in autonomous driving that leverages redundancy reduction through two complementary mechanisms: local road environment token aggregation into fixed descriptors and augmentation-invariant embeddings via Barlow Twins pre-training. The model processes variable-length road environment tokens into a fixed-size representation using local attention layers followed by a parallel decoder, then fuses these descriptors with ego trajectory embeddings through cross-attention to predict future trajectories. Experiments demonstrate that the Barlow Twins pre-training method improves minADE and minFDE by 12% and 15% respectively compared to baselines, outperforming contrastive learning approaches like PreTraM and SimCLR in semi-supervised settings.

## Method Summary
REDMotion processes local road graph tokens (agents within 25m, lanes within 50m) through local attention layers that gradually expand the receptive field, then uses a parallel decoder to aggregate these into fixed-size RED tokens. These descriptors are learned to be non-redundant through a Barlow Twins projector loss that minimizes correlation between augmented views of HD maps. The ego trajectory is encoded separately and fused with RED tokens via cross-attention, feeding into an MLP motion head that outputs 6 trajectory proposals with confidences. The model is pre-trained self-supervised on augmented HD maps using Barlow Twins, then fine-tuned on labeled trajectory data using negative multivariate log-likelihood loss with AdamW optimizer and cosine annealing learning rate.

## Key Results
- Achieves 20% lower minFDE and 29% lower minADE than baseline without pre-training
- Improves minADE by 12% and minFDE by 15% with Barlow Twins pre-training compared to no pre-training
- Outperforms PreTraM, Traj-MAE, and GraphDINO in semi-supervised settings (12.5% labeled data)
- Competitive performance with state-of-the-art methods (MultiPath++, Scene Transformer) on Waymo Motion Prediction Challenge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: REDMotion reduces redundancy between global descriptors to improve diversity and prevent mode collapse.
- Mechanism: The parallel decoder applies global cross-attention from RED tokens to all road environment tokens, and the projector computes a Barlow Twins loss between embeddings ZA and ZB from augmented views. This forces the RED tokens to learn complementary, non-redundant features.
- Core assumption: Different RED tokens can specialize on distinct aspects of the environment without overlapping representations.
- Evidence anchors:
  - [abstract] "The second type of redundancy reduction is obtained by self-supervised learning and applies the redundancy reduction principle to embeddings generated from augmented views of road environments."
  - [section 3.1] "The training objective is to approximate this cross-correlation matrix to the corresponding identity matrix, while reducing the redundancy between individual vector elements."
  - [corpus] Weak: No direct evidence in neighbors about redundancy reduction for diversity; corpus is largely about other motion prediction methods.

### Mechanism 2
- Claim: RED tokens aggregate variable-length road environment information into a fixed-size representation that preserves spatial context.
- Mechanism: Local attention layers expand the receptive field of each road environment token gradually (as in Figure 2), then the parallel decoder performs global cross-attention from RED tokens to all environment tokens, effectively pooling variable-length input into fixed-size RED tokens.
- Core assumption: Gradual receptive field expansion allows each RED token to capture a unique spatial context without losing local detail.
- Evidence anchors:
  - [section 3.1] "Local attention layers...allow us to process long input sequences...the receptive field of each token grows with an increasing number of local attention layers."
  - [section 3.1] "the parallel decoder of RED tokens...implements by a global cross-attention mechanism from RED tokens to road env tokens...global representation can be learned by RED tokens."
  - [corpus] Weak: No corpus neighbor explicitly discusses fixed-size pooling from variable-length tokens.

### Mechanism 3
- Claim: Pre-training on Barlow Twins with augmented HD map views improves generalization under limited labeled data.
- Mechanism: The model learns to produce similar embeddings for different augmentations (rotations, shifts, color jitter, color drop) of the same HD map, thereby capturing invariant features before fine-tuning on labeled trajectories.
- Core assumption: Augmented HD maps still represent the same underlying road environment, so invariant features learned during pre-training are useful for downstream motion prediction.
- Evidence anchors:
  - [abstract] "Our experiments reveal that our representation learning approach outperforms PreTraM, Traj-MAE, and GraphDINO in a semi-supervised setting."
  - [section 4.1] "our pre-training method improves the minADE score by 12% and the minFDE score by 15% compared to the baseline without pre-training."
  - [section 3.2] "We use uniform distributions to sample random rotation (max. +/- 10°) and shift augmentations (max. +/- 1m)."
  - [corpus] Weak: None of the corpus neighbors discuss Barlow Twins or augmentation-based pre-training for motion prediction.

## Foundational Learning

- Concept: Attention mechanisms and cross-attention for fusing embeddings.
  - Why needed here: RED tokens must aggregate road environment context, and ego trajectory embedding must fuse with RED via cross-attention to predict trajectories.
  - Quick check question: In cross-attention, what serves as queries, keys, and values when fusing trajectory embedding with RED tokens?

- Concept: Barlow Twins and redundancy reduction.
  - Why needed here: The projector loss encourages RED tokens to learn diverse, non-redundant features, preventing mode collapse.
  - Quick check question: What shape is the cross-correlation matrix that Barlow Twins tries to approximate to the identity matrix?

- Concept: Local vs. global attention in transformers.
  - Why needed here: Local attention reduces memory for long road environment sequences; global attention is needed in the parallel decoder to aggregate context into RED tokens.
  - Quick check question: How does the receptive field of a token change as the number of local attention layers increases?

## Architecture Onboarding

- Component map: Local road graph tokens + past ego trajectory -> Local attention layers -> Parallel decoder -> RED tokens -> Projector (Barlow Twins) -> Ego trajectory encoder -> Cross-attention fusion -> MLP motion head -> 6 trajectory proposals + confidences

- Critical path: Local attention → Parallel decoder → Cross-attention fusion → MLP head

- Design tradeoffs:
  - Local attention reduces memory but may limit long-range context unless enough layers are used
  - Fixed-size RED tokens simplify downstream fusion but may lose fine-grained spatial detail if too few tokens
  - Barlow Twins loss adds regularization but requires careful augmentation strength

- Failure signatures:
  - Poor minADE/minFDE → RED tokens not capturing enough context or redundancy reduction failing
  - Training instability → Augmentation too strong or projector loss too dominant
  - High parameter count → Too many RED tokens or overly wide MLPs

- First 3 experiments:
  1. Ablate the Barlow Twins projector loss: train without it and compare minADE/minFDE
  2. Vary number of RED tokens (e.g., 8, 16, 32) and measure impact on accuracy and parameter count
  3. Test different local attention window sizes (e.g., 8, 16, 32) to find sweet spot between memory and receptive field

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of REDMotion scale with dataset size beyond 100% of the Waymo Open Motion dataset?
- Basis in paper: [explicit] The paper mentions evaluating REDMotion on 100% of the training dataset, achieving 20% lower minFDE scores and 29% lower minADE scores compared to the baseline without pre-training.
- Why unresolved: The paper does not provide results for dataset sizes larger than 100% of the Waymo Open Motion dataset.
- What evidence would resolve it: Additional experiments evaluating REDMotion's performance on dataset sizes larger than 100% of the Waymo Open Motion dataset.

### Open Question 2
- Question: How does the performance of REDMotion compare to other state-of-the-art models when using ensemble methods?
- Basis in paper: [explicit] The paper states that REDMotion does not use ensembling, and compares its performance to other models' best single model versions.
- Why unresolved: The paper does not provide results for REDMotion using ensemble methods.
- What evidence would resolve it: Experiments evaluating REDMotion's performance using ensemble methods and comparing it to other state-of-the-art models using ensemble methods.

### Open Question 3
- Question: How does the choice of augmentation strategy affect the performance of REDMotion?
- Basis in paper: [explicit] The paper mentions using weaker augmentations for rasterized HD maps compared to vanilla Barlow Twins augmentations.
- Why unresolved: The paper does not provide a comprehensive comparison of different augmentation strategies for REDMotion.
- What evidence would resolve it: Experiments evaluating REDMotion's performance using different augmentation strategies and comparing the results.

### Open Question 4
- Question: How does the performance of REDMotion vary across different types of road environments (e.g., urban, suburban, highway)?
- Basis in paper: [explicit] The paper does not provide a detailed breakdown of REDMotion's performance across different road environments.
- Why unresolved: The paper does not include experiments specifically designed to evaluate REDMotion's performance in various road environments.
- What evidence would resolve it: Experiments evaluating REDMotion's performance in different road environments and comparing the results.

### Open Question 5
- Question: How does the performance of REDMotion change when using different motion prediction heads (e.g., MLP, transformer-based)?
- Basis in paper: [explicit] The paper mentions using an MLP as the motion head for REDMotion, but does not explore other options.
- Why unresolved: The paper does not provide results for REDMotion using different motion prediction heads.
- What evidence would resolve it: Experiments evaluating REDMotion's performance using different motion prediction heads and comparing the results.

## Limitations

- Barlow Twins-based redundancy reduction mechanism lacks direct empirical validation within the paper itself
- Reported improvements over PreTraM, Traj-MAE, and GraphDINO are based on unpublished results, making independent verification difficult
- Competitive but not superior performance compared to state-of-the-art methods (MultiPath++, Scene Transformer)

## Confidence

- High confidence: The overall architecture design (local attention → RED tokens → cross-attention fusion) is clearly specified and technically sound
- Medium confidence: The Barlow Twins pre-training approach is theoretically justified but lacks direct ablation evidence
- Low confidence: Claims about redundancy reduction preventing mode collapse are supported by mechanism description but not empirically validated

## Next Checks

1. Conduct an ablation study isolating the Barlow Twins projector loss by training REDMotion without it and comparing minADE/minFDE to the full model

2. Test REDMotion's sensitivity to augmentation strength by training with varying rotation (±5° to ±20°) and shift (±0.5m to ±2m) parameters to identify optimal ranges

3. Implement a baseline using the same architecture but with contrastive learning (like PreTraM) instead of Barlow Twins for direct comparison under identical semi-supervised settings