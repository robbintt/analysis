---
ver: rpa2
title: 'DRStageNet: Deep Learning for Diabetic Retinopathy Staging from Fundus Images'
arxiv_id: '2312.14891'
source_url: https://arxiv.org/abs/2312.14891
tags:
- drstagenet
- dfis
- learning
- dataset
- diabetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces DRStageNet, a deep learning model designed
  for robust staging of diabetic retinopathy (DR) from fundus images. The model leverages
  a self-supervised pretrained vision transformer (DINOv2) and employs multi-source
  domain fine-tuning across seven publicly available datasets to enhance generalization.
---

# DRStageNet: Deep Learning for Diabetic Retinopathy Staging from Fundus Images

## Quick Facts
- arXiv ID: 2312.14891
- Source URL: https://arxiv.org/abs/2312.14891
- Authors: 
- Reference count: 40
- Key outcome: DRStageNet achieves linearly weighted kappa of 0.747 on source domain and outperforms state-of-the-art benchmarks on external datasets for diabetic retinopathy staging

## Executive Summary
DRStageNet introduces a deep learning model for robust staging of diabetic retinopathy (DR) from fundus images. The model leverages self-supervised pretraining on natural images using DINOv2 and employs multi-source domain fine-tuning across seven publicly available datasets to enhance generalization. DRStageNet demonstrates superior performance in DR staging and screening, achieving a linearly weighted kappa of 0.747 on the source domain and outperforming state-of-the-art benchmarks on external datasets. The error analysis revealed that 59% of misclassifications were due to incorrect reference labels.

## Method Summary
DRStageNet uses a self-supervised pretrained vision transformer (DINOv2) with a regression head, trained using mean squared error loss. The model employs multi-source domain fine-tuning across seven diverse datasets containing 93,534 fundus images. Input preprocessing involves removing horizontal black regions, padding to square aspect ratio, and resizing to 518×518 pixels. The model is evaluated using linearly weighted kappa, multiclass accuracy, mean squared error, and mean absolute error for staging, along with AUC and F1 score for referable DR classification.

## Key Results
- Achieved linearly weighted kappa of 0.747 on source domain (EYEPACS)
- Outperformed state-of-the-art benchmarks on external datasets (APTOS 2019)
- 59% of misclassifications attributed to incorrect reference labels
- Generated high-resolution explainability heatmaps highlighting relevant DR lesions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised pretraining on natural images provides transferable visual features for DR staging.
- Mechanism: DINOv2 is pretrained on 142 million natural images using self-supervised learning, creating a robust feature representation that can be fine-tuned for DR staging.
- Core assumption: Visual patterns in natural images share sufficient similarity with DR lesions and retinal structures to enable effective transfer learning.
- Evidence anchors: [abstract] "We fine-tune DINOv2, a pretrained model of self-supervised vision transformer" and [section] "DINOv2 is a ViT that was trained on 142 million natural images using self-supervised learning (SSL)"

### Mechanism 2
- Claim: Multi-source domain fine-tuning improves generalization across diverse datasets.
- Mechanism: Training on seven diverse DR datasets prevents overfitting to a single domain and reduces shortcut learning.
- Core assumption: Combining diverse datasets creates a broader representation space that better captures the variability in real-world DR manifestations.
- Evidence anchors: [abstract] "implement a multi-source domain fine-tuning strategy to enhance generalization performance" and [section] "The intuition behind this second approach is that when training a model on a single dataset, it may overfit this specific domain distribution"

### Mechanism 3
- Claim: Modified GradRollout provides interpretable heatmaps for regression tasks.
- Mechanism: GradRollout adapts attention rollout by weighting attention matrices with gradients, creating high-resolution explainability for regression outputs.
- Core assumption: Gradients with respect to regression output effectively highlight relevant features for staging decisions.
- Evidence anchors: [section] "We adapted the grad-rollout [18] method to generate high-resolution explainability heatmaps for the regression task" and [section] "We hypothesized that, similarly to the classification setting, these gradients will emphasize the attribution of relevant information at each layer to a larger positive output"

## Foundational Learning

- Concept: Domain shift and generalization in medical imaging
  - Why needed here: The paper explicitly addresses failure to generalize due to distribution shifts between source and target domains
  - Quick check question: What happens when a model trained on US patients is deployed in India without domain adaptation?

- Concept: Self-supervised learning and transfer learning
  - Why needed here: The model uses DINOv2 pretrained via SSL on natural images, then fine-tuned for DR staging
  - Quick check question: How does SSL pretraining on natural images help when classifying fundus images?

- Concept: Ordinal regression vs classification
  - Why needed here: DR staging is treated as a regression task against ICDR grades rather than independent classes
  - Quick check question: Why might treating DR stages as ordinal regression be better than multi-class classification?

## Architecture Onboarding

- Component map: DINOv2 ViT-base (86M parameters) → FC regression head (2 layers, 512 hidden units, GeLU activation) → MSE loss
- Critical path: Input preprocessing → DINOv2 feature extraction → Regression head → Output scalar → MSE loss calculation
- Design tradeoffs: 
  - Large SSL model (DINOv2) provides better features but increases computational cost
  - Regression approach captures ordinal nature but may be harder to optimize than classification
  - Multi-source fine-tuning improves generalization but requires more data and computation
- Failure signatures: 
  - Poor performance on external datasets → domain shift or overfitting
  - Misclassifications far from diagonal → model not learning ordinal relationships
  - Heatmaps highlighting irrelevant regions → attention mechanism issues
- First 3 experiments:
  1. Single-source training on EYEPACS only, test on same dataset to establish baseline
  2. Single-source training on EYEPACS, test on external dataset to measure generalization gap
  3. Multi-source training on all datasets except one, test on held-out dataset to verify improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-source domain fine-tuning approach affect the model's performance on datasets with significantly different patient demographics or comorbidities not present in the training data?
- Basis in paper: [explicit] The paper discusses using multi-source domain fine-tuning across seven datasets with varying demographics, ethnicities, and comorbidities.
- Why unresolved: The paper does not provide specific results on how the model performs on datasets with unique demographics or comorbidities not included in the training set.
- What evidence would resolve it: Testing the model on datasets with distinct patient populations or rare comorbidities not represented in the training data.

### Open Question 2
- Question: Can the explainability heatmaps generated by DRStageNet be further improved to reduce false positives and enhance clinical interpretability?
- Basis in paper: [explicit] The paper discusses the current limitations of the explainability heatmaps, noting discrepancies and false positives.
- Why unresolved: The paper does not explore methods to improve the accuracy and reliability of the heatmaps.
- What evidence would resolve it: Developing and testing new explainability techniques or refining the existing method to produce more accurate and clinically relevant heatmaps.

### Open Question 3
- Question: How does the performance of DRStageNet compare to human experts in terms of diagnostic accuracy and inter-rater agreement?
- Basis in paper: [explicit] The paper mentions that most errors are one step off the diagonal, similar to human inter- and intra-rater inconsistency.
- Why unresolved: The paper does not provide a direct comparison between DRStageNet's performance and that of human experts.
- What evidence would resolve it: Conducting a study where DRStageNet's diagnoses are compared to those of a panel of experienced ophthalmologists, measuring both accuracy and agreement.

## Limitations
- Limited external validation with only one external dataset (APTOS 2019) used for validation
- No temporal validation to assess staging consistency over time for the same patient
- No clinical impact assessment through deployment studies or user studies

## Confidence

- Multi-source fine-tuning improves generalization: Medium confidence
- Self-supervised pretraining aids DR staging: Medium confidence  
- DRStageNet outperforms state-of-the-art: Medium confidence
- GradRollout provides interpretable heatmaps: Low confidence

## Next Checks

1. **External Multi-Center Validation**: Test DRStageNet on at least 3-5 additional external datasets from different geographic regions and imaging devices to verify true generalization capabilities.

2. **Temporal Stability Analysis**: Evaluate model performance on longitudinal fundus image pairs from the same patients taken months or years apart to assess staging consistency over time.

3. **Clinical Integration Study**: Conduct a prospective study with ophthalmologists using DRStageNet in a simulated clinical workflow to assess impact on diagnostic accuracy, efficiency, and user trust.