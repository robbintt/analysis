---
ver: rpa2
title: Enhancing Consistency in Multimodal Dialogue System Using LLM with Dialogue
  Scenario
arxiv_id: '2312.12808'
source_url: https://arxiv.org/abs/2312.12808
tags:
- dialogue
- sightseeing
- system
- user
- spots
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a dialogue system for a travel agency task
  in the Dialogue Robot Competition 2023. The system helps users plan visits to two
  Kyoto sightseeing spots by using a structured dialogue scenario to guide conversations,
  ensuring stable and consistent responses.
---

# Enhancing Consistency in Multimodal Dialogue System Using LLM with Dialogue Scenario

## Quick Facts
- arXiv ID: 2312.12808
- Source URL: https://arxiv.org/abs/2312.12808
- Reference count: 2
- Ranked 5th in impression evaluation and 6th in plan evaluation among 12 teams in the Dialogue Robot Competition 2023 preliminary round.

## Executive Summary
This paper presents a dialogue system for a travel agency task in the Dialogue Robot Competition 2023, designed to help users plan visits to two Kyoto sightseeing spots. The system uses a structured dialogue scenario to guide conversations and ensure stable, consistent responses. It employs GPT-3.5-turbo for response generation, integrating context-specific information about user requirements and sightseeing spots. Motion and speech controls are introduced to enhance user satisfaction by emphasizing key words and maintaining engagement. While the system excelled in naturalness and usefulness, there is room for improvement in satisfaction and reusability.

## Method Summary
The system uses a structured dialogue scenario to control conversation flow and generate consistent responses by feeding only relevant context and sightseeing spot information into GPT-3.5-turbo. The prompt design enforces scenario adherence by outputting both responses and dialogue acts. Motion (nodding, bowing, monitor gaze) and speech controls (emphasis, pronunciation guides) are applied to important words and sightseeing spot names to enhance user engagement and comprehension.

## Key Results
- Ranked 5th in impression evaluation and 6th in plan evaluation among 12 teams in the preliminary round.
- Excelled in naturalness and usefulness but had lower satisfaction and reusability scores.
- Successfully integrated motion and speech controls to emphasize key information and maintain user engagement.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured dialogue scenarios improve response consistency by constraining input context for the LLM.
- Mechanism: By using a predefined dialogue flow, the system feeds only relevant context and sightseeing spot data into GPT-3.5-turbo at each step, preventing context overload and maintaining focus on the task.
- Core assumption: The LLM's performance degrades when given long, mixed-context inputs; short, task-specific inputs yield higher quality and consistency.
- Evidence anchors:
  - [abstract] "responses may be compromised because the model must understand many contexts at once when processing the information"
  - [section II.A] "created a dialogue scenario to control the dialogue flow and generate consistent responses by inputting only the necessary dialogue context and sightseeing spot information"
- Break condition: If the scenario is too rigid, user engagement and satisfaction may drop.

### Mechanism 2
- Claim: Outputting both response and dialogue act forces the model to follow the intended flow and improves task adherence.
- Mechanism: The prompt design instructs the model to produce a response and a dialogue act simultaneously, constraining generation to the defined scenario.
- Core assumption: Explicit dialogue act generation aligns LLM output with the planned conversational structure.
- Evidence anchors:
  - [section II.B] "Simultaneously outputting the dialogue act and the response forces the model to generate a response that strictly follows the dialogue flow specified"
- Break condition: If the dialogue act is too prescriptive, natural conversational flow may suffer.

### Mechanism 3
- Claim: Motion and speech control enhance user satisfaction by emphasizing key information and maintaining engagement.
- Mechanism: Three types of motion (nodding, bowing, monitor gaze) and three levels of speech emphasis (volume, speed, pauses) are applied to important words and sightseeing spot names.
- Core assumption: Users better understand and retain information when key words are highlighted via multimodal cues.
- Evidence anchors:
  - [section II.C] "Motion and speech control were used to improve user satisfaction... emphasize important words based on system utterances and user situations"
- Break condition: Insufficient variation in motion or over-emphasis in speech can reduce naturalness and user immersion.

## Foundational Learning

- Concept: Dialogue scenario design for task-oriented dialogue systems
  - Why needed here: Provides structured flow to guide LLM responses and prevent context overload
  - Quick check question: What are the key steps in the dialogue scenario for this travel agency task?

- Concept: Prompt engineering for controlled LLM output
  - Why needed here: Ensures responses follow the scenario and include dialogue acts
  - Quick check question: How does the prompt format in Fig. 2 enforce scenario adherence?

- Concept: Multimodal interaction design (motion + speech control)
  - Why needed here: Enhances user engagement and comprehension in human-robot dialogue
  - Quick check question: Which words/phrases receive emphasis via motion and speech cues?

## Architecture Onboarding

- Component map:
  Dialogue Scenario Controller -> GPT-3.5-turbo -> Sightseeing Spot Search API -> Motion Control Module -> Speech Control Module -> User Interface

- Critical path:
  1. Icebreaker chat -> Interview 1 (requirements)
  2. GPT-3.5-turbo + scenario -> keywords -> search API
  3. Present spots + recommendation -> Interview 2 (second spot)
  4. Confirm plan -> Closing

- Design tradeoffs:
  - Structured scenario vs. conversational flexibility (affects satisfaction)
  - Context length vs. response quality (affects consistency)
  - Emphasis level vs. naturalness (affects user immersion)

- Failure signatures:
  - Plan evaluation low despite high impression scores -> possible scenario rigidity or unrealistic travel distances
  - Low satisfaction and reusability -> lack of conversational flexibility or insufficient engagement cues

- First 3 experiments:
  1. Vary prompt length and context specificity to test impact on response consistency
  2. Adjust motion frequency and speech emphasis to optimize satisfaction scores
  3. Modify scenario to allow user-initiated topic changes and measure naturalness and reusability impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the dialogue system improve user satisfaction and reusability by addressing the perceived constraints of the structured dialogue scenario?
- Basis in paper: [explicit] The paper mentions that the structured dialogue scenario may cause users to feel constrained, reducing satisfaction, and notes insufficient user immersion due to minimal motion or eye contact.
- Why unresolved: The authors acknowledge these issues but do not propose specific solutions or evaluate alternatives to the current structured scenario or motion controls.
- What evidence would resolve it: Empirical studies comparing user satisfaction and reusability scores between the current system and variants with more flexible dialogue flows, enhanced motion controls, or adaptive eye contact mechanisms.

### Open Question 2
- Question: How does the length and complexity of the dialogue context affect the quality and consistency of responses generated by GPT-3.5-turbo in multimodal dialogue systems?
- Basis in paper: [explicit] The paper states that long inputs may compromise the quality and consistency of generated responses because the model must understand many contexts simultaneously.
- Why unresolved: While the paper uses a dialogue scenario to mitigate this issue, it does not empirically test the impact of input length on response quality or explore methods to optimize input processing.
- What evidence would resolve it: Comparative analysis of response quality metrics (e.g., coherence, relevance) when varying the length and complexity of input contexts in the system.

### Open Question 3
- Question: How effective is the integration of motion and speech controls in enhancing user understanding and engagement in multimodal dialogue systems?
- Basis in paper: [explicit] The paper introduces motion and speech controls to emphasize important words and improve user satisfaction but does not provide quantitative evidence of their effectiveness.
- Why unresolved: The paper does not include user studies or metrics to measure the impact of these controls on user understanding or engagement.
- What evidence would resolve it: User studies measuring comprehension, engagement, and satisfaction with and without the motion and speech controls in place.

## Limitations

- No quantitative evaluation of consistency improvements; assessment relies entirely on subjective competition rankings.
- Lack of ablation studies to isolate the contribution of each mechanism to observed results.
- Task-specific design constraints; effectiveness for other domains or more complex planning tasks remains unverified.

## Confidence

- High Confidence: The basic architecture (LLM + structured scenario + multimodal output) is correctly described and technically sound.
- Medium Confidence: The mechanism claims about scenario-driven consistency and prompt-enforced dialogue acts are plausible based on standard LLM behavior patterns, but lack direct experimental validation within the paper.
- Low Confidence: Claims about multimodal enhancement effectiveness are weakest, as no user study or objective measures are provided beyond subjective satisfaction rankings.

## Next Checks

1. Conduct an ablation study measuring response consistency (e.g., task completion rate, dialogue act accuracy) when removing structured scenarios versus using free-form LLM responses.
2. Perform a controlled user study comparing user satisfaction and comprehension with/without multimodal emphasis (motion and speech control) while keeping dialogue content constant.
3. Test scenario flexibility by introducing user-initiated topic changes and measuring impacts on naturalness scores and task completion rates.