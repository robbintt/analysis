---
ver: rpa2
title: Iterative Shallow Fusion of Backward Language Model for End-to-End Speech Recognition
arxiv_id: '2310.11010'
source_url: https://arxiv.org/abs/2310.11010
tags:
- partial
- pblm
- speech
- decoding
- proc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces iterative shallow fusion (ISF) with a backward
  language model (BLM) for end-to-end speech recognition. Unlike traditional shallow
  fusion with a forward LM, ISF applies the BLM iteratively during decoding to improve
  hypothesis scoring.
---

# Iterative Shallow Fusion of Backward Language Model for End-to-End Speech Recognition

## Quick Facts
- arXiv ID: 2310.11010
- Source URL: https://arxiv.org/abs/2310.11010
- Reference count: 0
- Key outcome: Iterative shallow fusion (ISF) with backward language model (BLM) improves end-to-end speech recognition by preventing early hypothesis pruning during decoding, with performance comparable to forward LM fusion and further improvement when combined.

## Executive Summary
This paper introduces iterative shallow fusion (ISF) with a backward language model (BLM) for end-to-end speech recognition. Unlike traditional shallow fusion that integrates a forward language model during decoding, ISF applies the BLM iteratively to recalculate scores for partial hypotheses, preventing premature pruning of potentially correct sequences. A partial sentence-aware BLM (PBLM) is trained on reversed text data including partial sentences to enhance ISF effectiveness. Experiments on TED-LIUM2 demonstrate that ISF with PBLM achieves performance comparable to forward LM fusion and further improves when combined, while limiting ISF to early decoding stages with post-processing yields good results while reducing computational cost.

## Method Summary
The method combines a Conformer-Transformer ASR system with external language models integrated through shallow fusion and iterative shallow fusion during decoding. The base ASR model uses a Conformer encoder, Transformer decoder, and CTC module trained on the TED-LIUM2 corpus. Three external LSTMLMs are trained: a forward LM (FLM), a backward LM (BLM), and a partial sentence-aware BLM (PBLM). ISF applies the BLM iteratively during beam search decoding to recalculate scores for partial hypotheses before beam pruning decisions. The PBLM is trained on reversed text data including partial sentences to better handle the incomplete sequences encountered during ISF. The method also explores limiting ISF application to early decoding stages and combining SF with FLM and ISF with PBLM to leverage complementary information.

## Key Results
- ISF with PBLM achieves word error rates comparable to traditional shallow fusion with FLM on TED-LIUM2
- Combining SF-FLM and ISF-PBLM yields further performance improvement due to complementary information capture
- Limiting ISF to early decoding stages with post-processing reduces computational cost while maintaining good accuracy
- Performance is sensitive to the interval parameter I for ISF application, with smaller intervals generally performing better

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative shallow fusion (ISF) using a backward language model (BLM) prevents early pruning of promising hypotheses during beam search.
- Mechanism: By applying the BLM iteratively in the backward direction during decoding, ISF recalculates scores for partial hypotheses that might otherwise be discarded early due to their incomplete forward context. This gives the decoder more information about potential word completions before making irreversible pruning decisions.
- Core assumption: The backward LM captures useful complementary context that is not available in the forward direction during early decoding stages.
- Evidence anchors:
  - [abstract] "By performing ISF, early pruning of prospective hypotheses can be prevented during decoding"
  - [section] "we expect that, by applying the BLM along with an FLM during decoding... more accurate LM scores can be given to partial ASR hypotheses, and thus we can prevent incorrect early beam pruning of the prospective hypotheses"

### Mechanism 2
- Claim: The partial sentence-aware BLM (PBLM) trained on reversed partial sentences is more effective for ISF than a standard BLM trained only on complete sentences.
- Mechanism: The PBLM learns to model sequences that start mid-sentence and proceed backward, matching the partial hypotheses encountered during ISF. This allows more accurate scoring of incomplete sequences compared to a BLM that only knows how to handle full sentences.
- Core assumption: The distribution of partial sequences during ISF is sufficiently similar to naturally occurring sentence fragments to benefit from specialized training.
- Evidence anchors:
  - [abstract] "we train a partial sentence-aware BLM (PBLM) using reversed text data including partial sentences, considering the framework of ISF"
  - [section] "a normal BLM... is not necessarily suitable for ISF, since it models token sequences starting from the end of complete sentences, whereas we need to apply it to partial hypotheses"

### Mechanism 3
- Claim: Combining SF with FLM and ISF with PBLM yields better performance than either method alone due to complementary information capture.
- Mechanism: The FLM provides forward context information that helps with prefix prediction, while the PBLM provides backward context information that helps with suffix prediction and overall sequence coherence. Together they cover both directions of dependency.
- Core assumption: The forward and backward LMs capture orthogonal information that is both useful for speech recognition.
- Evidence anchors:
  - [abstract] "by combining SF and ISF, further performance improvement can be obtained thanks to the complementarity of the FLM and BLM"
  - [section] "we intend to obtain performance improvement based on the complementarity of the FLM and the BLM, whose effectiveness has been confirmed in many studies for post-processing"

## Foundational Learning

- Concept: Beam search decoding with language model integration
  - Why needed here: The paper relies on understanding how beam search works and how language models are integrated during decoding through shallow fusion.
  - Quick check question: How does shallow fusion modify the scoring function during beam search compared to vanilla beam search?

- Concept: Forward vs backward language models
  - Why needed here: The paper exploits the complementary nature of forward (predicting next word) and backward (predicting previous word) LMs, which requires understanding their directional differences.
  - Quick check question: What is the fundamental difference in what forward and backward LMs model, and why does this matter for partial hypothesis scoring?

- Concept: Sequence-to-sequence ASR with attention mechanisms
  - Why needed here: The paper uses an attention-based encoder-decoder (AED) ASR system as the base model, so understanding how these models generate hypotheses is crucial.
  - Quick check question: In an attention-based ASR model, how does the decoder generate hypotheses token by token, and at what point can a language model be integrated?

## Architecture Onboarding

- Component map: Input speech → Conformer encoder → Transformer decoder (with FLM scores) → Iterative BLM scoring (PBLM) → Hypothesis selection → Output transcript
- Critical path: The audio signal flows through the Conformer encoder for feature extraction, then through the Transformer decoder with FLM integration for initial hypothesis generation, followed by iterative BLM scoring for partial hypotheses, and finally hypothesis selection to produce the output transcript.
- Design tradeoffs: The ISF approach trades computational cost (iterative scoring of partial sequences) for potential accuracy gains by preventing premature hypothesis pruning. The PBLM adds training complexity but enables more effective ISF.
- Failure signatures: Degradation in WER when interval I is too large (infrequent ISF application), when limiting hypothesis length L is too aggressive (premature termination of ISF), or when PBLM is poorly trained on partial sequences.
- First 3 experiments:
  1. Replicate baseline SF-FLM performance to establish the reference point
  2. Implement ISF-PBLM with I=1 and no length limitation to test the core mechanism
  3. Test the combined SF-FLM+ISF-PBLM approach to verify the complementarity claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of iterative shallow fusion (ISF) with backward language models (BLM) compare to other advanced language model integration techniques like density ratio approaches or internal language model estimation?
- Basis in paper: [explicit] The paper mentions these methods as alternatives that outperform traditional shallow fusion and inherit its simplicity, but does not compare their performance to ISF.
- Why unresolved: The paper focuses on comparing ISF with traditional shallow fusion using forward LMs and post-processing with BLMs, but does not benchmark against more recent integration methods.
- What evidence would resolve it: Direct experimental comparisons of ISF performance against density ratio approaches and internal language model estimation methods on the same ASR tasks and datasets.

### Open Question 2
- Question: What is the optimal interval (I) for applying ISF during decoding, and how does this parameter affect computational cost versus accuracy trade-offs?
- Basis in paper: [explicit] The paper experiments with intervals I = 1, 2, 5, 10, and ∞, finding that smaller intervals generally perform better, but does not determine an optimal value or provide a systematic analysis of the cost-accuracy relationship.
- Why unresolved: While the paper shows trends in performance with different intervals, it does not provide a comprehensive analysis of the trade-offs or identify an optimal interval setting.
- What evidence would resolve it: A detailed study varying the interval parameter across a wider range and analyzing the relationship between computational cost, accuracy, and interval size, potentially including adaptive interval strategies.

### Open Question 3
- Question: How does limiting the length of partial hypotheses (L) to apply ISF affect the overall performance, and what is the optimal value of L for different ASR tasks and datasets?
- Basis in paper: [explicit] The paper investigates limiting L to reduce computational cost and finds that limiting to about half the average sentence length (30 tokens) with post-processing achieves good results, but does not determine an optimal L or analyze its effects across different tasks.
- Why unresolved: The paper provides initial results for one dataset but does not explore how L affects performance across different ASR tasks, languages, or dataset characteristics.
- What evidence would resolve it: Systematic experiments varying L across multiple ASR tasks and datasets, analyzing the relationship between L, computational cost, and accuracy, and identifying task-specific optimal values.

## Limitations

- Narrow experimental scope with only one ASR architecture (Conformer-Transformer) and one benchmark dataset (TED-LIUM2) limits generalization claims.
- Lack of direct ablation comparing PBLM versus BLM performance in ISF weakens justification for the partial sentence-aware variant.
- Missing critical implementation details including exact iterative shallow fusion algorithm, weighting factor optimization, and hyperparameter settings.

## Confidence

- High confidence in the core ISF mechanism: The iterative application of backward LM during decoding is well-defined and the computational cost reduction strategies are explicitly described.
- Medium confidence in PBLM effectiveness: While the conceptual motivation is sound, there is no direct ablation showing PBLM outperforms BLM in ISF.
- Medium confidence in complementarity of FLM and PBLM: The claim is supported by combined performance improvement but lacks detailed ablation studies.
- Low confidence in generalization: Claims about broad applicability are not supported by experiments beyond TED-LIUM2.

## Next Checks

1. **Ablation Study on LM Variants**: Conduct controlled experiments comparing ISF performance with standard BLM versus PBLM to directly validate whether the partial sentence-aware training provides measurable benefits.

2. **Cross-Domain Robustness Test**: Evaluate the combined SF-FLM+ISF-PBLM approach on at least two additional ASR datasets from different domains to assess generalization and identify potential overfitting.

3. **Delayed Fusion Comparison**: Implement and compare against delayed fusion techniques on the same datasets and metrics, particularly examining scenarios where post-decoding rescoring might outperform or complement the first-pass ISF approach.