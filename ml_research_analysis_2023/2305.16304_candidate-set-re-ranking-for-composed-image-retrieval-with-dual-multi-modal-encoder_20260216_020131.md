---
ver: rpa2
title: Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal
  Encoder
arxiv_id: '2305.16304'
source_url: https://arxiv.org/abs/2305.16304
tags:
- image
- candidate
- retrieval
- re-ranking
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-stage model for composed image retrieval.
  The first stage filters out easy negative candidates using vector distance, while
  the second stage re-ranks the remaining candidates by explicitly matching the reference
  image, text query, and candidate image triplet.
---

# Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder

## Quick Facts
- arXiv ID: 2305.16304
- Source URL: https://arxiv.org/abs/2305.16304
- Reference count: 40
- Two-stage model achieves 71.34% Recall@50 on Fashion-IQ, outperforming existing approaches

## Executive Summary
This paper introduces a two-stage composed image retrieval method that first filters candidates using vector distance metrics and then re-ranks the remaining candidates using a dual-encoder architecture. The approach effectively separates easy and hard negatives, allowing the re-ranking stage to focus computational resources on candidates requiring fine-grained multi-modal reasoning. Experiments on Fashion-IQ and CIRR datasets demonstrate state-of-the-art performance, with Recall@50 improving from 69.68% to 71.34% on Fashion-IQ.

## Method Summary
The method employs a two-stage pipeline for composed image retrieval. Stage 1 uses a vision-and-language pre-trained model (BLIP) to filter candidates by computing cosine similarity between query embeddings and pre-computed candidate embeddings, reducing the candidate set to a manageable size. Stage 2 employs a dual-encoder architecture that separately attends to reference image and text query features, allowing fine-grained interaction with the filtered candidates. The dual-encoder uses zt features from Stage 1 to avoid memory limitations while preserving reference image information.

## Key Results
- Achieves 71.34% Recall@50 on Fashion-IQ dataset, outperforming existing methods
- Improves Recall@50 from 69.68% to 71.34% compared to baseline approaches
- Demonstrates effective separation of easy and hard negatives through two-stage filtering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-filtering removes easy negatives and reduces candidate set size
- Mechanism: Uses fast vector distance metrics to prune majority of candidates
- Core assumption: Easy and hard negatives can be distinctly separated
- Evidence anchors:
  - [abstract]: "We observe that for composed image retrieval, easy and hard negatives can be distinctly separated."
  - [section 3.1]: "The first stage of our approach aims to filter out the majority of candidates leaving only a few of the more difficult candidates for further analysis in the second step."
- Break condition: If easy/hard negatives cannot be reliably distinguished

### Mechanism 2
- Claim: Dual-encoder allows separate attention to reference image and query text
- Mechanism: Parallel encoders process reference-candidate and text-candidate interactions
- Core assumption: Separate attention improves discrimination over joint encoding
- Evidence anchors:
  - [section 3.2]: "The intuition is to allow I ′ T to separately attend to the two elements in each query q for relevancy, namely t and IR."
  - [abstract]: "Our second stage employs a dual-encoder architecture, which effectively attends to the input triplet of reference-text-candidate and re-ranks the candidates."
- Break condition: If merging layer fails to combine encoder outputs effectively

### Mechanism 3
- Claim: Using zt as IR surrogate avoids memory limitations while preserving information
- Mechanism: First stage outputs zt containing reference image information for cross-attention
- Core assumption: zt retains sufficient reference image information for effective re-ranking
- Evidence anchors:
  - [section 3.2]: "Since we do not further finetune the Candidate Filtering model in the second stage, zt can be precomputed for each query of q = ( IR, t)."
  - [section 3.2]: "Empirically, we confirm that our design choices yield a better result."
- Break condition: If zt loses critical reference image information

## Foundational Learning

- Vision-and-Language Pre-trained Models
  - Why needed here: Method relies on BLIP's pre-trained image and text encoders for both stages
  - Quick check question: Can you explain how BLIP's cross-attention layer enables image-text interaction in the filtering stage?

- Contrastive Learning
  - Why needed here: Both stages use contrastive loss functions to learn discriminative embeddings
  - Quick check question: What is the difference between loss functions used in filtering and re-ranking stages?

- Transformer Architecture
  - Why needed here: Dual-encoder design uses transformer layers with cross-attention for multi-modal reasoning
  - Quick check question: How does cross-attention in re-ranking model differ from standard self-attention?

## Architecture Onboarding

- Component map:
  - Query (reference image + text) → Stage 1 (BLIP encoder → cosine similarity → top-K candidates) → Stage 2 (Dual-encoder → cross-attention → merging → MLP scorer) → Final ranking

- Critical path:
  1. Query (IR + t) → Stage 1 → top-K candidates
  2. Query + top-K candidates → Stage 2 → re-ranked candidates
  3. Final ranking based on Stage 2 scores

- Design tradeoffs:
  - Pre-filtering vs. exhaustive triplet scoring: Trade inference time for improved discrimination
  - Using zt vs. direct image features: Trade memory efficiency for potential information loss
  - Dual-encoder vs. single-encoder: Trade model complexity for potentially better multi-modal reasoning

- Failure signatures:
  - Stage 1: Low ground truth coverage in top-K candidates indicates filtering too aggressive
  - Stage 2: Poor ranking performance despite good filtering suggests dual-encoder design issues
  - Overall: Significant performance gap between stages suggests architectural mismatch

- First 3 experiments:
  1. Baseline: Implement single-stage model with direct triplet scoring to establish performance upper bound
  2. Filter-only: Test candidate filtering stage alone to verify ground truth coverage and speed
  3. Dual-encoder ablation: Compare dual-encoder design against single-encoder baseline with same merging strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does choice of pre-trained VLP model affect two-stage CIR approach performance?
- Basis in paper: [explicit] States "we base our designs on the pre-trained vision-and-language (VLP) network BLIP [23], though other VLP models might be used"
- Why unresolved: Only experiments with BLIP, no comparison against CLIP, ALIGN, or Florence
- What evidence would resolve it: Empirical comparison using different VLP backbones on same datasets

### Open Question 2
- Question: What is the impact of different image encoder configurations (e.g., ViT-B vs. ViT-L) on CIR performance?
- Basis in paper: [explicit] States "We initialize the image and text encoders with the BLIP w/ ViT-B pre-trained weights"
- Why unresolved: Only uses ViT-B variant, larger models like ViT-L not tested
- What evidence would resolve it: Performance comparison using different ViT sizes while keeping other components constant

### Open Question 3
- Question: How sensitive is two-stage approach to choice of K (number of candidates filtered for re-ranking)?
- Basis in paper: [explicit] Conducts ablation studies on K values showing performance varies
- Why unresolved: Doesn't systematically explore full range of K values or provide principled method for choosing K
- What evidence would resolve it: Comprehensive analysis of performance vs. K across wider range of values

## Limitations
- Qualitative observations about easy/hard negative separation lack quantitative validation
- Dual-encoder architecture superiority lacks ablation studies comparing different merging strategies
- Using zt as reference image surrogate is empirically justified but not theoretically analyzed for information preservation

## Confidence
- **High Confidence**: Two-stage pipeline design and overall architecture are well-specified and reproducible
- **Medium Confidence**: Empirical performance improvements on Fashion-IQ and CIRR datasets are credible
- **Low Confidence**: Theoretical justification for dual-encoder superiority and information preservation in zt

## Next Checks
1. **Negative Separation Validation**: Conduct quantitative analysis of easy vs. hard negative separation by measuring correlation between vector distance metrics and final re-ranking scores
2. **Dual-Encoder Ablation**: Implement and compare re-ranking stage using single-encoder with joint attention versus proposed dual-encoder design
3. **zt Information Retention**: Compare re-ranking performance when using raw reference image features versus zt embeddings from filtering stage