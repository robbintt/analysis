---
ver: rpa2
title: Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based
  Reachability Analysis
arxiv_id: '2309.07675'
source_url: https://arxiv.org/abs/2309.07675
tags:
- learning
- representation
- goal
- gara
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hierarchical reinforcement learning algorithm,
  GARA, that learns a symbolic goal representation from scratch while simultaneously
  learning a hierarchical policy. The key innovation is using reachability analysis
  to group states into symbolic goals based on their reachability relations, enabling
  interpretable, transferable, and data-efficient learning.
---

# Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis

## Quick Facts
- arXiv ID: 2309.07675
- Source URL: https://arxiv.org/abs/2309.07675
- Reference count: 33
- Key outcome: GARA learns a symbolic goal representation that decomposes task structure and transfers well to new environments, achieving better performance than state-of-the-art HRL methods like HIRO.

## Executive Summary
This paper introduces GARA (Goal space Abstraction via Reachability Analysis), a hierarchical reinforcement learning algorithm that learns a symbolic goal representation from scratch while simultaneously learning a hierarchical policy. The key innovation uses reachability analysis to group states into symbolic goals based on their reachability relations, enabling interpretable, transferable, and data-efficient learning. Experiments on complex navigation tasks demonstrate that GARA learns a goal representation that decomposes the task structure and transfers well to new environments, achieving better performance than state-of-the-art HRL methods like HIRO.

## Method Summary
GARA implements a feudal HRL architecture where a high-level Q-learning agent selects target sets (goals) from a dynamically learned partition of the state space, and a low-level UVFA learns policies to achieve those goals. The algorithm starts with a single partition {S} and refines it through iterative reachability analysis using a k-forward model that approximates transition relations among sets of states. The partition refinement process splits sets when they mostly intersect or miss target sets, capturing the bisimulation relation among states. The learned symbolic representation is interpretable as a directed graph where nodes describe goals and edges capture reachability relations among them.

## Key Results
- GARA learns a successful hierarchical policy with performance approaching handcrafted representations on U-shaped and 4-rooms mazes
- The learned representation transfers well to more complex environments with minimal additional training
- GARA achieves better sample efficiency and performance compared to HIRO and other baseline HRL methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GARA learns a symbolic goal representation by iteratively refining a partition of the state space based on reachability relations.
- Mechanism: The algorithm starts with a single partition {S} and refines it by identifying states that reach the same target set within k steps. It uses a k-forward model to approximate reachability and splits sets when they mostly intersect or mostly miss the target set.
- Core assumption: The reachability relation Rk can be approximated by a neural network trained on data collected during episodes.
- Evidence anchors:
  - [abstract] "The algorithm uses symbolic reachability analysis for neural networks to approximate the transition relation among sets of states and to refine the goal representation."
  - [section III.A] "GARA uses the result of each episode, stored in the memory D (Line 16), to train the k-forward model Fk"
  - [corpus] Weak - no direct matches to the specific mechanism, but related papers discuss reachability analysis for neural networks.

### Mechanism 2
- Claim: The learned symbolic representation enables data-efficient learning by decomposing the task into manageable subgoals.
- Mechanism: By grouping states that behave similarly (i.e., reach the same target sets), the high-level agent can select goals that are reachable from the current state, reducing exploration and improving sample efficiency.
- Core assumption: The bisimulation relation among states (states that behave similarly) is preserved by the partition refinement process.
- Evidence anchors:
  - [abstract] "This discretisation of the environment is used to orient top-down process of the goal-directed exploration"
  - [section II.B] "The abstract goal space, the partition G, satisfies the property: ∀ G, G′ ∈ G .Rk(G, G′) ⊆ G′"
  - [section IV.B] "GARA learns a successful hierarchical policy with a performance approaching the handcrafted representation"

### Mechanism 3
- Claim: The symbolic representation is transferable to new environments because it captures the task structure rather than specific state configurations.
- Mechanism: When transferred to a new environment, GARA refines the existing partition to adapt to the new geometry while preserving the reachability relations that capture the task structure.
- Core assumption: The task structure (e.g., navigating from one room to another) is preserved across similar environments, allowing the transferred representation to be refined rather than learned from scratch.
- Evidence anchors:
  - [abstract] "Experiments on complex navigation tasks demonstrate that GARA learns a goal representation that decomposes the task structure and transfers well to new environments"
  - [section IV.B] "GARA successfully leverages the learned representation and adapts it to solve a more complex task"

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: GARA operates in a continuous state space environment modeled as an MDP, where the agent learns a policy to maximize expected future reward.
  - Quick check question: What are the components of an MDP and how do they relate to the agent's decision-making process?

- Concept: Hierarchical Reinforcement Learning (HRL)
  - Why needed here: GARA is a feudal HRL algorithm that decomposes the task into subtasks managed by high-level and low-level agents.
  - Quick check question: How does feudal HRL differ from standard RL, and what are the roles of the high-level and low-level agents?

- Concept: Universal Value Function Approximators (UVFAs)
  - Why needed here: The low-level policy in GARA is a UVFA that takes the goal as an additional parameter, allowing it to learn different behaviors depending on the target set.
  - Quick check question: How does a UVFA differ from a standard value function approximator, and why is it useful in goal-conditioned RL?

## Architecture Onboarding

- Component map:
  - High-level agent: Q-learning agent that selects target sets (goals) from the current partition
  - Low-level agent: UVFA that learns to reach the selected target set
  - k-forward model: Neural network that approximates the reachability relation Rk
  - Partition refinement: Algorithm that splits the state space based on reachability analysis
  - Goal space abstraction: The learned symbolic representation of the environment

- Critical path:
  1. Initialize partition G = {S}
  2. Select initial state and target set using high-level policy
  3. Execute low-level policy to reach target set
  4. Store episode data in memory D
  5. Update k-forward model Fk with data from D
  6. Refine partition G using reachability analysis and Fk
  7. Repeat until convergence

- Design tradeoffs:
  - Coarse vs. fine partition: A coarser partition may lead to faster learning but less precise control, while a finer partition may capture the task structure better but require more data.
  - k value: A larger k may capture longer-term dependencies but may also lead to more complex reachability relations that are harder to approximate.
  - Transfer vs. from-scratch: Transferring a learned representation may speed up learning in a new environment but may also lead to suboptimal performance if the task structure differs significantly.

- Failure signatures:
  - Poor performance: The learned representation may not capture the task structure, leading to inefficient exploration and slow learning.
  - Instability: The partition refinement may lead to an unstable representation that changes frequently, making it difficult for the high-level agent to learn a stable policy.
  - Non-transferability: The learned representation may not transfer well to new environments, requiring extensive refinement or relearning.

- First 3 experiments:
  1. Evaluate GARA on a simple navigation task with a clear task structure (e.g., a grid world with distinct rooms) to verify that it learns a meaningful symbolic representation.
  2. Compare GARA's performance to a baseline HRL algorithm (e.g., HIRO) on a more complex navigation task to demonstrate the benefits of the learned representation.
  3. Transfer the learned representation from a simple task to a more complex one and evaluate whether GARA can adapt the representation to the new environment.

## Open Questions the Paper Calls Out

- How does GARA's performance scale with increasing dimensionality of the state space and complexity of the environment?
- How sensitive is GARA's performance to the choice of hyperparameters, particularly the threshold values for the SplitSet function?
- How does GARA's learned goal representation compare to other state abstraction methods in terms of interpretability and transfer learning capabilities?
- How does the choice of the k-forward model's architecture and training procedure affect GARA's performance and the quality of the learned goal representation?

## Limitations

- The reachability analysis relies on a k-forward model that may fail in environments with stochasticity or high-dimensional state spaces
- The partition refinement process requires careful tuning of hyperparameters (tre, tunref) that are not fully specified
- Transfer experiments only demonstrate adaptation to structurally similar environments, not environments with fundamentally different task structures

## Confidence

- High Confidence: The hierarchical architecture and basic learning algorithm are well-specified and reproducible. The experimental setup with navigation tasks and comparison baselines is clearly defined.
- Medium Confidence: The reachability analysis mechanism and partition refinement process are described with sufficient detail for implementation, though some implementation details (particularly FnnReach) require additional specification.
- Low Confidence: Claims about data efficiency improvements and transferability are supported by limited experiments. The paper shows benefits on specific navigation tasks but doesn't establish generalizability to other domains or more complex environmental variations.

## Next Checks

1. Implement GARA on a simple grid world with explicit reachability analysis to verify the partition refinement process works as intended before scaling to complex navigation tasks.

2. Conduct systematic ablation studies varying the k parameter and partition refinement thresholds to understand their impact on learning stability and final performance.

3. Test transfer capability on environments with structural differences (e.g., different room layouts, varying obstacle configurations) to establish the boundaries of transferability beyond the reported similar-structure adaptations.