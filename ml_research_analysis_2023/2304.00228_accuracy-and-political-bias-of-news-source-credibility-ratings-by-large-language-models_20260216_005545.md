---
ver: rpa2
title: Accuracy and Political Bias of News Source Credibility Ratings by Large Language
  Models
arxiv_id: '2304.00228'
source_url: https://arxiv.org/abs/2304.00228
tags:
- chatgpt
- ratings
- sources
- news
- credibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the ability of nine large language models (LLMs)
  from OpenAI, Google, and Meta to assess the credibility of news sources. The authors
  find that while LLMs can rate most tested news outlets, larger models more frequently
  refuse to provide ratings due to insufficient information, whereas smaller models
  are more prone to making errors.
---

# Accuracy and Political Bias of News Source Credibility Ratings by Large Language Models

## Quick Facts
- arXiv ID: 2304.00228
- Source URL: https://arxiv.org/abs/2304.00228
- Reference count: 28
- Key outcome: LLMs show high inter-model agreement (ρ=0.79) but only moderate alignment with human expert ratings (ρ=0.50), with liberal bias in default configurations

## Executive Summary
This paper examines nine large language models' ability to assess news source credibility, finding that while models can rate most sources, larger models more frequently refuse due to insufficient information while smaller models make more errors. The models exhibit high agreement among themselves but only moderate alignment with human expert evaluations. The study also reveals that all LLMs demonstrate liberal bias in credibility ratings when in default configurations, and assigning partisan roles consistently induces strong politically congruent bias.

## Method Summary
The study tested nine LLMs from OpenAI, Google, and Meta on their ability to rate the credibility of 7,523 news domains ranked by popularity. Models were prompted to provide credibility ratings on a 0-1 scale, which were then compared against human expert ratings from three sources: Lin et al.'s aggregate list, NewsGuard, and Media Bias/Fact Check. The researchers calculated Spearman correlation coefficients, AUC scores for binary classification, and analyzed political bias by examining sources with different US political leanings.

## Key Results
- LLMs exhibit high inter-model agreement (average Spearman's ρ=0.79) but moderate alignment with human expert evaluations (average ρ=0.50)
- Larger models more frequently refuse to rate sources due to insufficient information, while smaller models make more errors
- All LLMs show liberal bias in credibility ratings when using default configurations
- Assigning partisan roles to LLMs consistently induces strong politically congruent bias in their ratings

## Why This Works (Mechanism)

### Mechanism 1
LLMs can serve as an affordable reference for credibility ratings by leveraging their pre-trained knowledge about news sources. The models internalize patterns from training data that associate certain source characteristics with credibility, allowing them to generate ratings without external verification. This works under the assumption that the training corpus contained sufficient signal about source credibility for the model to learn these associations.

### Mechanism 2
Assigning partisan roles to LLMs induces congruent bias in their ratings by activating different response patterns in the model, causing it to adopt the perspective of the assigned role when evaluating sources. This mechanism assumes the model has learned to adjust its responses based on role assignments during training.

### Mechanism 3
Larger LLMs more frequently refuse to rate sources due to insufficient information, while smaller models make more errors because larger models have better calibration about their knowledge boundaries and are more likely to decline uncertain tasks, while smaller models attempt to complete tasks beyond their knowledge scope.

## Foundational Learning

- **Correlation vs. Agreement**: Understanding why high inter-LLM agreement (ρ=0.79) doesn't imply accurate ratings compared to human experts (ρ=0.50). *Quick check*: If all models agree with each other but disagree with humans, what does this tell us about their reliability?

- **Zero-shot learning**: The models perform ratings without task-specific training, relying on general knowledge. *Quick check*: What are the limitations of zero-shot evaluation compared to fine-tuned models for source credibility?

- **Binary classification thresholds**: Understanding how to convert continuous credibility scores into actionable binary labels. *Quick check*: Why might a threshold of 0.5 not be optimal for all credibility rating systems?

## Architecture Onboarding

- **Component map**: API endpoint → Prompt template → Response parser → Correlation calculator → Visualization generator
- **Critical path**: Domain input → LLM query → JSON parsing → Statistical analysis → Result aggregation
- **Design tradeoffs**: Speed vs. cost (concurrent queries reduce time but increase API cost); Accuracy vs. coverage (stricter prompts reduce errors but may miss sources)
- **Failure signatures**: Invalid JSON responses; Missing domain information; Correlation calculations with insufficient data points
- **First 3 experiments**:
  1. Test prompt variations with 10 domains to identify optimal formatting
  2. Compare ratings across different temperature settings (0.0 vs 0.7) with 50 domains
  3. Evaluate model refusal rates by incrementally increasing domain obscurity in batches of 100

## Open Questions the Paper Calls Out

### Open Question 1
How can LLMs be trained to better align their credibility ratings with human expert judgments? The paper suggests future LLMs should enhance alignment by explicitly integrating human expert judgments into both training and inference phases, but the exact mechanism and effectiveness remain unexplored.

### Open Question 2
Can LLMs effectively identify satirical sources and distinguish them from low-credibility sources? While ChatGPT recognizes 77.4% of satirical sources, some are misidentified as posting misleading news, and the paper doesn't explore reasons for misidentifications or improvement methods.

### Open Question 3
How do LLMs perform in assessing the credibility of non-English news sources compared to English sources? The paper finds ChatGPT's performance on non-English sources is generally consistent with English sources, with Italian-language sources being an exception, but doesn't explore factors contributing to performance differences across languages.

## Limitations

- Limited source coverage using only 7,523 domains from a popularity-based ranking, potentially excluding niche but credible sources
- Unclear methodology for role assignment to induce partisan bias in LLMs
- Potential confounding of model size effects with architectural differences between providers

## Confidence

- **High confidence**: LLMs show high inter-model agreement (ρ=0.79) but only moderate alignment with human experts (ρ=0.50)
- **Medium confidence**: All LLMs demonstrate liberal bias in default configurations
- **Medium confidence**: Larger models refuse more tasks while smaller models make more errors

## Next Checks

1. Systematically test the ChatGPT prompt across all nine LLMs to determine if observed differences in refusal rates and errors are truly model-size related or stem from prompt interpretation differences.

2. Conduct a robustness check by varying the threshold for determining liberal vs. conservative bias in source ratings and assess whether the observed liberal bias pattern holds across different classification thresholds.

3. Test model performance on a stratified sample of domains including both high-popularity and niche sources to determine if the 7,523-domain limitation affects the generalizability of findings about model accuracy and bias.