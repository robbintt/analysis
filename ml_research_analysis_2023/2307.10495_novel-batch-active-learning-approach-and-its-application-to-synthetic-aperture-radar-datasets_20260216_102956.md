---
ver: rpa2
title: Novel Batch Active Learning Approach and Its Application to Synthetic Aperture
  Radar Datasets
arxiv_id: '2307.10495'
source_url: https://arxiv.org/abs/2307.10495
tags:
- learning
- active
- data
- batch
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel batch active learning approach combining
  Dijkstra's Annus Core-Set (DAC) and LocalMax for synthetic aperture radar (SAR)
  data classification. The method improves efficiency by selecting query sets of multiple
  datapoints while maintaining high accuracy.
---

# Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets

## Quick Facts
- arXiv ID: 2307.10495
- Source URL: https://arxiv.org/abs/2307.10495
- Reference count: 30
- Primary result: Achieves 91.06% accuracy on FUSAR-Ship with 35% labeled data, outperforming CNN-based methods

## Executive Summary
This paper introduces a novel batch active learning approach combining Dijkstra's Annus Core-Set (DAC) and LocalMax for synthetic aperture radar (SAR) data classification. The method improves efficiency by selecting query sets of multiple datapoints while maintaining high accuracy. DAC creates a core-set by iteratively selecting nodes in annular regions to ensure uniform coverage of the data. LocalMax selects query sets based on local maximum conditions in the graph, ensuring diversity and information sharing between points. The pipeline achieves state-of-the-art performance on FUSAR-Ship and OpenSARShip datasets, outperforming CNN-based methods with 35% and 68% labeled data respectively.

## Method Summary
The approach uses transfer learning from pre-trained CNNs to extract features from SAR images, then constructs a similarity graph using K-nearest neighbors with angular distance. DAC generates a core-set by iteratively selecting nodes in annular regions based on local density, while LocalMax selects diverse batches by enforcing local maximum conditions on the acquisition function. The method uses Laplace learning for semi-supervised classification and uncertainty-based acquisition. Experiments demonstrate that this combination achieves nearly identical accuracy to sequential active learning while being 9-15 times faster, with batch sizes of 15.

## Key Results
- Achieves 91.06% accuracy on FUSAR-Ship dataset with only 35% labeled data
- Achieves 96.07% accuracy on OpenSARShip dataset with 68% labeled data
- Outperforms state-of-the-art CNN-based methods while being 9-15 times faster than sequential active learning

## Why This Works (Mechanism)

### Mechanism 1
DAC builds a core-set that is nearly uniform in the dataset by iteratively selecting nodes in annular regions. For each labeled node, DAC computes two annular sets: a candidate set (points between distances r and R) and a seen set (points within distance r). It randomly selects from the candidate set and updates both sets until all points are seen. The core assumption is that the dataset can be well-approximated by a graph where distances computed via Dijkstra's algorithm meaningfully reflect similarity in the embedding space.

### Mechanism 2
LocalMax selects diverse query sets by enforcing local maximum conditions on the acquisition function. For each candidate node, LocalMax checks if its acquisition value is greater than or equal to all its neighbors. If so, it's added to the query set, and all neighbors are removed from the candidate set. The core assumption is that acquisition function values are meaningful indicators of information gain, and enforcing local maxima prevents selecting redundant points.

### Mechanism 3
The combination of DAC and LocalMax achieves nearly identical accuracy to sequential active learning while being B times faster. DAC provides a good initial labeled set for early model training, while LocalMax efficiently selects diverse batches that maximize information gain. The speed-up comes from labeling B points in parallel instead of one at a time. The core assumption is that the core-set from DAC is sufficiently representative, and LocalMax can maintain diversity without sacrificing information gain.

## Foundational Learning

- **Graph-based semi-supervised learning with Laplace learning**: The method relies on graph structures to propagate label information from labeled to unlabeled nodes. *Quick check: How does the graph Laplacian matrix L = D - W relate to the smoothness of the classifier?*

- **Active learning acquisition functions (uncertainty, model-change, variance optimality)**: These functions quantify the value of labeling each unlabeled point to guide the selection process. *Quick check: What is the difference between uncertainty-based and model-change-based acquisition functions?*

- **Transfer learning from pre-trained CNNs**: The method uses pre-trained neural networks to extract useful features from SAR images before graph construction. *Quick check: Why might zero-shot transfer learning be preferable to fine-tuned transfer learning in some cases?*

## Architecture Onboarding

- **Component map**: Transfer learning (CNN feature extraction) → Graph construction (KNN + Gaussian kernel) → DAC (core-set selection) → LocalMax (batch active learning) → Laplace learning (semi-supervised classification)
- **Critical path**: Feature extraction → Graph construction → Core-set selection → Active learning loop (Laplace learning → LocalMax → label query)
- **Design tradeoffs**: Larger batch sizes increase efficiency but may reduce diversity; simpler core-set methods may be faster but less representative
- **Failure signatures**: Poor accuracy despite high labeled data proportion; slow convergence; batches with redundant points
- **First 3 experiments**:
  1. Run DAC with small r and R on a simple 2D dataset to visualize core-set coverage
  2. Test LocalMax on a synthetic graph with known acquisition values to verify diversity enforcement
  3. Compare accuracy of LocalMax vs random sampling on a small SAR subset with limited labels

## Open Questions the Paper Calls Out

### Open Question 1
Why does transfer learning from ImageNet work well for SAR ship classification but not for MSTAR vehicle classification? The paper identifies this as an open question but does not provide a theoretical explanation for the difference in transfer learning performance between ship and vehicle datasets.

### Open Question 2
How would specially designed neural networks for SAR data perform compared to standard ImageNet-pretrained architectures? The paper suggests but does not test SAR-specific architectures.

### Open Question 3
What is the optimal balance between exploration and exploitation in the core-set selection process? The paper uses density-based parameters for DAC but does not conduct sensitivity analysis on how different exploration-exploitation balances affect final performance.

## Limitations

- Performance heavily depends on quality of initial feature embeddings from transfer learning, which may not generalize well across all SAR datasets
- DAC algorithm's adaptive radius selection mechanism lacks detailed implementation specifications, potentially affecting reproducibility
- LocalMax method may struggle with noisy acquisition functions or when candidate set contains few local maxima, limiting batch size achievement

## Confidence

- **High confidence**: The core mechanism of combining DAC for core-set generation and LocalMax for batch selection is clearly defined and theoretically sound
- **Medium confidence**: The claim of achieving state-of-the-art performance is supported by experimental results but relies on specific dataset conditions and hyperparameters
- **Low confidence**: The generalization capability to other SAR datasets beyond the tested ones remains uncertain

## Next Checks

1. Implement DAC with different radius selection strategies to test robustness to parameter choices
2. Evaluate LocalMax performance with synthetic graphs containing varying levels of acquisition function noise
3. Test the complete pipeline on a third SAR dataset not used in the original experiments to assess generalization capabilities