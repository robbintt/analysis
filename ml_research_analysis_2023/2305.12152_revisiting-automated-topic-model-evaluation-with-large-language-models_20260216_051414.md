---
ver: rpa2
title: Revisiting Automated Topic Model Evaluation with Large Language Models
arxiv_id: '2305.12152'
source_url: https://arxiv.org/abs/2305.12152
tags: []
core_contribution: Large language models (LLMs) can evaluate topic model outputs.
  Specifically, they assess coherence of topic word sets more reliably than traditional
  automated metrics, correlating more strongly with human judgments.
---

# Revisiting Automated Topic Model Evaluation with Large Language Models

## Quick Facts
- arXiv ID: 2305.12152
- Source URL: https://arxiv.org/abs/2305.12152
- Authors: 
- Reference count: 16
- Large language models (LLMs) can evaluate topic model outputs more reliably than traditional automated metrics

## Executive Summary
This paper demonstrates that large language models can serve as robust tools for evaluating topic model outputs, particularly for assessing topic coherence and determining the optimal number of topics. The authors show that LLM-based coherence judgments correlate more strongly with human evaluations than existing automated metrics like NPMI and Cv. Additionally, they find that assigning LLM-generated labels to documents and selecting configurations with the highest label purity provides better guidance for topic count selection than traditional word-set coherence measures.

## Method Summary
The authors evaluate topic model outputs using ChatGPT to rate topic word coherence and assign labels to documents. They test this approach on NYT and Wiki datasets, comparing LLM-based assessments against human-annotated coherence ratings and intrusion detection tasks. For coherence evaluation, they prompt the LLM to rate shuffled word sets. For topic count selection, they assign labels to documents and calculate purity scores, then select the configuration with the highest purity. They measure correlation with human judgments using Spearman correlation and alignment with ground truth using Adjusted Rand Index (ARI).

## Key Results
- LLM coherence judgments correlate more strongly with human evaluations than traditional automated metrics
- Document label purity correlates better with optimal topic count selection than word-set coherence
- The approach provides interpretable, automated guidance for hyperparameter selection in topic modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM coherence judgments correlate strongly with human evaluations
- Mechanism: LLMs capture semantic relatedness between words in a topic, mirroring human cognitive assessment
- Core assumption: The LLM has sufficient training exposure to recognize semantic relationships
- Evidence anchors:
  - Correlates more strongly with human judgments than existing automated metrics
  - LLM assessment of top word coherence correlates more strongly with human evaluations than all other automated metrics
- Break condition: If the LLM lacks exposure to relevant domain vocabulary or if topic words are highly technical/rare

### Mechanism 2
- Claim: Document label purity correlates with optimal topic count
- Mechanism: Consistent document labeling within topics indicates coherent categories aligned with ground truth
- Core assumption: Ground truth labels represent meaningful categories
- Evidence anchors:
  - Assigning labels and choosing purest configurations correlates with underlying ground-truth partitions
  - Purity of LLM-assigned labels correlates with ARI, whereas word set coherence scores do not
- Break condition: If ground truth labels are noisy or if the document collection has no clear categorical structure

### Mechanism 3
- Claim: LLM-based evaluation replaces manual inspection for topic model selection
- Mechanism: LLMs can assess both word set coherence and document label purity at scale
- Core assumption: LLM assessments are reliable enough to substitute for human expert judgment
- Evidence anchors:
  - LLMs can serve as a robust, interpretable tool for both topic quality assessment and topic count selection
  - LLM-based coherence analysis correlates strongly with human preferences
- Break condition: If LLM assessments are inconsistent or if prompt engineering is inadequate

## Foundational Learning

- Concept: Topic coherence metrics (NPMI, Cv)
  - Why needed here: To understand the baseline against which LLM performance is compared
  - Quick check question: What does NPMI measure and why is it commonly used for topic coherence?

- Concept: Clustering evaluation metrics (ARI, AMI)
  - Why needed here: To measure alignment between LLM-guided topic assignments and ground truth
  - Quick check question: How does Adjusted Rand Index differ from simple accuracy in clustering evaluation?

- Concept: Document-topic distributions
  - Why needed here: To understand how topics are represented as distributions over documents for label assignment
  - Quick check question: What is the relationship between a document's topic distribution and its most probable topic assignment?

## Architecture Onboarding

- Component map: LLM prompt engine -> topic model output processor -> coherence evaluator -> label assigner -> purity calculator -> optimal k selector
- Critical path: Generate topic model outputs -> process through LLM for coherence and labeling -> calculate purity/ARI -> select optimal k
- Design tradeoffs: LLM-based evaluation offers better correlation with human judgment but lacks interpretability compared to traditional metrics
- Failure signatures: LLM outputs inconsistent ratings, purity scores don't correlate with ARI, or optimal k selection doesn't align with domain knowledge
- First 3 experiments:
  1. Test LLM coherence ratings on a small set of manually curated topics with known quality
  2. Run topic models with varying k on a sample corpus and compare LLM purity scores to ARI
  3. Evaluate sensitivity of optimal k selection to different prompt formulations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do other large language models (LLMs) compare to ChatGPT in evaluating topic model outputs for coherence and topic count selection?
- Basis in paper: The paper uses ChatGPT as the main LLM but states that "other comparable LLMs will deliver similar results" and suggests that "evaluation of LLMs other than ChatGPT" is an open future direction.
- Why unresolved: The paper only tested ChatGPT, so it's unknown if results would generalize to other LLMs with different architectures, training data, or capabilities.
- What evidence would resolve it: Testing the same topic model evaluation tasks (coherence rating, intrusion detection, topic count selection) with multiple other LLMs (e.g., Claude, LLaMA, BLOOM) and comparing their performance to ChatGPT's results.

### Open Question 2
- Question: Can the LLM-based approach for determining optimal topic count be formalized into a consistent, automated algorithm that doesn't require manual parameter tuning?
- Basis in paper: The paper states this is an identified future work direction: "Formalization of an LLM-guided algorithm for automatically finding optimal numbers of topics for a collection and research question."
- Why unresolved: The current approach uses heuristics like smoothing with sliding windows and relies on manual prompt engineering for different research questions, lacking a systematic framework.
- What evidence would resolve it: Developing a standardized pipeline that automatically generates appropriate prompts based on corpus characteristics and research goals, with consistent evaluation criteria for selecting the optimal topic count.

### Open Question 3
- Question: Does evaluating document-topic distributions (Tdc) rather than word-topic distributions (Tws) generalize better across different topic modeling algorithms and types of text?
- Basis in paper: The paper finds that LLM-assigned document labels correlate better with ground truth than word set coherence, supporting arguments from Doogan and Buntine (2021) that challenge the focus on Tws.
- Why unresolved: The study only tested this on three datasets (NYT, WikiText, Bills) using LDA. It's unclear if this finding holds for other algorithms like NMF or BERTopic, or for specialized domains like scientific literature or social media.
- What evidence would resolve it: Replicating the experiments across multiple topic modeling algorithms and diverse text corpora (scientific papers, tweets, legal documents) to test if document-based evaluation consistently outperforms word-based evaluation.

## Limitations

- Weak external validation with limited peer verification of findings
- Dependence on prompt engineering quality and consistency
- Reliance on ground truth labels for optimal topic count selection

## Confidence

**High Confidence**: The core claim that LLM coherence judgments correlate more strongly with human evaluations than traditional metrics is well-supported by direct experimental evidence and clear correlation metrics.

**Medium Confidence**: The claim about document label purity correlating with optimal topic count selection is supported by the experiments but relies heavily on the quality and representativeness of ground-truth labels.

**Low Confidence**: The broader claim that LLMs can fully replace manual inspection for topic model selection is asserted but not thoroughly validated across diverse domains and topic modeling approaches.

## Next Checks

1. Test the sensitivity of optimal k selection to different prompt formulations and LLM temperature settings to assess robustness.

2. Evaluate performance on domain-specific corpora (e.g., scientific literature, medical records) where topic words may be highly technical or specialized.

3. Compare the proposed LLM-based approach against recent neural topic models that incorporate contextual embeddings to determine relative advantages.