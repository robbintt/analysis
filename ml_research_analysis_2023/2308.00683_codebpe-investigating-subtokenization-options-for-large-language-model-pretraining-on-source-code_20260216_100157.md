---
ver: rpa2
title: 'CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining
  on Source Code'
arxiv_id: '2308.00683'
source_url: https://arxiv.org/abs/2308.00683
tags:
- level
- code
- subtokenization
- tokens
- unigramlm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how subtokenization choices affect the quality
  and efficiency of large language models pretrained on source code. It systematically
  compares various subtokenization strategies, including different levels of allowed
  composite token complexity, subtokenization algorithms (BPE vs.
---

# CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code

## Quick Facts
- arXiv ID: 2308.00683
- Source URL: https://arxiv.org/abs/2308.00683
- Reference count: 40
- Primary result: Grouping punctuation characters into single tokens reduces average sequence lengths by 17% without downstream performance drop

## Executive Summary
This paper systematically investigates how subtokenization choices affect the quality and efficiency of large language models pretrained on source code. The study compares various subtokenization strategies including different levels of composite token complexity, algorithms (BPE vs. UnigramLM), vocabulary sizes, and language transferability. The authors find that grouping punctuation characters into single tokens (CodeBPE/CodeUnigramLM) reduces sequence lengths by 17% without performance degradation, while allowing more complex composite tokens can reduce lengths by up to 40% with possible quality trade-offs. Smaller vocabularies (e.g., 10K) can improve performance by 0.5-2% in many tasks, albeit with increased sequence lengths. UnigramLM generally outperforms BPE, and subtokenizers trained on one language are largely transferable to another.

## Method Summary
The study uses PLBART as the base model and systematically varies subtokenization dimensions including allowed complexity of composite tokens (Levels 0-4), subtokenization algorithm (BPE vs. UnigramLM), vocabulary size (2K, 10K, 50K), language transferability, and stochastic subtokenization. The pretraining pipeline involves training PLBART models for 100k updates with different subtokenizations on a large codebase (230M Python functions, 470M Java functions, 47M NL descriptions). The models are evaluated on 8 downstream tasks including code generation, summarization, clone detection, and translation using metrics like CodeBLEU, BLEU, F1, and Computational Accuracy.

## Key Results
- Grouping punctuation chars in single tokens reduces average sequence length by 17% without downstream performance drop
- Smaller vocabularies (e.g., 10K) can improve performance by 0.5-2% in many tasks despite increased sequence lengths
- UnigramLM algorithm generally outperforms BPE for code subtokenization
- Subtokenizers trained on one language are largely transferable to another language
- Best-performing subtokenization improves downstream performance by 0.5-2% in most tasks and up to one standard deviation in two others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grouping punctuation characters into single tokens reduces sequence length without hurting performance
- Mechanism: Code has frequent punctuation patterns (e.g., '):', '}:', '));') that, when grouped into composite tokens, reduce the number of subtokens needed to represent the same sequence
- Core assumption: Punctuation patterns in code are syntactically meaningful units that can be merged without losing semantic information
- Evidence anchors:
  - [abstract]: "Grouping punctuation chars in single tokens reduces the average length by 17% without downstream performance drop"
  - [section 3]: "we consider several levels of allowed complexity of composite tokens...Level 1 makes one step further from Level 0 and allows punctuation char merges, e.g. '})' or ']):'"
  - [corpus]: Weak evidence - no direct corpus frequency analysis provided
- Break condition: If punctuation grouping removes syntactically meaningful boundaries that the model needs for correct code generation

### Mechanism 2
- Claim: UnigramLM algorithm outperforms BPE for code subtokenization
- Mechanism: UnigramLM better aligns with code identifier morphology by splitting CamelCase and snake_case identifiers into more natural subtokens
- Core assumption: Code identifiers follow consistent naming patterns that UnigramLM can exploit better than BPE's bottom-up merging approach
- Evidence anchors:
  - [section 4]: "we observe that UnigramLM subtokenization better resembles splitting into subtokens based on CamelCase or snake_case"
  - [section 4]: "For example, in Java–Python code translation, a cycle which traverses all unique element pairs in an array, converts to for l in range ( 0 , arr_size - 1 ) : for r in range ( l + 1 , arr_size ) :"
  - [corpus]: Weak evidence - only small sample of identifier comparisons provided
- Break condition: If BPE's bottom-up approach accidentally captures frequent identifier patterns that UnigramLM misses

### Mechanism 3
- Claim: Smaller vocabulary sizes can improve performance despite longer sequences
- Mechanism: Smaller vocabularies force more granular splitting of identifiers, making it easier for the model to compose words from smaller subtokens
- Core assumption: Code generation and summarization tasks benefit from more granular subtokenization because they involve more identifier-focused content
- Evidence anchors:
  - [section 5]: "In code generation and summarization data are more identifier-centered...e.g. the model often needs to choose a correct API based on the natural language description which seems to be easier by composing from smaller subtokens"
  - [section 5]: "Reducing vocabulary size increases the granularity of identifiers subtokenization, e.g. reachable is subtokenized as reachable with the 50K vocabulary, reach able – with 10K and re ach able – with 2K"
  - [corpus]: Moderate evidence - performance comparisons across vocabulary sizes provided
- Break condition: If the increased sequence length outweighs the benefits of more granular subtokenization for the specific task

## Foundational Learning

- Concept: Subtokenization vs Tokenization
  - Why needed here: Understanding the difference between whitespace-based tokenization and subtokenization is crucial for grasping why code-specific approaches work
  - Quick check question: What's the key difference between tokenization and subtokenization in the context of code processing?

- Concept: Vocabulary size vs sequence length tradeoff
  - Why needed here: The paper explores how smaller vocabularies increase sequence length but may improve performance - understanding this tradeoff is essential
  - Quick check question: How does reducing vocabulary size affect both sequence length and model performance?

- Concept: Composite tokens and code syntax
  - Why needed here: The paper's main contribution involves grouping punctuation and code patterns into composite tokens - understanding why this works requires knowing about code syntax
  - Quick check question: Why are punctuation patterns in code particularly suitable for grouping into composite tokens?

## Architecture Onboarding

- Component map: PLBART model -> Subtokenizer (SentencePiece) -> Pretraining pipeline -> Downstream evaluation tasks
- Critical path:
  1. Train subtokenizer on code corpus
  2. Preprocess code sequences using subtokenizer
  3. Train PLBART model with preprocessed sequences
  4. Finetune on downstream tasks
  5. Evaluate performance
- Design tradeoffs:
  - Vocabulary size vs sequence length vs performance
  - Subtokenization granularity vs model complexity
  - Code-specific vs general subtokenization strategies
- Failure signatures:
  - Performance degradation when subtokenization breaks syntactically meaningful code boundaries
  - Unexpected length increases when vocabulary size is reduced too much
  - Transferability issues when subtokenizer trained on one language used for another
- First 3 experiments:
  1. Compare Level 0 (baseline) vs Level 1 (punctuation grouping) on a small code generation task
  2. Test UnigramLM vs BPE on identifier subtokenization quality with a sample of 100 identifiers
  3. Measure sequence length impact of reducing vocabulary from 50K to 10K on a small pretraining subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do larger LMs pretrained on code perform with higher granularity subtokenizations like Level 4?
- Basis in paper: [inferred] The paper notes that Level 4 allows arbitrary composite tokens and achieves 40% length reduction, but performs worse in code-generative tasks. The authors suggest this might be due to computational constraints in their experiments and hypothesize that larger LMs might handle this granularity better.
- Why unresolved: The study used PLBART with limited computational resources and did not test larger models or training on higher granularity subtokenizations.
- What evidence would resolve it: Experiments with larger pretrained LMs (e.g., CodeBERT, CodeT5) trained on Level 4 or similar high-granularity subtokenizations, measuring both length efficiency and downstream performance.

### Open Question 2
- Question: Does the use of stochastic subtokenization (e.g., BPE-dropout) improve performance for code pretraining?
- Basis in paper: [explicit] The paper mentions that stochastic subtokenization has been proposed in NLP (Provilkov et al., 2020; Kudo, 2018) and adapted to pretrained LMs (Wang et al., 2021a), but does not test it for code.
- Why unresolved: The study focused on deterministic subtokenization strategies and did not include stochastic approaches in their experiments.
- What evidence would resolve it: Experiments comparing deterministic vs. stochastic subtokenization (e.g., BPE with and without dropout) on the same codebase and tasks, measuring both length efficiency and downstream performance.

### Open Question 3
- Question: How does subtokenization interact with natural language in mixed code-NL tasks?
- Basis in paper: [explicit] The paper notes that most subtokenizers are trained on code only, but some downstream tasks include natural language (e.g., code summarization, code generation from NL). It mentions that smaller vocabulary subtokenizers lead to higher length increases for natural text than for programming languages.
- Why unresolved: The study focused on code-specific subtokenization and did not deeply investigate optimal strategies for mixed code-NL tasks or cross-modal pretraining.
- What evidence would resolve it: Experiments comparing subtokenizers trained on code-only vs. code+NL data, measuring performance on mixed tasks, and analyzing how vocabulary size and granularity affect NL token sequences.

## Limitations
- Limited cross-language generalization - experiments only validate Java→Python and Python→Java transfers
- Evaluation scope constraints - only 8 tasks across 3 task types, missing tasks like bug detection or code repair
- Pretraining scale considerations - results may not generalize to larger-scale models (10B+ tokens)

## Confidence
- High confidence: The claim that grouping punctuation characters into single tokens reduces sequence length by 17% without hurting performance
- Medium confidence: The superiority of UnigramLM over BPE for code subtokenization
- Medium confidence: The claim that smaller vocabularies can improve performance despite longer sequences

## Next Checks
- Replicate the punctuation grouping experiment (Level 1) on a C++ codebase to test cross-language transferability of the 17% length reduction claim
- Conduct a systematic ablation study isolating the effects of UnigramLM algorithm, vocabulary size reduction, and composite token complexity on a representative subset of tasks
- Evaluate the identified best subtokenization strategy (Level 2 with UnigramLM, 10K vocabulary) on a code repair or bug detection task not included in the original study