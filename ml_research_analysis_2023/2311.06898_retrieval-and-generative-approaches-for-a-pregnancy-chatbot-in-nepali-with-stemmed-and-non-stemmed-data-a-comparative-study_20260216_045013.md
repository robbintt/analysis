---
ver: rpa2
title: 'Retrieval and Generative Approaches for a Pregnancy Chatbot in Nepali with
  Stemmed and Non-Stemmed Data : A Comparative Study'
arxiv_id: '2311.06898'
source_url: https://arxiv.org/abs/2311.06898
tags:
- chatbot
- dataset
- data
- multilingual
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study developed a pregnancy-related chatbot in Nepali using
  two NLP approaches: retrieval-based and generative-based. Retrieval-based models
  (BERT and DistilBERT) performed better on non-stemmed data, with DistilBERT achieving
  the highest testing accuracy of 91.37% and a micro F1-score of 0.9165.'
---

# Retrieval and Generative Approaches for a Pregnancy Chatbot in Nepali with Stemmed and Non-Stemmed Data : A Comparative Study

## Quick Facts
- arXiv ID: 2311.06898
- Source URL: https://arxiv.org/abs/2311.06898
- Reference count: 19
- Primary result: Retrieval models (BERT/DistilBERT) excel on non-stemmed Nepali data; generative models (transformer) perform better on stemmed data.

## Executive Summary
This study develops a pregnancy-related chatbot in Nepali using both retrieval-based (BERT/DistilBERT) and generative (transformer) NLP approaches. The research compares the performance of these models on stemmed versus non-stemmed datasets. Retrieval models show superior performance on non-stemmed data, with DistilBERT achieving the highest accuracy of 91.37%. In contrast, generative models demonstrate better results on stemmed data, achieving BLEU-1 and BLEU-2 scores of 0.3570 and 0.1413, respectively. These findings highlight the importance of data preprocessing and model selection in developing effective chatbots for low-resource languages.

## Method Summary
The study involved scraping 1,159 pregnancy-related question-answer pairs from medical websites, translating them to Nepali, and creating both stemmed and non-stemmed versions of the dataset. The final dataset contained 18,925 questions across 817 categories, split into 70% training, 20% validation, and 10% testing. Retrieval models (Multilingual BERT and DistilBERT) were fine-tuned on question-category pairs, while a generative transformer model was trained on question-answer pairs. The models were evaluated using micro F1-score for retrieval and BLEU-1/2 scores for generative tasks.

## Key Results
- DistilBERT achieved the highest testing accuracy of 91.37% and a micro F1-score of 0.9165 on non-stemmed data
- Transformer generative model achieved BLEU-1 and BLEU-2 scores of 0.3570 and 0.1413, respectively, on stemmed data
- Retrieval models performed better on non-stemmed data, while generative models excelled on stemmed data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual DistilBERT outperforms Multilingual BERT on non-stemmed Nepali pregnancy FAQ data in retrieval-based chatbot classification tasks.
- Mechanism: Pre-trained transformer encoders capture contextual word meaning better when input tokens retain inflectional morphology; stemming removes linguistic cues, reducing model discrimination.
- Core assumption: BERT/DistilBERT embeddings already encode sufficient linguistic structure to distinguish intent classes without preprocessing stripping.
- Evidence anchors:
  - [abstract] "BERT-based pre-trained models perform well on non-stemmed data"
  - [section] "Multilingual DistilBERT without stemming displayed the best performance across all metrics"
  - [corpus] Weak - corpus contains similar generative model studies but lacks direct non-stemmed vs stemmed BERT comparisons.
- Break condition: If test set vocabulary distribution differs markedly from training, morphological features may not generalize, causing accuracy drop.

### Mechanism 2
- Claim: Stemming improves transformer decoder performance in generative Nepali pregnancy Q&A generation.
- Mechanism: Stemming reduces vocabulary size, decreasing out-of-vocabulary (OOV) errors and improving attention alignment in sequence-to-sequence generation.
- Core assumption: Reduced vocabulary does not remove essential semantic distinctions required for coherent answer generation.
- Evidence anchors:
  - [abstract] "transformer models are better suited for generative tasks on stemmed data"
  - [section] "The transformer-based model for the pregnancy-related question answering bot with stemmed dataset has achieved better performance"
  - [corpus] Weak - corpus contains generative Nepali models but no direct stemmed vs non-stemmed generation study.
- Break condition: If stemming conflates distinct concepts (e.g., symptom vs. condition), generated responses lose precision.

### Mechanism 3
- Claim: Retrieval models trained on non-stemmed data maintain higher validation/training loss parallelism, reducing overfitting risk.
- Mechanism: Non-stemmed inputs preserve linguistic variety, enabling better generalization across validation splits; stemming may over-smooth, causing train/validation loss divergence.
- Core assumption: Dataset contains sufficient examples per class that morphological variation aids robustness.
- Evidence anchors:
  - [section] "Multilingual BERT models without stemming also exhibited comparable or slightly lower accuracy"
  - [section] "models with stemmed datasets lagged behind those without stemming"
  - [corpus] Weak - no corpus neighbor directly measures loss curve parallelism for stemmed vs non-stemmed.
- Break condition: With highly imbalanced classes, morphological preservation may not offset insufficient per-class samples.

## Foundational Learning

- Concept: Multilingual transformer pre-training (BERT/DistilBERT)
  - Why needed here: Provides contextualized embeddings for low-resource Nepali text classification.
  - Quick check question: What layer of BERT is typically used for sentence classification tasks?

- Concept: BLEU score computation
  - Why needed here: Evaluates n-gram overlap quality of generated answers against reference answers.
  - Quick check question: Does BLEU-2 penalize missing 2-word phrases in generated text?

- Concept: Data augmentation via scraping and translation
  - Why needed here: Expands limited Nepali pregnancy FAQ dataset from English sources.
  - Quick check question: Which Python library was used to scrape FAQ data in the study?

## Architecture Onboarding

- Component map:
  - Data pipeline -> scraping -> cleaning -> stemming option -> train/val/test split
  - Retrieval model: BERT/DistilBERT fine-tuning on question -> category
  - Generative model: Transformer encoder-decoder on question -> answer
  - Evaluation: Micro F1 for retrieval, BLEU-1/2 for generative

- Critical path:
  1. Prepare parallel stemmed/non-stemmed datasets
  2. Fine-tune retrieval models
  3. Train generative transformer
  4. Evaluate and compare metrics

- Design tradeoffs:
  - Stemming reduces vocab but may lose semantics; non-stemming preserves meaning but increases model complexity.
  - Smaller DistilBERT faster but slightly less accurate than BERT.
  - Transformer generative models need more data; retrieval models leverage pre-training more effectively.

- Failure signatures:
  - Retrieval: Large train/validation accuracy gap -> overfitting; low test F1 -> poor generalization.
  - Generative: BLEU near zero -> generation collapse; high training accuracy but low validation -> memorization.

- First 3 experiments:
  1. Fine-tune Multilingual BERT on non-stemmed data; measure micro F1.
  2. Fine-tune Multilingual DistilBERT on stemmed data; measure micro F1.
  3. Train transformer generative model on non-stemmed data; compute BLEU-1/2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the pregnancy chatbot differ when using alternative stemming algorithms like the Krovetz stemmer compared to the Porter stemmer?
- Basis in paper: [inferred] The paper mentions using the Porter stemming algorithm but doesn't explore alternative stemming methods.
- Why unresolved: The study focused on the Porter stemmer and did not investigate the impact of other stemming algorithms on model performance.
- What evidence would resolve it: Conducting experiments with alternative stemming algorithms and comparing their performance with the Porter stemmer on the same dataset would provide insights into their effectiveness.

### Open Question 2
- Question: What is the impact of using different data augmentation techniques, such as back-translation or synonym replacement, on the performance of the pregnancy chatbot?
- Basis in paper: [inferred] The paper mentions the need for a more comprehensive dataset and suggests data augmentation as a potential area for improvement.
- Why unresolved: The study did not explore the use of data augmentation techniques to enhance the dataset and improve model performance.
- What evidence would resolve it: Experimenting with various data augmentation techniques and evaluating their impact on model performance would provide insights into their effectiveness in improving the chatbot's accuracy and robustness.

### Open Question 3
- Question: How does the performance of the pregnancy chatbot change when trained on a larger and more diverse dataset that includes additional languages or dialects?
- Basis in paper: [inferred] The paper focuses on developing a Nepali language chatbot and mentions the potential for integrating the chatbot across platforms.
- Why unresolved: The study did not investigate the impact of using a larger and more diverse dataset, including multiple languages or dialects, on the chatbot's performance.
- What evidence would resolve it: Collecting and training the chatbot on a larger and more diverse dataset that includes multiple languages or dialects would provide insights into its ability to handle a wider range of user inputs and improve its overall performance.

## Limitations

- The study's findings are based on a relatively small dataset of 18,925 questions across 817 categories, which may limit generalizability to broader pregnancy-related queries.
- The specific stemming algorithm used for Nepali text is not explicitly stated, introducing potential variability in preprocessing effects.
- The evaluation focuses on technical metrics (F1, BLEU) without assessing user satisfaction or clinical accuracy of chatbot responses, which are critical for real-world deployment.

## Confidence

- High confidence: DistilBERT's superior performance on non-stemmed data for retrieval tasks (supported by direct metric comparison)
- Medium confidence: Transformer's better performance on stemmed data for generative tasks (supported by BLEU scores but limited by small dataset size)
- Low confidence: Generalization of findings to other low-resource languages or different medical domains (not tested)

## Next Checks

1. Conduct ablation studies varying the stemming algorithm to isolate its impact on both retrieval and generative model performance
2. Test model robustness by introducing adversarial examples with spelling variations and synonyms in Nepali
3. Evaluate clinical accuracy of generated responses by having medical professionals rate a sample of chatbot outputs for correctness and completeness