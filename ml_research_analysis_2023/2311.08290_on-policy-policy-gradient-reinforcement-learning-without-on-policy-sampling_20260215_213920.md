---
ver: rpa2
title: On-Policy Policy Gradient Reinforcement Learning Without On-Policy Sampling
arxiv_id: '2311.08290'
source_url: https://arxiv.org/abs/2311.08290
tags:
- policy
- sampling
- data
- on-policy
- props
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of sampling error in on-policy
  reinforcement learning, where finite trajectory samples may not accurately reflect
  the expected on-policy data distribution. To mitigate this issue, the authors propose
  Proximal Robust On-Policy Sampling (PROPS), an adaptive, off-policy sampling method
  that reduces sampling error by collecting data with a behavior policy that increases
  the probability of sampling under-sampled actions relative to the current policy.
---

# On-Policy Policy Gradient Reinforcement Learning Without On-Policy Sampling

## Quick Facts
- arXiv ID: 2311.08290
- Source URL: https://arxiv.org/abs/2311.08290
- Reference count: 26
- One-line primary result: PROPS reduces sampling error in on-policy policy gradient methods and improves data efficiency compared to standard on-policy sampling.

## Executive Summary
This paper addresses the challenge of sampling error in on-policy reinforcement learning, where finite trajectory samples may not accurately reflect the expected on-policy data distribution. To mitigate this issue, the authors propose Proximal Robust On-Policy Sampling (PROPS), an adaptive, off-policy sampling method that reduces sampling error by collecting data with a behavior policy that increases the probability of sampling under-sampled actions relative to the current policy. Unlike traditional on-policy algorithms that discard data from old policies, PROPS retains and adjusts historic data to be approximately on-policy. Empirical evaluations on continuous-action MuJoCo benchmark tasks and discrete-action tasks demonstrate that PROPS (1) decreases sampling error throughout training and (2) improves the data efficiency of on-policy policy gradient algorithms compared to standard on-policy sampling methods.

## Method Summary
The paper proposes PROPS, an adaptive, off-policy sampling method for on-policy policy gradient reinforcement learning. PROPS reduces sampling error by collecting data with a behavior policy that increases the probability of sampling under-sampled actions relative to the current policy. Unlike traditional on-policy algorithms that discard data from old policies, PROPS retains and adjusts historic data to be approximately on-policy. The behavior policy is updated using a clipped surrogate objective and a KL-divergence regularization term to prevent large policy updates and keep the behavior policy close to the target policy. The target policy is updated using Proximal Policy Optimization (PPO) with data from the replay buffer.

## Key Results
- PROPS decreases sampling error throughout training on continuous-action MuJoCo tasks and discrete-action tasks.
- PROPS improves the data efficiency of on-policy policy gradient algorithms compared to standard on-policy sampling methods.
- PROPS outperforms PPO with a replay buffer (PPO -BUFFER) on continuous-action MuJoCo tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: On-policy policy gradient algorithms learn more efficiently when they use on-policy data rather than on-policy sampling.
- Mechanism: PROPS adaptively increases the probability of sampling under-sampled actions relative to the current policy, reducing sampling error in the replay buffer. This correction allows the empirical data distribution to better match the expected on-policy distribution without requiring infinite samples.
- Core assumption: Reducing sampling error improves the accuracy of policy gradient estimates, leading to more efficient learning.
- Evidence anchors:
  - [abstract] "Our method, Proxable Robust On-Policy Sampling (PROPS), reduces sampling error by collecting data with a behavior policy that increases the probability of sampling actions that are under-sampled w.r.t. the current policy."
  - [section] "Adaptive action sampling provides an alternative means of producing on-policy data without the need for an infinite sample size."
  - [corpus] Weak: No direct corpus evidence of sampling error reduction in policy gradient methods.
- Break condition: If the behavior policy update causes the behavior policy to deviate too far from the target policy, sampling error may increase instead of decrease.

### Mechanism 2
- Claim: PROPS enables data reuse from old policies without introducing significant bias or variance.
- Mechanism: By periodically adjusting the behavior policy to reduce sampling error in the replay buffer, PROPS ensures that data from old policies remains approximately on-policy with respect to the current policy. This allows data to be reused for multiple policy updates.
- Core assumption: Data from old policies can be made approximately on-policy through behavior policy adjustments, enabling data reuse without introducing significant bias.
- Evidence anchors:
  - [abstract] "Rather than discarding data from old policies – as is commonly done in on-policy algorithms – PROPS uses data collection to adjust the distribution of previously collected data to be approximately on-policy."
  - [section] "Since the replay buffer D may contain data collected from older target policies, some samples in D may be very off-policy with respect to the current target policy..."
  - [corpus] Weak: No direct corpus evidence of data reuse in on-policy policy gradient methods.
- Break condition: If the behavior policy update fails to sufficiently reduce sampling error, data from old policies may introduce significant bias into gradient estimates.

### Mechanism 3
- Claim: PROPS reduces sampling error more effectively than on-policy sampling during RL training.
- Mechanism: PROPS uses a clipped surrogate objective (similar to PPO) with an auxiliary KL regularization term to prevent destructively large policy updates and ensure the behavior policy remains close to the target policy. This allows PROPS to efficiently learn a behavior policy that keeps the distribution of data in the replay buffer close to the expected distribution of the target policy.
- Core assumption: Preventing large policy updates and keeping the behavior policy close to the target policy are necessary for effective sampling error reduction.
- Evidence anchors:
  - [section] "Rather than using the ROS objective to update ϕ, we use a clipped surrogate objective... This objective is equivalent to the PPO objective for πθ with Aπϕ(s, a) = −1, ∀(s, a)."
  - [section] "To address the second challenge and prevent degenerate behavior policies, we introduce an auxiliary loss that incentivizes the agent to minimize the KL divergence between the behavior policy and target policy at states in the observed data."
  - [corpus] Weak: No direct corpus evidence of sampling error reduction with clipped surrogate objectives.
- Break condition: If the clipping coefficient or regularization coefficient are not properly tuned, PROPS may fail to effectively reduce sampling error.

## Foundational Learning

- Concept: Sampling error in on-policy policy gradient methods
  - Why needed here: Understanding sampling error is crucial for understanding why PROPS improves data efficiency.
  - Quick check question: What is the difference between on-policy sampling and on-policy data, and why does this distinction matter for policy gradient learning?
- Concept: Behavior policy and target policy in reinforcement learning
  - Why needed here: PROPS uses a separate behavior policy to collect data and a target policy to learn, so understanding this distinction is important.
  - Quick check question: What is the difference between a behavior policy and a target policy in reinforcement learning?
- Concept: Proximal Policy Optimization (PPO) and clipped surrogate objectives
  - Why needed here: PROPS builds on PPO's clipped surrogate objective, so understanding this concept is important for understanding how PROPS works.
  - Quick check question: What is the purpose of the clipping mechanism in PPO's surrogate objective?

## Architecture Onboarding

- Component map:
  - Target policy (πθ) -> Behavior policy (πϕ) -> Replay buffer (D) -> PROPS update
- Critical path:
  1. Initialize target policy parameters (θ) and behavior policy parameters (ϕ).
  2. Collect data using the behavior policy and add it to the replay buffer.
  3. Periodically update the behavior policy using the PROPS update to reduce sampling error.
  4. Update the target policy using data from the replay buffer.
- Design tradeoffs:
  - Buffer size: Larger buffer sizes allow more data reuse but require more data collection to impact the aggregate data distribution.
  - Clipping coefficient (ϵPROPS): Larger values allow more aggressive behavior policy updates but may increase sampling error.
  - Regularization coefficient (λ): Larger values keep the behavior policy closer to the target policy but may limit the effectiveness of sampling error reduction.
- Failure signatures:
  - High sampling error in the replay buffer despite PROPS updates.
  - Target policy updates that are too aggressive or too conservative.
  - Behavior policy that deviates significantly from the target policy.
- First 3 experiments:
  1. Run PROPS with different buffer sizes (e.g., b = 1, 2, 4) and compare sampling error and data efficiency.
  2. Run PROPS with different clipping coefficients (e.g., ϵPROPS = 0.1, 0.3, 0.5) and compare sampling error and data efficiency.
  3. Run PROPS with different regularization coefficients (e.g., λ = 0.01, 0.1, 0.3) and compare sampling error and data efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical convergence rate of PROPS compared to on-policy sampling in the policy optimization setting?
- Basis in paper: [inferred] The paper mentions that Zhong et al. (2022) showed ROS (on which PROPS is based) enjoys faster convergence in policy evaluation, but PROPS's theoretical analysis in the control setting is lacking.
- Why unresolved: The paper focuses on empirical evaluation and does not provide theoretical convergence guarantees for PROPS in the reinforcement learning setting with changing policies.
- What evidence would resolve it: A formal theoretical analysis showing the convergence rate of PROPS in the policy optimization setting, comparing it to on-policy sampling methods.

### Open Question 2
- Question: How can PROPS be extended to prioritize correcting sampling error for actions with large advantages?
- Basis in paper: [explicit] The paper mentions that "One limitation of PROPS is that the update indiscriminately increases the probability of under-sampled actions without considering their importance in gradient computation."
- Why unresolved: The current PROPS algorithm treats all under-sampled actions equally, regardless of their impact on the policy gradient.
- What evidence would resolve it: An extension of PROPS that incorporates advantage-weighted sampling error correction, with empirical results showing improved performance compared to the original PROPS.

### Open Question 3
- Question: Can PROPS be effectively integrated into off-policy RL algorithms for focused exploration?
- Basis in paper: [explicit] The paper suggests that "PROPS has the potential to be integrated into off-policy RL algorithms and used so that the empirical distribution more closely matches a desired exploration distribution."
- Why unresolved: The paper focuses on on-policy algorithms and does not explore the application of PROPS to off-policy methods.
- What evidence would resolve it: An implementation of PROPS within an off-policy RL algorithm (e.g., SAC or TD3) that demonstrates improved exploration efficiency and performance compared to standard off-policy methods.

## Limitations

- The paper does not provide a detailed analysis of how the behavior policy update in PROPS interacts with the target policy update using PPO.
- The paper only evaluates PROPS on a limited set of benchmark tasks and does not test its performance on more complex or diverse environments.
- The paper does not compare PROPS to other off-policy correction methods or explore the trade-offs between different approaches to reducing sampling error.

## Confidence

- Confidence in the claims is Medium. The empirical results show that PROPS can reduce sampling error and improve data efficiency on the tested tasks, but the lack of theoretical analysis and limited empirical evaluation make it difficult to assess the generality and robustness of the approach.

## Next Checks

1. Analyze the interaction between the behavior policy update and target policy update in PROPS. Investigate whether the two updates can lead to unstable or divergent behavior, and propose modifications to ensure stability.

2. Evaluate PROPS on a wider range of benchmark tasks, including more complex and diverse environments. Compare its performance to other off-policy correction methods and explore the trade-offs between different approaches to reducing sampling error.

3. Conduct an ablation study to understand the impact of different components of PROPS, such as the clipping coefficient, regularization coefficient, and buffer size, on its performance. This will help identify the key factors that contribute to its success and guide future improvements.