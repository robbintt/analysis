---
ver: rpa2
title: 'Q-SENN: Quantized Self-Explaining Neural Networks'
arxiv_id: '2312.13839'
source_url: https://arxiv.org/abs/2312.13839
tags:
- features
- q-senn
- feature
- class
- resnet50
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Q-SENN quantizes the sparse final layer of interpretable neural
  networks, enabling accurate image classification with only 5 interpretable features
  per class. The ternary weights enforce binary human-friendly features, improving
  fidelity, diversity, and grounding.
---

# Q-SENN: Quantized Self-Explaining Neural Networks

## Quick Facts
- arXiv ID: 2312.13839
- Source URL: https://arxiv.org/abs/2312.13839
- Reference count: 21
- Key outcome: Q-SENN achieves 97% of dense model accuracy on ImageNet-1K while providing interpretable explanations using only 5 features per class

## Executive Summary
Q-SENN introduces a quantization approach to Self-Explaining Neural Networks that enforces ternary weights ({−α, 0, α}) on the sparse final layer, resulting in more binary, contrastive features that improve interpretability. By limiting each class to only 5 interpretable features and iteratively fine-tuning the feature extractor, Q-SENN maintains high accuracy while producing explanations that are more human-friendly, grounded, and robust to spurious correlations. The method includes an automatic feature alignment technique using CLIP that enables verbalized explanations without additional annotations.

## Method Summary
Q-SENN quantizes the sparse final layer of interpretable neural networks using glm-saga to compute regularization paths, then thresholds and averages weights to enforce ternary values. The model iteratively fine-tunes the feature extractor with fixed quantized assignments (N=4 iterations) to remove spurious correlations and promote generalizable features. An optional CLIP-based alignment method verbalizes features by matching them to human concepts. The approach maintains 97% of dense model accuracy while using only 5 features per class on average.

## Key Results
- Maintains 97% of dense model accuracy on ImageNet-1K
- Outperforms competitors on all interpretability metrics (fidelity, diversity, grounding)
- Demonstrates exceptional robustness to spurious correlations on TravelingBirds dataset
- Achieves verbalized explanations without additional annotations through CLIP alignment

## Why This Works (Mechanism)

### Mechanism 1
- Quantization forces binary, contrastive features rather than class detectors
- Ternary weights prevent continuous activation tuning for specific classes
- Pushes feature extractor to learn general concepts applicable across multiple classes

### Mechanism 2
- Iterative fine-tuning removes spurious correlations by forcing feature generalization
- Each iteration fixes quantized layer and fine-tunes feature extractor
- Eliminates class-specific patterns in favor of shared concepts

### Mechanism 3
- Limiting to 5 features per class enables human-friendly explanations
- Leverages Miller's finding that humans follow decisions based on 5 cognitive aspects
- Creates concise, contrastive explanations that are easier to interpret

## Foundational Learning

- Concept: Feature selection and regularization paths
  - Why needed here: Understanding glm-saga's sparse solution computation is crucial for correct quantization implementation
  - Quick check question: What is the difference between ℓ1 and ℓ2 regularization in the context of feature selection?

- Concept: Contrastive learning and embedding spaces
  - Why needed here: CLIP-based alignment relies on understanding how contrastive training creates meaningful image-text relationships
  - Quick check question: How does the contrastive loss in CLIP encourage meaningful relationships between images and captions?

- Concept: Interpretability metrics (fidelity, diversity, grounding)
  - Why needed here: Evaluating Q-SENN requires understanding how these desiderata are measured
  - Quick check question: What's the difference between local and global interpretability, and why does Q-SENN excel at both?

## Architecture Onboarding

- Component map: Dense model training → Feature selection (glm-saga) → Quantization → Iterative fine-tuning → CLIP alignment (optional) → Evaluation
- Critical path: Dense training → Feature selection → Quantization → Iterative fine-tuning → Evaluation
- Design tradeoffs:
  - Sparsity vs. accuracy: 5 features per class for interpretability may hurt complex datasets
  - Iteration count: More iterations improve robustness but increase training time
  - CLIP alignment: Adds interpretability but requires additional computation and may introduce biases
- Failure signatures:
  - Significant accuracy drop after quantization: too aggressive thresholding or insufficient features
  - Low diversity@5: quantization too coarse or feature extractor learning non-diverse concepts
  - High dependence γ: features remain class-specific rather than general concepts
- First 3 experiments:
  1. Train dense model, apply glm-saga with default settings, verify regularization path computation
  2. Implement quantization with nwc=5, test on CUB-2011, measure accuracy and diversity@5 impact
  3. Run one iteration of fine-tuning, compare robustness to spurious correlations vs. non-iterative version

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Q-SENN's quantization method compare to other sparse and interpretable neural network approaches?
- Basis: Paper claims Q-SENN outperforms previous work but doesn't compare to other quantization techniques
- Resolution: Direct comparison with other quantization methods on same datasets and metrics

### Open Question 2
- Question: Can the CLIP-based feature alignment method be applied to other neural network types?
- Basis: Method described as "generally independent of the used CLIP-like model" but only tested on Q-SENN
- Resolution: Testing on other neural network architectures and comparing performance

### Open Question 3
- Question: How does the number of iterations (N) affect performance and interpretability?
- Basis: Paper uses N=4 without exploring different values
- Resolution: Experimenting with different N values and analyzing impact on performance metrics

## Limitations

- The quantization threshold (α=0.1) and optimal iteration count (N=4) are empirically determined and may require dataset-specific tuning
- CLIP-based alignment inherits potential biases from the CLIP model and may vary in effectiveness across different domains
- Computational cost of multiple iterations and potential accuracy degradation in more complex classification tasks

## Confidence

**High Confidence Claims:**
- Q-SENN achieves strong accuracy retention (97% of dense model performance) on ImageNet-1K
- Quantization mechanism successfully enforces ternary weights and binary-like features
- Iterative fine-tuning improves robustness to spurious correlations
- Using 5 features per class enables concise, human-friendly explanations

**Medium Confidence Claims:**
- CLIP-based feature alignment provides meaningful verbal explanations without additional annotations
- Diversity@5 and alignment metrics consistently outperform baseline methods
- Dependence γ effectively measures feature grounding across different datasets

## Next Checks

1. **Ablation study on iteration count**: Systematically evaluate Q-SENN performance across different iteration counts (N=1, 2, 4, 8) on ImageNet-1K to determine optimal trade-offs between robustness and computational cost.

2. **Cross-dataset generalization**: Test Q-SENN on datasets with varying levels of spurious correlations (e.g., from low to high correlation) to validate the claimed robustness improvements across the full spectrum of dataset characteristics.

3. **Threshold sensitivity analysis**: Perform a comprehensive grid search on the quantization threshold α and weight magnitude threshold ϵ to understand their impact on accuracy, diversity, and grounding metrics across multiple datasets.