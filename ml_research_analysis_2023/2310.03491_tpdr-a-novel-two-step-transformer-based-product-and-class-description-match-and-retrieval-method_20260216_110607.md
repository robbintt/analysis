---
ver: rpa2
title: 'TPDR: A Novel Two-Step Transformer-based Product and Class Description Match
  and Retrieval Method'
arxiv_id: '2310.03491'
source_url: https://arxiv.org/abs/2310.03491
tags:
- product
- query
- tpdr
- description
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method TPDR for matching noisy and
  short client product descriptions with detailed standardized product descriptions
  in catalogs. The problem is challenging due to vocabulary mismatches, missing information,
  and multi-language specifications.
---

# TPDR: A Novel Two-Step Transformer-based Product and Class Description Match and Retrieval Method

## Quick Facts
- **arXiv ID**: 2310.03491
- **Source URL**: https://arxiv.org/abs/2310.03491
- **Reference count**: 40
- **Primary result**: 71% recall@5 and 80% category @1 accuracy on product description matching task

## Executive Summary
This paper addresses the challenge of matching noisy, short client product descriptions with detailed standardized product descriptions in catalogs. The problem is particularly difficult due to vocabulary mismatches, missing information, and multi-language specifications. TPDR proposes a novel two-step approach: first using dual transformer encoders with attention mechanisms and contrastive learning for semantic matching, then re-ranking results using syntactic features like TF-IDF, Jaccard similarity, and BM25. The method was evaluated on 11 real-world datasets from a company, achieving 71% recall@5 for correct product retrieval and 80% accuracy for category identification.

## Method Summary
TPDR employs a dual transformer encoder architecture where both client descriptions and standardized product descriptions are mapped into a shared embedding space. The method uses contrastive learning with N-pair loss to pull relevant product descriptions closer to their corresponding queries while pushing non-relevant products away. After semantic matching through the transformer encoders, a re-ranking step applies syntactic features including TF-IDF cosine similarity, Jaccard bigram similarity, and BM25 scores to adjust rankings based on exact matches of product-specific characteristics like model numbers and dimensions. The training uses a novel TaG-Training mechanism with alternating optimization of the two encoders.

## Key Results
- Achieved 71% recall@5 for retrieving correct standardized products
- Correctly identified product categories in first position for 80% of cases
- Significantly outperformed purely syntactic or semantic baselines, solving cases that neither approach could handle alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual transformer encoders in shared embedding space allow semantic matching despite vocabulary mismatch.
- Mechanism: Two separate transformers encode initial client descriptions and standardized product descriptions into the same vector space, where corresponding pairs are encouraged to be close using contrastive learning.
- Core assumption: The vocabulary mismatch between client and product descriptions can be bridged by learning dense semantic representations rather than relying on exact token overlap.
- Evidence anchors:
  - [abstract] "TPDR employs the transformers as two encoders sharing the embedding vector space: one for encoding the IS and another for the SD, in which corresponding pairs (IS, SD) must be close in the vector space."
  - [section 3.1] "These encoders work as a logical framework to represent the queries, and product descriptions in the same embedding space"
- Break condition: If the vocabulary mismatch is too severe or the semantic correspondence is not learnable from the available training pairs, the encoders may fail to produce meaningful alignments.

### Mechanism 2
- Claim: Contrastive learning with N-pair loss pulls relevant products closer to queries while pushing non-relevant products away.
- Mechanism: The N-pair loss function optimizes the embedding functions so that a query's vector is closer to its relevant product than to any other non-relevant products in the batch.
- Core assumption: Using multiple negative examples per update step (N-1) is more effective than using only one, especially in retrieval tasks with many irrelevant items.
- Evidence anchors:
  - [section 3.3] "The core contribution of the N-pair loss... is to optimize the embedding functions... to map q_i and the corresponding c_i into dense vectors where the similarity sim(f_i, g_i) is greater than the similarity sim(f_i, g_j)"
  - [section 3.3] "The number of non-relevant products for a given query usually greatly surpasses the relevant ones, making a significant difference."
- Break condition: If the batch size is too small or the negative samples are not sufficiently diverse, the contrastive learning may not effectively discriminate between relevant and irrelevant products.

### Mechanism 3
- Claim: Two-step approach (semantic matching + syntactic re-ranking) handles product-specific characteristics missed by transformers.
- Mechanism: First, semantic matching using transformers retrieves a candidate list. Then, a re-ranking step based on syntactic features (TF-IDF cosine similarity, Jaccard bigrams, BM25) adjusts the ranking to account for exact matches on model, dimensions, etc.
- Core assumption: Transformers may miss product-specific characteristics like exact model numbers or dimensions due to their focus on semantic meaning, so a syntactic re-ranking step is needed to capture these details.
- Evidence anchors:
  - [abstract] "TPDR also exploits a (second) re-ranking step based on syntactic features that are very important for the exact matching (model, dimension) of certain products that may have been neglected by the transformers."
  - [section 3.2] "From the list of candidate products received from the Embedding-based retrieval, we use a Term-based retrieval to re-rank the product query pairs"
- Break condition: If the syntactic re-ranking overemphasizes exact matches at the expense of semantic relevance, or if the candidate list from the first step is too small, the final ranking may be suboptimal.

## Foundational Learning

- Concept: Self-attention and multi-head self-attention in transformers
  - Why needed here: Understanding how transformers capture semantic relationships between words in product descriptions is crucial for grasping the dual encoder architecture.
  - Quick check question: How does multi-head self-attention allow the model to focus on different aspects of the product descriptions simultaneously?

- Concept: Contrastive learning and N-pair loss
  - Why needed here: The effectiveness of the dual encoder approach relies on the contrastive learning mechanism to align relevant pairs and discriminate against non-relevant ones.
  - Quick check question: What is the key difference between N-pair loss and triplet loss in terms of the number of negative examples used per update step?

- Concept: Term-based retrieval methods (TF-IDF, BM25, Jaccard similarity)
  - Why needed here: The re-ranking step uses these syntactic features to adjust the semantic ranking based on exact matches and term overlap.
  - Quick check question: How does BM25 score a document differently from simple TF-IDF cosine similarity, and why might this be useful for product matching?

## Architecture Onboarding

- Component map:
  - Client Description → Query Encoder → Embedding-based Retrieval → Candidate List → Term-based Re-ranking → Final Ranking
  - Product Catalog → Product Encoder → Embedding-based Retrieval → Candidate List → Term-based Re-ranking → Final Ranking

- Critical path: Client description → Query Encoder → Embedding-based Retrieval → Candidate List → Term-based Re-ranking → Final Ranking

- Design tradeoffs:
  - Semantic vs. Syntactic: Prioritizing semantic matching may miss exact product specifications, while focusing on syntactic features may not handle vocabulary mismatch.
  - Efficiency vs. Effectiveness: Using the exact (brute_force) method for indexing is more effective but less efficient than approximate methods like HNSW.
  - Training Stability: Alternating optimization of the two encoders (TaG-Training) may lead to more stable training compared to joint optimization.

- Failure signatures:
  - Low MRR@1 or Recall@10: Indicates the model is not effectively matching relevant products to queries, possibly due to insufficient training data or poor encoder alignment.
  - High MRR@1 but low Recall@10: Suggests the model is finding some relevant products but missing others, potentially due to the re-ranking step being too restrictive.
  - Slow convergence or unstable training: May be caused by the alternating optimization of the two encoders or the difficulty of the contrastive learning task.

- First 3 experiments:
  1. Evaluate the effectiveness of the dual encoder approach with and without contrastive learning to measure the impact of the N-pair loss.
  2. Compare the performance of the two-step approach (semantic + syntactic) against purely semantic or purely syntactic baselines on a subset of the datasets.
  3. Analyze the impact of the TaG-Training mechanism by comparing it to joint optimization of the two encoders in terms of convergence speed and final performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of TPDR compare to other transformer-based methods specifically designed for product retrieval tasks?
- Basis in paper: [inferred] The paper mentions that TPDR significantly outperforms purely syntactic or semantic baselines, but does not provide a direct comparison with other transformer-based methods.
- Why unresolved: The paper focuses on comparing TPDR with purely syntactic and semantic baselines, but does not provide a comparison with other transformer-based methods specifically designed for product retrieval tasks.
- What evidence would resolve it: Conducting experiments comparing TPDR with other transformer-based methods designed for product retrieval tasks, such as product-specific BERT models or other recent transformer-based approaches.

### Open Question 2
- Question: How does the performance of TPDR vary across different product domains and dataset sizes?
- Basis in paper: [explicit] The paper mentions that TPDR was evaluated on 11 datasets from a real company, covering different application contexts and dataset sizes.
- Why unresolved: While the paper provides effectiveness results for different datasets, it does not provide a detailed analysis of how the performance of TPDR varies across different product domains and dataset sizes.
- What evidence would resolve it: Conducting experiments to analyze the performance of TPDR across different product domains and dataset sizes, and providing a detailed analysis of the results.

### Open Question 3
- Question: How does the choice of pre-trained embedding model affect the performance of TPDR?
- Basis in paper: [explicit] The paper mentions that LaBSE embeddings were used in TPDR due to their higher effectiveness compared to other alternatives such as BERTimbau and MUSE.
- Why unresolved: The paper does not provide a detailed analysis of how the choice of pre-trained embedding model affects the performance of TPDR.
- What evidence would resolve it: Conducting experiments to evaluate the performance of TPDR with different pre-trained embedding models and providing a detailed analysis of the results.

## Limitations

- Limited dataset information and lack of publicly available datasets prevent full reproducibility
- No ablation studies isolating the contribution of each component (dual encoders, N-pair loss, re-ranking)
- Limited discussion of computational efficiency and production deployment feasibility

## Confidence

**High confidence** in the two-step architectural design and its core rationale: semantic matching followed by syntactic re-ranking addresses vocabulary mismatch while preserving product-specific details.

**Medium confidence** in the N-pair loss implementation and TaG-Training mechanism effectiveness. The conceptual framework is sound, but lacks detailed implementation specifics.

**Low confidence** in the generalizability of results due to limited dataset information and lack of comparison with state-of-the-art transformer-based retrieval methods.

## Next Checks

1. Implement ablation studies to quantify the contribution of contrastive learning vs. TaG-Training vs. re-ranking step
2. Test the method on public product matching datasets (e.g., e-commerce product matching benchmarks) to assess generalizability
3. Measure inference latency and memory requirements for indexing product catalogs of different scales to evaluate production feasibility