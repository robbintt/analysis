---
ver: rpa2
title: 'United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning
  from Videos'
arxiv_id: '2311.03550'
source_url: https://arxiv.org/abs/2311.03550
tags:
- videos
- learning
- unitygraph
- frames
- procedure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles unsupervised procedure learning from videos,
  focusing on identifying key-steps and their order to perform a task given multiple
  videos of the same task. Previous approaches struggle with inter-videos context,
  making key-steps discovery challenging.
---

# United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning from Videos

## Quick Facts
- arXiv ID: 2311.03550
- Source URL: https://arxiv.org/abs/2311.03550
- Authors: 
- Reference count: 40
- Primary result: Proposes UnityGraph framework that models multiple videos of a task as a unified graph, achieving 2% average improvement on third-person datasets and 3.6% on EgoProceL over state-of-the-art

## Executive Summary
This paper addresses the challenge of unsupervised procedure learning from videos by proposing the UnityGraph framework. The method identifies key-steps and their order in a task given multiple videos of the same procedure. Previous approaches struggled with inter-video context, making key-steps discovery challenging. UnityGraph overcomes this limitation by modeling all videos of a task as a unified graph that captures both intra-video and inter-video context through temporal and spatial edges. The framework demonstrates significant improvements over state-of-the-art methods on benchmark datasets including ProceL, CrossTask, and EgoProceL.

## Method Summary
The UnityGraph framework operates through a four-step process. First, videos are segmented into clips and converted into embeddings using a pre-trained I3D ResNet-50 model. Second, a unified graph is constructed where clips are nodes connected by temporal edges (within the same video) and spatial edges (across videos with high cosine similarity). Third, Node2Vec algorithm enhances the clip embeddings by preserving neighborhood information in an unsupervised manner. Finally, KMeans clustering identifies key-steps, which are ordered based on average timestamps. The framework also incorporates background frame detection using hand-object interaction models to filter non-procedural frames.

## Key Results
- Achieves 2% average improvement over state-of-the-art on third-person datasets
- Demonstrates 3.6% improvement on EgoProceL egocentric dataset
- Shows increased performance with more videos per task (3 videos: 59.7 F1, 5 videos: 62.7 F1)
- Background frame filtering improves key-step identification accuracy

## Why This Works (Mechanism)

### Mechanism 1
UnityGraph captures both intra-video (temporal) and inter-video (semantic) context by connecting clips via spatial and temporal edges, enabling discovery of key-steps across multiple videos. Clips are nodes; temporal edges connect neighboring clips within the same video, while spatial edges connect clips from different videos with highest cosine similarity, creating a unified representation that models both relationships. Core assumption: Semantically similar key-steps across videos will have high cosine similarity in the embedding space from pre-trained I3D ResNet-50 features.

### Mechanism 2
Node2Vec enhances clip embeddings in an unsupervised manner by leveraging the graph structure to bring similar key-steps closer in embedding space. Node2Vec simulates biased random walks on UnityGraph, updating node embeddings to preserve neighborhood information, thus improving the clustering of semantically similar key-steps. Core assumption: The connections in UnityGraph (both temporal and spatial) provide sufficient signal for Node2Vec to learn meaningful embeddings that cluster key-steps.

### Mechanism 3
Background frame detection using hand-object interaction improves procedure learning by filtering non-procedural frames that lack hand-object interaction. Frames without detected hand-object interaction are classified as background and removed, reducing noise and improving key-step identification. Core assumption: Frames without hand-object interaction are predominantly background frames that do not contribute to understanding the procedure.

## Foundational Learning

- Concept: Graph representation learning (Node2Vec)
  - Why needed here: To enhance UnityGraph embeddings by preserving neighborhood information, improving clustering of semantically similar key-steps.
  - Quick check question: How does Node2Vec differ from DeepWalk in leveraging graph structure for embedding learning?

- Concept: Cosine similarity for semantic matching
  - Why needed here: To measure similarity between clip embeddings for creating spatial edges between semantically similar clips across videos.
  - Quick check question: Why is cosine similarity preferred over Euclidean distance for comparing high-dimensional clip embeddings?

- Concept: Clustering algorithms (KMeans)
  - Why needed here: To group enhanced UnityGraph embeddings into clusters representing key-steps of the procedure.
  - Quick check question: How does the choice of K (number of clusters) affect the identification of key-steps, and how is it determined?

## Architecture Onboarding

- Component map: Videos -> Clips -> I3D Embeddings -> UnityGraph -> Node2Vec -> Enhanced Embeddings -> KMeans -> Key-Steps
- Critical path:
  1. Clip videos into segments using window size, stride, and sampling rate
  2. Generate clip embeddings using pre-trained I3D ResNet-50
  3. Create UnityGraph with temporal and spatial edges
  4. Update embeddings using Node2Vec
  5. Cluster embeddings using KMeans
  6. Order clusters based on average timestamps
- Design tradeoffs:
  - Clip-level vs frame-level nodes: Clip-level reduces noise but may miss fine-grained actions
  - Edge creation: High similarity threshold reduces edges but may miss connections; low threshold increases edges but may add noise
  - Number of videos: More videos improve UnityGraph but increase computational complexity
- Failure signatures:
  - Low F1/IoU scores: Poor embedding quality, incorrect edge creation, or inappropriate clustering
  - Background frames not filtered: Hand-object detection model fails or background contains interactions
  - Key-steps not discovered: UnityGraph structure does not capture true semantic similarity
- First 3 experiments:
  1. Vary sampling rate, stride, and window size to optimize UnityGraph creation
  2. Test different Node2Vec parameters (walk count, walk length, window size) for embedding enhancement
  3. Compare results with and without background frame filtering using hand-object interaction detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of UnityGraph scale with an increasing number of videos, and what is the theoretical upper limit of video inputs for practical application?
- Basis in paper: [explicit] The paper mentions that the time complexity of GPL is exponential in the number of videos, but also shows improved performance with more videos in Table 9.
- Why unresolved: While the paper demonstrates that more videos improve performance, it does not explore the practical limits or the point of diminishing returns in terms of computational resources and performance gains.
- What evidence would resolve it: Empirical results showing performance metrics (F1 score, IoU) and computational time for datasets with a significantly larger number of videos than currently tested.

### Open Question 2
- Question: Can UnityGraph be effectively adapted for unsupervised procedure learning in domains with high object variability or when subjects use dissimilar objects for the same key-steps?
- Basis in paper: [inferred] The paper assumes similar objects are used for identical key-steps and acknowledges this as a limitation.
- Why unresolved: The paper does not provide experimental results or propose methods to handle scenarios with high object variability, which is a common real-world challenge.
- What evidence would resolve it: Experimental results comparing UnityGraph's performance on datasets with controlled object variability, and proposed methods to enhance the framework's robustness to object differences.

### Open Question 3
- Question: How does the inclusion of multi-modal data (e.g., text, audio) impact the performance of UnityGraph, and can it overcome the scalability issues associated with weakly supervised methods?
- Basis in paper: [explicit] The paper discusses the limitations of multi-modal approaches due to alignment issues and scalability concerns.
- Why unresolved: The paper does not explore the potential benefits of incorporating multi-modal data into UnityGraph, leaving the question of whether such an integration could enhance performance while maintaining scalability.
- What evidence would resolve it: Comparative experiments between the current UnityGraph and a multi-modal version, measuring performance improvements and assessing scalability.

## Limitations
- Reliance on pre-trained I3D ResNet-50 features may limit capture of fine-grained procedural differences
- Background filtering assumes hand-object interaction is a reliable indicator of procedural relevance
- Method requires multiple videos of the same task, limiting applicability to tasks with limited video data

## Confidence
- **High Confidence**: UnityGraph architecture design and basic framework implementation
- **Medium Confidence**: Improvement claims over state-of-the-art methods (2% on third-person, 3.6% on EgoProceL)
- **Medium Confidence**: Background frame filtering effectiveness using hand-object interaction detection

## Next Checks
1. Test UnityGraph performance on datasets with varying numbers of videos per task to determine minimum video requirement for effectiveness
2. Evaluate key-step identification accuracy when hand-object interaction detection fails or produces false positives
3. Compare performance using different video feature extractors (beyond I3D ResNet-50) to assess robustness to feature representation choices