---
ver: rpa2
title: A note on regularised NTK dynamics with an application to PAC-Bayesian training
arxiv_id: '2312.13259'
source_url: https://arxiv.org/abs/2312.13259
tags:
- training
- networks
- have
- network
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the dynamics of wide neural networks trained
  with regularisation terms that constrain parameters to remain close to their initial
  values. Such regularisation keeps the network in a lazy training regime, where the
  dynamics can be linearised around the initialisation.
---

# A note on regularised NTK dynamics with an application to PAC-Bayesian training

## Quick Facts
- arXiv ID: 2312.13259
- Source URL: https://arxiv.org/abs/2312.13259
- Authors: 
- Reference count: 40
- Primary result: Regularized neural networks in the lazy training regime have NTK dynamics that can be computed exactly, providing a tractable framework for analyzing generalization objectives like PAC-Bayes bounds

## Executive Summary
This paper extends neural tangent kernel (NTK) theory to analyze the dynamics of infinitely wide neural networks trained with regularization terms that constrain parameters to remain close to their initial values. The authors show that such regularization preserves lazy training dynamics, causing the NTK to remain constant during training in the infinite-width limit, with an additional regularization term appearing in the differential equation. This framework provides a tractable way to compute exact dynamics for regularized networks and is applied to PAC-Bayes training, where the regularization naturally emerges from KL divergence between posterior and prior distributions.

## Method Summary
The paper analyzes gradient flow training of fully-connected feed-forward neural networks with ℓ₂ regularization that constrains parameters to stay near initialization. The infinite-width limit is taken recursively layer-by-layer, and the resulting dynamics are governed by a modified NTK equation with an additional regularization term. For least squares regression, the resulting linear ODE can be solved exactly. The framework is then applied to PAC-Bayes training with Gaussian parameter distributions, showing how the KL divergence regularization fits naturally into the proposed regularized NTK framework.

## Key Results
- Regularization preserving lazy training dynamics causes NTK to remain constant during training in infinite-width limit
- Exact computation of regularized dynamics is possible for least squares regression
- PAC-Bayes training with Gaussian parameters naturally fits the regularized NTK framework
- Additional regularization term in NTK dynamics equals the regularization strength times identity matrix

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding ℓ₂ regularization that constrains parameters to remain close to their initialization preserves lazy training dynamics, causing the Neural Tangent Kernel (NTK) to remain constant during training.
- Mechanism: The regularization term enforces small parameter updates (ΔW), which keeps the Jacobian of the network's output fixed. This allows linearization of the dynamics around initialization, making the NTK deterministic and time-invariant in the infinite-width limit.
- Core assumption: The parameter updates remain small enough throughout training due to the regularization strength, and the network operates in the lazy training regime.
- Evidence anchors:
  - [abstract]: "The standard neural tangent kernel (NTK) governs the evolution during the training in the infinite-width limit, although the regularisation yields an additional term appears in the differential equation describing the dynamics."
  - [section 4.1]: "It is then natural to expect that a regularising term that enforce s the parameters to stay close to their initialisation will still favour linearised dynamics..."
  - [corpus]: Weak - no direct mention of lazy training preservation in corpus papers.
- Break condition: If regularization strength is too weak or learning rate too high, parameter updates may become large enough to exit the lazy regime, causing the NTK to evolve and invalidating the analysis.

### Mechanism 2
- Claim: The regularized training dynamics can be exactly computed in the infinite-width limit, providing a tractable framework for analyzing generalization objectives like PAC-Bayes bounds.
- Mechanism: By establishing that the NTK remains constant and the linearized dynamics hold, the authors derive exact ODEs governing the evolution of the network's output. These can be solved analytically for certain loss functions (e.g., least squares).
- Core assumption: The infinite-width limit (taken recursively layer-by-layer) and the regularization term combine to yield a closed-form expression for the dynamics.
- Evidence anchors:
  - [abstract]: "As an application, they analyse PAC-Bayes training of neural networks and show how the induced dynamics can be computed exactly."
  - [section 5]: "This is a linear ODE and can be solved exactly... we can easily derive that..."
  - [corpus]: Weak - corpus papers mention NTK but don't discuss exact computation of regularized dynamics.
- Break condition: If the loss function is non-convex or the regularization is non-linear, the dynamics may not have a closed-form solution, requiring numerical integration.

### Mechanism 3
- Claim: For PAC-Bayes training with Gaussian parameter distributions, the regularization term naturally appears as the KL divergence between posterior and prior, fitting exactly into the proposed regularized NTK framework.
- Mechanism: When training the means of Gaussian parameters initialized at zero, the KL divergence regularization term takes the form of an ℓ₂ constraint on the distance from initialization. This matches the proposed regularization framework, allowing PAC-Bayes bounds to be optimized within the NTK regime.
- Core assumption: The PAC-Bayes bound uses a Gaussian prior centered at initialization, and only the means are trained while variances remain fixed.
- Evidence anchors:
  - [section 6]: "The PAC-Bayesian bounds are generalisation guarantees that upperbound in high probability the population loss of stochastic architectures... One possible approach consists in considering the case when all the parameters of the network are independent Gaussian variables with unit variance..."
  - [corpus]: Missing - corpus papers don't mention PAC-Bayes bounds or their connection to regularization.
- Break condition: If the PAC-Bayes bound uses a different prior (e.g., not centered at initialization) or optimizes over variances as well, the regularization term may not fit the proposed framework.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and lazy training regime
  - Why needed here: The entire analysis relies on the NTK remaining constant during training, which only holds in the lazy regime where parameters don't move far from initialization.
  - Quick check question: What condition must be satisfied for a neural network to be in the lazy training regime?
- Concept: Gaussian process behavior of infinitely wide networks
  - Why needed here: The NTK analysis builds on the fact that before training, infinitely wide networks behave as Gaussian processes, and this property is used to derive the deterministic NTK.
  - Quick check question: How does the output distribution of an infinitely wide network at initialization relate to Gaussian processes?
- Concept: PAC-Bayes generalization bounds
  - Why needed here: The application to PAC-Bayes training requires understanding how these bounds work and how they can be optimized through regularization of parameter distributions.
  - Quick check question: What is the relationship between the KL divergence term in PAC-Bayes bounds and the regularization term in the proposed framework?

## Architecture Onboarding

- Component map: Input -> Fully connected layers with activation φ, widths nl -> Output F(x)
- Critical path: Initialize -> Compute NTK at t=0 -> Evolve output via regularized NTK dynamics -> Compute generalization bound
- Design tradeoffs:
  - Regularization strength λ: Larger λ keeps parameters closer to initialization (more stable NTK) but may hurt learning capacity
  - Widths nl: Larger widths improve NTK approximation but increase computational cost
  - Activation function φ: Must be smooth enough for NTK analysis to hold
- Failure signatures:
  - NTK evolves significantly during training (parameters move too far from initialization)
  - Numerical instability in solving the ODEs (poor conditioning of NTK matrix)
  - Generalization bounds become vacuous (regularization too strong relative to learning signal)
- First 3 experiments:
  1. Verify NTK constancy: Train a wide network with strong ℓ₂ regularization and measure NTK evolution over time
  2. Compare with unregularized case: Same network without regularization, observe NTK changes and performance
  3. PAC-Bayes application: Implement the Gaussian parameter model and verify that the KL regularization term matches the proposed framework

## Open Questions the Paper Calls Out
- Question: How do the dynamics change when using non-Gaussian priors in PAC-Bayes bounds?
  - Basis in paper: [inferred] The paper focuses on Gaussian priors for PAC-Bayes training but mentions that many other bounds could fit their framework with more general regularizers.
  - Why unresolved: The paper only analyzes the Gaussian case explicitly, leaving open how other priors affect the dynamics.
  - What evidence would resolve it: Deriving explicit dynamics for PAC-Bayes bounds with non-Gaussian priors and comparing them to the Gaussian case.
- Question: What is the impact of the lazy training regime on generalization bounds for wider networks?
  - Basis in paper: [explicit] The paper conjectures that the exact dynamics derived could be a starting point to obtain generalization bounds for more general kernel gradient descent algorithms.
  - Why unresolved: The paper does not explore the connection between lazy training and generalization bounds in depth.
  - What evidence would resolve it: Establishing explicit generalization bounds for wide networks trained with lazy dynamics and comparing them to non-lazy cases.
- Question: How do the dynamics scale with network depth in the infinite-width limit?
  - Basis in paper: [inferred] The paper analyzes dynamics for a fixed depth L and mentions that the infinite-width limit is taken recursively layer by layer.
  - Why unresolved: The paper does not investigate how the dynamics change as depth increases.
  - What evidence would resolve it: Deriving explicit dynamics for networks of varying depths and analyzing the impact on training and generalization.

## Limitations
- The analysis assumes the lazy training regime is maintained throughout training due to regularization, but the strength of regularization needed to guarantee this is not quantified.
- The connection between PAC-Bayes bounds and the proposed regularization framework, while theoretically sound, lacks empirical validation on real datasets.
- The infinite-width limit approximation may break down for networks of practical widths, and the computational cost of evaluating the NTK matrix grows quadratically with dataset size.

## Confidence
- NTK constancy under regularization: High - the mechanism is well-established in the literature and the additional regularization term in the differential equation is derived rigorously
- Exact computation of regularized dynamics: Medium - the mathematical derivation appears sound but the practical utility for complex loss functions is unclear
- PAC-Bayes application: Low - while the theoretical connection is established, there is no empirical validation or discussion of practical implementation challenges

## Next Checks
1. Empirical verification: Train finite-width networks with varying regularization strengths and measure NTK evolution to determine the minimum λ needed to maintain lazy training
2. Scalability analysis: Implement the PAC-Bayes training framework on a medium-sized dataset to evaluate computational feasibility and compare generalization performance against standard training
3. Loss function robustness: Test the exact dynamics computation on non-least-squares losses (e.g., cross-entropy) to determine which classes of problems remain tractable under this framework