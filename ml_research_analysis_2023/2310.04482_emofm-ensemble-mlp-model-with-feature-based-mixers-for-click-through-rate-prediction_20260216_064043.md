---
ver: rpa2
title: 'EMOFM: Ensemble MLP mOdel with Feature-based Mixers for Click-Through Rate
  Prediction'
arxiv_id: '2310.04482'
source_url: https://arxiv.org/abs/2310.04482
tags:
- feature
- fusion
- type-wise
- ftype
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses click-through rate (CTR) prediction in online
  advertising. The key idea is to use feature-based mixers (cross-attention modules)
  to fuse field-wise features, combined with hierarchical multi-layer perceptron (MLP)
  models.
---

# EMOFM: Ensemble MLP mOdel with Feature-based Mixers for Click-Through Rate Prediction

## Quick Facts
- arXiv ID: 2310.04482
- Source URL: https://arxiv.org/abs/2310.04482
- Reference count: 7
- Primary result: AUC 0.7472, Logloss 0.2147 on advertising dataset

## Executive Summary
EMOFM introduces an ensemble approach for click-through rate (CTR) prediction that combines feature-based mixers with hierarchical MLP structures. The model addresses the challenge of fusing field-wise features while preserving their individual characteristics through cross-attention mechanisms. Three variants (WM, HM, HMM) explore different architectural configurations, with experimental results showing significant improvements over existing baselines.

## Method Summary
The EMOFM model uses feature-based mixers (cross-attention modules) to fuse field-wise features, combined with hierarchical multi-layer perceptron (MLP) models. The architecture processes features through two stages: independent field encoding followed by cross-field fusion. Three variants are proposed: WM uses whole MLPs, HM uses hierarchical MLPs, and HMM combines hierarchical MLPs with mixers. The model is trained for one epoch using Adam optimizer with specific learning rates for embeddings and MLPs.

## Key Results
- Achieves AUC of 0.7472 and Logloss of 0.2147 on the test dataset
- Outperforms baseline models including DCNv2, DESTINE, EDCN, and FINAL
- Ablation studies confirm both type-wise predictors and Ftype information are critical for performance
- HM and HMM variants show superior performance compared to WM, validating the hierarchical structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature-based mixers using cross-attention improve cross-field feature fusion efficiency
- Mechanism: Cross-attention between field embeddings allows weighted aggregation while preserving dimensions through residual connections
- Core assumption: Cross-attention captures complex relationships between features from different fields better than simple concatenation
- Evidence anchors: [abstract] cross-attention characteristic; [section 2.2] specific crossatt implementation; [corpus] no direct CTR evidence
- Break condition: Performance degrades with very different dimensionalities or failed interaction capture

### Mechanism 2
- Claim: Hierarchical MLP structure balances field-wise feature extraction and cross-field fusion
- Mechanism: Two-stage processing with independent field encoding followed by combined cross-field processing
- Core assumption: Field-wise independent processing preserves domain-specific patterns better than direct fusion
- Evidence anchors: [abstract] HM and HMM variants; [section 2.3] two-stage processing description; [section 3] HM/HMM outperforming WM
- Break condition: Simple hierarchical separation may miss important patterns for highly non-linear interactions

### Mechanism 3
- Claim: Type-wise predictors and Ftype information significantly improve CTR prediction accuracy
- Mechanism: Separate predictors for each interaction type capture interaction-specific patterns
- Core assumption: Different interaction types have distinct feature importance and interaction patterns
- Evidence anchors: [abstract] critical role of type-wise predictors and Ftype; [section 2.3] four single models implementation; [section 3] ablation study results
- Break condition: Overfitting if Ftype information is noisy or types don't have meaningful pattern differences

## Foundational Learning

- Concept: Cross-attention mechanism
  - Why needed here: Understanding how cross-attention differs from self-attention and enables field-wise information exchange
  - Quick check question: What is the key difference between cross-attention and self-attention in terms of information flow between query and key-value pairs?

- Concept: Feature interaction modeling in CTR prediction
  - Why needed here: Grasping why explicit feature interactions matter more than pure MLP approaches for sparse categorical features
  - Quick check question: Why do traditional FM models still perform well in CTR prediction despite being simpler than deep learning approaches?

- Concept: Hierarchical model design
  - Why needed here: Understanding when to separate feature processing by field vs. when to fuse early
  - Quick check question: What are the trade-offs between processing features hierarchically vs. processing them jointly from the start?

## Architecture Onboarding

- Component map: Input → Field-wise embeddings → Mixer blocks (optional) → Stage 1 MLPs (per field) → Mixer blocks (optional) → Stage 2 MLPs → Type-wise predictors → Ensemble output
- Critical path: Embedding → Stage 1 MLPs → Stage 2 MLPs → Output (with mixers inserted at appropriate stages)
- Design tradeoffs: Mixers add computational overhead but improve fusion quality; hierarchical structure increases parameter count but improves specialization
- Failure signatures: Poor performance on unseen interaction types, overfitting to training data, or failure to converge during training
- First 3 experiments:
  1. Test baseline WM model without mixers to establish performance floor
  2. Add mixers at Stage 2 only to test fusion impact without field separation
  3. Test with and without type-wise predictors to measure Ftype contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EMOFM performance change with different feature distributions or unseen interaction types?
- Basis in paper: [inferred] Ftype only accessible during training with auxiliary model for inference
- Why unresolved: Only evaluated on given dataset, not tested on different distributions
- What evidence would resolve it: Testing on datasets with different distributions or unseen interaction types

### Open Question 2
- Question: Can mixers be generalized to other interaction types beyond cross-attention?
- Basis in paper: [explicit] Proposed plug-in mixers motivated by transformer efficiency
- Why unresolved: Only evaluates cross-attention mixers, not exploring alternatives
- What evidence would resolve it: Experimenting with self-attention or graph-based interactions in mixers

### Open Question 3
- Question: How does EMOFM scale with larger datasets or more complex feature interactions?
- Basis in paper: [inferred] Evaluated on millions of records but scalability not discussed
- Why unresolved: Only tested on single dataset without exploring scalability
- What evidence would resolve it: Testing on larger datasets or more complex interaction scenarios

## Limitations

- Dataset transparency issues prevent faithful reproduction due to missing feature hashing details
- Cross-attention module implementation details are incomplete, affecting reproducibility
- One-epoch training setup raises concerns about overfitting and model generalization

## Confidence

- **High Confidence**: Hierarchical MLP structure effectiveness (Mechanism 2) - well-supported by ablation study results
- **Medium Confidence**: Cross-attention contribution (Mechanism 1) - theoretically sound but lacks direct CTR corpus validation
- **Medium Confidence**: Type-wise predictors importance (Mechanism 3) - empirically demonstrated but generalizability unclear

## Next Checks

1. **Dataset Verification**: Attempt to obtain or simulate the described dataset structure to verify preprocessing assumptions and feature engineering pipeline
2. **Cross-Attention Implementation**: Implement cross-attention modules with varying attention dimensions and test sensitivity to embedding sizes
3. **Type-wise Ablation**: Systematically remove individual Ftype predictors to quantify their marginal contribution versus potential overfitting effects