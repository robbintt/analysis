---
ver: rpa2
title: Exploring Sparsity in Graph Transformers
arxiv_id: '2312.05479'
source_url: https://arxiv.org/abs/2312.05479
tags:
- pruning
- graph
- gtsp
- layers
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes GTSP, the first comprehensive sparsification
  framework for Graph Transformers (GTs), aiming to reduce their computational costs
  while maintaining performance. GTSP prunes redundancy in GTs across four dimensions:
  input nodes, attention heads, model layers, and model weights.'
---

# Exploring Sparsity in Graph Transformers

## Quick Facts
- arXiv ID: 2312.05479
- Source URL: https://arxiv.org/abs/2312.05479
- Reference count: 3
- Key outcome: First comprehensive sparsification framework for Graph Transformers, achieving up to 50% parameter reduction and 47.4% FLOPs reduction while maintaining or improving accuracy

## Executive Summary
This paper introduces GTSP, a novel sparsification framework for Graph Transformers that reduces computational costs across four dimensions: input nodes, attention heads, model layers, and model weights. By employing differentiable masks and a top-k selection strategy, GTSP achieves significant reductions in parameters and FLOPs while maintaining or even improving model performance. The framework demonstrates effectiveness across three prominent GT models and large-scale datasets, with notable improvements on the OGBG-HIV dataset. The study also provides valuable insights into attention head characteristics and attention mechanism behavior, contributing to the broader understanding of Graph Transformers.

## Method Summary
GTSP operates through differentiable masks across four pruning dimensions: input nodes (token pruning), attention heads, model layers, and model weights. The framework uses Gumbel-Softmax for token selection, importance scores for head pruning, random drop-layer for layer pruning, and gradual magnitude pruning with regrowth for weights. The method is applied to baseline GT models (GraphTrans, Graphormer, GraphGPS) on standard graph classification datasets, evaluating performance through parameter reduction, FLOPs reduction, and accuracy/ROC-AUC metrics.

## Key Results
- Achieves up to 50% parameter reduction and 47.4% FLOPs reduction while maintaining or improving accuracy
- Demonstrates 1.8% increase in Area Under the Curve accuracy on OGBG-HIV dataset with 30% reduction in Floating Point Operations
- Identifies that up to 50% of attention heads and model neurons are redundant and can be pruned without significant performance loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different attention heads in Graph Transformers capture distinct types of information, allowing selective pruning without severe performance loss.
- Mechanism: Attention heads specialize in focusing on different structural patterns (e.g., long-range vs. neighbor information). Pruning redundant heads removes duplicated computations while retaining the unique signal captured by each head.
- Core assumption: Attention heads have identifiable and non-overlapping functional roles; redundancy can be quantified and removed.
- Evidence anchors:
  - [abstract] "Removing a large percentage of the attention heads does not significantly affect performance. The attention heads serve distinct roles in capturing various types of information, such as long-range and neighbor information within the graph."
  - [section] "Figure 5 illustrates the attention probability... only a limited number of tokens (e.g., node 15) receive high attention scores... Figure 4a illustrates the redundancy (distance) among 16 × 16 pairs of attention matrices... certain attention heads can be safely removed."

### Mechanism 2
- Claim: Sparsifying model weights mitigates overfitting and can improve generalization, especially on large-scale datasets.
- Mechanism: Gradually pruning small-magnitude weights reduces model capacity during training, forcing the model to rely on more essential features and reducing overfitting. Weight sparsity also lowers computational cost without hurting accuracy.
- Core assumption: Overparameterization is a primary cause of overfitting in Graph Transformers, and the pruned weights are truly redundant.
- Evidence anchors:
  - [abstract] "In general, up to 50% of the model's neurons are redundant and can be pruned, which prevents over-fitting during training and can potentially improve accuracy."
  - [section] "Figure 6 illustrates the training progress... as the sparsity increases, the training curve comes to more closely approximate the test curve. This confirms that our weight pruning strategies do indeed help to alleviate over-fitting in GTs."

### Mechanism 3
- Claim: Layer redundancy increases with depth, and selectively dropping layers accelerates training and reduces over-smoothing.
- Mechanism: Deeper layers in Graph Transformers exhibit higher similarity in their representations, indicating redundancy. Removing redundant layers reduces computational cost and prevents over-smoothing, a common problem in deep graph models.
- Core assumption: The functional contribution of deeper layers diminishes due to redundancy, and removing them does not significantly impact discriminative power.
- Evidence anchors:
  - [abstract] "Redundancy is evident among adjacent layers within the network, with deeper layers displaying even greater redundancy in relation to their neighboring layers. Selectively trimming certain layers not only accelerates training but also alleviates the over-smoothing issue on the graph."
  - [section] "Figure 4b illustrates the redundancy (distance) among pairs of attention matrices... deeper layers exhibit greater similarity with each other, suggesting increased redundancy at deeper levels... our pruning strategy effectively reduces it."

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: Understanding multi-head self-attention is essential to grasp how Graph Transformers differ from Graph Neural Networks and why attention redundancy exists.
  - Quick check question: What is the computational complexity of self-attention in terms of input length, and why does it matter for large graphs?

- Concept: Graph Neural Networks (GNNs) and their limitations
  - Why needed here: Graph Transformers often build on GNNs, so understanding their inductive biases and locality constraints explains why GTs can outperform GNNs.
  - Quick check question: How do GNNs typically aggregate neighborhood information, and what is a key limitation of this approach?

- Concept: Sparsity and pruning in deep learning
  - Why needed here: The core of GTSP is removing redundant components (heads, layers, weights, tokens). Understanding pruning strategies is critical to evaluating the approach.
  - Quick check question: What is the difference between unstructured and structured pruning, and which is more common in Graph Transformers?

## Architecture Onboarding

- Component map: Input graph -> Node feature matrix & adjacency matrix -> GTSP framework (token, head, layer, weight pruning) -> Pruned Graph Transformer -> Output predictions
- Critical path: Forward pass → mask application → sparsity-aware computation → backward pass with mask gradients → mask update and pruning
- Design tradeoffs: Balancing sparsity level vs. accuracy; pruning speed vs. end-to-end differentiability; layer pruning compatibility vs. feature dimension constraints
- Failure signatures: Sudden accuracy drop after pruning a critical component; unstable training due to premature mask updates; memory blowup from inefficient mask storage
- First 3 experiments:
  1. Run GTSP with only head pruning on a small dataset (e.g., NCI1) and compare accuracy/FLOPs to baseline.
  2. Apply token pruning on a large graph dataset (e.g., OGBG-HIV) and observe FLOPs reduction vs. accuracy drop.
  3. Combine layer pruning with weight pruning and measure both parameter reduction and generalization on a medium-scale dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does joint pruning of all compressible components in Graph Transformers (GTs) affect their performance compared to individual pruning strategies?
- Basis in paper: [explicit] The authors state that a joint pruning of all compressible components, considering the complicated interactions between them, is left as a topic for future research.
- Why unresolved: The paper focuses on the individual pruning of each component (input nodes, attention heads, model layers, and model weights) separately, without exploring the combined effect of pruning all components simultaneously.
- What evidence would resolve it: Experimental results comparing the performance of GTs with individually pruned components versus jointly pruned components across various datasets and GT models.

### Open Question 2
- Question: What is the optimal pruning ratio for different graphs when applying Graph Transformer Sparsification (GTSP) techniques?
- Basis in paper: [inferred] The authors mention that adjusting the pruning ratio for different graphs is a challenge and merits further exploration.
- Why unresolved: The paper does not provide a detailed analysis of how the pruning ratio should be adjusted based on the characteristics of different graphs, such as their size, density, or task-specific requirements.
- What evidence would resolve it: A comprehensive study examining the impact of different pruning ratios on the performance of GTs for various graph types and tasks, potentially leading to guidelines or heuristics for selecting optimal pruning ratios.

### Open Question 3
- Question: How does the sparsity introduced by GTSP affect the interpretability of Graph Transformers, particularly in identifying key nodes or substructures within graphs?
- Basis in paper: [explicit] The authors observe that GTSP enhances the interpretability of GTs by identifying key nodes in the graph that contribute significantly to graph property identification.
- Why unresolved: While the paper provides a visualization of token selection, it does not delve into the broader implications of sparsity on the interpretability of GTs or provide a systematic analysis of how sparsity affects the identification of important nodes or substructures.
- What evidence would resolve it: A detailed analysis of the interpretability of GTs before and after applying GTSP, focusing on the identification of key nodes, substructures, or motifs, and how this relates to the task-specific performance of the models.

## Limitations
- Framework's generalizability to extremely large-scale graphs (>10M nodes) remains unproven
- Computational overhead of differentiable masks during training not explicitly quantified
- Weight pruning assumes small-magnitude weights are redundant, which may not hold for all graph learning tasks

## Confidence
- High confidence: The empirical results showing 50% parameter reduction and 47.4% FLOPs reduction while maintaining or improving accuracy are well-supported by the experimental data.
- Medium confidence: The mechanism explaining how weight pruning prevents overfitting through gradual capacity reduction is theoretically sound but could benefit from more ablation studies.
- Low confidence: The claims about improved generalization on OGBG-HIV specifically need further validation on additional large-scale datasets.

## Next Checks
1. **Ablation study on pruning order**: Test different sequential orders of applying the four pruning dimensions to understand interaction effects and identify optimal pruning strategies.
2. **Scalability benchmark**: Evaluate GTSP on graphs with >10M nodes to quantify the computational overhead of differentiable masks and identify practical limits for each pruning dimension.
3. **Task transferability analysis**: Apply a GTSP-pruned model trained on one graph classification task to a different but related task to assess whether the pruning preserves cross-task generalization capabilities.