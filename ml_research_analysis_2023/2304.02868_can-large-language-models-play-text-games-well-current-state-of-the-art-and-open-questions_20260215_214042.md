---
ver: rpa2
title: Can Large Language Models Play Text Games Well? Current State-of-the-Art and
  Open Questions
arxiv_id: '2304.02868'
source_url: https://arxiv.org/abs/2304.02868
tags:
- chatgpt
- game
- zork
- down
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates whether ChatGPT can play text games by
  evaluating its performance on the classic game Zork. The study examines three key
  questions: (1) Can ChatGPT learn a world model from game walkthroughs?'
---

# Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions

## Quick Facts
- arXiv ID: 2304.02868
- Source URL: https://arxiv.org/abs/2304.02868
- Reference count: 3
- ChatGPT scored 10 points playing Zork, comparable to existing systems but far below human performance

## Executive Summary
This paper investigates whether ChatGPT can effectively play text games by evaluating its performance on Zork. The study examines three key questions: whether ChatGPT can learn a world model from game walkthroughs, infer goals during gameplay, and whether Zork serves as a good evaluation benchmark. Results show ChatGPT struggles with world model learning and navigation, achieving low accuracy even after reading walkthroughs. When playing Zork directly, ChatGPT scored 10 points, but with human intervention this improved to 40 points, surpassing current state-of-the-art systems. The findings suggest that while ChatGPT shows promise, it lacks fundamental properties of intelligence needed for effective text game play.

## Method Summary
The study evaluates ChatGPT's text game playing abilities through three experimental scenarios using the classic game Zork. First, ChatGPT is fed step-by-step walkthroughs and asked to answer navigation questions and predict destinations. Second, it is asked to infer goals at each step of gameplay. Third, ChatGPT plays Zork directly with human mediation, where a human translates game states into natural language and feeds ChatGPT's responses back as game commands. The evaluation measures accuracy in world model learning, goal inference capabilities, and final game scores.

## Key Results
- ChatGPT achieved only 35% accuracy on unseen navigation questions after reading walkthroughs, indicating poor world model construction
- When playing Zork directly, ChatGPT scored 10 points, comparable to existing systems but far below human performance
- With human intervention and memory augmentation, ChatGPT's score improved to 40 points, surpassing current state-of-the-art systems
- ChatGPT rarely gave meaningful goal inferences, mostly guessing low-level actions rather than identifying higher-level objectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT can use in-context learning from game walkthroughs to build partial world models
- Mechanism: When fed step-by-step game walkthroughs, ChatGPT can memorize location transitions and object descriptions, allowing it to answer simple navigation questions about seen paths
- Core assumption: ChatGPT's attention mechanism can retain and retrieve sequential game state information when provided in the correct format
- Evidence anchors: ChatGPT seems to perform decent, achieving a 75% accuracy on one-step questions after 70 steps of walkthrough

### Mechanism 2
- Claim: ChatGPT can simulate game play through human-mediated communication
- Mechanism: A human acts as an interface between ChatGPT and the game engine, translating game states into natural language and feeding ChatGPT's responses back as game commands
- Core assumption: ChatGPT can generate contextually appropriate game actions when provided with valid action options in natural language format
- Evidence anchors: ChatGPT scored 10 points, comparable to existing systems but far below human performance

### Mechanism 3
- Claim: ChatGPT can improve game performance with memory augmentation
- Mechanism: Reminding ChatGPT of its previous actions and current achievements helps it make more informed decisions and reduces repetition
- Core assumption: ChatGPT's decision-making improves when it maintains awareness of its game progress and avoids redundant actions
- Evidence anchors: When reminded of previous actions, ChatGPT could achieve a score of 40 within only 45 steps

## Foundational Learning

- Concept: World model construction
  - Why needed here: Understanding the game environment and spatial relationships is fundamental to effective text game play
  - Quick check question: Can the agent accurately predict the outcome of movement commands and navigate between locations?

- Concept: Goal inference
  - Why needed here: Text games often have implicit objectives that must be discovered through exploration and interaction
  - Quick check question: Can the agent identify high-level objectives beyond immediate actions and plan accordingly?

- Concept: Sequential decision making
  - Why needed here: Text games require chaining together multiple actions to achieve complex objectives
  - Quick check question: Can the agent plan multi-step sequences of actions to solve puzzles or reach distant locations?

## Architecture Onboarding

- Component map:
  Game engine (Zork implementation) -> Communication interface (human mediator) -> LLM (ChatGPT) -> World model (implicit in ChatGPT's knowledge) -> Goal tracker (implicit in ChatGPT's reasoning)

- Critical path:
  1. Game state → Natural language description
  2. Natural language → ChatGPT input
  3. ChatGPT response → Action selection
  4. Action → Game execution
  5. Score update → Progress tracking

- Design tradeoffs:
  Using a human mediator adds flexibility but limits scalability
  Relying on in-context learning vs. fine-tuning for game-specific knowledge
  Balancing between exploration and exploitation in unknown environments

- Failure signatures:
  Inability to navigate unseen paths
  Forgetting previous actions and achievements
  Generating nonsensical or repetitive responses
  Failing to infer high-level goals

- First 3 experiments:
  1. Test ChatGPT's ability to answer navigation questions about seen vs. unseen paths
  2. Evaluate performance with and without memory augmentation (reminding of previous actions)
  3. Compare ChatGPT's goal inference capabilities at different stages of gameplay

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can large language models learn to construct accurate world models from game walkthroughs alone?
- Basis in paper: The paper explicitly tests this by feeding ChatGPT the correct Zork walkthrough and asking it to answer navigation questions and predict destinations
- Why unresolved: ChatGPT failed to construct an accurate world model, performing poorly on navigation questions even after reading the walkthrough
- What evidence would resolve it: Future studies could test larger LLMs on more complex games with longer walkthroughs, measuring their ability to answer navigation questions and predict destinations in unseen scenarios

### Open Question 2
- Question: Can large language models infer goals during gameplay, or do they merely guess low-level actions?
- Basis in paper: The paper explicitly tests this by asking ChatGPT to state goals during gameplay and found it mostly guessed low-level actions rather than inferring meaningful objectives
- Why unresolved: ChatGPT rarely gave meaningful goal inferences, often suggesting actions that had already been completed or focusing on immediate tasks rather than higher-level objectives
- What evidence would resolve it: Future studies could track whether larger LLMs can maintain and update goal representations across longer gameplay sessions, and whether they can identify abstract objectives

### Open Question 3
- Question: Will increasing the size of large language models lead to emergent capabilities in text game play, or are fundamental architectural changes needed?
- Basis in paper: The paper concludes that ChatGPT lacks fundamental properties of intelligence needed for text game play, but doesn't rule out whether larger models might develop these capabilities
- Why unresolved: The paper demonstrates current limitations but doesn't test whether scaling up model size would overcome these limitations
- What evidence would resolve it: Testing future generations of larger LLMs on the same benchmarks, comparing their performance on world model learning, goal inference, and overall game scores to current models

## Limitations
- ChatGPT's performance heavily depends on prompt format and quality, making it difficult to isolate true capabilities from prompt engineering effects
- Human-mediated interface introduces variability and may artificially constrain or enhance ChatGPT's performance
- Evaluation is limited to a single classic text game (Zork), which may not generalize to other text game environments

## Confidence
- Low Confidence: Claims about ChatGPT's ability to learn world models from walkthroughs - performance suggests memorization rather than true understanding
- Medium Confidence: Claims about goal inference capabilities - ChatGPT can identify simple objectives but struggles with complex goals
- Medium Confidence: Claims about human-mediated gameplay improvement - 40-point score shows promise but methodology differs from existing systems

## Next Checks
1. Implement controlled prompt ablation study to isolate the effect of prompt engineering on performance
2. Evaluate ChatGPT on multiple text games with varying complexity to assess generalization
3. Create automated interface without human mediation to quantify the contribution of human assistance versus LLM capabilities