---
ver: rpa2
title: '2DDATA: 2D Detection Annotations Transmittable Aggregation for Semantic Segmentation
  on Point Cloud'
arxiv_id: '2309.11755'
source_url: https://arxiv.org/abs/2309.11755
tags:
- bounding
- point
- data
- points
- cloud
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes 2DDATA, a method to improve 3D semantic segmentation
  by leveraging 2D detection annotations. It introduces a Local Object Branch that
  processes points within 2D bounding boxes, allowing the 3D encoder to benefit from
  the prior knowledge of object locations.
---

# 2DDATA: 2D Detection Annotations Transmittable Aggregation for Semantic Segmentation on Point Cloud

## Quick Facts
- **arXiv ID**: 2309.11755
- **Source URL**: https://arxiv.org/abs/2309.11755
- **Reference count**: 22
- **Key outcome**: 2DDATA slightly outperforms 2DPASS in overall mIoU while adding minimal parameters (4.4M to 45.6M total)

## Executive Summary
This paper proposes 2DDATA, a method to improve 3D semantic segmentation by leveraging 2D detection annotations. It introduces a Local Object Branch that processes points within 2D bounding boxes, allowing the 3D encoder to benefit from the prior knowledge of object locations. 2DDATA is built on the 2DPASS architecture and replaces its Multi-Scale Fusion-to-Single Knowledge Distillation with the Local Object Branch. Experiments on the NuScenes dataset show that 2DDATA slightly outperforms 2DPASS in overall mIoU while adding minimal parameters. The method demonstrates the potential of using 2D annotations to enhance 3D models without increasing inference cost.

## Method Summary
2DDATA is a 3D semantic segmentation method that leverages 2D bounding box annotations to improve point cloud segmentation performance. The architecture consists of a 2D encoder, 3D encoder, and a novel Local Object Branch. During training, the method projects 3D points onto the 2D image plane, extracts corresponding image features, and processes points within bounding boxes through the Local Object Branch using class-aware attention. Knowledge is distilled from the fusion module to the 3D encoder using KL divergence loss. During inference, only the 3D encoder runs, maintaining zero additional computational cost. The method is built upon the 2DPASS architecture, replacing its MSFSKD component with the Local Object Branch.

## Key Results
- 2DDATA achieves 0.46 mIoU improvement over 2DPASS baseline on NuScenes validation set
- Method adds only 4.4M parameters to the 45.6M parameter 3D encoder
- Outperforms several state-of-the-art methods including PolarNet, SalsaNet, and Cylinder3D
- Maintains competitive performance while adding minimal inference cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 2DDATA improves 3D segmentation by transmitting bounding box prior information to the 3D encoder through a modality-specific Local Object Branch.
- Mechanism: The Local Object Branch processes points within 2D bounding boxes, using class-aware attention to inject prior knowledge about object classes into the 3D feature space. This allows the 3D encoder to benefit from object-level information without requiring full 3D annotations.
- Core assumption: Points within a 2D bounding box have high probability of belonging to the class labeled by that bounding box, making the bounding box a useful prior for segmentation.
- Evidence anchors:
  - [abstract] "We demonstrate that our simple design can transmit bounding box prior information to the 3D encoder model"
  - [section] "Local Object Branch is designed to deal with the points in a specific 2D bounding box, the 2D bounding box is a strong prior that indicate the points in the region is highly possible belonging to the class of bounding box labeled"
  - [corpus] Weak match - related works focus on weakly supervised segmentation but don't directly validate the bounding box prior transmission mechanism
- Break condition: If the assumption that bounding box regions contain mostly homogeneous object classes fails, the prior knowledge transmission becomes noisy and degrades segmentation quality.

### Mechanism 2
- Claim: The 2DDATA architecture achieves knowledge distillation from 2D to 3D without adding inference cost by only activating the fusion module during training.
- Mechanism: During training, 2DDATA fuses 2D image features with 3D point cloud features through the Local Object Branch, then uses KL divergence loss to distill this knowledge into the 3D encoder. During inference, only the 3D encoder runs, keeping computational overhead minimal.
- Core assumption: Knowledge distillation can effectively transfer information from the training-time fusion module to the inference-time 3D encoder without requiring the fusion module at inference.
- Evidence anchors:
  - [abstract] "2DDATA is built upon the 2DPASS architecture and replaces its Multi-Scale Fusion-to-Single Knowledge Distillation with the Local Object Branch"
  - [section] "The fusion module is only used during training, makes the training design more flexible"
  - [corpus] Weak match - related works discuss knowledge distillation but don't specifically validate zero-inference-cost multi-modality transfer
- Break condition: If the knowledge distillation process fails to effectively transfer information, the 3D encoder won't benefit from the 2D features, and performance gains will be minimal.

### Mechanism 3
- Claim: 2DDATA leverages the complementary information between 2D bounding boxes and 3D point clouds to improve segmentation of objects that are difficult to detect with LiDAR alone.
- Mechanism: By focusing on points within bounding boxes and using class-aware attention, 2DDATA enhances the 3D encoder's ability to segment objects with strong 2D visual features but sparse or ambiguous 3D representations.
- Core assumption: Objects that are well-represented in 2D images but poorly captured by LiDAR (due to size, reflectivity, or occlusion) will benefit most from this cross-modal information transfer.
- Evidence anchors:
  - [abstract] "By collecting more information from one existing modality, in our case, 2D bounding boxes in the camera, we expect the prior knowledge of bounding boxes is transmitted to the 3D encoder model"
  - [section] "The RGB image can be crafted for inquiring more 2D information"
  - [corpus] Weak match - related works focus on fusion but don't specifically validate improvement for poorly-represented 3D objects
- Break condition: If objects are equally well-represented in both modalities or if 2D information is noisy/corrupted, the complementary benefit diminishes and may even introduce errors.

## Foundational Learning

- Concept: Point cloud projection and coordinate transformation
  - Why needed here: Understanding how 3D points are projected onto 2D image planes and how coordinate systems are transformed is fundamental to implementing the Local Object Branch
  - Quick check question: How do you project a 3D point from LiDAR coordinates to 2D image pixel coordinates using camera intrinsic and extrinsic matrices?
  - Answer: Use the formula [ui, vi, 1]^T = (1/zi) × K × T × [xi, yi, zi, 1]^T where K is the camera intrinsic matrix and T is the camera extrinsic matrix

- Concept: Knowledge distillation and KL divergence
  - Why needed here: The training process relies on transferring knowledge from the fusion module to the 3D encoder using KL divergence loss
  - Quick check question: What is the mathematical form of KL divergence loss used in 2DDATA and what does it optimize?
  - Answer: LxM = DKL(S2D3D_l || S3D_l) where it pushes the 3D predictions closer to the fused predictions, enforcing knowledge transfer

- Concept: Attention mechanisms and class embeddings
  - Why needed here: The Local Object Branch uses class-aware attention to weight features based on the relationship between point classes and bounding box classes
  - Quick check question: How does the class-aware attention mechanism in 2DDATA use cosine similarity between class embeddings?
  - Answer: It computes Esimb = E · Eboxb / max(||E|| · ||Eboxb||, ε) to get similarity scores, then uses these as attention weights for the point features

## Architecture Onboarding

- Component map: 2D Encoder -> Point-to-Pixel Matching -> Local Object Branch -> 3D Encoder -> Class Embeddings -> Loss Functions

- Critical path: 
  1. Project 3D points to 2D image plane
  2. Extract corresponding 2D features
  3. Process points within bounding boxes through Local Object Branch
  4. Apply class-aware attention using bounding box class information
  5. Compute segmentation predictions and losses
  6. Backpropagate through both 2D and 3D branches

- Design tradeoffs:
  - Parameter efficiency vs. performance: Local Object Branch adds ~4.4M parameters to the 45.6M parameter 3D encoder, providing performance gains with minimal overhead
  - Training complexity vs. inference simplicity: Complex training with fusion module but simple inference with only 3D encoder
  - Bounding box quality vs. segmentation accuracy: Performance depends on accurate 2D bounding box annotations

- Failure signatures:
  - Poor segmentation of classes not covered by bounding boxes: Indicates Local Object Branch is dominating the 3D encoder's learning
  - Degraded performance on objects at image boundaries: Suggests point-to-pixel matching errors
  - Training instability: May indicate issues with the KL divergence loss weighting or class embedding initialization

- First 3 experiments:
  1. Baseline comparison: Train 2DDATA and 2DPASS on the same subset of NuScenes to verify the 0.46 mIoO improvement claim
  2. Ablation study: Remove the Local Object Branch to confirm it's the source of improvement and measure parameter impact
  3. Class-specific analysis: Evaluate performance on the interest classes (adult, child, bicycle, car, truck, motorcycle) vs. non-interest classes to understand the competition between Local Object Branch and 3D encoder

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of 2DDATA scale when using 2D segmentation annotations instead of 2D bounding boxes, and what is the optimal granularity of 2D segmentation masks for improving 3D semantic segmentation?
- Basis in paper: [explicit] The authors suggest that 2D segmentation annotations could provide more fine-grained prior information compared to 2D bounding boxes, potentially making the model more powerful.
- Why unresolved: The paper only uses 2D bounding boxes and does not explore the impact of using 2D segmentation masks or test different levels of segmentation granularity.
- What evidence would resolve it: Experiments comparing 2DDATA performance with 2D bounding boxes versus 2D segmentation masks of varying granularity on the same dataset, showing quantitative improvements in mIoU and class-specific metrics.

### Open Question 2
- Question: What is the impact of using automated 2D annotations (e.g., from pre-trained models) on the performance of 2DDATA, and how does annotation quality affect the trade-off between annotation cost and model accuracy?
- Basis in paper: [explicit] The authors mention that automated annotations could be used but might be biased, and the impacts need to be further addressed.
- Why unresolved: The paper does not test the use of automated 2D annotations or analyze how annotation quality affects model performance.
- What evidence would resolve it: Experiments using automated 2D annotations of varying quality levels, comparing their performance to human-annotated data, and analyzing the relationship between annotation quality, model accuracy, and annotation cost.

### Open Question 3
- Question: How does the Local Object Branch compete with the 3D encoder in terms of feature learning, and what is the optimal balance between these two components to maximize overall segmentation performance?
- Basis in paper: [explicit] The authors observe that class-specific mIoU for interest classes is lower than 2DPASS, suggesting competition between the Local Object Branch and 3D encoder, and mention that adjusting KL divergence loss weights might help.
- Why unresolved: The paper does not perform an in-depth analysis of the interaction between the Local Object Branch and 3D encoder or systematically explore the optimal balance between them.
- What evidence would resolve it: Detailed ablation studies varying the Local Object Branch architecture, KL divergence loss weights, and feature fusion strategies, along with analysis of feature representations to understand the competition dynamics.

## Limitations
- Modest performance improvement (0.46 mIoU) compared to baseline 2DPASS
- Heavy dependence on quality of 2D bounding box annotations
- Competitive but not dominant performance against state-of-the-art methods
- Potential competition between Local Object Branch and 3D encoder for feature learning

## Confidence

- **High Confidence**: The core claim that 2DDATA can transmit bounding box prior information to 3D encoders is well-supported by the experimental results and ablation studies. The architectural modifications are clearly described and reproducible.
- **Medium Confidence**: The assertion that knowledge distillation achieves zero-inference-cost multi-modality transfer is plausible but not extensively validated. The claim about improving segmentation of poorly-represented 3D objects needs more targeted analysis.
- **Low Confidence**: The paper doesn't adequately address potential failure modes, such as performance degradation on classes not covered by 2D bounding boxes or the impact of point-to-pixel matching errors.

## Next Checks
1. Conduct a targeted ablation study removing the Local Object Branch to quantify its exact contribution and verify the parameter efficiency claim.
2. Perform class-specific analysis comparing performance on interest classes (adult, child, bicycle, car, truck, motorcycle) versus non-interest classes to understand the competition between Local Object Branch and 3D encoder.
3. Evaluate model robustness by testing on corrupted 2D images or synthetic LiDAR data to assess sensitivity to modality-specific noise.