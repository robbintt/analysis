---
ver: rpa2
title: Can Language Models Solve Graph Problems in Natural Language?
arxiv_id: '2305.10037'
source_url: https://arxiv.org/abs/2305.10037
tags:
- node
- graph
- reasoning
- path
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NLGraph, a benchmark designed to evaluate
  whether large language models (LLMs) can explicitly solve graph problems expressed
  in natural language. The benchmark contains 29,370 problems across eight graph reasoning
  tasks of varying complexity, from simple connectivity and shortest path to advanced
  problems like maximum flow and simulating graph neural networks.
---

# Can Language Models Solve Graph Problems in Natural Language?

## Quick Facts
- arXiv ID: 2305.10037
- Source URL: https://arxiv.org/abs/2305.10037
- Reference count: 40
- Primary result: LLMs demonstrate preliminary graph reasoning abilities on simple tasks but struggle with complex problems, with proposed prompting techniques improving performance by 3.07% to 16.85%.

## Executive Summary
This paper introduces NLGraph, a benchmark designed to evaluate whether large language models can explicitly solve graph problems expressed in natural language. The benchmark contains 29,370 problems across eight graph reasoning tasks of varying complexity. The authors evaluate several LLMs using different prompting methods and find that while LLMs demonstrate preliminary graph reasoning abilities on simpler tasks, their performance diminishes on more complex problems and they are brittle to spurious correlations. The paper proposes two instruction-based prompting techniques—Build-a-Graph Prompting and Algorithmic Prompting—which improve LLM performance on graph reasoning tasks.

## Method Summary
The authors created NLGraph, a benchmark with 29,370 problems across eight graph reasoning tasks (connectivity, cycle, topological sort, shortest path, maximum flow, bipartite graph matching, Hamilton path, GNN simulation). They evaluated LLMs using various prompting techniques including zero-shot, few-shot, chain-of-thought, least-to-most, and self-consistency, plus two proposed methods (Build-a-Graph and Algorithmic Prompting). Models were evaluated on exact match accuracy and partial credit metrics for specific tasks.

## Key Results
- LLMs perform significantly better than random baseline on simple graph tasks (connectivity, cycle detection, shortest path)
- Performance degrades substantially on complex tasks like maximum flow and GNN simulation
- Proposed Build-a-Graph and Algorithmic Prompting techniques improve performance by 3.07% to 16.85% on simpler tasks
- LLMs are highly brittle to spurious correlations in graph representations, with performance drops exceeding 40% on certain datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs possess preliminary graph reasoning abilities on simple tasks
- Mechanism: LLMs can process textual graph descriptions and extract structural information to solve connectivity, cycle detection, and shortest path problems
- Core assumption: The textual representation of graphs provides sufficient information for LLMs to construct mental models of graph structure
- Evidence anchors: [abstract] "language models do demonstrate preliminary graph reasoning abilities", [section 4.1] "LLM performance on the connectivity, cycle, and shortest path tasks is significantly better than the RANDOM baseline"
- Break condition: Performance drops below random baseline, or accuracy remains at chance level

### Mechanism 2
- Claim: Advanced prompting techniques like chain-of-thought improve performance on simple graph tasks
- Mechanism: By generating intermediate reasoning steps, LLMs can break down graph problems into manageable sub-tasks
- Core assumption: LLMs can maintain logical consistency across multiple reasoning steps when explicitly prompted
- Evidence anchors: [abstract] "chain-of-thought prompting, least-to-most, and self-consistency successfully enhance the graph reasoning abilities of LLMs on simple tasks", [section 4.2] "chain-of-thought [Wei et al., 2022] and self-consistency [Wang et al., 2023] successfully improve performance on simple graph reasoning tasks"
- Break condition: When prompting becomes counterproductive (performance drops below zero-shot baseline)

### Mechanism 3
- Claim: LLMs are brittle to spurious correlations in graph representations
- Mechanism: LLMs rely on superficial patterns (like node mention frequency) rather than true structural reasoning
- Core assumption: LLMs learn statistical patterns from training data rather than causal relationships
- Evidence anchors: [abstract] "LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings", [section 4.4] "LLMs perform much worse than the general dataset with a performance drop of more than 40% across various settings"
- Break condition: When graph problems are constructed to break expected patterns (like chain vs clique structures)

## Foundational Learning

- Concept: Graph theory fundamentals (nodes, edges, paths, cycles, connectivity)
  - Why needed here: Understanding graph structure is essential for interpreting NLGraph problems and designing solutions
  - Quick check question: What's the difference between a path and a cycle in graph theory?

- Concept: Algorithm complexity and graph problem classification
  - Why needed here: Different graph problems require different algorithmic approaches, understanding this helps explain why LLMs struggle with complex tasks
  - Quick check question: Why might topological sort be harder for LLMs than connectivity checking?

- Concept: Prompt engineering and in-context learning
  - Why needed here: The paper relies heavily on different prompting techniques, understanding these is crucial for replicating and extending the work
  - Quick check question: How does chain-of-thought prompting differ from standard prompting?

## Architecture Onboarding

- Component map: NLGraph benchmark -> LLM evaluation -> Prompting techniques -> Performance analysis
- Critical path: Graph generation -> Problem formulation -> LLM evaluation -> Result analysis
- Design tradeoffs: Balanced vs imbalanced datasets, partial credit vs exact match evaluation, random vs structured graph generation
- Failure signatures: Performance drops on complex tasks, sensitivity to spurious correlations, in-context learning ineffectiveness
- First 3 experiments:
  1. Replicate connectivity task results with different LLMs
  2. Test BAG prompting on cycle detection task
  3. Evaluate spurious correlation brittleness with chain and clique datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design more effective prompting techniques to improve LLMs' performance on complex graph reasoning tasks like Hamilton path and bipartite graph matching?
- Basis in paper: Explicit - The paper proposes Build-a-Graph and Algorithmic Prompting, which improve performance on simpler tasks but remain ineffective for complex problems.
- Why unresolved: The paper demonstrates that while these techniques help with easier graph reasoning tasks, they fail to significantly improve performance on more complex problems.
- What evidence would resolve it: Systematic experiments comparing various prompting strategies on complex graph reasoning tasks, with clear demonstrations of performance improvements over baseline methods.

### Open Question 2
- Question: What is the relationship between problem scale (graph size, path length, etc.) and LLM performance in graph reasoning tasks?
- Basis in paper: Inferred - The paper shows that model performance steadily drops as the number of nodes on the shortest path increases.
- Why unresolved: While the paper provides initial evidence of scale sensitivity, a comprehensive understanding of how different problem dimensions affect LLM performance remains lacking.
- What evidence would resolve it: Detailed empirical studies mapping LLM performance across various graph sizes, densities, and problem dimensions.

### Open Question 3
- Question: Can fine-tuning approaches effectively elicit graph reasoning abilities in LLMs, and if so, what training strategies work best?
- Basis in paper: Explicit - The paper mentions fine-tuning as a potential future direction but notes that their initial attempts with OPT-2.7B failed.
- Why unresolved: The paper's limited fine-tuning experiments were unsuccessful, but the reasons for failure and potential alternative approaches remain unexplored.
- What evidence would resolve it: Successful fine-tuning experiments using various architectures, training objectives, and data augmentation strategies.

## Limitations

- Evaluation focuses on English-language graph problems, limiting generalizability to other languages
- Benchmark relies on synthetic data generation rather than real-world graph problems
- Evaluation is limited to specific LLM architectures (GPT-3/4 variants)
- Computational resources required for comprehensive evaluation may limit reproducibility

## Confidence

**High Confidence (4/5):**
- LLMs demonstrate preliminary graph reasoning abilities on simple tasks
- Performance degrades significantly on complex graph problems
- Standard prompting techniques improve performance on simple tasks
- LLMs are brittle to spurious correlations in graph representations

**Medium Confidence (3/5):**
- The proposed Build-a-Graph and Algorithmic Prompting techniques provide consistent improvements
- In-context learning remains effective for complex graph reasoning tasks
- The partial credit metrics adequately capture LLM performance nuances

**Low Confidence (2/5):**
- The benchmark comprehensively represents all important graph reasoning capabilities
- The synthetic graph generation methods produce realistic problem distributions
- The observed performance gaps are primarily due to model limitations

## Next Checks

1. **Cross-linguistic Generalization Test**: Evaluate the same NLGraph benchmark using multilingual LLMs to determine whether graph reasoning capabilities transfer across languages, particularly for tasks where LLMs show strong performance in English.

2. **Real-world Problem Transfer**: Replace synthetic graph problems with real-world graph datasets (e.g., social networks, biological networks) to test whether LLM performance patterns observed on NLGraph persist when dealing with naturally occurring graph structures and noise.

3. **Alternative Architecture Evaluation**: Test the NLGraph benchmark on open-source LLMs with comparable parameter counts (e.g., LLaMA-2, Mistral) to determine whether the observed limitations are model-specific or represent fundamental constraints in LLM-based graph reasoning approaches.