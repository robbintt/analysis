---
ver: rpa2
title: A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models
arxiv_id: '2303.10420'
source_url: https://arxiv.org/abs/2303.10420
tags:
- text
- shot
- answer
- gpt-3
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study comprehensively analyzes the capabilities of GPT-3 and
  GPT-3.5 series models across nine natural language understanding tasks using 21
  datasets. The research evaluates six representative models, including davinci, text-davinci-001,
  code-davinci-002, text-davinci-002, text-davinci-003, and gpt-3.5-turbo, in both
  zero-shot and few-shot scenarios.
---

# A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models

## Quick Facts
- arXiv ID: 2303.10420
- Source URL: https://arxiv.org/abs/2303.10420
- Reference count: 40
- Six models evaluated across nine NLU tasks using 21 datasets, revealing that RLHF training compromises task-solving capabilities while improving human-like responses.

## Executive Summary
This study provides a comprehensive evaluation of GPT-3 and GPT-3.5 series models across nine natural language understanding tasks using 21 datasets. The research examines six representative models including davinci, text-davinci-001, code-davinci-002, text-davinci-002, text-davinci-003, and gpt-3.5-turbo in both zero-shot and few-shot scenarios. The findings reveal that model performance does not improve monotonically with evolution, particularly after the introduction of RLHF training. While RLHF enhances human-like response generation, it compromises task-solving capabilities in certain cases. The study also identifies opportunities for improvement in model robustness.

## Method Summary
The study evaluates six GPT models using OpenAI's official API across 21 datasets spanning nine NLU tasks. Each task is tested in zero-shot and few-shot scenarios with three prompts per dataset. TextFlint is used to assess model robustness through various input transformations. Performance metrics include accuracy, F1 score, and exact match, with careful prompt design to ensure consistent evaluation conditions. The analysis compares models across different training strategies and architectural generations to understand capability evolution.

## Key Results
- Model performance does not improve gradually with evolution; RLHF-trained models show reduced task-solving accuracy despite enhanced human-like responses
- In-context learning effectiveness varies significantly by task complexity, with basic tasks performing well in zero-shot while complex tasks require few-shot examples
- Robustness remains a challenge across all model versions, with no proportional improvement alongside capability enhancements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLHF training strategy improves human-like response generation at the cost of task-solving accuracy.
- Mechanism: RLHF uses human feedback to reward more natural, aligned responses, which may favor fluency over task precision.
- Core assumption: Human raters prefer conversational style over strict correctness in NLU outputs.
- Evidence anchors:
  - [abstract] states RLHF "enhances the models' ability to generate human-like responses, it also compromises their ability to solve some tasks."
  - [section 4.2.2] shows text-davinci-003 (RLHF-trained) underperforms text-davinci-002 on MRC tasks.
  - [corpus] includes a related paper "How Robust is GPT-3.5 to Predecessors?" suggesting RLHF may trade off robustness.
- Break condition: If human feedback explicitly rewards task accuracy, RLHF could improve both fluency and performance.

### Mechanism 2
- Claim: In-context learning ability depends on pre-training exposure and task complexity.
- Mechanism: The davinci model's strong performance without fine-tuning shows foundational pre-training covers basic instruction understanding; few-shot examples help with complex tasks like NER and POS.
- Core assumption: Pre-training data included diverse instruction-following patterns.
- Evidence anchors:
  - [section 4.1.1] shows davinci performs well on NLI, SC, SM, WSC in zero-shot but improves in few-shot for NER and POS.
  - [section 4.1.2] reveals davinci lacks instruction comprehension without explicit cues like "Answer".
  - [corpus] lacks direct evidence of pre-training composition.
- Break condition: If pre-training corpus lacked instruction-following data, in-context learning would fail even in zero-shot.

### Mechanism 3
- Claim: Model robustness does not improve proportionally with capability upgrades.
- Mechanism: Architectural or training changes that boost performance do not necessarily enhance resistance to input perturbations.
- Core assumption: Robustness requires explicit adversarial training or data augmentation.
- Evidence anchors:
  - [abstract] states "there is still room for improvement in areas such as model robustness."
  - [section 4.2.2] shows robustness gaps persist across model generations in MRC and NER tasks.
  - [corpus] neighbor paper "How Robust is GPT-3.5 to Predecessors?" directly addresses robustness.
- Break condition: If future models incorporate dedicated robustness training, this pattern could reverse.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Understanding how models generalize from few examples without fine-tuning is central to comparing zero-shot vs few-shot performance.
  - Quick check question: What is the difference between in-context learning and traditional fine-tuning?
- Concept: Instruction following vs task solving
  - Why needed here: Distinguishing between a model's ability to parse instructions and its ability to execute the task is key to interpreting performance drops after RLHF.
  - Quick check question: Why might a model produce fluent but incorrect answers?
- Concept: Prompt sensitivity
  - Why needed here: Evaluating how different prompts affect performance is necessary for fair model comparison.
  - Quick check question: How can prompt wording alter a model's output without changing the underlying task?

## Architecture Onboarding

- Component map: Base GPT-3 series (davinci, text-davinci-001) → GPT-3.5 base (code-davinci-002) → InstructGPT (text-davinci-002) → RLHF-enhanced (text-davinci-003) → Chat-optimized (gpt-3.5-turbo).
- Critical path: Input prompt → tokenization → transformer layers → output generation → evaluation metric.
- Design tradeoffs: Larger models (davinci) have better raw generation but weaker instruction comprehension; smaller chat models (gpt-3.5-turbo) are cheaper but may sacrifice task accuracy.
- Failure signatures: Non-analyzable outputs (especially in POS/NER), format deviations, poor robustness to perturbations.
- First 3 experiments:
  1. Run zero-shot evaluation on a simple NLI dataset with and without "Answer" suffix to confirm instruction dependence.
  2. Compare one-shot vs three-shot performance on NER to measure in-context learning gains.
  3. Test a robustness transformation (e.g., AddDiff) on MRPC to verify stability across model versions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific training strategies (FeedME, PPO, etc.) impact model performance on different NLU tasks?
- Basis in paper: [explicit] The paper discusses how different training strategies like FeedME and PPO affect model capabilities, noting that RLHF enhances human-like responses but compromises task-solving capabilities in some cases.
- Why unresolved: The paper does not provide detailed analysis of how each specific training strategy impacts performance on individual NLU tasks, only general observations about their effects.
- What evidence would resolve it: A systematic evaluation comparing model performance across all NLU tasks when trained with different strategies, controlling for other variables.

### Open Question 2
- Question: Why does increasing the number of examples in prompts not always improve model performance?
- Basis in paper: [explicit] The paper states that "few-shot scenarios do not always improve model performance" and notes this depends on model, task, prompt design, and example selection.
- Why unresolved: The paper does not provide a clear explanation for when and why few-shot learning fails to improve performance, only that it is task-dependent.
- What evidence would resolve it: Controlled experiments varying prompt length, example quality, and task complexity to identify patterns in when few-shot learning is effective versus detrimental.

### Open Question 3
- Question: What architectural differences between GPT-3.5 models explain their varying performance across tasks?
- Basis in paper: [inferred] The paper observes that gpt-3.5-turbo performs well on some tasks but poorly on others, suggesting architectural differences, but does not investigate these differences directly.
- Why unresolved: The paper does not explore the underlying architectural changes between GPT-3.5 models that could explain their task-specific performance variations.
- What evidence would resolve it: Comparative analysis of model architectures, parameter counts, and attention mechanisms between GPT-3.5 variants to identify correlations with task performance.

## Limitations
- API rate limits prevented full dataset testing for some models, potentially introducing sampling bias
- Focus on nine NLU tasks may not generalize to other domains like code generation or multi-modal applications
- Pre-training data composition for each model evolution remains undisclosed, limiting mechanistic explanations

## Confidence
- High confidence: The finding that RLHF-trained models show reduced task-solving performance compared to pre-RLHF models, supported by direct comparisons in MRC tasks.
- Medium confidence: The claim that in-context learning ability varies by task complexity, as this relies on performance patterns across multiple datasets with different characteristics.
- Low confidence: The assertion that model robustness hasn't improved proportionally with capability upgrades, as the study provides directional evidence but lacks comprehensive robustness benchmarks.

## Next Checks
1. Replicate the MRC task comparisons between text-davinci-002 and text-davinci-003 using a fixed subset of examples to verify the performance drop after RLHF implementation.
2. Test prompt sensitivity by systematically varying instruction wording (e.g., adding/removing "Answer" suffix) across all models on NLI tasks to confirm instruction dependence patterns.
3. Evaluate model robustness by applying TextFlint transformations to POS tagging datasets and measuring performance degradation across all six model versions to validate robustness claims.