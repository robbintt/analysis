---
ver: rpa2
title: 'Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for
  Indonesian Language'
arxiv_id: '2311.01757'
source_url: https://arxiv.org/abs/2311.01757
tags:
- sentiment
- aspect
- task
- tasks
- term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Indo LEGO-ABSA is an Indonesian language model for aspect-based
  sentiment analysis that extends the LEGO-ABSA framework using multitask learning
  and prompting. It employs the mT5 model to simultaneously perform five tasks: Aspect
  Sentiment Triplet Extraction (79.55% F1), Unified ABSA (86.09% F1), Aspect Opinion
  Pair Extraction (79.85% F1), Aspect Term Extraction (87.45% F1), and Opinion Term
  Extraction (88.09% F1).'
---

# Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language

## Quick Facts
- arXiv ID: 2311.01757
- Source URL: https://arxiv.org/abs/2311.01757
- Reference count: 33
- Key outcome: Achieves F1-scores of 79.55-88.09% across five ABSA tasks using multitask learning with mT5

## Executive Summary
Indo LEGO-ABSA is an Indonesian language model for aspect-based sentiment analysis that extends the LEGO-ABSA framework using multitask learning and prompting. The model employs the mT5 architecture to simultaneously perform five ABSA tasks on hotel domain data, achieving strong performance metrics. Error analysis identified annotation inconsistencies, informal language, and complex sentence structures as key challenges. The study demonstrates that multitask learning with appropriate prompt formats improves ABSA performance in Indonesian compared to single-task approaches.

## Method Summary
Indo LEGO-ABSA uses the mT5 model with a generative approach based on the LEGO-ABSA framework. The model is trained on a hotel domain dataset in Indonesian with 5000 texts (3000 train, 1000 validation, 1000 test). It employs multitask learning to simultaneously perform five ABSA tasks: Aspect Sentiment Triplet Extraction, Unified ABSA, Aspect Opinion Pair Extraction, Aspect Term Extraction, and Opinion Term Extraction. The model uses specific prompt and answer formats, with sentiment element tuples separated by semicolons. Training involves 10 epochs with learning rate of 3e-4, batch size of 8, and 2 gradient accumulation steps.

## Key Results
- Aspect Sentiment Triplet Extraction: 79.55% F1
- Unified ABSA: 86.09% F1
- Aspect Opinion Pair Extraction: 79.85% F1
- Aspect Term Extraction: 87.45% F1
- Opinion Term Extraction: 88.09% F1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multitask learning with prompting improves ABSA performance by leveraging shared inductive biases across tasks
- Mechanism: The model is trained on multiple ABSA tasks simultaneously using a shared mT5 architecture. The prompt format incorporates task-specific instructions that help the model distinguish between tasks while sharing learned representations.
- Core assumption: Tasks within ABSA share sufficient semantic overlap that joint training provides regularization and knowledge transfer benefits
- Evidence anchors:
  - [abstract] "The model uses a generative approach with the LEGO-ABSA prompt and answer format, trained on a hotel domain dataset in Indonesian"
  - [section] "We label the outcome of the comparison as Indo LEGO-ABSA. Subsequently, we compare Indo LEGO-ABSA with GAS Indonesia... The comparison result can be seen in Table II"
  - [corpus] Weak evidence - no corpus papers directly support this specific multitask + prompt combination for Indonesian
- Break condition: Tasks become too dissimilar or prompts interfere with each other's task representations

### Mechanism 2
- Claim: The LEGO-ABSA prompt format improves performance by providing structured output expectations familiar to the mT5 architecture
- Mechanism: The prompt includes the answer format within the input, using T5's denoising training pattern. This reduces the cognitive load on the model by making output expectations explicit and consistent across tasks.
- Core assumption: mT5's pre-training on similar sequence-to-sequence patterns makes it more effective when prompted with structured output formats
- Evidence anchors:
  - [section] "Our answer format follows GAS's pattern (extraction paradigm), with sentiment element tuples separated by semicolons. Additionally, we incorporate LEGO-ABSA's prompt structure, integrating the answer format directly within the prompt"
  - [section] "The obtained results include an f1-score of 79.55% for the Aspect Sentiment Triplet Extraction task, 86.09% for Unified Aspect-based Sentiment Analysis"
  - [corpus] Weak evidence - no corpus papers specifically validate this prompt format choice
- Break condition: The prompt format becomes too rigid, preventing the model from adapting to novel task requirements

### Mechanism 3
- Claim: Using mT5 (base) as the foundational model provides optimal balance between capacity and efficiency for Indonesian ABSA
- Mechanism: mT5-base has sufficient parameters (580M) to capture Indonesian linguistic patterns while remaining computationally efficient for multitask training. The model's multilingual pre-training provides a foundation for Indonesian-specific fine-tuning.
- Core assumption: The mT5 architecture's multilingual capabilities transfer effectively to Indonesian, and the base size is sufficient for the hotel domain dataset
- Evidence anchors:
  - [section] "We choose MBART (large), mT5 (base), and XGLM (564 million parameters) as our model options" and "We obtained candidate solution outcomes using mT5 as the foundational model"
  - [section] "The obtained results include an f1-score of 79.55% for the Aspect Sentiment Triplet Extraction task, 86.09% for Unified Aspect-based Sentiment Analysis"
  - [corpus] Weak evidence - no corpus papers specifically compare mT5-base to other models for Indonesian ABSA
- Break condition: The dataset grows substantially in size or complexity, requiring larger model capacity

## Foundational Learning

- Concept: Sequence-to-sequence modeling fundamentals
  - Why needed here: The entire framework relies on generating structured output from input text using encoder-decoder architecture
  - Quick check question: Can you explain how the attention mechanism in transformer-based seq2seq models enables context-aware generation?

- Concept: Prompt engineering principles
  - Why needed here: The performance depends critically on how prompts are structured to guide the model's generation behavior
  - Quick check question: What are the key differences between prefix prompts and one-token prompts, and when would each be preferable?

- Concept: Multitask learning optimization
  - Why needed here: The model must balance learning across five different ABSA tasks without catastrophic forgetting
  - Quick check question: How does multitask training typically affect convergence rates compared to single-task training, and what strategies help manage this?

## Architecture Onboarding

- Component map: Input → Prompt processing → Encoding → Decoding → Structured output generation → F1-score evaluation

- Critical path: Input → Prompt processing → Encoding → Decoding → Structured output generation → F1-score evaluation

- Design tradeoffs:
  - Model size vs. inference speed (mT5-base chosen over larger alternatives)
  - Prompt specificity vs. generalization (LEGO-ABSA format chosen over simpler prompts)
  - Task complexity vs. training stability (all five tasks trained jointly rather than sequentially)
  - Dataset size vs. model capacity (3000 training examples with mT5-base)

- Failure signatures:
  - Low F1-scores across all tasks suggest fundamental architecture issues
  - Task-specific performance drops indicate task interference or prompt confusion
  - Inconsistent output formats suggest prompt structure problems
  - Overfitting on training data suggests insufficient model regularization

- First 3 experiments:
  1. Single-task training for ASTE only to establish baseline performance
  2. Add one additional task (ATE) to test multitask benefits
  3. Implement all five tasks with different prompt formats to identify optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Indo LEGO-ABSA compare to multilingual models fine-tuned on English ABSA datasets?
- Basis in paper: [inferred] The paper only compares Indo LEGO-ABSA to GAS Indonesia and vanilla LEGO-ABSA, both trained on Indonesian data. No comparison is made to English-trained models.
- Why unresolved: The study focuses exclusively on Indonesian language models and datasets, leaving a gap in understanding cross-lingual transfer capabilities.
- What evidence would resolve it: Experimental results comparing Indo LEGO-ABSA performance on English ABSA datasets against English-trained models like BERT-PT or fine-tuned T5 models.

### Open Question 2
- Question: What is the optimal task combination strategy for Indo LEGO-ABSA when considering computational efficiency versus performance?
- Basis in paper: [explicit] The paper states that "all task levels, including single, basic, and advanced tasks" were used, but does not explore different combinations or their trade-offs.
- Why unresolved: While the paper identifies that multitask learning improves performance, it doesn't investigate which specific task combinations provide the best efficiency-performance balance.
- What evidence would resolve it: Systematic experiments comparing different task combination strategies with metrics for both performance and computational cost.

### Open Question 3
- Question: How does Indo LEGO-ABSA perform on domain-specific ABSA tasks beyond hotel reviews, such as social media or product reviews?
- Basis in paper: [explicit] The model is trained and evaluated only on hotel domain data, with no exploration of other domains.
- Why unresolved: The paper demonstrates success in one specific domain but doesn't establish whether the approach generalizes to other text types with different linguistic characteristics.
- What evidence would resolve it: Performance evaluation of Indo LEGO-ABSA on ABSA datasets from multiple domains with domain-specific metrics.

### Open Question 4
- Question: What is the impact of different prompt formats on Indo LEGO-ABSA's performance for various ABSA subtasks?
- Basis in paper: [explicit] The paper uses LEGO-ABSA prompt format but only mentions prefix prompt and one token prompt as alternatives without experimental comparison.
- Why unresolved: While the study identifies the optimal prompt format, it doesn't explore how different prompt formats affect specific ABSA tasks differently.
- What evidence would resolve it: Controlled experiments testing different prompt formats across all ABSA subtasks with task-specific performance metrics.

## Limitations
- Dataset size of 5000 examples is relatively small for deep learning approaches
- Hotel domain specificity limits generalizability to other Indonesian text types
- No ablation studies to isolate the contribution of each component (multitask learning vs. prompt format vs. mT5 architecture)

## Confidence
**High Confidence**: The overall framework architecture combining mT5 with multitask learning and prompting is technically sound; F1-score metrics and evaluation methodology follow standard ABSA practices; error analysis identifying annotation inconsistencies is well-documented.

**Medium Confidence**: The claim that multitask learning improves performance over single-task approaches (no ablation studies provided); the effectiveness of the LEGO-ABSA prompt format specifically (compared to simpler alternatives); the generalizability of results beyond the hotel domain.

**Low Confidence**: The assumption that mT5-base is the optimal model size for this task (no comparison with other sizes); claims about knowledge transfer between tasks (not empirically validated); the impact of dataset modifications for implicit aspect terms (implementation details not fully specified).

## Next Checks
1. **Ablation Study Implementation**: Train single-task versions of each ABSA task using the same mT5-base architecture and prompts, then compare performance to the multitask model to quantify the contribution of multitask learning.

2. **Cross-Domain Validation**: Test the trained Indo LEGO-ABSA model on restaurant reviews or product reviews in Indonesian to assess domain generalization and identify performance drops that would indicate domain-specific overfitting.

3. **Prompt Format Comparison**: Implement and evaluate simpler prompt formats (prefix-only or one-token prompts) alongside the LEGO-ABSA format to determine if the complex prompt structure provides measurable performance benefits or if simpler alternatives could achieve similar results.