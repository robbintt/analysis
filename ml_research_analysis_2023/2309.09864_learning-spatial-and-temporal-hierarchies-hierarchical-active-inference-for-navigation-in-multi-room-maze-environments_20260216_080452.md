---
ver: rpa2
title: 'Learning Spatial and Temporal Hierarchies: Hierarchical Active Inference for
  navigation in Multi-Room Maze Environments'
arxiv_id: '2309.09864'
source_url: https://arxiv.org/abs/2309.09864
tags:
- environment
- agent
- rooms
- each
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hierarchical active inference model for
  navigation in multi-room maze environments. The model combines a cognitive map,
  an allocentric model, and an egocentric model to learn spatial and temporal hierarchies
  from pixel-based observations.
---

# Learning Spatial and Temporal Hierarchies: Hierarchical Active Inference for navigation in Multi-Room Maze Environments

## Quick Facts
- arXiv ID: 2309.09864
- Source URL: https://arxiv.org/abs/2309.09864
- Reference count: 40
- This paper introduces a hierarchical active inference model for navigation in multi-room maze environments that learns spatial and temporal hierarchies from pixel-based observations.

## Executive Summary
This paper presents a hierarchical active inference model for autonomous navigation in multi-room maze environments. The model learns spatial and temporal hierarchies from pixel-based observations through a three-layer architecture consisting of a cognitive map, an allocentric model, and an egocentric model. The cognitive map captures the maze structure as a topological graph, the allocentric model constructs place representations by integrating observations and poses, and the egocentric model manages motion and imagines action consequences. Experiments in mini-grid environments demonstrate the model's ability to efficiently explore new environments and reach goals while outperforming baseline RL models.

## Method Summary
The proposed model operates within a hierarchical active inference scheme, planning at different time scales. The top layer builds a topological graph representing the environment's structure, using continuous attractor networks to maintain spatial relationships between locations. The middle layer constructs place representations by integrating sequences of observations and poses, creating coherent representations of each room. The bottom layer manages motion and imagines action consequences using an egocentric perspective. Each layer minimizes Expected Free Energy (EFE) at its respective timescale, balancing goal-directed behavior and curiosity-driven exploration. The models are trained separately using free energy loss functions with stochastic gradient descent on data collected from random action sequences in training environments.

## Key Results
- The model achieves high exploration success (>90% coverage) and goal-reaching performance in multi-room mazes
- Outperforms baseline RL models (C-BET, RND, Curiosity, Count) in navigation tasks
- Demonstrates resistance to aliasing and accurate place recognition across different rooms
- Shows generalization capability to larger room sizes beyond training data
- Enables accurate long-term predictions across rooms with low computational demands regardless of environment scale

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hierarchical active inference balances goal-directed behavior and curiosity-driven exploration across nested timescales
- **Mechanism**: The model uses Expected Free Energy (EFE) minimization at each layer with different temporal resolutions: coarse (locations), mid (places), and fine (actions). This allows simultaneous planning at different spatial resolutions while maintaining coherent exploration-exploitation trade-offs
- **Core assumption**: Different layers can optimize EFE independently while maintaining coherent behavior through the hierarchical structure
- **Evidence anchors**:
  - [abstract]: "combining curiosity-driven exploration with goal-oriented behaviour at the different levels of reasoning from context to place to motion"
  - [section]: "The model operates within a hierarchical active inference scheme, planning at different time scales"
  - [corpus]: Weak - no direct corpus evidence for this specific hierarchical EFE balancing mechanism
- **Break condition**: If the hierarchical coordination breaks, layers may conflict in their exploration/exploitation decisions, leading to suboptimal navigation

### Mechanism 2
- **Claim**: The cognitive map learns topological structure as a graph while maintaining spatial relationships through continuous attractor networks
- **Mechanism**: The top layer builds a topological graph where nodes represent distinct locations (rooms) and edges represent traversable connections. Relative rotation and translation are tracked using continuous attractor networks to maintain spatial coherence between nodes
- **Core assumption**: The CAN can accurately maintain spatial relationships between locations without absolute coordinates
- **Evidence anchors**:
  - [section]: "The cognitive map forms a comprehensive representation of the environment, enabling the agent to navigate by formulating beliefs over its surroundings"
  - [section]: "In order to maintain the spatial structure between locations, the agent keeps track of its relative rotation and translation using a continuous attractor network (CAN)"
  - [corpus]: Weak - no direct corpus evidence for the CAN-based spatial relationship maintenance
- **Break condition**: If the CAN fails to track relative positions accurately, the topological graph may lose spatial coherence, making navigation planning unreliable

### Mechanism 3
- **Claim**: Allocentric model creates coherent place representations by integrating sequences of observations and poses
- **Mechanism**: The middle layer constructs place representations (zT) by integrating sequences of observations (sT) and poses (pT) over time. When observations no longer align with predictions, the model resets and creates a new place representation
- **Core assumption**: Integration of sequential observations and poses can create stable, distinctive place representations that capture room structure
- **Evidence anchors**:
  - [section]: "This model functions at a finer time scale (t), forming a belief over the place by integrating a sequence of observations (st) and poses (pt) to generate this representation"
  - [section]: "As the agent moves from one place to another, once the current observations do not align with the previously formed prediction about the place, the allocentric model resets its place description"
  - [corpus]: Weak - no direct corpus evidence for this specific sequential integration mechanism
- **Break condition**: If the integration process fails to distinguish between similar rooms, the agent may confuse different locations, leading to navigation errors

## Foundational Learning

- **Concept**: Free Energy Principle
  - **Why needed here**: Active inference is fundamentally built on minimizing free energy, which balances accuracy and complexity in model predictions
  - **Quick check question**: What is the relationship between prediction error minimization and free energy minimization in active inference?

- **Concept**: Generative modeling
  - **Why needed here**: The entire system is based on a hierarchical generative model that explains observations through latent states and actions
  - **Quick check question**: How does the generative model structure in Equation 1 relate to the three-layer architecture?

- **Concept**: Bayesian inference
  - **Why needed here**: The model performs Bayesian inference to estimate latent states (places, locations) from observations
  - **Quick check question**: What role does the approximate posterior Qϕ(z|ot, pt) play in the allocentric model's learning?

## Architecture Onboarding

- **Component map**: Observation → Egocentric prediction → Allocentric place integration → Cognitive map location update → Hierarchical EFE planning → Action selection
- **Critical path**: Observation → Egocentric prediction → Allocentric place integration → Cognitive map location update → Hierarchical EFE planning → Action selection
- **Design tradeoffs**:
  - Hierarchy vs. simplicity: More layers enable better temporal abstraction but increase complexity
  - Graph-based vs. metric maps: Topological representation is more scalable but less precise
  - Sequential integration vs. frame-based representation: Better for aliasing but requires more memory
- **Failure signatures**:
  - Egocentric model: High prediction error, frequent collisions
  - Allocentric model: Confusion between similar rooms, place drift
  - Cognitive map: Incorrect topology, disconnected components
- **First 3 experiments**:
  1. Test egocentric model in a simple maze with known layout to verify collision detection and action prediction
  2. Test allocentric model's place recognition by having it revisit the same room through different doors
  3. Test full hierarchy in a 2x2 room environment with two visually identical rooms to verify topological learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the hierarchical active inference model handle changes in the environment, such as altered tile colors or dynamic obstacles?
- Basis in paper: [inferred] The paper mentions that the model might struggle to detect environmental changes, although it suggests that a new place will replace or be added with the previous one in the cognitive map.
- Why unresolved: The paper does not provide specific details on how the model adapts to environmental changes or handles dynamic obstacles.
- What evidence would resolve it: Experiments demonstrating the model's performance in environments with altered tile colors or dynamic obstacles, showing how it adapts and maintains accurate navigation.

### Open Question 2
- Question: Can the model generalize to more complex and realistic environments beyond the mini-grid setup, such as the Memory maze or Habitat environments?
- Basis in paper: [explicit] The paper mentions that while the model demonstrates promising performance in a mini-grid environment, its application to more realistic scenarios could lead to more practical implementations.
- Why unresolved: The paper does not provide experimental results or analysis of the model's performance in more complex and realistic environments.
- What evidence would resolve it: Experimental results and analysis of the model's performance in more complex and realistic environments, demonstrating its ability to generalize and adapt to different scenarios.

### Open Question 3
- Question: How does the model handle the trade-off between exploration and exploitation in different types of environments, and can it learn an optimal balance?
- Basis in paper: [explicit] The paper discusses the model's ability to balance goal-directed behavior and curiosity-driven exploration at different time scales, but does not delve into the specifics of how it handles the trade-off in different environments.
- Why unresolved: The paper does not provide a detailed analysis of the model's exploration-exploitation trade-off in various environments or discuss how it learns an optimal balance.
- What evidence would resolve it: Experiments and analysis showing the model's performance in different environments with varying levels of exploration and exploitation, demonstrating its ability to learn and maintain an optimal balance.

## Limitations
- Limited empirical validation with only basic RL baselines compared, lacking benchmark against advanced hierarchical RL methods
- Generalization claims tested only on larger room sizes, not on more complex topological structures or different maze layouts
- Computational efficiency claims lack quantitative analysis of runtime or memory requirements

## Confidence

- **High confidence**: The hierarchical architecture design and its conceptual motivation for multi-scale spatial reasoning
- **Medium confidence**: The effectiveness of EFE minimization for balancing exploration-exploitation at different timescales, based on the experimental results in controlled mini-grid environments
- **Low confidence**: Claims about resistance to aliasing and accurate long-term predictions across rooms, as these are only demonstrated in limited experimental settings

## Next Checks

1. **Cross-environment generalization**: Test the model on maze environments with different topological structures (e.g., circular layouts, multiple paths between rooms) to verify the cognitive map's ability to learn diverse spatial hierarchies

2. **Benchmark comparison**: Compare performance against state-of-the-art hierarchical RL methods (e.g., HIRO, Feudal Networks) on identical navigation tasks to assess relative effectiveness

3. **Robustness analysis**: Systematically vary observation noise, partial observability, and room appearance similarity to quantify the model's resistance to aliasing and place recognition accuracy under challenging conditions