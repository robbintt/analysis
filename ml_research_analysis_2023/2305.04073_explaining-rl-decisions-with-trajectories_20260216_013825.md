---
ver: rpa2
title: Explaining RL Decisions with Trajectories
arxiv_id: '2305.04073'
source_url: https://arxiv.org/abs/2305.04073
tags:
- trajectory
- data
- trajectories
- attribution
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach for explaining the decisions
  of reinforcement learning (RL) agents by attributing their actions to the trajectories
  encountered during training. The authors propose a method that encodes trajectories
  using sequence modeling techniques, clusters them using these embeddings, and then
  analyzes the sensitivity of the original RL agent's policy to the trajectories present
  in each cluster.
---

# Explaining RL Decisions with Trajectories

## Quick Facts
- arXiv ID: 2305.04073
- Source URL: https://arxiv.org/abs/2305.04073
- Reference count: 40
- One-line primary result: A novel method that explains RL decisions by attributing them to behaviorally meaningful trajectory clusters from training data

## Executive Summary
This paper presents a novel approach for explaining the decisions of reinforcement learning (RL) agents by attributing their actions to the trajectories encountered during training. The authors propose a method that encodes trajectories using sequence modeling techniques, clusters them using these embeddings, and then analyzes the sensitivity of the original RL agent's policy to the trajectories present in each cluster. The method is evaluated on diverse environments, including grid-worlds, Atari games, and MuJoCo continuous control tasks. The authors demonstrate the effectiveness of their approach in generating reliable trajectory explanations and conduct a human study to show the alignment between human understanding of the task and the attributed trajectories.

## Method Summary
The method encodes trajectories as fixed-length vectors using sequence encoders (e.g., transformers or LSTMs), clusters these embeddings using X-Means, and trains explanation policies on datasets excluding each cluster. For each decision state, it compares the original policy's action with the explanation policy's action and measures the Wasserstein distance between data embeddings to identify which cluster most influenced the decision. The approach works across discrete and continuous action spaces and is validated on grid-worlds, Atari Seaquest, and MuJoCo HalfCheetah environments using different offline RL algorithms.

## Key Results
- The method successfully generates trajectory-based explanations that align with human understanding of RL agent behavior
- Attribution is effective across diverse environments including discrete control, Atari games, and continuous control tasks
- The approach provides actionable insights into which training trajectories most influence specific decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Trajectory embeddings cluster semantically meaningful behaviors, allowing cluster attribution to isolate which behavioral patterns influenced a given decision.
- Mechanism: The algorithm encodes each trajectory as a fixed-length vector using a sequence encoder (e.g., transformer). These embeddings are clustered (e.g., X-Means), and each cluster represents a distinct behavioral pattern. By training explanation policies on data with a cluster removed, and measuring action and embedding distances, the method identifies the cluster whose absence most changes the agent's decision.
- Core assumption: Similar trajectories produce similar embeddings, and clusters formed in embedding space correspond to semantically distinct behavioral patterns.
- Evidence anchors:
  - [abstract] "... encode trajectories in offline training data individually as well as collectively ... then attribute policy decisions to a set of trajectories in this encoded space by estimating the sensitivity of the decision with respect to that set."
  - [section] "We encode the trajectory data using sequence encoders and cluster the output trajectory embeddings using the X-means algorithm. More specifically, we obtain 10 trajectory clusters for grid-world, 8 for Seaquest, and 10 for HalfCheetah. These clusters represent meaningful high-level behaviors..."
  - [corpus] No direct evidence; relies on clustering validity and embedding quality.
- Break condition: If embeddings do not preserve behavioral similarity (e.g., due to poor encoder or inappropriate sequence length), clusters will be meaningless and attribution unreliable.

### Mechanism 2
- Claim: Removing a cluster of trajectories and retraining an explanation policy reveals whether that cluster's behaviors are necessary for the original decision.
- Mechanism: For each cluster, the complementary dataset excludes all trajectories in that cluster. An explanation policy trained on this dataset will perform differently if the excluded cluster contains critical behavioral information. The distance between the original action and the explanation policy's action (in continuous or discrete space) quantifies the cluster's influence.
- Core assumption: The absence of a behaviorally important cluster causes the policy to deviate from the original action in a measurable way.
- Evidence anchors:
  - [abstract] "... attribute policy decisions to a set of trajectories in this encoded space by estimating the sensitivity of the decision with respect to that set."
  - [section] "We want to find the smallest set of trajectories, the absence of which from the training data leads to different behavior at the state under consideration."
  - [corpus] Weak; relies on assumption that policy retraining on complementary data will produce divergent actions when critical trajectories are missing.
- Break condition: If the RL algorithm is robust to small data perturbations or if trajectories are highly redundant, removing one cluster may not change the policy enough to detect.

### Mechanism 3
- Claim: Set-level trajectory embedding enables efficient comparison between entire datasets and their complements.
- Mechanism: Embeddings of all trajectories in a set are summed, normalized, and passed through softmax to produce a single data embedding. This allows measuring how "far" a complementary dataset is from the full dataset in embedding space using a metric like Wasserstein distance.
- Core assumption: The permutation-invariant set encoding preserves relative importance of trajectory clusters and can be meaningfully compared using a distributional distance metric.
- Evidence anchors:
  - [section] "We follow the set-encoding procedure prescribed in (Zaheer et al., 2017) where we first sum the embeddings of the trajectories in the collection, normalize this sum by division with a constant and further apply a non-linearity, in our case, simply, softmax over the feature dimension to generate a single data embedding."
  - [corpus] No direct evidence; method is described but effectiveness not independently verified.
- Break condition: If trajectory embeddings are not comparable (e.g., different scales or units), the set encoding and Wasserstein comparison may be meaningless.

## Foundational Learning

- Concept: Sequence modeling with transformers / LSTMs for trajectory encoding.
  - Why needed here: Trajectories are variable-length sequences of (state, action, reward) tuples; sequence models can compress them into fixed-length embeddings capturing temporal structure.
  - Quick check question: How does an LSTM-based trajectory encoder differ from a transformer-based one in handling long trajectories?

- Concept: Clustering algorithms (e.g., X-Means) and evaluation of cluster semantic meaning.
  - Why needed here: To group trajectories into behaviorally meaningful sets that can be individually tested for influence on decisions.
  - Quick check question: What criteria determine whether trajectory clusters represent distinct, interpretable behaviors?

- Concept: Offline reinforcement learning and policy retraining on modified datasets.
  - Why needed here: Explanation policies must be trained on datasets lacking specific clusters to measure the effect of those clusters on decisions.
  - Quick check question: Why is it important that explanation policies are trained under identical conditions except for the data modification?

## Architecture Onboarding

- Component map: Trajectory sequence encoder -> X-Means clustering module -> Set embedding generator -> Offline RL trainer -> Attribution decision module
- Critical path:
  1. Encode all trajectories → 2. Cluster embeddings → 3. Generate complementary datasets → 4. Train explanation policies → 5. For each state: compare original vs explanation actions and data embeddings → 6. Select most influential cluster
- Design tradeoffs:
  - Using pre-trained vs. trained-from-scratch encoders: pre-trained saves time but may not fit domain specifics.
  - Number of clusters: too few → coarse attribution; too many → noisy attribution and higher computational cost.
  - Choice of action distance metric: must be compatible with action space (discrete vs continuous).
- Failure signatures:
  - All clusters attributed with equal frequency → embeddings not discriminative.
  - No change in explanation policy actions → dataset modification too small or RL too robust.
  - Attribution selects random cluster → embedding or distance metrics not aligned with decision sensitivity.
- First 3 experiments:
  1. Run on grid-world with known behavioral clusters; verify that attribution matches human intuition.
  2. Test sensitivity by adding/removing known critical trajectories; ensure attribution changes accordingly.
  3. Compare attribution results across two different offline RL algorithms on same data to check consistency.

## Open Questions the Paper Calls Out
None explicitly called out in the provided text.

## Limitations
- The method's effectiveness depends on trajectory embeddings clustering semantically meaningful behaviors, which may fail with poor encoders or inappropriate sequence lengths
- Attribution may be unreliable if the RL algorithm is robust to data perturbations or if trajectories are highly redundant
- The paper lacks direct evidence that clusters represent distinct, interpretable behaviors, relying instead on downstream attribution results

## Confidence
- **High confidence** in the mathematical formulation of trajectory encoding, clustering, and attribution via action and data embedding distances
- **Medium confidence** in the assumption that clusters correspond to semantically meaningful behaviors, as evidence is primarily downstream (attribution works) rather than upstream (clusters are interpretable)
- **Low confidence** in the method's generalizability to domains with long, high-dimensional trajectories or where behavioral patterns are subtle and difficult to cluster

## Next Checks
1. Visualize trajectory embeddings using PCA/t-SNE for each environment to verify that clusters represent distinct, interpretable behaviors before proceeding to attribution
2. Test attribution sensitivity by systematically adding/removing known critical trajectories and verifying that attribution changes accordingly
3. Compare attribution results across two different offline RL algorithms on the same data to assess whether the method consistently identifies behaviorally important trajectories