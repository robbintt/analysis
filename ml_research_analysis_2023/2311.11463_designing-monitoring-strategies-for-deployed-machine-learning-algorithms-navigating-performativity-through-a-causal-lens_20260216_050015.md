---
ver: rpa2
title: 'Designing monitoring strategies for deployed machine learning algorithms:
  navigating performativity through a causal lens'
arxiv_id: '2311.11463'
source_url: https://arxiv.org/abs/2311.11463
tags:
- monitoring
- data
- algorithm
- treatment
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper highlights the complexity of designing post-market\
  \ surveillance strategies for machine learning (ML)-based medical devices, particularly\
  \ in handling performativity\u2014where an ML algorithm affects the data-generating\
  \ process. Using causal inference and statistical process control, the authors propose\
  \ a four-step framework to define monitoring criteria, identify sources of bias,\
  \ describe strategies, and compare them."
---

# Designing monitoring strategies for deployed machine learning algorithms: navigating performativity through a causal lens

## Quick Facts
- arXiv ID: 2311.11463
- Source URL: https://arxiv.org/abs/2311.11463
- Reference count: 28
- Primary result: Monitoring for algorithmic fairness can detect model decay more effectively than traditional performance metrics

## Executive Summary
This paper addresses the challenge of designing post-market surveillance strategies for ML-based medical devices, particularly when algorithms affect their own data-generating process (performativity). Using causal inference and statistical process control, the authors propose a four-step framework to define monitoring criteria, identify sources of bias, describe strategies, and compare them. The approach is demonstrated through a hypothetical ML-based risk prediction algorithm for postoperative nausea and vomiting (PONV), showing that fairness-focused monitoring (like strong calibration) can outperform traditional metrics in detecting model decay.

## Method Summary
The authors propose a four-step framework for designing monitoring strategies: (1) define candidate monitoring criteria based on average and subgroup-specific predictive values (PPV/NPV) and algorithmic fairness, (2) enumerate sources of bias and define causal models to address them, (3) describe candidate monitoring strategies using CUSUM control charts for different data sources and criteria, and (4) compare the strategies through simulation studies evaluating their statistical power and operating characteristics.

## Key Results
- Monitoring for algorithmic fairness (e.g., strong calibration) can be more effective at detecting model decay than traditional metrics like average PPV/NPV
- The choice between observational and interventional data sources involves trade-offs between convenience and bias control
- The framework successfully handles performativity by explicitly modeling the causal relationships between the ML algorithm, treatment decisions, and outcomes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The framework leverages causal inference to identify and adjust for performativity bias, ensuring valid monitoring even when the ML algorithm changes the data-generating process.
- **Mechanism**: By explicitly modeling the causal relationships between the ML algorithm, treatment decisions, and outcomes, the framework can differentiate between changes in the algorithm's performance and changes induced by its own effects on treatment patterns.
- **Core assumption**: The causal model accurately captures the key sources of bias, particularly interfering medical interventions (IMI).
- **Evidence anchors**:
  - [abstract] "Using causal inference and statistical process control, the authors propose a four-step framework to define monitoring criteria, identify sources of bias, describe strategies, and compare them."
  - [section] "Unlike the setting of model validation, there is much less agreement on which performance metrics to monitor. Different monitoring criteria impact how interpretable the resulting test statistic is, what assumptions are needed for identifiability, and the speed of detection."
- **Break condition**: If the causal model is misspecified or fails to account for important sources of bias beyond IMI, the monitoring strategy will be invalid.

### Mechanism 2
- **Claim**: Monitoring algorithmic fairness metrics (e.g., strong calibration) can be more sensitive to model decay than traditional performance metrics like AUC.
- **Mechanism**: Fairness metrics often impose stricter requirements on model behavior across subgroups or individuals, making them more sensitive to subtle changes in the model's predictions or the data distribution.
- **Core assumption**: The strong calibration requirement is a meaningful indicator of model reliability for the intended use case.
- **Evidence anchors**:
  - [abstract] "Through simulation, they demonstrate that monitoring for algorithmic fairness (e.g., strong calibration) can be more effective at detecting model decay than traditional metrics."
  - [section] "An even stricter criterion than checking subgroup-specific PPV/NPVs is to check that the algorithm's risk predictions are not overly extreme, in that the predicted risk should not be more extreme than the true adverse event rate for any subgroup for some tolerance δ ≥ 0."
- **Break condition**: If the strong calibration requirement is not relevant to the clinical context or if the tolerance level δ is set too high, the monitoring strategy may not be effective.

### Mechanism 3
- **Claim**: The choice between observational and interventional data sources involves a trade-off between convenience and bias control.
- **Mechanism**: Observational data is readily available but contains biases like confounding and selection bias. Interventional data, such as from a randomized trial, can eliminate these biases but is more expensive and ethically complex to collect.
- **Core assumption**: The interventional study can be designed to be both ethical and feasible given the specific context.
- **Evidence anchors**:
  - [abstract] "Although the former is the most convenient source of monitoring data, it exhibits well-known biases, such as confounding, selection, and missingness."
  - [section] "On the other hand, a carefully designed interventional study that randomizes individuals can explicitly eliminate such biases, but the ethics, feasibility, and cost of such an approach must be carefully considered."
- **Break condition**: If the interventional study is not properly designed or if the ethical considerations are not adequately addressed, the data collected may not be valid or may not be collected at all.

## Foundational Learning

- **Concept: Causal Inference**
  - Why needed here: To identify and adjust for biases introduced by the ML algorithm's interaction with its environment.
  - Quick check question: What is the difference between associational and causal inference, and why is causal inference important for monitoring ML algorithms in healthcare?

- **Concept: Statistical Process Control (SPC)**
  - Why needed here: To design sequential hypothesis tests that can detect changes in the ML algorithm's performance over time.
  - Quick check question: What is the difference between a control chart and a hypothesis test, and how are they used together in SPC?

- **Concept: Algorithmic Fairness**
  - Why needed here: To ensure that the ML algorithm performs well across different subgroups and does not discriminate against certain populations.
  - Quick check question: What are some common fairness metrics used in ML, and how do they relate to the concept of calibration?

## Architecture Onboarding

- **Component map**:
  - Causal model -> Monitoring criteria -> Data source -> Monitoring procedure

- **Critical path**:
  1. Define potential monitoring criteria
  2. Enumerate sources of bias and define the causal model
  3. Describe candidate monitoring strategies
  4. Compare the pros and cons of candidate strategies
  5. Implement and deploy the chosen monitoring strategy

- **Design tradeoffs**:
  - Interpretability vs. sensitivity: More interpretable metrics (e.g., average PPV/NPV) may be less sensitive to model decay than less interpretable metrics (e.g., strong calibration)
  - Data requirements vs. bias control: Observational data is more convenient but contains biases, while interventional data eliminates biases but is more expensive and complex to collect
  - Assumptions vs. power: Procedures with weaker assumptions may have lower power to detect model decay

- **Failure signatures**:
  - False positives: The monitoring strategy incorrectly detects a change in the ML algorithm's performance when there is none
  - False negatives: The monitoring strategy fails to detect a change in the ML algorithm's performance when there is one
  - High detection delay: The monitoring strategy takes too long to detect a change in the ML algorithm's performance

- **First 3 experiments**:
  1. Simulate data from the causal model with no performance decay and verify that the monitoring strategy does not fire false alarms
  2. Simulate data from the causal model with a known performance decay and verify that the monitoring strategy detects the decay within a reasonable time frame
  3. Compare the performance of different monitoring strategies (e.g., different monitoring criteria, data sources) on the same simulated data to determine which strategy is most effective

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different randomization weights in interventional settings affect the statistical power and detection speed of monitoring procedures?
- Basis in paper: [explicit] The paper mentions varying randomization weights as a simulation setting and notes that the interventional setting's effectiveness depends on the treatment for which risk increased.
- Why unresolved: The paper only briefly mentions this as a potential simulation setting but does not provide detailed results or analysis on how different randomization weights impact the monitoring procedures.
- What evidence would resolve it: Conducting comprehensive simulation studies with various randomization weight configurations and analyzing their effects on statistical power and detection speed.

### Open Question 2
- Question: How sensitive are the monitoring procedures to violations of their underlying assumptions, such as conditional exchangeability and positivity?
- Basis in paper: [explicit] The paper discusses the importance of these assumptions for certain monitoring procedures but does not provide detailed analysis on the sensitivity to their violations.
- Why unresolved: The paper acknowledges the assumptions but does not conduct extensive simulations or theoretical analysis to understand the impact of their violations on the procedures' performance.
- What evidence would resolve it: Performing sensitivity analyses by intentionally violating these assumptions in simulations and observing the effects on the procedures' performance.

### Open Question 3
- Question: How can observational data be augmented with interventional data to improve the effectiveness of monitoring procedures?
- Basis in paper: [inferred] The paper mentions this as a future research direction but does not explore it in detail.
- Why unresolved: The paper does not provide any specific methods or results on combining observational and interventional data for monitoring purposes.
- What evidence would resolve it: Developing and testing methods that combine observational and interventional data, and evaluating their performance in simulation studies or real-world applications.

## Limitations
- The framework relies heavily on correct specification of the causal model, particularly identification of interfering medical interventions
- The simulation study uses a hypothetical PONV prediction algorithm rather than a real-world case, limiting generalizability
- The practical trade-offs between observational and interventional data sources are discussed but not empirically validated in real healthcare settings

## Confidence

High confidence in the theoretical benefits of fairness monitoring, medium confidence in causal inference-based monitoring, and low confidence in practical data source trade-offs without empirical validation:

- **Causal inference-based monitoring (mechanism 1)**: Medium confidence
- **Fairness metrics sensitivity (mechanism 2)**: High confidence
- **Observational vs. interventional data trade-offs (mechanism 3)**: Low confidence

## Next Checks

1. Validate the causal model specification using real clinical data where ground truth treatment assignments and outcomes are known.
2. Test the monitoring framework on an actual deployed ML-based medical device with known performance degradation over time.
3. Conduct a stakeholder evaluation with clinicians, regulators, and patients to assess the practical feasibility and interpretability of different monitoring criteria.