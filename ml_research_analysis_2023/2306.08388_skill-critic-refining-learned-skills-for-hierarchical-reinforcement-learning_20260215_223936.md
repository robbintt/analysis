---
ver: rpa2
title: 'Skill-Critic: Refining Learned Skills for Hierarchical Reinforcement Learning'
arxiv_id: '2306.08388'
source_url: https://arxiv.org/abs/2306.08388
tags:
- policy
- skill
- prior
- skills
- skill-critic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Skill-Critic addresses the challenge of improving unreliable low-level
  skills learned from sparse or low-quality demonstrations in hierarchical reinforcement
  learning. The method proposes joint optimization of high-level skill selection and
  low-level skill execution policies, initialized and regularized by offline data.
---

# Skill-Critic: Refining Learned Skills for Hierarchical Reinforcement Learning

## Quick Facts
- arXiv ID: 2306.08388
- Source URL: https://arxiv.org/abs/2306.08388
- Reference count: 40
- Improves unreliable low-level skills learned from sparse demonstrations in hierarchical RL through joint optimization

## Executive Summary
Skill-Critic addresses the challenge of improving unreliable low-level skills learned from sparse or low-quality demonstrations in hierarchical reinforcement learning. The method proposes joint optimization of high-level skill selection and low-level skill execution policies, initialized and regularized by offline data. The core innovation is fine-tuning both policies simultaneously while maintaining structure from pre-trained skill and action priors. In experiments across maze navigation, trajectory planning, and autonomous racing tasks, Skill-Critic significantly outperforms baselines including standard SAC, BC+SAC, and SPiRL.

## Method Summary
Skill-Critic is a two-stage approach that first pre-trains skill embeddings from offline demonstrations using a VAE framework, then jointly fine-tunes high-level skill selection and low-level action policies online. The method reformulates hierarchical RL as two parallel MDPs, allowing standard policy optimization algorithms to be applied to both levels. Both policies are regularized with KL divergence terms that measure deviation from priors trained on demonstrations, creating a balance between exploiting learned behaviors and exploring new ones. The algorithm uses an H-step reward structure in the high-level policy to maintain strong credit assignment signals for sparse rewards.

## Key Results
- Skill-Critic achieves higher episode rewards compared to SAC, BC+SAC, and SPiRL baselines
- The method demonstrates faster task completion times in maze navigation and trajectory planning tasks
- In Gran Turismo Sport racing environment, Skill-Critic shows reduced collision rates compared to SPiRL
- Joint optimization of high-level and low-level policies significantly outperforms using stationary low-level skills

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Skill-Critic improves low-level policy performance by fine-tuning both high-level skill selection and low-level action policies simultaneously.
- **Mechanism**: The algorithm reformulates hierarchical RL as two parallel MDPs (high-level and low-level), allowing standard policy optimization algorithms to be applied to both policies. The low-level policy is initialized with a decoder trained on offline demonstrations and regularized during online training.
- **Core assumption**: The skill decoder from offline data provides a reasonable initialization for the low-level policy, and joint optimization can improve upon this initialization.
- **Evidence anchors**:
  - [abstract]: "Skill-Critic's low-level policy fine-tuning and demonstration-guided regularization are essential for good performance"
  - [section]: "Our Skill-Critic approach aims to leverage joint high-level and low-level policy optimization to critique the skills themselves during skill selection"
  - [corpus]: Weak - the corpus papers focus on hierarchical learning but don't specifically address joint optimization of skill improvement
- **Break condition**: If the offline demonstrations are of extremely low quality or cover only a tiny fraction of the skill space, the initialization may be too poor for joint optimization to succeed.

### Mechanism 2
- **Claim**: The skill prior and action prior regularize exploration in a way that guides the policies toward useful behaviors without constraining them to the offline data distribution.
- **Mechanism**: Both policies are regularized with KL divergence terms that measure deviation from priors trained on demonstrations. This creates a balance between exploiting learned behaviors and exploring new ones.
- **Core assumption**: The regularization strength (temperature parameters αz and αa) can be effectively tuned to balance exploration and exploitation.
- **Evidence anchors**:
  - [abstract]: "these policies are initialized and regularized by the latent space learned from offline demonstrations"
  - [section]: "We guide HL skill selection with an offline data-informed skill prior [6], and we extend this notion to initialize and regularize the LL skills using an action prior informed by offline data"
  - [corpus]: Weak - corpus papers discuss priors but don't specifically address the balance between uniform and non-uniform regularization in hierarchical settings
- **Break condition**: If the regularization is too strong, the policies may be unable to discover necessary skills not present in the demonstrations. If too weak, the benefits of the demonstration guidance are lost.

### Mechanism 3
- **Claim**: The H-step reward structure in the high-level policy allows long-horizon sparse rewards to propagate effectively through the skill selection process.
- **Mechanism**: By accumulating rewards over the skill horizon H before discounting, the algorithm maintains stronger credit assignment signals for sparse rewards compared to single-step discounting.
- **Core assumption**: The skill horizon H is appropriately chosen relative to the task structure and reward sparsity.
- **Evidence anchors**:
  - [section]: "By using the H-step reward, ˜rH, the discount factor, γz, decays exponentially only every H steps. This objective function for policy optimization is valid since, for fixed-horizon skills, M H can be represented with rH .= ˜rH when evaluated only at instances when the skills change"
  - [corpus]: Weak - corpus papers don't specifically discuss H-step reward structures in hierarchical RL
- **Break condition**: If H is too large relative to the task structure, the accumulated reward may become noisy. If too small, the benefits of temporal abstraction are diminished.

## Foundational Learning

- **Concept**: Variational Auto-Encoder (VAE) for skill embedding
  - Why needed here: The VAE extracts temporally extended skills from demonstration data by learning a latent skill space that captures meaningful action sequences
  - Quick check question: How does the VAE balance reconstruction accuracy with latent space regularization during skill embedding training?

- **Concept**: Hierarchical MDP reformulation as parallel MDPs
  - Why needed here: This reformulation allows standard RL algorithms to be applied to both levels of the hierarchy, avoiding the need for specialized hierarchical RL algorithms
  - Quick check question: What is the relationship between the value functions in the high-level and low-level MDPs, and how does this relate to the semi-MDP formulation?

- **Concept**: Soft Actor-Critic with non-uniform priors
  - Why needed here: The algorithm extends SAC by replacing uniform entropy regularization with KL divergence regularization to priors, which guides exploration based on demonstration data
  - Quick check question: How does the target KL divergence parameter control the trade-off between following the prior and learning from online experience?

## Architecture Onboarding

- **Component map**: Demonstration → Skill embedding (VAE training) → Policy initialization → Joint optimization → Improved skills
- **Critical path**: Demonstration → Skill embedding (VAE training) → Policy initialization → Joint optimization → Improved skills
- **Design tradeoffs**: Fixed skill horizon vs. adaptive horizons; non-uniform priors vs. uniform exploration; parallel MDPs vs. joint semi-MDP optimization
- **Failure signatures**: 
  - Poor performance despite demonstration data → Check skill embedding quality and prior regularization strength
  - Unstable training → Verify Q-value estimation and check for distribution shift in low-level policy
  - Inability to discover new skills → Examine action prior variance and KL regularization targets
- **First 3 experiments**:
  1. Run Skill-Critic on maze navigation with SPiRL warm-start disabled to verify the importance of proper initialization
  2. Vary the action prior variance σˆa to observe its effect on exploration vs. exploitation balance
  3. Compare independent vs. Skill-Critic Q-value estimation methods in the low-level policy to verify the proposed estimation approach

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several important open questions arise:

- How does Skill-Critic perform when applied to high-dimensional visual observations rather than low-dimensional state representations?
- What is the effect of varying the skill horizon H on the performance of Skill-Critic in different environments?
- How does Skill-Critic handle tasks with changing or dynamic environments where the demonstrations may not be directly applicable?
- What is the impact of the quality and diversity of demonstration data on the performance of Skill-Critic?

## Limitations

- The method's effectiveness depends heavily on the quality and diversity of offline demonstrations, which is not thoroughly explored for extreme cases of limited or biased data
- Computational overhead of maintaining and training two separate policies with their own critics is not fully characterized relative to single-level alternatives
- While joint optimization is claimed to improve both policies simultaneously, ablation studies focus primarily on low-level policy fine-tuning rather than isolating high-level skill selection improvements

## Confidence

**High Confidence Claims**:
- Skill-Critic outperforms SAC, BC+SAC, and SPiRL baselines in maze navigation and racing tasks
- Demonstration-guided regularization improves performance compared to unguided exploration
- Joint high-level and low-level policy optimization is more effective than stationary low-level skills

**Medium Confidence Claims**:
- The H-step reward structure specifically contributes to better credit assignment for sparse rewards
- The specific regularization strengths (αz and αa) are optimal for all tested environments
- The parallel MDP reformulation is superior to alternative hierarchical RL approaches

## Next Checks

1. **Robustness to Demonstration Quality**: Systematically vary the quality and coverage of offline demonstrations (e.g., using only partial demonstrations or demonstrations from suboptimal policies) to test whether Skill-Critic degrades gracefully or catastrophically when initialization data is poor.

2. **Ablation of Regularization Components**: Isolate the contribution of the skill prior vs. action prior regularization by testing variants that use only one or neither, to confirm that both components are essential as claimed.

3. **Scalability to More Complex Tasks**: Evaluate Skill-Critic on tasks with significantly longer horizons, more diverse skill requirements, or continuous skill parameters (rather than discrete skill selection) to test whether the method scales beyond the relatively constrained environments tested.