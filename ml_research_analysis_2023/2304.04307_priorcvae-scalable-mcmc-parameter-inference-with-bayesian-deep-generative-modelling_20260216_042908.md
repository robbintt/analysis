---
ver: rpa2
title: 'PriorCVAE: scalable MCMC parameter inference with Bayesian deep generative
  modelling'
arxiv_id: '2304.04307'
source_url: https://arxiv.org/abs/2304.04307
tags:
- priorcvae
- priors
- inference
- priorvae
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PriorCVAE, a conditional variational autoencoder
  approach to encode Gaussian process priors while retaining the ability to explicitly
  estimate hyperparameters during Bayesian inference. Unlike PriorVAE, which loses
  hyperparameter information, PriorCVAE conditions the decoder on hyperparameters,
  enabling joint encoding and inference.
---

# PriorCVAE: scalable MCMC parameter inference with Bayesian deep generative modelling

## Quick Facts
- arXiv ID: 2304.04307
- Source URL: https://arxiv.org/abs/2304.04307
- Authors: 
- Reference count: 18
- One-line primary result: PriorCVAE enables joint encoding of Gaussian process priors and hyperparameters using conditional VAEs, achieving higher effective sample size per second compared to full MCMC while maintaining accurate hyperparameter inference.

## Executive Summary
PriorCVAE introduces a conditional variational autoencoder approach to encode Gaussian process priors while retaining the ability to explicitly estimate hyperparameters during Bayesian inference. Unlike standard VAEs that lose hyperparameter information, PriorCVAE conditions the decoder on hyperparameters, enabling joint encoding and inference. The method is demonstrated on both stationary and non-stationary kernels, showing accurate reconstruction of priors and successful hyperparameter estimation. Empirical results show that PriorCVAE achieves higher effective sample size per second compared to full MCMC and outperforms Laplace approximation and ADVI in posterior estimation.

## Method Summary
PriorCVAE uses a conditional VAE architecture where the decoder is conditioned on Gaussian process hyperparameters, enabling joint encoding of function realizations and hyperparameters. The model is trained on GP prior evaluations over a fixed spatial grid with varying hyperparameters, using a modified reconstruction loss that incorporates learned variance for better uncertainty calibration. During inference, the trained decoder serves as a surrogate for the GP prior in MCMC sampling, allowing explicit estimation of hyperparameters alongside other model parameters. The approach reduces computational complexity from O(n³) to O(d³) where d << n, making it scalable for large datasets.

## Key Results
- PriorCVAE achieves higher effective sample size per second compared to full MCMC inference
- The method successfully estimates hyperparameters including lengthscale and integral of intensity functions
- PriorCVAE outperforms Laplace approximation and ADVI in posterior estimation accuracy
- The approach demonstrates effective extrapolation capabilities for hyperparameters within reasonable distance of the training range

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning the VAE decoder on hyperparameters allows joint encoding of function realizations and hyperparameters, enabling explicit hyperparameter inference during MCMC.
- Mechanism: By conditioning the CVAE on hyperparameters (c = θ), the decoder Dψ(z, θ) learns to generate priors corresponding to specific hyperparameter values. This disentangles the prior from the hyperparameter space, allowing MCMC to sample θ directly rather than inferring it from the function realization alone.
- Core assumption: The hyperparameters can be represented as a conditioning variable c that the CVAE can learn to use for accurate reconstruction.
- Evidence anchors:
  - [abstract] "To overcome this limitation, we condition the VAE on stochastic process hyperparameters. This allows the joint encoding of hyperparameters with GP realizations and their subsequent estimation during inference."
  - [section 3.1] "The encoder and decoder can now condition on the hyperparameters of the GPs, i.e. c = θ, allowing us to generate approximate evaluations of the prior for specific hyperparameters, and also to perform inference on hyperparameters later on."
  - [corpus] Weak - no direct citations found for CVAE-based hyperparameter conditioning in GP priors.
- Break condition: If the hyperparameters cannot be represented as a discrete or continuous conditioning variable that the CVAE can learn to condition on effectively, or if the conditioning variable is not informative enough to distinguish between different priors.

### Mechanism 2
- Claim: The modified reconstruction loss with learned variance σ²_vae provides better uncertainty calibration than standard MSE loss.
- Mechanism: Instead of using MSE reconstruction loss, the paper uses a probabilistic formulation: -1/(2σ²_vae) * MSE + c, where σ²_vae is a hyperparameter. This scales the reconstruction term by the inverse variance, effectively controlling the trade-off between reconstruction accuracy and KL regularization.
- Core assumption: The generated samples from the decoder are Normally distributed around the true function values with variance σ²_vae.
- Evidence anchors:
  - [section 3.1] "Rather than using traditional mean squared error (MSE) as the reconstruction loss, we utilise, instead, the original probabilistic formulation in the form of log-likelihood assuming that the reconstructed sample fPriorCVAE is Normally distributed with the centre at fGP and standard deviation σ²_vae."
  - [section 3.1] "Here σ_vae is a hyperparameter in the neural network training process which affects the amount of uncertainty learned by PriorCVAE and leads to better uncertainty calibration."
  - [corpus] Weak - no direct citations found for this specific modification to VAE reconstruction loss.
- Break condition: If the assumption about the Normal distribution of the reconstruction error is incorrect, or if σ²_vae is not properly tuned, leading to under- or over-confidence in the learned priors.

### Mechanism 3
- Claim: The CVAE architecture enables extrapolation with respect to hyperparameters, allowing inference for hyperparameter values outside the training range.
- Mechanism: The trained decoder can generate priors for hyperparameter values outside the range seen during training by conditioning on these out-of-range values. The quality of extrapolation depends on the proximity of the test value to the training range.
- Core assumption: The decoder learns a smooth, continuous mapping from the hyperparameter space to the prior space that generalizes beyond the training range.
- Evidence anchors:
  - [section 4.3] "Even though we only trained the decoder with lengthscales drawn from (0.01, 0.4) interval, it is possible to condition PriorCVAE draws on lengthscales l̃ which lie outside of this interval."
  - [section 4.3] "Figure 10 compares the quality of extrapolation for l̃ = 0.5 and l̃ = 0.9, showing that the prior quality is high close to the interval, and deteriorates for l̃ further away."
  - [corpus] Weak - no direct citations found for CVAE-based extrapolation in GP hyperparameter inference.
- Break condition: If the relationship between hyperparameters and priors is highly non-linear or discontinuous, the decoder may not be able to extrapolate effectively, leading to poor quality priors for out-of-range hyperparameters.

## Foundational Learning

- Concept: Gaussian Processes and their computational complexity
  - Why needed here: Understanding why encoding GP priors with deep generative models is beneficial (O(n³) complexity) and how PriorCVAE reduces this to O(d³) where d << n.
  - Quick check question: What is the computational complexity of exact GP inference and why does it become prohibitive for large n?

- Concept: Variational Autoencoders and Conditional VAEs
  - Why needed here: Understanding the architecture differences between VAE and CVAE, and how conditioning on hyperparameters enables explicit hyperparameter inference.
  - Quick check question: How does a CVAE differ from a standard VAE in terms of architecture and training objective?

- Concept: Markov Chain Monte Carlo and its convergence diagnostics
  - Why needed here: Understanding how MCMC is used for inference in the presence of learned priors, and how to assess convergence and effective sample size.
  - Quick check question: What are R-hat and effective sample size (ESS), and why are they important for assessing MCMC convergence?

## Architecture Onboarding

- Component map: Data generation -> CVAE model (Encoder E_γ(f, θ) and Decoder D_ψ(z, θ)) -> Training loop (optimize ELBO with modified reconstruction loss) -> Inference pipeline (use trained decoder in MCMC)

- Critical path:
  1. Generate training data: Draw GP samples f ~ GP(0, k(θ)) for various θ
  2. Train CVAE: Optimize LP PriorCVAE with conditioning on θ
  3. Validate priors: Check that generated priors match GP priors for various θ
  4. Perform inference: Use trained decoder as drop-in replacement for GP prior in MCMC inference

- Design tradeoffs:
  - CVAE vs VAE: CVAE allows hyperparameter inference but may be slightly less efficient than VAE
  - σ²_vae tuning: Higher values give more weight to reconstruction, lower values to KL regularization
  - Latent space dimension d: Larger d may capture more complex priors but increases training complexity

- Failure signatures:
  - Poor reconstruction quality: Indicates CVAE not learning the mapping from (z, θ) to f well
  - Inability to infer θ: Suggests conditioning not informative enough or decoder not disentangling θ from f
  - Degraded extrapolation: Decoder may not generalize well to θ values outside training range

- First 3 experiments:
  1. Binary hyperparameter conditioning: Encode lengthscale l ∈ {0.1, 0.4} and verify inference can distinguish between the two cases
  2. Continuous hyperparameter conditioning: Encode lengthscale l ~ U(0.01, 0.99) and verify accurate inference across the range
  3. Non-stationary kernel: Encode product of linear and RBF kernels with conditioning on lengthscale and verify inference quality and extrapolation ability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the PriorCVAE's ability to extrapolate with respect to hyperparameters compare to other methods that do not use conditioning?
- Basis in paper: [explicit] The paper demonstrates that PriorCVAE can extrapolate with respect to hyperparameters, but the quality of priors deteriorates for hyperparameters further away from the training range.
- Why unresolved: While the paper shows that PriorCVAE can extrapolate, it does not compare its extrapolation ability to other methods that do not use conditioning.
- What evidence would resolve it: Empirical comparisons of PriorCVAE's extrapolation ability with other methods on a range of tasks.

### Open Question 2
- Question: How does the choice of the σ²_vae hyperparameter affect the calibration of the decoder in PriorCVAE?
- Basis in paper: [explicit] The paper introduces the σ²_vae hyperparameter to improve the calibration of the decoder by utilizing a more rigorous statistical approach to VAE reconstruction loss.
- Why unresolved: The paper does not provide a detailed analysis of how the choice of σ²_vae affects the calibration of the decoder.
- What evidence would resolve it: Systematic experiments varying σ²_vae and measuring the calibration of the decoder on a range of tasks.

### Open Question 3
- Question: How does PriorCVAE's performance compare to other approximate inference methods on tasks with complex non-stationary kernels?
- Basis in paper: [inferred] The paper demonstrates that PriorCVAE can learn non-stationary GPs, but does not compare its performance to other approximate inference methods on tasks with complex non-stationary kernels.
- Why unresolved: While the paper shows that PriorCVAE can learn non-stationary GPs, it does not provide a comprehensive comparison of its performance to other methods on tasks with complex non-stationary kernels.
- What evidence would resolve it: Empirical comparisons of PriorCVAE's performance with other approximate inference methods on a range of tasks with complex non-stationary kernels.

## Limitations

- Extrapolation capability is limited, with prior quality degrading significantly for hyperparameter values far from the training range
- Performance on irregularly spaced data and non-Euclidean domains remains untested
- The computational benefits over standard MCMC depend heavily on the choice of GP kernel and data dimensionality

## Confidence

- Mechanism 1: Medium - The conditioning approach shows promising results but lacks direct citations and the assumption about informative conditioning is not fully validated
- Mechanism 2: Low - The modification to VAE reconstruction loss introduces additional hyperparameters without comprehensive analysis of their impact
- Mechanism 3: Low - Extrapolation capability is demonstrated but shows significant degradation for out-of-range values with no comparison to baseline methods

## Next Checks

1. Test extrapolation limits systematically by evaluating prior quality across a grid of hyperparameter values spanning multiple orders of magnitude beyond the training range.

2. Benchmark computational efficiency against exact GP inference for varying numbers of observations to establish the break-even point.

3. Apply PriorCVAE to a real-world spatiotemporal dataset with irregularly spaced observations to assess performance in practical scenarios.