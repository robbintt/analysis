---
ver: rpa2
title: 'LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering
  to ChatGPT'
arxiv_id: '2306.17103'
source_url: https://arxiv.org/abs/2306.17103
tags:
- lyrics
- transcription
- arxiv
- music
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LyricWhiz, a robust, multilingual, and zero-shot
  automatic lyrics transcription method that achieves state-of-the-art performance
  on various lyrics transcription datasets, even in challenging genres such as rock
  and metal. The core idea is to leverage Whisper, a weakly supervised robust speech
  recognition model, as the "ear" for transcribing audio, and GPT-4, a chat-based
  large language model, as the "brain" for contextualized output selection and correction.
---

# LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT

## Quick Facts
- arXiv ID: 2306.17103
- Source URL: https://arxiv.org/abs/2306.17103
- Reference count: 0
- Primary result: Zero-shot ALT method using Whisper + GPT-4 achieves state-of-the-art WER across multiple languages and genres

## Executive Summary
This paper introduces LyricWhiz, a training-free automatic lyrics transcription method that leverages Whisper for audio transcription and GPT-4 for post-processing and ensemble reasoning. The approach achieves state-of-the-art performance on various lyrics transcription datasets, particularly excelling in challenging genres like rock and metal. The method works across multiple languages without fine-tuning, addressing a significant gap in the ALT literature. The authors also contribute MulJam, the first publicly available large-scale multilingual lyrics transcription dataset with CC-BY-NC-SA license.

## Method Summary
LyricWhiz uses Whisper-large with a simple "lyrics:" prompt to transcribe audio recordings, followed by GPT-4 post-processing that ensembles multiple Whisper predictions to select the most accurate transcription. The approach employs PANNs for vocal detection to filter non-vocal recordings and removes transcriptions that are too short or long. The method operates entirely in a zero-shot manner without any fine-tuning on domain-specific singing data, relying on Whisper's weak supervision from 680,000 hours of audio data covering 96 languages.

## Key Results
- Achieves lower Word Error Rate compared to existing methods on English lyrics transcription
- Effectively transcribes lyrics across multiple languages (English, French, Spanish, Italian, Russian, German)
- Outperforms baseline methods particularly in challenging genres like rock and metal
- Creates MulJam, the first publicly available large-scale multilingual lyrics transcription dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Whisper's weak supervision on 680,000 hours of audio data enables zero-shot multilingual ALT without fine-tuning.
- Mechanism: Whisper learns robust acoustic-phonetic mappings across 96 languages during pretraining, which generalize to singing voice despite domain shift.
- Core assumption: Singing voice characteristics fall within the acoustic variability captured by the speech pretraining data.
- Evidence anchors:
  - [abstract] "leveraging Whisper, a weakly supervised robust speech recognition model"
  - [section 2.2] "scaled the weakly supervised ASR to 680,000 hours of labeled audio data, which covers 96 languages"
- Break condition: If singing voice contains phonetic patterns or prosody entirely absent from Whisper's training data, zero-shot transfer fails.

### Mechanism 2
- Claim: GPT-4's contextualized post-processing corrects Whisper errors through multi-prediction ensemble reasoning.
- Mechanism: GPT-4 analyzes multiple Whisper outputs, identifies inconsistencies, and selects the most accurate transcription based on linguistic coherence and contextual clues.
- Core assumption: GPT-4 can reliably distinguish correct from incorrect lyrics transcriptions using its language understanding capabilities.
- Evidence anchors:
  - [abstract] "GPT-4, today's most performant chat-based large language model"
  - [section 3.2] "we employ ChatGPT as an expert in lyrics to ensemble these multiple predictions"
- Break condition: If GPT-4's language model biases lead it to prefer grammatically correct but semantically wrong transcriptions.

### Mechanism 3
- Claim: Simple prefix prompt "lyrics:" prevents Whisper from outputting non-lyric content like music descriptions or advertisements.
- Mechanism: The prefix constrains Whisper's output distribution toward transcription-relevant tokens by activating task-specific internal representations.
- Core assumption: Whisper's attention mechanisms respond to task-specific prefixes similarly to general LLMs.
- Evidence anchors:
  - [section 3.1] "we utilize the input prompt designed in Whisper as a prefix prompt to guide it toward the lyric transcription task"
  - [section 3.1] "using the simplest prompt, 'lyrics:', effectively prevents the model from outputting descriptions of the music"
- Break condition: If complex musical contexts confuse the prefix's constraining effect.

## Foundational Learning

- Concept: Weakly supervised learning
  - Why needed here: Whisper was trained on weakly labeled data, which is crucial for understanding how it generalizes to new domains like music
  - Quick check question: What distinguishes weakly supervised learning from fully supervised learning in speech recognition?

- Concept: Chain-of-Thought prompting
  - Why needed here: The method uses CoT to decompose lyrics post-processing into analysis, selection, and prediction phases
  - Quick check question: How does Chain-of-Thought prompting improve reasoning performance in large language models?

- Concept: Zero-shot learning
  - Why needed here: LyricWhiz operates without domain-specific fine-tuning, relying on pretrained model generalization
  - Quick check question: What factors determine whether a pretrained model can perform well on a new task without fine-tuning?

## Architecture Onboarding

- Component map: Whisper (audio-to-text transcriber) → Prompt filter → Multiple inference runs → GPT-4 (post-processor) → Final output
- Critical path: Audio input → Whisper transcription → Prompt application → Multiple predictions → GPT-4 ensemble → Output lyrics
- Design tradeoffs: Using pretrained models eliminates training data requirements but limits control over model behavior and may introduce unexpected biases
- Failure signatures: High WER with nonsensical outputs suggests Whisper's acoustic model struggles with singing voice; consistent grammatical corrections that change meaning indicate GPT-4 overcorrection
- First 3 experiments:
  1. Run Whisper with different prompts ("lyrics:", "transcript:", "sing") on a small music sample and compare output quality
  2. Generate 3-5 Whisper predictions for the same audio and manually assess consistency
  3. Test GPT-4 post-processing with ground truth lyrics included to measure selection accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LyricWhiz perform on lyrics transcription tasks in languages beyond the six tested in the paper?
- Basis in paper: [explicit] The paper mentions that Whisper covers 96 languages and that the authors tested Whisper's performance on multiple benchmark datasets, but only evaluated six languages in their dataset construction and testing.
- Why unresolved: The paper only evaluates six languages (English, French, Spanish, Italian, Russian, and German) in the MulJam dataset, leaving the performance on the remaining 90 languages covered by Whisper untested.
- What evidence would resolve it: Comprehensive testing of LyricWhiz on lyrics transcription tasks across all 96 languages covered by Whisper, with results reported in terms of WER and other relevant metrics.

### Open Question 2
- Question: How does the performance of LyricWhiz compare to other state-of-the-art ALT methods when fine-tuned on domain-specific singing datasets?
- Basis in paper: [inferred] The paper compares LyricWhiz to existing methods, including W2V2-ALT, which is a transfer learning method based on ASR self-supervised models. However, the paper does not explore the potential benefits of fine-tuning LyricWhiz on domain-specific singing datasets.
- Why unresolved: The paper does not investigate the impact of fine-tuning LyricWhiz on domain-specific singing datasets, which could potentially improve its performance compared to other state-of-the-art ALT methods.
- What evidence would resolve it: Experimental results comparing the performance of fine-tuned LyricWhiz to other state-of-the-art ALT methods on various benchmark datasets, including those used in the paper.

### Open Question 3
- Question: How does the quality of the MulJam dataset compare to other multilingual lyrics transcription datasets, such as DALI, in terms of linguistic diversity, genre coverage, and annotation accuracy?
- Basis in paper: [explicit] The paper introduces MulJam as the first publicly available, large-scale, multilingual lyrics transcription dataset without copyright restrictions. However, it only briefly mentions the DALI dataset and does not provide a detailed comparison of the two datasets.
- Why unresolved: The paper does not provide a comprehensive comparison of MulJam to other multilingual lyrics transcription datasets, such as DALI, in terms of linguistic diversity, genre coverage, and annotation accuracy.
- What evidence would resolve it: A detailed comparative analysis of MulJam and other multilingual lyrics transcription datasets, including metrics such as linguistic diversity, genre coverage, and annotation accuracy, as well as qualitative assessments of the datasets' quality and usability.

## Limitations

- Performance claims rely on generalization from speech pretraining without testing acoustic domain gap for extreme singing techniques
- GPT-4 post-processing effectiveness depends on language model biases that may systematically prefer grammatically correct over phonetically accurate lyrics
- Multilingual performance claims are not empirically validated beyond six languages despite Whisper supporting 96 languages

## Confidence

- High Confidence: Technical implementation using Whisper for transcription and GPT-4 for post-processing is clearly specified and reproducible
- Medium Confidence: Reported WER improvements are plausible but lack comparison against state-of-the-art specialized ALT models on the same test sets
- Low Confidence: Claims about robust multilingual performance across all 96 supported languages are not empirically validated

## Next Checks

1. **Acoustic Domain Gap Analysis**: Test Whisper's transcription accuracy on singing voice versus speech across different vocal techniques (clean singing, growling, rapping, whispering) to quantify the acoustic generalization gap.

2. **GPT-4 Post-processing Bias Evaluation**: Create test cases where grammatically correct but semantically wrong lyrics alternatives exist, and measure GPT-4's accuracy in selecting the ground truth transcription versus preferring more "standard" language.

3. **Multilingual Prompt Robustness**: Evaluate the simple "lyrics:" prefix effectiveness across all 96 supported languages by testing transcription quality with and without prompts, and with language-specific prompt variations.