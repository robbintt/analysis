---
ver: rpa2
title: 'SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in
  Both Image Classification and Generation'
arxiv_id: '2310.12508'
source_url: https://arxiv.org/abs/2310.12508
tags:
- forgetting
- unlearning
- salun
- data
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of machine unlearning (MU) in
  both image classification and generation tasks. The authors introduce the concept
  of "weight saliency" in MU, drawing parallels with input saliency in model explanation.
---

# SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation

## Quick Facts
- arXiv ID: 2310.12508
- Source URL: https://arxiv.org/abs/2310.12508
- Reference count: 15
- One-line primary result: SalUn achieves nearly 100% unlearning accuracy in preventing conditional diffusion models from generating harmful images, with only 0.2% gap compared to exact unlearning in high-variance random data forgetting scenarios.

## Executive Summary
This paper introduces SalUn, a novel machine unlearning (MU) method that leverages gradient-based weight saliency to identify and update only the most influential model weights during the unlearning process. By directing MU's attention toward specific weights rather than the entire model, SalUn improves both effectiveness and efficiency compared to traditional approaches. The method is demonstrated to be effective for both image classification and generation tasks, achieving state-of-the-art unlearning performance while maintaining model utility on remaining data.

## Method Summary
SalUn employs gradient-based weight saliency to identify influential model weights for targeted unlearning. The method computes a weight saliency map by thresholding the gradient of a forgetting loss with respect to model weights, creating a binary mask highlighting weights most responsible for the forgetting data's influence. For classification tasks, SalUn integrates this saliency map with random labeling, updating only the salient weights while keeping non-salient weights intact. For generation tasks, it uses misaligned concept conditioning during diffusion training while preserving generation quality. The approach is tested on CIFAR-10, CIFAR-100, SVHN for classification, and Imagenette for generation tasks.

## Key Results
- SalUn outperforms existing MU baselines, achieving nearly 100% unlearning accuracy in preventing conditional diffusion models from generating harmful images
- On CIFAR-10, SalUn shows only 0.2% gap compared to exact unlearning in high-variance random data forgetting scenarios
- The method demonstrates improved effectiveness and efficiency by updating only influential weights rather than the entire model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SalUn's gradient-based weight saliency map identifies influential model weights for targeted unlearning, improving accuracy and stability over full-model approaches.
- Mechanism: The method computes a weight saliency map by thresholding the gradient of a forgetting loss with respect to model weights. This creates a binary mask highlighting weights most responsible for the forgetting data's influence. Only these weights are updated during unlearning, while the rest remain unchanged.
- Core assumption: The gradient magnitude correlates with a weight's influence on the forgetting data's effect, and this relationship is stable across different unlearning scenarios.
- Evidence anchors:
  - [abstract] "This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency."
  - [section 5] "Drawing inspiration from gradient-based input saliency maps... we pose the question of whether a weight saliency map can be constructed to aid MU."
  - [corpus] Weak - no direct corpus evidence found supporting this mechanism's effectiveness in practice.
- Break condition: If the gradient magnitude doesn't reliably indicate weight influence (e.g., due to model architecture or training dynamics), the saliency map becomes ineffective and unlearning performance degrades.

### Mechanism 2
- Claim: Integrating weight saliency with random labeling (RL) provides a principled MU solution that balances unlearning efficacy with model preservation.
- Mechanism: SalUn applies the weight saliency mask to RL's optimization problem, updating only the salient weights with randomly labeled forgetting data while keeping non-salient weights intact. This prevents over-forgetting while ensuring effective removal of the forgetting data's influence.
- Core assumption: Random labeling is an effective MU baseline that can be enhanced through targeted weight updates, and the saliency mask correctly identifies which weights to update.
- Evidence anchors:
  - [section 5] "We find that integrating weight saliency with the RL (random labeling) method provides a promising MU solution."
  - [section 5] "In image classification, RL assigns a random image label to a forgetting data point and then fine-tunes the model on the randomly labeled Df. In SalUn, we leverage the idea of RL to update (4)."
  - [corpus] Weak - no direct corpus evidence found validating this integration approach.
- Break condition: If RL's effectiveness is scenario-dependent or the saliency mask incorrectly identifies weights, the integration may not improve performance and could even harm it.

### Mechanism 3
- Claim: The hard-thresholding implementation of SalUn produces strictly sparse weight saliency maps that benefit MU's effectiveness compared to soft-thresholding variants.
- Mechanism: Hard-thresholding creates a binary weight saliency map where weights above a threshold are marked for updating. This enforces sparsity and provides clear separation between weights to update and those to keep intact.
- Core assumption: Strict sparsity enforced by hard-thresholding is beneficial for MU, and the chosen threshold (e.g., median gradient value) effectively separates influential from non-influential weights.
- Evidence anchors:
  - [section 5] "While this hard-thresholding approach performs well, we can also develop an alternative implementation of SalUn that employs soft thresholding and may allow for a more flexible saliency map determination."
  - [section 5] "Notably, SalUn-soft exhibits a larger MIA gap with Retrain compared to SalUn. We hypothesize that the use of hard thresholding in SalUn produces a strictly sparse weight saliency map that benefits MU's effectiveness."
  - [corpus] Weak - no direct corpus evidence found comparing hard vs. soft thresholding performance.
- Break condition: If the hard threshold is poorly chosen or the binary nature of the mask discards useful information, performance may suffer compared to more nuanced approaches.

## Foundational Learning

- Concept: Gradient-based input saliency maps in model explanation
  - Why needed here: SalUn draws direct inspiration from these techniques, applying similar principles to model weights instead of inputs
  - Quick check question: How do gradient-based saliency maps work in standard model explanation, and what makes them effective for identifying influential features?

- Concept: Influence functions and their role in data attribution
  - Why needed here: The paper references influence function analysis as a baseline MU method, and understanding it helps contextualize SalUn's approach
  - Quick check question: What are influence functions, and how do they differ from gradient-based approaches in identifying data influence on model parameters?

- Concept: Machine unlearning problem formulation and evaluation metrics
  - Why needed here: The paper operates within the MU framework and uses specific metrics (UA, MIA, RA, TA, RTE) to evaluate performance
  - Quick check question: What are the key challenges in MU, and how do the different evaluation metrics capture various aspects of unlearning effectiveness?

## Architecture Onboarding

- Component map: Weight saliency map generator -> Unlearning optimizer -> Evaluation metrics calculator -> Integration layer

- Critical path: 1. Compute forgetting loss gradient w.r.t. model weights 2. Generate weight saliency map through thresholding 3. Apply saliency mask to RL optimization problem 4. Train model with masked updates for small number of epochs 5. Evaluate unlearning performance using established metrics

- Design tradeoffs:
  - Hard vs. soft thresholding: Hard provides strict sparsity but may discard useful information; soft is more flexible but may not enforce sufficient sparsity
  - Weight saliency sparsity level: Higher sparsity means fewer updates but may miss important weights; lower sparsity means more updates but risks over-forgetting
  - Unlearning duration: Longer training may improve unlearning but increases computational cost and risk of forgetting useful information

- Failure signatures:
  - Poor unlearning accuracy despite correct implementation: Indicates saliency map isn't identifying right weights or forgetting loss isn't appropriate
  - High variance in performance across runs: Suggests instability in gradient computation or thresholding process
  - Degradation in non-forgetting class performance: Indicates over-forgetting or incorrect saliency mask application

- First 3 experiments:
  1. Implement weight saliency map generation and visualize gradients for a simple model on CIFAR-10
  2. Apply hard-thresholding saliency to a basic RL baseline and compare performance with vanilla RL
  3. Test SalUn on a small-scale image generation task (e.g., class-wise forgetting in DDPM on CIFAR-10)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SalUn's performance compare to exact unlearning (retraining from scratch) across different model architectures beyond ResNet-18, VGG-16, and Swin-T?
- Basis in paper: [inferred] The paper tests SalUn on CIFAR-10, CIFAR-100, and SVHN datasets with ResNet-18, VGG-16, and Swin-T models. However, it doesn't provide a comprehensive comparison of SalUn's performance across various model architectures.
- Why unresolved: The paper focuses on specific datasets and model architectures, leaving the generalizability of SalUn to other architectures unexplored.
- What evidence would resolve it: Testing SalUn on a wider range of model architectures, such as Vision Transformers, EfficientNet, or other popular architectures, and comparing its performance to exact unlearning in terms of accuracy, stability, and computational efficiency.

### Open Question 2
- Question: Can SalUn be effectively applied to other types of generative models beyond diffusion models, such as GANs or VAEs?
- Basis in paper: [inferred] The paper demonstrates SalUn's effectiveness in preventing conditional diffusion models from generating harmful images. However, it doesn't explore its applicability to other generative model types.
- Why unresolved: The paper's focus on diffusion models leaves the question of SalUn's effectiveness on other generative models unanswered.
- What evidence would resolve it: Applying SalUn to other generative models like GANs or VAEs and evaluating its performance in terms of unlearning accuracy, generation quality, and computational efficiency.

### Open Question 3
- Question: How does SalUn's performance scale with the size and complexity of the dataset and model?
- Basis in paper: [inferred] The paper tests SalUn on relatively small datasets (CIFAR-10, CIFAR-100, SVHN) and models (ResNet-18, VGG-16, Swin-T). It doesn't provide insights into how SalUn's performance might change with larger, more complex datasets and models.
- Why unresolved: The paper's experiments are limited in scale, leaving the question of SalUn's scalability unanswered.
- What evidence would resolve it: Testing SalUn on larger datasets like ImageNet or COCO and more complex models like large-scale Vision Transformers or other state-of-the-art architectures, and evaluating its performance in terms of unlearning accuracy, stability, and computational efficiency.

## Limitations
- Limited validation of the core assumption that gradient magnitude directly correlates with weight influence on forgetting data
- Lack of a principled method for choosing optimal hard-thresholding parameter values
- Experiments focused primarily on benchmark datasets with relatively small models, leaving uncertainty about performance on larger, more complex architectures
- Computational efficiency claims not thoroughly validated across different hardware configurations

## Confidence

- High Confidence: The basic formulation of weight saliency and its integration with random labeling for classification tasks. The experimental results showing SalUn's effectiveness in image generation unlearning.
- Medium Confidence: The mechanism by which gradient-based weight saliency improves unlearning accuracy. The superiority of hard-thresholding over soft-thresholding implementations.
- Low Confidence: The generalizability of results to larger, more complex models and real-world scenarios. The claimed computational efficiency benefits across different hardware setups.

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary the hard-thresholding parameter across a range of values and measure its impact on unlearning accuracy, remaining accuracy, and computational efficiency. This would validate whether the hard-thresholding choice is robust or requires careful tuning.

2. **Large-Scale Model Validation**: Apply SalUn to larger models (e.g., ResNet-50 or Vision Transformer) on more complex datasets to verify if the performance gains observed on CIFAR-10 scale to realistic applications. Measure both unlearning effectiveness and computational overhead.

3. **Gradient Correlation Study**: Design experiments to empirically validate the assumption that gradient magnitude correlates with weight influence on forgetting data. This could involve ablation studies where weights are ranked by gradient magnitude and their individual contributions to forgetting data influence are measured.