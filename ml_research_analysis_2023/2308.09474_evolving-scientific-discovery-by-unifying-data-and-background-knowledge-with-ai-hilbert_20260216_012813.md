---
ver: rpa2
title: Evolving Scientific Discovery by Unifying Data and Background Knowledge with
  AI Hilbert
arxiv_id: '2308.09474'
source_url: https://arxiv.org/abs/2308.09474
tags:
- scientific
- polynomial
- data
- optimization
- laws
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AI Hilbert, a novel automated approach to scientific
  discovery that leverages polynomial optimization techniques to derive polynomial
  scientific laws consistent with background theory and experimental data. The key
  innovation is formulating scientific discovery as a polynomial optimization problem,
  where axioms and laws are expressed as polynomial equalities/inequalities, and the
  goal is to find a new law that best fits the data while being approximately consistent
  with the background theory.
---

# Evolving Scientific Discovery by Unifying Data and Background Knowledge with AI Hilbert

## Quick Facts
- arXiv ID: 2308.09474
- Source URL: https://arxiv.org/abs/2308.09474
- Reference count: 40
- Key outcome: AI Hilbert discovers scientific laws by formulating discovery as polynomial optimization problems with formal proofs via Positivstellensatz certificates

## Executive Summary
AI Hilbert presents a novel automated approach to scientific discovery that combines background theory with experimental data through polynomial optimization. The method expresses scientific axioms and laws as polynomial equalities/inequalities, then searches for new laws that best fit experimental data while maintaining consistency with existing background theory. A key innovation is the use of Positivstellensatz certificates from real algebraic geometry to formally prove the correctness of derived laws, addressing the "hallucination" problem common in deep learning approaches. The approach demonstrates its effectiveness by rediscovering several famous scientific laws including Kepler's Third Law, Einstein's Relativistic Time Dilation Law, and the Hagen-Poiseuille Equation.

## Method Summary
AI Hilbert formulates scientific discovery as a polynomial optimization problem where background axioms and laws are expressed as polynomial equalities and inequalities. The method searches for new laws that minimize the distance to experimental data while remaining approximately consistent with background theory. For consistent theories, this is solved via semidefinite programming; for inconsistent theories, mixed-integer optimization is used to identify the subset of axioms that best explain the data. The approach leverages Positivstellensatz certificates to provide formal proofs of correctness for derived laws, distinguishing it from data-driven approaches that lack such guarantees. By bounding the degree of polynomial certificates, AI Hilbert offers fine-grained control over computational tractability.

## Key Results
- Successfully rediscovers Kepler's Third Law, Hagen-Poiseuille Equation, Einstein's Relativistic Time Dilation Law, and Bell Inequalities from background theory and data
- Outperforms state-of-the-art approaches in recovering scientific laws, especially in challenging problem settings with inconsistent background theories
- Provides formal proofs of correctness for derived laws as a byproduct of the optimization process
- Demonstrates ability to handle both consistent and inconsistent background theories while controlling computational complexity through degree-bounded certificates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI Hilbert discovers scientific laws by searching for polynomials that are both consistent with background theory and fit experimental data using polynomial optimization.
- Mechanism: The approach formulates scientific discovery as a polynomial optimization problem where axioms and laws are expressed as polynomial equalities/inequalities. Positivstellensatz certificates from real algebraic geometry are used to formally prove the correctness of derived laws. This allows the system to find a new law that best fits the data while being approximately consistent with background theory.
- Core assumption: All axioms and scientific laws can be expressed as polynomial equalities and inequalities.
- Evidence anchors:
  - [abstract] "We propose a solution to this problem when all axioms and scientific laws are expressible via polynomial equalities and inequalities"
  - [section 1.3] "Given a set of background axioms, theorems, and laws expressible as a basic semialgebraic set (i.e., a system of polynomial equalities and inequalities) and observations from experimental data, we derive new laws representable as polynomial expressions that are either exactly or approximately consistent with existing laws and experimental data by solving polynomial optimization problems via linear and semidefinite optimization."

### Mechanism 2
- Claim: AI Hilbert can handle both consistent and inconsistent background theories, offering fine-grained control over tractability by bounding the degree of polynomial certificates.
- Mechanism: For inconsistent theories, AI Hilbert uses best subset selection to determine the axioms which best explain the data, allowing it to identify sources of inconsistency. By bounding the degree of polynomial certificates, the approach controls computational complexity, running in polynomial time with complete and correct background theory or non-deterministic polynomial time with partially correct background theory.
- Core assumption: The degree of polynomial certificates can be bounded to control computational complexity.
- Evidence anchors:
  - [abstract] "AI Hilbert can handle both consistent and inconsistent background theories, and offers fine-grained control over tractability by bounding the degree of polynomial certificates."
  - [section 2.2] "Observe that, if the degree of our new scientific law q is fixed and the degree of the polynomial multipliers in the definition in dc is also fixed, then Problem (3) can be solved in polynomial time with a consistent set of axioms"

### Mechanism 3
- Claim: AI Hilbert provides formal proofs of the correctness of discovered laws as a byproduct of the optimization problems, addressing the "hallucination" problem of deep learning approaches.
- Mechanism: By leveraging the Positivstellensatz, AI Hilbert automatically proves the validity of scientific discoveries. This is in contrast to deep learning techniques that often provide no formal proofs and are prone to proposing laws that cannot be automatically proven or disproven.
- Core assumption: The Positivstellensatz can be used to certify the correctness of derived laws.
- Evidence anchors:
  - [abstract] "We obtain formal proofs of the correctness of our laws as a byproduct of the optimization problems."
  - [section 1.1] "As such, any new laws derived by these systems cannot easily be explained or justified. On the other hand, our approach is based on a new notion of distance that demonstrates the compatibility of a symbolic law with a set of background theories based on the distance between a law and its projection onto the set of symbolic laws derivable from our theory."

## Foundational Learning

- Concept: Polynomial optimization and sum-of-squares (SOS) techniques
  - Why needed here: AI Hilbert relies on polynomial optimization to search over the space of scientific laws and SOS techniques to certify the correctness of derived laws using the Positivstellensatz.
  - Quick check question: What is the relationship between sum-of-squares polynomials and positive semidefinite matrices?

- Concept: Real algebraic geometry and the Positivstellensatz
  - Why needed here: The Positivstellensatz is the fundamental theorem that allows AI Hilbert to provide formal proofs of the correctness of derived laws by representing polynomials as systems of sum-of-squares polynomials.
  - Quick check question: What are the conditions under which Putinar's Positivestellensatz holds?

- Concept: Mixed-integer optimization
  - Why needed here: AI Hilbert uses mixed-integer optimization to handle inconsistent background theories by selecting the subset of axioms that best explain the data, and to control the complexity of the scientific discovery process.
  - Quick check question: How does mixed-integer optimization differ from pure continuous optimization, and what are its computational implications?

## Architecture Onboarding

- Component map:
  - Input layer: Background axioms and experimental data
  - Optimization engine: Polynomial optimization solver (e.g., Mosek)
  - Certificate generation: Sum-of-squares decomposition using Positivstellensatz
  - Output layer: Discovered scientific law with formal proof of correctness

- Critical path:
  1. Parse and preprocess input axioms and data
  2. Formulate polynomial optimization problem
  3. Solve optimization problem using semidefinite programming
  4. Generate Positivstellensatz certificate
  5. Validate and output discovered law with proof

- Design tradeoffs:
  - Expressiveness vs. tractability: Bounding the degree of polynomial certificates controls computational complexity but may limit the expressiveness of the search space
  - Data efficiency vs. background theory reliance: Using background theory allows discovery with less data but requires accurate and complete axioms
  - Implicit vs. explicit discovery: Searching for implicit polynomial functions allows discovery of laws that cannot be easily expressed explicitly but may be harder to interpret

- Failure signatures:
  - Optimization solver fails to converge: May indicate ill-conditioned problem or need for reformulation
  - Certificate generation fails: May indicate that the discovered law is not derivable from the background theory or that the Positivstellensatz conditions are not met
  - Discovered law is trivial (e.g., q = 0): May indicate that the constraints are too restrictive or that the background theory is inconsistent

- First 3 experiments:
  1. Reproduce the derivation of Kepler's Third Law from background theory and data as described in Section 2.6
  2. Derive Einstein's Relativistic Time Dilation Law from background theory and data as described in Section 2.5
  3. Derive the Hagen-Poiseuille Equation from background theory alone as described in Section 2.4

## Open Questions the Paper Calls Out

- Question: How does AI Hilbert perform when applied to scientific domains beyond physics, such as biology, chemistry, or economics?
- Basis in paper: [explicit] The paper demonstrates AI Hilbert on physics problems, but does not explore other scientific domains.
- Why unresolved: The paper does not provide evidence of AI Hilbert's performance in non-physics domains, which could have different types of data and background knowledge structures.
- What evidence would resolve it: Applying AI Hilbert to datasets and background theories from biology, chemistry, or economics and comparing its performance to existing methods in those domains.

- Question: How does the choice of hyperparameters, such as the degree bound for polynomial certificates and the penalty weight Î», impact AI Hilbert's ability to discover valid scientific laws?
- Basis in paper: [explicit] The paper mentions that AI Hilbert requires hyperparameter optimization but does not explore the impact of these choices in depth.
- Why unresolved: The paper does not provide a systematic analysis of how different hyperparameter settings affect AI Hilbert's performance or provide guidance on how to choose them.
- What evidence would resolve it: Conducting a sensitivity analysis of AI Hilbert's performance across a range of hyperparameter values and providing recommendations for choosing them based on the characteristics of the problem.

- Question: How does AI Hilbert scale to larger and more complex scientific problems, and what are the computational bottlenecks that limit its scalability?
- Basis in paper: [explicit] The paper mentions that AI Hilbert's runtime and memory usage are functions of the number of symbolic variables and the degree of the proof certificates, but does not explore its scalability limits in detail.
- Why unresolved: The paper does not provide a systematic study of AI Hilbert's performance on problems with varying sizes and complexities, or identify the specific factors that limit its scalability.
- What evidence would resolve it: Benchmarking AI Hilbert on a range of problems with increasing numbers of variables and constraints, and analyzing the computational resources required to solve them.

## Limitations

- Computational scalability is limited to relatively small problems with up to 100 data points and simple polynomial degrees
- Restricted to scientific domains where axioms and laws can be expressed as polynomial equalities/inequalities
- Limited experimental validation across diverse scientific domains beyond classical physics

## Confidence

- High confidence in theoretical foundations and mathematical correctness of the approach
- Medium confidence in practical applicability due to limited experimental validation
- Low confidence in ability to handle modern scientific domains involving differential equations or quantum mechanics

## Next Checks

1. Test the approach on a real-world scientific discovery problem from a modern field (e.g., systems biology or materials science) where background theories involve differential equations and the data is noisy and high-dimensional.

2. Evaluate the scalability limits by systematically increasing the number of background axioms, polynomial degrees, and data points to determine the computational complexity in practice.

3. Compare the discovered laws against those found by state-of-the-art symbolic regression methods and human experts on the same discovery tasks, measuring both accuracy and interpretability.