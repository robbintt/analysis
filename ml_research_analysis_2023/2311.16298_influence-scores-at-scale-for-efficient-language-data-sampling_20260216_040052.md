---
ver: rpa2
title: Influence Scores at Scale for Efficient Language Data Sampling
arxiv_id: '2311.16298'
source_url: https://arxiv.org/abs/2311.16298
tags:
- data
- scores
- training
- sampling
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the effectiveness of influence scores\u2014\
  metrics that estimate the importance of training examples to model performance\u2014\
  in the context of language classification tasks, particularly when fine-tuning pretrained\
  \ language models. The authors evaluate five influence scores (VoG, EL2N, Forgetting\
  \ Scores, PVI, and TracIn) on the SNLI dataset to determine which scores best identify\
  \ influential data for efficient model training."
---

# Influence Scores at Scale for Efficient Language Data Sampling

## Quick Facts
- arXiv ID: 2311.16298
- Source URL: https://arxiv.org/abs/2311.16298
- Reference count: 40
- Primary result: VoG scores consistently outperform other influence scores and random sampling in maintaining accuracy while pruning up to 50% of training data, especially with dataset normalization.

## Executive Summary
This paper investigates the effectiveness of influence scores—metrics that estimate the importance of training examples to model performance—in the context of language classification tasks, particularly when fine-tuning pretrained language models. The authors evaluate five influence scores (VoG, EL2N, Forgetting Scores, PVI, and TracIn) on the SNLI dataset to determine which scores best identify influential data for efficient model training. They find that VoG (variance of gradients) consistently outperforms other scores and random sampling in maintaining accuracy while pruning up to 50% of training data, especially when using dataset normalization. The method is further validated in a large-scale NLU model stack used in commercial voice assistants, where pruning roughly half of the training data using VoG scores resulted in no statistically significant regressions in key performance metrics. These findings suggest that VoG is a simple, scalable, and effective tool for efficient data selection in language model fine-tuning.

## Method Summary
The paper evaluates five influence scores (VoG, EL2N, Forgetting Scores, PVI, and TracIn) for data pruning in language model fine-tuning. VoG computes the variance of gradients of model outputs with respect to input embeddings across training checkpoints. The authors first establish baseline accuracy by fine-tuning BERT models on full SNLI training data. They then compute influence scores on the training data, prune data by sampling easiest/hardest examples based on scores, and fine-tune models on pruned data. Performance is compared to random sampling baselines across different pruning percentages and normalization schemes. The method is validated on both SNLI and a large-scale internal NLU model stack.

## Key Results
- VoG consistently outperforms other influence scores and random sampling in maintaining accuracy while pruning up to 50% of training data.
- Dataset-normalized VoG scores are more effective than class-normalized scores for pruning in class-imbalanced settings.
- VoG requires only a single training run and no hyperparameter tuning, making it computationally efficient.
- In large-scale NLU experiments, pruning roughly half of training data using VoG scores resulted in no statistically significant regressions in key performance metrics.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient variance (VoG) scores capture which training examples are most informative for model performance in fine-tuning.
- Mechanism: VoG computes the variance of gradients of model outputs with respect to input embeddings across training checkpoints. High variance indicates the example causes unstable or large gradient changes, meaning it is influential for learning.
- Core assumption: Training examples with high gradient variance are more critical to model performance than those with low variance.
- Evidence anchors: [abstract], [section 2.1]
- Break condition: If gradient variance does not correlate with model performance improvements, or if the variance is dominated by noise rather than signal.

### Mechanism 2
- Claim: Dataset-normalized VoG scores are more effective than class-normalized scores for pruning in class-imbalanced settings.
- Mechanism: Dataset-normalized scores account for the overall distribution of training data, preventing over-pruning of majority classes that can happen with class-normalized scores.
- Core assumption: In highly imbalanced datasets, class-normalized scores can bias pruning toward majority classes, harming minority class performance.
- Evidence anchors: [abstract], [section 3.3]
- Break condition: If the dataset is class-balanced or if the class imbalance does not significantly affect model performance.

### Mechanism 3
- Claim: VoG scores can be computed efficiently in a "one-shot" manner without extensive hyperparameter tuning.
- Mechanism: VoG only requires gradients from a single training run, avoiding the need for multiple model checkpoints or ensemble averaging used by other influence scores.
- Core assumption: A single training run provides sufficient information to estimate example influence accurately.
- Evidence anchors: [abstract], [section 2.1]
- Break condition: If single-run gradients are insufficient to capture example influence, or if the model requires early stopping or other tuning to stabilize gradients.

## Foundational Learning

- Concept: Gradient variance as a measure of example influence
  - Why needed here: Understanding why variance in gradients indicates importance is crucial to interpreting VoG scores and their effectiveness.
  - Quick check question: Why would a training example that causes large, unstable gradient changes be more important for model learning than one with small, stable gradients?

- Concept: Normalization schemes for influence scores
  - Why needed here: The choice between class-normalization and dataset-normalization significantly impacts pruning effectiveness, especially in imbalanced datasets.
  - Quick check question: In a dataset with 90% of examples from one class, how might class-normalized scores bias pruning compared to dataset-normalized scores?

- Concept: One-shot vs. multi-run influence estimation
  - Why needed here: VoG's efficiency advantage comes from requiring only one training run, unlike methods needing multiple checkpoints or ensemble models.
  - Quick check question: What are the computational trade-offs between computing influence scores from one training run versus multiple runs or checkpoints?

## Architecture Onboarding

- Component map:
  - BERT encoder (L=4, H=512) → classifier head (3-layer FC with 64-dim intermediate) → gradients logged during training
  - VoG computation: gradients of outputs w.r.t. embeddings → variance across checkpoints → normalization
  - Pruning pipeline: compute VoG scores → probabilistic sampling (softmax or linear) → retrain on pruned data

- Critical path:
  1. Train BERT model on full dataset, logging gradients at checkpoints
  2. Compute VoG scores from logged gradients
  3. Normalize scores (dataset or class)
  4. Sample training data based on scores
  5. Retrain model on sampled data
  6. Evaluate performance

- Design tradeoffs:
  - VoG vs. other scores: VoG is simpler and requires only one training run, but may miss some nuanced influence signals captured by methods like TracIn or PVI
  - Normalization choice: Dataset-normalization helps in imbalanced settings but may over-penalize majority classes in balanced datasets
  - Sampling method: Softmax sampling is aggressive but preserves score ratios; linear sampling retains more easy examples

- Failure signatures:
  - VoG scores not correlating with performance improvements
  - Class-imbalanced pruning degrading minority class performance (indicates normalization issue)
  - Single-run VoG scores too noisy or unstable (may need more checkpoints or smoothing)

- First 3 experiments:
  1. Run BERT training on SNLI, logging gradients at 10 checkpoints; compute VoG scores and compare pruning results to random baseline
  2. Test class-normalized vs. dataset-normalized VoG scores on a small imbalanced dataset; measure impact on minority class accuracy
  3. Implement linear sampling (probability proportional to score) and compare to softmax sampling on a small dataset to see effect on easy example retention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does influence scoring work better for language tasks when computed on intermediate layers (like encoder hidden states) versus final classifier outputs?
- Basis in paper: [explicit] The authors tested TracIn scores using both last-layer classifier weights and last encoder hidden state, finding encoder hidden state gave more stable and better results for data pruning in SNLI experiments.
- Why unresolved: The study only tested one alternative (classifier weights), leaving open whether other intermediate layers or different model architectures might yield even better influence scores.
- What evidence would resolve it: Systematic comparison of influence scores computed at multiple layers across different model architectures (e.g., BERT, GPT, smaller models) on various language tasks.

### Open Question 2
- Question: Can influence scores effectively identify misannotated examples in noisy real-world datasets, or do they primarily surface influential examples that mitigate noise effects?
- Basis in paper: [explicit] In SNLI experiments with added label noise, the authors found V oG scores did not reliably find misannotated examples (similarity index ~0.11) but still outperformed random pruning in maintaining accuracy.
- Why unresolved: The experiments only tested one type of noise (isotropic label shuffling) on a single dataset, and the mechanism by which influence scores help in noisy settings remains unclear.
- What evidence would resolve it: Testing influence scores on datasets with known misannotations and different types of noise (e.g., class imbalance, ambiguous labels) to compare their ability to identify defects versus influential examples.

### Open Question 3
- Question: How does the effectiveness of influence scoring for data pruning change as model size increases?
- Basis in paper: [explicit] The authors compared BERTSMALL and BERTBASE on SNLI, finding both benefited from V oG-based pruning, but only tested two model sizes.
- Why unresolved: The study did not explore a wider range of model sizes or architectures, leaving open whether influence scoring remains effective for very large language models or smaller models.
- What evidence would resolve it: Systematic experiments varying model size and architecture (e.g., BERT, GPT, smaller models) on multiple datasets to measure pruning effectiveness and potential scaling laws.

### Open Question 4
- Question: Can influence scores be effectively generalized to decoder-only architectures like GPT for data-efficient pretraining?
- Basis in paper: [inferred] The authors mention that their work focused on supervised settings using BERT architectures and suggest extending influence scores to pretraining and decoder-only models as a future direction.
- Why unresolved: The paper did not test influence scores on decoder-only models, and the computational and methodological challenges of doing so in pretraining are not addressed.
- What evidence would resolve it: Implementation and testing of influence scores (e.g., V oG) on decoder-only models during pretraining, comparing data efficiency and performance to standard training methods.

## Limitations
- Evaluation primarily conducted on SNLI dataset and a single in-house NLU stack, limiting generalizability across diverse language tasks and model architectures.
- Choice of hyperparameters (e.g., number of checkpoints for VoG computation, pruning percentages) appears influential but is not extensively explored.
- Computational efficiency claims, while promising, are not benchmarked against other methods in terms of wall-clock time or memory usage.

## Confidence
- High confidence in VoG's effectiveness on SNLI: The results are consistent across multiple pruning scenarios and normalization schemes.
- Medium confidence in large-scale NLU results: While the internal experiments show positive results, the lack of detailed methodology and independent verification reduces confidence.
- Medium confidence in computational efficiency claims: The "one-shot" computation is clearly specified, but comparative benchmarks are absent.

## Next Checks
1. Replicate the SNLI experiments with different pruning percentages (e.g., 20%, 40%, 60%) and compare VoG performance to other influence scores across multiple random seeds to assess stability.
2. Test VoG on a more diverse set of language tasks (e.g., GLUE benchmark) and with different model architectures (e.g., RoBERTa, T5) to evaluate generalizability.
3. Conduct ablation studies on the number of checkpoints used for VoG computation to determine the minimum required for effective pruning and assess computational trade-offs.