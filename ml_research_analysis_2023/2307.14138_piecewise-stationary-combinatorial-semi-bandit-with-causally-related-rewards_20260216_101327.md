---
ver: rpa2
title: Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards
arxiv_id: '2307.14138'
source_url: https://arxiv.org/abs/2307.14138
tags:
- arms
- graph
- base
- algorithm
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the piecewise stationary combinatorial semi-bandit
  problem where causal relationships between rewards may evolve over time. The authors
  propose a novel decision-making policy, PS-SEM-UCB-Gr, which combines UCB-based
  learning, change-point detection via GLR tests, and online graph learning to adapt
  to both distribution changes and causal graph structure changes.
---

# Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards

## Quick Facts
- arXiv ID: 2307.14138
- Source URL: https://arxiv.org/abs/2307.14138
- Reference count: 37
- Key outcome: Novel algorithm PS-SEM-UCB-Gr combines UCB learning, GLR change-point detection, and online graph learning to handle piecewise-stationary combinatorial semi-bandits with evolving causal relationships

## Executive Summary
This paper tackles the challenge of piecewise-stationary combinatorial semi-bandits where reward distributions and causal relationships between rewards can change over time. The authors propose a novel algorithm that adapts to both distribution changes and causal graph structure changes through a combination of UCB-based learning, GLR change-point detection, and online graph learning. A key innovation is the introduction of a "group restart" strategy that restarts learning for arms within the same group upon detecting a change, improving upon local and global restart methods. Theoretical analysis provides a regret upper bound that accounts for the number of distribution and graph changes, and experiments on synthetic and real-world data demonstrate superior performance compared to state-of-the-art benchmarks.

## Method Summary
The PS-SEM-UCB-Gr algorithm addresses piecewise-stationary combinatorial semi-bandits by combining UCB-based learning for expected rewards, GLR change-point detection for distribution changes, online graph learning via SEM optimization, and a novel "group restart" strategy for adapting to changes. The algorithm uses collected feedback to estimate the adjacency matrix of the causal graph at each time step and detects graph changes via a graph-change detection mechanism. When a change is detected in any arm within a group, all arms in that group are restarted together, leveraging prior knowledge that arms within a group tend to have their reward distributions change together.

## Key Results
- The group restart strategy outperforms local and global restart methods by exploiting known arm groupings to limit unnecessary restarts
- The online graph learning mechanism allows the algorithm to adapt to changes in the causal structure between rewards without prior knowledge of the graph
- Theoretical regret bounds reflect both the number of distribution and graph changes, with experiments showing superior performance on synthetic and real-world COVID-19 data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The group restart strategy improves regret by exploiting known arm groupings to limit unnecessary restarts.
- Mechanism: When a change is detected in any arm within a group, all arms in that group are restarted together. This leverages the prior knowledge that arms within a group tend to have their reward distributions change together, avoiding the cost of restarting unrelated arms (as in global restart) while preventing the delays that occur with local restart when multiple arms in a group change.
- Core assumption: The prior information about groupings of arms is accurate and that distribution changes within a group are correlated.
- Evidence anchors:
  - [abstract] "we introduce the notion of group restart as a new alternative restarting strategy in the decision making process in structured environments."
  - [section] "we propose group restart strategy as an efficient alternative in structured environments where grouping information might be either available in advance or learned from the data."
  - [corpus] Weak: No direct citations found, but the concept aligns with literature on structured bandits with arm groupings.
- Break condition: If the assumed groupings are inaccurate or the correlation between arm changes within groups is weak, the benefit of group restart diminishes and could even increase regret compared to local restart.

### Mechanism 2
- Claim: The online graph learning mechanism allows the algorithm to adapt to changes in the causal structure between rewards without prior knowledge of the graph.
- Mechanism: The algorithm uses collected feedback (both semi-bandit and overall reward) to estimate the adjacency matrix of the causal graph at each time step using a piece-wise static graph learning formulation. When the graph structure changes, the algorithm detects this via a graph-change detection mechanism and initiates a Graph Learning Data Generation (GLDG) phase to re-estimate the graph.
- Core assumption: The Structural Equation Model (SEM) formulation accurately represents the causal relationships between rewards, and there are sufficient data samples (at least K+1 rounds) between graph changes for accurate graph estimation.
- Evidence anchors:
  - [abstract] "Finally, our algorithm integrates a mechanism to trace the variations of the underlying graph structure, which captures the causal relationships between the rewards in the bandit setting."
  - [section] "Considering the required knowledge of Wt in finding the optimal decision vector, we propose an online graph learning framework..."
  - [corpus] Weak: While related works on graph bandits exist, the specific combination of online SEM-based graph learning with change detection is not directly evidenced in the corpus.
- Break condition: If the SEM assumption is violated, the graph estimation will be inaccurate, leading to suboptimal decisions. If graph changes occur too frequently, the GLDG phases may not have enough data for accurate estimation.

### Mechanism 3
- Claim: The combination of UCB-based learning, GLR change-point detection, and the specific restart strategies leads to sublinear regret that reflects both the number of distribution and graph changes.
- Mechanism: The algorithm uses UCB indices for arm selection, GLR tests to detect changes in arm distributions, and a restart strategy (group restart) to reset learning after changes. The theoretical regret bound incorporates terms for the number of distribution changes (Ng for each group), the number of graph changes (NW), and the cost of restarts and delays. The group restart strategy is shown to be more efficient than local or global restarts in certain scenarios.
- Core assumption: The GLR change-point detector is effective in identifying true changes, and the UCB index updates correctly track the mean rewards after restarts.
- Evidence anchors:
  - [abstract] "Theoretically, we establish a regret upper bound that reflects the effects of the number of structural- and distribution changes on the performance."
  - [section] "We provide the theoretical analysis of the regret upper bound for our algorithm. Our regret bound reflects the effects of both the number of causal graph changes and the number of distribution changes."
  - [corpus] Weak: The specific regret bound structure and its comparison to local/global restarts is not directly evidenced in the corpus.
- Break condition: If the GLR detector has high false positive or false negative rates, or if the UCB indices do not converge quickly after restarts, the regret could be higher than the theoretical bound.

## Foundational Learning

- Concept: Piecewise Stationary Bandit Problem
  - Why needed here: The problem formulation explicitly assumes that the reward distributions of base arms can change over time at unknown change-points, which is a key departure from the stationary bandit setting.
  - Quick check question: What is the difference between a stationary and a piecewise stationary bandit environment?

- Concept: Combinatorial Semi-Bandit Problem
  - Why needed here: The agent selects a subset (super arm) of base arms at each time step, and only observes the rewards of the selected arms. This increases the complexity of the learning problem compared to selecting a single arm.
  - Quick check question: In a combinatorial semi-bandit, what type of feedback does the agent receive for the selected arms?

- Concept: Structural Equation Models (SEM) for Causal Relationships
  - Why needed here: The reward generation process is modeled using an SEM where the overall reward of each arm is causally influenced by the overall rewards of other arms according to a directed acyclic graph. This graph can change over time.
  - Quick check question: How does an SEM represent the causal relationships between the rewards of different arms?

## Architecture Onboarding

- Component map: UCB-based learning module -> GLR change-point detector -> Group restart strategy -> Online graph learning module -> Graph-change detection mechanism -> Combinatorial optimization module

- Critical path:
  1. Collect feedback (semi-bandit and overall reward) for the selected super arm.
  2. Update UCB indices for each base arm based on new feedback.
  3. Run GLR tests to check for changes in arm distributions.
  4. If a change is detected in an arm, restart UCB learning for all arms in its group.
  5. Check for changes in the causal graph using the graph-change detection mechanism.
  6. If a graph change is detected, initiate a GLDG phase to re-estimate the graph.
  7. Solve the combinatorial optimization problem to select the next super arm.

- Design tradeoffs:
  - Group restart vs. local vs. global restart: Group restart aims to balance the cost of unnecessary restarts (global) with the delay in detecting changes (local) by exploiting known arm groupings.
  - Online graph learning vs. prior knowledge: The algorithm learns the graph online, which is more practical but potentially less accurate than having prior knowledge.
  - GLR change-point detection: The choice of GLR test and its parameters affects the trade-off between detecting true changes and avoiding false alarms.

- Failure signatures:
  - High regret: Could indicate inaccurate arm groupings (for group restart), poor graph estimation, or ineffective change detection.
  - Frequent GLDG phases: May suggest that graph changes are occurring too rapidly for accurate estimation.
  - UCB indices not converging: Could indicate issues with the restart strategy or the GLR detector not correctly identifying changes.

- First 3 experiments:
  1. Implement the PS-SEM-UCB-Gr algorithm with a simple synthetic dataset where arm groupings and graph structure are known. Verify that the algorithm correctly identifies change-points and restarts learning within groups.
  2. Compare the performance (regret) of PS-SEM-UCB-Gr with different restart strategies (group, local, global) on a synthetic dataset with varying degrees of correlation between arm changes within groups.
  3. Test the graph learning component by providing synthetic feedback data with known graph structures and evaluating the accuracy of the estimated adjacency matrices over time.

## Open Questions the Paper Calls Out
- How does the proposed group restart strategy compare to other adaptive restarting strategies in terms of regret performance, especially in scenarios with varying modularity of the network structure?
- How does the performance of the PS-SEM-UCB-Gr algorithm change when the underlying graph structure is not a DAG, but contains cycles or other complex structures?
- How does the presence of noise in the SEM formulation affect the performance of the PS-SEM-UCB-Gr algorithm, and what modifications can be made to improve its robustness to noise?

## Limitations
- The effectiveness of the group restart strategy relies on accurate prior knowledge of arm groupings, which may not be available or correct in practice
- The online graph learning mechanism's accuracy depends on the validity of the SEM assumption and sufficient data between graph changes
- The theoretical regret bounds may be conservative and not fully capture the practical performance gains

## Confidence
- High: The core algorithmic framework combining UCB learning, GLR change-point detection, and graph learning is well-established and theoretically grounded
- Medium: The group restart strategy and its theoretical comparison to local/global restarts are sound, but the practical benefits may vary depending on the specific problem instance
- Medium: The experimental results demonstrate the algorithm's effectiveness, but the synthetic dataset may not fully represent the complexity of real-world problems

## Next Checks
1. Test the algorithm on a real-world dataset (beyond the COVID-19 example) with known arm groupings and causal relationships to validate the practical benefits of the group restart strategy and online graph learning
2. Conduct a sensitivity analysis to assess the impact of inaccurate arm groupings on the performance of the group restart strategy, and compare it with the robustness of local and global restarts
3. Evaluate the algorithm's performance under different levels of graph change frequency and data availability between changes to understand the limitations of the online graph learning component