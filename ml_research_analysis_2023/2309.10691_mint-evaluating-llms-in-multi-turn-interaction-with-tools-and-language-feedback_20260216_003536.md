---
ver: rpa2
title: 'MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback'
arxiv_id: '2309.10691'
source_url: https://arxiv.org/abs/2309.10691
tags:
- feedback
- solution
- language
- arxiv
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MINT, a benchmark for evaluating LLMs' abilities
  in multi-turn interactions using tools and natural language feedback. MINT simulates
  real-world scenarios where LLMs solve complex tasks through iterative exchanges
  with users and external tools.
---

# MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback

## Quick Facts
- arXiv ID: 2309.10691
- Source URL: https://arxiv.org/abs/2309.10691
- Authors: 
- Reference count: 40
- One-line primary result: MINT shows LLMs benefit from tool interactions (1-8% gain per turn) and natural language feedback (2-17% gain), but better single-turn performance doesn't guarantee better multi-turn performance

## Executive Summary
This paper introduces MINT, a benchmark for evaluating Large Language Models' (LLMs) abilities in multi-turn interactions using tools and natural language feedback. MINT simulates real-world scenarios where LLMs solve complex tasks through iterative exchanges with users and external tools. The evaluation framework leverages GPT-4 to provide natural language feedback, ensuring reproducibility and scalability. By repurposing diverse datasets across reasoning, coding, and decision-making tasks, MINT offers a compact and challenging set of 586 instances. Analysis of 20 LLMs reveals that while LLMs generally benefit from tool interactions and language feedback, better single-turn performance does not guarantee better multi-turn performance, and supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities.

## Method Summary
MINT evaluates LLMs' multi-turn interaction capabilities by providing a fixed budget of k turns for each task. LLMs can use Python tools for computation and receive GPT-4-generated natural language feedback between turns. The framework measures Success Rate with k-turn budget (SRk), improvement per additional turn, and improvement with natural language feedback. The benchmark curates 586 instances from 8 diverse datasets through careful filtering and sampling to balance coverage and evaluation efficiency. Evaluation involves tracking whether LLMs follow formatting instructions, produce executable code, and successfully complete tasks with varying levels of tool use and feedback.

## Key Results
- LLMs benefit from tool interactions with performance gains of 1-8% per additional turn with tool use
- Natural language feedback improves LLM performance by 2-17%, with more informative feedback yielding better results
- Better single-turn performance does not guarantee better multi-turn performance (e.g., Claude-2 vs Claude-1)
- Supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-turn interactions with tools improve LLM performance more than single-turn interactions
- Mechanism: Each additional turn with tool use provides new information (calculation results, error messages) that allows the LLM to refine its approach and improve task success rate
- Core assumption: The LLM effectively utilizes information from tool interactions to improve its subsequent reasoning
- Evidence anchors:
  - [abstract] "LLMs generally benefit from tool interactions and language feedback, with performance gains of 1–8% per additional turn with tool use"
  - [section] "We measure LLMs' tool-augmented task-solving capability by analyzing its performance gain with increased numbers of turns of tool use"
  - [corpus] Weak evidence - only 1 of 7 related papers directly addresses multi-turn tool interactions
- Break condition: If the LLM fails to parse or utilize tool outputs, or if tool interactions introduce noise rather than useful information

### Mechanism 2
- Claim: Natural language feedback significantly improves LLM task performance
- Mechanism: GPT-4-generated feedback provides targeted guidance that helps the LLM correct mistakes and refine its approach in subsequent turns
- Core assumption: The LLM can effectively interpret and apply natural language feedback to improve its solutions
- Evidence anchors:
  - [abstract] "LLMs generally benefit from tool interactions and language feedback, with performance gains of 2–17% with natural language feedback"
  - [section] "We measure LLM's ability to leverage natural language feedback with the performance gain upon receiving GPT-4 generated feedback"
  - [corpus] Weak evidence - only 1 of 7 related papers directly addresses feedback in multi-turn interactions
- Break condition: If the feedback is not actionable or the LLM cannot correctly interpret the feedback

### Mechanism 3
- Claim: Single-turn performance is not predictive of multi-turn performance
- Mechanism: Multi-turn interactions require different capabilities (iterative refinement, tool utilization, feedback incorporation) than single-turn tasks
- Core assumption: The skills needed for multi-turn interactions are distinct from those needed for single-turn tasks
- Evidence anchors:
  - [abstract] "Better single-turn performance does not guarantee better multi-turn performance"
  - [section] "While Claude-2 outperforms its predecessor Claude-1 in single-turn evaluation, the latter benefits more and achieves better performance with > 2 turns"
  - [corpus] Moderate evidence - 3 of 7 related papers discuss multi-turn interactions and performance evaluation
- Break condition: If the relationship between single-turn and multi-turn performance becomes more predictable with larger models or different training approaches

## Foundational Learning

- Concept: Interactive task-solving frameworks
  - Why needed here: Understanding how to structure multi-turn interactions with tools and feedback is essential for implementing and extending MINT
  - Quick check question: What are the key components of an interactive task-solving framework, and how do they differ from single-turn evaluation?

- Concept: Tool integration with language models
  - Why needed here: MINT relies on LLMs' ability to generate and execute Python code as tools, requiring understanding of how to safely integrate external tools
  - Quick check question: What are the potential risks and benefits of allowing LLMs to execute generated code during evaluation?

- Concept: Natural language feedback generation and utilization
  - Why needed here: The benchmark uses GPT-4 to generate feedback and measures how well LLMs can utilize this feedback
  - Quick check question: How does the quality of feedback generation impact the LLM's ability to improve performance in subsequent turns?

## Architecture Onboarding

- Component map: LLM agent -> Tool execution environment -> Feedback generator -> Metrics calculator
- Critical path: The evaluation workflow is: (1) LLM receives task, (2) LLM decides to use tools or propose solution, (3) if using tools, execute code and return output, (4) provide feedback if enabled, (5) repeat until solution proposed or max turns reached, (6) calculate metrics
- Design tradeoffs: The benchmark trades comprehensiveness (29k instances) for efficiency (586 instances) through careful filtering and sampling, and uses GPT-4 feedback instead of human feedback for scalability
- Failure signatures: Common failures include LLMs not following formatting instructions, producing syntax errors in generated code, or failing to improve despite feedback
- First 3 experiments:
  1. Run a simple task (e.g., GSM8K) with k=1 to verify basic functionality
  2. Test with k=5 and no feedback to measure tool utilization capability
  3. Test with k=5 and feedback to measure feedback utilization capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal frequency and informativeness of natural language feedback that maximizes LLM performance improvement?
- Basis in paper: [explicit] Section B.1 discusses ablation studies on feedback informativeness and frequency
- Why unresolved: While the paper shows that binary feedback is less effective than textual feedback and dense feedback is better than sparse, it doesn't identify optimal settings or explore intermediate frequencies
- What evidence would resolve it: A comprehensive study testing various feedback frequencies (e.g., every turn, every other turn, only on proposal) and informativeness levels (binary, minimal textual, detailed textual) to find the optimal combination

### Open Question 2
- Question: How do training artifacts and data contamination affect LLM performance in multi-turn interactions?
- Basis in paper: [explicit] Section 3.5 identifies artifacts in ShareGPT data and issues with CodeLlama-Instruct's [PYTHON] tag
- Why unresolved: The paper detects these issues but doesn't systematically study their impact or develop methods to prevent such contamination
- What evidence would resolve it: A controlled study examining how different types of training artifacts affect multi-turn performance and developing automated detection methods

### Open Question 3
- Question: Does RLHF training on multi-turn data improve LLM performance in multi-turn interactions?
- Basis in paper: [inferred] Section 3.2 shows RLHF hurts multi-turn performance, but notes that current RLHF only involves single-turn samples
- Why unresolved: The paper observes negative effects of RLHF but can't draw general conclusions due to lack of multi-turn RLHF training
- What evidence would resolve it: Training and evaluating LLMs with RLHF on multi-turn data and comparing their performance to base and single-turn RLHF models

### Open Question 4
- Question: How does self-feedback compare to feedback from other models in terms of quality and effectiveness?
- Basis in paper: [explicit] Section 3.3 and 3.4 show models benefit less from self-feedback compared to feedback from other models
- Why unresolved: The paper demonstrates the phenomenon but doesn't explore why it occurs or whether this generalizes across tasks
- What evidence would resolve it: A systematic comparison of self-feedback versus feedback from models of varying capabilities across multiple task types and interaction lengths

### Open Question 5
- Question: What is the relationship between single-turn performance and multi-turn interaction capability?
- Basis in paper: [explicit] Section 3.2 finds that better single-turn performance doesn't guarantee better multi-turn performance
- Why unresolved: The paper identifies this disconnect but doesn't explore the underlying reasons or develop predictive metrics
- What evidence would resolve it: A study correlating various single-turn capabilities (reasoning, tool use, etc.) with multi-turn performance to identify which single-turn skills best predict multi-turn success

## Limitations

- Tool Execution Safety and Reliability: The evaluation framework allows LLMs to generate and execute Python code, but safety measures and robustness to potentially harmful code are not thoroughly validated.
- Dataset Representativeness and Generalization: The 586 instances may not fully capture the breadth of real-world multi-turn interaction scenarios due to specific filtering criteria and sampling strategy.
- Generalizability of GPT-4 Feedback: The benchmark relies on GPT-4 for feedback generation, but the paper doesn't validate whether this feedback is consistently helpful across different task types or LLM capabilities.

## Confidence

- Multi-turn performance benefits (High confidence): Well-supported by measured performance gains and clear evaluation methodology
- Single-turn vs multi-turn performance (Medium confidence): Supported by limited empirical evidence from a small set of models
- SIFT and RLHF hurting multi-turn capabilities (Medium confidence): Observed trend with limited explanation of underlying mechanisms

## Next Checks

1. **Safety and Robustness Testing**: Implement a controlled experiment testing how the evaluation framework handles potentially harmful or resource-intensive code generation, including timeout handling, resource limits, and sandboxing effectiveness.

2. **Dataset Representativeness Analysis**: Conduct statistical analysis comparing the 586 MINT instances against their source datasets to quantify coverage and potential biases. Test whether models' performance on MINT correlates with performance on the full source datasets.

3. **Feedback Quality Validation**: Run a controlled study where human evaluators rate the quality and actionability of GPT-4 generated feedback across different task types and difficulty levels, comparing this to model performance improvements to establish correlation strength.