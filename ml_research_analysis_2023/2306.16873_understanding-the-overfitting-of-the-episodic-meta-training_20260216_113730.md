---
ver: rpa2
title: Understanding the Overfitting of the Episodic Meta-training
arxiv_id: '2306.16873'
source_url: https://arxiv.org/abs/2306.16873
tags:
- nnskl
- few-shot
- meta-training
- learning
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses overfitting in the meta-training stage of\
  \ two-stage few-shot learning frameworks, caused by over-discrimination\u2014models\
  \ relying too heavily on base-class features while neglecting generalization to\
  \ novel classes. The authors propose using knowledge distillation techniques to\
  \ preserve generalizable knowledge."
---

# Understanding the Overfitting of the Episodic Meta-training

## Quick Facts
- **arXiv ID**: 2306.16873
- **Source URL**: https://arxiv.org/abs/2306.16873
- **Reference count**: 40
- **Key outcome**: Addresses overfitting in meta-training of two-stage few-shot learning by using knowledge distillation (SKL and NNSKL) to preserve generalizable knowledge, achieving state-of-the-art results on miniImageNet, tieredImageNet, CUB, and CIFAR-FS.

## Executive Summary
This paper addresses overfitting in the meta-training stage of two-stage few-shot learning frameworks, caused by over-discrimination—models relying too heavily on base-class features while neglecting generalization to novel classes. The authors propose using knowledge distillation techniques to preserve generalizable knowledge. They introduce Symmetric Kullback-Leibler (SKL) divergence to penalize differences between student and teacher classifier outputs and Nearest Neighbor SKL (NNSKL) to focus on relationships between query and support embeddings. By combining these with supervised contrastive loss, their meta-training process mitigates overfitting and improves performance. Experiments on miniImageNet, tieredImageNet, CUB, and CIFAR-FS show consistent improvements over standard meta-training, achieving state-of-the-art results. Ablation studies confirm the effectiveness of SKL and NNSKL, with best results when used together. The method also reduces dimensional collapse and maintains better feature separation for novel classes.

## Method Summary
The method combines supervised contrastive loss with two knowledge distillation techniques: Symmetric KL (SKL) and Nearest Neighbor SKL (NNSKL). SKL penalizes divergence between student and teacher classifier output distributions over individual images, while NNSKL extends this to task-level by penalizing divergence between nearest-neighbor classifier outputs. The teacher model is selected as the best validation accuracy during meta-training. The combined loss function balances base-class discrimination with novel-class generalization, reducing overfitting and improving few-shot performance.

## Key Results
- Achieves state-of-the-art performance on miniImageNet, tieredImageNet, CUB, and CIFAR-FS benchmarks
- Reduces dimensional collapse in embedding space, preserving feature separation for novel classes
- Ablation studies show SKL and NNSKL are complementary, with best results when combined

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Over-discrimination in meta-training causes severe overfitting by forcing embeddings to collapse into a low-dimensional subspace dominated by base-class discrimination.
- Mechanism: Supervised contrastive loss optimizes embeddings to be close to class prototypes, but with few-shot support sets, prototypes are noisy. This pushes query embeddings toward a subspace spanned by these unreliable prototypes, causing dimensional collapse and forgetting novel-class generalization patterns.
- Core assumption: The variance of estimated prototypes in low-shot regimes is high enough to mislead query embeddings.
- Evidence anchors:
  - [abstract] "model relies too heavily on the superficial features that are useful for base class discrimination while neglecting the knowledge that exhibits robust novel class generalization"
  - [section] "over-discrimination causes severe dimensional collapse on the base classes, where the embedding vectors occupy a low dimensional subspace of the full dimensional embedding space"
  - [corpus] "Perturbing the Gradient for Alleviating Meta Overfitting" - related work on meta-overfitting suggests mutual non-exclusivity and lack of diversity cause global functions to fit meta-training tasks poorly.
- Break condition: If base-class and novel-class distributions share sufficient overlap, the collapse may not harm generalization.

### Mechanism 2
- Claim: Knowledge distillation via symmetric KL (SKL) and nearest-neighbor KL (NNSKL) preserves generalizable knowledge from a teacher model during meta-training.
- Mechanism: SKL penalizes divergence between student and teacher classifier output distributions over individual images, implicitly restricting student embeddings from drifting far from teacher embeddings. NNSKL extends this to task-level by penalizing divergence between nearest-neighbor classifier outputs, preserving relationships between query and support embeddings.
- Core assumption: The teacher model (selected as best validation accuracy during meta-training) contains generalizable features that the student can benefit from.
- Evidence anchors:
  - [abstract] "restrict the symmetric Kullback-Leibler (SKL) divergence between the output distribution of the linear classifier of the teacher model and that of the student model"
  - [section] "NNSKL takes few-shot tasks as input and penalizes the symmetric KL divergence between the output distributions of the nearest-neighbor classifier of the teacher and the student models"
  - [corpus] "Exploring complementary strengths of invariant and equivariant representations for few-shot learning" - related work on distillation for few-shot learning.
- Break condition: If the teacher model overfits to base classes, it may not provide useful generalizable knowledge.

### Mechanism 3
- Claim: Combining SKL and NNSKL in meta-training balances base-class discrimination and novel-class generalization, reducing overfitting and improving few-shot performance.
- Mechanism: Supervised contrastive loss (SC) boosts base-class discrimination but causes over-discrimination. SKL prevents individual embeddings from collapsing to base-class modes. NNSKL enforces task-level consistency between student and teacher nearest-neighbor classifiers, preserving relationships needed for novel-class generalization. Together, they mitigate the trade-off.
- Core assumption: The student model can benefit from both instance-level and task-level distillation signals without being overwhelmed.
- Evidence anchors:
  - [abstract] "By combining SKL and NNSKL in meta-training, the model achieves even better performance and surpasses state-of-the-art results on several benchmarks"
  - [section] "By combining SKL and NNSKL with the supervised contrastive loss, the proposed meta-training process considers novel class generalization during training and consistently improved few-shot performance"
  - [corpus] Weak - no direct corpus evidence for combined SKL+NNSKL; this is a novel contribution.
- Break condition: If either SKL or NNSKL dominates, the balance may be lost and overfitting may reoccur.

## Foundational Learning

- Concept: Singular value decomposition (SVD) and dimension collapse
  - Why needed here: To diagnose whether embeddings are collapsing into a low-dimensional subspace, indicating over-discrimination.
  - Quick check question: If an embedding matrix has 640 singular values but only the first 64 are large, what does this imply about the embedding space?
- Concept: Knowledge distillation and KL divergence
  - Why needed here: To understand how teacher-student regularization preserves generalizable knowledge during meta-training.
  - Quick check question: What is the difference between symmetric KL divergence and standard KL divergence?
- Concept: Supervised contrastive loss and prototype-based classification
  - Why needed here: To understand how the loss function drives embeddings toward class prototypes and why this can cause over-discrimination.
  - Quick check question: How does the variance of support set prototypes affect the gradient of the supervised contrastive loss?

## Architecture Onboarding

- Component map: Input -> Backbone (ResNet12) -> Embedding -> (FC for SKL, NN for NNSKL/SC) -> Loss -> Backward pass
- Critical path: Input → Backbone → Embedding → (FC for SKL, NN for NNSKL/SC) → Loss → Backward pass
- Design tradeoffs:
  - SKL vs NNSKL: Instance-level vs task-level distillation; SKL is simpler but NNSKL better preserves task relationships.
  - Teacher selection: Best validation accuracy vs fixed pre-trained model; dynamic selection adapts but may overfit to validation set.
  - Loss weights (λ₁, λ₂): Must balance distillation strength vs SC strength; too high causes underfitting, too low ineffective.
- Failure signatures:
  - Overfitting: Validation accuracy drops while training loss decreases; singular values collapse.
  - Underfitting: Both training and validation accuracy remain low; gradients vanish due to strong distillation.
  - Mode collapse: Embeddings cluster to few modes; KL divergence too strong.
- First 3 experiments:
  1. Train with only SC loss; plot singular value spectrum and validation accuracy to confirm overfitting.
  2. Add SKL only; compare singular values and validation accuracy to SC baseline.
  3. Add NNSKL only; compare to SKL and SC to isolate task-level benefits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of teacher model affect the effectiveness of SKL and NNSKL in mitigating overfitting?
- Basis in paper: [explicit] The paper mentions selecting the teacher model as the one with the best validation accuracy during meta-training.
- Why unresolved: The paper does not explore the impact of using different teacher models or the timing of teacher model selection on the performance of SKL and NNSKL.
- What evidence would resolve it: Comparative studies showing the performance of SKL and NNSKL using different teacher models or at different stages of meta-training.

### Open Question 2
- Question: What are the long-term effects of using SKL and NNSKL on the model's ability to generalize to even more diverse datasets?
- Basis in paper: [inferred] The paper demonstrates improved performance on several benchmark datasets but does not explore long-term generalization capabilities.
- Why unresolved: The experiments focus on specific datasets and do not extend to testing the model's generalization to unseen or more diverse datasets over time.
- What evidence would resolve it: Longitudinal studies evaluating the model's performance on a wide range of datasets over extended periods.

### Open Question 3
- Question: How do SKL and NNSKL influence the model's performance in low-resource or imbalanced data scenarios?
- Basis in paper: [inferred] The paper addresses overfitting in few-shot learning but does not specifically investigate scenarios with limited or imbalanced data.
- Why unresolved: The experiments are conducted on balanced datasets, and there is no analysis of how SKL and NNSKL perform under data scarcity or imbalance.
- What evidence would resolve it: Experiments and analyses showing the impact of SKL and NNSKL on model performance in low-resource or imbalanced data environments.

## Limitations

- The teacher model selection based on validation accuracy during meta-training could introduce overfitting to the validation set, potentially limiting generalization.
- The effectiveness of NNSKL relies on the assumption that the nearest-neighbor classifier from the teacher model provides reliable task-level signals, which may not hold if base and novel class distributions differ significantly.
- The paper lacks detailed implementation specifics for the rotation prediction task and nearest-neighbor classifier output extraction, which could affect reproducibility.

## Confidence

- **High**: The mechanism of over-discrimination causing dimensional collapse is well-supported by evidence and aligns with established learning theory.
- **Medium**: The effectiveness of SKL and NNSKL knowledge distillation is demonstrated empirically but lacks strong theoretical justification for why this specific combination works better than alternatives.
- **Medium**: The state-of-the-art results on benchmarks are promising but could be influenced by hyperparameter tuning and specific experimental conditions.

## Next Checks

1. **Teacher Model Validation**: Analyze the distribution of teacher model selection over multiple runs to assess whether the validation-based selection is consistently choosing robust models or overfitting to the validation set.
2. **Singular Value Analysis**: Conduct a comprehensive analysis of singular value distributions across different training configurations (SC only, SC+SKL, SC+NNSKL, SC+SKL+NNSKL) to quantify dimensional collapse reduction.
3. **Cross-Domain Generalization**: Evaluate the method's performance on cross-domain few-shot tasks (e.g., miniImageNet→CUB) to assess whether the knowledge distillation preserves truly generalizable features or overfits to domain-specific patterns.