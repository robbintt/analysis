---
ver: rpa2
title: 'Diversity and Diffusion: Observations on Synthetic Image Distributions with
  Stable Diffusion'
arxiv_id: '2311.00056'
source_url: https://arxiv.org/abs/2311.00056
tags:
- images
- accuracy
- diversity
- disamb2
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Using synthetic images from text-to-image models like Stable Diffusion
  to train image classifiers fails to match the performance of training with natural
  images, despite synthetic images appearing realistic. Four main issues limit synthetic
  data: prompt ambiguity, poor adherence to prompts, lack of diversity, and failure
  to accurately represent target concepts.'
---

# Diversity and Diffusion: Observations on Synthetic Image Distributions with Stable Diffusion

## Quick Facts
- arXiv ID: 2311.00056
- Source URL: https://arxiv.org/abs/2311.00056
- Reference count: 10
- Primary result: Synthetic images exhibit less diversity and centroid shifts that hurt classification performance compared to natural images

## Executive Summary
Using synthetic images from text-to-image models like Stable Diffusion to train image classifiers fails to match the performance of training with natural images, despite synthetic images appearing realistic. Four main issues limit synthetic data: prompt ambiguity, poor adherence to prompts, lack of diversity, and failure to accurately represent target concepts. We quantified diversity and concept shifts in CLIP embeddings and found that synthetic images exhibit less diversity and can have centroid shifts that hurt classification. Our results highlight the need for better alignment between synthetic and natural image distributions for effective classifier training.

## Method Summary
The study compares synthetic images generated by Stable Diffusion v1.5 with natural images from ImageNet-1K for training image classifiers. Synthetic images are created using prompts derived from ImageNet class descriptions with various modifications (Disamb, Disamb2.5, PromptAug) to control diversity and concept representation. CLIP embeddings are computed for both synthetic and natural images to measure diversity through Centroid Distance and classification performance through Centroid Accuracy. A ResNet-RS-152 classifier is trained on each dataset and evaluated on the ImageNet-1K validation set.

## Key Results
- Synthetic images exhibit significantly less diversity than natural images, measured through Centroid Distance in CLIP embedding space
- Centroid shifts between synthetic and natural image distributions negatively impact classification performance
- Linear separability of CLIP embeddings for prompts and images was observed, an unexpected finding that affects concept representation

## Why This Works (Mechanism)

### Mechanism 1: Reduced Diversity in Synthetic Images
- Claim: Synthetic image datasets exhibit reduced diversity compared to natural image datasets, causing classifiers to underperform.
- Mechanism: The diffusion process in Stable Diffusion introduces variation, but this variation is constrained and does not fully cover the breadth of natural image distributions.
- Core assumption: CLIP embeddings capture semantic content in a way that reflects diversity differences between synthetic and natural images.
- Evidence anchors:
  - [abstract] "synthetic images exhibit less diversity and can have centroid shifts that hurt classification"
  - [section] "Centroid Distance increases (gets more diverse), Disamb (1.23×10−5) < Disamb 2.5 (2 .05×10−5) < PromptAug (2.49×10−5)"
- Break condition: If CLIP embeddings fail to accurately represent semantic diversity, or if the diffusion process is modified to introduce broader variation.

### Mechanism 2: Centroid Shifts Cause Classification Errors
- Claim: Centroid shifts in CLIP embedding space between synthetic and natural images lead to classification errors.
- Mechanism: When the average embedding (centroid) of synthetic images for a class differs from the centroid of natural images for the same class, a classifier trained on synthetic images will misclassify natural images that fall near the natural centroid but far from the synthetic centroid.
- Core assumption: Classification performance depends on the proximity of query embeddings to class centroids in the embedding space.
- Evidence anchors:
  - [abstract] "synthetic images exhibit less diversity and can have centroid shifts that hurt classification"
  - [section] "accuracy is affected by both centroid-shift and diversity. Our Centroid Accuracy and Centroid Distance techniques in combination isolate the effects of diversity and show that diversity has a larger impact on accuracy than centroid-shift."
- Break condition: If the embedding space geometry changes such that centroid proximity is no longer the dominant factor in classification.

### Mechanism 3: CLIP Embedding Linear Separability
- Claim: CLIP embeddings of prompts and images are linearly separable, which affects how well synthetic images represent the intended concepts.
- Mechanism: CLIP is trained to reward correct prompt-image pairings without requiring the prompt and image embeddings to be identical. This allows for a separation in embedding space that can cause synthetic images to drift from their generating prompts.
- Core assumption: The linear separability of prompt and image embeddings in CLIP space is a fundamental property that influences image generation fidelity.
- Evidence anchors:
  - [abstract] "surprising insights into the geometry of CLIP embeddings"
  - [section] "the embeddings are linearly separable — an intuitively surprising finding as successful zero-shot classifiers and most clustering is rarely visualized as such"
- Break condition: If CLIP training objectives or architecture are modified to reduce or eliminate linear separability.

## Foundational Learning

- Concept: CLIP model and its contrastive training
  - Why needed here: CLIP maps both text prompts and images into a shared embedding space, which is central to analyzing diversity and centroid shifts.
  - Quick check question: What is the dimensionality of CLIP's text and image embeddings, and how are they normalized?

- Concept: Diffusion models and guidance scale
  - Why needed here: Diffusion models generate images by gradually denoising random noise, and the guidance scale controls adherence to the text prompt, affecting diversity.
  - Quick check question: What happens to image coherence and diversity as the guidance scale is varied?

- Concept: Centroid Distance and Centroid Accuracy metrics
  - Why needed here: These metrics quantify diversity and classification performance in CLIP embedding space, respectively.
  - Quick check question: How is Centroid Distance calculated, and what does a higher value indicate?

## Architecture Onboarding

- Component map: Stable Diffusion -> CLIP model -> Centroid Distance/Angular metrics -> ResNet-RS-152 classifier
- Critical path:
  1. Generate synthetic images using Stable Diffusion with specific prompts
  2. Compute CLIP embeddings for both synthetic and natural images
  3. Calculate Centroid Distance and Centroid Accuracy to assess diversity and classification performance
  4. Train classifiers on different datasets and evaluate their performance
- Design tradeoffs:
  - Higher guidance scale in Stable Diffusion leads to better prompt adherence but reduced diversity
  - Using more diverse prompts can improve synthetic dataset quality but may introduce ambiguity
  - Centroid-based metrics simplify analysis but may mask the full distribution shape
- Failure signatures:
  - Low Centroid Accuracy indicates poor alignment between synthetic and natural image distributions
  - High Centroid Distance suggests insufficient diversity in synthetic images
  - Linear separability of prompt and image embeddings in CLIP space can cause concept drift
- First 3 experiments:
  1. Generate synthetic images for a subset of ImageNet classes using Stable Diffusion with varying guidance scales
  2. Compute CLIP embeddings for both synthetic and natural images, and calculate Centroid Distance
  3. Train a ResNet-RS-152 classifier on synthetic images and evaluate its performance on natural images

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific modifications to CLIP's training process would be required to eliminate the linear separability between text and image embeddings while maintaining or improving classification performance?
- Basis in paper: [explicit] The paper notes that CLIP embeddings for text and images are linearly separable, which was an unexpected finding given that zero-shot classifiers and clustering typically work well with these embeddings.
- Why unresolved: The paper identifies this phenomenon but does not explore modifications to CLIP's architecture or training objective that could address it.
- What evidence would resolve it: Comparative experiments showing classification performance and embedding geometry when CLIP is trained with modified objectives that explicitly encourage text-image embedding alignment versus maintaining separation.

### Open Question 2
- Question: How does the diversity of synthetic images generated by different TTI models (beyond Stable Diffusion) compare to natural image datasets when measured using the Centroid Distance metric, and does this correlate with downstream classification performance?
- Basis in paper: [inferred] The paper extensively analyzes diversity in Stable Diffusion-generated images using Centroid Distance and relates it to classification performance, but only examines one TTI system.
- Why unresolved: The study is limited to Stable Diffusion, leaving open whether these findings generalize to other generative models like DALL-E 2, Imagen, or newer models.
- What evidence would resolve it: Systematic comparison of Centroid Distances and classification accuracies across multiple TTI systems using identical evaluation protocols and datasets.

### Open Question 3
- Question: What is the optimal balance between prompt adherence (clip_guidance_scale) and diversity for synthetic data generation that maximizes classification performance across diverse object categories?
- Basis in paper: [explicit] The paper shows that different cgs values affect both diversity and classification accuracy, with intermediate values (2.5) outperforming extremes, but does not systematically explore the optimal trade-off.
- Why unresolved: The study uses a few discrete cgs values but doesn't perform a comprehensive sweep or analyze the relationship between prompt adherence, diversity, and accuracy in a unified framework.
- What evidence would resolve it: A detailed experimental study mapping cgs values to both Centroid Distance and classification accuracy across multiple object categories, identifying regions of optimal trade-off.

### Open Question 4
- Question: Beyond k-nearest neighbors, what alternative classification algorithms could better leverage the full distribution of synthetic image embeddings to improve performance over centroid-based methods?
- Basis in paper: [explicit] The paper notes that k-NN with k=5 sometimes outperforms centroid-based classification and suggests this warrants further investigation, but doesn't explore other methods.
- Why unresolved: The study only briefly examines k-NN variations and suggests more sophisticated methods could be beneficial without specifying which approaches might work best.
- What evidence would resolve it: Comparative experiments testing various classification algorithms (e.g., Gaussian mixture models, kernel density estimation, deep metric learning) on synthetic image embeddings against centroid and k-NN baselines.

## Limitations

- The study focuses exclusively on Stable Diffusion v1.5, potentially limiting generalizability to other text-to-image models
- The analysis relies on CLIP embeddings as a proxy for semantic diversity, which may not capture all relevant aspects of image distributions
- The paper does not explore potential mitigation strategies or whether synthetic data might be useful for pre-training versus fine-tuning

## Confidence

- High confidence in the core finding that synthetic images exhibit less diversity and centroid shifts that hurt classification performance
- Medium confidence in the assumption that CLIP embeddings fully capture semantic diversity differences
- Medium confidence in the generalizability of findings to other text-to-image models beyond Stable Diffusion

## Next Checks

1. Test with different diffusion models (DALL-E 2, Imagen, newer Stable Diffusion versions) and CLIP variants to assess robustness of findings
2. Conduct ablation studies varying guidance scale across a wider range to isolate diversity effects on classification performance
3. Explore whether combining synthetic and natural data yields performance benefits for classifier training