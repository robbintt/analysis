---
ver: rpa2
title: Exploring the Limits of Deep Image Clustering using Pretrained Models
arxiv_id: '2303.17896'
source_url: https://arxiv.org/abs/2303.17896
tags:
- clustering
- image
- learning
- temi
- pretrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the limits of image clustering by leveraging
  pretrained feature extractors, aiming to classify images without labels. The core
  method, TEMI (Teacher Ensemble-weighted Pointwise Mutual Information), introduces
  a self-distillation framework with a novel objective that combines temperature-scaled
  pointwise mutual information and instance weighting.
---

# Exploring the Limits of Deep Image Clustering using Pretrained Models

## Quick Facts
- arXiv ID: 2303.17896
- Source URL: https://arxiv.org/abs/2303.17896
- Reference count: 15
- Primary result: TEMI achieves 61.6% clustering accuracy on ImageNet without labels

## Executive Summary
This paper investigates the limits of image clustering by leveraging pretrained feature extractors. The proposed TEMI (Teacher Ensemble-weighted Pointwise Mutual Information) framework introduces a self-distillation approach that combines temperature-scaled PMI with instance weighting to improve clustering accuracy. The method significantly outperforms k-means clustering, achieving state-of-the-art results of 61.6% accuracy on ImageNet and 12.2% gains on CIFAR100 across 17 different pretrained models.

## Method Summary
TEMI uses a self-distillation framework with teacher-student architecture. It mines k-NN pairs in pretrained feature space to generate pseudo-labels, then trains clustering heads using a temperature-scaled PMI objective with instance weighting based on teacher predictions. The teacher parameters are updated via exponential moving average, and ensemble averaging over multiple heads provides stability. The method is tested across 17 pretrained models including supervised ConvNets, self-supervised vision transformers, and CLIP models.

## Key Results
- TEMI improves over k-means by 6.1% on ImageNet and 12.2% on CIFAR100
- Self-supervised vision transformers achieve 61.6% accuracy on ImageNet, state-of-the-art for unsupervised clustering
- TEMI consistently outperforms baseline methods across all 17 pretrained models tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-distillation with teacher ensemble-weighted PMI attenuates false positive pairs while exploiting structure in pretrained feature spaces.
- Mechanism: k-NN mining generates image pairs likely to share labels, instance weighting emphasizes true positives, and ensemble averaging stabilizes teacher targets.
- Core assumption: Nearest neighbors in pretrained feature space are more likely to share the same label than random pairs.
- Evidence anchors: 6.1% and 12.2% gains over k-means on ImageNet and CIFAR100; model averaging produces more accurate predictions.
- Break condition: If pretrained features don't encode label-related information, k-NN pairs become uninformative and weighting fails.

### Mechanism 2
- Claim: Temperature-scaled PMI with instance weighting balances cluster utilization and discriminative confidence.
- Mechanism: β parameter reduces overconfidence early in training, instance weighting emphasizes high-confidence pairs, uniform cluster utilization emerges naturally under balanced priors.
- Core assumption: Balanced cluster utilization is desirable and achievable through objective design.
- Evidence anchors: β introduced to reduce overconfidence without affecting optimal solution; ˜q(c) ≈ const emerges from optimization.
- Break condition: If true class distribution is highly imbalanced, forcing uniform utilization may hurt performance.

### Mechanism 3
- Claim: Vision transformers capture more transferable label-related features than ConvNets.
- Mechanism: Self-supervised ViT backbones achieve higher clustering accuracy than supervised ConvNets when fine-tuned with TEMI.
- Core assumption: Self-supervised ViT pretraining produces features more aligned with semantic similarity than supervised ConvNet pretraining.
- Evidence anchors: 61.6% accuracy on ImageNet using self-supervised ViTs; state-of-the-art results achieved.
- Break condition: If downstream dataset has very different characteristics from ImageNet, transferability advantage may disappear.

## Foundational Learning

- Concept: Pointwise mutual information (PMI)
  - Why needed here: PMI measures association between image pairs in feature space, forming basis of clustering objective.
  - Quick check question: What does PMI measure between two random variables, and why is it suitable for clustering?

- Concept: Self-distillation with teacher-student framework
  - Why needed here: Teacher provides stable targets while student learns to cluster, avoiding degenerate solutions.
  - Quick check question: How does exponential moving average of teacher parameters help stabilize training?

- Concept: Nearest neighbor mining in feature space
  - Why needed here: k-NN pairs serve as pseudo-labels indicating likely same-class relationships without ground truth.
  - Quick check question: Why might nearest neighbors in feature space not always share the same class label?

## Architecture Onboarding

- Component map:
  Pretrained backbone (g) -> k-NN mining module -> Student head (hs) and Teacher head (ht) -> Instance weighting module -> Loss aggregation

- Critical path:
  1. Extract features from pretrained backbone
  2. Mine k-NN pairs
  3. Forward pass through student and teacher heads
  4. Compute instance weights from teacher predictions
  5. Calculate TEMI loss and backpropagate to student

- Design tradeoffs:
  - Multiple heads improve stability but increase memory/computation
  - Larger k in k-NN mining increases recall but also noise
  - β parameter trades off between uniform cluster utilization and discriminative power

- Failure signatures:
  - All samples collapse to one cluster → Likely β too low or k too small
  - Clusters are highly imbalanced → β too high or instance weighting too aggressive
  - No improvement over k-means → Pretrained features lack label structure or k-NN pairs too noisy

- First 3 experiments:
  1. Implement k-means baseline on pretrained features to establish baseline clustering accuracy
  2. Add single-head PMI loss without instance weighting to verify basic framework works
  3. Introduce instance weighting and multiple heads, tune β on validation set without labels using entropy criteria

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TEMI's performance compare to state-of-the-art supervised methods on small-scale datasets when both use models pretrained on external data?
- Basis in paper: Table 4 shows TEMI DINO ViT-B/16 achieves 94.5% ACC on CIFAR10 and 63.2% on CIFAR20, but comparison to supervised methods is not discussed in depth.
- Why unresolved: Paper provides comparison but lacks detailed analysis of relative performance versus supervised methods.
- What evidence would resolve it: Detailed analysis comparing TEMI's performance to supervised methods across multiple small-scale datasets.

### Open Question 2
- Question: What is the impact of using different numbers of nearest neighbors (k-NN) on TEMI's clustering performance across various datasets and pretrained models?
- Basis in paper: Different k-NN values used for different datasets (25-NN for ImageNet, 150-NN for CIFAR20) but impact of varying this hyperparameter is not explored.
- Why unresolved: Choice of k-NN is critical but its effect on clustering accuracy is not thoroughly investigated.
- What evidence would resolve it: Ablation study showing clustering accuracy for different k-NN values across multiple datasets and pretrained models.

### Open Question 3
- Question: How does the proposed instance weighting mechanism affect the clustering of semantically ambiguous classes?
- Basis in paper: Paper discusses that w(x,x') assigns higher weights to true positive pairs and can negatively impact training with true positive pairs only; mentions cases of ambiguous human-annotated labels.
- Why unresolved: While behavior is identified, paper doesn't explore how instance weighting affects clustering of inherently ambiguous classes.
- What evidence would resolve it: Analysis of clustering performance for semantically ambiguous classes with and without instance weighting.

## Limitations
- Performance bounded by quality and transferability of pretrained features
- Method assumes nearest neighbors in feature space share labels, which may not hold for fine-grained distinctions
- Self-supervised vision transformer advantage may not generalize to datasets very different from ImageNet

## Confidence
- **High confidence**: TEMI consistently improves over k-means baselines across 17 pretrained models and multiple datasets
- **Medium confidence**: Advantage of self-supervised vision transformers over supervised ConvNets for feature transferability
- **Medium confidence**: Effectiveness of instance weighting and ensemble averaging based on ablation studies

## Next Checks
1. Test TEMI on out-of-distribution datasets (medical imaging, satellite imagery) to evaluate feature transferability limits
2. Compare TEMI with state-of-the-art clustering methods that use ground truth labels for hyperparameter tuning
3. Conduct ablation studies varying k-NN parameters and β across different pretrained model families