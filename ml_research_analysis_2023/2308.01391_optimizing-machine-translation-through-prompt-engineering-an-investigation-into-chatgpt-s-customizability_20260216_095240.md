---
ver: rpa2
title: 'Optimizing Machine Translation through Prompt Engineering: An Investigation
  into ChatGPT''s Customizability'
arxiv_id: '2308.01391'
source_url: https://arxiv.org/abs/2308.01391
tags:
- translation
- translations
- target
- prompts
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores how specifying translation purpose and target
  audience in prompts affects ChatGPT-generated translations. Using prompts aligned
  with ISO translation standards, researchers evaluated marketing texts and culturally
  dependent idioms.
---

# Optimizing Machine Translation through Prompt Engineering: An Investigation into ChatGPT's Customizability

## Quick Facts
- **arXiv ID**: 2308.01391
- **Source URL**: https://arxiv.org/abs/2308.01391
- **Reference count**: 0
- **Primary result**: Purpose- and audience-driven prompt engineering significantly improves ChatGPT translation quality compared to baseline MT, producing more natural and contextually appropriate results.

## Executive Summary
This study investigates how specifying translation purpose and target audience in prompts affects ChatGPT-generated translations. Using prompts aligned with ISO translation standards, researchers evaluated marketing texts and culturally dependent idioms through qualitative analysis by a professional translator and cosine similarity calculations. The findings demonstrate that purpose- and audience-driven prompts significantly improve translation quality, producing more natural and contextually appropriate results than conventional machine translation. Notably, dynamic equivalence prompts enabled creative substitutions (e.g., replacing a Japanese singer with culturally equivalent English-speaking singers), supporting practical professional translation workflows.

## Method Summary
The research used two Japanese source texts: a marketing text about cosmetics and a culturally dependent idiom, plus a dynamic equivalence test case. Researchers employed OpenAI's ChatGPT API to generate translations using prompt engineering that specified "Purpose of the translation" and "Target audience" parameters, producing multiple variants per prompt. Baseline translations were obtained from DeepL, Google Translate, and ChatGPT without custom prompts. Cosine similarity between source and target embeddings was calculated using OpenAI's text-embedding-ada-002 API, and all translations underwent qualitative evaluation by a professional translator. The approach compared purpose-driven, audience-driven, and dynamic equivalence translations against baseline MT outputs.

## Key Results
- Purpose- and audience-driven prompts significantly improved translation quality by industry standards, producing more natural and contextually appropriate results
- Dynamic equivalence prompts enabled creative cultural substitutions (e.g., Hibari Misora → Ella Fitzgerald) that better served target audience comprehension
- Baseline translations ranked highest by cosine similarity but were not necessarily preferred by the human evaluator, highlighting limitations of embedding-based metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Purpose- and audience-specific prompts alter ChatGPT's translation output to better align with professional translation standards.
- Mechanism: The prompt provides explicit translation specifications (purpose, target audience, locale, register) that guide the LLM to prioritize contextual appropriateness over literal accuracy.
- Core assumption: ChatGPT's generation process can be steered by meta-level instructions that frame the task beyond simple language conversion.
- Evidence anchors:
  - [abstract] "integration of the purpose and target audience into prompts can indeed modify the generated translations, generally enhancing the translation quality by industry standards."
  - [section] "This transformation, aimed at satisfying the criteria of purpose and target audience, generally improved the translation quality by industry standards."
  - [corpus] Weak evidence; no directly comparable studies in the corpus addressing purpose/audience-driven prompt engineering for translation quality.

### Mechanism 2
- Claim: Cosine similarity between source and target embeddings correlates with professional translator preference for translation quality.
- Mechanism: The embedding model (ada-002) captures semantic similarity; translations that preserve meaning while adapting culturally tend to have higher cosine similarity to the source.
- Core assumption: Semantic preservation measured by embeddings aligns with human notions of "good translation" beyond literal accuracy.
- Evidence anchors:
  - [section] "Reviewing the cosine similarity rankings, the baseline translations... are ranked 1st and 2nd... v1, v3, and v2 rank 3rd, 4th, and 5th, respectively."
  - [corpus] Weak evidence; no corpus entries explicitly validate cosine similarity as a proxy for translation quality assessment.

### Mechanism 3
- Claim: Dynamic equivalence prompts enable ChatGPT to generate creative, culturally adapted translations that are more appropriate for the target audience.
- Mechanism: By instructing the model to replace culturally bound terms with equivalents meaningful in the target culture, the translation becomes more natural and accessible.
- Core assumption: LLMs can understand and apply the concept of dynamic equivalence when explicitly prompted, allowing substitutions beyond literal translation.
- Evidence anchors:
  - [section] "Specifically, we substituted the renowned Japanese singer Hibari Misora with Ella Fitzgerald... facilitating a high degree of creative translation."
  - [corpus] Weak evidence; no corpus entries specifically address dynamic equivalence prompting strategies.

## Foundational Learning

- Concept: Translation specifications (purpose, audience, locale, register)
  - Why needed here: These parameters define the quality criteria for a translation, guiding both human translators and LLM outputs toward contextually appropriate results.
  - Quick check question: How would you adjust a marketing translation for a teenage audience versus a corporate audience?

- Concept: Cosine similarity and semantic embeddings
  - Why needed here: Provides a quantitative measure of semantic alignment between source and target texts, complementing qualitative human evaluation.
  - Quick check question: What does a high cosine similarity score between two sentences indicate about their semantic content?

- Concept: Dynamic equivalence vs. formal correspondence
  - Why needed here: Different translation strategies are appropriate for different contexts; understanding when to prioritize meaning over form is crucial for effective translation.
  - Quick check question: When would you choose to substitute a culturally specific reference with a more universal one in a translation?

## Architecture Onboarding

- Component map: Prompt engineering interface → ChatGPT API → Embedding API (ada-002) → Human evaluation layer
- Critical path: Define translation specs → Craft prompts → Generate translations → Calculate cosine similarity → Qualitative assessment → Iterate prompts
- Design tradeoffs: Balancing literal accuracy with cultural adaptation; computational cost of embedding comparisons vs. human evaluation time; prompt complexity vs. model consistency
- Failure signatures: Literal translations that ignore purpose/audience; low cosine similarity despite fluent output; inappropriate cultural substitutions; inconsistent results across prompt variants
- First 3 experiments:
  1. Vary only the purpose parameter in prompts for the same source text and compare output differences.
  2. Test different target audience specifications (e.g., age groups, expertise levels) and measure impact on translation style.
  3. Implement A/B testing with and without cosine similarity feedback to refine prompt engineering process.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does prompt engineering affect translation quality across different language pairs, particularly low-resource or structurally distant languages?
- Basis in paper: [explicit] The paper notes that most existing studies focus on high-resource European languages, while this study investigates Japanese-English translation, suggesting a gap in understanding performance across diverse language pairs.
- Why unresolved: The study only examined Japanese-English translation; broader linguistic diversity in testing is needed to generalize findings.
- What evidence would resolve it: Comparative experiments testing prompt-engineered translations across multiple language pairs, including low-resource and structurally diverse languages, with qualitative and quantitative evaluations.

### Open Question 2
- Question: What is the optimal structure and wording of prompts to maximize translation quality while maintaining consistency with industry standards?
- Basis in paper: [inferred] The study used specific prompts for purpose and audience but acknowledges the need for further experimentation with prompt development and structure.
- Why unresolved: The research used a limited set of manually crafted prompts; systematic exploration of prompt variations is needed.
- What evidence would resolve it: Controlled experiments testing different prompt structures, wordings, and parameter combinations to identify optimal configurations for translation quality.

### Open Question 3
- Question: How do cosine similarity metrics correlate with human judgments of translation quality, particularly for creative or culturally adapted translations?
- Basis in paper: [explicit] The study used cosine similarity alongside human evaluation, noting that high cosine similarity doesn't always align with "good translation" criteria for marketing or cultural content.
- Why unresolved: The relationship between semantic similarity and translation quality is complex, especially when cultural adaptation is prioritized over literal accuracy.
- What evidence would resolve it: Large-scale studies correlating cosine similarity scores with human quality assessments across different translation types and purposes.

## Limitations

- The qualitative assessment relies on a single professional translator's judgment, introducing subjectivity that may not represent broader professional consensus
- The study uses only two source texts (marketing copy and one idiom), limiting generalizability across different text types and domains
- Cosine similarity correlation with translation quality is weakly supported and may not capture creative or culturally adapted translations effectively

## Confidence

**High Confidence**: The claim that specifying purpose and target audience in prompts produces different translations than baseline MT is well-supported by the qualitative analysis showing improved naturalness and contextual appropriateness.

**Medium Confidence**: The assertion that dynamic equivalence prompts enable culturally appropriate substitutions is supported by the case study, but this represents a single example and may not generalize to all cultural references.

**Low Confidence**: The correlation between cosine similarity scores and translation quality is weakly supported, as the study acknowledges that higher-ranked translations by cosine similarity were not necessarily preferred by the human evaluator.

## Next Checks

1. **Multi-evaluator validation**: Replicate the qualitative assessment with 5-10 professional translators rating the same set of translations using standardized rubrics for fluency, accuracy, and appropriateness to test consistency and reliability.

2. **Cross-linguistic and domain generalization**: Test the prompt engineering approach with at least 3 additional language pairs and 5 different text domains (legal, medical, technical, literary, news) to assess consistent quality improvements across diverse contexts.

3. **A/B testing with embedding feedback**: Conduct an experiment where prompt engineering is iteratively refined using both qualitative feedback and cosine similarity scores, comparing final outputs to those generated using only qualitative feedback to validate embedding metrics for prompt optimization.