---
ver: rpa2
title: Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps
  Domain
arxiv_id: '2310.05063'
source_url: https://arxiv.org/abs/2310.05063
tags:
- time
- series
- forecasting
- pre-training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces three large-scale time series forecasting
  datasets from the cloud operations domain, with the largest containing billions
  of observations. It aims to address the lack of large-scale public time series data
  for studying pre-training and transfer learning.
---

# Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain

## Quick Facts
- arXiv ID: 2310.05063
- Source URL: https://arxiv.org/abs/2310.05063
- Reference count: 40
- Key outcome: Introduces three large-scale time series datasets and shows a 27% error reduction with Transformer pre-training in cloud operations

## Executive Summary
This paper addresses the critical gap in large-scale time series data for studying pre-training and transfer learning by introducing three massive CloudOps datasets totaling over 1.8 billion observations. The authors demonstrate that masked encoder Transformers with rotary positional embeddings achieve state-of-the-art zero-shot forecasting performance, outperforming classical baselines by 27% and deep learning approaches by 4% on the largest dataset. The work establishes empirical scaling laws showing predictable performance improvements with both model and dataset size increases.

## Method Summary
The method employs masked encoder Transformers pre-trained on CloudOps time series data using reconstruction objectives, then evaluated through zero-shot forecasting on held-out test sets. The architecture uses rotary positional embeddings for relative temporal information and Student-T distribution heads for probabilistic forecasting. Training follows standard masked language modeling with instance normalization and feature extraction including log scaling, lags, and date/time features. The approach is evaluated across three datasets (azure2017, borg2011, ali2018) using sMAPE and CRPS metrics.

## Key Results
- Masked encoder Transformers achieve 27% lower error than classical baselines on the largest dataset
- Performance scales predictably with both model size and dataset size
- Zero-shot transfer outperforms fine-tuning in the in-collection setting
- Student-T parametric heads provide robust probabilistic forecasts across diverse patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masked encoder Transformers with rotary positional embeddings and Student-T heads achieve strong zero-shot forecasting in the in-collection setting.
- Mechanism: The masked encoder learns bidirectional context representations across both pre-training and prediction ranges, enabling it to reconstruct masked future values without autoregressive dependencies. Rotary positional embeddings provide precise relative positional information that outperforms date/time features for capturing temporal dynamics.
- Core assumption: The in-collection setting preserves sufficient semantic similarity between pre-training and target time series for zero-shot transfer to work effectively.
- Evidence anchors:
  - [abstract] "We show that it is a strong zero-shot baseline and benefits from further scaling, both in model and dataset size."
  - [section 4.2] "Our first observation is that although encoder-decoder models have higher parameter counts, they did not outperform masked encoder or the encoder models."
  - [corpus] Weak evidence - related work focuses on scaling laws and LLM transfer, but lacks direct empirical validation of masked encoder zero-shot performance in time series.
- Break Condition: Distribution shift between pre-training and target collections exceeds the model's representational capacity, causing zero-shot performance to degrade below baseline models trained directly on the target data.

### Mechanism 2
- Claim: Performance scales predictably with model size and dataset size in the pre-training phase.
- Mechanism: Larger models capture more complex temporal patterns and relationships, while larger datasets provide diverse examples that improve generalization. The scaling relationship follows a power law where error decreases with both parameters and observations.
- Core assumption: The CloudOps domain exhibits consistent statistical properties across collections that benefit from increased model capacity and data diversity.
- Evidence anchors:
  - [abstract] "We show that it is a strong zero-shot baseline and benefits from further scaling, both in model and dataset size."
  - [section 4.4] "On our largest dataset, azure2017, we observe a clear trend where performance improves as model size and number of observations increase."
  - [corpus] Moderate evidence - related papers discuss scaling laws for pre-training but lack direct experimental validation on time series forecasting tasks.
- Break Condition: The scaling relationship saturates when model capacity exceeds the complexity of the underlying temporal patterns, or when dataset size provides diminishing returns due to redundancy.

### Mechanism 3
- Claim: Student-T parametric distribution heads provide robust probabilistic forecasts across diverse time series patterns.
- Mechanism: The Student-T distribution's heavy tails accommodate outliers and non-Gaussian patterns common in operational time series, while the independence assumption simplifies computation without sacrificing accuracy.
- Core assumption: Time series in the CloudOps domain exhibit heavy-tailed distributions and outliers that benefit from Student-T modeling over Gaussian assumptions.
- Evidence anchors:
  - [section 4.3.1] "We compared the parametric distribution approach (Student-T) with several quantile function and normalizing flow based heads, and found that taking the simple approach of a parametric distribution proved to be a simple and robust choice, performing well across datasets and metrics."
  - [section 4.3.2] "Amongst the pre-trained methods, OFA surprisingly shows significant promise being adapted from a language model trained on text data."
  - [corpus] Weak evidence - related work focuses on transformer architectures but lacks direct comparison of probabilistic heads on time series forecasting.
- Break Condition: When time series exhibit strong dependencies between dimensions or multimodal distributions that the independent Student-T assumption cannot capture effectively.

## Foundational Learning

- Concept: Positional encodings in Transformers
  - Why needed here: Transformers are permutation equivariant and require positional information to capture temporal dependencies in sequential data
  - Quick check question: What happens to Transformer performance if you remove all positional encodings from time series forecasting tasks?

- Concept: Masked language modeling pre-training
  - Why needed here: Enables learning robust representations by reconstructing missing values, which transfers to forecasting future values in time series
  - Quick check question: How does masked reconstruction differ from autoregressive prediction in terms of information flow during training?

- Concept: Parametric probability distributions for forecasting
  - Why needed here: Provides uncertainty quantification essential for operational decision-making in cloud environments
  - Quick check question: Why might a Student-T distribution be preferred over Gaussian for operational time series with potential anomalies?

## Architecture Onboarding

- Component map: Masked encoder Transformer -> Rotary positional embeddings -> Student-T head -> Instance normalization -> Feature extraction (log scale, lags, date/time)
- Critical path: Input time series -> Feature extraction -> Masked encoding -> Distribution parameter prediction -> Forecast output
- Design tradeoffs: Masked encoder offers computational efficiency vs encoder-decoder but requires careful attention masking; Student-T provides robustness but assumes independence
- Failure signatures: Degraded zero-shot performance indicates distribution shift; overfitting manifests as improving pre-training loss but worsening validation performance
- First 3 experiments:
  1. Baseline comparison: Train masked encoder vs encoder-decoder on pre-training data, evaluate zero-shot forecasting performance
  2. Positional encoding ablation: Compare RoPE vs sinusoidal vs learned embeddings with and without date/time features
  3. Probabilistic head comparison: Student-T vs quantile functions vs normalizing flows on validation set performance and calibration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do masked encoders compare to flatten encoder models in terms of computational efficiency and performance when input length is variable?
- Basis in paper: [inferred] The authors mention that masked encoders have lower parameter count and are more flexible with variable input length, but a direct comparison of computational costs and performance is challenging.
- Why unresolved: The paper only provides a qualitative comparison and highlights the pros and cons of each approach, but does not provide a detailed quantitative analysis of their computational efficiency and performance trade-offs.
- What evidence would resolve it: A comprehensive benchmark comparing the computational cost (e.g., FLOPs, memory usage) and performance (e.g., accuracy, speed) of masked encoders and flatten encoder models on datasets with variable input lengths.

### Open Question 2
- Question: How does the performance of pre-trained time series models scale with increasing dataset size and model size on smaller datasets?
- Basis in paper: [explicit] The authors observe a clear trend of improved performance with increasing model size and number of observations on the largest dataset (azure2017), but the relationship is more ambiguous on smaller datasets (borg2011 and ali2018).
- Why unresolved: The authors hypothesize that the models may be overfitting on the smaller datasets, but they do not provide a detailed analysis of the scaling behavior on these datasets.
- What evidence would resolve it: A fine-grained analysis of the validation performance on intermediate checkpoints during pre-training on the smaller datasets, along with an investigation of the optimal number of iterations and dataset size for each model size.

### Open Question 3
- Question: How does the zero-shot forecasting performance of pre-trained models compare to fine-tuned models on the CloudOps datasets?
- Basis in paper: [explicit] The authors find that pre-trained models outperform models trained from scratch and that further fine-tuning yields no benefits over zero-shot forecasts.
- Why unresolved: The authors provide a potential reason for this observation (i.e., the pre-training data is sufficiently diverse), but they do not explore this hypothesis further or investigate the conditions under which fine-tuning might be beneficial.
- What evidence would resolve it: A detailed analysis of the impact of fine-tuning on the zero-shot forecasting performance, including an investigation of the optimal fine-tuning strategy (e.g., learning rate, number of epochs) and the conditions under which fine-tuning is most effective.

## Limitations
- In-collection pre-training may inflate zero-shot transfer performance compared to cross-domain scenarios
- Datasets represent a specific operational domain that may not capture full time series diversity
- Focus on short forecasting horizons (24 hours) leaves long-term prediction questions open

## Confidence

- **High confidence**: Scaling relationships between model size, dataset size, and forecasting performance are well-supported by extensive experimental results across three large datasets.
- **Medium confidence**: Zero-shot transfer capabilities demonstrated in in-collection scenarios may not generalize to truly out-of-distribution transfers.
- **Low confidence**: Student-T heads' robustness across diverse time series patterns is primarily based on within-domain evaluation.

## Next Checks

1. **Cross-domain transfer evaluation**: Test zero-shot forecasting performance when pre-training on one CloudOps collection and evaluating on a completely different operational domain (e.g., financial or sensor data) to assess true generalization capabilities.

2. **Scaling law extrapolation**: Conduct experiments with even larger model sizes (beyond the 96M parameter model) and dataset sizes to validate whether current scaling trends continue or if saturation effects emerge.

3. **Long-horizon forecasting**: Extend evaluation to multi-day and multi-week forecasting horizons to determine if the observed performance benefits persist for longer-term predictions, which are critical for operational planning.