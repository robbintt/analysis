---
ver: rpa2
title: 'Explaining with Contrastive Phrasal Highlighting: A Case Study in Assisting
  Humans to Detect Translation Differences'
arxiv_id: '2312.01582'
source_url: https://arxiv.org/abs/2312.01582
tags:
- highlights
- association
- linguistics
- computational
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes contrastive phrasal highlighting to explain
  predictions of models that compare two texts. The method uses phrase alignment to
  erase phrase pairs from sentence pairs, selecting the pair whose erasure maximally
  increases similarity score.
---

# Explaining with Contrastive Phrasal Highlighting: A Case Study in Assisting Humans to Detect Translation Differences

## Quick Facts
- arXiv ID: 2312.01582
- Source URL: https://arxiv.org/abs/2312.01582
- Reference count: 27
- Key outcome: Contrastive phrasal highlighting improves human detection of translation differences with F1 +10% for fine-grained divergences and Recall +15% for negation errors

## Executive Summary
This paper introduces contrastive phrasal highlighting, a method that explains predictions of text comparison models by identifying minimal phrase pairs whose removal maximizes semantic divergence scores. The approach uses phrase alignment to extract contrastive phrasal pairs and applies an erasure strategy to select those that most effectively explain meaning differences. Evaluated on explaining semantic divergences in human translations, the method outperforms standard saliency techniques (LIME, SHAP) at matching human rationales and improves human annotation accuracy for detecting fine-grained meaning differences and critical translation errors.

## Method Summary
The method implements a phrase-alignment-guided erasure strategy to explain predictions of semantic divergence models. It extracts candidate phrasal pairs using statistical phrase alignment, then applies an erasure strategy that selects the pair whose removal maximally increases the divergence score while applying a brevity reward for minimal highlights. The approach is evaluated through proxy evaluation against human rationales and two human studies measuring annotation accuracy for fine-grained meaning differences and critical translation errors.

## Key Results
- Contrastive phrasal highlights match human rationales with F1 62% vs 39-41% for standard saliency methods
- Human annotation accuracy for fine-grained meaning differences improves by F1 +10%
- Recall for detecting negation errors increases by +15% when using contrastive highlights

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Phrase alignment guided erasure identifies contrastive phrasal pairs that maximize semantic divergence
- **Mechanism**: Uses statistical phrase alignment to extract candidate phrasal pairs, then applies erasure strategy selecting pair whose removal maximally increases divergence score with brevity reward
- **Core assumption**: Semantic divergence can be modeled as continuous score, and removing minimal contrastive phrases maximally changes this score
- **Evidence anchors**: [abstract] "We design a phrase-alignment-guided erasure strategy to explain the predictions of a semantic divergence model and highlight minimal phrase pairs in the two inputs that result in meaning differences."
- **Break condition**: If phrase alignment quality degrades in low-resource settings, extracted phrasal pairs may not capture true semantic divergences

### Mechanism 2
- **Claim**: Contrastive phrasal highlights match human rationales better than token-level saliency methods
- **Mechanism**: Explicitly models relationships between tokens across languages through phrase alignment, capturing interdependent semantic relationships that token-level methods miss
- **Core assumption**: Human explanations of semantic differences naturally focus on phrase pairs rather than individual tokens
- **Evidence anchors**: [abstract] "we show that the resulting highlights match human rationales of cross-lingual semantic differences better than popular post-hoc saliency techniques"
- **Break condition**: If human rationales actually focus on individual tokens in certain contexts, phrasal approach may not provide advantages

### Mechanism 3
- **Claim**: Contrastive phrasal highlights improve human annotation reliability for semantic divergences
- **Mechanism**: Providing bilingual speakers with highlights of contrastive phrase pairs reduces cognitive load and directs attention to likely divergence points
- **Core assumption**: Humans can more reliably detect semantic divergences when guided by contrastive phrasal highlights rather than reading entire sentence pairs
- **Evidence anchors**: [abstract] "Two human studies show it significantly improves bilingual speakers' ability to detect fine-grained meaning differences in translations (F1 +10%) and critical translation errors (Recall +15% for negation errors)."
- **Break condition**: If highlights become overly noisy or contain too many false positives, they may distract rather than help annotators

## Foundational Learning

- **Concept**: Phrase alignment and statistical machine translation
  - Why needed here: The approach builds on phrase alignment techniques from SMT to extract contrastive phrasal pairs. Understanding how word alignments are derived from multilingual embeddings and how phrase pairs are extracted is fundamental.
  - Quick check question: What makes a phrase pair "consistent with word alignments" in the context of phrase extraction?

- **Concept**: Contrastive explanations and counterfactual reasoning
  - Why needed here: The method is based on generating counterfactual instances by erasing phrase pairs and selecting those that maximally change the model prediction. Understanding the theory behind contrastive explanations is essential.
  - Quick check question: How does the brevity reward term BR(S,p) encourage minimal explanations in the selection process?

- **Concept**: Semantic divergence modeling
  - Why needed here: The explanandum is a divergence ranking model that scores semantic similarity between sentence pairs. Understanding how such models are trained and what they capture is crucial.
  - Quick check question: Why does the approach assume that removing contrastive phrases increases the similarity score?

## Architecture Onboarding

- **Component map**: Sentence pair → Phrase alignment extraction → Candidate counterfactual generation → Erasure and scoring → Brevity-weighted selection → Contrastive phrasal highlights
- **Critical path**: Sentence pair → Phrase alignment extraction → Candidate counterfactual generation → Erasure and scoring → Brevity-weighted selection → Contrastive phrasal highlights
- **Design tradeoffs**: 
  - Token-level vs. phrasal highlighting: phrasal captures semantic relationships but is computationally heavier
  - Single vs. multiple explanations: iterative extraction provides comprehensive coverage but may introduce redundancy
  - Brevity reward weighting: balances informativeness with cognitive load
- **Failure signatures**:
  - Poor phrase alignment quality → noisy or irrelevant phrasal highlights
  - Overly aggressive brevity reward → highlights miss important divergences
  - Model sensitivity to small edits → highlights may not reflect true semantic differences
- **First 3 experiments**:
  1. Validate phrase alignment quality on a held-out parallel corpus by comparing extracted phrasal pairs with human judgments
  2. Test sensitivity of contrastive highlighting to brevity reward weight by running ablation studies
  3. Evaluate on synthetic divergences with known ground truth to verify that highlights correctly identify inserted/deleted content

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of contrastive phrasal highlights vary across different language pairs and linguistic phenomena (e.g., syntactic vs. semantic divergences)?
- Basis in paper: [explicit] The paper mentions evaluating on English-French and English-Spanish for fine-grained semantic divergences, and English-Portuguese for critical error detection.
- Why unresolved: The study only covers a limited set of language pairs and specific types of divergences. It's unclear if the method generalizes to other language pairs or handles different linguistic phenomena equally well.
- What evidence would resolve it: Testing the method on a broader range of language pairs and linguistic phenomena, such as more distant language pairs or other types of errors (e.g., grammatical errors, word order issues), would provide insights into its generalizability.

### Open Question 2
- Question: How does the quality of word alignments impact the effectiveness of contrastive phrasal highlights?
- Basis in paper: [explicit] The method relies on word alignments to extract phrase pairs for erasure. The paper acknowledges that alignment quality may vary in low-resource settings.
- Why unresolved: The study focuses on high-resource language pairs where alignment quality is expected to be high. The impact of alignment errors on the method's performance is not explored.
- What evidence would resolve it: Evaluating the method on low-resource language pairs or with artificially degraded alignment quality would reveal how sensitive it is to alignment errors and inform potential improvements or limitations.

### Open Question 3
- Question: What is the optimal way to present and integrate contrastive phrasal highlights into human annotation workflows for different tasks and user groups?
- Basis in paper: [inferred] The study shows that highlights can improve annotation accuracy and agreement, but also notes that annotators don't rely solely on them and have concerns about false positives.
- Why unresolved: The study provides a basic presentation of highlights but doesn't explore different presentation formats (e.g., color-coding, tooltips) or integration strategies (e.g., as suggestions vs. definitive answers). The optimal approach may vary depending on the task complexity, user expertise, and desired level of human control.
- What evidence would resolve it: Conducting user studies with different presentation formats and integration strategies, and comparing their impact on annotation performance, user satisfaction, and trust in the system, would help identify the most effective approaches for different contexts.

## Limitations
- Dependence on high-quality phrase alignment that may not transfer well to low-resource language pairs or domains with significant structural differences
- Effectiveness relies heavily on divergence model's sensitivity to phrasal edits, which may vary across different model architectures
- Brevity reward mechanism may sometimes under-highlight important but longer divergences
- Human study sample size (50 sentence pairs) and bilingual annotator pool may not capture full spectrum of translation quality issues

## Confidence

- **High confidence**: The core mechanism of phrase-alignment-guided erasure and its theoretical foundation in contrastive explanation literature. The synthetic evaluation showing superior F1 scores against human rationales (62% vs 39-41%) provides strong empirical support.
- **Medium confidence**: The generalizability of results to languages beyond English-German, as the human studies were limited to these pairs. The assumption that phrasal highlights are universally more helpful than token-level explanations may not hold across all annotation tasks.
- **Medium confidence**: The claim that contrastive phrasal highlighting significantly improves human annotation accuracy, based on the human study results showing F1 +10% for fine-grained differences and +15% recall for negation errors.

## Next Checks
1. **Phrase alignment robustness test**: Evaluate the method on a diverse set of language pairs (e.g., English-Chinese, English-Arabic) with varying degrees of linguistic similarity to assess whether the phrase alignment quality degrades and how this affects highlight quality.

2. **Brevity reward ablation study**: Systematically vary the brevity reward weight parameter across a range of values and measure the tradeoff between highlight conciseness and annotation accuracy to identify the optimal balance point.

3. **Cross-domain generalization**: Apply the method to translation divergences in specialized domains (medical, legal, technical) where phrasal meanings may be more context-dependent and evaluate whether the highlights maintain their effectiveness compared to general web-crawled data.