---
ver: rpa2
title: Towards a fuller understanding of neurons with Clustered Compositional Explanations
arxiv_id: '2310.18443'
source_url: https://arxiv.org/abs/2310.18443
tags:
- cluster
- activation
- explanations
- activations
- coex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Clustered Compositional Explanations, a method
  to analyze neurons' behavior by clustering their activation ranges and applying
  compositional explanations to each cluster. Unlike existing methods that focus only
  on highest activations, the approach uses K-means clustering and a novel heuristic
  (MMESH) to efficiently compute explanations across multiple activation ranges.
---

# Towards a fuller understanding of neurons with Clustered Compositional Explanations

## Quick Facts
- arXiv ID: 2310.18443
- Source URL: https://arxiv.org/abs/2310.18443
- Reference count: 40
- Key outcome: Proposes Clustered Compositional Explanations method that uses K-means clustering and MMESH heuristic to analyze neurons across activation ranges, achieving superior performance over baselines with IoU 0.15±0.07 vs 0.08±0.03

## Executive Summary
This paper introduces Clustered Compositional Explanations, a novel method for analyzing neural network neurons by examining their behavior across different activation ranges rather than focusing solely on highest activations. The approach uses K-means clustering to group activation values and applies compositional explanations to each cluster independently using a novel MMESH heuristic that makes the computation feasible. When applied to ResNet18 on the ADE20k dataset, the method reveals new insights including unspecialized activations in lowest ranges and progressive specialization across clusters, while demonstrating superior performance metrics compared to baseline methods like NetDissect and CoEx.

## Method Summary
The method extends compositional explanations by clustering neuron activation ranges using K-means and applying a novel MMESH heuristic to efficiently compute explanations across multiple clusters. Rather than using a single high threshold for activation analysis, the approach partitions activation values into K clusters and independently searches for logical formulas in each cluster using beam search guided by the MMESH heuristic. This heuristic estimates intersection-over-union scores using bounding box coordinates and intersection sizes, reducing computational complexity from thousands of visited states to approximately 129 per unit while maintaining solution quality.

## Key Results
- Identified unspecialized activations in lowest activation clusters, often corresponding to unions of colors or general concepts
- Demonstrated progressive specialization phenomenon where higher activation clusters show more specific concept recognition
- Achieved superior performance metrics: IoU (0.15±0.07 vs 0.08±0.03), sample coverage (0.69±0.24 vs 0.53±0.25), activation coverage (0.37±0.17 vs 0.15±0.10)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustered Compositional Explanations discovers new insights by analyzing activation ranges beyond just the highest activations.
- Mechanism: The method clusters neuron activation ranges using K-means, then applies compositional explanations to each cluster independently, revealing behaviors at different activation levels.
- Core assumption: Neurons exhibit different compositional behaviors at different activation levels, and a single threshold (like top 0.005%) misses important patterns.
- Evidence anchors:
  - [abstract] "Unlike existing methods that focus only on highest activations, the approach uses K-means clustering and a novel heuristic (MMESH) to efficiently compute explanations across multiple activation ranges."
  - [section] "We argue that using an arbitrarily high value for the threshold gives us only a partial view of the concepts recognized by a neuron."

### Mechanism 2
- Claim: The MMESH heuristic makes the multi-cluster approach computationally feasible.
- Mechanism: MMESH estimates IoU scores using bounding box coordinates and intersection sizes, reducing the number of visited states from ~39,000 to ~129 per unit.
- Core assumption: Bounding box coordinates and partial intersection information can reliably estimate the IoU without computing the full formula search.
- Evidence anchors:
  - [section] "The vanilla implementation would require ncls × (n − 1) × b times the computation time of NetDissect. Even employing a parallelization of the computation over units, the base required time... make the application of the CoEx algorithm practically unfeasible"
  - [section] "Table 1 compares the average number of states visited during the computation of the baselines and our MMESH... reducing the number of visited states by two orders of magnitude."

### Mechanism 3
- Claim: The method reveals the phenomenon of "unspecialized activations" in the lowest activation ranges.
- Mechanism: By clustering activations and examining the lowest cluster (Cluster 1), the method finds that these activations often correspond to default labels like unions of colors or general concepts, rather than specialized concepts.
- Core assumption: Unspecialized activations represent the default behavior of neurons when they're not actively recognizing specific concepts.
- Evidence anchors:
  - [section] "By inspecting the labels, we observe that they are often a union of colors (i.e., Black OR Blue OR Grey) or a mix of colors and general concepts"
  - [section] "We found (Table 3) that Cluster 1 is almost always associated with unspecialized activations and Cluster 2 half of the time."

## Foundational Learning

- Concept: Compositional Explanations (CoEx) and Network Dissection
  - Why needed here: The paper builds directly on these existing methods, extending them from single-threshold analysis to multi-cluster analysis. Understanding how CoEx works (beam search over logical formulas) and how NetDissect works (concept alignment via IoU) is essential to grasp the contribution.
  - Quick check question: What's the fundamental difference between CoEx and NetDissect in terms of the explanations they return?

- Concept: Beam search and heuristics in combinatorial optimization
  - Why needed here: The paper uses beam search to explore the space of logical formulas, and introduces a new heuristic (MMESH) to make this search tractable across multiple clusters. Understanding how beam search works and why heuristics are needed is crucial.
  - Quick check question: Why can't we just use exhaustive search for the logical formulas in this multi-cluster setting?

- Concept: Clustering algorithms and their impact on downstream analysis
  - Why needed here: The method relies on K-means clustering to group activation ranges, and the choice of clustering algorithm and number of clusters significantly affects the results. Understanding clustering basics helps evaluate the approach's validity.
  - Quick check question: How might different clustering algorithms (like hierarchical vs K-means) affect the activation ranges and resulting explanations?

## Architecture Onboarding

- Component map:
  Data pipeline: Input images → ResNet18/DenseNet/AffineNet → activation maps → clustering → compositional explanation search

- Critical path:
  1. Extract activations for all neurons across dataset
  2. Apply K-means clustering to each neuron's activation values
  3. For each cluster, run MMESH-guided beam search to find best logical formula
  4. Compute evaluation metrics for each cluster's explanation
  5. Aggregate and compare results across clusters and baseline methods

- Design tradeoffs:
  - Number of clusters (5 vs 10 vs 15): More clusters provide finer granularity but increase computation and risk label repetition; fewer clusters miss subtle activation patterns
  - Clustering algorithm choice: K-means is fast but may not capture semantic groupings as well as hierarchical clustering
  - Beam search width (10→5): Wider beam explores more formulas but increases computation; narrower beam risks missing optimal formulas
  - Formula length limit (3): Longer formulas might capture more complex relationships but increase computation and risk overfitting

- Failure signatures:
  - All clusters return nearly identical labels → clustering isn't capturing meaningful activation differences
  - Extremely high variance in evaluation metrics across clusters → clustering is too fragmented or some clusters are too small
  - MMESH heuristic consistently underestimates IoU → beam search misses optimal formulas
  - CPU/memory usage spikes during clustering or beam search → need to optimize data structures or parallelize

- First 3 experiments:
  1. Run on a small subset (10 neurons, 100 images) with default settings to verify the pipeline works and produces reasonable clusters
  2. Compare different numbers of clusters (5, 10, 15) on the same neurons to observe how granularity affects explanations and metrics
  3. Test MMESH against vanilla CoEx on a single neuron to verify the heuristic reduces computation while maintaining explanation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of clustering algorithm affect the identification of unspecialized activations and progressive specialization phenomena?
- Basis in paper: [explicit] The paper states "The choice of K-Means is motivated by the need of a fast clustering algorithm that aggregates activations associated with a shared semantic and that can be applied to a large quantity of data" but acknowledges this as a limitation
- Why unresolved: The paper only uses K-Means clustering and does not compare it with other clustering algorithms or explore whether different algorithms might better capture the activation ranges
- What evidence would resolve it: Systematic comparison of different clustering algorithms (e.g., hierarchical clustering, DBSCAN) applied to the same datasets and architectures, measuring their impact on identifying unspecialized activations and progressive specialization

### Open Question 2
- Question: Are the observed phenomena of unspecialized activations and progressive specialization universal across different domains beyond image data?
- Basis in paper: [inferred] The paper notes "The extracted insights refer to the image data case. While the heuristic and the approach are domain agnostic, the application on a different domain could extract different kinds of insights"
- Why unresolved: The current analysis is limited to image data, and the paper acknowledges this as a limitation without exploring other domains
- What evidence would resolve it: Application of Clustered Compositional Explanations to other domains (e.g., text, audio, time series) and comparison of whether similar patterns of unspecialized activations and progressive specialization emerge

### Open Question 3
- Question: What is the optimal number of clusters that balances computational efficiency with capturing distinct compositional behaviors?
- Basis in paper: [explicit] The paper states "We hypothesize that these results can open the door to further research on a novel algorithm that reduces repeated labels over clusters and finds the optimal number of clusters"
- Why unresolved: The paper uses a fixed number of 5 clusters but acknowledges that increasing the number of clusters leads to repeated labels and marginal quality loss, without determining the optimal number
- What evidence would resolve it: Systematic evaluation of different cluster numbers across multiple architectures and datasets, measuring the trade-off between computational cost, label diversity, and the quality metrics proposed in the paper

## Limitations
- Computational complexity remains significant even with MMESH heuristic, limiting scalability to larger models
- Assumes semantically distinct behaviors can be captured by activation value ranges alone
- Focuses only on ResNet18 and ADE20k dataset, requiring validation across diverse architectures and tasks
- Heuristic's reliance on bounding box coordinates may introduce approximation errors that accumulate across clusters

## Confidence
- High confidence in the computational efficiency improvements from MMESH heuristic
- Medium confidence in the novelty of discovering unspecialized activations in lowest clusters
- Medium confidence in the overall effectiveness of multi-cluster analysis
- Low confidence in generalizability across different architectures and datasets

## Next Checks
1. Test the method on different model architectures (DenseNet, AlexNet) to verify architectural generalizability
2. Apply the approach to multiple datasets (ImageNet, COCO) to assess domain robustness
3. Conduct ablation studies comparing K-means clustering with alternative clustering methods to evaluate sensitivity to clustering choice