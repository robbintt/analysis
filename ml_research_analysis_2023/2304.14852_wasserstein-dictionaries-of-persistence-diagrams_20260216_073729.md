---
ver: rpa2
title: Wasserstein Dictionaries of Persistence Diagrams
arxiv_id: '2304.14852'
source_url: https://arxiv.org/abs/2304.14852
tags:
- diagrams
- persistence
- ensemble
- wasserstein
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to represent an ensemble of persistence
  diagrams using a linear encoding based on weighted Wasserstein barycenters of a
  dictionary of optimized diagrams. The method uses a multi-scale gradient descent
  approach that alternates between optimizing the barycentric weights and the atom
  diagrams, leveraging analytic gradients and shared-memory parallelism for efficiency.
---

# Wasserstein Dictionaries of Persistence Diagrams

## Quick Facts
- arXiv ID: 2304.14852
- Source URL: https://arxiv.org/abs/2304.14852
- Authors: 
- Reference count: 40
- This paper presents a method to represent an ensemble of persistence diagrams using a linear encoding based on weighted Wasserstein barycenters of a dictionary of optimized diagrams.

## Executive Summary
This paper introduces a novel approach for encoding ensembles of persistence diagrams using Wasserstein barycenters and a dictionary of optimized diagrams. The method employs a multi-scale gradient descent algorithm that alternately optimizes barycentric weights and atom diagrams, leveraging analytic gradients and shared-memory parallelism for efficiency. Experiments on public datasets demonstrate that the approach can compress diagrams with high fidelity while enabling effective dimensionality reduction for visualization purposes.

## Method Summary
The method represents persistence diagrams as weighted combinations of optimized atom diagrams through Wasserstein barycenters. A multi-scale gradient descent algorithm interleaves the optimization of barycentric weights and atom diagrams using analytic gradients. The approach exploits shared-memory parallelism for efficient computation of barycentric approximations and gradient evaluations. The multi-scale strategy progressively increases persistence resolution during optimization to improve solution quality.

## Key Results
- Wasserstein dictionaries can be computed in minutes for large examples
- Achieves compression factors up to 20.98 with relative reconstruction errors as low as 0.04
- Provides a trade-off between metric preservation and cluster separation compared to MDS and t-SNE for dimensionality reduction

## Why This Works (Mechanism)

### Mechanism 1
The gradient descent interleaving of barycentric weights and atom diagrams leads to a convex optimization problem. The optimization problem is formulated as a joint optimization of two sub-problems: (1) optimizing the barycentric weights for a fixed dictionary, and (2) optimizing the dictionary atoms for fixed barycentric weights. This interleaving is based on the convexity of the weight energy and atom energy functions. The core assumption is that optimal matchings between input diagrams and their barycentric approximation remain constant during optimization.

### Mechanism 2
The multi-scale strategy improves optimization solution quality by prioritizing the most persistent pairs. The strategy iteratively optimizes the dictionary at progressively finer persistence resolutions, starting with a low persistence threshold and gradually increasing resolution. The core assumption is that the most persistent pairs correspond to the most salient features of the data. This approach can converge to local minima that don't encode classes well if the most persistent pairs aren't the most discriminative for class separation.

### Mechanism 3
Shared-memory parallelism improves time performance by enabling independent computation of barycentric approximations for each input diagram. The algorithm also parallelizes gradient evaluations for weight energy and atom energy. The core assumption is that computations for each input diagram and each point in the barycentric approximation are independent. Parallelism may not improve performance if computations aren't truly independent or if there's contention for shared resources.

## Foundational Learning

- **Wasserstein distance between persistence diagrams**: The metric used to compare persistence diagrams in the Wasserstein dictionary encoding problem. Quick check: What is the definition of the L2-Wasserstein distance between two persistence diagrams X1 and X2?

- **Wasserstein barycenter of persistence diagrams**: The central object representing a linear combination of dictionary atoms that approximates an input diagram. Quick check: What is the definition of the Wasserstein barycenter of a set of persistence diagrams D with barycentric weights λλλ?

- **Gradient descent optimization**: The algorithm used to optimize barycentric weights and dictionary atoms. Quick check: What is the update rule for a gradient descent step in the optimization of the barycentric weights?

## Architecture Onboarding

- **Component map**: Input diagrams -> Multi-scale gradient descent -> Optimized dictionary and barycentric weights -> Compressed representation and/or 2D embedding

- **Critical path**: Compute initial dictionary and weights (k-means++-like initialization) -> Iterate until convergence: (1) Compute barycentric approximation for each input diagram, (2) Update barycentric weights using gradient descent, (3) Update dictionary atoms using gradient descent) -> Return final dictionary and weights

- **Design tradeoffs**: Number of atoms (m) vs compression factor and reconstruction error; Persistence resolution vs optimization quality and time performance; Parallelism vs synchronization overhead

- **Failure signatures**: Energy not decreasing or oscillating during optimization; Poor reconstruction error or cluster separation in applications; Slow convergence or high time performance

- **First 3 experiments**: 1) Verify gradient computations for weight energy and atom energy functions, 2) Test multi-scale strategy on small synthetic ensemble and visualize energy evolution, 3) Benchmark time performance and parallelization speedup on medium-sized ensemble

## Open Questions the Paper Calls Out

- What is the theoretical convergence rate of the multi-scale gradient descent algorithm for optimizing the Wasserstein dictionary? The paper describes the approach but doesn't provide theoretical analysis of convergence rate.

- How does the choice of the number of atoms (m) in the Wasserstein dictionary affect the quality of data reduction and dimensionality reduction? The paper discusses setting m based on ground-truth classes but lacks systematic impact studies.

- Can the Wasserstein dictionary approach be extended to handle persistence diagrams with additional features, such as persistence landscapes or silhouettes? The paper focuses on persistence diagrams but doesn't explore extensions to other topological descriptors.

## Limitations

- The non-convexity of the dictionary energy may lead to convergence to local minima depending on initialization.
- The method's performance for very large ensembles beyond tested examples remains uncertain.
- The multi-scale strategy assumes more persistent pairs are more informative, which may not hold for all data types.

## Confidence

- **High Confidence**: The gradient descent framework with analytic gradients and shared-memory parallelism is well-specified and reproducible.
- **Medium Confidence**: The effectiveness of the multi-scale strategy and k-means++-like initialization is supported by experimental results but lacks extensive ablation studies.
- **Low Confidence**: The generalizability of the method to extremely large ensembles and non-standard persistence diagram distributions is not well-established.

## Next Checks

1. **Energy Evolution Analysis**: Implement logging of the energy ED during optimization to verify monotonic decrease and identify potential convergence issues.

2. **Initialization Sensitivity**: Test the algorithm with multiple random initializations to assess the stability of the final dictionary and weights.

3. **Dataset Scalability**: Apply the method to significantly larger ensembles than those tested in the paper to evaluate computational efficiency and encoding quality at scale.