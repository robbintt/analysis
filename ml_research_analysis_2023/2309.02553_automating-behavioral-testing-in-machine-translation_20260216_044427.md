---
ver: rpa2
title: Automating Behavioral Testing in Machine Translation
arxiv_id: '2309.02553'
source_url: https://arxiv.org/abs/2309.02553
tags:
- translation
- property
- nllb
- candidate
- pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for automated behavioral
  testing of machine translation systems using Large Language Models (LLMs). The authors
  leverage LLMs to generate diverse source sentences containing specific linguistic
  properties and to create candidate sets of valid translations.
---

# Automating Behavioral Testing in Machine Translation

## Quick Facts
- arXiv ID: 2309.02553
- Source URL: https://arxiv.org/abs/2309.02553
- Authors: 
- Reference count: 18
- Key outcome: This paper introduces a novel framework for automated behavioral testing of machine translation systems using Large Language Models (LLMs).

## Executive Summary
This paper presents an automated framework for behavioral testing of machine translation systems using Large Language Models (LLMs). The approach leverages LLMs to generate diverse source sentences containing specific linguistic properties and to create candidate sets of valid translations, which are then used to evaluate MT models' performance on targeted capabilities. Experiments across three language pairs (English→German, English→Spanish, English→Japanese) with multiple open-source and commercial MT models reveal that while overall pass rates align with traditional accuracy metrics, the proposed method uncovers important differences and potential bugs in MT systems that are often overlooked by standard evaluation approaches.

## Method Summary
The framework uses LLMs to automatically generate source sentences containing tagged property values and candidate translation sets (either exhaustive or contrastive) for behavioral testing. For each property type (integers, decimals, physical units, emojis, names, currencies, web terms, idioms), the LLM generates 1,000 diverse source sentences with exactly one tagged property value. The same LLM then generates candidate translations for each property value, which are used to evaluate MT outputs through exact string matching or semantic similarity measures. Macro pass rates are computed by first averaging within each property value, then across property values to account for long-tail distributions.

## Key Results
- Overall pass rates align with traditional accuracy metrics while revealing additional insights about MT system behavior
- The framework successfully uncovers important differences and potential bugs in MT systems that standard evaluation approaches miss
- Macro pass rates computed across property values provide more reliable evaluation than simple averages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automated generation of diverse source sentences via LLM reduces human effort in creating behavioral tests.
- Mechanism: LLMs generate sentences containing specific linguistic properties using a general template prompt, leveraging multilingual capabilities and in-context learning to produce natural, diverse examples without manual crafting.
- Core assumption: LLMs can generate diverse, natural sentences that include exactly one tagged property value when prompted appropriately.
- Evidence anchors:
  - [abstract] "We use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations."
  - [section 3] "We design a general template for prompting LLMs, in our case ChatGPT, OpenAI’s model built on InstructGPT. This allows us to generate diverse source language sentences that contain property values suitable for testing different capabilities."
- Break condition: If LLM-generated sentences become repetitive or fail to contain the required property value, or if the generated sentences are unnatural and do not reflect real-world usage patterns.

### Mechanism 2
- Claim: Automated candidate set generation via LLM enables efficient verification of translation correctness.
- Mechanism: LLMs generate either exhaustive candidate sets for simple properties or contrastive candidate pairs for complex properties, allowing the framework to check whether MT outputs contain valid translations without manually defining all possible correct answers.
- Core assumption: LLMs can generate comprehensive candidate sets that include all valid translations or meaningful contrastive pairs when given appropriate demonstrations.
- Evidence anchors:
  - [abstract] "We propose using Large Language Models to generate candidate sets of ground-truth translations of the property values in cases where exhaustive candidate sets are plausible."
  - [section 4.1] "we propose using the in-context learning and multilingual capabilities of instruction-tuned LLMs to accomplish the task. For each property value xv, we generate a set of translation candidates Cxv with ChatGPT."
- Break condition: If generated candidate sets are incomplete or contain incorrect translations, leading to false negatives or false positives in evaluation.

### Mechanism 3
- Claim: Macro pass rates computed across property values provide more reliable evaluation than simple averages.
- Mechanism: Instead of averaging across all sentences, the framework computes pass rates by first averaging within each property value, then averaging across property values, accounting for the long-tail distribution of property values.
- Core assumption: Property values follow a long-tail distribution where some values appear frequently while others appear rarely, making simple averaging misleading.
- Evidence anchors:
  - [section 6.1] "Certain values appear relatively frequently, while many other values appear only once across the generated test set. This can make pass rates overly sensitive to whether models happen to perform well for these particular values."
  - [section 6.1] "we assume a generative story in which property values are drawn from a uniform distribution, and consequently compute the expected pass rate as the macro average across property values."
- Break condition: If the assumption about property value distribution is incorrect, or if property values are not the appropriate unit of analysis for the evaluation task.

## Foundational Learning

- Concept: In-context learning and few-shot prompting
  - Why needed here: The framework relies on LLMs to generate both source sentences and candidate translations without fine-tuning, using only prompt demonstrations.
  - Quick check question: How many demonstrations should be provided in the prompt to achieve good performance without overwhelming the model's context window?

- Concept: Behavioral testing methodology
  - Why needed here: The framework is built on the principle of evaluating system behavior through controlled input-output pairs rather than aggregate metrics.
  - Quick check question: What distinguishes behavioral testing from traditional accuracy-based evaluation in NLP?

- Concept: Statistical significance testing
  - Why needed here: The framework uses paired bootstrap and confidence intervals to compare models and assess the reliability of results.
  - Quick check question: Why are confidence intervals more informative than point estimates when reporting pass rates?

## Architecture Onboarding

- Component map: LLM Source Generator -> LLM Candidate Generator -> MT Model -> Pass-Fail Detector -> Evaluation Metrics
- Critical path: Source sentence generation -> Candidate generation -> Translation generation -> Pass-fail detection -> Metric computation
- Design tradeoffs: Exhaustive candidate sets provide precise evaluation but are only feasible for simple properties; contrastive pairs work for complex properties but require semantic similarity measures.
- Failure signatures: Low diversity in generated sentences, incomplete candidate sets, high false positive/negative rates in pass-fail detection, unstable confidence intervals.
- First 3 experiments:
  1. Generate 100 source sentences for integer translation and manually verify diversity and property tagging accuracy.
  2. Generate candidate sets for a simple property (like decimals) and check for completeness and correctness.
  3. Run a small-scale evaluation comparing two MT models on a single property to verify the end-to-end pipeline works.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do false negatives in candidate set generation affect the relative ranking of MT models in behavioral testing?
- Basis in paper: [explicit] The authors note that false negatives in candidate sets lead to underestimated pass rates but argue that relative ordering between models can still be reasonably approximated without human intervention.
- Why unresolved: The paper provides qualitative evidence through examples but does not conduct systematic experiments to quantify how FN rates impact model comparisons across different properties and languages.
- What evidence would resolve it: A controlled study comparing model rankings with and without candidate set corrections across multiple test properties and language pairs would clarify the impact of FNs on relative model evaluation.

### Open Question 2
- Question: Can the proposed framework be extended to test properties that are not continuous text chunks?
- Basis in paper: [inferred] The authors explicitly state this as a limitation, noting the framework only works for properties appearing as continuous text chunks.
- Why unresolved: The paper focuses on continuous text properties like numbers and idioms but does not explore how to handle scattered or discontinuous properties.
- What evidence would resolve it: Successful implementation and evaluation of the framework on scattered properties (e.g., agreement phenomena, non-contiguous idioms) would demonstrate extensibility beyond the current limitations.

### Open Question 3
- Question: How does the diversity of generated source sentences impact the reliability of behavioral test results?
- Basis in paper: [explicit] The authors measure n-gram diversity in generated sentences and find that diversity remains high even after 500 sentences, but do not investigate how this affects test reliability.
- Why unresolved: While the paper shows that sentences remain diverse, it does not examine whether this diversity translates to more reliable or comprehensive test coverage.
- What evidence would resolve it: Correlation analysis between sentence diversity metrics and test result stability across multiple generation rounds would reveal the relationship between diversity and reliability.

## Limitations

- The framework's effectiveness is heavily dependent on the quality of LLM-generated candidate sets, which may be incomplete or contain errors.
- Results may not generalize across different LLM models, as the paper only uses ChatGPT (gpt-3.5-turbo) specifically.
- The framework only works for properties appearing as continuous text chunks in source and target languages, limiting its applicability.

## Confidence

- High Confidence: The core methodology of using LLMs for automated behavioral test generation is sound and well-supported.
- Medium Confidence: Claims about uncovering "important differences and potential bugs" are supported by specific examples but would benefit from more extensive validation.
- Low Confidence: The framework's effectiveness for properties requiring contrastive candidate pairs is less established due to the complexity of semantic similarity measures.

## Next Checks

1. Implement a systematic evaluation of candidate set completeness across all property types by sampling 500 additional items per property and comparing generated candidates against human-annotated gold standards to measure recall rates.

2. Replicate the framework using different LLM models (e.g., Claude, Llama, or open-source alternatives) to assess whether results are consistent or model-dependent, particularly for properties with contrastive candidate sets.

3. Conduct a detailed error analysis by manually annotating 200 randomly sampled pass/fail cases from the full evaluation to quantify false positive and false negative rates, then refine the semantic similarity thresholds and candidate generation prompts accordingly.