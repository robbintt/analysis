---
ver: rpa2
title: 'Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs'
arxiv_id: '2309.15096'
source_url: https://arxiv.org/abs/2309.15096
tags:
- relu
- kernel
- network
- convex
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges convex reformulations of finite-width neural
  networks with infinite-width NTK theory. It shows that the convex program for a
  gated ReLU network is equivalent to Multiple Kernel Learning (MKL) with masking
  kernels, and that the NTK corresponds to a specific fixed weighting of these kernels
  that ignores target labels.
---

# Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs

## Quick Facts
- arXiv ID: 2309.15096
- Source URL: https://arxiv.org/abs/2309.15096
- Reference count: 40
- Key outcome: Shows NTK corresponds to fixed weighting of masking kernels; IRLS learns optimal kernel weights achieving 26/33 better test accuracies than NTK on UCI datasets

## Executive Summary
This paper establishes a fundamental connection between convex reformulations of finite-width neural networks and infinite-width NTK theory. The authors show that the convex program for gated ReLU networks is equivalent to Multiple Kernel Learning with masking kernels, where the NTK corresponds to a specific fixed weighting of these kernels. By using iterative reweighting (IRLS), they learn optimal data-dependent kernel weights that outperform the NTK on training data, achieving higher accuracy than NTK in 26 out of 33 UCI dataset experiments.

## Method Summary
The method reformulates the gated ReLU network optimization as a group lasso problem with masking kernel feature maps. IRLS is applied to solve this convex problem, initialized with NTK weights, to learn optimal kernel weights within the masking kernel family. The approach involves constructing masking kernels from hyperplane arrangements of the data, solving the group lasso via IRLS, and optionally converting the solution back to a ReLU network using cone decomposition. Experiments validate the method on 1D and UCI datasets, comparing training and test accuracy against NTK baselines.

## Key Results
- The NTK of a gated ReLU network equals a specific weighted combination of masking kernels with weights determined by data distribution, not target labels
- IRLS initialized with NTK weights converges to optimal kernel weights, improving training accuracy over NTK
- On 33 UCI datasets, IRLS-learned kernels achieved higher test accuracy than NTK in 26 cases
- Theoretical prediction error bounds are provided for the gated ReLU network convex reformulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The NTK can be interpreted as a specific weighted combination of masking kernels that ignores target labels.
- Mechanism: The NTK corresponds to a convex combination of masking kernels where the weights are determined solely by the input data distribution (standard normal), not by the target labels. This is shown in Theorem 5.1 where KG(η̃) = H, with η̃i = P[diag(1{Xh≥0})=Di] where h ~ N(0,Id).
- Core assumption: The infinite-width NTK of a gated ReLU network equals the expectation of masking kernels under standard normal gate distribution.
- Evidence anchors:
  - [abstract] "the NTK corresponds to a specific fixed weighting of these kernels that ignores target labels"
  - [section] "Theorem 5.1. Let KG(η̃) ∈ Rn×n be the weighted masking kernel... Then, KG(η̃) = H"
  - [corpus] Weak - no direct corpus support for this specific NTK-masking kernel connection
- Break condition: If the gate distribution changes from standard normal, or if the NTK is evaluated on finite-width networks instead of infinite-width.

### Mechanism 2
- Claim: IRLS can improve the NTK weights to learn the optimal data-dependent kernel within the masking kernel family.
- Mechanism: By iteratively reweighting the least squares problem (16) using IRLS, the algorithm converges to the optimal weights that minimize the group lasso objective, which corresponds to the optimal kernel for the given data and targets.
- Core assumption: The group lasso problem with masking feature matrices is convex and can be solved efficiently via IRLS.
- Evidence anchors:
  - [abstract] "By using iterative reweighting (IRLS), the authors learn optimal data-dependent kernel weights that outperform the NTK on training data"
  - [section] "By applying IRLS initialized with the NTK weights, we fix the NTK and find the weights of the optimal MKL kernel"
  - [corpus] Weak - no corpus papers discussing IRLS for fixing NTK specifically
- Break condition: If the IRLS algorithm gets stuck in local minima (though convexity suggests this shouldn't happen), or if the masking kernel family doesn't contain the optimal kernel.

### Mechanism 3
- Claim: The convex reformulation of gated ReLU networks is equivalent to Multiple Kernel Learning with masking kernels.
- Mechanism: The group lasso problem (8) can be rewritten as an MKL problem where the feature maps are masking kernels generated by the gate set G. This equivalence allows kernel-based analysis of finite-width networks.
- Core assumption: The gate set G is complete/minimally complete so that all possible data maskings are covered.
- Evidence anchors:
  - [abstract] "the convex program for a gated ReLU network is equivalent to Multiple Kernel Learning (MKL) with masking kernels"
  - [section] "Theorem 4.3. The non-convex gated ReLU problem (7) with a minimally complete gate set G is equivalent to performing multiple kernel learning (12) with the masking feature maps generated by G"
  - [corpus] Weak - no direct corpus support for this specific convex-MKL equivalence
- Break condition: If the gate set is not complete, or if the network architecture changes beyond two layers.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: The paper builds its entire argument around NTK being suboptimal and needing "fixing" through kernel reweighting
  - Quick check question: What is the key limitation of NTK that motivates this work?

- Concept: Multiple Kernel Learning (MKL)
  - Why needed here: The paper interprets the convex program as an MKL problem to show that NTK is just one specific weighting within a family of possible kernels
  - Quick check question: How does MKL differ from standard kernel methods in terms of learning the kernel?

- Concept: Group Lasso
  - Why needed here: The convex reformulation uses group lasso regularization, which the paper shows is equivalent to MKL
  - Quick check question: What is the relationship between group lasso and squared group ℓ1-norm?

## Architecture Onboarding

- Component map: Data matrix X, targets y -> Masking kernel construction -> Group lasso formulation -> IRLS optimization -> Optimal kernel weights and predictions
- Critical path: Data → Masking kernel construction → Group lasso formulation → IRLS optimization → Optimal kernel → Predictions
- Design tradeoffs:
  - Exact convex program (3) vs relaxed gated ReLU (8): exponential constraints vs kernel family limitation
  - Full gate set vs sampled gate set: completeness guarantee vs computational efficiency
  - NTK weights vs learned weights: theoretical simplicity vs empirical performance
- Failure signatures:
  - Suboptimal training accuracy: suggests kernel family is too restrictive
  - Slow IRLS convergence: suggests poor initialization or ill-conditioned problem
  - High test error despite good training fit: suggests overfitting to masking kernel family
- First 3 experiments:
  1. Verify NTK vs optimal MKL on simple 1D dataset with all hyperplane arrangements enumerated
  2. Test IRLS convergence from NTK initialization on synthetic data with known optimal kernel
  3. Compare test accuracy on UCI datasets between NTK, MKL-optimal, and standard neural network baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the iterative reweighting approach be extended to achieve generalization guarantees on test data comparable to those for kernel methods?
- Basis in paper: [explicit] The authors note that while their theory links NTK optimization properties to finite-width networks via MKL, they do not derive explicit generalization results on the test set. They suggest applying existing generalization theory for kernel methods [44, 45] to the MKL interpretation of the convex reformulation as a promising future direction.
- Why unresolved: The paper provides prediction error bounds for the gated ReLU network on training data but does not extend these to test data or compare them to generalization bounds for kernel methods.
- What evidence would resolve it: Deriving explicit generalization error bounds for the IRLS-improved MKL kernel on test data and comparing them to existing bounds for kernel methods like NTK.

### Open Question 2
- Question: How does the choice of the gate set G in the gated ReLU network affect the quality of the learned kernel and final model performance?
- Basis in paper: [explicit] The paper shows that the NTK corresponds to a specific weighting of masking kernels generated by a complete gate set G. It also discusses that the convex reformulation can be solved with a randomly sampled subset of hyperplane arrangements as an approximation. However, it does not systematically study the impact of different gate set choices on performance.
- Why unresolved: The theoretical results hold for minimally complete gate sets, but the practical impact of gate set choice (e.g., random vs. learned vs. structured) on the learned kernel and model accuracy is not explored.
- What evidence would resolve it: Empirical studies comparing model performance using different gate set construction strategies (e.g., random sampling, greedy selection, learned gates) on various datasets.

### Open Question 3
- Question: Can the convex reformulation and MKL interpretation be extended to other activation functions beyond ReLU, and what would be the corresponding kernel families?
- Basis in paper: [inferred] The paper focuses on ReLU networks but mentions that the approach can be directly extended to various neural network architectures like threshold/binary networks [46]. The convex reformulation and MKL framework rely on the cone structure induced by the activation, suggesting potential applicability to other piecewise linear activations.
- Why unresolved: While the paper hints at extensibility to other architectures, it does not provide theoretical results or experiments for activation functions beyond ReLU (e.g., leaky ReLU, absolute value, or learnable activations).
- What evidence would resolve it: Developing the convex reformulation for other activation functions, characterizing the corresponding kernel families, and demonstrating improved performance over standard kernels on benchmark datasets.

## Limitations
- The masking kernel family may not contain the globally optimal kernel for all datasets
- Equivalence between convex reformulations and MKL relies on complete gate sets, which become computationally intractable for larger datasets
- Theoretical prediction error bounds assume the optimal kernel is within the masking family, not empirically validated across diverse datasets

## Confidence
- High confidence: Mathematical equivalence between gated ReLU convex programs and MKL with masking kernels (Theorem 4.3) is rigorously proven
- Medium confidence: IRLS convergence to optimal kernel weights is supported by experiments but lacks theoretical convergence guarantees for specific kernel weights
- Low confidence: Broader claim that this approach "fixes" NTK for practical deep learning applications is overstated, limited to specific architectures

## Next Checks
1. **Kernel family completeness test**: Systematically evaluate whether the masking kernel family contains the optimal kernel for diverse UCI datasets by comparing MKL-optimal performance against kernel ridge regression with Gaussian kernels tuned via cross-validation
2. **Generalization bound validation**: Empirically verify the prediction error bounds by computing the ratio of training to generalization error across multiple dataset splits, testing whether the theoretical relationship holds in practice
3. **Architecture scalability study**: Test the approach on deeper networks (3+ layers) and different activation functions (ReLU, tanh, ELU) to determine if the convex-MKL equivalence extends beyond the two-layer gated ReLU setting