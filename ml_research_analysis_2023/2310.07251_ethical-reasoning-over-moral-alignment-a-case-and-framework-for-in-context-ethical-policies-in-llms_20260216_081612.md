---
ver: rpa2
title: 'Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context
  Ethical Policies in LLMs'
arxiv_id: '2310.07251'
source_url: https://arxiv.org/abs/2310.07251
tags:
- ethical
- moral
- policies
- values
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that Large Language Models (LLMs) should not
  be aligned to specific moral values due to value pluralism and cultural diversity.
  Instead, LLMs should have generic ethical reasoning capabilities and support "in-context"
  ethical policies.
---

# Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs

## Quick Facts
- arXiv ID: 2310.07251
- Source URL: https://arxiv.org/abs/2310.07251
- Authors: 
- Reference count: 14
- Key outcome: Proposes framework for ethical reasoning in LLMs using in-context policies rather than moral alignment, demonstrating GPT-4's near-perfect ethical reasoning (93.3% accuracy) across three abstraction levels.

## Executive Summary
This paper argues against aligning Large Language Models to specific moral values due to value pluralism and cultural diversity, instead proposing a framework for in-context ethical policies that enable generic ethical reasoning capabilities. The authors introduce a three-level abstraction framework (Level 0: specific values, Level 1: abstract values, Level 2: fundamental principles) and evaluate five GPT models across moral dilemmas from different ethical frameworks. Results show GPT-4 achieves near-perfect ethical reasoning while smaller models exhibit biases toward Western and individualistic values, demonstrating that ethical reasoning ability scales with model size and that Level 1 policies provide optimal balance between abstraction and practical applicability.

## Method Summary
The study evaluates five GPT models (GPT-3.5-turbo, GPT-4, GPT-3/davinci, text-davinci-002, text-davinci-003) on their ability to perform ethical reasoning using four moral dilemmas (Heinz, Monica, Rajesh, Timmy) and 18 ethical policies per dilemma at three abstraction levels for three ethical frameworks. Models were tested with six permutations of answer options to mitigate ordering bias, using temperature=0, top probabilities=0.95, frequency penalty=0, and presence penalty=1. Accuracy was measured against ground truth resolutions, with bias and confusion scores computed to assess model behavior under different policies.

## Key Results
- GPT-4 demonstrates near-perfect ethical reasoning with 93.3% accuracy across all policy levels
- Smaller models show significant degradation at Level 2 (fundamental principles) compared to Levels 0 and 1
- Models exhibit biases toward Western and individualistic values when operating without explicit ethical policies
- Level 1 policies (abstract values) provide optimal balance between abstraction and practical applicability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's superior ethical reasoning stems from its ability to perform multi-level abstraction policy reasoning.
- Mechanism: The framework defines ethical policies at three levels: Level 0 (specific values), Level 1 (abstract values), Level 2 (fundamental principles). GPT-4 demonstrates near-perfect performance across all levels, while smaller models degrade significantly at Level 2.
- Core assumption: Ethical reasoning capability scales with model size and can be measured through structured policy evaluation.
- Evidence anchors:
  - [abstract] "Results show GPT-4 has near-perfect ethical reasoning (93.3% accuracy), while other models show biases towards Western and individualistic values."
  - [section 4.2] "GPT-4 displays near perfect ethical reasoning ability under all policies, with an average accuracy of 93.29% compared to 70% accuracy of our best human annotator"
  - [corpus] Weak - only 25 related papers found, average neighbor FMR=0.482 suggests moderate relevance
- Break condition: When policies require cultural knowledge not present in training data, or when multiple simultaneous policies create conflicts beyond the model's reasoning capacity.

### Mechanism 2
- Claim: In-context ethical policies enable value-neutral LLMs to perform ethical reasoning without value alignment.
- Mechanism: By providing explicit ethical policies as part of the prompt, the model can reason consistently with those policies rather than relying on internalized values. This decouples ethical decision-making from model training.
- Core assumption: LLMs can perform logical and ethical reasoning when given clear policy constraints, even if they lack inherent moral alignment.
- Evidence anchors:
  - [section 3.3] "We expect the LLM to state that a concrete resolution is not possible in the given situation" - showing models can recognize policy limitations
  - [section 3.4] "Level 1 policies require both linguistic and logical as well as ethical reasoning and can provide an optimal abstraction level for an ethical policy"
  - [corpus] Weak - no direct corpus evidence supporting this mechanism
- Break condition: When policies are underspecified or contradictory, or when the model's reasoning ability cannot bridge the gap between policy and practical resolution.

### Mechanism 3
- Claim: Ethical reasoning ability correlates with model size and training methodology (RLHF vs supervised fine-tuning).
- Mechanism: Larger models like GPT-4 show near-perfect ethical reasoning, while smaller models and those with aggressive alignment through RLHF show biases and reduced reasoning ability.
- Core assumption: Model capacity and training approach directly impact the ability to perform nuanced ethical reasoning.
- Evidence anchors:
  - [section 4.2] "Despite being an optimized version of text-davinci-003 with additional RLHF training, ChatGPT also exhibited a notable internal bias"
  - [section 4.2] "GPT-4 has close to 50% accuracy, which is also the random baseline since almost in all cases the models choose from two options - y and ¬y"
  - [corpus] Weak - only indirect evidence from related papers about LLM capabilities
- Break condition: When model size reaches diminishing returns or when training methodology introduces biases that override reasoning capabilities.

## Foundational Learning

- Concept: Normative ethics (deontology, virtue ethics, consequentialism)
  - Why needed here: The framework is theory-neutral and supports policies from any ethical formalism. Understanding these approaches is essential for creating and evaluating ethical policies.
  - Quick check question: What are the three main branches of normative ethics and how do they differ in their approach to ethical decision-making?

- Concept: Value pluralism and cultural diversity in ethics
  - Why needed here: The paper argues against universal moral alignment due to value pluralism. Understanding that different cultures weigh values differently is crucial for the framework's design.
  - Quick check question: How does value pluralism challenge the idea of universal moral principles, and what evidence does the paper provide for cultural differences in ethical reasoning?

- Concept: In-context learning and prompt engineering
  - Why needed here: The framework relies on providing ethical policies through prompts rather than model training. Understanding how to effectively structure prompts is essential for implementation.
  - Quick check question: How does the prompt structure in the paper balance task definition, ethical policy, and user input to enable ethical reasoning?

## Architecture Onboarding

- Component map:
  - Input layer: Task definition (τ), ethical policy (π), user input (x)
  - Processing layer: LLM ethical reasoning engine
  - Output layer: Ethically consistent response or "can't decide" indicator (ϕ)
  - Policy management: Framework for creating and managing ethical policies at different abstraction levels

- Critical path:
  1. Receive task definition, ethical policy, and user input
  2. LLM processes input with ethical policy constraints
  3. Generate response or indicate impossibility
  4. Return ethically consistent output

- Design tradeoffs:
  - Policy abstraction vs. practical applicability (Level 0 vs Level 1 vs Level 2)
  - Model size vs. ethical reasoning capability
  - Cultural specificity vs. generalizability of policies
  - Prompt complexity vs. model performance

- Failure signatures:
  - Model consistently returns "can't decide" (ϕ) - policy may be underspecified
  - Model ignores policy and returns baseline response - internal bias present
  - Model performs poorly on certain cultural contexts - cultural bias in training data
  - Performance degrades at higher abstraction levels - insufficient reasoning capacity

- First 3 experiments:
  1. Test baseline performance without ethical policies to identify inherent biases
  2. Evaluate model performance across different policy abstraction levels (0, 1, 2)
  3. Compare performance across different ethical formalisms (virtue, consequentialist, deontological)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ethical reasoning capabilities be effectively infused into LLMs to encompass diverse moral principles and cultural values across languages?
- Basis in paper: [explicit] The authors mention that cross-lingual transfer of ethical reasoning abilities is an area of investigation.
- Why unresolved: The paper highlights the need for further research in this area but does not provide a concrete solution or framework for achieving cross-lingual ethical reasoning transfer.
- What evidence would resolve it: Evidence could include successful implementation and evaluation of ethical reasoning capabilities in LLMs that are effective across multiple languages and cultures.

### Open Question 2
- Question: How can complex policies with simultaneous partial orderings of rules be effectively represented and reasoned with by LLMs?
- Basis in paper: [explicit] The authors mention that in practical settings, there will be multiple policies with simultaneous partial orderings of rules, and representation of complex policies as well as LLMs' capability to reason with those require further investigation.
- Why unresolved: The paper does not provide a specific framework or methodology for handling complex policies with multiple partial orderings of rules.
- What evidence would resolve it: Evidence could include successful implementation and evaluation of LLMs' ability to reason with complex policies and multiple partial orderings of rules.

### Open Question 3
- Question: How can ethical policies be effectively aligned with LLMs while avoiding biases and ensuring representation of diverse ethical values?
- Basis in paper: [explicit] The authors discuss the potential biases in LLMs and the need for diverse cultural representation in ethical reasoning datasets.
- Why unresolved: The paper highlights the challenges of aligning ethical policies with LLMs but does not provide a concrete solution or methodology for addressing these challenges.
- What evidence would resolve it: Evidence could include successful implementation and evaluation of ethical policies that are aligned with LLMs and demonstrate fair representation of diverse ethical values.

## Limitations

- The evaluation framework relies on a relatively small set of moral dilemmas (4) that may not capture the full complexity of real-world ethical decision-making.
- The study uses English-language prompts exclusively, limiting generalizability to multilingual contexts and cross-cultural ethical reasoning.
- Ground truth resolutions are based on human annotations that may contain cultural biases, and the framework does not address how to handle conflicting policies when multiple ethical frameworks apply simultaneously.

## Confidence

- **High Confidence**: GPT-4 demonstrates superior ethical reasoning capability compared to smaller models (93.3% accuracy vs ~70% for best human annotator). The correlation between model size and ethical reasoning ability is well-supported by the data.
- **Medium Confidence**: Level 1 policies provide optimal balance between abstraction and practical applicability. While the results support this claim, more diverse ethical scenarios would strengthen the evidence.
- **Medium Confidence**: Cultural bias in smaller models towards Western and individualistic values. The evidence is suggestive but limited by the small number of evaluation cases.

## Next Checks

1. **Cross-cultural validation**: Test the framework with ethical policies and dilemmas from non-Western cultural contexts to verify the claimed cultural neutrality of the approach.

2. **Scalability assessment**: Evaluate whether the observed correlation between model size and ethical reasoning holds for models larger than GPT-4, and identify potential diminishing returns.

3. **Conflict resolution testing**: Design experiments with multiple simultaneous ethical policies to test how models handle conflicting value systems and identify resolution strategies.