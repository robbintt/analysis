---
ver: rpa2
title: Advancing Post Hoc Case Based Explanation with Feature Highlighting
arxiv_id: '2311.03246'
source_url: https://arxiv.org/abs/2311.03246
tags:
- image
- test
- which
- feature
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of providing more comprehensive
  post-hoc explanations for image classification by black-box AI models. The authors
  propose two algorithms - one based on latent representations and one using superpixels
  - that can isolate multiple salient feature parts in a test image and link them
  to corresponding parts in explanatory cases from the training data.
---

# Advancing Post Hoc Case Based Explanation with Feature Highlighting

## Quick Facts
- arXiv ID: 2311.03246
- Source URL: https://arxiv.org/abs/2311.03246
- Reference count: 11
- Primary result: Feature highlighting algorithms improve user calibration for ambiguous misclassifications compared to whole-image explanations

## Executive Summary
This paper introduces novel algorithms for post-hoc explanation of black-box image classification models by highlighting specific feature parts and linking them to explanatory cases from training data. The approach addresses limitations of previous methods that only used whole images for explanations, enabling more detailed and faithful explanations. The algorithms are evaluated computationally against other saliency methods and through a user study (N=163) on ImageNet data, demonstrating their effectiveness in calibrating user perceptions of model correctness for ambiguous classifications.

## Method Summary
The authors propose two algorithms for feature highlighting in image classification explanations: a latent-based method that extracts representations from convolutional layers and a superpixel-based method that works with any ANN architecture. Both algorithms identify salient regions in test images and find matching regions in nearest neighbors from training data using L2 norm minimization with saliency constraints. The methods are evaluated through computational ablation tests comparing CAM, FAM, Random maps, and LIME, as well as a user study measuring calibration of user feelings of correctness.

## Key Results
- Feature highlighting algorithms improve explanation comprehensiveness by isolating multiple salient feature parts rather than using whole images
- CAM-based approaches outperform FAM, Random maps, and LIME in computational ablation tests
- User study demonstrates that feature highlighting appropriately calibrates feelings of correctness for ambiguous misclassifications
- The superpixel-based method provides ANN-agnostic capabilities compared to CNN-specific latent-based approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The latent-based algorithm improves explanation comprehensiveness by isolating multiple feature parts in a test image and linking them to corresponding parts in explanatory cases from the training data.
- Mechanism: The algorithm extracts a final representation from convolutional layers, identifies salient regions using activation maps, and finds matching regions in nearest neighbors using L2 norm minimization constrained by saliency thresholds.
- Core assumption: Salient regions identified through activation maps correspond to the most discriminative features learned by the model.
- Evidence anchors:
  - [abstract]: "The authors propose two algorithms - one based on latent representations and one using superpixels - that can isolate multiple salient feature parts in a test image and link them to corresponding parts in explanatory cases from the training data."
  - [section]: "To select salient areas in the test image, the presence of some activation map Mtest ∈ I R(h,w), giving the importance of each spatial region in C is assumed (e.g., FAMs)."
- Break condition: If the activation map fails to accurately identify truly salient regions or if nearest neighbors don't contain similar feature parts.

### Mechanism 2
- Claim: The superpixel-based method provides ANN-agnostic feature highlighting by using superpixel segmentation to isolate image regions and matching them to training data.
- Mechanism: The method uses superpixel algorithms to segment images, occludes all but one superpixel to obtain its latent representation, and finds the closest matching region in nearest neighbors based on L2 distance and saliency constraints.
- Core assumption: Occluding all but one superpixel region preserves the discriminative information needed to find matching regions in training data.
- Evidence anchors:
  - [abstract]: "This allows for more detailed and faithful explanations compared to previous methods that only used whole images."
  - [section]: "Formally, consider a test image's most salient superpixel region ωtest ∈ I R(d), where d is the number of extracted features in the penultimate layer."
- Break condition: If superpixel segmentation doesn't align with natural image features or if the occlusion process destroys necessary context.

### Mechanism 3
- Claim: Feature highlighting appropriately calibrates user's feelings of correctness for ambiguous misclassifications by showing what specific features the model focused on.
- Mechanism: By displaying highlighted regions alongside explanatory cases, users can understand why the model made certain decisions, particularly for ambiguous cases where multiple interpretations are possible.
- Core assumption: Users can accurately interpret highlighted features and use them to assess model correctness.
- Evidence anchors:
  - [abstract]: "Results demonstrate that the proposed approach appropriately calibrates a user's feelings of 'correctness' for ambiguous classifications in real world data on the ImageNet dataset."
  - [section]: "This may raise ethical concerns as feature highlighting could give users the impression that 'incorrect' classifications seem less incorrect, but this concern is likely due to the fact that some images could plausibly be labelled as multiple different classes."
- Break condition: If users misinterpret highlighted features or if the highlighting doesn't accurately represent model reasoning.

## Foundational Learning

- Concept: Convolutional Neural Networks (CNNs)
  - Why needed here: The latent-based algorithm specifically works with CNN architectures and their convolutional layer outputs.
  - Quick check question: What is the output shape of the final convolutional layer in a typical CNN architecture?

- Concept: Feature Attribution Methods
  - Why needed here: Understanding how different feature attribution methods (CAM, FAM, LIME) work is crucial for evaluating the proposed algorithms.
  - Quick check question: How does Class Activation Mapping (CAM) differ from Feature Activation Maps (FAM) in identifying salient regions?

- Concept: Nearest Neighbor Search
  - Why needed here: The algorithms rely on finding nearest neighbors in the training data to provide explanatory cases.
  - Quick check question: What distance metric is typically used for nearest neighbor search in image classification tasks?

## Architecture Onboarding

- Component map: Test image processing -> Feature extraction (CNN/ANN) -> Saliency detection (CAM/FAM/LIME) -> Nearest neighbor retrieval -> Region matching (L2 distance + saliency constraints) -> User interface display

- Critical path: 1) Extract features from test image using CNN or ANN, 2) Identify salient regions using chosen highlighting method, 3) Retrieve nearest neighbors from training data, 4) Find matching regions in nearest neighbors using L2 distance and saliency constraints, 5) Display highlighted regions alongside explanatory cases.

- Design tradeoffs: The latent-based method is faster but CNN-specific, while the superpixel method is slower but ANN-agnostic. The choice of highlighting method (CAM vs FAM vs LIME) affects both performance and generalization capabilities.

- Failure signatures: Poor explanation quality may result from inaccurate saliency detection, mismatched regions in nearest neighbors, or failure to capture the true discriminative features. User confusion may arise from misinterpretation of highlighted features or unclear connections to explanatory cases.

- First 3 experiments:
  1. Compare feature highlighting methods (CAM, FAM, LIME) on a small dataset to evaluate their effectiveness in isolating salient regions.
  2. Test the nearest neighbor matching algorithm with synthetic data to verify that it correctly identifies corresponding regions.
  3. Conduct a small-scale user study with a limited set of images to assess whether users can interpret highlighted features correctly.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the discussion and related work, several areas for future research emerge:

- How can the proposed algorithms be extended to handle more complex and diverse image datasets beyond ImageNet and CUB-200?
- How do the proposed algorithms perform in comparison to other state-of-the-art XAI methods, such as those based on counterfactual explanations or generative models?
- How can the proposed algorithms be adapted to handle video data or other sequential data types?

## Limitations

- The user study was conducted on Mechanical Turk with a specific demographic (N=163), limiting generalizability to broader populations.
- The effectiveness of explanations may vary significantly across different user groups and contexts.
- The study focuses on calibration of feelings of correctness rather than actual accuracy improvements in human decision-making.

## Confidence

- High confidence: The algorithmic mechanisms for feature highlighting and nearest neighbor matching are technically sound and well-defined
- Medium confidence: The computational ablation tests comparing different saliency methods show clear performance differences
- Medium confidence: The user study results demonstrating calibration effects for ambiguous misclassifications, though subject to sampling limitations

## Next Checks

1. Conduct a follow-up study with a more diverse participant pool and measure actual decision accuracy improvements, not just feelings of correctness
2. Test the algorithms on additional datasets beyond ImageNet to assess generalization capabilities
3. Implement a real-time validation framework to monitor how users interact with the explanations during actual classification tasks