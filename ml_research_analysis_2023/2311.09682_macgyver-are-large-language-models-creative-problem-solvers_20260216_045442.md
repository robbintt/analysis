---
ver: rpa2
title: 'MacGyver: Are Large Language Models Creative Problem Solvers?'
arxiv_id: '2311.09682'
source_url: https://arxiv.org/abs/2311.09682
tags:
- problem
- human
- gpt-4
- problems
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MACGYVER, a dataset of 1,683 creative problem-solving
  tasks designed to require unconventional use of everyday objects. The dataset is
  generated through a labor-efficient pipeline combining LLM generation and human
  verification.
---

# MacGyver: Are Large Language Models Creative Problem Solvers?

## Quick Facts
- arXiv ID: 2311.09682
- Source URL: https://arxiv.org/abs/2311.09682
- Authors: 
- Reference count: 40
- Large language models significantly underperform humans on creative problem-solving tasks requiring unconventional object use

## Executive Summary
This paper introduces MACGYVER, a dataset of 1,683 creative problem-solving tasks that require unconventional use of everyday objects. The dataset is generated through a labor-efficient pipeline combining LLM generation and human verification. Human and LLM performance is compared on a subset of 188 problems. Humans significantly outperform all tested LLMs, with GPT-4 achieving only 62.9% correct responses compared to 65.9% for humans. The authors analyze LLM errors and propose two prompting strategies—iterative step-wise reflection and divergent-convergent thinking—to improve performance. These strategies reduce infeasible solutions by 9.7% and increase efficient solutions by 6.5% respectively. The work highlights complementary strengths of humans and AI in creative problem-solving.

## Method Summary
The MACGYVER dataset is generated through a progressive problem-refinement approach where GPT-4 creates increasingly challenging problems by adding constraints that veto previous solutions and introducing distractor objects. Human responses are collected from 252 participants using Prolific, with each problem receiving 4 responses. LLM responses are evaluated by humans using a standardized interface that classifies solutions as feasible/efficient/inefficient/infeasible. The evaluation includes GPT-3.5, GPT-4, PaLM2, Claude2, and Llama2 variants, with responses generated using Nucleus sampling (T=0.7, p=0.95). Two prompting strategies are tested: iterative step-wise reflection (verifying physical feasibility of each step) and divergent-convergent thinking (exploring tool affordances before solution generation).

## Key Results
- Humans achieve 65.9% correct responses versus 62.9% for GPT-4 on creative problem-solving tasks
- GPT-4 struggles with problems requiring domain-specific knowledge, correctly identifying only 14.3% of unsolvable problems
- Iterative step-wise reflection reduces infeasible solutions by 9.7% and increases efficient solutions by 5.9%
- Divergent-convergent thinking increases efficient solutions by 6.5% while maintaining solution quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The progressive problem refinement approach creates increasingly challenging problems that push LLMs beyond simple pattern matching.
- **Mechanism:** Starting with a basic problem setup, the pipeline iteratively adds constraints that veto previous solutions and introduces distractor objects, forcing the LLM to find more creative solutions or determine infeasibility.
- **Core assumption:** GPT-4's performance degradation across refinement iterations indicates the problems become genuinely more challenging rather than just more verbose.
- **Evidence anchors:**
  - [abstract]: "We design a novel and labor-efficient pipeline to collect progressively more challenging problem-solving scenarios"
  - [section]: "We propose a progressive problem-refinement approach that gradually increases problem complexity by 1) adding specific object properties as constraints to veto a previous solution and 2) adding distracting objects"
  - [corpus]: Weak evidence - only mentions 5 related papers with no citation data
- **Break condition:** If LLMs could simply ignore constraints or recognize patterns in distractor placement, they might solve refined problems without genuine creative thinking.

### Mechanism 2
- **Claim:** The iterative step-wise reflection prompting strategy reduces physically infeasible solutions by forcing LLMs to verify each proposed step.
- **Mechanism:** After generating an initial solution, the LLM is prompted to verify if each step is physically feasible and afforded. If not, it modifies the solution iteratively until no more modifications are needed.
- **Core assumption:** LLMs generate solutions in a way that can be meaningfully verified and improved through self-reflection.
- **Evidence anchors:**
  - [abstract]: "We provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniques such as iterative step-wise reflection"
  - [section]: "After the LLM generates an initial solution, we prompt it to verify if each step is physically feasible and afforded"
  - [corpus]: Weak evidence - no direct citation to support this specific mechanism
- **Break condition:** If LLMs cannot properly evaluate their own outputs or if verification steps introduce additional errors, the strategy may fail to improve performance.

### Mechanism 3
- **Claim:** The divergent-convergent thinking prompting strategy improves efficiency by forcing LLMs to first explore tool affordances before attempting solutions.
- **Mechanism:** The LLM is prompted to first brainstorm the affordances of each presented object (divergent thinking) and conclude whether they are useful, followed by generating the steps towards the stated goal (convergent thinking).
- **Core assumption:** Structured exploration of tool affordances before solution generation leads to more efficient solutions.
- **Evidence anchors:**
  - [abstract]: "We demonstrate the potential of enhancing their problem-solving ability with novel prompting techniques such as... divergent-convergent thinking"
  - [section]: "The LLM is prompted to first brainstorm the affordance of each presented object and conclude whether they are useful, followed by generating the steps towards the stated goal"
  - [corpus]: Weak evidence - no direct citation to support this specific mechanism
- **Break condition:** If the affordance analysis step introduces confusion or if the LLM fails to properly synthesize the divergent exploration into a convergent solution.

## Foundational Learning

- **Concept:** Functional fixedness and cognitive bias
  - **Why needed here:** The dataset is specifically designed to trigger functional fixedness, requiring agents to use familiar objects in innovative ways
  - **Quick check question:** What is functional fixedness and why does it matter for creative problem-solving tasks?

- **Concept:** Divergent vs convergent thinking
  - **Why needed here:** The paper explicitly references Guilford's creativity framework and uses divergent-convergent thinking as a prompting strategy
  - **Quick check question:** How do divergent and convergent thinking differ in the context of creative problem-solving?

- **Concept:** Affordance theory
  - **Why needed here:** The error analysis shows LLMs struggle with understanding tool affordances in unconventional contexts
  - **Quick check question:** What are affordances and why are they crucial for physical reasoning in problem-solving tasks?

## Architecture Onboarding

- **Component map:** Problem generation pipeline (LLM + human verification) -> Benchmark evaluation framework (human vs LLM comparison) -> Prompting strategy modules (iterative reflection, divergent-convergent thinking)
- **Critical path:** Problem generation → Human verification → LLM evaluation → Performance analysis → Prompting strategy implementation
- **Design tradeoffs:** Automated problem generation trades off some control for scalability; human verification adds quality but increases cost; multiple prompting strategies add complexity but may improve performance
- **Failure signatures:** LLM overconfidence (suggesting solutions to unsolvable problems), hallucination (using unavailable tools), and physical infeasibility (proposing impossible actions)
- **First 3 experiments:**
  1. Compare GPT-4 performance across problem refinement iterations to validate progressive difficulty
  2. Test iterative step-wise reflection prompting on a subset of problems to measure reduction in infeasible solutions
  3. Evaluate divergent-convergent thinking prompting to measure improvement in solution efficiency

## Open Questions the Paper Calls Out

- **Open Question 1:** What specific evaluation metrics could be developed to automatically assess the quality and creativity of solutions to unconventional problem-solving tasks like those in the MACGYVER dataset?
  - **Basis in paper:** [inferred] The paper mentions that existing automatic evaluation techniques fall short for assessing the efficacy of solutions, and that their experiments rely on human evaluation which is slow and costly.
  - **Why unresolved:** The paper identifies this as a limitation and suggests it as a future direction, but does not propose specific metrics or approaches.
  - **What evidence would resolve it:** Development and validation of new automated metrics that correlate well with human judgments of solution quality and creativity for unconventional problem-solving tasks.

- **Open Question 2:** How do different prompting strategies (beyond the two proposed in the paper) affect the performance of large language models on unconventional problem-solving tasks?
  - **Basis in paper:** [explicit] The paper proposes two prompting strategies (iterative step-wise reflection and divergent-convergent thinking) and shows they improve LLM performance, but suggests investigating other planning and reasoning strategies.
  - **Why unresolved:** The paper only explores two specific prompting strategies and calls for investigation of other approaches.
  - **What evidence would resolve it:** Comparative evaluation of multiple prompting strategies on LLM performance for unconventional problem-solving, including analysis of their relative effectiveness.

- **Open Question 3:** How does the performance of large language models on unconventional problem-solving tasks change when they are provided with visual information about the problem context and objects?
  - **Basis in paper:** [inferred] The paper discusses LLMs' limitations in understanding tool affordances and physical feasibility, suggesting they lack direct interaction with the physical world. It also mentions that human solvers contemplate tasks purely in their minds.
  - **Why unresolved:** The paper does not explore the impact of visual information on LLM performance for these tasks.
  - **What evidence would resolve it:** Experimental comparison of LLM performance on unconventional problem-solving with and without visual information about the problem context and objects.

## Limitations

- The dataset construction relies on GPT-4's judgment for progressive refinement, creating potential circularity where models are tested on problems their own variants helped create
- Human evaluation introduces subjectivity - different annotators might disagree on what constitutes "physically feasible" solutions
- The sample size for direct human-LLM comparison (188 problems) represents only 11% of the full dataset, limiting generalizability

## Confidence

- **High confidence**: Humans outperform current LLMs on this benchmark (measured at 65.9% vs 62.9% accuracy)
- **Medium confidence**: The two prompting strategies improve LLM performance by specific percentages (9.7% reduction in infeasible solutions, 6.5% increase in efficient solutions)
- **Low confidence**: The generalizability of these results to other creative problem-solving domains beyond the curated MACGYVER tasks

## Next Checks

1. **Cross-validation with independent problem generation**: Recreate the benchmark using a different LLM (e.g., Claude or PaLM2) for problem generation to test whether GPT-4's involvement in dataset creation biases results

2. **Extended human evaluation**: Scale human evaluation to 500+ problems across multiple annotator pools to establish more robust performance baselines and measure inter-annotator agreement

3. **Real-world deployment test**: Apply the iterative step-wise reflection prompting to actual creative problem-solving tasks in domains like engineering design or scientific hypothesis generation to validate practical utility beyond benchmark performance