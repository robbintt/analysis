---
ver: rpa2
title: 'Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement
  Learning'
arxiv_id: '2312.10385'
source_url: https://arxiv.org/abs/2312.10385
tags:
- policy
- cost
- good
- trajectories
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel framework for safe reinforcement learning
  that does not rely on local cost constraints. Instead, it uses imitation learning
  to imitate "good" trajectories and avoid "bad" trajectories generated by incrementally
  improving policies.
---

# Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement Learning

## Quick Facts
- arXiv ID: 2312.10385
- Source URL: https://arxiv.org/abs/2312.10385
- Reference count: 40
- Outperforms state-of-the-art constrained RL algorithms on Safety-Gym environments while satisfying cost constraints

## Executive Summary
This paper introduces a novel framework for safe reinforcement learning that bypasses the need for local cost constraints by using imitation learning to incrementally improve policies. The approach learns from trajectory-level rewards and costs, classifying trajectories as "good" or "bad" using an oracle based on reward thresholds and cost constraints. The proposed algorithm, SIM (Self-imitation Safe RL), demonstrates superior performance compared to existing constrained RL methods across six challenging Safety-Gym environments, achieving higher expected rewards while maintaining cost constraint satisfaction. Notably, SIM also works effectively when cost functions are unknown, relying only on an oracle to identify constraint-violating trajectories.

## Method Summary
The method proposes a trajectory-based approach to safe RL that learns from demonstrations of good and bad trajectories. It starts with a pre-trained policy and iteratively improves it by maximizing the likelihood of good trajectories while minimizing that of bad ones. The algorithm uses a non-adversarial distribution matching approach, estimating the ratio of bad to good+policy occupancy through a cooperative classifier. This avoids the instability of adversarial training while still effectively guiding the policy away from unsafe behaviors. The method is shown to work even when cost functions are unknown, requiring only an oracle to flag constraint-violating trajectories.

## Key Results
- SIM outperforms state-of-the-art constrained RL algorithms (FOCOPS, CUP, CPO, PPO-Lagrangian) on six Safety-Gym environments
- Achieves higher expected rewards while satisfying cost constraints across all tested environments
- Demonstrates robustness in settings with unknown cost functions or when starting from low-quality initial policies
- The non-adversarial approach provides stability and scalability compared to adversarial imitation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Imitating good trajectories and avoiding bad ones can incrementally improve a policy without relying on local cost constraints.
- Mechanism: The algorithm learns from trajectory-level rewards and costs, using an oracle to classify trajectories as good (high reward, low cost) or bad (low reward or high cost). By maximizing the likelihood of good trajectories and minimizing that of bad ones, the policy improves without estimating per-state costs.
- Core assumption: There exists a policy that can assign zero probability to bad trajectories while maintaining high probability on good ones, and that such a policy is feasible under the cost constraint.
- Evidence anchors:
  - [abstract]: "our approach is able to work from any starting policy or set of trajectories and improve on it"
  - [section 3.2]: Proposition 1 shows that if bad trajectories are defined as those with low reward and high cost, the optimal policy is feasible and offers higher expected reward than the pre-trained policy
  - [corpus]: Weak - no direct citations to other works using similar trajectory-level imitation learning
- Break condition: If the initial policy cannot generate any good trajectories (e.g., strict constraints make feasible trajectories impossible), the algorithm cannot improve.

### Mechanism 2
- Claim: The non-adversarial distribution matching approach (SIM) is more stable than adversarial methods like GAIL.
- Mechanism: Instead of adversarial training between policy and discriminator, SIM uses a cooperative discriminator K that estimates the ratio of bad to good+policy occupancy. This avoids the instability of GAN-style training while still guiding the policy away from bad trajectories.
- Core assumption: The KL divergence between the mixed good+policy distribution and the bad distribution can be effectively maximized without adversarial training.
- Evidence anchors:
  - [section 3.3]: "Due to the non-adversarial nature of the algorithm, it provides higher stability while being scalable"
  - [section 4]: Describes the cooperative update of K and π, contrasting with adversarial training
  - [corpus]: Weak - no citations comparing adversarial vs non-adversarial imitation learning in this context
- Break condition: If the classifier K cannot effectively distinguish good from bad trajectories, the policy improvement signal becomes weak.

### Mechanism 3
- Claim: The method works even when cost functions are unknown, as long as an oracle can identify constraint-violating trajectories.
- Mechanism: By using an oracle that flags trajectories as "violated" based on cost constraints, the algorithm can avoid bad trajectories without needing the explicit cost function. This extends the method to settings where cost modeling is difficult or impossible.
- Core assumption: An oracle exists that can reliably determine whether a trajectory violates the cost constraint, even without the explicit cost function.
- Evidence anchors:
  - [abstract]: "demonstrate that our approach is able to outperform top benchmark approaches... or even unknown cost constraints"
  - [section 3.2]: Proposition 3(iii) shows that with an oracle identifying violated trajectories, π* is guaranteed to be feasible
  - [corpus]: Weak - no citations to works handling unknown cost functions in constrained RL
- Break condition: If the oracle's classification of violated vs non-violated trajectories is inaccurate, the algorithm may incorrectly label good trajectories as bad or vice versa.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and their extension to Constrained MDPs (CMDPs)
  - Why needed here: The entire framework builds on CMDPs where the agent must maximize reward while satisfying cost constraints. Understanding the difference between state-wise and trajectory-wise constraints is crucial.
  - Quick check question: What is the key difference between a standard MDP and a CMDP in terms of constraints?

- Concept: Imitation Learning (IL) and Behavioral Cloning
  - Why needed here: The algorithm uses IL techniques to imitate good trajectories and avoid bad ones. Understanding how BC maximizes likelihood of demonstrations is foundational to grasping the BC-GB variant.
  - Quick check question: Why does pure Behavioral Cloning often fail to match expert performance in new states?

- Concept: Distribution Matching in IL
  - Why needed here: The algorithm uses distribution matching (specifically KL divergence between occupancy measures) rather than pure likelihood maximization. This is central to understanding how the policy is shaped to match good trajectories.
  - Quick check question: What is the relationship between occupancy measures and the state-action distribution in IL?

## Architecture Onboarding

- Component map:
  - Initial policy π0 (trained with relaxed constraints) -> Trajectory generator -> Oracle -> Classifier K -> Policy π -> Good trajectory set ΩG and bad trajectory set ΩB

- Critical path:
  1. Generate trajectories from current policy
  2. Classify trajectories using oracle
  3. Update classifier K to estimate bad vs good+policy ratio
  4. Update policy π to maximize KL divergence from bad distribution
  5. Repeat until convergence

- Design tradeoffs:
  - Using trajectory-level vs state-level constraints: Trajectory-level is more faithful to CMDP but requires storing and processing entire trajectories
  - Adversarial vs non-adversarial training: Adversarial methods (like GAIL) can be unstable; non-adversarial (like SIM) trades some expressiveness for stability
  - Fixed vs dynamic thresholds: Fixed thresholds (RG, RB) are simpler but may not adapt to policy improvement; dynamic thresholds can better guide learning but add complexity

- Failure signatures:
  - Policy improvement stalls: Check if good trajectories are being generated and if thresholds are appropriately set
  - High variance in training: Classifier K may not be converging; try increasing its training iterations
  - Constraint violations persist: Verify oracle is correctly identifying bad trajectories; check if initial policy π0 is too poor to generate feasible trajectories

- First 3 experiments:
  1. Run SIM with a pre-trained initial policy on SafetyPointGoal-v0; verify it outperforms PPO-Lagrangian in both reward and constraint satisfaction
  2. Test SIM with only good trajectories (λ=1) vs only bad trajectories (λ=0) to verify the importance of both sets
  3. Implement the unknown cost function variant with an oracle; verify SIM can still learn a feasible policy without explicit cost function

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SIM be extended to work effectively with high-dimensional continuous action spaces, such as those found in robotic manipulation tasks?
- Basis in paper: [inferred] The paper demonstrates SIM's effectiveness on Safety-Gym and Mujoco environments, but does not explore high-dimensional continuous action spaces.
- Why unresolved: The paper focuses on relatively simple action spaces and does not address the challenges of scaling to more complex domains.
- What evidence would resolve it: Testing SIM on robotic manipulation benchmarks like Meta-World or D'Claw would demonstrate its scalability to high-dimensional continuous action spaces.

### Open Question 2
- Question: How does SIM's performance scale with the complexity of the constraint function, particularly for non-linear or time-varying constraints?
- Basis in paper: [explicit] The paper mentions that SIM works with unknown cost functions but does not explore complex constraint structures.
- Why unresolved: The experiments focus on linear cost constraints, and the paper does not address how SIM handles more complex constraint formulations.
- What evidence would resolve it: Testing SIM on environments with non-linear or time-varying constraints would demonstrate its ability to handle complex constraint structures.

### Open Question 3
- Question: Can SIM be combined with model-based RL approaches to improve sample efficiency, especially in scenarios with sparse rewards or high-cost constraints?
- Basis in paper: [inferred] The paper presents SIM as a model-free approach and does not explore integration with model-based methods.
- Why unresolved: The experiments focus on model-free RL and do not address potential synergies with model-based approaches.
- What evidence would resolve it: Implementing SIM within a model-based RL framework and comparing its sample efficiency to the current model-free version would demonstrate the benefits of such an integration.

## Limitations
- Claims are supported by empirical results on Safety-Gym environments but limited to these specific domains
- Relies on having access to an oracle for trajectory classification, which may not be available in all real-world settings
- Does not provide theoretical guarantees on convergence rates or sample efficiency compared to existing methods
- Performance when starting from very poor initial policies is not extensively explored

## Confidence

**High confidence**: The mechanism of using trajectory-level imitation learning to avoid bad trajectories and imitate good ones is clearly described and supported by the algorithm's structure and the Safety-Gym experiments.

**Medium confidence**: The claim that the non-adversarial approach provides better stability than adversarial methods is plausible given the algorithm's design, but lacks direct empirical comparison with adversarial methods like GAIL in this specific context.

**Medium confidence**: The assertion that the method works with unknown cost functions, relying only on an oracle, is theoretically supported by Proposition 3 but would benefit from more extensive empirical validation in varied environments.

## Next Checks

1. Test SIM's performance when starting from a randomly initialized policy (not pre-trained) to assess its ability to bootstrap from poor initial conditions.

2. Implement and compare SIM with an adversarial variant (similar to GAIL) to empirically validate the claimed stability benefits of the non-adversarial approach.

3. Apply SIM to a real-world robotics task where an oracle can flag safety violations, to validate the method's applicability beyond simulation environments.