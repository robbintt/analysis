---
ver: rpa2
title: 'MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection'
arxiv_id: '2309.15670'
source_url: https://arxiv.org/abs/2309.15670
tags:
- bangla
- https
- emotion
- emotions
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a comprehensive method for Bangla multi-label
  emotion detection, addressing the scarcity of annotated corpora for this low-resource
  language. The approach involves scraping Facebook comments from nine news events,
  followed by context-based annotation to create a corpus of 108,950 comments.
---

# MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection

## Quick Facts
- arXiv ID: 2309.15670
- Source URL: https://arxiv.org/abs/2309.15670
- Authors: 
- Reference count: 40
- Key outcome: Bangla-BERT-Base achieves 83.23% accuracy for multi-label emotion detection in Bangla text

## Executive Summary
This study addresses the challenge of multi-label emotion detection in Bangla, a low-resource language, by creating the MONOVAB corpus of 108,950 Facebook comments annotated with five emotions (anger, contempt, disgust, enjoyment, sadness). The researchers employ a context-based annotation approach and evaluate various machine learning, deep learning, and transformer-based models. Bangla-BERT-Base emerges as the top-performing model with 83.23% accuracy, demonstrating the effectiveness of transformer architectures for this task. A web application is also developed to showcase the model's capabilities.

## Method Summary
The research involves scraping Facebook comments from nine news events, followed by context-based annotation to create a multi-label emotion corpus. Various models are evaluated including ML algorithms (Logistic Regression, Multinomial Naive Bayes, SVM, Random Forest, KNN), DL approaches (LSTM, BiLSTM, CNN-LSTM, CNN-BiLSTM), and transformer models (Bangla-Electra, Bangla-BERT-Base, Multilingual BERT, BanglaBert). The classifier chain approach is applied for multi-label classification, and a web application is developed to demonstrate the pre-trained model's performance.

## Key Results
- Bangla-BERT-Base achieves the highest accuracy at 83.23% for multi-label emotion detection
- Context-based annotation improves emotion understanding in Bangla text
- MONOVAB corpus contains 108,950 manually annotated Facebook comments
- The web application successfully demonstrates pre-trained model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-based annotation enables better multi-label emotion detection in Bangla by providing background information to resolve ambiguity.
- Mechanism: Annotators use news context to interpret emotional nuances that might be unclear from text alone, leading to more accurate multi-label assignments.
- Core assumption: Annotators can effectively leverage contextual information to disambiguate emotional intent in Bangla text.
- Evidence anchors:
  - [abstract] "To make this annotation more fruitful, the context-based approach has been used."
  - [section] "Context-based annotation refers to the annotation of data depending on the underlying context... Contexts were supplied to the annotators of the associated news articles during the annotation phase so that they could comprehend the emotions of comments more precisely before annotating them."
  - [corpus] Weak evidence: While the corpus includes context from news events, the paper doesn't provide quantitative comparison with non-contextual annotation approaches.
- Break condition: If annotators lack domain knowledge or context information is insufficient, annotation quality may not improve.

### Mechanism 2
- Claim: Bangla-BERT-Base outperforms other transformer models for multi-label emotion detection due to its pre-training on Bangla-specific linguistic patterns.
- Mechanism: The model leverages deep contextual embeddings learned from Bangla text to capture subtle emotional expressions that single-language or multilingual models miss.
- Core assumption: Bangla-BERT-Base has been pre-trained on sufficient Bangla data to capture language-specific emotional cues.
- Evidence anchors:
  - [abstract] "Bangla-Bert-Base achieving the highest accuracy of 83.23% for multi-label emotion recognition."
  - [section] "Bangla-Bert-Base has the highest accuracy at 83.23 percent, which is remarkable... Compared to other algorithms, Bangla-Bert-Base has the best overall performance superiority."
  - [corpus] The corpus contains 108,950 comments from Bangla social media, providing diverse training data for Bangla-BERT-Base.
- Break condition: If the pre-training corpus for Bangla-BERT-Base lacks emotional diversity or is too small, performance gains may diminish.

### Mechanism 3
- Claim: Multi-label classification using classifier chains improves performance over single-label approaches by capturing emotion co-occurrence patterns.
- Mechanism: The classifier chain approach models dependencies between emotion labels, allowing the model to learn that certain emotions frequently occur together.
- Core assumption: Emotion labels in Bangla text exhibit meaningful co-occurrence patterns that can be exploited.
- Evidence anchors:
  - [section] "MNB algorithm achieves the greatest performance among all ML methods... In this study, this system performed poorly in categorizing emotions with multiple labels."
  - [section] "This model has also been applied as a classifier chain approach, resulting in improved multi-label classification accuracy."
  - [corpus] Weak evidence: The corpus contains multi-label annotations but doesn't explicitly analyze co-occurrence statistics.
- Break condition: If emotion labels are largely independent, the added complexity of classifier chains provides no benefit.

## Foundational Learning

- Concept: Multi-label classification vs multi-class classification
  - Why needed here: The paper addresses detecting multiple emotions per text instance, requiring understanding of how this differs from predicting a single emotion category.
  - Quick check question: If a comment expresses both anger and sadness, would a multi-class classifier or multi-label classifier be more appropriate?

- Concept: Transformer architecture fundamentals
  - Why needed here: Understanding BERT's self-attention mechanism is crucial for grasping why it outperforms other approaches for Bangla emotion detection.
  - Quick check question: How does BERT's bidirectional context capture differ from unidirectional RNN approaches like LSTM?

- Concept: Context-based annotation methodology
  - Why needed here: The study's novel approach uses news context to improve annotation quality, requiring understanding of how contextual information aids human annotation.
  - Quick check question: What information would you need from a news article to accurately annotate the emotions in related social media comments?

## Architecture Onboarding

- Component map: Data scraping → Preprocessing → Context-based annotation → Feature extraction (tokenization/TF-IDF) → Model training (ML/DL/BERT) → Web application deployment
- Critical path: Annotation quality → Feature representation → Model selection → Web application performance
- Design tradeoffs: Context-based annotation improves quality but increases cost; classifier chains capture label dependencies but add complexity; transformer models need more resources but achieve better accuracy
- Failure signatures: Low inter-annotator agreement indicates annotation issues; poor model performance on certain emotions suggests feature representation problems; web application latency indicates deployment issues
- First 3 experiments:
  1. Compare context-based vs non-contextual annotation agreement rates
  2. Evaluate feature extraction methods (tokenization vs TF-IDF) on a held-out validation set
  3. Benchmark Bangla-BERT-Base against multilingual BERT on a balanced emotion detection task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the 'MONOVAB' corpus be further expanded to include a broader range of emotions beyond anger, contempt, disgust, enjoyment, and sadness?
- Basis in paper: [explicit] The paper mentions that these five emotions were chosen based on the nature of the news events, but acknowledges that other emotions may be present in the data.
- Why unresolved: The study focused on these specific emotions due to their relevance to the selected news events, but a more comprehensive set of emotions could provide a richer understanding of emotional expression in Bangla text.
- What evidence would resolve it: Expanding the corpus to include additional emotions and evaluating the performance of existing models on this extended dataset would demonstrate the feasibility and benefits of a more comprehensive emotion set.

### Open Question 2
- Question: How can the context-based annotation approach be further refined to improve the accuracy and consistency of emotion labeling in Bangla text?
- Basis in paper: [explicit] The paper highlights the use of context-based annotation to improve understanding of emotions, but acknowledges that this approach relies on human annotators and may be subject to subjective interpretations.
- Why unresolved: While context-based annotation is a valuable technique, there is room for improvement in terms of standardizing the annotation process and minimizing the impact of individual annotator biases.
- What evidence would resolve it: Developing a more rigorous annotation guideline and evaluating the inter-annotator agreement would provide insights into the reliability and consistency of the context-based annotation approach.

### Open Question 3
- Question: How can the performance of transformer-based models like Bangla-BERT-Base be further enhanced for multi-label emotion recognition in Bangla text?
- Basis in paper: [explicit] The paper demonstrates the superior performance of Bangla-BERT-Base for multi-label emotion recognition, but acknowledges that there is still room for improvement, particularly for emotions like disgust and sadness.
- Why unresolved: While transformer-based models have shown promise, their performance on specific emotions may vary, and further optimization techniques may be needed to improve their overall effectiveness.
- What evidence would resolve it: Experimenting with different pre-training strategies, fine-tuning techniques, and model architectures for Bangla-BERT-Base could lead to improved performance on challenging emotions and enhance its overall effectiveness for multi-label emotion recognition.

## Limitations
- The emotion taxonomy is limited to five basic emotions, potentially missing nuanced emotional expressions
- Corpus creation relies on Facebook comments from news events, which may not represent full spectrum of Bangla emotional expression
- Annotation process lacks quantitative validation of inter-annotator agreement or comparison with non-contextual methods
- Evaluation metrics focus primarily on overall accuracy without detailed analysis of class-specific performance

## Confidence
- High Confidence: The experimental methodology for comparing different ML, DL, and transformer models is well-established and reproducible
- Medium Confidence: The claim that context-based annotation improves quality is supported by the approach description but lacks quantitative validation
- Low Confidence: The generalizability of results beyond Facebook comments from news events to other Bangla text domains remains unproven

## Next Checks
1. Calculate and report Cohen's kappa or similar agreement metrics between annotators to quantify annotation reliability, particularly for multi-label assignments
2. Evaluate the pre-trained Bangla-BERT-Base model on an independent Bangla emotion detection dataset from a different domain (e.g., Twitter, literature) to assess generalization capability
3. Perform statistical analysis of label co-occurrence patterns in the corpus to validate the assumption that emotions exhibit meaningful dependencies that classifier chains can exploit