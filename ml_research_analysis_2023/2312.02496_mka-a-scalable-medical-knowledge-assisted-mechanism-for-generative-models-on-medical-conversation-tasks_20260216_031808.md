---
ver: rpa2
title: 'MKA: A Scalable Medical Knowledge Assisted Mechanism for Generative Models
  on Medical Conversation Tasks'
arxiv_id: '2312.02496'
source_url: https://arxiv.org/abs/2312.02496
tags:
- medical
- knowledge
- generative
- neural
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a scalable Medical Knowledge Assisted (MKA)
  mechanism to enhance neural generative models for medical conversation tasks by
  integrating medical-specific knowledge graphs and a token concatenation policy.
  MKA addresses the limitation of existing generative models that lack medical-specific
  knowledge, which hinders their performance on medical dialogue tasks.
---

# MKA: A Scalable Medical Knowledge Assisted Mechanism for Generative Models on Medical Conversation Tasks

## Quick Facts
- arXiv ID: 2312.02496
- Source URL: https://arxiv.org/abs/2312.02496
- Reference count: 40
- Key outcome: MKA mechanism improves medical dialogue generation by integrating medical knowledge graphs, achieving state-of-the-art performance on MedDialog and MedDG datasets

## Executive Summary
This paper proposes a Medical Knowledge Assisted (MKA) mechanism to enhance neural generative models for medical conversation tasks by integrating medical-specific knowledge graphs. The mechanism addresses the limitation of existing generative models that lack medical-specific knowledge, which hinders their performance on medical dialogue tasks. MKA introduces a medical knowledge subgraph generator, topic detector, and medical knowledge extractor to generate and incorporate relevant medical knowledge information tuples into the dialogue context. Evaluation on MedDialog and MedDG datasets demonstrates that MKA-enhanced models outperform original methods across multiple automatic evaluation metrics, including improved perplexity, BLEU, NIST, METEOR, Entropy, and Dist scores, indicating enhanced language quality, semantic similarity, and lexical diversity in generated responses.

## Method Summary
The MKA mechanism integrates medical knowledge graphs with neural generative models through a token concatenation policy. The approach begins with patient self-report information (department and disease/symptom) which is used to generate a medical knowledge subgraph from a global medical knowledge base containing 6 entity types and 6 relation types. A topic detector analyzes patient questions to determine relevant topics, guiding a medical knowledge extractor to retrieve specific knowledge from the subgraph. The extracted knowledge information tuple is then concatenated with the dialogue context using a defined token concatenation policy. This processed input is fed into neural generative models (Transformer and BERT-GPT) to generate doctor responses. The mechanism is evaluated on MedDialog-CN and MedDG datasets using standard automatic metrics including perplexity, BLEU, NIST, METEOR, Entropy, and Dist scores.

## Key Results
- MKA-BERT-GPT achieves state-of-the-art performance with improved perplexity, BLEU, NIST, METEOR, Entropy, and Dist scores
- Models combined with MKA outperform original methods in multiple automatic evaluation metrics
- The mechanism demonstrates enhanced language quality, semantic similarity, and lexical diversity in generated responses

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The medical knowledge subgraph generator improves medical dialogue performance by extracting relevant medical knowledge based on patient self-report information.
- **Mechanism:** The subgraph generator takes patient self-report (department and disease/symptom) as input and generates a medical knowledge subgraph from a global medical knowledge base containing relevant entities and relations specific to the patient's case.
- **Core assumption:** Patient self-report contains sufficient information to generate a relevant subgraph that captures the medical context needed for accurate dialogue generation.
- **Evidence anchors:** The abstract states the mechanism contains 6 types of medical-related information including department, drug, check, symptom, disease, food. The methodology section describes the subgraph generation process.
- **Break condition:** If patient self-report is incomplete or contains ambiguous medical information, the subgraph generation may fail to capture relevant knowledge, leading to poor dialogue responses.

### Mechanism 2
- **Claim:** The token concatenation policy effectively integrates medical knowledge information into dialogue context for neural generative models.
- **Mechanism:** The token processor reorganizes input tokens by concatenating the medical knowledge information tuple with the dialogue context using a specific structure that combines patient self-report, medical knowledge information, previous doctor response, and current patient question.
- **Core assumption:** The specific token concatenation policy preserves the semantic relationship between medical knowledge and dialogue context, allowing neural models to learn meaningful connections.
- **Evidence anchors:** The abstract mentions the specific token concatenation policy is defined to effectively inject medical information into input data. The methodology describes the token processing approach.
- **Break condition:** If the token concatenation disrupts the natural flow of dialogue or overwhelms the model with excessive medical information, it may degrade rather than improve performance.

### Mechanism 3
- **Claim:** Topic detection reduces redundancy by identifying relevant medical knowledge information for each question.
- **Mechanism:** The topic detector analyzes the patient question in each conversation turn and determines the topic (disease, symptom, drug, check, food), which guides the medical knowledge extractor to retrieve only relevant knowledge from the subgraph.
- **Core assumption:** Patient questions can be accurately categorized into predefined topics that align with the medical knowledge graph structure.
- **Evidence anchors:** The methodology section describes how the topic detector inputs the patient question and infers the related question topic, with content matching the relation set.
- **Break condition:** If questions are ambiguous or span multiple topics, the detector may misclassify them, leading to irrelevant knowledge being injected into the dialogue.

## Foundational Learning

- **Concept: Medical Knowledge Graphs**
  - Why needed here: Medical dialogue systems require domain-specific knowledge to generate clinically accurate responses. General language models lack this specialized medical knowledge.
  - Quick check question: What are the six entity types used in the medical knowledge graph described in the paper?

- **Concept: Neural Generative Models for Dialogue**
  - Why needed here: The paper builds on existing neural generative models (Transformer, BERT-GPT) and extends them with medical knowledge integration.
  - Quick check question: What is the primary difference between information retrieval-based methods and neural generative methods for medical chatbots?

- **Concept: Token Concatenation Policies**
  - Why needed here: Standard neural models don't know how to incorporate external knowledge. A defined policy is needed to structure how medical knowledge information is combined with dialogue context.
  - Quick check question: In the token concatenation policy, what elements are combined to form the input sequence for the generative model?

## Architecture Onboarding

- **Component map:**
  Patient Self-Report → Medical Knowledge Subgraph Generator → Medical Knowledge Subgraph
  Medical Knowledge Subgraph + Question → Topic Detector → Question Topic
  Question Topic + Medical Knowledge Subgraph → Medical Knowledge Extractor → Medical Knowledge Information Tuple
  Medical Knowledge Information Tuple + Dialogue Context → Token Processor → Processed Input
  Processed Input → Neural Generative Model → Doctor Response

- **Critical path:** Patient Self-Report → Medical Knowledge Subgraph Generator → Topic Detector → Medical Knowledge Extractor → Token Processor → Neural Generative Model
  This is the core pipeline that integrates medical knowledge into dialogue generation.

- **Design tradeoffs:**
  - The medical knowledge subgraph generation adds computational overhead but improves response accuracy
  - Token concatenation policy must balance between injecting enough medical knowledge and maintaining natural dialogue flow
  - Topic detection simplifies knowledge extraction but may miss nuanced questions that span multiple topics

- **Failure signatures:**
  - Poor performance on questions about diseases/symptoms not mentioned in patient self-report
  - Responses that contain irrelevant medical information
  - Degradation in language quality or diversity metrics
  - Inability to handle complex multi-turn conversations with shifting topics

- **First 3 experiments:**
  1. Implement and test the medical knowledge subgraph generator with synthetic patient self-reports to verify it generates appropriate subgraphs
  2. Test the token concatenation policy by comparing model performance with and without medical knowledge integration on a small dataset
  3. Evaluate the topic detector's accuracy by manually annotating a sample of patient questions and comparing to detector output

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions. However, several important questions arise from the methodology and results that warrant further investigation.

## Limitations
- The paper lacks sufficient detail on the neural generative model architectures used within the MKA framework, making it difficult to assess whether improvements stem from knowledge integration or model architecture choices.
- The medical knowledge subgraph generation relies heavily on patient self-report accuracy, which may not always contain complete or accurate medical information in real-world scenarios.
- The generalization capability of the MKA mechanism to handle complex medical dialogues beyond the scope of the MedDialog and MedDG datasets is unclear.

## Confidence
- **High confidence:** The general approach of integrating medical knowledge graphs into dialogue systems is well-established in related literature, and standard evaluation metrics provide reliable benchmarks.
- **Medium confidence:** The specific implementation of the token concatenation policy and medical knowledge subgraph generation algorithm, while theoretically sound, lacks sufficient technical detail for full verification.
- **Low confidence:** The generalization capability of MKA to handle complex medical dialogues, particularly for rare diseases or complex multi-topic conversations beyond the evaluated datasets.

## Next Checks
1. Conduct ablation studies by training the same neural generative models (Transformer and BERT-GPT) without the MKA mechanism on identical datasets to isolate the contribution of knowledge integration to performance improvements.
2. Test the MKA mechanism on external medical dialogue datasets not used in the original evaluation to assess its generalization capability across different medical domains and conversation styles.
3. Implement human evaluation studies comparing responses generated by MKA-enhanced models versus baseline models on clinically relevant criteria such as medical accuracy, appropriateness, and completeness to validate the automatic metric improvements.