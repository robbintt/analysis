---
ver: rpa2
title: 'InstructEval: Systematic Evaluation of Instruction Selection Methods'
arxiv_id: '2307.00259'
source_url: https://arxiv.org/abs/2307.00259
tags:
- instruction
- instructions
- tasks
- performance
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents InstructEval, a systematic evaluation suite
  for instruction selection methods in in-context learning (ICL). The suite includes
  13 open-source LLMs from 4 model families and 9 diverse tasks spanning classification,
  multiple-choice QA, and generative QA.
---

# InstructEval: Systematic Evaluation of Instruction Selection Methods

## Quick Facts
- arXiv ID: 2307.00259
- Source URL: https://arxiv.org/abs/2307.00259
- Reference count: 16
- Key outcome: Manual/Curated instructions often outperform sophisticated automatic induction methods in zero-shot settings, while task-agnostic instructions dominate in few-shot settings

## Executive Summary
This paper presents InstructEval, a systematic evaluation suite for instruction selection methods in in-context learning (ICL). The authors evaluate 7 popular instruction selection methods across 13 open-source LLMs and 9 diverse tasks, measuring performance through 5 metrics including zero-shot accuracy, few-shot accuracy, and sensitivity measures. The key finding is that manually-written or simple task-agnostic instructions often outperform sophisticated automatic instruction induction methods, challenging the assumption that more complex methods yield better results. The evaluation suite is released to enable more rigorous and generalizable instruction selection methods in the future.

## Method Summary
InstructEval evaluates instruction selection methods for in-context learning using 13 autoregressive LLMs (1.1B-20B parameters) from 4 model families (BLOOM, GPT Neo, LLaMA, OPT) across 9 tasks spanning classification, multiple-choice QA, and generative QA. The evaluation compares 7 instruction selection methods including null instruction, generic instruction, PromptSource, ad hoc, low perplexity, APE, and RLPrompt using standardized prompt templates. Performance is measured through zero-shot accuracy, few-shot accuracy, perturbation accuracy, selectional sensitivity, and permutational sensitivity, with results aggregated using mean relative gain calculations to normalize across different models and tasks.

## Key Results
- Manual/Curated instructions excel in zero-shot settings while task-agnostic instructions dominate in few-shot settings
- Automatic instruction induction methods (APE, RLPrompt) show inconsistent performance across task types and model families
- Model scale affects instruction sensitivity, with larger models being less dependent on instruction quality when demonstrations are provided

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-agnostic instructions often outperform task-specific automatic induction methods because they provide cleaner signal without task-specific noise
- Core assumption: Demonstrations contain sufficient information for models to understand task requirements without explicit task-specific instructions
- Evidence anchors: [abstract] "using curated manually-written instructions and simple instructions without any task-specific descriptions often elicits superior ICL performance than that of automatic instruction-induction methods"

### Mechanism 2
- Claim: Model scale affects instruction sensitivity, with larger models being less dependent on instruction quality when demonstrations are provided
- Core assumption: Larger models have better pattern recognition capabilities that allow them to learn task semantics from demonstrations regardless of instruction quality
- Evidence anchors: [section] "large models are able to grasp task semantics from demonstrations (when they are provided), whereas the performance of small models is more sensitive to the instruction used"

### Mechanism 3
- Claim: Instruction selection method effectiveness varies significantly across task types and model families
- Core assumption: Model architecture and task type create fundamental differences in how instructions are processed and utilized
- Evidence anchors: [section] "the automatic instruction induction methods APE and RLPrompt do show above-average performance for certain model families and task-types, but this behavior is inconsistent"

## Foundational Learning

- Concept: Mean relative gain calculation
  - Why needed here: Provides normalized comparison across models and tasks with varying baseline performances
  - Quick check question: If method A achieves 80% accuracy on task X for model M1 (population mean 70%) and 60% on task Y for model M2 (population mean 50%), what are the mean relative gains?

- Concept: Sensitivity metrics (selectional and permutational)
  - Why needed here: Captures instruction robustness to demonstration choice and ordering for practical deployment
  - Quick check question: If using instruction I yields accuracies of 90%, 85%, and 80% across three different demonstration sets, what is its selectional sensitivity?

- Concept: Zero-shot vs few-shot distinction
  - Why needed here: Differentiates between model capabilities when provided only instructions versus when provided both instructions and demonstrations
  - Quick check question: Why might null instructions perform better than task-specific instructions in few-shot settings but worse in zero-shot settings?

## Architecture Onboarding

- Component map: Evaluation suite → Model selection → Task selection → Instruction method selection → Prompt generation → Model inference → Metric calculation → Aggregation
- Critical path: Instruction selection method → Prompt template application → Model inference → Accuracy calculation → Mean relative gain computation
- Design tradeoffs: Comprehensive evaluation (13 models, 9 tasks, 7 methods) vs computational cost; simple aggregation methods vs more sophisticated statistical approaches
- Failure signatures: Inconsistent performance across models suggests method doesn't generalize; high sensitivity scores indicate instruction instability; poor zero-shot performance suggests instructions aren't informative enough
- First 3 experiments:
  1. Compare null instructions vs generic instructions across all models for a single classification task to establish baseline performance differences
  2. Test PromptSource instructions vs ad-hoc instructions for the same task to measure manual vs automated instruction quality
  3. Evaluate the same instruction methods across different task types (classification vs multiple-choice) to identify task-specific effectiveness patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of instruction choice on model performance in extremely low-resource settings where only one or two demonstrations are available?
- Basis in paper: [inferred] The paper evaluates few-shot settings using K=6 demonstrations but does not explore the effects of instruction choice when even fewer demonstrations are available
- Why unresolved: The authors acknowledge that instruction choice has a more significant impact in zero-shot settings compared to few-shot settings, but they do not specifically investigate the effects of instruction choice when demonstrations are extremely limited
- What evidence would resolve it: Experiments comparing instruction selection methods across different numbers of demonstrations (e.g., 1, 2, 3, 6) would reveal how instruction choice affects model performance as demonstration availability decreases

### Open Question 2
- Question: How do instruction selection methods perform on tasks that require more complex reasoning or multi-step problem solving?
- Basis in paper: [inferred] The paper focuses on relatively straightforward tasks like classification, multiple-choice QA, and generative QA, but does not explore more complex reasoning tasks
- Why unresolved: The authors evaluate instruction selection methods on a range of task types, but the tasks included are generally not highly complex
- What evidence would resolve it: Evaluating instruction selection methods on a diverse set of complex reasoning tasks, such as mathematical problem solving, logical reasoning, or scientific question answering, would provide insights into their performance on more challenging tasks

### Open Question 3
- Question: How do instruction selection methods affect the interpretability and trustworthiness of model outputs?
- Basis in paper: [explicit] The authors mention that prompts sensitive to demonstration permutation often exhibit lower accuracies and are less reliable, particularly in low-resource domains
- Why unresolved: While the paper touches on the reliability of prompts, it does not delve into how instruction selection methods might influence the interpretability or trustworthiness of model outputs
- What evidence would resolve it: Analyzing the outputs of models using different instruction selection methods and assessing their interpretability and trustworthiness through human evaluation would shed light on the impact of instruction choice on these aspects

## Limitations

- Method selection coverage is limited to 7 approaches, excluding other significant instruction selection techniques
- Task diversity may be insufficient, as all tasks come from standard benchmarks without real-world complexity
- Model family representativeness is limited, excluding instruction-tuned models and other significant architectures

## Confidence

- High Confidence: Manual/Curated instructions excel in zero-shot settings while task-agnostic instructions dominate in few-shot settings
- Medium Confidence: Instruction selection methods show limited generalizability across task types and model families
- Low Confidence: Automatic instruction induction methods lack practical utility across all scenarios

## Next Checks

- Cross-Validation on Additional Tasks: Validate findings by evaluating the same instruction methods on at least 5 additional tasks from different domains
- Model Family Expansion: Replicate the evaluation using instruction-tuned models to determine if instruction sensitivity patterns hold for instruction-tuned models
- Real-World Deployment Testing: Conduct a small-scale user study where practitioners apply top-performing instruction methods to their actual use cases, measuring both performance and ease of implementation