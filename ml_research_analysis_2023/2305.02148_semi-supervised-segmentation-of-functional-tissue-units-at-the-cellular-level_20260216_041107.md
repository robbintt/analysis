---
ver: rpa2
title: Semi-Supervised Segmentation of Functional Tissue Units at the Cellular Level
arxiv_id: '2305.02148'
source_url: https://arxiv.org/abs/2305.02148
tags:
- hubmap
- images
- data
- segmentation
- dice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a semi-supervised approach for segmenting
  functional tissue units (FTUs) at the cellular level, aiming to minimize domain
  gaps and class imbalance between the HPA and HubMAP datasets. The method combines
  state-of-the-art deep learning semantic segmentation with domain adaptation and
  semi-supervised learning techniques.
---

# Semi-Supervised Segmentation of Functional Tissue Units at the Cellular Level

## Quick Facts
- arXiv ID: 2305.02148
- Source URL: https://arxiv.org/abs/2305.02148
- Reference count: 0
- Key outcome: Dice score of 0.83419 on private Kaggle test set

## Executive Summary
This paper presents a semi-supervised approach for segmenting functional tissue units (FTUs) at the cellular level, addressing domain gaps and class imbalance between HPA and HuBMAP datasets. The method combines state-of-the-art deep learning semantic segmentation with domain adaptation and semi-supervised learning techniques. By using advanced architectures such as Unet++, Mix Vision Transformer, and EfficientNet, along with heavy data augmentation and pseudo-labeling, the approach achieves near state-of-the-art performance on the private Kaggle test set.

## Method Summary
The approach uses an ensemble of 15 models based on Unet++ and Unet architectures with EfficientNet B7 and Mix Vision Transformer backbones. Key techniques include domain adaptation through pixel size scaling and histogram matching to align HPA and HuBMAP color spaces, heavy color and geometric augmentation, and pseudo-labeling of external GTEx and HPA data. The training pipeline employs Adam optimizer with learning rate scheduling, mixed-precision training, and a composite loss function combining binary cross-entropy, Dice, Focal, and Jaccard losses.

## Key Results
- Achieved Dice score of 0.83419 on private Kaggle test set
- Significantly improved FTU segmentation accuracy across both HPA and HuBMAP datasets
- Demonstrated effectiveness of domain adaptation and semi-supervised learning techniques

## Why This Works (Mechanism)

### Mechanism 1
- Domain adaptation via pixel size scaling and histogram matching aligns HPA and HuBMAP color spaces.
- Resizing training images to match HuBMAP pixel resolution and matching HPA color histograms to HuBMAP reduces the domain gap by normalizing spatial and chromatic discrepancies.
- Core assumption: The color and scale differences between datasets are the dominant sources of model confusion, and normalizing these will make the model invariant to staining protocols.
- Evidence anchors:
  - "We tackled this issue by rescaling our train dataset to the target HuBMAP resolution... we applied histogram matching [38] of all train data to GTEX and HubMAP images."
  - "The color spaces between HPA and HuBMAP datasets were also different due to different stain methods... we decided to make the neural network invariant to color variations by applying heavy color augmentations such as histogram matching [38] to match the color distribution of the training images to that of HuBMAP dataset."

### Mechanism 2
- Pseudo-labeling of external GTEX and HPA data enriches training distribution and improves generalization.
- Adding unannotated images from GTEX (H&E stained) and HPA, then labeling them with a strong ensemble model, augments the training set with diverse examples that approximate the target domain's appearance.
- Core assumption: External data with similar staining protocols captures the variation present in HuBMAP, and pseudo-labels are sufficiently accurate to serve as training targets.
- Evidence anchors:
  - "We used data from GTEX [40] and HPA [19-26] portals to complement the initial training data... We pseudo-labeled both HPA and HuBMAP images with the best ensemble (according to the Cross Validation Score) available at the time of labeling."
  - "From GTEX we downloaded prostate, large intestine, kidneys, and spleen data for patients with no apparent pathologies... Overall, we have added between 57-61K of additional HPA images for each organ."

### Mechanism 3
- Heavy color and geometric augmentation, plus CutMix within classes, forces the model to focus on shape and texture rather than color.
- Applying random hue, contrast, gamma changes and geometric transformations teaches the model to rely on structural cues, while CutMix replaces image regions with same-class patches to encourage robust feature learning.
- Core assumption: Color is not a reliable signal for FTU segmentation across protocols, so training invariance to color improves cross-domain performance.
- Evidence anchors:
  - "To provide additional robustness to scale and geometrical differences in FTU shapes, we also applied a range of geometric augmentations... The main idea behind the color augmentation was to suggest to the model that the color is not important and that it had to look for other features."
  - "We applied it with a probability of 0.5 and used uniform distribution to sample which part of the original image to replace with a patch from a different image. The key trick though was to apply CutMix augmentation within a single class."

## Foundational Learning

- Concept: Domain adaptation and semi-supervised learning.
  - Why needed here: The HPA training set and HuBMAP test set differ in staining protocol and pixel size, creating a domain gap that hurts generalization; semi-supervised techniques help leverage unlabeled or weakly labeled external data.
  - Quick check question: What is the main difference between the HPA and HuBMAP datasets that necessitates domain adaptation?

- Concept: U-Net and nested U-Net architectures for segmentation.
  - Why needed here: These architectures efficiently capture both local and contextual features in medical images, which is essential for delineating complex FTU boundaries.
  - Quick check question: How does the skip-connection design in U-Net benefit segmentation of fine tissue structures?

- Concept: Loss function composition (BCE + Dice + Focal + Jaccard).
  - Why needed here: Combining losses balances pixel-wise accuracy, overlap metrics, class imbalance handling, and gradient stability, which is critical in imbalanced segmentation tasks.
  - Quick check question: Why might using only binary cross-entropy be insufficient for this segmentation problem?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Heavy augmentation (color + geometric + CutMix) -> Encoder-decoder model (U-Net++ or U-Net with EfficientNet B7 or Mix Vision Transformer) -> Multi-loss computation -> Ensemble aggregation

- Critical path:
  1. Load and preprocess images (scale + color match)
  2. Apply augmentations (color + geometric + CutMix)
  3. Forward pass through encoder-decoder model
  4. Compute multi-loss and backpropagate
  5. Aggregate predictions across folds and augmentations

- Design tradeoffs:
  - Choice of backbone (EfficientNet vs. Vision Transformer) trades speed for performance; Vision Transformer improves cross-domain generalization but needs larger crops
  - Loss composition increases training stability but adds complexity; simpler loss might train faster but generalize worse
  - Heavy augmentation improves robustness but risks overfitting to augmentation patterns

- Failure signatures:
  - Domain gap remains: Dice score on HuBMAP is much lower than on HPA in cross-validation
  - Overfitting: High CV score but low private leaderboard score
  - Noisy predictions: Many small disconnected regions after thresholding

- First 3 experiments:
  1. Baseline: Train a single U-Net with EfficientNet B7 on HPA only, evaluate Dice on both HPA and HuBMAP
  2. Add pixel size scaling and histogram matching, retrain, compare Dice improvements on HuBMAP
  3. Introduce heavy color augmentation and CutMix, retrain, measure effect on both cross-validation and HuBMAP performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which tissue thickness variation affects segmentation accuracy, and can this be explicitly modeled or compensated for in the architecture?
- Basis in paper: [inferred] The paper mentions that HPA images have a fixed tissue thickness of 4 µm, while HuBMAP images vary from 4 µm to 10 µm, and the authors chose not to explicitly solve this problem but instead used additional data and pseudo-labeling. However, the exact impact of thickness variation on segmentation performance is not quantified.
- Why unresolved: The paper acknowledges the problem but does not provide a detailed analysis of how tissue thickness affects segmentation accuracy or how pseudo-labeling and additional data compensate for this variation.
- What evidence would resolve it: A controlled experiment comparing segmentation accuracy on datasets with varying tissue thicknesses, and a quantitative analysis of how pseudo-labeling and additional data mitigate thickness-related errors.

### Open Question 2
- Question: How does the choice of specific augmentation techniques (e.g., histogram matching, CutMix) contribute to the overall performance gain, and can their individual effects be isolated?
- Basis in paper: [explicit] The paper describes the use of various augmentation techniques, including histogram matching and CutMix, and provides an ablation study showing performance improvements. However, the specific contribution of each technique is not isolated.
- Why unresolved: While the paper shows that augmentation techniques improve performance, it does not provide a detailed breakdown of how each technique contributes to the overall improvement.
- What evidence would resolve it: An ablation study where each augmentation technique is applied individually and in combination, with a detailed analysis of their individual and combined effects on segmentation accuracy.

### Open Question 3
- Question: What is the optimal strategy for pseudo-labeling in this semi-supervised setting, and how does it compare to other semi-supervised learning approaches?
- Basis in paper: [explicit] The paper describes the use of pseudo-labeling with data from GTEx and HPA portals, and mentions that pseudo-labeling was repeated twice. However, it does not compare this approach to other semi-supervised learning methods or provide a detailed analysis of the optimal pseudo-labeling strategy.
- Why unresolved: The paper does not provide a comprehensive comparison of pseudo-labeling with other semi-supervised learning techniques, nor does it explore the optimal parameters for pseudo-labeling.
- What evidence would resolve it: A comparative study of different semi-supervised learning approaches, including various pseudo-labeling strategies, with a detailed analysis of their performance and the optimal parameters for each method.

## Limitations

- Pseudo-labeling approach lacks specific implementation details (confidence thresholds, sampling strategies) that would allow precise replication
- Exact values for pixel scaling factors used in domain adaptation are referenced but not explicitly provided
- Claims of "near state-of-the-art" performance lack comprehensive comparisons with other published methods on the same test set

## Confidence

- **High Confidence**: Domain adaptation through pixel size scaling and histogram matching is well-supported by results showing improved performance on HuBMAP data. Architectural choices are standard and well-justified.
- **Medium Confidence**: Heavy augmentation strategy and CutMix within classes are supported by ablation studies and improved cross-validation scores, though individual contributions are not isolated.
- **Low Confidence**: Pseudo-labeling of external GTEx and HPA data lacks specific implementation details that would allow precise replication. Performance gains from this approach are supported by results but not independently verifiable.

## Next Checks

1. **Pseudo-label quality validation**: Generate pseudo-labels using different confidence thresholds (0.7, 0.8, 0.9) and measure their impact on final Dice scores to quantify sensitivity to pseudo-label quality.

2. **Domain adaptation ablation**: Train models with and without each domain adaptation component (pixel scaling, histogram matching, color augmentation) on a subset of data to isolate the contribution of each technique to cross-domain performance.

3. **External data contribution**: Compare model performance with and without pseudo-labeled GTEx data to quantify the actual benefit of external data augmentation, using consistent training durations and hyperparameters.