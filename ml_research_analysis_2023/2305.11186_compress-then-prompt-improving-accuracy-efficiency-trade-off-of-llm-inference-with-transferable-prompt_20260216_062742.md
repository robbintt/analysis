---
ver: rpa2
title: 'Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
  with Transferable Prompt'
arxiv_id: '2305.11186'
source_url: https://arxiv.org/abs/2305.11186
tags:
- prompt
- compressed
- llms
- learned
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) are hard to deploy on single GPUs
  due to their size and inference costs. We show that model compression (e.g., pruning,
  quantization) trades accuracy for efficiency.
---

# Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt

## Quick Facts
- **arXiv ID**: 2305.11186
- **Source URL**: https://arxiv.org/abs/2305.11186
- **Reference count**: 40
- **Primary result**: Learned soft prompts improve perplexity of compressed LLaMA-7B models to match uncompressed performance while maintaining efficiency.

## Executive Summary
This paper addresses the challenge of deploying large language models (LLMs) on single GPUs by combining model compression with prompt learning. The authors demonstrate that while compression techniques like pruning and quantization improve efficiency, they degrade model accuracy. They propose learning soft prompts in conjunction with compressed models to restore lost performance without sacrificing the efficiency gains from compression. Their method shows that learned prompts can transfer across different datasets, tasks, and compression levels, with experimental results showing compressed LLaMA-7B models achieving perplexity scores comparable to uncompressed models.

## Method Summary
The approach involves first compressing an LLM using either SparseGPT (for pruning) or GPTQ (for quantization). Then, soft prompts are learned by training additive prompt tokens jointly with the compressed model's weights fixed. The prompt learning process optimizes the prompt embeddings to improve model performance on the target task while maintaining the compressed model's efficiency benefits. The method is evaluated on language modeling tasks using datasets like C4, Wikitext-2, and Penn Treebank, with experiments conducted on various OPT and LLaMA model sizes.

## Key Results
- Learned soft prompts improve compressed LLaMA-7B perplexity to match uncompressed model performance
- Prompts exhibit transferability across datasets (C4, Wikitext-2, PTB) and compression methods
- Hard prompts explicitly informing models of their compressed state can improve performance for some queries
- The addition of prompt tokens does not significantly increase inference latency when they account for less than 10% of the sequence length

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compression-aware prompts restore lost commonsense reasoning in compressed LLMs
- Mechanism: Hard prompts explicitly tell the model about its compressed state, triggering internal correction mechanisms. Learned prompts, optimized alongside compressed weights, adapt more precisely to the model's deficiencies
- Core assumption: The model can infer its compression state from the prompt and adjust its internal representations accordingly
- Evidence anchors:
  - [abstract]: "We demonstrate that adding carefully designed hard prompts can improve compressed model performance for some queries"
  - [section 3.2]: "By explicitly informing the model of its compressed state, LLMs can understand the instruction and magically adjust to generate more relevant responses for some of the questions"
  - [corpus]: Weak evidence. No corpus entries discuss prompt-based recovery of commonsense reasoning in compressed models
- Break condition: If the model's compression artifacts are too severe for even explicit prompts to guide correction, or if the model cannot map the prompt's information to internal adjustments

### Mechanism 2
- Claim: Learned prompts improve accuracy without sacrificing efficiency
- Mechanism: The soft prompt learning method trains additive prompt tokens jointly with the compressed model, enhancing performance while keeping the compressed weights fixed
- Core assumption: The learned prompt can effectively influence the model's attention mechanisms to compensate for compression-induced errors
- Evidence anchors:
  - [abstract]: "We propose a soft prompt learning method that trains prompts in conjunction with compressed models, improving accuracy without sacrificing efficiency"
  - [section 4.2]: "We optimize the following objective... the model parameter θ is fixed and not updated. And the trainable parameters are the embedding of the prompt tokens"
  - [corpus]: Weak evidence. Corpus entries focus on prompt compression for efficiency, not accuracy recovery in compressed models
- Break condition: If the prompt tokens cannot sufficiently influence the model's generation process to overcome compression errors

### Mechanism 3
- Claim: Learned prompts exhibit transferability across datasets, tasks, and compression levels
- Mechanism: Prompts learned on one dataset or compression level can be applied to others, improving performance without retraining
- Core assumption: The learned prompt captures general patterns of compression-induced errors that are consistent across different contexts
- Evidence anchors:
  - [abstract]: "Our learned prompts can be transferred across datasets, tasks, and compression levels"
  - [section 5.4]: "We assess the performance of employing prompts derived from a compressed LLM on other compressed LLMs, employing various compression approaches and levels"
  - [corpus]: Weak evidence. No corpus entries discuss prompt transferability across compression levels
- Break condition: If the prompt's effectiveness is highly dependent on the specific dataset, task, or compression level, limiting its generalizability

## Foundational Learning

- Concept: Prompt engineering and its role in in-context learning
  - Why needed here: Understanding how prompts guide LLM behavior is crucial for designing effective compression-aware prompts
  - Quick check question: How do prompts influence the attention mechanisms and output generation of LLMs?

- Concept: Model compression techniques (pruning, quantization)
  - Why needed here: Familiarity with compression methods is essential to understand how they affect model performance and how prompts can mitigate these effects
  - Quick check question: What are the trade-offs between different compression techniques in terms of accuracy and efficiency?

- Concept: Transfer learning and its limitations
  - Why needed here: Transferability of learned prompts relies on the principles of transfer learning, making it important to understand its capabilities and constraints
  - Quick check question: What factors influence the transferability of learned representations across different tasks or domains?

## Architecture Onboarding

- Component map: Input → Prompt encoder → Compressed LLM (with prompt tokens) → Output
- Critical path: Input → Prompt encoder → Compressed LLM (with prompt tokens) → Output
- Design tradeoffs:
  - Prompt length vs. latency: Longer prompts may improve accuracy but increase inference time
  - Prompt transferability vs. task-specificity: General prompts may be less effective than task-specific ones but offer broader applicability
  - Compression level vs. prompt effectiveness: Higher compression may require more sophisticated prompts to maintain performance
- Failure signatures:
  - Degradation in output quality when using prompts on certain tasks or datasets
  - Increased latency due to longer prompts
  - Inconsistent performance across different compression levels
- First 3 experiments:
  1. Evaluate the impact of hard prompts on a compressed LLM's performance across various tasks
  2. Train soft prompts on a compressed LLM and assess their transferability to different datasets
  3. Investigate the relationship between compression level and the effectiveness of learned prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the learned prompts from one compression method (e.g., pruning) be effectively transferred to another compression method (e.g., quantization) at the same compression level?
- Basis in paper: [inferred] The paper mentions "certain degree of transferability of prompts learned between different compression types, especially when the compression level is less" but does not quantify the effectiveness or provide a comprehensive analysis
- Why unresolved: The paper only provides a qualitative statement about transferability between different compression types without specific metrics or experiments comparing prompt effectiveness across compression methods
- What evidence would resolve it: Experiments comparing perplexity scores of models using prompts learned from different compression methods at the same compression level, with statistical significance testing

### Open Question 2
- Question: What is the relationship between the length of learned prompts and their effectiveness across different compression levels and datasets?
- Basis in paper: [explicit] The paper uses 100 prompt tokens but mentions that "the addition of prompt tokens does not significantly increase the latency of LLM inference, particularly when the inserted tokens account for less than 10% of the original sequence length" suggesting an exploration of prompt length
- Why unresolved: The paper does not systematically investigate how varying prompt lengths affects performance, nor does it determine an optimal prompt length for different scenarios
- What evidence would resolve it: Experiments varying prompt lengths (e.g., 10, 50, 100, 200 tokens) across different compression levels and datasets, measuring both effectiveness and efficiency trade-offs

### Open Question 3
- Question: How do learned prompts affect the robustness of compressed LLMs to adversarial inputs or out-of-distribution data?
- Basis in paper: [inferred] The paper focuses on improving performance on standard benchmarks but does not address robustness to adversarial examples or distribution shifts, which is a critical aspect of real-world deployment
- Why unresolved: The experiments are limited to standard benchmark datasets, and there is no investigation into how prompts affect model behavior under adversarial conditions or with data from different distributions
- What evidence would resolve it: Experiments testing model performance on adversarial examples or out-of-distribution data with and without learned prompts, measuring robustness metrics like attack success rate or accuracy drop

## Limitations

- Weak empirical grounding for the claim that compression-aware prompts restore commonsense reasoning, with only qualitative statements supporting this mechanism
- Limited quantitative evidence for the transferability of prompts across different compression methods and levels
- Incomplete specification of implementation details, making exact reproduction challenging

## Confidence

**High confidence**: Mechanism 2 (learned prompts improving accuracy without efficiency loss) - The methodology of training soft prompts with fixed model weights is well-documented and clearly demonstrates accuracy improvements without efficiency degradation.

**Medium confidence**: Overall approach validity - The core methodology of combining compression with prompt learning is sound, though some implementation details remain unspecified.

**Low confidence**: Mechanism 1 (hard prompts restoring commonsense reasoning) - The "magical adjustment" claim lacks empirical support beyond a single qualitative statement without quantitative validation.

**Low confidence**: Mechanism 3 (transferability across compression levels) - While theoretically appealing, the evidence for cross-compression-level transferability is minimal and the effectiveness of prompts across different compression methods is questionable.

## Next Checks

1. **Ablation study on hard prompt effectiveness**: Systematically remove the compression-awareness component from hard prompts while keeping other elements constant, then measure performance degradation across multiple reasoning tasks. This would validate whether explicit compression state information is actually driving the claimed improvements.

2. **Cross-compression-level transferability validation**: Train prompts on models compressed with one method (e.g., SparseGPT) and evaluate them on models compressed with a different method (e.g., GPTQ) at various sparsity/quantization levels. Measure performance drops to quantify the true limits of prompt transferability.

3. **Prompt length and complexity analysis**: Systematically vary prompt length and complexity while measuring both accuracy improvements and latency increases. This would establish the optimal efficiency-accuracy trade-off and validate the claim that prompts improve accuracy "without sacrificing efficiency" across different use cases.