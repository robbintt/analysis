---
ver: rpa2
title: Steering Language Models With Activation Engineering
arxiv_id: '2308.10248'
source_url: https://arxiv.org/abs/2308.10248
tags:
- steering
- vector
- actadd
- activation
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces activation engineering as a method for steering
  large language model (LLM) outputs at inference time by modifying intermediate activations.
  The authors present Activation Addition (ActAdd), a technique that computes steering
  vectors by taking differences between activation patterns elicited by contrasting
  prompt pairs (e.g., "I love weddings" vs "I hate weddings").
---

# Steering Language Models With Activation Engineering

## Quick Facts
- arXiv ID: 2308.10248
- Source URL: https://arxiv.org/abs/2308.10248
- Authors: 
- Reference count: 31
- One-line primary result: Activation Addition (ActAdd) achieves state-of-the-art performance on sentiment shift and detoxification by adding steering vectors to intermediate activations

## Executive Summary
This paper introduces activation engineering as a method for steering large language model outputs at inference time by modifying intermediate activations. The authors present Activation Addition (ActAdd), a technique that computes steering vectors by taking differences between activation patterns elicited by contrasting prompt pairs. By adding these vectors to the forward pass, ActAdd achieves state-of-the-art performance on tasks like sentiment shift and detoxification using models including LLaMA-3 and OPT.

## Method Summary
Activation Addition (ActAdd) computes steering vectors by taking activation differences between contrasting prompt pairs (e.g., "I love weddings" vs "I hate weddings"). These vectors are then added to intermediate activations during the forward pass, with an injection coefficient that controls the magnitude of the steering effect. The method requires no optimization or labeled data, works with just two prompt samples, and scales naturally with model size. Experiments show ActAdd provides natural-language control over output properties while preserving performance on off-target tasks, with minimal computational overhead compared to fine-tuning or RLHF approaches.

## Key Results
- ActAdd achieves SOTA on negative-to-positive sentiment shift using LLaMA-3 and OPT models
- The method requires only two prompt samples and no labeled data or optimization
- ActAdd preserves performance on off-target tasks like ConceptNet knowledge retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding steering vectors at intermediate layers shifts model behavior toward target properties
- Mechanism: The steering vector captures feature differences between contrasting prompts, and adding it to intermediate activations biases subsequent computation toward those features
- Core assumption: Model representations are sufficiently linear for feature directions to transfer across different contexts
- Evidence anchors: 
  - [abstract] "By tactically adding in e.g. the 'Love' - 'Hate' steering vector during the forward pass, we achieve SOTA on negative-to-positive sentiment shift"
  - [section] "Adding h+ alone is less effective... hence the use of a counterbalanced prompt p− to help implicitly specify the desired direction"
  - [corpus] "Unlike past work which learned these steering vectors... our Activation Addition (ActAdd) method computes them by taking the activation differences that result from pairs of prompts"

### Mechanism 2
- Claim: The steering vector's effect compounds through the residual stream architecture
- Mechanism: Adding the steering vector at layer l modifies the input to all subsequent layers, causing a cascade of changes that reinforce the target behavior
- Core assumption: The residual stream allows later layers to amplify and refine the steering signal
- Evidence anchors:
  - [abstract] "ActAdd yields inference-time control over high-level output properties"
  - [section] "We find that, as expected from past work, intervening at middle layers is most effective"
  - [corpus] "We see that the intervention... rises in effectiveness until l = 6, and then declines"

### Mechanism 3
- Claim: The steering vector represents a stable direction in activation space that generalizes across contexts
- Mechanism: The difference vector between contrasting prompts captures a consistent feature representation that persists even when applied to different user prompts
- Core assumption: The same feature direction can be used to influence model behavior across different contexts and prompts
- Evidence anchors:
  - [abstract] "ActAdd works with just two prompt samples"
  - [section] "Interestingly, our steering vectors are not specified by taking the difference between desired outputs... Both prompts (p+, p−) are (say) wedding-related"
  - [corpus] "Our hypothesis... is more specific: that neural networks represent features of the input as directions in activation space"

## Foundational Learning

- Concept: Residual stream architecture in Transformers
  - Why needed here: Understanding how activation additions propagate through the network is crucial for grasping why ActAdd works
  - Quick check question: What happens to a steering vector added at layer l as it flows through subsequent layers?

- Concept: Linear feature representations in neural networks
  - Why needed here: ActAdd assumes features are represented as directions in activation space that can be manipulated via vector addition
  - Quick check question: Why does adding a steering vector work better than just using one of the original prompts?

- Concept: Tokenization and embedding space
  - Why needed here: Understanding how tokens map to vectors and how steering vectors differ from simple token injection is important for distinguishing ActAdd from prompting
  - Quick check question: How does ActAdd differ from prepending a token to the input prompt?

## Architecture Onboarding

- Component map: Transformer layers → Residual streams → Multi-head attention → Feed-forward networks → Output logits
- Critical path: Forward pass through layers → Extract activations at layer l → Add steering vector → Continue forward pass → Generate output
- Design tradeoffs: Adding at earlier layers affects more subsequent computation but may require larger coefficients; adding at later layers requires smaller coefficients but affects less computation
- Failure signatures: Broken syntax or incoherent output when injection coefficient is too large; no steering effect when coefficient is too small; steering works only for similar prompts
- First 3 experiments:
  1. Test ActAdd with a simple sentiment vector (Love-Hate) at different layers and coefficients
  2. Measure perplexity changes for wedding-related vs. unrelated text under ActAdd
  3. Test whether adding random vectors produces similar steering effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do activation engineering techniques like ActAdd scale to more complex cognitive tasks like reasoning or planning?
- Basis in paper: [inferred] The paper notes that GPT-2 (1.5B parameters) is not sophisticated enough to demonstrate ActAdd's effect on reasoning tasks, suggesting limitations for more complex cognitive work.
- Why unresolved: The experiments were limited to GPT-2, which is too small for sophisticated reasoning. The paper speculates about future models but doesn't provide evidence.
- What evidence would resolve it: Testing ActAdd on larger language models (e.g., GPT-3, LLaMA-2) with reasoning benchmarks would show whether the technique scales to more complex cognitive tasks.

### Open Question 2
- Question: What is the precise mechanism by which ActAdd influences model behavior without degrading general performance?
- Basis in paper: [inferred] The paper shows ActAdd preserves performance on off-target tasks like ConceptNet, but doesn't explain why steering vectors don't interfere with general knowledge retrieval.
- Why unresolved: While the paper demonstrates preservation of general performance, it doesn't explain the theoretical basis for why adding steering vectors doesn't corrupt other model capabilities.
- What evidence would resolve it: Detailed mechanistic analysis showing which model components are affected by ActAdd and how general knowledge pathways remain intact would clarify the mechanism.

### Open Question 3
- Question: How can we systematically identify effective prompt pairs (p+, p-) for new steering objectives without extensive trial and error?
- Basis in paper: [explicit] The paper states that finding suitable prompt pairs currently requires searching over prompt pairs, which makes it less user-friendly than prompt engineering.
- Why unresolved: The paper demonstrates that ActAdd works with as few as two samples but doesn't provide a systematic method for generating effective prompt pairs for arbitrary objectives.
- What evidence would resolve it: A method for automatically generating or selecting optimal prompt pairs based on target objectives would eliminate the need for manual trial and error.

## Limitations

- Architecture Generalization: The method's effectiveness across diverse architectures (e.g., non-Transformer models) remains uncertain
- Robustness to Prompt Diversity: The method's sensitivity to prompt pair selection and semantic similarity is not fully characterized
- Computational Overhead: The actual memory and latency impact of recording activations at intermediate layers was not quantified

## Confidence

**High Confidence**: The basic mechanism of computing steering vectors from activation differences and adding them during inference is well-supported by experimental results.

**Medium Confidence**: The claim that ActAdd provides "natural-language control" over output properties is supported by qualitative examples but would benefit from more rigorous user studies.

**Medium Confidence**: The assertion that ActAdd "scales naturally with model size" is based on experiments with different model sizes but lacks systematic analysis of scaling laws.

## Next Checks

1. **Cross-Architecture Transferability Test**: Apply ActAdd steering vectors derived from GPT-2 to LLaMA and OPT models, and vice versa, measuring both steering effectiveness and unintended task interference to validate architecture-agnostic claims.

2. **Prompt Pair Robustness Analysis**: Systematically vary the semantic similarity between steering prompt pairs (p+ and p-) and measure the resulting steering effectiveness across diverse downstream tasks to quantify the method's sensitivity to prompt selection.

3. **Computational Overhead Benchmarking**: Implement ActAdd on production-grade models and measure end-to-end latency, memory usage, and throughput compared to baseline inference, particularly focusing on the impact of activation recording at different intermediate layers.