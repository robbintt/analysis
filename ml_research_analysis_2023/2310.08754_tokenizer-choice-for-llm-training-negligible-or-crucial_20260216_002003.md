---
ver: rpa2
title: 'Tokenizer Choice For LLM Training: Negligible or Crucial?'
arxiv_id: '2310.08754'
source_url: https://arxiv.org/abs/2310.08754
tags:
- tokenizer
- tokenizers
- uni00000013
- performance
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the influence of tokenizer choice on the
  downstream performance of large language models (LLMs). The authors conduct a comprehensive
  study by training 24 monolingual and multilingual LLMs with varying tokenizer algorithms,
  parameterizations, and vocabulary sizes.
---

# Tokenizer Choice For LLM Training: Negligible or Crucial?

## Quick Facts
- arXiv ID: 2310.08754
- Source URL: https://arxiv.org/abs/2310.08754
- Authors: 
- Reference count: 40
- Key outcome: This work investigates the influence of tokenizer choice on the downstream performance of large language models (LLMs). The authors conduct a comprehensive study by training 24 monolingual and multilingual LLMs with varying tokenizer algorithms, parameterizations, and vocabulary sizes. Their results show that tokenizer choice significantly impacts model performance, training and inference costs. Contrary to common belief, common tokenizer evaluation metrics like fertility and parity are not always predictive of model downstream performance. The study highlights the importance of carefully selecting tokenizers for LLM training, especially for multilingual settings, to avoid severe performance degradation and additional computational costs.

## Executive Summary
This paper investigates the critical role of tokenizer selection in large language model training through an extensive empirical study. The authors train 24 monolingual and multilingual LLMs using different tokenizer algorithms (BPE and Unigram), vocabulary sizes, and parameterizations to systematically evaluate their impact on downstream performance. Their results demonstrate that tokenizer choice significantly affects both model performance and computational costs, with monolingual English tokenizers causing severe degradation when applied to multilingual data. The study challenges conventional wisdom by showing that common tokenizer evaluation metrics like fertility and parity do not reliably predict downstream model performance.

## Method Summary
The authors conduct a comprehensive empirical study by training 24 tokenizers (12 monolingual English and 12 multilingual) using BPE and Unigram algorithms with vocabulary sizes of 33k, 50k, 82k, and 100k tokens. For each tokenizer, they train a 2.6B parameter decoder-only transformer model on 52B tokens from two datasets (monolingual English and multilingual European languages). The models are evaluated on zero-shot downstream tasks covering commonsense reasoning, reading comprehension, translation, and math. The study measures both intrinsic tokenizer metrics (fertility and parity) and extrinsic performance metrics, along with computational costs per word during training and inference.

## Key Results
- Monolingual English tokenizers applied to multilingual data cause severe downstream performance degradation and increase training costs by up to 68%
- Multilingual tokenizers require approximately three times larger vocabulary sizes than monolingual English tokenizers for optimal performance
- Common tokenizer evaluation metrics (fertility and parity) do not reliably predict downstream model performance
- Tokenizer choice significantly impacts computational costs during both training and inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Monolingual English tokenizers applied to multilingual data severely degrade downstream performance.
- Mechanism: Monolingual tokenizers lack sufficient vocabulary coverage for non-English languages, resulting in excessive tokenization into subwords, increasing sequence length and reducing model capacity for long-range dependencies.
- Core assumption: The tokenizer's vocabulary must adequately represent all languages in the dataset for optimal model performance.
- Evidence anchors:
  - [abstract] "While English-only tokenizers have been applied to the training of multi-lingual LLMs in the past, we find that this approach results in a severe downstream performance degradation and additional training costs of up to 68%"
  - [section] "Firstly, it can be observed that applying a monolingual tokenizer to multilingual data results in significantly higher fertility and parity scores"
  - [corpus] Weak - no direct corpus evidence provided, but supported by tokenizer vocabulary analysis
- Break condition: When vocabulary size is sufficiently large to cover multiple languages, or when using a multilingual tokenizer trained on balanced multilingual data.

### Mechanism 2
- Claim: Larger vocabulary sizes reduce fertility and computational costs for multilingual tokenizers but not for monolingual English tokenizers.
- Mechanism: Multilingual documents contain more diverse vocabulary, requiring larger token vocabularies to achieve low fertility. Once English tokenizers reach ~33k vocabulary, additional tokens provide minimal fertility improvement but increase computational costs.
- Core assumption: Vocabulary size requirements scale with language diversity in the training corpus.
- Evidence anchors:
  - [abstract] "we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary size increases of factor three in comparison to English"
  - [section] "with increasing vocabulary size, fertility and parity reduce in all cases" and "for monolingual English tokenizers, the fertility is less dependent on the vocabulary when tokenizing English documents, implying that 33k might be a sufficiently large vocabulary"
  - [corpus] Weak - corpus evidence would require vocabulary distribution analysis across languages
- Break condition: When vocabulary reaches saturation point for the specific language(s) being tokenized.

### Mechanism 3
- Claim: Fertility and parity metrics are not reliable predictors of downstream model performance.
- Mechanism: Low fertility and high parity indicate efficient tokenization but don't capture the quality of morphological segmentations or semantic information preservation, which are crucial for downstream tasks.
- Core assumption: Intrinsic tokenizer metrics don't fully capture the impact of tokenization quality on model learning capabilities.
- Evidence anchors:
  - [abstract] "we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable choice for tokenizer evaluation"
  - [section] "we find that there is no distinct correlation across all tasks and languages" and "intrinsic tokenizer metrics such as fertility and parity need to be taken with a grain of salt and supposedly are only predictive of downstream model performance in certain bounds"
  - [corpus] Weak - requires corpus-level analysis of task-specific performance vs. intrinsic metrics correlation
- Break condition: When developing task-specific evaluation metrics that better capture the relationship between tokenization quality and downstream performance.

## Foundational Learning

- Concept: Tokenizer algorithms (BPE vs. Unigram)
  - Why needed here: Different algorithms have distinct properties - BPE builds vocabularies greedily while Unigram uses probabilistic sampling, affecting their performance on morphologically rich languages
  - Quick check question: Why might Unigram tokenizers perform better than BPE for inflected languages like German or French?

- Concept: Vocabulary size scaling laws
  - Why needed here: Understanding how vocabulary size requirements scale with language diversity is crucial for efficient multilingual model training
  - Quick check question: Why does increasing vocabulary size have diminishing returns for English tokenizers but significant impact for multilingual tokenizers?

- Concept: Computational cost modeling
  - Why needed here: Tokenization affects sequence length, which directly impacts training and inference computational costs in transformer models
  - Quick check question: How does sequence length affect the computational complexity of transformer attention mechanisms?

## Architecture Onboarding

- Component map:
  - Dataset preparation and language balancing
  - Tokenizer training with varying algorithms and vocabularies
  - Model architecture (2.6B parameter decoder-only transformer)
  - Training infrastructure (52B tokens, scaling law-based)
  - Evaluation framework (intrinsic metrics + downstream tasks)

- Critical path:
  1. Dataset preparation and language balancing
  2. Tokenizer training with varying algorithms and vocabularies
  3. Model training for each tokenizer configuration
  4. Intrinsic evaluation (fertility, parity, vocabulary overlap)
  5. Extrinsic evaluation (downstream task performance)
  6. Computational cost analysis

- Design tradeoffs:
  - Vocabulary size vs. computational efficiency
  - Algorithm choice vs. language-specific performance
  - Training data diversity vs. tokenizer quality
  - Model size vs. training/inference costs

- Failure signatures:
  - High fertility scores indicating poor tokenization
  - Large parity discrepancies across languages
  - Poor downstream task performance despite good intrinsic metrics
  - Unexpected computational cost increases

- First 3 experiments:
  1. Compare fertility and parity scores for monolingual vs. multilingual tokenizers on their respective datasets
  2. Measure downstream performance differences between BPE and Unigram tokenizers for English and multilingual settings
  3. Analyze computational cost variations across different vocabulary sizes and tokenizer configurations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do intrinsic metrics like fertility and parity correlate with downstream performance for specific language families (e.g., agglutinative vs. fusional languages) across different vocabulary sizes?
- Basis in paper: [explicit] The paper notes that fertility and parity do not consistently predict downstream performance and hypothesizes this may depend on vocabulary size and language characteristics.
- Why unresolved: The study tested a broad set of languages but did not systematically analyze performance differences within specific language families or morphological types.
- What evidence would resolve it: Detailed analysis of performance across language families with varying vocabulary sizes, explicitly linking morphological properties to intrinsic metric correlations.

### Open Question 2
- Question: What is the impact of tokenizer choice on inference costs for sequence lengths beyond the 2048 limit used in this study?
- Basis in paper: [inferred] The paper discusses computational costs for training and inference at a fixed sequence length but does not explore the effects of longer sequences.
- Why unresolved: The study's fixed sequence length of 2048 may not reflect real-world scenarios where longer sequences are common, and the impact of tokenizer choice on these scenarios is unknown.
- What evidence would resolve it: Experiments comparing inference costs across different tokenizers at various sequence lengths, particularly for longer sequences.

### Open Question 3
- Question: How does the choice of tokenizer affect the model's ability to generalize to languages not included in the training corpus?
- Basis in paper: [inferred] The study focuses on five European languages but does not address the model's performance on unseen languages.
- Why unresolved: The paper's multilingual experiments are limited to specific languages, leaving open the question of how well models generalize to new languages with different tokenizers.
- What evidence would resolve it: Training and evaluation on a diverse set of languages, including those not present in the tokenizer training data, to assess cross-linguistic generalization.

## Limitations
- Limited linguistic diversity: The study only examines five European languages, preventing generalization to languages with different morphological characteristics or non-Latin scripts
- Reliance on imperfect evaluation metrics: Intrinsic metrics like fertility and parity show inconsistent correlation with downstream performance, suggesting they may not capture the full impact of tokenization quality
- Single architecture constraint: Computational cost analysis is based on one specific model architecture (2.6B parameter decoder-only transformer), which may not generalize to other architectures

## Confidence
- High Confidence: Monolingual English tokenizers severely degrade multilingual model performance and increase computational costs
- Medium Confidence: Multilingual tokenizers require approximately three times larger vocabularies than monolingual English tokenizers
- Low Confidence: The study doesn't adequately address how tokenizer choice affects model training dynamics beyond final performance metrics

## Next Checks
1. **Cross-linguistic validation**: Replicate the study with a more diverse language set including at least one agglutinative language and one non-Latin script language to test the generalizability of the three-factor vocabulary scaling rule.

2. **Task-specific correlation analysis**: Conduct a detailed correlation analysis between intrinsic tokenizer metrics and downstream performance for individual tasks to identify which tasks are most sensitive to tokenization quality and why.

3. **Training dynamics study**: Monitor and analyze model training convergence speed, stability, and loss curves across different tokenizer configurations to understand how tokenization affects the learning process itself, not just final performance.