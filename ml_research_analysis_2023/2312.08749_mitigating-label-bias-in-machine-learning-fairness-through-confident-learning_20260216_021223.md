---
ver: rpa2
title: 'Mitigating Label Bias in Machine Learning: Fairness through Confident Learning'
arxiv_id: '2312.08749'
source_url: https://arxiv.org/abs/2312.08749
tags:
- bias
- learning
- fairness
- label
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses label bias in machine learning, where sensitive
  attributes can skew labels, leading to unfair outcomes. The authors propose a method
  that uses confident learning to filter out the fairest instances, thereby mitigating
  bias.
---

# Mitigating Label Bias in Machine Learning: Fairness through Confident Learning

## Quick Facts
- **arXiv ID**: 2312.08749
- **Source URL**: https://arxiv.org/abs/2312.08749
- **Reference count**: 15
- **Primary result**: Method consistently achieves lowest fairness violations and test errors compared to baselines including confident learning, LongReMix, label correction, and group peer loss

## Executive Summary
This paper addresses label bias in machine learning where sensitive attributes can skew labels, leading to unfair outcomes. The authors propose a method that uses confident learning to filter out the fairest instances, thereby mitigating bias. Their approach involves training two models on subsets of data split by sensitive attributes, extending confidence intervals with a robust mean estimator, and using a co-teaching paradigm for cross-validation. The method is model-agnostic and focuses on data selection rather than modifying the model itself.

## Method Summary
The method trains two models (θA and θB) on subsets of data split by sensitive attributes (DA and DB), extends confidence intervals using a robust mean estimator (ψ(x) = log(1 + x + x²/2)), employs a co-teaching paradigm for cross-validation, and filters fairest instances based on extended confidence intervals. This approach estimates the joint distribution between corrupted labels and true labels using confident learning, enabling targeted filtering of biased instances without requiring access to true labels. The method is designed to be model-agnostic and focuses on data selection rather than modifying the model architecture itself.

## Key Results
- Consistently achieves lowest fairness violations across multiple metrics (Demographic Parity Distance, DEO, p%) compared to baseline methods
- Maintains competitive test accuracy while improving fairness metrics
- Demonstrates effectiveness on both synthetic and real-world datasets (Adult, COMPAS, Credit Loan Data, Law School Admissions)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Filtering instances based on robust confidence intervals mitigates selection bias caused by low confidence scores from underrepresented groups
- **Mechanism**: Extends probabilistic threshold using truncation function ψ(x) = log(1 + x + x²/2) to dampen influence of extremely high confidence scores, creating wider confidence intervals that include more instances from disadvantaged groups
- **Core assumption**: Low confidence scores in underrepresented groups reflect systemic disadvantage rather than labeling errors
- **Break condition**: If truncation function doesn't sufficiently expand confidence interval to include representative samples from disadvantaged groups

### Mechanism 2
- **Claim**: Training two separate models on subsets split by sensitive attributes enables cross-validation that reduces selection bias
- **Mechanism**: Trains θA on DA and θB on DB, then evaluates both models on entire dataset D, selecting instances based on whichever model's confidence score exceeds its threshold
- **Core assumption**: Models trained on separate demographic groups capture different aspects of data distribution, providing more balanced view when combined
- **Break condition**: If models become too correlated or one model dominates selection process

### Mechanism 3
- **Claim**: Using confident learning to estimate joint distribution between corrupted and true labels enables targeted filtering of biased instances
- **Mechanism**: Estimates Qy=j,z=i using confident joint matrix ¯Cy=j,z=i, which calibrates for class imbalance and identifies off-diagonal elements representing instances where observed labels differ from true labels
- **Core assumption**: Confident learning framework can accurately estimate relationship between observed biased labels and underlying true labels using only biased dataset
- **Break condition**: If confident learning estimation fails to accurately capture label corruption pattern

## Foundational Learning

- **Concept**: Confident Learning and Noise Estimation
  - **Why needed**: Method relies on estimating joint distribution between corrupted and true labels to identify which instances contain bias
  - **Quick check**: How does confident learning estimate probability that observed label y corresponds to true label z?

- **Concept**: Fairness Metrics and Demographic Parity
  - **Why needed**: Approach evaluates success using fairness metrics like demographic parity distance and equal opportunity difference
  - **Quick check**: What is the difference between demographic parity and equal opportunity as fairness metrics?

- **Concept**: Truncation Functions and Robust Statistics
  - **Why needed**: Method uses specific truncation function ψ(x) = log(1 + x + x²/2) to extend confidence intervals
  - **Quick check**: Why would logarithmic truncation function be more robust than simple averaging for confidence score thresholds?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Model training -> Confidence estimation -> Selection mechanism -> Final training
- **Critical path**: 
  1. Split data by sensitive attribute
  2. Train θA on DA and θB on DB
  3. Compute confidence scores and extended thresholds
  4. Build confident joint matrices for both models
  5. Select instances not in off-diagonal elements
  6. Train final model on selected data
- **Design tradeoffs**: Two separate models increase computational cost but provide better bias detection; truncation function choice affects sensitivity to noise vs. bias; hyperparameter Ns balances bias removal and data retention
- **Failure signatures**: High fairness violations despite low test error (model predicting majority class); low test error but high fairness violations (insufficient bias removal); very low test error and very low fairness violations (potential data leakage)
- **First 3 experiments**:
  1. Run on synthetic data with known bias parameters to verify method correctly identifies and removes biased instances
  2. Compare performance with and without truncation function on imbalanced datasets
  3. Test sensitivity to hyperparameter Ns by sweeping values and observing fairness-performance tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed method perform on datasets with multiple sensitive attributes (e.g., gender and race simultaneously)?
- **Basis**: Paper only evaluates on datasets with single sensitive attribute, leaving multi-attribute scenarios unexplored
- **What evidence would resolve it**: Testing method on datasets like Adult Income or COMPAS with multiple sensitive attributes and comparing fairness metrics across different attribute combinations

### Open Question 2
- **Question**: What is the computational overhead of the proposed method compared to baseline approaches, especially on large-scale datasets?
- **Basis**: Paper does not provide runtime or computational efficiency comparisons with baseline methods
- **What evidence would resolve it**: Benchmarking method's training and inference time on large-scale datasets (e.g., ImageNet) and comparing to baselines

### Open Question 3
- **Question**: How sensitive is the method to the choice of the truncation function ψ(x) in Eq. (4)? Would alternative robust mean estimators improve performance?
- **Basis**: Paper uses ψ(x) = log(1 + x + x²/2) but does not explore other influence functions or their impact on fairness or accuracy
- **What evidence would resolve it**: Experimenting with alternative influence functions (e.g., Huber loss, Tukey's biweight) and analyzing their effect on fairness violations and test error across multiple datasets

### Open Question 4
- **Question**: Can the method be extended to handle semi-supervised or unsupervised settings where true labels are entirely absent?
- **Basis**: Method assumes access to biased labels and sensitive attributes but does not address scenarios where labels are missing or unsupervised
- **What evidence would resolve it**: Adapting method to semi-supervised learning frameworks and evaluating performance on unlabeled datasets

## Limitations

- The truncation function ψ(x) = log(1 + x + x²/2) is presented without theoretical justification for why this specific form optimally balances bias removal and data retention
- Selection of hyperparameter Ns, which controls number of instances removed, appears critical but lacks clear guidance on how to set this value for different datasets
- Method assumes confident learning can accurately estimate relationship between observed biased labels and true labels, but accuracy under various bias scenarios is not thoroughly explored

## Confidence

- **High confidence**: Experimental results showing improved fairness metrics compared to baseline methods are well-supported with multiple datasets and evaluation metrics
- **Medium confidence**: Mechanism by which co-teaching between models trained on different demographic groups reduces selection bias is theoretically sound but could benefit from more empirical validation
- **Medium confidence**: Approach of using confident learning to estimate label corruption patterns is reasonable but accuracy of these estimates under different types of bias is not thoroughly explored

## Next Checks

1. **Truncation function sensitivity**: Systematically vary truncation function parameters and evaluate how this affects both fairness metrics and test accuracy to determine optimal settings for different bias types

2. **Confident learning estimation accuracy**: Compare confident learning-estimated joint distribution with ground truth on synthetic data with known corruption patterns to validate estimation accuracy across different noise levels

3. **Co-teaching correlation analysis**: Measure correlation between θA and θB models during training to quantify actual benefit of cross-validation and identify when approach provides diminishing returns