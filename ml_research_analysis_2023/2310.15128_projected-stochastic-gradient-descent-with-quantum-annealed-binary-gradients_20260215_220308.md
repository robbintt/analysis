---
ver: rpa2
title: Projected Stochastic Gradient Descent with Quantum Annealed Binary Gradients
arxiv_id: '2310.15128'
source_url: https://arxiv.org/abs/2310.15128
tags:
- binary
- quantum
- training
- neural
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QP-SBGD, a novel hybrid classical-quantum
  optimizer for training binary neural networks (BNNs) and binary graph neural networks
  (BGNNs) on quantum hardware. The method projects real-valued gradients onto binary
  variables by solving a Quadratic Unconstrained Binary Optimization (QUBO) problem
  on a quantum annealer, enabling scalable, layer-wise training.
---

# Projected Stochastic Gradient Descent with Quantum Annealed Binary Gradients

## Quick Facts
- arXiv ID: 2310.15128
- Source URL: https://arxiv.org/abs/2310.15128
- Reference count: 40
- Key outcome: QP-SBGD achieves O(1/√T) convergence and superior accuracy on binary classification tasks by projecting gradients onto binary variables via quantum-annealed QUBO problems.

## Executive Summary
This paper introduces QP-SBGD, a hybrid classical-quantum optimizer for training binary neural networks (BNNs) and binary graph neural networks (BGNNs). The method projects real-valued gradients onto binary variables by solving a Quadratic Unconstrained Binary Optimization (QUBO) problem on a quantum annealer, enabling scalable, layer-wise training. Under mild assumptions, the authors prove convergence rates of O(1/√T) for the continuous variant and convergence to a fixed point for the projected variant. Empirical results show that QP-SBGD outperforms or matches state-of-the-art baselines such as BinaryConnect, signSGD, and ProxQuant on tasks including the Rosenbrock function minimization, binary logistic regression, binary MLP classification on MNIST, and binary GCN training on citation graphs. Notably, the method achieves superior accuracy on binary classification tasks and demonstrates robustness to batch size and learning rate variations.

## Method Summary
QP-SBGD is a layer-wise stochastic optimizer that projects real-valued gradients onto binary variables by solving a QUBO problem on a quantum annealer. The method consists of a classical preprocessing step to compute real-valued gradients and Jacobian maps, followed by a quantum module to solve the QUBO problems and obtain binary gradient projections, and a classical update step to apply the binary gradient projections to update weights layer-by-layer. The algorithm is implemented in a hybrid loop, where the forward pass, loss computation, gradient calculation, Jacobian formation, QUBO construction, quantum annealing, binary projection, and weight update are repeated iteratively.

## Key Results
- QP-SBGD achieves O(1/√T) convergence rate for the continuous variant and convergence to a fixed point for the projected variant under mild assumptions.
- The method outperforms or matches state-of-the-art baselines such as BinaryConnect, signSGD, and ProxQuant on binary classification tasks, including MNIST and citation graphs.
- QP-SBGD demonstrates robustness to batch size and learning rate variations, and is scalable to larger networks on resource-limited quantum hardware through layer-wise training.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The binary projection of gradients via QUBO formulation preserves sufficient directional information for convergence.
- **Mechanism**: Instead of binarizing weights post-training or using simple sign projections, the method maps the gradient to binary variables by solving a QUBO problem. This QUBO form is compatible with quantum annealing, enabling the search over discrete solutions to be delegated to quantum hardware.
- **Core assumption**: The binary gradient approximation (via the binary map Π) still points in a direction sufficiently aligned with the true gradient so that the learning process can make progress.
- **Evidence anchors**:
  - [abstract] "QP-SBGD approximately maps the gradient onto binary variables, by solving a quadratic constrained binary optimisation."
  - [section] "We empirically validate our assumptions required to prove these results, which we demonstrate are milder compared to the assumptions made in existing algorithms such as signSGD [58]."
  - [corpus] Weak/no evidence: No corpus neighbor discusses the specific QUBO gradient projection approach or its convergence properties.
- **Break condition**: If the binary gradient approximation becomes statistically uncorrelated with the true gradient (e.g., in highly non-convex regions), the method could stall.

### Mechanism 2
- **Claim**: Layer-wise training makes the method scalable to larger networks on resource-limited quantum hardware.
- **Mechanism**: Instead of solving one monolithic QUBO for the entire network, the algorithm computes weight updates layer-by-layer and incrementally over data batches. This reduces the size of each QUBO problem, enabling practical deployment on current quantum annealers with limited qubit counts.
- **Core assumption**: Decomposing the problem into smaller QUBOs per layer does not significantly degrade the overall optimization trajectory.
- **Evidence anchors**:
  - [abstract] "Our algorithm is implemented layer-wise, making it suitable to train larger networks on resource-limited quantum hardware."
  - [section] "As quantum hardware resources are limited, closed-form, one-shot approaches...fail to train even modest-sized neural networks. As a remedy, we show how to deploy QP-SBGD scalably, by computing weight updates layer-wise..."
  - [corpus] Weak/no evidence: Corpus neighbors do not address layer-wise decomposition for quantum-annealed neural network training.
- **Break condition**: If inter-layer dependencies are strong, layer-wise updates may cause instability or slow convergence.

### Mechanism 3
- **Claim**: Convergence proofs hold under milder assumptions than existing binary optimizers.
- **Mechanism**: The theoretical framework assumes the Co-Directionality Probability (CDP) condition, which is empirically verified to hold more often than the Success Probability (SP) assumption in signSGD. This allows proving O(1/√T) convergence for the continuous variant and convergence to a fixed point for the projected variant.
- **Core assumption**: The binary projection direction aligns with the true gradient with probability greater than 1/2.
- **Evidence anchors**:
  - [abstract] "Under practically reasonable assumptions, we show that this update rule converges with a rate of O(1/√T)."
  - [section] "Drawing connections to the state-of-the-art binary gradient optimisers such as signSGD [58], we ensure that the convergence rate of (i) is O(1/√T), and for (ii), we show that if a fixed point exists in the space of binary variables, our algorithm will converge to it."
  - [corpus] Weak/no evidence: Corpus neighbors do not discuss convergence proofs or assumptions for quantum-annealed or binary gradient methods.
- **Break condition**: If the CDP assumption fails in practice (e.g., gradients become nearly orthogonal to their binary projections), convergence guarantees no longer apply.

## Foundational Learning

- **Concept**: Quadratic Unconstrained Binary Optimization (QUBO)
  - Why needed here: The gradient projection step is formulated as a QUBO problem, which is the native input format for quantum annealers like D-Wave.
  - Quick check question: Given a matrix U and vector v, how would you construct the QUBO formulation for the binary projection arg min_g ||v - g^T U||^2?

- **Concept**: Binary Neural Networks (BNNs)
  - Why needed here: The method trains neural networks with binary weights, reducing computational cost and enabling direct deployment on binary hardware.
  - Quick check question: What is the key difference between training a BNN with signSGD versus using the proposed QP-SBGD method?

- **Concept**: Quantum Annealing
  - Why needed here: Quantum annealing is used to solve the QUBO problems that arise from projecting gradients onto binary variables, enabling efficient exploration of the discrete solution space.
  - Quick check question: Why is quantum annealing particularly well-suited for solving QUBO problems compared to classical methods?

## Architecture Onboarding

- **Component map**: Classical preprocessing -> Quantum module (QUBO solving) -> Classical update -> Hybrid loop

- **Critical path**: Forward pass -> Loss computation -> Gradient calculation -> Jacobian formation -> QUBO construction -> Quantum annealing -> Binary projection -> Weight update -> Next iteration

- **Design tradeoffs**:
  - Quantum vs. classical: Quantum annealing can find high-quality discrete solutions but is limited by hardware size and noise; classical simulated annealing is more accessible but may yield lower-quality solutions.
  - Layer-wise vs. global: Layer-wise decomposition enables scalability but may slow convergence if inter-layer dependencies are strong.
  - Batch size: Larger batches provide more stable gradient estimates but increase QUBO size and quantum hardware demands.

- **Failure signatures**:
  - Convergence stalls: Likely due to poor binary gradient alignment (CDP assumption failing).
  - High variance in accuracy: Often caused by quantum hardware noise or suboptimal QUBO solutions.
  - Out-of-memory errors: Can occur if QUBO sizes exceed quantum hardware capacity; indicates need for smaller layers or batching.

- **First 3 experiments**:
  1. **Rosenbrock minimization**: Validate that the method can optimize a simple continuous function and compare convergence to signSGD and classical SGD.
  2. **Binary logistic regression**: Test binary gradient projection and QUBO solving on a small synthetic dataset with known structure.
  3. **Binary MLP on MNIST with handcrafted features**: Evaluate end-to-end training on a real vision task, comparing accuracy and convergence to BinaryConnect and ProxQuant baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of quantum noise on the convergence and performance of QP-SBGD compared to classical optimizers?
- Basis in paper: [explicit] The paper mentions that the experiments were conducted on both D-Wave quantum annealer and classical simulated annealers, but does not explicitly compare the impact of quantum noise on convergence and performance.
- Why unresolved: The paper does not provide a detailed analysis of the impact of quantum noise on the algorithm's performance.
- What evidence would resolve it: Experiments comparing the convergence and performance of QP-SBGD on quantum hardware with and without noise mitigation techniques, and comparing these results to classical optimizers.

### Open Question 2
- Question: How does the performance of QP-SBGD scale with the size of the neural network and the dataset?
- Basis in paper: [inferred] The paper mentions that the algorithm is implemented layer-wise to enable scalable training, but does not provide extensive empirical evidence on how the performance scales with network size and dataset size.
- Why unresolved: The paper only presents results for relatively small-scale experiments, and does not explore the scaling behavior of the algorithm.
- What evidence would resolve it: Experiments training larger neural networks on larger datasets, and analyzing the impact of network size and dataset size on convergence and performance.

### Open Question 3
- Question: What are the limitations of QP-SBGD in terms of the types of neural network architectures and tasks it can handle?
- Basis in paper: [explicit] The paper focuses on training binary neural networks and binary graph neural networks, but does not explore the limitations of the algorithm for other types of neural network architectures or tasks.
- Why unresolved: The paper does not provide a comprehensive analysis of the algorithm's limitations for different types of neural networks and tasks.
- What evidence would resolve it: Experiments applying QP-SBGD to different types of neural network architectures and tasks, and analyzing the algorithm's performance and limitations in each case.

## Limitations
- The method's performance critically depends on quantum annealer quality and availability, with current results relying on classical simulated annealing rather than actual quantum hardware.
- Convergence proofs assume the Co-Directionality Probability (CDP) condition, but empirical validation of this assumption across diverse network architectures and loss landscapes is limited.
- Layer-wise decomposition enables scalability, but no results are shown for networks deeper than 3 layers, leaving inter-layer dependencies and gradient propagation unverified.

## Confidence
- **High Confidence**: O(1/√T) convergence rate for continuous variant under stated assumptions
- **Medium Confidence**: Layer-wise training effectiveness and empirical accuracy improvements
- **Low Confidence**: Quantum annealer performance benefits and deep network scalability

## Next Checks
1. **Hardware Validation**: Run QP-SBGD on actual quantum annealing hardware (D-Wave) to measure performance degradation from noise and limited connectivity compared to classical solvers.

2. **Deep Network Scaling**: Evaluate QP-SBGD on 10+ layer binary networks to verify layer-wise training maintains convergence and accuracy as depth increases.

3. **CDP Assumption Testing**: Systematically measure binary gradient alignment (ΠB · ∇L) across different network architectures, loss functions, and optimization stages to validate the critical Co-Directionality Probability assumption.