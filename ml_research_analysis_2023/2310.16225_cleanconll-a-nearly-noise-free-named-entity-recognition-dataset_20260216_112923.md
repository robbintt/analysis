---
ver: rpa2
title: 'CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset'
arxiv_id: '2310.16225'
source_url: https://arxiv.org/abs/2310.16225
tags:
- label
- entity
- labels
- conll
- misc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CLEAN CONLL, a corrected version of the widely
  used CoNLL-03 named entity recognition (NER) dataset. The authors address the problem
  of annotation noise and inconsistency in CoNLL-03, which can lead to unreliable
  benchmarking of NER models.
---

# CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset

## Quick Facts
- arXiv ID: 2310.16225
- Source URL: https://arxiv.org/abs/2310.16225
- Authors: 
- Reference count: 28
- Key outcome: CleanCoNLL is a corrected version of CoNLL-03 with significantly improved annotation quality (1% vs 10% errors), achieving higher NER model F1-scores and reducing false errors due to annotation noise from 47% to 6%.

## Executive Summary
This paper presents CleanCoNLL, a nearly noise-free version of the widely used CoNLL-03 named entity recognition dataset. The authors address the problem of annotation noise and inconsistency in CoNLL-03, which can lead to unreliable benchmarking of NER models. To create CleanCoNLL, they add entity linking information from AIDA to disambiguate entities and use automatic consistency checks based on Wikidata categories. They then perform three iterative rounds of cross-checking with trained models to identify and correct potential inconsistencies. The result is a dataset with significantly improved annotation quality, as evidenced by manual evaluation showing only around 1% annotation errors compared to 10% in the original CoNLL-03.

## Method Summary
The authors developed CleanCoNLL through a three-phase annotation process. First, they added entity linking annotations from AIDA to CoNLL-03 and automatically derived NER labels from Wikidata categories, manually annotating the remaining 25.6% of entities. Second, they performed three iterative rounds of cross-checking using trained models to identify potential inconsistencies between model predictions and gold labels, manually resolving flagged annotations. Finally, they addressed a special class of entities (adjectival affiliation) through a dedicated annotation process. The final dataset includes both MISC-reverted and non-reverted versions to accommodate different use cases.

## Key Results
- Manual evaluation shows CleanCoNLL has only ~1% annotation errors compared to 10% in original CoNLL-03
- State-of-the-art NER models achieve higher F1-scores on CleanCoNLL (86.8→89.4) compared to CoNLL-03
- The share of correct predictions falsely counted as errors due to annotation noise drops from 47% to 6%
- Automatic Wikidata category mapping agrees with 85.8% of final labels in CleanCoNLL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Leveraging entity linking from AIDA improves NER annotation consistency.
- Mechanism: Entity linking provides unambiguous Wikipedia article references, enabling automatic derivation of NER labels via Wikidata category mappings.
- Core assumption: The Wikidata category system reliably maps to CoNLL-03 entity types (ORG, LOC, PER).
- Evidence anchors:
  - [abstract] "Our effort adds a layer of entity linking annotation both for better explainability of NER labels and as additional safeguard of annotation quality."
  - [section] "We leverage the category system in Wikidata, and discuss how this approach acts as consistency checks to safeguard annotation quality."
  - [corpus] The automatic mapping agrees with 85.8% of final labels in CleanCoNLL.
- Break condition: If Wikidata categories become unreliable or mismatched to NER labels, or if AIDA coverage is insufficient for critical entities.

### Mechanism 2
- Claim: Cross-checking with trained models iteratively reduces annotation noise.
- Mechanism: Train models on 95% of corpus, predict on held-out 5%, manually inspect model-predicted labels vs gold labels to identify potential errors.
- Core assumption: State-of-the-art models trained on large portions of data can reliably detect annotation inconsistencies.
- Evidence anchors:
  - [abstract] "We then perform three iterative rounds of cross-checking with trained models to identify and correct potential inconsistencies."
  - [section] "All annotations flagged as potential errors were then manually inspected by annotators."
  - [corpus] Three rounds of cross-checking corrected 521, 98, and 98 labels respectively.
- Break condition: If models overfit to noisy training data or fail to generalize to unseen entities, cross-checking may introduce more errors.

### Mechanism 3
- Claim: Manual annotation with source information improves error resolution accuracy.
- Mechanism: Annotators compare model predictions, current gold labels, and entity link sources to make informed decisions.
- Core assumption: Providing annotators with multiple label sources reduces ambiguity and improves consistency.
- Evidence anchors:
  - [abstract] "Our effort adds a layer of entity linking annotation both for better explainability of NER labels and as additional safeguard of annotation quality."
  - [section] "They were given the entity span within document context and the different label possibilities including their source."
  - [corpus] Inter-annotator agreement of 79.3% in first round, improving to 94.9% in manual entity link annotation.
- Break condition: If annotators become biased toward model predictions or source information is misleading.

## Foundational Learning

- Concept: Entity Linking and Wikidata Category System
  - Why needed here: Essential for automatic NER label derivation and consistency checking.
  - Quick check question: How does Wikidata's instance_of and subclass_of relations enable mapping Wikipedia articles to NER types?

- Concept: Cross-Validation and Iterative Refinement
  - Why needed here: Core methodology for identifying and correcting annotation errors systematically.
  - Quick check question: Why is it important to train models on 95% of data and predict on held-out 5% during cross-checking?

- Concept: Inter-Annotator Agreement and Resolution Protocols
  - Why needed here: Ensures annotation quality and handles disagreements systematically.
  - Quick check question: What are the advantages of having two annotators independently label and then resolve disagreements?

## Architecture Onboarding

- Component map: Data ingestion (CoNLL-03 → Reiss correction → AIDA entity links) → Automatic processing (Wikidata category mapping → NER label derivation) → Cross-checking pipeline (Train → Predict → Flag → Manual inspection → Update) → Quality assessment (Manual evaluation of 100 sentences per version)

- Critical path: Data ingestion → Automatic processing → Cross-checking (3 rounds) → Quality assessment → Release

- Design tradeoffs:
  - Precision vs. Recall: Favoring precision in automatic mappings leads to more manual work but higher quality
  - Model complexity: Using multiple simpler models vs. one complex model for cross-checking
  - Annotation variants: Providing both MISC-reverted and non-reverted versions for different use cases

- Failure signatures:
  - Low inter-annotator agreement indicates unclear guidelines or ambiguous cases
  - Persistent errors across cross-checking rounds suggest systematic issues in automatic processing
  - High disagreement between model predictions and gold labels may indicate model bias or annotation noise

- First 3 experiments:
  1. Verify automatic Wikidata mapping accuracy on a sample of 100 entities
  2. Test cross-checking pipeline on a small subset (1 batch) to validate methodology
  3. Compare annotation quality between MISC-reverted and non-reverted versions on 50 sentences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the annotation process be improved to handle ambiguous cases where both the model prediction and the original annotation could be considered valid?
- Basis in paper: [explicit] The paper mentions that during manual evaluation, annotators found contexts ambiguous where two different NER interpretations might be seen as valid, and these cases were specifically marked.
- Why unresolved: The paper acknowledges the existence of such ambiguous cases but does not provide a concrete solution for how to handle them consistently in future annotation efforts.
- What evidence would resolve it: A proposed methodology or set of guidelines for resolving ambiguous cases, along with empirical results showing its effectiveness in reducing inconsistencies.

### Open Question 2
- Question: What is the potential impact of using a more fine-grained NER label set on the automatic mapping approach using Wikidata categories?
- Basis in paper: [inferred] The paper mentions that their automatic mapping approach uses broad NER labels (ORG, LOC, PER, MISC) and that specific rules for MISC or additional labels could be added depending on the use case and domain.
- Why unresolved: The paper does not explore the effects of using a more granular label set on the accuracy and coverage of the automatic mapping approach.
- What evidence would resolve it: An experimental comparison of the automatic mapping approach using different levels of NER label granularity, with metrics on precision, recall, and overall label coverage.

### Open Question 3
- Question: How does the choice of base corpus (Reiss version vs. original CoNLL-03) affect the final annotation quality and consistency in CleanCoNLL?
- Basis in paper: [explicit] The paper states that they used the Reiss version as the starting point for their relabeling effort, but also mentions that their own analysis and prior work found that large numbers of errors still remain in this version.
- Why unresolved: The paper does not provide a direct comparison of the annotation quality and consistency that would have been achieved if they had started from the original CoNLL-03 instead of the Reiss version.
- What evidence would resolve it: A side-by-side comparison of the annotation quality and consistency achieved when starting from the original CoNLL-03 versus the Reiss version, using the same relabeling methodology.

## Limitations
- Quality improvement claims rely heavily on the accuracy of Wikidata category mappings and AIDA entity linking, both of which introduce dependencies on external knowledge bases that may contain their own inconsistencies
- Manual evaluation sample size (100 sentences) may not fully capture the distribution of annotation errors across all entity types
- Three-round cross-checking methodology assumes that model predictions are reliable indicators of true labels, which may not hold if the models themselves are trained on noisy data

## Confidence
- **High Confidence:** The observed improvement in F1-scores (86.8→89.4) and reduction in false errors (47%→6%) on CleanCoNLL is well-supported by experimental results. The manual evaluation showing 1% vs 10% annotation errors provides strong evidence of quality improvement.
- **Medium Confidence:** The effectiveness of Wikidata category-based automatic mappings depends on the reliability of the category system and the coverage of AIDA entity linking. While 85.8% agreement between automatic and final labels is promising, edge cases may introduce errors.
- **Medium Confidence:** The cross-checking methodology's effectiveness relies on the assumption that models can reliably identify annotation errors, but this depends on model quality and the nature of the annotation noise.

## Next Checks
1. **Coverage Validation:** Systematically evaluate the coverage and accuracy of AIDA entity linking across different entity types (PER, ORG, LOC, MISC) to identify potential blind spots in the annotation process.

2. **Model Independence Test:** Repeat the cross-checking process using different NER models (not just BERT-based) to verify that the identified annotation corrections are not model-specific artifacts.

3. **Longitudinal Stability:** Track the stability of CleanCoNLL annotations over time by re-evaluating a subset of corrected entities after 6-12 months to ensure that the improvements are not temporary or context-dependent.