---
ver: rpa2
title: 'Towards Regulatable AI Systems: Technical Gaps and Policy Opportunities'
arxiv_id: '2306.12609'
source_url: https://arxiv.org/abs/2306.12609
tags:
- data
- systems
- what
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the feasibility of vetting AI systems against
  public sector procurement checklists, using the World Economic Forum''s AI Procurement
  in a Box and the Canadian Directive on Automated Decision-Making as case studies.
  The authors categorize checklist criteria into six areas: data checks, system monitoring,
  global explanations, local explanations, objective design, and privacy.'
---

# Towards Regulatable AI Systems: Technical Gaps and Policy Opportunities

## Quick Facts
- arXiv ID: 2306.12609
- Source URL: https://arxiv.org/abs/2306.12609
- Reference count: 40
- Primary result: Analysis of vetting AI systems against public sector procurement checklists reveals technical gaps and interdisciplinary needs

## Executive Summary
This paper examines the feasibility of vetting AI systems against regulatory requirements by analyzing two public sector procurement checklists: the World Economic Forum's AI Procurement in a Box and the Canadian Directive on Automated Decision-Making. The authors categorize checklist criteria into six AI-familiar categories and assess what can currently be done, what needs technical innovation, and what requires interdisciplinary engagement. The study identifies key gaps in areas like data quality metrics for unstructured data, efficient monitoring of multiple metrics, meaningful explanations for AI decisions, and standardized privacy definitions.

## Method Summary
The authors conducted a close examination of technical criteria from two public sector procurement checklists, grouping them into six familiar AI research categories: data checks, system monitoring, global explanations, local explanations, objective design, and privacy. For each category, they summarized existing technical approaches, identified areas needing innovation, and highlighted interdisciplinary requirements. The analysis focused on the technical feasibility of meeting regulatory requirements rather than policy implications.

## Key Results
- Current AI methods can address some regulatory requirements but need innovation for increasingly complex systems
- Data quality assessment for unstructured data requires better metrics connecting quality to outcomes
- Monitoring multiple metrics efficiently without overwhelming engineers with false positives/negatives is a key challenge
- Defining "meaningful explanations" requires interdisciplinary engagement beyond technical solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper's categorization of checklist criteria into six AI-familiar categories creates a structured mapping between regulatory requirements and technical capabilities
- Mechanism: By grouping criteria into familiar AI research categories, the paper enables AI researchers to quickly identify which regulatory requirements map to their existing technical knowledge and where gaps exist
- Core assumption: AI researchers and engineers have foundational knowledge in these six categories
- Evidence anchors:
  - [abstract]: "We first group the technical criteria contained in these two checklists into categories that will be familiar to AI researchers and engineers"
  - [section]: "We first group the technical criteria contained in these two checklists into categories that will be familiar to AI researchers and engineers: (pre-training) data checks, (post-hoc) system monitoring, global explanation, local explanation, objective design, privacy, and human + AI systems."
- Break condition: If the six categories do not adequately capture the full scope of regulatory requirements, or if AI practitioners lack familiarity with these categories

### Mechanism 2
- Claim: The paper's three-tier classification (what we know how to do, what needs technical innovation, what requires interdisciplinary engagement) provides a clear roadmap for advancing regulatable AI systems
- Mechanism: This classification system helps stakeholders understand the current state of technical capabilities, identify specific research directions, and recognize when broader collaboration is needed beyond AI methods alone
- Core assumption: The classification accurately reflects the current state of AI research and development
- Evidence anchors:
  - [abstract]: "we consider the technical half of the question: To what extent can AI experts vet an AI system for adherence to regulatory requirements?"
  - [section]: "For each category, we briefly summarize existing technical approaches that could be used to construct AI systems that meet those criteria. Next, we identify areas where relevant technical approaches may exist, but additional technical innovation is needed to be able to vet increasingly complex AI systems being used in increasingly varied contexts."
- Break condition: If the classification is too simplistic or fails to capture nuanced requirements that span multiple categories

### Mechanism 3
- Claim: The paper's use of real regulatory frameworks grounds the analysis in practical, implementable requirements rather than theoretical ideals
- Mechanism: By examining actual checklists that are being piloted and implemented, the paper ensures its findings are relevant to current regulatory efforts and not just academic exercises
- Core assumption: The selected regulatory frameworks represent common patterns in AI regulation
- Evidence anchors:
  - [abstract]: "We investigate this question through the lens of two public sector procurement checklists"
  - [section]: "we closely examine the technical criteria from two existing procurement checklists: the World Economic Forum's AI Procurement in a Box (WEF) and the Canadian Directive in Automated Decision-Making (CDADM)"
- Break condition: If the regulatory frameworks examined are too narrow or do not reflect broader regulatory trends

## Foundational Learning

- Concept: Differential Privacy
  - Why needed here: The paper discusses privacy requirements and mentions differential privacy as a widely-accepted theoretical notion of privacy
  - Quick check question: What is the fundamental guarantee provided by differential privacy?

- Concept: Model Interpretability Methods
  - Why needed here: The paper extensively discusses global and local explanations, including inherently interpretable models and explanation techniques
  - Quick check question: What is the key difference between global and local explanations in machine learning?

- Concept: Reinforcement Learning Objective Design
  - Why needed here: The paper discusses objective design and mentions RL settings in the context of monitoring
  - Quick check question: In reinforcement learning, what is the relationship between reward functions and agent behavior?

## Architecture Onboarding

- Component map: Data Validation Pipeline → System Monitoring → Model Inspection (Global/Local Explanations) → Objective Verification → Privacy Assessment → Human-AI Interaction Design
- Critical path: Data checks → Model training/verification → System monitoring → Explanation generation → Privacy preservation → Human oversight integration
- Design tradeoffs:
  - Privacy vs. Performance: Differential privacy can reduce model accuracy
  - Explainability vs. Accuracy: Simpler interpretable models may have lower performance than complex black-box models
  - Monitoring vs. Resource Usage: More extensive monitoring requires more computational resources
- Failure signatures:
  - Privacy violations: Model leaking sensitive information about training data
  - Inadequate explanations: Users cannot understand or contest AI decisions
  - Monitoring failures: System behaves unexpectedly without triggering alerts
- First 3 experiments:
  1. Implement data quality checks on a sample dataset and evaluate their effectiveness in identifying potential biases
  2. Create a simple inherently interpretable model and compare its performance and explainability to a black-box model
  3. Set up a basic monitoring system that tracks model performance and flags anomalies based on predefined metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the minimal metrics required to capture context-dependent data quality issues, particularly for unstructured data like images and social media posts?
- Basis in paper: [explicit] "What statistics or meta-data would we need to be confident? To ensure reliable utilization of datasets, additional metrics are necessary to precisely determine the range of applications for which a dataset can be safely used."
- Why unresolved: The paper identifies this as a key gap but doesn't propose specific metrics. The challenge lies in balancing comprehensive data documentation with privacy concerns and the inherent complexity of unstructured data
- What evidence would resolve it: Development and validation of a standardized set of metrics that effectively capture data quality and context appropriateness for unstructured data, with empirical studies showing their correlation with model performance and bias

### Open Question 2
- Question: How can we efficiently monitor multiple metrics for AI systems without overwhelming engineers with false positives and negatives?
- Basis in paper: [explicit] "Monitoring multiple metrics increases the risk of false positives and false negatives, which can overwhelm engineers. How can we monitor many metrics efficiently while not incorrectly flagging too many cases for review and not missing important deviations?"
- Why unresolved: Current monitoring approaches struggle with the trade-off between comprehensive coverage and actionable insights. The proliferation of AI systems with many interacting parts exacerbates this challenge
- What evidence would resolve it: Development of adaptive monitoring systems that intelligently prioritize metrics based on context and historical data, with demonstrated effectiveness in real-world AI deployments

### Open Question 3
- Question: What constitutes a "meaningful explanation" for AI decisions, and how does this definition vary across different contexts and user groups?
- Basis in paper: [explicit] "The biggest question raised by these guidelines is what is the definition of a 'meaningful explanation' [...] This definition will depend on the socio-technical context of the task."
- Why unresolved: The paper highlights the contextual nature of meaningful explanations but doesn't provide a framework for determining what information is necessary and sufficient for different use cases
- What evidence would resolve it: Empirical studies across diverse AI applications (e.g., healthcare, criminal justice, autonomous vehicles) that identify the specific elements users need for different types of decisions, validated through user studies and real-world implementation

## Limitations

- The study focuses exclusively on two public sector procurement checklists, which may not represent the full diversity of regulatory requirements across different jurisdictions and use cases
- The categorization of criteria into six AI-familiar categories may oversimplify complex regulatory concepts or miss nuanced requirements that span multiple categories
- The assessment of current technical capabilities and gaps is based on existing literature and may not fully capture emerging research or practical implementations

## Confidence

- High confidence: The paper's mechanism for mapping regulatory requirements to technical categories is well-founded and provides a clear framework for understanding the relationship between AI capabilities and regulatory needs
- Medium confidence: The classification of requirements into what we know how to do, what needs technical innovation, and what requires interdisciplinary engagement is generally accurate but may oversimplify complex interactions between these areas
- Low confidence: The paper's conclusions about the sufficiency of current technical approaches for meeting regulatory requirements are based on a limited set of regulatory frameworks and may not generalize to all contexts

## Next Checks

1. Conduct a systematic review of additional regulatory frameworks and procurement checklists from different jurisdictions to assess whether the six-category framework captures the full scope of regulatory requirements

2. Perform a Delphi study with AI researchers, policymakers, and regulatory experts to validate the paper's classification of requirements and assess the accuracy of its gap analysis

3. Develop and pilot test a prototype AI system that implements the identified technical approaches across all six categories, measuring its effectiveness in meeting regulatory requirements and identifying any practical challenges not captured in the theoretical analysis