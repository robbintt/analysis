---
ver: rpa2
title: 'Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation'
arxiv_id: '2305.01569'
source_url: https://arxiv.org/abs/2305.01569
tags:
- pickscore
- images
- dataset
- human
- pick-a-pic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Pick-a-Pic, an open dataset of over 500,000
  user preferences for text-to-image generation. The dataset is collected through
  a web application that allows users to generate images and specify their preferences.
---

# Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation

## Quick Facts
- arXiv ID: 2305.01569
- Source URL: https://arxiv.org/abs/2305.01569
- Reference count: 21
- Over 500,000 user preferences for text-to-image generation collected through a web application

## Executive Summary
This paper introduces Pick-a-Pic, an open dataset of over 500,000 user preferences for text-to-image generation collected from real users with genuine interest in image generation. The authors develop PickScore, a CLIP-based scoring function trained on this dataset that achieves superhuman performance in predicting human preferences (70.2% accuracy compared to humans' 68.0%). The paper recommends using PickScore for evaluating future text-to-image models and suggests Pick-a-Pic prompts as a more relevant evaluation dataset than MS-COCO. The authors also demonstrate that PickScore can enhance existing text-to-image models through ranking-based optimization.

## Method Summary
The method involves collecting user preferences through a web application where users generate images from prompts and compare pairs of images to indicate preferences, with an option for ties. The collected data undergoes quality control including NSFW filtering and is divided into training, validation, and test sets. PickScore is implemented as a finetuned CLIP-H model that computes preference scores through the inner product of text and image embeddings. The model is trained using a KL-divergence objective that minimizes the difference between human preference distributions and the softmax-normalized scores of image pairs.

## Key Results
- PickScore achieves superhuman performance in predicting human preferences (70.2% accuracy vs humans' 68.0%)
- PickScore correlates better with human rankings than other automatic evaluation metrics like FID
- PickScore can enhance existing text-to-image models via ranking, improving output quality in human evaluations

## Why This Works (Mechanism)

### Mechanism 1
Real users generating images for personal interest provide higher-quality, more diverse prompts than paid annotators. Users' personal investment in image generation correlates with more meaningful preference judgments, resulting in authentic preference data that captures genuine user intent.

### Mechanism 2
Pairwise preference comparisons with ties provide better quality data than absolute ratings. Two-image comparison with tie option reduces cognitive load and ambiguity compared to absolute scoring on a scale, leading to more reliable annotations through easier binary-like decisions with option for indifference.

### Mechanism 3
PickScore trained on authentic preferences achieves superhuman performance on preference prediction. Finetuning CLIP with pairwise preference objective learns a scoring function that better captures human judgment criteria than zero-shot or aesthetics-only approaches, leveraging the information about image quality in human preference data.

## Foundational Learning

- **Human preference modeling in NLP**: Understanding how RLHF techniques from NLP improve language models provides foundation for applying similar approaches to text-to-image generation.
  - *Quick check*: What are the key differences between InstructGPT's reward modeling and how PickScore is trained?

- **CLIP architecture and embeddings**: PickScore builds on CLIP's text-image joint embedding space; understanding how CLIP encodes semantic relationships is crucial for the scoring function's design.
  - *Quick check*: How does CLIP's learned representation space differ from traditional image features like Inception embeddings used in FID?

- **Elo rating system for model evaluation**: The paper uses Elo ratings to compare model performance based on real user preferences, leveraging the iterative nature of ratings for fair comparisons.
  - *Quick check*: How does the iterative nature of Elo ratings help when comparing many models with limited pairwise comparisons?

## Architecture Onboarding

- **Component map**: Web application -> Data pipeline -> Scoring function -> Evaluation framework
- **Critical path**: User generates images → selects preference → data stored → scoring function trained → evaluation conducted → insights applied to improve models
- **Design tradeoffs**: Data quality vs. scale (stricter quality controls reduce dataset size but improve reliability); Model complexity vs. performance (larger CLIP variants may improve accuracy but increase computational cost); User experience vs. data collection (simple interface encourages participation but may limit data richness)
- **Failure signatures**: Low inter-rater agreement suggests ambiguous preference criteria; Poor correlation between PickScore and human judgments indicates overfitting; High proportion of ties may indicate insufficient discrimination in task design
- **First 3 experiments**: 1) Reproduce baseline accuracy comparing random baseline, CLIP-H, and aesthetics predictor on validation set; 2) Ablation study on training data using subsets to measure robustness; 3) Correlation analysis computing Spearman correlation between PickScore rankings and human rankings on MS-COCO captions

## Open Questions the Paper Calls Out

### Open Question 1
How does the size and quality of the Pick-a-Pic dataset compare to other human preference datasets for text-to-image generation, and what specific characteristics make it more effective for training scoring functions? The paper highlights advantages of Pick-a-Pic's size and real user data but lacks detailed quantitative comparison with other datasets in terms of impact on model performance.

### Open Question 2
What are the specific factors that contribute to the superior performance of PickScore compared to other scoring functions like ImageReward and HPS, and how can these factors be isolated and quantified? The paper suggests differences in implementation, data scale, or data distribution but does not provide detailed analysis of individual contributions of factors like model size, backbone, hyperparameters, data scale, and data distribution.

### Open Question 3
How does the choice of evaluation prompts (e.g., Pick-a-Pic vs. MS-COCO) affect the perceived performance of text-to-image models, and what are the implications for model development and deployment? While the paper provides evidence for superiority of Pick-a-Pic prompts, it does not explore broader implications for development and deployment of text-to-image models in different contexts.

## Limitations
- Potential selection bias in the Pick-a-Pic dataset from voluntary users may not represent full diversity of user preferences across demographics or cultural contexts
- The claim that Pick-a-Pic prompts are more "relevant" than MS-COCO lacks quantitative comparison metrics beyond anecdotal evidence
- The superhuman performance margin (70.2% vs 68.0%) is small, raising questions about whether this constitutes meaningful improvement or could be attributed to statistical variance

## Confidence

- **High Confidence**: Methodology for collecting user preferences through pairwise comparisons with tie options is well-justified and technically sound; Use of CLIP-based architecture for preference learning follows established practices
- **Medium Confidence**: Claim of superhuman performance should be interpreted cautiously given small margin of improvement over human performance
- **Medium Confidence**: Recommendation to use Pick-a-Pic prompts as more relevant evaluation dataset than MS-COCO is reasonable but lacks comprehensive comparative analysis

## Next Checks

1. **Dataset Diversity Audit**: Analyze demographic and cultural diversity of users who contributed to Pick-a-Pic to assess potential selection bias and generalizability of preferences

2. **Cross-dataset Performance Validation**: Test PickScore's performance on preference data collected through different methods (e.g., crowdworker platforms) to verify robustness beyond original dataset

3. **Fine-grained Preference Analysis**: Conduct controlled experiments to determine whether PickScore captures specific quality dimensions (composition, color harmony, detail) or simply learns coarse-grained preference patterns