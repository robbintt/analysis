---
ver: rpa2
title: Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection
arxiv_id: '2308.03300'
source_url: https://arxiv.org/abs/2308.03300
tags:
- audio
- learning
- datasets
- fake
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in fake audio detection
  across different datasets. The key challenge is that most fake audio detection algorithms
  degrade significantly when dealing with audio from different datasets, due to the
  orthogonal weight modification method not considering the similarity of genuine
  audio across datasets.
---

# Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection

## Quick Facts
- **arXiv ID:** 2308.03300
- **Source URL:** https://arxiv.org/abs/2308.03300
- **Reference count:** 33
- **Primary result:** Proposed Regularized Adaptive Weight Modification (RAWM) significantly improves cross-dataset fake audio detection performance with lower EER compared to baselines.

## Executive Summary
This paper addresses catastrophic forgetting in fake audio detection when models are fine-tuned on new datasets. Most existing methods degrade significantly when dealing with audio from different datasets because orthogonal weight modification doesn't consider the similarity of genuine audio across datasets. The authors propose RAWM, a continual learning approach that adaptively computes weight modification direction based on the ratio of genuine to fake utterances, while introducing regularization to preserve old feature distributions. The method achieves significantly lower Equal Error Rates (EER) across multiple datasets including ASVspoof2019LA, ASVspoof2015, VCC2020, and In-the-Wild, without requiring replay of old data.

## Method Summary
The proposed Regularized Adaptive Weight Modification (RAWM) combines adaptive weight modification (AWM) with a regularization constraint. AWM computes modification direction based on the ratio of genuine to fake utterances, ensuring effective detection on new datasets while preserving knowledge of the old model. A regularization loss (based on LwF-style distillation) forces the network to remember the old feature distribution when dealing with genuine audio from different acoustic conditions. The method uses Wav2vec 2.0 as feature extractor and S-CNN as classifier, with RAWM inserted between classifier and loss during fine-tuning. Notably, RAWM doesn't require old data replay, making it practical for real-world deployment.

## Key Results
- RAWM achieves significantly lower EER compared to baselines (EWC, LwF, OWM, DFWF) in cross-dataset experiments
- The method generalizes well to related tasks like speech emotion recognition and image recognition
- RAWM maintains performance while requiring no replay of old data, unlike experience-replay-based methods
- The regularization constraint effectively prevents forgetting when genuine audio distributions differ across datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive weight modification direction based on genuine/fake audio ratio preserves old knowledge while learning new.
- Mechanism: When fine-tuning, compute modification direction R = PN + mQN where PN aligns with old inputs subspace and QN is orthogonal to OWM projector scaled by β (genuine/fake ratio). This ensures genuine audio is trained in a direction similar to old data, preventing forgetting.
- Core assumption: Genuine audio feature distributions are more similar across datasets than fake audio distributions.
- Evidence anchors:
  - [abstract] "When fine-tuning a detection network, our approach adaptively computes the direction of weight modification according to the ratio of genuine utterances and fake utterances."
  - [section 4.1] "The adaptive modification direction ensures the network can effectively detect fake audio on the new dataset while preserving its knowledge of old model"
  - [corpus] Weak - corpus papers focus on image/text tasks, not audio fake detection. No direct evidence for genuine audio similarity assumption.
- Break condition: If genuine audio feature distributions differ significantly across datasets (e.g., different acoustic conditions), the similarity assumption fails and adaptive modification may cause forgetting.

### Mechanism 2
- Claim: Regularization constraint forces model to maintain old feature distribution when genuine audio differs across datasets.
- Mechanism: Use LwF-style distillation loss Lreg between teacher (frozen old model) and student (fine-tuned model) softmax outputs. This regularizes new model to produce similar inference distribution as old model.
- Core assumption: Maintaining similar output distribution preserves learned knowledge even when input distributions differ.
- Evidence anchors:
  - [section 4.2] "We first replicate the pre-trained model into two models with the same parameters... use the loss function to slash the distinction between the 'soft labels' yo and the softmax output yn of the student model"
  - [section 4.1] "We introduce an extra regularization forcing the model to remember the previous inference distribution"
  - [corpus] Weak - no direct evidence about audio fake detection regularization, but LwF has been validated in other domains.
- Break condition: If regularization weight η is too high, learning new tasks is severely impaired. If too low, forgetting occurs.

### Mechanism 3
- Claim: Not requiring old data makes the method practical and avoids replay-based forgetting.
- Mechanism: Both AWM and regularization only use current batch data - no replay buffer needed. AWM computes modification direction from current batch statistics, regularization only needs frozen teacher model.
- Core assumption: Current batch statistics are sufficient to approximate necessary modification directions without replay.
- Evidence anchors:
  - [abstract] "In addition, compared with the experience-replay-based method, RAWM does not require old data"
  - [section 4] "Our method does not require any replay of previous samples"
  - [corpus] Weak - corpus papers mention replay-based methods but don't validate non-replay approach for audio fake detection.
- Break condition: If batch statistics don't represent full data distribution, modification directions may be inaccurate leading to forgetting.

## Foundational Learning

- Concept: Orthogonal projection and subspaces
  - Why needed here: AWM uses orthogonal projectors P and Q to modify weight directions in subspaces orthogonal to previous inputs
  - Quick check question: Given a vector v and projector P, what is the result of Pv? What does this represent geometrically?

- Concept: Catastrophic forgetting and continual learning
  - Why needed here: The paper addresses catastrophic forgetting when fine-tuning on new datasets - understanding this phenomenon is crucial
  - Quick check question: What happens to a model's performance on old data when fine-tuned on new data without any regularization?

- Concept: Feature distribution similarity across domains
  - Why needed here: The core assumption is that genuine audio features are more similar across datasets than fake features - understanding how to measure and validate this is important
  - Quick check question: How would you quantify feature distribution similarity between two datasets? What metrics would you use?

## Architecture Onboarding

- Component map: Raw audio → Wav2vec2 → S-CNN → RAWM modification → Loss (classification + regularization)
- Critical path: Raw audio → Wav2vec2 → S-CNN → RAWM modification → Loss (classification + regularization)
- Design tradeoffs:
  - Memory vs performance: No replay buffer saves memory but may hurt performance vs replay-based methods
  - Computation: RAWM adds matrix operations during fine-tuning but negligible compared to forward/backward pass
  - Hyperparameters: η, m, Treg require tuning - too aggressive regularization hurts learning, too little hurts forgetting
- Failure signatures:
  - High forgetting: Check if η is too low or if genuine/fake ratio assumption is violated
  - Poor learning: Check if η is too high or if AWM modification direction is incorrect
  - Instability: Check if m hyperparameter causes gradient explosion
- First 3 experiments:
  1. Single dataset fine-tuning: Fine-tune on one dataset, check if RAWM hurts performance vs baseline
  2. Two dataset sequence: S → T1, compare EER on both datasets vs fine-tuning, EWC, LwF, OWM
  3. Few-shot learning: Fine-tune on 100 samples of new dataset, verify RAWM still works with limited data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Regularized Adaptive Weight Modification (RAWM) algorithm vary with different hyperparameters, such as the learning rate and the constant m?
- Basis in paper: The paper mentions that the constant m and the learning rate are hyperparameters that can be tuned, but does not provide a detailed analysis of their impact on the performance of RAWM.
- Why unresolved: The paper does not provide a comprehensive analysis of how different hyperparameters affect the performance of RAWM. This could be an area for further research to optimize the algorithm's performance.
- What evidence would resolve it: Conducting experiments with different values of the learning rate and the constant m, and analyzing their impact on the performance of RAWM, would provide evidence to answer this question.

### Open Question 2
- Question: How does the Regularized Adaptive Weight Modification (RAWM) algorithm perform in scenarios where the feature distribution of genuine audio is not concentrated, but rather dispersed across different datasets?
- Basis in paper: The paper assumes that the feature distribution of genuine audio is more concentrated than that of fake audio, but does not explore scenarios where this assumption may not hold.
- Why unresolved: The paper does not explore scenarios where the feature distribution of genuine audio is dispersed across different datasets. This could be an area for further research to understand the limitations and potential improvements of RAWM.
- What evidence would resolve it: Conducting experiments with datasets where the feature distribution of genuine audio is dispersed, and analyzing the performance of RAWM, would provide evidence to answer this question.

### Open Question 3
- Question: How does the Regularized Adaptive Weight Modification (RAWM) algorithm perform in scenarios where the feature distribution of genuine audio is not concentrated, but rather dispersed across different datasets?
- Basis in paper: The paper assumes that the feature distribution of genuine audio is more concentrated than that of fake audio, but does not explore scenarios where this assumption may not hold.
- Why unresolved: The paper does not explore scenarios where the feature distribution of genuine audio is dispersed across different datasets. This could be an area for further research to understand the limitations and potential improvements of RAWM.
- What evidence would resolve it: Conducting experiments with datasets where the feature distribution of genuine audio is dispersed, and analyzing the performance of RAWM, would provide evidence to answer this question.

## Limitations
- The core assumption that genuine audio feature distributions are more similar across datasets than fake distributions is asserted but not empirically validated
- Hyperparameter sensitivity (particularly for η and m) is not thoroughly explored in the paper
- Performance under various data imbalance scenarios between genuine and fake audio remains unclear

## Confidence

**High Confidence:** The method's ability to improve cross-dataset EER performance compared to baseline continual learning methods is well-supported by the experimental results across multiple datasets.

**Medium Confidence:** The mechanism of adaptive weight modification direction based on genuine/fake ratios is theoretically sound, but its effectiveness depends heavily on the unverified assumption about feature distribution similarities.

**Low Confidence:** The claim that no old data replay is required for effective continual learning is only partially supported, as the method still needs a frozen teacher model for regularization.

## Next Checks

1. **Distribution Similarity Analysis:** Quantify and compare genuine vs. fake audio feature distributions across ASVspoof2019LA, ASVspoof2015, and VCC2020 using KL divergence or MMD metrics to validate the core similarity assumption.

2. **Ablation Study on Hyperparameters:** Systematically vary η (0.1, 0.5, 1.0, 2.0) and m (0.1, 0.5, 1.0, 2.0) to map the sensitivity landscape and identify optimal ranges for stable performance.

3. **Teacher Model Dependency Test:** Evaluate performance when using a non-frozen teacher model versus the frozen version to quantify how much the regularization depends on having a completely static reference model.