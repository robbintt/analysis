---
ver: rpa2
title: 'GUARD: A Safe Reinforcement Learning Benchmark'
arxiv_id: '2305.13681'
source_url: https://arxiv.org/abs/2305.13681
tags:
- velocity
- hinge
- angular
- link
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GUARD is a benchmark for safe reinforcement learning (RL) that
  addresses the challenge of comparing diverse safe RL algorithms. It provides a generalized
  framework with a wide range of RL agents, tasks, and safety constraint specifications,
  as well as a unified benchmarking platform with comprehensive coverage of state-of-the-art
  safe RL algorithms.
---

# GUARD: A Safe Reinforcement Learning Benchmark

## Quick Facts
- arXiv ID: 2305.13681
- Source URL: https://arxiv.org/abs/2305.13681
- Reference count: 40
- Key outcome: GUARD provides a comprehensive, modular benchmark for safe RL with 11 agents, 7 tasks, 8 constraint types, and 8 state-of-the-art algorithms

## Executive Summary
GUARD addresses the challenge of comparing safe reinforcement learning algorithms by providing a unified, extensible benchmark framework. It supports a wide range of robot types, task specifications, and safety constraint formulations while implementing state-of-the-art algorithms with standardized components. The benchmark enables fair comparison through consistent implementation (TRPO backbone, MLP policies) and offers researchers the flexibility to customize tasks and algorithms for specific research needs.

## Method Summary
GUARD implements 11 different robot agents including high-dimensional types like drones and humanoid walkers, 7 task specifications covering 3D reaching, pushing, chasing, and defense scenarios, and 8 safety constraint types including hazards and ghosts. The benchmark includes 8 state-of-the-art safe RL algorithms (TRPO, CPO, PCPO, and various Lagrangian/hierarchical variants) all built on a TRPO backbone with standardized MLP policies (64,64) and training procedures. Experiments run for 200 epochs with 30000 steps per epoch, collecting metrics on reward performance and constraint violations across different algorithm configurations.

## Key Results
- Comprehensive coverage of safe RL algorithms with self-contained implementations
- Effective constraint handling demonstrated through cost avoidance in experiments
- Hierarchical methods show promise for cost reduction over reward maximization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GUARD improves safe RL algorithm comparison by providing unified implementation
- Mechanism: Standardizes all algorithms with TRPO backbone, MLP policies, and training procedures
- Core assumption: TRPO backbone provides sufficient performance guarantee for fair comparison
- Evidence: Explicitly stated in methodology section with detailed implementation specifications

### Mechanism 2
- Claim: Modular architecture enables easy extension and customization
- Mechanism: Separates agents, tasks, and constraints into distinct components with self-contained algorithm implementations
- Core assumption: Modular separation prevents interference between components
- Evidence: Design principles stated in abstract and methodology

### Mechanism 3
- Claim: Diverse agent and task coverage enables thorough evaluation
- Mechanism: Supports 11 agents and 7 tasks spanning real-world applications
- Core assumption: Selected configurations represent practical safety-critical scenarios
- Evidence: Explicit enumeration of supported agent types and task specifications

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and Constrained Markov Decision Processes (CMDPs)
  - Why needed here: Safe RL operates within CMDP framework where constraints must be satisfied while optimizing rewards
  - Quick check question: What is the key difference between the policy sets Π and ΠC in CMDPs?

- Concept: Trust Region Policy Optimization (TRPO) and its extensions
  - Why needed here: TRPO serves as backbone for most safe RL algorithms in GUARD
  - Quick check question: How does TRPO ensure policy updates remain within a "trust region" during optimization?

- Concept: Constraint handling methods (Lagrangian, projection, barrier functions)
  - Why needed here: Safe RL algorithms use different approaches to handle constraints
  - Quick check question: What is the fundamental difference between how Lagrangian methods and projection methods handle constraint violations?

## Architecture Onboarding

- Component map: Agent library -> Task library -> Constraint library -> Algorithm implementations
- Critical path: Select agent/task/constraint -> Choose algorithm -> Run training (TRPO backbone, 200 epochs) -> Collect metrics -> Compare results
- Design tradeoffs: Prioritizes standardization and comparability over algorithm-specific optimizations
- Failure signatures: High variance in cost metrics, longer convergence times on high-dimensional tasks, reward-cost tradeoffs in hierarchical methods
- First 3 experiments:
  1. Baseline TRPO on Goal_Point_8Hazards
  2. Compare CPO vs TRPO-Lagrangian on same task
  3. Test hierarchical method (TRPO-SL) on Goal_Arm3_8Ghosts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do safe RL algorithms perform on tasks with moving targets compared to static targets?
- Basis: Paper mentions Chase and Defense tasks with dynamic targets but lacks comparative analysis
- Why unresolved: No controlled comparison between static and dynamic target performance
- What evidence would resolve it: Controlled experiments comparing static vs dynamic target tasks

### Open Question 2
- Question: What is the impact of the number of safety constraints on algorithm performance?
- Basis: 8 different constraint specifications described but impact not analyzed
- Why unresolved: No experiments varying constraint numbers to isolate their effect
- What evidence would resolve it: Systematic experiments varying constraint numbers

### Open Question 3
- Question: How do hierarchical safe RL methods compare to end-to-end methods in terms of sample efficiency?
- Basis: Paper notes hierarchical methods may be better for cost reduction but doesn't discuss sample efficiency
- Why unresolved: Focus on reward performance and cost avoidance, not sample efficiency
- What evidence would resolve it: Study measuring samples required for convergence of different approaches

## Limitations

- Standardization approach may limit performance for algorithms benefiting from different optimization strategies
- Comparative advantage over existing benchmarks stated but not empirically validated
- Hierarchical method effectiveness for cost reduction mentioned without supporting experimental evidence

## Confidence

**High Confidence**: Structural claims about modular architecture and standardized implementation are explicitly detailed and align with standard RL framework design patterns.

**Medium Confidence**: Extensibility claims are supported by design principles but lack empirical validation through demonstrated integration of new components.

**Low Confidence**: Comparative advantage claims over existing benchmarks are stated but not empirically validated through systematic evaluation.

## Next Checks

1. Test modular architecture by integrating a new constraint type to verify claimed extensibility
2. Run controlled experiments comparing GUARD against Safety-Gym and SafeRL-Kit benchmarks
3. Conduct ablation studies on TRPO backbone by testing alternative backbones (PPO, SAC) to assess standardization limitations