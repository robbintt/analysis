---
ver: rpa2
title: 'TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational
  Graphs'
arxiv_id: '2308.13490'
source_url: https://arxiv.org/abs/2308.13490
tags:
- graph
- graphs
- dataset
- learning
- layout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TpuGraphs, a large-scale performance prediction
  dataset on tensor computation graphs running on Tensor Processing Units (TPUs).
  The dataset contains 31 million pairs of graphs and configurations, averaging over
  7,700 nodes per graph, collected from popular open-source ML models.
---

# TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs

## Quick Facts
- arXiv ID: 2308.13490
- Source URL: https://arxiv.org/abs/2308.13490
- Reference count: 40
- Primary result: Introduces TpuGraphs dataset with 31M graph-configuration pairs for performance prediction on TPUs

## Executive Summary
This paper introduces TpuGraphs, a large-scale performance prediction dataset for tensor computation graphs running on Tensor Processing Units. The dataset contains 31 million pairs of graphs and configurations, with graphs averaging over 7,700 nodes collected from popular open-source ML models. The primary use case is to rank the performance of different compilation configurations of a given graph. The paper provides baseline models based on Graph Neural Networks (GNNs) and evaluates their performance using metrics like Ordered Pair Accuracy (OPA) and top-K error. The best model achieves 25.3% top-1 error on the most challenging layout collection.

## Method Summary
TpuGraphs provides a benchmark for predicting execution times of tensor computation graphs on TPUs, focusing on ranking different compilation configurations. The dataset is derived from 5,131 open-source ML models, containing 31 million graph-configuration pairs. Baseline models use Graph Neural Networks with early-join configuration features, trained using Graph Segment Training for scalability. The models employ ranking losses (pairwise hinge and ListMLE) and are evaluated using Ordered Pair Accuracy and top-K error metrics.

## Key Results
- TpuGraphs contains 31 million pairs of graphs and configurations
- Average graph size is over 7,700 nodes per graph
- Best model achieves 25.3% top-1 error on the most challenging layout collection
- Models perform significantly better on easier default collections compared to the challenging layout collection

## Why This Works (Mechanism)

### Mechanism 1
Large graph property prediction tasks require scalability techniques because memory constraints prevent training on entire graphs. Graph Segment Training (GST) divides large graphs into smaller segments, training on one segment at a time to bound memory usage. Core assumption: Graph segments contain sufficient information for accurate property prediction when combined with stale embedding dropout.

### Mechanism 2
Ranking performance across configurations is more effective than regression for autotuning applications. Pairwise hinge loss and ListMLE loss functions optimize the model to correctly rank configurations rather than predict exact execution times. Core assumption: The primary use case is selecting top-performing configurations, not predicting precise execution times.

### Mechanism 3
Early-join of configuration features with node features improves model performance compared to late-join approaches. Configuration features are replicated across all nodes and processed through the GNN alongside node features, allowing the model to learn how configurations interact with different node types. Core assumption: Configuration information should be considered at the node level rather than only at the graph level.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) for graph property prediction
  - Why needed here: The dataset consists of tensor computation graphs where we need to predict execution time properties based on graph structure and configuration.
  - Quick check question: How does a GNN aggregate information from neighboring nodes to make predictions about the entire graph?

- Concept: Graph segmentation and hierarchical pooling
  - Why needed here: Large graphs (up to 44,000 nodes) cannot fit in memory, requiring techniques to train on graph segments while maintaining prediction quality.
  - Quick check question: What is the trade-off between segment size and prediction accuracy in Graph Segment Training?

- Concept: Ranking loss functions vs regression loss functions
  - Why needed here: The primary use case is selecting top configurations, making ranking accuracy more important than precise time predictions.
  - Quick check question: Why might pairwise hinge loss be more effective than MSE for configuration ranking tasks?

## Architecture Onboarding

- Component map: Input features (node features + configuration features) -> GNN layers (GraphSAGE/GCN) -> Graph pooling -> Output prediction

- Critical path:
  1. Graph feature extraction from protobuf/HLO format
  2. Graph segmentation (for layout models)
  3. GNN forward pass on segments
  4. Graph embedding aggregation
  5. Configuration feature integration (early-join)
  6. Prediction and ranking loss computation

- Design tradeoffs:
  - Segment size vs. memory usage: Larger segments provide better accuracy but require more memory
  - Early-join vs. late-join: Early-join generally performs better but increases computation per node
  - GNN depth: More layers can capture complex patterns but increase training time and risk overfitting
  - Loss function choice: Ranking losses vs. regression losses based on use case requirements

- Failure signatures:
  - High validation loss with low training loss: Overfitting, try more data augmentation or regularization
  - Very high top-1 error but low top-10 error: Model ranks correctly but struggles to identify single best configuration
  - Training instability: Check learning rate and consider gradient clipping
  - Memory errors during training: Reduce segment size or batch size

- First 3 experiments:
  1. Baseline comparison: Train with full graph vs. Graph Segment Training with different segment sizes (100, 500, 1000 nodes)
  2. Architecture ablation: Compare early-join vs. late-join configuration feature integration on tile collection
  3. Loss function comparison: Train with MSE vs. pairwise hinge loss on layout collection using the same architecture

## Open Questions the Paper Calls Out

### Open Question 1
How can we leverage the repeated subgraph structure in tensor computation graphs to devise a more compact representation that is easier to learn? The paper identifies that tensor computation graphs often contain repeated blocks of neural network layers, appearing as repeated subgraphs, and suggests that exploiting this structure could lead to more compact representations.

### Open Question 2
How can we address the data imbalance problem in TpuGraphs, where some types of graphs (e.g., ResNet) are significantly more represented than others, to improve the quality of the learned model? The paper acknowledges that the dataset may be imbalanced, with some types of architectures more represented than others, and suggests this skew may lead to better performance on common types of graphs but poorer performance on uncommon types.

### Open Question 3
How can we combine analytical modeling with learned models to improve the accuracy and efficiency of performance prediction for tensor programs? The paper discusses the challenges of developing purely analytical cost models for modern hardware and suggests that combining analytical modeling with learned models could be a promising direction for future research.

## Limitations
- The 25.3% top-1 error on the most challenging layout collection represents a significant performance gap that limits practical utility
- The effectiveness of Graph Segment Training for maintaining prediction quality on very large graphs remains to be validated across different model architectures
- The generalization capability of GNN models trained on TpuGraphs to unseen computational patterns and hardware configurations is uncertain

## Confidence
- High confidence: The dataset construction methodology and baseline GNN implementations
- Medium confidence: The ranking-based evaluation approach being superior to regression for autotuning applications
- Low confidence: The scalability of current models to handle future, even larger tensor computational graphs

## Next Checks
1. Conduct ablation studies comparing full-graph training vs. Graph Segment Training across multiple segment sizes to quantify the trade-off between memory efficiency and prediction accuracy
2. Evaluate model generalization by testing on computational graphs from models not included in the training set, particularly those with significantly different architectures
3. Benchmark the performance of ranking-based losses (pairwise hinge, ListMLE) against regression-based approaches on downstream autotuning tasks to validate the ranking assumption