---
ver: rpa2
title: 'MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical Question
  Answering'
arxiv_id: '2309.16035'
source_url: https://arxiv.org/abs/2309.16035
tags:
- medical
- facts
- language
- retrieval
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explores the use of retrieval augmented generation (RAG)
  to enhance the performance of large language models (LLMs) in medical question answering
  (QA). The proposed approach, MKRAG, incorporates relevant medical facts extracted
  from an external knowledge base into the LLM's prompt, aiming to improve its ability
  to provide accurate answers.
---

# MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical Question Answering

## Quick Facts
- arXiv ID: 2309.16035
- Source URL: https://arxiv.org/abs/2309.16035
- Reference count: 3
- Primary result: MKRAG improves Vicuna-7B accuracy from 44.46% to 48.54% on MedQA-SMILE

## Executive Summary
This paper presents MKRAG, a retrieval augmented generation approach for medical question answering that injects relevant medical facts into LLM prompts to improve accuracy. The method uses a two-step retrieval strategy to extract facts from an external knowledge base and incorporates them into the prompt via in-context learning. Experiments on the MedQA-SMILE dataset show that MKRAG achieves a 4.08% accuracy improvement over baseline Vicuna-7B, with performance sensitive to the number of retrieved facts and the choice of embedding model (Contriever vs. SapBert).

## Method Summary
MKRAG employs retrieval augmented generation to enhance medical QA by injecting relevant medical facts into LLM prompts. The method uses an external Disease Database containing 44,561 triplets as the knowledge base. For each question and answer candidate, it retrieves relevant facts using a two-step strategy: first performing a broad search to find facts related to answer candidates, then refining by selecting facts most relevant to the question. These facts are incorporated into the prompt for Vicuna-7B, which generates answers evaluated via string-matching against ground truth.

## Key Results
- MKRAG improves Vicuna-7B accuracy from 44.46% to 48.54% on MedQA-SMILE
- Performance increases with more retrieved facts (4→8→16), but shows diminishing returns
- Contriever embedding model slightly outperforms SapBert (48.54% vs 48.07%)
- The two-step retrieval strategy effectively reduces redundant facts while maintaining relevance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating medical facts into LLM prompts via RAG improves accuracy by providing domain-specific context that compensates for knowledge gaps.
- Mechanism: The approach retrieves relevant medical facts from an external knowledge base and injects them into the prompt. This gives the LLM direct access to accurate medical information, bypassing its internal knowledge limitations.
- Core assumption: The retrieved facts are both relevant to the question and accurate enough to guide the LLM toward the correct answer.
- Evidence anchors:
  - [abstract] "the proposed approach, MKRAG, incorporates relevant medical facts extracted from an external knowledge base into the LLM's prompt, aiming to improve its ability to provide accurate answers."
  - [section] "we employ a pre-trained language model gz to convert it into an embedding... we select the Disease Database [Yasunaga et al., 2021] as our knowledge base F"
  - [corpus] Weak - related papers focus on retrieval augmentation but don't specifically validate the fact-injection mechanism used here.
- Break condition: If retrieved facts are irrelevant, contradictory, or if the LLM cannot properly integrate the additional context into its reasoning.

### Mechanism 2
- Claim: Using a two-step retrieval strategy (broad search followed by refinement) improves fact relevance and reduces noise in the prompt.
- Mechanism: First, retrieve facts related to answer candidates, then filter those facts by their relevance to the question. This ensures the facts provided are contextually aligned with both the question and the answer options.
- Core assumption: Facts relevant to both the question and answer candidates are more useful than facts relevant to only one or the other.
- Evidence anchors:
  - [section] "we need to remove these redundant facts... we select the top-k facts from the initial facts set FI that exhibit high similarity to zq, forming the refined facts set FR"
  - [abstract] "We introduce a tailored fact extraction strategy specifically designed for medical QA."
  - [corpus] Weak - no direct evidence in related work about this specific two-step refinement process.
- Break condition: If the refinement step removes too many relevant facts or if the initial broad search retrieves too few useful facts.

### Mechanism 3
- Claim: The choice of embedding model (Contriever vs. SapBert) affects retrieval quality due to differences in pre-training strategy and semantic understanding.
- Mechanism: Contriever uses contrastive learning pre-training, which may provide better semantic representations for retrieval tasks compared to SapBert's self-alignment approach focused on biomedical entities.
- Core assumption: Broader pre-training on diverse text data leads to better retrieval performance in specialized domains.
- Evidence anchors:
  - [section] "Contriever slightly outperformed SapBert, securing an accuracy of 48.54% compared to SapBert's 48.07%"
  - [abstract] "the results show that the number of retrieved facts and the choice of retrieval model (Contriever vs. SapBert) impact the model's performance"
  - [corpus] Moderate - related papers discuss embedding models but don't directly compare these specific approaches in medical QA context.
- Break condition: If the retrieval model fails to capture semantic relationships relevant to medical terminology and concepts.

## Foundational Learning

- Concept: Embedding models and semantic similarity
  - Why needed here: The entire retrieval mechanism depends on converting text (facts, questions, answers) into vector representations and measuring their similarity to find relevant facts.
  - Quick check question: How would you implement the similarity function s(za, zf) = (za)^T zf using cosine similarity instead of dot product?

- Concept: In-context learning and prompt engineering
  - Why needed here: The model editing approach relies on adding relevant facts to the prompt rather than fine-tuning, requiring understanding of how LLMs process contextual information.
  - Quick check question: What are the token limits for Vicuna-7B, and how might they constrain the number of facts you can include in a prompt?

- Concept: Knowledge base construction and medical terminology
  - Why needed here: The quality of the external knowledge base (Disease Database) directly impacts retrieval performance, requiring understanding of medical entity relationships and fact representation.
  - Quick check question: How would you handle medical terms with multiple aliases when constructing the knowledge base embeddings?

## Architecture Onboarding

- Component map: Retriever (embedding model + similarity search) → Fact Selector (broad search + refinement) → Prompt Builder (fact injection) → LLM (Vicuna) → Answer Evaluator (string matching)
- Critical path: Question embedding → Broad search for answer-related facts → Refinement based on question relevance → Prompt construction → LLM inference → Answer validation
- Design tradeoffs: More facts improve accuracy but hit token limits; broader pre-training improves retrieval but may add noise; two-step retrieval improves relevance but adds computational overhead
- Failure signatures: Accuracy plateaus despite more facts (token limit reached); retrieval fails to find relevant facts (embedding model mismatch); LLM ignores injected facts (prompt formatting issues)
- First 3 experiments:
  1. Test different embedding models (Contriever vs. SapBert) on a small subset to measure retrieval quality impact
  2. Vary the number of facts (4, 8, 16) to find the optimal balance between accuracy and token limits
  3. Compare different prompt templates to see how fact presentation affects LLM performance

## Open Questions the Paper Calls Out

- Question: What is the optimal number of retrieved medical facts to include in the prompt for maximum accuracy without exceeding model input limitations?
- Basis in paper: [explicit] The paper evaluates different numbers of facts (4, 8, 16) and finds a positive correlation between fact count and performance.
- Why unresolved: The study only tests three specific values, leaving the precise optimal number undetermined.
- What evidence would resolve it: Systematic testing of fact numbers across a wider range, determining the point of diminishing returns where additional facts no longer improve accuracy or exceed input limits.

- Question: How does the performance of MKRAG compare to fine-tuning or retraining approaches for medical domain adaptation?
- Basis in paper: [inferred] The paper emphasizes MKRAG's efficiency compared to fine-tuning/retraining but doesn't directly compare performance.
- Why unresolved: The study focuses on MKRAG's effectiveness without benchmarking against traditional adaptation methods.
- What evidence would resolve it: Direct performance comparison between MKRAG and models fine-tuned or retrained specifically for medical QA tasks.

- Question: What is the impact of fact quality and relevance on MKRAG's performance beyond simple quantity?
- Basis in paper: [explicit] The paper emphasizes that "quality of incorporated medical facts is critical to the efficacy of model editing."
- Why unresolved: The study only considers fact count, not quality metrics or more sophisticated relevance scoring methods.
- What evidence would resolve it: Evaluation of MKRAG performance using fact sets ranked by different quality/relevance metrics rather than just top-K selection.

## Limitations

- The study focuses exclusively on multiple-choice questions from a single standardized test dataset, limiting generalizability to real-world medical QA.
- Evaluation relies on simple string-matching for accuracy, potentially missing nuanced cases where model responses are semantically correct but not exact matches.
- Vicuna-7B's token limitations constrain how many facts can be injected, creating a tradeoff between context richness and model capacity.

## Confidence

- Medium confidence in the core claims about MKRAG's effectiveness. The 4.08% accuracy improvement is statistically meaningful but modest, and the ablation study on fact count shows diminishing returns that suggest approaching token limits.

## Next Checks

1. **Generalization Test**: Evaluate MKRAG on diverse medical QA datasets beyond MedQA-USMILE, including open-ended questions and clinical notes, to assess real-world applicability.

2. **Fact Relevance Analysis**: Manually annotate a sample of retrieved facts to quantify precision and recall, determining whether the two-step retrieval strategy actually improves fact quality or just reduces quantity.

3. **Token Efficiency Study**: Systematically vary prompt templates and fact presentation formats to determine the optimal way to inject facts without exceeding token limits, potentially exploring fact summarization or selection algorithms.