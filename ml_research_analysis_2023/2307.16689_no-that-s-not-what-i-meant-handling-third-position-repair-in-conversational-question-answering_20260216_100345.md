---
ver: rpa2
title: 'No that''s not what I meant: Handling Third Position Repair in Conversational
  Question Answering'
arxiv_id: '2307.16689'
source_url: https://arxiv.org/abs/2307.16689
tags:
- repair
- tprs
- dialogue
- rewrite
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REPAIR-QA, the first large dataset of Third
  Position Repairs (TPRs) in conversational question answering. The dataset consists
  of TPR turns, dialogue contexts, and candidate repairs for training TPR execution
  models.
---

# No that's not what I meant: Handling Third Position Repair in Conversational Question Answering

## Quick Facts
- arXiv ID: 2307.16689
- Source URL: https://arxiv.org/abs/2307.16689
- Authors: 
- Reference count: 18
- Key outcome: Introduces REPAIR-QA, the first large dataset of Third Position Repairs (TPRs) in conversational QA, demonstrating significant improvements in TPR execution when models are exposed to this data.

## Executive Summary
This paper addresses the challenge of Third Position Repairs (TPRs) in conversational question answering, where users correct their own previously asked questions. The authors introduce REPAIR-QA, the first large-scale dataset specifically designed for TPRs in conversational QA contexts. They evaluate both a fine-tuned T5 model and OpenAI's GPT-3 LLMs on TPR execution tasks, showing that while GPT-3 models perform poorly on TPRs without training, they significantly improve when provided with few-shot examples from REPAIR-QA. The fine-tuned T5 model achieves the best performance across automatic evaluation metrics, highlighting the importance of specialized training data for handling miscommunication in conversational AI systems.

## Method Summary
The authors collected TPR data through Amazon Mechanical Turk by having workers complete dialogue snippets from the AmbigQA dataset, creating REPAIR-QA which contains TPR turns, dialogue contexts, and candidate repairs. They fine-tuned a T5 model on this dataset for TPR execution and evaluated both T5 and GPT-3 models (Davinci and Curie) using automatic metrics (BERTScore, BLEU, Exact Match) and human evaluation. The evaluation included both intrinsic assessment of TPR execution quality and extrinsic evaluation of TPR processing in downstream conversational QA tasks, comparing model performance with and without exposure to TPR examples.

## Key Results
- GPT-3 models show poor out-of-box performance on TPRs but significantly improve with few-shot examples from REPAIR-QA
- Fine-tuned T5 model achieves best performance across all automatic metrics on REPAIR-QA test set
- BERTScore correlates better with human judgment of semantic correction quality than BLEU or Exact Match for TPR execution
- GPT-3's TPR processing capability in conversational QA improves substantially when exposed to TPR examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning T5 on REPAIR-QA improves TPR execution accuracy compared to zero-shot or few-shot prompting of large language models
- Mechanism: The fine-tuned T5 model learns specific semantic transformation patterns from ambiguous questions to their corrected forms by seeing many examples of TPR dialogues, capturing subtle context-dependent corrections
- Core assumption: TPR patterns are sufficiently regular across examples to be learned from supervised data
- Evidence anchors: [abstract] poor out-of-the-box performance improves when exposed to REPAIR-QA; [section 3.1] fine-tuned T5 achieves best performance on all metrics

### Mechanism 2
- Claim: GPT-3 models show poor intrinsic TPR execution but improve significantly when given few-shot examples from REPAIR-QA
- Mechanism: Without exposure to TPR patterns, GPT-3's pre-training corpus lacks sufficient examples of this specific repair type; few-shot examples in the prompt provide context for recognizing and executing the repair transformation
- Core assumption: Few-shot prompting can compensate for missing domain knowledge in pre-training data
- Evidence anchors: [abstract] poor performance improves with REPAIR-QA examples; [section 4] TPR processing capability improves with handful of examples

### Mechanism 3
- Claim: BERTScore correlates better with human judgment of semantic correction quality than exact match or BLEU for TPR execution
- Mechanism: TPRs often involve minimal word changes but significant semantic correction; BERTScore captures semantic similarity better than surface-level metrics when core meaning is preserved despite different wording
- Core assumption: Semantic similarity is more important than lexical overlap for evaluating repair quality
- Evidence anchors: [section 3.2] Davinci produces rewrites capturing meaning despite different words; [section 3.1] differential evaluation showing GPT-3-Davinci outperforming on BERTScore

## Foundational Learning

- Concept: Sequence-to-sequence modeling
  - Why needed here: TPR execution is framed as transforming an ambiguous question into a corrected, unambiguous form, which is a classic seq2seq task
  - Quick check question: What is the input-output relationship in the TPR execution task?

- Concept: Few-shot learning in large language models
  - Why needed here: GPT-3's improvement with TPR examples demonstrates the effectiveness of in-context learning for this task
  - Quick check question: How does providing examples in the prompt affect GPT-3's ability to handle TPRs?

- Concept: Semantic similarity metrics
  - Why needed here: Evaluating whether corrected questions adequately address the TPR requires measuring semantic equivalence, not just lexical overlap
  - Quick check question: Why might BERTScore be more appropriate than BLEU for evaluating TPR execution?

## Architecture Onboarding

- Component map: Data collection pipeline (AMT interface) -> T5 fine-tuning pipeline (REPAIR-QA training) -> GPT-3 prompting pipeline (few-shot evaluation) -> Evaluation pipeline (automatic metrics + human evaluation) -> Conversational QA integration (end-to-end testing)
- Critical path: Data collection → Model training/evaluation → Performance analysis → Integration testing
- Design tradeoffs: Fine-tuning T5 requires computational resources and dataset curation but provides better performance than few-shot prompting, which is cheaper but less reliable
- Failure signatures: Poor BLEU scores with good BERTScore (or vice versa) indicates mismatch between surface form and semantic quality; unknown responses indicate model uncertainty or inability to process TPRs
- First 3 experiments:
  1. Run fine-tuned T5 model on REPAIR-QA test set and compare automatic metrics to baseline
  2. Evaluate GPT-3-Davinci with and without TPR examples in the prompt on the same test set
  3. Perform human evaluation of model outputs focusing on semantic correction quality vs. surface form

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the quality of Third Position Repairs (TPRs) collected through dialogue completion tasks?
- Basis in paper: Inferred
- Why unresolved: The current data collection method relies on crowd workers completing dialogue snippets, which may not fully capture natural dynamics of TPRs in conversation
- What evidence would resolve it: Collecting TPRs through more interactive, real-time dialogue settings where participants engage in natural conversation could provide more authentic data

### Open Question 2
- Question: What are the limitations of using GPT-3 models for handling TPRs, and how can these be addressed?
- Basis in paper: Explicit
- Why unresolved: The paper shows that GPT-3 models, even when exposed to TPR examples, still perform poorly in interpreting and integrating TPRs in conversational QA tasks
- What evidence would resolve it: Further experimentation with larger and more diverse TPR datasets, fine-tuning GPT-3 models specifically for TPR tasks, and exploring alternative model architectures

### Open Question 3
- Question: How does the performance of TPR execution models vary across different types of TPRs?
- Basis in paper: Inferred
- Why unresolved: The paper does not provide a detailed analysis of how well TPR execution models perform on different categories of TPRs
- What evidence would resolve it: Conducting a detailed analysis of TPR execution model performance on various types of TPRs, categorized by factors such as ambiguity level and context-dependency

### Open Question 4
- Question: What are the implications of TPRs for the design of conversational AI systems?
- Basis in paper: Explicit
- Why unresolved: While the paper highlights the importance of handling TPRs for robust conversational AI, it does not explore broader implications for system design
- What evidence would resolve it: Investigating the impact of integrating TPR handling capabilities into conversational AI systems, including user satisfaction and system robustness

## Limitations
- Data collection relies on crowdsourced annotations without detailed validation of annotator expertise and quality control procedures
- Evaluation uses automatic metrics as primary measures despite paper's own finding that BERTScore correlates better with human judgment than surface-level metrics
- Downstream conversational QA evaluation uses only GPT-3 models, not comparing with the fine-tuned T5 model in the same setup

## Confidence

**High Confidence**: The claim that REPAIR-QA is the first large dataset of TPRs in conversational QA is well-supported by systematic data collection methodology and thorough comparison with existing repair-related datasets.

**Medium Confidence**: The finding that GPT-3 models show poor out-of-the-box TPR performance but improve significantly with few-shot examples is supported by evaluation results, though human evaluation reveals that even with examples, responses can still be inconsistent or unknown.

**Low Confidence**: The claim that fine-tuned T5 achieves "best performance" on all automatic metrics requires scrutiny given the discussion about BERTScore's superiority for semantic correction quality.

## Next Checks

1. **Human Evaluation Validation**: Conduct a systematic human evaluation study where annotators rate model outputs on semantic correction quality, context-dependence, and overall usefulness for TPR execution. Compare these ratings directly against the automatic metrics to establish which metrics best predict human judgment of TPR quality.

2. **Cross-Architecture Conversational QA Testing**: Evaluate both the fine-tuned T5 model and GPT-3 models in the same downstream conversational QA setup, measuring their ability to handle TPRs in ongoing dialogue contexts.

3. **Generalization and Robustness Testing**: Test model performance on TPR examples that were not in the training distribution, including TPRs with different linguistic patterns, domains outside the original data collection, and TPRs involving multiple repair turns.