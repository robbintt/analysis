---
ver: rpa2
title: Task Selection and Assignment for Multi-modal Multi-task Dialogue Act Classification
  with Non-stationary Multi-armed Bandits
arxiv_id: '2309.09832'
source_url: https://arxiv.org/abs/2309.09832
tags:
- task
- tasks
- proposed
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multi-task learning (MTL) in
  multi-modal dialogue act classification, where traditional random task selection
  strategies can lead to negative transfer and suboptimal performance. The authors
  propose a novel method based on non-stationary multi-armed bandits (MAB) with discounted
  Thompson Sampling (TS) to dynamically select and assign tasks during training.
---

# Task Selection and Assignment for Multi-modal Multi-task Dialogue Act Classification with Non-stationary Multi-armed Bandits

## Quick Facts
- arXiv ID: 2309.09832
- Source URL: https://arxiv.org/abs/2309.09832
- Authors: [Not specified in source]
- Reference count: 0
- Primary result: Novel MAB-based method significantly outperforms baselines and state-of-the-art in multi-modal dialogue act classification

## Executive Summary
This paper addresses the challenge of negative transfer in multi-task learning for multi-modal dialogue act classification by proposing a dynamic task selection method based on non-stationary multi-armed bandits with discounted Thompson Sampling. The approach treats each task as an arm in a bandit problem, where task utility changes over training stages. Experimental results on the IEMOCAP dataset show statistically significant improvements over single-task and multi-task baselines, particularly for minority classes, with the method surpassing the current state-of-the-art model.

## Method Summary
The proposed method uses a non-stationary multi-armed bandit (MAB) controller with discounted Thompson Sampling to dynamically select tasks during training for multi-modal dialogue act classification. Each task is treated as an arm, with rewards defined as performance improvements relative to the current best model. The MAB controller uses Gaussian priors and adjusts sampling variances to encourage exploration of tasks that may become more useful in different training stages. The architecture combines BERT for text features, VGGish for audio features, and a transformer encoder-decoder for multimodal fusion, with separate task heads for primary dialogue act classification and auxiliary tasks.

## Key Results
- Statistically significant improvement over single-task and multi-task baselines (p < 0.05)
- Superior performance on minority classes with higher stability and consistent performance
- Surpasses current state-of-the-art model in UAR and F1 metrics

## Why This Works (Mechanism)

### Mechanism 1
The non-stationary MAB with discounted Thompson Sampling dynamically identifies task utility across training stages by treating each task as an arm, sampling expected rewards from Gaussian posteriors, and updating task selection probabilities based on discounted cumulative rewards. It encourages exploration by increasing sampling variance for unselected tasks while decreasing it for selected tasks.

### Mechanism 2
The reward definition (performance improvement relative to current best model) enables task assignment by quantifying marginal utility, where at each round reward rt = Vt(it) - Vbest_t measures how much the selected task improves validation performance.

### Mechanism 3
The discounted factor and sampling variance adjustments prevent premature convergence to suboptimal task combinations, with γ discounting historical rewards to ensure recent performance dominates, and τmax bounds and s-controlled slopes adjusting exploration rates.

## Foundational Learning

- Concept: Multi-armed bandit problem and Thompson Sampling
  - Why needed here: The paper models task selection as a sequential decision problem where each task has unknown, time-varying utility.
  - Quick check question: How does Thompson Sampling balance exploration and exploitation in non-stationary environments?

- Concept: Multi-task learning and negative transfer
  - Why needed here: Understanding why random task selection can harm performance is crucial for appreciating the need for dynamic task assignment.
  - Quick check question: What conditions typically lead to negative transfer in multi-task learning?

- Concept: Gaussian priors and posterior updating
  - Why needed here: The MAB controller uses Gaussian distributions to model task reward uncertainty and update beliefs based on observed performance.
  - Quick check question: How does the variance of Gaussian posteriors affect exploration behavior in Thompson Sampling?

## Architecture Onboarding

- Component map: Feature extraction (BERT, VGGish) -> Transformer encoder/decoder -> Task head selection (MAB) -> Parameter update -> Validation performance measurement -> MAB reward update
- Critical path: Feature extraction → Transformer fusion → Task head selection (MAB) → Parameter update → Validation performance measurement → MAB reward update
- Design tradeoffs:
  - Fixed vs. learned task selection: MAB provides dynamic adaptation but adds complexity
  - Reward definition: Using validation improvement encourages short-term gains but may miss long-term benefits
  - Exploration parameters: Need careful tuning of γ, τmax, and s for different dataset characteristics
- Failure signatures:
  - Performance worse than single-task baseline: Indicates poor task selection or negative transfer
  - High variance across runs: Suggests insufficient exploration or unstable reward signals
  - Majority class improvement only: Implies minority class degradation from task selection
- First 3 experiments:
  1. Run single-task baseline vs. random task selection to confirm negative transfer exists
  2. Implement MAB controller with simple reward (validation accuracy) and compare to random selection
  3. Add minority class performance monitoring to verify stability improvements claimed in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method generalize to primary tasks from diverse research fields beyond dialogue act classification?
- Basis in paper: [explicit] The authors state in the conclusion that "Future work should also consider verifying the generalisability of the method, such as investigating primary tasks from diverse research fields."
- Why unresolved: The paper only evaluates the method on a single dataset and task domain (multi-modal dialogue act classification), so its performance on other types of tasks remains untested.
- What evidence would resolve it: Empirical results showing the method's effectiveness on a range of primary tasks from different domains (e.g., computer vision, natural language processing, speech recognition) with varying numbers of classes and data distributions.

### Open Question 2
- Question: What is the optimal choice of the discounted factor γ and other hyperparameters (τ1(i), τmax bound, s) for the MAB controller in different training scenarios?
- Basis in paper: [inferred] The paper uses fixed values for these hyperparameters without justification or sensitivity analysis. Different tasks or data distributions might require different settings for optimal performance.
- Why unresolved: The authors do not explore the impact of hyperparameter choices or provide guidance on how to select them for new tasks.
- What evidence would resolve it: A systematic study of the method's performance under different hyperparameter configurations, possibly with recommendations for setting them based on task characteristics.

### Open Question 3
- Question: How does the proposed method compare to other task selection strategies in MTL, such as uncertainty-based or loss-based approaches?
- Basis in paper: [inferred] The paper only compares to a random task selection baseline and the current state-of-the-art model. Other task selection strategies are not mentioned or evaluated.
- Why unresolved: Without comparisons to alternative approaches, it is unclear whether the MAB-based method is the most effective way to select tasks in MTL.
- What evidence would resolve it: Experimental results showing the performance of the proposed method against other task selection strategies on the same datasets and tasks.

### Open Question 4
- Question: Can the proposed method handle dynamic changes in task utility over time, or is it limited to the non-stationary setting with a known number of tasks?
- Basis in paper: [inferred] The method assumes a fixed set of tasks and uses a non-stationary MAB model to adapt to changes in task utility. It is unclear how the method would handle the addition or removal of tasks during training.
- Why unresolved: The paper does not discuss scenarios where the set of available tasks changes over time, which could be a common occurrence in real-world applications.
- What evidence would resolve it: Experiments demonstrating the method's performance when tasks are added or removed during training, or theoretical analysis of the method's properties in such dynamic settings.

## Limitations
- Limited evaluation on a single dataset and task domain, raising questions about generalizability
- Fixed hyperparameters without sensitivity analysis or guidance for different tasks
- No comparison to alternative task selection strategies beyond random selection

## Confidence
- Task utility dynamics and MAB effectiveness: Medium-High
- Performance improvement claims: Medium
- Minority class stability: Medium-Low (limited evidence provided)
- State-of-the-art comparison: Low (single comparison, unclear methodology)

## Next Checks
1. Ablation study: Remove the MAB controller and use static task scheduling to quantify the specific contribution of dynamic task selection versus the base architecture improvements.
2. Cross-domain validation: Apply the method to a different multi-modal dialogue dataset (e.g., MOSI, MOSEI) to test generalizability beyond IEMOCAP.
3. Hyperparameter sensitivity analysis: Systematically vary γ, τmax, and s parameters to understand their impact on performance and identify optimal ranges for different task configurations.