---
ver: rpa2
title: 'Spike No More: Stabilizing the Pre-training of Large Language Models'
arxiv_id: '2312.16903'
source_url: https://arxiv.org/abs/2312.16903
tags:
- pre-training
- initialization
- language
- scaled
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the cause of loss spikes during pre-training
  of large language models (LLMs). It identifies two primary factors leading to exploding
  gradients: (1) rapid amplification of the residual branch norm during forward propagation,
  and (2) intensification of gradients before and after layer normalizations (LNs).'
---

# Spike No More: Stabilizing the Pre-training of Large Language Models

## Quick Facts
- **arXiv ID:** 2312.16903
- **Source URL:** https://arxiv.org/abs/2312.16903
- **Reference count:** 28
- **Primary result:** Identifies and addresses loss spikes in LLM pre-training through embedding modifications combined with scaled initialization

## Executive Summary
This paper analyzes the cause of loss spikes during pre-training of large language models (LLMs). It identifies two primary factors leading to exploding gradients: (1) rapid amplification of the residual branch norm during forward propagation, and (2) intensification of gradients before and after layer normalizations (LNs). The authors provide requirements to prevent exploding gradients and introduce a simple modification to embeddings combined with scaled initialization. Experiments show that this combination effectively prevents loss spikes and enables training with larger learning rates, leading to better performance. The method achieves lower perplexity scores compared to vanilla training, particularly for larger models.

## Method Summary
The method combines scaled initialization with embedding modifications to prevent loss spikes during LLM pre-training. The key insight is that while scaled initialization prevents residual branch explosion, it creates LN gradient amplification issues. The authors propose two solutions: multiplying embeddings by √d ("Scaled Embed") or applying layer normalization to embeddings ("Embed LN"). These modifications increase the standard deviation of embeddings to be close to 1, reducing LN gradient amplification. The method is tested on GPT-2 architecture using the C4 corpus with Adam optimizer, learning rate 5.0 × 10^-4, batch size 528, and sequence length 2048.

## Key Results
- Loss spikes are effectively prevented through the combination of scaled initialization and embedding modifications
- The method enables training with larger learning rates (1e-3), improving performance
- Lower perplexity scores are achieved compared to vanilla training, especially for larger models
- Both "Scaled Embed" and "Embed LN" modifications are effective at stabilizing training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rapid amplification of residual branch norm during forward propagation causes exploding gradients
- Mechanism: In plain initialization, FFN(x) and Attention(x) produce outputs with standard deviations close to or exceeding 1. When these are added to the residual branch across multiple layers, the variance compounds (Equation 6), causing exponential growth in the residual branch norm.
- Core assumption: Input vectors to each layer follow normal distributions with mean 0, allowing analysis of only standard deviation for norm estimation.
- Evidence anchors:
  - [abstract] "rapid amplification of the norm of the residual branch during forward propagation"
  - [section] "the standard deviation of FFN(x) is close to 1... the plain initialization exponentially increases the standard deviation of the residual branch during forward computation"
  - [corpus] Weak - no direct mention in related papers
- Break condition: Using scaled initialization where W2 and WO are scaled to 1/√2N reduces the variance of FFN(x) and Attention(x) below 1, preventing residual branch amplification.

### Mechanism 2
- Claim: Intensification of gradients before and after layer normalizations (LNs) causes exploding gradients
- Mechanism: The gradient norm of LN is inversely proportional to the standard deviation of its input (Equation 13). When embeddings are initialized with very small variance (N(0, σ) where σ << 1), the gradient norm before LN becomes extremely large, especially in shallow layers.
- Evidence anchors:
  - [abstract] "intensification of gradients before and after layer normalizations (LNs) in each layer"
  - [section] "the LN increases its gradient norm drastically when std(x) ≪ 1... LNs certainly increase gradients due to Equation (13) especially in shallow layers"
  - [corpus] Weak - related papers focus on general spike mitigation but not specific LN gradient amplification mechanism
- Break condition: Increasing the standard deviation of embeddings to be close to 1 (either by scaling embeddings by √d or applying LN to embeddings) reduces the LN gradient amplification.

### Mechanism 3
- Claim: Scaled initialization prevents residual branch explosion but creates LN gradient explosion
- Mechanism: While scaled initialization successfully reduces FFN(x) and Attention(x) variances below 1, preventing residual branch amplification, it doesn't address the LN gradient amplification issue. The combination of scaled initialization with embedding modifications is needed to prevent both types of explosions.
- Evidence anchors:
  - [abstract] "a popular initialization method... promotes the exploding gradients due to LNs although it mitigates the gradient explosion arising from the residual branch"
  - [section] "the scaled initialization prevents the rapid growth of the residual branch... However, in this figure, we prevent the explosion due to LNs to focus on only the explosion due to the residual branch. If we remove the modification, the gradient norms in the scaled initialization also explode"
  - [corpus] Moderate - AdaGC and other papers mention initialization effects but don't analyze this specific tradeoff
- Break condition: Without addressing LN gradient amplification (e.g., by using only scaled initialization without embedding modifications), loss spikes will still occur.

## Foundational Learning

- **Concept:** Normal distribution properties and variance propagation
  - Why needed here: The analysis relies on tracking how variances propagate through layer computations to predict gradient behavior
  - Quick check question: If two independent random variables with variance σ₁² and σ₂² are added, what is the variance of their sum?

- **Concept:** Layer normalization gradient behavior
  - Why needed here: Understanding how LN gradient norms scale with input standard deviation is crucial for analyzing exploding gradients
  - Quick check question: If the input to an LN has very small standard deviation, what happens to the LN gradient norm?

- **Concept:** Transformer architecture components (residual connections, attention mechanisms)
  - Why needed here: The exploding gradient mechanisms specifically involve how residual connections and attention outputs interact with initialization
  - Quick check question: In a residual connection, what happens to the norm when you repeatedly add outputs with standard deviation close to 1?

## Architecture Onboarding

- **Component map:** Embeddings → LN (optional) → Transformer layers → Output projection
  Each Transformer layer: LN → Attention → Residual add → LN → FFN → Residual add
  Initialization: scaled initialization with W2 and WO scaled by 1/√2N

- **Critical path:** Embedding initialization → LN gradient amplification → Residual branch amplification → Gradient explosion during backprop

- **Design tradeoffs:**
  - Scaled initialization prevents residual branch explosion but requires embedding modifications to prevent LN explosion
  - Applying LN to embeddings adds computational overhead but stabilizes training
  - Larger learning rates can be used with stabilization, improving performance

- **Failure signatures:**
  - Loss spikes occurring early in training (especially with large models)
  - Gradient norms increasing exponentially in shallow layers
  - Training divergence when using large learning rates without stabilization

- **First 3 experiments:**
  1. Compare gradient norms with plain vs scaled initialization to observe residual branch explosion
  2. Apply √d scaling to embeddings and observe LN gradient behavior
  3. Train with larger learning rates (1e-3) to test stability improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scaled initialization method affect the training stability and performance of LLMs across different model sizes and configurations?
- Basis in paper: [explicit] The paper mentions that the scaled initialization method is widely used in LLM pre-training and that it prevents the explosion due to the residual branch but causes the explosion due to LNs.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of scaled initialization on different model sizes and configurations, leaving uncertainty about its generalizability.
- What evidence would resolve it: Conducting experiments with various model sizes and configurations to observe the effects of scaled initialization on training stability and performance.

### Open Question 2
- Question: What are the specific factors that lead to the exploding gradients due to LNs, and how can they be mitigated effectively?
- Basis in paper: [explicit] The paper identifies the exploding gradients due to LNs as a significant issue and suggests that making the standard deviation of embeddings close to 1 can mitigate it.
- Why unresolved: The paper does not delve into the detailed mechanisms of how LNs contribute to exploding gradients, nor does it explore all possible mitigation strategies.
- What evidence would resolve it: Further theoretical and empirical analysis to uncover the underlying mechanisms of LN-induced exploding gradients and testing various mitigation strategies.

### Open Question 3
- Question: How do different sequence lengths impact the training stability and performance of LLMs, and what are the optimal sequence lengths for different model sizes?
- Basis in paper: [explicit] The paper mentions that using a short sequence stabilizes the training at the early stage and discusses the impact of sequence length on the exploding gradients issue.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between sequence lengths and training stability across different model sizes and configurations.
- What evidence would resolve it: Conducting experiments with varying sequence lengths for different model sizes to determine the optimal sequence lengths for training stability and performance.

## Limitations

- The analysis relies on theoretical variance propagation through normal distributions, which may not perfectly match real-world training dynamics
- The study focuses on GPT-2 architecture, and validation across different model architectures would strengthen the findings
- The paper doesn't address interactions with other optimization techniques like gradient clipping or weight decay

## Confidence

**High Confidence**: The identification of two primary mechanisms causing exploding gradients (residual branch amplification and LN gradient intensification) is well-supported by mathematical analysis and experimental evidence.

**Medium Confidence**: The claim that combining scaled initialization with embedding modifications consistently prevents loss spikes across all model sizes and configurations. While experiments show effectiveness, the paper doesn't provide extensive ablation studies across different model scales.

**Low Confidence**: The assertion that these modifications enable significantly better performance through larger learning rates. The experiments show lower perplexity scores, but without comparisons to other spike-mitigation techniques or downstream task performance analysis, it's unclear if this represents fundamental improvement or just faster convergence.

## Next Checks

1. **Cross-architecture validation:** Test the embedding modification techniques on BERT, T5, or other transformer variants to verify the generalizability of the findings beyond GPT-style models.

2. **Ablation study with other optimizers:** Compare the proposed method against other spike-mitigation techniques (like ZClip or SPAM) when using different optimizers and learning rate schedules to determine relative effectiveness.

3. **Downstream task performance analysis:** Evaluate whether the lower perplexity scores achieved with larger learning rates translate to improved performance on downstream tasks like GLUE benchmarks or question answering datasets.