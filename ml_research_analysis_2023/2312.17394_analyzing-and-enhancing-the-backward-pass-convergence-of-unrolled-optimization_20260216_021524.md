---
ver: rpa2
title: Analyzing and Enhancing the Backward-Pass Convergence of Unrolled Optimization
arxiv_id: '2312.17394'
source_url: https://arxiv.org/abs/2312.17394
tags:
- optimization
- which
- pass
- convergence
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the backward-pass convergence of unrolled optimization
  algorithms in deep learning. It shows that backpropagation through unrolled optimization
  is asymptotically equivalent to solving a linear system by fixed-point iteration.
---

# Analyzing and Enhancing the Backward-Pass Convergence of Unrolled Optimization

## Quick Facts
- arXiv ID: 2312.17394
- Source URL: https://arxiv.org/abs/2312.17394
- Reference count: 40
- Primary result: Folded Optimization system achieves faster and more accurate backpropagation through unrolled optimization by treating it as solving a linear system

## Executive Summary
This paper analyzes the backward-pass convergence of unrolled optimization algorithms in deep learning, revealing that backpropagation through unrolled optimization is asymptotically equivalent to solving a linear system by fixed-point iteration. This insight enables the construction of more efficient backpropagation rules from unrolled solver implementations. The proposed Folded Optimization system demonstrates computational efficiency and flexibility across various optimization problem forms, including nonconvex problems. Experimental results show advantages in both accuracy and speed compared to existing methods like cvxpy.

## Method Summary
The paper introduces Folded Optimization, a system for generating analytically differentiable optimization solvers from unrolled solver implementations. The core idea involves unfolding at a precomputed optimal solution to separate forward and backward passes, then using analytical models for Jacobians to solve the resulting linear system. The system provides three approaches: Linear Fixed-Point Iteration (LFPI), Krylov Subspace Methods (GMRes), and Jacobian Extraction. Implementation is provided through the fold-opt PyTorch library, which can interface with various blackbox solvers like Gurobi and IPOPT.

## Key Results
- Demonstrated computational advantages over cvxpy in accuracy and speed across multiple tasks
- Showed that backward-pass convergence rate depends on optimization stepsize choice
- Validated effectiveness on end-to-end optimization tasks including multilabel classification, portfolio optimization, and AC optimal power flow
- Established that Krylov subspace methods (GMRes) can improve backward-pass convergence compared to fixed-point iteration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The backward pass of unrolled optimization is asymptotically equivalent to solving a linear system by fixed-point iteration.
- Mechanism: When unrolled optimization iterations converge to an optimal solution x*, backpropagation through the unrolled computational graph converges to the solution of a linear system (I - Φ)J = Ψ, where Φ and Ψ are Jacobians derived from differentiating the update function U at the fixed point.
- Core assumption: The optimization algorithm (U) converges for the given parameters and the Jacobian Φ is nonsingular with spectral radius ρ(Φ) < 1.
- Evidence anchors:
  - [abstract]: "This paper provides theoretical insights into the backward pass of unrolled optimization, showing that it is asymptotically equivalent to the solution of a linear system by a particular iterative method."
  - [section]: "Theorem 1. The backward pass of an unfolding of algorithm (U), starting at the point xk = x*, is equivalent to linear fixed-point iteration on the linear system (DFP), and will converge to its unique solution at an asymptotic rate of - log ρ(Φ)."
- Break condition: The optimization algorithm fails to converge, Φ is singular, or ρ(Φ) ≥ 1.

### Mechanism 2
- Claim: Unfolding at a precomputed optimal solution separates forward and backward passes, enabling more efficient backpropagation.
- Mechanism: By initializing the unfolded optimization at x* = x*(c), the forward pass becomes an identity function, allowing the backward pass to be computed independently using analytical models for the Jacobians Φ and Ψ.
- Core assumption: The optimal solution x*(c) can be precomputed by a blackbox solver and the update function U is differentiable at (x*, c).
- Evidence anchors:
  - [abstract]: "Building on this analysis, the paper proposes a system for generating analytically differentiable optimization solvers from unrolled solver implementations called folded optimization."
  - [section]: "The essence of fixed-point folding is to use the computational graph of the update step U to backpropagate the function c → x*(c) by modeling and solving the linear system (DFP), after the optimal solution x*(c) is separately furnished by any blackbox optimization solver."
- Break condition: The update function U is not differentiable at (x*, c) or the optimal solution cannot be precomputed accurately.

### Mechanism 3
- Claim: Krylov subspace methods like GMRes can solve the linear system (DFP) more efficiently than fixed-point iteration, improving backward-pass convergence.
- Mechanism: Instead of using linear fixed-point iteration to solve (I - Φ)Tv = g, GMRes iteratively builds a Krylov subspace and finds the minimal residual solution within that subspace, converging faster when Φ is not a contractive mapping.
- Core assumption: The linear system (DFP) can be solved using only matrix-vector products with Φ and Ψ, without explicitly forming these matrices.
- Evidence anchors:
  - [abstract]: "Several practical pitfalls of unrolling are demonstrated in light of these insights, and a system called Folded Optimization is proposed to construct more efficient backpropagation rules from unrolled solver implementations."
  - [section]: "The fold-opt library implements a variant of GMRes to perform backpropagation as an alternative to LFPI."
- Break condition: The matrix-vector products with Φ and Ψ are not efficiently computable or the linear system is too ill-conditioned for GMRes to converge.

## Foundational Learning

- Concept: Automatic Differentiation (AD)
  - Why needed here: Unrolling optimization requires AD to propagate gradients through the entire chain of operations executed by an iterative optimization solver.
  - Quick check question: What is the primary difference between forward-mode and reverse-mode automatic differentiation, and which is typically used for backpropagation in neural networks?

- Concept: Implicit Function Theorem
  - Why needed here: The differential fixed-point conditions (DFP) are derived using the implicit function theorem to differentiate the fixed-point equation x* = U(x*, c) with respect to c.
  - Quick check question: State the implicit function theorem and explain how it is used to derive the DFP equation (I - Φ)J = Ψ.

- Concept: Spectral Radius and Convergence of Iterative Methods
  - Why needed here: The convergence rate of the backward pass using fixed-point iteration depends on the spectral radius ρ(Φ) of the Jacobian Φ, as shown in Theorem 1.
  - Quick check question: Define the spectral radius of a matrix and explain its relationship to the convergence of linear fixed-point iteration.

## Architecture Onboarding

- Component map:
  Folded Optimization System -> User Interface (fold-opt library) -> Backpropagation Algorithms -> Optimization Solvers
  User Interface -> Input: Differentiable update function U, blackbox optimization oracle x*(c) -> Output: Jacobian-gradient product g → gTJ

- Critical path:
  1. Precompute optimal solution x*(c) using blackbox solver
  2. Apply differentiable update function U once to obtain computational graph
  3. Choose backpropagation algorithm (LFPI, GMRes, or Jacobian Extraction)
  4. Compute Jacobian-gradient product g → gTJ using chosen algorithm
  5. Return gTJ as backpropagation result

- Design tradeoffs:
  - Accuracy vs. Efficiency: Using GMRes instead of LFPI improves convergence speed but increases computational cost per iteration
  - Expressiveness vs. Efficiency: Allowing blackbox solvers increases the range of optimization problems that can be handled but may sacrifice some computational efficiency compared to specialized differentiable solvers
  - Flexibility vs. Complexity: Providing multiple backpropagation algorithms gives users flexibility but increases the complexity of the system

- Failure signatures:
  - Slow convergence or divergence of the backward pass: May indicate that ρ(Φ) is too large or the linear system (DFP) is ill-conditioned
  - Inaccurate optimal solutions x*(c): May cause the backward pass to converge to incorrect gradients
  - High computational cost: May indicate that the chosen backpropagation algorithm is not well-suited for the problem at hand

- First 3 experiments:
  1. Implement and test f-PGDa on a simple constrained optimization problem (e.g., projected gradient descent on a quadratic function with linear constraints)
  2. Compare the convergence of LFPI and GMRes for backpropagation on a fixed-point unfolding problem
  3. Apply f-SQP to a nonlinear optimization problem with nonlinear constraints (e.g., portfolio optimization with risk constraints)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of stepsize in unfolded optimization affect the convergence rate of the backward pass, and can this be optimized independently of the forward pass?
- Basis in paper: [explicit] The paper shows that the convergence rate of the backward pass is highly dependent on the chosen stepsize α, and that this dependence becomes more pronounced as the choice of α leads to slower convergence.
- Why unresolved: While the paper demonstrates the effect of stepsize on backward-pass convergence, it does not explore methods to optimize the stepsize specifically for the backward pass, independent of the forward pass.
- What evidence would resolve it: Experiments comparing different stepsize optimization strategies for the backward pass, or theoretical analysis of the optimal stepsize choice for backward-pass convergence.

### Open Question 2
- Question: How do different linear system solvers (e.g., GMRes, Jacobian extraction) compare in terms of efficiency and accuracy for solving the differential fixed-point conditions in folded optimization?
- Basis in paper: [explicit] The paper presents three different approaches to solving the linear system (DFP) in folded optimization: LFPI, GMRes, and Jacobian extraction. It mentions that GMRes typically converges in fewer iterations than LFPI, but also discusses the trade-offs in terms of computational cost.
- Why unresolved: While the paper provides some theoretical analysis of the different solvers, it does not provide a comprehensive empirical comparison of their efficiency and accuracy across different problem classes.
- What evidence would resolve it: A systematic study comparing the performance of different solvers on a variety of optimization problems, measuring both convergence speed and accuracy.

### Open Question 3
- Question: How can folded optimization be extended to handle more complex optimization problems, such as those with non-smooth or non-convex constraints?
- Basis in paper: [inferred] The paper focuses on optimization problems with smooth objective functions and constraints, and mentions that some optimization subproblems (e.g., projections) may require iterative methods that are difficult to differentiate.
- Why unresolved: The paper does not explore how folded optimization can be adapted to handle optimization problems with non-smooth or non-convex constraints, which are common in many real-world applications.
- What evidence would resolve it: Theoretical analysis of the extension of folded optimization to handle non-smooth or non-convex constraints, along with empirical results demonstrating its effectiveness on such problems.

## Limitations
- Analysis assumes convergence of unrolled optimization algorithm, which may not hold for all problem instances or hyperparameter settings
- Computational benefits depend on availability of accurate blackbox solvers for precomputing optimal solutions
- Experimental evaluation is limited to specific problem domains and does not exhaustively compare against all relevant baselines

## Confidence
- Theoretical claims regarding asymptotic equivalence are supported by rigorous mathematical derivations (High confidence)
- Folded Optimization framework builds logically on these insights (Medium confidence)
- Experimental results demonstrating computational advantages are promising but based on limited benchmarks (Medium confidence)

## Next Checks
1. **Convergence robustness testing**: Systematically evaluate backward-pass convergence across varying problem scales, initialization conditions, and optimization algorithm parameters to identify failure modes and characterize reliable conditions.
2. **Cross-baseline comparison**: Implement and benchmark against additional differentiable optimization frameworks beyond cvxpy to better contextualize reported performance improvements.
3. **Scaling analysis**: Assess computational efficiency for large-scale optimization problems (e.g., high-dimensional control systems or large portfolio optimization tasks) to evaluate practical scalability beyond demonstrated examples.