---
ver: rpa2
title: Search Strategies for Self-driving Laboratories with Pending Experiments
arxiv_id: '2312.03466'
source_url: https://arxiv.org/abs/2312.03466
tags:
- function
- experiments
- test
- asynchronous
- delay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a simulation framework to study optimization
  strategies for self-driving laboratories (SDLs) with delayed feedback from asynchronous
  parallel experiments. The simulator is built using real SDL data and three test
  functions (Ackley, Levy, and SDL).
---

# Search Strategies for Self-driving Laboratories with Pending Experiments

## Quick Facts
- arXiv ID: 2312.03466
- Source URL: https://arxiv.org/abs/2312.03466
- Reference count: 34
- Key outcome: Introduces a simulation framework to study optimization strategies for self-driving laboratories with delayed feedback from asynchronous parallel experiments, finding that delay increases cumulative regret and no single strategy consistently outperforms others.

## Executive Summary
This paper presents a simulation framework to evaluate optimization strategies for self-driving laboratories (SDLs) operating with delayed feedback from asynchronous parallel experiments. The simulator is built using real SDL data and three test functions (Ackley, Levy, and SDL) to model the trade-off between experimental throughput and optimizer performance. Four optimization strategies—random, expected improvement, noisy expected improvement, and mode cycling—are compared across different delay amounts and problem dimensionalities. The results demonstrate that increasing delay and dimensionality both negatively impact optimizer performance, with dimensionality having a larger effect, and suggest that the choice of optimization strategy should be tailored to the specific problem characteristics.

## Method Summary
The authors developed a Bayesian optimization simulator that models pending experiments in SDLs with configurable delay and dimensionality. The simulator uses a ground truth Gaussian Process model built from real SDL data (177 experiments for conductivity optimization) and synthetic test functions (Ackley, Levy, and SDL). Four optimization strategies (random, EI, qNEI, mode cycling) were implemented with pending points masks. The simulation ran 30 optimization trials with 10 random initial points and 100 total observations, testing delays of 0, 1, 3, 5, 7 and dimensions of 3, 5, 7. Cumulative regret was used as the primary performance metric, calculated as the area between the running best average curve and the global maximum.

## Key Results
- Delay increases cumulative regret in Bayesian optimization, with regret growing as delay and dimensionality increase
- Increasing problem dimensionality has a larger impact on optimizer performance than increasing delay
- No single optimization strategy consistently outperforms others across different delay and dimensionality scenarios
- The trade-off between experimental throughput and optimizer performance is problem-dependent

## Why This Works (Mechanism)

### Mechanism 1
Delayed feedback from asynchronous parallel experiments increases cumulative regret in Bayesian optimization. When experiments run in parallel across multiple stages, new experiments must be chosen before previous ones complete, forcing the optimizer to use incomplete information and leading to suboptimal decisions. This assumes the ground truth response surface remains static during optimization.

### Mechanism 2
Increasing problem dimensionality has a larger impact on optimizer performance than increasing delay. Higher dimensional search spaces create exponentially more local optima and complex response surfaces, making the optimization problem fundamentally harder regardless of feedback timing. This assumes the increase in regret from dimensionality is additive to the increase from delay.

### Mechanism 3
No single optimization strategy consistently outperforms others across different delay and dimensionality scenarios. Different strategies have complementary strengths—EI balances exploration/exploitation, qNEI accounts for noise, and mode cycling adapts between strategies—but none dominates uniformly across all problem conditions.

## Foundational Learning

- Concept: Bayesian Optimization with Gaussian Processes
  - Why needed here: The simulator uses GP models to represent both synthetic test functions and real SDL data, and all optimization strategies rely on GP posterior predictions
  - Quick check question: What are the two components that make up the acquisition function in EI, and how do they balance exploration vs exploitation?

- Concept: Asynchronous Parallel Computing and Delayed Feedback
  - Why needed here: The core contribution involves understanding how parallel experiment execution affects optimization performance through delayed feedback
  - Quick check question: If an SDL has 4 stages and each experiment takes 10 minutes, what is the minimum delay between when an experiment is initiated and when its result becomes available to the optimizer?

- Concept: Cumulative Regret as Performance Metric
  - Why needed here: The paper uses cumulative regret to quantify optimizer performance, requiring understanding of how it's calculated and what it represents
  - Quick check question: If the global maximum is 10.0 and the running best after 50 iterations averages 8.5, what is the cumulative regret contribution from iteration 50?

## Architecture Onboarding

- Component map: Simulator core -> Ground truth model (GP) -> Optimization strategies (random, EI, qNEI, mode cycling) -> Performance tracking (cumulative regret) -> Analysis pipeline
- Critical path: 1. Initialize GP model with random points 2. Select next experiment using chosen acquisition function 3. Simulate experiment result with noise 4. Update GP model with new observation 5. Track performance metrics 6. Repeat until convergence or max iterations
- Design tradeoffs: Using synthetic functions vs real data allows controlled experiments but may not capture real material behavior complexity; fixed number of trials (30) provides statistical significance but limits exploration; fixed initial points (10) simplifies comparison but may not be optimal
- Failure signatures: EI and qNEI performing worse than random search indicates implementation error or GP model misspecification; cumulative regret decreasing with increased delay suggests incorrect delay implementation or metric calculation error; mode cycling failing to converge points to incorrect cycle length or acquisition function parameters
- First 3 experiments: 1. Run all strategies on Ackley function with delay=0, dimensions=3 to establish baseline performance 2. Increase delay to 3 on same Ackley setup to observe impact of delayed feedback 3. Increase dimensions to 5 on delay=3 Ackley to examine combined effect of delay and dimensionality

## Open Questions the Paper Calls Out

### Open Question 1
Which specific acquisition strategy performs best in SDLs with delayed feedback, and under what conditions? The authors found no evidence that one strategy outperforms others in the presence of delay, especially in higher dimensions. This remains unresolved due to limited trials, iterations, and strategy testing. More extensive testing with additional trials, iterations, and strategies would help determine optimal performance conditions.

### Open Question 2
How does the trade-off between experimental throughput and optimizer performance change with different SDL configurations and problem characteristics? The authors note this trade-off must be balanced by SDL operators and that the decision seems problem-dependent. This remains unresolved because only a limited range of configurations were tested. Testing a wider range of SDL setups and problem characteristics would help determine how the trade-off varies.

### Open Question 3
How well do the simulation results generalize to real-world SDLs and other optimization problems? The authors note their results are true only for the three test functions presented and may vary when exploring other material spaces. This remains unresolved because the study used limited test functions and simulated environments. Implementing strategies on real-world SDLs and testing them on diverse optimization problems would validate generalizability.

## Limitations
- Simulation relies on synthetic test functions that may not fully capture real material discovery complexity
- Fixed mode cycling cycle length (20 iterations) was chosen arbitrarily and may not be optimal
- Limited statistical analysis of the trade-off between dimensionality and delay impact

## Confidence

- Claim: Delayed feedback reduces Bayesian optimizer performance | Confidence: Medium
- Claim: Dimensionality has larger impact than delay on optimizer performance | Confidence: Low
- Claim: No single strategy consistently outperforms others | Confidence: Medium

## Next Checks

1. Perform formal statistical tests (e.g., paired t-tests or ANOVA) on cumulative regret results across different strategies, delays, and dimensions to quantify whether observed differences are statistically significant

2. Deploy the best-performing strategies on an actual SDL system to verify that simulator predictions match real-world performance, particularly focusing on the dimensionality vs delay trade-off claim

3. Conduct experiments varying the mode cycling cycle length and other strategy hyperparameters to determine optimal configurations for different delay and dimensionality scenarios