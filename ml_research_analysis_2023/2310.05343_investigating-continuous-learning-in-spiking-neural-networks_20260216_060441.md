---
ver: rpa2
title: Investigating Continuous Learning in Spiking Neural Networks
arxiv_id: '2310.05343'
source_url: https://arxiv.org/abs/2310.05343
tags:
- learning
- classes
- training
- mnist
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigated the use of spiking neural networks (SNNs)
  for continuous learning and compared their performance to conventional models. Three
  phases of experimentation were conducted: transfer learning with conventional models
  (ResNet50, ResNet101, VGG19), training a Nengo model, and converting conventional
  models to SNNs.'
---

# Investigating Continuous Learning in Spiking Neural Networks

## Quick Facts
- arXiv ID: 2310.05343
- Source URL: https://arxiv.org/abs/2310.05343
- Reference count: 27
- Primary result: SNNs retain partial information about previous classes during incremental learning but still struggle with accurate classification of old classes.

## Executive Summary
This study investigates continuous learning capabilities of spiking neural networks (SNNs) compared to conventional deep learning models. The research demonstrates that while conventional models suffer from catastrophic forgetting with accuracy dropping below 20% on previous classes, SNN models show evidence of retaining some information about earlier learned classes through elevated output probabilities. The findings suggest that SNNs have potential to address catastrophic forgetting, though they currently lack sufficient information retention to correctly identify previous classes.

## Method Summary
The research employed three experimental phases using MNIST and Fashion MNIST datasets. First, conventional models (ResNet50, ResNet101, VGG19) were trained using transfer learning and tested for catastrophic forgetting. Second, a Nengo model was trained incrementally. Third, conventional models were converted to SNNs using NengoDL. The training involved incremental addition of two classes at a time with 10 epochs per phase, evaluating retention of previous classes. SNNs used firing rates between 10-20 with synapse smoothing at 0.001 for regularization.

## Key Results
- Conventional models experienced catastrophic forgetting, dropping below 20% accuracy on previous classes
- SNN models retained partial information about previous classes, showing elevated output probabilities for correct classes
- SNN inference latency increased with firing rate regularization and new class additions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SNN models retain partial information about previous classes during incremental learning.
- Mechanism: Temporal spike patterns encode class-specific firing sequences, allowing residual activation to persist across training phases.
- Core assumption: Spiking neurons maintain graded response patterns that can reflect prior learned associations.
- Evidence anchors:
  - [abstract] "SNN models were able to retain some information about previous classes. Although many of the previous classes were still identified as the current trained classes, the output probabilities showed a higher than normal value to the actual class."
  - [section] "It is shown that after incremental training the SNNs are able to retain some level of information from the previous classes when compared to conventional models, but they still are unable retain enough information to correctly identify them from the current classes used for training."
  - [corpus] Weak - related papers focus on SNN architecture and performance but not specifically on catastrophic forgetting retention.
- Break condition: If spike timing precision degrades below a threshold, residual class encoding is lost.

### Mechanism 2
- Claim: Converting conventional models to SNNs introduces latency that may allow retention of older class information.
- Mechanism: The conversion process expands temporal resolution, spreading inference over multiple timesteps, which may preserve weaker class activations.
- Core assumption: Longer inference time creates a temporal buffer for lower-amplitude spike patterns.
- Evidence anchors:
  - [abstract] "Although many of the previous classes were still identified as the current trained classes, the output probabilities showed a higher than normal value to the actual class."
  - [section] "With each variation in firing rate to regularize the model as well as when new classes are added, the latency of inference becomes larger as illustrated in the second frame of the figures."
  - [corpus] Weak - latency is discussed but not directly linked to forgetting mitigation in cited works.
- Break condition: If latency increases beyond a point where inference remains stable, performance may degrade.

### Mechanism 3
- Claim: Spike rate regularization balances new learning with old knowledge preservation.
- Mechanism: L2 regularization of spike rates limits extreme shifts in neuron activation patterns, preserving traces of previous class representations.
- Core assumption: Controlled spike rate smoothing prevents drastic overwriting of synaptic weights tied to prior classes.
- Evidence anchors:
  - [section] "Regularization and optimization for the SNNs are different as the goal is to optimize the rate of firing of each neuron along with smoothing the synapses for the regularization."
  - [corpus] Weak - regularization is mentioned in corpus but not explicitly tied to catastrophic forgetting.
- Break condition: If regularization strength is too high, learning of new classes is inhibited.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Explains the baseline problem that SNNs are attempting to address.
  - Quick check question: What happens to accuracy on old classes when a model is trained on new ones without retention strategies?

- Concept: Spiking neural network firing dynamics
  - Why needed here: Understanding how spike timing and rate affect information retention is key to interpreting SNN behavior.
  - Quick check question: How does the timing of neuron spikes differ from continuous activation in conventional networks?

- Concept: Transfer learning and incremental learning
  - Why needed here: The study uses pretrained models and adds classes incrementally, so understanding these paradigms is essential.
  - Quick check question: In transfer learning, what is preserved from the original model, and what is retrained?

## Architecture Onboarding

- Component map:
  Input layer → Convolutional layers (with spiking neurons) → Output layer (classification)

- Critical path:
  1. Load pretrained ResNet/VGG model.
  2. Convert to SNN using NengoDL.
  3. Apply incremental training on two classes at a time.
  4. Evaluate retention of previous classes.

- Design tradeoffs:
  - Latency vs. accuracy: Higher spike rates improve accuracy but increase inference time.
  - Conversion fidelity vs. training efficiency: Converting from CNNs preserves some structure but loses temporal dynamics.
  - Regularization strength vs. plasticity: Stronger regularization preserves old knowledge but slows new learning.

- Failure signatures:
  - Complete collapse of previous class accuracy.
  - Output probabilities showing no elevation for correct previous classes.
  - Spike patterns become uniform across all classes.

- First 3 experiments:
  1. Train a small CNN on MNIST, then incrementally add classes and measure forgetting.
  2. Convert the same CNN to SNN and repeat incremental training, compare retention.
  3. Vary spike rate regularization strength and observe impact on forgetting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms in biological dreaming processes could be adapted to improve information retention in SNNs for continuous learning?
- Basis in paper: [explicit] The paper suggests mimicking biological dreaming processes to strengthen old information as a potential future direction.
- Why unresolved: The paper does not provide details on how dreaming processes could be implemented in SNNs or what specific aspects of dreaming are most relevant to information retention.
- What evidence would resolve it: Empirical studies demonstrating improved information retention in SNNs after implementing dream-like processes based on biological dreaming mechanisms.

### Open Question 2
- Question: How does the performance of vision transformers compare to conventional models and SNNs when applied to continuous learning tasks?
- Basis in paper: [explicit] The paper mentions investigating other successful architectures such as vision transformers as a future direction.
- Why unresolved: The paper does not include any experiments or comparisons involving vision transformers in the context of continuous learning.
- What evidence would resolve it: Comparative experiments showing the performance of vision transformers versus conventional models and SNNs on continuous learning tasks, including measures of catastrophic forgetting.

### Open Question 3
- Question: What are the optimal neuron firing rates and synapse smoothing values for SNNs to balance accuracy and inference time in continuous learning scenarios?
- Basis in paper: [explicit] The paper discusses optimizing neuron firing rates and synapse smoothing for regularization but notes the trade-off between accuracy and inference time.
- Why unresolved: The paper provides a general range for optimal values but does not explore the full parameter space or provide a definitive set of optimal values for continuous learning tasks.
- What evidence would resolve it: Systematic experiments varying neuron firing rates and synapse smoothing values to identify the optimal combination that maximizes accuracy while minimizing inference time in continuous learning scenarios.

## Limitations

- The SNN models show elevated output probabilities for previous classes but still fail to correctly classify them
- Specific SNN architecture parameters and conversion details remain unclear
- Evidence is primarily based on probability outputs rather than actual classification accuracy on old classes

## Confidence

- SNN retention of partial information: High
- Catastrophic forgetting in conventional models: High
- SNN ability to overcome catastrophic forgetting: Low
- Spike timing encoding mechanism: Medium
- Conversion process parameters: Low

## Next Checks

1. Test if SNN probability elevation translates to measurable accuracy improvement on previous classes using class-specific thresholding
2. Compare retention across different SNN conversion parameters (spike rates, synapse smoothing values) to identify optimal configurations
3. Implement ablation study removing regularization to quantify its contribution to retention versus pure temporal encoding effects