---
ver: rpa2
title: Large-scale Language Model Rescoring on Long-form Data
arxiv_id: '2306.08133'
source_url: https://arxiv.org/abs/2306.08133
tags:
- rescoring
- language
- data
- en-in
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the application of large-scale language models
  (LLMs) like T5 and PaLM to rescore automatic speech recognition (ASR) hypotheses
  for long-form data, specifically YouTube videos. By integrating LLMs into the ASR
  decoding framework, the authors demonstrate up to 8% relative reduction in Word
  Error Rate (WER) and up to 30% relative reduction in Salient Term Error Rate (STER)
  over strong baselines.
---

# Large-scale Language Model Rescoring on Long-form Data

## Quick Facts
- arXiv ID: 2306.08133
- Source URL: https://arxiv.org/abs/2306.08133
- Reference count: 0
- This paper demonstrates up to 8% relative WER reduction and 30% relative STER reduction on long-form YouTube video data using LLM rescoring.

## Executive Summary
This paper explores the application of large-scale language models (LLMs) like T5 and PaLM to rescore automatic speech recognition (ASR) hypotheses for long-form data, specifically YouTube videos. By integrating LLMs into the ASR decoding framework, the authors demonstrate significant improvements in both Word Error Rate (WER) and Salient Term Error Rate (STER) over strong baselines. The work introduces technical advances in lattice processing to avoid tree-like digraph topology and demonstrates that LLM gains are additive with conventional neural language models. The results highlight the potential of LLMs for improving ASR performance on long-form, conversational speech data.

## Method Summary
The authors train conformer-based first-pass ASR models using Hybrid Autoregressive Transducer (HAT) factorization with limited label context (n=2) on YouTube and Voice Search data. They generate segment lattices using VAD-based segmentation and rescore these lattices with T5/MT5 and PaLM LLMs trained on C4/MC4 and WEBDOC corpora. The LLMs are fine-tuned for span corruption and prefix LM tasks. During rescoring, context from the previous 1-best hypothesis is carried over to maintain topic coherence across segment boundaries. The final hypothesis is selected by combining LLM scores with internal LM and acoustic likelihoods using HAT factorization.

## Key Results
- Up to 8% relative reduction in Word Error Rate (WER) on YouTube en-us and en-in data
- Up to 30% relative reduction in Salient Term Error Rate (STER) on YouTube en-in data
- LLM gains are additive with conventional neural LMs through proper density ratio modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Improved lattice quality through limited context and path merging significantly enhances LLM rescoring performance.
- Mechanism: By constraining the ASR model to use only the previous two labels (n=2), multiple hypotheses sharing the same context collapse into a single state. This creates a non-tree digraph topology where the number of paths grows polynomially rather than exponentially, reducing lattice redundancy and improving diversity.
- Core assumption: A non-tree digraph lattice with controlled path growth is more informative for rescoring than a trie-like tree lattice.
- Evidence anchors:
  - [section] "When using an ASR model where the label context is bound by n [34], beam-search hypotheses sharing the same label context of length n will correspond to the same state in the segment lattice. This results in lattice with a proper (non-tree) digraph topology where the number of paths can grow up to exponentially in the number of states."
  - [section] "This was shown to lead to a significant improvement in lattice quality: lattice diversity improvement and oracle WER reduction [34]."
- Break condition: If the context window n is increased significantly (e.g., n>3), the path explosion would negate the benefits of state merging and degrade lattice quality.

### Mechanism 2
- Claim: Carrying 1-best hypothesis context from previous segments improves rescoring accuracy.
- Mechanism: During lattice rescoring, the 1-best hypothesis from the previous segment(s) is appended to the input of the LLM. This provides continuity across segment boundaries, allowing the LLM to maintain topic coherence and better predict rare terms that span segments.
- Core assumption: Long-form ASR segments are contextually dependent, and discontinuity between segments hurts recognition of rare, topic-specific terms.
- Evidence anchors:
  - [abstract] "Improved lattice processing that results in a lattice with a proper (non-tree) digraph topology and carrying context from the 1-best hypothesis of the previous segment(s) results in significant wins in rescoring with LLMs."
  - [section] "We observe that carrying over previous context outperforms no context. However, longer contexts do not seem to provide additional wins."
- Break condition: If segments are truly independent (e.g., separate utterances), carrying context would add noise rather than signal.

### Mechanism 3
- Claim: LLM gains are additive with conventional neural LMs through proper density ratio modeling.
- Mechanism: The HAT factorization framework allows separation of the internal LM score from the acoustic likelihood. During rescoring, the external LLM score is interpolated after subtracting the internal LM contribution, preserving the benefits of both models without interference.
- Core assumption: The internal LM and external LLM capture complementary aspects of the language model - the former learns acoustic co-occurrence patterns, the latter benefits from vast text-only training.
- Evidence anchors:
  - [section] "Thus, inference search maximizes: S(Y, X ) = SHA T(Y|X) − µS ILM(Y ) + νS ELM(Y ), (3)"
  - [section] "We also find that the gains in performance from the combination of LLMs trained on vast quantities of available data (such as C4) and conventional neural LMs is additive and significantly outperforms a strong first-pass baseline with a maximum entropy LM."
- Break condition: If the external LLM is trained on data too similar to the internal LM's training, the additive gains would diminish.

## Foundational Learning

- Concept: HAT (Hybrid Autoregressive Transducer) factorization
  - Why needed here: HAT enables clean separation of acoustic and language model components, critical for proper external LM integration during rescoring.
  - Quick check question: What are the three components of the HAT score, and how does each contribute to the final hypothesis score?

- Concept: Lattice rescoring vs. first-pass decoding
  - Why needed here: Understanding how hypotheses are represented in lattices and how rescoring modifies their scores is essential for implementing LLM rescoring correctly.
  - Quick check question: How does a non-tree digraph lattice differ structurally from a trie, and why does this matter for hypothesis diversity?

- Concept: Code-switching in ASR
  - Why needed here: The en-in task involves frequent English-Hindi switching, requiring specialized fine-tuning of multilingual LMs and understanding of transliteration impacts.
  - Quick check question: Why was Hindi data transliterated to Latin script for the MT5 fine-tuning, and what challenge does this address?

## Architecture Onboarding

- Component map: ASR first-pass (conformer encoder + HAT decoder) -> Lattice generation -> Segment VAD -> LLM rescoring (with context) -> Final hypothesis selection. Key external components: T5/MT5/PaLM models, conformer neural LM, MaxEnt LM.

- Critical path: Audio -> conformer encoder -> HAT decoder (limited context) -> lattice (non-tree digraph) -> segment boundaries -> 1-best context carry -> LLM rescoring -> final output. Any bottleneck in lattice quality or context propagation directly impacts final WER.

- Design tradeoffs: Limited label context (n=2) improves lattice quality but may miss longer-range dependencies; carrying context from previous segments improves coherence but adds latency; larger LLMs give better performance but increase computational cost and may overfit to domain.

- Failure signatures: No improvement from LLM rescoring suggests lattice quality issues (e.g., too few hypotheses, poor diversity); degradation with context carry suggests segment independence; additive gains not materializing suggests internal/external LM overlap.

- First 3 experiments:
  1. Compare WER with and without context carry on dev set to confirm Mechanism 2.
  2. Disable state merging (increase n) and measure oracle WER increase to validate Mechanism 1.
  3. Replace LLM with a smaller conformer LM in rescoring to confirm the additive gains of Mechanism 3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on other code-switching language pairs beyond en-in (English-Hindi)?
- Basis in paper: [explicit] The paper focuses on en-in code-switching and notes the imbalance in training data for Hindi compared to English.
- Why unresolved: The study only explores one code-switching language pair, leaving generalization to other language pairs unclear.
- What evidence would resolve it: Experiments evaluating LLMs on code-switching tasks involving different language pairs, such as Spanish-English or Mandarin-English.

### Open Question 2
- Question: What is the impact of increasing the size of the fine-tuning corpus for code-switched languages on LLM performance?
- Basis in paper: [inferred] The paper notes that MT5's performance on en-in improves with fine-tuning on WEBDOC, suggesting data size and quality are important.
- Why unresolved: The study uses a relatively small fine-tuning corpus (WEBDOC) compared to the massive pre-training data, and the effect of scaling this corpus is unknown.
- What evidence would resolve it: Experiments comparing LLM performance on code-switching tasks using fine-tuning corpora of varying sizes and quality.

### Open Question 3
- Question: How do LLMs compare to conventional LMs in terms of computational efficiency during rescoring?
- Basis in paper: [explicit] The paper demonstrates that LLMs outperform conventional LMs in terms of WER and STER but does not discuss computational efficiency.
- Why unresolved: The study focuses on accuracy improvements but does not address the trade-off between accuracy gains and computational costs.
- What evidence would resolve it: Experiments measuring the computational resources (e.g., time, memory) required for LLM rescoring compared to conventional LM rescoring.

## Limitations

- Lattice processing complexity: The paper claims significant improvements from non-tree digraph lattice topology, but the exact mechanism for achieving this structure during ASR decoding is not fully specified, creating uncertainty about reproducibility.

- Context carry optimization: While benefits from carrying 1-best hypothesis context are demonstrated, the optimal context length appears to plateau quickly without exploring whether this is due to diminishing returns or potential negative effects from accumulated noise.

- Domain generalization: The additive gains from combining LLMs with conventional neural LMs assume complementary learning, but the paper does not extensively test how these gains hold when LLM training data significantly overlaps with the internal LM's training data.

## Confidence

**High confidence:** The claim that LLM rescoring provides up to 8% relative WER reduction and 30% relative STER reduction on long-form data is well-supported by experimental results across multiple model sizes and languages.

**Medium confidence:** The claim about lattice quality improvement through non-tree digraph topology is theoretically justified but relies on specific implementation details that are not fully specified.

**Low confidence:** The claim that additive gains are consistently achievable across all domain combinations is supported by limited experimentation and does not extensively test cross-domain generalization.

## Next Checks

1. **Lattice structure verification:** Implement the non-tree digraph lattice processing and measure the actual path growth rate and diversity metrics. Compare oracle WER and path counts with a baseline trie structure to verify the claimed improvements in lattice quality.

2. **Context length ablation study:** Systematically test context carry lengths from 0 to 5 previous segments on the development set. Measure not only WER/STER but also analyze whether longer contexts introduce noise by examining hypothesis consistency and rare term recognition patterns.

3. **Cross-domain generalization test:** Train the LLM rescoring system on YouTube data and evaluate on a completely different long-form domain (e.g., podcasts, audiobooks, or conference recordings). Measure whether the additive gains from LLM + neural LM combination persist when the acoustic and linguistic characteristics differ substantially from the training data.