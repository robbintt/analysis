---
ver: rpa2
title: 'AudioFormer: Audio Transformer learns audio feature representations from discrete
  acoustic codes'
arxiv_id: '2308.07221'
source_url: https://arxiv.org/abs/2308.07221
tags:
- audio
- learning
- tasks
- acoustic
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AudioFormer, a transformer-based framework
  that treats audio classification as natural language understanding. The method uses
  a neural audio codec (EnCodec) to discretize audio signals into acoustic codes,
  which are then processed by a masked language model (MLM) using RoFormer as the
  backbone.
---

# AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes

## Quick Facts
- arXiv ID: 2308.07221
- Source URL: https://arxiv.org/abs/2308.07221
- Authors: 
- Reference count: 40
- Key outcome: AudioFormer achieves state-of-the-art single-modal audio classification performance with mAP scores of 53.9 on AudioSet-2M, 45.1 on AudioSet-20k, and 66 on FSD50K

## Executive Summary
This paper introduces AudioFormer, a transformer-based framework that treats audio classification as natural language understanding. The method uses a neural audio codec (EnCodec) to discretize audio signals into acoustic codes, which are then processed by a masked language model (MLM) using RoFormer as the backbone. To further improve performance, the authors propose Multi-Positive sample Contrastive (MPC) learning, which leverages multiple acoustic codes from the same audio as positive examples. The model is pre-trained on AudioSet-2M and fine-tuned on downstream tasks, achieving state-of-the-art performance in single-modal audio classification.

## Method Summary
AudioFormer converts audio waveforms into discrete acoustic codes using EnCodec, then applies a RoFormer-based transformer with MLM pretraining. The model processes 4 quantized acoustic codes from each audio input, using the first code as an anchor for both MLM and MPC contrastive learning. During fine-tuning, two methods are employed: FT1 with standard classification head, and FT2 with MPC learning that pulls positive samples (other acoustic codes from the same audio) closer to the anchor representation.

## Key Results
- Achieves mAP of 53.9 on AudioSet-2M, 45.1 on AudioSet-20k, and 66 on FSD50K
- Outperforms existing multimodal audio-visual models on select datasets
- State-of-the-art performance in single-modal audio classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating audio classification as natural language understanding (NLU) enables transfer of proven NLP techniques like masked language modeling (MLM)
- Mechanism: Discrete acoustic codes produced by a neural audio codec (EnCodec) act as "tokens" analogous to text tokens. A transformer trained with MLM can learn to reconstruct masked tokens, capturing contextual relationships across the audio sequence
- Core assumption: The discrete acoustic codes preserve sufficient semantic and acoustic information to allow meaningful reconstruction, similar to word tokens in text
- Evidence anchors: [abstract] "we introduce a novel perspective by considering the audio classification task as a form of natural language understanding (NLU)" and [section] "we introduce neural audio codec technology into the domain of audio comprehension tasks, treating audio comprehension as a facet of NLU"
- Break condition: If the discrete codes lose critical information during quantization, the MLM will fail to reconstruct meaningful patterns, and downstream performance will degrade

### Mechanism 2
- Claim: Multi-Positive sample Contrastive (MPC) learning leverages the inherent structure of acoustic codes to improve representation quality
- Mechanism: Multiple quantized acoustic codes (C1…C4) from the same audio are treated as positive samples for each other. Contrastive learning pulls these together in representation space, allowing the anchor (C1) to absorb knowledge from the other codes while retaining its own discriminative features
- Core assumption: The quantized codes from the same audio contain complementary information, with C1 carrying the richest content and others carrying progressively less but still useful context
- Evidence anchors: [abstract] "pioneer the integration of a Multi-Positive sample Contrastive (MPC) learning approach. This method enables the learning of joint representations among multiple discrete acoustic codes within the same audio input" and [section] "we introduced a Multi-Positive sample Contrastive (MPC) learning mechanism. Taking the first row of acoustic codes (QT1) as the anchor, we pulled the remaining rows of codes (QT1…N) closer to the anchor"
- Break condition: If the acoustic codes are too redundant or the contrastive signal is too weak, the MPC loss will not provide meaningful gradients, and model performance may plateau or degrade

### Mechanism 3
- Claim: RoPE (Rotary Position Embeddings) enable effective modeling of long audio sequences without losing relative positional information
- Mechanism: RoPE modifies attention scores so that the attention between tokens at positions m and n depends only on their relative offset (m-n). This allows the transformer to learn contextual relationships over sequences longer than typical NLP models (750 tokens for 10s audio vs 512 for BERT)
- Core assumption: The relative position information encoded by RoPE is sufficient to capture the sequential dependencies in audio, even though audio tokens are not naturally ordered like words in a sentence
- Evidence anchors: [section] "RoPE has been demonstrated to excel in learning representations of long texts... AudioFormer faces the challenge of dealing with longer input lengths compared to models like BERT" and [section] "RoPE is a form of static relative position encoding... It modifies the standard attention calculation for attention scores"
- Break condition: If the audio sequence contains highly redundant or periodic patterns, relative position encoding may not provide enough discriminative signal, leading to collapsed representations

## Foundational Learning

- Concept: Discrete audio tokenization via neural codecs
  - Why needed here: AudioFormer relies on converting continuous audio waveforms into discrete acoustic codes that can be processed by a transformer like text tokens. Without this step, the MLM framework cannot be applied
  - Quick check question: What is the output dimension of EnCodec for a 10-second audio clip, and how many quantizers are used?

- Concept: Masked Language Modeling (MLM) pretraining
  - Why needed here: MLM is the core pretraining objective that forces the model to learn contextual representations by reconstructing masked tokens. This is essential for building a strong audio feature extractor
  - Quick check question: Why does AudioFormer use a 25% masking rate instead of the typical 15% used in BERT?

- Concept: Contrastive learning with multiple positive samples
  - Why needed here: MPC learning uses the multiple quantized codes from the same audio as positive pairs, improving the model's ability to capture joint representations. This is a key differentiator from standard single-code approaches
  - Quick check question: In MPC, which acoustic code is chosen as the anchor and why?

## Architecture Onboarding

- Component map: EnCodec tokenizer -> RoFormer backbone -> MLM head -> Classification head -> MPC loss module
- Critical path: Input audio → EnCodec → 4 acoustic code matrices → Select C1 (anchor) → feed to RoFormer → masked tokens → Compute MLM loss + MPC loss → Backpropagate to update only anchor model parameters
- Design tradeoffs:
  - Using discrete codes instead of Mel spectrograms reduces input dimensionality but requires careful quantization to preserve information
  - MPC learning improves performance but adds training complexity and requires freezing parameters of C2…C4 during fine-tuning
  - RoPE enables long sequence modeling but may not capture all temporal nuances compared to learned positional embeddings
- Failure signatures:
  - Poor MLM reconstruction accuracy → codes are losing too much information during quantization
  - MPC loss dominates or vanishes → improper temperature scaling or redundant positive samples
  - Downstream task performance plateaus → model is overfitting to AudioSet-2M or masking rate is suboptimal
- First 3 experiments:
  1. Train with only MLM (no MPC) on AudioSet-2M and evaluate mAP on AudioSet-20k to establish baseline
  2. Vary masking rate (15%, 25%, 35%) and compare downstream performance to find optimal setting
  3. Test MPC with different temperature (τ) values and α weights to tune contrastive learning strength

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- MPC loss implementation details lack explicit specifications for Circle Loss and LSEP loss parameters
- Comparative analysis against multimodal models is limited to a few datasets
- No quantitative analysis of information preservation during EnCodec quantization

## Confidence
- High Confidence: The core MLM pretraining framework with RoFormer backbone and treating audio classification as NLU are well-established and clearly explained
- Medium Confidence: The MPC learning mechanism shows performance improvements but exact implementation details create some uncertainty
- Low Confidence: Generalization to other domains or tasks remains unclear due to limited cross-domain testing

## Next Checks
1. Ablation study on MPC components: Systematically vary temperature parameter τ (0.01 to 0.1) and balancing coefficient α (0.8 to 0.99) to determine their impact on downstream performance
2. Quantization fidelity analysis: Compare AudioFormer performance using different EnCodec bitrates (1.5, 3, 6 kbps) to quantify compression vs accuracy trade-off
3. Cross-domain generalization test: Fine-tune pre-trained AudioFormer on completely different audio domains (medical audio, industrial sounds) to evaluate generalization beyond AudioSet distribution