---
ver: rpa2
title: Policy Gradient Algorithms Implicitly Optimize by Continuation
arxiv_id: '2305.06851'
source_url: https://arxiv.org/abs/2305.06851
tags:
- policy
- continuation
- mirror
- function
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a new theoretical interpretation and justification
  of policy-gradient algorithms in reinforcement learning. The authors formulate direct
  policy optimization in the optimization by continuation framework, showing that
  optimizing affine Gaussian policies and performing entropy regularization can be
  interpreted as implicitly optimizing deterministic policies by continuation.
---

# Policy Gradient Algorithms Implicitly Optimize by Continuation

## Quick Facts
- arXiv ID: 2305.06851
- Source URL: https://arxiv.org/abs/2305.06851
- Reference count: 19
- Key outcome: Policy gradient algorithms implicitly optimize deterministic policies through continuation, where exploration serves to compute continuation of returns rather than maximize immediate returns

## Executive Summary
This paper provides a novel theoretical interpretation of policy gradient algorithms in reinforcement learning through the lens of optimization by continuation. The authors demonstrate that optimizing affine Gaussian policies and performing entropy regularization can be understood as implicitly optimizing deterministic policies via continuation. The key insight is that exploration in policy gradient algorithms consists of computing a continuation of the return of the policy at hand, suggesting that policy variance should be history-dependent functions adapted to avoid local extrema rather than to maximize immediate returns.

## Method Summary
The paper formulates direct policy optimization in the optimization by continuation framework, focusing on affine Gaussian policies. The method involves analyzing how Gaussian policy optimization implicitly optimizes a deterministic policy by continuation, where the continuation variance σ = Σ′/(x - xtarget)² for an affine deterministic policy μθ. The authors also examine how entropy regularization bounds the covariance of the mirror policy from below, equivalent to decreasing continuation covariance. For general continuation distributions, the framework allows mirror policies that depend on history of states and actions.

## Key Results
- Gaussian policy optimization implicitly optimizes deterministic policies through continuation
- Entropy regularization can be interpreted as continuation of return via covariance bounding
- History-dependent policies enable more general regularization of objective functions
- Policy variance should be history-dependent to avoid local extrema

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gaussian policy optimization implicitly optimizes a deterministic policy by continuation.
- Mechanism: Optimizing a Gaussian policy with mean μθ and variance Σ′ is equivalent to optimizing a deterministic policy μθ by continuation, where continuation variance σ = Σ′/(x - xtarget)².
- Core assumption: The policy is affine and covariance depends only on current state.
- Evidence anchors: Property 3 proves existence of Gaussian mirror policy πGP′θ with variance Σ′θ(s) = ∇θμθ(s)ᵀΣ(s)∇θμθ(s).

### Mechanism 2
- Claim: Entropy regularization can be interpreted as continuation of return.
- Mechanism: Adding entropy regularization bounds covariance of mirror policy from below, equivalent to decreasing continuation covariance.
- Core assumption: Original policy is Gaussian with constant covariance and regularization term is lower bound on covariance.
- Evidence anchors: Loewner ordering between covariance matrices implies order between determinants, and Gaussian entropy is concave function of determinant.

### Mechanism 3
- Claim: History-dependent policies enable optimization by continuation.
- Mechanism: For general continuation distribution, mirror policy depends on history of states and actions, allowing more general regularization.
- Core assumption: Policy is affine Gaussian and continuation distribution is Gaussian.
- Evidence anchors: Property 4 extends Property 3 to more general continuation distributions, allowing mirror policies that depend on history.

## Foundational Learning

- Concept: Optimization by continuation
  - Why needed here: Used to reinterpret policy gradient algorithms and provide theoretical justification for their performance
  - Quick check question: What is the main idea behind optimization by continuation, and how does it differ from traditional optimization methods?

- Concept: Mirror policies
  - Why needed here: Policies with same return as continuation of original policy, enabling reinterpretation of policy gradient algorithms
  - Quick check question: What is a mirror policy, and how is it related to the continuation of the return of a policy?

- Concept: Gaussian policies and entropy regularization
  - Why needed here: Commonly used in policy gradient algorithms and interpretable in optimization by continuation framework
  - Quick check question: How do Gaussian policies and entropy regularization relate to optimization by continuation framework?

## Architecture Onboarding

- Component map:
  MDP -> Policy -> Return -> Continuation -> Mirror Policy

- Critical path:
  1. Define MDP and policy parameterization
  2. Formulate optimization as maximizing expected return
  3. Interpret optimization as continuation framework
  4. Derive mirror policy and properties
  5. Analyze implications for policy gradient algorithms

- Design tradeoffs:
  - Affine vs. non-affine policies: Affine allows closed-form mirror policy, non-affine may be more expressive
  - Gaussian vs. non-Gaussian policies: Gaussian has well-defined entropy, other distributions may suit certain tasks
  - History-dependent vs. Markov policies: History-dependent allows general regularization, Markov is simpler and more efficient

- Failure signatures:
  - Poor performance: Policy converging to local optima or insufficient exploration
  - Instability: Optimization sensitive to hyperparameters or learning rate
  - Slow convergence: Stuck in plateau or step size too small

- First 3 experiments:
  1. Compare Gaussian policy with/without entropy regularization on simple continuous control task
  2. Analyze effect of continuation covariance on performance and stability
  3. Implement history-dependent policy and compare to Markov policy on task with complex dynamics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal history-dependent policy for updating variance of mirror policies to avoid local extrema?
- Basis in paper: Suggested from Section 5.2, where variance should be updated based on functional gradient of another measure with respect to policy variance
- Why unresolved: Paper does not provide concrete method for updating variance of mirror policies
- What evidence would resolve it: Concrete algorithm or method for updating variance that can be empirically tested

### Open Question 2
- Question: How can theoretical results be generalized to non-affine and non-Gaussian policies?
- Basis in paper: Acknowledged in Section 6 regarding assumptions of affine policies and Gaussian policies
- Why unresolved: Paper does not provide generalization to non-affine policies and alternative to Gaussian policies
- What evidence would resolve it: Theoretical extension to non-affine policies and alternative to Gaussian policies that can be empirically tested

### Open Question 3
- Question: What is effect of alternative regularization strategies on optimization procedure?
- Basis in paper: Mentioned in Section 6 regarding benefits of other regularization strategies enforcing spread of other distributions
- Why unresolved: Paper does not provide analysis of alternative regularization strategies
- What evidence would resolve it: Empirical or theoretical analysis of alternative regularization strategies compared to entropy regularization

## Limitations

- Theoretical framework relies heavily on affine policy assumption, limiting generalization to complex nonlinear policies
- Connection between entropy regularization and continuation variance established mathematically but lacks extensive empirical validation
- Analysis focuses primarily on continuous action spaces with less clarity on discrete action domains
- History-dependent variance mechanism theoretically sound but may face practical implementation challenges

## Confidence

- **High confidence**: Mathematical derivations connecting affine Gaussian policies to deterministic policy optimization via continuation
- **Medium confidence**: Interpretation of entropy regularization as continuation variance bounding
- **Medium confidence**: Extension to history-dependent policies

## Next Checks

1. Implement and compare affine Gaussian policies with varying continuation variances on standard continuous control benchmarks (e.g., OpenAI Gym's MuJoCo tasks) to empirically verify the smoothing effect

2. Test whether continuation framework provides similar insights when applied to non-affine policies (e.g., neural network-based policies), examining if core theoretical connections still hold approximately

3. Adapt theoretical framework to discrete action spaces by examining how continuation concept applies to categorical distributions, identifying which aspects require modification