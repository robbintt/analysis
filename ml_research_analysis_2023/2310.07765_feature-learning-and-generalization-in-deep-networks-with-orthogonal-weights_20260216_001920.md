---
ver: rpa2
title: Feature Learning and Generalization in Deep Networks with Orthogonal Weights
arxiv_id: '2310.07765'
source_url: https://arxiv.org/abs/2310.07765
tags:
- orthogonal
- gaussian
- networks
- initializations
- depth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the impact of orthogonal weight initialization
  on deep neural networks, addressing the issue of fluctuations that grow with network
  depth in Gaussian-initialized networks. The authors analytically demonstrate that
  orthogonal weight initialization yields depth-independent preactivation fluctuations
  for both linear and tanh activation functions, in contrast to Gaussian initialization
  where fluctuations scale linearly with depth.
---

# Feature Learning and Generalization in Deep Networks with Orthogonal Weights

## Quick Facts
- arXiv ID: 2310.07765
- Source URL: https://arxiv.org/abs/2310.07765
- Reference count: 40
- Primary result: Orthogonal weight initialization yields depth-independent preactivation fluctuations and better generalization compared to Gaussian initialization in deep networks.

## Executive Summary
This paper studies the impact of orthogonal weight initialization on deep neural networks, addressing the issue of fluctuations that grow with network depth in Gaussian-initialized networks. The authors analytically demonstrate that orthogonal weight initialization yields depth-independent preactivation fluctuations for both linear and tanh activation functions, in contrast to Gaussian initialization where fluctuations scale linearly with depth. Empirically, they measure the neural tangent kernel (NTK) and its descendants at initialization, finding that these correlators saturate at a depth of approximately 20 for orthogonal initialization, while continuing to grow without bound for Gaussian initialization. Numerical experiments on MNIST and CIFAR-10 classification tasks confirm that orthogonal initialization leads to faster training and better generalization, particularly in the regime where network width is comparable to depth. The study provides a theoretical link between orthogonal initialization and improved performance, attributing it to the depth-independent NTK fluctuations which preserve finite-width feature learning while reducing overall noise.

## Method Summary
The authors compare orthogonal and Gaussian weight initialization in deep MLPs trained on MNIST and CIFAR-10 classification tasks. They implement MLPs with configurable initialization (Gaussian or orthogonal via QR decomposition), width n=30, tanh activations, MSE loss, and full-batch gradient descent with early stopping. The training uses tensorial learning rate scaling with weight learning rate 1/n per layer, bias learning rate 1/ℓ per layer, and global learning rate η=10 (or 10⁻³ for Adam). Experiments measure training dynamics, best test loss, NTK evolution during training, and performance with different optimizers across varying depths.

## Key Results
- Orthogonal weight initialization produces depth-independent preactivation fluctuations for tanh activations, while Gaussian initialization shows fluctuations scaling linearly with depth.
- NTK correlators saturate at depth ~20 for orthogonal initialization but grow without bound for Gaussian initialization.
- Orthogonal initialization leads to faster training and better generalization on MNIST and CIFAR-10, especially when network width is comparable to depth.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonal weight initialization leads to depth-independent preactivation fluctuations in deep networks.
- Mechanism: Orthogonal matrices preserve the norm of preactivations across layers, preventing exponential growth or decay of signals and keeping fluctuations bounded regardless of depth.
- Core assumption: Preactivations follow a uniform distribution on the sphere after each layer when using orthogonal weights.
- Evidence anchors:
  - [abstract]: "rectangular networks with tanh activations and weights initialized from the ensemble of orthogonal matrices have corresponding preactivation fluctuations which are independent of depth, to leading order in inverse width."
  - [section 3]: Exact preactivation distribution derivation shows that orthogonal weights preserve the norm of inputs across all depths.
  - [corpus]: Limited direct evidence; corpus focuses on different aspects of neural network theory.
- Break condition: If the activation function is not in the K* = 0 universality class (e.g., ReLU), or if the network deviates significantly from the theoretical assumptions.

### Mechanism 2
- Claim: Orthogonal initialization suppresses NTK fluctuations, preserving finite-width feature learning while reducing noise.
- Mechanism: NTK correlators saturate at moderate depth (~20) for orthogonal weights, whereas they grow without bound for Gaussian weights, leading to better generalization and faster training.
- Core assumption: NTK correlators govern the evolution of observables during training and are directly related to feature learning and generalization.
- Evidence anchors:
  - [abstract]: "we demonstrate numerically that, at initialization, all correlators involving the neural tangent kernel (NTK) and its descendants at leading order in inverse width...saturate at a depth of ~20, rather than growing without bound as in the case of Gaussian initializations."
  - [section 5]: Measurements of NTK statistics show that correlators for orthogonal initialization saturate at depth ~20, while Gaussian correlators continue to grow.
  - [corpus]: Limited direct evidence; corpus focuses on different aspects of neural network theory.
- Break condition: If the width of the network becomes too small relative to depth (L/n ≳ 1), or if the assumptions about NTK dynamics break down.

### Mechanism 3
- Claim: Orthogonal initialization eliminates the need for depth-dependent fine-tuning of hyperparameters.
- Mechanism: Unlike Gaussian weights, orthogonal weights maintain criticality at finite width without requiring layer-specific adjustments to initialization hyperparameters.
- Core assumption: The recursion relations for NTK and preactivation correlators are depth-independent for orthogonal weights.
- Evidence anchors:
  - [section 4.4]: "the infinite-width hyperparameters CW = 1, Cb = 0 still maintain criticality at leading order in 1/n, independent of the depth of the network."
  - [section 6.3]: Orthogonal networks show consistent performance across different depths, supporting the idea of depth-independent hyperparameters.
  - [corpus]: Limited direct evidence; corpus focuses on different aspects of neural network theory.
- Break condition: If the network architecture deviates significantly from the theoretical model, or if other factors (e.g., optimization algorithm) introduce depth-dependent effects.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its role in training dynamics
  - Why needed here: NTK governs the evolution of observables during training and is directly related to feature learning and generalization. Understanding NTK is crucial for grasping why orthogonal initialization leads to better performance.
  - Quick check question: What is the NTK, and how does it relate to the training dynamics of neural networks?

- Concept: Critical initialization and its importance in deep networks
  - Why needed here: Critical initialization prevents exponential growth or decay of signals propagating through the network, which is essential for maintaining stable training dynamics. Orthogonal initialization achieves criticality without the need for depth-dependent fine-tuning.
  - Quick check question: What is critical initialization, and why is it important for deep networks?

- Concept: Universality classes of activation functions
  - Why needed here: Different activation functions belong to different universality classes, which determine how preactivation fluctuations behave with depth. Understanding these classes is crucial for predicting the behavior of networks with different activation functions.
  - Quick check question: What are universality classes of activation functions, and how do they affect the behavior of deep networks?

## Architecture Onboarding

- Component map: Input layer -> Hidden layers (fully connected with orthogonal weights) -> Output layer (fully connected with linear/softmax activation) -> Loss function (MSE/cross-entropy) -> Optimizer (full-batch GD with scaled learning rates)

- Critical path:
  1. Initialize weights using orthogonal matrices (or QR decomposition for non-square matrices)
  2. Forward pass through the network
  3. Compute loss and gradients
  4. Update weights using scaled learning rates
  5. Repeat steps 2-4 until convergence or early stopping

- Design tradeoffs:
  - Orthogonal vs. Gaussian initialization: Orthogonal initialization leads to depth-independent fluctuations and better generalization, but may require more computational resources for weight initialization.
  - Width vs. depth: Wider networks generally perform better, but orthogonal initialization mitigates the need for very wide networks by reducing fluctuations.
  - Tanh vs. ReLU activation: Tanh activation functions lead to depth-independent fluctuations with orthogonal initialization, while ReLU does not.

- Failure signatures:
  - Diverging loss: May indicate that the learning rate is too high or that the network is not properly initialized.
  - Slow training: May indicate that the learning rate is too low or that the network is too deep relative to its width.
  - Poor generalization: May indicate that the network is overfitting or that the initialization is not optimal.

- First 3 experiments:
  1. Train a shallow network (L=2) with orthogonal initialization and compare its performance to a network with Gaussian initialization.
  2. Train a deep network (L=20) with orthogonal initialization and measure the saturation of NTK correlators at initialization.
  3. Train networks with varying widths and depths using orthogonal initialization, and plot the best validation loss as a function of L/n to verify depth-independent performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical explanation for the depth saturation scale of ~20 in orthogonal networks?
- Basis in paper: [explicit] The paper notes that NTK correlators saturate at depth ~20 for orthogonal networks, but does not provide a theoretical derivation for this specific value.
- Why unresolved: The paper only provides empirical measurements of this phenomenon, not a theoretical explanation for why this particular depth scale emerges.
- What evidence would resolve it: A derivation showing how architectural choices and data properties determine this depth scale would resolve this question.

### Open Question 2
- Question: How do orthogonal weight initializations affect variance in trained network predictions?
- Basis in paper: [inferred] The paper speculates that orthogonal networks might exhibit reduced variance in predictions compared to Gaussian networks, but does not measure this directly.
- Why unresolved: While the paper discusses reduced NTK fluctuations, it does not connect this to variance in actual network outputs after training.
- What evidence would resolve it: Direct measurements of prediction variance across multiple training runs for orthogonal vs Gaussian networks would resolve this question.

### Open Question 3
- Question: Is there an optimal global learning rate for orthogonal networks that can be derived analytically?
- Basis in paper: [inferred] The paper uses a fixed learning rate and notes differences between ordinary GD and scaled GD, but does not derive an optimal learning rate.
- Why unresolved: The paper does not explore the relationship between NTK correlator dynamics and optimal learning rates for orthogonal networks.
- What evidence would resolve it: Analytical derivation of optimal learning rates from NTK correlator analysis, validated by empirical measurements across different network depths.

## Limitations
- Analytical results are limited to tanh activations in the K* = 0 universality class, with uncertainty about generalizability to other activation functions.
- Empirical evidence is limited to specific architectures and datasets (MNIST, CIFAR-10), leaving performance on other datasets and architectures unexplored.
- The exact mechanism by which orthogonal initialization improves generalization is not fully elucidated, with the causal relationship between reduced NTK fluctuations and improved performance not rigorously established.

## Confidence

- **High Confidence**: The analytical derivation of depth-independent preactivation fluctuations for orthogonal initialization in tanh networks.
- **Medium Confidence**: The empirical observation that orthogonal initialization leads to better generalization and faster training on MNIST and CIFAR-10.
- **Low Confidence**: The claim that orthogonal initialization eliminates the need for depth-dependent hyperparameter tuning.

## Next Checks
1. Extend the analytical and empirical results to ReLU and other common activation functions to assess the generality of the findings.
2. Test the impact of orthogonal initialization on convolutional neural networks and other architectures commonly used in practice.
3. Conduct a thorough hyperparameter sweep to determine if orthogonal initialization truly eliminates the need for depth-dependent tuning.