---
ver: rpa2
title: Summarization-based Data Augmentation for Document Classification
arxiv_id: '2312.00513'
source_url: https://arxiv.org/abs/2312.00513
tags:
- data
- classification
- text
- training
- curriculum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of document classification by
  proposing SUMMaug, a summarization-based data augmentation method. The core idea
  involves generating concise summaries of original training documents and optionally
  merging fine-grained labels to match the summarized content.
---

# Summarization-based Data Augmentation for Document Classification

## Quick Facts
- **arXiv ID**: 2312.00513
- **Source URL**: https://arxiv.org/abs/2312.00513
- **Reference count**: 15
- **Key outcome**: SUMMaug improves document classification accuracy through curriculum learning with summarized data, outperforming baseline augmentation methods

## Executive Summary
This paper introduces SUMMaug, a summarization-based data augmentation method for document classification that leverages curriculum learning. The approach generates concise summaries of training documents and uses them to create easy-to-learn examples, optionally coarsening labels to match the summarized content's semantic level. Through experiments on IMDb datasets, the method demonstrates significant improvements in classification accuracy and robustness compared to standard augmentation techniques like AEDA, with curriculum fine-tuning consistently outperforming mixed fine-tuning approaches.

## Method Summary
SUMMaug employs text summarization to create abstractive, easy-to-read examples from original training documents for document classification tasks. The method generates summaries using a BART-based model and optionally coarsens labels when fine-grained distinctions are lost during summarization. Two training strategies are proposed: mixed fine-tuning (combining original and augmented data) and curriculum fine-tuning (first training on summaries with coarsened labels, then on original data). The approach is evaluated on IMDb-2 (binary sentiment) and IMDb-10 (10-class rating) datasets using RoBERTa-Large as the classifier.

## Key Results
- SUMMaug with curriculum fine-tuning achieves higher accuracy than mixed fine-tuning on both IMDb-2 and IMDb-10 datasets
- The method is particularly effective in low-resource settings, demonstrating robustness when training data is limited
- Label coarsening is essential when using summarized data for fine-grained classification tasks, with optimal performance when coarsening to N≤4 classes

## Why This Works (Mechanism)

### Mechanism 1
Curriculum learning with summarized data improves model performance by first training on easier, shorter examples before moving to complex full documents. The model is initially fine-tuned on concise summaries containing core semantic content, allowing it to learn basic patterns before being exposed to full document complexity. This staged learning process mirrors human learning patterns.

### Mechanism 2
Label coarsening is necessary when using summarized data because fine-grained distinctions are lost during summarization. Multiple fine-grained labels are mapped to coarser categories that better match the summarized content's semantic level, addressing the incompatibility between original labels and summarized inputs.

### Mechanism 3
Summarization-based augmentation provides more diverse training data than simple text perturbation methods by generating entirely new semantic content rather than modified versions of existing text. This approach creates genuinely new training examples with different semantic patterns, exposing the model to diverse perspectives of the same document.

## Foundational Learning

- **Concept: Curriculum learning**
  - Why needed here: The method explicitly uses curriculum learning to first train on easier summarized examples before full documents, which is central to the SUMMaug approach
  - Quick check question: What is the main benefit of using curriculum learning with summarized data compared to training only on full documents?

- **Concept: Text summarization evaluation**
  - Why needed here: The quality of the summarization model directly impacts the effectiveness of the augmentation, requiring understanding of ROUGE scores, faithfulness, and informativeness metrics
  - Quick check question: How would you evaluate whether the summaries preserve enough information for accurate classification?

- **Concept: Label space transformation**
  - Why needed here: The method requires mapping fine-grained labels to coarser categories when using summarized data, which involves understanding label distribution and semantic relationships
  - Quick check question: What criteria would you use to determine the optimal number of coarse-grained labels when coarsening a 10-class problem?

## Architecture Onboarding

- **Component map**: Document → BART summarization model → Summary + Coarsened label → Training (curriculum or mixed) → Fine-tuned RoBERTa

- **Critical path**: Document → Summarization model → Summary + Coarsened label → Training (curriculum or mixed) → Fine-tuned RoBERTa

- **Design tradeoffs**:
  - Quality vs. speed: Better summarization models may be slower but produce more useful summaries
  - Label coarsening granularity: Too coarse loses discriminative power; too fine doesn't solve the summarization mismatch problem
  - Curriculum vs. mixed training: Curriculum may require more training time but shows better performance; mixed is simpler but less effective

- **Failure signatures**:
  - Performance degrades when using summarized data without label coarsening
  - No improvement over baseline when using mixed fine-tuning instead of curriculum
  - Standard deviation increases, indicating unstable training
  - Performance collapses on low-resource settings if summaries are poor quality

- **First 3 experiments**:
  1. Compare accuracy of curriculum fine-tuning with SUMMaug vs. mixed fine-tuning on IMDb-2 with all training data available
  2. Test different levels of label coarsening (N=2,3,4,5,10) on IMDb-10 to find optimal granularity
  3. Evaluate performance degradation when using poor-quality summarization (e.g., extractive instead of abstractive) on a subset of data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of summarization model affect the performance of SUMMaug?
- Basis in paper: [inferred] The paper mentions using BART trained on CNN-Dailymail but suggests exploring different summarization models in future work
- Why unresolved: The study only used one summarization model, limiting the understanding of how different models might impact performance
- What evidence would resolve it: Comparative experiments using various summarization models (e.g., different architectures or training datasets) to evaluate their effects on SUMMaug's accuracy and robustness

### Open Question 2
- Question: What is the optimal label coarsening function for different document classification tasks?
- Basis in paper: [explicit] The paper notes that the label coarsening function was chosen simply and suggests further exploration of its impact
- Why unresolved: The study did not explore a range of label coarsening functions, leaving the optimal function for different tasks unclear
- What evidence would resolve it: Systematic experimentation with various label coarsening functions across different datasets to identify the most effective approach for each task

### Open Question 3
- Question: How does SUMMaug perform across diverse domains and classification tasks?
- Basis in paper: [inferred] The paper acknowledges the limitation of using datasets from the same domain and suggests applying SUMMaug to other tasks in future work
- Why unresolved: The experiments were limited to movie review datasets, which may not generalize to other domains or tasks
- What evidence would resolve it: Evaluations of SUMMaug on datasets from various domains (e.g., medical, legal, technical) and classification tasks (e.g., multi-label, multi-class) to assess its generalizability

## Limitations
- Performance depends heavily on the quality of the summarization model, with poor summaries leading to degraded classification accuracy
- The method requires careful selection of label coarsening granularity, with insufficient coarsening leading to instability and poor performance
- Experiments are limited to movie review datasets, raising questions about generalizability to other document types and domains

## Confidence
- **High**: Curriculum learning benefits are well-established and consistently observed across experiments
- **Medium**: The necessity and optimal approach for label coarsening in summarization-based augmentation
- **Medium**: Generalization of SUMMaug's effectiveness to document types beyond movie reviews

## Next Checks
1. **Cross-domain generalization test**: Apply SUMMaug to a different document classification dataset (e.g., news articles or scientific papers) to assess whether the curriculum learning benefits transfer beyond movie reviews, particularly testing the label coarsening strategy on new label distributions

2. **Ablation on summarization quality**: Systematically vary the summarization model quality (e.g., using extractive vs. abstractive methods, or models with different ROUGE scores) while keeping all other components constant to isolate the impact of summary quality on downstream classification performance

3. **Perturbation comparison experiment**: Directly compare SUMMaug against a strong perturbation-based augmentation method (like AEDA with optimized parameters) on identical datasets and training protocols to empirically validate whether summarization provides superior diversity benefits for document understanding