---
ver: rpa2
title: Foundation Model's Embedded Representations May Detect Distribution Shift
arxiv_id: '2310.13836'
source_url: https://arxiv.org/abs/2310.13836
tags:
- linear
- gpt-2
- pre-trained
- training
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of distribution shifts between
  training and testing datasets on transfer learning performance, particularly when
  using pre-trained foundation models. The authors focus on the Sentiment140 dataset,
  which contains an automatically labeled training set (P) and a manually curated
  test set (M), where the two sets are drawn from different distributions.
---

# Foundation Model's Embedded Representations May Detect Distribution Shift

## Quick Facts
- **arXiv ID**: 2310.13836
- **Source URL**: https://arxiv.org/abs/2310.13836
- **Reference count**: 40
- **Primary result**: Pre-trained GPT-2 can detect distribution shifts between training and testing datasets, and linear probes on its embeddings may outperform fine-tuning.

## Executive Summary
This paper investigates how distribution shifts between training and testing datasets affect transfer learning performance using pre-trained foundation models. Focusing on the Sentiment140 dataset, the authors demonstrate that pre-trained GPT-2 can effectively separate the distributional shift between an automatically labeled training set (P) and a manually curated test set (M) using PCA on final embeddings. Surprisingly, linear probes trained on these embeddings outperform full fine-tuning on P when evaluated on M, suggesting that fine-tuning may introduce bias rather than improving performance. This highlights the importance of understanding distribution shifts and the potential limitations of fine-tuning in transfer learning scenarios.

## Method Summary
The authors use the Sentiment140 dataset containing 1.6M automatically labeled training examples (P) and 359 manually curated test examples (M). They employ pre-trained GPT-2 to extract final embedding vectors from tweets, apply PCA for dimensionality reduction, and train logistic regression classifiers (linear probes) on small samples of M. They compare this approach against full fine-tuning of GPT-2 on P with a classification head. Performance is evaluated using accuracy on the test set M, with experiments designed to assess both distribution shift detection and sentiment classification capabilities.

## Key Results
- PCA on GPT-2 embeddings can completely separate P and M, indicating detectable distribution shifts
- Linear probes on pre-trained GPT-2 embeddings outperform full fine-tuning on P when evaluated on M
- The distributional shift between P and M is significant enough to affect fine-tuning outcomes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pre-trained GPT-2 model can detect distributional shifts between training (P) and testing (M) datasets.
- Mechanism: The model's final embedding vectors, when analyzed via PCA, show distinct separation between the two datasets, indicating different underlying distributions.
- Core assumption: The distributional shift between P and M is captured in the high-dimensional feature space of GPT-2.
- Evidence anchors:
  - [abstract] "We show that Sentiment140's test dataset M is not sampled from the same distribution as the training dataset P"
  - [section] "PCA of this kernel, using the pre-trained weights of GPT-2, we can separate P and M through just the first two principal components."
- Break condition: If the datasets are sampled from the same distribution, PCA would not show separation.

### Mechanism 2
- Claim: Linear probes on pre-trained GPT-2 outperform fine-tuning for sentiment classification on M.
- Mechanism: The pre-trained features are already well-suited for sentiment classification, and fine-tuning on P introduces bias rather than improvement.
- Core assumption: Pre-trained GPT-2 features are transferable and effective for the target task without further adaptation.
- Evidence anchors:
  - [abstract] "linear probes trained on the final embedding of pre-trained GPT-2 are surprisingly robust and are competitive with and may even outperform fine-tuning"
  - [section] "Linear probes on pre-trained GPT-2's representations are robust and may even outperform overall fine-tuning"
- Break condition: If the pre-training corpus is not relevant to the target task, linear probes may underperform.

### Mechanism 3
- Claim: Fine-tuning on P may introduce bias and hamper performance on M.
- Mechanism: Fine-tuning adapts the model to P's distribution, which differs from M, leading to poorer generalization.
- Core assumption: The distributional shift between P and M is significant enough to affect fine-tuning outcomes.
- Evidence anchors:
  - [abstract] "experiments on pre-trained GPT-2 show that the features learnable from P do not improve (and in fact hamper) performance on M"
  - [section] "changes in the model due to fine-tuning on P are not reflected in sentiment classification on M"
- Break condition: If the distributions of P and M were similar, fine-tuning would likely improve performance.

## Foundational Learning

- Concept: Distributional shift
  - Why needed here: Understanding how differences between training and testing distributions affect model performance.
  - Quick check question: What is the impact of distributional shift on model generalization?

- Concept: Transfer learning
  - Why needed here: The paper investigates whether pre-trained models can be effectively used for new tasks without extensive fine-tuning.
  - Quick check question: How does transfer learning differ from traditional supervised learning?

- Concept: Linear probing
  - Why needed here: A method to evaluate the quality of intermediate representations without altering the pre-trained model.
  - Quick check question: What is the purpose of using linear probes in model evaluation?

## Architecture Onboarding

- Component map: Pre-trained GPT-2 model -> Final embedding layer -> Linear classifier -> PCA for dimensionality reduction
- Critical path: Extract final embeddings → Apply PCA → Train linear classifier → Evaluate on M
- Design tradeoffs: Fine-tuning vs. linear probing for sentiment classification
- Failure signatures: Poor performance on M indicates distributional shift or inadequate pre-training
- First 3 experiments:
  1. Perform PCA on final embeddings of GPT-2 for both P and M to visualize distributional shift
  2. Train a linear probe on pre-trained GPT-2 using M and evaluate on the remainder of M
  3. Fine-tune GPT-2 on P and evaluate on M to compare with linear probing results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ability to differentiate distribution shifts between training and testing datasets correlate with the number of parameters in pre-trained foundation models?
- Basis in paper: [inferred] The authors observe that Falcon 7B, with more parameters, can completely differentiate the shift like GPT-2, while other models like Bert-base cannot. They suggest this might correlate with the number of parameters but acknowledge other factors like the training corpus.
- Why unresolved: The paper does not disentangle the effects of model size from other factors like training corpus, making it unclear whether parameter count is the primary driver of distribution shift detection.
- What evidence would resolve it: Experiments comparing models of varying sizes trained on the same corpus, or models of the same size trained on different corpora, would help isolate the impact of parameter count.

### Open Question 2
- Question: Does fine-tuning on the training dataset (P) improve or impair the model's performance on the testing dataset (M), and why?
- Basis in paper: [explicit] The authors find that linear probes trained on pre-trained GPT-2's representations are robust and may even outperform full fine-tuning, suggesting that fine-tuning on P might introduce bias rather than improving performance on M.
- Why unresolved: The paper does not provide a definitive explanation for why fine-tuning might impair performance, only that it does not improve it. The underlying mechanisms remain unclear.
- What evidence would resolve it: Detailed analysis of the changes in model weights and representations before and after fine-tuning, along with ablation studies on different fine-tuning strategies, could shed light on the impact of fine-tuning.

### Open Question 3
- Question: What are the limitations of using linear probes as a baseline for evaluating the capabilities of pre-trained foundation models in transfer learning tasks?
- Basis in paper: [explicit] The authors use linear probes as a baseline and find them surprisingly effective, but they also acknowledge that other tools may be better suited to uncover the existing capabilities of pre-trained models.
- Why unresolved: The paper does not explore alternative evaluation methods or discuss the specific limitations of linear probes in this context.
- What evidence would resolve it: Comparative studies using different evaluation methods, such as non-linear probes or zero-shot learning, could highlight the strengths and weaknesses of linear probes as a baseline.

## Limitations

- The analysis of distributional shift detection using PCA is based on a single dataset (Sentiment140) with a specific train-test split.
- The linear probing experiments use a small subset of M (200 examples) for training, which may not be representative.
- The claim that fine-tuning on P "hampers" performance is based on relative comparisons rather than absolute performance baselines.

## Confidence

- **High Confidence**: The observation that PCA can separate P and M in embedding space is well-supported by the evidence provided, as it relies on direct visualization of the data.
- **Medium Confidence**: The claim that linear probes can outperform fine-tuning is supported by experimental results but lacks extensive ablation studies or comparisons with alternative methods.
- **Low Confidence**: The assertion that fine-tuning "introduces bias" rather than improving performance is speculative and not directly tested.

## Next Checks

1. Test the PCA separation and linear probing performance on additional datasets with known distribution shifts (e.g., CIFAR-10-C, ImageNetV2) to assess whether the findings generalize beyond Sentiment140.

2. Evaluate how the performance of linear probes scales with increasing training sample sizes from M, to determine if the observed robustness is due to the small sample size or inherent feature quality.

3. Compare linear probing against other fine-tuning strategies, such as few-shot learning or parameter-efficient fine-tuning (e.g., adapters), to determine if the underperformance is specific to full fine-tuning or a broader issue with adaptation methods.