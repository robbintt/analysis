---
ver: rpa2
title: 'Dealing with zero-inflated data: achieving SOTA with a two-fold machine learning
  approach'
arxiv_id: '2310.08088'
source_url: https://arxiv.org/abs/2310.08088
tags:
- data
- two-fold
- classification
- demand
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of zero-inflated data, where
  machine learning models must predict a few non-zero values within a broader range
  of zero values. The authors propose a two-fold machine learning approach to tackle
  this issue, which consists of a classification model to predict zero vs.
---

# Dealing with zero-inflated data: achieving SOTA with a two-fold machine learning approach

## Quick Facts
- arXiv ID: 2310.08088
- Source URL: https://arxiv.org/abs/2310.08088
- Reference count: 19
- Primary result: Two-fold ML approach improves home appliance classification by 27-49% in key metrics and outperforms baselines in shuttle demand prediction

## Executive Summary
This paper addresses the challenge of zero-inflated data where machine learning models must predict a few non-zero values within a broader range of zero values. The authors propose a two-fold machine learning approach consisting of a classification model to predict zero vs. non-zero occurrences followed by a regression or classification model to predict actual values. Applied to home appliances classification and airport shuttle demand prediction, this approach significantly outperforms state-of-the-art methods in both domains, with particularly strong improvements in weighted average precision, recall, F1, and AUC ROC metrics.

## Method Summary
The two-fold approach separates the modeling task into two stages: first, a classifier predicts whether an event occurs (activity vs. no activity or demand vs. no demand), and second, another model predicts the actual value or category for non-zero events. For home appliances classification, the pipeline uses XGBoost for activity detection followed by VGG11 for appliance type classification. For shuttle demand prediction, Histogram-Based Gradient Boosting (HGB) classifies demand occurrence, followed by regressors (LR, MLP, or SVR) to predict passenger counts. The method is evaluated on the UK-DALE dataset for appliances and a four-year shuttle demand dataset for three routes.

## Key Results
- Home appliances classification: Weighted average Precision improved by 27%, Recall by 34%, F1 by 49%, and AUC ROC by 27% compared to state-of-the-art
- The two-fold approach is estimated to be four times more energy-efficient than the single-model baseline
- Shuttle demand prediction: Two-fold models outperformed all other models in all cases with statistically significant differences

## Why This Works (Mechanism)

### Mechanism 1
The two-fold hierarchical structure addresses zero-inflation by first learning when non-zero events occur, then predicting their values, which reduces the learning burden for the second stage. A classifier isolates the subset of samples with non-zero targets, allowing the second model to train on a less imbalanced distribution, improving feature learning for minority classes. This breaks down if the classifier misclassifies many non-zero events as zero, causing the second model to lose important training examples.

### Mechanism 2
Zero-inflated data distorts performance metrics; separating classification from regression allows metric alignment with actual prediction goals. Using AUC ROC for the classifier stage measures discriminative ability without being skewed by the prevalence of zeros, while MASE variants for the regressor stage isolate true demand events and penalize false positives. These metrics better reflect real-world utility than accuracy or F1 on imbalanced data, but break down if the second-stage model is applied to data with many false positives from the classifier.

### Mechanism 3
Energy efficiency improves because the computationally expensive deep learning model (VGG11) is invoked only when activity is predicted, reducing total inference cost. The lightweight XGBoost classifier quickly filters out inactive samples, avoiding VGG11 inference on the majority of zero-inflated data points. This energy savings disappears if the classifier's false positive rate is high, causing unnecessary VGG11 invocations.

## Foundational Learning

- Concept: Zero-inflated data modeling
  - Why needed here: The dataset contains a large proportion of zeros, which standard regression/classification models handle poorly because they dilute learning signals for non-zero events.
  - Quick check question: In a dataset where 80% of target values are zero, what type of modeling approach can separate the occurrence problem from the value estimation problem?

- Concept: Hurdle and mixture models
  - Why needed here: These statistical frameworks formalize the idea of modeling zero vs. non-zero occurrences separately, which the paper implements as a two-fold ML approach.
  - Quick check question: What statistical model family splits the modeling task into a "hurdle" for zero occurrence and a separate distribution for positive values?

- Concept: Evaluation metrics for imbalanced data
  - Why needed here: Accuracy and RMSE are misleading when zeros dominate; AUC ROC and MASE variants better capture discriminative and regression quality on sparse events.
  - Quick check question: Which metric summarizes a classifier's ability to discriminate across all thresholds without being affected by class imbalance?

## Architecture Onboarding

- Component map: Classifier 1 (XGBoost for activity vs. no activity) → Classifier 2 (VGG11 for appliance type on active segments) OR Classifier 1 (HGB for demand occurrence) → Regressor (LR, MLP, SVR for passenger count)
- Critical path: For classification: Input → XGBoost → (if active) VGG11 → Output. For regression: Input → HGB classifier → (if demand) Regressor → Output
- Design tradeoffs: Tradeoff between computational cost and accuracy—VGG11 is powerful but expensive, so filtering with XGBoost/HGB is crucial; using global vs. local models balances data richness against overfitting to route-specific patterns
- Failure signatures: High false positive rate from classifier leads to unnecessary heavy model inference or spurious regression predictions; poor second-stage model performance suggests the classifier is misfiltering samples
- First 3 experiments:
  1. Train XGBoost classifier alone on full dataset and evaluate AUC ROC; check if it cleanly separates zero vs. non-zero events
  2. Train VGG11 or regressor on all data (no two-fold filtering) to establish baseline performance
  3. Combine classifier + second-stage model and compare metrics (AUC ROC, MASEI/II) against baselines to verify improvement

## Open Questions the Paper Calls Out
The paper identifies three main open questions for future research: applying graph machine learning algorithms to geographic and temporal data of shuttle routes to improve demand forecasting accuracy compared to traditional two-fold regression models; measuring the energy efficiency of two-fold machine learning models compared to single-step deep learning models across different hardware configurations and dataset sizes; and determining what additional features or data sources could significantly improve the accuracy of predicting the number of passengers in shuttle demand forecasting.

## Limitations
- Energy efficiency claims are estimated rather than measured, lacking empirical validation of the computational savings
- Some hyperparameter settings are not fully specified, which may affect reproducibility
- The specific implementation details of recurrence plot generation for VGG11 input are not provided

## Confidence
- **High confidence** in the classification improvements and metric enhancements, as these are directly measured and reported with statistical significance
- **Medium confidence** in the energy efficiency claims, which are based on computational complexity assumptions rather than actual measurements
- **Medium confidence** in the generalizability of the approach, as it is demonstrated on only two specific use cases

## Next Checks
1. Implement instrumentation to measure actual inference time and energy consumption of the two-fold approach versus single-model baselines under identical conditions
2. Systematically vary the classification threshold in the first stage to quantify the tradeoff between false positives (unnecessary second-stage computation) and false negatives (missed events)
3. Apply the two-fold approach to a third zero-inflated dataset from a different domain to assess robustness and identify any domain-specific limitations