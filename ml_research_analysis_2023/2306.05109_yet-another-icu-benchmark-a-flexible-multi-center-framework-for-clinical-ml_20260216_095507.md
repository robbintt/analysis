---
ver: rpa2
title: 'Yet Another ICU Benchmark: A Flexible Multi-Center Framework for Clinical
  ML'
arxiv_id: '2306.05109'
source_url: https://arxiv.org/abs/2306.05109
tags:
- data
- table
- prediction
- eicu
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Yet Another ICU Benchmark (YAIB), a modular
  framework designed to standardize and reproduce clinical machine learning experiments
  in intensive care units. YAIB supports major open-access ICU datasets (MIMIC III/IV,
  eICU, HiRID, AUMCdb) and provides five predefined prediction tasks (mortality, acute
  kidney injury, sepsis, kidney function, and length of stay).
---

# Yet Another ICU Benchmark: A Flexible Multi-Center Framework for Clinical ML

## Quick Facts
- arXiv ID: 2306.05109
- Source URL: https://arxiv.org/abs/2306.05109
- Reference count: 40
- One-line primary result: YAIB demonstrates that dataset choice and preprocessing significantly impact ICU prediction performance, often more than model architecture.

## Executive Summary
This paper introduces Yet Another ICU Benchmark (YAIB), a modular framework designed to standardize and reproduce clinical machine learning experiments in intensive care units. YAIB supports major open-access ICU datasets (MIMIC III/IV, eICU, HiRID, AUMCdb) and provides five predefined prediction tasks (mortality, acute kidney injury, sepsis, kidney function, and length of stay). The framework includes transparent preprocessing pipelines, extensible training code for multiple ML and deep learning models, and Bayesian hyperparameter optimization. The authors demonstrate that dataset choice, cohort definition, and preprocessing significantly impact prediction performance, often more than model architecture.

## Method Summary
YAIB provides a modular multi-dataset framework specifically designed for extensibility, supporting four major open-source ICU datasets. The benchmark uses the ricu R package for data harmonization and includes five predefined prediction tasks developed in collaboration with clinicians. The framework implements extensive ML and DL baselines with Bayesian hyperparameter optimization using 5-fold cross-validation. Models are trained using PyTorch Lightning with configurable preprocessing pipelines and cohort definitions.

## Key Results
- Dataset choice, cohort definition, and preprocessing have major impact on prediction performance—often more so than model class
- Dynamic feature generation consistently improves performance across all tasks and datasets
- Performance variability between datasets is significant, with AUROC ranging from 0.81-0.89 for mortality prediction across different datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The benchmark improves reproducibility by providing a unified, modular pipeline for clinical ML experiments.
- **Mechanism**: By standardizing cohort definition, preprocessing, and evaluation across multiple ICU datasets, YAIB removes the variability introduced by custom implementations.
- **Core assumption**: Clinical prediction tasks are comparable only when the experimental setup (cohort selection, preprocessing, and model training) is consistent.
- **Evidence anchors**:
  - [abstract]: "YAIB enables unified model development" and "the choice of dataset, cohort definition, and preprocessing have a major impact on the prediction performance — often more so than model class."
  - [section]: "We provide a modular multi-dataset framework specifically designed for extensibility" and "all steps are shown schematically in Figure 1."
  - [corpus]: Weak. The corpus papers focus on specific tasks or models but do not emphasize unified benchmarking frameworks.
- **Break condition**: If dataset-specific preprocessing requirements cannot be abstracted into a common pipeline, the unified approach may fail to capture dataset-specific nuances.

### Mechanism 2
- **Claim**: YAIB accelerates method development by providing ready-to-use implementations of state-of-the-art models.
- **Mechanism**: Researchers can directly compare new models against established baselines without reimplementing common architectures.
- **Core assumption**: Model development overhead is reduced when baseline implementations are readily available and easily integrated.
- **Evidence anchors**:
  - [abstract]: "Our benchmark comes with five predefined established prediction tasks...developed in collaboration with clinicians."
  - [section]: "We provide extensive ML and DL baselines for five clinical prediction tasks trained across four major open-source ICU datasets."
  - [corpus]: Weak. While some corpus papers mention specific models, they do not emphasize the availability of baseline implementations.
- **Break condition**: If the provided baseline models are not state-of-the-art or lack flexibility for customization, researchers may not find them useful.

### Mechanism 3
- **Claim**: YAIB facilitates fair comparison of models by harmonizing data and task definitions.
- **Mechanism**: By using the ricu R package to standardize ICU data formats and providing configurable preprocessing pipelines, YAIB ensures that differences in performance are due to model architecture, not data handling.
- **Core assumption**: Data harmonization is essential for meaningful model comparison across different datasets.
- **Evidence anchors**:
  - [abstract]: "Our benchmark uses the ricu R package as a basis for the harmonization of the different ICU databases."
  - [section]: "Our benchmark was designed to be a standard for existing and future large-scale ICU EHR datasets."
  - [corpus]: Weak. The corpus papers focus on individual datasets or models but do not emphasize data harmonization across datasets.
- **Break condition**: If the harmonization process oversimplifies dataset-specific features, it may lead to loss of critical information for certain tasks.

## Foundational Learning

- **Concept**: ICU data heterogeneity and its impact on model performance.
  - **Why needed here**: Understanding why data harmonization is crucial for comparing models across datasets.
  - **Quick check question**: What are the main sources of heterogeneity in ICU data, and how does YAIB address them?

- **Concept**: The role of preprocessing in clinical ML.
  - **Why needed here**: Recognizing how preprocessing choices affect model performance and comparability.
  - **Quick check question**: How does YAIB's preprocessing pipeline ensure consistency across different datasets?

- **Concept**: Cross-validation and hyperparameter optimization in clinical ML.
  - **Why needed here**: Understanding the experimental setup used to evaluate models in YAIB.
  - **Quick check question**: What is the rationale behind using 5-fold cross-validation and Bayesian hyperparameter optimization in YAIB?

## Architecture Onboarding

- **Component map**: ricu (data harmonization) -> RECI PYS (preprocessing) -> PyTorch Lightning (model training/evaluation)
- **Critical path**: The critical path is the end-to-end pipeline from cohort definition to model evaluation, as shown in Figure 1. Ensuring each step is correctly implemented is crucial for reproducibility.
- **Design tradeoffs**: YAIB trades off flexibility for standardization. While it provides a unified interface, it may not capture all dataset-specific nuances. However, this tradeoff is necessary for fair comparison across datasets.
- **Failure signatures**: Common failure modes include incorrect cohort definitions, inconsistent preprocessing, and hyperparameter tuning issues. Monitoring these areas can help identify and resolve problems.
- **First 3 experiments**:
  1. Train a baseline logistic regression model for ICU mortality prediction on MIMIC-IV using YAIB's default settings.
  2. Compare the performance of different deep learning architectures (e.g., LSTM, GRU, Transformer) for AKI prediction on HiRID.
  3. Evaluate the impact of different preprocessing choices (e.g., with/without dynamic feature generation) on sepsis prediction performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different sepsis definitions impact the generalizability of machine learning models across clinical settings?
- Basis in paper: [explicit] The paper demonstrates that varying sepsis definitions significantly impact AUROC and AUPRC, with different definitions leading to different prevalence rates and model performance.
- Why unresolved: The paper shows that sepsis definitions vary widely and affect model performance, but does not provide a framework for determining which definition is most clinically useful or generalizable.
- What evidence would resolve it: Comparative studies across multiple clinical settings using different sepsis definitions, along with clinical validation of model predictions.

### Open Question 2
- Question: What is the optimal balance between static and dynamic features for ICU mortality prediction across different datasets?
- Basis in paper: [explicit] The paper shows that dynamic feature generation consistently outperforms task definitions without them, but the impact varies significantly across datasets.
- Why unresolved: While the paper demonstrates the importance of dynamic features, it does not determine the optimal combination or threshold of static vs. dynamic features for different prediction tasks.
- What evidence would resolve it: Systematic ablation studies across multiple datasets and tasks to identify the most informative feature combinations.

### Open Question 3
- Question: How does the choice of exclusion criteria affect the clinical validity and generalizability of ICU prediction models?
- Basis in paper: [explicit] The paper shows that different exclusion criteria can account for up to half of the performance difference in mortality prediction models.
- Why unresolved: The paper demonstrates the impact of exclusion criteria on model performance but does not provide guidelines for selecting criteria that balance model accuracy with clinical relevance.
- What evidence would resolve it: Clinical studies comparing model predictions using different exclusion criteria against actual patient outcomes in diverse ICU settings.

## Limitations
- The framework's standardization may not capture all dataset-specific nuances required for certain clinical tasks
- Dependency on external packages (ricu, RECI PYS) introduces potential points of failure
- Performance comparisons are limited to the specific datasets and tasks chosen for demonstration

## Confidence
- High confidence in core claims about standardization benefits and performance variability across datasets
- Medium confidence in comparative performance claims, as these depend on specific datasets and tasks
- Major uncertainties include whether the framework's standardization sufficiently captures dataset-specific nuances

## Next Checks
1. Reproduce baseline results across all four datasets using the default configuration to verify consistency
2. Test the framework with an additional ICU dataset not originally included to assess extensibility
3. Compare performance variations when using different cohort definitions for the same prediction task