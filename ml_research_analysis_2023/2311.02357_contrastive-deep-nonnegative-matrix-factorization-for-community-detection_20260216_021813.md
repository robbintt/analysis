---
ver: rpa2
title: Contrastive Deep Nonnegative Matrix Factorization for Community Detection
arxiv_id: '2311.02357'
source_url: https://arxiv.org/abs/2311.02357
tags:
- matrix
- community
- network
- detection
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses three limitations of existing NMF-based community
  detection methods: inability to capture hierarchical information, neglect of node
  attributes, and difficulty in learning global structure information. To address
  these issues, the authors propose a new algorithm called Contrastive Deep Nonnegative
  Matrix Factorization (CDNMF).'
---

# Contrastive Deep Nonnegative Matrix Factorization for Community Detection

## Quick Facts
- arXiv ID: 2311.02357
- Source URL: https://arxiv.org/abs/2311.02357
- Reference count: 0
- Key outcome: Proposed CDNMF achieves 1.82% higher ACC and 5.42% higher NMI than suboptimal baselines on Cora dataset.

## Executive Summary
This paper addresses three key limitations of existing NMF-based community detection methods: inability to capture hierarchical community structure, neglect of node attributes, and difficulty in learning global structural information. The authors propose Contrastive Deep Nonnegative Matrix Factorization (CDNMF), which introduces a deep NMF architecture to extract hierarchical information, a contrastive learning framework to unify graph topology and node attributes, and a debiased negative sampling layer to enhance community-level information learning. Experiments on Cora, Citeseer, and PubMed datasets demonstrate that CDNMF outperforms state-of-the-art methods in both accuracy and normalized mutual information.

## Method Summary
CDNMF combines deep NMF layers with contrastive learning to perform community detection on attributed graphs. The method first constructs two views of the data: one from network topology (adjacency matrix) and one from node attributes. Deep NMF layers factorize both views into hierarchical representations, with graph regularization preserving local neighborhood structure. A contrastive learning framework then aligns the representations from both views while pushing apart negative samples. The debiased negative sampling layer improves contrastive learning by excluding nodes with the same pseudo-community labels from negative samples. The final community assignments are obtained by applying argmax to the topology view representation.

## Key Results
- CDNMF achieves 0.6081 ACC and 0.4006 NMI on Cora dataset
- Performance improvements of 1.82% in ACC and 5.42% in NMI over suboptimal baselines
- Fast convergence within approximately 20 epochs across all tested datasets
- Superior performance compared to state-of-the-art methods on Citeseer and PubMed datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep NMF (DNMF) captures hierarchical community structure better than shallow NMF.
- Mechanism: By stacking multiple NMF layers, each successive layer refines the node representation space, allowing the model to learn multi-scale community memberships rather than a single global partition.
- Core assumption: Hierarchical community structure exists in real networks and can be progressively uncovered by successive NMF factorizations.
- Evidence anchors:
  - [abstract] "we deepen NMF to strengthen its capacity for information extraction"
  - [section] "Real-world networks have complex organizational principles, which are difficult to extract by shallow NMF"
  - [corpus] Weak evidence: No direct citation of hierarchical NMF performance in corpus; claim is based on model design rationale.
- Break condition: If the network lacks clear hierarchical structure, deeper layers may overfit or fail to converge to meaningful partitions.

### Mechanism 2
- Claim: Contrastive learning unifies topology and attribute views to improve community detection accuracy.
- Mechanism: By treating adjacency-derived and attribute-derived representations as two views, the model maximizes agreement between them while pushing apart negative samples, enforcing consistency across modalities.
- Core assumption: Node attributes and network topology are aligned in community structure, so their embeddings should be consistent.
- Evidence anchors:
  - [abstract] "constructs network topology and node attributes as two contrasting views"
  - [section] "For each node vi, we consider the representation vector Vp(:, i) generated on the topology view as the anchor point, the representation vector Hm(:, i) generated on the node attribute view as the positive sample"
  - [corpus] Weak evidence: No direct contrastive learning ablation in corpus; effect inferred from contrastive framework.
- Break condition: If attributes are noisy or irrelevant to community structure, the contrastive objective may degrade performance.

### Mechanism 3
- Claim: Debiased negative sampling reduces false negatives by filtering out nodes with same pseudo-community labels.
- Mechanism: Pseudo labels from current DNMF output are used to exclude same-community nodes from negative samples, improving the quality of contrastive negatives.
- Core assumption: Pseudo-community labels become increasingly accurate as training proceeds, so excluding them improves contrastive learning.
- Evidence anchors:
  - [abstract] "utilize a debiased negative sampling layer and learn node similarity at the community level"
  - [section] "we first obtain the pseudo labels by: Cp = {c∗ i }n i=1 = argmax (Vp)" and "remove all nodes that have the same pseudo label as vi"
  - [corpus] No direct corpus support; mechanism inferred from method description.
- Break condition: If pseudo labels are poor early in training, excluding them may bias negatives and harm convergence.

## Foundational Learning

- Concept: Nonnegative Matrix Factorization (NMF)
  - Why needed here: NMF decomposes adjacency and feature matrices into nonnegative factors, providing interpretable community membership scores.
  - Quick check question: In NMF, what do the columns of the basis matrix U represent?

- Concept: Graph Laplacian and regularization
  - Why needed here: The graph Laplacian term preserves local neighborhood structure during deep factorization, ensuring smoothness across communities.
  - Quick check question: What is the effect of adding tr(VpLVpT) to the objective?

- Concept: Contrastive learning objective
  - Why needed here: The InfoNCE-style loss encourages embeddings from topology and attribute views to align for same nodes while separating different communities.
  - Quick check question: In the contrastive loss, what role does the temperature parameter τ play?

## Architecture Onboarding

- Component map:
  Input: adjacency matrix A, node feature matrix X
  DNMF Layer: multi-layer factorization for A → Vp, X → Hm
  Graph Regularization: tr(VpLVpT) + tr(HmLHmT)
  Debiased Negative Sampling: generate eNi from pseudo labels
  Contrastive Layer: InfoNCE loss over (Vp(:,i), Hm(:,i)) pairs
  Output: community labels from argmax(Vp)

- Critical path:
  1. Pre-train each NMF layer on A and X separately.
  2. Initialize factors with pre-trained values.
  3. Alternate: generate pseudo labels → update negative sets → optimize total loss.

- Design tradeoffs:
  - Deeper DNMF improves hierarchical capture but increases computation and risk of overfitting.
  - Debiased negatives improve quality but rely on early-stage pseudo-label accuracy.
  - Contrastive framework adds complexity but can fuse multimodal information.

- Failure signatures:
  - Poor ACC/NMI despite convergence: likely modality misalignment or weak pseudo labels.
  - Degraded performance vs shallow NMF: possible overfitting or bad negative sampling.
  - Slow convergence: learning rates or temperature τ may be suboptimal.

- First 3 experiments:
  1. Run DNMF alone (γ=0) to verify hierarchical gains over shallow NMF.
  2. Add contrastive loss only (β=0) to measure cross-view consistency benefit.
  3. Combine all components and compare ACC/NMI on Cora vs baselines.

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Limited empirical validation for debiased negative sampling mechanism and its dependence on early pseudo-label quality
- Assumption of strong alignment between node attributes and topology may not hold in all real-world networks
- Claims about hierarchical NMF superiority lack direct ablation study validation across diverse datasets

## Confidence
- High confidence: Overall framework design and mathematical formulation are clearly specified
- Medium confidence: Reported performance improvements appear reasonable though exact hyperparameters unspecified
- Low confidence: Claims about hierarchical NMF superiority and debiased negative sampling benefits lack direct empirical validation

## Next Checks
1. Ablation study: Remove the deep NMF component (use shallow NMF only) and compare ACC/NMI on all three datasets to quantify the actual contribution of hierarchical factorization.

2. Negative sampling analysis: Run experiments with standard (non-debiased) negative sampling to isolate the contribution of the debiased approach and test sensitivity to early pseudo-label quality.

3. Attribute relevance test: Evaluate performance on Cora and Citeseer after randomly permuting node attributes to determine if contrastive learning degrades as expected when attributes lack community signal.