---
ver: rpa2
title: Expanding the Vocabulary of BERT for Knowledge Base Construction
arxiv_id: '2310.08291'
source_url: https://arxiv.org/abs/2310.08291
tags:
- language
- knowledge
- tokens
- vocabulary
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Vocabulary Expandable BERT (VE-BERT) to address
  the challenge of knowledge base construction using pretrained language models. The
  key problem is that BERT is not designed to predict multi-token entities, which
  are common in knowledge bases.
---

# Expanding the Vocabulary of BERT for Knowledge Base Construction

## Quick Facts
- arXiv ID: 2310.08291
- Source URL: https://arxiv.org/abs/2310.08291
- Reference count: 13
- Key result: VE-BERT achieves 0.362 F1 on validation and 0.323 on hidden test sets, outperforming ChatGPT-3 despite having only 0.13B parameters

## Executive Summary
This paper addresses knowledge base construction by modifying BERT to handle multi-token entities as single tokens. The proposed VE-BERT model expands BERT's vocabulary and uses a Token-Recode method to initialize embeddings for newly added entities. Task-specific pre-training on Wikipedia further enhances performance. The model achieves competitive results on a knowledge base construction task, demonstrating that targeted architectural modifications can be more effective than simply scaling model size.

## Method Summary
VE-BERT expands BERT's vocabulary to include multi-token entities as single tokens, modifying both token embedding and output embedding layers. For newly added entities, Token-Recode initializes embeddings by averaging constituent token embeddings and normalizing the result. The model undergoes task-specific pre-training on a Wikipedia corpus filtered for entities in the knowledge base vocabulary, followed by fine-tuning on the target task. Inference uses relation-specific thresholds to select predicted entities from the expanded vocabulary.

## Key Results
- VE-BERT achieves F1 score of 0.362 on validation set and 0.323 on hidden test set
- Outperforms ChatGPT-3 (125× larger) on the knowledge base construction task
- Token-Recode initialization provides comparable performance to full re-pretraining with less computation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanding BERT's vocabulary to include multi-token entities as single tokens enables direct prediction of complete entities.
- Mechanism: The model modifies the token embedding and output embedding layers to treat phrases like "United States of America" as single tokens rather than sequences of separate tokens. This allows the model to predict the entire entity in one step instead of assembling it from individual token predictions.
- Core assumption: Multi-token entities can be meaningfully represented as single tokens without losing semantic relationships between their constituent parts.
- Evidence anchors:
  - [abstract] "expand the language model's vocabulary while preserving semantic embeddings for newly added words"
  - [section] "we modify the token embedding layer and the output embedding layer of BERT. We expand the vocabulary of language model... the object composed of multiple tokens... is treated as a distinct token"
- Break condition: If the semantic relationships between constituent tokens are critical for accurate prediction, collapsing them into a single token may lose necessary context.

### Mechanism 2
- Claim: Token-Recode method provides high-quality initial embeddings for newly added entities by averaging constituent token embeddings.
- Mechanism: For each new multi-token entity, the model computes its embedding as the average of the embeddings of its constituent tokens, then normalizes the result. This initialization preserves semantic information from the original vocabulary.
- Core assumption: The average of constituent token embeddings captures sufficient semantic information about the multi-token entity.
- Evidence anchors:
  - [abstract] "provide an initial semantic vector for newly added entities"
  - [section] "We introduce modifications to the token embedding and output embedding components of the BERT model, enabling the generation of embeddings for newly introduced phrases based on their constituent tokens"
  - [section] "the token and output embeddings for the phrase 'United States of America' are computed as the average of the respective embeddings for its tokens"
- Break condition: If constituent tokens have conflicting semantic meanings, averaging may produce a meaningless intermediate representation.

### Mechanism 3
- Claim: Task-specific pre-training on Wikipedia corpus filtered for knowledge base entities improves model performance for knowledge base construction.
- Mechanism: The model is pre-trained on sentences containing entities from the knowledge base vocabulary, focusing the model's attention on the types of entities and relationships relevant to the target task.
- Core assumption: Pre-training on domain-relevant text transfers useful knowledge that improves downstream task performance.
- Evidence anchors:
  - [abstract] "We adopt task-specific re-pretraining on masked language model to further enhance the language model"
  - [section] "We collect sentences from wikipedia, filter sentences according to the frequency of the entities in our vocabulary"
  - [section] "The sentence in our corpus is selected based on the criterion that the sentences include the entities listed in our vocabulary"
- Break condition: If the filtered corpus is too small or unrepresentative, pre-training may not provide meaningful benefits or could introduce bias.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: Understanding MLM is crucial because VE-BERT builds upon BERT's MLM framework to predict multi-token entities
  - Quick check question: In MLM, what percentage of tokens are typically masked during pre-training?

- Concept: Tokenization and Subword Units
  - Why needed here: VE-BERT modifies how tokens are represented, requiring understanding of how standard tokenization works and why subword units are used
  - Quick check question: Why do modern language models like BERT use subword tokenization rather than word-level tokenization?

- Concept: Embedding Dimensionality and Normalization
  - Why needed here: The Token-Recode method involves averaging and normalizing embeddings, requiring understanding of vector operations in embedding space
  - Quick check question: What is the purpose of normalizing embeddings after averaging constituent token embeddings?

## Architecture Onboarding

- Component map:
  Input -> Token Embedding Layer (modified for multi-token entities) -> Transformer Layers -> Output Embedding Layer (modified for multi-token entities) -> Token-Recode Layer -> Pre-training Module -> Fine-tuning Module -> Disambiguation Layer

- Critical path: Token-Recode initialization → Pre-training on Wikipedia → Fine-tuning on KB task → Inference with threshold selection

- Design tradeoffs:
  - Memory vs. Coverage: Larger vocabulary enables more entity predictions but increases memory requirements
  - Semantic Preservation vs. Simplification: Averaging embeddings is simple but may lose nuanced relationships
  - Pre-training vs. Fine-tuning: More pre-training could improve performance but increases computational cost

- Failure signatures:
  - Poor performance on multi-token entities suggests Token-Recode initialization is inadequate
  - Overfitting to Wikipedia domain suggests pre-training corpus needs diversification
  - Inconsistent predictions across similar inputs suggests threshold selection needs refinement

- First 3 experiments:
  1. Verify Token-Recode produces meaningful embeddings by checking cosine similarity between averaged embeddings and reference embeddings for known multi-token entities
  2. Test pre-training effectiveness by comparing performance with and without task-specific pre-training on a validation set
  3. Validate threshold selection by analyzing precision-recall curves for different relation types and identifying optimal thresholds

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the traditional sense, but several important questions emerge from the methodology and results:

### Open Question 1
- Question: How does VE-BERT's performance scale with larger knowledge base vocabularies?
- Basis in paper: [explicit] The paper discusses vocabulary expansion and performance on current datasets but does not explore scaling to much larger vocabularies.
- Why unresolved: The current experiments use a limited vocabulary size, and the impact of significantly larger vocabularies on model performance and efficiency is unknown.
- What evidence would resolve it: Systematic experiments scaling vocabulary size from current levels to orders of magnitude larger, measuring F1 scores and computational efficiency.

### Open Question 2
- Question: What is the optimal balance between task-specific pre-training and fine-tuning for different types of knowledge base predicates?
- Basis in paper: [inferred] The paper shows benefits of both task-specific pre-training and fine-tuning, but doesn't explore optimal balance for different predicate types.
- Why unresolved: Different predicate types (e.g., factual vs. privacy-related) may benefit differently from pre-training vs. fine-tuning, but this relationship is not characterized.
- What evidence would resolve it: Comparative experiments varying pre-training vs. fine-tuning ratios for different predicate categories, measuring performance differences.

### Open Question 3
- Question: How does VE-BERT's performance compare to transformer-based models with similar parameter counts on knowledge base construction tasks?
- Basis in paper: [explicit] The paper compares VE-BERT to much larger models (ChatGPT-3) but doesn't compare to transformer variants with similar parameter counts.
- Why unresolved: The current comparison is to much larger models, making it unclear if the performance is due to architecture or parameter count advantages.
- What evidence would resolve it: Head-to-head comparisons of VE-BERT with transformer variants (XLNet, GPT variants) of similar parameter counts on the same knowledge base construction tasks.

### Open Question 4
- Question: How does the Token-Recode method perform on languages with different morphological structures compared to English?
- Basis in paper: [inferred] The paper focuses on English Wikipedia data and doesn't explore performance on morphologically rich languages.
- Why unresolved: Languages with complex morphology may require different tokenization and embedding strategies, but this is not investigated.
- What evidence would resolve it: Experiments applying VE-BERT with Token-Recode to knowledge base construction tasks in morphologically rich languages (e.g., Finnish, Turkish, Arabic) and comparing performance to English.

## Limitations

- The Token-Recode method may produce suboptimal embeddings when constituent tokens have conflicting semantic meanings
- Performance depends heavily on the quality and coverage of the Wikipedia corpus used for pre-training
- The approach has only been validated on a single knowledge base construction task, limiting generalizability claims

## Confidence

- Mechanism 1 (Vocabulary Expansion): Medium confidence - The approach is well-defined but assumes semantic preservation when collapsing multi-token entities to single tokens
- Mechanism 2 (Token-Recode): Low confidence - The averaging method is simple but may produce suboptimal embeddings for complex entities
- Mechanism 3 (Task-specific Pre-training): Medium confidence - Pre-training on domain-relevant data is reasonable, but the effectiveness depends heavily on corpus quality and size

## Next Checks

1. **Embedding Quality Validation**: Measure the cosine similarity between Token-Recode averaged embeddings and reference embeddings for a sample of multi-token entities from the knowledge base. Compare performance across different entity types (locations, organizations, etc.) to identify patterns where averaging succeeds or fails.

2. **Ablation Study on Pre-training**: Systematically test VE-BERT performance with varying amounts of task-specific pre-training (0%, 25%, 50%, 100% of current duration) to quantify the marginal benefit and identify diminishing returns. Include an analysis of which entity types benefit most from pre-training.

3. **Cross-domain Transfer Test**: Evaluate VE-BERT on a knowledge base construction task from a different domain (e.g., biomedical entities) without additional pre-training. Measure performance drop to assess domain dependence and identify which components (vocabulary expansion, Token-Recode, or both) are most domain-specific.