---
ver: rpa2
title: 'Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis,
  Edge of Stability, and Route to Chaos'
arxiv_id: '2311.02076'
source_url: https://arxiv.org/abs/2311.02076
tags:
- training
- sharpness
- dynamics
- learning
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes the sharpness dynamics of neural networks during
  training, focusing on phenomena such as progressive sharpening and edge of stability.
  By studying a simplified 2-layer linear network (UV model) trained on a single example,
  the authors uncover the underlying mechanisms behind these sharpness trends.
---

# Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos

## Quick Facts
- arXiv ID: 2311.02076
- Source URL: https://arxiv.org/abs/2311.02076
- Reference count: 40
- This work analyzes sharpness dynamics during neural network training, uncovering fixed point mechanisms behind progressive sharpening and edge of stability.

## Executive Summary
This paper investigates the dynamics of sharpness (as measured by the top Hessian eigenvalue) during neural network training, focusing on phenomena like progressive sharpening and the edge of stability. By studying a simplified 2-layer linear network (UV model) trained on a single example, the authors identify fixed points in function space and analyze the vector field of function updates to reveal the underlying mechanisms behind these sharpness trends. They demonstrate that progressive sharpening and edge of stability arise from the dynamics of fixed points in function space, with sharpness increasing until it reaches 2/η, after which the trajectory oscillates along the edge of stability manifold. The predictions from this simplified model are then verified in realistic architectures with real and synthetic datasets, showing that various insights generalize to real-world scenarios.

## Method Summary
The authors employ a two-pronged approach: first, they analyze a simplified 2-layer linear network (UV model) trained on a single example to derive analytical insights into sharpness dynamics; second, they validate these insights through experiments on realistic architectures (FCNs, CNNs, ResNets) trained on real datasets (MNIST, Fashion-MNIST, CIFAR-10) and synthetic datasets. They track sharpness using the top eigenvalue of the Hessian, compute power spectra of sharpness trajectories, and examine the effect of different parameterizations (standard, maximal update). The analysis combines theoretical fixed-point analysis with empirical validation across multiple architectures and datasets.

## Key Results
- Progressive sharpening and edge of stability phenomena are explained through fixed point dynamics in function space
- Early sharpness reduction is caused by flow toward a saddle point, followed by progressive sharpening as the trajectory moves away
- Period-doubling route to chaos emerges at the edge of stability due to the sub-quadratic nature of the loss function
- The findings generalize across multiple realistic architectures and datasets, validating the simplified UV model's predictive power

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive sharpening and edge of stability arise from the dynamics of fixed points in function space.
- Mechanism: As training proceeds, the sharpness (top Hessian eigenvalue) increases until it reaches 2/η, at which point the zero-loss fixed line becomes unstable. The trajectory then oscillates along the edge of stability manifold between fixed points II and IV.
- Core assumption: The function space dynamics of the UV model accurately captures the essential sharpness phenomenology observed in real-world neural networks.
- Evidence anchors:
  - [abstract] The authors demonstrate that a 2-layer linear network (UV model) trained on a single example exhibits all of the essential sharpness phenomenology observed in real-world scenarios.
  - [section] The analysis reveals the emergence of various sharpness phenomena arising from the stability and position of the dynamical fixed points.
  - [corpus] Weak. Related works discuss edge of stability but do not provide mechanistic explanations tied to fixed point dynamics in function space.
- Break condition: If the assumptions about the UV model's ability to capture real-world sharpness dynamics are violated, or if the model parameterization significantly deviates from the assumptions.

### Mechanism 2
- Claim: Early sharpness reduction is due to the flow towards a saddle point in function space.
- Mechanism: At early training times, the trajectory moves closer to the saddle point II, resulting in an interim decrease in sharpness. Eventually, the trajectory moves away from this saddle and enters the progressive sharpening region.
- Core assumption: The saddle point II in function space exists and has a significant influence on the early training dynamics.
- Evidence anchors:
  - [section] The analysis shows that initializations in the sharpness reduction region undergo a decrease in sharpness as the flow is towards saddle point II.
  - [corpus] Weak. Related works mention sharpness reduction but do not provide a mechanistic explanation tied to saddle point dynamics.
- Break condition: If the assumptions about the existence and influence of saddle point II are violated, or if the model parameterization significantly deviates from the assumptions.

### Mechanism 3
- Claim: The period-doubling route to chaos at the edge of stability is a consequence of the sub-quadratic nature of the loss function.
- Mechanism: The loss on the edge of stability manifold is sub-quadratic near its minimum, leading to a cubic map in the gradient descent dynamics. This cubic map is known to exhibit a period-doubling route to chaos.
- Core assumption: The loss function on the edge of stability manifold is sub-quadratic near its minimum.
- Evidence anchors:
  - [section] The analysis shows that the loss on the edge of stability manifold is of the form L(θ) ≈ 1/2(a∥θ∥2 - b)2, which is sub-quadratic near its minimum.
  - [corpus] Weak. Related works mention chaos and period-doubling but do not provide a mechanistic explanation tied to the sub-quadratic nature of the loss.
- Break condition: If the assumptions about the sub-quadratic nature of the loss function are violated, or if the model parameterization significantly deviates from the assumptions.

## Foundational Learning

- Concept: Fixed point analysis in dynamical systems
  - Why needed here: To understand the stability and behavior of the sharpness dynamics in the UV model.
  - Quick check question: What is the condition for a fixed point to be stable or unstable in a dynamical system?

- Concept: Hessian matrix and its eigenvalues
  - Why needed here: To measure the sharpness of the loss landscape and track its evolution during training.
  - Quick check question: How does the top eigenvalue of the Hessian relate to the sharpness of the loss landscape?

- Concept: Neural network parameterization and initialization
  - Why needed here: To understand how different parameterizations (standard, maximal update) affect the sharpness dynamics and training regimes.
  - Quick check question: How do standard and maximal update parameterizations differ in terms of the effective width and initialization of the network?

## Architecture Onboarding

- Component map:
  UV model (2-layer linear network) -> Gradient descent with MSE loss -> Sharpness tracking (top Hessian eigenvalue) -> Fixed point analysis in function space

- Critical path:
  1. Initialize the UV model with the given parameters
  2. Compute the function space dynamics using the update equations
  3. Analyze the fixed points and their stability
  4. Track the evolution of sharpness during training
  5. Identify the training regimes (early transient, intermediate saturation, progressive sharpening, edge of stability)

- Design tradeoffs:
  - Model complexity vs. analytical tractability: The UV model is a simplified 2-layer linear network, which allows for analytical tractability but may not capture all the complexities of real-world neural networks.
  - Single example vs. dataset: The UV model is trained on a single example, which simplifies the analysis but may not fully represent the behavior of networks trained on large datasets.

- Failure signatures:
  - Sharpness does not exhibit the expected phenomenology (progressive sharpening, edge of stability, etc.)
  - Fixed points do not exist or have unexpected stability properties
  - Function space dynamics do not match the expected behavior

- First 3 experiments:
  1. Verify the existence and stability of the fixed points for a given set of parameters
  2. Track the evolution of sharpness during training and identify the training regimes
  3. Analyze the effect of different parameterizations (standard, maximal update) on the sharpness dynamics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the dynamics of the sharpness evolution change in more complex architectures, such as transformers or recurrent neural networks, compared to the simpler models studied in this paper?
- Basis in paper: [inferred] The paper primarily focuses on feedforward neural networks (FCNs, CNNs, and ResNets) and a simplified UV model. The authors acknowledge the limitations of their study and suggest that extending their analysis to other architectures could provide further insights.
- Why unresolved: The paper does not explore the behavior of sharpness dynamics in architectures beyond feedforward networks. The complexity and non-linearity of these architectures might introduce new phenomena or alter the existing ones observed in simpler models.
- What evidence would resolve it: Systematic experiments on the sharpness dynamics of transformers and recurrent neural networks, comparing them to the findings in this paper, would provide valuable insights into the generalizability of the observed phenomena.

### Open Question 2
- Question: How does the presence of batch normalization or other normalization techniques affect the sharpness dynamics during training?
- Basis in paper: [inferred] The paper does not explicitly consider the impact of normalization techniques on sharpness dynamics. The authors mention that their findings are robust for reasonable batch sizes (around 512), but they do not explore the effect of normalization techniques.
- Why unresolved: Normalization techniques are commonly used in deep learning and can significantly influence the training dynamics. Understanding their impact on sharpness evolution could provide a more comprehensive understanding of the training process.
- What evidence would resolve it: Experiments comparing the sharpness dynamics of networks with and without normalization techniques, such as batch normalization or layer normalization, would shed light on their influence on the observed phenomena.

### Open Question 3
- Question: Can the insights from the UV model and the observed phenomena be leveraged to develop more effective optimization algorithms for deep learning?
- Basis in paper: [inferred] The paper provides a detailed analysis of the sharpness dynamics and identifies the underlying mechanisms behind various phenomena. While the authors do not explicitly propose new optimization algorithms, their findings could potentially inspire the development of improved methods.
- Why unresolved: The connection between the observed phenomena and the design of optimization algorithms is not explicitly explored in the paper. Further research is needed to translate the insights into practical improvements in optimization techniques.
- What evidence would resolve it: Theoretical analysis and empirical experiments demonstrating the effectiveness of optimization algorithms that incorporate the insights from this paper, such as algorithms that explicitly target sharpness reduction or leverage the period-doubling route to chaos, would provide strong evidence for their potential impact.

## Limitations
- The UV model's simplifications (single example, linear network) may not fully capture the complexity of real neural networks
- The period-doubling route to chaos analysis assumes a specific sub-quadratic loss form that may not generalize across all network architectures
- The analytical tractability relies heavily on the linear structure and single-example training, potentially missing non-linear and dataset-dependent effects

## Confidence
- **High**: The existence of progressive sharpening and edge of stability phenomena across realistic architectures; the role of initialization and parameterization in determining training regimes
- **Medium**: The mechanistic explanation of sharpness reduction via saddle point dynamics; the connection between sub-quadratic loss and period-doubling route to chaos
- **Low**: The universality of the UV model's fixed-point analysis for all neural network architectures; the precise conditions under which the period-doubling route to chaos emerges in realistic settings

## Next Checks
1. Test the UV model predictions on non-linear networks (e.g., ReLU networks) to verify if the fixed-point analysis and sharpness phenomenology persist beyond linear architectures
2. Experimentally verify the period-doubling route to chaos in architectures beyond the tested FCNs and ResNets, particularly in modern transformer architectures
3. Investigate the effect of data augmentation and batch normalization on the sharpness dynamics and whether they alter the fixed-point structure predicted by the UV model