---
ver: rpa2
title: Stabilizing Backpropagation in 16-bit Neural Training with Modified Adam Optimizer
arxiv_id: '2307.16189'
source_url: https://arxiv.org/abs/2307.16189
tags:
- numerical
- neural
- instability
- adam
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of numerical instability in 16-bit
  neural network training, particularly when using optimizers like Adam and RMSProp.
  The authors identify the epsilon hyperparameter as the main source of instability,
  especially when its reciprocal exceeds the maximum representable value in 16-bit
  floating point.
---

# Stabilizing Backpropagation in 16-bit Neural Training with Modified Adam Optimizer

## Quick Facts
- arXiv ID: 2307.16189
- Source URL: https://arxiv.org/abs/2307.16189
- Reference count: 18
- Primary result: Modified Adam optimizer stabilizes 16-bit neural training by capping denominator, achieving 45% faster training and improved accuracy on MNIST

## Executive Summary
This paper addresses numerical instability in 16-bit neural network training, specifically when using Adam and RMSProp optimizers. The authors identify the epsilon hyperparameter as the primary source of instability, particularly when its reciprocal exceeds the maximum representable value in 16-bit floating point. To resolve this, they propose a modified Adam optimizer that ensures the denominator in the gradient update remains within a safer numerical range by using a maximum function. Experimental results on MNIST with a simple deep neural network show that this modification improves numerical stability and test accuracy while achieving a 45% reduction in training time compared to 32-bit precision settings.

## Method Summary
The paper proposes a modified Adam optimizer that introduces a numerical guarantee method to stabilize 16-bit training. The key modification is adding a max function to the denominator calculation in the Adam update rule: `wt = wt-1 - η · m̂t / max(sqrt(v̂t), ε)`. This ensures the denominator never becomes smaller than sqrt(epsilon), preventing division by extremely small values that cause overflow. The authors implement this using TensorFlow's mixed precision capabilities and test it on the MNIST dataset with a simple linear model. The method is compared against standard Adam in both 16-bit and 32-bit precision settings, measuring test accuracy and training time.

## Key Results
- Modified Adam optimizer improves numerical stability in 16-bit training by capping the denominator with a maximum function
- Test accuracy on MNIST is maintained or improved compared to 32-bit precision with standard Adam
- 45% reduction in training time achieved with 16-bit precision using the modified optimizer
- Numerical instability occurs when epsilon reciprocal exceeds 65504 (max float16 value), causing overflow to infinity

## Why This Works (Mechanism)

### Mechanism 1
The reciprocal of epsilon causes overflow in 16-bit precision because its value can exceed the maximum representable float16 number. When epsilon is small (e.g., 1e-7), its reciprocal (1/epsilon) becomes large (1e+7). In 16-bit floating-point format, the maximum value is approximately 65504. If 1/epsilon exceeds this, it overflows to infinity, causing numerical instability in weight updates. This overflow propagates through the Adam update rule, destabilizing training.

### Mechanism 2
The modified Adam optimizer stabilizes training by capping the denominator in the update rule, preventing division by extremely small values. The update rule is modified to use `max(sqrt(v), epsilon)` instead of `sqrt(v) + epsilon`. This ensures the denominator never becomes smaller than sqrt(epsilon), avoiding large weight updates that cause overflow. The instability arises specifically when `v` (the moving average of squared gradients) becomes very small, making `sqrt(v) + epsilon` close to epsilon and thus its reciprocal large.

### Mechanism 3
16-bit computations are inherently faster than 32-bit, and the modified optimizer allows leveraging this speedup without sacrificing accuracy. By maintaining numerical stability in 16-bit precision, the modified Adam enables training at half the bit-width, reducing memory bandwidth and computational cost. The paper reports a 45% reduction in training time compared to 32-bit precision settings, attributed to reduced data movement and faster arithmetic in 16-bit.

## Foundational Learning

- **IEEE 754 floating-point representation (especially float16)**
  - Why needed here: Understanding the range and precision limits of float16 is essential to grasp why epsilon's reciprocal causes overflow and why 16-bit training is both risky and beneficial.
  - Quick check question: What is the maximum finite value representable in IEEE 754 binary16 (float16)?

- **Adam optimizer mechanics (momentum, adaptive learning rates)**
  - Why needed here: The paper's modification targets the denominator in Adam's update rule; knowing how m and v are computed and used is necessary to understand the fix.
  - Quick check question: In Adam, what are the roles of the first moment (m) and second moment (v) estimates?

- **Backpropagation and gradient descent fundamentals**
  - Why needed here: The paper focuses on instability during backpropagation; understanding how gradients are computed and applied is key to following the analysis.
  - Quick check question: Why is backpropagation more susceptible to numerical instability than forward propagation in neural networks?

## Architecture Onboarding

- **Component map**: Input layer (MNIST images) -> Linear (fully connected) layer -> Adam optimizer with modified update rule -> 16-bit floating-point computation (TensorFlow mixed precision) -> Loss function and backpropagation
- **Critical path**: 
  1. Forward pass: input → linear layer → output
  2. Compute loss
  3. Backward pass: compute gradients
  4. Adam update: apply modified rule with max(sqrt(v), epsilon)
  5. Repeat for each batch
- **Design tradeoffs**:
  - 16-bit precision: faster, less memory, risk of overflow/underflow
  - 32-bit precision: safer numerically, slower, higher memory use
  - Epsilon choice: too small → overflow; too large → poor convergence
  - max(sqrt(v), epsilon): stabilizes denominator but may slow adaptation if epsilon is large
- **Failure signatures**:
  - Weights become NaN or ±∞ during training
  - Loss becomes NaN or diverges rapidly
  - Test accuracy drops to near zero after a few epochs
  - Training time increases unexpectedly (if switching to 32-bit fallback)
- **First 3 experiments**:
  1. Run standard Adam (float32) on MNIST linear model; record accuracy and time.
  2. Run standard Adam (float16) without modification; observe if weights become NaN/Inf.
  3. Run modified Adam (float16) with max(sqrt(v), epsilon); compare accuracy and time to previous runs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the modified Adam optimizer perform on more complex neural network architectures beyond a single linear layer, such as convolutional neural networks (CNNs) and Vision Transformers (ViTs)?
- **Basis in paper**: [explicit] The paper mentions the need for further validation on CNNs and ViTs, as the current experiments were limited to a simple DNN with a single linear layer.
- **Why unresolved**: The current study only tested the method on a basic linear layer, so its generalizability to more complex architectures is unknown.
- **What evidence would resolve it**: Experiments comparing the modified Adam optimizer's performance on CNNs and ViTs against standard optimizers, measuring both accuracy and numerical stability.

### Open Question 2
- **Question**: What are the effects of the proposed epsilon modification on training time, memory requirements, and hyperparameter sensitivity compared to the standard Adam optimizer?
- **Basis in paper**: [inferred] The paper mentions a 45% reduction in training time with 16-bit precision but does not explore broader impacts on memory or hyperparameter sensitivity.
- **Why unresolved**: The study focused primarily on numerical stability and accuracy, leaving other performance metrics unexplored.
- **What evidence would resolve it**: Systematic experiments measuring memory usage, training time, and sensitivity to learning rate and other hyperparameters for both the modified and standard Adam optimizers.

### Open Question 3
- **Question**: Is there an optimal epsilon value for the modified Adam optimizer that balances numerical stability and model performance across different tasks and architectures?
- **Basis in paper**: [explicit] The paper discusses the role of epsilon in numerical instability but does not identify an optimal value or provide a principled method for choosing it.
- **Why unresolved**: The experiments tested multiple epsilon values but did not analyze which value is optimal or how to select it for different scenarios.
- **What evidence would resolve it**: A comprehensive study varying epsilon across tasks and architectures to identify trends and provide guidelines for optimal epsilon selection.

## Limitations

- **Limited experimental scope**: Evaluation is restricted to a single MNIST linear model, providing no evidence of scalability to deeper or more complex architectures.
- **Hyperparameter analysis incomplete**: The paper does not explore the full hyperparameter space or compare against alternative stabilization techniques like loss scaling or gradient clipping.
- **Hardware dependency**: Speedup claims depend on specific hardware support for 16-bit arithmetic and TensorFlow's mixed-precision implementation, limiting reproducibility across platforms.

## Confidence

- **High Confidence**: The identification of epsilon's reciprocal overflow as a source of instability in 16-bit Adam is well-supported by numerical analysis and experimental observation.
- **Medium Confidence**: The modified Adam optimizer improves stability and test accuracy on the MNIST linear model; however, results may not generalize to more complex models or datasets.
- **Low Confidence**: The 45% training time reduction claim, as it is based on a single experimental setup and lacks comparison to other stabilization methods or hardware configurations.

## Next Checks

1. **Scalability Test**: Apply the modified Adam optimizer to deeper neural networks (e.g., multi-layer perceptrons, convolutional networks) on standard benchmarks (e.g., CIFAR-10, ImageNet) to verify if the stability and speedup benefits persist.

2. **Hyperparameter Sweep**: Systematically vary epsilon and the lower limit in the max function across a range of values to identify optimal settings and robustness to hyperparameter choice.

3. **Comparison to Alternatives**: Benchmark the modified Adam against other 16-bit training stabilization techniques (e.g., loss scaling, gradient clipping) on the same tasks to assess relative effectiveness and efficiency.