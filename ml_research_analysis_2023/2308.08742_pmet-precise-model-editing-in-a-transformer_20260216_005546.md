---
ver: rpa2
title: 'PMET: Precise Model Editing in a Transformer'
arxiv_id: '2308.08742'
source_url: https://arxiv.org/abs/2308.08742
tags:
- knowledge
- hidden
- mhsa
- states
- pmet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of imprecise updates in model
  editing of large language models. Existing methods update the weights of the feed-forward
  network (FFN) using hidden states that contain irrelevant information from other
  components, leading to suboptimal performance.
---

# PMET: Precise Model Editing in a Transformer

## Quick Facts
- arXiv ID: 2308.08742
- Source URL: https://arxiv.org/abs/2308.08742
- Reference count: 18
- PMET achieves 3.3% average reliability enhancement over state-of-the-art on COUNTERFACT dataset

## Executive Summary
This paper addresses the imprecision problem in model editing of large language models where existing methods update feed-forward network (FFN) weights using hidden states containing irrelevant information from other components. The authors propose PMET (Precise Model Editing in a Transformer), which simultaneously optimizes hidden states of both multi-head self-attention (MHSA) and FFN components while only using optimized FFN hidden states to update FFN weights. Experiments demonstrate PMET outperforms existing methods in efficacy, generalization, and consistency while maintaining specificity, showing 3.3% reliability improvement over state-of-the-art on COUNTERFACT and 0.4% on zsRE datasets.

## Method Summary
PMET introduces a novel approach to model editing by simultaneously optimizing hidden states of both MHSA and FFN components through additive parameters (δ) while preserving the original weights. The method leverages the observation that MHSA encodes general knowledge extraction patterns and doesn't require weight updates when new knowledge is introduced. Instead of directly updating weights, PMET optimizes hidden states to expand the function space for target knowledge representation, then uses only the optimized FFN hidden states for precise weight updates. A square root spreading mechanism distributes residuals across critical layers to enhance precision while maintaining model specificity.

## Key Results
- PMET shows 3.3% average reliability enhancement over state-of-the-art on COUNTERFACT dataset
- On zsRE dataset, PMET achieves 0.4% average improvement in reliability
- PMET strikes better balance between reliability and specificity compared to baselines, with performance becoming more pronounced as number of edited knowledge instances increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MHSA encodes general knowledge extraction patterns and does not require weight updates when new knowledge is introduced.
- Mechanism: MHSA functions as a knowledge extractor, continuously capturing various types of knowledge and storing general extraction patterns. By optimizing MHSA hidden states without updating weights, PMET leverages these patterns to enhance target knowledge representation in FFN.
- Core assumption: MHSA hidden states undergo frequent changes across layers while maintaining extraction patterns, whereas FFN hidden states stabilize after a certain layer.
- Evidence anchors:
  - [abstract] "we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced."
  - [section] "we believe that MHSA works as a knowledge extractor and stores certain general knowledge extraction patterns. This suggests the potential for supplementary optimization of TC hidden states of the MHSA to expand the function space, without necessitating updates to its weights."
  - [corpus] Weak evidence - corpus neighbors discuss MHSA roles but lack direct support for this specific claim about general patterns.
- Break condition: If MHSA is shown to store significant factual knowledge requiring weight updates, or if its extraction patterns prove too specific to be generalizable across edits.

### Mechanism 2
- Claim: Simultaneous optimization of both MHSA and FFN hidden states expands the function space for target knowledge representation.
- Mechanism: By adding optimizable parameters to both MHSA and FFN hidden states, PMET creates a larger solution space for aligning representations with target knowledge. Only FFN hidden states are then used for weight updates, ensuring precision.
- Core assumption: The combined optimization space of MHSA and FFN hidden states provides more flexibility than optimizing FFN alone.
- Evidence anchors:
  - [abstract] "we introduce PMET, which simultaneously optimizes Transformer Component (TC, namely MHSA and FFN) hidden states, while only using the optimized TC hidden states of FFN to precisely update FFN weights."
  - [section] "we suggest optimizing the Transformer Component (TC, namely MHSA and FFN) hidden states directly of FFN to memorize target knowledge for precise updates on FFN weights."
  - [corpus] Moderate evidence - related papers discuss multi-component optimization but lack specific evidence for this dual hidden state approach.
- Break condition: If optimization bottlenecks persist or if MHSA hidden state optimization proves detrimental to overall performance.

### Mechanism 3
- Claim: Square root spreading of residuals to critical layers improves reliability while maintaining model specificity.
- Mechanism: Instead of evenly distributing residuals across layers, square root spreading (Rl = V1 - W0K1/√(L-l+1)) concentrates information in earlier critical layers, enhancing target knowledge updates without excessive model changes.
- Core assumption: Non-linear spreading better preserves update information while minimizing unnecessary changes to unaffected knowledge.
- Evidence anchors:
  - [abstract] "we adopt a square root spread to convey more precise information to critical layers"
  - [section] "PMET adopts a square root spread to convey more precise information to critical layers"
  - [corpus] Weak evidence - corpus neighbors don't address residual spreading techniques directly.
- Break condition: If square root spreading consistently underperforms even spreading in reliability metrics, or if it causes disproportionate damage to specificity.

## Foundational Learning

- Concept: Transformer architecture components (MHSA, FFN, residual connections)
  - Why needed here: Understanding how information flows through these components is crucial for identifying why PMET's approach improves precision
  - Quick check question: What are the three sources of information flow in Transformer layer hidden states, and how does each contribute to knowledge representation?

- Concept: Knowledge editing vs. fine-tuning
  - Why needed here: PMET is a model editing technique that modifies internal knowledge without full retraining, requiring understanding of weight-based vs. hidden state-based approaches
  - Quick check question: How does model editing differ from fine-tuning in terms of computational cost and scope of knowledge modification?

- Concept: Hidden state optimization vs. weight optimization
  - Why needed here: PMET optimizes hidden states rather than directly updating weights, which is central to its precision advantage
  - Quick check question: What is the key difference between optimizing hidden states and updating model weights, and why might hidden state optimization be more precise?

## Architecture Onboarding

- Component map: Input -> MHSA (knowledge extraction) -> FFN (factual storage) -> Residual connections -> PMET optimizer (hidden state optimization) -> Weight updates

- Critical path:
  1. Input knowledge clue sequence
  2. Process through MHSA and FFN layers
  3. Apply PMET optimization to TC hidden states
  4. Use only FFN hidden state updates for weight modification
  5. Evaluate edited model performance

- Design tradeoffs:
  - Precision vs. computational overhead: PMET's dual optimization increases accuracy but requires more computation than single-component approaches
  - Reliability vs. specificity: Square root spreading improves target knowledge updates but may slightly reduce preservation of unrelated knowledge
  - General patterns vs. factual knowledge: Avoiding MHSA weight updates preserves extraction patterns but may limit handling of factual knowledge stored in MHSA

- Failure signatures:
  - Optimization bottlenecks in TC hidden state alignment
  - Degradation in specificity metrics when square root spreading is used
  - Performance parity with baselines indicating insufficient precision gains

- First 3 experiments:
  1. Compare PMET vs. MEMIT on COUNTERFACT dataset with 1K edits to verify reliability improvements
  2. Test ablation removing δai (MHSA hidden state optimization) to confirm contribution of dual optimization
  3. Evaluate even vs. square root spreading on 5K edits to measure impact on precision and specificity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does MHSA store factual knowledge beyond general knowledge extraction patterns, and if so, what is the extent and nature of this factual knowledge storage?
- Basis in paper: The paper states that "MHSA works as a knowledge extractor and stores certain general knowledge extraction patterns" and "indicating its storage of a small amount of factual knowledge" in the abstract, but this finding requires further exploration.
- Why unresolved: While the paper provides evidence that MHSA stores some factual knowledge, the exact amount and characteristics of this storage are not fully characterized.
- What evidence would resolve it: Systematic experiments varying the types and amounts of factual knowledge to determine MHSA's capacity limits and storage mechanisms would clarify this.

### Open Question 2
- Question: What is the optimal strategy for spreading residuals across critical layers - even spread versus square root spread - and how does this choice impact different performance metrics?
- Basis in paper: The ablation experiments show that "square root spreads in PMET enhances reliability but leads to larger changes in the model, ultimately affecting specificity" compared to even spreading.
- Why unresolved: While the paper demonstrates that square root spread generally performs better, the optimal spread strategy may depend on specific editing scenarios and model architectures.
- What evidence would resolve it: Comprehensive experiments comparing various spread strategies across different datasets, model sizes, and editing scenarios would determine optimal strategies for different contexts.

### Open Question 3
- Question: How does the balance between reliability and specificity change with the number of edited knowledge instances, and what is the theoretical limit of this trade-off?
- Basis in paper: The ablation experiments show that "PMET strikes a good balance between reliability and specificity, and this balance becomes more pronounced as the number of edited knowledge increases."
- Why unresolved: The paper demonstrates the trade-off exists and changes with edit count, but the theoretical underpinnings and limits of this relationship remain unexplored.
- What evidence would resolve it: Mathematical modeling of the reliability-specificity trade-off combined with extensive empirical testing across varying edit counts and model architectures would establish theoretical limits and practical guidelines.

## Limitations
- The computational overhead of optimizing both MHSA and FFN hidden states simultaneously is not thoroughly analyzed
- The paper's reliance on specific transformer architectures (GPT-J and GPT-NeoX) raises questions about generalizability to other model types
- The exact mechanism of how MHSA stores only general extraction patterns while avoiding factual knowledge storage remains theoretical

## Confidence
**High Confidence Claims:**
- The general architecture of PMET and its basic approach of simultaneous hidden state optimization
- The superiority of PMET over baseline methods on the COUNTERFACT and zsRE datasets
- The importance of precision in model editing tasks

**Medium Confidence Claims:**
- The specific mechanism of square root residual spreading improving reliability
- The claim that MHSA stores only general extraction patterns without factual knowledge
- The effectiveness of dual optimization compared to single-component approaches

**Low Confidence Claims:**
- The exact mathematical formulation of the function space expansion
- The universal applicability of the covariance matrix computation method
- The long-term stability of edited models under continued use

## Next Checks
1. **Ablation Study on MHSA Optimization**: Remove the δai parameter (MHSA hidden state optimization) from PMET and compare performance on COUNTERFACT dataset to quantify the contribution of dual optimization.

2. **Residual Spreading Comparison**: Implement and test even spreading (Rl = V1 - W0K1) alongside square root spreading on the zsRE dataset to measure impact on both precision and specificity metrics.

3. **MHSA Weight Update Impact**: Conduct controlled experiments where MHSA weights are updated alongside FFN weights, measuring the trade-off between precision gains and specificity preservation to validate the core claim about MHSA's role in general pattern extraction.