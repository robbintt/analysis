---
ver: rpa2
title: 'Large Language Models on Lexical Semantic Change Detection: An Evaluation'
arxiv_id: '2312.06002'
source_url: https://arxiv.org/abs/2312.06002
tags:
- word
- target
- change
- bert
- meaning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) on lexical semantic
  change detection, a task that identifies how word meanings shift over time. While
  traditional methods like PPMI and SGNS have been used for this task, the authors
  explore whether LLMs can provide better results.
---

# Large Language Models on Lexical Semantic Change Detection: An Evaluation

## Quick Facts
- arXiv ID: 2312.06002
- Source URL: https://arxiv.org/abs/2312.06002
- Reference count: 28
- Primary result: GPT-4 outperforms traditional methods and BERT on both corpus-level and instance-level lexical semantic change detection tasks

## Executive Summary
This paper evaluates large language models (LLMs) on lexical semantic change detection, a task that identifies how word meanings shift over time. While traditional methods like PPMI and SGNS have been used for this task, the authors explore whether LLMs can provide better results. They evaluate three types of methods: traditional count-based models (PPMI and SGNS), contextualized word representations (BERT), and LLM-based methods (GPT-4). The evaluation uses the TempoWiC dataset, which contains tweets annotated for semantic change. For corpus-level detection, GPT-4 achieves the highest Pearson correlation (-0.66, p=2.12e-5) with human annotations, outperforming BERT (0.63, p=0.00675) and traditional methods. For instance-level detection, GPT-4 also outperforms BERT, with F1 scores of 0.63 vs. 0.62 and accuracy of 0.72 vs. 0.64. The authors highlight that GPT-4's strong performance, even without fine-tuning, demonstrates the potential of LLMs for semantic change detection tasks.

## Method Summary
The study evaluates three generations of language models on lexical semantic change detection using the TempoWiC dataset of annotated tweets. Traditional count-based methods (PPMI and SGNS) build co-occurrence matrices and use Procrustes alignment for vector space comparison. BERT-based methods generate contextualized embeddings that are clustered and averaged to form sense-specific representations. LLM-based methods use zero-shot prompting with GPT-4 to directly compare word meanings across time periods. The evaluation measures Pearson correlation for corpus-level detection and F1 score/accuracy for instance-level detection against human annotations.

## Key Results
- GPT-4 achieves highest Pearson correlation (-0.66, p=2.12e-5) for corpus-level detection
- GPT-4 outperforms BERT on instance-level detection with F1 score of 0.63 vs 0.62
- Traditional methods show near-zero correlation on the TempoWiC dataset
- GPT-4's strong performance occurs without any task-specific fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GPT-4 captures contextual meaning shifts through in-context reasoning without task-specific fine-tuning.
- **Mechanism**: The model uses its pre-trained knowledge to compare semantic contexts across time periods by evaluating paired sentences, relying on its general language understanding rather than learned vector spaces.
- **Core assumption**: The pre-training corpus includes diverse enough temporal and cultural contexts for GPT-4 to recognize meaning shifts in modern tweet language.
- **Evidence anchors**:
  - [abstract] "Our work presents novel prompting solutions and a comprehensive evaluation that spans all three generations of language models"
  - [section] "we present a pair of sentences that share a target word and ask if the meaning of the target word between the two tweets is different"
  - [corpus] Weak: no explicit corpus-level evidence for temporal generalization capability
- **Break condition**: If the temporal gap between pre-training data and test tweets exceeds the model's effective context window, performance degrades.

### Mechanism 2
- **Claim**: BERT-based contextualized embeddings enable instance-level semantic change detection by clustering and averaging representations.
- **Mechanism**: BERT generates distinct embeddings for the same word in different contexts, which can be clustered and averaged to form sense-specific representations, enabling direct cosine distance comparison.
- **Core assumption**: WordPiece tokenization and averaging produce stable sense representations even when words are split into subword units.
- **Evidence anchors**:
  - [section] "BERT representations under different contexts are clustered together and averaged to form the final representation of a word or a sense"
  - [section] "BERT produces an individual embedding for each input word, which enables BERT-based methods to perform instance-level meaning change detection"
  - [corpus] Moderate: evidence from Hu et al. (2019) cited but not directly tested in this paper
- **Break condition**: If context similarity falls below clustering threshold, sense averaging produces noisy representations.

### Mechanism 3
- **Claim**: Traditional count-based methods fail on low-resource tweet data due to sparse co-occurrence statistics.
- **Mechanism**: PPMI and SGNS require sufficient corpus size to build reliable co-occurrence matrices; sparse data leads to unstable vector spaces and poor alignment.
- **Core assumption**: Semantic shift detection requires stable vector representations across time periods, which depend on sufficient token counts.
- **Evidence anchors**:
  - [section] "traditional models, particularly count-based ones, demand a large amount of training data to maintain performance"
  - [section] "PPMI matrices for the three years 2019, 2020, and 2021" - shows experimental setup but poor results
  - [corpus] Strong: Table 2 shows traditional methods achieve near-zero correlation on TempoWiC dataset
- **Break condition**: If dataset size increases significantly, traditional methods may regain competitive performance.

## Foundational Learning

- **Concept**: Corpus-level vs instance-level semantic change detection
  - Why needed here: Different evaluation tasks require different model capabilities; corpus-level detects overall trends while instance-level identifies specific meaning shifts
  - Quick check question: What evaluation metric would you use for corpus-level detection? (Answer: Pearson correlation with human annotations)

- **Concept**: Orthogonal Procrustes alignment for vector space comparison
  - Why needed here: Traditional methods require aligning vector spaces from different time periods to enable meaningful distance calculations
  - Quick check question: What mathematical operation does OP perform on matrices A and B? (Answer: Finds optimal orthogonal transformation minimizing squared Euclidean distance)

- **Concept**: Prompt engineering for zero-shot LLM inference
  - Why needed here: GPT-4's performance heavily depends on prompt structure, as shown by the significant F1 score difference between question-in-query and instruction formats
  - Quick check question: What format produced the highest F1 score in the study? (Answer: Instruction format without date information)

## Architecture Onboarding

- **Component map**: Tweet preprocessing -> Representation generation (PPMI/SGNS/BERT/GPT-4) -> Distance calculation -> Evaluation (correlation/F1/accuracy)
- **Critical path**: Tweet pair processing -> Target word extraction -> Model-specific representation -> Comparison operation -> Ground truth matching
- **Design tradeoffs**: Zero-shot prompting trades fine-tuning costs for prompt engineering complexity; traditional methods trade computational efficiency for data requirements
- **Failure signatures**: Traditional methods show near-zero correlation; BERT shows accuracy-cap plateau; GPT-4 shows prompt sensitivity
- **First 3 experiments**:
  1. Replicate PPMI/SGNS pipeline on a larger corpus to test data-size dependency
  2. Test BERT with different clustering thresholds to optimize instance-level detection
  3. Compare GPT-4 with different prompt formats on a subset of TempoWiC to quantify prompt impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of large language models contribute to their superior performance in lexical semantic change detection compared to traditional methods like PPMI and SGNS?
- Basis in paper: [explicit] The paper notes that GPT-4 outperforms traditional methods and BERT in both corpus-level and instance-level semantic change detection, but does not provide a detailed analysis of why this is the case.
- Why unresolved: The paper hypothesizes that GPT-4's large parameter size and extensive training data contribute to its effectiveness, but this is not thoroughly investigated or confirmed.
- What evidence would resolve it: Comparative studies isolating the effects of model size, training data diversity, and architectural differences on semantic change detection performance would clarify the specific contributions of LLMs.

### Open Question 2
- Question: How does the performance of large language models in lexical semantic change detection vary with different types of datasets, particularly those with different resource levels or time spans?
- Basis in paper: [inferred] The paper evaluates LLMs on the TempoWiC dataset, a low-resource annotated tweet dataset, but does not explore performance across datasets with varying characteristics.
- Why unresolved: The paper focuses on a single dataset, limiting insights into how LLM performance might change with different data characteristics.
- What evidence would resolve it: Testing LLMs on diverse datasets with varying sizes, time spans, and annotation qualities would reveal their robustness and adaptability to different lexical semantic change detection tasks.

### Open Question 3
- Question: What are the limitations of using large language models for lexical semantic change detection, particularly in terms of interpretability and computational efficiency?
- Basis in paper: [inferred] The paper highlights the strong performance of LLMs but does not discuss potential drawbacks such as interpretability issues or computational demands.
- Why unresolved: The focus on performance metrics leaves questions about practical implementation challenges unaddressed.
- What evidence would resolve it: Analyses comparing the computational costs, energy consumption, and interpretability of LLM-based methods versus traditional approaches would provide a more comprehensive understanding of their practical applicability.

## Limitations
- Single-dataset evaluation on TempoWiC tweets limits generalizability to other domains and time periods
- Prompt engineering details and hyperparameter choices not fully specified, making exact replication challenging
- Focuses on diachronic word meaning shifts, may not generalize to other semantic phenomena like polysemy evolution

## Confidence

- **High**: GPT-4's superior performance over traditional methods and BERT on both corpus-level and instance-level detection tasks
- **Medium**: The generalizability of prompt engineering techniques across different semantic change detection scenarios
- **Medium**: The specific mechanisms by which GPT-4 captures semantic shifts without fine-tuning

## Next Checks
1. Evaluate the same prompt engineering approach on a different semantic change dataset (e.g., SemEval or historically-annotated corpora) to test cross-domain robustness
2. Test the sensitivity of GPT-4's performance to temporal distance between pre-training data and test tweets by comparing performance on recent vs. older tweet datasets
3. Compare GPT-4's performance against fine-tuned BERT models on the same task to quantify the zero-shot vs. fine-tuned trade-off