---
ver: rpa2
title: Explicit Planning Helps Language Models in Logical Reasoning
arxiv_id: '2303.15714'
source_url: https://arxiv.org/abs/2303.15714
tags:
- system
- reasoning
- deduction
- selection
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a language model-based system for multi-step
  logical reasoning that incorporates explicit planning during inference. The key
  idea is to look ahead into future reasoning steps when making decisions, allowing
  the system to make more informed choices at each step.
---

# Explicit Planning Helps Language Models in Logical Reasoning

## Quick Facts
- **arXiv ID**: 2303.15714
- **Source URL**: https://arxiv.org/abs/2303.15714
- **Reference count**: 40
- **Primary result**: Explicit planning during inference significantly improves language model performance on multi-step logical reasoning tasks

## Executive Summary
This paper presents a language model-based system for multi-step logical reasoning that incorporates explicit planning during inference. The key innovation is looking ahead into future reasoning steps when making decisions, allowing the system to make more informed choices at each step. The method addresses the challenge of model exploitation where imperfect models can misguide planning. Experiments on the Entailment Bank dataset show that the proposed system significantly outperforms competing methods, with a small T5 model (1.5B parameters) performing competitively with GPT-3-davinci.

## Method Summary
The system uses beam search with three components: a selection model (T5) that chooses relevant premises, a deduction model (T5) that generates new statements, and a verification model (DeBERTa) that judges entailment. During inference, the system maintains a buffer of ongoing reasoning paths and uses explicit planning to roll out future steps for each candidate decision. A training strategy with contrastive learning refines the verification model to mitigate model exploitation by suppressing scores for non-provable goals while preserving scores for correct proofs.

## Key Results
- The proposed system significantly outperforms competing methods on the Entailment Bank dataset
- With a small T5 model (1.5B parameters), it performs competitively with GPT-3-davinci
- Using GPT-3.5, it significantly outperforms chain-of-thought prompting on the PrOntoQA dataset
- The system achieves better AUROC and F1 scores compared to baselines across different training data sizes

## Why This Works (Mechanism)

### Mechanism 1: Explicit Planning for Informed Decisions
- **Claim**: Looking ahead into future effects provides information that helps distinguish between seemingly similar reasoning choices
- **Mechanism**: The system modifies selection/deduction scores by adding α∆u or β∆v (where ∆ is the max log-probability of proving the goal in future steps)
- **Core assumption**: Future roll-outs provide valuable information for current decision-making
- **Evidence anchors**: [abstract] "Explicit planning enables the system to make more informed reasoning decisions at each step by looking ahead into their future effects."
- **Break condition**: If the verification model is too inaccurate, roll-outs provide misleading information

### Mechanism 2: Contrastive Learning for Verification Model Refinement
- **Claim**: Contrastive learning teaches the verification model to distinguish between true and false proofs
- **Mechanism**: The system synthesizes non-provable hypotheses and tunes pver to suppress scores for reasoning paths that cannot prove these goals
- **Core assumption**: Model exploitation occurs when verification models are "fooled" by spurious features
- **Evidence anchors**: [abstract] "We propose a training strategy that safeguards the planning process from being led astray by spurious features."
- **Break condition**: If synthesized hypotheses are too similar to provable ones, contrastive learning provides insufficient signal

### Mechanism 3: Beam Search for Multiple Reasoning Paths
- **Claim**: Multiple reasoning paths may exist, and some may be better than others
- **Mechanism**: The system maintains a buffer of up to Binf ongoing reasoning paths, expanding each by finding top Bsel selections and Bded deductions
- **Core assumption**: Good reasoning paths may not appear promising at early steps
- **Evidence anchors**: [section] "Some may be better than the others (e.g., they are shorter) but they may not appear to be promising at the early steps"
- **Break condition**: If beam sizes are too small, good paths may be missed; if too large, computational cost becomes prohibitive

## Foundational Learning

- **Concept**: Beam search
  - **Why needed here**: The system needs to explore multiple reasoning paths rather than committing to one choice at each step
  - **Quick check question**: If Binf=3 and at step 2 we have paths with log-probabilities -1.0, -1.5, -2.0, and we expand to get new paths with log-probabilities -1.2, -1.3, -1.7, -1.8, -2.1, which paths remain in the buffer?

- **Concept**: Contrastive learning
  - **Why needed here**: The system needs to learn to distinguish between true and false proofs when used with planning
  - **Quick check question**: In the contrastive loss log(pver(̄x0|̄y)/(pver(̄x0|̄y)+pver(x0|y))), what happens to the loss if pver(̄x0|̄y) increases while pver(x0|y) stays constant?

- **Concept**: Prompt tuning
  - **Why needed here**: The system uses pretrained LMs as components but needs to adapt them to the logical reasoning task without fine-tuning all parameters
  - **Quick check question**: If we have a T5 model and add special tokens ENC, SP0, SP1, DEC to the input, which parameters are trained during prompt tuning?

## Architecture Onboarding

- **Component map**: Selection model (T5) -> Deduction model (T5) -> Verification model (DeBERTa) -> Planning module -> Buffer
- **Critical path**: During inference, for each path in buffer: SELECT → PLAN (if D>0) → choose top Bsel selections → for each selection: DEDUCE → PLAN (if D>0) → choose top Bded deductions → update buffer → repeat until max steps or goal proved
- **Design tradeoffs**: Beam size vs. computation, planning depth vs. accuracy, regularization strength vs. model exploitation
- **Failure signatures**: High false positive rate (model exploitation), low true positive rate (planning ineffective), slow inference (beam sizes/planning depth too large), poor performance on distractor theories
- **First 3 experiments**: 1) Compare D=0 vs D=3 on development set to verify planning improves performance, 2) Test contrastive learning with/without regularization to measure false positive rate, 3) Evaluate beam search with Binf=1, 3, 5 to find optimal tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How would the performance of the proposed system change if it were trained on the Version-II dataset that includes distractors in the theory?
- **Basis in paper**: The paper mentions that the systems were trained on Version-I but evaluated on Version-II, showing worse performance on Version-II due to distractors
- **Why unresolved**: The authors did not actually train and test their system on Version-II data
- **What evidence would resolve it**: Training the full system on Version-II training data and evaluating on Version-II test data

### Open Question 2
- **Question**: What is the impact of varying the depth of planning (D) parameter on the system's performance and computational efficiency?
- **Basis in paper**: The paper mentions that the planning depth D is a hyperparameter that can be varied
- **Why unresolved**: The paper does not provide an analysis of how different values of D affect performance or efficiency
- **What evidence would resolve it**: Conducting experiments with different values of D and measuring resulting performance and computational time

### Open Question 3
- **Question**: How does the proposed system compare to formal logic systems when given high-quality translations of natural language statements into formal logic?
- **Basis in paper**: The paper mentions that they attempted to use a first-order logic (FOL) system but failed due to poor quality translations
- **Why unresolved**: The authors did not provide a direct comparison between their system and formal logic systems when given good translations
- **What evidence would resolve it**: Creating high-quality translations of the Entailment Bank dataset into formal logic and comparing performance

## Limitations

- The method's reliance on accurate verification models is a critical limitation that could lead to model exploitation
- The optimal configuration of beam sizes and planning depth is not definitively established
- The computational cost scales with beam sizes and planning depth, creating efficiency trade-offs
- The method's effectiveness on more complex reasoning tasks beyond the Entailment Bank dataset remains unproven

## Confidence

**High Confidence**: The core mechanism of explicit planning improving reasoning decisions by looking ahead into future steps is well-supported by experimental results and ablation studies.

**Medium Confidence**: The contrastive learning approach for mitigating model exploitation shows promise but requires careful tuning and needs further validation across different reasoning domains.

**Low Confidence**: The optimal configuration of beam sizes (Binf, Bsel, Bded) and planning depth (D) is not definitively established, as the paper uses specific values without systematic exploration.

## Next Checks

1. **Planning Depth Sensitivity Analysis**: Systematically vary the planning depth D from 0 to 5 for both selection and deduction steps on the Entailment Bank development set to quantify marginal benefits and identify optimal depth.

2. **Cross-Dataset Generalization**: Evaluate the trained system on the PrOntoQA dataset using chain-of-thought prompting as a baseline, then apply explicit planning to quantify performance gains on a different logical reasoning task.

3. **Model Exploitation Stress Test**: Create a synthetic dataset with increasingly subtle distractors and non-provable goals to systematically measure the system's robustness to model exploitation, comparing the contrastive learning approach against baseline planning.