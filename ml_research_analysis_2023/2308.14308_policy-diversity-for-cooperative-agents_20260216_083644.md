---
ver: rpa2
title: Policy Diversity for Cooperative Agents
arxiv_id: '2308.14308'
source_url: https://arxiv.org/abs/2308.14308
tags:
- different
- policy
- policies
- known
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating diverse cooperative
  policies for multi-agent reinforcement learning, which is crucial for game designers
  to create interesting and varied gameplay experiences. The authors propose a method
  called Moment-Matching Policy Diversity (MMPD) that generates different team policies
  by formalizing the difference between team policies as the difference in actions
  of selected agents in different policies.
---

# Policy Diversity for Cooperative Agents

## Quick Facts
- arXiv ID: 2308.14308
- Source URL: https://arxiv.org/abs/2308.14308
- Reference count: 18
- The paper proposes a method called Moment-Matching Policy Diversity (MMPD) that generates different team policies by formalizing the difference between team policies as the difference in actions of selected agents in different policies.

## Executive Summary
This paper addresses the challenge of generating diverse cooperative policies for multi-agent reinforcement learning, which is crucial for game designers to create interesting and varied gameplay experiences. The authors propose a method called Moment-Matching Policy Diversity (MMPD) that generates different team policies by formalizing the difference between team policies as the difference in actions of selected agents in different policies. The approach is theoretically motivated by showing that it implements a constrained optimization problem using Maximum Mean Discrepancy (MMD) to regularize the difference between trajectory distributions. The effectiveness of MMPD is demonstrated on a challenging team-based shooter, where it successfully generates significantly different team policies compared to existing methods.

## Method Summary
The paper proposes Moment-Matching Policy Diversity (MMPD), a method for generating diverse cooperative policies in multi-agent reinforcement learning. MMPD formalizes the difference between team policies as the difference in actions of selected agents in different policies. The approach uses Maximum Mean Discrepancy (MMD) to regularize the difference between trajectory distributions, implementing a constrained optimization problem. The method is built on Soft Actor-Critic (SAC) and selects agent subsets and known policy sets to generate new policies that differ in actions, ensuring trajectory diversity. The paper demonstrates the effectiveness of MMPD on a mini team-based shooter environment with 3 agents.

## Key Results
- MMPD successfully generates significantly different team policies compared to existing methods in a team-based shooter environment.
- The method can create diverse policies by using different agent sets and different known policy sets, leading to more varied trajectories and gameplay experiences.
- Fr´echet Distance is used to measure the diversity between trajectories of agents under different policies, validating the effectiveness of MMPD.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Policy diversity is achieved by constraining action choices of selected agents to differ from known policies.
- Mechanism: The algorithm penalizes trajectories where selected agents' actions match those of known policies, pushing the learned policy toward divergent behavior.
- Core assumption: The difference in actions of selected agents is a sufficient proxy for policy diversity in multi-agent settings.
- Evidence anchors:
  - [abstract] "This method can generate different team policies to varying degrees by formalizing the difference between team policies as the difference in actions of selected agents in different policies."
  - [section] "We argue that the difference in the actions of the same agent in the same scenario capture well the kinds of different policies we are seeking."
  - [corpus] No direct support found; this is an assumption based on the paper's reasoning.
- Break condition: If the selected agents' action differences do not translate into meaningful team-level strategic diversity, the method fails.

### Mechanism 2
- Claim: The approach implements a constrained optimization problem using Maximum Mean Discrepancy (MMD) to regularize trajectory distributions.
- Mechanism: By maximizing the MMD between trajectory distributions of the new policy and known policies, the method ensures statistically significant differences in agent behaviors over time.
- Core assumption: MMD provides a meaningful measure of distributional difference for multi-agent trajectories.
- Evidence anchors:
  - [abstract] "Theoretically, we show that our method is a simple way to implement a constrained optimization problem that regularizes the difference between two trajectory distributions by using the maximum mean discrepancy."
  - [section] "We first let a set of N state samples from different moment t be defined as {st}N t=1. Then the moment-matching state-action samples of a total of K agents from policy p and q can be found separately..."
  - [corpus] No direct support found; the connection to MMD is theoretical and not validated by external sources.
- Break condition: If the MMD constraint is too loose or too tight, it may either fail to enforce diversity or prevent learning useful policies.

### Mechanism 3
- Claim: Iterative policy generation with different agent sets and known policy sets produces diverse strategies.
- Mechanism: By rotating which agents are constrained and which policies serve as references, the algorithm explores a broader policy space than single-agent diversification methods.
- Core assumption: Changing the set of constrained agents and reference policies leads to exploration of orthogonal policy dimensions.
- Evidence anchors:
  - [abstract] "The primary results show that MMPD can create diverse policies by using different agent sets and different known policy sets, leading to more varied trajectories and gameplay experiences."
  - [section] "We can also generate a different joint-policy by placing more known policies in the known policy set Πset so that the actions of agent in L will be different from the actions of each policy in Πset..."
  - [corpus] No direct support found; this is based on the paper's experimental claims.
- Break condition: If the policy space is low-dimensional or the agent interactions are highly coupled, changing agent sets may not yield significantly different strategies.

## Foundational Learning

- Concept: Maximum Mean Discrepancy (MMD)
  - Why needed here: MMD provides a principled way to measure distributional differences between trajectory samples, enabling a theoretically grounded diversity constraint.
  - Quick check question: What is the mathematical definition of MMD between two distributions p and q using a kernel k?
  - Answer: MMD(p, q, H) = (E[k(x, x′)] − 2E[k(x, y)] + E[k(y, y′)])^(1/2)

- Concept: Multi-Agent Reinforcement Learning (MARL) and centralized training with decentralized execution (CTDE)
  - Why needed here: The method operates in a CTDE framework where a centralized critic can access all agents' actions but each agent selects actions based on local observations.
  - Quick check question: In CTDE, what information is available during centralized training but not during decentralized execution?
  - Answer: During training, the global state and all agents' actions are available; during execution, each agent only has access to its local observation.

- Concept: Policy-based reinforcement learning and actor-critic methods
  - Why needed here: The method builds on SAC (Soft Actor-Critic) and modifies the policy update to include diversity constraints.
  - Quick check question: In SAC, what is the role of the entropy term in the policy objective?
  - Answer: The entropy term encourages exploration by penalizing deterministic policies, balancing exploitation and exploration.

## Architecture Onboarding

- Component map: Transition buffer D -> Policy network θ -> Agent selection module -> Known policy set Πset -> Penalty reward r

- Critical path:
  1. Collect trajectories from known policies into buffer D.
  2. Select L agents and known policies (Πset).
  3. Generate actions from new policy θ and compare to known policies.
  4. Apply penalty if actions match, otherwise proceed normally.
  5. Update policy θ using SAC on penalized/rewarded transitions.
  6. Iterate with different agent sets and policy sets.

- Design tradeoffs:
  - Agent selection granularity vs. computational cost: Larger L increases diversity but slows training.
  - Penalty magnitude vs. learning stability: Too high a penalty can prevent learning; too low may not enforce diversity.
  - Number of reference policies vs. diversity: More reference policies increase diversity potential but also computational overhead.

- Failure signatures:
  - Policy collapses to known strategies: Penalty is too low or agent selection is ineffective.
  - Learning fails to converge: Penalty is too high, causing the policy to get stuck in local minima.
  - Generated policies are not meaningfully different: MMD constraint is not properly implemented or the policy space is too constrained.

- First 3 experiments:
  1. Train two policies using different agent sets (L1 = {agent1}, L2 = {agent1, agent2}) and compare trajectory diversity using Fréchet Distance.
  2. Train policies using different known policy sets (Πset1 = {π1}, Πset2 = {π1, π2}) and measure the impact on diversity.
  3. Vary the penalty reward r and observe its effect on both policy performance and diversity metrics.

## Open Questions the Paper Calls Out
None

## Limitations
- The method's reliance on agent-level action differences as a proxy for team-level policy diversity is an assumption without external validation.
- Critical hyperparameters for SAC and MMD are unspecified, making exact reproduction challenging and potentially affecting results.
- The experimental setup uses a simplified team-based shooter with only 3 agents total, which may not scale to more complex multi-agent environments.

## Confidence
- **High confidence** in the theoretical framework connecting MMD regularization to trajectory diversity constraints.
- **Medium confidence** in the empirical effectiveness of MMPD for generating diverse policies.
- **Low confidence** in the generalizability of agent-level action differences as a sufficient proxy for team-level strategic diversity.

## Next Checks
1. Test MMPD in environments with larger agent teams (e.g., 5+ agents) to evaluate scalability and whether action-level diversity constraints still produce meaningful team-level strategic differences.
2. Compare MMPD's diversity metrics against alternative approaches like DIAYN or policy distillation methods to benchmark its effectiveness in generating truly distinct team behaviors.
3. Conduct ablation studies by varying the penalty reward r and agent selection strategy L to determine the sensitivity of diversity outcomes to these design choices and identify optimal configurations.