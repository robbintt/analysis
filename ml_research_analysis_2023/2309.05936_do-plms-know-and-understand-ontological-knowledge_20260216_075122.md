---
ver: rpa2
title: Do PLMs Know and Understand Ontological Knowledge?
arxiv_id: '2309.05936'
source_url: https://arxiv.org/abs/2309.05936
tags:
- soft
- knowledge
- ontological
- reasoning
- plms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Ontological knowledge, comprising classes, properties, and their
  relationships, is integral to world knowledge. While previous studies have explored
  factual and linguistic knowledge within PLMs, a systematic investigation into ontological
  knowledge is lacking.
---

# Do PLMs Know and Understand Ontological Knowledge?

## Quick Facts
- arXiv ID: 2309.05936
- Source URL: https://arxiv.org/abs/2309.05936
- Authors: 
- Reference count: 40
- Key outcome: Ontological knowledge, comprising classes, properties, and their relationships, is integral to world knowledge. While previous studies have explored factual and linguistic knowledge within PLMs, a systematic investigation into ontological knowledge is lacking. This paper probes whether PLMs can store and understand ontological knowledge, focusing on memorizing types of entities, hierarchical relationships among classes and properties, and domain and range constraints of properties. Additionally, the paper investigates whether PLMs can perform logical reasoning with given ontological knowledge according to entailment rules. The experimental results demonstrate that PLMs can memorize certain ontological knowledge and apply implicit knowledge in reasoning, but both memorizing and reasoning performances are less than perfect, indicating incomplete knowledge and understanding.

## Executive Summary
This paper systematically investigates whether pretrained language models (PLMs) can store and understand ontological knowledge, including entity types, hierarchical relationships among classes and properties, and domain/range constraints. Through prompt-based probing experiments, the authors evaluate PLMs' ability to memorize ontological facts and perform logical reasoning using RDFS entailment rules. The study reveals that while PLMs can memorize certain ontological knowledge, their reasoning capabilities are limited, particularly when premises are not explicitly provided. The findings highlight both the potential and limitations of PLMs in handling structured semantic knowledge.

## Method Summary
The paper constructs an ontology from DBpedia and Wikidata, generating five memorizing tasks (type, subclass, subproperty, domain, range) and six reasoning tasks based on RDFS entailment rules. Prompt-based probing is employed using both manual and soft (learnable) templates. Soft tokens are trained on the memorizing task training sets, and multiple [MASK] tokens with mean-pooling are used for candidate scoring. Performance is evaluated using Top-K Recall, Mean Reciprocal Rank, and MRRa metrics for memorizing tasks, and MRR for reasoning tasks.

## Key Results
- PLMs can memorize certain ontological knowledge (entity types, hierarchical relationships) with measurable accuracy above baseline
- PLMs demonstrate reasoning ability with explicitly provided premises but performance drops significantly with implicit premises
- Soft prompts generally improve performance on memorizing tasks but have limited impact on reasoning tasks
- Both memorizing and reasoning performances are less than perfect, indicating incomplete knowledge and understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretrained language models (PLMs) can store ontological knowledge during pretraining by capturing hierarchical relationships between classes and properties in the training corpus.
- Mechanism: During pretraining on large-scale corpora, PLMs learn to associate entity types, their hierarchical relationships, and property constraints through co-occurrence patterns and syntactic structures.
- Core assumption: The pretraining corpus contains sufficient instances and descriptions of ontological relationships that can be extracted by the model.
- Evidence anchors:
  - [abstract] "PLMs can memorize certain ontological knowledge"
  - [section 4.1] "PLMs have a certain memory of the concerned ontological relationships"
  - [corpus] Weak - no direct corpus analysis provided
- Break condition: If the pretraining corpus lacks sufficient ontological knowledge or the model cannot generalize from surface patterns to semantic understanding.

### Mechanism 2
- Claim: PLMs can perform logical reasoning with ontological knowledge by applying entailment rules when premises are explicitly or implicitly provided.
- Mechanism: When given premises (explicitly or implicitly), PLMs use their learned representations to apply ontological entailment rules and derive conclusions through masked language modeling.
- Core assumption: The model has learned the semantic meaning behind ontological relationships, not just surface patterns.
- Evidence anchors:
  - [abstract] "PLMs can apply implicit ontological knowledge in reasoning"
  - [section 4.2] "PLMs can utilize the implicit ontological knowledge and select the correct entailment rule to make inferences"
  - [corpus] Weak - corpus neighbors show related work on knowledge probing but no direct evidence
- Break condition: If PLMs rely solely on priming effects or fail to generalize reasoning beyond surface patterns.

### Mechanism 3
- Claim: Soft prompts improve ontological knowledge probing by providing trainable continuous representations that better capture semantic relationships.
- Mechanism: Learnable soft tokens replace or augment manual prompt templates, allowing the model to optimize prompt representations for specific ontological relationships during fine-tuning.
- Core assumption: Soft tokens can learn representations that better align with the semantic structure of ontological knowledge than discrete language prompts.
- Evidence anchors:
  - [section 3.1.1] "Soft prompts that consist of learnable soft tokens instead of manually defined templates"
  - [section 4.3] "using soft templates generally improves the performance of memorizing tasks"
  - [corpus] Weak - no corpus evidence directly supporting soft prompt effectiveness
- Break condition: If soft tokens overfit to training data or fail to generalize to unseen ontological relationships.

## Foundational Learning

- Concept: Ontological knowledge representation
  - Why needed here: Understanding classes, properties, and their relationships is essential for designing appropriate probing tasks and interpreting results.
  - Quick check question: Can you explain the difference between subclass and subproperty relationships in an ontology?

- Concept: Masked language modeling
  - Why needed here: The probing tasks rely on PLMs' ability to fill masked tokens, which is the fundamental mechanism being evaluated.
  - Quick check question: How does the choice of pooling method (mean, max, first) affect multi-token candidate scoring in masked language models?

- Concept: Prompt engineering and template design
  - Why needed here: Effective probing requires carefully designed prompts that can elicit the desired knowledge from PLMs.
  - Quick check question: What are the trade-offs between manual and soft prompt templates in terms of performance and generalizability?

## Architecture Onboarding

- Component map:
  Ontology construction pipeline (DBpedia, Wikidata integration) -> Memorizing task generator (cloze completion for TP, SCO, SPO, DM, RG) -> Reasoning task generator (RDFS entailment rules with premise combinations) -> PLM probing framework (BERT/RoBERTa variants with manual/soft prompts) -> Evaluation metrics (R@K, MRR, MRRa for multi-label classification)

- Critical path:
  1. Build ontology from DBpedia/Wikidata
  2. Generate memorizing and reasoning datasets
  3. Implement prompt-based probing with multiple template strategies
  4. Run experiments and collect performance metrics
  5. Analyze results to assess knowledge storage and reasoning capabilities

- Design tradeoffs:
  - Manual vs. soft prompts: Manual prompts are interpretable but may not capture semantic nuances; soft prompts are more flexible but less interpretable
  - Explicit vs. implicit premise provision: Explicit premises may cause priming effects; implicit premises test true understanding
  - Multiple vs. single mask tokens: Multiple masks allow better multi-token prediction but increase computational complexity

- Failure signatures:
  - Poor performance on memorizing tasks indicates insufficient knowledge storage
  - Low reasoning accuracy with implicit premises suggests limited semantic understanding
  - Soft prompts failing to improve performance may indicate insufficient training data or poor prompt design

- First 3 experiments:
  1. Test BERT-base-cased with manual prompts on the memorizing task to establish baseline performance
  2. Compare manual vs. soft prompts on a subset of the memorizing tasks to evaluate prompt effectiveness
  3. Test reasoning performance with explicitly given premises to verify the probing framework works before testing implicit premises

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ontological knowledge differ between decoder-only models like ChatGPT and encoder-based PLMs?
- Basis in paper: [explicit] The paper compares ChatGPT with BERT/RoBERTa, showing superior performance in both memorizing and reasoning tasks. However, the comparison is limited to a subset of ontological knowledge and a single decoder-only model.
- Why unresolved: The study uses a specific dataset and only one decoder-only model, limiting generalizability. The underlying mechanisms for knowledge storage and reasoning in decoder-only models remain unclear.
- What evidence would resolve it: Systematic comparison of ontological knowledge across multiple decoder-only and encoder-based models using diverse datasets, with analysis of their internal representations.

### Open Question 2
- Question: What are the optimal prompting strategies for probing ontological knowledge in PLMs?
- Basis in paper: [inferred] The paper explores various prompt templates and conjunction methods, finding soft templates generally improve memorization tasks but have limited impact on reasoning. The effectiveness of different approaches varies by task type.
- Why unresolved: The study tests a limited set of prompt variations and does not explore the full design space of prompt engineering techniques. The interaction between prompt design and model architecture is not fully characterized.
- What evidence would resolve it: Comprehensive ablation studies testing diverse prompt templates, conjunctions, and fine-tuning strategies across multiple PLM architectures and ontological knowledge types.

### Open Question 3
- Question: Can PLMs be pretrained to better encode and reason with ontological knowledge?
- Basis in paper: [explicit] The paper finds PLMs have incomplete knowledge and understanding of ontology, particularly struggling with paraphrased knowledge. This suggests room for improvement in pretraining methods.
- Why unresolved: The study identifies limitations but does not propose or test specific pretraining modifications. The relationship between pretraining objectives, data, and ontological knowledge acquisition is not established.
- What evidence would resolve it: Controlled experiments comparing standard pretraining with ontology-aware objectives, incorporating structured knowledge during pretraining, and measuring downstream reasoning performance.

### Open Question 4
- Question: How does the depth and breadth of ontological knowledge in PLMs scale with model size?
- Basis in paper: [explicit] The paper notes that larger models do not necessarily perform better at memorizing ontological knowledge, contradicting findings for other knowledge types. This suggests unique scaling properties for ontological knowledge.
- Why unresolved: The study uses a limited set of model sizes and does not systematically investigate scaling laws for ontological knowledge specifically. The relationship between parameter count and ontological reasoning ability is unclear.
- What evidence would resolve it: Extensive experiments across a wide range of model sizes, measuring ontological knowledge acquisition and reasoning performance, with analysis of scaling exponents and plateaus.

## Limitations

- Ontology construction validity: The specific methodology for aligning equivalent properties and cleansing domain/range constraints is not fully detailed, introducing uncertainty about the accuracy of the constructed ontology.
- Corpus analysis gap: No direct corpus analysis is provided to verify that ontological relationships are sufficiently represented in the pretraining data, leaving the core assumption about knowledge extraction unverified.
- Soft prompt generalization: The evaluation does not assess whether soft prompts generalize beyond training data or simply overfit to specific prompt formats, limiting conclusions about their effectiveness.

## Confidence

- Ontological Knowledge Storage: High - Experimental results consistently show measurable accuracy above baseline for memorizing tasks.
- Logical Reasoning Capability: Medium - PLMs demonstrate reasoning with explicit premises but performance drops significantly with implicit premises, suggesting partial understanding.
- Soft Prompt Effectiveness: Medium - Soft prompts improve memorizing tasks but without comparison to alternative approaches, it's unclear if improvement is specifically due to soft tokens.

## Next Checks

- Check 1: Perform corpus analysis to verify the presence and distribution of ontological knowledge patterns that PLMs would need to learn, validating the assumption that pretraining data contains sufficient ontological information.
- Check 2: Test PLM performance on ontological knowledge from different domains or knowledge bases not present in the pretraining corpus to validate whether PLMs have learned general ontological reasoning capabilities.
- Check 3: Conduct an ablation study varying the level of premise explicitness (explicit, implicit, absent) across different reasoning tasks to quantify the priming effect and determine whether PLMs are truly reasoning or simply retrieving memorized associations.