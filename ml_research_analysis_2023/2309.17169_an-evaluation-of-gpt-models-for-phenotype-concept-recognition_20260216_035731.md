---
ver: rpa2
title: An evaluation of GPT models for phenotype concept recognition
arxiv_id: '2309.17169'
source_url: https://arxiv.org/abs/2309.17169
tags:
- prompt
- common
- phenotype
- abnormality
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated GPT-3.5 and GPT-4.0 models for phenotype concept
  recognition using the Human Phenotype Ontology (HPO). Seven prompts with varying
  specificity and two models were tested against a gold standard corpus of 228 abstracts.
---

# An evaluation of GPT models for phenotype concept recognition

## Quick Facts
- arXiv ID: 2309.17169
- Source URL: https://arxiv.org/abs/2309.17169
- Reference count: 0
- GPT models achieved lower performance than state-of-the-art tools, with best F1 scores of 0.41 (GPT-3.5) and 0.33 (GPT-4.0) compared to 0.62 for current best tools.

## Executive Summary
This study evaluates GPT-3.5 and GPT-4.0 models for phenotype concept recognition using the Human Phenotype Ontology (HPO). Seven prompts with varying specificity were tested against a gold standard corpus of 228 abstracts. Results showed that GPT models significantly underperform compared to state-of-the-art specialized tools, achieving F1 scores of 0.41 and 0.33 versus 0.62 for current best approaches. Key challenges include non-deterministic outputs, low concordance between runs, and poor concept mapping accuracy, despite minimal hallucination rates.

## Method Summary
The study tested GPT-3.5 and GPT-4.0 models using seven different prompt formulations on a gold standard corpus of 228 PubMed abstracts containing 2,773 HPO term mentions. Prompts varied from basic instructional prompts to few-shot learning approaches. Model outputs were evaluated for concept recognition accuracy by comparing extracted HPO IDs against the gold standard using precision, recall, and F1 scores at both macro and micro levels. The evaluation focused on concept mapping accuracy rather than just text span recognition.

## Key Results
- GPT models achieved F1 scores of 0.41 (GPT-3.5) and 0.33 (GPT-4.0), significantly below the 0.62 F1 score of state-of-the-art specialized tools
- Non-deterministic outputs created low concordance between runs, with only 75.82% common correct HPO IDs across 5 runs of the same model and prompt
- Hallucinations were minimal, but systematic errors in HPO ID assignment were observed across different runs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT models struggle with phenotype concept recognition because they lack domain-specific training on biomedical ontology structures.
- Mechanism: Without fine-tuning on HPO or similar biomedical ontologies, GPT models rely on general language patterns rather than ontology-specific semantic relationships.
- Core assumption: General language understanding is insufficient for precise ontology-based concept mapping.
- Evidence anchors:
  - [abstract] "Both models have a consistent behaviour across prompts. Prompts 1 and 2 - defining the task as an extraction of phenotypes and clinical abnormalities, followed by an alignment to HPO IDs - achieve the best precision..."
  - [section] "As a task, concept recognition can also be modelled as named entity recognition (used to detect entity boundaries in the text) followed by entity alignment (used to match the candidates extracted from the text to ontology concepts / IDs)."
  - [corpus] Weak - corpus provides neighbor papers but no direct evidence on GPT model performance limitations.
- Break condition: Performance improves significantly with domain-specific fine-tuning on biomedical ontologies.

### Mechanism 2
- Claim: Non-deterministic outputs from GPT models create unreliable concept recognition results.
- Mechanism: GPT models generate different outputs on identical inputs due to inherent stochasticity in their sampling process.
- Core assumption: Stochastic generation is problematic for tasks requiring reproducible, precise concept mapping.
- Evidence anchors:
  - [abstract] "The non-deterministic nature of the outcomes and the lack of concordance between different runs using the same prompt and input makes the use of these LLMs in clinical settings problematic."
  - [section] "A final experiment was performed to understand the concordance across different runs of the same model and prompt. We ran five times the annotation experiment using gpt-4.0 Prompt 1. Overall, all runs achieved the same precision and recall, with very minor differences (+/- 0.01). The concordance in the results produced by the runs was, however, surprisingly low."
  - [corpus] Weak - corpus does not directly address GPT model stochasticity issues.
- Break condition: Deterministic generation methods or post-processing validation are implemented to ensure consistent outputs.

### Mechanism 3
- Claim: GPT models exhibit high concept mapping error rates despite minimal hallucination, indicating systematic recognition failures.
- Mechanism: Models frequently assign incorrect HPO IDs to recognized terms, suggesting they understand the concepts but cannot map them accurately to the ontology.
- Core assumption: Recognition accuracy is separable from mapping accuracy, and mapping is the harder problem.
- Evidence anchors:
  - [abstract] "Hallucinations were minimal, but consistent errors in HPO ID assignment were observed."
  - [section] "It is interesting to note the nature of failures in concept mapping. For example, gpt-3.5 tags the text 'Angelman's syndrome' with Decreased body weight (HP:0004325), and 'Prader-Willi syndrome', 'bilateral acoustic neuromas' or 'Neurofibromatosis type 2' with Intellectual disability, profound (HP:0002187), while gpt-4.0 tags, consistently, the same text spans..."
  - [corpus] Weak - corpus provides context but no direct evidence on concept mapping error patterns.
- Break condition: Concept mapping accuracy approaches state-of-the-art levels through improved prompting or model adaptation.

## Foundational Learning

- Concept: Human Phenotype Ontology (HPO) structure and terminology
  - Why needed here: Understanding HPO is essential for evaluating phenotype concept recognition performance and interpreting results
  - Quick check question: What are the main categories in HPO and how are they organized hierarchically?

- Concept: Natural Language Processing evaluation metrics
  - Why needed here: Precision, recall, and F1 scores are used to quantify model performance and compare against baselines
  - Quick check question: How do macro and micro F1 scores differ in their calculation and interpretation?

- Concept: Large Language Model prompting strategies
  - Why needed here: Different prompt formulations significantly impact GPT model outputs for concept recognition tasks
  - Quick check question: What is the difference between zero-shot, few-shot, and instruction-based prompting?

## Architecture Onboarding

- Component map: Clinical text abstracts -> GPT API calls with prompts -> JSON outputs with HPO IDs -> Evaluation against gold standard
- Critical path: Load gold standard corpus and HPO concepts -> Generate prompts for each model variant -> Call GPT APIs and collect outputs -> Extract HPO IDs and compare against gold standard -> Calculate precision, recall, and F1 scores
- Design tradeoffs:
  - General vs. domain-specific models: GPT models lack HPO training but offer flexibility
  - Deterministic vs. stochastic generation: GPT provides varied outputs but reduces reproducibility
  - Prompt complexity vs. performance: More specific prompts improve results but require careful engineering
- Failure signatures:
  - Low concordance between runs indicates non-deterministic behavior
  - Systematic assignment of incorrect HPO IDs suggests mapping issues
  - Poor performance compared to specialized tools indicates fundamental limitations
- First 3 experiments:
  1. Test basic prompt engineering variations to establish baseline performance
  2. Implement few-shot learning with HPO examples to assess knowledge transfer
  3. Compare macro vs. micro F1 scores to identify systematic recognition vs. mapping issues

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can GPT models be effectively fine-tuned or adapted for reliable phenotype concept recognition while maintaining their few-shot learning capabilities?
- Basis in paper: [inferred] The study showed that off-the-shelf GPT models performed significantly worse than specialized tools (F1 0.41-0.33 vs 0.62), and the authors suggested that few-shot learning was limited by the inability to include the full ontology.
- Why unresolved: The paper only tested out-of-the-box GPT models without any fine-tuning or adaptation strategies. It's unclear whether domain-specific fine-tuning, retrieval-augmented generation, or hybrid approaches could bridge the performance gap.
- What evidence would resolve it: Head-to-head comparison studies of fine-tuned GPT models against both current best-in-class tools and out-of-the-box GPT models using the same evaluation framework and datasets.

### Open Question 2
- Question: What specific prompt engineering techniques or architectures could reduce the non-deterministic outputs and improve concordance between different runs of GPT models for phenotype concept recognition?
- Basis in paper: [explicit] The study found significant divergence in concept mapping errors across multiple runs of the same model and prompt (only 75.82% common correct HPO IDs across 5 runs).
- Why unresolved: While the paper identified the non-deterministic nature as a limitation, it didn't explore prompt engineering strategies, temperature settings, or other techniques to improve consistency.
- What evidence would resolve it: Systematic experiments testing different prompt structures, few-shot examples, temperature settings, and model parameters to identify configurations that minimize output variability while maintaining performance.

### Open Question 3
- Question: Could combining GPT models with traditional NLP pipelines (e.g., NER + ontology matching) improve phenotype concept recognition accuracy compared to either approach alone?
- Basis in paper: [explicit] The study tested GPT models in isolation and found they performed worse than specialized tools like PhenoTagger (F1 0.41 vs 0.62), but didn't explore hybrid approaches.
- Why unresolved: The paper evaluated GPT models as standalone solutions but didn't investigate whether combining them with established NLP techniques could leverage the strengths of both approaches.
- What evidence would resolve it: Comparative studies testing hybrid pipelines (e.g., GPT for NER followed by traditional ontology matching, or vice versa) against both pure GPT and pure traditional approaches using standardized evaluation metrics.

## Limitations
- Evaluation limited to 228 abstracts focused on rare diseases, potentially missing broader clinical text diversity
- Non-deterministic GPT outputs create significant reproducibility challenges across runs
- Study only tested out-of-the-box GPT models without exploring fine-tuning or adaptation strategies

## Confidence
*High Confidence Claims:*
- GPT models achieve lower performance than state-of-the-art specialized tools for phenotype concept recognition
- Concept mapping errors are systematic and consistent across different runs
- Hallucinations are minimal in GPT outputs for this task
- Non-deterministic behavior is a fundamental characteristic of current GPT implementations

*Medium Confidence Claims:*
- GPT models are not currently suitable for reliable phenotype concept recognition in clinical settings
- Prompt engineering has limited impact on improving overall concept recognition performance
- The gap between GPT models and specialized tools represents a fundamental architectural limitation

*Low Confidence Claims:*
- Future GPT versions will not overcome current limitations without architectural changes
- No prompt engineering approach could significantly improve GPT performance for this task
- The specific failure patterns observed will generalize to all clinical text domains

## Next Checks
1. **Reproducibility Validation**: Run the exact same experiments across multiple days and with different API endpoints to quantify the variability in non-deterministic outputs and establish confidence intervals for performance metrics.

2. **Domain Generalization Test**: Evaluate the same GPT models and prompts on a broader corpus of clinical text, including primary care notes, radiology reports, and discharge summaries, to assess whether performance degradation is consistent across text types.

3. **Fine-tuning Impact Assessment**: Implement a small-scale fine-tuning experiment using the HPO corpus to determine whether domain adaptation can meaningfully improve concept mapping accuracy and reduce systematic errors.