---
ver: rpa2
title: An Offline Metric for the Debiasedness of Click Models
arxiv_id: '2304.09560'
source_url: https://arxiv.org/abs/2304.09560
tags:
- click
- relevance
- policy
- cmip
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem that current evaluation metrics
  for click models do not ensure their robustness to shifts in the ranking policy,
  which can lead to poor performance in downstream tasks. The authors introduce the
  concept of debiasedness of a click model with respect to the logging policy and
  prove that it is a necessary condition for consistent and unbiased relevance estimation,
  as well as for invariance under policy shift.
---

# An Offline Metric for the Debiasedness of Click Models

## Quick Facts
- arXiv ID: 2304.09560
- Source URL: https://arxiv.org/abs/2304.09560
- Reference count: 40
- Key outcome: Introduces CMIP metric to quantify debiasedness of click models, showing it improves prediction of out-of-distribution performance and off-policy model selection

## Executive Summary
This paper addresses a critical gap in click model evaluation by introducing a new metric called CMIP (Conditional Mutual Information with the Logging Policy) that measures a model's debiasedness. Click models trained on biased click data often fail to generalize when ranking policies change, limiting their effectiveness in real-world applications. The authors prove that debiasedness is necessary for consistent relevance estimation and policy shift invariance, then demonstrate that CMIP can predict out-of-distribution performance and improve model selection strategies.

## Method Summary
The authors propose CMIP as a metric to quantify debiasedness by measuring conditional independence between a model's relevance estimates and the logging policy, given true relevance. They generate semi-synthetic click data using the MSLR-WEB10K dataset with various user models (PBM, DBN, MixtureDBN, Carousel) and ranking policies (Uniform, LambdaMART, NoisyOracle). Click models (PBM, DBN, UBM, NCM, CACM) are trained on this data, and CMIP is computed using a k-NN sampling approach combined with binary classification for KL divergence estimation. The metric is evaluated by its ability to predict out-of-distribution perplexity and improve regret in off-policy model selection.

## Key Results
- CMIP effectively predicts out-of-distribution perplexity when combined with existing metrics (ind PPL, nDCG)
- Model selection strategies incorporating CMIP achieve lower regret than those using only ind PPL and nDCG
- The combination "top-4 CMIP, top-4 PPL: nDCGâ†‘" achieves the lowest average regret
- CMIP provides complementary information to existing metrics, improving prediction consistency across different experimental configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CMIP detects lack of robustness to policy shift by measuring conditional independence between relevance estimates and logging policy beyond true relevance
- Mechanism: CMIP estimates conditional mutual information (CMI) between a new model's relevance scores and the logging policy's implicit relevance, conditioned on true relevance. Lower CMI indicates higher debiasedness. This works because if a model is debiased, knowing the logging policy's relevance doesn't help predict the model's relevance beyond knowing the true relevance.
- Core assumption: The estimated CMI accurately reflects the true conditional independence between relevance scores
- Evidence anchors:
  - [abstract] "we propose an evaluation metric based on conditional independence testing to detect a lack of robustness to covariate shift in click models"
  - [section 4.1] "Ëœð‘… â«« ð‘…ð‘™ | ð‘… â‡ â‡’ ð¼ ( Ëœð‘…; ð‘…ð‘™ | ð‘…) = 0"
- Break condition: If the CMI estimation method fails to accurately capture the conditional independence relationship, CMIP will not reliably detect debiasedness

### Mechanism 2
- Claim: CMIP improves prediction of out-of-distribution performance when combined with existing metrics
- Mechanism: CMIP provides information about a model's bias relative to the logging policy that is complementary to perplexity (ind PPL) and ranking quality (nDCG). By including CMIP in regression models predicting ood PPL, the prediction becomes more consistent across different experimental setups and covariate shifts.
- Core assumption: CMIP captures a distinct aspect of model quality that is not fully represented by ind PPL and nDCG
- Evidence anchors:
  - [section 6.2] "using a combination of multiple metrics leads to better predictions of ood performance... combinations including CMIP are notably more consistent across different configurations"
  - [section 6.3] "most strategies based on CMIP outperform those without it"
- Break condition: If CMIP does not capture a meaningful aspect of model quality, or if its relationship with ood performance is not stable across configurations, it will not improve predictions

### Mechanism 3
- Claim: CMIP enables better off-policy model selection by reducing regret
- Mechanism: By quantifying debiasedness, CMIP helps identify models that will generalize better to new ranking policies. Selection strategies that incorporate CMIP (e.g., top-4 CMIP, top-4 PPL: nDCGâ†‘) achieve lower regret than strategies based only on ind PPL and nDCG.
- Core assumption: Models with lower CMIP (higher debiasedness) will have lower regret when deployed on new policies
- Evidence anchors:
  - [section 6.3] "the lowest average regret is obtained by 'top-4 CMIP, top-4 PPL: nDCGâ†‘'"
- Break condition: If debiasedness does not correlate with generalization performance, CMIP-based selection strategies will not outperform others

## Foundational Learning

- Concept: Conditional independence testing
  - Why needed here: CMIP is fundamentally a measure of conditional independence between a new model's relevance scores and the logging policy's implicit relevance, given true relevance
  - Quick check question: Given random variables X, Y, and Z, what does it mean for X to be independent of Y given Z? (Answer: Knowing Z, knowing X doesn't help predict Y)

- Concept: Mutual information and its conditional variant
  - Why needed here: CMIP estimates conditional mutual information, which quantifies the average reduction in uncertainty about one variable given another, conditioned on a third variable
  - Quick check question: What is the relationship between conditional independence and conditional mutual information? (Answer: X â«« Y | Z if and only if I(X;Y|Z) = 0)

- Concept: Click models and examination hypothesis
  - Why needed here: Understanding the structure of click models (e.g., PBM, DBN) and the examination hypothesis is crucial for understanding why debiasedness is a meaningful property and how CMIP applies to these models
  - Quick check question: According to the examination hypothesis, what three factors determine whether a user clicks on a document? (Answer: The document must be examined, deemed relevant, and the examination must result in a click)

## Architecture Onboarding

- Component map: Data preprocessing -> Policy simulation -> User simulation -> Click model training -> CMIP estimation -> Evaluation
- Critical path: 1) Generate synthetic clicks with various policies and user models 2) Train click models on training data 3) Compute CMIP using k-NN sampling and binary classification 4) Evaluate prediction of ood performance and regret
- Design tradeoffs:
  - Computational cost vs. accuracy: More sophisticated CMI estimation methods may be more accurate but also more computationally expensive
  - Model complexity: Including more click models in experiments increases coverage but also increases computational cost
  - Simulation realism: More complex user models may better capture real-world behavior but also increase simulation complexity
- Failure signatures:
  - Poor correlation between CMIP and ood performance: May indicate issues with CMI estimation or that debiasedness is not the primary factor determining ood performance
  - High variance in CMIP estimates: May indicate insufficient data or issues with the k-NN sampling approach
  - CMIP not improving prediction when combined with other metrics: May indicate that CMIP does not capture a distinct aspect of model quality
- First 3 experiments:
  1. Verify CMIP computation on a simple synthetic dataset where the ground truth debiasedness is known
  2. Compare CMIP values for DCTR and PBM models on the PBM user + NoisyOracle policy configuration (expect DCTR to have higher CMIP)
  3. Test the relationship between CMIP and ood PPL by computing both for all models across all configurations and performing regression analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can CMIP be adapted to handle pairwise or listwise relevance annotations instead of pointwise annotations?
- Basis in paper: [explicit] The authors mention that CMIP currently uses pointwise relevance annotations and suggest that pairwise or listwise annotations could also be used.
- Why unresolved: The paper does not explore or validate the performance of CMIP with alternative annotation types.
- What evidence would resolve it: Experimental results comparing CMIP's performance using pointwise, pairwise, and listwise annotations on the same dataset.

### Open Question 2
- Question: How should CMIP be interpreted when human relevance annotations are imperfect or biased?
- Basis in paper: [explicit] The authors note that CMIP assumes human annotations are perfect predictors of relevance and question how to interpret results when this assumption is violated.
- Why unresolved: The paper does not investigate the robustness of CMIP to annotation noise or bias.
- What evidence would resolve it: Studies measuring CMIP's performance and interpretability under varying levels of annotation quality or bias.

### Open Question 3
- Question: Can CMIP be used effectively with click data collected from fully randomized rankings instead of requiring human relevance annotations?
- Basis in paper: [explicit] The authors suggest this as future work, noting that click feedback from randomized rankings could replace expert annotations.
- Why unresolved: The paper does not explore or validate this alternative approach to computing CMIP.
- What evidence would resolve it: Experiments demonstrating CMIP's performance when computed using randomized ranking data instead of relevance annotations.

## Limitations

- CMIP relies on accurate estimation of conditional mutual information, which may be challenging with high-dimensional relevance scores or complex true relevance distributions
- The experimental results are based on semi-synthetic data with simplified user behavior assumptions, which may not fully capture real-world search complexity
- The paper assumes perfect human relevance annotations, but CMIP's behavior and interpretation with imperfect or biased annotations remains unexplored

## Confidence

- **High Confidence**: The theoretical foundation linking debiasedness to conditional independence and its necessity for consistent relevance estimation
- **Medium Confidence**: The effectiveness of CMIP in predicting out-of-distribution performance and improving off-policy model selection
- **Medium Confidence**: The claim that including CMIP improves regression predictions more consistently than using ind PPL and nDCG alone

## Next Checks

1. Test CMIP's predictive power on a real-world dataset with known policy shifts, such as the TREC Deep Learning track data where different ranking algorithms were used across years
2. Compare CMIP against alternative debiasedness metrics, such as those based on propensity weighting or causal inference, to assess its relative performance and robustness
3. Investigate the sensitivity of CMIP to different k-NN sampling strategies and classifier architectures to determine the stability of the metric across different estimation approaches