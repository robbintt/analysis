---
ver: rpa2
title: Automated title and abstract screening for scoping reviews using the GPT-4
  Large Language Model
arxiv_id: '2311.07918'
source_url: https://arxiv.org/abs/2311.07918
tags:
- review
- gpt-4
- reviews
- source
- scoping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces GPTscreenR, an R package using the GPT-4 Large
  Language Model for automated title and abstract screening in scoping reviews. It
  employs a chain-of-thought approach to improve complex reasoning during screening.
---

# Automated title and abstract screening for scoping reviews using the GPT-4 Large Language Model

## Quick Facts
- arXiv ID: 2311.07918
- Source URL: https://arxiv.org/abs/2311.07918
- Reference count: 24
- Accuracy: 84% on six validation studies

## Executive Summary
This study introduces GPTscreenR, an R package that automates title and abstract screening for scoping reviews using the GPT-4 Large Language Model. The package employs a chain-of-thought prompting approach to improve complex reasoning during screening tasks. Validation against six scoping reviews showed GPTscreenR achieved 84% accuracy, 71% sensitivity, and 89% specificity compared to human consensus decisions. While these results are comparable to zero-shot approaches, they still fall short of human reviewer agreement, highlighting both the potential and limitations of LLM-based screening.

## Method Summary
GPTscreenR implements a chain-of-thought prompting strategy with GPT-4 to automate screening of titles and abstracts in scoping reviews. The package uses structured conversation templates with explicit roles (system, user, assistant) and requires users to provide review descriptions using the PCC framework or custom formats. The core function screen_sources() iterates through documents, constructs chain-of-thought conversations, sends them to the OpenAI API, and caches results. The approach trades speed and cost for improved accuracy and auditability by storing full conversation transcripts for human review and error correction.

## Key Results
- GPTscreenR achieved 84% accuracy, 71% sensitivity, and 89% specificity compared to human consensus
- Performance was comparable to zero-shot prompting but below human reviewer agreement levels
- Chain-of-thought approach reduced omission errors by forcing explicit enumeration of inclusion criteria

## Why This Works (Mechanism)

### Mechanism 1
- Chain-of-thought prompting reduces omission errors by forcing the model to enumerate and evaluate each inclusion criterion explicitly before making a final decision
- Sequential breakdown prevents the model from overlooking relevant criteria buried among distractors
- Core assumption: GPT-4's errors stem primarily from overlooking criteria rather than from misunderstanding them

### Mechanism 2
- Structured conversation template with explicit roles creates a consistent reasoning environment that reduces variance in model responses
- Fixed system instructions and standardized prompt structure lead to more predictable and reliable outputs
- Core assumption: GPT-4's performance is sensitive to prompt formatting and role definitions

### Mechanism 3
- Caching screening outcomes and conversation transcripts enables human review and error correction
- Reviewers can audit GPT-4's process and identify systematic errors or biases in its screening approach
- Core assumption: Human review of model reasoning can identify patterns that lead to errors

## Foundational Learning

- Concept: Cohen's kappa statistic
  - Why needed here: To compare human-GPT-4 agreement against human-human agreement, providing context for the model's reliability
  - Quick check question: If two reviewers agree on 85% of decisions and Cohen's kappa is 0.52, what does this tell you about their agreement beyond chance?

- Concept: Sensitivity and specificity in classification
  - Why needed here: These metrics evaluate the model's ability to correctly identify relevant (sensitivity) and irrelevant (specificity) sources
  - Quick check question: If a screening tool has 71% sensitivity and 89% specificity, what percentage of relevant sources might it miss?

- Concept: Zero-shot vs. few-shot prompting
  - Why needed here: Understanding the difference helps explain why chain-of-thought was chosen over simpler approaches
  - Quick check question: How does providing examples in few-shot prompting differ from the step-by-step reasoning in chain-of-thought prompting?

## Architecture Onboarding

- Component map: R package with internal API wrapper functions for OpenAI communication -> user-facing screening functions (review_description, screen_source, screen_sources)
- Critical path: User calls screen_sources() -> iterates through screen_source() for each document -> constructs chain-of-thought conversation -> sends to OpenAI API -> receives decision and transcript -> caches results to file
- Design tradeoffs: Chain-of-thought approach trades speed and cost for improved accuracy and auditability
- Failure signatures: High false negative rates suggest systematic missing of inclusion criteria; high false positive rates indicate over-inclusion; API failures prevent screening entirely
- First 3 experiments:
  1. Run the same source through screen_source() multiple times to test decision consistency
  2. Compare chain-of-thought results against zero-shot results on a small, manually verified sample
  3. Analyze conversation transcripts from edge cases to identify reasoning patterns

## Open Questions the Paper Calls Out

- What is the optimal balance between sensitivity and specificity for LLM-based screening in scoping reviews?
- How does the chain-of-thought approach perform compared to other advanced prompting techniques for LLM-based screening?
- How does LLM-based screening accuracy vary across different types of research topics and inclusion criteria complexity?

## Limitations

- Validation conducted on only six scoping reviews, limiting generalizability across research domains
- Sensitivity of 71% indicates substantial proportion of relevant sources may be missed
- Performance comparison with human reviewers may be affected by consensus decision process noise

## Confidence

**High confidence**: Technical implementation and core functionality are well-documented and reproducible
**Medium confidence**: Accuracy metrics are reliable within validation context but generalizability uncertain
**Low confidence**: Assumption that chain-of-thought consistently outperforms simpler approaches across all screening tasks

## Next Checks

1. Test GPTscreenR across a broader range of scoping reviews from different disciplines to assess generalizability
2. Quantify computational costs and time savings compared to human screening across different review sizes
3. Systematically analyze conversation transcripts from false negative and false positive cases to identify systematic biases