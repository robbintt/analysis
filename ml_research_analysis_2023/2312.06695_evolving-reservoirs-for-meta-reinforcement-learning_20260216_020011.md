---
ver: rpa2
title: Evolving Reservoirs for Meta Reinforcement Learning
arxiv_id: '2312.06695'
source_url: https://arxiv.org/abs/2312.06695
tags:
- learning
- reservoirs
- reservoir
- tasks
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ER-MRL, a computational model integrating
  Reservoir Computing, Meta Reinforcement Learning, and Evolutionary Algorithms to
  study how neural structures optimized at evolutionary scales can enhance agents'
  learning at developmental scales. The method evolves reservoir hyperparameters to
  facilitate Reinforcement Learning policy learning.
---

# Evolving Reservoirs for Meta Reinforcement Learning

## Quick Facts
- arXiv ID: 2312.06695
- Source URL: https://arxiv.org/abs/2312.06695
- Reference count: 40
- Key outcome: ER-MRL combines reservoir computing, meta-RL, and evolution to evolve reservoir hyperparameters that improve policy learning in partially observable and locomotion tasks

## Executive Summary
This paper introduces ER-MRL, a computational model that integrates Reservoir Computing, Meta Reinforcement Learning, and Evolutionary Algorithms to study how neural structures optimized at evolutionary scales can enhance agents' learning at developmental scales. The method evolves reservoir hyperparameters to facilitate Reinforcement Learning policy learning. Experiments demonstrate that evolved reservoirs enable solving partially observable tasks by reconstructing missing information, generate oscillatory dynamics beneficial for locomotion tasks, and facilitate generalization to new tasks unseen during evolution.

## Method Summary
ER-MRL uses a nested optimization framework with an outer evolutionary loop (CMA-ES) that evolves reservoir hyperparameters (spectral radius, input scaling, leak rate) and an inner developmental loop (PPO) that learns action policies using reservoir states as inputs. The reservoir acts as a context-generating system that provides additional state information to the policy network. The approach is tested across partially observable control tasks (CartPole, Pendulum) and locomotion environments (Ant, HalfCheetah, Swimmer), comparing performance against baseline RL agents with feedforward policies.

## Key Results
- Evolved reservoirs enable solving partially observable tasks by reconstructing missing velocity information from recurrent dynamics
- Reservoir hyperparameters generate oscillatory dynamics that facilitate learning in locomotion tasks
- Reservoirs evolved for one task can generalize to new tasks with different morphologies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evolved reservoir hyperparameters enable reconstruction of missing velocity information in partially observable environments.
- Mechanism: Recurrent reservoir dynamics integrate information over time, allowing the agent to infer velocity states not directly observable.
- Core assumption: The reservoir's recurrent structure and learned hyperparameters capture temporal dependencies sufficient to reconstruct missing state variables.
- Evidence anchors:
  - [abstract] "evolved reservoirs enable solving partially observable tasks by reconstructing missing information"
  - [section 3.1] "our approach leads to performance scores close to those obtained by a RL algorithm with full observability. This indicates that the evolved reservoir is able to reconstruct missing information related to velocities from its own internal recurrent dynamics."
  - [corpus] No direct corpus evidence found for this specific reconstruction mechanism.

### Mechanism 2
- Claim: Evolved reservoir hyperparameters generate oscillatory dynamics that facilitate learning in locomotion tasks.
- Mechanism: Reservoir acts as a meta-learned Central Pattern Generator (CPG), producing rhythmic patterns that help coordinate limb movements.
- Core assumption: The reservoir's intrinsic dynamics can produce oscillations useful for locomotion when evolved with appropriate hyperparameters.
- Evidence anchors:
  - [abstract] "generate oscillatory dynamics beneficial for locomotion tasks"
  - [section 3.2] "performance gains are observed in environments like... locomotion tasks (Ant, HalfCheetah, Swimmer)" and "the evolved reservoir may generate beneficial oscillatory patterns"
  - [corpus] No direct corpus evidence found for CPG-like behavior in this specific context.

### Mechanism 3
- Claim: Evolved reservoir hyperparameters facilitate generalization to new tasks unseen during evolution.
- Mechanism: Reservoir hyperparameters capture abstract properties useful across diverse environments, enabling rapid adaptation to new tasks.
- Core assumption: Reservoir hyperparameters evolved across multiple environments encode generalizable structural properties.
- Evidence anchors:
  - [abstract] "facilitate generalization to new tasks unseen during evolution"
  - [section 3.3] "reservoirs evolved for two of these tasks could be adaptable to the third, indicating potential generalization across different morphologies"
  - [corpus] No direct corpus evidence found for this specific meta-learning generalization mechanism.

## Foundational Learning

- Concept: Reservoir Computing fundamentals
  - Why needed here: Understanding how reservoir hyperparameters control network dynamics is essential for grasping how evolution shapes reservoir properties.
  - Quick check question: What three hyperparameters control the spectral radius, input scaling, and leak rate in the reservoir?

- Concept: Meta-reinforcement learning framework
  - Why needed here: The nested optimization loop (evolution of HPs → development of policy) is the core architecture.
  - Quick check question: How does the outer evolutionary loop differ from the inner developmental loop in terms of objectives and timescales?

- Concept: Partial observability in MDPs
  - Why needed here: Understanding POMDPs explains why reconstructing missing information is challenging and valuable.
  - Quick check question: What key information is missing in the partially observable versions of CartPole and Pendulum tasks?

## Architecture Onboarding

- Component map: Outer loop (CMA-ES evolves reservoir HPs) → Reservoir generation (using evolved HPs) → Inner loop (PPO learns policy using reservoir context) → Environment interaction
- Critical path: Reservoir hyperparameter evolution → Reservoir instantiation → Policy learning using reservoir context → Task performance evaluation
- Design tradeoffs: Computational cost of evaluating many reservoir configurations vs. performance benefits; simpler feedforward policies vs. richer reservoir representations
- Failure signatures: Reservoir hyperparameters evolve but don't improve policy learning; reservoir context provides no advantage over raw observations; generalization fails on new tasks
- First 3 experiments:
  1. Implement partial observability by removing velocity observations and test if reservoir improves performance over feedforward policy
  2. Test reservoir oscillatory properties by visualizing reservoir neuron activations during locomotion tasks
  3. Evaluate generalization by evolving reservoirs on two tasks and testing on a third with different morphology

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the evolved reservoir act as a Central Pattern Generator (CPG) to generate oscillatory dynamics that facilitate learning in locomotion tasks?
- Basis in paper: [explicit] The paper discusses how evolved reservoirs could generate oscillatory dynamics useful for solving locomotion tasks, similar to CPGs, and presents preliminary data suggesting this in the Swimmer environment.
- Why unresolved: The paper acknowledges that carefully testing this hypothesis would require more analysis, and only presents preliminary data for one environment.
- What evidence would resolve it: Additional experiments and quantitative analyses across multiple locomotion tasks, with detailed comparison of the reservoir's dynamics to known CPG characteristics, would be needed to confirm if the evolved reservoir functions as a CPG.

### Open Question 2
- Question: What is the impact of varying the number of reservoirs and neurons within them on the performance of ER-MRL agents?
- Basis in paper: [explicit] The paper mentions conducting additional experiments to investigate the impact of varying the number of reservoirs and neurons, noting that altering the number of neurons had a limited effect, particularly on tasks with extensive observation and action spaces.
- Why unresolved: The paper does not provide detailed results or analysis of these experiments, and the impact of varying the number of reservoirs is only briefly mentioned.
- What evidence would resolve it: A comprehensive study varying the number of reservoirs and neurons, with detailed performance analysis across different tasks, would be needed to understand the optimal configuration for different environments.

### Open Question 3
- Question: Can the ER-MRL approach be integrated with more sophisticated Meta-RL algorithms to further improve performance?
- Basis in paper: [inferred] The paper discusses the integration of Reservoir Computing with Reinforcement Learning and Meta-RL, and mentions that combining ER-MRL with more sophisticated Meta-RL algorithms could offer a means to initialize RL policy weights with purposefully selected values rather than random ones.
- Why unresolved: The paper does not explore this integration or provide any experimental results.
- What evidence would resolve it: Experiments integrating ER-MRL with advanced Meta-RL algorithms, comparing performance with and without this integration, would be needed to determine if this approach can lead to further improvements.

## Limitations
- Limited direct evidence for how reservoir dynamics specifically achieve missing information reconstruction
- Preliminary analysis of oscillatory dynamics without detailed visualization or CPG comparison
- Limited cross-task generalization experiments with only three task combinations tested

## Confidence
- Reconstruction mechanism: Medium confidence - performance evidence exists but lacks direct mechanistic demonstration
- Oscillatory dynamics: Low confidence - performance improvements observed but CPG-like behavior not directly verified
- Generalization capability: Medium confidence - some cross-task transfer demonstrated but limited experimental scope

## Next Checks
1. Implement reservoir state visualization during locomotion tasks to directly observe oscillatory patterns and compare against known CPG signatures
2. Conduct ablation studies removing the reservoir component to quantify the exact contribution of reservoir dynamics to policy performance
3. Test reservoir generalization across a broader set of task combinations, including environments with different state dimensionalities and action spaces