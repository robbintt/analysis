---
ver: rpa2
title: 'Tailor: Altering Skip Connections for Resource-Efficient Inference'
arxiv_id: '2301.07247'
source_url: https://arxiv.org/abs/2301.07247
tags:
- skip
- connections
- remover
- shortener
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing skip connections
  in deep neural networks (NNs) for hardware efficiency. Skip connections, while crucial
  for training convergence, are costly in hardware due to increased memory utilization
  and bandwidth requirements.
---

# Tailor: Altering Skip Connections for Resource-Efficient Inference

## Quick Facts
- **arXiv ID**: 2301.07247
- **Source URL**: https://arxiv.org/abs/2301.07247
- **Reference count**: 40
- **Primary result**: Hardware-aware training that dynamically alters skip connections during training can reduce FPGA resource utilization by up to 34% for BRAMs, 13% for FFs, and 16% for LUTs while maintaining minimal accuracy loss.

## Executive Summary
This paper addresses the challenge of optimizing skip connections in deep neural networks for hardware efficiency. Skip connections, while crucial for training convergence, are costly in hardware due to increased memory utilization and bandwidth requirements. The authors introduce Tailor, a hardware-software codesign tool that dynamically alters skip connections during training to produce resource-efficient NNs. Tailor offers two training methods: Skip Remover, which removes skip connections entirely, and Skip Shortener, which shortens skip connections to reduce their lifespan. The hardware designs leverage FPGA-specific optimizations, such as using shift registers instead of BRAMs for shorter skip connections. Results demonstrate that Tailor reduces resource utilization by up to 34% for BRAMs, 13% for FFs, and 16% for LUTs, while maintaining minimal to no accuracy loss.

## Method Summary
Tailor is a hardware-software codesign tool that dynamically alters skip connections during training to produce resource-efficient NNs. It uses knowledge distillation (KD) with a fixed teacher model and a student model whose skip connections are gradually modified. Two methods are offered: Skip Remover removes skip connections entirely, while Skip Shortener shortens them to reduce their lifespan. The hardware designs leverage FPGA-specific optimizations, such as using shift registers instead of BRAMs for shorter skip connections. The approach involves gradually modifying skip connections during retraining while maintaining accuracy through KD loss. This allows the network to adapt to the new topology while preserving learned features.

## Key Results
- Hardware resource utilization reduced by up to 34% for BRAMs, 13% for FFs, and 16% for LUTs on FPGA implementations
- Minimal to no accuracy loss maintained across tested networks (ResNets on image classification, QuartzNets on ASR)
- FPGA-specific optimizations enabled by shortened skip connections, including shift register implementation instead of BRAMs
- Effective across multiple datasets (CIFAR-10, CIFAR-100, SVHN, ImageNet, Oxford Nanopore Reads, LibriSpeech)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradually removing or shortening skip connections during hardware-aware training reduces hardware resource utilization without significant accuracy loss.
- Mechanism: Dynamic model alteration during training allows the network to adapt to the new topology while maintaining learned features through knowledge distillation from a fixed teacher model.
- Core assumption: Skip connections are essential for training convergence but not necessary for inference accuracy once the model is trained.
- Evidence anchors:
  - [abstract] "TAILOR uses a retraining method that gradually alters the network, resulting in little to no loss in accuracy."
  - [section] "During the retraining stage, SKIP REMOVER removes a given skip connection every few epochs. SKIP SHORTENER takes a similar iterative approach and, every few epochs, splits a given skip connection into multiple shorter ones."
  - [corpus] Weak. Corpus papers don't directly address hardware-aware skip connection modification during training.

### Mechanism 2
- Claim: Shortening skip connections allows hardware-specific optimizations (shift registers instead of BRAMs) that reduce resource utilization.
- Mechanism: By reducing the lifespan of skip connections to fit within a single dataflow stage, the hardware implementation can use simpler, more efficient memory structures like shift registers instead of BRAMs.
- Core assumption: The hardware implementation can exploit shortened skip connection lifespans to use more efficient memory types.
- Evidence anchors:
  - [abstract] "Tailor improves resource utilization by up to 34% for BRAMs, 13% for FFs, and 16% for LUTs for on-chip, dataflow-style architectures."
  - [section] "As seen in Tab. VII, the lifetime of each shortened skip connection is a little less than half the lifetime of the traditional one. With shorter lifetimes, we find that the SKIP SHORTENER's skip connections' FIFOs can now be implemented using shift registers instead of BRAMs, which is what the traditional design still uses."
  - [corpus] Weak. No corpus papers specifically discuss shift register optimization for shortened skip connections.

### Mechanism 3
- Claim: Skip connections increase hardware complexity due to irregular topology and data buffering requirements.
- Mechanism: Traditional skip connections require extra dataflow stages and FIFOs for data cloning and buffering, increasing resource utilization and complicating the hardware design.
- Core assumption: Skip connections create irregular data flow patterns that require additional hardware resources to manage.
- Evidence anchors:
  - [abstract] "Skip connections are generally detrimental to hardware efficiency... This is due to their long lifetimes, which span several NN layers, increasing memory utilization and bandwidth requirements."
  - [section] "To be low latency and high throughput, the design uses task-level pipelining (i.e., HLS dataflow) for each NN layer... Since FIFOs can only be read from once, skip connections complicate the design. We must spend a dataflow stage on cloning the skip connection data from its input FIFO into two other FIFOs..."
  - [corpus] Weak. No corpus papers directly analyze skip connection hardware complexity.

## Foundational Learning

- **Hardware-aware training**: Traditional training doesn't consider hardware implementation constraints, leading to models that are accurate but inefficient in hardware.
  - Quick check question: What is the key difference between hardware-aware training and traditional training?

- **Knowledge distillation**: Used to maintain accuracy when modifying network topology during training by transferring knowledge from a teacher model to a student model.
  - Quick check question: In the context of Tailor, what are the roles of the teacher and student models?

- **FPGA dataflow architectures**: Understanding how HLS dataflow stages and FIFOs work is crucial for appreciating how skip connections impact hardware resource utilization.
  - Quick check question: Why do skip connections complicate dataflow-style FPGA implementations?

## Architecture Onboarding

- **Component map**:
  - HLS dataflow stages for each layer or small group of layers
  - FIFOs for streaming data between stages
  - Additional dataflow stages and FIFOs specifically for skip connections (traditional design)
  - Shift registers for shortened skip connections (optimized design)
  - BRAMs for weight storage and FIFOs (traditional design)
  - DSPs for convolutional layer computations

- **Critical path**: Convolutional layers dominate the critical path. Skip connection handling (cloning, addition, buffering) is not on the critical path. Resource reduction from removing skip connection handling doesn't affect latency.

- **Design tradeoffs**:
  - SKIP REMOVER: Best resource utilization but potential accuracy loss for deep networks
  - SKIP SHORTENER: Moderate resource utilization with minimal accuracy loss, FPGA-specific optimizations possible
  - Traditional: Maximum accuracy but highest resource utilization
  - Precision tradeoff: 8-bit precision uses more LUTs and FFs, 16-bit uses more DSPs and BRAMs

- **Failure signatures**:
  - Accuracy degradation when removing skip connections from deep networks
  - Resource utilization not scaling linearly with number of filters
  - Increased latency when skip connection handling affects critical path
  - Convergence failure when skip connections are removed from networks that require them

- **First 3 experiments**:
  1. Implement a traditional ResNet20-style skip connection block in HLS dataflow, measure resource utilization and latency
  2. Apply SKIP REMOVER to the same block, implement without skip connections, measure resource utilization and latency
  3. Apply SKIP SHORTENER to the same block, implement with shortened skip connections using shift registers, measure resource utilization and latency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the hardware resource reduction achieved by Tailor scale when applied to more complex or larger neural network architectures beyond ResNets and QuartzNets?
- Basis in paper: [inferred] The paper demonstrates resource reductions on ResNets and QuartzNets, but does not explore larger or more complex architectures.
- Why unresolved: The study is limited to specific architectures, and the scalability of the method to more complex networks remains untested.
- What evidence would resolve it: Experimental results showing resource reductions on a broader range of neural network architectures, including larger and more complex models.

### Open Question 2
- Question: Can the hardware-aware training approach of Tailor be effectively combined with other optimization techniques, such as pruning or quantization, to achieve further resource efficiency?
- Basis in paper: [explicit] The paper mentions quantization as a separate process but does not explore combining Tailor with other optimization techniques.
- Why unresolved: The interaction between Tailor and other optimization methods is not investigated, leaving potential synergies unexplored.
- What evidence would resolve it: Empirical studies demonstrating the combined effects of Tailor with pruning or quantization on resource efficiency and accuracy.

### Open Question 3
- Question: What are the limitations of the Tailor approach when applied to neural networks with irregular skip connection topologies, such as those found in models with bottleneck blocks?
- Basis in paper: [explicit] The paper discusses the challenges of applying Tailor to ResNet-50 due to its bottleneck block topology.
- Why unresolved: The paper does not provide a comprehensive analysis of the limitations and potential solutions for irregular skip connection topologies.
- What evidence would resolve it: Detailed analysis and experimental results showing the performance of Tailor on various irregular skip connection topologies and potential adaptations to handle them.

## Limitations
- Hardware-specific optimizations (shift registers for shortened skips) are FPGA-specific and may not translate to other hardware platforms
- Training methodology reproducibility is limited by missing implementation details (exact criteria for selecting skip connections, specific parameter values)
- Generalization across network architectures has not been thoroughly tested beyond ResNets and QuartzNets

## Confidence

- **High confidence**: Skip connections increase hardware resource utilization in dataflow FPGA implementations
- **Medium confidence**: Gradually removing/shortening skip connections during training maintains accuracy while reducing resource utilization
- **Medium confidence**: FPGA-specific optimizations (shift registers for shortened skips) provide resource savings
- **Low confidence**: Approach generalizes to all deep neural network architectures and hardware platforms

## Next Checks
1. Implement the same skip connection optimization approach on ASIC and GPU targets to verify if the resource savings and accuracy retention generalize beyond FPGAs.
2. Apply Tailor to very deep networks (ResNet-50/101/152), Transformers, and RNNs on diverse tasks (object detection, segmentation, NLP) to assess robustness across architectures.
3. Systematically vary α and β parameters and test on networks known to require skip connections for convergence to determine the boundaries of when skip removal causes accuracy degradation.