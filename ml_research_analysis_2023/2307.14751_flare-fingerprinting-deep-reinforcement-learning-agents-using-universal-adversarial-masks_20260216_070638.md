---
ver: rpa2
title: 'FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial
  Masks'
arxiv_id: '2307.14751'
source_url: https://arxiv.org/abs/2307.14751
tags:
- adversarial
- verification
- flare
- policies
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FLARE proposes the first fingerprinting mechanism for verifying
  the ownership of stolen Deep Reinforcement Learning (DRL) policies. The core idea
  is to generate non-transferable universal adversarial masks that can successfully
  transfer from a victim policy to its modified versions but not to independently
  trained policies.
---

# FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks

## Quick Facts
- arXiv ID: 2307.14751
- Source URL: https://arxiv.org/abs/2307.14751
- Authors: 
- Reference count: 40
- Key outcome: Proposes the first fingerprinting mechanism for verifying ownership of stolen DRL policies using non-transferable universal adversarial masks, achieving 100% action agreement with stolen copies and zero false positives against independent policies.

## Executive Summary
FLARE introduces the first fingerprinting mechanism for verifying ownership of stolen Deep Reinforcement Learning (DRL) policies. The core innovation is generating non-transferable universal adversarial masks that successfully transfer from victim policies to their stolen versions but not to independently trained policies. These masks are used as fingerprints during verification by measuring action agreement values over perturbed states. Empirical evaluations demonstrate that FLARE achieves perfect detection of stolen copies while maintaining zero false positives, with minimal impact on agent performance during verification (average impact of 0.02 in Pong and 0.22 in MsPacman).

## Method Summary
FLARE generates non-transferable universal adversarial masks as fingerprints by optimizing for high fooling rates on the victim policy while ensuring low transferability to independent policies. The fingerprint generation process uses maximum-confidence adversarial masks and incorporates a non-transferability score that considers multiple independent policies. During verification, the suspected policy is evaluated on states perturbed by these masks, and high action agreement with the victim's actions indicates ownership. The method employs majority voting over multiple fingerprints to produce a final ownership verdict, with verification performed over short time windows to minimize performance impact.

## Key Results
- Achieves 100% action agreement with stolen policy copies during verification
- Maintains zero false positives when tested against independently trained policies
- Demonstrates robustness against fine-tuning and pruning model modification attacks
- Verification impact on agent performance is minimal (0.02 in Pong, 0.22 in MsPacman)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-transferable universal adversarial masks can distinguish stolen DRL policies from independently trained ones.
- Mechanism: Masks are generated to maximize action disagreement between victim and independent policies while forcing stolen policies to behave similarly when the mask is applied.
- Core assumption: The decision boundary geometry of a specific DRL policy is unique enough that perturbations affecting it won't transfer to other independently trained policies.
- Evidence anchors:
  - [abstract] "non-transferable, universal adversarial masks... can successfully transfer from a victim policy to its modified versions but not to independently trained policies"
  - [section 2.2.1] "FLARE aims to find a universal mask ð’“ that maximizes the loss function... and is bounded by ðœ– in ð‘™âˆž-norm"
  - [corpus] Weak: No direct corpus evidence for non-transferable masks in DRL; closest is [22824] on adaptive gradient-masked adversarial attacks

### Mechanism 2
- Claim: Action agreement over short time windows with adversarial masks can reliably indicate ownership.
- Mechanism: During verification, suspected policy is evaluated on states perturbed by fingerprint masks, and high agreement with victim actions indicates it's a stolen copy.
- Core assumption: A stolen policy will maintain similar behavior to victim policy even when faced with same adversarial perturbations, while independent policies will not.
- Evidence anchors:
  - [abstract] "FLARE employs these masks as fingerprints to verify... ownership... by measuring an action agreement value over states perturbed via such masks"
  - [section 2.2.2] "ð´ð´ is calculated as 1/ð‘€ Ãð‘¡ð‘ ð‘¡ð‘Žð‘Ÿð‘¡ +ð‘€ âˆ’1 ð‘¡ =ð‘¡ð‘ ð‘¡ð‘Žð‘Ÿð‘¡ ð´ð´(ðœ‹V, ðœ‹S, ð’”ð‘¡ , ð’“)"
  - [corpus] Weak: No direct corpus evidence for action agreement verification in DRL fingerprinting; closest is [191231] on RAT attacks

### Mechanism 3
- Claim: Universal adversarial masks are more robust to model modifications than individual adversarial examples.
- Mechanism: Mask generation process incorporates non-transferability score considering multiple independent policies, making it harder for simple modifications to evade detection.
- Core assumption: Model modifications preserving original policy's behavior will still be vulnerable to same adversarial masks, while significant alterations will degrade performance.
- Evidence anchors:
  - [section 2.2.1] "FLARE computes the non-transferability score (ð‘›ð‘¡ð‘  ) for a universal adversarial mask ð’“ on an episode ð‘’ð‘ð‘ "
  - [section 3.3.1] Empirical results showing FLARE maintains high action agreement even after fine-tuning and pruning attacks
  - [corpus] Weak: No direct corpus evidence for robustness of universal masks to model modifications in DRL

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation of reinforcement learning
  - Why needed here: The paper operates within the MDP framework, where states, actions, rewards, and policies are formally defined
  - Quick check question: In an MDP, what tuple represents the complete environment specification, and what does each component represent?

- Concept: Transferability of adversarial examples
  - Why needed here: The core innovation relies on generating adversarial examples that transfer from victim to stolen policies but not to independent ones
  - Quick check question: What property of adversarial examples makes them useful for fingerprinting, and how does FLARE ensure this property is directional (victimâ†’stolen but not victimâ†’independent)?

- Concept: Universal adversarial perturbations vs. individual adversarial examples
  - Why needed here: FLARE uses universal masks rather than per-state perturbations, which affects both effectiveness and computational efficiency
  - Quick check question: How does the optimization objective for universal perturbations differ from that of individual adversarial examples, and what advantage does this provide for fingerprinting?

## Architecture Onboarding

- Component map: Fingerprint Generation -> Mask Storage -> Verification Episode Execution -> Action Agreement Calculation -> Majority Voting -> Ownership Decision

- Critical path: Fingerprint generation â†’ mask storage â†’ verification episode execution â†’ action agreement calculation â†’ majority voting â†’ ownership decision

- Design tradeoffs:
  - Window size (M): Larger windows provide more data but may degrade agent performance; smaller windows preserve performance but may lack statistical power
  - Number of fingerprints: More fingerprints improve reliability but increase computational cost and may reduce effectiveness against modified policies
  - Perturbation constraint (Îµ): Larger Îµ increases fooling rate but may make masks more transferable; smaller Îµ preserves non-transferability but may reduce effectiveness

- Failure signatures:
  - High action agreement with independent policies â†’ non-transferable masks are transferring, possibly due to similar policy architectures
  - Low action agreement with stolen policies â†’ masks not effective against modified versions, possibly due to extensive retraining
  - Excessive performance degradation during verification â†’ window size or perturbation magnitude too large

- First 3 experiments:
  1. Verify that fingerprints generated for a victim policy produce high action agreement with the original policy but low agreement with an independent policy trained with the same algorithm
  2. Test robustness by applying fine-tuning to a stolen policy and measuring whether action agreement remains high
  3. Evaluate impact of verification on agent performance by measuring return difference with and without fingerprint application

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is FLARE against more advanced model extraction attacks that could steal the exact policy of the victim agent?
- Basis in paper: [explicit] The authors mention they did not consider model extraction attacks through imitation learning and were unable to obtain good stolen policies when attempting the attack proposed by Chen et al. [7].
- Why unresolved: The authors acknowledge this as a limitation and state they could not obtain good stolen policies using the Chen et al. attack, suggesting more research is needed on this type of attack.
- What evidence would resolve it: Implementing and testing more sophisticated model extraction attacks on FLARE, and evaluating if the extracted policies can successfully evade the fingerprinting verification.

### Open Question 2
- Question: Can FLARE be extended to continuous action spaces, and if so, how would the verification process need to be adapted?
- Basis in paper: [explicit] The authors hypothesize that FLARE could potentially be extended to continuous tasks by checking how much the suspected agent deviates from the original action value, but leave this for future work.
- Why unresolved: The authors have not yet implemented or tested FLARE on continuous action spaces, so the effectiveness and required adaptations are unknown.
- What evidence would resolve it: Implementing FLARE on a continuous action space environment and evaluating the action agreement and robustness against attacks compared to the discrete case.

### Open Question 3
- Question: How does the choice of hyperparameters (perturbation constraint, fooling rate threshold, non-transferability score threshold) impact the performance and robustness of FLARE?
- Basis in paper: [explicit] The authors performed a grid search for the perturbation constraint and set other hyperparameters to fixed values, but did not extensively explore the impact of varying these parameters.
- Why unresolved: The authors only briefly discuss their hyperparameter choices and do not provide a comprehensive analysis of how different values affect FLARE's effectiveness, integrity, and robustness.
- What evidence would resolve it: Conducting a systematic study of FLARE's performance across a range of hyperparameter values, including plots of action agreement, fooling rate, and robustness metrics against attacks for different settings.

## Limitations

- The paper lacks detailed hyperparameter specifications for fingerprint generation, particularly the perturbation constraint epsilon, minimum fooling rate threshold, and non-transferability score threshold.
- While demonstrating robustness against fine-tuning and pruning attacks, the paper does not evaluate more sophisticated model modification techniques that could potentially evade detection.
- The implementation details of the maximum-confidence adversarial mask generation method are not fully provided, making exact reproduction challenging.

## Confidence

- **High Confidence:** The core mechanism of using non-transferable universal adversarial masks for fingerprinting DRL policies is well-founded and supported by empirical results showing 100% action agreement with stolen copies and zero false positives.
- **Medium Confidence:** The robustness claims against model modification attacks are supported by experiments, but the limited scope of evaluated attacks (fine-tuning and pruning) leaves uncertainty about resistance to more advanced evasion techniques.
- **Medium Confidence:** The performance impact during verification (0.02 in Pong, 0.22 in MsPacman) is measured, but the sensitivity to window size and perturbation magnitude tradeoffs could benefit from more extensive analysis.

## Next Checks

1. Reproduce the fingerprint generation process with varying perturbation constraints to determine the optimal balance between fooling rate and non-transferability score.
2. Test the method against more sophisticated model modification attacks, including distillation and architecture changes, to evaluate robustness claims comprehensively.
3. Conduct sensitivity analysis on the verification window size to establish guidelines for minimizing performance impact while maintaining reliable ownership detection.