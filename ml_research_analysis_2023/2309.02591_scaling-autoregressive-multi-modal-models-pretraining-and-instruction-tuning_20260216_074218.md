---
ver: rpa2
title: 'Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning'
arxiv_id: '2309.02591'
source_url: https://arxiv.org/abs/2309.02591
tags:
- image
- text
- break
- generation
- cm3leon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CM3Leon, a retrieval-augmented, token-based,
  decoder-only multi-modal language model capable of generating and infilling both
  text and images. CM3Leon extends the CM3 architecture by scaling up and training
  on more diverse instruction-style data, including a large-scale retrieval-augmented
  pretraining stage and a second multi-task supervised fine-tuning stage.
---

# Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning

## Quick Facts
- **arXiv ID:** 2309.02591
- **Source URL:** https://arxiv.org/abs/2309.02591
- **Reference count:** 40
- **Primary result:** CM3Leon achieves state-of-the-art text-to-image generation with 5x less training compute, reaching 0.88 MS-COCO FID in zero-shot evaluation

## Executive Summary
This paper presents CM3Leon, a retrieval-augmented, token-based, decoder-only multi-modal language model capable of generating and infilling both text and images. CM3Leon extends the CM3 architecture through scaling up and training on more diverse instruction-style data, including a large-scale retrieval-augmented pretraining stage and a second multi-task supervised fine-tuning stage. The model is trained on a large-scale licensed Shutterstock dataset and achieves state-of-the-art performance in text-to-image generation with 5x less training compute than comparable methods, achieving a zero-shot MS-COCO FID of 4.88. After supervised fine-tuning, CM3Leon demonstrates unprecedented levels of controllability in tasks ranging from language-guided image editing to image-controlled generation and segmentation. The paper introduces a self-contained contrastive decoding method that produces high-quality outputs.

## Method Summary
CM3Leon employs a retrieval-augmented pretraining approach using CLIP-based dense retrieval to expand world knowledge during training. The model uses a decoder-only transformer architecture with varying sizes (350M, 760M, 7B parameters) following multi-modal scaling laws. Training involves two stages: first, retrieval-augmented pretraining on the licensed Shutterstock dataset, then multi-task supervised fine-tuning on diverse instruction-style data including text-guided image editing, image-to-image grounded generation, spatially grounded image generation, and how-to-write tasks. The CM3 objective masks specific spans and relocates them to the end, with modifications to prevent masking across <break> tokens. A novel contrastive decoding method provides self-guidance for high-quality text and image generation.

## Key Results
- Zero-shot MS-COCO FID of 4.88, achieving state-of-the-art text-to-image generation performance
- 5x reduction in training compute compared to comparable methods
- Demonstrated unprecedented controllability across diverse tasks including language-guided image editing, image-controlled generation, and segmentation
- Effective self-contained contrastive decoding method producing high-quality outputs

## Why This Works (Mechanism)

### Mechanism 1: Retrieval Augmentation
- **Claim:** Retrieval augmentation during pretraining improves image generation quality by expanding the model's world knowledge.
- **Mechanism:** Static retrieval of relevant documents from a memory bank provides additional context during pretraining, enriching the model's understanding of concepts and entities.
- **Core assumption:** Retrieved documents are relevant and diverse enough to meaningfully augment training data without introducing noise or bias.
- **Break condition:** If retrieval mechanism fetches irrelevant or redundant documents, performance may degrade.

### Mechanism 2: CM3 Objective Function
- **Claim:** The CM3 objective enables flexible infilling and autoregressive generation for both images and text.
- **Mechanism:** Masks specific spans in input and relocates them to the end, allowing the model to learn both infilling and autoregressive generation tasks.
- **Core assumption:** Masking across <break> tokens is undesirable as it may lead to generating image content from arbitrary midpoints.
- **Break condition:** Poor masking strategy design may result in the model learning to generate incoherent or irrelevant content.

### Mechanism 3: Contrastive Decoding
- **Claim:** Contrastive decoding improves text and image generation quality by providing self-guidance.
- **Mechanism:** Defines a score per token based on log probability subtraction between strong (with text conditioning) and weak (without text conditioning) models.
- **Core assumption:** Strong and weak models provide complementary information for guiding the generation process.
- **Break condition:** Poorly calibrated strong and weak models may fail to effectively guide generation.

## Foundational Learning

- **Token-based autoregressive models**
  - Why needed: Understanding basics is crucial for grasping CM3Leon architecture and capabilities
  - Quick check: What is the difference between autoregressive and non-autoregressive models?

- **Retrieval-augmented pretraining**
  - Why needed: This is a key component of CM3Leon's training approach
  - Quick check: How does retrieval-augmented pretraining differ from standard pretraining?

- **Contrastive decoding**
  - Why needed: Novel technique introduced for improving text and image generation quality
  - Quick check: What is the main idea behind contrastive decoding, and how does it differ from traditional decoding methods?

## Architecture Onboarding

- **Component map:** Retrieval-augmented pretraining stage → Multi-task supervised fine-tuning stage → Contrastive decoding method
- **Critical path:** Pretraining on licensed Shutterstock dataset → Supervised fine-tuning on mixed image-text tasks → Contrastive decoding for inference
- **Design tradeoffs:** Retrieval augmentation trades computational efficiency for improved performance; contrastive decoding trades inference speed for higher-quality outputs
- **Failure signatures:** Retrieving irrelevant documents, masking across <break> tokens, poorly calibrated strong/weak models
- **First 3 experiments:**
  1. Train small CM3Leon model on Shutterstock subset to verify retrieval augmentation effectiveness
  2. Implement and test contrastive decoding on small dataset to evaluate impact on generation quality
  3. Fine-tune pre-trained CM3Leon on specific image-text task to assess flexibility and downstream performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the specific scaling laws for retrieval-augmented multimodal models, and how do they compare to scaling laws for non-retrieval-augmented models?
- **Basis:** Paper mentions "multi-modal scaling laws" as basis for training setup but doesn't provide specific details on how retrieval-augmentation affects scaling
- **Why unresolved:** Paper references scaling laws but doesn't explicitly derive or present them for retrieval-augmented case
- **What evidence would resolve it:** Detailed analysis of model performance and efficiency across different scales (model size, dataset size, number of retrieved documents) with and without retrieval-augmentation

### Open Question 2
- **Question:** How does the quality of retrieved documents impact final model performance, and what retrieval strategies are most effective?
- **Basis:** Paper discusses retrieval augmentation in detail but doesn't provide comprehensive analysis of how different retrieval qualities affect performance
- **Why unresolved:** Paper mentions importance of retrieval but doesn't systematically explore impact of retrieval quality on model outputs
- **What evidence would resolve it:** Experiments varying quality of retrieved documents and measuring impact on model performance across various tasks

### Open Question 3
- **Question:** What is the optimal balance between text and image data in training dataset for achieving best multimodal performance?
- **Basis:** Paper uses dataset with 3B text tokens and large number of images but doesn't explore impact of different text-to-image ratios
- **Why unresolved:** Paper doesn't experiment with different ratios of text to image data
- **What evidence would resolve it:** Training multiple models with varying ratios of text to image data and comparing performance on range of multimodal tasks

### Open Question 4
- **Question:** How does CM3Leon's performance on zero-shot tasks compare to models trained specifically for those tasks?
- **Basis:** Paper mentions zero-shot MS-COCO FID scores and demonstrates zero-shot capabilities but doesn't provide comprehensive comparison with specialized models
- **Why unresolved:** Paper shows impressive zero-shot results but doesn't directly compare to models trained specifically for tasks demonstrated
- **What evidence would resolve it:** Direct comparison of CM3Leon's zero-shot performance against models specifically trained for tasks

### Open Question 5
- **Question:** What are the limitations of the contrastive decoding method proposed, and how does it perform on tasks beyond text-to-image generation?
- **Basis:** Paper introduces contrastive decoding method and compares it to other decoding strategies but doesn't explore limitations or performance on wide range of tasks
- **Why unresolved:** Paper focuses on benefits of CD-K for text-to-image generation but doesn't discuss potential drawbacks or effectiveness on other generation tasks
- **What evidence would resolve it:** Extensive evaluation of CD-K across various multimodal tasks and analysis of strengths and weaknesses compared to other decoding methods

## Limitations

- **Dataset opacity:** Core performance claims rely on proprietary Shutterstock dataset with undisclosed composition and licensing terms
- **Training compute claims:** "5x less training compute" claim lacks specific baseline details and doesn't account for model size or data diversity differences
- **Retrieval augmentation validation:** Limited ablation showing isolated contribution of retrieval to performance; no demonstration of what happens without retrieval

## Confidence

**High Confidence:** Architectural extensions to CM3 (scaling, instruction tuning, contrastive decoding) are technically sound with sufficient implementation details for reproduction

**Medium Confidence:** State-of-the-art MS-COCO FID score of 4.88 is impressive but depends heavily on undisclosed dataset and training setup, making independent verification challenging

**Low Confidence:** Broad claims about "unprecedented levels of controllability" across diverse tasks lack quantitative comparisons against specific baselines for each task type

## Next Checks

1. **Ablation Study:** Train version of CM3Leon without retrieval augmentation and compare zero-shot MS-COCO FID scores to isolate contribution of retrieval to reported 4.88 score

2. **Open Dataset Reproduction:** Attempt to reproduce core results using only publicly available licensed image-text datasets (LAION, CC12M) with same model sizes and training procedures to test generalizability

3. **Task-Specific Evaluation:** Conduct controlled experiments comparing CM3Leon's performance on individual task types (image editing, segmentation, grounded generation) against established specialized models using standardized metrics for each task