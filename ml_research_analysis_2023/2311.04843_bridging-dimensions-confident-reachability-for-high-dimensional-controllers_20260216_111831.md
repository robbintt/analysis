---
ver: rpa2
title: 'Bridging Dimensions: Confident Reachability for High-Dimensional Controllers'
arxiv_id: '2311.04843'
source_url: https://arxiv.org/abs/2311.04843
tags:
- verification
- discrepancy
- reachable
- systems
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges exhaustive closed-loop verification with high-dimensional
  controllers by approximating them with multiple low-dimensional controllers in different
  regions of the state space. The approximation error is bounded statistically using
  conformal prediction, enabling safety guarantees with high confidence.
---

# Bridging Dimensions: Confident Reachability for High-Dimensional Controllers

## Quick Facts
- arXiv ID: 2311.04843
- Source URL: https://arxiv.org/abs/2311.04843
- Reference count: 40
- Key outcome: Approximates high-dimensional controllers with multiple low-dimensional controllers and uses conformal prediction to provide statistical safety guarantees with high confidence.

## Executive Summary
This paper addresses the challenge of verifying safety for high-dimensional neural network controllers by approximating them with multiple low-dimensional controllers (LDCs) in different regions of the state space. The approach uses knowledge distillation to train LDCs that minimize approximation error while maintaining verifiability through reduced Lipschitz constants. Statistical discrepancy bounds between HDC and LDC behaviors are computed using conformal prediction, and these bounds are used to inflate reachability analysis results, providing high-confidence safety guarantees for the original high-dimensional controller.

## Method Summary
The method bridges exhaustive closed-loop verification with high-dimensional controllers through a multi-step process. First, multiple low-dimensional controllers are trained using knowledge distillation to approximate the high-dimensional controller's behavior in different state space regions, with a two-objective gradient descent that balances MSE and Lipschitz loss. Second, reachability analysis is performed on each LDC to compute overapproximated reachable sets. Third, statistical discrepancy bounds are computed using conformal prediction, either trajectory-based (comparing full trajectories) or action-based (comparing control outputs). Finally, the LDC reachability results are inflated by these discrepancy bounds to provide safety guarantees for the high-dimensional controller.

## Key Results
- Multi-LDC approaches outperform single-LDC and pure conformal prediction baselines in true positive rate and F1-score
- Action-based inflation requires significantly less data than trajectory-based methods while maintaining similar performance
- The approach successfully verifies safety for inverted pendulum and mountain car benchmarks with image-based controllers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-dimensional controllers can be approximated by multiple low-dimensional controllers operating in different state space regions.
- Mechanism: The system's behavior in each state space region is approximated by a simpler controller that operates on the same state space but uses a smaller neural network. These controllers are trained using knowledge distillation to minimize approximation error while maintaining verifiability through reduced Lipschitz constants.
- Core assumption: The behavior of the HDC can be well-approximated by multiple LDCs, each operating on a subset of the state space.
- Evidence anchors:
  - [abstract] "Our key insight is that the behavior of a high-dimensional controller can be approximated with several low-dimensional controllers in different regions of the state space."
  - [section] "Instead of verifying an HDC's safety directly over a complicated input space, our key idea is to approximate it with several LDCs so that we can reduce the HDC reachability problem to several LDC reachability problems."
  - [corpus] Weak evidence - corpus papers focus on verification methods but don't discuss this specific approximation mechanism.
- Break condition: The approximation error between HDC and LDC becomes too large for certain state space regions, making the statistical bounds unreliable.

### Mechanism 2
- Claim: Statistical bounds between HDC and LDC behaviors can be computed using conformal prediction to provide high-confidence reachability guarantees.
- Mechanism: The difference between HDC and LDC behaviors is quantified using either trajectory-based or action-based discrepancy functions. These functions are computed statistically using conformal prediction, which provides probabilistic bounds on the approximation error without distributional assumptions.
- Core assumption: The HDC-LDC discrepancy follows an i.i.d. distribution that can be bounded using conformal prediction.
- Evidence anchors:
  - [abstract] "Then, we inflate low-dimensional reachability results with statistical approximation errors, yielding a high-confidence reachability guarantee for the high-dimensional controller."
  - [section] "We will quantify the difference between LDCs and HDCs using discrepancy functions, inspired by the prior work on non-neural-network hybrid systems."
  - [corpus] Moderate evidence - corpus includes papers on statistical verification and conformal prediction, supporting this mechanism.
- Break condition: The i.i.d. assumption for conformal prediction breaks down, particularly when the approximation quality varies significantly across state space regions.

### Mechanism 3
- Claim: Reachability analysis of LDCs can be inflated with discrepancy bounds to provide safety guarantees for the HDC.
- Mechanism: The reachable sets computed for LDCs are expanded (inflated) by the statistical discrepancy bounds. For trajectory-based inflation, the reachable sets are expanded by the maximum predicted trajectory difference. For action-based inflation, the output bounds of the LDC neural network are expanded by the action discrepancy.
- Core assumption: The inflated LDC reachable sets will contain the true HDC reachable sets with high probability.
- Evidence anchors:
  - [abstract] "Then, if low-dimensional reachability results are inflated with statistical approximation errors, they yield a high-confidence reachability guarantee for the high-dimensional controller."
  - [section] "We investigate two inflation techniques — based on trajectories and actions — both of which show convincing performance in two OpenAI gym benchmarks."
  - [corpus] Weak evidence - corpus papers discuss reachability analysis but not this specific inflation mechanism.
- Break condition: The inflation process becomes too conservative, leading to false negatives where truly safe regions are classified as unsafe.

## Foundational Learning

- Concept: Conformal prediction for uncertainty quantification
  - Why needed here: Provides statistical bounds on HDC-LDC discrepancy without distributional assumptions, enabling high-confidence verification guarantees.
  - Quick check question: How does conformal prediction compute uncertainty bounds differently from traditional statistical methods?

- Concept: Knowledge distillation for model compression
  - Why needed here: Enables training of simpler, verifiable LDCs that approximate complex HDCs while maintaining performance.
  - Quick check question: What are the two conflicting objectives in knowledge distillation for verification-aware training?

- Concept: Reachability analysis for safety verification
  - Why needed here: Provides exhaustive overapproximation of all possible system behaviors from given initial conditions, forming the basis for safety guarantees.
  - Quick check question: What is the difference between a reachable set and a reachable tube in verification?

## Architecture Onboarding

- Component map:
  HDC -> Image Generator -> LDC -> Knowledge Distillation -> Reachability Analyzer -> Safety Checker
  LDC -> Conformal Prediction -> Statistical Discrepancy Bounds
  Reachability Analyzer + Discrepancy Bounds -> Inflated Reachable Tubes

- Critical path:
  1. Train LDCs using knowledge distillation (Step 1)
  2. Perform reachability analysis on LDCs (Step 2)
  3. Compute statistical discrepancy bounds (Step 3)
  4. Inflate reachability results with discrepancy bounds (Step 4)
  5. Check safety of inflated reachable tubes (Step 5)

- Design tradeoffs:
  - Single LDC vs. Multiple LDCs: Single LDC is simpler but may have higher approximation error; multiple LDCs reduce error but increase computational complexity
  - Trajectory-based vs. Action-based inflation: Trajectory-based is more accurate but requires expensive system simulation; action-based is more practical but less precise
  - Grid resolution: Finer grids provide better approximation but increase the number of LDCs to train and verify

- Failure signatures:
  - High overapproximation error in LDC reachability indicates need to reduce LDC Lipschitz constant
  - Large discrepancy bounds indicate need to retrain LDC with lower MSE threshold or refine state space grid
  - Conservative verification results indicate discrepancy bounds may be too large relative to actual HDC behavior

- First 3 experiments:
  1. Verify the inverted pendulum benchmark with 1 LDC using action-based inflation
  2. Compare trajectory-based vs. action-based inflation on mountain car benchmark
  3. Evaluate effect of grid resolution on verification accuracy for inverted pendulum

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Lipschitz constant threshold $\lambda$ and MSE threshold $\epsilon$ in the two-objective gradient descent method affect the trade-off between approximation accuracy and verifiability of the LDC?
- Basis in paper: [explicit] The paper states that the thresholds $\lambda$ and $\epsilon$ are used to stop the two-objective gradient descent algorithm when both loss functions satisfy their specified thresholds.
- Why unresolved: The paper does not provide specific values for $\lambda$ and $\epsilon$ or discuss how to choose these thresholds.
- What evidence would resolve it: Experiments that vary $\lambda$ and $\epsilon$ and measure the resulting approximation accuracy and verifiability of the LDC.

### Open Question 2
- Question: How does the choice of the state-space grid size affect the performance of the statistical discrepancy bounds?
- Basis in paper: [explicit] The paper mentions that the state-space grid is used to calculate the statistical discrepancy functions in each region separately, but does not discuss how the grid size affects the performance.
- Why unresolved: The paper does not provide any analysis or experiments on the effect of grid size on the statistical discrepancy bounds.
- What evidence would resolve it: Experiments that vary the grid size and measure the resulting statistical discrepancy bounds and verification performance.

### Open Question 3
- Question: How does the confidence level $\alpha$ affect the tightness of the inflated reachable tubes and the verification performance?
- Basis in paper: [explicit] The paper uses $\alpha$ to calculate the statistical discrepancy bounds, but does not discuss how it affects the inflated reachable tubes or verification performance.
- Why unresolved: The paper does not provide any analysis or experiments on the effect of $\alpha$ on the inflated reachable tubes or verification performance.
- What evidence would resolve it: Experiments that vary $\alpha$ and measure the resulting inflated reachable tubes and verification performance.

## Limitations

- Approximation quality relies on iterative retraining and gridding, suggesting initial approximations may not be sufficiently accurate
- Conformal prediction assumes i.i.d. distribution of discrepancies, which may not hold for complex control tasks
- Computational overhead of training multiple LDCs versus a single LDC with higher approximation error is not quantified

## Confidence

- High confidence: The experimental methodology and evaluation metrics are clearly defined and reproducible
- Medium confidence: The theoretical framework connecting conformal prediction to verification bounds is sound but relies on strong assumptions
- Low confidence: The generalizability of results to more complex control tasks beyond the two OpenAI Gym benchmarks

## Next Checks

1. Cross-task generalization: Validate the approach on a third control task with different dynamics (e.g., cart-pole or lunar lander) to assess robustness beyond inverted pendulum and mountain car.

2. Confidence level calibration: Perform systematic experiments to quantify the actual confidence levels achieved by the inflated reachability results versus the claimed bounds, including false positive and false negative rates.

3. Scalability analysis: Evaluate the approach on larger state spaces and more complex image-based controllers to determine the practical limits of the LDC approximation strategy and the computational overhead of the iterative retraining process.