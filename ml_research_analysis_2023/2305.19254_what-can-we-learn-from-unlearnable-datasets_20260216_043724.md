---
ver: rpa2
title: What Can We Learn from Unlearnable Datasets?
arxiv_id: '2305.19254'
source_url: https://arxiv.org/abs/2305.19254
tags:
- unlearnable
- datasets
- data
- perturbations
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Unlearnable datasets aim to protect data privacy by preventing
  DNNs from generalizing, but this paper challenges their effectiveness. The authors
  find that DNNs can learn generalizable features from such datasets, potentially
  compromising privacy.
---

# What Can We Learn from Unlearnable Datasets?

## Quick Facts
- arXiv ID: 2305.19254
- Source URL: https://arxiv.org/abs/2305.19254
- Reference count: 40
- One-line primary result: DNNs can learn generalizable features from unlearnable datasets, and class-wise linearly separable perturbations can be removed via orthogonal projection attacks

## Executive Summary
Unlearnable datasets are designed to protect data privacy by preventing deep neural networks from generalizing, but this paper reveals critical vulnerabilities in these approaches. The authors demonstrate that networks can actually learn useful features from such datasets that remain generalizable to clean test data through Deep Feature Reweighting. They also show that linearly separable perturbations, previously thought necessary for unlearnable datasets, are not required for effectiveness. The paper introduces an orthogonal projection attack that exploits class-wise, linearly separable perturbations to recover learnable data at a fraction of the computational cost of adversarial training.

## Method Summary
The study systematically evaluates unlearnable datasets through controlled experiments on CIFAR-10, SVHN, CIFAR-100, and ImageNet Subset using various unlearnable dataset methods. The authors employ Deep Feature Reweighting to assess whether features learned during poison training retain generalizability to clean test data. They test linear separability of perturbations through logistic regression models and develop an orthogonal projection attack that removes class-wise perturbations by projecting data orthogonally to learned perturbation patterns. The attack is compared against adversarial training to demonstrate computational efficiency and effectiveness.

## Key Results
- DNNs can learn generalizable features from unlearnable datasets despite low test accuracy during poison training
- Linear separability of perturbations is not a necessary condition for unlearnable datasets to prevent generalization
- Orthogonal projection attack effectively removes class-wise, linearly separable perturbations and is computationally cheaper than adversarial training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DNNs can extract generalizable features from unlearnable datasets despite low test accuracy.
- Mechanism: During poison training, the network learns a feature extractor that captures useful information from the original clean data, which can later be reweighted using Deep Feature Reweighting (DFR) to achieve high test performance.
- Core assumption: Features learned during training on unlearnable datasets are not completely corrupted and retain some relevance for clean test data generalization.
- Evidence anchors:
  - [abstract] "we find that networks actually can learn useful features that can be reweighed for high test performance"
  - [section 4.2] "We borrow the DFR method to better understand the utility of features learned during poison training"

### Mechanism 2
- Claim: Linear separability of perturbations is not a necessary condition for unlearnable datasets to prevent generalization.
- Mechanism: Even when perturbations are not linearly separable, unlearnable datasets can still prevent standard training from achieving high test accuracy through other mechanisms (e.g., error-minimizing or error-maximizing effects).
- Core assumption: Linear separability is one possible mechanism but not the only way to create effective unlearnable datasets.
- Evidence anchors:
  - [abstract] "We provide a counterexample, demonstrating that linear separability of perturbations is not a necessary condition"
  - [section 4.3] "Autoregressive Perturbations (AR) are not linearly separable and are, in fact, less separable than clean CIFAR-10 images"

### Mechanism 3
- Claim: Class-wise, linearly separable perturbations can be removed to recover learnable data.
- Mechanism: By training a linear classifier on the unlearnable dataset to learn the perturbation patterns, then projecting the data orthogonally to these learned patterns, the predictive but semantically meaningless features can be eliminated.
- Core assumption: Class-wise perturbations create simple, perfectly predictive features that a linear model can identify and remove.
- Evidence anchors:
  - [abstract] "we propose an orthogonal projection attack which allows learning from unlearnable datasets"
  - [section 4.4.1] "we train a simple logistic regression model on poison image pixels, in an attempt to optimize the most predictive linear image features"

## Foundational Learning

- Concept: Deep Feature Reweighting (DFR)
  - Why needed here: DFR is used to evaluate whether features learned during poison training retain generalizability to clean test data.
  - Quick check question: What does DFR do to a feature extractor trained on unlearnable data, and why is this useful for evaluating privacy protection?

- Concept: Linear separability
  - Why needed here: Understanding linear separability is crucial for analyzing how unlearnable datasets work and for developing attacks like orthogonal projection.
  - Quick check question: How does linear separability of perturbations relate to the effectiveness of unlearnable datasets, and what happens when perturbations are not linearly separable?

- Concept: Orthogonal projection
  - Why needed here: Orthogonal projection is the core technique used in the attack to remove class-wise, linearly separable perturbations from unlearnable datasets.
  - Quick check question: How does projecting data orthogonally to learned linear weights help in recovering learnable data from unlearnable datasets?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Model training -> Feature extraction -> Linear model training -> Orthogonal projection -> Evaluation

- Critical path:
  1. Train model on unlearnable dataset
  2. Save checkpoints for DFR analysis
  3. Train linear model on unlearnable data
  4. Perform orthogonal projection using learned weights
  5. Retrain model on projected data
  6. Evaluate test accuracy

- Design tradeoffs:
  - Using class-wise vs. sample-wise perturbations: Class-wise are easier to attack but may be more effective at preventing generalization
  - Perturbation magnitude: Larger perturbations may be more effective but risk being perceptible
  - Computational cost: Orthogonal projection is cheaper than adversarial training but may not work on all dataset types

- Failure signatures:
  - Low DFR test accuracy: Indicates that features learned during poison training are not generalizable
  - Orthogonal projection fails: Suggests perturbations are too complex or not linearly separable
  - High test accuracy on unlearnable data: Indicates the dataset is not effective at preventing unauthorized learning

- First 3 experiments:
  1. Train ResNet-18 on a sample-wise perturbed unlearnable dataset and evaluate DFR test accuracy to check if generalizable features are learned
  2. Apply orthogonal projection to a class-wise perturbed unlearnable dataset and retrain to see if test accuracy improves
  3. Test linear separability of perturbations from a new unlearnable dataset method to determine if it fits the counterexample pattern

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are there unexplored unlearnable dataset methods that do not rely on linearly separable perturbations?
- Basis in paper: [explicit] The paper presents AR perturbations as a counterexample to the hypothesis that unlearnable datasets require linearly separable perturbations, suggesting there could be underexplored methods.
- Why unresolved: The paper only explores one counterexample (AR perturbations) and does not investigate other potential methods.
- What evidence would resolve it: Developing and testing new unlearnable dataset methods that do not rely on linearly separable perturbations, and comparing their effectiveness to existing methods.

### Open Question 2
- Question: Can the Orthogonal Projection attack be improved to handle sample-wise perturbed unlearnable datasets more effectively?
- Basis in paper: [inferred] The paper mentions that the Orthogonal Projection attack struggles against sample-wise perturbed datasets like Adversarial Poisoning, AR, and Unlearnable Examples, suggesting room for improvement.
- Why unresolved: The paper only presents the basic Orthogonal Projection attack and does not explore potential modifications or improvements.
- What evidence would resolve it: Developing and testing enhanced versions of the Orthogonal Projection attack that can better handle sample-wise perturbed datasets, and comparing their performance to the original attack.

### Open Question 3
- Question: How do unlearnable datasets perform against more sophisticated attacks, such as those using generative models or advanced optimization techniques?
- Basis in paper: [explicit] The paper mentions that attacks using diffusion models, adversarial training, and error-maximizing augmentations have been proposed, but does not evaluate their effectiveness against unlearnable datasets.
- Why unresolved: The paper only tests the Orthogonal Projection attack and adversarial training, and does not explore other potential attacks.
- What evidence would resolve it: Evaluating the performance of unlearnable datasets against a variety of advanced attacks, and comparing their effectiveness to the attacks tested in the paper.

## Limitations
- The orthogonal projection attack may not generalize to more sophisticated unlearnable dataset methods that employ sample-wise or adaptive perturbations
- The effectiveness of the attack has only been tested on a limited set of architectures and datasets
- The paper assumes class-wise perturbations, which may not represent all possible unlearnable dataset designs

## Confidence
- Claim: DNNs can learn generalizable features from unlearnable datasets → Medium
- Claim: Linear separability is not necessary for unlearnable datasets → Medium
- Claim: Orthogonal projection attack is effective for class-wise perturbations → High
- Claim: Orthogonal projection generalizes to sample-wise perturbations → Low

## Next Checks
1. Test the orthogonal projection attack on unlearnable datasets with sample-wise perturbations to assess its effectiveness beyond class-wise scenarios.

2. Evaluate the attack's performance on larger and more diverse datasets (e.g., ImageNet-1k) to determine scalability and generalizability.

3. Investigate the impact of different neural network architectures (e.g., Vision Transformers) on the effectiveness of the orthogonal projection attack.