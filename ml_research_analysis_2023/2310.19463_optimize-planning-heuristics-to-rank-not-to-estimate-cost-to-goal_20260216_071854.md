---
ver: rpa2
title: Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal
arxiv_id: '2310.19463'
source_url: https://arxiv.org/abs/2310.19463
tags:
- heuristic
- search
- states
- optimal
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning heuristic functions
  for optimal planning, particularly for forward search algorithms like A and greedy
  best-first search (GBFS). The authors propose a new approach that focuses on optimizing
  the ranking of states in the search frontier, rather than directly estimating the
  cost-to-goal.
---

# Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal

## Quick Facts
- arXiv ID: 2310.19463
- Source URL: https://arxiv.org/abs/2310.19463
- Reference count: 40
- The proposed ranking-based approach consistently outperforms regression-based methods in planning domains

## Executive Summary
This paper proposes a novel approach to learning heuristic functions for optimal planning by focusing on ranking states in the search frontier rather than estimating cost-to-goal. The authors derive theoretical conditions for strictly optimal efficiency in forward search algorithms (A* and GBFS) that depend on relative state rankings rather than absolute heuristic values. They demonstrate that learning to rank is statistically easier than regression-based approaches, with faster convergence rates from a learning theory perspective. Extensive experiments across eight diverse planning domains show their ranking-based loss functions consistently outperform regression-based approaches in terms of solved problems and expanded states.

## Method Summary
The method involves designing loss functions based on ranking tailored to specific search algorithms. For A*, they propose a loss function (L*) that compares states on and off the optimal path, while for GBFS, they propose a slightly different loss function (Lgbfs). The approach uses logistic loss as a surrogate for the 0-1 ranking loss, training neural networks (CNNs for grid domains, Strips-HGN for PDDL domains) with one problem instance per batch using SGD. The learned heuristics are then evaluated on A* and GBFS search algorithms with 5-10 second time limits per instance.

## Key Results
- Ranking-based loss functions consistently outperform regression-based approaches across all tested domains
- The proposed methods achieve higher fractions of solved problems and require fewer expanded states
- Statistical learning theory shows ranking converges faster than regression (O(√ln np/√np) vs O(√ln ns/√ns))

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A heuristic that perfectly ranks states in the Open list guarantees strictly optimal efficiency in forward search.
- Mechanism: The ranking condition ensures the search always expands states on the optimal path before any off-path states, preventing exploration of suboptimal branches.
- Core assumption: The space of possible heuristic functions is sufficiently large to contain a perfect ranking for the problem.
- Evidence anchors:
  - [abstract]: "They derive necessary and sufficient conditions for a heuristic to guarantee strictly optimal efficiency in forward search, which depends on the relative ranking of states rather than their absolute heuristic values."
  - [section]: "Theorem 1. The forward search with a merit function f(s) = αg(s) + βh(s) and a heuristic h is strictly optimally efficient on a problem instance if and only if h is a perfect ranking on it."
  - [corpus]: Weak - no corpus papers directly support this specific theoretical claim.
- Break condition: If multiple optimal paths exist with different structures, the ranking heuristic must consistently prefer one path or resolve ties appropriately.

### Mechanism 2
- Claim: Learning to rank states is statistically easier than learning to estimate cost-to-goal.
- Mechanism: The ranking problem has fewer constraints (monotonic invariance) and larger solution space than regression, leading to faster convergence rates.
- Core assumption: The excess error convergence rate for ranking (O(√ln np/√np)) is faster than for regression (O(√ln ns/√ns)).
- Evidence anchors:
  - [abstract]: "Furthermore, from a learning theory point of view, it discusses why optimizing cost-to-goal h* is unnecessarily difficult."
  - [section]: "From classical bounds on true error [54] and [7], we can derive (see Appendix for details) that the excess error of ranking loss converges to zero at a rate √ln np/√np, which is slightly faster than that of regression √ln ns/√ns."
  - [corpus]: Weak - no corpus papers provide statistical learning theory evidence for this specific comparison.
- Break condition: When the training set contains conflicting solution paths, the ranking loss may not converge to zero.

### Mechanism 3
- Claim: The proposed ranking loss functions consistently outperform regression-based approaches in planning domains.
- Mechanism: Ranking losses utilize states both on and off the optimal path, avoiding the limitations of regression that only uses optimal path states.
- Core assumption: The neural network architecture can effectively learn the ranking patterns from the training data.
- Evidence anchors:
  - [abstract]: "The results show that their proposed loss functions consistently outperform regression-based approaches in terms of the fraction of solved problems and the number of expanded states."
  - [section]: "Table 1 shows the fraction of solved mazes in percent for all combinations of search algorithms (A* and GBFS) and heuristic functions optimized against compared loss functions. The results match the above theory, as the proposed ranking losses are always better than the L2 regression loss."
  - [corpus]: Weak - no corpus papers directly compare these specific ranking loss functions to regression approaches.
- Break condition: If the problem domain has very different characteristics than the training data, the learned ranking may not generalize well.

## Foundational Learning

- Concept: Statistical learning theory bounds
  - Why needed here: The paper relies on generalization bounds to justify why ranking converges faster than regression
  - Quick check question: What is the key difference between the convergence rates for ranking (O(√ln np/√np)) and regression (O(√ln ns/√ns))?

- Concept: Perfect ranking heuristic
  - Why needed here: The theoretical foundation requires understanding what makes a heuristic "perfect" for strict optimal efficiency
  - Quick check question: Why does the perfect ranking definition only compare states in the Open list, not states on the optimal path?

- Concept: Forward search algorithms (A* and GBFS)
  - Why needed here: The paper proposes different loss functions for different search variants based on their merit function structure
  - Quick check question: How do the α and β parameters in f(s) = αg(s) + βh(s) determine whether we get A* or GBFS behavior?

## Architecture Onboarding

- Component map: Problem state → Neural network → Heuristic value → Search algorithm → Solution path
- Critical path: Problem state → Neural network → Heuristic value → Search algorithm → Solution path
- Design tradeoffs: Ranking vs regression loss (statistical convergence vs goal-awareness), single vs multi-head networks
- Failure signatures: Poor generalization to larger problems, sensitivity to training set conflicts, suboptimal efficiency in A* with GBFS-optimized heuristics
- First 3 experiments:
  1. Compare L* vs L2 on a small grid domain with known optimal paths
  2. Test generalization from 5×5 to 7×7 sliding puzzles
  3. Evaluate sensitivity to training set conflicts by creating multiple solution paths for the same problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific conditions under which ranking-based loss functions consistently outperform regression-based loss functions across different planning domains?
- Basis in paper: [explicit] The authors demonstrate that ranking-based loss functions consistently outperform regression-based approaches in terms of the fraction of solved problems and the number of expanded states.
- Why unresolved: The paper does not provide a detailed analysis of the specific characteristics of planning domains that make ranking-based loss functions more effective.
- What evidence would resolve it: A systematic study comparing the performance of ranking-based and regression-based loss functions across a diverse set of planning domains with varying characteristics (e.g., problem size, branching factor, solution path length) would provide insights into the conditions under which each approach excels.

### Open Question 2
- Question: How does the performance of ranking-based loss functions compare to regression-based loss functions when the training set contains conflicting solution paths?
- Basis in paper: [explicit] The paper mentions that ranking-based loss functions are potentially sensitive to conflicts in the training set, which occur when the training set contains examples of two (or more) different solution paths of the same problem instance.
- Why unresolved: The paper does not provide a detailed analysis of how the performance of ranking-based loss functions is affected by conflicting solution paths in the training set.
- What evidence would resolve it: An experimental study comparing the performance of ranking-based and regression-based loss functions on planning problems with multiple optimal solutions, using training sets with varying degrees of conflict, would provide insights into how each approach handles conflicting solution paths.

### Open Question 3
- Question: What is the theoretical justification for the claim that optimizing ranking is easier than regression from a statistical learning theory perspective?
- Basis in paper: [explicit] The authors argue that learning to rank is easier than regression used in estimating cost-to-goal from a statistical learning theory perspective.
- Why unresolved: The paper does not provide a detailed theoretical analysis of why optimizing ranking is easier than regression.
- What evidence would resolve it: A rigorous theoretical analysis comparing the sample complexity and convergence rates of ranking-based and regression-based loss functions would provide a deeper understanding of the theoretical advantages of ranking-based approaches.

## Limitations
- The theoretical claims about ranking optimality depend heavily on the neural network's capacity to learn perfect rankings
- Experimental comparisons are limited to specific baselines and may not reflect performance against state-of-the-art neural planners
- The 5-10 second time limit per instance may mask performance differences in longer-horizon problems

## Confidence
- Confidence Level: Medium
  - The theoretical claims about ranking optimality are sound within the idealized framework, but their practical applicability depends heavily on the neural network's capacity to learn the perfect ranking function
  - The experimental results show consistent improvements across domains, but the comparisons are somewhat limited
- Confidence Level: Low
  - The single-problem-per-batch training approach raises questions about generalization and convergence properties

## Next Checks
1. Scale generalization test: Train the ranking loss on 5×5 sliding puzzles and evaluate on 7×7 and 8×8 instances to assess whether the neural network can learn generalizable ranking patterns beyond the training scale
2. Conflict resolution experiment: Create training instances with multiple optimal paths of different structures and measure whether the ranking loss can learn to resolve these conflicts, or whether it degrades to random tie-breaking
3. Architecture capacity study: Systematically vary the neural network depth and width for both grid and PDDL domains, measuring the relationship between representational capacity and ranking performance to validate the assumption that sufficient capacity exists to learn perfect rankings