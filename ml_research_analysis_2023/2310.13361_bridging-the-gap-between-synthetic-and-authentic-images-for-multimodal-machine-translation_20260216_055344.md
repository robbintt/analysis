---
ver: rpa2
title: Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine
  Translation
arxiv_id: '2310.13361'
source_url: https://arxiv.org/abs/2310.13361
tags:
- images
- synthetic
- authentic
- translation
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multimodal machine translation
  (MMT), where the model requires an associated image during inference. The authors
  propose using a text-to-image generation model to generate synthetic images from
  source sentences.
---

# Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation

## Quick Facts
- arXiv ID: 2310.13361
- Source URL: https://arxiv.org/abs/2310.13361
- Reference count: 29
- Key outcome: The proposed method achieves state-of-the-art performance on Multi30K by bridging the distribution gap between synthetic and authentic images using Optimal Transport and KL divergence losses.

## Executive Summary
This paper addresses the challenge of multimodal machine translation (MMT) where models require images during inference but most real-world scenarios lack paired images. The authors propose using a text-to-image generation model to create synthetic images from source sentences, but synthetic images often differ in distribution from authentic images, causing performance degradation. To solve this, they minimize the gap between synthetic and authentic images by aligning their input representations using Optimal Transport theory and ensuring output distribution consistency with KL divergence. Experimental results on Multi30K show their method achieves state-of-the-art performance even without authentic images during inference.

## Method Summary
The method trains a multimodal transformer using both synthetic images (generated from source sentences via Stable Diffusion) and authentic images from the Multi30K dataset. It incorporates two consistency losses: an Optimal Transport loss that minimizes the distance between visual representations of synthetic and authentic images, and a KL divergence loss that ensures prediction consistency between the two image types. The visual features are extracted using CLIP, and the model is trained to produce similar translations regardless of whether it receives synthetic or authentic images as input.

## Key Results
- Achieves state-of-the-art BLEU scores on Multi30K test sets for En-De and En-Fr translation tasks
- Successfully bridges the distribution gap between synthetic and authentic images
- Maintains high performance even when only synthetic images are available during inference
- Improves representation and prediction consistency between image types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing the gap between synthetic and authentic image representations using Optimal Transport (OT) theory improves translation quality.
- Mechanism: OT treats visual representations as probability distributions and finds the minimum cost mapping between them, aligning distributions without forcing identical representations.
- Core assumption: Visual representations of synthetic and authentic images can be meaningfully compared as distributions, and aligning them improves model consistency.
- Evidence anchors: [abstract] "leverage the Optimal Transport (OT) theory to mitigate the disparity of the representations"; [section] "measuring the representation similarity between synthetic and authentic images based on the optimal transport distance"
- Break condition: If the cost function in OT doesn't accurately reflect true similarity or if CLIP features encode fundamentally incompatible information for synthetic vs authentic images.

### Mechanism 2
- Claim: KL divergence loss between output distributions ensures prediction consistency when using synthetic vs authentic images.
- Mechanism: KL divergence directly measures the difference in predicted probability distributions over target words when the model is fed synthetic images versus authentic images.
- Core assumption: The model's output distribution is sensitive to image type, and reducing this sensitivity through KL divergence leads to better performance.
- Evidence anchors: [abstract] "Kullback-Leibler (KL) divergence to ensure consistency of output distributions"; [section] "prediction consistency loss, defined as the Kullback-Leibler (KL) divergence between the two probability distributions"
- Break condition: If the model's output is not significantly influenced by image modality or if KL divergence is not the right measure of distributional similarity.

### Mechanism 3
- Claim: Training with both synthetic and authentic images during training bridges the distribution shift at inference.
- Mechanism: Including synthetic images in training helps the model learn to handle both types of images, reducing the mismatch between training (authentic) and inference (synthetic) phases.
- Core assumption: The distribution of synthetic images overlaps sufficiently with authentic images that including them in training provides useful signal.
- Evidence anchors: [abstract] "introduce synthetic images during training and feed synthetic images and authentic images to the MMT model"; [section] "training the MMT model using a combination of synthetic and authentic images"
- Break condition: If synthetic images are too dissimilar from authentic images, training with them could harm performance, or if the model overfits to synthetic patterns that don't generalize.

## Foundational Learning

- Concept: Multimodal machine translation (MMT) and the role of visual context in translation quality.
  - Why needed here: The paper builds on MMT, which integrates visual information into neural machine translation to enhance language understanding.
  - Quick check question: What is the primary motivation for incorporating images into machine translation models?

- Concept: Text-to-image generation models and their limitations in producing images that match the distribution of authentic images.
  - Why needed here: The paper uses Stable Diffusion to create synthetic images, but these often follow different distributions compared to authentic images.
  - Quick check question: Why might synthetic images generated from text descriptions differ in distribution from authentic images paired with the same text?

- Concept: Optimal Transport (OT) theory and its application in aligning distributions.
  - Why needed here: OT is used to measure and minimize the distance between visual representations of synthetic and authentic images, treating them as probability distributions.
  - Quick check question: How does Optimal Transport differ from simple distance metrics like L2 when comparing two distributions?

## Architecture Onboarding

- Component map: Source sentence → CLIP Image Encoder (authentic + synthetic) → FFN → Multimodal Transformer Encoder → Transformer Decoder → Translation output
- Critical path: Source sentence → CLIP Image Encoder (authentic + synthetic) → FFN → Multimodal Transformer Encoder → Transformer Decoder → Translation output. Consistency losses computed in parallel during training.
- Design tradeoffs: Using both synthetic and authentic images increases training data diversity but requires generating synthetic images for each training example. OT and KL losses add computational overhead but improve consistency. CLIP for visual features is convenient but may not be optimal for this task.
- Failure signatures: If model performs similarly with or without images, visual modality may not be effectively integrated. If performance degrades with synthetic images at inference despite training with them, distribution alignment may be insufficient. Large discrepancies between authentic and synthetic image representations indicate OT loss is ineffective.
- First 3 experiments:
  1. Train baseline Multimodal Transformer with only authentic images, then evaluate with synthetic images to establish performance gap.
  2. Train with both synthetic and authentic images (no consistency losses) to see if this alone improves consistency.
  3. Add KL divergence loss to model trained with both image types to measure its impact on prediction consistency.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the limitations and future work implications, several key areas remain unexplored:

- How does the proposed method perform on languages other than English, German, and French, given that text-to-image generation models are primarily trained on English data?
- How does the method handle generation of synthetic images for long or complex source sentences that may be challenging for text-to-image models?
- How does the proposed method compare to other approaches for incorporating visual information in MMT, such as image retrieval or attention mechanisms?

## Limitations

- The method depends heavily on the quality and distribution of synthetic images generated by text-to-image models, which can vary significantly
- Assumes CLIP embeddings provide adequate visual representations for both authentic and synthetic images, but doesn't validate this assumption
- Only evaluated on English-to-German and English-to-French translation tasks, limiting generalizability to other language pairs
- Doesn't analyze how variations in image generation (different prompts, seeds, or models) affect downstream performance

## Confidence

- High Confidence: The core experimental finding that training with synthetic images plus consistency losses improves translation performance compared to using only authentic images during training
- Medium Confidence: The specific mechanism claims about optimal transport and KL divergence being the primary drivers of improvement
- Medium Confidence: The claim of "state-of-the-art" performance on Multi30K benchmark

## Next Checks

1. Conduct a detailed statistical comparison of CLIP embeddings for synthetic versus authentic images to quantify the representation gap before and after applying the OT loss, validating the claimed 100% cosine similarity.

2. Perform a more granular ablation study isolating the effects of OT loss versus KL divergence loss, including intermediate conditions (OT only, KL only) to better understand their relative contributions to performance gains.

3. Evaluate the model's performance when synthetic images are generated with different text-to-image models or with perturbed prompts to assess robustness to variations in synthetic image quality and distribution.