---
ver: rpa2
title: Planning with Logical Graph-based Language Model for Instruction Generation
arxiv_id: '2308.13782'
source_url: https://arxiv.org/abs/2308.13782
tags:
- language
- texts
- logical
- instructional
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating logical and executable
  instructional texts using large language models. The authors propose Logical-GLM,
  which combines language models with AI planning by constructing a logical bayes
  graph to capture domain knowledge.
---

# Planning with Logical Graph-based Language Model for Instruction Generation

## Quick Facts
- arXiv ID: 2308.13782
- Source URL: https://arxiv.org/abs/2308.13782
- Reference count: 11
- One-line primary result: Logical-GLM generates more logical and executable instructions than strong baselines using far fewer parameters.

## Executive Summary
This paper addresses the challenge of generating logical and executable instructional texts using large language models. The authors propose Logical-GLM, which combines language models with AI planning by constructing a logical Bayes graph to capture domain knowledge. An EM-style framework is used where the logical graph and language model guide each other during training. Experiments on the VirtualHome dataset show that Logical-GLM outperforms strong baselines like ChatGPT and GPT2 variants on metrics like BLEU, ROUGE, executability and correctness, despite using far fewer parameters. The method effectively generates more logical instructional texts with better interpretability compared to pure language models.

## Method Summary
Logical-GLM uses an EM-style framework to alternately optimize logical graphs and language models, allowing each to guide the other toward better logic internalization. The logical graph captures domain-specific causal rules from PDDL-structured traces, while the language model generalizes these rules to unseen tasks. During training, generated texts from the logical graph are used to update the language model, which then refines the graph's heuristic search. The method combines a preprocessing module (spaCy parser, K-means clustering, frequency thresholding), a logical Bayes Graph with heuristic search, and a language model trained with cross-entropy loss.

## Key Results
- Logical-GLM outperforms strong baselines like ChatGPT and GPT2 variants on BLEU, ROUGE, executability and correctness metrics
- The method achieves superior performance despite using far fewer parameters than baseline models
- Generated instructions show better interpretability through the usage of logical graphs that reflect the inner mechanism of language models

## Why This Works (Mechanism)

### Mechanism 1
Logical-GLM uses an EM-style framework to alternately optimize logical graphs and language models, allowing each to guide the other toward better logic internalization. The logical graph captures domain-specific causal rules from PDDL-structured traces, while the language model generalizes these rules to unseen tasks. During training, generated texts from the logical graph are used to update the language model, which then refines the graph's heuristic search.

### Mechanism 2
The logical graph search uses a multi-factor heuristic (distance, expected length, Bayes transition probability, language model probability) to select action sequences that are both executable and logically consistent. The heuristic value H combines four terms: hdis (can the current node reach the target?), hlen (estimated program length), hbayes (transition probability from Bayes network), and hlm (language model's grounding probability).

### Mechanism 3
Preprocessing natural language instructions into PDDL-like action traces removes redundant and incorrect information, enabling more accurate logical graph construction. Semantic parsing extracts core words and objects; K-means clusters objects/tasks; action pair frequencies within k steps are thresholded to retain only significant logical relations.

## Foundational Learning

- **PDDL (Planning Domain Definition Language)**
  - Why needed here: Provides a formal, structured representation of actions, objects, and logical relations that can be extracted from natural language instructions
  - Quick check question: What are the two main components of a PDDL action schema, and why are they important for instruction generation?

- **EM (Expectation-Maximization) algorithm**
  - Why needed here: Guides the iterative optimization of logical graphs and language models, allowing each to improve based on the other's output
  - Quick check question: In the context of Logical-GLM, what are the "E-step" and "M-step" equivalent operations?

- **Bayes networks for causal modeling**
  - Why needed here: Represents the transition probabilities between actions in the logical graph, encoding causal dependencies
  - Quick check question: How does assigning transition probabilities to graph edges help guide instruction generation?

## Architecture Onboarding

- **Component map:** Preprocessing module (spaCy parser, K-means clustering, frequency thresholding) -> Logical Bayes Graph (nodes, edges with transition probabilities, heuristic search) -> Language Model (encoder-decoder, trained with cross-entropy loss) -> EM-style training loop (alternating graph search and LM update)

- **Critical path:** 1) Preprocess training instructions into PDDL-like traces 2) Build logical Bayes graph from traces 3) Initialize language model 4) EM loop: Graph search to generate instructions -> Update LM with generated + ground-truth instructions -> Use updated LM to refine graph heuristics 5) Evaluate on test tasks

- **Design tradeoffs:** Smaller graph (fewer nodes/edges) → faster search but less expressive domain logic; Larger language model → better generalization but higher compute cost; More preprocessing steps → cleaner graph but more data prep time; Different heuristic weightings → affects balance between logical consistency, conciseness, and language model fluency

- **Failure signatures:** Instructions fail to execute in simulation → graph lacks necessary preconditions/postconditions; Instructions are illogical or nonsensical → graph search heuristics are misweighted or LM generalization is poor; Training does not converge → EM alternation is unstable or loss function is inappropriate

- **First 3 experiments:** 1) Vary the frequency threshold ϵ for retaining action pairs; measure effect on graph size and instruction executability 2) Disable the language model heuristic (set ω3=0) and compare instruction quality; test if Bayes-only search suffices 3) Use only ground-truth instructions (no generated ones) to train the LM; compare to full EM approach on unseen tasks

## Open Questions the Paper Calls Out

- How can Logical-GLM be extended to interactive decision-making tasks and how can exploration be guided by experience?
- How can planning model learning and plan recognition techniques be integrated with Logical-GLM to generate higher quality instructions?
- How does the program length robustness of Logical-GLM change when constructing logical graphs with longer paths?

## Limitations

- The proposed EM-style framework relies on mutual optimization between logical graphs and language models, but the paper provides limited empirical evidence that this alternation yields better generalization than fixed combinations
- The heuristic weighting (ω1, ω2, ω3) and preprocessing thresholds (k, ϵ) are not extensively validated, creating uncertainty about optimal configurations
- The logical graph construction depends heavily on the quality of PDDL-like preprocessing, which may lose important information if semantic parsing or clustering is imperfect

## Confidence

- Logical-GLM outperforms strong baselines on multiple metrics: Medium
- EM-style alternation between graph and LM improves logic internalization: Low
- Four-factor heuristic effectively guides logical, concise instruction generation: Medium

## Next Checks

1. Conduct ablation studies disabling the language model heuristic (set ω3=0) to test whether Bayes-only search suffices for logical instruction generation
2. Vary the preprocessing frequency threshold ϵ and measure effects on graph size, instruction executability, and logical consistency to validate the sensitivity of the approach to this parameter
3. Test the EM framework's convergence by comparing training with alternating optimization versus training with fixed logical graphs and language models