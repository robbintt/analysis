---
ver: rpa2
title: Knowledge Distillation-based Information Sharing for Online Process Monitoring
  in Decentralized Manufacturing System
arxiv_id: '2302.12004'
source_url: https://arxiv.org/abs/2302.12004
tags:
- data
- network
- knowledge
- manufacturing
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a knowledge distillation-based information
  sharing (KD-IS) framework for enhancing online process monitoring in decentralized
  manufacturing systems. The method enables data-rich units to distill and share knowledge
  with data-poor units, improving monitoring performance while preserving data privacy.
---

# Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System

## Quick Facts
- arXiv ID: 2302.12004
- Source URL: https://arxiv.org/abs/2302.12004
- Reference count: 40
- This study proposes a knowledge distillation-based information sharing (KD-IS) framework for enhancing online process monitoring in decentralized manufacturing systems, achieving 80.5% accuracy and 80.9% F-score in detecting process anomalies.

## Executive Summary
This paper introduces a knowledge distillation-based information sharing framework for online process monitoring in decentralized manufacturing systems. The method enables data-rich units to distill and share knowledge with data-poor units without exchanging raw data, preserving privacy while improving monitoring performance. A case study using two fused filament fabrication printers demonstrates that the proposed approach achieves 80.5% accuracy and 80.9% F-score in detecting process anomalies, outperforming traditional training methods by 4-5%. The framework also shows comparable performance to data-rich models while reducing training time by 25% and eliminating data privacy concerns.

## Method Summary
The KD-IS framework uses knowledge distillation where a teacher network trained on data-rich unit's data produces soft target probabilities. A student network on the data-poor unit is trained using a combined loss function that includes both cross-entropy (for its own data) and KL divergence (to match the teacher's soft targets). The temperature parameter controls the smoothness of probability distributions during distillation. Both networks use identical 1D CNN architectures with window-based vibration sensor data from FFF printers, achieving privacy-preserving knowledge transfer without sharing raw data.

## Key Results
- KD-IS student network achieves 80.5% accuracy and 80.9% F-score in anomaly detection
- 4-5% improvement in accuracy and F-score compared to traditional training methods
- 25% reduction in training time compared to data-rich network baseline
- Comparable performance to data-rich models when teacher and student have identical architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation enables performance transfer from data-rich to data-poor units without sharing raw data
- Mechanism: The teacher model's soft target probabilities capture "dark knowledge" about class relationships, which the student model learns to reproduce using only its own limited data
- Core assumption: Soft targets from the teacher model contain generalizable information about process patterns that transcends exact data distribution matches
- Evidence anchors:
  - [abstract]: "The method enables data-rich units to distill and share knowledge with data-poor units"
  - [section]: "Soft target, i.e., the probability that the input data belongs to each class, is proposed as the logit of teacher network"
  - [corpus]: Weak evidence - corpus mentions federated learning but not knowledge distillation for manufacturing

### Mechanism 2
- Claim: Temperature scaling in softmax allows control over knowledge granularity during distillation
- Mechanism: Higher temperature values produce softer probability distributions that emphasize relative class relationships rather than sharp predictions, making the knowledge more transferable
- Core assumption: Softer probability distributions contain more generalizable information about class relationships than hard labels
- Evidence anchors:
  - [section]: "a higher ð‘‡ð‘‡ produces a softer probability distribution among different classes"
  - [section]: "when the student model is very small compared to the teacher model, lower temperatures work better"
  - [corpus]: No direct evidence found in corpus

### Mechanism 3
- Claim: The KD-IS framework preserves privacy while achieving comparable performance to data-sharing approaches
- Mechanism: By transferring only distilled knowledge (soft targets) rather than raw data, the framework maintains competitive accuracy while eliminating data privacy concerns
- Core assumption: Knowledge distilled from a teacher model can be as effective as direct data sharing for training student models
- Evidence anchors:
  - [abstract]: "The method enables data-rich units to distill and share knowledge with data-poor units, improving monitoring performance while preserving data privacy"
  - [section]: "student network trained with knowledge distillation has a 4%-5% improvement regarding accuracy and F-score"
  - [section]: "student network saves almost 25% of training time compared with benchmark method"

## Foundational Learning

- Concept: Kullback-Leibler (KL) divergence
  - Why needed here: Measures the difference between teacher's soft target distribution and student's output distribution, forming a core part of the loss function
  - Quick check question: What does KL divergence measure between two probability distributions?

- Concept: Softmax temperature scaling
  - Why needed here: Controls the smoothness of probability distributions to make knowledge more transferable across different data distributions
  - Quick check question: How does increasing temperature affect the softmax output distribution?

- Concept: Cross-entropy loss
  - Why needed here: Ensures the student model still learns to correctly classify its own data while mimicking the teacher's knowledge
  - Quick check question: What is the relationship between cross-entropy loss and classification accuracy?

## Architecture Onboarding

- Component map: Data collection -> Teacher training -> Soft target computation -> Student training with distillation loss -> Evaluation
- Critical path: Data collection â†’ Teacher training â†’ Soft target computation â†’ Student training with distillation loss â†’ Evaluation
- Design tradeoffs:
  - Higher temperature improves knowledge transfer but may reduce discriminative power
  - More training data for teacher improves soft target quality but increases computational cost
  - Network architecture complexity affects distillation effectiveness
- Failure signatures:
  - Student performance worse than baseline (no distillation)
  - Training instability or divergence during student training
  - Poor generalization to test data despite good training performance
- First 3 experiments:
  1. Train teacher and student networks without distillation to establish baseline performance
  2. Implement knowledge distillation with temperature=1 to test basic framework functionality
  3. Vary temperature parameter to find optimal setting for knowledge transfer effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the KD-IS framework when all units in the decentralized manufacturing system have limited training data rather than having one data-rich unit?
- Basis in paper: [explicit] The paper mentions this as a limitation in the conclusions section, stating "it is also possible that all units have limited training data. In the future, knowing sharing without data-rich unit will be explored."
- Why unresolved: The current framework assumes the existence of at least one data-rich unit to serve as the teacher model. The effectiveness of knowledge sharing when all units have similar, limited data remains unexplored.
- What evidence would resolve it: Empirical results comparing KD-IS performance across scenarios where all units have limited data versus scenarios with one data-rich unit, using the same experimental setup with FFF printers.

### Open Question 2
- Question: How does the KD-IS framework perform when applied to more complex 3D printing geometries with intricate paths and features?
- Basis in paper: [explicit] The paper notes in the conclusions that "the design of the test parts is relatively simple in this study" and "the robustness of the proposed method to complex printing paths needs further investigation when geometry design becomes more complicated."
- Why unresolved: The current case study uses simple cube geometries, which may not adequately represent the challenges of more complex designs with intricate features and printing paths.
- What evidence would resolve it: Experimental results comparing KD-IS performance on simple versus complex geometries, measuring accuracy and robustness across different part designs with varying complexity.

### Open Question 3
- Question: Can the KD-IS framework be effectively combined with other knowledge sharing approaches like federated learning or multi-task learning, and what are the potential benefits or drawbacks?
- Basis in paper: [explicit] The paper states "The proposed KD-IS framework does not have conflicts with other existing knowledge sharing approaches such as federated learning and multi-task learning" and suggests potential integration.
- Why unresolved: While the paper suggests compatibility with other approaches, it doesn't provide empirical evidence or theoretical analysis of how combining these methods would affect performance.
- What evidence would resolve it: Comparative studies showing performance differences between standalone KD-IS, federated learning, multi-task learning, and hybrid approaches combining these methods, using metrics like accuracy, training time, and data privacy protection.

## Limitations
- Limited to cases where teacher and student models have identical architectures
- Validation only on FFF 3D printing with vibration data from two identical printers
- Fixed temperature setting (T=15) without sensitivity analysis

## Confidence

**High Confidence (Level 1):**
- KD-IS preserves data privacy while transferring knowledge between units
- The framework reduces training time compared to data-rich network training
- Soft targets capture generalizable information when teacher/student data distributions are similar

**Medium Confidence (Level 2):**
- 4-5% improvement in accuracy and F-score compared to traditional training
- Comparable performance to data-rich models when teacher and student have identical architectures
- Temperature scaling at T=15 provides optimal knowledge transfer

**Low Confidence (Level 3):**
- Generalizability to heterogeneous manufacturing systems
- Performance with different model architectures or sensor types
- Robustness across different manufacturing processes and data distributions

## Next Checks

**Validation Check 1: Architecture Heterogeneity Test**
Test KD-IS with different network architectures between teacher and student (e.g., teacher with more layers, different kernel sizes, or attention mechanisms). Measure performance degradation and identify architectural constraints that limit knowledge transfer effectiveness.

**Validation Check 2: Cross-Process Generalization**
Apply KD-IS to a different manufacturing process (e.g., CNC machining with acoustic emissions or thermal imaging). Compare performance against the FFF printing baseline to assess domain transferability and identify process characteristics that enable effective knowledge distillation.

**Validation Check 3: Temperature Sensitivity Analysis**
Systematically vary the temperature parameter T from 1 to 30 across multiple runs and manufacturing scenarios. Map the relationship between temperature values and performance metrics to identify optimal ranges and understand when higher/lower temperatures are beneficial.