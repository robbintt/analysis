---
ver: rpa2
title: 'Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional
  Neuronal Network Processing Pipeline'
arxiv_id: '2304.06793'
source_url: https://arxiv.org/abs/2304.06793
tags:
- ieee
- available
- online
- sensor
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Speck, a System-on-Chip (SoC) integrating a
  128x128 event-based vision sensor with a low-latency, asynchronous spiking convolutional
  neural network (sCNN) processor. The chip is designed for edge computing, enabling
  high-speed, sparse, and energy-efficient visual processing.
---

# Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline

## Quick Facts
- **arXiv ID:** 2304.06793
- **Source URL:** https://arxiv.org/abs/2304.06793
- **Reference count:** 40
- **Key outcome:** 128x128 event-based vision sensor with 9-layer sCNN, 327K neurons, 3.36µs latency per event, 86.17% N-MNIST accuracy (ANN2SNN), 98.56% (BPTT)

## Executive Summary
Speck is a 65nm SoC integrating an event-based vision sensor with an asynchronous spiking convolutional neural network processor. The chip processes incoming pixel events using inverse convolution, mapping each event to multiple output neurons and updating only relevant weights. With 327K neurons across 9 layers, Speck achieves low latency (3.36µs per event) and competitive energy efficiency for edge visual processing tasks.

## Method Summary
The chip uses a 128x128 event-based vision sensor with temporal contrast encoding, feeding events into an asynchronous sCNN processor. The sCNN employs inverse convolution, processing each event by mapping it to output neurons and updating kernel weights in memory. Training uses the Sinabs framework with ANN2SNN and BPTT methods on the N-MNIST dataset (100 epochs, learning rate 1e-3). The asynchronous pipeline uses 4-phase handshake protocols and QDI Dual Rail encoding for low-latency, always-on operation.

## Key Results
- 3.36µs processing latency per event
- 86.17% on-chip accuracy on N-MNIST (ANN2SNN training)
- 98.56% on-chip accuracy on N-MNIST (BPTT training)
- 327K neurons across 9 sCNN layers in 30mm² die area

## Why This Works (Mechanism)

### Mechanism 1
Event-based sensing combined with event-driven sCNN processing enables significant reduction in data transmission and computation latency. By encoding only changes in pixel intensity (events) rather than full frames, the sensor reduces data volume dramatically. The sCNN processor operates inversely—processing each event as it arrives by mapping it to output neurons and updating only relevant weights—minimizing unnecessary computations. This works when event sparsity is high enough that skipping zero multiplications outweighs any overhead from event routing and inverse convolution logic.

### Mechanism 2
Asynchronous, fully handshake-based pipeline design eliminates clock domain synchronization delays and enables low-latency, always-on operation. Each processing block uses 4-phase handshake protocols and Quasi Delay Insensitive (QDI) Dual Rail encoding, allowing blocks to operate immediately when data arrives without waiting for a global clock. This supports continuous low-latency processing and immediate idle state entry when no events occur. The approach works when handshake overhead is negligible compared to savings from removing clock distribution and synchronization logic.

### Mechanism 3
Kernel-based inverse convolution with memory compression enables high synaptic connectivity without proportional memory growth. Instead of storing a full weight matrix, the processor stores only non-zero kernel weights and uses address compression to map events to neurons. This allows the same kernel to be reused across multiple spatial locations, drastically reducing memory requirements while supporting large numbers of synaptic connections. This works when kernels are sparse and repetitive enough that storing and reusing them is more efficient than dense matrix multiplication.

## Foundational Learning

- **Event-based vision sensors (DVS, ATIS)**: Understanding how temporal contrast encoding works is essential to grasp why the system processes sparse event streams instead of frames. *Quick check: What is the primary difference between a frame-based camera and an event-based sensor in terms of data output?*

- **Spiking Neural Networks (SNNs) and inverse convolution**: The sCNN architecture processes events by inverse convolution—mapping each event to multiple neurons—rather than sliding a kernel over a frame. This is central to the design's efficiency. *Quick check: How does inverse convolution differ from standard convolution in terms of data flow and computation?*

- **Asynchronous digital design and QDI logic**: The chip's performance relies on asynchronous handshake protocols and delay-insensitive logic. Without this knowledge, the timing and power benefits are unclear. *Quick check: What is the role of the 4-phase handshake in ensuring hazard-free data flow in an asynchronous pipeline?*

## Architecture Onboarding

- **Component map**: 128x128 event-based vision sensor → sensor preprocessing pipeline → 9 sCNN convolution cores → readout core → output pins
- **Critical path**: Event → sensor interface → preprocessing → kernel mapping in sCNN core → neuron compute → pooling → readout → output. Bottleneck likely in neuron compute or kernel memory access.
- **Design tradeoffs**: Asynchronous vs synchronous eliminates clock skew and enables low-latency idle but adds handshake overhead; kernel memory compression saves area but limits kernel diversity; fixed 9-layer pipeline simplifies design but reduces adaptability.
- **Failure signatures**: High event loss or dropped events (check sensor arbitration or FIFO overflow); unexpected latency spikes (inspect handshake stalls or neuron state update delays); inaccurate classifications (verify kernel weights, neuron thresholds, and readout thresholds); power spikes (monitor when event rate increases).
- **First 3 experiments**: 
  1. Feed synthetic event streams with controlled sparsity into the sCNN cores and measure latency vs event rate to validate inverse convolution efficiency.
  2. Bypass the sensor and inject pre-recorded N-MNIST events directly into the preprocessing pipeline to benchmark classification accuracy and energy per inference.
  3. Stress-test the NoC routing by generating high fan-out event patterns to identify potential bottlenecks or deadlock conditions.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Speck scale when increasing the number of sCNN layers beyond 9? The paper states that Speck supports 9 sCNN layers and discusses its performance in comparison to other processors, but does not provide data on how performance changes with more layers. Experimental data showing latency, energy per inference, and accuracy with different numbers of sCNN layers would resolve this.

### Open Question 2
What is the impact of different kernel sizes on the energy efficiency and latency of Speck? While the paper mentions that Speck supports different kernel sizes (3x3, 4x4, 7x7) and compares its performance with other processors in terms of synaptic connections, it does not provide detailed energy and latency measurements for different kernel sizes. Detailed energy consumption and latency measurements for each kernel size configuration would resolve this.

### Open Question 3
How does the Speck's asynchronous architecture affect its performance in real-time applications with varying input data rates? The paper emphasizes the asynchronous nature of Speck's architecture and its benefits for low latency and energy efficiency, but lacks empirical data on how the chip performs under different input data rates. Experimental results showing latency, throughput, and energy efficiency when processing input data streams with different event rates and patterns would resolve this.

## Limitations
- Lack of quantitative validation for sparsity-driven latency benefits across varying event densities
- Performance comparison with other sCNN processors lacks benchmark consistency
- Memory compression and kernel reuse benefits lack quantitative evidence of actual savings

## Confidence

- **High Confidence**: The chip architecture and processing pipeline design are well-documented and internally consistent
- **Medium Confidence**: The claimed latency of 3.36µs per event is plausible given the described mechanisms but lacks direct experimental validation across varying event densities
- **Low Confidence**: The memory compression and kernel reuse benefits are theoretically sound but lack quantitative evidence showing actual memory savings and their impact on area and energy

## Next Checks

1. Measure sCNN processing latency across a range of event densities (from sparse to dense) to validate the claimed inverse convolution efficiency and identify potential density-dependent performance degradation.
2. Perform a detailed area and energy analysis comparing the kernel-based memory compression approach against dense matrix implementations to quantify the actual savings.
3. Benchmark the chip against other asynchronous sCNN processors using standardized datasets and metrics to provide a more rigorous performance comparison.