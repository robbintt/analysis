---
ver: rpa2
title: 'TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents'
arxiv_id: '2312.01279'
source_url: https://arxiv.org/abs/2312.01279
tags:
- shapley
- which
- value
- text
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TextGenSHAP, a method for generating post-hoc
  explanations of large language models (LLMs) for text generation tasks. It addresses
  the challenge of applying Shapley values to LLMs with long input contexts and autoregressive
  outputs.
---

# TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents

## Quick Facts
- arXiv ID: 2312.01279
- Source URL: https://arxiv.org/abs/2312.01279
- Authors: 
- Reference count: 40
- Primary result: TextGenSHAP reduces explanation generation times from hours to minutes for token-level explanations and to seconds for document-level explanations

## Executive Summary
TextGenSHAP introduces a method for generating scalable post-hoc explanations of large language models for text generation tasks. The approach addresses the challenge of applying Shapley values to LLMs with long input contexts by adapting them to hierarchically-structured input text and autoregressively-decoded outputs. By incorporating transformer-specific architectural modifications including speculative decoding, Flash Attention, and encoder in-place resampling, TextGenSHAP achieves significant speed improvements while maintaining explanation quality for long-document question answering tasks.

## Method Summary
TextGenSHAP adapts Shapley values for text generation by leveraging the hierarchical structure of documents. It employs hierarchical Shapley values (Owen values) to efficiently allocate computation to important document regions, reducing the number of required permutations. The method incorporates three key architectural modifications: speculative decoding to reduce autoregressive sampling calls, Flash Attention with block sparsity for linear memory scaling, and in-place encoder resampling to eliminate redundant encoding computations. These optimizations enable TextGenSHAP to generate explanations for long documents in minutes or seconds rather than hours.

## Key Results
- TextGenSHAP reduces explanation generation time from hours to minutes for token-level explanations and to seconds for document-level explanations
- The method improves long-document question answering performance by localizing important words and sentences
- TextGenSHAP enhances document retrieval systems by identifying relevant content regions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Speculative decoding reduces autoregressive sampling calls by verifying candidate outputs before full generation
- Mechanism: Builds a speculation tree during autoregressive sampling and uses it to predict candidate answers for resampled inputs, avoiding redundant decoder calls when predictions match
- Core assumption: Perturbed inputs closely resemble already decoded samples, allowing high prediction success rates
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 2
- Claim: Flash Attention with block sparsity enables linear memory scaling and hardware alignment for long-context processing
- Mechanism: Replaces quadratic attention computation with block-sparse matrices that scale linearly with input size and align with GPU hardware boundaries
- Core assumption: Block-sparse attention matrices can be constructed without losing significant model accuracy
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 3
- Claim: In-place encoder resampling eliminates redundant encoding computations across different document subsets
- Mechanism: Computes the encoder feature matrix once per example, then modifies only the encoder-decoder attention mechanism to handle different document subsets
- Core assumption: Input fragments are independent enough that encoder features can be reused across different resampling scenarios
- Evidence anchors: [abstract], [section], [corpus]

## Foundational Learning

- Concept: Shapley value computation through permutation sampling
  - Why needed here: Understanding how traditional Shapley values work helps grasp why TextGenSHAP's modifications are necessary for scalability
  - Quick check question: How does permutation sampling approximate the Shapley value, and why does this become computationally prohibitive for long documents?

- Concept: Hierarchical Owen value extension
  - Why needed here: TextGenSHAP uses hierarchical Shapley values to efficiently allocate computation to important document regions
  - Quick check question: How does the Owen value extend Shapley values to hierarchical structures, and why is this beneficial for long documents?

- Concept: Speculative decoding fundamentals
  - Why needed here: Understanding speculative decoding helps explain why TextGenSHAP can achieve such significant speedups
  - Quick check question: What is speculative decoding, and how does it reduce computational overhead in autoregressive generation?

## Architecture Onboarding

- Component map: TextGenSHAP -> Speculative Decoding -> Flash Attention -> In-place Encoder Resampling
- Critical path: The autoregressive decoding loop is the main performance bottleneck, optimized through speculative decoding and hierarchical sampling
- Design tradeoffs: The system trades some precision in Shapley value estimation for significant speed improvements using fewer permutations and hierarchical sampling
- Failure signatures: Poor speculative decoding performance, incorrect hierarchical partitioning, or inefficient block sparsity implementation could degrade performance
- First 3 experiments:
  1. Benchmark TextGenSHAP against baseline Shapley values on a small document QA dataset to verify speedup claims
  2. Test hierarchical sampling thresholds to find optimal balance between speed and accuracy
  3. Compare different block sparsity configurations to maximize Flash Attention performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TextGenSHAP's performance scale when applied to models with contexts exceeding 100K tokens?
- Basis in paper: [inferred] The paper mentions that current architectures can handle contexts up to millions of tokens, but TextGenSHAP is only benchmarked up to 20K tokens
- Why unresolved: The paper does not test TextGenSHAP on extremely long contexts, leaving uncertainty about computational efficiency and accuracy at scale
- What evidence would resolve it: Benchmark results showing TextGenSHAP's runtime and explanation quality on models with 100K+ token contexts

### Open Question 2
- Question: What is the impact of using different decoding algorithms (e.g., nucleus sampling, temperature scaling) on TextGenSHAP's speculative decoding efficiency?
- Basis in paper: [explicit] The paper mentions that speculative decoding can be extended to support other sampling methods but only tests greedy decoding
- Why unresolved: Different decoding algorithms may affect the overlap between speculative and true outputs, potentially changing the speedup benefits
- What evidence would resolve it: Comparative experiments measuring speculative decoding success rates and runtime improvements across multiple decoding strategies

### Open Question 3
- Question: How does TextGenSHAP perform on multilingual or cross-lingual text generation tasks?
- Basis in paper: [inferred] The paper focuses on English datasets but mentions multilingual datasets exist
- Why unresolved: The hierarchical structure and tokenization may behave differently across languages, potentially affecting the Shapley value calculations
- What evidence would resolve it: Experiments applying TextGenSHAP to multilingual question answering datasets and comparing performance across language pairs

### Open Question 4
- Question: What is the optimal threshold for the hierarchical Shapley value to balance computational efficiency and explanation accuracy?
- Basis in paper: [explicit] The paper uses a fixed threshold of 10% for selecting important documents but does not explore sensitivity to this parameter
- Why unresolved: The threshold may need adjustment based on task, model size, or context length
- What evidence would resolve it: Ablation studies varying the threshold parameter and measuring the resulting impact on explanation quality and computation time

## Limitations

- Scalability claims are primarily demonstrated on relatively small document sizes (up to 20K tokens), with true performance on very long documents unverified
- The multiple architectural modifications create a complex system that may be difficult to implement and debug
- The claim of consistent performance improvement in long-document question answering is based on limited experiments and may not generalize to other tasks

## Confidence

- High Confidence: The theoretical foundation of using Shapley values for model explainability is well-established
- Medium Confidence: The specific implementation details for accelerating Shapley value computation are plausible but depend heavily on implementation details and hardware configuration
- Low Confidence: The claim that TextGenSHAP consistently improves long-document question answering performance is based on limited experiments

## Next Checks

1. **Benchmark Against Independent Implementation**: Re-implement TextGenSHAP from the paper's description and compare its performance and accuracy against a baseline Shapley value implementation on standardized datasets (Natural Questions, MIRACL). This would verify the claimed speedups and validate the hierarchical sampling approach.

2. **Ablation Study of Components**: Systematically disable each architectural modification (speculative decoding, Flash Attention, in-place resampling) to determine their individual contributions to overall performance. This would help identify which optimizations are essential versus nice-to-have.

3. **Cross-Domain Generalization Test**: Apply TextGenSHAP to a different text generation task (e.g., summarization, machine translation) with long inputs to verify that the approach generalizes beyond question answering. This would test whether the hierarchical Shapley value approach is task-agnostic or QA-specific.