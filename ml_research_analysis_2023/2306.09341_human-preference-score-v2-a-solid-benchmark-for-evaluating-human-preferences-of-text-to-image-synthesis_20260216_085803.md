---
ver: rpa2
title: 'Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences
  of Text-to-Image Synthesis'
arxiv_id: '2306.09341'
source_url: https://arxiv.org/abs/2306.09341
tags:
- images
- dataset
- prompts
- human
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Human Preference Dataset v2 (HPD v2), the
  largest dataset for human preference annotations on text-to-image generation outputs.
  It contains 798k pairwise comparisons on 430k images from 9 diverse generative models
  and COCO Captions, with bias-mitigated prompts.
---

# Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis

## Quick Facts
- arXiv ID: 2306.09341
- Source URL: https://arxiv.org/abs/2306.09341
- Reference count: 40
- Primary result: HPS v2 achieves 83.3% accuracy on HPD v2 test set, outperforming prior metrics like ImageReward and PickScore

## Executive Summary
This paper introduces Human Preference Dataset v2 (HPD v2) and Human Preference Score v2 (HPS v2), addressing the critical need for reliable evaluation of text-to-image generation models. HPD v2 is the largest human preference dataset with 798k pairwise comparisons across 9 diverse generative models, while HPS v2 is a CLIP-based model fine-tuned on this dataset that achieves 83.3% accuracy in predicting human preferences. The paper also establishes a comprehensive benchmark with 3200 carefully designed evaluation prompts across four styles, enabling fair and stable automatic evaluation without user studies.

## Method Summary
The authors fine-tune the CLIP-H model on HPD v2 using KL-divergence loss to predict human preferences in pairwise comparisons. The training uses 4k steps with AdamW optimizer, batch size of 128, and learning rate of 3.3e-6. Prompts are cleaned using ChatGPT to remove style-word bias, and the model is evaluated on held-out prompts and images from the training distribution. HPS v2 scores are computed as the mean similarity between prompt and image embeddings, with standard deviation reported across multiple prompt groups to ensure stability.

## Key Results
- HPS v2 achieves 83.3% accuracy on HPD v2 test set, outperforming prior metrics (ImageReward, HPS, PickScore)
- Generalizes better across model distributions compared to existing preference predictors
- Validated as sensitive to algorithmic improvements in text-to-image generation
- Establishes a benchmark with 3200 prompts across 4 styles (Animation, Concept-art, Painting, Photo)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP-based finetuning on human preference pairs yields a more generalizable preference predictor than prior reward models.
- Mechanism: Finetuning the CLIP encoder to map prompt-image similarity to a probability that the image is preferred over alternatives, using pairwise preference labels. This captures semantic alignment between prompt and image, not just visual quality.
- Core assumption: CLIP's cross-modal alignment can be adapted to model human aesthetic preference through pairwise ranking loss.
- Evidence anchors:
  - [abstract] "By fine-tuning CLIP on HPD v2, we obtain Human Preference Score v2 (HPS v2), a scoring model that can more accurately predict human preferences..."
  - [section 4.1] Training details show CLIP-H [14] finetuned with KL-divergence loss on pairwise data.
  - [corpus] Weak. The neighbor papers describe similar CLIP-based approaches but don't benchmark against this exact setup.
- Break condition: If CLIP's text encoder fails to capture nuanced stylistic or compositional aspects of human preference, or if the preference labels are noisy.

### Mechanism 2
- Claim: A diverse image source distribution (9 generative models + COCO Captions) improves generalization across unseen models.
- Mechanism: By including images from models with different architectures (GAN, autoregressive, diffusion) and scales, the model learns preference patterns not tied to a single training distribution.
- Core assumption: Preference patterns are largely model-agnostic, and training on a wider distribution reduces overfitting to any one model.
- Evidence anchors:
  - [abstract] "HPD v2 incorporates images generated from 9 recent text-to-image generative models..."
  - [section 3.2] Table 2 shows the image sources and diversity of models.
  - [corpus] Weak. The corpus papers don't provide ablation studies on source diversity.
- Break condition: If certain model families (e.g., GANs vs diffusion) have fundamentally different preference patterns that are not captured by CLIP's embedding space.

### Mechanism 3
- Claim: Cleaning prompts with ChatGPT reduces style-word bias, leading to fairer evaluation across models trained on different data.
- Mechanism: Removing high-frequency style words ("Greg Rutkowski", "ArtStation") and organizing prompts into clear sentences prevents models from exploiting dataset-specific cues.
- Core assumption: Model performance on biased prompts does not reflect general text-to-image generation capability.
- Evidence anchors:
  - [abstract] "The style words are also highly biased, leading to issues in training and evaluation. To tackle this prompt bias, we employ ChatGPT to remove style words..."
  - [section 3.1] Examples show before/after prompt cleaning and reduction in style-word frequency.
  - [corpus] Weak. Neighbor papers mention bias but don't detail prompt cleaning procedures.
- Break condition: If ChatGPT's cleaning removes important descriptive elements or introduces its own bias.

## Foundational Learning

- Concept: Cross-modal representation learning (CLIP)
  - Why needed here: The core model is a finetuned CLIP; understanding how CLIP maps text and images into a shared space is essential to grasp how preferences are predicted.
  - Quick check question: What is the shape and type of the embeddings produced by CLIP's text and image encoders?

- Concept: Pairwise ranking loss
  - Why needed here: The training objective is to rank two images conditioned on the same prompt; knowing how KL-divergence is used for ranking is key to understanding the model's optimization.
  - Quick check question: How does the KL-divergence loss in Eq. 3 enforce that the preferred image gets a higher score?

- Concept: Bias in evaluation prompts
  - Why needed here: The paper argues that certain style words bias evaluation; understanding this is critical for interpreting benchmark results.
  - Quick check question: What is an example of a style word that could bias evaluation, and why does it matter?

## Architecture Onboarding

- Component map:
  CLIP backbone (text encoder + image encoder) -> Pairwise ranking head (softmax + KL-divergence loss) -> Prompt cleaning pipeline (ChatGPT-based preprocessing) -> Benchmark prompt set (3200 prompts across 4 styles)

- Critical path:
  1. Load CLIP model
  2. Clean prompts (remove style words, rephrase)
  3. Load image pairs and preference labels
  4. Compute CLIP similarity scores for each image given prompt
  5. Apply softmax to get preference probabilities
  6. Optimize KL-divergence loss

- Design tradeoffs:
  - Using CLIP (pretrained, efficient) vs. training from scratch (more flexible but data-hungry)
  - Cleaning prompts (reduces bias, but may lose some descriptive nuance)
  - Training on 9 models (better generalization, but more complex data pipeline)

- Failure signatures:
  - Overfitting to CLIP's bias (e.g., favoring certain image styles)
  - Low accuracy on HPD v2 test set (poor generalization)
  - Large variance in benchmark scores (unstable evaluation)

- First 3 experiments:
  1. Run inference on a small subset of HPD v2 test pairs to sanity-check CLIP finetuning.
  2. Ablation: Compare HPS v2 vs. vanilla CLIP Score on a held-out prompt set.
  3. Test generalization: Evaluate HPS v2 on images from a model not seen during training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do human preferences for text-to-image generation outputs vary across different demographic groups, and how might this affect the generalizability of HPS v2?
- Basis in paper: [inferred] The paper mentions that the dataset was annotated by 57 annotators, but does not provide demographic information about these annotators. It also notes that "human preference is highly diverse" and that "there might be bias induced by the limited number of annotators."
- Why unresolved: The paper does not provide demographic information about the annotators, nor does it analyze how preferences might vary across different demographic groups. This information would be crucial for understanding the generalizability of the model across different user populations.
- What evidence would resolve it: Collecting demographic information about annotators (with their consent) and analyzing preference patterns across different demographic groups would provide insight into potential biases and generalizability issues.

### Open Question 2
- Question: How do the performance and generalization capabilities of HPS v2 compare to other preference prediction models when evaluated on prompts and images from text-to-image generation models not included in the training data?
- Basis in paper: [explicit] The paper states that "HPS v2 exhibits a better accuracy on both benchmarks, demonstrating its strong capability of generalization." However, it also notes that "previous datasets mainly contain images generated from Stable Diffusion and its variants" and that "their generalization capabilities are not yet validated."
- Why unresolved: While the paper demonstrates that HPS v2 outperforms other models on its own test set, it does not provide a comprehensive comparison of generalization capabilities across a wider range of text-to-image generation models not included in the training data.
- What evidence would resolve it: Conducting experiments to evaluate the performance of HPS v2 and other preference prediction models on a diverse set of text-to-image generation outputs from models not included in the training data would provide a clearer picture of their relative generalization capabilities.

### Open Question 3
- Question: How do the design choices for the evaluation prompts (e.g., number of prompts, balance across styles) affect the stability and fairness of the HPS v2 benchmark?
- Basis in paper: [explicit] The paper states that "800 prompts are divided into groups of 80, and HPS v2 is computed on each group. The mean and standard deviation of the HPS v2 scores are then reported. We chose the amount of 800 to ensure that HPS v2 is statistically stable across all evaluated models, while also avoiding excessive computational overhead."
- Why unresolved: While the paper provides reasoning for the choice of 800 prompts, it does not explore how different numbers of prompts or different balances across styles might affect the stability and fairness of the benchmark.
- What evidence would resolve it: Conducting experiments with different numbers of prompts and different balances across styles, and analyzing the resulting stability and fairness of the benchmark, would provide insight into the optimal design choices for the evaluation prompts.

## Limitations

- Limited generalization validation: Claims about cross-model generalization are based on held-out prompts from training models, not entirely unseen architectures
- Dataset composition bias: 88.4% of training samples come from diffusion-based models, potentially limiting fair evaluation of other architectures
- Unknown prompt cleaning impact: No quantitative comparison of performance with/without ChatGPT prompt cleaning

## Confidence

- **High confidence**: Technical implementation of CLIP finetuning and KL-divergence loss
- **Medium confidence**: Accuracy improvements over prior metrics (given dataset overlap concerns)
- **Low confidence**: Claims about bias reduction and true cross-model generalization

## Next Checks

1. **Cross-architecture validation**: Evaluate HPS v2 on text-to-image models from architectures not represented in HPD v2 training data (e.g., latent diffusion vs autoregressive models).

2. **Prompt cleaning ablation**: Train and evaluate HPS v2 variants with and without ChatGPT prompt cleaning to quantify the impact on accuracy and bias.

3. **Dataset composition analysis**: Create balanced subsets of HPD v2 across model types and measure how performance varies with training distribution shifts.