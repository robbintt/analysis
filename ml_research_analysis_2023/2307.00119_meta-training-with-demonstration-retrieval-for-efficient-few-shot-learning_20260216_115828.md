---
ver: rpa2
title: Meta-training with Demonstration Retrieval for Efficient Few-shot Learning
arxiv_id: '2307.00119'
source_url: https://arxiv.org/abs/2307.00119
tags:
- tasks
- meta-training
- demonstrations
- question
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a retrieval-augmented meta-training approach
  that combines a dense passage retriever with a BART-large model to retrieve semantically
  similar labeled demonstrations for each input during both meta-training and fine-tuning.
  The method constructs a diverse demonstration memory bank from multiple QA tasks
  and trains the model to generate answers given retrieved demonstrations.
---

# Meta-training with Demonstration Retrieval for Efficient Few-shot Learning

## Quick Facts
- arXiv ID: 2307.00119
- Source URL: https://arxiv.org/abs/2307.00119
- Reference count: 29
- Key outcome: Retrieval-augmented meta-training approach combining DPR with BART-large achieves 83.9-89.2 F1 on QA tasks and 72.9-84.4 on classification tasks, outperforming FewshotQA and RAG baselines.

## Executive Summary
This paper introduces a retrieval-augmented meta-training approach that combines dense passage retrieval with a BART-large model to retrieve semantically similar labeled demonstrations for each input during both meta-training and fine-tuning. The method constructs a diverse demonstration memory bank from multiple QA tasks and trains the model to generate answers given retrieved demonstrations. On single-GPU few-shot learning tasks including extractive QA (SQuAD), multiple-choice QA (QASC), and classification (TREC, MNLI, QNLI), the approach achieves strong performance across all task types.

## Method Summary
The approach uses meta-training to teach a BART-large model how to effectively leverage retrieved demonstrations for few-shot learning. During meta-training, the model learns to generate answers given an input question and a set of retrieved demonstrations from a memory bank containing examples from 16 QA tasks. The dense passage retriever (DPR) finds semantically similar demonstrations based on maximum inner product search, and the BART component generates answers using cross-entropy loss. This trained model is then fine-tuned on few-shot examples for specific downstream tasks, maintaining the retrieval-augmented generation process throughout.

## Key Results
- Achieves F1 scores of 83.9-89.2 on extractive QA tasks (SQuAD, BioASQ) and 82.6-85.9 on multiple-choice QA tasks
- Outperforms FewshotQA (68.9-82.6 F1) and RAG (80.0-88.9 F1) baselines on QA tasks
- Achieves 72.9-84.4 accuracy on classification tasks (TREC, MRPC, MNLI, QNLI)
- Demonstrates superior performance of meta-training over standard parameter-efficient approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Meta-training with retrieved demonstrations teaches the model to effectively leverage demonstrations for few-shot learning.
- Mechanism: The model learns a learned retrieval-augmented generation process where it uses DPR to find semantically similar demonstrations and BART to generate answers conditioned on these demonstrations, improving its ability to generalize from few examples.
- Core assumption: The model can learn to effectively combine retrieved demonstrations with input to generate correct answers during meta-training, and this learned behavior transfers to few-shot settings.
- Evidence anchors: [abstract] "By separating external knowledge from model parameters, we can use meta-training to train parameter-efficient models that generalize well on a larger variety of tasks."

### Mechanism 2
- Claim: Using a diverse demonstration memory bank provides varied supervision that improves generalization across different tasks.
- Mechanism: The memory bank contains labeled examples from 16 different QA tasks, allowing the model to learn from a wide range of question types and answer formats during meta-training.
- Core assumption: Exposure to diverse demonstration types during meta-training improves the model's ability to handle varied downstream tasks with few examples.
- Evidence anchors: [abstract] "We construct a meta-training set from UNIFIED QA and CROSS FIT, and propose a demonstration bank based on UNIFIED QA tasks."

### Mechanism 3
- Claim: The combination of meta-training and retrieval augmentation is more effective than either component alone.
- Mechanism: Meta-training teaches the model to use demonstrations effectively, while retrieval provides task-specific context; together they enable better few-shot performance than parameter-efficient methods or retrieval-augmented models without meta-training.
- Core assumption: The synergy between learned demonstration utilization (from meta-training) and task-specific retrieval provides multiplicative benefits over individual components.
- Evidence anchors: [abstract] "Our approach outperforms a variety of targeted parameter-efficient and retrieval-augmented few-shot methods on QA, NLI, and text classification tasks"

## Foundational Learning

- Concept: Dense Passage Retrieval (DPR)
  - Why needed here: DPR is used to retrieve semantically similar demonstrations from the memory bank based on maximum inner product search between encoded inputs and demonstrations.
  - Quick check question: How does DPR determine which demonstrations are most relevant to a given input?

- Concept: Sequence-to-Sequence Modeling with BART
  - Why needed here: BART is used as the generator component that produces answers given the input and retrieved demonstrations, trained with cross-entropy loss.
  - Quick check question: What is the role of marginalizing over retrieved demonstrations in the generation process?

- Concept: Meta-learning/Meta-training
  - Why needed here: Meta-training teaches the model to learn from demonstrations across multiple tasks, improving its few-shot generalization ability.
  - Quick check question: How does meta-training differ from standard multi-task learning in this context?

## Architecture Onboarding

- Component map: DPR retriever → BART generator → demonstration memory bank (UnifiedQA tasks) → meta-training collection (18 QA tasks from MetaICL)
- Critical path: Input → DPR encoding → demonstration retrieval → BART generation with marginalization → output
- Design tradeoffs: Larger memory banks provide more diverse demonstrations but increase retrieval time; more retrieved demonstrations improve performance but increase computational cost
- Failure signatures: Poor retrieval relevance (demonstrations don't match inputs), meta-training not improving over random initialization, format mismatch between meta-training and evaluation tasks
- First 3 experiments:
  1. Verify DPR retrieves relevant demonstrations by checking if retrieved examples contain answer substrings
  2. Test BART generation with 0, 1, 5, 10 demonstrations to find optimal number
  3. Compare meta-training on all QA tasks vs. semantically similar tasks to validate subsampling approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between diversity and relevance in retrieved demonstrations for different task types?
- Basis in paper: [inferred] The paper shows DPR (Wiki) performs better than DPR (PAQ) and Contriever despite retrieving less relevant demonstrations, suggesting diversity matters. The authors note that completely random retrieval is not effective, but neither is highly constrained retrieval.
- Why unresolved: The paper does not provide a quantitative framework for balancing diversity and relevance, nor does it explore how this balance might vary across different task types (QA vs classification, knowledge-intensive vs non-knowledge-intensive).
- What evidence would resolve it: Systematic experiments varying retrieval diversity while measuring downstream performance across different task types, potentially using diversity metrics like distinct n-grams in retrieved demonstrations.

### Open Question 2
- Question: How do semantic priming effects from retrieved demonstrations contribute to performance improvements on classification tasks where labels are not present in the demonstrations?
- Basis in paper: [explicit] The authors note that retrieval-augmented models outperform FewshotQA by a large margin on MNLI(-mm), but question why this happens since label space is small and labels are less informative.
- Why unresolved: The paper only speculates about semantic priming effects without providing empirical evidence or identifying specific features in demonstrations that contribute to this effect.
- What evidence would resolve it: Controlled experiments ablating specific features from retrieved demonstrations (e.g., question text, answer text, context text) to measure their individual contributions to classification performance.

### Open Question 3
- Question: What is the theoretical upper bound for demonstration retrieval performance, and how far are current methods from this bound?
- Basis in paper: [explicit] The authors create an oracle memory using labeled test examples and show significant performance improvements, indicating room for improvement in the memory bank.
- Why unresolved: The paper only establishes one point on the performance curve (oracle memory) without systematically exploring what aspects of demonstrations contribute most to performance gains.
- What evidence would resolve it: Experiments varying the quality, relevance, and coverage of demonstration memories while measuring performance gaps relative to oracle performance, potentially revealing key bottlenecks in current approaches.

## Limitations

- The effectiveness of meta-training depends on the assumption that demonstrations retrieved during meta-training will be semantically similar to evaluation examples, but this relationship is not empirically validated.
- The computational overhead of maintaining and querying a large demonstration memory bank during both training and inference phases is not quantified, which could impact practical deployment.
- The generalization claims across different task types are based on results from a limited set of tasks, and the paper does not provide extensive ablation studies on different task combinations or demonstration bank sizes.

## Confidence

- **High Confidence**: The mechanism of using DPR for demonstration retrieval and BART for generation is technically sound and well-established. The experimental setup using standard few-shot learning benchmarks provides a reliable evaluation framework.
- **Medium Confidence**: The claim that meta-training improves few-shot performance is supported by ablation studies, but the exact contribution of meta-training versus the retrieval mechanism is not clearly separated. The reported improvements over baselines like FewshotQA and RAG are promising but may depend on specific implementation details.
- **Low Confidence**: The generalization claims across different task types (QA, classification, NLI) are based on results from a limited set of tasks, and the paper does not provide extensive ablation studies on different task combinations or demonstration bank sizes.

## Next Checks

1. **Demonstration Retrieval Quality**: Conduct a qualitative analysis of retrieved demonstrations to verify semantic relevance to evaluation inputs, measuring the percentage of retrieved demonstrations that contain answer substrings or are topically related to the query.

2. **Ablation on Meta-training Data**: Test the model with meta-training on semantically dissimilar tasks to determine if the semantic similarity subsampling is crucial for performance, comparing against random task selection.

3. **Computational Overhead Analysis**: Measure and report the computational cost (time and memory) of maintaining the demonstration memory bank and performing retrieval during both meta-training and inference, comparing against parameter-efficient baselines.