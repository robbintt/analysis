---
ver: rpa2
title: 'Embedding structure matters: Comparing methods to adapt multilingual vocabularies
  to new languages'
arxiv_id: '2309.04679'
source_url: https://arxiv.org/abs/2309.04679
tags:
- script
- languages
- language
- ident
- full
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates methods for adapting multilingual vocabularies
  to new languages by replacing large cross-lingual vocabularies with compact, language-specific
  ones. The authors propose several simple techniques for re-initializing token embedding
  matrices after vocabulary specialization, based on script-wise sub-distributions
  observed in the XLM-R embedding space.
---

# Embedding structure matters: Comparing methods to adapt multilingual vocabularies to new languages

## Quick Facts
- arXiv ID: 2309.04679
- Source URL: https://arxiv.org/abs/2309.04679
- Reference count: 12
- This paper investigates methods for adapting multilingual vocabularies to new languages by replacing large cross-lingual vocabularies with compact, language-specific ones.

## Executive Summary
This paper explores efficient methods for adapting multilingual models like XLM-R to new languages by replacing their large cross-lingual vocabularies with smaller, specialized ones. The authors propose simple embedding re-initialization techniques based on script-wise and position-wise sub-distributions observed in the pre-trained embedding space. They compare these methods to the FOCUS approach, which uses an auxiliary embedding model for semantic similarity. Through extensive experiments on 8 diverse languages across two tasks (POS tagging and NER), the study demonstrates that vocabulary replacement with proper embedding initialization provides an efficient way to improve performance in low-resource languages, with script-based initialization techniques performing on par with the more complex FOCUS method.

## Method Summary
The study focuses on adapting multilingual models by replacing their original vocabularies with language-specific ones and re-initializing the embedding matrices. The authors train new SentencePiece tokenizers on target language data (32,770 vocabulary size) and implement several embedding re-initialization strategies: REINIT-IDENT (copy overlapping tokens), REINIT-SCRIPT (initialize based on script-wise mean and std dev), REINIT-POSN (initialize based on word-initial vs word-medial sub-distributions), and FOCUS (similarity-based initialization using FastText). They conduct Language-Adaptive Pre-Training (LAPT) for 100k steps with masked language modeling, comparing frozen vs trainable transformer layers, and evaluate on POS tagging and NER tasks.

## Key Results
- Simple script-based initialization techniques (REINIT-SCRIPT+POSN+IDENT) perform on par with the more complex FOCUS method
- LAPT with trainable transformer layers consistently outperforms frozen variants across all tested languages and initialization strategies
- Replacing cross-lingual vocabularies with specialized ones provides efficient improvements for low-resource languages, with computational benefits over full vocabulary LAPT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Script- and position-wise sub-distribution initialization preserves pre-trained embedding structure better than random initialization
- Mechanism: When replacing vocabularies, new embeddings initialized based on the mean and standard deviation of existing script and position clusters align more closely with the frozen transformer's expectations
- Core assumption: The XLM-R transformer encoder has learned to expect different embedding subspaces for different scripts and token positions
- Evidence anchors:
  - [abstract] "Simple embedding re-initialization techniques based on script-wise sub-distributions rival techniques such as FOCUS"
  - [section 3.1] "we see that the highest-resource scripts in XLM-R (Common, Latin, and Cyrillic) have relatively divergent distributions"
  - [corpus] Weak evidence - no direct citations, but related works suggest similar sub-distribution alignment approaches
- Break condition: If the frozen transformer doesn't actually learn script/position distinctions, or if new scripts have fundamentally different distributional properties

### Mechanism 2
- Claim: Training the transformer layers during LAPT is crucial for downstream performance
- Mechanism: The transformer learns to adapt to new embeddings during LAPT, rather than expecting the original pre-trained distribution
- Core assumption: The transformer encoder can adapt its processing to new embedding distributions given sufficient training
- Evidence anchors:
  - [section 5] "LAPT-FULL always outperforms LAPT-EMB. I.e. training with trainable transformer layers outperforms training with frozen ones"
  - [abstract] "We demonstrate that: 1) Embedding-replacement techniques in the monolingual transfer literature are inadequate for adapting multilingual models"
  - [corpus] Moderate evidence - related work shows freezing vs unfreezing makes difference in transfer learning
- Break condition: If the transformer becomes too brittle to adapt to new distributions, or if catastrophic forgetting overwhelms adaptation

### Mechanism 3
- Claim: Vocabulary replacement with LAPT provides efficient specialization for low-resource languages
- Mechanism: By creating a compact, language-specific vocabulary and embedding matrix, the model focuses computational resources on relevant tokens while retaining cross-lingual knowledge
- Core assumption: Low-resource languages are poorly covered by large cross-lingual vocabularies, and computational efficiency matters for practical deployment
- Evidence anchors:
  - [abstract] "Replacing cross-lingual vocabularies with smaller specialized ones provides an efficient method to improve performance in low-resource languages"
  - [section 5] "LAPT with the full XLM-R vocabulary is much more computationally costly than training new vocabulary"
  - [corpus] Moderate evidence - efficiency-focused adaptation methods are common in low-resource NLP
- Break condition: If the efficiency gains don't translate to practical deployment benefits, or if specialization loses too much cross-lingual transfer ability

## Foundational Learning

- Concept: PCA visualization for embedding space analysis
  - Why needed here: Understanding the structure of pre-trained embedding spaces is crucial for designing effective re-initialization strategies
  - Quick check question: Can you explain what the principal components represent in a PCA plot of embeddings?

- Concept: Tokenization and vocabulary design in multilingual models
  - Why needed here: The study focuses on replacing vocabularies, requiring understanding of how subword tokenizers work across languages
  - Quick check question: How do SentencePiece/BPE tokenizers handle scripts they haven't seen during training?

- Concept: Catastrophic forgetting in transfer learning
  - Why needed here: The paper contrasts freezing vs training transformer layers, which relates to forgetting pre-trained knowledge
  - Quick check question: What mechanisms can prevent catastrophic forgetting when fine-tuning pre-trained models?

## Architecture Onboarding

- Component map: XLM-R (transformer encoder + embedding matrix) -> LAPT training loop (MLM) -> Downstream task fine-tuning (POS/NER) -> Evaluation

- Critical path:
  1. Train new vocabulary on target language data
  2. Initialize new embedding matrix using chosen strategy
  3. Conduct LAPT (100k steps with evaluation checkpoints)
  4. Select best checkpoint based on MLM loss
  5. Fine-tune on downstream task
  6. Evaluate and report metrics

- Design tradeoffs:
  - Efficiency vs performance: Full vocabulary LAPT gives slightly better results but costs 2-3x more FLOPs
  - Complexity vs effectiveness: Simple script-based initialization rivals complex similarity-based methods
  - Training stability vs adaptation: Freezing layers prevents forgetting but limits adaptation capability

- Failure signatures:
  - Poor initialization: Random embeddings perform 20+ points worse than any structured method
  - Overfitting: If dev set MLM loss doesn't improve after 10k steps with frozen layers
  - Catastrophic forgetting: If performance on high-resource languages drops significantly during LAPT

- First 3 experiments:
  1. Monolingual LAPT with REINIT-RANDOM on Armenian (very low-resource) to establish baseline
  2. Monolingual LAPT with REINIT-SCRIPT+POSN+IDENT on Estonian (high-resource) to test upper bound
  3. Multilingual LAPT with REINIT-FOCUS on Uralic family to test cross-lingual adaptation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do embedding re-initialization techniques perform when adapting to higher-level tasks like natural language inference or question answering in low-resource languages?
- Basis in paper: [inferred] The paper notes that evaluation is limited to POS tagging and NER due to lack of standard datasets for higher-level tasks in under-resourced languages.
- Why unresolved: The paper only evaluates on low-level tasks due to data availability constraints, leaving the effectiveness of techniques on complex tasks unknown.
- What evidence would resolve it: Experiments applying the same vocabulary replacement and embedding re-initialization techniques to models trained on NLI, QA, or other high-level tasks with newly curated datasets for low-resource languages.

### Open Question 2
- Question: Do the observed benefits of script-wise and position-wise embedding initialization generalize to other multilingual models beyond XLM-R?
- Basis in paper: [explicit] The authors note that all experiments are conducted using XLM-R as the base model and suggest future work should test other model types.
- Why unresolved: The paper only tests one multilingual model architecture, so it's unclear if the embedding structure patterns and initialization benefits are universal.
- What evidence would resolve it: Systematic experiments applying the same initialization techniques to other multilingual models like mBERT, mT5, or larger-scale models like BLOOM, comparing performance across architectures.

### Open Question 3
- Question: Does training with related languages provide additional benefits for very low-resource languages beyond what vocabulary replacement and embedding re-initialization achieve?
- Basis in paper: [explicit] The authors note that Erzya (very low-resource) performs better with Uralic family multilingual training than with unrelated languages, but this pattern doesn't hold for Sami.
- Why unresolved: The paper shows mixed evidence - related languages help Erzya but not clearly Sami, suggesting the relationship between language relatedness and transfer benefits needs further investigation.
- What evidence would resolve it: Controlled experiments varying language relatedness in training data while keeping vocabulary replacement and embedding initialization constant, across multiple very low-resource languages to identify patterns.

## Limitations
- Limited experimental scope covering only 8 languages from diverse families with uneven resource levels
- Evaluation methodology focuses on task performance without deeper analysis of what the model actually learns during LAPT
- Fixed 32,770 vocabulary size across all languages may not be optimal for languages with very different morphological complexity

## Confidence
- High confidence: LAPT with trainable transformer layers consistently outperforms frozen variants across all tested languages and initialization strategies
- Medium confidence: Relative performance rankings of different initialization strategies, with small differences and limited sample size
- Low confidence: Claims about computational efficiency and practical deployment benefits, lacking actual wall-clock training times and memory requirements

## Next Checks
1. **Generalization to extreme low-resource conditions**: Replicate the main experiments with genuinely low-resource languages (â‰¤100K training tokens) to test whether observed patterns hold when the target language data is severely limited.

2. **Cross-lingual transfer analysis**: After LAPT specialization, test whether models retain or improve cross-lingual transfer ability by evaluating on zero-shot or few-shot transfer tasks involving the specialized language.

3. **Ablation of initialization components**: Systematically test which aspects of the REINIT-SCRIPT+POSN+IDENT method are essential by ablating individual components and measuring performance degradation.