---
ver: rpa2
title: Minimal Random Code Learning with Mean-KL Parameterization
arxiv_id: '2307.07816'
source_url: https://arxiv.org/abs/2307.07816
tags:
- mean-kl
- variational
- mean-var
- weight
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of compressing variational Bayesian
  neural networks while maintaining predictive performance and robustness. The core
  method, Minimal Random Code Learning (MIRACLE), uses relative entropy coding to
  compress weight samples from a variational posterior distribution, but traditionally
  requires computationally expensive KL divergence annealing.
---

# Minimal Random Code Learning with Mean-KL Parameterization

## Quick Facts
- arXiv ID: 2307.07816
- Source URL: https://arxiv.org/abs/2307.07816
- Authors: 
- Reference count: 9
- This paper proposes Mean-KL parameterization to eliminate computationally expensive KL divergence annealing in MIRACLE compression while maintaining predictive performance.

## Executive Summary
This paper addresses the challenge of compressing variational Bayesian neural networks while maintaining predictive performance and robustness. The core method, Minimal Random Code Learning (MIRACLE), uses relative entropy coding to compress weight samples from a variational posterior distribution, but traditionally requires computationally expensive KL divergence annealing. The authors propose Mean-KL parameterization, which constrains the KL divergence by construction rather than through annealing, allowing the optimization to focus directly on minimizing distortion. Experiments on MNIST with a LeNet-5 architecture show that Mean-KL parameterization converges in half the number of iterations compared to conventional mean-variance parameterization while maintaining comparable classification accuracy.

## Method Summary
The paper proposes Mean-KL parameterization as an alternative to conventional mean-variance (Mean-Var) parameterization for variational distributions in Minimal Random Code Learning (MIRACLE). While Mean-Var requires KL divergence annealing during training to balance distortion and compression costs, Mean-KL directly constrains the KL divergence through mathematical construction. The parameterization uses τw (inverse temperature) and γw (prior shift) parameters to recover mean and variance via the Lambert W function, ensuring the KL constraint |μw - ν| < ρ√(2κw) is satisfied. This eliminates the need for annealing and allows optimization to focus solely on minimizing distortion. The method is evaluated on MNIST using LeNet-5 architecture with local information budgets of 20 bits per weight block.

## Key Results
- Mean-KL parameterization converges in half the number of iterations compared to Mean-Var (Figure 2)
- Maintains comparable classification accuracy (0.77-0.96% error vs 0.79-0.87% for Mean-Var across different compression ratios)
- Mean-KL produces heavier-tailed distributions leading to compressed weight samples that maintain 80% accuracy after 30% random pruning (vs 50% for Mean-Var)
- Shows particular strength when pruning based on KL divergence to Dirac delta, maintaining 90% accuracy after pruning 90% of weights (vs random guessing for Mean-Var)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mean-KL parameterization eliminates the need for KL divergence annealing during training.
- Mechanism: By directly constraining the KL divergence through the parameterization (Equation 2 and 3), the optimization focuses solely on minimizing the distortion term rather than balancing it against the KL term.
- Core assumption: The constraint |μw - ν| < ρ√(2κw) can be satisfied for all weights given appropriate initialization and training.
- Break condition: If the constraint cannot be satisfied for some weights (e.g., due to extreme initialization), the parameterization breaks down and training fails.

### Mechanism 2
- Claim: Mean-KL parameterization produces variational distributions with heavier tails.
- Mechanism: The Mean-KL parameterization (Equation 3) allows for a wider range of variances compared to Mean-Var, leading to distributions that are not sharply peaked at zero.
- Core assumption: Heavier tails in the variational distribution lead to more robust compressed weight samples.
- Break condition: If the optimization drives all weights toward zero variance, the tails disappear and the mechanism fails.

### Mechanism 3
- Claim: Mean-KL produces compressed weight samples that are more robust to pruning.
- Mechanism: The heavier-tailed distributions learned by Mean-KL create compressed samples where the remaining weights after pruning still contain meaningful information, rather than collapsing to random guessing.
- Core assumption: The pruning strategies (random, absolute value, KL divergence) effectively test different aspects of distribution quality.
- Break condition: If pruning removes critical weights regardless of the distribution shape, robustness gains disappear.

## Foundational Learning

- Concept: Relative Entropy Coding (REC)
  - Why needed here: REC is the compression mechanism that encodes weight samples from the variational posterior, with the KL divergence determining the coding cost.
  - Quick check question: What determines the number of samples K drawn from Pw during REC encoding?

- Concept: KL Divergence Annealing
  - Why needed here: Understanding why traditional Mean-Var requires annealing helps appreciate why Mean-KL is beneficial - it bypasses this expensive step.
  - Quick check question: In Mean-Var, what two terms are being balanced during optimization, and why is annealing needed?

- Concept: Lambert W Function
  - Why needed here: The Lambert W function appears in the Mean-KL parameterization formula (Equation 3) to recover variance from the KL constraint.
  - Quick check question: What is the defining equation that the Lambert W function satisfies?

## Architecture Onboarding

- Component map:
  Variational posterior Qw (Gaussian, parameterized by Mean-KL) -> Coding distribution Pw (Gaussian, fixed during training) -> Relative Entropy Coding (REC) encoder/decoder -> LeNet-5 model on MNIST -> Block partitioning for tractable K values

- Critical path:
  1. Initialize Mean-KL parameters (τw and γw)
  2. During training, convert to (µw, σw²) for density evaluation
  3. Sample from Qw, encode using REC
  4. Evaluate cross-entropy loss on MNIST
  5. Backpropagate through the entire pipeline

- Design tradeoffs:
  - Mean-KL vs Mean-Var: Faster convergence vs. slightly higher error at high compression ratios
  - Block size: Larger blocks reduce overhead but increase K exponentially
  - Information quota allocation: How to distribute C across weights (currently via softmax)

- Failure signatures:
  - Training diverges: Check if constraint |μw - ν| < ρ√(2κw) is being violated
  - Poor compression: Verify that DKL[Qw||Pw] ≈ C after training
  - Accuracy drops after compression: Check if block partitioning is too coarse

- First 3 experiments:
  1. Verify faster convergence: Compare iteration count to reach same cross-entropy with Mean-Var vs Mean-KL
  2. Test KL constraint satisfaction: After training, measure actual DKL[Qw||Pw] and compare to target C
  3. Validate pruning robustness: Apply random pruning at 30% and compare accuracy between parameterizations

## Open Questions the Paper Calls Out
1. How does Mean-KL parameterization perform when scaled to larger neural network architectures beyond LeNet-5 on MNIST?
2. What is the theoretical relationship between the heavier tails of Mean-KL distributions and their improved robustness to pruning?
3. Can the pruning strategy based on KL divergence to a Dirac delta be generalized to other types of pruning criteria?

## Limitations
- The Mean-KL parameterization assumes that the constraint |μw - ν| < ρ√(2κw) can always be satisfied, but this may not hold for extreme weight values or poor initialization choices
- The paper demonstrates results only on MNIST with LeNet-5 architecture; generalization to more complex datasets and deeper architectures remains untested
- Block partitioning strategy and information budget allocation across blocks is simplified, which may not scale well to larger networks with heterogeneous layer importance

## Confidence
- High confidence: Faster convergence of Mean-KL compared to Mean-Var (supported by convergence plots showing half the iterations)
- Medium confidence: Improved robustness to pruning with Mean-KL (based on pruning experiments but limited to three specific strategies)
- Medium confidence: Heavier-tailed distributions from Mean-KL parameterization (visual evidence from Figure 1 but no quantitative tail analysis)

## Next Checks
1. Test Mean-KL on CIFAR-10 with a deeper CNN architecture to verify scalability beyond MNIST/LeNet-5
2. Quantitatively measure KL divergence constraint satisfaction throughout training to confirm the parameterization consistently enforces DKL[Qw||Pw] ≈ C
3. Perform ablation study removing block partitioning to isolate whether robustness gains come from the Mean-KL parameterization itself or from the compression strategy