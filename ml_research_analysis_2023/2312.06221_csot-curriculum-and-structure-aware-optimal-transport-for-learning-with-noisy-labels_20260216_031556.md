---
ver: rpa2
title: 'CSOT: Curriculum and Structure-Aware Optimal Transport for Learning with Noisy
  Labels'
arxiv_id: '2312.06221'
source_url: https://arxiv.org/abs/2312.06221
tags:
- learning
- labels
- csot
- diag
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses learning with noisy labels (LNL), a challenging
  task where models can easily overfit to corrupted labels. Existing approaches rely
  heavily on model predictions and ignore the global and local structure of the sample
  distribution.
---

# CSOT: Curriculum and Structure-Aware Optimal Transport for Learning with Noisy Labels

## Quick Facts
- arXiv ID: 2312.06221
- Source URL: https://arxiv.org/abs/2312.06221
- Reference count: 40
- Key outcome: Novel OT formulation that considers global and local sample distribution structure for robust label denoising and relabeling

## Executive Summary
This paper addresses the challenge of learning with noisy labels (LNL), where models can easily overfit to corrupted labels. Existing approaches rely heavily on model predictions and ignore the global and local structure of the sample distribution. The authors propose Curriculum and Structure-aware Optimal Transport (CSOT), which considers both inter- and intra-distribution structure of samples to construct a robust denoising and relabeling allocator. CSOT incrementally assigns reliable labels to samples with highest confidence, ensuring global discriminability and local coherence.

## Method Summary
CSOT introduces a novel optimal transport formulation that simultaneously considers global sample distribution structure through class centroids and local coherence through regularized terms. The method uses a curriculum approach that starts with a small budget of high-confidence samples and gradually increases coverage during training. The optimization employs an efficient scaling iteration method within a generalized conditional gradient framework to solve the non-convex objective. Training involves two stages: supervised learning on clean samples followed by semi-supervised learning using the denoised dataset.

## Key Results
- Achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and WebVision datasets with various noise rates
- Up to 3.7x speedup compared to vanilla Dykstra's algorithm for solving the OT problem
- Maintains computational efficiency even with large batch sizes up to 3000 samples
- Demonstrates superior robustness to symmetric and pair noise across different noise ratios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CSOT improves label correction by considering both global and local sample distribution structure
- **Mechanism:** By introducing local coherent regularized terms ΩP and ΩL, CSOT encourages assigning larger weights to nearby samples with similar predictions or labels, preventing nearby samples from being mapped to distant class centroids
- **Core assumption:** The intrinsic coherence structure within the sample distribution can be captured through cosine similarity in feature space
- **Evidence anchors:** [abstract], [section 4.1], [corpus]

### Mechanism 2
- **Claim:** Curriculum constraints prevent early-stage error accumulation by incrementally selecting high-confidence samples
- **Mechanism:** CSOT relaxes the equality constraint to allow only a fraction of samples with highest confidence to be selected initially, with the budget m progressively increasing during training
- **Core assumption:** Early in training, the model's predictions are unreliable, so we should only trust samples it's most confident about
- **Evidence anchors:** [abstract], [section 4.2], [corpus]

### Mechanism 3
- **Claim:** Efficient scaling iteration makes CSOT computationally tractable for large-scale problems
- **Mechanism:** The proposed lightspeed computational method uses matrix-vector multiplications instead of matrix-matrix multiplications, achieving up to 3.7x speedup over vanilla Dykstra's algorithm
- **Core assumption:** The OT problem with curriculum constraints can be reformulated to allow efficient iterative solutions
- **Evidence anchors:** [section 5.2], [section 6.3], [corpus]

## Foundational Learning

- **Concept: Optimal Transport (OT)**
  - Why needed here: CSOT is fundamentally an OT formulation that maps samples to class centroids while minimizing transport cost
  - Quick check question: What is the primary objective of the classical OT formulation in equation (1)?

- **Concept: Curriculum Learning**
  - Why needed here: The curriculum budget m controls how many samples get labeled at each training stage, preventing overfitting to early mistakes
  - Quick check question: How does the curriculum budget m change during training according to section 4.2?

- **Concept: Regularization in OT**
  - Why needed here: The entropic regularization term ε ⟨Q, log Q⟩ makes the OT problem computationally tractable via Sinkhorn algorithm
  - Quick check question: What role does the entropic regularization weight ε play in the OT formulation?

## Architecture Onboarding

- **Component map:** Noisy training set -> Feature extractor -> CSOT solver -> Coupling matrix -> Clean and corrupted datasets -> Mixup + consistency + self-supervised losses -> Final model

- **Critical path:**
  1. Compute softmax predictions P and cosine similarity matrix S
  2. Solve CSOT optimization problem to get coupling matrix Q
  3. Derive pseudo labels and confidence scores from Q
  4. Split dataset into clean and corrupted subsets
  5. Train model using mixup, label consistency, and self-supervised losses

- **Design tradeoffs:**
  - Higher κ values increase local coherence but may introduce incorrect consistency under high noise
  - Smaller ε values produce sharper pseudo labels but may reduce computational efficiency
  - Larger curriculum budget m increases coverage but risks including unreliable samples

- **Failure signatures:**
  - If clean accuracy plateaus early, the curriculum budget may be too conservative
  - If corrected accuracy decreases, the local coherent regularization may be too strong
  - If training time increases significantly, the CSOT solver parameters may need adjustment

- **First 3 experiments:**
  1. Verify CSOT solves correctly on small synthetic dataset with known ground truth
  2. Compare clean and corrected accuracy curves against baseline methods
  3. Test sensitivity to κ and ε hyperparameters on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CSOT scale with extremely large batch sizes (e.g., 100k samples) and a high number of classes (e.g., 1000+ classes) in terms of both accuracy and computational efficiency?
- Basis in paper: [inferred] The paper demonstrates efficiency gains on moderate batch sizes (up to 3000 samples) but does not explore extremely large-scale scenarios.
- Why unresolved: The scalability of the efficient scaling iteration method to massive datasets and class spaces remains untested, particularly regarding memory constraints and GPU limitations.
- What evidence would resolve it: Experimental results showing accuracy and runtime metrics for CSOT on datasets with significantly larger batch sizes and class counts, compared to existing methods.

### Open Question 2
- Question: Can the local coherent regularization terms (ΩP and ΩL) be further generalized to incorporate higher-order neighborhood structures or graph-based relationships beyond pairwise cosine similarity?
- Basis in paper: [explicit] The paper uses pairwise cosine similarity for local coherence but acknowledges the potential for more sophisticated neighborhood modeling.
- Why unresolved: The choice of similarity metric and the scope of local coherence are not fully explored, leaving room for alternative formulations that might capture richer structural information.
- What evidence would resolve it: Comparative experiments evaluating CSOT variants with different neighborhood definitions (e.g., k-NN graphs, attention mechanisms) and their impact on denoising and relabeling performance.

### Open Question 3
- Question: How does CSOT perform under extreme label noise rates (e.g., 95%+) where even the assumption of a significant fraction of clean labels may break down?
- Basis in paper: [inferred] The paper shows robustness up to 90% noise but does not test the limits of the method under near-total label corruption.
- Why unresolved: The curriculum budget and confidence-based selection may become unreliable when the signal-to-noise ratio is extremely low, potentially requiring alternative strategies.
- What evidence would resolve it: Experiments on datasets with artificially injected noise rates exceeding 90%, comparing CSOT to methods specifically designed for extreme noise scenarios.

### Open Question 4
- Question: Is there a theoretical guarantee on the convergence rate or optimality of the GCG algorithm used to solve the non-convex CSOT objective, and how does it compare to other optimization approaches?
- Basis in paper: [inferred] The paper presents the GCG algorithm but does not provide theoretical analysis of its convergence properties for the specific CSOT formulation.
- Why unresolved: While empirical results show good performance, the lack of theoretical guarantees limits understanding of the algorithm's behavior and potential failure modes.
- What evidence would resolve it: A formal proof of convergence (e.g., to a stationary point) for the GCG algorithm on the CSOT objective, along with empirical comparisons to alternative solvers like projected gradient descent.

### Open Question 5
- Question: Can CSOT be extended to handle multi-label classification tasks where each sample may belong to multiple classes simultaneously?
- Basis in paper: [inferred] The current formulation assumes single-label classification, but the underlying optimal transport framework could potentially be adapted for multi-label scenarios.
- Why unresolved: Multi-label learning introduces additional complexity in defining the coupling constraints and cost matrix, requiring modifications to the current CSOT formulation.
- What evidence would resolve it: A modified version of CSOT that handles multi-label cases, validated on benchmark datasets like PASCAL VOC or MS-COCO, showing improved performance over existing noisy label methods for multi-label tasks.

## Limitations
- Computational efficiency claims lack absolute timing benchmarks and scalability analysis for extremely large datasets
- Empirical evaluation focuses on standard benchmark datasets without testing on more diverse data distributions
- Does not explore performance under extreme label noise rates (95%+) where fundamental assumptions may break down
- Lacks theoretical guarantees on convergence rate or optimality of the GCG algorithm for the specific CSOT formulation

## Confidence
- **High confidence:** The core OT formulation combining global transport cost with local coherent regularization is mathematically sound and the algorithmic framework is well-defined
- **Medium confidence:** The curriculum constraint mechanism and its gradual budget increase appear reasonable, though sensitivity to specific parameter choices could affect performance
- **Low confidence:** The claims about computational efficiency relative to existing OT-based LNL methods lack absolute timing comparisons and scalability analysis

## Next Checks
1. Verify the efficient scaling iteration implementation against the original Dykstra's algorithm on synthetic OT problems with known solutions
2. Test CSOT's robustness to different noise patterns (instance-dependent noise, asymmetric noise) beyond the symmetric and pair noise evaluated
3. Conduct ablation studies on the curriculum budget schedule to determine optimal m0 and ramp-up rates for different noise ratios