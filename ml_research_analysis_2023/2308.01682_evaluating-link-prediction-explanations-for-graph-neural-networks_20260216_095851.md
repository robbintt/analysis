---
ver: rpa2
title: Evaluating Link Prediction Explanations for Graph Neural Networks
arxiv_id: '2308.01682'
source_url: https://arxiv.org/abs/2308.01682
tags:
- graph
- explanations
- link
- prediction
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces quantitative metrics for evaluating explanations
  in link prediction (LP) tasks with Graph Neural Networks (GNNs). The authors propose
  two validation approaches: one using ground-truth synthetic datasets (SBM, WS) and
  another using insertion/deletion curves for real-world datasets (CORA, PubMed, DDI)
  where ground truth is unavailable.'
---

# Evaluating Link Prediction Explanations for Graph Neural Networks

## Quick Facts
- arXiv ID: 2308.01682
- Source URL: https://arxiv.org/abs/2308.01682
- Reference count: 40
- Primary result: Integrated Gradients generally performs best for link prediction explanations, while decoder choice significantly impacts explanation quality

## Executive Summary
This paper addresses the challenge of evaluating explanations for link prediction tasks in Graph Neural Networks (GNNs). The authors propose two complementary validation approaches: ground truth synthetic datasets (SBM, WS) for precise metric calculation, and insertion/deletion curves for real-world datasets without ground truth. They evaluate four explainability methods (GNNExplainer, Integrated Gradients, Deconvolution, LRP) across two GNN architectures (VGAE, GIN) and two decoders (Inner Product, Cosine Distance). Results show that Integrated Gradients consistently outperforms other methods, while GNNExplainer suffers from hyperparameter sensitivity. The study also reveals that decoder choice significantly impacts explanation quality, with cosine similarity causing issues due to normalization.

## Method Summary
The authors train GNN models (VGAE and GIN encoders) with two different decoders (inner product and cosine similarity) on both synthetic and real-world datasets. For synthetic datasets (SBM, WS), they compute sensitivity and specificity metrics using ground truth edge importance. For real-world datasets (CORA, PubMed, DDI), they use insertion/deletion curves where edges are added/removed based on attribution scores and model performance is measured. Four explainability methods are evaluated: GNNExplainer (perturbation-based), Integrated Gradients (gradient-based), Deconvolution, and LRP (decomposition-based). The area under the insertion/deletion curve serves as a quantitative metric when ground truth is unavailable.

## Key Results
- Integrated Gradients consistently outperforms other explainers across most datasets and architectures
- GNNExplainer shows high sensitivity to hyperparameters, particularly the mask penalty parameter
- Inner product decoders produce better explanations than cosine similarity decoders due to normalization issues
- GIN models leverage network structure more effectively than VGAE models, as evidenced by higher area scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ground truth synthetic datasets (SBM, WS) enable precise validation of explanation quality
- Mechanism: Controlled generation allows defining binary masks for important edges/features, enabling calculation of confusion matrices and metrics like sensitivity/specificity
- Core assumption: Trained models reflect the generative logic of synthetic graphs
- Evidence anchors:
  - [abstract]: "We first perform a validation of the explanation methods on synthetic datasets such as Stochastic Block Models and Watts-Strogatz graphs, where we can define the ground truth for the explanations and thus compute the confusion matrices"
  - [section]: "In these experiments, whether an edge is present or not is clearly defined by the generative model"
- Break condition: If the model learns spurious correlations not present in the generative model, ground truth validation becomes invalid

### Mechanism 2
- Claim: Area score metrics provide quantitative comparison of explanations without ground truth
- Mechanism: Insertion/deletion curves measure how model output changes when features/edges are added/removed by attribution importance, with area scores comparing to random baseline
- Core assumption: Good explanations assign high scores to features/edges that most impact model predictions
- Evidence anchors:
  - [abstract]: "For real-world datasets with no ground-truth (CORA, PubMed and DDI) we exploit an adaptation of the insertion/deletion curves"
  - [section]: "The area under the curve of the fraction of features inserted/removed versus the output of the model provides a quantitative evaluation of the explanation"
- Break condition: If model output is insensitive to feature/edge changes, insertion/deletion curves provide no meaningful signal

### Mechanism 3
- Claim: Decoder choice significantly impacts explanation quality
- Mechanism: Different similarity measures (inner product vs cosine) affect how masking features/edges changes model output, with normalization in cosine causing degenerate solutions
- Core assumption: Explanation methods rely on gradients/mutual information that depend on decoder behavior
- Evidence anchors:
  - [abstract]: "We discuss how underlying assumptions and technical details specific to the link prediction task, such as the choice of distance between node embeddings, can influence the quality of the explanations"
  - [section]: "The similarity measure used in the decoder significantly impacts the explanation quality"
- Break condition: If explanation method is completely decoder-agnostic, choice of similarity measure won't matter

## Foundational Learning

- Concept: Graph Neural Networks fundamentals
  - Why needed here: Understanding message passing, node embeddings, and encoder/decoder architecture is essential for grasping how explanations work
  - Quick check question: What is the difference between node classification and link prediction in GNNs?

- Concept: Explainability methods taxonomy
  - Why needed here: The paper evaluates methods from perturbation, gradient-based, and decomposition classes
  - Quick check question: How do perturbation-based explainers differ from gradient-based explainers?

- Concept: Evaluation metrics for explanations
  - Why needed here: Understanding confusion matrices, sensitivity/specificity, and area scores is crucial for interpreting results
  - Quick check question: What does an area score of 0.8 versus 0.2 indicate about an explainer's performance?

## Architecture Onboarding

- Component map: Data preparation (graph, node features, train/test split) -> Model training (encoder + decoder) -> Attribution (4 explainers) -> Evaluation (ground truth metrics or area scores)

- Critical path: Data → Train Model → Generate Explanations → Evaluate → Compare

- Design tradeoffs:
  - Synthetic vs real data: Ground truth vs real-world relevance
  - Decoder choice: Inner product allows better explanations than cosine similarity
  - Explainer type: Model-agnostic (GNNExplainer) vs model-aware (gradient-based)

- Failure signatures:
  - GNNExplainer producing sparse masks when explanations should be dense
  - Area scores near zero indicating explainer no better than random
  - High sensitivity but low specificity suggesting many false positives

- First 3 experiments:
  1. Run GNNExplainer on synthetic SBM data and check sensitivity/specificity
  2. Compare inner product vs cosine decoder using IG explainer on same model
  3. Calculate area scores for all explainers on CORA dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can feature-based explanations be integrated with edge-based explanations in link prediction tasks?
- Basis in paper: [inferred] The paper notes that "The integration of feature and edge explanation scores, often overlooked in GML XAI, is a promising area for future research."
- Why unresolved: Current explainability methods typically focus on either node features or edges separately, but real-world link prediction often depends on both. There's no established methodology for combining these two types of explanations.
- What evidence would resolve it: Development and empirical validation of integrated explanation methods that provide unified explanations incorporating both features and edges, tested across multiple datasets and GNN architectures.

### Open Question 2
- Question: How do distance encoding and hyperbolic encoding features affect explanation quality in link prediction?
- Basis in paper: [explicit] "Regarding the LP frameworks that incorporate features such as distance encoding and hyperbolic encoding, we believe that there should be a community-wide discussion about how such features can be incorporated in the proposed explanations."
- Why unresolved: Current attribution methods cannot assign explanations to these advanced encoding features, limiting their applicability in modern GML pipelines.
- What evidence would resolve it: Modified explainability methods that can attribute importance scores to distance-encoded and hyperbolic-encoded features, with quantitative evaluation on benchmark datasets.

### Open Question 3
- Question: What is the optimal validation protocol for link prediction explainability when ground truth is unavailable?
- Basis in paper: [explicit] "Regarding the validation of explanation and explainers, few works have considered the study and evaluation of GML explainers for LP" and the authors propose insertion/deletion curves as a solution.
- Why unresolved: The paper proposes area scores based on insertion/deletion curves, but this remains an open question whether this is the optimal approach compared to potential alternatives.
- What evidence would resolve it: Comparative studies evaluating different validation protocols (insertion/deletion, counterfactuals, perturbation-based) on their ability to distinguish between good and bad explanations across diverse datasets.

## Limitations
- Ground truth synthetic datasets may not fully capture the complexity of real-world link prediction scenarios
- Area score metrics rely on insertion/deletion curves which may not capture all aspects of explanation quality
- GNNExplainer shows high sensitivity to hyperparameters, potentially making results unstable

## Confidence
- High: The methodology for synthetic dataset validation with ground truth metrics (sensitivity/specificity)
- Medium: The comparison between different explainers and their relative performance rankings
- Low: The generalizability of findings to larger, more complex real-world graphs beyond the tested datasets

## Next Checks
1. Test explanation methods on additional real-world datasets with different graph characteristics to assess robustness
2. Implement cross-validation for GNNExplainer hyperparameters to quantify sensitivity effects
3. Compare area score metrics against alternative explanation evaluation methods like perturbation robustness testing