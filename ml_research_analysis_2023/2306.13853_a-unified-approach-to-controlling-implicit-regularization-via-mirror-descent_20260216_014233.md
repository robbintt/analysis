---
ver: rpa2
title: A Unified Approach to Controlling Implicit Regularization via Mirror Descent
arxiv_id: '2306.13853'
source_url: https://arxiv.org/abs/2306.13853
tags:
- descent
- mirror
- implicit
- loss
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the implicit regularization of mirror descent\
  \ (MD) for linear classification with strictly monotone losses like logistic and\
  \ exponential losses. While prior work showed that gradient descent (GD) converges\
  \ to the $\\ell2$-max-margin solution, and MD generalizes GD\u2019s implicit bias\
  \ to other geometries in regression, it was unknown whether MD does the same in\
  \ classification."
---

# A Unified Approach to Controlling Implicit Regularization via Mirror Descent

## Quick Facts
- **arXiv ID**: 2306.13853
- **Source URL**: https://arxiv.org/abs/2306.13853
- **Reference count**: 40
- **Primary result**: Mirror descent with homogeneous potentials converges in direction to generalized max-margin solutions for strictly monotone losses in linear classification.

## Executive Summary
This paper establishes that mirror descent (MD) with homogeneous potential functions converges in direction to a generalized maximum-margin solution for linear classification with strictly monotone losses like logistic and exponential losses. While gradient descent is known to converge to the ℓ₂-max-margin solution, this work generalizes that implicit bias to arbitrary geometries through MD. The authors prove both direction convergence and provide convergence rate analysis for different step size schemes, including a polynomial rate for normalized MD. Experiments with linear and deep models demonstrate that different choices of the potential function induce implicit regularization with respect to different norms, leading to varying generalization performances.

## Method Summary
The paper studies mirror descent for linear classification with homogeneous potential functions ψ(w) = 1/p ‖w‖^p_p for p > 1 (p-GD). The method updates weights using the mirror map ∇ψ and Bregman divergence D_ψ. The key insight is that homogeneity ensures the Bregman divergence telescopes properly, allowing analysis of direction convergence. The authors prove that p-GD converges in direction to a generalized max-margin solution ur_ψ. They also introduce normalized MD (N-MD) with time-varying step sizes η_t = η₀√(t+1)/L(w_t) that achieves polynomial convergence rates. Experiments cover synthetic 2D and 100D datasets, MNIST with fully-connected and convolutional architectures, and CIFAR-10/ImageNet.

## Key Results
- Mirror descent with homogeneous potentials converges in direction to generalized maximum-margin solutions for strictly monotone losses
- Normalized mirror descent achieves polynomial convergence rates, faster than poly-logarithmic rates of fixed-step MD
- Different p-values in p-GD induce implicit regularization with respect to different norms, leading to different generalization performances
- p=3 consistently outperforms other values on CIFAR-10 and ImageNet, while p=1.1 leads to sparser but worse generalizing models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Mirror descent with homogeneous potential functions converges in direction to a generalized maximum-margin solution for linear classification with strictly monotone losses.
- **Mechanism**: The homogeneity of the potential function ensures that the Bregman divergence is also homogeneous, allowing the algorithm to "chase" a moving target (a scaled version of the regularized direction) during each iteration. This moving target approach compensates for the lack of a finite minimizer in strictly monotone losses.
- **Core assumption**: The potential function is β-absolutely homogeneous and strictly convex.
- **Evidence anchors**: Abstract states MD converges in direction to generalized maximum-margin solution; homogeneous potentials ensure Bregman divergence telescopes properly.

### Mechanism 2
- **Claim**: Normalized mirror descent with time-varying step sizes achieves a polynomial convergence rate to the maximum-margin solution, faster than the poly-logarithmic rate of fixed-step MD.
- **Mechanism**: By scaling the gradient update by the inverse of the loss, the algorithm implicitly adapts the step size to the progress of optimization. This normalization accelerates convergence by preventing iterates from growing too slowly in early stages.
- **Core assumption**: The potential function satisfies Assumption 1 with β < 3 and is continuously twice differentiable.
- **Evidence anchors**: Abstract mentions poly-logarithmic or polynomial convergence rates under different step size schemes; normalized update rule scales by 1/L(w_t).

### Mechanism 3
- **Claim**: Different choices of the homogeneous potential function (i.e., different values of p in p-GD) induce implicit regularization with respect to different norms, leading to models with different generalization performances.
- **Mechanism**: The choice of p in p-GD determines the geometry of implicit regularization. Smaller p values lead to sparser models (more weights near zero), while larger p values lead to models with smaller maximum weights. This implicit bias affects the model's capacity and generalization ability.
- **Core assumption**: The dataset is linearly separable, and the loss function is strictly monotone.
- **Evidence anchors**: Abstract demonstrates MD produces learned models with different regularizers; experiments show p=3 outperforms other values on CIFAR-10/ImageNet.

## Foundational Learning

- **Concept: Bregman Divergence**
  - Why needed here: Bregman divergence is the core "distance" measure used in mirror descent, replacing the Euclidean distance in gradient descent. Understanding its properties (e.g., additivity, homogeneity) is crucial for analyzing the convergence of MD.
  - Quick check question: What is the Bregman divergence associated with the potential function ψ(w) = 1/2‖w‖²²?
    - Answer: The Bregman divergence is 1/2‖w - w'‖²², which is the squared Euclidean distance.

- **Concept: Homogeneous Potential Functions**
  - Why needed here: The homogeneity of the potential function ensures that the implicit regularization of MD is with respect to a generalized geometry (not just the ℓ₂ norm). This allows MD to induce implicit biases for a wider class of learning problems.
  - Quick check question: What is the Minkowski functional of a potential function ψ that is β-absolutely homogeneous?
    - Answer: The Minkowski functional is ‖w‖_ψ = inf{c > 0 : ψ(w/c) ≤ 1}, which defines a norm.

- **Concept: Implicit Regularization**
  - Why needed here: Implicit regularization is the phenomenon where optimization algorithms favor certain solutions over others, even without explicit regularization. Understanding implicit regularization is crucial for explaining the generalization performance of over-parameterized models.
  - Quick check question: What is the implicit regularization of gradient descent for linear classification with logistic loss?
    - Answer: Gradient descent converges in direction to the ℓ₂-maximum margin solution.

## Architecture Onboarding

- **Component map**:
  - Potential function ψ → Mirror map ∇ψ → Bregman divergence D_ψ → Step size schedule → Loss function → Weight updates

- **Critical path**:
  1. Choose a homogeneous potential function ψ
  2. Compute the mirror map ∇ψ
  3. Initialize the weight vector w₀
  4. For each iteration t: compute gradient ∇L(wt), update using MD rule, optionally normalize by loss
  5. Monitor convergence to maximum-margin solution

- **Design tradeoffs**:
  - Choice of p in p-GD: Different p values lead to different implicit biases and generalization performances (smaller p → sparser models, larger p → smaller maximum weights)
  - Step size schedule: Fixed step sizes lead to poly-logarithmic convergence, while normalized MD with time-varying step sizes leads to polynomial convergence
  - Computational cost: p-GD is coordinate-wise separable and thus efficient to implement, but p-norm computation may be expensive for large p

- **Failure signatures**:
  - Divergence of iterates: May indicate step size too large or unsuitable potential function
  - Slow convergence: May indicate step size too small or potential function not homogeneous
  - Poor generalization: May indicate implicit bias induced by chosen potential function is not appropriate for dataset

- **First 3 experiments**:
  1. Implement p-GD with p = 2 (equivalent to gradient descent) on a simple linearly separable 2D dataset and verify convergence to ℓ₂-maximum margin solution
  2. Implement p-GD with p = 1.1 on the same dataset and observe sparsity of resulting model
  3. Implement normalized p-GD with p = 2 on MNIST using fully-connected architecture and compare convergence rate and generalization to standard p-GD

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does p-GD with different values of p lead to better generalization performance compared to standard SGD (p=2)?
- Basis in paper: The paper shows p=3 consistently outperforms other values on CIFAR-10 and ImageNet, while p=1.1 leads to sparser networks but worse generalization, but theoretical conditions are not established.
- Why unresolved: The theoretical framework focuses on implicit regularization and convergence properties but does not directly address generalization performance. The relationship between p-norm regularization and generalization in deep learning remains an open theoretical question.
- What evidence would resolve it: A theoretical analysis linking p-norm regularization induced by p-GD to generalization bounds, combined with extensive empirical studies across different architectures and datasets.

### Open Question 2
- Question: Can the implicit regularization properties of p-GD be extended to more sophisticated optimization algorithms like Adam or RMSprop?
- Basis in paper: The paper concludes with a discussion on extending analysis to algorithms like Adam or RMSprop, noting that while simple adaptive step sizes were analyzed, commonly used algorithms remain unexplored.
- Why unresolved: Current analysis is limited to mirror descent with homogeneous potentials. Implicit bias properties of more complex adaptive optimizers are not well understood, especially in context of over-parameterized models.
- What evidence would resolve it: A formal analysis showing how adaptive optimizers like Adam or RMSprop can be interpreted as special cases of mirror descent or related algorithms, along with experiments demonstrating their implicit regularization properties.

### Open Question 3
- Question: How does the choice of p in p-GD affect the implicit bias in highly nonlinear deep neural networks compared to linear models?
- Basis in paper: The paper shows p-GD with different values of p leads to different weight distributions in deep networks, but theoretical connection between p and implicit bias in nonlinear settings is not established.
- Why unresolved: The theoretical framework is developed for linear classification problems. Extending this to nonlinear deep networks requires new techniques to characterize implicit bias in these complex settings.
- What evidence would resolve it: A theoretical framework extending implicit bias analysis of p-GD to nonlinear models, possibly through neural tangent kernels or other approximations, combined with experiments validating theoretical predictions.

## Limitations
- Analysis critically depends on homogeneity of potential function, which may not hold for all common choices used in practice
- Polynomial convergence rate proof for normalized MD requires β < 3 constraint, with unclear implications for practical choices
- Experiments are limited to separable cases and do not fully characterize performance on non-separable data

## Confidence

- **High confidence**: The direction convergence to generalized max-margin solutions (Mechanism 1) - supported by rigorous proofs and consistent with known results for GD
- **Medium confidence**: The polynomial convergence rate for normalized MD (Mechanism 2) - theoretical proof exists but requires careful parameter tuning in practice
- **Medium confidence**: The relationship between p-values and generalization (Mechanism 3) - supported by experiments but lacks theoretical characterization of when each p is optimal

## Next Checks

1. Test p-GD on non-separable datasets to verify whether implicit bias properties extend beyond the separable case
2. Compare performance of different p-values on datasets with known structure (e.g., sparse features) to validate claimed relationship between p and model properties
3. Implement and test alternative potential functions that are not homogeneous to identify whether theoretical results extend or break down