---
ver: rpa2
title: Understanding Unimodal Bias in Multimodal Deep Linear Networks
arxiv_id: '2312.00935'
source_url: https://arxiv.org/abs/2312.00935
tags:
- networks
- fusion
- modality
- linear
- late
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work develops a theory of unimodal bias in multimodal learning\
  \ using deep linear networks. It derives the duration of the unimodal phase\u2014\
  the period when a network learns from only one modality\u2014as a function of network\
  \ depth, dataset statistics, and initialization."
---

# Understanding Unimodal Bias in Multimodal Deep Linear Networks

## Quick Facts
- arXiv ID: 2312.00935
- Source URL: https://arxiv.org/abs/2312.00935
- Reference count: 40
- This work develops a theory of unimodal bias in multimodal learning using deep linear networks.

## Executive Summary
This paper analyzes unimodal bias in multimodal deep linear networks, where networks learn from only one modality during an initial learning phase before incorporating the second modality. The theory derives the duration of this unimodal phase as a function of network depth, dataset statistics, and initialization. The work reveals that deeper fusion layers, stronger inter-modality correlations, and greater disparities in input-output correlations all prolong this phase. In the overparameterized regime, this transient unimodal phase can become permanent, leading to generalization deficits. The analysis also identifies a "superficial modality preference" where networks prioritize the faster-to-learn modality rather than the one contributing more to the output.

## Method Summary
The method employs continuous-time gradient flow approximations to analyze multimodal deep linear networks with configurable fusion layer depth. Networks are initialized with small random weights and trained using full batch gradient descent with MSE loss. The analysis derives theoretical predictions for unimodal phase duration using rank-one ansatz and balancing properties between layers. The study examines both underparameterized and overparameterized regimes, comparing early, intermediate, and late fusion architectures. Validation includes synthetic data generation with controlled correlation structures and systematic variation of fusion layer position, network depth, and initialization scale.

## Key Results
- Unimodal phase duration increases with fusion layer depth due to reduced inter-modality feedback during early learning
- Stronger inter-modality correlations create saddle manifolds that trap learning and prolong the unimodal phase
- Networks exhibit "superficial modality preference" - learning the faster-to-learn modality first regardless of its contribution to output
- In overparameterized regimes, late fusion architectures can suffer permanent unimodal bias leading to generalization deficits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deeper fusion layers increase unimodal phase duration by reducing inter-modality feedback during early learning.
- Mechanism: When fusion occurs at layer Lf, post-fusion weights cannot influence pre-fusion layers until modality A is learned. This delays modality B's learning until later epochs.
- Core assumption: Small initialization and rank-one structure of early phase weights hold throughout.
- Evidence anchors:
  - [abstract] "deeper the layer at which fusion occurs, the longer the unimodal phase"
  - [section 4.2.1] Derivation of time ratio formula with Lf term showing longer unimodal phase with deeper fusion
  - [corpus] Weak - no direct citation supporting depth-delay mechanism
- Break condition: If initialization is large, balancing properties break and rank-one assumption fails.

### Mechanism 2
- Claim: Stronger inter-modality correlations prolong unimodal phase by creating saddle manifolds that trap learning.
- Mechanism: Cross-covariance ΣAB ≠ 0 introduces saddle manifolds MA, MB. Networks linger near these saddles during unimodal phase, slowing modality B's learning.
- Core assumption: Cross-covariance terms dominate early phase dynamics.
- Evidence anchors:
  - [section 3.2.2] "network mis-attributes some of the output contributed by modality B to modality A by exploiting their correlations"
  - [section 3.2.1] Denominator term ∥ΣyxB − ΣyxAΣA⁻¹ΣAB∥ governs speed of modality B learning during unimodal phase
  - [corpus] None - corpus lacks direct citation of saddle-based mechanism
- Break condition: If modalities are uncorrelated (ΣAB = 0), saddles disappear and unimodal phase shortens.

### Mechanism 3
- Claim: Disparate input-output correlations create superficial modality preference - networks learn faster modality first even if it contributes less.
- Mechanism: Networks prioritize modality with larger input-output correlation norm ∥Σyx∥, not necessarily the one with larger contribution to output.
- Core assumption: Small initialization ensures ∥ΣyxA∥ > ∥ΣyxB∥ determines learning order.
- Evidence anchors:
  - [section 3.2.3] "prioritize the modality that is faster to learn, which is not necessarily the modality that yields the larger decrease in loss"
  - [section 3.2.3] Conditions ∥ΣyxA∥ > ∥ΣyxB∥ and ΣyxAΣA⁻¹ΣyxAᵀ < ΣyxBΣB⁻¹ΣyxBᵀ create superficial preference
  - [corpus] None - corpus does not discuss superficial preference phenomenon
- Break condition: If input-output correlations are balanced, preference disappears.

## Foundational Learning

- Concept: Gradient descent dynamics in deep linear networks
  - Why needed here: The entire analysis relies on continuous-time gradient flow approximations for multimodal fusion architectures
  - Quick check question: What happens to gradient terms when modalities are uncorrelated vs correlated?

- Concept: Fixed points and saddle manifolds in multimodal learning
  - Why needed here: Unimodal phase emerges from network visiting saddle manifolds MA, MB before reaching global solution
  - Quick check question: How do saddle manifolds differ between early vs late fusion architectures?

- Concept: Balancing properties in deep linear networks
  - Why needed here: Equal norm constraints between layers enable rank-one ansatz and tractable analysis
  - Quick check question: What balancing property holds between pre-fusion layers versus pre- and post-fusion layers?

## Architecture Onboarding

- Component map: Input modalities A, B -> separate pre-fusion branches -> fusion layer at depth Lf -> shared post-fusion layers -> output

- Critical path:
  1. Small random initialization
  2. Modality A learns first via saddle MA
  3. Network lingers in unimodal phase
  4. Modality B learns via saddle MB
  5. Convergence to global pseudo-inverse

- Design tradeoffs:
  - Early fusion (Lf=1): No unimodal phase, but less independent feature learning
  - Late fusion (Lf=L): Long unimodal phase, risk of permanent bias in overparameterized regime
  - Intermediate fusion: Trade-off between phase duration and feature independence

- Failure signatures:
  - Permanent unimodal bias: Loss plateaus during unimodal phase and generalization error increases
  - Mis-attribution: Weights overshoot/undershoot during unimodal phase when modalities are correlated
  - Superficial preference: Network learns faster modality first despite lower contribution

- First 3 experiments:
  1. Vary fusion layer depth Lf from 1 to L and measure unimodal phase duration
  2. Vary correlation coefficient ρ in input covariance and observe effect on time ratio
  3. Compare underparameterized vs overparameterized regimes for early vs late fusion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does the unimodal bias observed in linear networks translate to ReLU networks with nonlinear target functions?
- Basis in paper: [explicit] The paper mentions that results derived for linear networks carry over to two-layer ReLU networks when the target task is linear, but acknowledges that heterogeneity in data and nonlinearity in neural networks can induce behaviors not seen in linear networks. A specific nonlinear example (y = xA + XOR(xB)) is provided where late fusion networks succeed while early fusion networks can fail.
- Why unresolved: The paper only provides one specific nonlinear example and states that more general conditions are out of scope. The mechanisms by which nonlinearity and data heterogeneity affect unimodal bias are not systematically explored.
- What evidence would resolve it: A comprehensive study comparing unimodal bias across different network architectures (early, intermediate, late fusion) and nonlinear target functions, identifying conditions under which the linear theory breaks down.

### Open Question 2
- Question: How does the initialization scale interact with network depth and fusion layer position to influence the duration of the unimodal phase in overparameterized regimes?
- Basis in paper: [explicit] The paper derives that initialization scale affects the time ratio in intermediate fusion networks and shows overparameterized early fusion networks can learn both modalities in one transitional period while late fusion networks can overfit the first modality during the unimodal phase. The interaction between initialization, depth, and fusion position is mentioned but not fully characterized.
- Why unresolved: The analysis focuses on the underparameterized regime with small initialization, and the overparameterized regime is only briefly discussed in terms of overfitting behavior without a complete theoretical treatment of initialization effects.
- What evidence would resolve it: A systematic investigation of how different initialization scales affect learning dynamics in overparameterized multimodal networks across various fusion architectures and depths.

### Open Question 3
- Question: What are the precise trade-offs between choosing shallow versus deep fusion layers for balancing unimodal bias reduction and effective feature learning in heterogeneous multimodal tasks?
- Basis in paper: [explicit] The paper hypothesizes that shallow fusion layers help alleviate unimodal bias through reciprocal cooperation while deep fusion layers aid unimodal feature learning by allowing independent extraction of heterogeneous features. However, this is presented as a qualitative trade-off without quantitative characterization.
- Why unresolved: The paper presents the trade-off conceptually but does not provide a framework for quantifying when one objective should be prioritized over the other, or how to systematically choose fusion layer depth for specific tasks.
- What evidence would resolve it: A principled framework that quantifies the benefits of bias reduction versus feature learning for different fusion depths across various types of multimodal data heterogeneity, potentially leading to a decision procedure for architecture selection.

## Limitations
- Analysis assumes small random initialization and continuous-time gradient flow approximations
- Extension to ReLU networks is limited to specific cases and relies on rank-one structure assumptions
- Overparameterized regime claims are based on theoretical predictions that may not hold in practice
- Full batch gradient descent assumption limits applicability to mini-batch or adaptive optimization methods

## Confidence
- Continuous-time gradient flow analysis for deep linear networks: High
- Extension to two-layer ReLU networks: Medium
- Permanent unimodal bias claims in overparameterized regime: Low
- Saddle manifold stability in non-linear or stochastic settings: Low

## Next Checks
1. Test unimodal phase predictions across different initialization scales to verify the "rich feature learning regime" boundary
2. Empirically measure saddle manifold visitation frequencies in finite learning rate settings versus theoretical predictions
3. Evaluate permanent bias claims by training multimodal networks beyond the theoretically predicted unimodal phase duration and measuring generalization gaps