---
ver: rpa2
title: 'FootGPT : A Large Language Model Development Experiment on a Minimal Setting'
arxiv_id: '2308.08610'
source_url: https://arxiv.org/abs/2308.08610
tags:
- region
- team
- gameweek
- footgpt
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the feasibility of developing a specific-purpose
  language model for football data interpretation using a minimal experimental setting.
  A one billion parameter causal language model was fine-tuned with a curated dataset
  of Italian football league statistics, employing Low-Rank Adaptation (LORA) and
  8-bit quantization.
---

# FootGPT : A Large Language Model Development Experiment on a Minimal Setting

## Quick Facts
- arXiv ID: 2308.08610
- Source URL: https://arxiv.org/abs/2308.08610
- Authors: 
- Reference count: 21
- One-line primary result: Demonstrates feasibility of developing specialized language models for football data interpretation using small models, minimal training, and curated datasets.

## Executive Summary
This study explores the development of a specialized language model for football data interpretation using a minimal experimental setting. A one billion parameter causal language model was fine-tuned with a curated dataset of Italian football league statistics, employing Low-Rank Adaptation (LORA) and 8-bit quantization. Despite the constrained model size, limited training duration, and small dataset, the fine-tuned model demonstrated promising interpretive capabilities, showcasing glimpses of accurate understanding of football gameplay. This experiment highlights the potential of domain adaptation for specialized tasks using small language models, emphasizing the importance of dataset quality over model size.

## Method Summary
The method involved fine-tuning a "falcon-rw-1b" model using LORA and 8-bit quantization with a dataset curated from Wyscout football data for the 2017/2018 Serie A season. A commercial LLM was used to generate question-answer pairs and explanatory paragraphs from structured football statistics. The dataset was then used to fine-tune the base model for approximately 2 epochs (~5700 batches).

## Key Results
- Fine-tuned 1B parameter model demonstrated interpretable capabilities for football data.
- LORA + quantization enabled efficient adaptation without full fine-tuning.
- Dataset quality proved more critical than quantity for specialized task performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain adaptation with LORA + quantization can preserve base LLM linguistic capability while enabling specialized task performance with far fewer parameters than full fine-tuning.
- Mechanism: Low-rank updates to frozen weight matrices allow the model to specialize on football data without overwriting general language skills. Quantization reduces compute cost while maintaining usable accuracy.
- Core assumption: Base LLM has learned robust linguistic priors that can be leveraged for downstream tasks; domain-specific data is high quality and well-aligned to task goals.
- Evidence anchors:
  - [abstract] "One particular reason for choosing such a domain is the reliance on structured data, tables of attributes" - supports structured dataset use.
  - [section III] "curated a linguistic dataset comprising diverse question-answer pairs" - shows dataset quality focus.
  - [corpus] Weak - no corpus neighbor directly supports this mechanism.
- Break condition: If base LLM has not seen similar structured domain data, low-rank updates may not generalize; catastrophic forgetting still possible with very low rank.

### Mechanism 2
- Claim: Teacher-student distillation using a commercial LLM can generate high-quality, task-aligned training data for small models.
- Mechanism: Commercial LLM synthesizes diverse, linguistically rich question-answer pairs from structured football stats, effectively transferring domain knowledge without requiring large training corpora.
- Core assumption: Commercial LLM's instruction-following capability is sufficient to produce high-quality distilled data aligned with target task.
- Evidence anchors:
  - [abstract] "leveraged a powerful commercially available LLM to phrase templates and synthesize diverse verbal expressions" - directly states distillation approach.
  - [section III] "we have leveraged a powerful commercially available LLM to phrase templates and synthesize diverse verbal expressions" - confirms dataset generation method.
  - [corpus] Weak - no corpus neighbor discusses LLM distillation for dataset curation.
- Break condition: If teacher LLM outputs inconsistent or non-task-aligned responses, distilled dataset may mislead small model.

### Mechanism 3
- Claim: Quality of training data is more important than quantity for task specialization in small LLMs.
- Mechanism: Focused, well-curated dataset with explicit reasoning steps (via CoT prompts) can induce complex task behaviors in small models, despite limited parameter count.
- Core assumption: Even small models can learn task-specific reasoning if prompts encode explicit reasoning chains and domain knowledge.
- Evidence anchors:
  - [abstract] "the most significant aspect of developing accurate language models may be the proper dataset content and training strategy compared to the number of neural parameters" - directly states dataset quality importance.
  - [section I] "by constraining training (fine tuning) on a specific domain with a properly prepared dataset... much smaller models can achieve it as well" - supports quality-over-quantity claim.
  - [corpus] Weak - corpus contains no neighbor discussing dataset quality vs quantity trade-offs.
- Break condition: If dataset lacks diversity or is too narrow, model cannot generalize beyond training distribution.

## Foundational Learning

- Concept: Low-Rank Adaptation (LORA)
  - Why needed here: Enables efficient fine-tuning of large LLMs without full parameter updates, reducing memory and compute cost while avoiding catastrophic forgetting.
  - Quick check question: How does LORA differ from full fine-tuning in terms of parameter updates and computational overhead?
- Concept: Quantization in LLMs
  - Why needed here: Reduces model size and inference latency, making deployment of fine-tuned models on limited hardware feasible.
  - Quick check question: What trade-offs does 8-bit quantization introduce in terms of accuracy vs efficiency?
- Concept: Teacher-Student Distillation
  - Why needed here: Allows small models to benefit from knowledge of large commercial LLMs without direct access to large model weights.
  - Quick check question: What are the risks of using a commercial LLM as a teacher for domain-specific data generation?

## Architecture Onboarding

- Component map:
  - Wyscout football dataset -> JSON tables -> verbal QA pairs -> LORA matrices -> 8-bit quantized model
- Critical path:
  1. Load and preprocess Wyscout football dataset
  2. Generate QA pairs using commercial LLM distillation
  3. Fine-tune base model with LORA + quantization
  4. Evaluate model on football interpretation tasks
- Design tradeoffs:
  - Model size vs. training efficiency: 1B param base + LORA vs. full fine-tuning
  - Dataset size vs. quality: 11k samples vs. larger but noisier corpora
  - Compute budget vs. task accuracy: short training duration with quantization
- Failure signatures:
  - Hallucinations in responses → insufficient domain alignment or dataset noise
  - Low task accuracy → base model lacks required linguistic priors or distillation quality poor
  - Training instability → LORA rank too low or quantization errors
- First 3 experiments:
  1. Ablation: Train with and without LORA to measure catastrophic forgetting and compute cost.
  2. Dataset size scaling: Train with 5k, 11k, 20k samples to measure quality vs quantity effects.
  3. Quantization impact: Compare 8-bit vs 16-bit fine-tuned model accuracy and inference speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much does dataset quality influence performance compared to model size in fine-tuned language models for domain-specific tasks?
- Basis in paper: [explicit] The paper states "the most significant aspect of developing accurate language models may be the proper dataset content and training strategy compared to the number of neural parameters, training duration or dataset size."
- Why unresolved: The study used a minimal experimental setting with a small dataset and model, so it's unclear how results would scale with larger datasets or models.
- What evidence would resolve it: Controlled experiments varying dataset quality and model size independently while keeping other factors constant.

### Open Question 2
- Question: What is the optimal balance between verbal explanations and numerical data in training datasets for football statistics interpretation?
- Basis in paper: [explicit] The paper discusses generating verbal question-answer pairs and paragraphs from structured football data, but doesn't explore the optimal ratio of verbal to numerical content.
- Why unresolved: The study used a specific approach to dataset curation but didn't compare it to alternatives with different balances of verbal and numerical content.
- What evidence would resolve it: Comparative experiments with datasets varying the proportion of verbal explanations to numerical data.

### Open Question 3
- Question: How does the inclusion of regional decomposition of events impact the model's ability to interpret football gameplay?
- Basis in paper: [explicit] The paper includes regional decomposition of events in the dataset, but doesn't isolate its effect on model performance.
- Why unresolved: The study used regional decomposition as part of the dataset but didn't conduct experiments to measure its specific impact on model performance.
- What evidence would resolve it: Experiments comparing model performance with and without regional decomposition data in the training set.

### Open Question 4
- Question: What is the minimum dataset size required to achieve reliable performance in domain-specific language models for football statistics?
- Basis in paper: [inferred] The study used a small dataset (11,413 datapoints) and observed promising results, but didn't explore the lower bounds of dataset size.
- Why unresolved: The study didn't conduct experiments with varying dataset sizes to determine the minimum required for reliable performance.
- What evidence would resolve it: Experiments systematically reducing dataset size while monitoring model performance to identify the point of diminishing returns.

## Limitations
- Constrained experimental setting with 1B parameter model and minimal training duration.
- Reliance on qualitative evaluation without quantitative benchmarks.
- Use of commercial LLM for dataset generation introduces potential variability in data quality.

## Confidence
- Dataset Quality Over Quantity (Medium Confidence): Supported by experimental design but lacks quantitative validation.
- Feasibility of Domain Adaptation (High Confidence): Demonstrated through successful fine-tuning but limited in scope.
- Potential for Specialized Task Performance (Low Confidence): Qualitative results promising but limited by minimal training duration.

## Next Checks
1. Conduct a quantitative evaluation of the model's performance on football interpretation tasks, comparing it against both larger models and baselines trained on different dataset sizes or qualities.
2. Perform an ablation study to assess the impact of dataset quality versus quantity by training models on datasets of varying sizes and qualities, and measuring their performance on domain-specific tasks.
3. Test the model's ability to generalize to unseen football data or related domains (e.g., other sports) to evaluate the robustness of the domain adaptation approach.