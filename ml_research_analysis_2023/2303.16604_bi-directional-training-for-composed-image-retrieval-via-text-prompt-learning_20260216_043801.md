---
ver: rpa2
title: Bi-directional Training for Composed Image Retrieval via Text Prompt Learning
arxiv_id: '2303.16604'
source_url: https://arxiv.org/abs/2303.16604
tags:
- text
- image
- training
- reversed
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a bi-directional training scheme for composed
  image retrieval that leverages both forward and reversed queries to improve model
  performance. The method uses learnable tokens prepended to text to encode directionality,
  fine-tunes text encoders to handle both directions, and modifies contrastive loss
  with a specialized negative sampling strategy.
---

# Bi-directional Training for Composed Image Retrieval via Text Prompt Learning

## Quick Facts
- arXiv ID: 2303.16604
- Source URL: https://arxiv.org/abs/2303.16604
- Reference count: 40
- This paper proposes a bi-directional training scheme for composed image retrieval that leverages both forward and reversed queries to improve model performance.

## Executive Summary
This paper introduces a bi-directional training approach for composed image retrieval (CIR) that learns from both forward queries (reference image + modification text → target image) and reversed queries (target image + reversed text → reference image). The method uses learnable tokens prepended to text to encode directionality, fine-tunes text encoders to handle both directions, and modifies contrastive loss with a specialized negative sampling strategy. Applied to a BLIP-based baseline model, the approach achieves state-of-the-art results on Fashion-IQ and CIRR datasets with consistent improvements over strong baselines.

## Method Summary
The proposed method builds on a BLIP-based vision-and-language model and introduces a two-stage training pipeline. First, the text encoder is fine-tuned by prepending learnable direction tokens ([FORWARD], [BACKWARD]) to modification text inputs. Second, a combiner module is trained using a contrastive loss that incorporates both forward queries and reversed queries, with a modified negative sampling strategy that samples from target images in the reversed direction. This bi-directional approach enables the model to learn richer representations by leveraging information from both query directions while minimizing false negatives through careful sampling design.

## Key Results
- Achieves state-of-the-art performance on Fashion-IQ dataset, improving average Recall@10 and Recall@50 by 1.5 points over previous best methods
- Sets new benchmark on CIRR dataset with significant gains in Recall@1, Recall@5, and RecallSubset@1 metrics
- Demonstrates consistent improvements across both datasets while requiring minimal architectural changes to existing CIR frameworks
- Shows that BLIP encoders outperform CLIP encoders as the vision-and-language backbone for CIR tasks

## Why This Works (Mechanism)

### Mechanism 1
Prepending learnable tokens to text inputs enables the text encoder to learn directionality-specific embeddings. The text encoder learns to associate specific token patterns with semantic reversal, producing different embeddings for the same text based on prepended direction tokens. Core assumption: Vision-and-language pretrained models can adapt to learn new token semantics through finetuning. Evidence anchors: [abstract] "To encode the bi-directional query we prepend a learnable token to the modification text that designates the direction of the query and then finetune the parameters of the text embedding module". Break condition: If the text encoder cannot effectively associate directionality with tokens, or if the pretrained model's architecture is too rigid to adapt.

### Mechanism 2
The modified negative sampling strategy creates more coherent bi-directional contrastive learning. By sampling negatives from the same pool (target images) for both forward and reversed queries, the model learns consistent contrastive relationships across directions. Core assumption: Consistent negative sampling across directions improves the stability and effectiveness of bi-directional training. Evidence anchors: [abstract] "We employ a modified sampling strategy for negative samples, such that the loss is more coherent in the bi-directional training scheme". Break condition: If the sampling strategy creates too many false negatives in the reversed direction, degrading overall performance.

### Mechanism 3
Using BLIP instead of CLIP encoders provides better performance due to domain alignment with CIR data. BLIP's training on more complex text and diverse image-text pairs makes it better suited for the sophisticated language in CIR datasets. Core assumption: The quality of the vision-and-language backbone directly impacts downstream CIR performance. Evidence anchors: [abstract] "We propose to change the CLIP [25] image and text encoders in both stages to BLIP [18]". Break condition: If the domain gap between BLIP's pretraining data and CIR datasets is larger than assumed.

## Foundational Learning

- Concept: Contrastive learning with batch-based classification loss
  - Why needed here: The paper uses BBC loss for training, which requires understanding how to compute similarity between embeddings and how negative sampling works
  - Quick check question: In the contrastive loss formula, what does the denominator represent and why is it important?

- Concept: Vision-and-language pretraining and finetuning strategies
  - Why needed here: The paper builds on BLIP, a VLP model, and finetunes its text encoder with special tokens
  - Quick check question: What is the key difference between finetuning a VLP model's text encoder versus its image encoder in terms of computational cost?

- Concept: Image retrieval evaluation metrics (Recall@K)
  - Why needed here: The paper uses Recall@K and RecallSubset@K metrics, which require understanding how they measure retrieval performance
  - Quick check question: Why might Recall@1 be more sensitive to false negatives than Recall@10?

## Architecture Onboarding

- Component map:
  Text Encoder (BLIP) -> Image Encoder (BLIP) -> Combiner Module -> Loss Functions (Forward + Reversed contrastive loss)

- Critical path: Text input → Token prepending → Text encoder finetuning → Combiner training with bi-directional loss → Inference with forward queries only

- Design tradeoffs:
  - Using learnable tokens vs. fixed tokens: Learnable tokens adapt to the dataset but require additional training parameters
  - Bi-directional training vs. single direction: Bi-directional provides more training signal but introduces false negatives
  - BLIP vs. CLIP: BLIP performs better but may have different computational characteristics

- Failure signatures:
  - Poor performance on forward queries despite bi-directional training: Indicates false negatives in reversed direction are overwhelming the training signal
  - High variance in RecallSubset@K: Suggests the combiner architecture struggles with fine-grained distinctions
  - No improvement over baseline: Could indicate the token learning mechanism isn't effective

- First 3 experiments:
  1. Implement token prepending and text encoder finetuning with only forward queries to verify the BLIP baseline improvement
  2. Add reversed queries with the proposed negative sampling strategy and evaluate the impact on validation metrics
  3. Test alternative negative sampling schemes (e.g., sampling from reference images) to confirm the proposed approach is optimal

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implicit questions arise from the methodology and results:

1. How can the impact of false negatives in reversed queries be systematically measured and mitigated?
2. What is the relationship between text complexity and the model's ability to learn reversed semantics?
3. How does the choice of negative sampling strategy affect performance across different metrics and datasets?

## Limitations

- The bi-directional training approach introduces false negatives in reversed queries due to the one-to-many mapping from text to images, which can degrade performance if not properly managed
- The learnable token mechanism, while effective, lacks theoretical grounding for why specific token patterns would consistently encode directionality across diverse datasets and language variations
- The paper does not explore how the method performs on datasets with different text complexity levels or investigate potential adaptations for handling more sophisticated language inputs

## Confidence

- High confidence: The core performance improvements on Fashion-IQ and CIRR datasets are well-supported by quantitative results and ablation studies
- Medium confidence: The mechanism by which prepended tokens encode directionality is plausible but not rigorously proven
- Medium confidence: The modified negative sampling strategy shows promise but the paper does not explore alternative sampling schemes or provide theoretical justification

## Next Checks

1. Systematically measure the false negative rate in reversed queries across different dataset splits and analyze its correlation with performance degradation, particularly for the RecallSubset@K metric where the paper notes combiner limitations

2. Visualize and analyze the learned embeddings of the [FORWARD] and [BACKWARD] tokens across different training epochs to verify they encode consistent directionality patterns rather than dataset-specific artifacts

3. Implement and compare alternative negative sampling strategies (e.g., sampling from reference images in reversed queries, or using harder negative mining) to rigorously test whether the proposed strategy is optimal or simply one of several viable approaches