---
ver: rpa2
title: 'The CoT Collection: Improving Zero-shot and Few-shot Learning of Language
  Models via Chain-of-Thought Fine-Tuning'
arxiv_id: '2305.14045'
source_url: https://arxiv.org/abs/2305.14045
tags:
- answer
- arxiv
- rationale
- instruction
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the CoT Collection, a new instruction-tuning
  dataset that augments the existing Flan Collection with an additional 1.84 million
  chain-of-thought rationales across 1,060 tasks. By continually fine-tuning Flan-T5
  (3B & 11B) with the CoT Collection, the authors demonstrate improved zero-shot task
  accuracy on the BIG-Bench-Hard benchmark, achieving an average improvement of +4.34%
  (Flan-T5 3B) and +2.60% (Flan-T5 11B).
---

# The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning

## Quick Facts
- **arXiv ID**: 2305.14045
- **Source URL**: https://arxiv.org/abs/2305.14045
- **Reference count**: 26
- **Key outcome**: CoT fine-tuning improves zero-shot and few-shot performance on BIG-Bench-Hard and domain-specific tasks

## Executive Summary
This paper introduces the CoT Collection, a new instruction-tuning dataset that augments the existing Flan Collection with an additional 1.84 million chain-of-thought rationales across 1,060 tasks. By continually fine-tuning Flan-T5 (3B & 11B) with the CoT Collection, the authors demonstrate improved zero-shot task accuracy on the BIG-Bench-Hard benchmark, achieving an average improvement of +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B). Additionally, the instruction-tuned models exhibit stronger few-shot learning capabilities on four domain-specific tasks, resulting in an improvement of +2.24% (Flan-T5 3B) and +2.37% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until the max length by a +13.98% margin.

## Method Summary
The paper extracts chain-of-thought rationales from the Flan Collection using large language models with chain-of-thought prompting. These rationales are then filtered and cleaned using a combination of automated metrics (ROSCoe) and human evaluation. The resulting CoT Collection is used to continually fine-tune Flan-T5 models (3B and 11B). The fine-tuned models are evaluated on zero-shot performance using BIG-Bench-Hard and on few-shot learning using four domain-specific tasks with LoRA-based parameter-efficient fine-tuning.

## Key Results
- CoT fine-tuning improves zero-shot accuracy on BIG-Bench-Hard by +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B)
- CoT fine-tuning enhances few-shot learning capabilities, improving performance by +2.24% (Flan-T5 3B) and +2.37% (Flan-T5 11B) on domain-specific tasks
- CoT fine-tuned models outperform ChatGPT with demonstrations on few-shot tasks by +13.98%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-thought fine-tuning with large-scale CoT rationale data improves zero-shot generalization on unseen tasks for smaller LMs.
- Mechanism: By training LMs to generate step-by-step rationales alongside answers, they learn internal reasoning patterns that transfer to new tasks without task-specific demonstrations.
- Core assumption: The CoT rationales extracted from diverse tasks are sufficiently diverse and informative to encode general reasoning patterns.
- Evidence anchors: [abstract] reports +4.34% and +2.60% average improvement on BIG-Bench-Hard for Flan-T5 3B and 11B models respectively.

### Mechanism 2
- Claim: CoT fine-tuning enables stronger few-shot learning capabilities than direct fine-tuning or ICL with LLMs.
- Mechanism: The LM learns to reason step-by-step, making it easier to adapt to new tasks with minimal examples by generating intermediate rationales during few-shot adaptation.
- Core assumption: Generating rationales is a useful inductive bias that helps the model generalize from few examples.
- Evidence anchors: [abstract] reports +2.37% improvement on 4 domain-specific tasks for Flan-T5 11B and +2.24% for Flan-T5 3B, outperforming ChatGPT with ICL.

### Mechanism 3
- Claim: Instruction tuning with CoT rationales further improves direct performance on seen tasks.
- Mechanism: CoT fine-tuning enhances the LM's general reasoning ability, which indirectly improves performance on tasks it has already been instruction-tuned on.
- Core assumption: Improved reasoning ability translates to better performance even on tasks where reasoning was not explicitly required.
- Evidence anchors: [abstract] reports a +1.98% total average improvement for Flan-T5 11B when evaluating both direct and CoT performance.

## Foundational Learning

- Concept: Chain-of-thought (CoT) prompting
  - Why needed here: CoT prompting is the core mechanism being studied; understanding it is essential to grasp why CoT fine-tuning works.
  - Quick check question: What is the difference between zero-shot CoT prompting and few-shot CoT prompting?

- Concept: Instruction tuning
  - Why needed here: The paper builds on instruction-tuned models (Flan-T5) and studies the effect of additional CoT fine-tuning; knowing what instruction tuning does is key.
  - Quick check question: How does instruction tuning differ from standard supervised fine-tuning?

- Concept: Few-shot learning and parameter-efficient fine-tuning (LoRA)
  - Why needed here: The paper evaluates few-shot adaptation using LoRA; understanding both concepts is necessary to interpret the results.
  - Quick check question: What is the main advantage of LoRA over full fine-tuning in a few-shot setting?

## Architecture Onboarding

- Component map: Flan-T5 (3B & 11B) -> CoT Collection (1.88M rationales) -> CoT fine-tuning -> Evaluation on BBH and domain-specific tasks
- Critical path: 1) Extract CoT rationales from Flan Collection using ICL with LLMs. 2) Filter and clean the rationales. 3) Continually fine-tune Flan-T5 with the CoT Collection. 4) Evaluate on zero-shot (BBH) and few-shot (domain-specific) tasks.
- Design tradeoffs: Using CoT fine-tuning adds computational cost and may risk overfitting to the rationales, but can improve generalization. Using LoRA for few-shot adaptation reduces parameter updates but may limit adaptation capacity.
- Failure signatures: If zero-shot performance does not improve, it may indicate the rationales are not diverse or informative enough. If few-shot performance degrades, it may indicate the CoT fine-tuning interferes with task-specific adaptation.
- First 3 experiments:
  1. Evaluate zero-shot performance of Flan-T5 with and without CoT fine-tuning on BBH.
  2. Evaluate few-shot performance of Flan-T5 and C2F2 with LoRA fine-tuning on a small set of domain-specific tasks.
  3. Ablation study: Compare CoT fine-tuning to direct instruction tuning on a subset of the CoT Collection.

## Open Questions the Paper Calls Out
- How can we ensure the quality and diversity of CoT rationales generated by LLMs?
- How can we reduce the reliance on proprietary LLM APIs for generating CoT rationales?
- How can we scale up the generation of CoT rationales to cover a wider range of tasks and domains?
- How can we leverage CoT rationales to improve the performance of smaller language models on a wider range of tasks?
- How can we develop more efficient and effective methods for fine-tuning language models with CoT rationales?
- How can we integrate CoT rationales into the training process of language models to improve their reasoning capabilities?
- How can we leverage CoT rationales to improve the interpretability and explainability of language model predictions?

## Limitations
- The improvement margins on BIG-Bench-Hard are modest (+4.34% for 3B, +2.60% for 11B), raising questions about practical significance
- Few-shot evaluation is limited to only four domain-specific tasks, restricting generalizability
- Comparison to ChatGPT may not account for all confounding factors like context length and demonstration format

## Confidence

- **High Confidence**: The methodology for extracting and filtering CoT rationales is well-specified and reproducible. The reported improvements on BBH and domain-specific tasks are supported by experimental results.
- **Medium Confidence**: The mechanism claims (CoT fine-tuning improves generalization and few-shot learning) are plausible given the results, but the causal relationship between rationale diversity and transfer performance is not fully established.
- **Low Confidence**: The claim that CoT fine-tuning outperforms ChatGPT with demonstrations is based on a narrow comparison that may not account for all confounding factors.

## Next Checks

1. Conduct a quantitative analysis of the CoT rationales' diversity across tasks and domains to establish a stronger link between rationale quality and transfer performance.
2. Test the few-shot learning capabilities on a broader set of domains (e.g., STEM, humanities) to assess generalizability beyond legal and medical tasks.
3. Evaluate the impact of CoT fine-tuning with varying scales of rationale data (e.g., 100K, 500K, 1M) to determine the optimal trade-off between data size and performance gains.