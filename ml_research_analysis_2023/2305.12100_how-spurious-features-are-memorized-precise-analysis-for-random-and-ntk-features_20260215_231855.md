---
ver: rpa2
title: 'How Spurious Features Are Memorized: Precise Analysis for Random and NTK Features'
arxiv_id: '2305.12100'
source_url: https://arxiv.org/abs/2305.12100
tags:
- probability
- lemma
- have
- least1
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a rigorous theoretical analysis of how spurious
  features are memorized in deep learning models. The authors characterize this memorization
  through two terms: (i) the stability of the model with respect to individual training
  samples, and (ii) the feature alignment between the spurious feature and the full
  sample.'
---

# How Spurious Features Are Memorized: Precise Analysis for Random and NTK Features

## Quick Facts
- arXiv ID: 2305.12100
- Source URL: https://arxiv.org/abs/2305.12100
- Reference count: 40
- Key outcome: This paper provides a rigorous theoretical analysis of how spurious features are memorized in deep learning models, characterizing this through model stability and feature alignment.

## Executive Summary
This paper provides a rigorous theoretical analysis of how spurious features are memorized in deep learning models. The authors characterize this memorization through two terms: (i) the stability of the model with respect to individual training samples, and (ii) the feature alignment between the spurious feature and the full sample. While stability is a well-established concept in learning theory connected to generalization error, the feature alignment term is novel. The key technical results give precise characterizations of the feature alignment for random features (RF) and neural tangent kernel (NTK) regression settings. The authors prove that the memorization of spurious features weakens as the generalization capability increases, unveiling the role of the model and its activation function. The theoretical predictions are validated through numerical experiments on standard datasets (MNIST, CIFAR-10).

## Method Summary
The paper analyzes spurious feature memorization through a dual-term mechanism involving model stability and feature alignment. The authors focus on empirical risk minimization (ERM) with complete interpolation on two specific model classes: random features (RF) regression and neural tangent kernel (NTK) regression. They compute the feature alignment term for both RF and NTK settings under over-parameterization assumptions, proving that this term concentrates to a constant γ that depends on the activation function's Hermite coefficients and the fraction of spurious features. The theoretical framework is validated through experiments on synthetic Gaussian data and standard datasets (MNIST, CIFAR-10) using various neural network architectures.

## Key Results
- Spurious feature memorization is characterized by two distinct components: model stability with respect to individual samples and feature alignment between spurious and full samples.
- The memorization of spurious features weakens as generalization capability increases, with precise bounds on the feature alignment term for RF and NTK models.
- The activation function's Hermite coefficients play a crucial role in determining the strength of spurious feature memorization, with different activations leading to different memorization behaviors.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The power of spurious feature memorization is quantified by two terms: model stability with respect to individual training samples, and feature alignment between the spurious feature and the full sample.
- Mechanism: The authors establish a mathematical relationship where the attack's effectiveness depends on the product of these two terms. When the model is stable (low stability value), even if feature alignment is high, the attack's impact is limited. Conversely, when stability is low, high feature alignment leads to strong memorization.
- Core assumption: The feature alignment term F(zm,z) can be approximated by a constant γ > 0 independent of the original sample z.
- Evidence anchors:
  - [abstract] "the power of this black-box attack can be exactly analyzed through two distinct components: (i) the feature alignment F(zm,z1) between zm and z1... (ii) the stability Sz1 of the model with respect to z1"
  - [section 4] "Lemma 4.1 relates the stability with respect to z1 evaluated on the two samples z and z1 through the quantity Fϕ(z,z1), which captures the similarity between z and z1 in the feature space"
  - [corpus] Weak evidence - no direct citations on this specific dual-term mechanism, but related work on stability-generalization connections exists
- Break condition: If the feature alignment cannot be approximated by a constant γ, or if the model is not trained to complete interpolation (K not invertible), the mechanism fails.

### Mechanism 2
- Claim: The memorization of spurious features weakens as the generalization capability increases.
- Mechanism: Through analysis of the feature alignment term for random features (RF) and neural tangent kernel (NTK) regression settings, the authors prove that better generalization (lower generalization error) leads to weaker spurious feature memorization. This is captured by the lower bound on γRF and γNTK which depend on the activation function's Hermite coefficients and the fraction of spurious features.
- Core assumption: The models operate in an over-parameterized regime where N log³N = o(k) for RF and N log⁸N = o(kd) for NTK.
- Evidence anchors:
  - [abstract] "We prove that the memorization of spurious features weakens as the generalization capability increases"
  - [section 5] "The combination of Theorem 1 and Lemma 4.1 unveils a remarkable proportionality relation between stability and privacy: the more the algorithm is stable (and, therefore, capable of generalizing), the smaller the effect of the reconstruction attack"
  - [corpus] No direct citations found on this specific relationship between generalization and spurious feature memorization
- Break condition: If the over-parameterization assumptions are violated, or if the data distribution doesn't satisfy Lipschitz concentration properties, the mechanism breaks down.

### Mechanism 3
- Claim: The activation function plays a crucial role in determining the strength of spurious feature memorization through its Hermite coefficients.
- Mechanism: The lower bound on γ (γRF or γNTK) is explicitly expressed in terms of the activation function's Hermite coefficients. Activations with dominant low-order Hermite coefficients lead to higher feature alignment and thus stronger memorization, while those with dominant high-order coefficients result in weaker memorization.
- Core assumption: The activation function has a well-defined Hermite expansion with non-zero coefficients beyond the linear term.
- Evidence anchors:
  - [section 5] "the lower bound increases with α – which is expected, since α represents the fraction of the input to which the attacker has access – and it depends in a non-trivial way on the activation function via its Hermite coefficients"
  - [section 6] "we are able to express the limit γNTK of the feature alignment in a closed form involving α and the Hermite coefficients of the derivative of the activation"
  - [corpus] No direct citations found on this specific role of Hermite coefficients in spurious feature memorization
- Break condition: If the activation function is linear (no higher-order Hermite coefficients), or if the Hermite expansion doesn't converge appropriately, the mechanism fails.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) and its linearization properties
  - Why needed here: The paper analyzes spurious feature memorization in the NTK regime, where the neural network behaves like a kernel method. Understanding NTK is crucial for following the theoretical analysis and proofs.
  - Quick check question: What is the key difference between training a neural network in the NTK regime versus standard deep learning training?

- Concept: Random Features (RF) model and its relationship to kernel methods
  - Why needed here: The paper provides analysis for both RF and NTK models. The RF model serves as a simpler, analytically tractable model that exhibits similar behavior to NTK.
  - Quick check question: How does the RF model relate to kernel methods, and what are its key advantages for theoretical analysis?

- Concept: Feature alignment and its role in model behavior
  - Why needed here: The feature alignment term F(zm,z) is central to the paper's theoretical framework. It captures the similarity between samples in feature space and directly impacts the attack's effectiveness.
  - Quick check question: How does feature alignment differ from standard notions of similarity or distance between data points?

## Architecture Onboarding

- Component map: Data (independent components x,y) -> Model training (RF or NTK regression) -> Feature alignment computation F(zm,z) -> Stability analysis Sz(z) -> Attack effectiveness prediction
- Critical path: Data → Model training → Feature alignment computation → Stability analysis → Attack effectiveness prediction
- Design tradeoffs:
  - Over-parameterization vs. computational efficiency
  - Choice of activation function (impacts Hermite coefficients and memorization strength)
  - Fraction of spurious features (α = dy/d) vs. task performance
- Failure signatures:
  - Feature alignment not concentrating to a constant γ
  - Kernel matrix K not invertible (model cannot interpolate)
  - Data not satisfying Lipschitz concentration properties
- First 3 experiments:
  1. Replicate Figure 1: Test and attack accuracies as a function of training samples N for different architectures (FC, CNN) on synthetic and standard datasets
  2. Verify Theorem 1: Plot feature alignment FRF(zm1,z1) vs. number of samples N for different activation functions and α values
  3. Verify Theorem 2: Plot feature alignment FNTK(zm1,z1) vs. number of samples N for different activation functions and α values, confirming the closed-form expression for γNTK

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications emerge from the analysis:

### Open Question 1
- Question: What is the precise relationship between feature alignment and generalization error for different neural network architectures beyond RF and NTK models?
- Basis in paper: [explicit] The paper shows that feature alignment and generalization error are inversely related for RF and NTK models, and empirically demonstrates a similar trend for various neural network architectures in Figure 1.
- Why unresolved: The paper only provides empirical evidence for a few specific architectures and does not offer a theoretical explanation for why this relationship holds more broadly.
- What evidence would resolve it: A theoretical analysis proving that feature alignment and generalization error are inversely related for a wider class of neural network architectures, or additional empirical studies on a more diverse set of architectures.

### Open Question 2
- Question: How does the choice of activation function affect the feature alignment and, consequently, the privacy of the model?
- Basis in paper: [explicit] The paper shows that the lower bound on the feature alignment for RF models depends on the activation function's Hermite coefficients, and for NTK models, the limit of the feature alignment is expressed in terms of the Hermite coefficients of the derivative of the activation function.
- Why unresolved: The paper only provides a theoretical analysis for specific activation functions and does not explore the impact of different activation functions on feature alignment and privacy.
- What evidence would resolve it: A comprehensive study comparing the feature alignment and privacy guarantees of models with different activation functions, both theoretically and empirically.

### Open Question 3
- Question: Can the framework of stability and feature alignment be extended to analyze the privacy of models trained with differential privacy?
- Basis in paper: [inferred] The paper focuses on the privacy of ERM-trained models and mentions differential privacy as a potential avenue for future work in the conclusions.
- Why unresolved: The paper does not provide any theoretical or empirical analysis of how stability and feature alignment relate to the privacy guarantees of differentially private models.
- What evidence would resolve it: A theoretical analysis showing how stability and feature alignment can be used to quantify the privacy of differentially private models, or empirical studies comparing the privacy guarantees of differentially private and non-private models using the proposed framework.

## Limitations
- The theoretical analysis relies heavily on over-parameterization assumptions that may not hold in practical scenarios with limited data or computational resources.
- The paper assumes complete interpolation of training data, which may not be achievable or desirable in real-world applications due to noise or computational constraints.
- The feature alignment mechanism assumes the spurious feature can be approximated by a constant γ, which may not hold for complex data distributions or when the spurious feature has non-trivial structure.

## Confidence
- High confidence in the dual-term mechanism (stability × feature alignment) explaining spurious feature memorization, as this follows established learning theory principles.
- Medium confidence in the relationship between generalization capability and spurious feature memorization weakening, due to limited empirical validation across diverse architectures and datasets.
- Low confidence in the specific role of Hermite coefficients in determining memorization strength, as this analysis is highly theoretical and lacks extensive experimental support.

## Next Checks
1. Test the feature alignment mechanism on non-Gaussian data distributions (e.g., CIFAR-10 with natural images) to verify if the constant γ approximation holds.
2. Evaluate the spurious feature memorization framework on architectures beyond RF and NTK, such as standard CNNs or transformers, to assess generalizability.
3. Experimentally validate the Hermite coefficient hypothesis by comparing memorization strength across activation functions with known Hermite expansions (ReLU, tanh, softplus) on multiple datasets.