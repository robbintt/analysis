---
ver: rpa2
title: 'Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep Learning
  Approaches'
arxiv_id: '2307.06218'
source_url: https://arxiv.org/abs/2307.06218
tags:
- poetry
- arabic
- meter
- dataset
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Ashaar introduces a deep learning framework for Arabic poetry analysis
  and generation, building four datasets and five pre-trained models. It tackles meter,
  theme, and era classification, diacritization, Arudi-style extraction, and conditional
  generation.
---

# Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep Learning Approaches

## Quick Facts
- arXiv ID: 2307.06218
- Source URL: https://arxiv.org/abs/2307.06218
- Reference count: 25
- Primary result: Introduces a deep learning framework for Arabic poetry analysis and generation, building four datasets and five pre-trained models.

## Executive Summary
Ashaar presents a comprehensive deep learning framework for Arabic poetry analysis and generation. The system addresses multiple tasks including meter classification, theme and era classification, diacritization, Arudi-style extraction, and conditional poetry generation. The framework builds four specialized datasets and employs transformer-based models for classification tasks alongside a character-based GPT architecture for generation. The approach successfully handles the complexities of Arabic poetry, including its rich morphological structure and strict rhythmic patterns.

## Method Summary
The Ashaar framework combines transformer-based models for classification tasks with a character-level GPT architecture for poetry generation. Four datasets were created: Ashaar (poetry with meter, theme, era annotations), Ashaar diacritized, Ashaar arudi, and Ashaar tafeelah. The system uses transformer architectures for meter, theme, and era classification, while the generation component employs a GPT model with character-level tokenization to preserve rhythm and meter. The pipeline includes preprocessing, augmentation, and fine-tuning procedures for both classification and generation tasks.

## Key Results
- Character-level tokenization achieves top-3 accuracy of 69.40% for rhythm preservation in poetry generation
- Zero-shot diacritization achieves up to 50% error rate, showing significant performance
- The framework demonstrates strong performance in meter classification and conditional generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Character-level tokenization preserves meter and rhythm better than subword tokenization for Arabic poetry generation.
- Mechanism: Character-level tokenization avoids the vocabulary shift and contextual mismatches that occur when subwords (BPE) are applied to a morphologically rich language like Arabic, especially when mixing classical and modern vocabulary.
- Core assumption: The semantic and phonetic structure of Arabic poetry is more faithfully preserved at the character level than at the subword

## Foundational Learning

### Concept 1: Transformer-based Classification
- Why needed: To accurately classify Arabic poetry by meter, theme, and era using contextual understanding
- Quick check: Verify classification accuracy on benchmark datasets

### Concept 2: Character-level Tokenization
- Why needed: To preserve the rhythmic and phonetic structure of Arabic poetry during generation
- Quick check: Compare rhythm preservation with subword tokenization methods

### Concept 3: GPT Architecture for Generation
- Why needed: To generate coherent and contextually appropriate Arabic poetry
- Quick check: Evaluate generated poetry for semantic coherence and adherence to poetic structures

## Architecture Onboarding

### Component Map
Data Preprocessing -> Transformer Models -> Classification Tasks
Data Preprocessing -> Character-level GPT -> Poetry Generation

### Critical Path
The critical path for poetry generation involves character-level tokenization of input text, processing through the GPT model, and output generation while preserving meter and rhythm constraints.

### Design Tradeoffs
The choice between character-level and subword tokenization represents a key tradeoff between rhythm preservation and vocabulary diversity. Character-level tokenization better maintains poetic structure but may limit semantic richness.

### Failure Signatures
Poor classification accuracy may indicate insufficient data augmentation or preprocessing issues. Generated poetry lacking rhythm suggests problems with tokenization or model fine-tuning.

### First 3 Experiments
1. Train classification models on the Ashaar dataset and evaluate accuracy
2. Fine-tune the GPT model on poetry data and assess rhythm preservation
3. Compare character-level and subword tokenization performance on generation tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the character-based GPT model's performance in Arabic poetry generation compare to word-piece models when handling diacritized text?
- Basis in paper: [explicit] The paper states that poetry doesn't work well with word pieces, especially with partially diacritized text, leading to the use of a character-based tokenization approach.
- Why unresolved: The paper doesn't provide a direct comparison of the character-based model's performance against word-piece models in Arabic poetry generation tasks.
- What evidence would resolve it: Comparative experiments between character-based and word-piece models on the same poetry generation tasks, focusing on metrics like BLEU score, rhythm preservation, and diacritization accuracy.

### Open Question 2
- Question: What is the impact of dataset size and quality on the performance of Arabic poetry generation models?
- Basis in paper: [inferred] The paper discusses creating larger datasets (Ashaar) and cleaning procedures, suggesting an interest in dataset quality's effect on model performance.
- Why unresolved: The paper doesn't explicitly analyze how different dataset sizes or qualities affect the model's performance in generating poetry.
- What evidence would resolve it: Experiments varying dataset sizes and qualities while keeping other factors constant, then measuring the impact on generated poetry's fluency, coherence, and adherence to poetic structures.

### Open Question 3
- Question: How does the Ashaar framework's ability to predict Arudi style compare to human experts in terms of accuracy and consistency?
- Basis in paper: [explicit] The paper introduces a method for predicting Arudi style and reports an average similarity score of 93.41% with a 43% exact match rate on a test set.
- Why unresolved: The paper doesn't compare the framework's performance against human experts, leaving the relative accuracy and consistency unclear.
- What evidence would resolve it: A comparative study where the framework and human experts predict Arudi styles for the same set of poems, followed by an analysis of accuracy rates and consistency in predictions.

## Limitations

- The evaluation relies heavily on automated metrics without human validation of poetic quality or aesthetic merit
- Character-level tokenization may limit vocabulary diversity and semantic richness compared to subword methods
- The 50% DER for zero-shot diacritization indicates significant performance degradation

## Confidence

- **High confidence**: Technical framework description and dataset construction methodology are well-documented and reproducible
- **Medium confidence**: Reported performance metrics are plausible but lack comparative baselines
- **Low confidence**: Qualitative claims about advancement lack comprehensive human evaluation

## Next Checks

1. Conduct blind human assessments comparing Ashaar-generated poetry against classical Arabic poetry and other AI-generated poems
2. Implement and evaluate Ashaar against existing Arabic poetry analysis and generation systems using identical datasets
3. Profile inference time, memory usage, and hardware requirements for both classification and generation tasks