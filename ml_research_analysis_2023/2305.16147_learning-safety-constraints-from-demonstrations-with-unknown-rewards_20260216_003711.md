---
ver: rpa2
title: Learning Safety Constraints from Demonstrations with Unknown Rewards
arxiv_id: '2305.16147'
source_url: https://arxiv.org/abs/2305.16147
tags:
- reward
- demonstrations
- safe
- constraints
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of learning shared safety constraints
  from demonstrations with unknown reward functions in Constrained Markov Decision
  Processes (CMDPs). The proposed method, CoCoRL, constructs a convex safe set from
  demonstration feature expectations, ensuring safety even for suboptimal demonstrations.
---

# Learning Safety Constraints from Demonstrations with Unknown Rewards

## Quick Facts
- **arXiv ID**: 2305.16147
- **Source URL**: https://arxiv.org/abs/2305.16147
- **Reference count**: 40
- **Primary result**: CoCoRL learns safe constraints from demonstrations with unknown rewards, achieving low constraint violations (<5%) while maintaining high returns in driving simulation.

## Executive Summary
This paper addresses the challenge of learning shared safety constraints from demonstrations where each demonstration has an unknown reward function. The proposed method, CoCoRL, constructs a convex safe set from demonstration feature expectations, ensuring safety even for suboptimal demonstrations. The approach provides theoretical guarantees for safety and convergence under optimality assumptions, and demonstrates strong empirical performance across gridworld and driving simulation environments, outperforming IRL-based methods that often learn unsafe policies.

## Method Summary
CoCoRL operates in a Constrained Markov Decision Process (CMDP) setting where demonstrations from safe policies are available but each has an unknown reward function. The method constructs a convex safe set by taking the convex hull of the feature expectations of safe demonstrations. This safe set is then used to solve an inferred CMDP for new tasks with known reward functions. The approach leverages the convexity of the true safe set (defined by linear constraints) to guarantee that any policy within the constructed safe set is safe, even when demonstrations are suboptimal.

## Key Results
- CoCoRL guarantees safety for any policy in the constructed safe set by leveraging convexity, even with suboptimal demonstrations
- Under optimality assumptions, CoCoRL converges to the true safe set with no policy regret
- In driving simulation, CoCoRL achieves constraint violation rates below 5% while maintaining high returns, outperforming IRL methods that frequently violate constraints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CoCoRL guarantees safety for any policy in the constructed safe set S by leveraging convexity.
- **Mechanism**: The safe set S is the convex hull of the demonstrations' feature expectations. Since the true safe set F (policies satisfying constraints) is convex (Lemma 1), and all demonstrations are safe, S is a subset of F. Therefore, any policy in S is safe.
- **Core assumption**: The true safe set F is convex, which holds because it is defined by linear constraints.
- **Evidence anchors**:
  - [abstract]: "CoCoRL constructs a convex safe set based on demonstrations, which provably guarantees safety even for potentially sub-optimal (but safe) demonstrations."
  - [section 4.1]: "Lemma 1. For any CMDP, suppose π1, π2 ∈ F = {π|∀j : Jj(π) ≤ ξj}, and let π12 be a policy such that f(π12) = λf(π1) + (1 − λ)f(π2) with λ ∈ [0, 1]. Then π12 ∈ F."
- **Break condition**: If the true safe set is not convex (e.g., due to non-linear constraints), the safety guarantee fails.

### Mechanism 2
- **Claim**: CoCoRL converges to the true safe set with no policy regret under optimality assumptions.
- **Mechanism**: Under exact optimality (Assumption 1) or Boltzmann-rationality (Assumption 2), the safe set S eventually covers all vertices of the true safe set F. Since the true safe set is a convex polyhedron, its vertices define the hypothesis class. As more demonstrations are collected, the probability of missing a vertex decreases, leading to zero regret asymptotically.
- **Core assumption**: The demonstrations are optimal (or approximately optimal under Boltzmann rationality).
- **Evidence anchors**:
  - [section 4.2]: "Theorem 3 (Convergence, exact optimality). Under Assumption 1, for any δ > 0, after k ≥ log(δ/fv(d, n))/ log(1 − δ/fv(d, n)), we have P (R(r, Sk) > 0) ≤ δ... In particular, limk→∞ Er [R(r, Sk)] = 0."
- **Break condition**: If demonstrations are not near-optimal, convergence to the true safe set is not guaranteed.

### Mechanism 3
- **Claim**: CoCoRL can learn constraints from demonstrations with unknown rewards by inferring a shared constraint penalty.
- **Mechanism**: CoCoRL treats the demonstrations as samples from a Boltzmann policy distribution with shared constraints. By constructing the safe set S and solving an inferred CMDP, it finds a policy that satisfies the shared constraints for any new reward function.
- **Core assumption**: The demonstrations are safe and share the same constraints.
- **Evidence anchors**:
  - [abstract]: "CoCoRL can learn constraints from demonstrations with different unknown rewards without knowledge of the environment dynamics."
  - [section 3.1]: "We consider an infinite-horizon discounted CMDP without reward function and cost functions... We have access to demonstrations from k safe policies D = {π1*, . . . , πk*}, that have k unknown reward functions r1, . . . , rk, but share the same constraints."
- **Break condition**: If the demonstrations do not share the same constraints, the method fails.

## Foundational Learning

- **Concept**: Convex Polyhedra
  - Why needed here: The safe set S is a convex polyhedron, and understanding its properties is crucial for the safety and convergence guarantees.
  - Quick check question: What is the relationship between the vertices of a convex polyhedron and its linear constraints?

- **Concept**: Constrained Markov Decision Processes (CMDPs)
  - Why needed here: CoCoRL operates in a CMDP setting, where the agent must maximize reward under constraints.
  - Quick check question: How does a CMDP differ from a standard MDP?

- **Concept**: Inverse Reinforcement Learning (IRL)
  - Why needed here: CoCoRL is compared to IRL-based methods, and understanding IRL's limitations in CMDPs is important.
  - Quick check question: Why can't IRL guarantee safety in CMDPs?

## Architecture Onboarding

- **Component map**: Demonstration collection -> Feature expectation estimation -> Convex hull construction -> Inferred CMDP solving -> Policy evaluation

- **Critical path**:
  1. Collect demonstrations.
  2. Estimate feature expectations.
  3. Construct the safe set S.
  4. Solve the inferred CMDP.
  5. Evaluate the policy on the new task.

- **Design tradeoffs**:
  - Using a conservative safe set (e.g., guaranteed hull) vs. a less conservative one (e.g., convex hull) for exact safety.
  - Incremental convex hull construction vs. constructing from all demonstrations to reduce numerical issues.

- **Failure signatures**:
  - Unsafe policies: The safe set S does not contain the true safe set F.
  - High regret: The safe set S does not cover all vertices of the true safe set F.
  - Numerical instability: The convex hull construction fails due to degenerate inputs.

- **First 3 experiments**:
  1. Gridworld environment with known constraints: Verify safety and performance.
  2. Gridworld environment with unknown constraints: Test constraint inference.
  3. Driving simulation: Test scalability and transfer to new tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CoCoRL's performance scale with increasing dimensionality of the feature space and number of constraints?
- Basis in paper: [explicit] The paper mentions that the sample complexity depends on the dimensionality of the feature space, as suggested by Theorem 3. However, it notes that there is no noticeable difference in sample complexity for different numbers of constraints.
- Why unresolved: While the paper provides some experimental results on single-state CMDPs showing performance with varying dimensions and constraint numbers, it doesn't extensively explore the scalability of CoCoRL in high-dimensional feature spaces or with a large number of constraints.
- What evidence would resolve it: Experiments systematically varying the dimensionality of the feature space and the number of constraints, measuring sample complexity and performance.

### Open Question 2
- Question: Can CoCoRL handle demonstrations that are not perfectly safe, i.e., demonstrations that occasionally violate the constraints?
- Basis in paper: [inferred] The paper assumes that all demonstrations are safe. It mentions that learning from potentially unsafe demonstrations requires improved models of human demonstrations or additional (safety) labels.
- Why unresolved: The paper doesn't explore the robustness of CoCoRL to noisy or imperfect demonstrations, which is a more realistic scenario.
- What evidence would resolve it: Experiments where demonstrations are generated with a small probability of constraint violation, and evaluating CoCoRL's ability to still learn safe constraints.

### Open Question 3
- Question: How does the choice of constrained RL algorithm for solving the inferred CMDP affect CoCoRL's performance and safety guarantees?
- Basis in paper: [explicit] The paper mentions using different constrained RL algorithms depending on the environment, such as linear programming for tabular environments and a constrained cross-entropy method for the driving environment. It also notes that the CEM occasionally fails to find a feasible policy when the safe set is small.
- Why unresolved: The paper doesn't investigate the impact of different constrained RL algorithms on the overall performance and safety of CoCoRL. It also doesn't explore ways to improve the feasibility of the solution within the safe set.
- What evidence would resolve it: Experiments comparing CoCoRL's performance and safety using different constrained RL algorithms, and investigating techniques to improve the feasibility of the solution.

## Limitations

- Safety guarantees depend critically on the convexity of the true safe set, which may fail for non-linear constraints
- Convergence results rely heavily on strong optimality assumptions that may not hold in practice
- The method assumes shared constraints across demonstrations, which may not be realistic

## Confidence

- **Safety guarantee mechanism (Mechanism 1)**: High confidence - supported by formal proof (Lemma 1) and directly stated in the abstract
- **Convergence guarantee (Mechanism 2)**: Medium confidence - theoretical guarantees exist but depend on strong optimality assumptions
- **Constraint inference mechanism (Mechanism 3)**: Medium confidence - framework is well-defined but experimental validation across diverse scenarios is limited

## Next Checks

1. **Convexity robustness test**: Evaluate performance when constraints are non-linear (e.g., quadratic constraints) to verify where the safety guarantee breaks down.

2. **Demonstration quality sensitivity**: Systematically vary demonstration optimality (from expert to random) and measure the degradation in safe set quality and policy performance.

3. **High-dimensional scalability**: Test the method with high-dimensional feature spaces (e.g., raw pixel inputs) to identify numerical stability issues in convex hull construction and assess the need for feature preprocessing.