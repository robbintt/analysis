---
ver: rpa2
title: Automatic Radiology Report Generation by Learning with Increasingly Hard Negatives
arxiv_id: '2305.07176'
source_url: https://arxiv.org/abs/2305.07176
tags:
- report
- image
- reports
- negative
- hard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the challenge of automatic radiology report generation
  where images and reports are similar due to common anatomy, making it hard to capture
  individual image uniqueness. It proposes a framework to learn discriminative features
  by distinguishing image-report pairs from their closest peers, termed "hard negatives."
  The method synthesizes increasingly hard negative reports in feature space using
  min-max alternating optimization, challenging the model to learn finer distinctions.
---

# Automatic Radiology Report Generation by Learning with Increasingly Hard Negatives

## Quick Facts
- arXiv ID: 2305.07176
- Source URL: https://arxiv.org/abs/2305.07176
- Reference count: 37
- Primary result: Synthesizing increasingly hard negative reports in feature space improves radiology report generation by forcing the model to learn discriminative features that distinguish subtle differences between similar images.

## Executive Summary
This paper addresses the challenge of automatic radiology report generation where images and reports are similar due to common anatomy, making it difficult to capture individual image uniqueness. The authors propose a framework that learns discriminative features by distinguishing image-report pairs from their closest peers, termed "hard negatives." The key innovation is synthesizing increasingly hard negative reports in feature space using min-max alternating optimization, which challenges the model to learn finer-grained distinctions. Experimental results on IU-XRay and MIMIC-CXR datasets demonstrate significant improvements over state-of-the-art methods, with notable gains in BLEU, METEOR, CIDER, and clinical metrics.

## Method Summary
The framework uses an encoder-decoder structure with shared parameters to generate radiology reports from chest X-ray images. It identifies hard negative reports using a pre-trained transformer model (RadBERT) and synthesizes increasingly hard negative reports in the feature space through min-max alternating optimization. The model alternates between minimizing loss for report generation and maximizing alignment loss to create harder negatives. The final loss function combines cross-entropy loss for report generation, contrastive loss for aligning image and report features, and cosine similarity loss for learning distinctive image features. The method extends beyond dataset granularity by synthesizing negatives in feature space rather than just sampling from the dataset.

## Key Results
- The proposed framework achieves significant improvements over state-of-the-art methods on IU-XRay and MIMIC-CXR datasets, with BLEU-4 scores improving by 1.2-1.5 points.
- The optimization-based synthesis strategy outperforms simple linear combination approaches, demonstrating the effectiveness of the proposed min-max alternating optimization.
- The method generalizes to generic image captioning tasks on the COCO dataset, showing robustness beyond medical imaging applications.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthesizing harder negatives in feature space forces the model to learn discriminative features that distinguish subtle differences between similar images and reports.
- Mechanism: The method uses min-max alternating optimization to treat increasingly hard negatives as auxiliary variables. At each iteration, the model minimizes loss for report generation while maximizing alignment loss to create harder negatives, pushing the model to capture finer-grained mismatches.
- Core assumption: Creating harder negatives in the feature space (rather than just sampling from the dataset) extends the model's learning beyond the training set's granularity and forces it to learn more discriminative representations.
- Evidence anchors:
  - [abstract] "to attain more discriminative features, we gradually raise the difficulty of such a learning task by creating increasingly hard negative reports for each image in the feature space during training"
  - [section] "By treating the increasingly hard negatives as auxiliary variables, we formulate this process as a min-max alternating optimisation problem"
  - [corpus] Weak evidence - related papers focus on evaluation metrics rather than the optimization mechanism itself
- Break condition: If the projection step doesn't maintain the "report" manifold property, the synthesized negatives could degrade report quality rather than improve it.

### Mechanism 2
- Claim: The optimization-based synthesis strategy (Eq. 7) outperforms simple linear combination by incorporating both image and text modalities and maintaining manifold constraints.
- Mechanism: The method projects the harder negative onto the line segment between the current negative and the positive report feature, constrained to stay on the "report" manifold, rather than using unconstrained linear combinations.
- Core assumption: The line segment between a report feature and its hard negative approximates the "report" manifold in feature space, and constraining synthesis to this line preserves report quality while increasing difficulty.
- Evidence anchors:
  - [section] "we propose an optimisation-based approach. Let LF denote our final loss function... we can explicitly express LF as LF (θ; ˜u−1,··· , ˜u−n ). To synthesise the harder negatives with the training process, we propose a min-max optimisation problem"
  - [section] "we impose the constrain that the synthesised harder negative must reside on the line ˜u−i ui"
  - [corpus] Weak evidence - no direct comparison of optimization-based vs linear combination strategies in related works
- Break condition: If the projection step moves the negative too far from the true "report" manifold, the synthesized samples may become unrealistic and harm learning.

### Mechanism 3
- Claim: The modified CLIP loss with additional similarity matrix captures harder negatives better than standard random sampling.
- Mechanism: The method computes three similarity matrices - one for positive pairs, one for prior hard negatives, and one for synthesized harder negatives (with diagonals inverted), then averages them to create a final matrix that better represents true difficulty.
- Core assumption: Standard CLIP randomly samples negatives that may not be truly "hard," while computing similarity with synthesized harder negatives provides more meaningful contrastive signals.
- Evidence anchors:
  - [section] "we propose to compute an additional similarity matrix between image features and the synthesised harder negative report features and then average the two similarity matrices to obtain a final similarity matrix"
  - [section] "S3(zi, ˜u−j ) = zi· ˜u−j /∥zi∥·∥ ˜u−j∥... we need to push the diagonal elements of this matrix to be small as the off-diagonal ones do"
  - [corpus] Weak evidence - related papers focus on evaluation rather than contrastive loss modifications
- Break condition: If the synthesized negatives are too aggressive, the modified loss could create unstable training dynamics.

## Foundational Learning

- Concept: Min-max alternating optimization
  - Why needed here: This framework requires solving an optimization problem where we alternate between minimizing loss for report generation and maximizing loss to create harder negatives.
  - Quick check question: Can you explain how the min-max problem in Eq. (3) reduces to standard minimization at each iteration?

- Concept: Triplet loss and contrastive learning
  - Why needed here: The method uses triplet loss as a basis for understanding how to increase loss by moving harder negatives toward positive features, then constrains this movement to maintain manifold properties.
  - Quick check question: How does the gradient ∂L(˜u−i)/∂˜u−i = 2(zi − ˜u−i) tell us which direction to move the synthesized negative?

- Concept: Manifold learning and projection
  - Why needed here: The method needs to ensure synthesized negatives stay on the "report" manifold in feature space, which requires understanding how to project points onto lines and maintain semantic validity.
  - Quick check question: Why is projecting zi onto the line ˜u−i ui a reasonable approximation for maintaining the "report" manifold property?

## Architecture Onboarding

- Component map: Image → Encoder A/B → Feature extraction → Decoder A/B → Report features → CLIP loss computation with synthesized negatives → Loss optimization
- Critical path: Image → Encoder A/B → Feature extraction → Decoder A/B → Report features → CLIP loss computation with synthesized negatives → Loss optimization
- Design tradeoffs: The method trades computational complexity (synthesizing negatives at each iteration) for improved discriminative learning; uses shared parameters across encoders/decoders to avoid extra network weights.
- Failure signatures: If the method fails, you might see: (1) degradation in BLEU/METEOR scores despite harder negatives, (2) unstable training loss curves, (3) reports that become too generic or too dissimilar to ground truth.
- First 3 experiments:
  1. Run baseline model (XproNet) with standard CLIP loss only to establish baseline performance
  2. Add the MoCHi-inspired linear combination strategy to see if harder negatives help without the optimization framework
  3. Implement the full optimization-based synthesis with projection to test the complete framework

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method vary with different values of the hyperparameter α that controls the rate of hardness increase of the synthesized harder negative reports?
- Basis in paper: [explicit] The paper investigates the impact of α on the model's performance in Table 3 (right), showing that the optimal value depends on the dataset.
- Why unresolved: While the paper provides results for different α values on IU-XRay and COCO datasets, it does not explore the full range of possible α values or provide a comprehensive analysis of its impact on performance across various datasets.
- What evidence would resolve it: A systematic study of the model's performance with a wide range of α values on multiple datasets, including medical and non-medical datasets, would provide insights into the optimal α value and its generalizability.

### Open Question 2
- Question: How does the proposed method compare to other techniques for generating hard negatives, such as adversarial training or meta-learning approaches?
- Basis in paper: [inferred] The paper focuses on synthesizing hard negatives using a min-max optimization approach and compares it to a MoCHi-inspired strategy. However, it does not compare its performance to other hard negative generation techniques.
- Why unresolved: The paper does not explore alternative methods for generating hard negatives, leaving a gap in understanding the relative effectiveness of different approaches.
- What evidence would resolve it: A comprehensive comparison of the proposed method with other hard negative generation techniques, such as adversarial training or meta-learning, on various datasets would provide insights into the relative strengths and weaknesses of different approaches.

### Open Question 3
- Question: How does the proposed method perform on datasets with different levels of similarity between images and reports, such as datasets with more diverse or less diverse content?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the proposed method on IU-XRay and MIMIC-CXR datasets, which contain medical images and reports. However, it does not explore its performance on datasets with different levels of similarity between images and reports.
- Why unresolved: The paper does not provide insights into how the proposed method performs on datasets with varying levels of similarity between images and reports, limiting its generalizability.
- What evidence would resolve it: Testing the proposed method on datasets with different levels of similarity between images and reports, such as datasets with more diverse or less diverse content, would provide insights into its robustness and generalizability.

## Limitations

- The framework's effectiveness heavily depends on the quality of the hard negative synthesis process, which relies on accurate identification of similar image-report pairs.
- The method assumes that the line segment between a report feature and its hard negative approximates the "report" manifold, but this geometric assumption may not hold in all feature spaces.
- The min-max optimization introduces computational overhead and may be sensitive to hyperparameter settings, particularly the rate of hardness increase and margin values.

## Confidence

- **High Confidence**: The empirical improvements in standard metrics (BLEU, METEOR, CIDER) and clinical metrics are well-documented through quantitative comparisons with baseline models. The framework's extension to generic image captioning tasks also demonstrates robustness.
- **Medium Confidence**: The theoretical justification for why progressively harder negatives improve discriminative learning is sound, but the specific mechanisms of feature space synthesis and manifold projection lack extensive theoretical grounding. The superiority of the optimization-based approach over linear combinations is demonstrated empirically but could benefit from more rigorous ablation studies.
- **Low Confidence**: The claims about avoiding extra network weights through parameter sharing, while technically true, may underestimate the computational burden of the min-max optimization process. The clinical significance of the reported improvements, while measured, may not fully capture real-world diagnostic utility.

## Next Checks

1. **Ablation Study**: Conduct a comprehensive ablation study comparing the optimization-based synthesis approach with simpler linear combination methods across multiple random seeds to verify the statistical significance of performance differences.

2. **Manifold Analysis**: Visualize and analyze whether synthesized negatives actually remain on the "report" manifold in feature space by examining the semantic coherence of generated reports as the hardness parameter increases.

3. **Robustness Testing**: Test the framework's performance when the hard negative identification step (RadBERT) is replaced with different similarity metrics or when trained on datasets with varying levels of report diversity to assess generalizability.