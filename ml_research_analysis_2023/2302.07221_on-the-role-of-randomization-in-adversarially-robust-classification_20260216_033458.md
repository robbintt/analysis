---
ver: rpa2
title: On the Role of Randomization in Adversarially Robust Classification
arxiv_id: '2302.07221'
source_url: https://arxiv.org/abs/2302.07221
tags:
- classi
- adversarial
- risk
- randomized
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper clarifies the role of randomization in adversarial robustness
  by proving that for any randomized binary classifier, there exists a deterministic
  classifier with strictly better adversarial risk. The authors introduce level-set
  classifiers and show that the adversarial risk of any randomized classifier can
  be decomposed as a convex combination of adversarial risks of these deterministic
  classifiers.
---

# On the Role of Randomization in Adversarially Robust Classification

## Quick Facts
- arXiv ID: 2302.07221
- Source URL: https://arxiv.org/abs/2302.07221
- Authors: 
- Reference count: 40
- Key outcome: For any randomized binary classifier, there exists a deterministic classifier with strictly better adversarial risk.

## Executive Summary
This paper challenges the common belief that randomization is necessary for building adversarially robust classifiers. Through theoretical analysis, the authors prove that any randomized binary classifier can be outperformed by a deterministic classifier in terms of adversarial risk. The key insight is that the adversarial risk of any randomized classifier can be decomposed as a convex combination of adversarial risks of deterministic level-set classifiers. This decomposition guarantees that at least one level-set classifier must have better or equal adversarial risk than the original randomized classifier. The authors demonstrate this principle through three main comparisons: finite mixtures vs weighted ensembles, input noise injection vs randomized smoothing, and general randomized vs deterministic classifiers.

## Method Summary
The paper employs a theoretical approach to analyze the role of randomization in adversarial robustness. The core methodology involves constructing level-set classifiers from randomized classifiers and comparing their adversarial risks. For experimental validation, the authors use pretrained models from Cohen et al. (2019) and Dbouk & Shanbhag (2022) on CIFAR-10 and CIFAR-100 datasets. They employ AutoAttack with ℓ2 and ℓ∞ norms to measure adversarial risk, with specific parameters (40 EoT iterations, 50 PGD steps for noise injection; 100 attack iterations for mixtures). The theoretical results are then validated through empirical comparisons between the original randomized classifiers and their deterministic counterparts.

## Key Results
- For any randomized binary classifier, there exists a deterministic classifier with strictly better adversarial risk.
- Finite mixtures of classifiers are no more robust than weighted ensembles when level sets are closed under union and intersection.
- Input noise injection classifiers are less robust than their randomized smoothing counterparts with the same noise distribution.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: For any randomized binary classifier, there exists a deterministic classifier with strictly better adversarial risk.
- Mechanism: The adversarial risk of any randomized binary classifier can be decomposed as a convex combination of adversarial risks of level-set classifiers (deterministic classifiers of the form hα(x) = 1{h(x)>α}). This decomposition guarantees that at least one level-set classifier must have better or equal adversarial risk than the original randomized classifier.
- Core assumption: The randomized classifier h is measurable and upper-semicontinuous, ensuring the existence of the supremum and interchangeability of integration and supremum operations.
- Evidence anchors:
  - [abstract]: "we prove that for any randomized classifier, there is always a deterministic one with an adversarial risk at least as good"
  - [section]: Theorem 5.5 shows Rϵ(h) = ∫₀¹ Rϵ(hα)dα ≥ minα Rϵ(hα)
  - [corpus]: No direct evidence found in related papers about this specific decomposition mechanism
- Break condition: If the randomized classifier is not upper-semicontinuous or not measurable, the decomposition may not hold.

### Mechanism 2
- Claim: Finite mixtures of classifiers do not improve robustness beyond what weighted ensembles can achieve.
- Mechanism: When the level sets of base classifiers form a family closed under union and intersection, any finite mixture can be represented as a convex combination of weighted ensembles, which are deterministic classifiers. Therefore, no additional robustness is gained through randomization.
- Core assumption: The family of level sets A = {f⁻¹(1): f∈F} is closed under union and intersection operations.
- Evidence anchors:
  - [section]: Theorem 5.6 states "If A is closed under union and intersection, then inf f∈F Rϵ(f) = inf h∈M(F) Rϵ(h)"
  - [abstract]: "ensembles of classifiers are more robust than mixtures of classifiers"
  - [corpus]: No direct evidence found in related papers about this specific closure property requirement
- Break condition: If the family of level sets is not closed under union and intersection, mixtures may provide additional robustness benefits.

### Mechanism 3
- Claim: Input noise injection classifiers are less robust than their randomized smoothing counterparts.
- Mechanism: The level-set classifiers derived from input noise injection are equivalent to randomized smoothing classifiers with different thresholds. Since these are deterministic, they must be at least as robust as the original randomized classifier.
- Core assumption: The noise distribution used in input noise injection is the same as the one used in randomized smoothing.
- Evidence anchors:
  - [section]: "for any input noise injection classifier, there exists a deterministic randomized smoothing classifier with threshold α that has better adversarial risk"
  - [abstract]: "randomized smoothing is more robust than input noise injection"
  - [corpus]: No direct evidence found in related papers about this specific comparison
- Break condition: If the noise distributions differ between the two approaches, the comparison may not hold.

## Foundational Learning

- Concept: Level-set classifiers
  - Why needed here: They provide the deterministic counterpart to any randomized classifier, enabling the main theoretical result
  - Quick check question: Given a randomized classifier h that outputs probability 0.7 for a particular input, what would the level-set classifier h₀.₆ output for that same input?

- Concept: Adversarial risk decomposition
  - Why needed here: It's the mathematical foundation that proves randomized classifiers cannot be more robust than some deterministic classifier
  - Quick check question: If Rϵ(h) = 0.3∫₀¹ Rϵ(hα)dα, what is the minimum possible value of Rϵ(h) given that all Rϵ(hα) values are between 0 and 1?

- Concept: Convex combinations in robustness analysis
  - Why needed here: The theorem relies on showing that the adversarial risk of a randomized classifier is a convex combination of deterministic classifiers' risks
  - Quick check question: If a randomized classifier's risk is a convex combination of three deterministic classifiers with risks 0.2, 0.5, and 0.8, what is the range of possible values for the randomized classifier's risk?

## Architecture Onboarding

- Component map: Base hypothesis set F -> Randomized classifier h -> Level-set classifiers hα -> Adversarial risk function Rϵ -> Noise distribution η

- Critical path:
  1. Start with a randomized classifier h
  2. Construct level-set classifiers hα for all α ∈ [0,1]
  3. Compute adversarial risks Rϵ(hα) for each level-set classifier
  4. Find the level-set classifier with minimum adversarial risk
  5. Compare this minimum to Rϵ(h) to verify the theorem

- Design tradeoffs:
  - Computing exact adversarial risk is intractable for most practical classifiers
  - Monte Carlo approximations can be used but introduce additional randomness
  - The number of level-set classifiers to consider grows exponentially with the number of base classifiers in mixtures

- Failure signatures:
  - If the randomized classifier is not measurable or upper-semicontinuous, the theorem's proof breaks down
  - If the noise distribution in input noise injection differs from randomized smoothing, the comparison may not hold
  - If the family of level sets is not closed under union and intersection, mixtures may provide additional benefits

- First 3 experiments:
  1. Implement a simple binary classifier and its corresponding randomized version using input noise injection
  2. Compute the level-set classifiers and compare their adversarial risks
  3. Repeat with a finite mixture classifier and compare against weighted ensembles

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the theoretical findings about randomization in binary classification extend to multi-class classification settings?
- Basis in paper: [explicit] The authors explicitly state that "a natural extension of the present work would be to deal with multi-class classification" in the conclusion section, noting that "in which the deterministic classifiers that were used in Theorem 5.5 are not straightforward to define."
- Why unresolved: The paper only provides proofs and analysis for binary classification, leaving the extension to multi-class scenarios as future work. The deterministic classifiers used in the binary case (level-set classifiers) may not have direct analogs in multi-class settings.
- What evidence would resolve it: Theoretical extensions of the main theorem to multi-class classification, along with experimental validation showing whether the conclusions about deterministic classifiers outperforming randomized ones hold in multi-class scenarios.

### Open Question 2
- Question: What is the computational complexity of finding the optimal deterministic classifier that outperforms a given randomized classifier?
- Basis in paper: [inferred] The paper proves that such a deterministic classifier exists for any randomized classifier, but does not address the computational complexity of finding it. The proof relies on decomposing the adversarial risk of randomized classifiers into convex combinations of deterministic classifiers, which suggests an optimization problem.
- Why unresolved: While the existence is proven, the paper does not provide algorithms or analyze the computational complexity of actually finding the optimal deterministic classifier in practice.
- What evidence would resolve it: Algorithmic approaches with computational complexity analysis for finding the optimal deterministic classifier, along with empirical studies on the scalability of these approaches.

### Open Question 3
- Question: How do the theoretical findings apply to different types of adversarial attacks beyond ℓ2 and ℓ∞ norms?
- Basis in paper: [explicit] The experiments in Section 6 only consider ℓ2 and ℓ∞ adversaries, with specific parameters (e.g., "budget for the ℓ2 adversary is set to ϵ = 1").
- Why unresolved: The paper's theoretical results and experimental validation are limited to specific types of adversarial attacks, leaving open the question of how generalizable the findings are to other attack scenarios.
- What evidence would resolve it: Theoretical analysis extending the main results to other norms or attack types, along with experimental validation on a broader range of adversarial attacks.

## Limitations
- Theoretical results rely on assumptions about measurability and upper-semicontinuity that may not hold in practical deep learning implementations.
- Experimental validation is limited to CIFAR-10 and CIFAR-100 datasets with specific pretrained models, limiting generalizability.
- The comparison between input noise injection and randomized smoothing assumes identical noise distributions, which may not be practically achievable.

## Confidence

**High confidence**: The fundamental theorem proving deterministic classifiers can match or exceed randomized classifiers in adversarial robustness (Theorem 5.5)

**Medium confidence**: The equivalence between finite mixtures and weighted ensembles when level sets are closed under union/intersection (Theorem 5.6), as this depends on specific structural assumptions

**Medium confidence**: The experimental comparisons between noise injection and randomized smoothing, given the dependence on pretrained model quality and attack implementation details

## Next Checks

1. Test the deterministic-to-randomized conversion on additional datasets (ImageNet, SVHN) and with different model architectures to verify generalizability

2. Implement custom noise injection and randomized smoothing schemes with carefully matched noise distributions to validate the theoretical comparison

3. Conduct ablation studies varying the closure properties of level set families to understand when mixtures provide additional benefits