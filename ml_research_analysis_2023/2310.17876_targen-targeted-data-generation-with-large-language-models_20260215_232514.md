---
ver: rpa2
title: 'TarGEN: Targeted Data Generation with Large Language Models'
arxiv_id: '2310.17876'
source_url: https://arxiv.org/abs/2310.17876
tags:
- dataset
- output
- data
- premise
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TarGEN introduces a seedless multi-step prompting strategy for
  generating high-quality synthetic datasets using LLMs. It creates task-specific
  prompts to generate linguistic instance seeds and label-constrained data, then employs
  self-correction to identify and fix mislabeled instances.
---

# TarGEN: Targeted Data Generation with Large Language Models

## Quick Facts
- arXiv ID: 2310.17876
- Source URL: https://arxiv.org/abs/2310.17876
- Reference count: 40
- Primary result: Seedless LLM-based synthetic dataset generation achieves comparable or better performance than original data across SuperGLUE tasks

## Executive Summary
TarGEN introduces a seedless multi-step prompting strategy for generating high-quality synthetic datasets using LLMs. It creates task-specific prompts to generate linguistic instance seeds and label-constrained data, then employs self-correction to identify and fix mislabeled instances. On SuperGLUE tasks, models trained on synthetic data match or slightly exceed performance on original data (~1-2% improvement), with larger gains under instruction tuning. Pre-training T5-3B on the synthetic dataset outperforms Self-Instruct by 4.14% on OpenLLM. The synthetic dataset exhibits comparable or higher diversity and bias alignment with the original, demonstrating TarGEN's effectiveness in producing robust synthetic benchmarks without seed examples.

## Method Summary
TarGEN uses a 4-step pipeline to generate synthetic datasets: (1) generate contexts for semantic diversity, (2) generate instance seeds (linguistic elements unique to each task instance), (3) generate label-constrained instances using seeds, and (4) apply self-correction to fix mislabeled instances. The approach uses ChatGPT as the LLM for generation and self-correction. Models are trained on both original and synthetic datasets separately and evaluated on original test sets. The method is compared against Self-Instruct by pre-finetuning T5-3B models.

## Key Results
- Models trained on TarGEN synthetic data match or slightly exceed performance on original SuperGLUE data (~1-2% improvement)
- Pre-training T5-3B on TarGEN synthetic dataset outperforms Self-Instruct by 4.14% on OpenLLM
- Synthetic dataset shows comparable or higher lexical diversity and lower semantic similarity than original datasets
- Self-correction effectively reduces label noise, ensuring reliable labels in generated data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The seedless multi-step prompting strategy enables generation of high-quality, diverse synthetic data without requiring seed examples.
- Mechanism: The approach generates "instance seeds" (linguistic elements unique to each task instance) through controlled generation steps, then uses label-constrained prompts to create data instances. This bypasses the need for original task instances as exemplars.
- Core assumption: Task descriptions and label constraints alone contain sufficient information to generate diverse, valid instances.
- Evidence anchors:
  - [abstract] "An advantage of TarGEN is its seedless nature; it does not require specific task instances, broadening its applicability beyond task replication."
  - [section 2.1] "This circumvents the need for any seed instances from the original dataset; i.e. for any given task, a dataset can be generated from scratch by formulating its generation function from the task description."

### Mechanism 2
- Claim: The self-correction module significantly reduces label noise and improves dataset quality.
- Mechanism: A single meta-prompt augmented with task instructions and validation examples is used to re-label instances. The LLM evaluates whether generated instances align with their labels and the task description, correcting mislabeled instances.
- Core assumption: LLMs have sufficient understanding of the task to identify and correct labeling errors through in-context learning.
- Evidence anchors:
  - [abstract] "We augment TarGEN with a method known as self-correction empowering LLMs to rectify inaccurately labeled instances during dataset creation, ensuring reliable labels."
  - [section 2.3] "Self-correction consists of a single meta-prompt that is common to all tasks. The task instructions and task-specific validation examples are used to augment the meta-prompt and tailor it to each generated dataset."

### Mechanism 3
- Claim: TarGEN-generated datasets exhibit comparable or higher diversity and bias alignment than original datasets.
- Mechanism: By generating contexts, instance seeds, and using label-constrained prompts, the approach naturally creates diverse samples. Bias alignment is achieved because the generation process mirrors the original dataset's label distribution without introducing systematic biases.
- Core assumption: Diversity and bias characteristics can be controlled through prompt engineering and controlled generation steps.
- Evidence anchors:
  - [abstract] "A comprehensive analysis of the synthetic dataset compared to the original dataset reveals that the synthetic dataset demonstrates similar or higher levels of dataset complexity and diversity."
  - [section 4] "Furthermore, our dataset has comparable lexical diversity...and consistently displays lower cosine similarity between intra-data text pairs, highlighting the dataset's rich and distinct content."

## Foundational Learning

- Concept: Label-constrained text generation
  - Why needed here: It allows control over label distribution while generating diverse instances based on instance seeds.
  - Quick check question: How does label-constrained generation differ from standard conditional generation in terms of output control?

- Concept: Self-correction through in-context learning
  - Why needed here: It leverages the LLM's existing knowledge to identify and correct labeling errors without additional training.
  - Quick check question: What are the limitations of using in-context learning for self-correction compared to fine-tuning?

- Concept: Dataset diversity metrics (lexical, semantic, V-usable information)
  - Why needed here: These metrics are used to compare synthetic and original datasets to validate the generation quality.
  - Quick check question: How does V-usable information differ from traditional difficulty metrics like accuracy gaps?

## Architecture Onboarding

- Component map: Task description parser → Context generator → Instance seed generator → Label-constrained instance generator → Self-correction module → Dataset output
- Critical path: Task description → Instance seed generation → Label-constrained instance generation → Self-correction → Final dataset
- Design tradeoffs:
  - Seedless vs seed-based generation: Seedless offers broader applicability but may have lower fidelity to original tasks
  - Single-step vs multi-step self-correction: Single-step is simpler but may miss complex errors
  - Context generation: More contexts increase diversity but also increase generation cost
- Failure signatures:
  - Low diversity: Generated instances are too similar to each other or to the prompts
  - High noise: Self-correction fails to identify or correct labeling errors
  - Bias mismatch: Generated dataset has different bias characteristics than original
- First 3 experiments:
  1. Generate a small synthetic dataset for a simple task (like BoolQ) and compare diversity metrics with original
  2. Test self-correction on a dataset with known labeling errors to measure correction accuracy
  3. Train a model on synthetic data and evaluate on original test set to measure task fidelity

## Open Questions the Paper Calls Out
- How does TarGEN's performance scale with different LLM sizes and architectures when generating synthetic datasets?
- Can TarGEN be effectively adapted for multilingual synthetic dataset generation and evaluation?
- What is the impact of different instance seed types (e.g., sentences, passages, atomic elements) on the quality and diversity of generated synthetic datasets?

## Limitations
- The seedless approach relies heavily on the quality and completeness of task descriptions, which may limit effectiveness for tasks with ambiguous or underspecified instructions.
- Self-correction depends on the LLM's understanding of the task, which may vary across different task types and domains.
- The evaluation focuses on SuperGLUE tasks, which may not generalize to other domains or more complex reasoning tasks.

## Confidence
- High confidence: The synthetic dataset exhibits comparable or higher diversity metrics and lower semantic similarity than original datasets
- Medium confidence: Model performance on synthetic data matches or slightly exceeds original data
- Medium confidence: The seedless nature provides practical advantages for dataset generation

## Next Checks
1. Test TarGEN on a diverse set of tasks outside SuperGLUE (e.g., from BigBench or other benchmarks) to evaluate generalizability across different task types and domains.
2. Conduct ablation studies to measure the individual contributions of context generation, instance seed generation, and self-correction to final dataset quality.
3. Evaluate the robustness of self-correction by intentionally introducing varying levels of noise in the generated data and measuring correction accuracy.