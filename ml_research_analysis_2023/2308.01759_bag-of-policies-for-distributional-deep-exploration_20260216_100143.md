---
ver: rpa2
title: Bag of Policies for Distributional Deep Exploration
arxiv_id: '2308.01759'
source_url: https://arxiv.org/abs/2308.01759
tags:
- learning
- exploration
- return
- heads
- distributional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficient exploration in complex
  environments, specifically in distributional reinforcement learning (RL). The authors
  propose Bag of Policies (BoP), a general-purpose approach that can be built on top
  of any return distribution estimator by maintaining a population of its copies.
---

# Bag of Policies for Distributional Deep Exploration

## Quick Facts
- **arXiv ID:** 2308.01759
- **Source URL:** https://arxiv.org/abs/2308.01759
- **Reference count:** 6
- **Primary result:** BoP improves exploration efficiency in distributional RL by maintaining an ensemble of policy heads that learn distinct learning signals through Thompson sampling.

## Executive Summary
This paper addresses the challenge of efficient exploration in complex environments by proposing Bag of Policies (BoP), a general-purpose approach that maintains a population of distributional actor-critic copies. BoP implements Thompson sampling through an ensemble of heads, where each episode is controlled by a single randomly selected head while all heads learn from the collected data off-policy. This creates distinct learning signals that diversify behavior and enable deep exploration. The authors demonstrate that BoP, built on top of Bayesian Distributional Policy Gradients (BDPG), provides greater robustness and speed during learning on Atari games compared to other multi-worker algorithms.

## Method Summary
BoP is an ensemble-based exploration method that maintains multiple distributional actor-critic heads. Each episode is controlled by one randomly sampled head, with all heads learning from the collected data through off-policy updates. The method generates distinct Bellman targets for each head while sharing the same underlying data, creating unique learning signals. BoP is implemented with BDPG, approximating posterior distributions of both return distributions and policies. The framework uses convolutional feature extractors, per-head actor-critic networks, and V-trace for off-policy advantage estimation, with training occurring in parallel across multiple environments.

## Key Results
- BoP significantly improves sample efficiency compared to single-head BDPG on Atari games
- The method outperforms other multi-worker algorithms in most tested environments
- The combination of global ensemble-based exploration and local curiosity bonus creates accumulatable optimism effects
- BoP demonstrates greater robustness during learning compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Thompson sampling via multiple distributional heads encourages deep exploration by maintaining uncertainty about which policy is optimal.
- Mechanism: Each episode is controlled by one randomly sampled head, but all heads learn from the same data off-policy. This setup allows each head to specialize in different uncertain regions of the state space, creating an ensemble of policies that explore more thoroughly than a single policy.
- Core assumption: The diversity between heads is sufficient to maintain distinct learning signals for each head.
- Evidence anchors:
  - [abstract] "BoP consists of an ensemble of multiple heads that are updated independently...leading to distinct learning signals for each head which diversify learning and behaviour."
  - [section] "Crucially, we are now able to leverage the epistemic uncertainty of the agent in a two-pronged manner: First, through the Thompson sampling enabled by an ensemble of policies and return distributions (heads)."
  - [corpus] Weak evidence - no direct mention of Thompson sampling in corpus papers.

### Mechanism 2
- Claim: The combination of global ensemble-based exploration and local curiosity bonus creates accumulatable optimism.
- Mechanism: BDPG already provides a local curiosity bonus based on epistemic uncertainty. By adding Thompson sampling through an ensemble, BoP combines both global (ensemble-based) and local (curiosity-based) optimism, which can accumulate to improve exploration efficiency.
- Core assumption: Global and local optimism can work synergistically rather than redundantly.
- Evidence anchors:
  - [abstract] "As BDPG is already an optimistic method, this pairing helps to investigate if optimism is accumulatable in distributional RL."
  - [section] "Another benefit of building upon BDPG is that it allows to analyze global posterior uncertainty along with local curiosity bonus simultaneously for exploration."
  - [corpus] Weak evidence - no direct mention of accumulatable optimism in corpus papers.

### Mechanism 3
- Claim: Off-policy training of all heads on the same data while maintaining distinct Bellman targets preserves diversity.
- Mechanism: All heads learn from the same data collected by the active head, but each head maintains its own Bellman target. This creates unique learning signals for each head while maintaining data efficiency.
- Core assumption: The combination of shared data and distinct Bellman targets is sufficient to maintain head diversity.
- Evidence anchors:
  - [section] "During training, each episode is controlled by only one of the heads and the collected state-action pairs are used to update all heads off-policy, leading to distinct learning signals for each head which diversify learning and behaviour."
  - [section] "each head i can form a return estimate gi t at the current timestep and produce a Bellman target xi t by sampling from itself for each of the future timesteps."
  - [corpus] Weak evidence - no direct mention of off-policy training with distinct Bellman targets in corpus papers.

## Foundational Learning

- **Distributional Reinforcement Learning (DiRL)**: Understanding how DiRL learns the full return distribution rather than just the mean is crucial since BoP is built on distributional RL methods. Quick check: What is the key difference between DiRL and standard RL in terms of what they learn about the return?

- **Thompson Sampling**: BoP uses Thompson sampling through its ensemble of policies to enable deep exploration. Quick check: How does Thompson sampling differ from epsilon-greedy exploration in terms of how actions are selected?

- **Epistemic vs Aleatoric Uncertainty**: BoP leverages both types of uncertainty - epistemic uncertainty through the ensemble and aleatoric uncertainty through the return distribution modeling. Quick check: What is the difference between epistemic uncertainty and aleatoric uncertainty in the context of RL?

## Architecture Onboarding

- **Component map**: Input (pixels) -> Convolutional feature extractor -> Ensemble of K heads (each with actor network, critic network, and return distribution model) -> Output (action probabilities and value distributions)

- **Critical path**:
  1. Sample active head k
  2. Roll out episode with policy Ï€k
  3. Collect state-action-reward tuples
  4. For each head i, generate return samples gi t
  5. Compute advantages Ai t and Bellman targets xi t for each head
  6. Update all components using the collected data

- **Design tradeoffs**:
  - Number of heads: More heads provide better exploration but increase computation time linearly
  - Off-policy vs on-policy: All heads learn off-policy from the same data for efficiency, but only the active head learns on-policy
  - Shared vs individual Bellman targets: Individual targets maintain diversity but reduce data efficiency

- **Failure signatures**:
  - All heads converging to similar policies (loss of diversity)
  - No improvement in exploration despite multiple heads
  - Training instability due to off-policy learning

- **First 3 experiments**:
  1. Implement single-head version of BoP and verify it matches BDPG performance
  2. Add 3 heads and test on a simple exploration problem (e.g., Montezuma's Revenge)
  3. Compare performance with varying numbers of heads (1, 3, 5) on multiple Atari games

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Bag of Policies (BoP) algorithm outperform other multi-worker/multi-head algorithms in terms of asymptotic performance, or is the improvement primarily in sample efficiency?
- Basis in paper: [inferred] The paper mentions that BoP improves significantly on sample efficiency compared to BDPG, but does not necessarily improve on asymptotic performance. It also states that BoP outperforms other multi-worker algorithms in most selected environments.
- Why unresolved: The paper does not provide a direct comparison of asymptotic performance between BoP and other algorithms.
- What evidence would resolve it: Experimental results comparing the asymptotic performance of BoP with other multi-worker/multi-head algorithms on a diverse set of tasks.

### Open Question 2
- Question: What is the optimal number of ensemble members (heads) for the Bag of Policies (BoP) algorithm, and how does it vary across different environments?
- Basis in paper: [explicit] The paper discusses the impact of the number of ensemble members on the performance of BoP, stating that a small number of 3 to 5 heads strikes a good balance between effective exploration and training speed.
- Why unresolved: The paper does not provide a definitive answer on the optimal number of heads for different environments.
- What evidence would resolve it: Systematic experiments varying the number of heads in BoP across different environments and analyzing the trade-off between performance and computational cost.

### Open Question 3
- Question: Can the Bag of Policies (BoP) algorithm be extended to continuous action spaces, and how would it compare to existing methods for continuous control?
- Basis in paper: [inferred] The paper focuses on discrete action spaces, but the concept of BoP could potentially be applied to continuous action spaces by using a distributional actor-critic approach.
- Why unresolved: The paper does not explore the application of BoP to continuous action spaces.
- What evidence would resolve it: Implementation and evaluation of BoP in continuous control tasks, comparing its performance with state-of-the-art methods like DDPG, TD3, and SAC.

## Limitations

- The evidence for BoP's mechanisms relies heavily on theoretical claims rather than direct experimental validation
- There is limited analysis of why the method works beyond correlation with performance metrics
- The claim about "accumulatable optimism" is speculative without experimental isolation of global vs local exploration effects

## Confidence

- **Mechanism 1 (Thompson sampling for deep exploration)**: Medium confidence - The theoretical framework is sound, but there's no direct evidence that the diversity between heads is maintained throughout training or that Thompson sampling is actually occurring.
- **Mechanism 2 (Accumulatable optimism)**: Low confidence - This is largely a theoretical claim without experimental validation. The paper doesn't provide evidence that global and local optimism work synergistically rather than redundantly.
- **Mechanism 3 (Off-policy training with distinct Bellman targets)**: Medium confidence - The mechanism is clearly described, but there's no empirical validation that this specific combination is what maintains head diversity.

## Next Checks

1. **Diversity monitoring experiment**: Implement a metric to track policy similarity (e.g., cosine similarity or KL divergence) between all pairs of heads throughout training. This would directly test whether Mechanism 1's assumption about maintaining diversity holds in practice.

2. **Ablation study on exploration components**: Create variants that isolate global exploration (ensemble-based) from local exploration (curiosity bonus) by training with only one component at a time. This would test the "accumulatable optimism" claim in Mechanism 2.

3. **Single-head comparison with curiosity bonus**: Implement a single-head version of BoP with the same curiosity bonus as BDPG. Compare its performance to both standard BDPG and full BoP to determine whether the ensemble exploration adds value beyond what the local curiosity bonus already provides.