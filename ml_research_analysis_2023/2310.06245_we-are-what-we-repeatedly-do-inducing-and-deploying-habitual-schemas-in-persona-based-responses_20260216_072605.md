---
ver: rpa2
title: 'We are what we repeatedly do: Inducing and deploying habitual schemas in persona-based
  responses'
arxiv_id: '2310.06245'
source_url: https://arxiv.org/abs/2310.06245
tags:
- schema
- generation
- event
- responses
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to persona-based dialogue
  generation that leverages explicit event schemas to capture habitual knowledge associated
  with personas. The method involves inducing schemas from unstructured persona facts
  using large language models (LLMs) to generate generic passages, and then retrieving
  relevant schemas to condition an LLM for generating dialogue responses.
---

# We are what we repeatedly do: Inducing and deploying habitual schemas in persona-based responses

## Quick Facts
- arXiv ID: 2310.06245
- Source URL: https://arxiv.org/abs/2310.06245
- Reference count: 12
- Primary result: Schema-based generation improves persona-based dialogue response diversity and engagement compared to baseline LLM generation

## Executive Summary
This paper introduces a novel approach to persona-based dialogue generation that leverages explicit event schemas to capture habitual knowledge associated with personas. The method involves inducing schemas from unstructured persona facts using large language models (LLMs) to generate generic passages, and then retrieving relevant schemas to condition an LLM for generating dialogue responses. Two generation modes are explored: unconstrained generation and few-shot paraphrase generation. The approach is evaluated on the PersonaChat dataset, showing that schema-based generation improves response diversity and engagement compared to baselines, while maintaining controllability through the paraphrase mode.

## Method Summary
The proposed method involves inducing event schemas from persona facts using GPT-3.5-TURBO with few-shot prompting. The schema induction pipeline generates generic passages from persona facts, then induces structured schemas with headers, preconditions, episodes, and other components. During dialogue, a multi-level retrieval system using sentence transformer embeddings identifies relevant schemas based on dialogue context. The LLM then generates responses conditioned on the dialogue history and retrieved schema facts, with two generation modes: unconstrained generation and few-shot paraphrase generation where the LLM is prompted to rephrase a given sentence while maintaining persona consistency.

## Key Results
- Schema-based generation shows moderate human preferences over baseline LLM generation in terms of engagement and relevance
- Generated schemas capture typical knowledge associated with events, demonstrating effective schema induction
- Unconstrained generation mode produces more diverse responses while paraphrase mode provides better controllability
- Automatic evaluation shows improved response diversity metrics for schema-based methods

## Why This Works (Mechanism)

### Mechanism 1
Schema induction creates structured representations of habitual knowledge from persona facts. LLM generates generic passages describing typical events from persona facts, then another LLM induces structured schemas from those passages. Core assumption: LLMs can reliably translate unstructured persona facts into coherent narrative descriptions and then extract structured knowledge from those narratives.

### Mechanism 2
Schema retrieval improves response relevance by providing context-specific habitual knowledge. Sentence transformer embeddings are computed for both schemas and dialogue turns; the most similar schema is retrieved and its facts are used to condition the LLM for response generation. Core assumption: Semantic similarity between dialogue context and schema embeddings is sufficient to identify relevant schemas.

### Mechanism 3
Few-shot paraphrasing enables controllable response generation while maintaining persona consistency. LLM is prompted with examples of paraphrasing and the dialogue history + schema facts to rephrase a given raw utterance into a persona-consistent response. Core assumption: LLMs can learn the paraphrasing task from in-context examples and apply it correctly when conditioned on schema knowledge.

## Foundational Learning

- Concept: Schema representation of habitual knowledge
  - Why needed here: Schemas capture the structured, typical knowledge about events that personas engage in, which is more expressive than simple fact lists and enables richer dialogue generation.
  - Quick check question: What are the key components of an event schema as defined in this paper?

- Concept: Large language model in-context learning
  - Why needed here: Both schema induction and response generation rely on conditioning LLMs with examples and knowledge rather than fine-tuning, making the approach more flexible and data-efficient.
  - Quick check question: How does in-context learning differ from traditional fine-tuning in terms of model adaptation?

- Concept: Information retrieval for dialogue systems
  - Why needed here: Schema retrieval selects relevant habitual knowledge based on dialogue context, ensuring that generated responses are topically appropriate.
  - Quick check question: What similarity metric is used to match dialogue context with schema embeddings?

## Architecture Onboarding

- Component map: Persona facts → Schema induction pipeline → Schema storage → Schema retrieval → Response generation pipeline → Two generation modes (unconstrained and few-shot paraphrase)

- Critical path: Persona facts → Schema induction → Schema storage → Schema retrieval (during dialogue) → Response generation

- Design tradeoffs:
  - Using LLMs for schema induction vs. hand-coding schemas: LLMs are more flexible and scalable but introduce potential for hallucination and require careful prompt engineering
  - Unconstrained vs. paraphrase generation: Unconstrained allows more creative responses but less control; paraphrase is more controllable but may be more constrained in output diversity

- Failure signatures:
  - Schema induction produces incoherent or irrelevant schemas
  - Retrieval system selects wrong schemas (low relevance)
  - Response generation ignores schema facts or hallucinates incorrect information
  - Paraphrase mode fails to preserve the meaning of the raw utterance

- First 3 experiments:
  1. Validate schema induction by checking if generated schemas capture typical knowledge (human evaluation as done in the paper)
  2. Test schema retrieval accuracy by measuring relevance of retrieved schemas to various dialogue contexts
  3. Compare response quality (diversity, relevance, controllability) between baseline, unconstrained, and paraphrase modes using both automatic and human evaluation metrics

## Open Questions the Paper Calls Out
- How does the proposed method perform in languages other than English?
- The paper acknowledges that the experiments were limited to the English language and that performance may degrade in lower-resource languages.

## Limitations
- Schema induction process relies heavily on LLM capabilities without rigorous evaluation of hallucination risks or factual consistency
- Limited evaluation of schema retrieval accuracy and the impact of retrieved schemas on response quality
- Few-shot paraphrase mode's effectiveness depends on prompt engineering quality, which is not fully disclosed

## Confidence
- Schema induction effectiveness: **Medium** - Human evaluation shows schemas capture typical knowledge, but no quantitative analysis of schema quality or hallucination rates
- Retrieval system performance: **Low** - Limited evaluation of retrieval accuracy or schema-context matching quality
- Response quality improvements: **Medium** - Human evaluations show moderate preferences for schema-based methods, but automatic metrics show mixed results

## Next Checks
1. Conduct quantitative analysis of schema fact accuracy and hallucination rates through fact-checking against external knowledge sources
2. Evaluate schema retrieval accuracy using precision@k metrics on a held-out test set of dialogue-schema pairs
3. Perform ablation study removing schema conditioning to measure its direct contribution to response quality improvements