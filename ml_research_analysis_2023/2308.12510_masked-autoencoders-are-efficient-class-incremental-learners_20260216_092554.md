---
ver: rpa2
title: Masked Autoencoders are Efficient Class Incremental Learners
arxiv_id: '2308.12510'
source_url: https://arxiv.org/abs/2308.12510
tags:
- learning
- task
- tasks
- replay
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Class Incremental Learning (CIL) aims to learn new classes sequentially
  while avoiding catastrophic forgetting of previous knowledge. This work introduces
  Masked Autoencoders (MAEs) as efficient learners for CIL, leveraging their ability
  to reconstruct original images from randomly selected patches.
---

# Masked Autoencoders are Efficient Class Incremental Learners

## Quick Facts
- arXiv ID: 2308.12510
- Source URL: https://arxiv.org/abs/2308.12510
- Reference count: 40
- Primary result: Achieves 79.12% average accuracy and 68.40% last-phase accuracy on CIFAR-100 (10-task setting), outperforming previous methods by 4-6% absolute gain

## Executive Summary
This work introduces Masked Autoencoders (MAEs) as efficient learners for Class Incremental Learning (CIL), addressing catastrophic forgetting by leveraging MAE's ability to reconstruct images from randomly selected patches. The bilateral MAE framework combines image-level and embedding-level fusion to improve reconstruction quality and representation stability. By storing exemplars as random patches rather than full images, the method achieves up to 4× storage efficiency while maintaining state-of-the-art performance on CIFAR-100, ImageNet-Subset, and ImageNet-Full datasets.

## Method Summary
The method uses a bilateral MAE architecture with two complementary branches - main and detailed - fused at both image and embedding levels. During training, images are masked at 75% ratio, allowing efficient storage of unmasked patches and indices instead of full images. The main branch captures global structure while the detailed branch uses frequency masking to extract high-frequency details. This dual supervision stabilizes learning and enriches replay data, with exemplars reconstructed during inference to prevent forgetting of previous tasks.

## Key Results
- Achieves 79.12% average accuracy and 68.40% last-phase accuracy on CIFAR-100 (10-task setting)
- Outperforms previous methods by 4-6% absolute gain
- Stores 4× more exemplars compared to conventional replay methods
- Demonstrates superior stability compared to GAN-based replay approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masked Autoencoders can reliably reconstruct original images from randomly selected patches, enabling efficient exemplar storage for replay.
- Mechanism: By masking a high proportion of patches (e.g., 75%) during training, MAE learns to reconstruct complete images from sparse inputs. This allows storing only the unmasked patches and their indices instead of full images, reducing storage needs by ~4x.
- Core assumption: The decoder can generalize from partial cues to reconstruct high-quality images that preserve task-relevant information.
- Evidence anchors:
  - [abstract]: "MAEs can reliably reconstruct original input images from randomly selected patches, which we use to store exemplars from past tasks more efficiently for CIL."
  - [section]: "A masking ratio of 0.75 allows us to save 4× the number of (reconstructible) samples compared to conventional replay methods."
- Break condition: If the masking ratio is too high (>95%) or too low (<10%), reconstruction quality degrades, reducing replay data utility.

### Mechanism 2
- Claim: Bilateral MAE with image-level and embedding-level fusion improves reconstruction quality and representation stability.
- Mechanism: Two complementary branches—main and detailed—are fused at both image and embedding levels. The main branch captures global structure while the detailed branch extracts high-frequency details via frequency masking. This dual supervision stabilizes learning and enriches replay data.
- Core assumption: Frequency-domain supervision in the detailed branch enhances attention to high-frequency details critical for realistic reconstruction.
- Evidence anchors:
  - [section]: "Embedding-level fusion from the two branches also maintains stable and diverse embeddings, and our framework can thus achieve a better trade-off between plasticity and stability."
  - [section]: "We define a frequency-masking function M(·) that converts its argument to the frequency domain, then masks out low frequencies using a circular mask around the origin."
- Break condition: If frequency masking is not used, the detailed branch may not effectively capture high-frequency details, reducing replay data diversity.

### Mechanism 3
- Claim: MAE-based replay is more stable than GAN-based replay because it uses partial cues to infer global information.
- Mechanism: Unlike GANs, which can suffer from mode collapse or unstable generation across tasks, MAE reconstructs images from fixed patches in a task-agnostic way, avoiding forgetting in the generative model.
- Core assumption: Task-agnostic reconstruction from patches is more stable than task-specific generative modeling.
- Evidence anchors:
  - [abstract]: "Compared to previous generative methods, replay by MAE is more stable because it uses partial cues to infer global information, which is task-agnostic and suffers less forgetting across tasks."
  - [section]: "This relieves the unstable generation effect of GANs across tasks with stationary image patches."
- Break condition: If patches are not representative of the full image distribution, reconstruction may fail to capture task-relevant variations.

## Foundational Learning

- Concept: Class Incremental Learning (CIL)
  - Why needed here: The entire method is designed to prevent catastrophic forgetting in sequential learning of new classes.
  - Quick check question: What is the primary challenge CIL aims to solve compared to standard classification?

- Concept: Masked Autoencoders (MAEs)
  - Why needed here: MAEs are the core architecture enabling efficient replay data generation and self-supervised representation learning.
  - Quick check question: How does the masking ratio affect both reconstruction quality and storage efficiency?

- Concept: Embedding-level and Image-level Fusion
  - Why needed here: Fusion techniques are used to combine complementary information from two MAE branches, improving both classification and reconstruction.
  - Quick check question: Why is frequency-domain supervision used in the detailed branch rather than spatial-domain supervision?

## Architecture Onboarding

- Component map:
  - Input: Masked image patches (75% masked by default)
  - Main Branch: MAE encoder → embedding fusion → classifier + shared decoder
  - Detailed Branch: MLP layer → frequency-masked reconstruction → detailed loss
  - Output: Classification probabilities + reconstructed image for replay

- Critical path:
  1. Apply random masking to input image
  2. Pass through both MAE branches
  3. Compute classification, reconstruction, and detailed losses
  4. Backpropagate combined loss
  5. Store unmasked patches and indices for replay

- Design tradeoffs:
  - Masking ratio: Higher ratios save more storage but risk poorer reconstruction
  - Branch complexity: Two branches improve quality but increase parameters
  - Frequency masking: Enhances detail capture but adds computational overhead

- Failure signatures:
  - Poor reconstruction quality → increase masking ratio or adjust loss weights
  - High forgetting rates → verify replay data diversity and embedding stability
  - Slow convergence → check masking ratio balance between branches

- First 3 experiments:
  1. Baseline MAE with single branch and standard reconstruction loss
  2. Bilateral MAE with frequency-domain detailed loss (vary r2)
  3. Full system with exemplar storage optimization (vary masking ratio r)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of bilateral MAE scale with larger datasets beyond ImageNet-Full, and what are the practical limits of this approach?
- Basis in paper: [explicit] The paper mentions experiments on CIFAR-100, ImageNet-Subset, and ImageNet-Full, but does not explore larger datasets or scalability limits.
- Why unresolved: The paper only tests on medium-scale datasets and does not provide insights into how the method would perform on much larger datasets or in scenarios with significantly more classes or tasks.
- What evidence would resolve it: Results from experiments on larger datasets (e.g., JFT-300M, Open Images) or synthetic scalability tests showing performance degradation points would provide insights.

### Open Question 2
- Question: What is the optimal trade-off between masking ratio and exemplar storage efficiency across different incremental learning scenarios?
- Basis in paper: [explicit] The paper explores masking ratios (0.4, 0.75, 0.90) but notes the trade-off between reconstruction quality and storage efficiency without identifying universal optimal parameters.
- Why unresolved: Different datasets and task distributions might benefit from different masking ratios, and the paper only provides results for a limited set of ratios on specific datasets.
- What evidence would resolve it: Systematic ablation studies across diverse datasets and task configurations showing optimal masking ratios for different scenarios.

### Open Question 3
- Question: How does the bilateral MAE framework compare to other self-supervised learning methods for CIL in terms of computational efficiency and memory usage during inference?
- Basis in paper: [inferred] The paper focuses on training efficiency and storage but does not provide detailed comparisons of inference-time computational requirements compared to other self-supervised CIL methods.
- Why unresolved: While the paper demonstrates superior accuracy, it does not address whether the method's benefits come at the cost of increased inference-time computation or memory requirements.
- What evidence would resolve it: Detailed profiling of inference-time computational costs and memory usage compared to other state-of-the-art CIL methods across different hardware platforms.

## Limitations
- Masking ratio optimization is dataset-dependent and may not generalize across different task distributions
- Exemplar storage, while more efficient than full images, still requires maintaining growing replay buffers
- Method's computational overhead during inference due to dual-branch architecture and reconstruction requirements

## Confidence
- **High confidence**: MAE-based replay is more stable than GAN-based replay due to task-agnostic reconstruction from patches
- **High confidence**: Bilateral MAE with dual branches and fusion improves both reconstruction quality and classification stability
- **Medium confidence**: The 4× storage efficiency gain is dataset-dependent and may vary with masking ratio optimization
- **Medium confidence**: Performance improvements over state-of-the-art methods are consistent but may not transfer to non-image domains

## Next Checks
1. Test the masking ratio sensitivity by training with r=0.5, r=0.75, and r=0.9 on the same datasets to quantify the reconstruction-storage tradeoff
2. Implement ablation studies removing frequency masking from the detailed branch to isolate its contribution to high-frequency detail preservation
3. Evaluate performance degradation when exemplar storage is constrained to fixed memory budgets across all tasks to test real-world scalability