---
ver: rpa2
title: Finding the right XAI method -- A Guide for the Evaluation and Ranking of Explainable
  AI Methods in Climate Science
arxiv_id: '2303.00652'
source_url: https://arxiv.org/abs/2303.00652
tags:
- explanation
- methods
- network
- evaluation
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of selecting appropriate Explainable
  Artificial Intelligence (XAI) methods for climate science applications, where ground
  truth explanations are typically unavailable. The authors introduce a systematic
  evaluation framework based on five key properties: robustness, faithfulness, randomization,
  complexity, and localization.'
---

# Finding the right XAI method -- A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science

## Quick Facts
- arXiv ID: 2303.00652
- Source URL: https://arxiv.org/abs/2303.00652
- Reference count: 33
- Primary result: Input-contribution XAI methods (Integrated Gradients, LRP variants, InputGradients) outperform gradient-only methods in climate science explanations

## Executive Summary
This paper addresses the challenge of selecting appropriate Explainable AI (XAI) methods for climate science applications where ground truth explanations are unavailable. The authors introduce a systematic evaluation framework based on five key properties: robustness, faithfulness, randomization, complexity, and localization. Through comprehensive testing on a temperature prediction task using both MLP and CNN architectures, the study finds that methods considering input contributions consistently perform best across multiple evaluation metrics. The research also reveals that theoretically promising explanation-enhancing techniques like SmoothGrad and Integrated Gradients do not improve performance as expected in climate applications.

## Method Summary
The framework evaluates XAI methods using five properties measured through perturbation-based metrics. Temperature maps from CESM1 climate simulations are preprocessed and used to train both MLP and CNN architectures for decade prediction. Multiple XAI methods (Integrated Gradients, LRP variants, InputGradients, SmoothGrad, NoiseGrad, FusionGrad, and Gradient) are applied and evaluated using the proposed framework. Results are compared against a random baseline and visualized using spyder plots for method ranking. The framework provides practical guidance for selecting XAI methods based on their performance across different properties.

## Key Results
- Input-contribution methods (Integrated Gradients, LRP variants, InputGradients) show superior performance in faithfulness, robustness, and complexity
- Gradient-based explanation enhancement techniques (SmoothGrad, NoiseGrad, FusionGrad) fail to improve performance as theoretically expected
- Architecture-dependent differences exist between MLP and CNN in explanation properties
- LRP-z and InputGradients emerge as optimal choices for the climate classification task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XAI evaluation metrics can distinguish between explanation methods based on multiple properties even without ground truth.
- Mechanism: The framework uses multiple complementary metrics (robustness, faithfulness, randomization, complexity, localization) that capture different aspects of explanation quality. Each metric measures a specific property through perturbation-based analysis, allowing quantitative comparison.
- Core assumption: Properties like robustness and faithfulness can be meaningfully measured through controlled perturbations without requiring ground truth explanations.
- Evidence anchors:
  - [abstract] "we introduce XAI evaluation in the climate context and discuss different desired explanation properties, namely robustness, faithfulness, randomization, complexity, and localization"
  - [section] "XAI research has developed metrics that assess different properties an explanation method should fulfill"
- Break condition: If the perturbation patterns don't meaningfully distinguish between methods, or if the chosen metrics don't capture the properties relevant to the specific application.

### Mechanism 2
- Claim: Explanation methods that consider input contributions perform better in faithfulness, robustness, and complexity.
- Mechanism: Methods like Integrated Gradients, InputGradients, and LRP variants multiply the gradient by the input or propagate relevance through the network, which captures the actual contribution of features to the prediction. This leads to more stable and interpretable explanations.
- Core assumption: Input contributions contain meaningful information about feature relevance that gradient-only methods miss.
- Evidence anchors:
  - [abstract] "we find that XAI methods Integrated Gradients, layer-wise relevance propagation, and input times gradients exhibit considerable robustness, faithfulness, and complexity"
  - [section] "explanation methods with input contribution, such as Integrated Gradients, InputGradients, and LRP, consistently achieved the best rankings in faithfulness, robustness, and complexity"
- Break condition: If the input contributions are corrupted by noise or if the network architecture makes input contributions irrelevant to the prediction.

### Mechanism 3
- Claim: Gradient-based explanation enhancement techniques (SmoothGrad, NoiseGrad, FusionGrad) don't improve performance as theoretically expected in climate applications.
- Mechanism: These methods add perturbations to either the input or network parameters and average the resulting explanations. While theoretically designed to reduce noise, in practice they may average out important evidence when dealing with high internal variability in climate data.
- Core assumption: The theoretical benefits of perturbation averaging apply less in domains with high inherent variability and uncertainty.
- Evidence anchors:
  - [abstract] "explanations using input perturbations, such as SmoothGrad and Integrated Gradients, do not improve robustness and faithfulness, contrary to previous claims"
  - [section] "we find consistent or decreasing ranks of these XAI methods compared to the explanation method which is used as a baseline for enhancements"
- Break condition: If the climate data has lower variability or if the network is more robust to perturbations, the enhancement techniques might work as intended.

## Foundational Learning

- Concept: Perturbation-based evaluation methodology
  - Why needed here: The core of XAI evaluation relies on measuring how explanations change under controlled perturbations to assess properties like robustness and faithfulness.
  - Quick check question: How would you design a perturbation experiment to test whether an explanation method is robust to small input changes?

- Concept: Multi-metric evaluation framework
  - Why needed here: No single metric can capture all aspects of explanation quality; multiple complementary metrics are needed to provide a comprehensive assessment.
  - Quick check question: Why might you need both a robustness metric and a faithfulness metric to fully evaluate an explanation method?

- Concept: Normalization of evaluation scores across methods
  - Why needed here: Different explanation methods may produce explanations on different scales, requiring normalization to make comparison meaningful.
  - Quick check question: What normalization approach would you use if you want higher scores to always indicate better performance?

## Architecture Onboarding

- Component map: Data Preprocessing -> Model Training -> Explanation Generation -> Evaluation -> Ranking -> Selection
- Critical path: Data → Model Training → Explanation Generation → Evaluation → Ranking → Selection. Each step must complete successfully for the final recommendation to be valid.
- Design tradeoffs: The framework trades computational complexity (evaluating multiple metrics across multiple explanation methods) for comprehensiveness in evaluation. Perturbation-based metrics are computationally expensive but necessary for property assessment.
- Failure signatures: Inconsistent rankings across metrics suggest either metric incompatibility with the data domain or fundamental differences in explanation quality. Low scores across all metrics suggest either poor explanation methods or evaluation metric incompatibility.
- First 3 experiments:
  1. Run the framework on a simple synthetic dataset where ground truth explanations are known to validate metric behavior.
  2. Compare MLP and CNN explanations on the same task to identify architecture-dependent differences.
  3. Test the random baseline evaluation to ensure metrics can distinguish random explanations from meaningful ones.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do explanation-enhancing techniques like SmoothGrad and Integrated Gradients fail to improve performance in climate science applications despite theoretical expectations?
- Basis in paper: [explicit] The paper explicitly states that "explanations using input perturbations, such as SmoothGrad and Integrated Gradients, do not improve robustness and faithfulness, contrary to previous claims."
- Why unresolved: The paper attributes this to the high internal variability in climate data leading to increased network uncertainty, but does not provide a detailed mechanistic explanation for why the smoothing effect doesn't work as intended.
- What evidence would resolve it: Comparative experiments testing these methods on datasets with varying levels of noise and variability, or theoretical analysis showing how the perturbation averaging interacts with climate data properties.

### Open Question 2
- Question: How do architecture-dependent differences (MLP vs CNN) affect the localization and complexity properties of XAI methods in climate science?
- Basis in paper: [explicit] The paper states "We find architecture-dependent performance differences regarding robustness, complexity and localization skills of different XAI methods."
- Why unresolved: While the paper observes differences, it doesn't provide a deep analysis of why CNNs show more clustered evidence compared to MLPs or how this impacts the effectiveness of different XAI methods.
- What evidence would resolve it: Detailed visualization and analysis of learned features in both architectures, or ablation studies systematically varying architectural components.

### Open Question 3
- Question: Why does the Random Logit (RL) metric perform poorly for climate classification tasks compared to the Model Parameter Test (MPT) metric?
- Basis in paper: [explicit] The paper states "we find that the MPT metric is favorable for classification tasks defined on continuous data" and suggests this is due to "close temperature map resemblance would lead to a decreasing score."
- Why unresolved: The paper provides an explanation but doesn't explore alternative metrics or validate this hypothesis through additional experiments.
- What evidence would resolve it: Experiments using datasets with more distinct class boundaries, or development of hybrid metrics that combine the strengths of both RL and MPT approaches.

## Limitations
- The framework's effectiveness depends on perturbation-based metrics which may be affected by climate data's inherent high internal variability
- The study focuses exclusively on temperature prediction, limiting generalizability to other climate variables
- Computational cost of evaluating multiple metrics across numerous explanation methods may be prohibitive for larger-scale applications

## Confidence
- **High Confidence**: Input-contribution methods (IG, InputGradients, LRP) consistently outperform gradient-only methods in faithfulness, robustness, and complexity
- **Medium Confidence**: Explanation-enhancing techniques (SmoothGrad, NoiseGrad, FusionGrad) do not improve performance as theoretically expected in climate applications
- **Medium Confidence**: The ranking framework provides practical guidance for method selection

## Next Checks
1. Validate the framework on a synthetic dataset with known ground truth explanations to confirm metric behavior and identify potential limitations in perturbation-based evaluation.

2. Test the ranking methodology across different climate prediction tasks (e.g., precipitation, extreme events) to assess generalizability beyond temperature prediction.

3. Conduct ablation studies by systematically removing individual metrics to determine which properties contribute most to distinguishing between explanation methods in climate science contexts.