---
ver: rpa2
title: Neural Network-Based Rule Models With Truth Tables
arxiv_id: '2309.09638'
source_url: https://arxiv.org/abs/2309.09638
tags:
- rules
- datasets
- tt-rules
- dataset
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a neural network framework, called TT-rules,
  that transforms a deep neural network into an interpretable rule-based model. The
  method extracts rules from a trained TTnet model, optimizes the rule set by reducing
  rule size and number, and visualizes the rules using Reduced Ordered Binary Decision
  Diagrams.
---

# Neural Network-Based Rule Models With Truth Tables

## Quick Facts
- arXiv ID: 2309.09638
- Source URL: https://arxiv.org/abs/2309.09638
- Reference count: 40
- This paper introduces TT-rules, a framework that transforms neural networks into interpretable rule-based models, achieving equal or higher performance compared to other interpretable methods while maintaining a balance between performance and complexity.

## Executive Summary
This paper introduces TT-rules, a neural network framework that transforms a trained TTnet model into an interpretable rule-based model. The framework extracts rules from the neural network, optimizes them through two post-training steps (DCT injection and Truth Table Correlation), and visualizes them using Reduced Ordered Binary Decision Diagrams. TT-rules achieves equal or higher performance compared to other interpretable methods while maintaining a balance between performance and complexity, and notably presents the first accurate rule-based model capable of fitting large tabular datasets, including two real-life DNA datasets with over 20K features.

## Method Summary
TT-rules is built upon Truth Table nets (TTnet), which use Learning Truth Table (LTT) blocks as fundamental components. The framework extracts rules from a trained TTnet model in DNF form, then applies two optimization steps: DCT injection to reduce rule size and Truth Table Correlation to reduce rule count. The optimized rules are then converted to ROBDD format for visualization and interpretation. The approach supports binary classification, multi-label classification, and regression tasks for tabular datasets.

## Key Results
- TT-rules achieves equal or higher performance compared to other interpretable methods
- Successfully scales to large tabular datasets, including DNA datasets with over 20K features
- Reduces model complexity by 14.3×, 34×, and 180× on three datasets (Compas, Adult, HELOC)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TT-rules achieves both global and exact interpretability by transforming a trained TTnet model into a rule-based model through extraction of necessary and sufficient rules
- Mechanism: TT-rules extracts rules from a trained TTnet model and converts these into a rule set R, then optimizes R by reducing rule size through DCT injection and reducing rule number through Truth Table Correlation metric
- Core assumption: The complete distribution of LTT blocks can be computed in practical and constant time
- Evidence anchors: [abstract], [section], corpus papers on rule-based learning and neuro-symbolic approaches

### Mechanism 2
- Claim: The framework scales to large datasets with tens of thousands of features by leveraging the architecture of TTnet and performing feature reduction during rule extraction
- Mechanism: TT-rules uses the scalable architecture of TTnet and reduces input features by generating a compressed rule set that represents essential decision logic
- Core assumption: LTT block architecture with n ≤ 9 inputs can effectively capture complex feature relationships through patch-based processing
- Evidence anchors: [abstract], [section], corpus papers on rule-based learning and neuro-symbolic approaches

### Mechanism 3
- Claim: TT-rules outperforms other interpretable methods by balancing performance and complexity through its two-step optimization process
- Mechanism: The framework first reduces rule size through DCT injection and then reduces rule number through Truth Table Correlation metric
- Core assumption: Optimization steps can reduce complexity without significant accuracy loss
- Evidence anchors: [abstract], [section], corpus papers on rule-based learning

## Foundational Learning

- Concept: Truth Table Nets (TTnet) and Learning Truth Table (LTT) blocks
  - Why needed here: TT-rules is built on top of TTnet, which uses LTT blocks as fundamental building components
  - Quick check question: How does the LTT block design ensure that the complete distribution can be computed in practical and constant time regardless of network complexity?

- Concept: Boolean formulas and Reduced Ordered Binary Decision Diagrams (ROBDDs)
  - Why needed here: Rules extracted from TTnet are transformed into ROBDDs for visualization and interpretation
  - Quick check question: What is the relationship between a DNF formula and its equivalent ROBDD representation, and why is ROBDD preferred for visualization?

- Concept: Rule-based model evaluation metrics (AUC, RMSE, accuracy, rule count, complexity)
  - Why needed here: The paper compares TT-rules against other interpretable methods using these metrics
  - Quick check question: How is the complexity of a rule-based model defined in this context, and why is it important for evaluating interpretability?

## Architecture Onboarding

- Component map: Input preprocessing layer (Batch Normalization + Step function) -> LTT block layers (CNN filters with binary inputs/outputs, n ≤ 9) -> Final layer (sparse binary or floating-point linear regression) -> Rule extraction module (DNF conversion using Quine-McCluskey) -> Optimization module (DCT injection and Truth Table Correlation) -> ROBDD visualization module

- Critical path: Input → LTT layers → Final layer → Rule extraction → DCT injection → Truth Table Correlation → ROBDD conversion

- Design tradeoffs:
  - Binary vs floating-point final layer: Binary provides stricter interpretability but may sacrifice some performance; floating-point offers better performance but reduced interpretability
  - Rule count vs rule size: More rules with smaller size may be easier to interpret than fewer rules with larger size
  - DCT injection vs exact rules: DCTs make rules more general and smaller but may introduce approximation

- Failure signatures:
  - Poor performance on test data despite good training performance: Possible overfitting or insufficient rule optimization
  - Very large number of rules: May indicate need for stronger Truth Table Correlation filtering
  - Rules that are too large to be interpretable: May require more aggressive DCT injection or different LTT block configuration

- First 3 experiments:
  1. Train a simple TTnet on the Adult dataset and extract the initial rule set R without optimization to verify the basic extraction mechanism works
  2. Apply DCT injection to the initial rule set and measure the reduction in rule size and any impact on accuracy
  3. Apply Truth Table Correlation filtering to further reduce rule count and evaluate the performance-complexity tradeoff

## Open Questions the Paper Calls Out

- Open Question 1: Can the TT-rules framework be extended to automatically incorporate human knowledge into the rule extraction process?
- Open Question 2: How does the performance of TT-rules compare to other interpretable models when dealing with non-tabular data such as images or text?
- Open Question 3: What is the impact of different DCT injection strategies on the performance and complexity of the rule set?
- Open Question 4: How does the TT-rules framework handle missing data in tabular datasets?
- Open Question 5: Can the TT-rules framework be adapted for online learning scenarios where data arrives sequentially?

## Limitations

- Experimental validation is constrained to seven datasets with limited comparison to state-of-the-art interpretable methods
- Scalability claims, while impressive, are demonstrated primarily on DNA datasets without extensive testing across diverse tabular data domains
- The computational complexity of extracting and optimizing rules for very large networks remains unclear

## Confidence

- High confidence in the core mechanism of transforming TTnet outputs into interpretable rule sets
- Medium confidence in the optimization procedures (DCT injection and Truth Table Correlation) due to limited ablation studies
- Low confidence in the generalizability of results across different data types and problem domains

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of DCT injection and Truth Table Correlation to final performance
2. Test the framework on additional tabular datasets with varying characteristics (categorical, mixed-type, missing values)
3. Compare rule interpretability and model performance against established interpretable ML methods like decision trees and linear models using standardized benchmarks