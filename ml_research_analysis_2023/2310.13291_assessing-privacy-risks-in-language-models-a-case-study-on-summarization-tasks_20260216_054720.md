---
ver: rpa2
title: 'Assessing Privacy Risks in Language Models: A Case Study on Summarization
  Tasks'
arxiv_id: '2310.13291'
source_url: https://arxiv.org/abs/2310.13291
tags:
- attack
- data
- training
- membership
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates membership inference attacks on large language
  models used for text summarization. It addresses the risk of these models inadvertently
  exposing whether specific document-summary pairs were part of their training data.
---

# Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks

## Quick Facts
- arXiv ID: 2310.13291
- Source URL: https://arxiv.org/abs/2310.13291
- Reference count: 17
- Key outcome: Membership inference attacks on summarization models achieve AUC scores above 50%, with reference similarity and robustness signals being most effective

## Executive Summary
This study investigates membership inference attacks on large language models used for text summarization tasks. The research demonstrates that attackers can infer whether specific document-summary pairs were part of a model's training data by exploiting two key signals: the similarity between generated and reference summaries, and the robustness of generated summaries to input perturbations. Experiments on three datasets (SAMsum, CNNDM, MIMIC) show that membership can be inferred with AUC scores above 50%, reaching 67.01 AUC in some cases. The study also shows that attacks remain effective even without reference summaries by measuring summary robustness through self-similarity, and identifies defenses like differential privacy and L2 regularization that reduce attack success at the cost of model utility.

## Method Summary
The study implements membership inference attacks on text summarization models (BART and FLAN-T5) using shadow model training methodology. Data is split into Atrain, Aout, and Ball sets, with Bob training shadow models on Bin (subset of Ball) and using Bout for attack training. Attack features include ROUGE similarity scores between generated and reference summaries, confidence scores, and robustness measures from document augmentation. The attack classifier (RF, LR, SVM, MLP) predicts membership based on these features. Defenses tested include differential privacy with DP-SGD and L2 regularization. Model utility is measured using ROUGE-L F1 scores for summarization quality.

## Key Results
- Membership inference attacks achieve AUC scores above 50% on summarization models, with the full attack (similarity + robustness) reaching 67.01 AUC
- Document-only attacks without reference summaries still achieve above-chance accuracy by measuring summary robustness through self-similarity
- Increasing training data size and early stopping reduce attack effectiveness
- Defenses like differential privacy and L2 regularization lower attack success but reduce model utility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models memorize training samples more than non-training samples, leading to higher similarity between generated and reference summaries for training data.
- Mechanism: The training objective minimizes loss between generated and reference summaries, causing training samples to be better reconstructed and thus more similar to their references.
- Core assumption: The summarization model's training process creates differential performance between training and non-training samples.
- Evidence anchors:
  - [abstract]: "samples with significantly lower loss (indicating a similarity between the generated and reference summaries) are more likely to be part of the training dataset"
  - [section 4.1]: "The baseline attack is based on the observation that the generated summaries of training data often exhibit higher similarity to the reference summary, i.e., lower loss value"
- Break condition: If the model is trained with strong regularization or differential privacy, the memorization effect is reduced and similarity signals become less reliable.

### Mechanism 2
- Claim: Training data points are more robust to input perturbations than non-training data.
- Mechanism: According to the max-margin principle, training samples reside further from the decision boundary and exhibit greater resilience to perturbations.
- Core assumption: The model's learned decision boundary creates different perturbation responses for training vs non-training data.
- Evidence anchors:
  - [section 4.2]: "According to the max-margin principle, training data tends to reside further away from the decision boundary, thus exhibiting greater resilience to perturbations"
  - [section 4.2.1]: "the variance of training data is notably lower than non-training data, indicating that training samples are more robust against perturbations"
- Break condition: If the model is trained with data augmentation or adversarial training, the robustness differences between training and non-training data may diminish.

### Mechanism 3
- Claim: The attack can work even without reference summaries by approximating robustness through self-similarity.
- Mechanism: By measuring similarity between perturbed summaries and the original generated summary (instead of reference), the attack can estimate robustness and infer membership.
- Core assumption: The self-similarity variance between perturbed and original summaries correlates with membership status even without reference summaries.
- Evidence anchors:
  - [section 4.2.1]: "Bob estimates the document robustness by using generated summaries" and "this approximate robustness contains valuable membership signals"
  - [section 6]: "The proposed document only MI attack can be written as follows: g(·)D_only = g([sim(ˆsd_1, ˆs), ..., sim(ˆsd_n, ˆs)]))"
- Break condition: If the model generates highly diverse outputs for the same input across different runs, the self-similarity approximation becomes unreliable.

## Foundational Learning

- Concept: Membership inference attack fundamentals
  - Why needed here: Understanding the basic threat model and attack methodology is crucial for implementing and extending the attack
  - Quick check question: What is the difference between black-box and white-box membership inference attacks?

- Concept: Text similarity metrics (ROUGE, BLEU, etc.)
  - Why needed here: These metrics are used to quantify the similarity between generated and reference summaries as attack features
  - Quick check question: How does ROUGE-1 differ from ROUGE-L in measuring summary quality?

- Concept: Data augmentation techniques for NLP
  - Why needed here: Augmentation methods are used to create perturbed versions of documents to measure robustness
  - Quick check question: What are the advantages and disadvantages of back-translation vs word synonym replacement?

## Architecture Onboarding

- Component map: Document → Model API → Generated summary → Perturbation → Similarity scores → Classifier → Membership prediction
- Critical path: Document → Model API → Generated summary → Perturbation → Similarity scores → Classifier → Membership prediction
- Design tradeoffs: Higher augmentation numbers improve attack accuracy but increase API query cost and latency
- Failure signatures: Low AUC scores, high false positive rates, or inconsistent results across different model architectures
- First 3 experiments:
  1. Implement baseline attack using ROUGE similarity scores and evaluate on SAMsum dataset
  2. Add document augmentation with sentence swapping and compare performance to baseline
  3. Implement document-only attack and validate that it achieves above-chance accuracy without reference summaries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties make samples more susceptible to membership inference attacks?
- Basis in paper: [inferred] The paper notes that "it remains unclear what properties make samples more susceptible to MI attack" and mentions that "detected samples under TPR 0.1% have an average shorter reference length."
- Why unresolved: While the paper identifies some potential factors like shorter reference lengths, a comprehensive understanding of the properties that make samples vulnerable to MI attacks is still lacking.
- What evidence would resolve it: A detailed analysis of various sample characteristics (e.g., length, content, complexity) and their correlation with susceptibility to MI attacks would provide insights into the properties that make samples more vulnerable.

### Open Question 2
- Question: How effective are MI attacks on summarization models compared to other types of attacks in terms of information leakage?
- Basis in paper: [explicit] The paper states that "while the MI attack is a commonly used attack, its privacy leakage is limited. Other attacks pose a more significant threat in terms of information leakage."
- Why unresolved: The paper acknowledges that MI attacks have limited privacy leakage compared to other attacks but does not provide a comprehensive comparison or evaluation of other attacks on summarization tasks.
- What evidence would resolve it: A thorough evaluation and comparison of different attack methods (e.g., model inversion, attribute inference) on summarization models, measuring their effectiveness in terms of information leakage, would provide insights into the relative threat posed by MI attacks.

### Open Question 3
- Question: How can the trade-off between privacy and utility be effectively managed when applying defenses against MI attacks on summarization models?
- Basis in paper: [explicit] The paper discusses the use of defenses like differential privacy and L2 regularization, which lower attack success but at the cost of model utility.
- Why unresolved: The paper acknowledges the trade-off between privacy and utility but does not provide a detailed analysis or guidance on how to effectively manage this trade-off in practice.
- What evidence would resolve it: An in-depth study on the impact of different defense mechanisms on both privacy (e.g., attack success rate) and utility (e.g., summarization quality) metrics, along with strategies for optimizing the trade-off, would provide insights into effectively managing this balance.

## Limitations

- Attack effectiveness is modest, with AUC scores above 50% representing only slight improvements over random guessing
- Results are specific to BART and FLAN-T5 architectures for summarization tasks and may not generalize to other model types
- The study focuses on black-box access scenarios and doesn't explore white-box scenarios where model parameters could provide stronger attack signals

## Confidence

- **High confidence**: The methodology for implementing membership inference attacks on summarization models, including the use of similarity metrics and robustness measures as attack features. The experimental setup with shadow models and data splits is clearly specified and reproducible.
- **Medium confidence**: The claim that increasing training data size and early stopping reduce attack effectiveness. While demonstrated empirically, this finding may be dataset-dependent and requires validation across different domains and model architectures.
- **Low confidence**: The generalizability of the attack across different model types and tasks. The study focuses specifically on BART and FLAN-T5 for summarization, and results may not transfer to other model families or NLP tasks.

## Next Checks

1. **Cross-task validation**: Test the membership inference attack on a different NLP task (e.g., text classification or question answering) using the same BART and FLAN-T5 architectures to determine if the attack generalizes beyond summarization.

2. **Robustness to defense combinations**: Evaluate the combined effect of multiple defenses (e.g., differential privacy with early stopping and L2 regularization) to determine if stacked defenses provide additive privacy benefits or if there are diminishing returns.

3. **Alternative model architectures**: Implement the attack against other popular summarization models like Pegasus or T5 to verify that the attack effectiveness is not specific to BART and FLAN-T5 architectures.