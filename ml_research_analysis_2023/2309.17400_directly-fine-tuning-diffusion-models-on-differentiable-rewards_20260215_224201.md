---
ver: rpa2
title: Directly Fine-Tuning Diffusion Models on Differentiable Rewards
arxiv_id: '2309.17400'
source_url: https://arxiv.org/abs/2309.17400
tags:
- reward
- diffusion
- figure
- images
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DRaFT, a simple and efficient method for fine-tuning
  diffusion models to maximize differentiable reward functions like human preference
  scores. DRaFT works by backpropagating reward gradients through the full sampling
  chain to update model parameters.
---

# Directly Fine-Tuning Diffusion Models on Differentiable Rewards

## Quick Facts
- **arXiv ID**: 2309.17400
- **Source URL**: https://arxiv.org/abs/2309.17400
- **Reference count**: 40
- **Key outcome**: DRaFT achieves up to 200x faster training on aesthetic reward functions compared to RL-based methods

## Executive Summary
This paper introduces DRaFT, a method for fine-tuning diffusion models to maximize differentiable reward functions like human preference scores. The approach works by backpropagating reward gradients through the full sampling chain to update model parameters. To address computational costs, the authors employ gradient checkpointing and optimize LoRA weights instead of full model parameters. The method demonstrates substantial improvements over prior RL-based approaches on Stable Diffusion 1.4, achieving faster training and better performance on aesthetic reward functions. The authors also show DRaFT can be applied to diverse reward functions including image compressibility, object detection/removal, and adversarial examples.

## Method Summary
DRaFT fine-tunes diffusion models by backpropagating reward gradients through the denoising sampling chain. The method uses LoRA for parameter-efficient adaptation and gradient checkpointing to manage memory costs. Two key variants are introduced: DRaFT-K, which truncates backpropagation to the last K steps for efficiency, and DRaFT-LV, which reduces gradient variance for K=1 by averaging over multiple noise samples. The approach is demonstrated on Stable Diffusion 1.4 with various differentiable reward functions, showing significant improvements in sample efficiency and performance compared to RL-based methods.

## Key Results
- DRaFT achieves up to 200x faster training on aesthetic reward functions compared to RL-based methods
- DRaFT-K with K=1 provides the best reward-compute tradeoff
- DRaFT models generalize well across different prompt sets and reward functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backpropagating through the full diffusion sampling chain enables efficient gradient-based reward optimization
- Mechanism: The denoising process in diffusion models forms a recurrent computation graph where each step depends on the previous one. Backpropagation through all T steps computes gradients of the reward with respect to model parameters, enabling efficient fine-tuning without RL's sample inefficiency
- Core assumption: The sampling process is differentiable end-to-end and computational overhead is manageable with gradient checkpointing
- Evidence anchors: [abstract] Strong performance on various rewards, outperforming RL-based approaches; [section 4] Detailed description of computing gradients through multiple diffusion model calls

### Mechanism 2
- Claim: Truncating backpropagation to only the last K steps (DRaFT-K) improves training efficiency and performance
- Mechanism: Full backpropagation through long sampling chains can cause exploding gradients. Truncating to the last K steps reduces computational cost and gradient explosion, leading to more stable optimization and better sample efficiency
- Core assumption: Gradients from the last K steps contain sufficient information for effective fine-tuning, with earlier steps contributing mainly noise or instability
- Evidence anchors: [abstract] More efficient variants of DRaFT; [section 4] Empirically best reward vs. compute tradeoff with K=1

### Mechanism 3
- Claim: Computing lower-variance gradient estimates by averaging over multiple noise samples (DRaFT-LV) significantly improves training efficiency
- Mechanism: For K=1, single-sample gradients are noisy. Generating multiple examples with different noise samples and averaging their gradients reduces variance, leading to more stable and efficient optimization
- Core assumption: The reward function is sufficiently smooth that averaging over noise samples provides a better estimate of the expected gradient
- Evidence anchors: [abstract] Lower-variance gradient estimates for K=1; [section 4] Description of generating multiple examples to reduce variance

## Foundational Learning

- **Concept**: Backpropagation through time (BPTT) for recurrent neural networks
  - Why needed here: Understanding how to backpropagate through the sequential sampling process in diffusion models, analogous to BPTT in RNNs
  - Quick check question: How does gradient checkpointing reduce memory usage during BPTT in RNNs, and how is this applied in DRaFT?

- **Concept**: Reinforcement learning vs. direct policy gradient methods
  - Why needed here: Understanding why direct gradient-based fine-tuning (DRaFT) is more sample-efficient than RL approaches for diffusion models
  - Quick check question: What information does DRaFT preserve that RL discards, and how does this lead to better sample efficiency?

- **Concept**: Low-rank adaptation (LoRA) for parameter-efficient fine-tuning
  - Why needed here: Understanding how LoRA reduces the number of parameters that need to be optimized during fine-tuning, making DRaFT more memory-efficient
  - Quick check question: How does LoRA's low-rank decomposition of weight updates enable efficient fine-tuning while preserving the original model's capabilities?

## Architecture Onboarding

- **Component map**: Pre-trained diffusion model -> LoRA adapter modules -> Gradient checkpointing system -> Reward function module -> Sampling loop -> Optimizer

- **Critical path**:
  1. Sample noise and prompt
  2. Run DDIM sampling with classifier-free guidance
  3. Apply gradient checkpointing during sampling
  4. Compute reward on final image
  5. Backpropagate through sampling chain (truncated if using DRaFT-K)
  6. Update LoRA parameters with AdamW
  7. Repeat until convergence

- **Design tradeoffs**:
  - Full vs. truncated backpropagation: Full provides unbiased gradients but is computationally expensive and prone to exploding gradients; truncation improves efficiency but may lose information from early sampling steps
  - Number of noise samples for DRaFT-LV: More samples reduce gradient variance but increase compute; fewer samples are faster but noisier
  - LoRA rank: Higher rank allows more expressive fine-tuning but increases parameters; lower rank is more efficient but may limit adaptation capacity

- **Failure signatures**:
  - Reward collapse: All generated images become identical (reward hacking)
  - Gradient explosion: Training becomes unstable, loss diverges
  - Memory overflow: Out of memory errors during backpropagation
  - Poor sample quality: Fine-tuned model produces low-quality or unrealistic images

- **First 3 experiments**:
  1. Implement DRaFT-1 on a simple differentiable reward (e.g., aesthetic score) with a small prompt set to verify basic functionality
  2. Compare DRaFT-1 vs DRaFT-50 on the same task to observe the impact of truncated backpropagation
  3. Implement DRaFT-LV and compare its training efficiency against DRaFT-1 on a simple reward function

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does truncating backpropagation to only the last K steps (DRaFT-K) improve sample efficiency compared to full backpropagation (DRaFT-50)?
- Basis in paper: The authors observe that DRaFT-K performs better than DRaFT-50 and hypothesize that gradients may explode as K increases, leading to optimization difficulties
- Why unresolved: The authors provide initial analysis suggesting exploding gradients but lack definitive explanation or mathematical proof
- What evidence would resolve it: Rigorous mathematical analysis of gradient norms and optimization dynamics for different K values, including experiments measuring gradient norms and training stability

### Open Question 2
- Question: How does DRaFT generalize across different prompt sets and reward functions?
- Basis in paper: The authors demonstrate that DRaFT models trained on one prompt set (Pick-a-Pic) generalize well to another (HPDv2), and models trained on one reward function (PickScore) generalize fairly well to another (HPSv2)
- Why unresolved: The paper doesn't analyze factors influencing generalization such as similarity between prompt sets or reward functions, or the role of learned style
- What evidence would resolve it: Experiments systematically varying similarity between prompt sets and reward functions, analyzing impact on generalization performance with metrics quantifying similarity between learned styles

### Open Question 3
- Question: What are the limitations of DRaFT in terms of reward hacking and over-optimization?
- Basis in paper: The authors observe instances of reward hacking where fine-tuned models collapse to generating very similar high-reward images, and show examples of over-optimization for specific reward functions like JPEG incompressibility
- Why unresolved: The paper lacks comprehensive analysis of factors contributing to reward hacking and over-optimization or proposed solutions to mitigate these issues
- What evidence would resolve it: Experiments exploring the relationship between reward function design, training duration, and occurrence of reward hacking and over-optimization, potentially including new reward functions or training techniques that encourage diversity and prevent over-optimization

## Limitations
- Scalability to larger diffusion models (SD 1.5, SDXL) beyond Stable Diffusion 1.4 remains unclear
- Performance on non-aesthetic reward functions beyond demonstrated examples needs validation
- Long-term stability of fine-tuned models and potential for reward collapse over extended training is uncertain

## Confidence
- **High Confidence**: Basic mechanism of backpropagation through sampling chain is valid; LoRA + gradient checkpointing reduces memory costs as claimed
- **Medium Confidence**: DRaFT-K with K=1 provides optimal reward-compute tradeoff; DRaFT-LV effectively reduces gradient variance
- **Low Confidence**: DRaFT's superiority over all existing RL-based methods generalizes beyond the specific experiments shown

## Next Checks
1. Test DRaFT-K with K=1,5,10 on a held-out reward function (e.g., CLIP-based image-text alignment) to verify the claimed optimal tradeoff
2. Implement DRaFT-LV with 2, 4, and 8 noise samples to quantify the relationship between sample count and gradient variance reduction
3. Evaluate fine-tuned models on prompt distribution shift by testing on prompts outside the fine-tuning set to assess generalization