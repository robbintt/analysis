---
ver: rpa2
title: Remote Heart Rate Monitoring in Smart Environments from Videos with Self-supervised
  Pre-training
arxiv_id: '2310.15388'
source_url: https://arxiv.org/abs/2310.15388
tags:
- learning
- rppg
- self-supervised
- encoder
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a self-supervised contrastive learning approach
  for remote heart rate monitoring using facial videos, addressing the challenge of
  limited labeled data in remote photoplethysmography (rPPG) estimation. The method
  leverages spatial and temporal augmentations to pre-train a 3D convolutional encoder,
  followed by fine-tuning with labeled data using smooth L1 loss.
---

# Remote Heart Rate Monitoring in Smart Environments from Videos with Self-supervised Pre-training

## Quick Facts
- arXiv ID: 2310.15388
- Source URL: https://arxiv.org/abs/2310.15388
- Reference count: 40
- Key outcome: Self-supervised contrastive pre-training enables robust rPPG estimation with minimal labeled data, achieving MAE as low as 2.16 bpm on COHFACE dataset.

## Executive Summary
This paper addresses the challenge of remote heart rate monitoring from facial videos in smart environments where labeled data is scarce. The authors propose a two-stage approach combining self-supervised contrastive pre-training with supervised fine-tuning to learn robust representations for remote photoplethysmography (rPPG) estimation. By leveraging spatial and temporal augmentations, the method learns to extract physiological signals from subtle skin color variations while being invariant to common video distortions like compression and motion. Experiments demonstrate superior performance compared to fully supervised baselines and several prior works, with the approach showing particular robustness when labeled data is limited.

## Method Summary
The method employs a two-stage training procedure. First, a 3D convolutional encoder is pre-trained using self-supervised contrastive learning on unlabeled facial videos. Spatial augmentations (rotation, crop, flip) and temporal augmentations (shuffle, reorder, reverse) are applied to create positive pairs for the contrastive objective. The encoder learns representations invariant to these transformations while preserving the underlying physiological signal. In the second stage, the pre-trained encoder is fine-tuned using labeled data (video + PPG signal pairs) with smooth L1 loss on intermediate layer embeddings. The final heart rate is estimated from the predicted PPG signal using Welch peak detection. The approach focuses on specific facial regions (forehead and cheeks) where rPPG signals are strongest.

## Key Results
- Achieves MAE of 2.16 bpm and correlation of 0.94 on COHFACE dataset, outperforming fully supervised baselines
- Maintains strong performance (MAE 5.09 bpm) even with only 25% of labeled data available
- Shows improved robustness to MPEG-4 compression artifacts compared to baseline methods
- Demonstrates better generalization to challenging PURE dataset with natural movements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised contrastive pre-training learns robust facial representations that generalize across varying lighting, motion, and compression artifacts.
- Mechanism: By maximizing similarity between augmented views of the same video clip and minimizing similarity across different clips, the encoder learns to extract features invariant to spatial and temporal augmentations.
- Core assumption: Positive pairs (augmented versions of the same clip) share the same underlying physiological signal despite visual differences.
- Evidence anchors:
  - [abstract] "self-supervised contrastive learning for the estimation of remote photoplethysmography (PPG) and heart rate monitoring, thereby reducing the dependence on labeled data and enhancing performance"
  - [section] "The model is then trained to recognize these augmentations, for instance, by detecting that two different transformations applied to the same input sample are indeed renditions of the same information"
  - [corpus] Weak: No direct corpus evidence comparing contrastive vs supervised on rPPG tasks.

### Mechanism 2
- Claim: Using spatial and temporal augmentations separately allows the model to capture both stable skin color variations and dynamic motion patterns.
- Mechanism: Spatial augmentations (crop, rotate, flip) teach invariance to viewpoint changes, while temporal augmentations (shuffle, reverse, reorder) teach temporal coherence of the physiological signal.
- Core assumption: The heart rate signal is temporally consistent and spatially localized around the forehead and cheeks.
- Evidence anchors:
  - [section] "we use two categories of augmentations: (i) spatial, and (ii) (ii) temporal" and describes 3 spatial + 3 temporal augmentations
  - [section] "rPPG estimation relies on very slight changes in skin color"
  - [corpus] Weak: No corpus examples of rPPG methods using temporal augmentation.

### Mechanism 3
- Claim: Fine-tuning with smooth L1 loss on late-intermediate embeddings improves signal estimation by combining benefits of L1 and L2 losses.
- Mechanism: Smooth L1 loss is less sensitive to outliers than L2 and more stable than L1, allowing the model to fit noisy rPPG signals without being dominated by large errors.
- Core assumption: The encoder's intermediate layers retain sufficient signal information for regression, and the smooth L1 loss balances robustness and sensitivity.
- Evidence anchors:
  - [section] "We use the smooth L1 loss [74] for the second stage of training... This loss is a combination of both L1 and the L2 losses"
  - [section] "we use the output embeddings of the final four convolutional layers"
  - [corpus] Weak: No corpus examples of smooth L1 loss specifically for rPPG regression.

## Foundational Learning

- Concept: Contrastive learning and the InfoNCE loss
  - Why needed here: Enables learning from unlabeled video data by creating pseudo-labels through augmentations
  - Quick check question: What is the difference between positive and negative pairs in contrastive learning?

- Concept: Photoplethysmography and remote rPPG
  - Why needed here: Understanding the physiological basis of the task (blood volume changes → color variations → heart rate)
  - Quick check question: Which facial regions provide the strongest rPPG signals?

- Concept: 3D convolutions vs (2+1)D convolutions
  - Why needed here: Determines how spatial and temporal information is processed in video-based physiological estimation
  - Quick check question: How does (2+1)D decomposition affect temporal modeling in rPPG?

## Architecture Onboarding

- Component map:
  Data augmentation module → generates spatial/temporal transforms → 3D CNN encoder → Projection head (pre-training) → Smooth L1 loss + auxiliary losses (fine-tuning) → Welch peak detection → HR estimation

- Critical path:
  Pre-training (unlabeled video) → Fine-tuning (labeled video+PPG) → HR estimation via Welch peak detection

- Design tradeoffs:
  - Spatial vs temporal augmentations: spatial more stable, temporal captures dynamics
  - Negative pairs vs SimSiam: negatives improve discrimination but may introduce noise
  - Encoder choice (3D vs (2+1)D): 3D simpler, (2+1)D better temporal modeling but more parameters

- Failure signatures:
  - Poor correlation plots → encoder not learning physiological features
  - High MAE on MPEG-4 → model sensitive to compression artifacts
  - Large gap between pre-training and fine-tuning → domain shift or weak supervision

- First 3 experiments:
  1. Train with only spatial augmentations → check if temporal information is still captured
  2. Replace 3D encoder with (2+1)D → compare HR estimation accuracy
  3. Fine-tune on 10% labeled data → test robustness of pre-training

## Open Questions the Paper Calls Out

- Question: How do different combinations of spatial and temporal augmentations impact the robustness of self-supervised contrastive learning for rPPG estimation across varying facial movements and lighting conditions?
  - Basis in paper: [explicit] The authors discuss the use of 3 spatial and 3 temporal augmentations but suggest that further exploration of combinations could be beneficial.
  - Why unresolved: The study provides a foundation but does not exhaustively test all possible combinations or their impact under diverse real-world scenarios.
  - What evidence would resolve it: Systematic experiments testing all combinations of spatial and temporal augmentations across datasets with varying facial movements, lighting, and compression artifacts.

- Question: Can the proposed self-supervised contrastive learning approach be extended to estimate other physiological signals (e.g., blood pressure, oxygen saturation) from facial videos, and how would the performance compare to rPPG estimation?
  - Basis in paper: [inferred] The paper mentions that PPG signals can be used to derive various physiological parameters, suggesting potential for extension beyond HR.
  - Why unresolved: The study focuses on HR estimation, leaving the generalization to other physiological signals unexplored.
  - What evidence would resolve it: Experiments applying the method to estimate additional physiological signals and comparing performance metrics against ground truth data.

- Question: How does the performance of the self-supervised approach vary across different demographic groups (e.g., age, skin tone, cultural characteristics), and what biases might exist in the model?
  - Basis in paper: [explicit] The authors acknowledge the potential for bias due to variations in skin complexion, age, and other factors, suggesting a need for fairness analysis.
  - Why unresolved: The study does not include demographic-specific evaluations or bias mitigation strategies.
  - What evidence would resolve it: Comprehensive testing across diverse demographic groups with fairness metrics and bias mitigation techniques applied.

## Limitations
- Limited ablation studies on critical design choices like augmentation types and encoder architecture
- Dataset dependency with only two datasets (40 and 10 subjects) limiting generalizability
- Unclear mechanism of how contrastive learning specifically learns physiological versus general facial features

## Confidence

- High confidence: The overall experimental methodology is sound, with appropriate evaluation metrics (MAE, RMSE, correlation) and comparison against reasonable baselines.
- Medium confidence: The claim that self-supervised pre-training significantly improves performance with limited labeled data is supported by experiments showing better results with 25% labeled data versus fully supervised training.
- Low confidence: The assertion that temporal augmentations are crucial for capturing physiological dynamics lacks direct evidence, as the paper does not provide experiments isolating the contribution of temporal versus spatial augmentations.

## Next Checks

1. **Ablation on augmentation types**: Run experiments comparing models trained with only spatial augmentations, only temporal augmentations, and combined augmentations to quantify the contribution of each augmentation category to rPPG estimation performance.

2. **Cross-dataset generalization test**: Evaluate the pre-trained model on a third, completely different rPPG dataset (e.g., UBFC-RPPG or OBF dataset) without fine-tuning to assess the true generalization capability of the learned representations.

3. **Physiological signal analysis**: Compare the learned embeddings from the encoder against hand-crafted rPPG features (e.g., CHROM, POS) to determine whether the model is actually learning physiologically meaningful representations or just generic facial features that happen to correlate with heart rate.