---
ver: rpa2
title: 'Improving Knowledge Distillation for BERT Models: Loss Functions, Mapping
  Methods, and Weight Tuning'
arxiv_id: '2308.13958'
source_url: https://arxiv.org/abs/2308.13958
tags:
- loss
- distillation
- mapping
- layer
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work explores three techniques to improve TinyBERT distillation:
  KL divergence loss, learnable mapping between teacher and student layers, and tuning
  attention/representation loss weights. Experiments on CoLA and STS-B show KL divergence
  improves performance, especially with limited data on CoLA.'
---

# Improving Knowledge Distillation for BERT Models: Loss Functions, Mapping Methods, and Weight Tuning

## Quick Facts
- arXiv ID: 2308.13958
- Source URL: https://arxiv.org/abs/2308.13958
- Authors: 
- Reference count: 7
- One-line primary result: KL divergence loss improves TinyBERT distillation, especially with limited data on CoLA task

## Executive Summary
This work explores three techniques to improve TinyBERT distillation: KL divergence loss, learnable mapping between teacher and student layers, and tuning attention/representation loss weights. Experiments on CoLA and STS-B show KL divergence improves performance, especially with limited data on CoLA. Learnable mapping converges to similar weights regardless of initialization, suggesting a globally optimal mapping exists. Tuning loss weights shows attention loss is critical for CoLA but less so for STS-B. STS-B performance is largely unaffected by transformer layer distillation, indicating it may be unnecessary for some tasks. Overall, the findings suggest task-specific adjustments to distillation methods can yield improvements.

## Method Summary
The paper investigates three modifications to TinyBERT's task-specific distillation stage: replacing MSE loss with KL divergence for attention matrices, introducing learnable mapping functions between teacher and student layers, and tuning the relative weights of attention and representation losses. The experiments compare these modifications against a baseline TinyBERT implementation on CoLA (grammatical acceptability) and STS-B (semantic similarity) tasks from the GLUE benchmark, measuring Matthews Correlation Coefficient for CoLA and Pearson Correlation for STS-B.

## Key Results
- KL divergence loss improves performance on CoLA by 12.0% with limited training data compared to baseline MSE loss
- Learnable mapping functions converge to similar weight values regardless of initialization, suggesting a globally optimal mapping exists
- Attention loss weight is critical for CoLA performance but less important for STS-B, where all tested weights yield similar results
- Transformer layer distillation may be unnecessary for STS-B as performance remains stable without it

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KL divergence loss captures probabilistic differences between teacher and student attention distributions better than MSE in low-data regimes.
- Mechanism: KL divergence measures the information-theoretic difference between two probability distributions, while MSE operates on unnormalized attention scores. When training data is limited, attention matrices become sparse and noisy, making distributional comparisons more stable.
- Core assumption: Attention matrices approximate probability distributions and their relative differences matter more than absolute magnitude differences for distillation quality.
- Evidence anchors:
  - [abstract] "KL divergence improves performance in some scenarios, particularly when training with limited data for challenging tasks like CoLA"
  - [section 4.1] "KL divergence loss outperformed the baseline loss function in the CoLA task... a 12.0% improvement over the baseline when using half of the training data"
  - [corpus] Weak evidence - corpus contains no direct mentions of KL divergence vs MSE comparisons in distillation
- Break condition: If attention matrices are already well-behaved (dense, high-confidence) or if data is abundant, MSE may perform equally well or better.

### Mechanism 2
- Claim: Learnable layer mapping functions converge to globally optimal teacher-student layer correspondences.
- Mechanism: The learnable mapping uses differentiable parameters to combine teacher layer representations, allowing gradient-based optimization to find the best combination weights that minimize transformer loss across training.
- Core assumption: There exists a fixed optimal mapping between teacher and student layers that generalizes across different initializations and training runs.
- Evidence anchors:
  - [section 4.2] "weight vectors vk(m) converge to the same values regardless of initialization... hinting at a potentially interesting area for future exploration"
  - [abstract] "Learnable mapping converges to similar weights regardless of initialization, suggesting a globally optimal mapping exists"
  - [corpus] Weak evidence - corpus contains no direct mentions of convergence behavior in layer mapping functions
- Break condition: If the optimal mapping depends on initialization or if multiple local minima exist that perform similarly.

### Mechanism 3
- Claim: Attention loss contributes differently to different tasks, being crucial for classification tasks like CoLA but less important for similarity tasks like STS-B.
- Mechanism: Attention mechanisms capture syntactic and semantic dependencies that are more critical for grammatical acceptability classification than for semantic similarity scoring, where surface-level representations may suffice.
- Core assumption: The relative importance of attention vs representation loss depends on task-specific feature requirements and data characteristics.
- Evidence anchors:
  - [section 4.3] "setting α = 1 resulted in significantly worse performance compared to other values" for CoLA, while "all tested α values returned similar results" for STS-B
  - [abstract] "attention loss is critical for CoLA but less so for STS-B"
  - [corpus] Weak evidence - corpus contains no direct mentions of task-specific loss weighting effects
- Break condition: If the task characteristics change or if the student model architecture fundamentally alters how attention contributes to final predictions.

## Foundational Learning

- Concept: Knowledge Distillation Principles
  - Why needed here: Understanding the teacher-student framework and how soft labels transfer knowledge is essential for grasping why different loss functions and mappings matter
  - Quick check question: What is the fundamental difference between training a student model with hard labels versus soft teacher probabilities?

- Concept: Attention Mechanisms in Transformers
  - Why needed here: The paper focuses heavily on attention loss, requiring understanding of how attention matrices encode relationships and why distributional differences matter
  - Quick check question: How does the shape and content of attention matrices differ between early and late transformer layers?

- Concept: Probability Distribution Theory
  - Why needed here: KL divergence is a probabilistic measure, so understanding its properties and when it's appropriate vs MSE is crucial for interpreting the experimental results
  - Quick check question: What are the key mathematical differences between KL divergence and MSE when comparing probability distributions?

## Architecture Onboarding

- Component map: Embedding layer -> 4 student transformer layers -> prediction head (vs 12 teacher transformer layers)
- Critical path: For task-specific experiments, the critical path is: load pre-trained TinyBERT -> apply task-specific distillation with modified loss functions/mappings -> evaluate on GLUE tasks. The transformer layer distillation stage is where all experimental modifications occur.
- Design tradeoffs: Using KL divergence requires normalization of attention matrices (computational overhead) but may provide better gradient signals. Learnable mapping adds parameters but could find better teacher-student correspondences. Tuning α introduces hyperparameter search complexity but allows task-specific optimization.
- Failure signatures: If KL divergence causes training instability, attention matrices may have near-zero probabilities. If learnable mapping fails to converge, initialization or learning rate may be inappropriate. If α tuning doesn't improve performance, the task may not benefit from attention loss adjustment.
- First 3 experiments:
  1. Run baseline TinyBERT task-specific distillation on CoLA with default MSE loss and uniform mapping to establish performance floor
  2. Replace MSE with KL divergence loss while keeping all other parameters constant to isolate loss function effect
  3. Implement learnable mapping with different initializations while using baseline loss function to test mapping optimization potential

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the convergence of the LearnableMap weights to the same values regardless of initialization indicative of a globally optimal mapping function, and if so, what is the mathematical basis for this optimality?
- Basis in paper: [explicit] The authors observed that "weight vectors vk(m) converge to the same values regardless of initialization (A.1)" and hypothesized that "this may indicate convergence to a global minimum and the existence of a mapping that minimizes transformer loss."
- Why unresolved: The authors only observed this phenomenon but did not conduct a thorough mathematical analysis to prove the global optimality of the converged weights. They also noted that they "gathered this insight late in the project and could not experiment with it enough to get the best results from this."
- What evidence would resolve it: A rigorous mathematical proof demonstrating that the converged weights minimize a specific objective function related to transformer loss, along with additional experiments varying the architecture and data to confirm the generality of this convergence.

### Open Question 2
- Question: Under what specific conditions (e.g., task complexity, dataset size, model architecture) does KL divergence loss outperform MSE loss in knowledge distillation, and why?
- Basis in paper: [explicit] The authors found that "KL divergence loss outperformed the baseline loss function in the CoLA task" particularly "when training with limited data and for challenging tasks like CoLA," but "did not observe any significant improvement over the baseline for the STS-B task."
- Why unresolved: The experiments were limited to two tasks (CoLA and STS-B) and did not systematically explore the space of task complexities, dataset sizes, or model architectures. The underlying reasons for the differential performance are not explained.
- What evidence would resolve it: A comprehensive study varying task complexity, dataset sizes, and model architectures, coupled with an analysis of the attention distributions and their properties in each scenario to understand why KL divergence is more effective in certain conditions.

### Open Question 3
- Question: Is transformer layer loss distillation necessary for all NLP tasks, or are there specific task characteristics that determine its relevance?
- Basis in paper: [explicit] The authors found that "the performance did not change significantly in any experiment" for STS-B, leading them to hypothesize that "transformer layer distillation might not be crucial for this task." They confirmed this by skipping the transformer layer loss distillation and achieving "almost the same result (corr = 0.8717) as the baseline (corr = 0.8682)."
- Why unresolved: The conclusion is based on experiments with only one task (STS-B). The authors did not explore a diverse set of tasks to identify the characteristics that determine the relevance of transformer layer loss distillation.
- What evidence would resolve it: Experiments on a wide range of NLP tasks with varying characteristics (e.g., semantic similarity, sentiment analysis, natural language inference) to identify patterns in when transformer layer loss distillation is beneficial or redundant.

## Limitations
- Limited comparison to state-of-the-art distillation methods
- No analysis of computational efficiency overhead from proposed modifications
- Narrow exploration of hyperparameter space for loss weight tuning
- Absence of systematic ablation studies on technique interactions

## Confidence
- Medium confidence: KL divergence loss effectiveness claims - supported by reported improvements on CoLA but lacks comparison with alternative loss functions
- Medium confidence: Learnable mapping convergence claims - based on convergence observations but lacks statistical significance testing across multiple runs
- Low confidence: Task-specific loss weight recommendations - derived from limited experimental configurations and may not generalize to other tasks

## Next Checks
1. Run each experimental condition (baseline, KL divergence, learnable mapping, tuned weights) across 10+ random seeds and perform paired t-tests to establish statistical significance of observed improvements
2. Systematically test all combinations of the three techniques (KL divergence on/off, learnable mapping on/off, different α values) on both CoLA and STS-B to understand interaction effects and identify the most effective configuration
3. Measure and compare training time, memory usage, and inference latency for each experimental condition relative to the baseline to assess practical feasibility of the proposed modifications