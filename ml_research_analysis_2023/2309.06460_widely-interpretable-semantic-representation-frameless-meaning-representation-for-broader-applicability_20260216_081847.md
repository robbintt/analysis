---
ver: rpa2
title: 'Widely Interpretable Semantic Representation: Frameless Meaning Representation
  for Broader Applicability'
arxiv_id: '2309.06460'
source_url: https://arxiv.org/abs/2309.06460
tags:
- theme
- actor
- wiser
- role
- girl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WISeR, a novel semantic representation that
  improves upon Abstract Meaning Representation (AMR) by removing dependency on predefined
  semantic frames and using thematic roles instead of numbered arguments. The authors
  create a 1,000-sentence English dialogue corpus annotated in both WISeR and AMR.
---

# Widely Interpretable Semantic Representation: Frameless Meaning Representation for Broader Applicability

## Quick Facts
- arXiv ID: 2309.06460
- Source URL: https://arxiv.org/abs/2309.06460
- Reference count: 35
- This paper introduces WISeR, a novel semantic representation that improves upon Abstract Meaning Representation (AMR) by removing dependency on predefined semantic frames and using thematic roles instead of numbered arguments.

## Executive Summary
This paper introduces WISeR, a novel semantic representation that improves upon Abstract Meaning Representation (AMR) by removing dependency on predefined semantic frames and using thematic roles instead of numbered arguments. The authors create a 1,000-sentence English dialogue corpus annotated in both WISeR and AMR. They find that WISeR shows stronger inter-annotator agreement, especially for beginners, and that a seq-to-seq parser trained on WISeR outperforms one trained on AMR across all metrics, demonstrating that WISeR is easier for parsers to learn. The results suggest WISeR's thematic roles and open predicate structure make it more interpretable and generalizable than AMR's frame-based approach.

## Method Summary
The authors created WISeR, a frameless semantic representation that replaces AMR's numbered arguments (ARG0-ARG5) with interpretable thematic roles (actor, theme, benefactive, etc.) and removes dependency on predefined PropBank frames. They annotated a 1,000-sentence English dialogue corpus in both WISeR and AMR formats. For evaluation, they trained seq-to-seq parsers on AMR 3.0 and converted WISeR corpora, then compared performance across multiple metrics including Smatch scores and semantic role labeling accuracy. The study also included an AMR-to-WISeR conversion experiment on AMR 3.0 to analyze theoretical advantages.

## Key Results
- WISeR demonstrates stronger inter-annotator agreement than AMR, particularly for beginner annotators
- Seq-to-seq parsers trained on WISeR outperform those trained on AMR across all evaluation metrics
- WISeR parsers achieve approximately 1% higher Smatch scores, indicating the representation is easier to learn

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WISeR improves parser accuracy by removing the need for word sense disambiguation (WSD).
- Mechanism: By eliminating sense IDs from predicate labels, WISeR reduces the search space for parsers and removes a major source of annotation ambiguity. Parsers no longer have to disambiguate between senses during training, so they can focus on learning structural patterns.
- Core assumption: Removing WSD improves parsing accuracy because sense disambiguation is both hard for annotators and noisy for models.
- Evidence anchors:
  - [abstract] "WISeR shows stronger inter-annotator agreement for beginner and experienced annotators, with beginners becoming proficient in WISeR annotation more quickly."
  - [section 5.3] "Comparing the results on AMRt and WISeRc, the WISeR parser outperform the AMR parser on all categories, showing ≈1% higher Smatch scores, which implies that WISeR is easier to learn, enabling parsers to train more robust models. The No WSD (no word sense disambiguation) scores for WISeR are equivalent to the Smatch scores because predicates in WISeR are not distinguished by senses. Unsurprisingly, the WISeR parser shows higher scores on this category confirming that WSD introduces an extra burden on the AMR parser."
  - [corpus] The WSD comparison is not directly reported for the dialogue corpus, so the benefit is inferred from the AMR-to-WISeR conversion experiment only.
- Break condition: If WSD is not a significant source of parsing errors in a domain, or if the parser architecture already handles sense ambiguity effectively, removing WSD may not yield accuracy gains.

### Mechanism 2
- Claim: WISeR's thematic role labels are more interpretable for both humans and parsers than AMR's numbered arguments.
- Mechanism: Instead of opaque ARG0-ARG5 numbers, WISeR uses semantic role names like :actor, :theme, :benefactive, etc., which map directly to their meaning. This one-to-one correspondence reduces confusion during annotation and makes training data more transparent to models.
- Core assumption: Semantic clarity in labels improves annotation consistency and model learning efficiency.
- Evidence anchors:
  - [abstract] "WISeR shows stronger inter-annotator agreement for beginner and experienced annotators, with beginners becoming proficient in WISeR annotation more quickly."
  - [section 4.1] "AMR and WISeR have similar IAA among experts; however, IAA for WISeR is noticeably higher among beginners, implying that AMR has a steeper learning curve, although both schemes produce high-quality annotation once annotators reach the expert-level."
  - [section 5.4] "We hypothesize that the seq-to-seq parser benefits from the more natural relation names in WISeR that are learnt during the pre-training of BART."
- Break condition: If parsers do not leverage the interpretability of role names (e.g., using only structural features), the advantage of thematic labels may be minimal.

### Mechanism 3
- Claim: WISeR allows parsers to handle novel predicates more flexibly than AMR.
- Mechanism: By not relying on a fixed PropBank frame set, WISeR can introduce new predicate senses on the fly. This reduces out-of-vocabulary issues and allows models to generalize better to unseen domains or languages.
- Core assumption: Novel predicate coverage is a significant bottleneck for AMR parsers in low-resource or domain-specific settings.
- Evidence anchors:
  - [section 5.4] "We tested the seq-to-seq parser on the WSD and SRL tasks independently. [...] This shows a ≈0.3% increase when using WISeR roles over numbered arguments even with predicate senses, while removing predicate senses accounts for a larger ≈0.7% increase."
  - [section 2.1] "AMR contains several predicate senses that are not found in PropBank. These senses often represent idioms or multi-word constructions created ad-hoc during annotation (e.g., throw-under-bus-08, pack-sand-00)."
  - [corpus] The dialogue corpus introduces many domain-specific verbs; the paper does not quantify how many are novel, so the benefit is inferred rather than directly measured.
- Break condition: If the domain already has comprehensive predicate coverage or if the parser can handle OOV predicates via embeddings or copying, the advantage of an open predicate set may diminish.

## Foundational Learning

- Concept: Thematic roles (e.g., agent, patient, instrument, location, benefactive).
  - Why needed here: WISeR replaces numbered arguments with these roles, so understanding them is essential for annotation and parsing.
  - Quick check question: What thematic role would you assign to the recipient in "John gave Mary a book"? (Answer: benefactive)
- Concept: PropBank frames and numbered arguments.
  - Why needed here: WISeR is designed to replace this system; knowing its limitations clarifies why the change helps.
  - Quick check question: Why is ARG2 ambiguous in PropBank? (Answer: It can be instrument, benefactive, or attribute depending on the verb sense.)
- Concept: Abstract Meaning Representation (AMR) graph structure.
  - Why needed here: WISeR is a modification of AMR; understanding nodes, edges, and reentrancies is necessary to grasp the differences.
  - Quick check question: What is a reentrancy in AMR? (Answer: A node with more than one incoming edge, indicating shared argument.)

## Architecture Onboarding

- Component map: Sentence -> Tokenizer -> BART encoder -> Decoder (with thematic roles) -> Linearized graph -> PENMAN restoration -> Evaluation
- Critical path: Sentence → tokenizer → BART encoder → decoder (with thematic roles) → linearized graph → PENMAN restoration → evaluation
- Design tradeoffs: Thematic roles are interpretable but more numerous than numbered args; open predicates improve coverage but require more flexible vocabulary handling; removing WSD simplifies training but loses sense granularity.
- Failure signatures: Low Smatch on core roles suggests role label confusion; poor novel predicate recall indicates vocabulary mismatch; high IAA variance across annotators points to guideline ambiguity.
- First 3 experiments:
  1. Convert a small AMR corpus to WISeR and measure IAA gain for beginners vs. experts.
  2. Train a seq-to-seq parser on AMR and WISeR versions of the same data; compare Smatch on a shared dev set.
  3. Test parser recall on out-of-vocabulary predicates in a dialogue domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does WISeR handle temporal aspects like tense and aspect in its annotation scheme, given that AMR has incorporated some mechanisms for this but WISeR is described as frameless?
- Basis in paper: [explicit] The paper mentions that AMR has been enhanced to represent tense/aspect (Donatelli et al., 2018, 2019) but does not explicitly describe how WISeR handles these temporal aspects.
- Why unresolved: The paper focuses on WISeR's thematic roles and open predicate structure but does not detail how temporal information is represented without frames.
- What evidence would resolve it: Detailed explanation of WISeR's approach to encoding tense and aspect, perhaps through specific examples or comparison with AMR's mechanisms.

### Open Question 2
- Question: What is the impact of WISeR's thematic roles on the interpretability of semantic role labels for parsers, and how does this compare to AMR's numbered arguments?
- Basis in paper: [explicit] The paper states that WISeR's thematic roles improve interpretability and that parsers find WISeR easier to learn, but it does not quantify the interpretability gains.
- Why unresolved: While the paper provides parsing performance metrics, it does not directly measure the interpretability of semantic role labels.
- What evidence would resolve it: A study comparing parser interpretability metrics between WISeR and AMR, possibly through human evaluation or parser confusion matrices.

### Open Question 3
- Question: How does WISeR handle complex semantic phenomena like quantification and scope, which AMR has addressed through extensions?
- Basis in paper: [explicit] The paper references AMR's handling of quantifier scope (Pustejovsky et al., 2019; Lai et al., 2020; Bos, 2020) but does not describe WISeR's approach to these phenomena.
- Why unresolved: The paper emphasizes WISeR's simplicity and interpretability but does not discuss its capacity to represent complex semantic structures.
- What evidence would resolve it: Examples of WISeR annotations for sentences with complex quantification and scope, along with an explanation of how these are represented without frames.

## Limitations
- Evaluation relies heavily on a single 1,000-sentence dialogue corpus, limiting generalizability across domains and languages
- Parser experiments compare different training setups (AMR 3.0 vs. converted WISeR), making it difficult to isolate representation effects from data quality differences
- The paper does not quantify how often novel predicates actually appear in the dialogue corpus, leaving the practical impact of open predicates uncertain

## Confidence

**High confidence**: WISeR's thematic roles improve annotator agreement, especially for beginners, compared to AMR's numbered arguments.

**Medium confidence**: WISeR parsers outperform AMR parsers on Smatch scores, suggesting the representation is easier to learn.

**Low confidence**: The specific contribution of removing WSD to parsing accuracy, as the dialogue corpus results don't directly measure this benefit.

## Next Checks

1. **Replicate parsing experiments** on a larger, diverse corpus (e.g., AMR 3.0 + LDC2020T02) with both representations annotated from scratch rather than converted, to isolate representation effects from data quality differences.

2. **Measure novel predicate frequency** in the dialogue corpus and evaluate parser recall specifically on these cases, comparing performance between WISeR and AMR setups.

3. **Conduct ablation studies** on the parser to determine whether improvements come from thematic roles, open predicates, or WSD removal individually, using controlled experiments where each feature is toggled.