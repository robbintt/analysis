---
ver: rpa2
title: 'FairWASP: Fast and Optimal Fair Wasserstein Pre-processing'
arxiv_id: '2311.00109'
source_url: https://arxiv.org/abs/2311.00109
tags:
- optimal
- data
- which
- problem
- fairw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FairWASP is a pre-processing method for reducing bias in classification
  datasets without modifying the original data. It learns sample-level integer weights
  that minimize the Wasserstein distance to the original dataset while satisfying
  demographic parity constraints.
---

# FairWASP: Fast and Optimal Fair Wasserstein Pre-processing

## Quick Facts
- arXiv ID: 2311.00109
- Source URL: https://arxiv.org/abs/2311.00109
- Reference count: 40
- Key outcome: Fast pre-processing method that learns integer weights to minimize Wasserstein distance while satisfying demographic parity constraints

## Executive Summary
FairWASP is a pre-processing approach for reducing bias in classification datasets that learns sample-level integer weights to minimize the Wasserstein distance to the original dataset while ensuring demographic parity. The method reformulates the problem as a mixed-integer program and solves it using a highly efficient cutting plane algorithm. Experiments demonstrate that FairWASP significantly outperforms commercial solvers in speed while achieving competitive performance with existing methods in reducing disparities and preserving accuracy in downstream classification tasks.

## Method Summary
FairWASP is a pre-processing method that reduces bias in classification datasets by learning sample-level integer weights that minimize the Wasserstein distance to the original dataset while satisfying demographic parity constraints. The method reformulates the pre-processing task as a large-scale mixed-integer program (MIP) and solves it using a highly efficient cutting plane algorithm. The integer weights can be interpreted as duplicating or eliminating samples, making the method compatible with any classification algorithm. FairWASP was evaluated on synthetic and real datasets including Adult, Drug, Communities & Crime, and German Credit datasets, showing significant speed improvements over commercial solvers while maintaining competitive fairness-utility tradeoffs.

## Key Results
- FairWASP significantly outperforms commercial solvers in solving speed
- Achieves competitive fairness-utility tradeoffs compared to existing methods
- Returns integer weights that can be interpreted as duplicating or eliminating samples
- Preserves accuracy in downstream classification tasks while reducing disparities

## Why This Works (Mechanism)

### Mechanism 1
Integer weights are optimal for minimizing Wasserstein distance while satisfying demographic parity. This works by reformulating the pre-processing task as a mixed-integer program (MIP) and proving that integer weights achieve the same objective as real-valued weights. The core assumption is that the constraint matrix A and cost matrix C are such that the MIP solution is also optimal for the LP relaxation. This breaks if Assumption 1 fails (i.e., the LP relaxation has multiple optimal solutions).

### Mechanism 2
The cutting plane method efficiently solves the reformulated LP relaxation. This works by reformulating the LP relaxation as a dual problem with fewer variables, then using subgradient-based cutting plane methods. The core assumption is that the dual problem can be efficiently solved using subgradient methods. This breaks if the dual problem is not convex or subgradients are not accessible.

### Mechanism 3
FairWASP achieves competitive fairness-utility tradeoffs compared to existing methods. This works by pre-processing datasets by reweighting samples to minimize Wasserstein distance while satisfying demographic parity, resulting in improved downstream classifier performance. The core assumption is that downstream classifiers trained on reweighted datasets will perform similarly to those trained on the original dataset. This breaks if downstream classifiers are sensitive to dataset distribution changes.

## Foundational Learning

- Concept: Wasserstein distance
  - Why needed here: FairWASP uses Wasserstein distance to measure the discrepancy between the original and reweighted datasets
  - Quick check question: What is the definition of the Wasserstein distance between two probability distributions?

- Concept: Mixed-integer programming (MIP)
  - Why needed here: FairWASP reformulates the pre-processing task as an MIP to find optimal integer weights
  - Quick check question: What is the difference between an MIP and an LP?

- Concept: Cutting plane method
  - Why needed here: FairWASP uses the cutting plane method to efficiently solve the reformulated LP relaxation
  - Quick check question: How does the cutting plane method work for solving convex optimization problems?

## Architecture Onboarding

- Component map:
  Data preprocessing module -> MIP formulation module -> Cutting plane solver -> Weight application module -> Downstream classifier module

- Critical path:
  1. Input dataset and compute pairwise distances
  2. Reformulate the pre-processing task as an MIP
  3. Solve the LP relaxation using the cutting plane method
  4. Apply the computed integer weights to the dataset
  5. Train and evaluate the downstream classifier on the reweighted dataset

- Design tradeoffs:
  - Speed vs. accuracy: FairWASP prioritizes speed by using integer weights and an efficient cutting plane method, potentially at the cost of some accuracy
  - Complexity vs. interpretability: FairWASP's MIP formulation and cutting plane method are complex, but the resulting integer weights are interpretable as duplications or eliminations of samples

- Failure signatures:
  - If the cutting plane method fails to converge, the LP relaxation may not be solved optimally
  - If the downstream classifier is sensitive to dataset distribution changes, the fairness-utility tradeoff may degrade

- First 3 experiments:
  1. Verify that FairWASP correctly computes the pairwise distances between samples in the input dataset
  2. Test that FairWASP correctly reformulates the pre-processing task as an MIP
  3. Validate that FairWASP's cutting plane method converges to an optimal solution for the LP relaxation

## Open Questions the Paper Calls Out

### Open Question 1
How does FairWASP's integer weight solution compare to real-valued weight solutions in terms of downstream classification performance? The paper proves that integer weights are optimal for the Wasserstein distance minimization problem but doesn't directly compare the performance of FairWASP's integer weights versus real-valued weights on actual classification tasks. Running FairWASP with both integer and real-valued weights on the same datasets and comparing their downstream classification accuracy and fairness metrics would resolve this.

### Open Question 2
What is the impact of the choice of Wasserstein distance metric (e.g., 1-Wasserstein vs 2-Wasserstein) on FairWASP's performance? The paper mentions that different choices of Wasserstein distance can be used but only implements the 1-Wasserstein distance in experiments. Running FairWASP experiments with different Wasserstein distance metrics and comparing their performance in terms of fairness-utility tradeoff would resolve this.

### Open Question 3
How does FairWASP perform when applied to datasets with more than two demographic groups or outcome classes? The paper mentions that FairWASP can handle multiple demographic groups and outcome classes but only provides experimental results for binary cases. Applying FairWASP to datasets with multiple demographic groups and multiple outcome classes, and evaluating its performance in terms of fairness and accuracy would resolve this.

## Limitations
- Computational complexity may become infeasible for very large datasets
- Performance depends on the validity of Assumption 1 for real-world datasets
- Limited experimental validation on datasets with more than two demographic groups or outcome classes

## Confidence
- Mechanism 1 (Integer optimality): Medium
- Mechanism 2 (Cutting plane efficiency): Medium
- Mechanism 3 (Fairness-utility tradeoffs): Medium

## Next Checks
1. Verify the integer optimality claim by testing FairWASP on datasets where Assumption 1 may not hold
2. Benchmark FairWASP against other fair pre-processing methods on datasets with different characteristics (size, sparsity, dimensionality)
3. Conduct ablation studies to isolate the impact of the cutting plane method's efficiency on overall performance