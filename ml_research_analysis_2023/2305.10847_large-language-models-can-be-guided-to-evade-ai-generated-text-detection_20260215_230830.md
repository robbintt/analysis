---
ver: rpa2
title: Large Language Models can be Guided to Evade AI-Generated Text Detection
arxiv_id: '2305.10847'
source_url: https://arxiv.org/abs/2305.10847
tags:
- text
- sico
- detectors
- prompt
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method called SICO (Substitution-based In-Context
  example Optimization) to generate prompts that can guide large language models (LLMs)
  to produce text that can evade AI-generated text detection. The method works by
  iteratively substituting words and sentences in in-context examples to provide high-quality
  demonstrations for the LLM.
---

# Large Language Models can be Guided to Evade AI-Generated Text Detection

## Quick Facts
- arXiv ID: 2305.10847
- Source URL: https://arxiv.org/abs/2305.10847
- Authors: 
- Reference count: 40
- One-line primary result: SICO method enables LLMs to generate text that evades six existing AI-generated text detectors with average AUC reduction of 0.5

## Executive Summary
This paper introduces SICO (Substitution-based In-Context example Optimization), a method that guides large language models to generate text that evades AI-generated text detection. The approach iteratively optimizes in-context examples by substituting words and sentences to minimize proxy detector scores, enabling LLMs to produce text that appears more human-like. Extensive experiments demonstrate that SICO significantly outperforms paraphrase baselines and achieves human-level readability while successfully evading multiple detectors across three real-world tasks.

## Method Summary
SICO works by iteratively substituting words and sentences within in-context examples to optimize demonstrations for LLMs. The method uses a proxy detector to evaluate prompt utility, employing word-level substitutions via WordNet synonyms and sentence-level substitutions via LLM paraphrasing. Through this optimization process, the LLM generates text that minimizes detection scores without requiring external paraphrasing steps. The approach preserves semantic meaning while altering surface features to evade detectors trained on linguistic patterns.

## Key Results
- SICO decreases detector AUC scores by 0.5 on average across six existing detectors
- Method achieves human-level readability and task completion rates for generated text
- Successfully preserves semantic meaning during paraphrasing, outperforming other paraphrasers
- Enables ChatGPT to evade detection without requiring external paraphrasing steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SICO reduces detection by iteratively optimizing in-context examples to mimic human writing patterns.
- Mechanism: The method substitutes words and sentences in in-context examples to minimize the proxy detector's AI probability score. By doing so, the LLM generates text that appears more human-like, evading detectors trained to identify AI-generated patterns.
- Core assumption: LLMs can learn to generate human-like text when provided with optimized demonstrations that minimize AI-detection signals.
- Evidence anchors:
  - [abstract] "SICO iteratively substitutes words and sentences within in-context examples, aiming at providing high-quality demonstrations for LLMs to generate text that cannot be detected"
  - [section] "The optimization goal can be expressed as: y*ic = arg min_sim(y′ic,yic)>ϵ PAI(y′ic)"
- Break condition: If the proxy detector's feature space does not capture key differences between human and AI text, the optimization may fail to find effective substitutions.

### Mechanism 2
- Claim: Substitution-based optimization preserves semantic meaning while altering surface features to evade detection.
- Mechanism: Word-level substitutions use WordNet synonyms with matching part-of-speech tags, while sentence-level substitutions use LLM paraphrasing. This maintains semantic consistency while changing linguistic patterns detectors rely on.
- Core assumption: Detectors rely on surface-level linguistic features that can be altered without changing meaning, and semantic preservation is possible through controlled substitutions.
- Evidence anchors:
  - [section] "For word-level substitution, we use WordNet [33], a lexical database of English words, to construct a synonym substitution set. We restrict substitutions to content words... For sentence-level substitution, we utilize the LLM and the paraphrasing task instruction"
  - [section] "Our methods successfully preserves the semantic meaning during paraphrasing, and beats the other specifically trained paraphraser"
- Break condition: If detectors rely on deep semantic patterns rather than surface features, semantic-preserving substitutions may not evade detection.

### Mechanism 3
- Claim: In-context learning with optimized demonstrations is more effective than external paraphrasing for generating evasive text.
- Mechanism: Instead of generating text first and then paraphrasing it externally, SICO guides the LLM during generation to produce text that is inherently less detectable. This avoids semantic drift and maintains text quality.
- Core assumption: LLMs can generate evasive text directly when provided with optimized demonstrations, eliminating the need for post-hoc paraphrasing.
- Evidence anchors:
  - [abstract] "One main advantage of SICO over paraphrase attacks [14, 15] is that SICO can make LLMs directly generate text which cannot be detected, thus eliminating the need for an extra paraphrasing step"
  - [section] "In most settings, our method reduces the AUC score to less than 0.5, equivalent to the expected performance of a random classifier"
- Break condition: If the LLM's generation process is too rigid or if the optimized demonstrations do not sufficiently influence generation, this approach may not outperform external paraphrasing.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: SICO relies on ICL to make the LLM understand what kind of output is desired by providing demonstrations. Understanding ICL is crucial to grasp how SICO works.
  - Quick check question: How does in-context learning differ from fine-tuning, and why is it advantageous in this scenario?

- Concept: Word embeddings and semantic similarity
  - Why needed here: The method uses synonym substitution based on WordNet, which relies on understanding semantic relationships between words. This is important for ensuring substitutions maintain meaning.
  - Quick check question: Why is it important to restrict substitutions to content words and ensure they have the same part-of-speech tags?

- Concept: Detection methods for AI-generated text
  - Why needed here: Understanding how detectors work (training-based, statistical, watermarking) is essential to comprehend why SICO can evade them and how to improve detector robustness.
  - Quick check question: What are the key differences between training-based detectors and statistical methods, and how might each be vulnerable to different attack strategies?

## Architecture Onboarding

- Component map:
  LLM (ChatGPT/GPT-3.5/Vicuna) -> Proxy detector (e.g., ChatGPT Detector) -> SICO optimizer -> WordNet database

- Critical path:
  1. Collect human-AI text pairs for the task
  2. Extract language features distinguishing human from AI text
  3. Generate initial in-context examples using extracted features
  4. Iteratively optimize examples using word/sentence substitution to minimize proxy detector score
  5. Evaluate optimized prompt against target detectors

- Design tradeoffs:
  - Number of iterations vs. computational cost
  - Semantic preservation vs. detection evasion
  - Using proxy detector vs. target detector for optimization (transferability issues)
  - Word-level vs. sentence-level substitution strategies

- Failure signatures:
  - AUC scores remain high despite optimization attempts
  - Semantic similarity drops significantly after substitution
  - Optimization converges to suboptimal solutions
  - Method fails to generalize across different detectors

- First 3 experiments:
  1. Verify that the proxy detector can distinguish between human and AI text on the initial dataset
  2. Test the impact of varying the number of in-context examples (K) on detection evasion performance
  3. Compare the effectiveness of word-level vs. sentence-level substitution in reducing detection rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SICO's approach be extended to detect and evade detectors that use watermarking techniques?
- Basis in paper: [inferred] The paper discusses watermarking as a detection method and notes that SICO can act as an external paraphraser to evade this detection, but does not explicitly test this capability.
- Why unresolved: The paper does not provide experimental results on SICO's effectiveness against watermarking-based detectors.
- What evidence would resolve it: Experiments showing SICO's performance against detectors that use watermarking techniques, including quantitative metrics like AUC and detection accuracy.

### Open Question 2
- Question: How does the choice of proxy detector during SICO's training phase affect its transferability to other detectors?
- Basis in paper: [explicit] The paper explores using different proxy detectors (ChatGPT detector, OpenAI detector, and DetectGPT) and shows that SICO maintains high detection evasion performance, but the transferability is not fully analyzed.
- Why unresolved: The paper does not provide a comprehensive analysis of how the choice of proxy detector influences SICO's effectiveness against various types of detectors.
- What evidence would resolve it: Detailed experiments comparing SICO's performance when trained with different proxy detectors against a diverse set of detectors, including statistical, training-based, and watermarking methods.

### Open Question 3
- Question: What is the impact of SICO on the semantic integrity of the generated text, and how can this be quantitatively measured?
- Basis in paper: [explicit] The paper mentions that SICO preserves semantic meaning during paraphrasing, but the evaluation is limited to cosine similarity using a sentence encoder.
- Why unresolved: The paper does not explore other quantitative measures of semantic integrity or provide a comprehensive analysis of how SICO affects the meaning of the text.
- What evidence would resolve it: Additional experiments using various semantic similarity metrics, human evaluations, and qualitative analysis to assess the impact of SICO on the semantic integrity of the generated text.

## Limitations

- Proxy-Detector Dependence: The method's effectiveness heavily relies on the proxy detector's ability to distinguish human from AI text, which may not generalize to all detector types.
- Semantic Preservation Trade-offs: There's an inherent tension between maintaining semantic fidelity and maximizing detection evasion that could lead to subtle semantic drift over multiple iterations.
- Generalizability Across Domains: The method was validated on three specific tasks and may not generalize equally well to domains with different linguistic patterns or stricter semantic requirements.

## Confidence

- High Confidence: The SICO method successfully reduces detection rates across the tested detectors (AUC reduction of 0.5 on average). The experimental results are consistent and the methodology is clearly described.
- Medium Confidence: The claim that SICO-generated text achieves "human-level readability and task completion rates" is supported by the experiments but relies on specific evaluation metrics that may not capture all aspects of text quality.
- Low Confidence: The assertion that SICO "enables ChatGPT to successfully evade six existing detectors" may overstate the generalizability. While the average AUC reduction is substantial, individual detector performance varies.

## Next Checks

1. **Cross-Detector Robustness Test:** Evaluate SICO-generated text against a broader range of detectors including those not based on language models (e.g., statistical watermarking detectors, training-based classifiers).

2. **Long-Form Text Analysis:** Test the method's effectiveness on longer text sequences (multiple paragraphs or full documents) to assess whether the optimization scales and whether semantic drift becomes more pronounced in extended content.

3. **Adversarial Training Countermeasure:** Train a detector using SICO-generated text as negative examples and test whether this adversarial training can improve detection rates.