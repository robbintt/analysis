---
ver: rpa2
title: A Study of Bayesian Neural Network Surrogates for Bayesian Optimization
arxiv_id: '2305.20028'
source_url: https://arxiv.org/abs/2305.20028
tags:
- bayesian
- function
- neural
- optimization
- variance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study compares Bayesian neural networks (BNNs) to Gaussian\
  \ processes (GPs) as surrogate models for Bayesian optimization. It evaluates various\
  \ BNN architectures and inference methods\u2014including Hamiltonian Monte Carlo\
  \ (HMC), Stochastic Gradient Hamiltonian Monte Carlo (SGHMC), linearized Laplace\
  \ approximation (LLA), deep ensembles, infinite-width BNNs, and deep kernel learning\
  \ (DKL)\u2014on synthetic and real-world benchmarks with varying dimensionality,\
  \ objective counts, and input types."
---

# A Study of Bayesian Neural Network Surrogates for Bayesian Optimization

## Quick Facts
- arXiv ID: 2305.20028
- Source URL: https://arxiv.org/abs/2305.20028
- Reference count: 40
- Key outcome: BNNs show promise as BO surrogates but no single method dominates; HMC performs best among fully stochastic BNNs, infinite-width BNNs excel in high dimensions, and DKL is competitive despite lacking full stochasticity.

## Executive Summary
This study compares Bayesian neural networks (BNNs) to Gaussian processes (GPs) as surrogate models for Bayesian optimization across synthetic and real-world benchmarks. The research evaluates various BNN architectures and inference methods including Hamiltonian Monte Carlo, deep ensembles, infinite-width BNNs, and deep kernel learning. Results show that while no single method dominates across all problems, infinite-width BNNs are particularly promising for high-dimensional tasks, and deep kernel learning performs competitively despite lacking full stochasticity. The findings suggest that problem-specific tailoring of surrogate models is essential for optimal Bayesian optimization performance.

## Method Summary
The study evaluates BNN surrogates against standard GP baselines using Monte Carlo Expected Improvement acquisition functions across multiple synthetic and real-world benchmarks. The experiments test various BNN architectures (finite and infinite width) with different inference methods (HMC, SGHMC, LLA, deep ensembles, DKL). Input data is normalized to [0,1] and outputs standardized to zero mean and unit variance. The researchers compare performance based on maximum reward found over function evaluations, with standard error reported across multiple runs.

## Key Results
- No single BNN method dominates across all benchmark problems
- HMC generally outperforms other fully stochastic BNN inference methods
- Infinite-width BNNs show particular promise for high-dimensional optimization tasks
- Deep kernel learning performs competitively despite lacking full stochasticity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The non-stationary nature of BNN surrogates better captures objective function variation across the input space.
- **Mechanism**: Standard GPs assume translation invariance via kernel functions dependent only on distance, making them stationary. BNNs, particularly infinite-width and deep kernel learning variants, can represent non-stationary behavior by either using neural network-derived kernels or learning input representations that capture local variation.
- **Core assumption**: The true objective functions in BO exhibit meaningful non-stationarity that standard GP kernels cannot capture.
- **Evidence anchors**:
  - [abstract]: "BNNs can flexibly represent the non-stationary behavior typical of optimization objectives"
  - [section 4.4]: "gp surrogates...assumes the function is stationary and has similar mean and smoothness throughout the input space"
- **Break condition**: If benchmark objectives are actually stationary, the BNN advantage disappears and standard GPs may outperform.

### Mechanism 2
- **Claim**: Deep kernel learning performs competitively despite lacking full stochasticity due to small data sizes in BO.
- **Mechanism**: In BO, data is scarce (small number of expensive queries). Fully stochastic BNNs risk overfitting with limited data, while DKL combines learned representations with exact GP inference, providing good uncertainty quantification without requiring full posterior sampling.
- **Core assumption**: The small data regime in BO makes full stochasticity less valuable than representation learning with exact inference.
- **Evidence anchors**:
  - [abstract]: "(iii) full stochasticity may be unnecessary as deep kernel learning is relatively competitive"
  - [section 4.2]: "we find that the different approximate inference methods for finite-width BNNs lead to significantly different Bayesian optimization performance, with hmc generally finding higher rewards compared to sghmc, lla, and deep ensembles"
- **Break condition**: If BO problems had abundant cheap queries, full stochasticity might become advantageous.

### Mechanism 3
- **Claim**: Infinite-width BNNs excel in high dimensions due to their non-Euclidean similarity metrics and lack of hyperparameter learning.
- **Mechanism**: Infinite-width BNNs provide non-stationary, non-Euclidean similarity measures derived from neural network architectures without requiring representation learning or hyperparameter optimization. This is valuable in high dimensions where GPs struggle with curse of dimensionality and data-hungry representation learning.
- **Core assumption**: High-dimensional BO problems benefit more from strong priors and non-Euclidean metrics than from learned representations.
- **Evidence anchors**:
  - [abstract]: "(v) infinite-width BNNs are particularly promising, especially in high dimensions"
  - [section 4.4]: "The i-bnn has several conceptual advantages in this setting: (1) it provides a non-Euclidean and non-stationary similarity metric, which can be particularly valuable in high-dimensions; (2) it does not have any hyperparameters for learning, and thus is not 'data hungry'"
- **Break condition**: If high-dimensional problems have abundant data and simple structure, learned representations (DKL) might outperform strong priors.

## Foundational Learning

- **Concept**: Gaussian Process posterior conditioning
  - Why needed here: GPs form the baseline surrogate model; understanding how conditioning works is essential for comparing alternatives
  - Quick check question: Given training data, how do you compute the predictive mean and variance at a new test point using GP conditioning?

- **Concept**: Bayesian neural network approximate inference
  - Why needed here: BNNs require approximate inference (HMC, SGHMC, ensembles, etc.); understanding these methods is crucial for implementation
  - Quick check question: What's the key difference between full-batch HMC and stochastic gradient HMC in terms of computational cost and accuracy?

- **Concept**: Bayesian optimization acquisition functions
  - Why needed here: Surrogate models feed into acquisition functions; understanding this connection is necessary for practical implementation
  - Quick check question: How does expected improvement use the predictive distribution from a surrogate model to select the next query point?

## Architecture Onboarding

- **Component map**: Surrogate model -> Approximate inference method -> Acquisition function -> Optimization loop -> Evaluation metrics
- **Critical path**: Model selection → Inference method choice → Architecture design → Acquisition function → Optimization loop
- **Design tradeoffs**:
  - Stochasticity vs. computational efficiency (full BNNs vs. DKL)
  - Representation learning vs. strong priors (finite-width BNNs vs. infinite-width)
  - Accuracy vs. scalability (HMC vs. SGHMC)
  - Model complexity vs. data availability (architecture size)
- **Failure signatures**:
  - Poor uncertainty calibration (overestimated/underestimated variance)
  - Premature convergence to local optima
  - High computational cost preventing sufficient iterations
  - Model overfitting on small datasets
- **First 3 experiments**:
  1. Implement standard GP with Matérn-5/2 kernel on Branin function to establish baseline
  2. Implement DKL with same feature extractor architecture as HMC experiments for controlled comparison
  3. Run HMC on Ackley function with varying architecture widths to understand sensitivity to model size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do BNN surrogates compare to GP surrogates on real-world optimization problems with high non-stationarity?
- Basis in paper: [inferred] The paper shows BNNs handle non-stationary functions better than GPs in synthetic benchmarks, but real-world results are mixed.
- Why unresolved: The paper only provides limited real-world benchmark results without explicitly focusing on non-stationarity.
- What evidence would resolve it: A focused study comparing BNN and GP performance on real-world problems known to be highly non-stationary, with quantitative metrics on non-stationarity handling.

### Open Question 2
- Question: What is the impact of neural architecture search on BNN surrogate performance in practical Bayesian optimization settings?
- Basis in paper: [explicit] The paper shows neural architecture search can improve HMC performance, but notes it's often impractical due to high function evaluation requirements.
- Why unresolved: The paper demonstrates potential improvements but doesn't explore practical trade-offs or efficient search strategies.
- What evidence would resolve it: Experiments comparing BNN performance with and without architecture search, measuring the cost-benefit trade-off in terms of function evaluations and optimization efficiency.

### Open Question 3
- Question: Do infinite-width BNNs maintain their performance advantage over GPs as problem dimensionality increases beyond tested ranges?
- Basis in paper: [explicit] The paper shows infinite-width BNNs perform well in high-dimensional settings but only tests up to certain dimensions.
- Why unresolved: The paper doesn't test extreme high-dimensional cases where the theoretical advantages of infinite-width BNNs might break down.
- What evidence would resolve it: Systematic testing of infinite-width BNNs and GPs on problems with thousands to millions of dimensions, measuring performance degradation and identifying breaking points.

## Limitations
- Results show no universal dominance, requiring problem-specific model selection
- HMC computational costs may limit practical applicability
- Limited testing on extreme high-dimensional problems beyond tested ranges
- Focus on specific BNN architectures leaves questions about other variants

## Confidence
- High confidence: Standard GPs remain competitive due to simplicity and exact inference
- Medium confidence: DKL performs competitively despite lacking full stochasticity
- Medium confidence: Infinite-width BNNs excel in high dimensions
- Low confidence: No single method dominates universally across all problem types

## Next Checks
1. Test infinite-width BNN performance on high-dimensional problems with abundant data to verify the strong-prior advantage persists
2. Compare computational efficiency of HMC vs SGHMC across varying problem sizes to quantify scalability tradeoffs
3. Evaluate DKL with different kernel functions (beyond Matérn-5/2) to assess sensitivity to kernel choice