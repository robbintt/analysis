---
ver: rpa2
title: Proposal-Contrastive Pretraining for Object Detection from Fewer Data
arxiv_id: '2310.16835'
source_url: https://arxiv.org/abs/2310.16835
tags:
- object
- pretraining
- learning
- detection
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Proposal Selection Contrast (ProSeCo), a novel
  unsupervised pretraining approach for object detection that leverages the large
  number of object proposals generated by transformer-based detectors. ProSeCo uses
  a student-teacher architecture with a fixed pretrained backbone, where the teacher
  provides object proposals and the student is trained via a contrastive loss that
  incorporates localization information to select positive examples.
---

# Proposal-Contrastive Pretraining for Object Detection from Fewer Data

## Quick Facts
- arXiv ID: 2310.16835
- Source URL: https://arxiv.org/abs/2310.16835
- Reference count: 26
- ProSeCo achieves +5.2% mAP improvement over supervised pretraining on Mini-COCO 5% (5.9k images)

## Executive Summary
This paper presents ProSeCo, a novel unsupervised pretraining approach for object detection that leverages object proposals from transformer-based detectors. The method uses a student-teacher architecture with a fixed pretrained backbone, where the teacher provides object proposals and the student is trained via a contrastive loss that incorporates localization information. ProSeCo outperforms state-of-the-art pretraining methods on standard and few-shot benchmarks, achieving significant improvements in detection performance with limited data.

## Method Summary
ProSeCo uses a student-teacher architecture where the teacher (EMA-updated copy of student) generates object proposals from weakly augmented images, while the student processes strongly augmented images. The method employs a contrastive loss that treats overlapping object proposals (IoU ≥ δ) as positive examples, reducing the need for large batch sizes by leveraging the high number of proposals generated by transformer detectors. A fixed pretrained backbone ensures consistency between backbone and detection head features during pretraining, with localization guidance provided by Selective Search.

## Key Results
- +5.2% mAP improvement over supervised pretraining on Mini-COCO 5% (5.9k images)
- Outperforms state-of-the-art pretraining methods including DeiT and MAE on multiple benchmarks
- Achieves competitive results on Pascal VOC with only 5.9k training images

## Why This Works (Mechanism)

### Mechanism 1
ProSeCo reduces the need for large batch sizes by leveraging the high number of object proposals generated by transformer-based detectors for contrastive learning. This provides diverse instances for contrastive learning without requiring large image batches, addressing the memory constraints of dense prediction tasks.

### Mechanism 2
Introducing localization information in the contrastive loss improves learned features by considering overlapping object proposals as positive examples. Computing pairwise IoU between proposals and treating those with IoU ≥ δ as additional positives helps learn features that better capture object-level information.

### Mechanism 3
Maintaining consistency between backbone and detection head features during pretraining improves performance for dense downstream tasks. Using a fixed pretrained backbone that has been trained for local information, combined with a teacher-student architecture, ensures consistent feature learning between backbone and detection head.

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: ProSeCo uses a contrastive objective function to learn representations by contrasting positive pairs against negative pairs
  - Quick check question: How does the InfoNCE loss function encourage similar representations for positive pairs while pushing apart negative pairs?

- Concept: Transformer-based object detectors and object proposals
  - Why needed here: ProSeCo specifically leverages the large number of object proposals generated by transformer-based detectors
  - Quick check question: How do transformer-based object detectors like DETR generate object proposals, and why do they produce more proposals than traditional detectors?

- Concept: Teacher-student architecture and EMA updates
  - Why needed here: ProSeCo uses a student-teacher architecture with the teacher updated through EMA of the student's weights
  - Quick check question: What is the purpose of using EMA updates for the teacher model, and how does this differ from directly copying student weights?

## Architecture Onboarding

- Component map: Images -> Backbone (fixed pretrained) -> Teacher Transformer Detector -> Object Proposals -> Student Transformer Detector -> Predictions -> Hungarian Matching -> Localization-aware Contrastive Loss

- Critical path:
  1. Generate weak and strong augmented views of input images
  2. Student model processes strong view to produce predictions
  3. Teacher model processes weak view to produce object proposals
  4. Hungarian matching for proposal and box alignment
  5. Compute localization-aware contrastive loss
  6. Update student model parameters

- Design tradeoffs:
  - Fixed vs. trainable backbone: Fixed reduces parameters but may limit adaptation to detection task
  - Number of object proposals (N): Higher N increases effective batch size but adds computational cost
  - IoU threshold (δ) for positive examples: Higher δ reduces noise but may miss valid positives

- Failure signatures:
  - Poor localization performance: Check if Selective Search boxes are being properly matched
  - Contrastive loss not converging: Verify temperature parameters (τ=0.1, τt=0.07) are appropriately set
  - No improvement over baseline: Confirm backbone is pretrained for local information

- First 3 experiments:
  1. Run with default hyperparameters on Mini-COCO 5% to establish baseline performance
  2. Test with different IoU thresholds (δ) to find optimal value for localization-aware positives
  3. Compare with and without localization information in contrastive loss to verify its contribution

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of IoU threshold δ impact the performance of the localization-aware contrastive loss across different object sizes and densities in the dataset? The paper shows best results with δ = 0.5 but lacks detailed analysis across varying object characteristics.

### Open Question 2
What is the impact of updating the backbone during pretraining on the consistency between backbone and detection head features? The paper suggests future work could update the backbone but doesn't experiment with this approach.

### Open Question 3
How does the performance of ProSeCo compare to other self-supervised learning methods on datasets with varying levels of annotation? The paper doesn't compare with other self-supervised methods across different annotation levels.

### Open Question 4
What is the effect of different backbone architectures on the performance of ProSeCo? The paper uses ResNet-50 and mentions SCRL helps but doesn't explore various backbone options systematically.

## Limitations

- The localization-aware positive selection mechanism (IoU ≥ 0.5) lacks ablation studies on threshold sensitivity
- Fixed backbone design may limit adaptation to detection-specific features despite computational efficiency
- Reliance on Hungarian matching for proposal alignment could become a bottleneck with complex scenes

## Confidence

- High confidence: ProSeCo outperforms supervised pretraining on Mini-COCO 5% (5.2% mAP improvement)
- Medium confidence: Localization information improves contrastive learning
- Medium confidence: Batch size reduction through object proposals

## Next Checks

1. Ablation on IoU threshold: Systematically test δ values (0.3, 0.5, 0.7) to determine optimal localization-aware positive selection and quantify its contribution to performance gains.

2. Proposal diversity analysis: Measure the actual diversity and quality of object proposals generated by the transformer detector and correlate with downstream performance to validate the batch size reduction claim.

3. Backbone finetuning comparison: Compare fixed backbone vs. finetunable backbone variants of ProSeCo to quantify the tradeoff between efficiency and adaptation capability.