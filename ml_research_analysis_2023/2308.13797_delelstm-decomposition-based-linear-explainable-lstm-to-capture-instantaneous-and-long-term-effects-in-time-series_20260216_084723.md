---
ver: rpa2
title: 'DeLELSTM: Decomposition-based Linear Explainable LSTM to Capture Instantaneous
  and Long-term Effects in Time Series'
arxiv_id: '2308.13797'
source_url: https://arxiv.org/abs/2308.13797
tags:
- time
- variable
- series
- each
- delelstm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a decomposition-based linear explainable LSTM
  (DeLELSTM) to improve the interpretability of LSTM for time series forecasting.
  The core idea is to decompose the LSTM hidden state into two components: the past
  information from the previous time step and the dynamic change brought by the new
  observations.'
---

# DeLELSTM: Decomposition-based Linear Explainable LSTM to Capture Instantaneous and Long-term Effects in Time Series

## Quick Facts
- arXiv ID: 2308.13797
- Source URL: https://arxiv.org/abs/2308.13797
- Reference count: 33
- Key outcome: Decomposition-based linear explainable LSTM (DeLELSTM) achieves competitive forecasting performance and provides interpretable measures of instantaneous vs. long-term variable effects in time series.

## Executive Summary
This paper introduces DeLELSTM, a novel LSTM architecture that decomposes hidden states to separately capture instantaneous influences and long-term effects of each variable in time series forecasting. By using a tensorized LSTM to model each variable independently and combining it with a standard LSTM, the model approximates the standard LSTM hidden state as a linear combination of past information and dynamic changes. This decomposition enables transparent interpretation of variable importance over time while maintaining competitive predictive performance across three real-world datasets.

## Method Summary
DeLELSTM combines a standard LSTM (processing all variables together with shared hidden state Ht) with a tensorized LSTM (processing each variable separately with variable-specific hidden states ht). The key innovation is decomposing Ht as a linear combination of ht-1 (past information) and (ht - ht-1) (dynamic change) using least squares optimization. The coefficients αt and βt from this decomposition directly quantify each variable's instantaneous and long-term contributions. The model uses these coefficients to compute interpretable importance metrics and employs them in forecasting via a linear combination of the two components.

## Key Results
- Achieves competitive RMSE, MAE, and MAPE compared to baseline methods across PM2.5, Electricity, and Exchange datasets
- Successfully identifies known variable relationships (e.g., temperature affecting PM2.5 levels) through case studies
- Demonstrates ability to distinguish instantaneous influences from long-term effects through visualization of importance metrics over time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing Ht into linear combination of ht-1 and (ht - ht-1) separates instantaneous from long-term effects
- Mechanism: Standard LSTM produces Ht, tensorized LSTM produces variable-specific ht matrix, least squares finds optimal αt and βt coefficients, normalized coefficients define importance metrics
- Core assumption: Linear approximation preserves sufficient information for accurate forecasting
- Evidence anchors: [abstract] states decomposition enables instantaneous and long-term effect analysis; [section] describes using linear combination to approximate Ht
- Break condition: High least squares error causes decomposition to lose meaningful separation

### Mechanism 2
- Claim: Tensorized LSTM ensures each variable has dedicated hidden state for variable-specific decomposition
- Mechanism: Tensor-dot operations maintain per-variable structure through gates/memory cells, allowing Δht = ht - ht-1 to represent new information per variable
- Core assumption: Variable-specific hidden states capture all relevant dynamics without cross-variable interference
- Evidence anchors: [section] explains tensorized LSTM assigns unique hidden states making up matrix ht; [section] states each row only encodes information from one variable
- Break condition: High variable correlation causes per-variable states to lose important cross-variable dependencies

### Mechanism 3
- Claim: Linear regression weights provide transparent, interpretable measures of instantaneous vs. long-term importance
- Mechanism: Normalized absolute values of αt and βt coefficients define instantaneous importance In^d_t and long-term effect = 1 - In^d_t
- Core assumption: Coefficient magnitude meaningfully proxies for feature importance and normalization appropriately scales across variables/time
- Evidence anchors: [section] derives significance from magnitude of linear approximation weights; [section] defines instantaneous importance as ratio of normalized coefficients
- Break condition: Ill-conditioned least squares solution or unstable coefficients make importance measures unreliable

## Foundational Learning

- Concept: Tensor-dot operations and their role in maintaining per-variable structure in tensorized LSTM
  - Why needed here: Ensures each variable's gates and memory cells only interact with its own hidden state, preserving decomposition structure
  - Quick check question: How does the tensor-dot operation differ from standard matrix multiplication in the context of tensorized LSTM?

- Concept: Linear regression and least squares optimization for approximating hidden states
  - Why needed here: Decomposition relies on finding optimal coefficients αt and βt that minimize approximation error, a standard linear regression problem
  - Quick check question: What is the closed-form solution for the least squares problem used to compute αt and βt?

- Concept: Attention mechanisms vs. decomposition-based interpretability
  - Why needed here: Paper contrasts its approach with attention-based models, understanding limitations of attention is key to appreciating decomposition method
  - Quick check question: What are the main criticisms of attention-based interpretability mentioned in the paper?

## Architecture Onboarding

- Component map:
  xt → Standard LSTM → Ht → Least Squares → Ĥt → Predictor → yt+1
  xt → Tensorized LSTM → ht → Δht → Least Squares → Ĥt

- Critical path:
  Input sequence → Standard LSTM (shared hidden state) → Tensorized LSTM (variable-specific states) → Decomposition layer (least squares) → Approximated hidden state → Predictor → Forecast

- Design tradeoffs:
  - Decomposition adds interpretability but requires solving least squares problem at each time step (computational overhead)
  - Tensorized LSTM increases parameter count but enables per-variable analysis
  - Linear approximation may lose information if Ht is not well-represented by ht-1 and Δht

- Failure signatures:
  - High least squares error indicates poor decomposition quality
  - Unstable or near-zero coefficients suggest model not learning meaningful variable contributions
  - Similar importance scores across variables indicate lack of discriminative power

- First 3 experiments:
  1. Train DeLELSTM on synthetic dataset with known instantaneous vs. long-term effects; verify model recovers these effects
  2. Compare decomposition error (‖Ht - Ĥt‖) across different hidden state dimensions to find minimal size preserving decomposition quality
  3. Evaluate interpretability by checking if top-ranked variables by Gld_T align with known important variables in domain-specific dataset (e.g., temperature for PM2.5)

## Open Questions the Paper Calls Out
The paper acknowledges that the current model only provides explanations for individual variables and recognizes the need to address variable interactions in future work. It also notes that the interpretability of DeLELSTM relies on the quality of the linear decomposition approximation, which could be affected by architectural choices, though the paper does not explore how robust the explanations are to different configurations.

## Limitations
- Decomposition quality depends on linear approximation accuracy; high least squares error undermines interpretability
- Tensorized LSTM design assumes variables can be meaningfully processed independently, which may fail for highly correlated variables
- Interpretability metrics derived from least squares coefficients may not reflect true causal influence

## Confidence
- High confidence: Decomposition mechanism works as described when linear approximation error is low and tensorized LSTM properly maintains per-variable structure
- Medium confidence: Interpretability claims hold when variables are sufficiently independent and least squares solution is stable across time steps
- Low confidence: Global importance metrics accurately reflect variable significance across different datasets and time horizons

## Next Checks
1. Measure decomposition error (‖Ht - Ĥt‖) across different hidden state dimensions to identify minimal size that preserves both accuracy and interpretability
2. Test model sensitivity to initialization by training multiple runs with different random seeds and measuring coefficient stability over time
3. Compare instantaneous importance scores In^d_t against known variable response times in controlled experiment (e.g., synthetic data with immediate vs. delayed effects)