---
ver: rpa2
title: 'COMET-M: Reasoning about Multiple Events in Complex Sentences'
arxiv_id: '2305.14617'
source_url: https://arxiv.org/abs/2305.14617
tags:
- inferences
- event
- events
- comet
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating commonsense inferences
  for events in complex, multi-event sentences. The authors propose COMET-M, a model
  that builds on the existing COMET framework by fine-tuning it on a new dataset of
  35K human-written inferences for events within complex sentences.
---

# COMET-M: Reasoning about Multiple Events in Complex Sentences

## Quick Facts
- arXiv ID: 2305.14617
- Source URL: https://arxiv.org/abs/2305.14617
- Reference count: 14
- Primary result: COMET-M generates more specific, contextually relevant inferences for multiple events in complex sentences compared to baselines.

## Executive Summary
This paper addresses the challenge of generating commonsense inferences for events in complex, multi-event sentences. The authors propose COMET-M, a model that builds on the existing COMET framework by fine-tuning it on a new dataset of 35K human-written inferences for events within complex sentences. COMET-M uses a special token to indicate the target event in the input and is trained to generate event-specific inferences that are consistent with the overall context. The model is evaluated against several baselines, including off-the-shelf COMET and BART models, as well as models trained on automatically generated multi-event labels. Results show that COMET-M significantly outperforms these baselines on automatic metrics like ROUGE-L, BLEU, and BERTScore, as well as in human evaluations of likelihood, specificity, and relevance of the generated inferences. The authors also demonstrate that COMET-M can generate more distinct inferences for different events in the same sentence compared to COMET.

## Method Summary
COMET-M fine-tunes the BART-based COMET model on a human-annotated dataset of 35K multi-event commonsense inferences. The key innovation is the use of a special `<TGT>` token to mark the target event in the input, allowing the model to generate event-specific inferences while maintaining context awareness. The model is trained to generate inferences for three commonsense relations: causes, effects, and mental states. Training uses a sequence-to-sequence objective with the context, target event marker, and relation as input, and the target inference as output. The model is evaluated against baselines using automatic metrics and human judgment on specificity, relevance, and likelihood of the generated inferences.

## Key Results
- COMET-M significantly outperforms baselines on ROUGE-L, BLEU, and BERTScore metrics
- Human evaluation shows COMET-M generates more specific and contextually relevant inferences
- COMET-M produces more distinct inferences for different events in the same sentence compared to COMET
- Automatic labeling methods (COMET-M-Split, COMET-M-Overlap, COMET-M-NLI) improve performance over standard COMET but fall short of human-annotated data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a special `<TGT>` token to indicate the target event allows the model to focus inference generation on that event while still considering the full context.
- Mechanism: By explicitly marking which event in a multi-event sentence should be the focus, the model learns to generate inferences specific to that event while grounding them in the overall context.
- Core assumption: The model can learn to distinguish between multiple events in a complex sentence and generate event-specific inferences when given explicit targeting information.

### Mechanism 2
- Claim: Training on human-written multi-event inferences provides better supervision than automatically generated silver-standard labels.
- Mechanism: Human annotators can write inferences that are contextually appropriate and specific to target events, while automatic methods may produce inferences that are generic or inconsistent with the full context.
- Core assumption: Human-written inferences capture the nuanced understanding needed to generate contextually appropriate event-specific inferences.

### Mechanism 3
- Claim: Filtering automatically generated inferences using context consistency improves their quality as training data.
- Mechanism: By selecting only those inferences that are semantically consistent with the full context (either through cosine similarity or NLI-based entailment), the training data becomes more representative of the desired output.
- Core assumption: Context consistency is a good proxy for inference quality when generating multi-event inferences.

## Foundational Learning

- Concept: Event-Centric Commonsense Reasoning
  - Why needed here: The task requires understanding not just what events occur, but the causal relationships, effects, and mental states associated with them in complex contexts.
  - Quick check question: Can you explain the difference between generating inferences for "John insulted Mary" versus "Mary didn't reply when he called her" in the sentence "John insulted Mary, so she didn't reply when he called her"?

- Concept: Context-Aware Language Modeling
  - Why needed here: The model must understand how different events in a sentence relate to each other and generate inferences that are consistent with the overall narrative.
  - Quick check question: How would you modify a language model to pay attention to specific parts of input while still considering the entire context?

- Concept: Data Curation for Complex Reasoning Tasks
  - Why needed here: Creating high-quality training data for multi-event reasoning requires careful annotation strategies to capture the nuances of event relationships.
  - Quick check question: What challenges might arise when asking humans to write inferences for events in complex sentences versus simple sentences?

## Architecture Onboarding

- Component map: Input layer with `<TGT>` tokens -> BART encoder -> Relation tokens (causes, effects, mental states) -> Decoder -> Generated inferences
- Critical path: Preprocess input with `<TGT>` tokens -> Encode context using BART transformer -> Apply relation-specific conditioning -> Generate inferences using beam search -> Filter and evaluate generated inferences
- Design tradeoffs: Using `<TGT>` tokens vs. positional encoding for event identification, human-written vs. automatically generated training data, fine-tuning vs. prompt engineering for adapting COMET to multi-event scenarios
- Failure signatures: Conflated inferences mixing multiple events (similar to baseline COMET), inferences that ignore the context or target event specification, repetitive or generic inferences that lack specificity
- First 3 experiments: 1) Compare inference generation quality with and without `<TGT>` tokens on a small multi-event dataset, 2) Evaluate the impact of different filtering strategies (cosine similarity vs. NLI-based) on automatically generated training data, 3) Test model performance on held-out complex sentences from dialogue datasets to assess generalization

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, the authors acknowledge limitations including the need for more training data, the challenge of handling implicit events, and the potential for extending the approach to multi-sentence contexts.

## Limitations

- The model was trained on 35K inferences, and the authors note that 27% of generated inferences were not specific enough to the target event, suggesting potential data quantity or quality limitations
- The current approach only handles complex sentences with multiple events, not multi-sentence contexts like paragraphs or stories
- The model's performance on implicit events (events not explicitly mentioned in text) is not evaluated

## Confidence

**COMET-M improves multi-event inference generation** - High
**Special `<TGT>` token is critical for performance** - Medium
**Human-written data is superior to automatically generated labels** - Medium

## Next Checks

1. **Ablation study on `<TGT>` token usage**
   - Train a variant of COMET-M without the special token
   - Compare performance on a held-out test set
   - Analyze inference generation patterns to identify if the model conflates events without explicit targeting

2. **Out-of-distribution generalization test**
   - Evaluate COMET-M on complex sentences from different domains (e.g., scientific literature, technical documentation)
   - Compare performance drop relative to in-distribution evaluation
   - Analyze failure modes to identify if the model overfits to specific syntactic patterns

3. **Human evaluation refinement**
   - Conduct a more detailed human evaluation with finer-grained scales (7-point or continuous)
   - Include domain experts in the evaluation process
   - Perform detailed error analysis categorizing types of inference failures (context-ignoring, event-conflation, etc.)