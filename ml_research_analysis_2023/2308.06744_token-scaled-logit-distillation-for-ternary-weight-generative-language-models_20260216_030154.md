---
ver: rpa2
title: Token-Scaled Logit Distillation for Ternary Weight Generative Language Models
arxiv_id: '2308.06744'
source_url: https://arxiv.org/abs/2308.06744
tags:
- logit
- quantization
- language
- distillation
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces token-scaled logit distillation (TSLD), a
  novel knowledge distillation technique for generative language models (GLMs) undergoing
  ternary weight quantization. TSLD addresses the challenge of cumulative quantization
  errors in decoder models by reformulating the distillation objective to focus on
  token-level logit distributions.
---

# Token-Scaled Logit Distillation for Ternary Weight Generative Language Models

## Quick Facts
- **arXiv ID**: 2308.06744
- **Source URL**: https://arxiv.org/abs/2308.06744
- **Reference count**: 40
- **Primary result**: Achieves <1.0 perplexity degradation in ternary weight GLMs while preserving or improving accuracy on commonsense QA and arithmetic reasoning tasks

## Executive Summary
This paper introduces token-scaled logit distillation (TSLD), a novel knowledge distillation technique for generative language models undergoing ternary weight quantization. TSLD addresses the challenge of cumulative quantization errors in decoder models by reformulating the distillation objective to focus on token-level logit distributions. Additionally, it mitigates overfitting risks by adaptively scaling the logit distillation loss based on token confidence. Experiments across multiple GLMs (0.1B–6.7B parameters) demonstrate that TSLD achieves less than 1.0 perplexity degradation in language modeling and preserves or improves accuracy on commonsense QA and arithmetic reasoning tasks, outperforming existing PTQ and QAT methods. TSLD enables highly efficient ultra-low-precision GLM deployment without sacrificing accuracy.

## Method Summary
TSLD reformulates the distillation objective for ternary weight GLMs by focusing on token-level logit distributions rather than intermediate activations. The method computes a confidence-based scaling factor from the teacher model's cross-entropy loss for each token, downweighting high-confidence predictions that could cause student overfitting. During QAT, TSLD combines ground-truth loss with token-scaled logit distillation loss, where the scaling factor is inversely proportional to token confidence. The approach is implemented using custom CUDA kernels for efficient 2-/4-/8-bit matrix multiplication, and is evaluated across multiple GLM architectures (GPT-2, OPT, GPT-Neo, LLaMA) ranging from 0.1B to 6.7B parameters.

## Key Results
- Achieves <1.0 perplexity degradation on language modeling tasks across all tested model sizes
- Preserves or improves accuracy on commonsense QA (PIQA, OpenbookQA, ARC) and arithmetic reasoning (GSM8K) tasks
- Outperforms existing PTQ methods (OPTQ, AWQ) and QAT-KD baselines (Logit, Logit+GT, L2L+Logit) in both perplexity and task accuracy metrics
- Enables efficient deployment of ultra-low-precision GLMs without accuracy sacrifice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-scaled logit distillation prevents overfitting by reducing the contribution of high-confidence teacher predictions to the student loss.
- Mechanism: For each token, the method computes a confidence-based scaling factor from the teacher model's cross-entropy loss. High-confidence tokens (low entropy, low loss) get low scaling, while low-confidence tokens (high entropy, high loss) get high scaling.
- Core assumption: High-confidence teacher predictions can cause the student to memorize rather than learn generalization; downweighting these tokens during distillation reduces this risk.
- Evidence anchors:
  - [abstract] "adaptively scaling the logit distillation loss based on token confidence"
  - [section] "This approach utilizes the phenomenon of confidence disparity in token predictions from the teacher model. Our method, called TSLD, de-emphasizes Logit KD for high-confidence tokens to prevent overfitting"
  - [corpus] Weak evidence: no direct comparison to alternative confidence-scaling methods.
- Break condition: If the confidence disparity is not present or the relationship between confidence and overfitting is different, scaling may not help.

### Mechanism 2
- Claim: Logit distillation better recovers token-level prediction accuracy than layer-to-layer distillation in decoder models due to the cumulative nature of quantization error in masked self-attention.
- Mechanism: In decoder models, quantization errors accumulate along the sequence due to causal masking. Logit distillation directly aligns final token logits, which already incorporate all previous errors, rather than intermediate activations that may be mismatched.
- Core assumption: The final logit distribution is more informative for recovery than intermediate representations because it reflects the full error accumulation.
- Evidence anchors:
  - [abstract] "reformulating the distillation objective to focus on token-level logit distributions"
  - [section] "Logit KD, aligning with the characteristics of the decoder model, emerges as a natural choice for QAT"
  - [corpus] Weak evidence: no ablation comparing logit distillation with intermediate-only distillation.
- Break condition: If quantization error distribution is uniform or non-cumulative, layer-to-layer distillation may be equally or more effective.

### Mechanism 3
- Claim: Including ground-truth loss with logit distillation is necessary to avoid performance degradation in decoder QAT.
- Mechanism: Ground-truth loss provides direct supervision on correct token prediction, while logit distillation provides soft-label guidance. The combination ensures both hard alignment and smooth teacher signal.
- Core assumption: Decoder models in generation tasks rely heavily on correct next-token prediction, which requires explicit ground-truth signals in addition to teacher logits.
- Evidence anchors:
  - [abstract] "preserves or improves accuracy on commonsense QA and arithmetic reasoning tasks"
  - [section] "Necessity of Ground Truth Loss... decoder models derive it from the output logits of all input tokens, which could provide more detailed token-level prediction information"
  - [corpus] Weak evidence: no quantitative comparison of performance with and without ground-truth loss in isolation.
- Break condition: If the teacher model is near-perfect, or if the task does not require exact token-level alignment, ground-truth loss may be redundant.

## Foundational Learning

- Concept: Knowledge Distillation (KD)
  - Why needed here: KD transfers knowledge from a high-precision teacher to a low-precision student, compensating for accuracy loss in quantization.
  - Quick check question: In KD, what is the difference between feature distillation and logit distillation?

- Concept: Quantization-Aware Training (QAT)
  - Why needed here: QAT simulates quantization during training so the model learns to operate under low-precision constraints without catastrophic accuracy loss.
  - Quick check question: What is the key difference between QAT and Post-Training Quantization (PTQ)?

- Concept: Masked Self-Attention in Decoder Models
  - Why needed here: Understanding how causal masking causes cumulative quantization errors along the token sequence is critical for designing effective distillation.
  - Quick check question: In masked self-attention, how does the attention score for token i depend on tokens j < i?

## Architecture Onboarding

- Component map: Pre-trained decoder model (GPT-2/OPT) -> Fine-tuned teacher model -> Quantized student model
- Critical path: Initialize student from teacher -> Apply ternary/4-bit quantization -> Train with TSLD loss -> Evaluate perplexity and downstream task accuracy
- Design tradeoffs:
  - Memory vs. accuracy: L2L distillation requires storing intermediate activations (OOM for large models), while TSLD only needs final logits
  - Precision vs. hardware efficiency: Ternary weights reduce memory but increase quantization error; TSLD mitigates this with distillation
  - Ground-truth vs. teacher guidance: Including both prevents underfitting but risks overfitting if confidence scaling is too aggressive
- Failure signatures:
  - Perplexity degradation >1.0: Indicates insufficient distillation or improper scaling
  - OOM errors during training: Suggests need for smaller sequence length or gradient checkpointing
  - Accuracy drop on reasoning tasks: May indicate over-regularization or missing ground-truth supervision
- First 3 experiments:
  1. Reproduce baseline QAT without distillation on a small GPT-2 model and measure perplexity degradation.
  2. Implement TSLD with a fixed uniform scaling (no confidence-based adjustment) and compare to logit distillation.
  3. Apply TSLD to a 2-bit quantized OPT-1.3B and evaluate on PIQA commonsense QA task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the TSLD method perform when applied to other model architectures beyond GPT-2 and OPT, such as BERT or RoBERTa?
- Basis in paper: [explicit] The paper states that the method is specifically designed for GLMs, but does not explore its effectiveness on other model architectures.
- Why unresolved: The paper focuses solely on evaluating TSLD on GPT-2 and OPT models, leaving the question of its performance on other architectures unanswered.
- What evidence would resolve it: Conducting experiments to apply TSLD to other model architectures and comparing their performance to the original models.

### Open Question 2
- Question: Can the TSLD method be extended to handle other quantization techniques beyond ternary weight quantization, such as binary or 1-bit quantization?
- Basis in paper: [explicit] The paper mentions that TSLD is specifically designed for ternary weight quantization, but does not explore its effectiveness on other quantization techniques.
- Why unresolved: The paper focuses solely on evaluating TSLD on ternary weight quantization, leaving the question of its performance on other quantization techniques unanswered.
- What evidence would resolve it: Conducting experiments to apply TSLD to other quantization techniques and comparing their performance to the original models.

### Open Question 3
- Question: How does the TSLD method perform on larger models, such as those with 10 billion or more parameters?
- Basis in paper: [explicit] The paper evaluates TSLD on models with up to 6.7 billion parameters, but does not explore its effectiveness on larger models.
- Why unresolved: The paper focuses solely on evaluating TSLD on models with up to 6.7 billion parameters, leaving the question of its performance on larger models unanswered.
- What evidence would resolve it: Conducting experiments to apply TSLD to larger models and comparing their performance to the original models.

## Limitations
- The confidence-scaling mechanism lacks quantitative validation through ablation studies comparing it against alternative scaling strategies
- The claim about cumulative quantization error making logit distillation superior is asserted but not experimentally validated through controlled comparison with intermediate-layer distillation
- The necessity of ground-truth loss is stated but not isolated through experiments showing its marginal contribution when combined with teacher guidance

## Confidence
- **High confidence**: The core empirical observation that TSLD achieves <1.0 perplexity degradation on language modeling tasks is well-supported by multiple model sizes and datasets
- **Medium confidence**: The mechanism explanation for why token confidence scaling prevents overfitting is plausible but not rigorously tested
- **Low confidence**: The assertion that logit distillation is inherently superior to layer-to-layer distillation specifically because of cumulative quantization error in decoder models

## Next Checks
1. **Ablation of scaling mechanism**: Implement TSLD with three variants—uniform scaling (no confidence adjustment), inverse scaling (high-confidence tokens get higher weight), and TSLD's confidence-based scaling. Compare performance to isolate the contribution of the confidence mechanism.

2. **Intermediate distillation comparison**: Create a variant that distills intermediate activations (layer-to-layer) alongside or instead of final logits. Evaluate whether cumulative error accumulation truly makes logit distillation superior in decoder architectures.

3. **Ground-truth loss isolation**: Train models with only ground-truth loss, only logit distillation, and their combination. Measure the marginal benefit of including ground-truth supervision in the presence of teacher guidance to validate its necessity.