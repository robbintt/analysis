---
ver: rpa2
title: Learning to Reason and Memorize with Self-Notes
arxiv_id: '2305.00833'
source_url: https://arxiv.org/abs/2305.00833
tags:
- self-notes
- print
- context
- reasoning
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Notes, a method for augmenting language
  models with the ability to explicitly reason and memorize by generating supplementary
  tokens interleaved within the input context. Unlike prior approaches that postpone
  reasoning until after full context processing, Self-Notes allows models to deviate
  from the input stream at any time to generate reasoning steps or memory updates.
---

# Learning to Reason and Memorize with Self-Notes

## Quick Facts
- arXiv ID: 2305.00833
- Source URL: https://arxiv.org/abs/2305.00833
- Authors: 
- Reference count: 11
- Key outcome: Self-Notes method shows consistent improvements over vanilla and scratchpad baselines on five reasoning and state-tracking tasks, particularly for out-of-distribution sequence lengths.

## Executive Summary
Self-Notes introduces a method for augmenting language models with explicit reasoning and memory capabilities by generating supplementary tokens interleaved within the input context. Unlike prior approaches that postpone reasoning until after full context processing, Self-Notes allows models to deviate from the input stream at any time to generate reasoning steps or memory updates. The method is evaluated on five reasoning and state-tracking tasks, showing consistent improvements over vanilla and scratchpad baselines, particularly for out-of-distribution sequence lengths. The approach can be trained in supervised, semi-supervised, and unsupervised settings, and results suggest that the content of intermediate notes is more important than merely increasing compute.

## Method Summary
Self-Notes fine-tunes transformer-based language models to generate reasoning tokens (Self-Notes) interleaved with the original input context during processing. The model learns to generate start tokens that signal the beginning of reasoning steps, followed by reasoning tokens until an end token is generated. These Self-Notes can serve as intermediate reasoning steps or memory updates, allowing the model to perform reasoning "on the fly" rather than postponing it until after context processing. The method is trained using supervised learning with ground truth Self-Notes, semi-supervised learning with a subset of supervised data, or unsupervised learning by generating QA pairs as Self-Notes. During inference, the model generates Self-Notes before answering questions, with the generated notes enriching the context for final reasoning.

## Key Results
- Self-Notes consistently outperforms vanilla GPT-2 and scratchpad baselines across all five evaluated tasks
- The method shows significant improvements on out-of-distribution sequence lengths, demonstrating length generalization capabilities
- Unsupervised Self-Notes training achieves substantial perplexity reductions on WikiText-103 compared to vanilla GPT-2
- Semi-supervised training with only 25% of Self-Notes supervision still achieves strong performance on most tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-Notes allows the model to perform reasoning "on the fly" as it processes input tokens, rather than postponing reasoning until after the full context is read.
- Mechanism: The model generates reasoning tokens (Self-Notes) interleaved with the original input context during processing. These tokens can be used for intermediate reasoning steps and memory updates.
- Core assumption: The model can learn to generate useful reasoning steps that will be relevant for future reasoning.
- Evidence anchors:
  - [abstract] "Unlike recent chain-of-thought or scratchpad approaches, the model can deviate from the input context at any time to explicitly think and write down its thoughts."
  - [section 2] "Unlike a scratchpad, the model can interleave generated tokens with the input context as demonstrated in Fig. 1 (bottom)."
  - [corpus] Weak evidence - the corpus papers focus on different mechanisms (CoT, explicit reasoning) but don't directly validate the interleaving claim.
- Break condition: If the model generates irrelevant or incorrect Self-Notes that don't help with the final answer.

### Mechanism 2
- Claim: Self-Notes acts as working memory by allowing the model to write the latest state of an entity as new tokens while traversing the context.
- Mechanism: The model can update its memory about variables/entities by writing new values as Self-Notes, replacing older information that may be out of context window.
- Core assumption: The model can track state changes and write useful updates that replace older state information.
- Evidence anchors:
  - [abstract] "This allows the model to perform reasoning on the fly as it reads the context and even integrate previous reasoning steps, thus enhancing its memory with useful information and enabling multi-step reasoning."
  - [section 2] "For example, in a programming environment, assume x=5 initially, and then x gets incremented by 1. Assuming the model correctly writes x=6 as a Self-Note, it can safely remove the original x=5 statement from its context."
  - [corpus] Weak evidence - the corpus focuses on different memory mechanisms but doesn't validate this specific working memory approach.
- Break condition: If the model fails to track state changes accurately or writes Self-Notes that don't reflect the true state.

### Mechanism 3
- Claim: Self-Notes enables length generalization by allowing the model to generate intermediate reasoning steps that break down complex problems into simpler subproblems.
- Mechanism: The model can use Self-Notes to transform longer/harder problems into sequences of shorter problems it has seen during training.
- Core assumption: The model can learn to decompose complex reasoning into simpler steps that it can solve.
- Evidence anchors:
  - [abstract] "Our experiments on multiple tasks demonstrate that our method can successfully generalize to longer and more complicated instances from their training setup by taking Self-Notes at inference time."
  - [section 4.1] "We observe that the Vanilla GPT-2 model struggles to track the state of the variables over many statements, and significantly worsens for OOD sequence lengths. Self-Notes, which allows the model to generate intermediate print statements, achieves high accuracy on both the in-distribution and OOD statement splits."
  - [corpus] Weak evidence - the corpus papers focus on length generalization but don't specifically validate this decomposition mechanism.
- Break condition: If the model fails to decompose complex problems or the intermediate steps don't help with solving the final problem.

## Foundational Learning

- Concept: Autoregressive language modeling
  - Why needed here: The model predicts the next token in a sequence based on previous tokens, which is the foundation for generating Self-Notes.
  - Quick check question: What is the core task that autoregressive language models are trained to do?

- Concept: Transformer architecture
  - Why needed here: Self-Notes is applied to transformer-based language models, which have specific limitations (fixed computation per token, lack of memory) that Self-Notes addresses.
  - Quick check question: What are the key limitations of transformers that make them struggle with multi-step reasoning and state-tracking?

- Concept: Chain-of-thought and scratchpad methods
  - Why needed here: Self-Notes is compared against and builds upon these existing methods, so understanding their limitations is crucial.
  - Quick check question: How do chain-of-thought and scratchpad methods differ from Self-Notes in terms of when reasoning is performed?

## Architecture Onboarding

- Component map: Input context -> Self-Notes generation -> Answer generation
- Critical path:
  1. Feed input context token by token to model
  2. Model decides whether to generate start token for Self-Note
  3. If start token generated, model generates reasoning tokens until end token
  4. Generated Self-Notes are interleaved with original context
  5. Model continues processing remaining context tokens
  6. Model generates final answer based on enriched context

- Design tradeoffs:
  - Token budget: More Self-Notes means less room for original context
  - Supervision level: More supervised data helps but may be expensive to obtain
  - Start/end token design: Affects how easily model learns to generate Self-Notes
  - Context window: Limited by model architecture, affects how much memory Self-Notes can provide

- Failure signatures:
  - Model generates irrelevant Self-Notes that don't help with final answer
  - Self-Notes become too verbose and crowd out original context
  - Model fails to generate start tokens even when reasoning would help
  - Self-Notes contain errors that propagate to final answer

- First 3 experiments:
  1. Toy-Story task with 1-hop and 2-hop training, test on 3-hop and 4-hop
  2. Algorithmic task with 2-100 statements training, test on 101-200 statements
  3. Semi-supervised setting with varying percentages of Self-Notes supervision on Toy-Story task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Self-Notes scale with model size (e.g., 1B, 10B, 100B+ parameters) compared to vanilla and scratchpad baselines?
- Basis in paper: [inferred] The paper evaluates Self-Notes on a 124M parameter GPT-2 base model, but mentions that training larger models would require significant resources and is left for future work.
- Why unresolved: The paper does not provide empirical evidence on how Self-Notes performs with larger language models, which are becoming increasingly common.
- What evidence would resolve it: Training and evaluating Self-Notes on models of varying sizes (e.g., 1B, 10B, 100B+) and comparing their performance to vanilla and scratchpad baselines on the same tasks.

### Open Question 2
- Question: How does the choice of Self-Note start and end tokens impact the model's ability to generate effective reasoning steps and memory updates?
- Basis in paper: [explicit] The paper mentions that Self-Note start and end tokens are predefined sets (Nsta and Nend) but does not explore the impact of different token choices on performance.
- Why unresolved: The paper does not investigate whether certain token choices lead to better reasoning or memory retention compared to others.
- What evidence would resolve it: Conducting experiments with different Self-Note token choices and measuring their impact on reasoning accuracy and memory retention across various tasks.

### Open Question 3
- Question: Can Self-Notes be effectively combined with retrieval-based methods to enhance reasoning on knowledge-intensive tasks?
- Basis in paper: [inferred] The paper focuses on reasoning and memory within the model's context but does not explore integrating external knowledge sources.
- Why unresolved: The paper does not investigate whether combining Self-Notes with retrieval mechanisms could improve performance on tasks requiring external knowledge.
- What evidence would resolve it: Implementing a hybrid approach that interleaves Self-Notes with retrieved information and evaluating its performance on knowledge-intensive reasoning tasks compared to Self-Notes alone.

### Open Question 4
- Question: What is the optimal frequency and timing of Self-Note generation during context processing for different types of reasoning tasks?
- Basis in paper: [inferred] The paper allows the model to generate Self-Notes at any point during context processing but does not explore optimal generation strategies.
- Why unresolved: The paper does not investigate whether generating Self-Notes after every statement, only when certain conditions are met, or at other intervals leads to better performance.
- What evidence would resolve it: Experimenting with different Self-Note generation strategies (e.g., fixed intervals, adaptive based on context complexity, only when contradictions are detected) and measuring their impact on reasoning accuracy across various task types.

### Open Question 5
- Question: How does the length of Self-Notes (number of tokens) affect the model's reasoning performance and computational efficiency?
- Basis in paper: [inferred] The paper allows Self-Notes to be of fixed length but does not explore the impact of varying note lengths on performance.
- Why unresolved: The paper does not investigate whether longer, more detailed Self-Notes improve reasoning accuracy or if shorter notes are more efficient while maintaining performance.
- What evidence would resolve it: Conducting experiments with varying Self-Note lengths and measuring their impact on reasoning accuracy, computational cost, and memory usage across different task complexities.

## Limitations

- The method's effectiveness depends heavily on the quality of Self-Notes generated during training, and imperfect supervision may lead to suboptimal reasoning patterns
- Computational overhead of generating Self-Notes is not fully characterized, particularly for longer sequences where interleaved tokens could consume significant context window space
- Experiments focus primarily on synthetic tasks with controlled complexity, and real-world applications with noisier, more diverse contexts may present different challenges for the interleaving mechanism

## Confidence

**High confidence**: The core architectural innovation of interleaving reasoning tokens with input context is well-specified and the in-distribution results across multiple tasks are robust. The supervised learning results showing improvements over vanilla and scratchpad baselines are convincing, with clear performance gains on both standard and out-of-distribution sequence lengths.

**Medium confidence**: The semi-supervised and unsupervised learning results, while promising, have fewer evaluation details and the unsupervised results rely on perplexity rather than task-specific metrics. The mechanism by which Self-Notes enables length generalization is plausible but not fully validated - the experiments show improved performance but don't definitively prove that the decomposition into simpler subproblems is the primary mechanism.

**Low confidence**: Claims about the specific superiority of content over compute are based on limited comparisons and don't rule out alternative explanations such as different information flow patterns or training dynamics.

## Next Checks

1. **Robustness to noisy Self-Notes**: Generate corrupted or incomplete Self-Notes during training and evaluate how this affects downstream performance. This would test the method's sensitivity to the quality of intermediate reasoning steps and whether it can recover from imperfect supervision.

2. **Context window saturation analysis**: Systematically vary the token budget allocated to Self-Notes versus original context across different sequence lengths, measuring the point at which additional Self-Notes provide diminishing returns or begin to crowd out essential context information.

3. **Transfer to real-world tasks**: Apply Self-Notes to a naturally occurring multi-step reasoning task (such as mathematical word problems or logical inference from text) where ground truth reasoning steps are not available, and evaluate both the quality of generated Self-Notes and their impact on final answer accuracy.