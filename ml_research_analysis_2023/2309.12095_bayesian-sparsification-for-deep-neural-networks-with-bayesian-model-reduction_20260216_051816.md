---
ver: rpa2
title: Bayesian sparsification for deep neural networks with Bayesian model reduction
arxiv_id: '2309.12095'
source_url: https://arxiv.org/abs/2309.12095
tags:
- bayesian
- learning
- deep
- inference
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Bayesian model reduction (BMR) as a computationally
  efficient approach for sparsifying deep neural networks, particularly when combined
  with spike-and-slab priors. The authors demonstrate that BMR-based sparsification
  achieves competitive performance with state-of-the-art hierarchical horseshoe prior
  methods, while offering superior computational efficiency and sparsity rates.
---

# Bayesian sparsification for deep neural networks with Bayesian model reduction

## Quick Facts
- arXiv ID: 2309.12095
- Source URL: https://arxiv.org/abs/2309.12095
- Reference count: 16
- Key outcome: BMR-based sparsification achieves 90% pruning rates while maintaining competitive accuracy on image classification tasks

## Executive Summary
This paper presents Bayesian model reduction (BMR) as an efficient approach for sparsifying deep neural networks, particularly when combined with spike-and-slab priors. The authors demonstrate that BMR achieves competitive performance with state-of-the-art hierarchical horseshoe prior methods while offering superior computational efficiency and significantly higher sparsity rates. The approach is validated across multiple architectures including MLP, LeNet-5, Vision Transformers, and MLP-Mixers on image classification tasks (FashionMNIST, CIFAR10, CIFAR100), showing comparable top-1 accuracy (0.80-0.90) but lower expected calibration error and negative log-likelihood compared to alternative methods.

## Method Summary
The method employs Bayesian model reduction to perform post-hoc pruning of neural network weights based on posterior estimates from a simpler non-hierarchical generative model. The approach uses stochastic variational inference with a fully factorized mean-field approximation to obtain posterior estimates, then applies BMR to compute the change in variational free energy when switching from a flat prior to a spike-and-slab prior. Weights are pruned iteratively based on their contribution to the free energy, with the pruned model fine-tuned for 5 epochs. The method is tested on MLP, LeNet-5, Vision Transformer, and MLP-Mixer architectures across FashionMNIST, CIFAR10, and CIFAR100 datasets.

## Key Results
- BMR achieves up to 90% parameter pruning while maintaining competitive top-1 accuracy (0.80-0.90)
- Lower expected calibration error and negative log-likelihood compared to alternative methods
- Computational efficiency advantage over hierarchical horseshoe prior methods by avoiding expensive full hierarchical inference
- Effective across diverse architectures including convolutional networks and transformers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BMR leverages posterior estimates from a simple non-hierarchical model to efficiently prune weights in a hierarchical spike-and-slab model without expensive full hierarchical inference.
- Mechanism: By using Bayesian model reduction, BMR computes the change in free energy when switching from a flat prior to a spike-and-slab prior, using the posterior of the simpler model. This avoids optimizing over a large set of horseshoe prior hyperparameters.
- Core assumption: The approximate posterior from the flat model (with Gaussian prior) is sufficiently informative to guide weight pruning decisions in the spike-and-slab model.
- Break condition: If the flat model posterior poorly represents the true posterior of the hierarchical model, BMR-based pruning will misidentify important weights, leading to performance degradation.

### Mechanism 2
- Claim: Fully factorized mean-field approximation is sufficient for competitive performance when combined with BMR, avoiding the need for structured variational approximations.
- Mechanism: The mean-field posterior captures the essential uncertainty in weights, and BMR refines this into a sparse structure by evaluating each weight's contribution via free energy change.
- Core assumption: The factorized posterior variance captures meaningful uncertainty that correlates with pruning decisions.
- Break condition: If posterior correlations matter strongly for sparsity structure, mean-field approximation will miss critical pruning signals, reducing accuracy or calibration.

### Mechanism 3
- Claim: BMR-based pruning achieves higher sparsification rates (up to 90%) while maintaining accuracy, due to principled free energy-based weight elimination.
- Mechanism: Each weight's importance is quantified by its contribution to the variational free energy under the spike-and-slab prior; weights with minimal impact are pruned iteratively.
- Break condition: If the free energy metric over-prunes critical weights, accuracy will drop sharply; if it under-prunes, computational savings will be minimal.

## Foundational Learning

- Concept: Variational Inference and Free Energy
  - Why needed here: BMR relies on variational free energy as the key quantity for model comparison and pruning decisions.
  - Quick check question: What is the relationship between free energy and marginal likelihood in variational inference?

- Concept: Spike-and-Slab Priors
  - Why needed here: BMR is used here specifically with spike-and-slab priors to induce sparsity.
  - Quick check question: How does a spike-and-slab prior differ from a horseshoe prior in terms of parameter space and sparsity behavior?

- Concept: Reparameterization of Heavy-Tailed Priors
  - Why needed here: Horseshoe priors use reparameterization (Gamma quotients) to enable variational inference.
  - Quick check question: Why can't we directly use half-Cauchy priors in variational inference, and how does the Gamma quotient trick help?

## Architecture Onboarding

- Component map:
  - Data pipeline: Load FashionMNIST/CIFAR10/CIFAR100 → batch into NB=128
  - Model: MLP/LeNet/ViT/MLP-Mixer with specified layer sizes
  - Inference engine: Stochastic black-box VI for flat model → BMR post-hoc pruning
  - Optimization: AdaBelief optimizer with default hyperparams
  - Pruning loop: Iterate SVI → BMR pruning → fine-tune for 5 epochs

- Critical path:
  1. Train flat model with mean-field posterior
  2. Compute BMR-based free energy change for each weight
  3. Prune weights below threshold
  4. Fine-tune pruned model
  5. Evaluate ACC/ECE/NLL

- Design tradeoffs:
  - Mean-field vs structured posterior: Simpler/faster but potentially less accurate pruning
  - Spike-and-slab vs horseshoe: Higher sparsification vs easier inference
  - Pruning threshold: Higher sparsity vs higher risk of accuracy loss

- Failure signatures:
  - Sudden drop in ACC after pruning → too aggressive pruning
  - Slow convergence in SVI → learning rate too low or architecture too deep
  - High ECE but good ACC → miscalibration, likely under-pruning

- First 3 experiments:
  1. Train flat MLP on FashionMNIST, apply BMR pruning, compare sparsity vs accuracy vs Tiered-FF.
  2. Vary pruning threshold to find sweet spot between sparsity and accuracy.
  3. Swap spike-and-slab for horseshoe prior, compare performance and computational cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational efficiency of BMR-based sparsification scale with increasing model size and depth compared to hierarchical horseshoe prior methods?
- Basis in paper: The paper states that BMR "circumvents the need for expensive hierarchical model inversions" and is "more scalable than existing structured variational approximations," but does not provide quantitative comparisons of computational scaling.
- Why unresolved: While the paper demonstrates BMR's efficiency relative to horseshoe priors on specific architectures, it lacks systematic analysis of how this efficiency advantage changes as models become larger and deeper.
- What evidence would resolve it: Controlled experiments measuring wall-clock time and memory usage for BMR vs. horseshoe methods across models of increasing size and depth, plotted on log-log scales to reveal scaling relationships.

### Open Question 2
- Question: What is the theoretical relationship between the convergence rate of the iterative optimization and pruning approach in BMR and the sparsification rate achieved?
- Basis in paper: The paper mentions that the iterative optimization and pruning approach "achieves convergence after a few iterations" (capped at 4) but doesn't explain why this specific number works or how it relates to optimal sparsification.
- Why unresolved: The paper presents kmax=4 as an empirical choice without theoretical justification or analysis of how different iteration limits affect the final sparsification quality.
- What evidence would resolve it: Theoretical analysis linking the number of optimization-pruning iterations to final sparsity levels, or empirical studies varying kmax and measuring its effect on both convergence speed and final model performance.

### Open Question 3
- Question: How does BMR-based sparsification perform on tasks beyond image classification, particularly in domains with different data characteristics like time series or natural language processing?
- Basis in paper: The discussion section explicitly states "we argue its computational efficiency, and remarkable sparsification rate, position BMR as an appealing choice, enhancing the scalability and proficiency of contemporary deep learning networks across diverse machine learning challenges, extending well beyond computer vision."
- Why unresolved: The paper only validates BMR on image classification tasks despite claiming broader applicability, leaving its performance on other data types unexplored.
- What evidence would resolve it: Experiments applying BMR to at least two non-image domains (e.g., time series forecasting and text classification) with quantitative comparisons to existing sparsification methods in those domains.

## Limitations
- Mean-field approximation may miss posterior correlations important for pruning decisions
- Computational savings, while significant, still require multiple pruning iterations
- Experimental validation limited to image classification tasks only

## Confidence
- BMR computational efficiency claim: High (supported by clear mechanism and empirical timing data)
- Sparsity-accuracy tradeoff validity: Medium (competitive results but limited architectural diversity)
- Generalizability beyond image tasks: Low (no validation on non-vision domains)

## Next Checks
1. Test BMR with structured posterior approximations to quantify the cost of capturing posterior correlations
2. Evaluate performance on recurrent or transformer-based architectures for NLP tasks
3. Measure wall-clock time and memory usage across different model scales to verify claimed efficiency gains