---
ver: rpa2
title: Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language
  Models
arxiv_id: '2312.01714'
source_url: https://arxiv.org/abs/2312.01714
tags:
- examples
- demonstration
- reasoning
- question
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a retrieval-augmented multi-modal Chain-of-Thought
  (CoT) approach to enhance the reasoning capabilities of large language models (LLMs)
  for complex multi-modal tasks. The key idea is to dynamically and automatically
  select demonstration examples based on cross-modal and intra-modal similarities
  using retrieval mechanisms.
---

# Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models

## Quick Facts
- arXiv ID: 2312.01714
- Source URL: https://arxiv.org/abs/2312.01714
- Reference count: 15
- Key outcome: Proposed retrieval-augmented multi-modal CoT approach achieves 6.05% and 4.57% performance gains over state-of-the-art Chameleon on ScienceQA and MathVista datasets, respectively.

## Executive Summary
This paper introduces a retrieval-augmented multi-modal Chain-of-Thought (CoT) approach to enhance the reasoning capabilities of large language models (LLMs) for complex multi-modal tasks. The method dynamically selects demonstration examples based on cross-modal and intra-modal similarities using retrieval mechanisms, combined with stratified sampling to promote diversity in the demonstration set. Experiments on ScienceQA and MathVista benchmark datasets demonstrate significant improvements in performance, with GPT-4-based models achieving 6.05% and 4.57% increases over the state-of-the-art Chameleon approach, respectively. The proposed method also outperforms GPT-4V's zero-shot capabilities by 2.7% on average.

## Method Summary
The approach uses a retrieval mechanism that dynamically selects demonstration examples based on cross-modal and intra-modal similarities. It employs stratified sampling to categorize demonstrations into groups (text-only and text+image) and retrieves examples from different groups to ensure diversity. The retrieved demonstrations are combined with the test question and passed to an LLM for final answer prediction. The method is evaluated on the ScienceQA dataset, using its training set as the demonstration pool and measuring accuracy as the primary metric.

## Key Results
- GPT-4-based models achieve 6.05% and 4.57% performance gains over state-of-the-art Chameleon on ScienceQA and MathVista datasets, respectively.
- The proposed method outperforms GPT-4V's zero-shot capabilities by 2.7% on average.
- Stratified sampling outperforms random sampling across all ScienceQA categories.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal similarity retrieval improves demonstration relevance for multi-modal CoT.
- Mechanism: By encoding text and image parts separately and computing cosine similarities in embedding space, the method retrieves examples whose visual or textual content matches the query's context, providing more aligned reasoning steps.
- Core assumption: Cross-modal embeddings from CLIP preserve semantic similarity across text and image modalities.
- Evidence anchors:
  - [abstract] "dynamically and automatically select demonstration examples based on cross-modal and intra-modal similarities"
  - [section 3.1] Equation 2 shows combining intra-modal and cross-modal retrievals to form the final demonstration set.
  - [corpus] Retrieval methods in recent CoT studies (e.g., "Zero-Shot Verification-guided Chain of Thoughts") use similarity-based example selection.
- Break condition: If cross-modal embeddings fail to align semantically, retrieved examples may be irrelevant, harming reasoning quality.

### Mechanism 2
- Claim: Stratified sampling ensures diversity of demonstration examples.
- Mechanism: The demonstration pool is split into Qtxt (text-only) and Qimg (text+image) groups; examples are sampled from each group according to the query's modality, preventing over-representation of a single example type.
- Core assumption: Diversity across modalities correlates with improved robustness in multi-modal reasoning.
- Evidence anchors:
  - [abstract] "Stratified Sampling method of categorising demonstration examples into groups based on their types and then retrieving examples from different groups"
  - [section 3.2] Equation 3 shows the stratified retrieval formulation.
  - [section 4.3] Table 3 shows Stratified Sampling outperforms Random Sampling across all ScienceQA categories.
- Break condition: If the groups are not well-defined or balanced, stratification may not improve diversity.

### Mechanism 3
- Claim: Combining multiple retrieval strategies (T2T, T2I, I2T, I2I) captures complementary information.
- Mechanism: Different similarity metrics (text-text, text-image, image-text, image-image) are used to retrieve examples; combining them yields a richer demonstration set than any single method alone.
- Core assumption: Each retrieval method captures distinct aspects of relevance, and their combination is additive.
- Evidence anchors:
  - [section 4.3] Figure 4 shows ablation of retrieval methods with varying numbers of shots.
  - [section 4.3] "We can observe from Figure 2: ... Image-to-Image Retrieval ... accuracy improves significantly with increasing shots"
  - [corpus] Recent visual in-context learning studies (e.g., "What makes good examples for visual in-context learning?") explore multiple retrieval signals.
- Break condition: If retrieval methods are redundant or noisy, their combination may degrade performance.

## Foundational Learning

- Concept: In-context learning (ICL) with demonstration examples.
  - Why needed here: CoT relies on providing LLMs with reasoning traces from examples to guide their own reasoning process.
  - Quick check question: What is the role of the demonstration pool Q in the retrieval mechanism?

- Concept: Cross-modal embeddings (e.g., CLIP).
  - Why needed here: To compute similarities between text and image parts of queries and demonstrations.
  - Quick check question: Which encoder is used for cross-modal similarity retrieval?

- Concept: Stratified sampling in statistical learning.
  - Why needed here: To ensure representation from different example categories when forming the demonstration set.
  - Quick check question: How does stratified sampling differ from random sampling in this context?

## Architecture Onboarding

- Component map:
  Demonstration pool (Q) -> Text encoder (TEXT-ENCODER) -> Image encoder (VISUAL-ENCODER) -> Cross-modal encoder (CLIP) -> Retrieval module -> Stratified sampler -> Prompt constructor -> LLM inference

- Critical path:
  1. Encode query q into text and image embeddings.
  2. Retrieve demos via intra-modal and cross-modal similarities.
  3. Apply stratified sampling to form diverse demo set D.
  4. Construct prompt (D ⊕ q).
  5. Run LLM to produce answer.

- Design tradeoffs:
  - Retrieval granularity vs. computation: finer-grained similarity can improve relevance but increases cost.
  - Number of shots vs. prompt length: more demos improve coverage but may hit token limits.
  - Group balance in stratification vs. simplicity: more groups improve diversity but complicate sampling.

- Failure signatures:
  - Poor similarity metrics → irrelevant demos.
  - Over-stratification → too few examples per group.
  - Prompt overflow → truncated reasoning.

- First 3 experiments:
  1. Run with only intra-modal retrieval (T2T, I2I) to establish baseline relevance.
  2. Add cross-modal retrieval (T2I, I2T) and compare performance on image-only and text-only subsets.
  3. Vary number of shots (1–4) and measure impact on accuracy per question type.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of demonstration examples to use for retrieval-augmented CoT in multi-modal tasks?
- Basis in paper: [explicit] The paper states "We conduct comprehensive experiments to study the effect of varying k1, k2, k3, k4, results shown in in Figure 4."
- Why unresolved: The paper presents results for varying numbers of demonstrations (1-4 shots) but does not definitively conclude what the optimal number is, as performance varies across different retrieval methods and question types.
- What evidence would resolve it: A systematic comparison across all retrieval methods showing consistent performance gains or plateaus at a specific number of demonstrations would resolve this question.

### Open Question 2
- Question: How does the retrieval mechanism perform on other multi-modal tasks beyond question answering?
- Basis in paper: [inferred] The paper focuses on ScienceQA dataset and mentions "Future research should focus on refining the retrieval processes and extending the methodologies developed in this study to additional multi-modal tasks."
- Why unresolved: The current evaluation is limited to the ScienceQA dataset, and the paper does not provide results or insights on how the retrieval mechanism would perform on other multi-modal tasks such as text-to-image or text-to-video generation.
- What evidence would resolve it: Experiments applying the retrieval-augmented CoT approach to diverse multi-modal tasks with comprehensive performance comparisons would provide insights into its generalizability.

### Open Question 3
- Question: How does the retrieval mechanism impact the hallucination issue in multi-modal LLMs?
- Basis in paper: [explicit] The paper mentions "addressing the issue of hallucination is important" in the context of developing sophisticated multi-modal LLMs.
- Why unresolved: While the paper acknowledges the importance of addressing hallucination, it does not provide specific analysis or results on how the retrieval mechanism affects the generation of misleading rationales or outputs in multi-modal reasoning tasks.
- What evidence would resolve it: A detailed analysis comparing the hallucination rates in outputs generated with and without the retrieval mechanism, along with qualitative assessments of the generated rationales, would provide insights into its impact on this issue.

## Limitations
- The effectiveness of cross-modal similarity retrieval depends on the quality of CLIP embeddings, which may not reliably align text and image semantics across all domains.
- The stratified sampling approach assumes that grouping demonstrations by modality improves diversity, but its effectiveness may be dataset-specific and unclear for tasks with more complex categorization needs.
- The paper lacks detailed ablation studies isolating the impact of cross-modal retrieval versus stratified sampling, making it difficult to assess their relative contributions to performance gains.

## Confidence
- High Confidence: The overall retrieval-augmented CoT framework and its integration with LLM inference is technically sound. The use of stratified sampling to promote diversity is a well-established statistical technique.
- Medium Confidence: The specific implementation details of the retrieval mechanism (e.g., exact similarity thresholds, group definitions) are not fully specified, which limits reproducibility and introduces uncertainty about the method's robustness.
- Low Confidence: The claim that cross-modal embeddings from CLIP reliably preserve semantic similarity across modalities is critical but not empirically validated in the paper. This is a foundational assumption that, if flawed, would significantly impact the method's effectiveness.

## Next Checks
1. Conduct a quantitative analysis of cross-modal embedding alignment on a held-out subset of the ScienceQA dataset. Measure the correlation between cosine similarity in CLIP space and human-annotated semantic relevance.
2. Perform a detailed ablation study isolating the impact of intra-modal retrieval (T2T, I2I) versus cross-modal retrieval (T2I, I2T) on different question types (text-heavy, image-heavy, balanced). This will clarify which retrieval strategies are most effective.
3. Systematically vary the group definitions and sampling ratios in the stratified sampling method. Assess how these changes affect diversity metrics (e.g., modality balance) and downstream reasoning accuracy to identify optimal configurations.