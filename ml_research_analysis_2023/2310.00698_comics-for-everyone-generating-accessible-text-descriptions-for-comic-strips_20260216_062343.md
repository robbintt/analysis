---
ver: rpa2
title: 'Comics for Everyone: Generating Accessible Text Descriptions for Comic Strips'
arxiv_id: '2310.00698'
source_url: https://arxiv.org/abs/2310.00698
tags:
- comic
- text
- strips
- characters
- panel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using multimodal large language models (MLLMs)
  to generate accessible text descriptions for comic strips to help blind and low
  vision readers. The method first uses computer vision techniques to extract visual
  cues from the comic strip images, including panels, characters, and text.
---

# Comics for Everyone: Generating Accessible Text Descriptions for Comic Strips

## Quick Facts
- arXiv ID: 2310.00698
- Source URL: https://arxiv.org/abs/2310.00698
- Authors: 
- Reference count: 15
- Key outcome: Multimodal large language models generate accessible comic strip descriptions for blind/low vision users using extracted visual cues.

## Executive Summary
This paper introduces a system that uses multimodal large language models (MLLMs) to generate text descriptions of comic strips for visually impaired readers. The approach extracts visual cues like panels, characters, and text using computer vision techniques, then uses this structured context to prompt an MLLM for narrative generation. The method is evaluated on 60 comic strips from Dilbert, Garfield, and Peanuts, showing improved description quality compared to baseline prompts.

## Method Summary
The method uses computer vision to extract panels, characters, and text from comic strip images, then formats this information as structured context for an MLLM prompt. Panel extraction is done with Kumiko, character and text detection with Grounding DINO, character identification with CLIP using hand-crafted prompts, and text extraction with Azure ACS OCR. LLaVA-v1-13B-336px generates descriptions using both base and enhanced prompts with context. Evaluation metrics include detection accuracy and qualitative comparison of description quality.

## Key Results
- Grounding DINO achieved 92.14% mAP for character detection and 95.22% mAP for text detection
- CLIP achieved 0.79 F1 score for character identification
- Enhanced prompts produced more relevant and accurate descriptions compared to baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing structured visual context to the MLLM improves description quality
- Mechanism: Visual cues are formatted as structured context and appended to prompts, allowing the model to bypass difficult OCR tasks
- Core assumption: MLLM can effectively parse and use structured visual metadata for accurate descriptions
- Evidence anchors: Abstract states context is used as additional prompt information; enhanced prompt includes context section
- Break condition: Noisy or misaligned visual cues may cause inconsistent or hallucinated descriptions

### Mechanism 2
- Claim: Grounding DINO's open-set detection reliably extracts comic elements without prior training
- Mechanism: Text labels like "character" and "text" localize elements in comic images for downstream processing
- Core assumption: Text and character labels generalize to unseen comic styles and layouts
- Evidence anchors: Grounding DINO is used as state-of-the-art open-set detector for text and character elements
- Break condition: Stylized fonts or character designs may reduce detection accuracy

### Mechanism 3
- Claim: CLIP's zero-shot classification identifies characters using descriptive prompts
- Mechanism: Character images are matched to descriptive text prompts without labeled training data
- Core assumption: Descriptive prompts are sufficiently discriminative for unique character identification
- Evidence anchors: CLIP is used with hand-crafted prompts based on character physical descriptions
- Break condition: Similar-looking characters may share prompts, leading to misidentification

## Foundational Learning

- Concept: Computer vision pipeline for structured visual cue extraction
  - Why needed here: System depends on accurate extraction of panels, characters, and text for MLLM context
  - Quick check question: What computer vision technique is used to segment comic panels in this work?

- Concept: Multimodal large language model prompt engineering
  - Why needed here: Enhanced prompt combines base instructions with structured context for optimal performance
  - Quick check question: What two components make up the enhanced prompt in this approach?

- Concept: Zero-shot classification with CLIP
  - Why needed here: Character identification performed without labeled data using image-text matching
  - Quick check question: How does CLIP recognize characters without prior training on comic datasets?

## Architecture Onboarding

- Component map: Input comic strip -> Kumiko/OpenCV contours (panel extractor) -> Grounding DINO (object detector) -> CLIP (character classifier) -> Azure ACS OCR (dialogue extractor) -> JSON context formatter -> LLaVA-v1-13B-336px (MLLM) -> Natural language comic description
- Critical path: 1. Extract panels -> 2. Detect characters/text -> 3. Classify characters -> 4. OCR dialogue -> 5. Format context -> 6. Prompt MLLM -> 7. Generate description
- Design tradeoffs: Open-set detectors avoid dataset curation but may sacrifice precision; hand-crafted CLIP prompts are simple but may fail on character similarity; token limits may truncate enhanced context
- Failure signatures: Low text detection precision causes missing dialogue; character misidentification causes wrong speaker attribution; token limit exceeded reverts to baseline performance
- First 3 experiments: 1. Run baseline prompt-only and record output quality 2. Run enhanced prompt with context and compare accuracy 3. Vary Grounding DINO thresholds and measure impact on description quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the accuracy of character-dialogue associations in comic descriptions?
- Basis in paper: [explicit] Not passing character-dialogue association information leads to incorrect outputs as the model has to guess who is speaking
- Why unresolved: Current method relies on MLLM's ability to infer associations from context, which can lead to errors
- What evidence would resolve it: Experiments comparing different methods of incorporating character-dialogue associations and their impact on accuracy

### Open Question 2
- Question: What is the optimal way to handle text extraction from comic strips with stylized fonts?
- Basis in paper: [explicit] OCR quality can be affected by stylized fonts used in comic strips
- Why unresolved: Only Azure ACS OCR is used without exploring other OCR techniques or font-specific approaches
- What evidence would resolve it: Comparative studies of different OCR engines on diverse comic strips with various font styles

### Open Question 3
- Question: How can we mitigate hallucination effects and improve instruction-following capabilities of MLLMs in comic description tasks?
- Basis in paper: [explicit] MLLMs sometimes make up information not present in the comic strip image
- Why unresolved: Paper does not explore techniques to constrain model output or improve adherence to provided context
- What evidence would resolve it: Experiments testing methods like prompt engineering, fine-tuning, or using external knowledge bases

## Limitations
- Evaluation on only 60 comic strips from three popular series limits generalizability to diverse comic styles
- Character identification depends on hand-crafted prompts that may not scale to new comics
- No quantitative evaluation of description quality - only qualitative comparisons provided
- Assumes clean visual cues are extractable for any comic, but stylized layouts may degrade performance

## Confidence
- Medium: Detection pipeline shows reasonable performance on this dataset but may not generalize
- Low: Claims about description quality improvements lack standardized evaluation metrics
- Low: "For everyone" claim is overstated given narrow evaluation scope

## Next Checks
1. Conduct user study with blind and low vision participants to evaluate accessibility and usefulness of generated descriptions
2. Test system on broader and more diverse set of comic styles, genres, and publication eras to assess robustness
3. Implement and report quantitative metrics (BLEU, ROUGE, human judgment scores) for description quality and evaluate hallucination frequency