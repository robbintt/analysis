---
ver: rpa2
title: 'Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive
  Bias'
arxiv_id: '2308.00225'
source_url: https://arxiv.org/abs/2308.00225
tags:
- uni00000013
- bias
- uni00000011
- uni00000048
- decoy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study examines cognitive biases in instruction-tuned large
  language models (LLMs) by testing three well-established human biases: the decoy
  effect, certainty effect, and belief bias. The authors semi-automatically generate
  control and treatment datasets for each bias and measure model bias as the difference
  in choices between conditions.'
---

# Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias

## Quick Facts
- **arXiv ID**: 2308.00225
- **Source URL**: https://arxiv.org/abs/2308.00225
- **Reference count**: 19
- **Key outcome**: Instruction-tuned LLMs exhibit significantly higher cognitive biases than pretrained models across decoy, certainty, and belief bias tasks

## Executive Summary
This study reveals that instruction-tuned large language models (LLMs) exhibit systematic cognitive biases similar to those found in human decision-making. The researchers tested three well-established human biases - decoy effect, certainty effect, and belief bias - across multiple model families including GPT and T5 variants. Their findings show that instruction tuning and reinforcement learning from human feedback (RLHF) can introduce or amplify these biases in LLMs, with instruction-tuned models showing significantly higher bias scores than their pretrained counterparts. This raises important concerns about using these models in decision-support applications where unbiased reasoning is critical.

## Method Summary
The researchers employed a semi-automatic approach to generate control and treatment datasets for each cognitive bias type. For the decoy effect, they created scenarios where a third inferior option influences preference between two other options. The certainty effect tested risk preferences between certain and probabilistic outcomes. The belief bias experiments examined how prior beliefs affect logical reasoning judgments. They measured bias as the difference in choice frequencies between treatment and control conditions across multiple model variants, comparing pretrained models with their instruction-tuned counterparts. The analysis included models from GPT-3, Mistral, and T5 families across different sizes.

## Key Results
- Instruction-tuned models (Flan-T5, GPT3.5, GPT4) show significantly higher bias scores than pretrained models for all three tested biases
- GPT4 exhibits reduced bias in belief tasks but maintains strong decoy and certainty effects
- Larger model sizes (XXL variants) demonstrate more pronounced biases, consistent with findings that bigger models capture more complex patterns
- RLHF tuning further amplifies biases beyond standard instruction tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning introduces or amplifies cognitive biases in LLMs by exposing them to human-generated decision-making patterns that themselves contain biases.
- Mechanism: During instruction tuning, the model learns to replicate human-like behavior on decision-making tasks, including systematic biases like the decoy effect, certainty effect, and belief bias. The tuning process reinforces these patterns because they appear in the training data.
- Core assumption: Human instructions and demonstrations used for tuning contain examples of biased decision-making that the model treats as correct behavior.
- Evidence anchors:
  - [abstract] "Our findings highlight the presence of these biases in various models from the GPT-3, Mistral, and T5 families. Notably, we find a stronger presence of biases in models that have undergone instruction tuning"
  - [section 4] "Our findings reveal that the models fine-tuned on instructions and RLHF mostly exhibit significantly higher levels of bias compared to their pretrained counterparts"
  - [corpus] Weak evidence - the corpus papers focus on cognitive biases in LLMs but don't directly explain the mechanism of how instruction tuning introduces these biases
- Break condition: If the instruction tuning data is carefully curated to remove biased examples or explicitly debias decision-making patterns, the mechanism would fail.

### Mechanism 2
- Claim: Reinforcement learning from human feedback (RLHF) amplifies existing biases by optimizing for human-preferred responses that may contain cognitive biases.
- Mechanism: RLHF optimizes the model to produce responses that humans rate highly, but human raters themselves are subject to cognitive biases. The reward model learns to prefer biased responses because humans exhibit these biases.
- Core assumption: Human feedback during RLHF contains implicit preferences shaped by cognitive biases, and the reward model captures these preferences.
- Evidence anchors:
  - [abstract] "Our findings indicate that applying IT or RLHF tuning either introduces cognitive-like biases into text generation, or amplifies these biases if they already exist"
  - [section 5] "Our findings indicate that the application of reinforcement learning fine-tuning from human feedback has the potential to amplify biases within language models further"
  - [corpus] Moderate evidence - the corpus includes papers on cognitive biases in LLMs but lacks direct evidence about RLHF's role in amplifying biases
- Break condition: If RLHF uses debiased reward models or explicitly penalizes biased responses, this mechanism would break.

### Mechanism 3
- Claim: Larger models exhibit more pronounced biases because they better capture complex patterns in human decision-making data, including systematic biases.
- Mechanism: As model size increases, the capacity to learn and reproduce subtle patterns in human behavior improves, including cognitive biases that smaller models might miss or treat as noise.
- Core assumption: Cognitive biases are learnable patterns present in human-generated training data that larger models can capture more effectively.
- Evidence anchors:
  - [section 5] "Consistent with prior research on social biases (Tal et al., 2022), the larger XXL model exhibits higher bias scores for three bias types"
  - [section 6.2] "As Figure 8 shows, as the price range increases, the bias scores also exhibit higher values"
  - [corpus] Weak evidence - the corpus doesn't directly address the relationship between model size and bias amplification
- Break condition: If larger models develop better reasoning capabilities that allow them to recognize and avoid cognitive biases, this mechanism would break.

## Foundational Learning

- **Concept**: Prospect Theory
  - Why needed here: Understanding prospect theory is essential for interpreting the certainty effect and decoy effect results, as these biases are derived from this framework
  - Quick check question: What is the key difference between expected utility theory and prospect theory in how they handle risk and uncertainty?

- **Concept**: Syllogistic Reasoning
  - Why needed here: The belief bias analysis requires understanding how logical syllogisms work and what constitutes valid vs. invalid reasoning
  - Quick check question: What makes a syllogism logically valid, and how can belief bias interfere with this validity judgment?

- **Concept**: Experimental Design for Bias Detection
  - Why needed here: The study uses control and treatment conditions to measure bias - understanding this methodology is crucial for interpreting results
  - Quick check question: How does the difference between control and treatment conditions help quantify the presence of cognitive bias in model behavior?

## Architecture Onboarding

- **Component map**: Data generation → Prompt templates → Model API → Choice recording → Bias score calculation → Statistical analysis
- **Critical path**: Generate control and treatment datasets → Evaluate model choices on both datasets → Calculate bias scores → Compare across model variants → Analyze results
- **Design tradeoffs**: Using automated data generation ensures scalability but may miss nuanced bias scenarios; manual analysis would be more thorough but less reproducible
- **Failure signatures**: Low bias scores across all models might indicate issues with dataset generation or evaluation methodology; inconsistent results across model sizes might suggest implementation errors
- **First 3 experiments**:
  1. Replicate the certainty effect with DaVinci and DaVinci-003 using the same price and probability values to verify the basic mechanism
  2. Test the decoy effect with Flan-T5-XXL and Flan-T5-XL using the same product categories to examine size effects
  3. Evaluate belief bias with real-life vs. non-real objects to confirm the pattern observed in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do other cognitive biases beyond decoy effect, certainty effect, and belief bias manifest in instruction-tuned language models?
- Basis in paper: [explicit] The authors conclude with "Further exploration of both known and potentially unknown cognitive biases exhibited by language models is necessary to broaden our understanding."
- Why unresolved: The study only examined three specific biases. The authors acknowledge that other biases may exist and could emerge through instruction tuning.
- What evidence would resolve it: Systematic testing of language models against a comprehensive battery of cognitive bias tasks, including those not examined in this study.

### Open Question 2
- Question: Does the introduction of cognitive biases through instruction tuning generalize across different domains and tasks?
- Basis in paper: [inferred] The study tested biases in decision-making tasks (decoy, certainty) and logical reasoning (belief bias), but didn't explore whether these effects transfer to other domains like creative writing or factual question answering.
- Why unresolved: The experiments were limited to specific types of tasks that naturally elicit cognitive biases in humans. The study doesn't address whether instruction tuning introduces similar biases in other contexts.
- What evidence would resolve it: Testing instruction-tuned models across diverse task types to identify whether bias introduction is domain-specific or general.

### Open Question 3
- Question: Can debiasing methods be developed that specifically target instruction-induced cognitive biases without degrading task performance?
- Basis in paper: [explicit] The authors state "it is crucial to advance debiasing methods based on promising initial findings" and note that improving alignment with one objective may cause unintended behaviors with respect to others.
- Why unresolved: While the study identifies the presence of biases, it doesn't explore potential solutions or test whether existing debiasing approaches can mitigate these specific instruction-induced biases.
- What evidence would resolve it: Experimental comparison of various debiasing techniques on instruction-tuned models, measuring both bias reduction and task performance impacts.

## Limitations

- The automated dataset generation approach may not capture the full complexity of human decision-making contexts
- The study focuses on only three specific cognitive biases, limiting generalizability to other types of systematic decision-making errors
- The analysis doesn't distinguish between different sources of instruction tuning data or examine the temporal dynamics of bias emergence during training

## Confidence

- **High**: Clear evidence that instruction-tuned models exhibit stronger cognitive biases than pretrained models
- **Medium**: Evidence showing instruction tuning amplifies cognitive biases, though causation not definitively established
- **Low**: Support for the mechanism by which RLHF amplifies biases is primarily correlational

## Next Checks

1. Control for base model bias: Test the same bias scenarios on pretrained models of comparable size to determine whether observed bias differences are due to instruction tuning specifically or general model scaling effects.

2. Cross-dataset validation: Replicate the bias measurements using independently generated datasets with different value ranges and contexts to ensure the findings aren't artifacts of specific prompt templates.

3. Temporal analysis of bias emergence: Conduct ablation studies examining model behavior at different stages of instruction tuning to determine when and how cognitive biases are introduced or amplified during the fine-tuning process.