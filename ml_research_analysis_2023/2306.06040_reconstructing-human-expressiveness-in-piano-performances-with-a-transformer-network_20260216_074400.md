---
ver: rpa2
title: Reconstructing Human Expressiveness in Piano Performances with a Transformer
  Network
arxiv_id: '2306.06040'
source_url: https://arxiv.org/abs/2306.06040
tags:
- music
- performance
- performances
- expressiveness
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Transformer-based system for reconstructing
  human expressiveness in piano performances. The method uses transcribed scores obtained
  via a recent transcription model to train a multi-layer bidirectional Transformer
  encoder.
---

# Reconstructing Human Expressiveness in Piano Performances with a Transformer Network

## Quick Facts
- arXiv ID: 2306.06040
- Source URL: https://arxiv.org/abs/2306.06040
- Reference count: 26
- One-line primary result: Transformer-based system achieves state-of-the-art results in generating human-like piano performances from transcribed scores

## Executive Summary
This paper proposes a novel approach for reconstructing human expressiveness in piano performances using a multi-layer bidirectional Transformer encoder. The system uses transcribed scores obtained from a recent transcription model to train the network, incorporating pianist identities to control the sampling process and explore variations in expressiveness across different performers. The method achieves state-of-the-art results in generating expressive piano performances, with quantitative evaluation showing average prediction errors of 16.2 in MIDI velocity and 0.0183s in inter-onset interval. Listening tests with 19 participants demonstrate that the system's output is rated significantly higher than a baseline method, though still below human performances, particularly excelling in dynamics.

## Method Summary
The proposed method employs a multi-layer bidirectional Transformer encoder with 4 layers, 4 heads, and 128-dimensional hidden space. Input features include pitch, velocity, duration, bar, position, and inter-onset interval from score MIDIs, while output features are velocity, duration deviation, and inter-onset interval for performance MIDIs. The system is trained on the ATEPP dataset subset containing 457 performances by 6 pianists using Adam optimizer with initial learning rate of 1e-4 and cosine annealing warm restart scheduler. Transcribed scores are obtained through a performance-to-score transcription algorithm that removes expressive variations in timing, velocity, and pedalling. The model incorporates pianist identities through one-hot encoding embeddings to learn individual performance styles.

## Key Results
- Quantitative evaluation shows average prediction errors of 16.2 in MIDI velocity and 0.0183s in inter-onset interval
- Listening test participants rated the system's output significantly higher than baseline method (65.0 vs 54.2 in expressiveness)
- The system particularly excels in dynamics, achieving better performance in velocity prediction compared to duration deviation
- System performs better on compositions from the training set, highlighting dataset bias toward Beethoven compositions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Transformer encoder can effectively capture and reconstruct human expressiveness in piano performances by learning from transcribed scores and performances.
- Mechanism: The Transformer encoder learns complex patterns and nuances of human expressiveness through self-attention mechanisms, allowing it to model long-range dependencies and contextual information in sequential data. By training on transcribed scores and performances, the model learns to generate expressive variations in dynamics and timing.
- Core assumption: The transcribed scores retain sufficient expressiveness information, and the Transformer architecture can effectively learn and reproduce these expressive patterns.
- Evidence anchors:
  - [abstract]: "We propose a novel approach for reconstructing human expressiveness in piano performance with a multi-layer bi-directional Transformer encoder."
  - [section 2.3]: "The transcribed score midi data can be aligned with the performances at the note level without losing any structural generality in the music."
  - [corpus]: Weak evidence; related papers focus on music overpainting and quantization rather than expressiveness reconstruction.

### Mechanism 2
- Claim: Incorporating pianist identities allows the model to learn and reproduce individual performance styles.
- Mechanism: By including one-hot encoded pianist identities in the model architecture, the Transformer encoder can learn the unique expressive patterns and styles associated with each pianist. This allows the model to generate performances that capture the nuances of individual performers.
- Core assumption: The performance styles of individual pianists are consistent and can be learned from the training data.
- Evidence anchors:
  - [abstract]: "We integrate pianist identities to control the sampling process and explore the ability of our system to model variations in expressiveness for different pianists."
  - [section 2.4]: "The pianist's identity is represented using a one-hot encoding embedding, which is then concatenated to the last hidden state before the final prediction."
  - [corpus]: Weak evidence; related papers do not specifically address modeling individual performance styles.

### Mechanism 3
- Claim: Using transcribed scores instead of canonical scores allows the model to learn expressiveness from performances with improvisation and ornaments.
- Mechanism: Transcribed scores, obtained through a performance-to-score transcription algorithm, retain the expressive variations and ornaments present in the original performances. By training on these transcribed scores, the model can learn to generate performances with more nuanced expressiveness, including improvisation and ornaments that may not be explicitly notated in canonical scores.
- Core assumption: The performance-to-score transcription algorithm accurately captures the expressive variations and ornaments present in the original performances.
- Evidence anchors:
  - [abstract]: "We use transcribed scores obtained from an existing transcription model to train our model."
  - [section 2.2]: "The recently released ATEPP dataset [25] provides high-quality transcribed piano performances by world-renowned pianists."
  - [section 2.3]: "The transcription algorithm performs rhythm quantisation through a convolutional-recurrent neural network and a beat tracking algorithm to remove expressive variations in timing, velocity, and pedalling."

## Foundational Learning

- Concept: Music Information Retrieval (MIR) and symbolic music representation
  - Why needed here: Understanding the basics of MIR and symbolic music representation is crucial for working with piano performances and scores, as well as for designing and implementing the feature extraction and encoding processes.
  - Quick check question: What are the common symbolic music representations used in MIR, and how do they differ from audio representations?

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: Familiarity with the Transformer architecture and self-attention mechanisms is essential for understanding how the model learns to capture and reconstruct human expressiveness in piano performances.
  - Quick check question: How does the self-attention mechanism in the Transformer architecture allow the model to capture long-range dependencies and contextual information in sequential data?

- Concept: Expressive music performance and rendering
  - Why needed here: Knowledge of expressive music performance and rendering techniques is necessary for understanding the challenges and goals of the research, as well as for designing appropriate evaluation methods.
  - Quick check question: What are the key expressive features in piano performances, and how do they contribute to the overall expressiveness and style of a performance?

## Architecture Onboarding

- Component map:
  Input features (pitch, velocity, duration, bar, position, IOI) -> Transformer encoder (4 layers, 4 heads, 128D) -> Output features (velocity, duration deviation, IOI) -> Performance generation

- Critical path:
  1. Feature extraction and tokenization from transcribed scores and performances
  2. Training the Transformer encoder on the extracted features
  3. Generating expressive performances by inferring the output features from the input features
  4. Evaluating the generated performances using quantitative metrics and subjective listening tests

- Design tradeoffs:
  - Using transcribed scores instead of canonical scores allows the model to learn expressiveness from performances with improvisation and ornaments, but relies on the accuracy of the transcription algorithm.
  - Incorporating pianist identities enables the model to learn individual performance styles, but requires sufficient data for each pianist in the training set.
  - The Transformer architecture can effectively capture long-range dependencies and contextual information, but may require more computational resources compared to other architectures.

- Failure signatures:
  - Generated performances lack expressiveness and sound mechanical or robotic
  - The model fails to capture the nuances and variations associated with individual pianists
  - The generated performances do not align well with the input scores or contain errors in the expressive features

- First 3 experiments:
  1. Train the model on a small subset of the data and evaluate its ability to generate expressive performances with basic dynamics and timing variations.
  2. Incorporate pianist identities into the model and assess its ability to capture and reproduce individual performance styles.
  3. Test the model on compositions not present in the training data to evaluate its generalization capabilities and ability to generate expressive performances for unseen pieces.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the use of transcribed scores versus canonical scores affect the quality and expressiveness of generated piano performances across different musical styles (e.g., jazz vs classical)?
- Basis in paper: [explicit] The paper discusses the benefits of using transcribed scores for genres like jazz where canonical scores are less representative, and mentions potential issues with ornaments in classical music.
- Why unresolved: The study primarily focuses on classical piano performances and does not extensively explore different musical styles or genres beyond classical music.
- What evidence would resolve it: Conducting experiments that compare the model's performance on transcribed scores versus canonical scores across various musical styles, including jazz and other genres, and evaluating the expressiveness and accuracy of the generated performances.

### Open Question 2
- Question: What is the impact of performance recording environments on the transcribed velocity values, and how can this variability be accounted for in the model to improve the consistency of generated performances?
- Basis in paper: [explicit] The paper mentions that performance recording environments may impact the transcribed velocity values and contribute to differences in velocity distributions among pianists.
- Why unresolved: The paper acknowledges this variability but does not explore methods to normalize or account for these environmental factors in the model.
- What evidence would resolve it: Investigating the relationship between recording environments and velocity values, and developing techniques to normalize or adjust for these differences in the model to achieve more consistent performance outputs.

### Open Question 3
- Question: How can the model be extended to incorporate and model pedalling techniques in piano performances, and what impact would this have on the perceived expressiveness of the generated music?
- Basis in paper: [explicit] The paper suggests considering a separate system to model pedalling techniques or integrating pedalling information into the current feature encoding.
- Why unresolved: The current model does not include pedalling information, which is a significant aspect of expressive piano performance.
- What evidence would resolve it: Developing and integrating a pedalling model into the existing system, and conducting listening tests to evaluate the impact of pedalling on the expressiveness and naturalness of the generated performances.

### Open Question 4
- Question: How effective is the model in capturing and reproducing expressiveness in longer musical passages, and what architectural or training modifications could enhance its ability to maintain expressiveness over extended compositions?
- Basis in paper: [inferred] The paper notes that existing models struggle with recovering expressiveness over longer passages of music, and while the Transformer model is capable of capturing long-range dependencies, its effectiveness in this specific task is not fully explored.
- Why unresolved: The paper does not provide detailed analysis or experiments focused on the model's performance over longer musical passages.
- What evidence would resolve it: Conducting experiments that test the model's ability to generate expressive performances for longer compositions, and exploring architectural modifications such as attention mechanisms or training strategies to improve long-term expressiveness.

## Limitations
- Reliance on a narrow dataset (457 performances by 6 classical pianists playing only Beethoven compositions) limits generalizability to other composers, styles, and eras
- Accuracy of the performance-to-score transcription algorithm is critical but not independently validated
- Small listening test sample size (19 participants) may limit statistical significance of subjective evaluations
- Missing implementation details for key components like the transcription algorithm and OctupleMIDI tokenization method

## Confidence

**High Confidence**: The Transformer architecture's effectiveness in capturing sequential dependencies and the quantitative metrics (velocity error of 16.2, IOI error of 0.0183s) are well-supported by the experimental setup and results.

**Medium Confidence**: The incorporation of pianist identities to model individual styles shows promise, but the limited number of pianists and compositions in the dataset constrains the generalizability of these findings.

**Low Confidence**: The claim that transcribed scores retain sufficient expressiveness information for training is based on the assumption that the transcription algorithm accurately captures all expressive nuances, which is difficult to verify without detailed knowledge of the algorithm's performance.

## Next Checks

1. **Generalization Test**: Evaluate the model on performances by pianists not included in the training set and compositions outside the Beethoven repertoire to assess its ability to generalize beyond the specific dataset.

2. **Ablation Study**: Conduct experiments removing the pianist identity component and comparing performance with canonical scores (instead of transcribed scores) to isolate the contributions of these elements to the model's effectiveness.

3. **Algorithm Validation**: Implement and test the performance-to-score transcription algorithm [15] independently to verify its accuracy in capturing expressive variations and ornaments from original performances.