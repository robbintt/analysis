---
ver: rpa2
title: "A bi-objective $\u03B5$-constrained framework for quality-cost optimization\
  \ in language model ensembles"
arxiv_id: '2312.16119'
source_url: https://arxiv.org/abs/2312.16119
tags:
- cost
- language
- problem
- quality
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a bi-objective optimization framework for ensembling
  diverse open-source Large Language Models (LLMs) to achieve high response quality
  while maintaining cost efficiency. The authors formulate a quality-cost tradeoff
  as a bi-objective combinatorial optimization problem and introduce an additional
  budget constraint that reduces it to a 0/1 knapsack problem.
---

# A bi-objective $ε$-constrained framework for quality-cost optimization in language model ensembles

## Quick Facts
- arXiv ID: 2312.16119
- Source URL: https://arxiv.org/abs/2312.16119
- Reference count: 24
- Primary result: Proposed framework MODI outperforms existing approaches in response quality while significantly reducing costs, achieving better performance than LLM-BLENDER at only 20% of its cost.

## Executive Summary
This paper introduces a bi-objective optimization framework for ensembling diverse open-source Large Language Models (LLMs) that achieves high response quality while maintaining cost efficiency. The authors formulate a quality-cost tradeoff as a bi-objective combinatorial optimization problem and reduce it to a tractable 0/1 knapsack problem using an epsilon-constraint method. The framework employs a DeBERTa-based regression model to predict response quality and uses dynamic programming to select optimal model subsets within a budget constraint. Evaluated on the MixInstruct dataset, MODI demonstrates superior performance compared to existing ensembling approaches, achieving higher quality responses at significantly lower costs.

## Method Summary
The framework addresses the quality-cost tradeoff in LLM ensembling through bi-objective optimization, explicitly modeling response quality (maximizing sum of individual model scores) and inference cost (minimizing weighted sum of token costs) as separate objectives. An epsilon-constraint method fixes a budget per query, transforming the problem into a 0/1 knapsack formulation where quality scores become profits and costs become weights. A DeBERTa-v3-large regression model predicts response quality for each model-query pair using Huber loss and Adam optimization. The dynamic programming knapsack solver selects the optimal subset of models within the budget, and selected responses are combined using GEN-FUSER. The approach is evaluated on the MixInstruct dataset using BARTScore as the quality metric.

## Key Results
- MODI outperforms existing ensembling approaches in response quality while maintaining significant cost reduction
- Achieves better performance than LLM-BLENDER at only 20% of its cost
- Demonstrates effective quality-cost tradeoff through epsilon-constraint optimization

## Why This Works (Mechanism)

### Mechanism 1
The bi-objective optimization formulation captures the fundamental tradeoff between response quality and inference cost in LLM ensembling. By explicitly modeling both response quality (maximizing sum of individual model scores) and inference cost (minimizing weighted sum of token costs) as separate objectives, the framework creates a Pareto frontier that can be navigated via the epsilon constraint. This assumes individual model quality scores can be meaningfully summed to approximate ensemble quality, and cost can be linearly decomposed by model.

### Mechanism 2
The epsilon-constraint transformation reduces the bi-objective problem to a tractable 0/1 knapsack problem. By fixing a budget constraint (epsilon) on total cost, the framework converts the multi-objective optimization into a single-objective maximization problem where quality scores become "profits" and costs become "weights" in a knapsack formulation. This assumes a fixed budget per query is appropriate and the knapsack problem can be solved efficiently via dynamic programming for the problem size.

### Mechanism 3
The DeBERTa-based regression model provides accurate quality predictions that enable effective knapsack selection. By training a regression model to predict BARTScore quality for each model-response pair, the framework obtains the "profits" needed for the knapsack algorithm, enabling data-driven selection rather than heuristic approaches. This assumes the regression model can generalize quality predictions to unseen queries and that BARTScore predictions correlate with actual response quality.

## Foundational Learning

- Concept: Bi-objective optimization and Pareto efficiency
  - Why needed here: The framework explicitly models two competing objectives (quality and cost) that cannot be simultaneously optimized, requiring understanding of how to navigate Pareto frontiers
  - Quick check question: If one model has quality 0.8 at cost 10 and another has quality 0.6 at cost 5, what is the Pareto-optimal choice for a budget of 5?

- Concept: 0/1 knapsack problem and dynamic programming
  - Why needed here: The epsilon-constraint transforms the problem into a knapsack formulation that must be solved efficiently for each query
  - Quick check question: Given items with profits [3, 4, 5] and weights [2, 3, 4] with capacity 5, what is the maximum profit achievable?

- Concept: Regression model training and loss functions
  - Why needed here: The quality prediction model uses specific architectural choices (DeBERTa backbone, Huber loss) that affect its performance
  - Quick check question: Why might Huber loss be preferred over L2 loss when training on quality predictions with potential outliers?

## Architecture Onboarding

- Component map: Query input → Quality regression model (DeBERTa-based) → Predicted quality scores → Knapsack solver → Selected models → Fusion model (GEN-FUSER) → Final response
- Critical path: Query → Quality prediction → Knapsack selection → Fusion → Response
  - This sequence must complete within acceptable latency bounds for the system to be practical
- Design tradeoffs:
  - Budget tightness vs. quality: Tighter budgets reduce cost but may significantly degrade quality
  - Regression model complexity vs. inference speed: More complex models may predict quality better but increase latency
  - Number of models in selection set vs. knapsack solvability: More models increase selection quality but may make dynamic programming slower
- Failure signatures:
  - Consistently low quality despite available budget: Regression model may be underperforming
  - High variance in response quality: Budget allocation may be inappropriate for the query distribution
  - System timeouts: Knapsack solver may be too slow for the model set size
- First 3 experiments:
  1. Baseline comparison: Run the system with epsilon=∞ (select all models) to establish upper bound on quality and cost
  2. Budget sensitivity: Test system performance across different epsilon values (20%, 50%, 80% of max cost) to understand tradeoff curve
  3. Regression ablation: Compare knapsack selections using predicted vs. ground-truth quality scores to measure regression model impact

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MODI scale with larger ensembles of diverse open-source LLMs beyond the 8 models tested? The paper evaluates MODI with a fixed set of 8 models but doesn't explore scalability with larger ensembles or different combinations of diverse models. Experiments testing MODI with varying numbers of diverse open-source models (e.g., 10, 20, 50 models) and comparing performance metrics like quality and cost efficiency across different ensemble sizes would resolve this.

### Open Question 2
What is the impact of different budget constraints (ϵ values) on the quality-cost tradeoff in MODI, and how sensitive is the framework to these parameter choices? The paper mentions using different fractions of LLM-BLENDER's FLOPs as budget constraints but doesn't systematically explore the sensitivity of MODI's performance to various ϵ values. A comprehensive study varying ϵ across multiple budget levels (e.g., 10%, 30%, 50%, 80% of LLM-BLENDER's cost) and analyzing the resulting quality-cost tradeoff curves would resolve this.

### Open Question 3
How does MODI perform on tasks outside the MixInstruct dataset, particularly in specialized domains or with out-of-distribution queries? The paper only evaluates MODI on the MixInstruct dataset, which may not represent all types of queries or specialized domains. Testing MODI on diverse datasets from various domains (e.g., medical, legal, scientific) and out-of-distribution queries to assess its robustness and generalization capabilities would resolve this.

## Limitations
- Budget constraint may not generalize well to domains with highly variable query complexity or user cost-sensitivity
- Quality prediction generalization to unseen query types beyond instruction-following tasks is untested
- Assumes diverse models improve quality without systematic analysis of which combinations work synergistically

## Confidence

**High Confidence**: The fundamental formulation of quality-cost tradeoff as a bi-objective optimization problem and the knapsack reduction via epsilon-constraint are mathematically sound and well-established approaches. The experimental methodology using BARTScore and the MixInstruct dataset is clearly defined.

**Medium Confidence**: The specific quality prediction model architecture and training procedure should work as described, but the exact impact of hyperparameter choices on final performance is not thoroughly explored. The GEN-FUSER fusion approach is mentioned but not detailed.

**Low Confidence**: Claims about superiority over LLM-BLENDER at 20% of the cost are based on specific benchmark conditions. Without access to the exact cost calculation methodology and understanding how representative the MixInstruct dataset is of real-world usage, it's difficult to assess generalizability of these results.

## Next Checks

1. **Cross-Domain Robustness Test**: Evaluate the framework on a dataset from a different domain (e.g., medical, legal, or creative writing) to assess how well the quality regression model generalizes beyond instruction-following tasks.

2. **Budget Sensitivity Analysis**: Systematically vary the epsilon constraint across a wider range (10% to 200% of baseline cost) and measure the quality-cost Pareto curve to identify optimal operating points for different user preferences.

3. **Model Interaction Analysis**: Conduct ablation studies to identify which model combinations produce synergistic effects versus redundant or conflicting outputs, and whether the framework's performance depends on specific model diversity characteristics.