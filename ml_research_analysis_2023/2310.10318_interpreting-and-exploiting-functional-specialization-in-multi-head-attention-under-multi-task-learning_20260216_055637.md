---
ver: rpa2
title: Interpreting and Exploiting Functional Specialization in Multi-Head Attention
  under Multi-task Learning
arxiv_id: '2310.10318'
source_url: https://arxiv.org/abs/2310.10318
tags:
- learning
- attention
- task
- multi-task
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether the multi-head attention module
  evolves functional specialization after multi-task training, inspired by the functional
  specialization in the human brain. To quantify this phenomenon, the authors introduce
  an Important Attention-head Pruning (IAP) method that prunes the most important
  attention heads for each task and measures the performance impact.
---

# Interpreting and Exploiting Functional Specialization in Multi-Head Attention under Multi-task Learning

## Quick Facts
- arXiv ID: 2310.10318
- Source URL: https://arxiv.org/abs/2310.10318
- Reference count: 23
- Key outcome: Functional specialization evolves in multi-head attention after multi-task training, with degree affected by task similarity; Important Attention-head Training (IAT) improves performance without adding parameters

## Executive Summary
This paper investigates whether multi-head attention modules develop functional specialization after multi-task training, inspired by functional specialization observed in the human brain. The authors introduce an Important Attention-head Pruning (IAP) method to quantify this phenomenon by measuring performance drops when pruning task-specific important attention heads. They also propose an Important Attention-head Training (IAT) method that promotes functional specialization by fine-tuning only the most important attention heads for each task during the final phase of multi-task training, demonstrating improved performance on both multi-task and transfer learning without adding parameters.

## Method Summary
The study introduces two key methods: Important Attention-head Pruning (IAP) to quantify functional specialization by pruning top α% important heads per task and measuring performance impact, and Important Attention-head Training (IAT) to promote specialization by fine-tuning only task-specific important heads during the last δ% of multi-task training. The methods are evaluated on seven pre-trained transformer models (BERT variants, RoBERTa, DeBERTa, TinyBERT, GPT, GPT-2) across five GLUE datasets, using a dissociation score metric to quantify the degree of functional specialization between task pairs.

## Key Results
- Multi-head attention evolves functional specialization after multi-task training, with dissociation scores indicating task-specific head importance
- Task similarity negatively correlates with functional specialization degree - more similar tasks show weaker specialization
- IAT method improves both multi-task learning and transfer learning performance by promoting functional specialization without adding parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task training induces functional specialization in attention heads, where different heads become more important for specific tasks.
- Mechanism: During multi-task training, attention heads that contribute more to task performance receive higher importance scores. Pruning these heads causes larger performance drops on their respective tasks, indicating task-specific specialization.
- Core assumption: Attention head importance scores accurately reflect their contribution to task performance.
- Evidence anchors:
  - [abstract]: "Experimental results on seven pre-trained transformer models have demonstrated that multi-head attention does evolve functional specialization phenomenon after multi-task training"
  - [section 4.1]: "We calculate the head importance score I(i)_h ... on training samples to approximate the contribution of head h to task Ti"
  - [corpus]: Weak evidence - corpus papers focus on multi-task frameworks but not attention head specialization mechanisms
- Break condition: If attention heads are truly redundant, pruning any subset should have similar performance impact across all tasks.

### Mechanism 2
- Claim: Task similarity negatively affects the degree of functional specialization in attention heads.
- Mechanism: When tasks are more similar, they require overlapping sets of attention heads, reducing the dissociation between task-specific head importance. Dissociation scores decrease as task similarity increases.
- Core assumption: Task similarity metrics (CNM, DSE, AHP, CRA) accurately capture functional overlap between tasks.
- Evidence anchors:
  - [section 6.2]: "We observe that there is a significant negative correlation between the average dissociation score of task-pair and the similarity between tasks"
  - [section 6.2]: "The more similar tasks are, the lower the average dissociation score is, which suggests the weaker the functional specialization phenomenon is"
  - [corpus]: Weak evidence - corpus papers discuss task similarity but not its impact on attention head specialization
- Break condition: If task similarity doesn't affect head specialization, dissociation scores should be independent of task similarity metrics.

### Mechanism 3
- Claim: Training only the most important attention heads for each task promotes functional specialization and improves multi-task performance.
- Mechanism: By fine-tuning only task-specific important heads during the final phase of training, the model consolidates these heads' roles for each task, reducing negative transfer and improving overall performance.
- Core assumption: Limiting parameter updates to task-specific heads doesn't degrade shared representation learning.
- Evidence anchors:
  - [section 4.2]: "only the top α ∈ [0, 1] important attention heads for task Ti are tuned at the last δ ∈ [0, 1] multi-task training process"
  - [section 6.3]: "Experimental results on the GLUE dataset have demonstrated that our method alleviates the negative transfer among tasks and improves the performance"
  - [corpus]: Weak evidence - corpus papers discuss parameter-efficient multi-task learning but not attention head-specific training
- Break condition: If training only important heads degrades performance, average scores should decrease compared to full fine-tuning.

## Foundational Learning

- Concept: Multi-head attention mechanism in transformers
  - Why needed here: Understanding how attention heads process information is fundamental to analyzing functional specialization
  - Quick check question: How does multi-head attention differ from single-head attention in terms of information capture?

- Concept: Attention head importance scoring
  - Why needed here: The importance score determines which heads are specialized for which tasks
  - Quick check question: What does a high importance score for a head on a task indicate about that head's role?

- Concept: Pruning methodology for model interpretation
  - Why needed here: Pruning is used to quantify functional specialization by measuring performance impact
  - Quick check question: How does pruning the most important heads for a task help identify functional specialization?

## Architecture Onboarding

- Component map:
  Pre-trained transformer models (BERTBASE, BERTLARGE, RoBERTaBASE, DeBERTaV3BASE, TinyBERT, GPT, GPT-2) -> Multi-head attention module with n_h heads per layer -> Importance scoring mechanism (mask variables ξ_h) -> Pruning framework (top α% heads) -> Multi-task training scheduler (IAT with α and δ parameters)

- Critical path:
  1. Initialize pre-trained model
  2. Multi-task train with standard fine-tuning
  3. Calculate head importance scores for each task
  4. Prune top α% important heads per task
  5. Measure performance drops (dissociation scores)
  6. Apply IAT for functional specialization promotion

- Design tradeoffs:
  - Full fine-tuning vs. IAT: IAT reduces computation but may limit cross-task knowledge sharing
  - α parameter choice: Higher α preserves more heads but reduces specialization effect
  - δ parameter choice: Later training phase focuses on specialization but may limit initial adaptation

- Failure signatures:
  - No dissociation scores > 10%: Indicates lack of functional specialization
  - Negative dissociation scores: Incorrect importance scoring or pruning
  - Performance degradation with IAT: Over-specialization preventing cross-task benefits

- First 3 experiments:
  1. Verify functional specialization exists by calculating dissociation scores (α=30%) on a dual-task setup (e.g., MNLI + AG)
  2. Test task similarity effect by comparing dissociation scores between similar (MNLI+QQP) and dissimilar (MNLI+AG) task pairs
  3. Implement IAT on multi-task GLUE setup and measure performance improvement over baseline multi-task training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the functional specialization phenomenon observed in multi-head attention modules extend to other types of neural network modules, such as recurrent neural networks or graph neural networks?
- Basis in paper: [explicit] The paper focuses on multi-head attention modules and suggests investigating more neural network modules that may exhibit functional specialization under multi-task learning in the future.
- Why unresolved: The study is limited to transformer-based models, and the behavior of other types of neural network modules under multi-task learning has not been explored.
- What evidence would resolve it: Conducting similar experiments on recurrent neural networks or graph neural networks and observing if functional specialization occurs would provide evidence.

### Open Question 2
- Question: Are there alternative methods to estimate the importance of attention heads that could potentially show higher dissociation scores than the method used in this study?
- Basis in paper: [inferred] The paper acknowledges that other attention head importance estimation methods might exist and could potentially yield higher dissociation scores.
- Why unresolved: The study uses a specific method to estimate attention head importance, and it is unclear if other methods could provide different results.
- What evidence would resolve it: Applying different attention head importance estimation methods and comparing the resulting dissociation scores would provide evidence.

### Open Question 3
- Question: How do different task similarity metrics affect the observed relationship between task similarity and the degree of functional specialization in multi-head attention?
- Basis in paper: [explicit] The paper uses four task similarity metrics and finds a negative correlation between task similarity and functional specialization, but acknowledges that results might differ for other metrics.
- Why unresolved: The study uses a limited set of task similarity metrics, and it is unclear how other metrics might affect the observed relationship.
- What evidence would resolve it: Applying a broader range of task similarity metrics and analyzing their impact on the relationship between task similarity and functional specialization would provide evidence.

## Limitations

- The head importance scoring mechanism relies on approximation through mask variables that may not perfectly capture true contribution to task performance
- Task similarity metrics are based on cross-entropy and may not fully capture semantic or functional overlap between tasks
- The IAT method's effectiveness depends heavily on hyperparameter choices (α and δ) that may require task-specific tuning
- Experimental validation is limited to five GLUE datasets and seven pre-trained models, which may not generalize to all NLP tasks or model architectures

## Confidence

- **High confidence**: The observation that functional specialization exists in multi-head attention after multi-task training, supported by dissociation score calculations across seven transformer models
- **Medium confidence**: The negative correlation between task similarity and functional specialization degree, as the correlation is demonstrated but the underlying causal mechanism could involve additional factors
- **Medium confidence**: The effectiveness of IAT in improving multi-task and transfer learning performance, though the 1-2 point improvements, while statistically significant, may have limited practical impact in real-world applications

## Next Checks

1. **Ablation study on importance scoring**: Replace the current mask-based importance scoring with gradient-based attribution methods (like Integrated Gradients) to verify that the observed functional specialization is not an artifact of the scoring method.

2. **Cross-architecture validation**: Test the IAT method on encoder-decoder models (like T5 or BART) and smaller transformer variants to determine if the functional specialization phenomenon and its exploitation generalize beyond the current experimental scope.

3. **Long-tail task evaluation**: Apply the IAT method to a dataset with one significantly harder task (like CoLA in GLUE) to verify whether the specialization mechanism helps or hinders performance on tasks with limited training data.