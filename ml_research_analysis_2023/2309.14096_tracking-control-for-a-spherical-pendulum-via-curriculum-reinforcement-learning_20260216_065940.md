---
ver: rpa2
title: Tracking Control for a Spherical Pendulum via Curriculum Reinforcement Learning
arxiv_id: '2309.14096'
source_url: https://arxiv.org/abs/2309.14096
tags:
- learning
- trajectories
- tracking
- pendulum
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of learning tracking control for
  a spherical pendulum mounted on a 4-DOF robotic arm using curriculum reinforcement
  learning. The approach leverages a recent curriculum learning algorithm (CURROT)
  that generates tasks via constrained interpolation between task distributions, combined
  with massively parallelized simulations.
---

# Tracking Control for a Spherical Pendulum via Curriculum Reinforcement Learning

## Quick Facts
- arXiv ID: 2309.14096
- Source URL: https://arxiv.org/abs/2309.14096
- Reference count: 40
- One-line primary result: Learned policy matches optimal control baseline on real system with 2.3 cm average tracking error

## Executive Summary
This paper addresses the challenge of learning tracking control for a spherical pendulum mounted on a 4-DOF robotic arm using curriculum reinforcement learning. The authors employ the CURROT algorithm to generate progressive task curricula through constrained interpolation between task distributions, enabling the agent to master increasingly complex trajectories. By incorporating a Mahalanobis distance metric and improved sampling-based optimization, the method effectively navigates high-dimensional trajectory spaces while respecting the non-Euclidean structure of the problem. Experiments demonstrate successful transfer from simulation to real hardware, achieving tracking performance comparable to optimal control baselines.

## Method Summary
The approach uses Proximal Policy Optimization (PPO) with curriculum learning via the CURROT algorithm. The policy maps observations (robot joint positions and pendulum orientation) and a lookahead of the target trajectory to joint torques. The CURROT algorithm generates tasks by minimizing Wasserstein distance between current and target task distributions under a performance constraint. The authors improve upon the original CURROT by introducing a Mahalanobis distance metric that captures trajectory space structure and implementing an enhanced sampling-based optimization scheme. Training occurs in massively parallel simulations (2048 environments) for approximately 262 million steps, with evaluation on both simulation and real robot platforms.

## Key Results
- Learned policy achieves average tracking errors around 2.3 cm on the real robot
- Matches performance of optimal control baseline for 3D trajectory tracking
- Successfully learns state estimation and control jointly for nonlinear tracking tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Curriculum generation in high-dimensional task spaces improves learning efficiency for nonlinear tracking control
- Mechanism: CURROT constructs task distributions that progressively increase in complexity, allowing the agent to master easier tasks before tackling full target trajectories by optimizing Wasserstein distance under performance constraints
- Core assumption: Agent generalization allows good performance on target tasks even with imperfect training distribution match
- Evidence anchors:
  - [abstract] "Through an improved optimization scheme that better respects the non-Euclidean task structure, we allow the method to reliably generate curricula of trajectories to be tracked, resulting in faster and more robust learning"
  - [section III-D] "We are interested in learning a policy π that performs well on a target distribution µ(γ)=µ(c)... CURROT [13], which creates a curriculum of task distributions pi(c) by iteratively minimizing their Wasserstein distance W2(p, µ) to the target distribution µ(c) under a given distance function d(c1, c2)"

### Mechanism 2
- Claim: Mahalanobis distance metric captures non-Euclidean structure of trajectory spaces
- Mechanism: Incorporates system dynamics into distance metric to measure actual trajectory dissimilarity based on generated behavior rather than parameter representations
- Core assumption: Piece-wise constant jerk representation accurately captures essential system dynamics
- Evidence anchors:
  - [section IV-A] "For our trajectory representation, this corresponds to a Euclidean distance between elements in ker (Ψ(te)). However, according to Eq. (11), we know that the difference between two (one-dimensional) LTI system states is given by x1(t)−x2(t) = Ψ(t)(u1 − u2)"

### Mechanism 3
- Claim: Policy architecture with trajectory lookahead enables strong generalization
- Mechanism: Conditions policy on history of observations, past actions, and target trajectory lookahead to extract features that generalize across trajectory shapes
- Core assumption: Lookahead window captures essential information for immediate future control
- Evidence anchors:
  - [section III-A] "The control law generates torques on top of a gravity compensation term g(qw) based on a history of positional observations, applied torques, and information about the desired trajectory γ τ t=π(Ot, At, Tt)+g(qw,t)"
  - [section V-B] "By conditioning the policy behavior on limited-time lookahead windows of the target trajectory Tt, the learning agent seems capable of generalizing well to unseen trajectories"

## Foundational Learning

- Concept: Curriculum Learning
  - Why needed here: Spherical pendulum tracking is highly unstable and underactuated, making direct learning from target distribution difficult
  - Quick check question: What is the primary benefit of using a curriculum in reinforcement learning for complex control tasks?

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: Agent only has position measurements, not full state including velocities, requiring inference from observation history
  - Quick check question: How does the agent handle partial observability, and what information does it use to infer the hidden state?

- Concept: Wasserstein Distance and Optimal Transport
  - Why needed here: CURROT uses Wasserstein distance to measure dissimilarity between task distributions over continuous trajectory spaces
  - Quick check question: Why is Wasserstein distance more appropriate than Kullback-Leibler divergence for comparing task distributions in this context?

## Architecture Onboarding

- Component map: Simulation Environment -> Policy Network -> Curriculum Generator -> Performance Predictor -> Assignment Solver -> Trajectory Representation
- Critical path: Generate trajectory -> Simulate environment -> Collect observations and rewards -> Update policy -> Update curriculum distribution -> Repeat
- Design tradeoffs:
  - High-dimensional vs. low-dimensional trajectory representation: High-dimensional allows complex trajectories but makes curriculum generation challenging; low-dimensional simplifies generation but limits expressiveness
  - Euclidean vs. Mahalanobis distance: Euclidean is simpler but may not capture true trajectory dissimilarity; Mahalanobis incorporates domain knowledge but is more complex to compute
- Failure signatures:
  - Poor tracking performance despite good training distribution performance: Curriculum generation not effectively exploring task space or limited agent generalization
  - High variance across seeds: Unstable learning process or curriculum generation sensitivity to initial conditions
- First 3 experiments:
  1. Run baseline PPO algorithm without curriculum to establish performance benchmark
  2. Implement CURROT with Euclidean distance metric and compare to baseline
  3. Replace Euclidean distance with Mahalanobis distance and assess impact on curriculum generation and learning efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does generalization capability of the policy affect performance of curriculum generation methods like CURROT?
- Basis in paper: [explicit] Authors note curriculum success depends on agent generalization capability and observe CURROT can fail to generate target-similar trajectories, but agent generalization compensates
- Why unresolved: Paper demonstrates effect but doesn't provide quantitative measures of generalization's impact on curriculum effectiveness or methods to explicitly incorporate generalization into curriculum generation
- What evidence would resolve it: Experiments comparing curriculum learning performance across agents with varying generalization capabilities, or methods accounting for agent generalization when generating curricula

### Open Question 2
- Question: How does CURROT scale to extremely high-dimensional context spaces and what are practical limits of applicability?
- Basis in paper: [explicit] Authors test up to 399-dimensional spaces and observe stable performance for default trajectory representation, but Wasserstein distance increases with dimensionality for some variants
- Why unresolved: Experiments only go to 399 dimensions; paper doesn't explore theoretical or practical limits of scalability or identify point of ineffectiveness
- What evidence would resolve it: Experiments testing CURROT on tasks with 1000+ dimensional context spaces or theoretical analysis of computational complexity and convergence properties in high dimensions

### Open Question 3
- Question: What is optimal policy architecture for balancing generalization with curriculum structure exploitation in trajectory tracking?
- Basis in paper: [explicit] Authors demonstrate default architecture with trajectory lookahead enables strong generalization that masks curriculum generation shortcomings; simpler context vector makes curriculum effectiveness more apparent
- Why unresolved: Paper doesn't explore alternative architectures or provide systematic method for determining optimal balance between generalization and curriculum exploitation
- What evidence would resolve it: Comparative experiments testing various policy architectures (different lookahead levels, attention mechanisms, hierarchical representations) on curriculum learning tasks, measuring learning efficiency and final performance

## Limitations

- Approach relies heavily on accurate system dynamics modeling for Mahalanobis distance metric, with performance potentially degrading if real system deviates from simulation
- Partial observability assumption (position-only measurements) may not generalize to scenarios requiring full state information
- Trajectory representation using piece-wise constant jerk sequences may not capture more complex motion patterns

## Confidence

**High confidence**: Core claim that curriculum learning improves tracking performance compared to standard RL baselines is well-supported by simulation and real robot experiments; Mahalanobis distance mechanism is theoretically sound and empirically validated

**Medium confidence**: Generalization claim across different trajectory distributions is supported by experiments but may be sensitive to lookahead window and trajectory representation choices; real robot results are promising but based on limited trajectory shapes

**Low confidence**: Scalability to more complex systems or higher-dimensional task spaces hasn't been thoroughly explored, particularly regarding computational efficiency of assignment problem solver for large-scale applications

## Next Checks

1. **Cross-system generalization**: Test learned policy on different robotic platform or with modified system parameters to assess robustness to modeling inaccuracies and physical variations

2. **Curriculum ablation study**: Systematically remove components of curriculum generation process (e.g., Mahalanobis distance, performance prediction) to quantify individual contributions to learning efficiency and final performance

3. **Extended trajectory evaluation**: Evaluate tracking performance on trajectories significantly different from training distribution in shape, speed, and spatial coverage to understand generalization limits of learned policy