---
ver: rpa2
title: 'CAMMARL: Conformal Action Modeling in Multi Agent Reinforcement Learning'
arxiv_id: '2306.11128'
source_url: https://arxiv.org/abs/2306.11128
tags:
- agents
- other
- learning
- conformal
- cammarl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CAMMARL, a novel algorithm that combines multi-agent
  reinforcement learning with conformal prediction to improve decision-making. The
  key idea is to have agents predict sets of actions for their teammates with high
  probability, rather than exact actions.
---

# CAMMARL: Conformal Action Modeling in Multi Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2306.11128
- Source URL: https://arxiv.org/abs/2306.11128
- Authors: 
- Reference count: 40
- Key outcome: Combines multi-agent RL with conformal prediction to improve decision-making through high-probability action sets for teammates

## Executive Summary
This paper introduces CAMMARL, a novel algorithm that integrates conformal prediction with multi-agent reinforcement learning to improve cooperative decision-making. The key innovation is having agents predict sets of actions for their teammates with high probability rather than exact actions, using these sets as inputs to inform their policies. Experiments in two cooperative tasks demonstrate that CAMMARL outperforms several baselines including ones that use true actions or observations of other agents, achieving faster learning and higher returns while providing theoretical guarantees on prediction coverage.

## Method Summary
CAMMARL operates in partially observable cooperative multi-agent environments where agents must reason about teammates' actions. Each agent maintains a conformal prediction model that takes the other agent's observations as input and outputs action sets with guaranteed coverage probability (e.g., 95%). These action sets are encoded and concatenated with the agent's own observations to form its input state. Both the conformal model and the RL agents are trained using PPO, with the conformal model periodically updated from experience replay. The approach provides principled uncertainty quantification about teammate actions while avoiding the need for exact action reconstruction.

## Key Results
- CAMMARL outperforms multiple baselines (NOAM, TAAM, TOAM, GIAM, EAP) in both Cooperative Navigation and Level-based Foraging tasks
- CAMMARL agents learn faster than baseline methods and achieve returns close to an upper bound of performance
- The conformal prediction approach provides theoretical guarantees on the coverage of predicted action sets
- CAMMARL maintains performance advantages even when other agents use different policies or when action sets contain varying numbers of actions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAMMARL improves decision-making by providing high-probability action sets for teammates, reducing uncertainty in multi-agent coordination.
- Mechanism: Conformal prediction models output action sets with guaranteed coverage probability (e.g., 95%), allowing the learning agent to make informed decisions based on statistically valid uncertainty quantification rather than exact but potentially incorrect action predictions.
- Core assumption: The conformal prediction model can accurately learn the probability distribution over teammate actions from observation-action mappings.
- Evidence anchors:
  - [abstract] "For estimating such sets, we use the concept of conformal predictions, by means of which, we not only obtain an estimate of the most probable outcome but get to quantify the operable uncertainty as well."
  - [section] "We model other agents' actions in the form of confident sets, i.e., sets that contain other agents' true actions with a high probability."
- Break condition: If the conformal prediction model fails to achieve the desired coverage probability due to distributional shift or insufficient training data, the guarantees no longer hold and performance degrades.

### Mechanism 2
- Claim: CAMMARL outperforms baselines by avoiding the need for exact action reconstruction while still providing useful information for decision-making.
- Mechanism: Instead of reconstructing teammate actions (as in EAP or TAAM baselines), CAMMARL provides action sets that probabilistically contain the true action, allowing the learning agent to make decisions without requiring perfect prediction accuracy.
- Core assumption: Probabilistic action sets provide sufficient information for effective decision-making even when they don't contain the exact action.
- Evidence anchors:
  - [section] "In LBF, estimating the exact action of Nother can be difficult, and with unaccounted uncertainty in the predictions, Nself suffers from a lower return."
  - [section] "CAMMARL agents are able to distinctly perform better than EAP in LBF"
- Break condition: If the action sets become too large (low precision), they provide insufficient discriminatory information for decision-making.

### Mechanism 3
- Claim: CAMMARL achieves faster learning convergence by providing informative inputs that reduce the complexity of the policy learning problem.
- Mechanism: By providing structured uncertainty information about teammate actions rather than raw observations alone, CAMMARL reduces the dimensionality and complexity of the learning problem for the main agent.
- Core assumption: The additional information from conformal action sets reduces the effective state space complexity for the learning agent.
- Evidence anchors:
  - [section] "CAMMARL agents learn their policies faster than the other baselines"
  - [section] "Interestingly, in CN, CAMMARL can be seen to learn arguably faster"
- Break condition: If the conformal prediction model updates are too infrequent or the prediction quality is poor, the additional inputs may not provide meaningful information reduction.

## Foundational Learning

- Concept: Conformal prediction theory
  - Why needed here: CAMMARL relies on conformal prediction to generate statistically valid action sets with guaranteed coverage probability
  - Quick check question: What is the key guarantee provided by conformal prediction that makes it suitable for CAMMARL?

- Concept: Multi-agent reinforcement learning with partial observability
  - Why needed here: CAMMARL operates in partially observable environments where agents must reason about teammates' intentions
  - Quick check question: How does partial observability affect the need for teammate modeling in cooperative MARL?

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: The learning agents in CAMMARL use PPO for policy optimization
  - Quick check question: What are the key advantages of PPO that make it suitable for the CAMMARL framework?

## Architecture Onboarding

- Component map: Environment observations -> Conformal prediction model -> Action sets -> Agent policy (PPO) -> Actions -> Environment

- Critical path:
  1. Observe environment
  2. Generate conformal action sets for teammate
  3. Concatenate sets to self-agent observations
  4. Execute policies
  5. Store experiences
  6. Periodically train conformal model and agents

- Design tradeoffs:
  - Exact action prediction vs. action sets: Accuracy vs. uncertainty quantification
  - Conformal model update frequency: Timeliness vs. computational cost
  - Set encoding method: Binary encoding vs. embedding vs. padding

- Failure signatures:
  - Large action set sizes indicate poor model precision
  - Low coverage percentage indicates conformal guarantees not being met
  - Performance worse than NOAM baseline suggests conformal model is harmful

- First 3 experiments:
  1. Verify conformal prediction coverage meets theoretical guarantees (95%)
  2. Compare CAMMARL performance against NOAM baseline
  3. Measure action set size trends during training to verify decreasing uncertainty

## Open Questions the Paper Calls Out
- How would CAMMARL perform in more complex multi-agent environments with more than two agents or requiring higher levels of coordination?
- How would CAMMARL perform with alternative reinforcement learning algorithms besides PPO?
- How would the performance of CAMMARL change with different buffer sizes for the conformal model?

## Limitations
- Network architectures and hyperparameters are not fully specified, making exact reproduction difficult
- Conformal prediction model's calibration procedure details are sparse
- Evaluation focuses on only two cooperative environments, limiting generalizability

## Confidence

| Claim | Confidence |
|-------|------------|
| Core theoretical mechanism of conformal prediction sets | High |
| Empirical performance claims | Medium |
| Scalability to larger multi-agent systems | Low |

## Next Checks
1. **Coverage verification**: Run conformal prediction model in isolation to verify that the 95% coverage guarantee holds empirically across different prediction sets and timesteps.

2. **Baseline completeness**: Implement and test additional state-of-the-art baselines like QMIX or MADDPG to establish stronger comparative claims.

3. **Robustness analysis**: Test CAMMARL performance under varying observation noise levels and with heterogeneous agent capabilities to assess practical robustness.