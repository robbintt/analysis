---
ver: rpa2
title: Exploring Linguistic Properties of Monolingual BERTs with Typological Classification
  among Languages
arxiv_id: '2305.02215'
source_url: https://arxiv.org/abs/2305.02215
tags:
- languages
- bert
- language
- matrices
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether monolingual BERT models encode
  linguistic properties similar to their typological language similarities. We introduce
  a novel method, bidimensional CKA (biCKA), to compare weight matrices across BERT
  models.
---

# Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages

## Quick Facts
- arXiv ID: 2305.02215
- Source URL: https://arxiv.org/abs/2305.02215
- Reference count: 33
- Key outcome: Syntactic typological similarity correlates with weight matrix similarity in middle BERT layers (4-6), particularly in attention output and value matrices (Spearman coefficients up to 0.77, p < 0.01).

## Executive Summary
This study investigates whether monolingual BERT models encode linguistic properties that align with traditional typological language similarities. Using a novel bidimensional CKA (biCKA) method to compare weight matrices across 14 BERT models, the researchers find that syntactic typological similarity correlates most strongly with weight matrix similarity in middle BERT layers (4-6). This finding confirms that BERT models replicate traditional linguistic models, with syntactic information primarily encoded in middle layers. The study also observes that domain adaptation on semantically equivalent texts enhances cross-language weight similarity.

## Method Summary
The study compares monolingual BERT models across 14 languages using a novel bidimensional CKA (biCKA) metric to measure weight matrix similarity. Researchers extracted WALS typological feature vectors for the languages and computed syntactic and morphological similarity using cosine similarity. They then calculated Spearman correlations between biCKA similarity scores and typological similarity for all language pairs across different BERT layers. The analysis focused on attention-related weight matrices (Q, K, V, OA) in middle layers where linguistic information is most prominent.

## Key Results
- Syntactic typological similarity shows strongest correlation with weight matrix similarity in middle BERT layers (4-6)
- biCKA metric outperforms standard CKA in capturing weight matrix relationships for transformer architectures
- Domain adaptation on semantically equivalent texts enhances cross-language weight matrix similarity
- Attention output and value matrices show higher correlation with typological similarity than other weight matrices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Typological similarity correlates with BERT weight similarity in middle layers
- Mechanism: Languages with similar syntactic structures have BERT models with similar attention weights in middle layers, suggesting these layers encode syntax
- Core assumption: BERT weights reflect linguistic structure when trained on monolingual data
- Evidence anchors: abstract, section, weak corpus evidence
- Break condition: If typological similarity doesn't predict weight similarity, or if correlation appears in other layers instead

### Mechanism 2
- Claim: biCKA captures bidirectional weight matrix similarity better than standard CKA
- Mechanism: biCKA compares both W·W^T and W^T·W relationships in weight matrices, capturing full structural similarity for transformer weights
- Core assumption: Transformer weight matrices have meaningful row-column relationships beyond what standard CKA captures
- Evidence anchors: section, weak corpus evidence
- Break condition: If standard CKA shows similar results, or if biCKA doesn't improve correlation with typological similarity

### Mechanism 3
- Claim: Domain adaptation on semantically equivalent texts enhances cross-language weight similarity
- Mechanism: Training BERT on parallel or semantically aligned texts makes similar languages' models converge more in weight space
- Core assumption: Semantic equivalence training aligns linguistic representations across languages
- Evidence anchors: abstract, section, weak corpus evidence
- Break condition: If domain adaptation doesn't increase correlation, or if it decreases similarity

## Foundational Learning

- Concept: Typological classification and WALS database
  - Why needed here: The study relies on typological features to measure language similarity, which drives the correlation analysis
  - Quick check question: What's the difference between genealogical and typological language classification, and why does this study use typological?

- Concept: Transformer architecture and BERT layers
  - Why needed here: Understanding BERT's multi-layer structure and attention mechanisms is crucial for interpreting where linguistic information is encoded
  - Quick check question: Which BERT layers typically encode syntax vs. semantics, and how does this study's finding align with prior probing studies?

- Concept: Centered Kernel Alignment (CKA) and its limitations
  - Why needed here: The study uses CKA variants to compare weight matrices, so understanding its mechanics and why biCKA was developed is essential
  - Quick check question: Why doesn't standard CKA work well for comparing weight matrices, and what specific problem does biCKA solve?

## Architecture Onboarding

- Component map: WALS typological vectors → language similarity matrix → language pair rankings → BERT weight matrices (Q, K, V, OA, C, DI, DO) → biCKA similarity scores → layer-wise correlation with typological similarity → Spearman correlation calculation → statistical significance testing → visualization of correlation matrices

- Critical path: Typological similarity calculation → biCKA weight comparison → correlation analysis → result interpretation

- Design tradeoffs: biCKA vs. standard CKA (captures bidirectional relationships but is more complex), using typological vs. activation-based similarity (avoids probing tasks but depends on database quality)

- Failure signatures: Low or inconsistent correlations across layers, biCKA not outperforming standard CKA, correlations appearing in wrong layers (e.g., syntax in early layers)

- First 3 experiments:
  1. Replicate correlation analysis with standard CKA to verify biCKA's advantage
  2. Test correlation with different typological feature subsets to identify which features drive the signal
  3. Apply the methodology to multilingual BERT to compare with monolingual results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the bidimensional CKA method (biCKA) provide consistent and reliable measurements of weight matrix similarity across different transformer architectures beyond BERT?
- Basis in paper: The paper introduces biCKA as a novel method for comparing weight matrices, specifically for BERT models, but does not test its applicability to other transformer architectures.
- Why unresolved: The paper focuses exclusively on BERT models, leaving open the question of whether biCKA can generalize to other architectures like GPT or RoBERTa.
- What evidence would resolve it: Testing biCKA on a variety of transformer models and comparing results with other similarity metrics across different architectures.

### Open Question 2
- Question: How does the correlation between typological similarity and weight matrix similarity change when using different typological databases or feature sets beyond WALS?
- Basis in paper: The paper uses WALS for typological features but notes that WALS is incomplete for some languages, suggesting potential limitations in the feature set used.
- Why unresolved: The study relies on a single typological resource, and the correlation findings might be sensitive to the choice of features or database.
- What evidence would resolve it: Repeating the experiments with alternative typological databases or custom feature sets to see if the observed correlations hold.

### Open Question 3
- Question: Can the biCKA method be used to identify specific linguistic features encoded in individual attention heads or sub-layers within BERT layers, rather than just at the layer level?
- Basis in paper: The paper analyzes similarity at the layer level but does not explore finer-grained analysis within layers, such as individual attention heads.
- Why unresolved: The current methodology aggregates similarity across entire layers, potentially missing nuanced encoding of linguistic features at the head or sub-layer level.
- What evidence would resolve it: Applying biCKA to individual attention heads or sub-layers and correlating their similarity with specific typological features.

## Limitations

- The biCKA metric, while showing improved correlation over standard CKA, lacks extensive validation across different model architectures and tasks.
- The study's conclusions about domain adaptation enhancing weight similarity are based on observations rather than controlled experiments.
- The methodology depends on the completeness and quality of the WALS database, which has gaps for some languages.

## Confidence

- **High Confidence**: The correlation between typological similarity and BERT weight similarity in middle layers (4-6) is well-supported by statistical significance (p < 0.01) and aligns with prior linguistic probing studies.
- **Medium Confidence**: The mechanism by which biCKA captures weight matrix similarity better than standard CKA is theoretically sound but requires additional validation across different contexts.
- **Low Confidence**: The claim about domain adaptation enhancing cross-language weight similarity lacks experimental controls and detailed analysis of the underlying mechanisms.

## Next Checks

1. Conduct controlled experiments comparing biCKA performance with standard CKA across diverse model architectures and datasets to verify its general applicability.
2. Perform ablation studies on typological feature subsets to identify which specific features drive the observed correlations and their relative importance.
3. Test the methodology on multilingual BERT models to determine whether monolingual and multilingual models exhibit similar patterns of typological encoding.