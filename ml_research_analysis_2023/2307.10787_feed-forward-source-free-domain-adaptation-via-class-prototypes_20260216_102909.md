---
ver: rpa2
title: Feed-Forward Source-Free Domain Adaptation via Class Prototypes
arxiv_id: '2307.10787'
source_url: https://arxiv.org/abs/2307.10787
tags:
- domain
- adaptation
- source
- methods
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a feed-forward source-free domain adaptation
  (SFDA) method that uses class prototypes to improve accuracy without the need for
  back-propagation. The method constructs class prototypes under domain shift using
  a pre-trained model and pseudo-labels, and then classifies test examples by finding
  the closest prototype.
---

# Feed-Forward Source-Free Domain Adaptation via Class Prototypes

## Quick Facts
- arXiv ID: 2307.10787
- Source URL: https://arxiv.org/abs/2307.10787
- Reference count: 17
- This paper introduces a feed-forward source-free domain adaptation (SFDA) method that uses class prototypes to improve accuracy without the need for back-propagation. The method constructs class prototypes under domain shift using a pre-trained model and pseudo-labels, and then classifies test examples by finding the closest prototype. The approach is evaluated on standard benchmarks (Office, Office-Home, and VisDA-C) and achieves significant improvements in accuracy compared to the pre-trained model, while requiring only a small fraction of the adaptation time of existing SFDA methods. Specifically, the method achieves average accuracy increases of 5.6%, 5.4%, and 6.3% on the three benchmarks, respectively, while reducing adaptation time by over 90% compared to SHOT-IM and SHOT methods.

## Executive Summary
This paper proposes a novel feed-forward approach to source-free domain adaptation (SFDA) that constructs class prototypes from target domain features and uses nearest-prototype classification. The method addresses the challenge of adapting pre-trained models to new target domains without access to source data or labeled target examples. By leveraging confidence-weighted averaging of features and optionally refining pseudo-labels with a robust generative classifier (RoG), the approach achieves significant accuracy improvements while requiring only a small fraction of the adaptation time compared to existing SFDA methods.

## Method Summary
The method constructs class prototypes using features extracted by a pre-trained model and pseudo-labels from the target data. For each class, it computes a weighted average of feature vectors, where the weights are the softmax probabilities of the class from the pre-trained model. Classification is performed by finding the nearest prototype using cosine distance. An optional extension uses a robust generative classifier (RoG) based on Minimum Covariance Determinant (MCD) to improve pseudo-labels before prototype construction. The approach is evaluated on standard benchmarks (Office, Office-Home, and VisDA-C) and achieves significant improvements in accuracy compared to the pre-trained model, while requiring only a small fraction of the adaptation time of existing SFDA methods.

## Key Results
- Achieves average accuracy increases of 5.6%, 5.4%, and 6.3% on Office, Office-Home, and VisDA-C benchmarks respectively
- Reduces adaptation time by over 90% compared to SHOT-IM and SHOT methods
- Outperforms BN adaptation baseline by 4.3% on average across all benchmarks
- Shows that feed-forward SFDA can achieve competitive results with significantly lower computational cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prototypes computed from target domain features with confidence-weighted averaging better capture the true class centers under domain shift than the original source classifier.
- Mechanism: The method constructs prototypes as weighted averages of feature vectors where the weights are the softmax probabilities of the class from the pre-trained model. This effectively focuses the prototype on high-confidence examples and reduces noise from ambiguous cases.
- Core assumption: The pre-trained model's confidence scores are reasonably correlated with true class membership under domain shift, such that high-confidence predictions are more likely to be correct.
- Evidence anchors:
  - [abstract] "Our approach is based on computing prototypes of classes under the domain shift using a pre-trained model."
  - [section 3] "wi = [fϕ◦θ(xi)]l1 (argmaxfϕ◦θ(xi) == l). Only examples predicted to be of class l contribute to calculation of class prototype cl, for which we use an indicator function denoted by 1."
- Break condition: If the pre-trained model's predictions are highly random or systematically wrong, confidence weighting will amplify errors and prototypes will be misleading.

### Mechanism 2
- Claim: Replacing the classifier with a nearest-prototype lookup layer enables feed-forward adaptation without iterative optimization.
- Mechanism: After prototypes are constructed, classification is performed by computing cosine distance from a test example's features to all class prototypes and selecting the closest one. This bypasses the need for fine-tuning the classifier layer.
- Core assumption: The feature extractor fθ generalizes well enough under domain shift that cosine distances in feature space correlate with semantic similarity.
- Evidence anchors:
  - [abstract] "During inference we find the closest prototype to the features of the current test example, and we select the class of that prototype as prediction."
  - [section 3] "Predictions are made by extracting features from the test example and finding the closest prototype, using the selected distance measure."
- Break condition: If feature space becomes misaligned under domain shift, cosine distances will no longer reflect class similarity, and nearest-prototype will fail.

### Mechanism 3
- Claim: Combining prototypes with MCD-based pseudo-labels improves prototype quality and thus classification accuracy.
- Mechanism: RoG (Robust Generative classifier) based on Minimum Covariance Determinant is trained on the pre-trained model's target features and pseudo-labels to produce more reliable pseudo-labels, which are then used to compute better prototypes.
- Core assumption: MCD-based classifiers can produce more robust pseudo-labels under domain shift than the original classifier, especially in the presence of outliers.
- Evidence anchors:
  - [section 3] "We extend PDA with an initial step that uses robust generative classifier (RoG) based on Minimum Covariance Determinant [8] to obtain higher-quality pseudo-labels."
  - [section 5] "Combination of PDA with MCD leads to additional improvements in accuracy of PDA."
- Break condition: If MCD assumptions (e.g., Gaussian class distributions) are violated severely, pseudo-labels may degrade rather than improve.

## Foundational Learning

- Concept: Domain adaptation and source-free setting
  - Why needed here: The entire method operates in the SFDA scenario where only a pre-trained model and unlabeled target data are available.
  - Quick check question: What is the key difference between standard unsupervised domain adaptation and source-free domain adaptation?

- Concept: Prototypical networks and metric learning
  - Why needed here: The method is directly inspired by prototypical networks from few-shot learning; understanding how prototypes encode class information is critical.
  - Quick check question: How does a prototypical network classify a new example?

- Concept: Confidence weighting and softmax calibration
  - Why needed here: The weighting scheme uses softmax probabilities as weights; understanding how confidence relates to correctness is important for debugging.
  - Quick check question: Why might high softmax probability not always indicate a correct prediction?

## Architecture Onboarding

- Component map:
  - Pre-trained feature extractor (fθ)
  - Pre-trained classifier (fϕ) - only used for initial pseudo-labels
  - Prototype construction module (weighted averaging)
  - MCD pseudo-label refinement (optional)
  - Nearest-prototype classifier (cosine distance)

- Critical path:
  1. Extract features from target data using fθ
  2. Get initial pseudo-labels using fϕ
  3. Compute prototypes (optionally with MCD refinement)
  4. For inference, compute cosine distances to prototypes

- Design tradeoffs:
  - Speed vs. accuracy: PDA is much faster than backprop methods but may be less accurate; MCD refinement adds accuracy at some computational cost.
  - Confidence weighting vs. one-hot: Weighting reduces noise but depends on well-calibrated confidences.
  - Simplicity vs. performance: The basic PDA method is simple but may benefit from more sophisticated prototype construction.

- Failure signatures:
  - Accuracy close to source-only baseline → prototypes poorly constructed (e.g., due to bad pseudo-labels)
  - Degradation on certain classes → prototypes for those classes are inaccurate
  - Slow inference → likely due to inefficient prototype distance computation
  - High variance across runs → unstable pseudo-label quality or prototype construction

- First 3 experiments:
  1. Baseline: Run source-only model on target, measure accuracy
  2. BN update: Update batch norm stats, measure accuracy and time
  3. PDA basic: Build prototypes with confidence weighting, test nearest-prototype classification, measure accuracy and time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the accuracy of feed-forward SFDA methods be further improved to match or exceed back-propagation based methods?
- Basis in paper: [explicit] The authors state that using true labels leads to large improvements and suggest there is potential for development of new highly-accurate feed-forward SFDA methods.
- Why unresolved: The paper demonstrates that feed-forward methods like PDA achieve strong results but still lag behind back-propagation methods. The upper bound using true labels suggests room for improvement.
- What evidence would resolve it: Development and evaluation of new feed-forward SFDA methods that achieve accuracy close to or surpassing back-propagation methods like SHOT on standard benchmarks.

### Open Question 2
- Question: Can the prototype-based approach be extended to other domain adaptation scenarios beyond source-free adaptation?
- Basis in paper: [inferred] The method relies on constructing class prototypes using a pre-trained model and pseudo-labels, which could potentially be applied to other adaptation scenarios.
- Why unresolved: The paper focuses specifically on source-free domain adaptation and does not explore the applicability of the prototype-based approach to other scenarios.
- What evidence would resolve it: Experiments applying the prototype-based method to scenarios like semi-supervised domain adaptation or domain generalization, and comparing results to existing methods.

### Open Question 3
- Question: How sensitive is the PDA method to the choice of distance measure for finding the closest prototype?
- Basis in paper: [explicit] The authors mention selecting cosine distance for finding the closest prototype but do not explore other distance measures.
- Why unresolved: The paper uses cosine distance without comparing it to other possible distance measures like Euclidean or Mahalanobis distance.
- What evidence would resolve it: Experiments comparing the performance of PDA using different distance measures on standard benchmarks to determine the optimal choice.

## Limitations
- Prototype construction sensitivity: The quality of prototypes depends heavily on the initial pseudo-labels from the pre-trained model. If the model is poorly calibrated or makes systematic errors on the target domain, prototypes will be misleading, limiting accuracy gains.
- Domain shift assumptions: The method assumes the feature extractor generalizes reasonably well under domain shift. Severe feature space misalignment between source and target domains can break the cosine distance-based nearest-prototype classifier.
- MCD extension complexity: While RoG with MCD can improve pseudo-labels, it adds computational overhead and requires careful parameter tuning. The paper notes MCD assumes Gaussian class distributions, which may not hold in practice.

## Confidence
- High confidence: The feed-forward nature of PDA and its speed advantage over SHOT methods (confirmed by ablation times in Table 1).
- Medium confidence: The effectiveness of confidence-weighted averaging for prototype construction (plausible but depends on calibration quality).
- Medium confidence: The MCD extension's improvement on pseudo-labels (supported by results but MCD assumptions may not always hold).

## Next Checks
1. **Pseudo-label quality audit**: Measure the accuracy of pseudo-labels from the pre-trained model on a small labeled subset of the target domain to assess prototype reliability.
2. **Feature space analysis**: Visualize t-SNE embeddings of target features with prototypes overlaid to verify that cosine distances correlate with semantic similarity.
3. **MCD sensitivity test**: Run PDA with and without MCD refinement on a domain pair where Gaussian class assumptions are likely violated (e.g., Art→Clipart) to assess robustness.