---
ver: rpa2
title: 'PHD: Pixel-Based Language Modeling of Historical Documents'
arxiv_id: '2310.18343'
source_url: https://arxiv.org/abs/2310.18343
tags:
- historical
- dataset
- language
- scans
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces PHD, a pixel-based language model for historical
  document analysis that bypasses the need for OCR. The model is pretrained on synthetic
  historical scans and real 18th-19th century Caribbean newspapers.
---

# PHD: Pixel-Based Language Modeling of Historical Documents

## Quick Facts
- arXiv ID: 2310.18343
- Source URL: https://arxiv.org/abs/2310.18343
- Reference count: 27
- Primary result: PHD achieves up to 75% accuracy on historical QA tasks while bypassing OCR

## Executive Summary
PHD introduces a novel pixel-based language model for historical document analysis that operates directly on image data rather than requiring OCR conversion. By pretraining on synthetic historical scans and real 18th-19th century Caribbean newspapers, PHD learns to reconstruct masked image patches while developing strong language understanding capabilities. The model demonstrates superior performance on the GLUE benchmark compared to other pixel-based approaches and achieves promising results on historical QA tasks using the Runaways Slaves in Britain dataset, showing particular robustness to noise and degradation common in historical documents.

## Method Summary
PHD uses a ViT-MAE encoder-decoder architecture trained with masked image modeling on historical document images. The model converts text to images using a renderer, then learns to reconstruct randomly masked patches from context. Pretraining combines synthetic scans generated from BookCorpus and Wikipedia with real historical newspaper images. For downstream tasks, the decoder is replaced with classification heads. The approach bypasses OCR entirely, processing documents directly as images, which enables better handling of noise and degradation typical in historical documents.

## Key Results
- Achieves up to 75% accuracy on historical QA task using Runaways Slaves in Britain dataset
- Outperforms other pixel-based models on GLUE benchmark with strong language understanding
- Demonstrates robustness to noise and degradation in historical document processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PHD's training objective of reconstructing masked pixel patches enables it to learn both visual and linguistic representations simultaneously, bypassing the need for OCR.
- Mechanism: By treating text as images and training to predict masked regions, PHD learns to understand both the visual layout and semantic content of historical documents, allowing it to handle noise and degradation that would confuse traditional OCR-based approaches.
- Core assumption: The pixel reconstruction task captures sufficient linguistic information to enable downstream language understanding tasks.
- Evidence anchors:
  - [abstract] "We take advantage of recent advancements in pixel-based language models trained to reconstruct masked patches of pixels instead of predicting token distributions."
  - [section] "The training of PIXEL is analogous to BERT (Devlin et al., 2019). During pretraining, input strings are rendered as images, and the encoder and the decoder are trained jointly to reconstruct randomly masked image regions from the unmasked context."
  - [corpus] Weak - The related papers focus on OCR improvements and historical document processing but don't specifically validate pixel-based language modeling approaches.
- Break condition: If the masked patch reconstruction fails to capture sufficient linguistic information, PHD would perform poorly on downstream language understanding tasks.

### Mechanism 2
- Claim: PHD's ability to directly process image-based historical documents makes it robust to the noise and degradation common in historical scans.
- Mechanism: By learning from pixel-level representations rather than relying on OCR-generated text, PHD can handle noise, blurriness, and other artifacts that typically degrade OCR performance on historical documents.
- Core assumption: Pixel-based representations can capture the same linguistic information as text-based representations, despite noise and degradation.
- Evidence anchors:
  - [abstract] "PHD reconstructs masked image patches with high accuracy and demonstrates strong language understanding, outperforming other pixel-based models on the GLUE benchmark."
  - [section] "Therefore, while LLMs have proven remarkably successful in modern domains, their performance is considerably weaker when applied to historical texts (Manjavacas and Fonteyn, 2022; Baptiste et al., 2021, inter alia)."
  - [corpus] Weak - The related papers focus on OCR improvements and historical document processing but don't specifically validate pixel-based approaches for noise robustness.
- Break condition: If the pixel representations lose too much linguistic information due to noise, PHD would fail to perform language understanding tasks accurately.

### Mechanism 3
- Claim: PHD's pretraining on synthetic historical documents enables it to generalize to real historical documents despite the scarcity of training data.
- Mechanism: By generating synthetic historical documents that mimic the visual characteristics of real historical documents (fonts, layout, noise), PHD can learn to process historical documents without requiring large amounts of real historical data.
- Core assumption: The synthetic data generation process can create realistic enough historical documents to enable meaningful pretraining.
- Evidence anchors:
  - [abstract] "Due to the scarcity of real historical scans, we propose a novel method for generating synthetic scans to resemble real historical documents."
  - [section] "Given the paucity of large, high-quality datasets comprising historical scans, we pretrain our model using a combination of synthetic scans designed to resemble historical documents faithfully."
  - [corpus] Weak - The related papers focus on OCR improvements and historical document processing but don't specifically validate synthetic data generation for pretraining.
- Break condition: If the synthetic data doesn't accurately represent the characteristics of real historical documents, PHD would fail to generalize to real-world data.

## Foundational Learning

- Concept: Masked Image Modeling (MIM)
  - Why needed here: MIM is the core pretraining objective that enables PHD to learn both visual and linguistic representations from image-based text.
  - Quick check question: What is the key difference between MIM and traditional masked language modeling used in BERT?

- Concept: Pixel-based vs. Text-based Language Modeling
  - Why needed here: Understanding the fundamental difference between PHD's pixel-based approach and traditional text-based approaches is crucial for appreciating its advantages in handling historical documents.
  - Quick check question: How does PHD's pixel-based approach address the limitations of OCR-based approaches for historical documents?

- Concept: Synthetic Data Generation for Pretraining
  - Why needed here: PHD's ability to pretrain on synthetic historical documents is crucial for its effectiveness given the scarcity of real historical data.
  - Quick check question: What are the key challenges in generating synthetic historical documents that accurately represent real historical documents?

## Architecture Onboarding

- Component map: Text renderer -> Pixel-based encoder -> Pixel-based decoder -> Classification head
- Critical path:
  1. Text renderer generates image from text
  2. Encoder breaks image into patches and encodes them
  3. Masking is applied to patches
  4. Decoder reconstructs masked patches from context
  5. Classification head for downstream tasks
- Design tradeoffs:
  - Image resolution vs. computational efficiency
  - Synthetic data realism vs. diversity
  - Masking strategy (patch size, density) vs. reconstruction quality
- Failure signatures:
  - Poor reconstruction quality indicates issues with encoder/decoder or training
  - Low downstream task performance indicates insufficient linguistic understanding
  - Inability to handle noise indicates poor generalization from synthetic data
- First 3 experiments:
  1. Test PHD's ability to reconstruct masked patches on synthetic data
  2. Evaluate PHD's performance on GLUE benchmark with clean images
  3. Test PHD's robustness to noise by evaluating on degraded versions of GLUE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal model size and training duration for PHD to achieve maximum performance on historical document analysis tasks?
- Basis in paper: [explicit] The paper mentions that due to limited computational resources, they trained a relatively small-scale model for a limited number of steps, potentially impeding its ability to develop necessary capabilities. They suggest that insufficient computational capacity hindered comprehensive hyperparameter searches.
- Why unresolved: The paper acknowledges computational constraints but does not explore how increased model size or training duration would affect performance. This is a common trade-off in deep learning research between computational resources and model capabilities.
- What evidence would resolve it: Conducting experiments with varying model sizes (e.g., 100M, 500M, 1B parameters) and training durations (e.g., 1M, 2M, 5M steps) on the same historical document tasks, comparing performance metrics like accuracy, F1 scores, and qualitative reconstruction quality.

### Open Question 2
- Question: How can pixel-based language models like PHD be effectively evaluated for their ability to reconstruct masked image regions, especially for blurry completions?
- Basis in paper: [explicit] The paper highlights that evaluating blurry completions is challenging because unlike token-based models, multiple high-likelihood words may overlay and produce blurry completions, making quantitative analysis difficult.
- Why unresolved: Current evaluation methods rely heavily on qualitative inspection, which is subjective and may not capture the full extent of the model's capabilities. The paper calls for better evaluation methods to drive progress in this domain.
- What evidence would resolve it: Developing new evaluation metrics that can quantify the quality of blurry completions, such as measuring the entropy of pixel distributions, comparing against multiple ground truth variations, or using human evaluation studies with systematic scoring rubrics.

### Open Question 3
- Question: Can PHD be extended to handle multiple languages beyond English, particularly low-resource languages with limited OCR systems?
- Basis in paper: [explicit] The paper acknowledges focusing on English, a high-resource language with strong OCR systems, and notes that neglecting low-resource languages limits the model's potential impact where it could be more useful.
- Why unresolved: The paper's pretraining was done on English corpora and real historical English newspapers. Extending to other languages would require multilingual datasets and potentially different synthetic data generation strategies.
- What evidence would resolve it: Training PHD on multilingual historical corpora spanning multiple languages and scripts, then evaluating performance on language-specific historical document tasks. Testing zero-shot transfer to languages not seen during pretraining would also be valuable.

## Limitations

- Synthetic data generation process lacks detailed specifications for font choices and degradation parameters
- Evaluation on historical QA tasks uses a relatively small dataset (Runaways Slaves in Britain)
- Performance generalization to historical documents beyond Caribbean newspapers and 18th-19th century British runaways is uncertain

## Confidence

**High Confidence** (Level 1-2 uncertainty):
- PHD successfully reconstructs masked image patches as a pretraining objective
- The pixel-based approach enables processing of historical documents without OCR
- PHD demonstrates improved robustness to noise compared to text-based models

**Medium Confidence** (Level 3-4 uncertainty):
- PHD's performance on GLUE benchmark accurately reflects language understanding capabilities
- The synthetic data generation process produces sufficiently realistic historical documents
- PHD's 75% accuracy on historical QA represents robust real-world performance

**Low Confidence** (Level 5-6 uncertainty):
- PHD generalizes to historical documents beyond Caribbean newspapers and 18th-19th century British runaways
- The pixel-based approach scales to modern document types with different layouts and fonts
- PHD's advantages over OCR-based approaches remain significant in practical applications

## Next Checks

1. **Synthetic Data Realism**: Generate synthetic historical documents with varying font families, degradation levels, and layout parameters, then evaluate PHD's performance sensitivity to these variations to determine which factors most impact downstream task performance.

2. **Cross-Dataset Generalization**: Test PHD on additional historical document collections from different time periods, geographic regions, and languages to assess whether the 75% QA accuracy on Runaways Slaves in Britain generalizes to other historical contexts.

3. **OCR Comparison**: Implement a controlled comparison where PHD and state-of-the-art OCR systems process the same historical documents, measuring both accuracy and computational efficiency across documents with varying levels of noise and degradation.