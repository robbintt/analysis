---
ver: rpa2
title: 'LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking'
arxiv_id: '2311.02089'
source_url: https://arxiv.org/abs/2311.02089
tags:
- recommendation
- llamarec
- arxiv
- language
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LlamaRec, a two-stage framework for LLM-based
  ranking-based recommendation. It uses small sequential recommenders to retrieve
  candidates efficiently, then leverages a carefully designed prompt template and
  verbalizer approach to rank candidates without autoregressive generation, significantly
  improving inference efficiency.
---

# LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking

## Quick Facts
- arXiv ID: 2311.02089
- Source URL: https://arxiv.org/abs/2311.02089
- Authors: 
- Reference count: 40
- This paper presents LlamaRec, a two-stage framework for LLM-based ranking-based recommendation that achieves significant improvements in both recommendation performance and efficiency.

## Executive Summary
LlamaRec introduces a two-stage framework for LLM-based ranking in sequential recommendation, combining fast ID-based retrieval with LLM-based ranking. The approach uses a verbalizer-based ranking mechanism that avoids autoregressive generation, enabling efficient ranking without generating long text. By leveraging parameter-efficient fine-tuning and response-only loss computation, LlamaRec achieves 24.22% improvement on NDCG@10 and 14.31% average gain across all metrics compared to state-of-the-art baselines while maintaining sub-second inference times regardless of title length.

## Method Summary
LlamaRec employs a two-stage architecture where a linear recurrence-based LRURec retriever first identifies top-k candidate items from user interaction history, then a fine-tuned Llama 2 LLM ranks these candidates using a verbalizer-based approach. The method constructs prompts combining user history and candidate items with index letters, then applies instruction tuning with QLoRA to optimize only the response section (label tokens) for efficiency. This design avoids autoregressive generation while maintaining ranking accuracy, with inference time consistently under 1 second even for long titles.

## Key Results
- LlamaRec achieves 24.22% improvement on NDCG@10 and 14.31% average gain across all metrics on the Beauty dataset
- Inference time remains under 1 second regardless of title length, compared to over 56 seconds for generation-based approaches
- The verbalizer-based approach significantly outperforms autoregressive generation baselines in both accuracy and efficiency metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The verbalizer-based ranking approach avoids autoregressive generation, leading to significant inference speedup.
- Mechanism: Instead of generating text tokens autoregressively, the LLM directly outputs logits over a fixed set of index letters. The verbalizer maps these logits to ranking scores for candidate items in a single forward pass.
- Core assumption: The LLM head's output logits over index letters can effectively represent relative ranking preferences between candidate items.
- Evidence anchors:
  - [abstract]: "we adopt a verbalizer-based approach that transforms output logits into probability distributions over the candidate items. Therefore, the proposed LlamaRec can efficiently rank items without generating long text."
  - [section]: "we propose to leverage a simple verbalizer that efficiently transforms the output from the LLM head (i.e., output scores over all tokens) to ranking scores over candidate items."
  - [corpus]: Weak evidence. No corpus papers directly discuss this specific verbalizer mechanism for LLM-based ranking.

### Mechanism 2
- Claim: The two-stage retrieval-then-ranking architecture improves both efficiency and accuracy by limiting the LLM's ranking task to a smaller candidate set.
- Mechanism: A fast ID-based sequential recommender (LRURec) retrieves the top-k candidates. The LLM then only ranks these k items instead of the entire item space, reducing computational load while maintaining accuracy.
- Core assumption: LRURec can retrieve a candidate set that contains the relevant ground truth item with high probability, making LLM ranking effective on this subset.
- Evidence anchors:
  - [abstract]: "we use small-scale sequential recommenders to retrieve candidates based on the user interaction history. Then, both history and retrieved items are fed to the LLM in text via a carefully designed prompt template."
  - [section]: "Since LlamaRec is designed with a two-stage framework, it is possible to select arbitrary model for the retrieval stage. In this work, we adopt the linear recurrence-based LRURec as our retrieval modelùíáretriever."
  - [corpus]: Weak evidence. The corpus neighbors discuss retrieval-augmented approaches but don't specifically address this two-stage ranking setup.

### Mechanism 3
- Claim: Fine-tuning only the response section of the prompt improves training efficiency without sacrificing performance.
- Mechanism: The loss is computed only on the label tokens (index letters and EOS) rather than the entire prompt, reducing computation while still allowing the LLM to learn ranking preferences.
- Core assumption: Optimizing only the response section provides sufficient gradient signal for the LLM to learn effective ranking behavior.
- Evidence anchors:
  - [section]: "we apply instruction tuning and optimize the model on the response section of the prompt. That is, we only compute loss for label tokens (i.e., index letters and EOS token) in the prompt for each data example."
  - [section]: "This is because optimizing on the entire input does not yield further improvements, while reducing the loss computation to label section is slightly more efficient in training."
  - [corpus]: No direct corpus evidence for this specific optimization approach.

## Foundational Learning

- Concept: Large Language Model Fine-tuning and Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: LlamaRec needs to adapt a pre-trained LLM (Llama 2) to the ranking task without full fine-tuning of all parameters.
  - Quick check question: What is the difference between full fine-tuning and parameter-efficient fine-tuning (like QLoRA) in terms of trainable parameters and memory usage?

- Concept: Sequential Recommendation and Retrieval-Ranking Paradigm
  - Why needed here: The paper builds on sequential recommendation, where user behavior history is used to predict next items, and uses a two-stage retrieval-ranking approach common in large-scale recommendation systems.
  - Quick check question: Why do large-scale recommendation systems often use a two-stage approach (retrieval + ranking) instead of directly ranking all items?

- Concept: Prompt Engineering and Instruction Tuning
  - Why needed here: LlamaRec uses carefully designed prompts with instructions to guide the LLM's ranking behavior, requiring understanding of how to structure prompts effectively.
  - Quick check question: How does instruction tuning differ from standard fine-tuning, and why might it be particularly useful for adapting LLMs to recommendation tasks?

## Architecture Onboarding

- Component map: User history ‚Üí LRURec retrieval ‚Üí Prompt construction ‚Üí Llama 2 forward pass ‚Üí Verbalizer mapping ‚Üí Ranking scores
- Critical path: User history ‚Üí LRURec retrieval ‚Üí Prompt construction ‚Üí Llama 2 forward pass ‚Üí Verbalizer mapping ‚Üí Ranking scores
- Design tradeoffs:
  - Using ID-based retriever vs. text-based retriever: ID-based is faster but may miss semantically relevant items
  - Ranking top-k vs. all items: Faster but depends on retriever quality
  - Response-only loss vs. full prompt loss: More efficient but may miss some learning signals
- Failure signatures:
  - Slow inference: Likely autoregressive generation instead of verbalizer approach
  - Poor ranking accuracy: Retriever failing to include relevant items, or LLM not learning ranking preferences
  - Memory issues: Candidate set too large, or not using parameter-efficient fine-tuning
- First 3 experiments:
  1. Test retriever performance: Verify LRURec retrieves ground truth items within top-k candidates
  2. Test verbalizer approach: Compare inference time of verbalizer-based ranking vs. autoregressive generation
  3. Test prompt design: Experiment with different prompt templates and response-only vs. full-prompt loss computation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LlamaRec scale with larger LLM models (e.g., Llama 2 70B vs 7B) in terms of recommendation accuracy and efficiency trade-offs?
- Basis in paper: [explicit] The paper uses Llama 2 7B and discusses efficiency improvements, but does not explore scaling to larger models
- Why unresolved: The paper only evaluates the 7B variant and doesn't examine how performance/efficiency trade-offs change with model size
- What evidence would resolve it: Experimental results comparing different sized LLMs (7B, 13B, 34B, 70B) on the same datasets measuring both accuracy metrics (MRR, NDCG, Recall) and inference times

### Open Question 2
- Question: What is the impact of different retriever models (beyond LRURec) on the overall performance of the two-stage LlamaRec framework?
- Basis in paper: [explicit] The paper states "it is possible to select arbitrary model for the retrieval stage" but only evaluates LRURec
- Why unresolved: Only one retriever model is tested, leaving questions about whether alternative retrieval approaches could yield better performance
- What evidence would resolve it: Comparative experiments using different sequential recommenders (e.g., SASRec, NARM) as retrievers while keeping the same ranking stage

### Open Question 3
- Question: How does LlamaRec perform in cold-start scenarios where users have minimal interaction history?
- Basis in paper: [inferred] The paper focuses on sequential recommendation with established user histories but doesn't address cold-start scenarios
- Why unresolved: The methodology relies on user interaction history for both retrieval and ranking, but the paper doesn't discuss how it handles users with few/no interactions
- What evidence would resolve it: Experiments evaluating LlamaRec performance on users with varying amounts of interaction history, including those with only 1-2 interactions

## Limitations

- Dataset Bias and Generalizability: Experiments conducted on three specific datasets with fixed preprocessing limits understanding of performance on diverse domains or different sparsity patterns.
- Verbalizer Design Uncertainty: Limited details on how the mapping from LLM logits to candidate scores is implemented, with theoretical justification remaining under-supported.
- Retrieval-Ranking Dependency: Success critically depends on retriever including ground truth in candidate set, but lacks thorough analysis of cases where this fails.

## Confidence

- High Confidence: The claim that LlamaRec achieves significant performance improvements over baselines (24.22% NDCG@10 on Beauty, 14.31% average gain across metrics) is well-supported by the experimental results.
- Medium Confidence: The mechanism claim that the verbalizer-based approach enables sub-second inference is supported by timing comparison, but lacks detailed ablation studies.
- Low Confidence: The theoretical justification for why response-only loss computation is sufficient for learning ranking preferences is not well-established.

## Next Checks

1. **Retrieval Coverage Analysis**: Conduct an ablation study where you systematically vary the retriever's top-k parameter (10, 20, 50, 100) and measure both retrieval recall and downstream ranking performance to understand the tradeoff between efficiency and accuracy.

2. **Verbalizer Scaling Study**: Test the verbalizer approach's efficiency and accuracy as the number of candidates increases beyond 20, and compare against autoregressive generation baselines under identical conditions to verify the claimed 56-second speedup is consistent across scales.

3. **Prompt Design Ablation**: Implement and compare different prompt template variations (history-first vs. item-first ordering, varying history length, different instruction phrasings) while measuring both ranking performance and training stability to validate the prompt design choices.