---
ver: rpa2
title: On Monotonic Aggregation for Open-domain QA
arxiv_id: '2308.04176'
source_url: https://arxiv.org/abs/2308.04176
tags:
- sources
- knowledge
- such
- table
- retriever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of non-monotonicity in multi-source
  open-domain question answering (QA), where adding more knowledge sources can sometimes
  decrease accuracy. The authors identify that the unified retriever in existing methods
  fails to effectively cover multiple sources, leading to this problem.
---

# On Monotonic Aggregation for Open-domain QA

## Quick Facts
- arXiv ID: 2308.04176
- Source URL: https://arxiv.org/abs/2308.04176
- Reference count: 0
- Key result: JS-HR model achieves 56.3% Exact Match accuracy with text, table, and knowledge base sources, outperforming previous best of 55.1%

## Executive Summary
This paper addresses a fundamental problem in multi-source open-domain question answering (QA): adding more knowledge sources can sometimes decrease accuracy, a phenomenon known as non-monotonicity. The authors identify that the unified retriever used in existing methods fails to effectively cover multiple sources, causing this issue. They propose the Judge-Specialist framework, which uses specialist retrievers/readers for individual sources and a dedicated judge language model to select the final answer. This approach ensures monotonicity, meaning performance does not decrease when adding more sources. Experiments on the Natural Questions dataset demonstrate that the framework outperforms state-of-the-art multi-source QA methods while maintaining robustness against speech recognition noise.

## Method Summary
The Judge-Specialist framework consists of specialist retrievers and readers for each knowledge source (text, tables, knowledge bases) that retrieve relevant passages and generate answer candidates. A unified retriever provides additional context for a dedicated judge model, which evaluates candidates using both unified and specialist contexts. The judge combines probabilities from both context types to select the most trustworthy answer. For resource-constrained scenarios, a low-resource variant (JS-LR) employs multitask learning to share parameters across sources while maintaining near-constant resource requirements. The framework uses beam search for candidate generation, with beam sizes allocated proportionally to specialist performance.

## Key Results
- JS-HR achieves 56.3% Exact Match accuracy with text, table, and knowledge base sources
- Framework ensures monotonicity - performance does not decrease when adding sources
- JS-LR maintains near-constant resource requirements while achieving monotonicity
- Framework demonstrates robustness to speech recognition noise on NQ-speech dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The unified retriever fails to cover multiple sources effectively, causing non-monotonicity in multi-source ODQA.
- Mechanism: By separating specialist retrievers for each source and a judge to aggregate results, the system ensures that each source's strengths are fully utilized without interference from other sources.
- Core assumption: Specialist retrievers can independently retrieve relevant passages from their respective sources with higher accuracy than a unified retriever.
- Evidence anchors:
  - [section] "Our analysis in Section 4.2 shows that the unified retriever fails to cover multiple sources effectively, thereby causing non-monotonicity."
  - [abstract] "We identify the cause, and based on that we propose Judge-Specialist framework."
- Break condition: If the judge model cannot accurately evaluate and select the best answer from the candidates provided by specialist retrievers, monotonicity may still fail.

### Mechanism 2
- Claim: The judge model, using both unified and specialist contexts, can accurately select the final answer, ensuring monotonicity.
- Mechanism: The judge model evaluates answer candidates using two types of contexts - one from the unified retriever and one from the specialist retriever - and combines their probabilities to select the most trustworthy answer.
- Core assumption: The combined probability from both context types provides a more accurate assessment of answer quality than either context type alone.
- Evidence anchors:
  - [section] "We evaluate candidates with both types of context, and combine two LM probabilities for the final selection."
  - [abstract] "Our framework consists of (1) specialist retrievers/readers to cover individual sources, and (2) judge, a dedicated language model to select the final answer."
- Break condition: If the context from either the unified or specialist retriever is significantly worse than the other, the combined probability may not accurately reflect the true answer quality.

### Mechanism 3
- Claim: The Judge-Specialist framework ensures monotonicity in both high-resource and low-resource scenarios.
- Mechanism: By providing two variants (JS-HR and JS-LR), the framework can maintain performance while scaling to different resource constraints.
- Core assumption: Multitask learning in JS-LR can effectively share parameters across sources without significantly degrading performance.
- Evidence anchors:
  - [section] "For low-resource scenarios where JS-HR is not affordable, we employ multitask learning (MTL) to keep resource requirement near constant over the increase of sources."
  - [abstract] "We denote this model as JS-LR in Figure 2, from which we confirm that monotonicity holds for both JS-HR and JS-LR."
- Break condition: If the multitask learning in JS-LR is not effective, the performance may degrade as more sources are added.

## Foundational Learning

- Concept: Dense Passage Retrieval (DPR)
  - Why needed here: DPR is the foundation for the retriever component in the Judge-Specialist framework, enabling efficient retrieval of relevant passages from knowledge sources.
  - Quick check question: What is the main advantage of DPR over traditional retrieval methods like TF-IDF or BM25?

- Concept: Fusion-in-Decoder (FiD)
  - Why needed here: FiD is used as the reader component in both the specialist readers and the judge model, allowing for effective fusion of information from multiple passages.
  - Quick check question: How does FiD differ from other reader architectures in handling multiple retrieved passages?

- Concept: Multitask Learning (MTL)
  - Why needed here: MTL is employed in the JS-LR variant to reduce parameter size while maintaining performance across multiple sources.
  - Quick check question: What is the key principle behind multitask learning that allows it to share parameters effectively?

## Architecture Onboarding

- Component map:
  Specialist retrievers (one per source) -> Specialist readers (one per source) -> Unified retriever -> Judge model (T5-large) -> Final answer selection

- Critical path:
  1. Retrieve candidates from each specialist retriever
  2. Generate answer candidates from specialist readers
  3. Retrieve context from unified retriever
  4. Judge evaluates candidates using both contexts
  5. Select final answer

- Design tradeoffs:
  - JS-HR vs JS-LR: Performance vs resource efficiency
  - Beam size allocation: Higher for better performing specialists
  - Context type: Unified vs specialist for judge evaluation

- Failure signatures:
  - Non-monotonic performance when adding sources
  - Low judge confidence across all candidates
  - High variance in specialist reader performance

- First 3 experiments:
  1. Compare JS-HR performance with and without unified retriever context in judge evaluation
  2. Test different beam size allocation strategies for specialist retrievers
  3. Evaluate JS-LR performance degradation compared to JS-HR across increasing numbers of sources

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Judge-Specialist framework be further optimized to improve performance beyond the current state-of-the-art methods in multi-source open-domain QA?
- Basis in paper: [explicit] The authors mention that the focus of this work is to show the potential of the Judge-Specialist framework, not to optimize the judge/specialist model. They state that any such effort can be orthogonally combined with their framework.
- Why unresolved: The paper does not explore further optimizations of the judge or specialist models within the framework, leaving room for improvement in this area.
- What evidence would resolve it: Comparative experiments showing the performance gains achieved by optimizing the judge/specialist models within the Judge-Specialist framework, compared to the current implementation.

### Open Question 2
- Question: How does the performance of the Judge-Specialist framework scale with an increasing number of knowledge sources, and what are the limitations in terms of scalability?
- Basis in paper: [inferred] The paper discusses the performance of the framework with text, table, and knowledge base sources, but does not explore its behavior with a larger number of sources or the limitations of scalability.
- Why unresolved: The paper does not provide experimental results or analysis on the framework's performance with more than three knowledge sources or discuss any potential limitations in scalability.
- What evidence would resolve it: Experiments demonstrating the framework's performance and scalability with an increasing number of knowledge sources, along with an analysis of any limitations or challenges encountered.

### Open Question 3
- Question: How does the Judge-Specialist framework perform in open-domain QA tasks with knowledge sources beyond text, tables, and knowledge bases, such as images or videos?
- Basis in paper: [inferred] The paper focuses on text, table, and knowledge base sources, but does not explore the framework's performance with other types of knowledge sources like images or videos.
- Why unresolved: The paper does not provide experimental results or analysis on the framework's performance with knowledge sources beyond text, tables, and knowledge bases.
- What evidence would resolve it: Experiments demonstrating the framework's performance in open-domain QA tasks with diverse knowledge sources, including images and videos, along with an analysis of any challenges or limitations encountered.

## Limitations

- Limited evidence for unified retriever coverage analysis - specific quantitative evidence showing how and why the unified retriever fails to cover multiple sources is not fully detailed
- Single dataset evaluation on Natural Questions limits generalizability claims to other question-answering datasets
- Parameter count discrepancy between JS-HR and JS-LR variants not explicitly provided

## Confidence

- High confidence: The claim that the Judge-Specialist framework achieves 56.3% EM accuracy on multi-source QA (compared to 55.1% for UDT-QA) is supported by explicit numerical results
- Medium confidence: The claim about monotonicity being achieved across both JS-HR and JS-LR variants is supported by the paper's findings
- Medium confidence: The claim that the unified retriever fails to cover multiple sources effectively is supported by reference to analysis but specific mechanisms are not detailed

## Next Checks

1. Examine the specific quantitative evidence from Section 4.2 showing how the unified retriever fails to cover multiple sources effectively, including retrieval recall metrics broken down by source type

2. Obtain exact parameter counts for JS-HR and JS-LR models to verify resource efficiency claims, including comparison of parameter counts, training/inference time measurements, and memory usage

3. Evaluate the Judge-Specialist framework on additional open-domain QA datasets beyond Natural Questions (such as WebQuestions or TriviaQA) to verify that the monotonicity property holds across different data distributions and question types