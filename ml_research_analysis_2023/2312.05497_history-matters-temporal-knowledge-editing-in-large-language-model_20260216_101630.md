---
ver: rpa2
title: 'History Matters: Temporal Knowledge Editing in Large Language Model'
arxiv_id: '2312.05497'
source_url: https://arxiv.org/abs/2312.05497
tags:
- knowledge
- editing
- time
- temporal
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of Temporal Knowledge Editing (TKE),
  where models should retain historical knowledge while integrating new knowledge
  due to evolving worldly dynamics. The authors establish a benchmark AToKe to evaluate
  current model editing methods and find that existing methods catastrophically forget
  historical knowledge.
---

# History Matters: Temporal Knowledge Editing in Large Language Model

## Quick Facts
- arXiv ID: 2312.05497
- Source URL: https://arxiv.org/abs/2312.05497
- Authors: 
- Reference count: 28
- Key outcome: Introduces Temporal Knowledge Editing (TKE) task where models must retain historical knowledge while integrating new knowledge; proposes METO framework that edits both historical and new knowledge concurrently and optimizes time prediction for each fact.

## Executive Summary
This paper introduces the task of Temporal Knowledge Editing (TKE), where language models should retain historical knowledge while integrating new knowledge due to evolving worldly dynamics. The authors establish a benchmark AToKe to evaluate current model editing methods and find that existing methods catastrophically forget historical knowledge. To address this, they propose a framework METO that edits both historical and new knowledge concurrently and optimizes the model's prediction for the time of each fact. METO substantially improves the performance of edited models on utilizing historical knowledge while maintaining effectiveness on learning new knowledge.

## Method Summary
The paper evaluates existing knowledge editing methods (CFT, MEND, ROME, MEMIT) on a newly proposed benchmark AToKe, finding they catastrophically forget historical knowledge. The authors then propose METO, a framework that enhances existing editing methods by performing multi-editing on both historical (model-time knowledge) and current knowledge, followed by time objective optimization. METO first extracts the model's internal knowledge using cloze statements, then edits both historical and current facts concurrently while optimizing the model's prediction for the time of each fact.

## Key Results
- Existing model editing methods catastrophically forget historical knowledge when updating with new information
- METO improves performance on utilizing historical knowledge while maintaining effectiveness on learning new knowledge
- METO introduces some knowledge distortion, leading to a 1-5% decrease in performance on current knowledge
- METO's effectiveness may improve as models get larger and base editing methods get stronger

## Why This Works (Mechanism)

### Mechanism 1
- Claim: METO preserves historical knowledge by editing both historical and current facts concurrently
- Mechanism: METO first extracts model-time knowledge (Cm), then performs multi-editing on both historical (Cm) and current (Cm+) knowledge, and optimizes time prediction for each fact
- Core assumption: The model already knows Cm- is history and only needs Cm updated for temporal awareness
- Evidence anchors:
  - [abstract]: "METO edits both historical and new knowledge concurrently and optimizes the model's prediction for the time of each fact"
  - [section 5.1]: "We firstly query the language model with the cloze statement of current knowledge to extract the knowledge under model time, and then edit the model using both of them with timestamps"
  - [corpus]: Weak evidence - METO is a novel approach not found in corpus neighbors
- Break condition: If model-time knowledge extraction fails to correctly identify Cm, historical preservation will be ineffective

### Mechanism 2
- Claim: Time Objective Optimization enhances temporal awareness by editing time spans of knowledge
- Mechanism: After multi-editing, METO uses base editing methods to optimize the probability of time spans (e.g., "2017 to 2021") instead of objects
- Core assumption: Editing time spans helps model better organize knowledge on timeline
- Evidence anchors:
  - [section 5.1]: "we perform an additional task of optimizing the time objective of knowledge... editing target is changed from the object to the corresponding time"
  - [abstract]: "optimizes the model's prediction for the time of each fact"
  - [corpus]: Weak evidence - Time objective optimization is novel approach
- Break condition: If base editing methods cannot effectively modify time span predictions, temporal awareness enhancement will fail

### Mechanism 3
- Claim: Relative time questions are more difficult because they require temporal reasoning
- Mechanism: Explicit time questions only require factual recall while relative time questions need understanding of temporal ordering
- Core assumption: Models can store temporal facts but struggle with temporal reasoning
- Evidence anchors:
  - [section 4.2]: "it is more difficult for the model to answer questions about a piece of knowledge without explicitly providing it with a specific time, using a relative time expression"
  - [abstract]: "while existing model editing methods are effective at making models remember new knowledge, the edited model catastrophically forgets historical knowledge"
  - [corpus]: Moderate evidence - Similar findings in temporal reasoning literature
- Break condition: If model develops better temporal reasoning capabilities, this difficulty gap may narrow

## Foundational Learning

- Concept: Temporal Knowledge Graphs
  - Why needed here: Understanding how facts with timestamps form temporal chains is crucial for grasping TKE task
  - Quick check question: What is the difference between (U.S., President, Trump, 2017, 2021) and (U.S., President, Biden, 2021, N/A)?

- Concept: Model Time vs Real Time
  - Why needed here: Models have internal timeline based on training data, not real-world time
  - Quick check question: If a model trained on 2016 data is asked about current president in 2023, what timeline does it operate on?

- Concept: Knowledge Neuron Localization
  - Why needed here: Understanding how factual knowledge is stored in specific parameters is key to editing methods
  - Quick check question: What is the key insight behind ROME's approach to knowledge editing?

## Architecture Onboarding

- Component map: Base LLM (GPT-J) → Knowledge Extraction → Multi-Editing Framework → Time Objective Optimization → Edited Model
- Critical path: Model-time knowledge extraction → Multi-editing on historical and current knowledge → Time objective optimization
- Design tradeoffs: METO trades computational resources (multiple edits) for better historical knowledge preservation
- Failure signatures: Poor performance on historical questions indicates failed knowledge preservation; poor performance on current questions indicates failed new knowledge integration
- First 3 experiments:
  1. Run single edit on AT OKE-SE with baseline method (ROME) and measure CES, HES, CRS, HRS
  2. Run same edit with METO enhancement and compare performance metrics
  3. Run multiple edits on AT OKE-ME with baseline and METO-enhanced versions, measuring HES* after all edits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the METO framework handle cases where historical knowledge is spread across multiple editing operations in a chain?
- Basis in paper: [explicit] The paper mentions that METO edits both historical and new knowledge concurrently and optimizes the model's prediction for the time of each fact. However, it does not explicitly address how the framework handles cases where historical knowledge is spread across multiple editing operations.
- Why unresolved: The paper focuses on the overall effectiveness of METO in improving performance on utilizing historical knowledge but does not delve into the specific handling of historical knowledge spread across multiple editing operations.
- What evidence would resolve it: A detailed explanation of how METO handles cases where historical knowledge is spread across multiple editing operations, possibly with examples or a step-by-step breakdown of the process.

### Open Question 2
- Question: What are the potential implications of the "knowledge distortion" introduced by METO, and how does it impact the overall effectiveness of the framework?
- Basis in paper: [inferred] The paper mentions that METO introduces some knowledge distortion, which leads to a decrease in performance on current knowledge (1-5%). However, it does not provide a detailed analysis of the potential implications of this distortion and how it impacts the overall effectiveness of the framework.
- Why unresolved: The paper acknowledges the existence of knowledge distortion but does not explore its potential implications or how it affects the overall performance of METO.
- What evidence would resolve it: A thorough analysis of the potential implications of knowledge distortion introduced by METO, including its impact on the framework's overall effectiveness and possible strategies to mitigate or address this distortion.

### Open Question 3
- Question: How does the performance of METO compare to other state-of-the-art knowledge editing methods when evaluated on larger language models?
- Basis in paper: [explicit] The paper mentions that the performance of METO may improve as models get larger and the base editing method gets stronger. However, it does not provide a direct comparison of METO's performance on larger language models with other state-of-the-art knowledge editing methods.
- Why unresolved: The paper suggests that METO's performance may improve on larger models but does not provide empirical evidence or a direct comparison with other methods on such models.
- What evidence would resolve it: Experimental results comparing the performance of METO with other state-of-the-art knowledge editing methods on larger language models, including metrics such as accuracy, computational efficiency, and memory usage.

## Limitations

- METO introduces some knowledge distortion, leading to a 1-5% decrease in performance on current knowledge
- The effectiveness of METO is only evaluated on GPT-J (6B parameters) and may not generalize to larger models
- Lack of ablation studies makes it unclear whether multi-editing or time objective optimization is primarily responsible for performance improvements

## Confidence

- **High confidence**: The observation that existing knowledge editing methods catastrophically forget historical knowledge when new facts are added
- **Medium confidence**: The effectiveness of METO's dual-editing approach (lacks ablation studies)
- **Medium confidence**: The claim that relative time questions are inherently more difficult than explicit time questions

## Next Checks

1. **Ablation study**: Evaluate METO with only multi-editing enabled and then with only time objective optimization enabled to determine which component contributes more to historical knowledge preservation
2. **Model scaling analysis**: Test METO on larger language models (e.g., 175B parameters) to assess whether performance improvements scale with model size or if diminishing returns occur
3. **Temporal alignment validation**: Implement a systematic evaluation of model-time knowledge extraction accuracy by comparing extracted historical facts against ground truth timelines across diverse knowledge domains