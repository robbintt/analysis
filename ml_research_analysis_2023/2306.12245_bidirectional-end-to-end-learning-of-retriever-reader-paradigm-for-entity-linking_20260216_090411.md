---
ver: rpa2
title: Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking
arxiv_id: '2306.12245'
source_url: https://arxiv.org/abs/2306.12245
tags:
- reader
- retriever
- beer2
- entity
- entqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BEER2, a novel end-to-end learning framework
  for the retriever-reader paradigm in entity linking. The proposed method addresses
  the limitations of existing approaches by jointly training the retriever and reader
  in a bidirectional manner, allowing them to learn from each other and improve performance.
---

# Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking

## Quick Facts
- arXiv ID: 2306.12245
- Source URL: https://arxiv.org/abs/2306.12245
- Reference count: 37
- Key outcome: BEER2 achieves state-of-the-art results on entity linking benchmarks across multiple domains and languages

## Executive Summary
This paper introduces BEER2, a novel end-to-end learning framework for the retriever-reader paradigm in entity linking. The proposed method addresses the limitations of existing approaches by jointly training the retriever and reader in a bidirectional manner, allowing them to learn from each other and improve performance. BEER2 achieves state-of-the-art results on benchmarks across multiple domains and languages, including English and Chinese news, medical, speech, and short-text datasets. The key innovation is the use of two bidirectional data flows: (1) Retriever → Reader, where the retriever dynamically provides candidate entities to the reader, and (2) Reader → Retriever, where the reader's span predictions assist the retriever. This approach enables more effective interaction between the retriever and reader, leading to improved entity linking performance.

## Method Summary
BEER2 implements a bidirectional end-to-end training framework where a retriever and reader share a sentence encoder and exchange information through two data flows. The retriever uses both [CLS] representations and span positions from the reader to retrieve candidate entities, while the reader uses these candidates to predict mention spans and link them to entities. The model is trained jointly with Noise Contrastive Estimation for the retriever and span/ranking probability maximization for the reader. Training involves 10 epochs with Adam optimizer, linear decay, and learning rate warming up, using batch sizes of 8 for training and 32 for evaluation.

## Key Results
- BEER2 achieves state-of-the-art InKB Micro F1 scores across multiple domains and languages
- Bidirectional data flows between retriever and reader provide significant performance improvements over unidirectional approaches
- Parameter sharing between retriever and reader encoders reduces GPU memory usage by approximately 33% while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training of retriever and reader with bidirectional data flows enables mutual learning and performance improvement.
- Mechanism: The retriever dynamically provides candidate entities to the reader, while the reader's span predictions guide the retriever's entity selection. This bidirectional feedback loop allows both components to learn from each other's errors and strengths.
- Core assumption: The retriever's candidate selection and the reader's span predictions are sufficiently correlated to enable meaningful cross-component learning.
- Evidence anchors: [abstract] "BEER2 guides the retriever and the reader to learn from each other, make progress together, and ultimately improve EL performance." [section 2.2] "BEER2 contains two data flows in opposite directions: (1) Retriever → Reader. The retriever dynamically gets candidate entities and inputs them into the reader... (2) Reader → Retriever. The reader identifies mentions in the documents and inputs the span position information into the retriever..."

### Mechanism 2
- Claim: Span-based entity retrieval improves retrieval diversity and accuracy compared to using only [CLS] representations.
- Mechanism: The reader's span predictions are used to extract more precise span representations from the input text, which are then used alongside [CLS] representations for entity retrieval. This provides more entity-centric information than [CLS] alone.
- Core assumption: Span representations capture entity-specific semantics better than [CLS] representations for entity linking tasks.
- Evidence anchors: [section 2.1.1] "We also utilize the span positions information predicted by the reader for auxiliary retrieval... we use the span position predicted by the reader to obtain more accurate span representations than [CLS] representations..." [section 3.5.2] "From Table 2, we see that each of the data flows we design individually brings considerable improvements... the span information sent from the reader to the retriever not only effectively assists the work of the retriever, but also makes its own progress in the prediction of the span position."

### Mechanism 3
- Claim: End-to-end training with shared parameters between retriever and reader encoders improves efficiency and performance.
- Mechanism: The retriever's sentence encoder and the reader's encoder share parameters, reducing model complexity while allowing gradient propagation between components. This enables more efficient training and better coordination.
- Core assumption: Parameter sharing between retriever and reader does not significantly compromise their specialized functions while providing computational benefits.
- Evidence anchors: [section 2.1.2] "To enable end-to-end training, we make the reader encoder and the retriever's sentence encoder share parameters." [section 3.5.3] "The setting of BEER2's shared parameters can effectively reduce its demand for GPU memory... for large-scale encoders, the main GPU requires a maximum of 50G of memory when parameters are shared, while the main GPU requires a maximum of about 75G of memory when parameters are not shared."

## Foundational Learning

- Concept: Dense entity retrieval using dual-encoder architecture
  - Why needed here: The retriever needs to efficiently find relevant entities from a large knowledge base based on semantic similarity with the input text
  - Quick check question: How does the dual-encoder architecture compute similarity between input text and entities?

- Concept: Machine reading comprehension for span prediction
  - Why needed here: The reader must identify the exact span positions of mentions in the text and link them to retrieved entities
  - Quick check question: What is the difference between the span probability pspan and ranking probability prank in the reader's output?

- Concept: Bidirectional data flow in multi-component systems
  - Why needed here: The retriever and reader must exchange information in both directions to enable mutual learning and improvement
  - Quick check question: How does the retriever's candidate selection affect the reader's training data in real-time?

## Architecture Onboarding

- Component map: Input text -> Retriever (retrieves candidates) -> Reader (predicts spans and ranks) -> Output entity links
- Critical path: Input text → Retriever (retrieves candidates) → Reader (predicts spans and ranks) → Output entity links
- Design tradeoffs:
  - Parameter sharing reduces memory usage but may limit specialization
  - Span-based retrieval improves accuracy but depends on reader quality
  - End-to-end training enables interaction but increases complexity
- Failure signatures:
  - Retriever Recall@K drops significantly → Check retriever training or span information quality
  - Reader F1 score decreases → Verify bidirectional data flow or shared encoder performance
  - Both components perform poorly → Investigate end-to-end training configuration or data preprocessing
- First 3 experiments:
  1. Evaluate retriever performance (Recall@K) with different K values to find optimal candidate set size
  2. Test reader performance with different threshold (thr) values to optimize span-entity combination filtering
  3. Compare end-to-end trained model vs. pipeline-trained model to verify bidirectional learning benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the bidirectional data flow between retriever and reader affect long-term learning dynamics and stability during training?
- Basis in paper: [explicit] The paper mentions that BEER2 trains the retriever and reader jointly with bidirectional data flows (Retriever → Reader and Reader → Retriever), allowing them to learn from each other. However, the paper doesn't analyze the long-term effects of this interaction on training stability or convergence.
- Why unresolved: The paper focuses on demonstrating performance improvements but doesn't investigate how the bidirectional training affects training dynamics, such as convergence speed, stability, or potential oscillations between the two components.
- What evidence would resolve it: Detailed analysis of training curves, loss landscapes, and convergence behavior for both retriever and reader components when trained bidirectionally versus separately.

### Open Question 2
- Question: What is the impact of different knowledge base sizes on the effectiveness of span-based retrieval compared to [CLS]-based retrieval?
- Basis in paper: [inferred] The paper mentions that BEER2 uses both [CLS] representations and span position information for retrieval, and shows performance improvements. However, it doesn't systematically analyze how this advantage changes with knowledge base size or entity density.
- Why unresolved: The paper only briefly mentions that improvements are greater on datasets with larger knowledge bases, but doesn't provide a detailed analysis of how span-based retrieval scales with knowledge base size or entity distribution.
- What evidence would resolve it: Controlled experiments varying knowledge base sizes while keeping other factors constant, with detailed analysis of retrieval accuracy and efficiency trade-offs.

### Open Question 3
- Question: How does BEER2's performance generalize to cross-lingual entity linking scenarios?
- Basis in paper: [explicit] The paper demonstrates BEER2's effectiveness on English and Chinese datasets, showing language robustness. However, it doesn't explore cross-lingual scenarios where the input text and knowledge base are in different languages.
- Why unresolved: While the paper shows effectiveness across languages, it doesn't investigate whether the bidirectional training approach helps in cross-lingual settings or if it introduces any additional challenges.
- What evidence would resolve it: Experiments testing BEER2 on cross-lingual datasets, comparing performance against monolingual baselines and analyzing any specific challenges or advantages in cross-lingual scenarios.

## Limitations

- The paper lacks detailed analysis of how bidirectional learning affects long-term training dynamics and stability
- Performance improvements may depend on dataset-specific factors and domain similarity between training and test sets
- The shared encoder approach may compromise specialized optimization for either the retriever or reader components

## Confidence

**High Confidence**
- The retriever-reader paradigm with dual-encoder architecture is a well-established approach for entity linking
- End-to-end training with bidirectional data flows is technically feasible and implemented
- The memory efficiency gains from parameter sharing are measurable and significant

**Medium Confidence**
- The specific mechanisms of how bidirectional learning improves performance are theoretically sound but lack direct corpus validation
- The claim of state-of-the-art performance is supported by benchmark results but may depend on dataset-specific factors
- The span-based retrieval approach provides measurable improvements but the conditions for its effectiveness are not fully characterized

**Low Confidence**
- The generalizability of the approach to unseen domains and languages beyond the tested datasets
- The long-term stability and convergence properties of the bidirectional training process
- The impact of hyper-parameter choices (K, threshold) on different dataset characteristics

## Next Checks

1. **Ablation Study on Data Flow Interaction**: Design experiments to measure the combined effect of both Retriever → Reader and Reader → Retriever flows versus running them independently. This will clarify whether the bidirectional learning provides synergistic benefits beyond individual improvements.

2. **Cross-Domain Transfer Analysis**: Test the model on datasets from domains not seen during training to evaluate true generalizability. Compare performance against domain-specific fine-tuning approaches to quantify the benefits of the unified framework.

3. **Memory-Performance Trade-off Evaluation**: Implement and test a version with separate encoders for retriever and reader while keeping all other components identical. Measure the performance difference against the shared-parameter version across different memory constraints to establish the actual cost-benefit relationship.