---
ver: rpa2
title: Kernelised Normalising Flows
arxiv_id: '2307.14839'
source_url: https://arxiv.org/abs/2307.14839
tags:
- flows
- learning
- data
- kernel
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces kernelised normalising flows, replacing neural
  networks with kernel machines in coupling layers of flow-based models. The proposed
  method, called Ferumal flow, leverages kernel functions to compute scaling and translation
  operations in an invertible architecture.
---

# Kernelised Normalising Flows

## Quick Facts
- arXiv ID: 2307.14839
- Source URL: https://arxiv.org/abs/2307.14839
- Reference count: 35
- This work introduces kernelised normalising flows, replacing neural networks with kernel machines in coupling layers of flow-based models.

## Executive Summary
This paper introduces kernelised normalising flows, a novel approach that replaces neural networks with kernel machines in coupling layers of flow-based models. The proposed Ferumal flow achieves competitive or superior performance compared to neural network-based flows while using significantly fewer parameters (up to 93% reduction). The method shows particular strength in low-data regimes and demonstrates faster convergence, making it especially suitable for applications with limited data availability.

## Method Summary
The approach replaces neural network scaling and translation functions in coupling layers with kernel machines, specifically using Squared Exponential kernels with auxiliary points for efficient computation. The architecture follows RealNVP-like structure with alternating random/reversal permutations and ActNorm layers. Training uses Adam optimizer with cosine annealing learning rate decay, and the method is evaluated on synthetic 2D toy datasets and five real-world benchmark datasets from the UCI repository.

## Key Results
- Achieves similar or better log-likelihood scores on benchmark datasets compared to RealNVP and Glow
- Uses up to 93% fewer parameters than baseline neural network-based flows
- Shows particular strength in low-data regimes where traditional flows struggle with over-parameterization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Kernelised flows achieve competitive or superior performance compared to neural network-based flows while using significantly fewer parameters.
- Mechanism: Replacing neural network scaling and translation functions with kernel machines reduces the number of learnable parameters while maintaining expressiveness, particularly in low-data regimes.
- Core assumption: Kernel machines can provide sufficient expressiveness for density estimation tasks while being more parameter-efficient than neural networks.
- Evidence anchors:
  - [abstract] "Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability."
  - [section] "The kernelised flows show particular strength in low-data regimes, where traditional neural network flows struggle with over-parameterisation."
  - [corpus] Weak evidence - corpus lacks specific studies on kernel-based flow performance comparisons.
- Break Condition: If the kernel machine cannot capture the complexity of the data distribution, performance will degrade significantly compared to neural networks.

### Mechanism 2
- Claim: Kernelised flows converge faster than neural network-based flows.
- Mechanism: The use of auxiliary points and their data-dependent initialization provides better starting points for optimization, leading to faster convergence.
- Core assumption: Properly initialized auxiliary points can provide a good approximation of the data structure, leading to faster optimization.
- Evidence anchors:
  - [abstract] "The method is parameter-efficient and converges faster than baseline methods, making it especially suitable for applications with limited data availability."
  - [section] "These findings demonstrate that the Ferumal flow-based architecture exhibits faster convergence compared to the neural network baselines."
  - [corpus] No direct evidence in corpus regarding convergence speed comparisons between kernel and neural network flows.
- Break Condition: If the auxiliary points are not well-chosen or the optimization landscape is particularly challenging, the convergence advantage may not materialize.

### Mechanism 3
- Claim: Kernelised flows generalize better in low-data regimes compared to neural network-based flows.
- Mechanism: The number of parameters in kernelised flows scales more favorably with dataset size, preventing overfitting in scenarios with limited data.
- Core assumption: The parameter efficiency of kernel machines translates to better generalization in low-data settings.
- Evidence anchors:
  - [abstract] "Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability."
  - [section] "In scenarios with low data availability, a flow-based network can easily memorise the entire dataset, leading to an unsatisfactory performance on the test set."
  - [corpus] No direct evidence in corpus regarding generalization performance in low-data regimes.
- Break Condition: If the dataset size increases significantly, the parameter efficiency advantage may diminish, and neural networks could outperform.

## Foundational Learning

- Concept: Normalizing Flows and the Change of Variables Formula
  - Why needed here: Understanding the basic architecture of normalizing flows and how the change of variables formula enables density estimation is crucial for grasping the kernelisation approach.
  - Quick check question: How does the change of variables formula allow normalizing flows to perform exact density estimation?

- Concept: Kernel Methods and Reproducing Kernel Hilbert Spaces (RKHS)
  - Why needed here: The core innovation involves replacing neural networks with kernel machines, so understanding kernel methods and RKHS is essential.
  - Quick check question: What is the reproducing property of RKHS, and how does it enable efficient computation in kernel machines?

- Concept: Coupling Layers in Normalizing Flows
  - Why needed here: The kernelisation is applied specifically to coupling layers, so understanding their structure and invertibility is crucial.
  - Quick check question: How do coupling layers maintain invertibility while allowing for complex transformations?

## Architecture Onboarding

- Component map:
  Input data -> Permutation -> Split into two parts -> Kernel-based scaling and translation of one part -> Recombine -> Repeat for multiple layers
  Key components: Kernel function (Squared Exponential), Auxiliary points, Layer-wise transformations

- Critical path:
  1. Data preprocessing and initialization of auxiliary points
  2. Forward pass through coupling layers with kernel-based transformations
  3. Likelihood computation using the change of variables formula
  4. Backward pass for gradient computation
  5. Parameter update using stochastic gradient descent

- Design tradeoffs:
  - Parameter efficiency vs. expressiveness: Kernel machines offer fewer parameters but may have limited expressiveness compared to deep neural networks
  - Computational cost: Kernel methods can be computationally expensive, especially with large datasets
  - Convergence speed: Kernelised flows may converge faster but could get stuck in local optima more easily

- Failure signatures:
  - Poor likelihood scores on test data despite good training performance (overfitting)
  - Slow convergence or failure to converge
  - Numerical instability in kernel matrix computations

- First 3 experiments:
  1. Implement a simple 2D toy dataset (e.g., moons or pinwheel) and compare likelihood scores with a neural network-based flow
  2. Gradually increase dataset size and observe how parameter efficiency scales
  3. Test on a real-world tabular dataset (e.g., Power or Gas) and compare convergence speed and final performance with RealNVP and Glow baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can kernelised normalising flows be extended to handle ResFlow-like architectures that require explicit control of Lipschitz properties?
- Basis in paper: [inferred] The paper explicitly states that extending their approach to ResFlow-like architectures is left as a direction for future research.
- Why unresolved: ResFlow architectures impose constraints on Lipschitz properties of residual blocks, which are not directly compatible with the kernelised approach.
- What evidence would resolve it: A proof-of-concept implementation showing kernelised ResFlows with controlled Lipschitz properties, along with theoretical analysis of how kernel functions affect the Lipschitz constant.

### Open Question 2
- Question: Can incorporating kernels with strong inductive biases improve the performance of Ferumal flows beyond the current Squared Exponential kernel?
- Basis in paper: [explicit] The paper acknowledges that while they focus on Squared Exponential kernels, incorporating kernels with strong inductive biases may be a promising avenue for future research.
- Why unresolved: The current implementation uses only Squared Exponential kernels without exploring other kernel types that might be better suited for specific data distributions.
- What evidence would resolve it: Comparative experiments using different kernel types (e.g., polynomial, Laplacian, or custom task-specific kernels) on various datasets, demonstrating performance improvements over the Squared Exponential baseline.

### Open Question 3
- Question: How can Ferumal flows be scaled to handle large datasets efficiently while maintaining their parameter efficiency advantages?
- Basis in paper: [explicit] The paper identifies that vanilla Ferumal flows have computational costs quadratic in the number of data points, making them infeasible for large datasets.
- Why unresolved: While the paper proposes using auxiliary points as a solution, it doesn't fully address the scalability challenge for truly large-scale applications.
- What evidence would resolve it: An implementation demonstrating linear or near-linear scaling of Ferumal flows with dataset size, potentially through advanced techniques like random Fourier features or kernel approximations, validated on datasets with millions of samples.

## Limitations
- Computational scalability remains a challenge, with kernel methods typically facing quadratic complexity with dataset size
- Performance on high-dimensional data beyond the tested 2D and UCI benchmark datasets is unexplored
- Limited exploration of alternative kernel functions beyond Squared Exponential kernels

## Confidence
- Parameter efficiency claims: Medium confidence - substantial 93% reduction reported but needs independent verification
- Convergence speed claims: Medium confidence - faster training observed but limited comparative analysis against other modern flow architectures
- Low-data generalization: High confidence - theoretically grounded due to kernel methods being less prone to overfitting than neural networks

## Next Checks
1. Evaluate performance on high-dimensional datasets (e.g., ImageNet, CelebA) to assess computational feasibility and parameter efficiency scaling with dimensionality
2. Systematically test alternative kernel functions (Gaussian, Laplacian, Mat√©rn) to determine the robustness of performance improvements to kernel choice
3. Conduct ablation studies varying the number of auxiliary points to identify the minimum viable configuration for maintaining performance while maximizing parameter efficiency gains