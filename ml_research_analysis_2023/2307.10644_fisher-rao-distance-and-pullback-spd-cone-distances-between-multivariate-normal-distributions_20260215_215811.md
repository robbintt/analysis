---
ver: rpa2
title: Fisher-Rao distance and pullback SPD cone distances between multivariate normal
  distributions
arxiv_id: '2307.10644'
source_url: https://arxiv.org/abs/2307.10644
tags:
- distance
- fisher-rao
- matrix
- geodesics
- normal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a fast method to approximate the Fisher-Rao
  distance between multivariate normal distributions and proposes a novel pullback
  Hilbert cone distance based on embedding the normal manifold into the symmetric
  positive-definite cone. The Fisher-Rao distance is approximated by discretizing
  the geodesic and summing local Jeffreys divergences, with convergence guarantees
  as the discretization increases.
---

# Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions

## Quick Facts
- arXiv ID: 2307.10644
- Source URL: https://arxiv.org/abs/2307.10644
- Reference count: 40
- Primary result: Introduces fast approximation of Fisher-Rao distance and novel pullback Hilbert cone distance for multivariate normal distributions

## Executive Summary
This paper addresses the computational challenge of computing the Fisher-Rao distance between multivariate normal distributions by proposing two novel methods: a discretized geodesic approximation using Jeffreys divergences and a computationally efficient pullback Hilbert cone distance. The authors demonstrate how these distances can be leveraged for clustering and quantization tasks involving Gaussian distributions. Experimental results show that the proposed methods offer a practical trade-off between accuracy and computational efficiency, enabling scalable clustering of multivariate normals in various applications including GMM simplification.

## Method Summary
The paper introduces two complementary approaches to measuring distances between multivariate normal distributions. First, the Fisher-Rao distance is approximated by discretizing the geodesic path between two normals into T segments and summing the square roots of local Jeffreys divergences at each segment, with convergence guaranteed as T increases. Second, a novel pullback Hilbert cone distance is defined by embedding the normal manifold into the symmetric positive-definite cone via a diffeomorphic map, then computing the Hilbert projective distance using only the extreme eigenvalues of a matrix product. For clustering applications, the authors employ VP-trees for nearest neighbor search and miniball algorithms for computing smallest enclosing balls in the manifold, enabling efficient k-centers clustering and GMM quantization.

## Key Results
- Fisher-Rao distance can be approximated by discretizing geodesics and summing Jeffreys divergences, with convergence guarantees as discretization increases
- Pullback Hilbert cone distance is computationally efficient, requiring only extreme eigenvalues of a matrix product
- Clustering multivariate normals using these distances is enabled by VP-trees and smallest enclosing ball approximations
- Experimental results show effectiveness in GMM simplification and quantization tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Fisher-Rao distance can be approximated by discretizing geodesics and summing Jeffreys divergences between successive points.
- Mechanism: Discretize the geodesic between two multivariate normal distributions into T segments, compute the Jeffreys divergence at each segment, take the square root, and sum to approximate the true geodesic length.
- Core assumption: As T increases, the sum converges to the true Fisher-Rao distance because Jeffreys divergence locally approximates the Fisher-Rao metric.
- Evidence anchors:
  - [abstract]: "The Fisher-Rao distance is approximated by discretizing the geodesic and summing local Jeffreys divergences, with convergence guarantees as the discretization increases."
  - [section 2.3]: Explicit formula showing ρ̃T as the sum of √DJ over T segments, and lim T→∞ ρ̃T = ρFR.
  - [corpus]: No direct citations; corpus evidence is weak on this specific approximation method.
- Break condition: If the geodesic parameterization is not smooth or the Jeffreys divergence is not a good local approximation (e.g., for very different distributions), the convergence may fail.

### Mechanism 2
- Claim: The pullback Hilbert cone distance is computationally efficient because it only requires the extreme eigenvalues of a matrix product.
- Mechanism: Embed multivariate normals into the symmetric positive-definite cone via a diffeomorphic map, then compute the Hilbert projective distance using only the minimal and maximal eigenvalues of the matrix product, bypassing full SVD.
- Evidence anchors:
  - [abstract]: "The pullback Hilbert distance is computationally efficient, requiring only the extreme eigenvalues of a matrix product."
  - [section 4]: Definition of pullback Hilbert distance and note that only extreme eigenvalues are needed, and approximation via power method is possible.
  - [corpus]: No direct citations; corpus evidence is weak on this specific computational shortcut.
- Break condition: If the matrix product is ill-conditioned or the power method fails to converge, eigenvalue approximation may be inaccurate.

### Mechanism 3
- Claim: Clustering multivariate normals using Fisher-Rao or pullback Hilbert distances is enabled by nearest neighbor data structures and smallest enclosing ball approximations.
- Mechanism: Use VP-trees for nearest neighbor queries under the metric, and iterative algorithms (e.g., miniball) to compute smallest enclosing balls in the manifold for k-centers clustering.
- Evidence anchors:
  - [section 3.1]: VP-tree is suitable for metric spaces and used for NN queries with Fisher-Rao distance.
  - [section 3.2]: Miniball algorithm described for approximating smallest enclosing balls.
  - [corpus]: No direct citations; corpus evidence is weak on specific VP-tree + miniball clustering combination.
- Break condition: If the metric space properties are violated or the VP-tree search degrades to linear time, clustering efficiency is lost.

## Foundational Learning

- Concept: Riemannian geometry and Fisher-Rao metric on Gaussian manifolds
  - Why needed here: The Fisher-Rao distance is a geodesic distance induced by the Fisher information metric; understanding geodesics and length elements is essential to follow the approximation and pullback methods.
  - Quick check question: What is the infinitesimal Fisher-Rao length element ds²Fisher at a normal distribution (µ, Σ)?

- Concept: Symmetric positive-definite (SPD) cone and its Hilbert metric
  - Why needed here: The pullback Hilbert distance is defined on the SPD cone; knowing the Hilbert projective metric and its geodesics (straight lines) is key to understanding the new distance.
  - Quick check question: How is the Hilbert projective distance between two SPD matrices defined in terms of their eigenvalues?

- Concept: Matrix geometric mean and its inductive computation
  - Why needed here: The Riemannian geodesic on the SPD cone can be expressed via the matrix geometric mean; the arithmetic-harmonic inductive mean is used to compute it efficiently.
  - Quick check question: What is the iterative scheme (arithmetic-harmonic mean) to compute the matrix geometric mean?

## Architecture Onboarding

- Component map: Multivariate normals (µ, Σ) -> Fisher-Rao approx (geodesic + DJ sum) OR Pullback Hilbert (embedding + eigenvalue calc) -> VP-tree NN queries -> Miniball enclosing balls -> k-centers clustering

- Critical path:
  1. For Fisher-Rao approximation: Build geodesic (SVD-based) -> discretize -> compute DJ at each step -> sum square roots
  2. For pullback Hilbert: Embed via f -> compute matrix product inverse -> find extreme eigenvalues -> compute log ratio
  3. For clustering: Build VP-tree with chosen distance -> run NN queries -> update cluster centers via miniball

- Design tradeoffs:
  - Fisher-Rao approximation: More segments T -> better accuracy but higher cost; requires SVD for geodesic
  - Pullback Hilbert: Less accurate than Fisher-Rao but O(1) eigenvalue computation; faster for large-scale
  - Clustering: VP-tree speeds NN queries but worst-case still O(n); miniball is approximate, not exact

- Failure signatures:
  - Fisher-Rao: Divergence does not converge with increasing T (e.g., due to numerical instability)
  - Pullback Hilbert: Extreme eigenvalue estimation fails (e.g., power method diverges)
  - Clustering: VP-tree degrades to linear search; miniball fails to converge

- First 3 experiments:
  1. Implement geodesic discretization with T=10, 50, 100 and verify convergence to known Fisher-Rao distances (e.g., d=1 case)
  2. Compare Fisher-Rao approximation vs pullback Hilbert distance on synthetic MVN sets; check runtime and accuracy
  3. Run k-medoids clustering on a GMM simplification task using both distances; measure simplification quality and speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of the Fisher-Rao distance approximation method scale with the dimensionality of the multivariate normal distributions?
- Basis in paper: [explicit] The paper mentions that the Fisher-Rao distance is approximated by discretizing the geodesic and summing local Jeffreys divergences, with convergence guarantees as the discretization increases.
- Why unresolved: The paper provides a convergence guarantee but does not specify how the rate of convergence depends on the dimensionality of the distributions.
- What evidence would resolve it: Theoretical analysis or experimental results showing the relationship between the convergence rate and the dimensionality of the distributions.

### Open Question 2
- Question: What are the computational trade-offs between using the pullback Hilbert cone distance and the Fisher-Rao distance approximation for clustering tasks?
- Basis in paper: [explicit] The paper introduces the pullback Hilbert cone distance as a computationally light alternative to the Fisher-Rao distance approximation, requiring only the extreme eigenvalues of a matrix product.
- Why unresolved: The paper does not provide a detailed comparison of the computational efficiency and accuracy of the two methods for clustering tasks.
- What evidence would resolve it: Experimental results comparing the computational time and clustering accuracy of both methods across various dataset sizes and dimensionalities.

### Open Question 3
- Question: How does the choice of the parameter 'a' in the diffeomorphic embedding fa affect the performance of the pullback Hilbert cone distance in clustering tasks?
- Basis in paper: [explicit] The paper introduces a family of diffeomorphic embeddings fa : N (d) → P (d + 1) for a ∈ R>0, but does not discuss the impact of 'a' on the performance of the pullback Hilbert cone distance.
- Why unresolved: The paper does not explore the sensitivity of the pullback Hilbert cone distance to the choice of 'a' in the embedding.
- What evidence would resolve it: Experimental results showing the clustering performance of the pullback Hilbert cone distance for different values of 'a' across various datasets.

## Limitations

- Approximation accuracy bounds: The Fisher-Rao distance approximation via discretized Jeffreys divergence lacks explicit error bounds or convergence rate analysis.
- Computational cost characterization: No rigorous time complexity analysis is provided for the Fisher-Rao approximation versus the pullback Hilbert distance.
- Clustering validity in non-Euclidean manifolds: The use of VP-trees and miniball algorithms assumes certain metric properties that are demonstrated empirically but not theoretically proven.

## Confidence

- High confidence: The mathematical formulation of the pullback Hilbert distance and its computational shortcut (using extreme eigenvalues) are clearly specified and internally consistent.
- Medium confidence: The clustering methodology (VP-tree + miniball) is standard for metric spaces, but its effectiveness specifically for the Fisher-Rao and pullback Hilbert metrics on MVN distributions is demonstrated empirically but not theoretically proven.
- Low confidence: The convergence proof sketch for the Fisher-Rao approximation via Jeffreys divergence discretization is mentioned but not detailed.

## Next Checks

1. **Convergence rate experiment**: Systematically vary T in the Fisher-Rao approximation and measure error against ground truth distances (e.g., for d=1 normals where closed-form exists). Plot error vs T to empirically determine convergence rate and practical accuracy.

2. **Runtime benchmarking**: Implement both Fisher-Rao approximation and pullback Hilbert distance for various dimensions d and sample sizes. Measure wall-clock time and memory usage to verify the claimed computational advantage of the pullback method.

3. **VP-tree efficiency validation**: Test VP-tree NN queries under both metrics on synthetic MVN datasets of increasing size. Compare query times against linear search baselines to confirm the claimed efficiency gains for clustering.