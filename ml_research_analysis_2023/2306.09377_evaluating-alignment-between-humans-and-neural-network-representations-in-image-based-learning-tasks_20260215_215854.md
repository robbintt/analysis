---
ver: rpa2
title: Evaluating alignment between humans and neural network representations in image-based
  learning tasks
arxiv_id: '2306.09377'
source_url: https://arxiv.org/abs/2306.09377
tags:
- learning
- representations
- task
- were
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigated how well neural network representations\
  \ align with human learning trajectories in naturalistic tasks. In two experiments\u2014\
  category learning and reward learning\u2014participants successfully identified\
  \ relevant stimulus features within a few trials using realistic images."
---

# Evaluating alignment between humans and neural network representations in image-based learning tasks

## Quick Facts
- arXiv ID: 2306.09377
- Source URL: https://arxiv.org/abs/2306.09377
- Reference count: 0
- Key outcome: Multi-modal neural network representations (text+image) best predict human learning in naturalistic tasks

## Executive Summary
This study investigated how well neural network representations align with human learning trajectories in naturalistic cognitive tasks. Through two experiments involving category learning and reward learning with realistic images, the researchers found that participants could effectively identify relevant stimulus features within just a few trials. The study evaluated 86 pretrained neural network models and discovered that contrastive training with multi-modal data was a key predictor of models that best captured human generalization patterns.

The findings suggest that pretrained neural networks, particularly those trained on both text and imagery like CLIP models, can serve as effective tools for extracting representations that align with human cognition. While the size of training datasets emerged as a primary determinant of alignment with human choices, the study also revealed that intrinsic dimensionality of representations affects different model types in varying ways. These results have implications for both cognitive psychology, by demonstrating that humans can perform learning tasks with naturalistic stimuli, and for machine learning, by providing insights into which model architectures best capture fundamental aspects of human cognition.

## Method Summary
The study employed two behavioral experiments with human participants: a category learning task (91 participants) and a reward learning task (82 participants). Participants made choices among realistic images from the THINGS database to learn either category membership or reward associations. The researchers extracted representations from 86 pretrained neural network models spanning supervised, self-supervised, and multi-modal approaches. Linear models were then trained sequentially on these representations to predict human choices, with performance evaluated using cross-validated negative log likelihood. Representational Similarity Analysis (RSA) was used to compare alignment between different model representations and human-aligned features.

## Key Results
- Multi-modal models (text+image) consistently outperformed uni-modal models in predicting human choices across both experiments
- Training dataset size was identified as the primary determinant of alignment with human behavior
- Intrinsic dimensionality of representations had varying effects on alignment depending on model type
- CLIP models showed the best alignment with human representations among all evaluated models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Humans can generalize from limited trials because they identify relevant stimulus dimensions in high-dimensional spaces.
- Mechanism: The brain extracts sparse, meaningful features from naturalistic images and maps them to task-relevant categories or rewards using simple linear functions.
- Core assumption: Human similarity judgments on the THINGS database are represented in a 49-dimensional embedding that captures task-relevant dimensions.
- Evidence anchors:
  - [abstract] "Participants successfully identified the relevant stimulus features within a few trials, demonstrating effective generalization."
  - [section] "We found that humans learned to do this surprisingly quickly, suggesting that they could identify relevant stimulus dimensions within just a few trials and use this knowledge to guide choices."
  - [corpus] Weak corpus support; only indirect references to similarity judgments and feature extraction.
- Break condition: If the underlying embedding fails to capture human-relevant dimensions, or if tasks require non-linear mappings beyond simple linear functions.

### Mechanism 2
- Claim: Multi-modal (text+image) representations predict human choices better than uni-modal representations.
- Mechanism: Grounding visual features with language provides richer semantic context that better matches human conceptual representations.
- Core assumption: CLIP representations align more closely with human representations than either visual-only or text-only models.
- Evidence anchors:
  - [abstract] "representations from models trained on both text and image data consistently outperformed models trained solely on images."
  - [section] "Our RSA results showed that multi-modal representations were better aligned with visual representations than the generative task features."
  - [corpus] Direct support from corpus: "Connecting Concept Convexity and Human-Machine Alignment in Deep Neural Networks."
- Break condition: If tasks become purely visual without semantic components, or if language alignment is misaligned with task structure.

### Mechanism 3
- Claim: Simple learning strategies (linear models) are effective for modeling human learning in naturalistic tasks.
- Mechanism: When provided with sufficiently rich representations, linear models can capture the essential mapping from features to choices without complex learning rules.
- Core assumption: The representational space contains enough information that simple linear functions can approximate human decision policies.
- Evidence anchors:
  - [abstract] "We trained linear models over several DNN representations to test what kind of DNN representations contain this richness and can predict human behavior."
  - [section] "Our findings have implications both for cognitive psychology... by showing that people can do learning tasks with naturalistic stimuli and that we can model these processes."
  - [corpus] Weak support; corpus focuses on deepfake detection and forgetting curves rather than simple linear modeling.
- Break condition: If representations are insufficient or tasks require non-linear decision boundaries.

## Foundational Learning

- Concept: Linear classification and regression models
  - Why needed here: The study uses linear models to predict human choices from neural network representations
  - Quick check question: Can you explain how a logistic regression model predicts category membership from feature vectors?

- Concept: Representational similarity analysis (RSA)
  - Why needed here: Used to compare similarity between different model representations and human-aligned features
  - Quick check question: What does a high CKA (Centered Kernel Alignment) score between two representations indicate?

- Concept: Self-supervised vs supervised learning paradigms
  - Why needed here: The study distinguishes between these training approaches and their effects on representation quality
  - Quick check question: How does contrastive learning in CLIP differ from supervised classification training?

## Architecture Onboarding

- Component map: THINGS database images → representation extraction → linear modeling → human choice prediction
- Critical path: 
  1. Extract representations from all models for task stimuli
  2. Train linear models on each representation to predict human choices
  3. Compare model performance using cross-validated negative log likelihood
  4. Analyze representational similarity to understand alignment patterns
- Design tradeoffs:
  - Computational cost vs representation quality: Larger models like CLIP-ViT-L/14 perform better but require more resources
  - Feature dimensionality: PCA reduction to 49 dimensions matches task embedding but may lose information
  - Training objective alignment: Multi-modal models outperform uni-modal despite task being purely visual
- Failure signatures:
  - Poor model performance indicates either inadequate representations or that human choices depend on factors not captured in features
  - High variance in model predictions suggests instability in representation extraction or insufficient training data
  - Systematic biases in predictions reveal misalignment between model features and human-relevant dimensions
- First 3 experiments:
  1. Reproduce main results with subset of models to validate implementation
  2. Test PCA dimension sensitivity by varying number of components retained
  3. Evaluate impact of different linear model regularization parameters on prediction accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms or representations within CLIP models enable their superior alignment with human behavior compared to uni-modal models?
- Basis in paper: [explicit] The paper states that "representations from models trained on both text and image data consistently outperformed models trained solely on images" and that "language-aligned visual representations possess sufficient richness to describe human generalization in naturalistic settings."
- Why unresolved: While the paper demonstrates that CLIP models outperform others, it does not investigate the specific features or mechanisms within CLIP that drive this superior performance.
- What evidence would resolve it: Detailed analyses comparing the representational features of CLIP models to other models, particularly focusing on how multi-modal training affects feature extraction and generalization.

### Open Question 2
- Question: How do the effects of intrinsic dimensionality of representations vary across different model types in naturalistic learning tasks?
- Basis in paper: [explicit] The paper states that "intrinsic dimensionality of representations had different effects on alignment for different model types."
- Why unresolved: The paper identifies this as an effect but does not explore the specific nature of these varying effects across different model types.
- What evidence would resolve it: Systematic studies examining how different dimensionalities of representations affect performance across various model architectures and task types.

### Open Question 3
- Question: What are the limitations of using simple linear learning strategies to model human learning in naturalistic settings, and how can these models be improved?
- Basis in paper: [explicit] The paper states that "simple learning strategies can be very effective when modeling human learning in naturalistic cognitive tasks" but also suggests this could be extended.
- Why unresolved: While the paper demonstrates effectiveness, it does not explore the limitations or potential improvements to these simple models.
- What evidence would resolve it: Comparative studies testing the performance of linear models against more complex models across various naturalistic tasks and identifying specific scenarios where simple models fail.

## Limitations

- Reliance on linear models may oversimplify the complex, potentially non-linear processes underlying human learning
- The 49-dimensional embedding derived from human similarity judgments represents a specific conceptualization of "human-relevant" dimensions that may not generalize to all cognitive tasks
- The study focuses on pretrained models without examining how fine-tuning on task-specific data might improve alignment

## Confidence

- High confidence: The empirical finding that multi-modal models outperform uni-modal models in predicting human choices across both experiments
- Medium confidence: The conclusion that training dataset size is the primary determinant of model alignment with human behavior, given the correlational nature of this analysis
- Medium confidence: The generalizability of findings across different cognitive tasks, as the study examines only category learning and reward learning

## Next Checks

1. Test whether non-linear models (e.g., small MLPs) significantly outperform linear models on the same representations, which would challenge the paper's core assumption about representation sufficiency
2. Conduct a systematic ablation study varying the dimensionality of PCA reduction to determine the optimal feature space size for human prediction
3. Evaluate model alignment on additional cognitive tasks beyond category and reward learning to assess generalizability of the findings