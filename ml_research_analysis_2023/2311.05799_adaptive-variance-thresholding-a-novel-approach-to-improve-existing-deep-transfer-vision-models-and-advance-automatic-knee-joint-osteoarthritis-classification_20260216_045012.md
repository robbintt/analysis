---
ver: rpa2
title: 'Adaptive Variance Thresholding: A Novel Approach to Improve Existing Deep
  Transfer Vision Models and Advance Automatic Knee-Joint Osteoarthritis Classification'
arxiv_id: '2311.05799'
source_url: https://arxiv.org/abs/2311.05799
tags:
- learning
- data
- layer
- deep
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of knee-joint osteoarthritis
  (KOA) classification, which requires extensive datasets that are difficult to obtain
  due to medical data collection restrictions. The authors propose a novel approach
  using adaptive variance thresholding (AVT) followed by Neural Architecture Search
  (NAS) to improve existing deep transfer vision models.
---

# Adaptive Variance Thresholding: A Novel Approach to Improve Existing Deep Transfer Vision Models and Advance Automatic Knee-Joint Osteoarthritis Classification

## Quick Facts
- arXiv ID: 2311.05799
- Source URL: https://arxiv.org/abs/2311.05799
- Reference count: 40
- Key outcome: AVT improves KOA classification by reducing input space up to 60-fold, achieving 71.14% accuracy

## Executive Summary
This paper addresses the challenge of knee-joint osteoarthritis (KOA) classification, which requires extensive datasets that are difficult to obtain due to medical data collection restrictions. The authors propose a novel approach using adaptive variance thresholding (AVT) followed by Neural Architecture Search (NAS) to improve existing deep transfer vision models. AVT filters out less relevant features from pre-trained models, reducing the input vector space by up to 60-fold. NAS then optimizes the architecture for the refined feature space. When applied to an external model, the approach improved average accuracy to 71.14%, making it one of the top three KOA classification models. The method shows promise for enhancing KOA diagnosis using deep learning techniques.

## Method Summary
The proposed approach combines Adaptive Variance Thresholding (AVT) with Neural Architecture Search (NAS) to improve KOA classification using pre-trained deep learning models. AVT calculates the variance of each feature in the pre-trained model's output, then sets a threshold at a chosen percentile of those variances. Features below the threshold are removed, reducing dimensionality and eliminating near-static features that do not contribute to distinguishing KL grades. After AVT reduces the feature space, NAS searches for the optimal neural network architecture for the refined feature set. This two-stage approach first improves feature quality, then finds the best architecture to leverage those features. The method was tested on knee X-ray images from the Osteoarthritis Initiative (OAI), preprocessed to 224x224 pixels, and classified using the Kellgren and Lawrence (KL) grading system.

## Key Results
- AVT reduces input vector space dimensionality by up to 60-fold while maintaining or improving classification accuracy
- When applied to an external VGG19 model, the AVT+NAS approach improved average accuracy to 71.14%
- The method achieves classification performance comparable to top KOA models while reducing computational complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AVT improves KOA classification by removing low-variance features that add noise rather than signal to the classifier.
- Mechanism: AVT calculates the variance of each feature in the pre-trained model's output, then sets a threshold at a chosen percentile of those variances. Features below the threshold are removed, reducing dimensionality and eliminating near-static features that do not contribute to distinguishing KL grades.
- Core assumption: Features with low variance are less relevant for KOA classification, and their removal improves model performance by focusing on more discriminative features.
- Evidence anchors:
  - [abstract] "AVT filters out less relevant features from pre-trained models, reducing the input vector space by up to 60-fold."
  - [section] "Variance Thresholding is a popular feature selection method used to enhance machine learning model performance by removing excess features."
- Break condition: If the dataset contains features where relevant information is encoded in low-variance signals, AVT could remove useful features and degrade performance.

### Mechanism 2
- Claim: Combining AVT with Neural Architecture Search (NAS) optimizes both feature selection and classifier architecture for the specific KOA task.
- Mechanism: After AVT reduces the feature space, NAS searches for the optimal neural network architecture for the refined feature set. This two-stage approach first improves feature quality, then finds the best architecture to leverage those features.
- Core assumption: The optimal classifier architecture depends on the specific characteristics of the input feature space, and reducing feature space dimensionality enables more efficient NAS by reducing the search space.
- Evidence anchors:
  - [abstract] "AVT followed by Neural Architecture Search (NAS). This approach led to two key outcomes: an increase in the initial accuracy of the pre-trained KOA models and a 60-fold reduction in the NAS input vector space"
  - [section] "Feature selection refining classifier input and NAS automating the architecture design process, identifying optimal structures often missed in manual design, thus increasing potential model efficiency and accuracy."
- Break condition: If the NAS search space is not well-matched to the AVT-reduced feature space, or if the search budget is insufficient, NAS may not find a better architecture than a well-designed manual one.

### Mechanism 3
- Claim: The adaptive nature of AVT, which sets thresholds based on dataset-specific variance percentiles rather than fixed values, makes it more effective than static thresholding methods.
- Mechanism: Instead of manually setting a variance threshold, AVT automatically determines the threshold from the p-th percentile of feature variances in the current dataset. This adapts to different datasets and feature vector characteristics.
- Core assumption: The optimal variance threshold varies across datasets and feature vectors, and a percentile-based adaptive approach generalizes better than fixed thresholds.
- Evidence anchors:
  - [section] "Variance Thresholding is a popular feature selection method... However, the conventional method requires manually setting the threshold, which is not adaptable to different datasets... To overcome this, we propose Adaptive Variance Thresholding (AVT), which automatically sets the threshold from a user-defined percentile of the calculated feature variances."
  - [corpus] No direct evidence in corpus about adaptive thresholding, but related works focus on data augmentation and ensemble methods rather than feature selection approaches.
- Break condition: If the variance distribution across features is similar across datasets, the adaptive approach may offer no advantage over a well-chosen fixed threshold.

## Foundational Learning

- Variance and feature selection: Why needed here: Understanding variance as a measure of feature importance is crucial for grasping how AVT works to filter features. Quick check question: If a feature has zero variance across all samples, what does that tell you about its usefulness for classification?
- Neural Architecture Search: Why needed here: NAS is the second component of the proposed approach, and understanding how it automates architecture design is essential. Quick check question: What is the key difference between manual architecture design and NAS?
- Kellgren-Lawrence grading system: Why needed here: This is the classification system used for KOA in this study, so understanding its grades is important for interpreting results. Quick check question: How many KL grades are used to classify KOA severity?

## Architecture Onboarding

- Component map: Preprocess images -> Extract features with pre-trained CNN -> Apply AVT -> Run NAS on AVT-reduced features -> Evaluate classifier performance
- Critical path: Preprocess images → Extract features with pre-trained CNN → Apply AVT → Run NAS on AVT-reduced features → Evaluate classifier performance
- Design tradeoffs: AVT reduces dimensionality but may remove some useful features; NAS finds optimal architecture but is computationally expensive; choosing AVT percentile threshold involves balancing dimensionality reduction against information loss
- Failure signatures: If AVT threshold is too high, classifier performance may drop due to loss of important features; if NAS search space is too constrained, it may not find optimal architectures; if evaluation uses only accuracy, class imbalance effects may be masked
- First 3 experiments:
  1. Run the baseline model without AVT or NAS to establish performance baseline
  2. Apply AVT with different percentile thresholds (low, mid, high) and measure dimensionality reduction and impact on baseline accuracy
  3. For the best-performing AVT setting, run NAS to find optimized classifier architecture and compare performance to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the Adaptive Variance Thresholding (AVT) method perform with different percentile thresholds beyond the three tested (1.5%, 50%, and 98.5%)?
- Basis in paper: [explicit] The paper mentions that conducting an exhaustive search of AVT thresholds was computationally demanding and only tested three levels.
- Why unresolved: The computational cost of testing more thresholds limits exploration of optimal settings.
- What evidence would resolve it: Results from testing a wider range of AVT percentile thresholds to identify potential performance improvements.

### Open Question 2
- Question: How does the resolution size of input images affect the performance of the proposed AVT and NAS approach?
- Basis in paper: [inferred] The paper notes that the evaluation was limited to transfer learning architectures and a resolution size of 224 x 224, raising questions about scalability.
- Why unresolved: The study did not explore how different resolution sizes might impact model performance.
- What evidence would resolve it: Comparative results using various image resolution sizes to assess the impact on model accuracy and efficiency.

### Open Question 3
- Question: What is the potential for the proposed AVT and NAS methodology to generalize to other medical imaging domains or non-medical applications?
- Basis in paper: [explicit] The authors suggest that the methodology's adaptability could extend to other areas of medicine or different fields using deep transfer learning.
- Why unresolved: The study focused specifically on KOA classification, and broader applications would require further research.
- What evidence would resolve it: Validation of the approach on diverse datasets from different domains to demonstrate its generalizability and effectiveness.

## Limitations

- The study only tested two pre-trained models (EfficientNetV2M and VGG19), limiting generalizability to other architectures
- Computational cost of NAS is significant but not detailed in terms of training time or resources required
- The dataset size (8260 images from 4130 X-rays) is relatively small for deep learning tasks, raising concerns about overfitting

## Confidence

- **High confidence**: The core mechanism of AVT for dimensionality reduction is technically sound and well-explained
- **Medium confidence**: The combination of AVT with NAS shows promising results, but the improvement over baseline models is modest (71.14% accuracy)
- **Low confidence**: The adaptive threshold selection method's superiority over fixed thresholds is claimed but not empirically validated

## Next Checks

1. **Cross-architecture validation**: Test AVT+NAS on additional pre-trained models beyond EfficientNetV2M and VGG19 to verify generalizability
2. **Threshold sensitivity analysis**: Systematically evaluate AVT performance across a range of percentile thresholds (1%, 10%, 25%, 50%, 75%, 90%, 99%) to identify optimal settings
3. **Comparison with alternative feature selection**: Benchmark AVT against other feature selection methods (L1/L2 regularization, mutual information, recursive feature elimination) to establish relative effectiveness