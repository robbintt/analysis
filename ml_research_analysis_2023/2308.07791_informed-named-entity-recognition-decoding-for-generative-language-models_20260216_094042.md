---
ver: rpa2
title: Informed Named Entity Recognition Decoding for Generative Language Models
arxiv_id: '2308.07791'
source_url: https://arxiv.org/abs/2308.07791
tags:
- entity
- language
- named
- recognition
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses named entity recognition (NER) as an extractive
  task, using encoder-only models, which are not compatible with recent generative
  language models (LLMs) that dominate the NLP landscape. The authors propose Informed
  Named Entity Recognition Decoding (iNERD), a method that treats NER as a generative
  process.
---

# Informed Named Entity Recognition Decoding for Generative Language Models

## Quick Facts
- arXiv ID: 2308.07791
- Source URL: https://arxiv.org/abs/2308.07791
- Reference count: 8
- Key outcome: Achieves 94.2% F1 score on CoNLL-2003 using Llama-2-7B with iNERD decoding

## Executive Summary
This paper introduces Informed Named Entity Recognition Decoding (iNERD), a novel approach that treats named entity recognition as a generative task using decoder-only language models. Unlike traditional encoder-only models, iNERD leverages the language understanding capabilities of LLMs while incorporating an informed decoding scheme to prevent hallucinations. The method involves coarse-tuning decoder-only models like Llama and GPT-2 on merged NER datasets, then fine-tuning on individual datasets. The approach demonstrates remarkable adaptability, especially in scenarios with unknown entity class sets, and the authors provide their code and model weights publicly.

## Method Summary
The iNERD approach consists of three main phases: First, multiple NER datasets are merged and used to coarse-tune decoder-only models like GPT-2 and Llama using teacher forcing for 15 epochs. Second, an informed decoding algorithm is implemented to mask impossible tokens during generation, following a strict sequence of entity type token → type-content separator → entity content → entity separator. Finally, the coarse-tuned models are fine-tuned on individual datasets for a few epochs. The method treats NER as structured text generation rather than token classification, and LoRA adaptation is applied for models above 3 billion parameters to manage computational costs.

## Key Results
- Achieves 94.2% F1 score on CoNLL-2003 using Llama-2-7B with iNERD decoding
- Demonstrates strong performance on Few-NERD dataset (68.8% F1) despite having 66 entity types
- Shows superior adaptability to unknown entity class sets compared to traditional NER approaches
- Eliminates hallucinations through informed decoding while maintaining competitive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: iNERD eliminates hallucinations by masking out impossible tokens during generation.
- Mechanism: The decoding algorithm enforces strict rules about which tokens can follow previous ones, preventing the model from generating invalid sequences.
- Core assumption: The masked tokens are truly irrelevant to the task and their removal doesn't affect performance.
- Evidence anchors:
  - [abstract]: "employs an informed decoding scheme incorporating the restricted nature of information extraction into open-ended text generation, improving performance and eliminating any risk of hallucinations"
  - [section]: "This algorithm is implemented as a post-processing step and is executed after the model calculates the score over its vocabulary and before mapping this score to the actual token to be predicted"
  - [corpus]: Weak evidence - only 1 related paper mentions hallucination prevention, others focus on different aspects
- Break condition: If the masking rules are too restrictive and remove valid tokens, or if the model needs flexibility for novel entity types not covered by rules.

### Mechanism 2
- Claim: Coarse-tuning on merged NER datasets improves model performance across different entity types.
- Mechanism: Pre-training the model on diverse NER data helps it understand the general structure of named entities and their relationships.
- Core assumption: The model can generalize from coarse-grained entity types to fine-grained ones across datasets.
- Evidence anchors:
  - [section]: "We theorize that by simply letting the model get exposure to the general structure introduced in Equation 1 it can gather valuable insights and might even understand the link between a coarse-grained entity type like 'Organization' and its fine-grained subtype 'Company'"
  - [section]: "After evaluating the iNERD approach on its capabilities after coarse-tuning, we further fine-tune it on each dataset"
  - [corpus]: Weak evidence - only 1 related paper mentions multi-dataset training, others focus on different approaches
- Break condition: If the entity type vocabularies are too different across datasets, the model may struggle to reconcile them.

### Mechanism 3
- Claim: Casting NER as a generative task leverages the language understanding capabilities of LLMs.
- Mechanism: Instead of token classification, the model generates structured text containing entities, which aligns with how LLMs are pre-trained.
- Core assumption: The structured generation format is compatible with the model's pre-training and doesn't require significant adaptation.
- Evidence anchors:
  - [abstract]: "treats named entity recognition as a generative process. It leverages the language understanding capabilities of recent generative models"
  - [section]: "Compared to the approach introduced in Yan et al. (2021), the essential advantage of our framework for named entity decoding is that we do not add entity type tokens ξ as special tokens, but as regular tokens already known to the model"
  - [corpus]: Moderate evidence - 3 related papers discuss generative approaches to NER, but focus on different methods
- Break condition: If the structured format is too different from the model's pre-training, requiring extensive fine-tuning to adapt.

## Foundational Learning

- Concept: Transformer architecture (encoder-only vs decoder-only)
  - Why needed here: Understanding why decoder-only models are being repurposed for an extractive task
  - Quick check question: What's the key difference between encoder-only and decoder-only transformer architectures?

- Concept: Teacher forcing in sequence generation
  - Why needed here: The training process uses teacher forcing to correct mistakes, but inference requires a different approach
  - Quick check question: Why can't we use teacher forcing during inference in this setup?

- Concept: Named entity recognition tagging schemes (IOB, etc.)
  - Why needed here: Understanding the traditional NER approach being replaced by the generative method
  - Quick check question: What information does the IOB tagging scheme encode that the generative approach needs to capture?

## Architecture Onboarding

- Component map: Input preprocessing → Generative LLM → iNERD decoder → Output parsing
- Critical path: Input → LLM → iNERD decoder → Output parsing
- Design tradeoffs:
  - Simplicity vs. performance: The approach is simple but may not match complex encoder-only models on all datasets
  - Flexibility vs. control: The structured format enables adaptation but may limit creativity
  - Pre-training vs. fine-tuning: Leveraging pre-trained LLMs reduces training time but may limit task-specific optimization
- Failure signatures:
  - Poor performance on datasets with many entity classes (Few-NERD, OntoNotes)
  - Struggles with fine-grained entity distinctions in domain-specific datasets
  - May hallucinate if masking rules are incorrect or incomplete
- First 3 experiments:
  1. Test the iNERD decoder with a pre-trained LLM on a simple NER dataset (CoNLL-2003) to verify the basic pipeline works
  2. Compare coarse-tuning on merged vs. individual datasets to measure the benefit of multi-dataset training
  3. Evaluate the impact of different masking rules in the iNERD decoder on performance and hallucination rates

## Open Questions the Paper Calls Out
1. What is the impact of fine-tuning decoder-only models like Llama on a merged NER dataset compared to fine-tuning on individual datasets?
2. How does the size of the coarse-tuning dataset affect the performance of decoder-only models in NER tasks?
3. How do specialized NER techniques developed for encoder-only models, such as PL-Marker or Co-Regularization, perform when applied to generative language models and iNERD?

## Limitations
- Entity type vocabulary coverage may be insufficient for new domains or languages
- Evaluation lacks detailed error analysis and confusion matrices
- Generalization to significantly different LLM architectures remains unverified
- Scalability to datasets with hundreds of entity types is untested

## Confidence
**High Confidence**: Basic iNERD framework architecture, experimental setup with eight diverse datasets, performance improvements over traditional NER models
**Medium Confidence**: Hallucination elimination mechanism, coarse-tuning strategy effectiveness, adaptability to unknown entity class sets
**Low Confidence**: Future-proof claim for all upcoming LLMs, scalability to complex entity type systems, computational efficiency comparisons

## Next Checks
1. Conduct detailed hallucination analysis with ambiguous entity boundaries and test with unseen entity types
2. Evaluate model performance on completely new domains and languages not represented in training
3. Test approach limits with datasets containing 100+ entity types and measure performance degradation