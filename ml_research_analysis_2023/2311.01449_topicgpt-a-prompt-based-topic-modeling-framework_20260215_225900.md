---
ver: rpa2
title: 'TopicGPT: A Prompt-based Topic Modeling Framework'
arxiv_id: '2311.01449'
source_url: https://arxiv.org/abs/2311.01449
tags:
- topic
- topics
- topicgpt
- document
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TopicGPT is a prompt-based topic modeling framework that uses large
  language models to generate interpretable topics and document assignments from text
  corpora. It iteratively prompts an LLM to generate topics given seed topics and
  documents, then refines the topic list by merging duplicates and removing infrequent
  topics.
---

# TopicGPT: A Prompt-based Topic Modeling Framework

## Quick Facts
- arXiv ID: 2311.01449
- Source URL: https://arxiv.org/abs/2311.01449
- Reference count: 40
- TopicGPT achieves 0.74 harmonic mean purity on Wikipedia topics vs 0.64 for LDA baseline

## Executive Summary
TopicGPT is a prompt-based framework that uses large language models to generate interpretable topics and document assignments from text corpora. The framework iteratively prompts an LLM to generate topics given seed topics and documents, then refines the topic list by merging duplicates and removing infrequent topics. A second LLM stage assigns documents to topics with quoted evidence for interpretability. On two datasets with human-annotated topics, TopicGPT achieves higher harmonic mean purity than strong baselines: 0.74 vs 0.64 for LDA on Wikipedia, and 0.57 vs 0.52 on Congressional bills. Its topics include natural language labels and descriptions, improving interpretability over bag-of-words representations.

## Method Summary
TopicGPT uses two-stage LLM prompting: first generating topics iteratively from seed topics and sampled documents, then refining by merging near-duplicates and removing infrequent ones; second assigning documents to topics with supporting quotes. The framework is adaptable, allowing users to guide topics via seed topics and manual editing without retraining. It uses GPT-4/GPT-3.5-turbo for best results, though open-source models like Mistral-7B-instruct work for assignment but not generation.

## Key Results
- TopicGPT achieves 0.74 harmonic mean purity on Wikipedia topics vs 0.64 for LDA baseline
- TopicGPT achieves 0.57 harmonic mean purity on Congressional bills vs 0.52 for LDA baseline
- Topics include natural language labels and descriptions, improving interpretability over bag-of-words representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative topic generation with LLMs improves topical distinctiveness and coherence
- Mechanism: The LLM iteratively generates topics by either assigning a document to an existing topic or creating a new one, ensuring new topics are distinctive and match the specificity of existing topics
- Core assumption: The LLM can effectively identify semantic gaps in the current topic set and generate new topics that are both relevant and non-redundant
- Evidence anchors:
  - [abstract]: "TopicGPT produces topics that align better with human categorizations compared to competing methods: it achieves a harmonic mean purity of 0.74 against human-annotated Wikipedia topics compared to 0.64 for the strongest baseline"
  - [section]: "The intuition behind this iterative process is that it encourages newly generated topics to be distinctive and also match the specificity seen in other topics"
  - [corpus]: Weak - no direct corpus-level evidence of distinctiveness measurement
- Break condition: If the LLM fails to identify semantic gaps or generates overly specific/fine-grained topics that don't generalize

### Mechanism 2
- Claim: Refinement step eliminates redundancy and improves topic quality
- Mechanism: Uses Sentence-Transformer embeddings to identify topic pairs with cosine similarity ≥ 0.5, then prompts the LLM to merge near-duplicates
- Core assumption: Sentence embeddings can effectively capture semantic similarity between topics, and the LLM can make sensible merging decisions
- Evidence anchors:
  - [abstract]: "The resulting set of topics is then further refined to consolidate redundant topics and eliminate infrequent ones"
  - [section]: "We first use Sentence-Transformer embeddings (Reimers and Gurevych, 2019) to identify pairs of topics with cosine similarity ≥ 0.5"
  - [corpus]: Weak - no corpus-level evidence of how often merging occurs or its impact on final topic quality
- Break condition: If sentence embeddings fail to capture semantic similarity or LLM merges dissimilar topics

### Mechanism 3
- Claim: Quoted evidence improves interpretability and verifiability of topic assignments
- Mechanism: For each document-topic assignment, the LLM provides a direct quote from the document supporting the assignment
- Core assumption: Direct quotes can effectively demonstrate why a document belongs to a particular topic and make assignments verifiable
- Evidence anchors:
  - [abstract]: "These quotations make the method easily verifiable, addressing some of the validity concerns that plague traditional topic models"
  - [section]: "The final output contains the assigned topic label, a document-specific topic description, and a quote extracted from the document to support this assignment"
  - [corpus]: Weak - no corpus-level evidence of how often quotes are used or their effectiveness in practice
- Break condition: If quotes are not representative of the document or fail to clearly support the assignment

## Foundational Learning

- Concept: Topic modeling fundamentals
  - Why needed here: Understanding the limitations of traditional approaches (LDA, BERTopic) helps appreciate why LLM-based methods like TopicGPT were developed
  - Quick check question: What are the main limitations of bag-of-words topic representations in traditional topic models?

- Concept: Prompt engineering for LLMs
  - Why needed here: TopicGPT relies heavily on carefully crafted prompts for topic generation, assignment, and refinement
  - Quick check question: How does iterative prompting with growing seed topics help maintain topic distinctiveness?

- Concept: Clustering evaluation metrics
  - Why needed here: Understanding metrics like purity, NMI, and ARI is crucial for evaluating TopicGPT's performance
  - Quick check question: Why might harmonic mean purity be a better metric than simple purity for evaluating topic assignments?

## Architecture Onboarding

- Component map: Input corpus, seed topics -> Topic generation (LLM prompts) -> Refinement (similarity detection + merging) -> Topic assignment (LLM prompts with self-correction) -> Output: topic labels, descriptions, quotes
- Critical path: Topic generation → Refinement → Topic assignment → Evaluation
- Design tradeoffs: 
  - Cost vs. coverage: Sampling documents for topic generation vs. using entire corpus
  - Open-source vs. closed-source LLMs: Performance vs. cost and transparency
  - Single vs. multi-label assignment: Completeness vs. simplicity
- Failure signatures:
  - Too many/few topics: Indicates issues with generation or refinement thresholds
  - Low alignment scores: Could indicate poor topic generation or assignment
  - High proportion of misaligned topics: Suggests topics aren't capturing intended semantic concepts
- First 3 experiments:
  1. Run TopicGPT with default settings on a small subset of Bills data, compare P1 score to LDA baseline
  2. Test robustness by running with different seed topics and comparing alignment scores
  3. Evaluate open-source LLM (e.g., Mistral) for topic assignment and compare performance to GPT-3.5-turbo

## Open Questions the Paper Calls Out

- **How do the generated topics compare in quality to those produced by other state-of-the-art neural topic models like NVDM or ProdLDA?**
  - Basis in paper: [inferred] The paper compares TopicGPT to LDA and BERTopic but does not evaluate against other neural topic models
  - Why unresolved: The authors focused on comparing to baseline models but did not explore other neural topic modeling approaches
  - What evidence would resolve it: Running TopicGPT and other neural topic models on the same datasets and comparing their topic quality using metrics like coherence, diversity, and human evaluation

- **Can TopicGPT be extended to handle multilingual datasets effectively?**
  - Basis in paper: [explicit] The authors note that TopicGPT has not been evaluated on non-English datasets and that LLM capabilities in non-English languages are degraded
  - Why unresolved: The current experiments are limited to English datasets, and the authors acknowledge this as a limitation
  - What evidence would resolve it: Evaluating TopicGPT on multilingual datasets and comparing its performance to monolingual and multilingual baselines. Testing with different multilingual LLMs

- **How sensitive is TopicGPT's performance to the number and quality of seed topics provided?**
  - Basis in paper: [explicit] The authors mention that users can provide seed topics to guide the generated topics, but they do not systematically explore the impact of different seed topics
  - Why unresolved: The experiments use a fixed number of seed topics, and the impact of varying them is not studied
  - What evidence would resolve it: Running TopicGPT with different numbers and qualities of seed topics and measuring the impact on topic quality and alignment with ground truth

## Limitations
- Reliance on proprietary models (GPT-4, GPT-3.5-turbo) limits accessibility and reproducibility
- Performance of open-source models is significantly worse for topic generation compared to assignment
- Limited evaluation to English datasets, with acknowledged degradation of LLM capabilities in non-English languages

## Confidence
- **High confidence**: The framework's ability to generate interpretable topics with natural language labels and descriptions, supported by clear qualitative examples and human evaluation showing superior interpretability over bag-of-words representations
- **Medium confidence**: Performance improvements on the two benchmark datasets, as results are promising but limited to specific domains and datasets
- **Medium confidence**: Robustness across different prompts and data subsets, though the paper doesn't provide extensive sensitivity analysis or variance metrics

## Next Checks
1. Replicate the Bills dataset experiment with different seed topics (e.g., using only one seed topic instead of three) to test the framework's sensitivity to initialization and verify stability claims
2. Implement the same topic modeling pipeline using Mistral-7B-instruct for both topic generation and assignment, comparing harmonic mean purity to GPT-4 results to quantify the trade-off between performance and accessibility
3. Conduct a systematic ablation study varying the cosine similarity threshold for topic merging (e.g., testing 0.4, 0.6, 0.7) and document frequency thresholds to identify optimal parameter settings and their impact on final topic quality