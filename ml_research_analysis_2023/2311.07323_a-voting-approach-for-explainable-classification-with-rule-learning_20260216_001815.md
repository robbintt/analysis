---
ver: rpa2
title: A Voting Approach for Explainable Classification with Rule Learning
arxiv_id: '2311.07323'
source_url: https://arxiv.org/abs/2311.07323
tags:
- rule
- data
- learning
- voting
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel voting approach that combines rule
  learning methods with state-of-the-art unexplainable ML tools to achieve high accuracy
  while maintaining interpretability. The method addresses the limitation of rule-based
  classifiers, which typically underperform compared to complex models.
---

# A Voting Approach for Explainable Classification with Rule Learning

## Quick Facts
- arXiv ID: 2311.07323
- Source URL: https://arxiv.org/abs/2311.07323
- Reference count: 40
- Primary result: Achieves 97-98% accuracy on MNIST/Fashion-MNIST while maintaining interpretable rule-based explanations

## Executive Summary
This paper introduces a novel voting approach that combines rule learning methods with state-of-the-art unexplainable ML tools to achieve high accuracy while maintaining interpretability. The method addresses the limitation of rule-based classifiers, which typically underperform compared to complex models. By using ensemble learning, the approach leverages the strengths of both interpretable rule learners (e.g., FOIL, RIPPER) and black-box models to resolve prediction conflicts. Experiments on benchmark datasets and an industrial case study on dental bill classification demonstrate significant improvements in accuracy and precision compared to using rule learners alone.

## Method Summary
The voting approach combines multiple rule learners (FOIL and RIPPER) with a black-box decider method to resolve prediction conflicts. The system first trains rule learners on the dataset, then trains a decider model (such as XGBoost or neural networks) on the same data. During prediction, if rule learners agree on a class, that prediction is used with the supporting rule as explanation. If they disagree, the decider's prediction is used, and the system checks if any rule learner predicted the same class to provide partial explanation. The approach also implements a threshold-based partial satisfaction mechanism where rules with partially satisfied conditions can still trigger predictions if they exceed a configurable threshold.

## Key Results
- Achieves 97-98% accuracy on MNIST and Fashion-MNIST datasets
- Significantly outperforms single rule learners on benchmark datasets (Spambase, Heart Disease, Diabetes, COVID-19)
- Maintains interpretable predictions with deterministic rules in most cases
- Successfully handles large-scale industrial dataset with 500,000 instances and 435 attributes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The voting approach improves accuracy by combining rule learners with black-box deciders to resolve prediction conflicts.
- Mechanism: When rule learners produce conflicting predictions, the black-box decider provides a tiebreaker while preserving interpretability when possible.
- Core assumption: Rule learners often make conflicting predictions on complex examples, and a high-accuracy decider can reliably resolve these conflicts.
- Evidence anchors:
  - [abstract] "The method achieves accuracies on par with unexplainable methods while providing explainable predictions justified by deterministic rules."
  - [section 4.2] "the voting approach consults the prediction of the decider and checks if one of the chosen rule learners returns the same class... the resulting class is at least partially justified by a deterministic rule"
  - [corpus] Weak - related papers focus on explainable rule learning but not ensemble-based conflict resolution.
- Break condition: If the black-box decider is inaccurate or produces predictions that don't align with any rule learner, the system may lose both accuracy and interpretability.

### Mechanism 2
- Claim: Partial rule satisfaction with threshold-based fallback enables handling ambiguous multi-class predictions.
- Mechanism: When no rule is fully satisfied, the system computes the percentage of satisfied conditions and uses a threshold (70%) to decide if a partially matching rule is sufficient for prediction.
- Core assumption: Most ambiguous cases differ from rule conditions by only a small fraction of attributes, making partial satisfaction meaningful.
- Evidence anchors:
  - [section 4.2] "we investigate the rule sets in more detail and compute the number of satisfied conditions... choose the prediction from the decider, if there is at least one corresponding rule where the percentage of satisfied conditions exceeds the chosen threshold"
  - [section 5.1] "we choose the threshold to be equal to 0.7... one could draw basically every digit satisfying 50% of the conditions"
  - [corpus] Weak - related work focuses on rule learning but not threshold-based partial satisfaction strategies.
- Break condition: If the threshold is too low, predictions become meaningless; if too high, too many cases fall back to the decider.

### Mechanism 3
- Claim: Modular preprocessing and clustering enable rule learning on large datasets without losing accuracy.
- Mechanism: High-dimensional data is first reduced via dimensionality reduction, then clustered to create smaller subproblems. Rules are learned on each cluster independently, then combined.
- Core assumption: Data points in the same cluster share similar decision boundaries, so learning rules per cluster preserves accuracy while reducing complexity.
- Evidence anchors:
  - [section 3] "we split the whole procedure from given input examples to generating rules... Representation Learning... Input Selection... rule learner is applied on the data in its original form in order to preserve explainability"
  - [section 5.3] "we have been provided with a training data set consisting of about half a million instances with 435 attributes"
  - [corpus] Weak - related papers don't explicitly discuss modular clustering for scalable rule learning.
- Break condition: If clustering is poor, rules learned on clusters may not generalize, reducing accuracy.

## Foundational Learning

- Concept: Rule learning algorithms (FOIL, RIPPER)
  - Why needed here: These form the interpretable base models that generate the rules used for classification and explanation.
  - Quick check question: What is the main difference between FOIL and RIPPER in terms of handling noisy data?

- Concept: Ensemble learning methods (voting, bagging, boosting, stacking)
  - Why needed here: The voting mechanism is a specific ensemble strategy that combines multiple models to improve accuracy while preserving interpretability.
  - Quick check question: How does hard voting differ from soft voting in ensemble classification?

- Concept: Multi-class classification via binary decomposition
  - Why needed here: Rule learners typically produce binary classifiers, so multi-class problems are solved by training one binary classifier per class and combining results.
  - Quick check question: In the multi-class setup described, what happens if more than one binary classifier returns True for a given input?

## Architecture Onboarding

- Component map: Data preprocessing -> Rule learners (FOIL, RIPPER) -> Black-box decider -> Voting logic -> Output with explanation
- Critical path: Rule learner training -> Decider training -> Prediction phase with conflict resolution -> Explanation generation
- Design tradeoffs:
  - Higher accuracy decider improves overall performance but reduces interpretability if used too often
  - Lower threshold increases coverage but may reduce explanation quality
  - More rule learners improve coverage but increase training time
- Failure signatures:
  - Low agreement between rule learners -> high reliance on decider -> reduced interpretability
  - High percentage of "not explainable" predictions -> threshold too strict or data too complex
  - Slow inference -> too many rule sets or inefficient voting logic
- First 3 experiments:
  1. Train FOIL and RIPPER on Spambase, evaluate agreement rate and accuracy without decider
  2. Add a simple decision tree as decider, measure improvement in accuracy and reduction in "no prediction" cases
  3. Test partial satisfaction threshold on MNIST, tune threshold to balance coverage and explanation quality

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the text provided.

## Limitations
- Dependency on black-box deciders for complex cases reduces full explainability
- Need for threshold tuning which may vary by dataset and domain
- Computational overhead of training multiple rule learners and a decider model

## Confidence
- High confidence: Core mechanism of ensemble voting improving accuracy over single rule learners
- Medium confidence: Partial rule satisfaction mechanism and threshold-based fallback
- Medium confidence: Scalability claims through modular preprocessing and clustering

## Next Checks
1. Conduct ablation studies comparing accuracy with different combinations of rule learners (FOIL only, RIPPER only, both) to quantify individual contributions
2. Test the threshold sensitivity by systematically varying the partial satisfaction threshold (0.5 to 0.9) across multiple datasets to identify optimal ranges
3. Evaluate the approach on additional complex datasets with different characteristics (e.g., highly imbalanced, high dimensionality) to assess generalizability beyond the reported benchmarks