---
ver: rpa2
title: 'All Languages Matter: On the Multilingual Safety of Large Language Models'
arxiv_id: '2310.00905'
source_url: https://arxiv.org/abs/2310.00905
tags:
- safety
- languages
- llms
- language
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the multilingual safety of large language
  models (LLMs) by introducing XSAFETY, the first multilingual safety benchmark for
  LLMs. XSAFETY covers 14 safety issues across 10 languages and consists of 28,000
  annotated instances.
---

# All Languages Matter: On the Multilingual Safety of Large Language Models

## Quick Facts
- arXiv ID: 2310.00905
- Source URL: https://arxiv.org/abs/2310.00905
- Authors: Jing Li, Xiaotao Gu, Qianhui Wu, Jing Yi, Yong Jiang, Ke Xu, Tao Wang
- Reference count: 8
- This paper introduces XSAFETY, the first multilingual safety benchmark for LLMs, and demonstrates significant safety gaps across languages.

## Executive Summary
This paper addresses a critical gap in LLM safety evaluation by introducing XSAFETY, a comprehensive multilingual safety benchmark covering 14 safety issues across 10 languages. The authors evaluate four widely-used LLMs and demonstrate that all models produce significantly more unsafe responses for non-English queries than English ones, highlighting the urgent need for multilingual safety alignment. To address this gap, they propose simple yet effective prompting methods that can reduce unsafe responses from 19.1% to 9.7% for non-English queries, demonstrating that instruction-based approaches can effectively improve cross-lingual safety generalization.

## Method Summary
The authors created XSAFETY by translating existing monolingual safety benchmarks into 10 languages (English, Chinese, Spanish, French, Bengali, Arabic, Hindi, Russian, Japanese, German) while ensuring translation consistency through professional proofreading and n-gram diversity analysis. They evaluate multilingual safety using four LLMs (ChatGPT, PaLM2, LLaMA2-Chat, Vicuna) and employ ChatGPT itself as an automated safety evaluator using a prompt-based approach. To improve multilingual safety, they propose three prompting methods: SafePrompt (explicit safety instruction), XLingPrompt (English thinking before translation), and XSafePrompt (combining both approaches).

## Key Results
- All evaluated LLMs produce significantly more unsafe responses for non-English queries than English ones
- Bengali, Hindi, and Japanese are identified as the top-3 most unsafe languages, likely due to limited pretraining data
- SafePrompt and XLingPrompt methods reduce unsafe responses from 19.1% to 9.7% for non-English queries
- Commonsense Safety is identified as the most challenging scenario for multilingual safety
- Human evaluation confirms high consistency (89.8%) with the automated safety assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safety alignment learned in English does not fully transfer to non-English languages.
- Mechanism: The multilingual safety benchmark XSAFETY reveals that LLMs produce significantly more unsafe responses for non-English queries than English ones, indicating insufficient cross-lingual generalization of safety alignment.
- Core assumption: Safety alignment data is predominantly in English, and LLMs have limited exposure to safety training in other languages.
- Evidence anchors:
  - [abstract] "Experimental results show that all LLMs produce significantly more unsafe responses for non-English queries than English ones"
  - [section] "Experimental results show that all LLMs are significantly less safe in non-English languages than English"
- Break condition: If safety alignment data is collected and used equally across multiple languages, or if LLMs demonstrate strong cross-lingual transfer of safety knowledge without additional tuning.

### Mechanism 2
- Claim: Prompting methods can effectively improve multilingual safety by evoking safety knowledge and improving cross-lingual generalization.
- Mechanism: The proposed SafePrompt and XLingPrompt methods reduce the ratio of unsafe responses from 19.1% to 9.7% for non-English queries by explicitly instructing the model to answer safely and think in English before generating responses in the original language.
- Core assumption: LLMs can follow instructions to apply safety knowledge across languages when properly prompted.
- Evidence anchors:
  - [abstract] "Our prompting method can significantly reduce the ratio of unsafe responses from 19.1% to 9.7% for non-English queries"
  - [section] "Table 6 lists the unsafe ratio for different prompting methods. Clearly, all prompting methods can improve the multilingual safety of ChatGPT"
- Break condition: If the prompting methods do not consistently improve safety across different languages and safety scenarios, or if the model fails to understand and apply the safety instructions in the prompts.

### Mechanism 3
- Claim: The multilingual safety benchmark XSAFETY provides a comprehensive and consistent evaluation framework across 10 languages and 14 safety issues.
- Mechanism: By translating existing monolingual safety benchmarks and ensuring translation consistency through professional proofreading, XSAFETY enables systematic evaluation of multilingual safety, revealing performance gaps between English and non-English languages.
- Core assumption: Consistent translation of safety issues across languages preserves the semantic meaning and difficulty of the safety scenarios.
- Evidence anchors:
  - [section] "Table 2 lists the n-gram diversity of the translated corpora in different languages. Most languages share similar diversities of various n-grams"
  - [section] "We use n-gram diversity to measure the data distribution for each language l"
- Break condition: If the translation process introduces significant semantic drift or if the n-gram diversity analysis reveals substantial differences in data distribution across languages.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding how knowledge learned in one language (English) transfers to other languages is crucial for evaluating and improving multilingual safety.
  - Quick check question: What factors influence the effectiveness of cross-lingual transfer learning in large language models?

- Concept: Prompt engineering
  - Why needed here: Designing effective prompts to evoke safety knowledge and improve cross-lingual generalization is key to the proposed solution.
  - Quick check question: How do different prompt formulations (e.g., SafePrompt vs. XLingPrompt) affect the model's ability to generate safe responses in non-English languages?

- Concept: Multilingual benchmarking
  - Why needed here: Creating and using a comprehensive multilingual benchmark is essential for systematically evaluating and comparing the safety performance of LLMs across different languages.
  - Quick check question: What are the challenges in building a multilingual benchmark that ensures consistent evaluation across languages?

## Architecture Onboarding

- Component map:
  XSAFETY benchmark (28,000 instances, 10 languages, 14 safety issues) -> LLMs (ChatGPT, PaLM2, LLaMA2-Chat, Vicuna) -> ChatGPT safety evaluator -> Unsafe response ratio metrics

- Critical path:
  1. Build the XSAFETY benchmark by translating existing monolingual safety benchmarks
  2. Evaluate the multilingual safety of LLMs using XSAFETY
  3. Analyze the performance gaps between English and non-English languages
  4. Design and test prompting methods to improve multilingual safety
  5. Validate the effectiveness of prompting methods using XSAFETY

- Design tradeoffs:
  - Translation vs. original creation: Using translated benchmarks vs. creating original multilingual safety data
  - General vs. specific prompts: Balancing the effectiveness of general safety prompts vs. scenario-specific prompts
  - Evaluation vs. training: Using LLMs for safety evaluation vs. training a separate safety classifier

- Failure signatures:
  - High unsafe ratio in non-English languages: Indicates insufficient cross-lingual transfer of safety alignment
  - Ineffective prompting methods: Suggests that the model does not understand or apply the safety instructions
  - Inconsistent evaluation results: May indicate issues with the translation quality or evaluation methodology

- First 3 experiments:
  1. Evaluate the multilingual safety of a new LLM using XSAFETY and compare its performance across languages
  2. Test the effectiveness of a new prompting method on improving the multilingual safety of ChatGPT
  3. Analyze the n-gram diversity of the translated XSAFETY data to ensure consistent data distribution across languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does multilingual safety performance vary across different language families and resource levels?
- Basis in paper: [explicit] The paper identifies that Bengali, Hindi, and Japanese are the top-3 most unsafe languages, which are generally low-resource languages in the pretraining data of LLMs
- Why unresolved: The paper does not provide a detailed analysis of safety performance across different language families (e.g. Indo-European vs. Sino-Tibetan) or systematically examine how resource level impacts safety
- What evidence would resolve it: A comprehensive study mapping safety performance to language family classifications and resource levels (based on pretraining data size, number of speakers, etc.)

### Open Question 2
- Question: What is the optimal balance between English-based safety alignment and direct multilingual safety training?
- Basis in paper: [explicit] The paper shows that while English safety alignment generalizes to some extent, direct multilingual safety training is still necessary for optimal performance
- Why unresolved: The paper only tests simple prompting methods to improve cross-lingual generalization, but does not explore the trade-offs between different approaches or quantify the optimal balance
- What evidence would resolve it: Systematic experiments comparing different ratios of English vs. multilingual safety training data, and measuring the resulting safety performance across languages

### Open Question 3
- Question: How do different types of safety issues (e.g. commonsense vs. explicit content) generalize across languages?
- Basis in paper: [explicit] The paper identifies Commonsense Safety as the most challenging scenario for multilingual safety, requiring additional commonsense knowledge to comprehend unsafe content
- Why unresolved: The paper only provides a high-level categorization of safety issues and their performance across languages, but does not analyze the underlying reasons for different generalization patterns
- What evidence would resolve it: A detailed analysis of safety issue types, their linguistic characteristics, and how these relate to cross-lingual generalization performance

## Limitations

- The study relies on ChatGPT as an automated safety evaluator, which may have limitations in accurately assessing safety across all 14 safety issues and 10 languages
- The translation-based approach for creating XSAFETY may introduce semantic shifts that affect the consistency of safety evaluation across languages
- Results are based on evaluation of only four specific LLMs, which may not generalize to other model architectures or training approaches

## Confidence

**High confidence** in the core finding that LLMs produce significantly more unsafe responses for non-English queries than English ones. This is supported by extensive evaluation across 28,000 instances and multiple models, with clear statistical differences reported.

**Medium confidence** in the effectiveness of prompting methods for improving multilingual safety. While the 50% reduction in unsafe responses (from 19.1% to 9.7%) is promising, this evaluation is based on a single model (ChatGPT) and may vary with different model architectures or safety alignment approaches.

**Medium confidence** in the completeness of XSAFETY as a multilingual safety benchmark. While the benchmark covers 14 safety issues across 10 languages with 28,000 instances, the reliance on translated content rather than natively generated multilingual safety data may limit its ability to capture language-specific safety nuances.

## Next Checks

1. **Human evaluation validation**: Conduct comprehensive human evaluation across all 10 languages and safety categories to verify the accuracy of automated safety assessment, particularly for nuanced safety issues where cultural context may affect interpretation.

2. **Cross-model replication**: Test the proposed prompting methods (SafePrompt and XLingPrompt) across additional LLM architectures (e.g., Claude, GPT-4, open-source models) to assess generalizability of the safety improvements beyond ChatGPT.

3. **Temporal stability analysis**: Evaluate the multilingual safety performance and prompting method effectiveness over time as models receive updates and additional training, to assess whether safety improvements are stable or degrade with model evolution.