---
ver: rpa2
title: 'OptiMUS: Optimization Modeling Using MIP Solvers and large language models'
arxiv_id: '2310.06116'
source_url: https://arxiv.org/abs/2310.06116
tags:
- optimization
- problem
- code
- output
- optimus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OptiMUS is an LLM-based agent for formulating and solving mixed
  integer linear programming (MILP) problems from natural language descriptions. It
  generates mathematical formulations, writes solver code, executes it, and iteratively
  revises the code using automated testing and debugging until a valid solution is
  found.
---

# OptiMUS: Optimization Modeling Using MIP Solvers and large language models

## Quick Facts
- arXiv ID: 2310.06116
- Source URL: https://arxiv.org/abs/2310.06116
- Reference count: 40
- Primary result: OptiMUS solves 63% of optimization problems vs 24% with basic LLM prompting

## Executive Summary
OptiMUS is an LLM-based agent that automatically formulates and solves mixed integer linear programming (MILP) problems from natural language descriptions. The system generates mathematical formulations, writes solver code, executes it, and iteratively revises the code using automated testing and debugging until finding a valid solution. Using a novel benchmark dataset (NLP4LP) of 40 LP and MILP problems, experiments show OptiMUS solves 67% more problems than basic LLM prompting, with GPT-4 solving 63% of problems versus 24% for basic prompting.

## Method Summary
OptiMUS uses a multi-step workflow to solve optimization problems: it parses structured natural language optimization problems (SNOPs), generates mathematical formulations using LLMs, writes solver code, executes the code with data, and employs automated testing and debugging to iteratively refine solutions. The system separates problem formulation from data handling to accommodate large-scale optimization problems, uses problem rephrasing for data augmentation, and employs various prompting strategies including debugging and supervised testing.

## Key Results
- OptiMUS achieves 63% solve rate with GPT-4 compared to 24% with basic prompting
- Automated testing and debugging increases success rates significantly
- Problem rephrasing through augmentation improves solution success rates
- OptiMUS increases solve rate by 67% compared to direct prompting

## Why This Works (Mechanism)

### Mechanism 1: Formulation-Data Separation
- Claim: OptiMUS separates formulation from data to handle large-scale optimization problems
- Mechanism: The system uses LLMs to generate mathematical formulations from SNOP descriptions, then separately handles large numerical data through solver code generation and execution
- Core assumption: LLMs can effectively formulate problems from structured natural language descriptions while remaining within their context limitations
- Evidence anchors: Abstract mentions critical need to separate formulation from problem data due to memory requirements

### Mechanism 2: Automated Testing and Debugging
- Claim: Automated testing and debugging significantly improves success rates
- Mechanism: After generating solver code, OptiMUS automatically creates unit tests from the SNOP, executes the code, and uses error messages from failed tests to iteratively revise the code
- Core assumption: LLMs can effectively generate meaningful tests and use error feedback to correct code
- Evidence anchors: Abstract describes iterative revision using automated testing and debugging

### Mechanism 3: Problem Rephrasing
- Claim: Problem rephrasing through augmentation improves solution success rates
- Mechanism: OptiMUS automatically generates multiple rephrased versions of the problem using LLMs, then attempts to solve each rephrased version independently
- Core assumption: Different phrasings of the same problem can lead to different formulations that may be easier for LLMs to handle
- Evidence anchors: Abstract states OptiMUS increases solve rate by 67% compared to direct prompting

## Foundational Learning

- Concept: Mathematical optimization problem formulation
  - Why needed here: Understanding how to translate natural language descriptions into mathematical objectives and constraints is fundamental to OptiMUS's operation
  - Quick check question: Given "A company wants to maximize profit by choosing production quantities subject to resource constraints," can you write the general mathematical formulation?

- Concept: Mixed Integer Linear Programming (MILP)
  - Why needed here: OptiMUS specifically targets MILP problems, requiring understanding of linear objectives, linear constraints, and integer variables
  - Quick check question: What distinguishes an MILP from a pure LP, and why does this matter for solver selection?

- Concept: Natural Language Processing (NLP) techniques for code generation
  - Why needed here: OptiMUS relies on LLMs to generate both mathematical formulations and solver code from natural language descriptions
  - Quick check question: How do LLMs typically handle the transformation from descriptive text to structured code output?

## Architecture Onboarding

- Component map: SNOP Parser → Formulation Generator → Code Generator → Solver Executor → Test Generator → Debugger → (Optional) Re-phraser
- Critical path: 1. Parse SNOP input 2. Generate mathematical formulation 3. Generate solver code 4. Execute code with data 5. Run validity tests 6. Debug if necessary 7. Return solution
- Design tradeoffs: Separation of formulation and data enables handling large problems but requires careful coordination; automated testing improves reliability but adds computational overhead; re-phrasing increases success rates but multiplies computation time
- Failure signatures: Code execution errors → Debug with error messages; Test failures → Debug with constraint violation details; Low success rates → Try more augmentations or different solver
- First 3 experiments: 1. Run OptiMUS on a simple LP problem from NLP4LP dataset with GPT-4, measuring success rate 2. Compare basic prompting vs. OptiMUS with debugging on same problem set 3. Test impact of adding 1, 2, and 3 augmentations on subset of problems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would OptiMUS perform on optimization problems with significantly larger input data sizes that exceed the context limits of current LLMs?
- Basis in paper: The paper mentions that LLMs have limited context sizes and that optimization problems often involve large amounts of numerical data that could require gigabytes of memory
- Why unresolved: The paper doesn't test OptiMUS on problems with extremely large data sizes that would push against these limitations
- What evidence would resolve it: Testing OptiMUS on optimization problems with input data sizes ranging from 1GB to 100GB to measure performance degradation, execution success rates, and solution quality as context window limitations become more restrictive

### Open Question 2
- Question: What is the minimum level of expertise required for a human supervisor to effectively create and maintain supervised tests for OptiMUS?
- Basis in paper: The paper mentions that developing supervised tests is roughly five times faster than developing equivalent human tests from scratch
- Why unresolved: The paper provides no information about the background knowledge needed to create effective supervised tests
- What evidence would resolve it: A controlled study comparing test creation times and quality between optimization experts, data scientists, and general programmers

### Open Question 3
- Question: How does the performance of OptiMUS scale with the number of augmentation rephrasings beyond the five used in the paper?
- Basis in paper: The paper shows that adding one or two augmentations significantly improves performance, but notes that additional augmentations beyond that point almost do not change OptiMUS's performance anymore
- Why unresolved: The paper doesn't explore the upper bound of this relationship or test scenarios where more extensive augmentation might be beneficial
- What evidence would resolve it: Empirical testing of OptiMUS with 5, 10, 20, 50, and 100 augmentation rephrasings across various problem types

## Limitations
- 37% of problems remain unsolved even with advanced techniques
- Results heavily depend on GPT-4 performance and may not generalize to other LLM architectures
- The 40-problem benchmark, while carefully constructed, represents a relatively small sample of optimization problem diversity

## Confidence
- SNOP representation adequacy (Medium): The structured format may not capture all problem nuances
- Solver reliability (Low): Evaluation depends on specific solver implementations whose availability may vary
- GPT-4 dependency (Medium): Results heavily rely on a single model version

## Next Checks
1. Cross-solver validation: Test OptiMUS with alternative MILP solvers (CBC, SCIP) on the same problem set to verify solver-independence claims
2. SNOP robustness test: Systematically vary SNOP formatting and structure while keeping problem semantics constant to measure formulation sensitivity
3. Scale boundary analysis: Test OptiMUS on problems approaching context window limits to identify exact scaling boundaries and failure modes