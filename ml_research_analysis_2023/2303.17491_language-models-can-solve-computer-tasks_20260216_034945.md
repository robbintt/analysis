---
ver: rpa2
title: Language Models can Solve Computer Tasks
arxiv_id: '2303.17491'
source_url: https://arxiv.org/abs/2303.17491
tags:
- tasks
- task
- agent
- language
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) struggle to perform computer tasks
  in a few-shot setting due to challenges in task grounding, state grounding, and
  agent grounding. To address this, we propose Recursive Criticism and Improvement
  (RCI), a simple prompting scheme that enables an LLM to recursively criticize and
  improve its outputs to be better grounded.
---

# Language Models can Solve Computer Tasks

## Quick Facts
- **arXiv ID**: 2303.17491
- **Source URL**: https://arxiv.org/abs/2303.17491
- **Reference count**: 40
- **Primary result**: RCI prompting significantly outperforms existing LLM methods for automating computer tasks on the MiniWoB++ benchmark

## Executive Summary
This paper introduces Recursive Criticism and Improvement (RCI), a prompting scheme that enables large language models (LLMs) to recursively criticize and improve their outputs for better grounding in computer tasks. RCI addresses the challenges of task grounding, state grounding, and agent grounding by having the LLM generate outputs, identify problems with them, and then generate improved outputs based on the critiques. The method significantly outperforms existing approaches on the MiniWoB++ benchmark, surpassing state-of-the-art supervised learning, reinforcement learning, and LLM methods using only a handful of demonstrations per task.

## Method Summary
RCI prompting is a three-step process that enhances LLM outputs for computer tasks: (1) Task grounding, where the LLM generates a plan for solving the task; (2) State grounding, where the LLM refines task-grounded actions to be feasible in the current state; and (3) Agent grounding, where the LLM refines state-grounded actions to be admissible for the computer agent. The method uses pre-trained LLMs without fine-tuning, relying on in-context learning through few-shot examples. RCI is more computationally expensive than single-shot approaches but provides significant performance improvements.

## Key Results
- RCI with InstructGPT-3 + RLHF is competitive with state-of-the-art SL+RL methods using only 2-3 demonstrations per task
- RCI outperforms chain-of-thought (CoT) prompting on reasoning tasks
- The method surpasses state-of-the-art supervised learning, reinforcement learning, and LLM approaches on MiniWoB++ benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RCI improves task grounding by enabling the LLM to recursively refine its plan for solving the task.
- Mechanism: The LLM first generates a plan, then critiques it for problems, and finally improves it based on the critique.
- Core assumption: The LLM can identify problems in its own output and generate improved outputs based on those problems.
- Evidence anchors:
  - [abstract]: "RCI works by first having the LLM generate an output based on zero-shot prompting. Then, RCI prompts the LLM to identify problems with the given output. After the LLM has identified problems with the output, RCI prompts the LLM to generate an updated output."
  - [section]: "Task grounding. In the action sampling process, the first step involves generating a plan of actionable steps for task solving from LLMs. Subsequently, actions are sampled from the same LLMs, taking into account the present state, task, and generated plan."
- Break Condition: If the LLM is unable to identify problems in its own output or generate improved outputs based on those problems, RCI will not be effective in improving task grounding.

### Mechanism 2
- Claim: RCI improves state grounding by enabling the LLM to refine task-grounded actions to be feasible in the current state.
- Mechanism: The LLM first generates task-grounded actions, then critiques them for feasibility in the current state, and finally improves them based on the critique.
- Core assumption: The LLM can consider the current state when generating actions and identify problems with actions that are not feasible in the current state.
- Evidence anchors:
  - [abstract]: "State grounding connects high-level concepts derived from the task grounding step with actual HTML elements present in the current state, subsequently outputting the appropriate action."
  - [section]: "State grounding. In language-based agents, grounding actions in the environment is a crucial step to enable real-world task performance. The aim of this phase is to enhance the task-grounded actions to be feasible in the current state."
- Break Condition: If the LLM is unable to consider the current state when generating actions or identify problems with actions that are not feasible in the current state, RCI will not be effective in improving state grounding.

### Mechanism 3
- Claim: RCI improves agent grounding by enabling the LLM to refine state-grounded actions to be admissible for the computer agent.
- Mechanism: The LLM first generates state-grounded actions, then critiques them for admissibility for the computer agent, and finally improves them based on the critique.
- Core assumption: The LLM can consider the specifications of the computer agent when generating actions and identify problems with actions that are not admissible for the computer agent.
- Evidence anchors:
  - [abstract]: "Agent grounding ensures the correct formatting of the action output obtained from the state grounding step."
  - [section]: "Agent grounding. To ensure the successful integration of language-based methodologies in decision-making processes, it is imperative to establish a scalable framework that guarantees the admissibility of actions derived from the language model."
- Break Condition: If the LLM is unable to consider the specifications of the computer agent when generating actions or identify problems with actions that are not admissible for the computer agent, RCI will not be effective in improving agent grounding.

## Foundational Learning

- Concept: Zero-shot prompting
  - Why needed here: RCI builds upon zero-shot prompting by adding a recursive critique and improvement step. Understanding zero-shot prompting is crucial for understanding how RCI works.
  - Quick check question: What is zero-shot prompting and how does it differ from few-shot prompting?

- Concept: Chain-of-thought (CoT) prompting
  - Why needed here: RCI is compared to CoT prompting in terms of its effectiveness in enhancing the reasoning abilities of LLMs. Understanding CoT prompting is important for understanding the context of the comparison.
  - Quick check question: How does CoT prompting differ from standard prompting and what are its benefits?

- Concept: HTML code and DOM structure
  - Why needed here: The paper discusses grounding actions in the current state, which involves understanding the HTML code and DOM structure of the webpage. Familiarity with HTML and DOM is necessary for understanding how actions are grounded in the state.
  - Quick check question: What is the difference between HTML and DOM and how are they related?

## Architecture Onboarding

- Component map: LLM -> RCI (Task Grounding -> State Grounding -> Agent Grounding) -> Actions
- Critical path: The sequential execution of task grounding, state grounding, and agent grounding, with each step involving a recursive critique and improvement process.
- Design tradeoffs:
  - Computational cost: RCI is more expensive to run compared to approaches that just sample once from the LLM due to the recursive nature of the critique and improvement process.
  - Performance: RCI improves the performance of LLMs in computer tasks and reasoning tasks, but the extent of improvement may vary depending on the task and the LLM used.
- Failure signatures:
  - Inability to identify problems: If the LLM is unable to identify problems in its own output, RCI will not be effective in improving task grounding, state grounding, or agent grounding.
  - Inability to generate improved outputs: If the LLM is unable to generate improved outputs based on the identified problems, RCI will not be effective in enhancing the LLM's outputs.
- First 3 experiments:
  1. Evaluate RCI on a simple computer task with a well-defined plan and state.
  2. Compare RCI to standard prompting and CoT prompting on a reasoning task.
  3. Analyze the impact of different LLMs on the performance of RCI in computer tasks.

## Open Questions the Paper Calls Out

- What is the impact of increasing the loop count for implicit RCI on agent grounding?
- How does RCI prompting affect the computational efficiency of the LLM agent compared to single-shot prompting?
- Can RCI prompting be effectively combined with other prompting techniques beyond chain-of-thought?

## Limitations

- RCI's effectiveness depends on the LLM's ability to self-critique and improve its outputs, which may vary depending on the task complexity and the specific LLM used.
- The method's performance in long-horizon planning tasks where the agent needs to maintain a consistent plan across multiple steps is unclear.
- The paper focuses on a specific benchmark (MiniWoB++) and does not extensively explore other domains or real-world applications.

## Confidence

- **High confidence** in the effectiveness of RCI for the MiniWoB++ benchmark tasks
- **Medium confidence** in the scalability of RCI to more complex tasks or environments
- **High confidence** in the comparison with other prompting techniques like chain-of-thought

## Next Checks

1. **Evaluate RCI on a long-horizon planning task**: Test RCI's effectiveness in tasks that require maintaining a consistent plan across multiple steps, such as a complex game or a multi-step workflow, to assess its ability to handle long-term dependencies.

2. **Analyze failure cases and limitations**: Conduct a detailed analysis of scenarios where RCI fails or underperforms, such as tasks with ambiguous instructions or complex state representations, to understand the method's limitations and potential areas for improvement.

3. **Compare RCI with other prompting techniques**: Perform a comprehensive comparison of RCI with other prompting techniques, such as Chain-of-Thought (CoT) prompting and zero-shot prompting, across a diverse set of tasks to evaluate its relative strengths and weaknesses.