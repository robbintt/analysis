---
ver: rpa2
title: Identifying Interpretable Subspaces in Image Representations
arxiv_id: '2307.10504'
source_url: https://arxiv.org/abs/2307.10504
tags:
- concepts
- images
- features
- feature
- falcon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FALCON, a framework to automatically extract
  natural language explanations for features in image representation spaces. The method
  uses CLIP-based image captioning on highly activating image crops to generate concept
  words, then applies contrastive filtering using lowly activating images to eliminate
  spurious concepts.
---

# Identifying Interpretable Subspaces in Image Representations

## Quick Facts
- arXiv ID: 2307.10504
- Source URL: https://arxiv.org/abs/2307.10504
- Reference count: 40
- Primary result: FALCON extracts natural language concepts from image representations with precision 0.86 and recall 0.84 on AMT human evaluation.

## Executive Summary
This paper introduces FALCON, a framework for automatically explaining image representation features using natural language concepts. The method leverages CLIP-based image captioning on highly activating image crops to generate concept words, then applies contrastive filtering using lowly activating images to eliminate spurious concepts. FALCON demonstrates that analyzing feature groups rather than individual features explains larger portions of representation space. The framework also enables transfer of interpretable concepts across models via a simple linear transformation, improving scalability and interpretability for debugging downstream failures.

## Method Summary
FALCON extracts natural language explanations for image representation features by first identifying highly activating images for each feature, then cropping these images using GradCAM to focus on the most relevant regions. These cropped regions are matched to a large corpus of captions (LAION-400M) using CLIP's image and text encoders, with the top-5 captions per image scored using a word-level similarity metric. Concepts are extracted from these captions and filtered using a contrastive approach that removes spurious concepts appearing in lowly activating images. The method also includes a technique for transferring concepts across models through a linear transformation that maps representations from one model to another.

## Key Results
- FALCON achieves precision 0.86 and recall 0.84 on AMT human evaluation for concept extraction
- Feature groups explain larger portions of representation space than individual features
- Concepts can be transferred across models via a simple linear transformation without re-annotation

## Why This Works (Mechanism)

### Mechanism 1
CLIP-based image captioning with LAION-400M captions yields discriminative natural language concepts for image features. The pipeline computes CLIP image embeddings for cropped highly activating image regions, then matches these to CLIP text embeddings from LAION-400M captions. The top-5 captions per image are scored using a word-level similarity metric, and the highest scoring words are extracted as concepts. This works because CLIP's image and text encoders preserve semantic similarity such that matching cosine similarity scores between cropped regions and captions yields semantically meaningful concepts.

### Mechanism 2
Contrastive interpretation using lowly activating images removes spurious concepts from the top-5 set. For each feature, images are selected that activate all other features highly but the target feature weakly. Concepts extracted from these lowly activating images are treated as spurious and discarded from the final set. This works because spurious concepts are those that appear in both highly activating and lowly activating images; contrastive filtering removes them.

### Mechanism 3
Linear transformation between representation spaces allows transfer of interpretable concepts without re-annotation. A linear head is trained to map representations from a target (unseen) model to a source (interpretable) model by minimizing the L2 reconstruction error. The resulting matrix Z maps feature groups in the target to corresponding interpretable groups in the source, enabling concept transfer. This works because different vision models learn similar underlying concepts, so a simple linear mapping suffices for cross-model concept transfer.

## Foundational Learning

- **Concept: CLIP image-text embedding space**
  - Why needed here: FALCON relies on CLIP's ability to map images and text into a shared semantic space for caption retrieval and word scoring.
  - Quick check question: Does CLIP's image encoder produce embeddings that preserve semantic similarity for cropped image regions?

- **Concept: Cosine similarity for semantic matching**
  - Why needed here: Used to select top-5 captions per image and to score word concepts; central to both captioning and contrastive filtering.
  - Quick check question: Are the top-5 captions selected by cosine similarity consistently semantically relevant to the image crop?

- **Concept: Gradient-based localization (GradCAM)**
  - Why needed here: Crops highly activating image regions for concept extraction, ensuring that concepts are tied to the specific visual attributes the feature responds to.
  - Quick check question: Do GradCAM masks correctly highlight the image regions that maximally activate the feature?

## Architecture Onboarding

- **Component map:** Image encoder -> GradCAM -> CLIP image encoder -> Cosine similarity -> LAION-400M text encoder -> Word scoring -> Concept filtering -> Linear head

- **Critical path:**
  1. Load model and compute representation space
  2. For each target feature: extract top-activating images via threshold
  3. Compute GradCAM crops on these images
  4. Embed crops with CLIP image encoder; embed LAION captions with CLIP text encoder
  5. Compute cosine similarity matrix; select top-5 captions per image
  6. Extract words, score via frequency-weighted similarity, threshold to get concepts
  7. Extract lowly activating images; compute concepts; discard shared concepts as spurious
  8. Output final concept set

- **Design tradeoffs:**
  - Fixed captioning dataset vs. training a domain-specific captioner: fixed is simpler but may miss domain-specific concepts
  - Hard thresholding on feature activation vs. percentile-based: hard threshold is faster but may miss weakly but meaningfully activating features
  - Single feature vs. feature group analysis: groups explain more space but increase computational cost

- **Failure signatures:**
  - High variance in word scores: captions not semantically aligned with images
  - No lowly activating images found: contrastive filtering ineffective
  - Linear mapping Z has near-zero singular values: cross-model transfer infeasible

- **First 3 experiments:**
  1. Verify CLIP similarity matches human judgment on a small set of cropped images and captions
  2. Test GradCAM cropping: confirm that cropped regions retain the semantic content of the full image
  3. Validate concept transfer: train Z on a source-target pair with known shared concepts and check if transferred concepts match source

## Open Questions the Paper Calls Out

### Open Question 1
How do concepts learned by a model trained on painting images transfer to a model trained on sketch images? This remains an open direction for future work as stated in the limitations section. Experiments showing concept transfer performance between models trained on different image modalities like paintings and sketches would resolve this.

### Open Question 2
What is the equivalent of localized gradient heatmaps in the language space of vision-language models? Understanding the equivalent of localized gradient heatmaps in the language space is still unclear and requires further research. Development and validation of a method to compute saliency-like explanations in the language embedding space of vision-language models would resolve this.

### Open Question 3
How sparse is the transformation matrix Z when mapping representations between models? In practice Z is not sparse however it can be considered as 'nearly sparse' where most weights are close to zero. The paper does not provide quantitative analysis of the sparsity of Z. Empirical analysis of the distribution of weights in Z across different model pairs to quantify its sparsity would resolve this.

### Open Question 4
How does the expressiveness/specificity of the captioning dataset impact the quality of extracted concepts? One advantage of using a separate vocabulary with a vision-language model is the flexibility of controlling the expressiveness/specificity of the captioning dataset depending on the complexity of the target model. The paper compares FALCON with CLIP + LAION to BLIP-2 but does not systematically study the impact of dataset choice. Controlled experiments varying the size and specificity of the captioning dataset and measuring the resulting concept quality across different model types would resolve this.

## Limitations

- The contrastive filtering mechanism assumes spurious concepts will consistently appear in both highly and lowly activating images, but feature activation patterns may be more complex
- The linear transformation assumption for cross-model concept transfer is a significant limitation that may not hold for models with fundamentally different architectures
- The method's performance is heavily dependent on the quality and coverage of the captioning dataset and CLIP's semantic alignment for cropped regions

## Confidence

- **High confidence**: The overall pipeline of using CLIP + LAION for concept extraction and contrastive filtering for spurious concept removal
- **Medium confidence**: The claim that feature groups explain more representation space than individual features
- **Medium confidence**: The linear transformation for cross-model concept transfer

## Next Checks

1. Validate CLIP semantic alignment for cropped images by manually inspecting a sample of cropped image regions and their top-5 captions to ensure semantic relevance
2. Test contrastive filtering robustness by analyzing the distribution of concepts in highly vs. lowly activating images to confirm that spurious concepts are consistently removed
3. Evaluate linear mapping fidelity by training the linear head Z on a source-target model pair with known shared concepts and assessing the semantic similarity of transferred concepts to the source