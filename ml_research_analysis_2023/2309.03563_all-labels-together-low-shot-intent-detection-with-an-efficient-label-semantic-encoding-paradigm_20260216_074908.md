---
ver: rpa2
title: 'All Labels Together: Low-shot Intent Detection with an Efficient Label Semantic
  Encoding Paradigm'
arxiv_id: '2309.03563'
source_url: https://arxiv.org/abs/2309.03563
tags:
- intent
- intents
- shot
- pretraining
- one-to-all
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an end-to-end intent detection model, One-to-All,
  that encodes utterances and the entire label set together to fully utilize label
  semantics. It constructs input sequences by concatenating each utterance with grouped
  intents, then applies contrastive learning between utterances and their gold and
  non-gold intents.
---

# All Labels Together: Low-shot Intent Detection with an Efficient Label Semantic Encoding Paradigm

## Quick Facts
- arXiv ID: 2309.03563
- Source URL: https://arxiv.org/abs/2309.03563
- Reference count: 15
- One-to-All achieves state-of-the-art performance, improving accuracy by up to 13.64% over prior methods in extreme few-shot scenarios

## Executive Summary
This paper introduces One-to-All, an end-to-end intent detection model that encodes utterances and entire label sets together to fully utilize label semantics. The model constructs input sequences by concatenating each utterance with grouped intents, then applies contrastive learning between utterances and their gold and non-gold intents. Pretrained on paraphrase identification data, One-to-All demonstrates state-of-the-art performance on three intent detection tasks under few-shot (1-, 3-, 5-shot) and zero-shot settings, particularly excelling in extreme few-shot scenarios.

## Method Summary
One-to-All is an end-to-end intent detection model that concatenates each utterance with the complete intent label set as input, encoding them simultaneously. The model uses contrastive learning to compare utterance representations with intent representations, optimizing a custom loss that pushes correct pairs together and incorrect pairs apart. A novel pretraining strategy on paraphrase identification data (QQP) enhances zero-shot cross-domain generalization by teaching the model to capture semantic similarities and distinctions.

## Key Results
- Achieves state-of-the-art performance on BANKING77, HWU64, and CLINC150 datasets
- Improves accuracy by up to 13.64% over prior methods in extreme few-shot scenarios
- Demonstrates effective utilization of semantic label information and indirect supervision from paraphrasing
- Shows strong performance in zero-shot settings through paraphrase identification pretraining

## Why This Works (Mechanism)

### Mechanism 1
The concatenated input sequence construction enables full label semantics utilization by comparing each utterance against all label candidates simultaneously. By grouping intents into fixed-size batches with padding tokens and duplicating each utterance across all groups, the model receives direct semantic comparison signals for every intent label relative to the utterance. This mechanism assumes intent label semantics can be meaningfully compared to utterance semantics in a shared representation space.

### Mechanism 2
Contrastive learning between utterances and intents improves few-shot performance by explicitly moving utterances closer to their gold intents and away from incorrect ones. The model computes cosine similarities between utterance representations and intent representations, then optimizes a contrastive loss that pushes correct pairs together and incorrect pairs apart. This assumes intent labels serve as meaningful cluster centroids that can guide utterance representation learning.

### Mechanism 3
Paraphrase identification pretraining provides indirect supervision that improves zero-shot cross-domain generalization by teaching the model to capture semantic similarities and distinctions. During pretraining, sentences are treated as utterances and their paraphrases as gold labels, forcing the model to learn semantic matching capabilities transferable to unseen intent domains. This assumes semantic similarity understanding from paraphrase detection transfers to intent classification tasks.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: Enables the model to learn discriminative representations by explicitly comparing utterances with correct and incorrect intents
  - Quick check question: What is the primary difference between contrastive learning and standard classification loss?

- Concept: Semantic similarity
  - Why needed here: The model must understand semantic relationships between utterances and intent labels to perform meaningful comparisons
  - Quick check question: How does semantic similarity differ from lexical similarity in intent detection?

- Concept: Cross-domain generalization
  - Why needed here: The pretraining strategy aims to enable zero-shot performance on unseen intent domains
  - Quick check question: What factors influence a model's ability to generalize across different intent detection domains?

## Architecture Onboarding

- Component map: Input construction -> Encoder (RoBERTa-base) -> Projector (MLP) -> Contrastive loss -> Parameter update
- Critical path: Input construction → Encoder → Projector → Contrastive loss → Parameter update
- Design tradeoffs:
  - Fixed group size (k) vs. variable length: Fixed size simplifies batching but may introduce padding tokens
  - End-to-end vs. pipeline: End-to-end avoids cascading errors but requires more complex training
  - Contrastive vs. classification: Contrastive provides richer supervision but requires careful negative sampling
- Failure signatures:
  - Poor performance despite correct implementation: Likely indicates insufficient contrastive signal or poor projector mapping
  - High variance across runs: May indicate instability in contrastive learning or sensitivity to initialization
  - Degradation in zero-shot settings: Suggests pretraining strategy is ineffective for transfer learning
- First 3 experiments:
  1. Verify input construction by checking sequence lengths and padding patterns
  2. Test contrastive loss calculation with synthetic data to ensure correct positive/negative pairing
  3. Evaluate model performance with k=1 (no grouping) to confirm grouping strategy provides benefit

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of One-to-All scale when pretrained on a much larger collection of out-of-domain (OOD) datasets, beyond the two datasets used in this study? The authors acknowledge that using only two OOD datasets limits the potential of OOD pretraining and suggest that pretraining on a larger-scale OOD dataset could further boost the model's performance.

### Open Question 2
What is the optimal number of intents (k) to include in each input sequence for tasks with a very large number of intents, and how does this choice affect the model's ability to generalize? The authors explore the impact of varying k on model performance but only for the specific datasets used in the study.

### Open Question 3
How does the One-to-All model perform in a cross-lingual few-shot intent detection setting, where the training data is in a different language than the test data? The paper focuses on cross-domain generalization but does not explore cross-lingual scenarios.

## Limitations
- Contrastive learning mechanism's effectiveness lacks direct empirical validation through ablation studies
- Fixed group size parameter (k) introduces a hyperparameter that could significantly affect performance but is not systematically explored
- Paraphrase identification pretraining transfer mechanism assumes transfer occurs without direct validation

## Confidence
- High confidence: Overall performance claims on three benchmark datasets under few-shot settings
- Medium confidence: Specific contribution of contrastive learning versus semantic encoding
- Low confidence: Transfer learning claims from paraphrase identification pretraining

## Next Checks
1. Conduct ablation study comparing One-to-All with and without contrastive learning, and with and without semantic encoding strategy
2. Perform systematic analysis of intent label semantic similarity to determine when contrastive learning mechanism breaks down
3. Design experiments to test whether model's performance on paraphrase identification tasks correlates with zero-shot intent detection ability