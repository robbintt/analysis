---
ver: rpa2
title: Why bother with geometry? On the relevance of linear decompositions of Transformer
  embeddings
arxiv_id: '2310.06977'
source_url: https://arxiv.org/abs/2310.06977
tags:
- computational
- linguistics
- association
- geometry
- decomposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether linear decomposition approaches
  to Transformer embeddings provide meaningful explanations of model behavior. Using
  machine translation decoder embeddings, the authors examine two decomposition methods
  across different models and training conditions.
---

# Why bother with geometry? On the relevance of linear decompositions of Transformer embeddings

## Quick Facts
- arXiv ID: 2310.06977
- Source URL: https://arxiv.org/abs/2310.06977
- Reference count: 26
- Primary result: Linear decomposition-derived indicators correlate with corpus-level model performance but show high variability across training runs, suggesting geometry reflects model-specific rather than task-specific characteristics

## Executive Summary
This study investigates whether linear decomposition approaches to Transformer embeddings provide meaningful explanations of model behavior. Using machine translation decoder embeddings, the authors examine two decomposition methods across different models and training conditions. Their results show that while decomposition-derived indicators generally correlate with corpus-level model performance, there is high variability across different training runs. This variability suggests that geometry reflects model-specific characteristics more than sentence-specific computations. The study finds that similar training conditions do not guarantee similar vector spaces, and that sentence-level performance is often less predictive of geometry than corpus-level performance. These findings question the usefulness of geometry-based approaches for explaining Transformer model behavior, as observations about one model may not generalize to others.

## Method Summary
The authors train Transformer models for machine translation using the Marian-NMT library on subsets of the Tatoeba Challenge corpus, saving checkpoints every 1000 training steps. They apply two linear decomposition methods to decoder embeddings: sub-layer-wise (Dcpsl) and token-wise (Dcptok), which exploit residual connections and layer normalization to attribute contributions to different components. For each decomposition term, they compute scalar indicators (norm ratio and cosine similarity) and analyze their correlation with model performance metrics (BLEU, COMET, chrF++) at both corpus and sentence levels. They also use Dynamic Time Warping (DTW) to compare geometric patterns across different training runs.

## Key Results
- Decomposition-derived indicators effectively correlate with corpus-level model performance metrics (BLEU, COMET, chrF++) but show weaker correlations at the sentence level
- High variability exists in geometric measurements across different training runs with identical conditions, suggesting model-specific rather than task-specific geometric patterns
- Similar training conditions (same data, hyperparameters, seed variations) do not guarantee similar vector spaces in embedding geometry
- Different linear decomposition approaches lead to different interpretations of what Transformer geometry encodes, with varying correlation patterns and statistical significance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear decomposition approaches correlate with corpus-level model performance but not sentence-level performance.
- Mechanism: The decomposition-derived scalar indicators (norm ratio and cosine similarity) capture aggregate geometric patterns that align with overall translation quality metrics like BLEU, COMET, or chrF++ when evaluated across entire test sets.
- Core assumption: Model geometry at the corpus level reflects overall model capability, but sentence-level variations are dominated by input-specific factors rather than model geometry.
- Evidence anchors:
  - [abstract] "decomposition-derived indicators effectively correlate with model performance"
  - [section 4] "correlation magnitudes tend to be high" for corpus-level measurements, "correlation scores are often much lower" for sentence-level measurements
  - [corpus] Strong correlation between decomposition indicators and corpus-level BLEU/COMET/chrF++ scores across multiple models and runs
- Break condition: If input distribution shifts significantly (e.g., training on subtitles, testing on parliamentary debates), the correlation between geometry and performance breaks down.

### Mechanism 2
- Claim: Transformer embedding geometry is highly model-specific rather than determined by training conditions.
- Mechanism: Despite similar training conditions (same data, hyperparameters, and seed variations), models develop distinct geometric arrangements in their embedding spaces, as measured by dynamic time warping distances and permutation tests.
- Core assumption: Similar training conditions should produce similar geometric patterns if geometry primarily reflects training conditions rather than model-specific characteristics.
- Evidence anchors:
  - [abstract] "variation across different runs suggests a more nuanced take" and "geometry reflects model-specific characteristics more than sentence-specific computations"
  - [section 4] "variation in geometry across different runs for a same translation task can exceed what we observe for models trained for different translation tasks"
  - [corpus] DTW distance heatmaps show that distances between Russian seed models (s0, s1, s2) are not consistently lower than distances between models trained on different corpora
- Break condition: If a systematic geometric pattern emerges that is consistently tied to specific training conditions across multiple runs, this mechanism would be invalidated.

### Mechanism 3
- Claim: Different linear decomposition approaches (sub-layer-wise vs token-wise) lead to different interpretations of what Transformer geometry encodes.
- Mechanism: The two decomposition methods attribute contributions differently within the Transformer architecture, leading to divergent correlation patterns with performance metrics and different statistical significance in permutation tests.
- Core assumption: Both decomposition approaches should yield similar insights if they are capturing the same underlying geometric properties.
- Evidence anchors:
  - [abstract] "variation across different runs for a same translation task can exceed what we observe for models trained for different translation tasks"
  - [section 3.2] Description of Dcpsl (sub-layer-wise) and Dcptok (token-wise) decomposition methods
  - [corpus] Different correlation patterns and p-values between Dcpsl and Dcptok setups, with some setups showing p-value differences of up to two orders of magnitude
- Break condition: If both decomposition approaches consistently yield similar correlation patterns and statistical significance across all tested models and metrics, this mechanism would be weakened.

## Foundational Learning

- Concept: Linear algebra and vector space geometry
  - Why needed here: The entire study relies on decomposing high-dimensional embeddings into linear combinations and analyzing their geometric properties through vector norms and angles
  - Quick check question: Can you explain why the cosine similarity between two vectors ranges from -1 to 1 and what this means geometrically?

- Concept: Transformer architecture and residual connections
  - Why needed here: The decomposition methods exploit the residual connections and layer normalization structure of Transformers to attribute contributions to different components
  - Quick check question: How do residual connections in Transformers enable the linear decomposition approaches described in this paper?

- Concept: Statistical correlation and significance testing
  - Why needed here: The paper uses Spearman correlation, Pearson correlation, and permutation tests to establish relationships between geometric properties and model performance
  - Quick check question: What is the key difference between Spearman and Pearson correlation, and when would you use each?

## Architecture Onboarding

- Component map:
  - Tatoeba Challenge corpus -> Marian-NMT training -> Checkpoint saving every 1000 steps -> Embedding extraction -> Decomposition (Dcpsl/Dcptok) -> Scalar indicator computation -> Correlation analysis with performance metrics

- Critical path: Training -> Checkpoint generation -> Embedding extraction -> Decomposition -> Scalar indicator calculation -> Correlation analysis with performance metrics

- Design tradeoffs:
  - Computational efficiency vs. precision: Using 64-bit floats for numerical stability despite fp16 training
  - Sentence-level vs. corpus-level analysis: Sentence-level provides granularity but is noisier; corpus-level provides clearer patterns but less specific insights
  - Different decomposition methods capture different aspects but may lead to conflicting interpretations

- Failure signatures:
  - Numerical instability: Decomposition terms not summing to original embeddings within tolerance
  - Poor correlation patterns: Unexpected weak or inverse correlations between geometry and performance
  - High variance across seeds: Inconsistent patterns even with identical training conditions

- First 3 experiments:
  1. Verify decomposition numerical stability by checking that decomposed embeddings reconstruct original embeddings within tolerance
  2. Compare correlation patterns between corpus-level and sentence-level performance across different decomposition methods
  3. Test DTW distance patterns between different training runs to establish model-specific vs. training-condition-specific geometry

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do Transformer embeddings consistently reflect model quality across different random initializations?
- Basis in paper: [explicit] The paper shows high variability across different training runs and random seeds for Russian-to-English models
- Why unresolved: The study only tested three random seeds for Russian-to-English; results may not generalize to other language pairs or more seeds
- What evidence would resolve it: Testing multiple random seeds across various language pairs and architectures to establish if variability patterns persist

### Open Question 2
- Question: What causes the high variability in geometric measurements across similar training conditions?
- Basis in paper: [inferred] The authors note that similar training conditions don't guarantee similar vector spaces and observe comparable variation across different datasets
- Why unresolved: The paper doesn't investigate the underlying causes of geometric variability
- What evidence would resolve it: Analyzing internal model states, attention patterns, or weight distributions to identify sources of geometric differences

### Open Question 3
- Question: Are linear decomposition approaches more or less effective for explaining model behavior than non-linear approaches?
- Basis in paper: [explicit] The authors compare two linear decomposition methods and note they suggest different interpretations of decoder behavior
- Why unresolved: The study only examines linear decomposition methods; no comparison with non-linear explanation methods
- What evidence would resolve it: Direct comparison of linear vs. non-linear explanation methods on the same models and tasks

### Open Question 4
- Question: How do geometric patterns relate to specific linguistic phenomena in machine translation?
- Basis in paper: [inferred] The study focuses on general geometric patterns but doesn't examine their relationship to linguistic phenomena like syntax or semantics
- Why unresolved: The paper uses only general performance metrics without linguistic analysis
- What evidence would resolve it: Analyzing geometric patterns in relation to specific linguistic phenomena like agreement, word order, or semantic consistency

### Open Question 5
- Question: Can geometric patterns predict model generalization to out-of-distribution data?
- Basis in paper: [inferred] The authors note that geometry reflects model-specific characteristics rather than sentence-specific computations
- Why unresolved: The study only examines in-distribution performance and doesn't test generalization
- What evidence would resolve it: Testing models on out-of-domain data and analyzing whether geometric patterns correlate with generalization performance

## Limitations
- Findings are based on decoder-only Transformer embeddings from machine translation, limiting generalizability to other architectures and tasks
- Analysis focuses on static embeddings at specific checkpoints, not examining how geometric patterns evolve during training
- Only two specific linear decomposition methods were tested, leaving open the possibility that other approaches might yield different insights

## Confidence

**High confidence**: The observation that corpus-level geometric indicators correlate with overall model performance (BLEU, COMET, chrF++ scores) is well-supported by consistent correlation patterns across multiple models and decomposition methods. The finding that sentence-level correlations are weaker and more variable also shows strong empirical support.

**Medium confidence**: The claim that geometry reflects model-specific characteristics more than training conditions is supported by DTW distance analysis showing high variability across runs with identical training setups. However, the interpretation could be influenced by factors not controlled for in the study, such as subtle differences in initialization or optimization trajectories.

**Low confidence**: The broader implication that geometry-based approaches are not useful for explaining Transformer behavior may be overstated. While the study demonstrates limitations in the specific approaches tested, it does not rule out the possibility that carefully designed geometric analyses or alternative decomposition methods could provide meaningful insights.

## Next Checks

1. **Cross-task generalization test**: Apply the same decomposition and correlation analysis to Transformer models trained on different NLP tasks (sentiment analysis, question answering, text classification) to determine whether the observed model-specific geometric patterns extend beyond machine translation.

2. **Training dynamics analysis**: Track the evolution of scalar indicators and their correlation with performance metrics throughout the entire training process (not just at checkpoints) to understand whether geometric patterns stabilize over time and if convergence occurs across different runs.

3. **Alternative decomposition validation**: Implement and compare at least one additional decomposition method (such as non-linear or attention-based decompositions) to test whether the limitations observed are specific to the linear decomposition approaches used in this study or represent more fundamental constraints on geometry-based analysis of Transformer embeddings.