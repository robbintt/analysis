---
ver: rpa2
title: Towards Phytoplankton Parasite Detection Using Autoencoders
arxiv_id: '2303.08744'
source_url: https://arxiv.org/abs/2303.08744
tags:
- detection
- plankton
- phytoplankton
- anomalies
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting phytoplankton parasites
  from high-volume imaging data, where parasites are rare and difficult to annotate.
  The authors propose an unsupervised anomaly detection approach based on autoencoders
  that are trained only on healthy plankton images.
---

# Towards Phytoplankton Parasite Detection Using Autoencoders

## Quick Facts
- arXiv ID: 2303.08744
- Source URL: https://arxiv.org/abs/2303.08744
- Reference count: 40
- This paper proposes an unsupervised anomaly detection approach for detecting phytoplankton parasites using autoencoders, achieving F1 score of 0.75 compared to 0.86 for supervised methods.

## Executive Summary
This paper addresses the challenge of detecting phytoplankton parasites from high-volume imaging data, where parasites are rare and difficult to annotate. The authors propose an unsupervised anomaly detection approach based on autoencoders that are trained only on healthy plankton images. The method uses VQVAE models to reconstruct input images, then extracts features from the difference between original and reconstructed images to detect anomalies via one-class classifiers. Evaluated on nine phytoplankton species, the approach achieved an F1 score of 0.75, with species-specific tuning improving results further. For comparison, a supervised Faster R-CNN object detector was also tested, achieving an F1 score of 0.86 but requiring parasite-labeled training data.

## Method Summary
The approach trains a VQVAE autoencoder exclusively on healthy (non-parasitized) plankton images with data augmentation. The trained model then reconstructs both healthy and potentially parasite-containing samples. The difference between original and reconstructed images is computed and processed with HardNet feature extractors to create discriminative representations. These features are fed into a one-class classifier (Local Outlier Factor) trained only on healthy samples, which flags samples with low probability as anomalies. The method is designed to detect previously unseen anomalies without requiring labeled anomalous data, making it suitable for large-scale plankton monitoring where parasite data is scarce.

## Key Results
- The autoencoder-based approach achieved an F1 score of 0.75 for parasite detection across nine phytoplankton species
- Supervised Faster R-CNN achieved higher performance (F1: 0.86) but requires labeled parasite data
- Species-specific fine-tuning improved detection performance for 6 out of 9 species
- The method successfully detects anomalies by leveraging reconstruction errors, with parasites showing larger reconstruction differences than healthy samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Autoencoders trained only on healthy plankton images can detect parasites by poor reconstruction of anomalous samples.
- Mechanism: The autoencoder learns a compressed representation of healthy plankton. When it encounters parasites or other anomalies, it cannot reconstruct them well, resulting in large reconstruction errors.
- Core assumption: Anomalies introduce features not present in the training distribution of healthy samples, causing reconstruction errors.
- Evidence anchors:
  - [abstract]: "The method uses VQVAE models to reconstruct input images, then extracts features from the difference between original and reconstructed images to detect anomalies"
  - [section 3.1.1]: "we hypothesize that data from the NOK class will be reconstructed worse than the data from the OK class"
- Break condition: If anomalies are structurally similar to healthy samples, the autoencoder may reconstruct them acceptably.

### Mechanism 2
- Claim: Using difference images between original and reconstructed samples provides discriminative features for anomaly detection.
- Mechanism: The pixel-wise difference highlights areas where the autoencoder failed to reconstruct accurately, which likely correspond to anomalies.
- Core assumption: The difference image contains meaningful information about the location and nature of anomalies.
- Evidence anchors:
  - [abstract]: "The method uses VQVAE models to reconstruct input images, then extracts features from the difference between original and reconstructed images"
  - [section 3.1.2]: "A difference image between the original sample and the reconstructed sample is computed and used in the feature extraction"
- Break condition: If differences are dominated by noise or subtle variations in healthy samples, feature extraction may become unreliable.

### Mechanism 3
- Claim: One-class classifiers trained only on healthy samples can effectively discriminate between normal and anomalous plankton.
- Mechanism: Classifiers learn the distribution of features from healthy samples and flag samples with low probability as anomalies.
- Core assumption: Healthy plankton have lower intra-class variation than anomalies, making them separable in feature space.
- Evidence anchors:
  - [section 3.1.3]: "All classiﬁers are ﬁt on the dataset containing only OK samples"
  - [section 3.2]: "the architecture is supplemented by a one-class classiﬁcation module based on the predicted object labels"
- Break condition: If healthy samples are heterogeneous or anomalies share similar features with healthy samples, classification accuracy may degrade.

## Foundational Learning

- Concept: Autoencoder architecture and training
  - Why needed here: Understanding how autoencoders compress and reconstruct data is essential for interpreting their behavior on anomalies.
  - Quick check question: What happens to reconstruction quality when an autoencoder trained on healthy samples encounters data with features it hasn't seen before?

- Concept: Anomaly detection vs object detection
  - Why needed here: This work uses unsupervised anomaly detection instead of supervised object detection, which has implications for data requirements and generalizability.
  - Quick check question: What is the key difference in training data requirements between anomaly detection and object detection approaches?

- Concept: Feature extraction and similarity metrics
  - Why needed here: The pipeline uses feature extractors like HardNet to create discriminative representations from difference images for classification.
  - Quick check question: How do feature extractors like HardNet transform difference images into vectors suitable for one-class classification?

## Architecture Onboarding

- Component map:
  - Data preprocessing → Autoencoder (VQVAE) → Difference image computation → Feature extraction (HardNet) → One-class classification (LOF)
  - Alternative path: Data preprocessing → Faster R-CNN → Object detection + one-class classification

- Critical path:
  1. Train autoencoder on healthy plankton only
  2. Compute difference images for all samples
  3. Extract features using HardNet
  4. Apply one-class classifier to detect anomalies

- Design tradeoffs:
  - VQVAE vs VAE vs basic AE: VQVAE provides discrete latent space which may better separate classes
  - HardNet vs error metrics vs SIFT: HardNet captures local structure while error metrics are simpler
  - LOF vs SVM vs Isolation Forest: LOF is density-based and more sensitive to local structure

- Failure signatures:
  - Low F1 score across species: Autoencoder may be reconstructing anomalies too well
  - High false positive rate: Feature extractor or classifier may be too sensitive
  - Poor performance on specific species: Species-specific optimization may be needed

- First 3 experiments:
  1. Train basic AE on healthy samples, test reconstruction quality on anomalies
  2. Compare feature extraction methods (HardNet vs error metrics) on a single species
  3. Evaluate one-class classifiers (LOF vs SVM vs IF) on extracted features

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different autoencoder architectures perform in detecting various types of phytoplankton anomalies beyond parasites, such as physical damage or chemical stress indicators?
- Basis in paper: [explicit] The paper mentions that the autoencoder method detects anomalies broadly but does not specifically address different anomaly types or their varying detectability.
- Why unresolved: The study focuses on putative parasites, and the dataset contains only a limited variety of anomalies, making it unclear whether the method generalizes to other anomaly types.
- What evidence would resolve it: Testing the autoencoder approach on datasets with diverse anomaly types (e.g., physical damage, chemical stress) and comparing detection performance across these categories.

### Open Question 2
- Question: What is the optimal threshold for anomaly detection when balancing false positives and false negatives in ecological monitoring applications?
- Basis in paper: [explicit] The paper uses the Equal Error Rate (EER) for threshold selection but acknowledges that the optimal threshold may vary depending on the application context.
- Why unresolved: The study does not explore the impact of different threshold settings on ecological monitoring outcomes or provide guidance on threshold selection for specific use cases.
- What evidence would resolve it: Evaluating the method's performance under different threshold settings in real-world monitoring scenarios and assessing the trade-offs between false positives and false negatives.

### Open Question 3
- Question: How does the spatial resolution of imaging devices affect the detection accuracy of small anomalies like parasites?
- Basis in paper: [explicit] The paper notes that parasites are often small compared to the image size and limited spatial resolution poses a challenge for detection.
- Why unresolved: The study uses data from a specific imaging device (IFCB) and does not explore how varying spatial resolutions might impact detection performance.
- What evidence would resolve it: Conducting experiments with imaging devices of different spatial resolutions and analyzing the impact on detection accuracy for small anomalies.

## Limitations
- The unsupervised approach achieves lower detection accuracy (F1: 0.75) compared to supervised methods (F1: 0.86)
- Performance varies significantly across species, requiring species-specific fine-tuning for optimal results
- Limited evaluation on only known parasite types, with unclear generalizability to previously unseen anomalies

## Confidence
- **High Confidence:** The core mechanism of using reconstruction error for anomaly detection is well-established in the literature and demonstrated empirically.
- **Medium Confidence:** Species-specific performance variations and the effectiveness of fine-tuning are supported by results but require further validation across more diverse plankton types.
- **Low Confidence:** The claim that this approach can detect previously unseen anomalies is theoretical; the evaluation only tested known parasite types.

## Next Checks
1. Test the approach on previously unseen parasite types to validate generalization claims beyond known anomalies.
2. Conduct ablation studies comparing VQVAE against simpler autoencoder architectures to quantify the benefit of discrete latent spaces.
3. Evaluate performance across multiple imaging modalities (different flow cytometers or microscopy systems) to assess robustness to data source variations.