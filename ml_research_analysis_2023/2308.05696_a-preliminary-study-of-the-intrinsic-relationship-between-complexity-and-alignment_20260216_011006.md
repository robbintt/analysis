---
ver: rpa2
title: A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment
arxiv_id: '2308.05696'
source_url: https://arxiv.org/abs/2308.05696
tags:
- complexity
- instruction
- instructions
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the impact of complexity in instruction tuning
  for large language models. The authors propose Tree-Instruct, a method to systematically
  increase instruction complexity by adding nodes to the semantic tree of instructions.
---

# A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment

## Quick Facts
- arXiv ID: 2308.05696
- Source URL: https://arxiv.org/abs/2308.05696
- Reference count: 40
- Key outcome: Increasing instruction complexity through semantic tree node addition improves LLM performance, with a 24% win rate increase using 10-node complex instructions versus simple ones

## Executive Summary
This paper investigates how instruction complexity affects large language model alignment through a method called Tree-Instruct, which systematically increases instruction complexity by adding nodes to semantic trees. The study reveals that increasing complexity consistently improves performance, with a few complex instructions outperforming many simple ones under the same token budget. Surprisingly, curriculum instruction tuning shows limited effectiveness compared to directly training on complex instructions.

## Method Summary
The study uses Tree-Instruct, a method that transforms instructions into semantic trees and systematically adds nodes to increase complexity while maintaining thematic consistency. The process involves three steps: tree construction from original instructions, node expansion where additional nodes (primarily nouns and verbs) are added, and tree sentenceization to convert modified trees back to natural language instructions. The authors generate complex instruction sets with 3, 6, and 10 additional nodes and fine-tune LLaMA-13B-v1 on these datasets using AdamW optimizer. Performance is evaluated using the AlpacaEval leaderboard against text-davinci003.

## Key Results
- Adding 10 nodes to 1,000 instructions resulted in a 24% increase in win rate
- Under the same token budget, a few complex instructions outperform diverse yet simple instructions
- Curriculum instruction tuning may not be as effective as previously thought; direct training on complex instructions is more beneficial

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing complexity by adding semantic tree nodes consistently improves LLM performance
- Mechanism: Adding nodes increases structural depth and width, forcing the model to handle more intricate reasoning and compositional structures
- Core assumption: Semantic tree complexity is a reliable proxy for instruction complexity and directly correlates with performance gains
- Evidence anchors:
  - "Increasing complexity consistently leads to sustained performance improvements. For instance, using 1,000 instruction data and 10 nodes resulted in a substantial 24% increase in win rate."
  - "Tree-Instruct encompasses three steps: Step 1: Tree Construction... Step 2: Nodes Expansion... Step 3: Tree Sentenceization..."
- Break condition: If adding nodes introduces semantic drift or noise that confuses the model

### Mechanism 2
- Claim: A few complex instructions outperform many simple ones under the same token budget
- Mechanism: Complex instructions pack more semantic diversity and structural richness into fewer tokens, leading to more efficient learning per token
- Core assumption: Token efficiency scales with complexity due to richer representation per instruction
- Evidence anchors:
  - "Under the same token budget, a few complex instructions outperform diverse yet simple instructions."
  - "We find that as the complexity increases, the number of tokens also increases. Adding 10 nodes in the tree increases the average token length of samples from 186 to 607."
- Break condition: If the model overfits to specific complexity patterns and fails to generalize

### Mechanism 3
- Claim: Curriculum learning from easy to hard may be ineffective; training on complex instructions alone suffices
- Mechanism: Large LLMs with sufficient parameters can learn directly from complex instructions without needing a gradual ramp-up
- Core assumption: Model capacity is high enough that simpler instructions are redundant for learning
- Evidence anchors:
  - "Curriculum instruction tuning might not yield the anticipated results; focusing on increasing complexity appears to be the key."
  - "We try curriculum learning by gradually training samples on harder samples... With the same training steps, the curriculum learning approach does outperform training with a mixed difficulty of samples but still falls short compared to directly training with the added ten-nodes samples."
- Break condition: If model size or data quality is insufficient, making simpler instructions necessary

## Foundational Learning

- Concept: Semantic parsing and tree structures in NLP
  - Why needed here: The method relies on transforming instructions into semantic trees to systematically increase complexity
  - Quick check question: What is the difference between dependency trees and semantic trees, and which does Tree-Instruct use?

- Concept: Instruction tuning and alignment in LLMs
  - Why needed here: The study is about how complexity in instruction data affects alignment quality
  - Quick check question: How does instruction tuning differ from standard supervised fine-tuning?

- Concept: Curriculum learning principles
  - Why needed here: The paper tests whether curriculum learning (easy to hard) is beneficial or not
- Quick check question: In what scenarios does curriculum learning typically help, and why might it fail here?

## Architecture Onboarding

- Component map: Alpaca-1K → Tree-Instruct augmentation → GPT-4 generation → LLaMA-13B training → AlpacaEval evaluation
- Critical path: Generate complex instructions → fine-tune model → evaluate win rate vs text-davinci003
- Design tradeoffs: Complexity vs consistency (more nodes = more complex but risk thematic drift), token budget (complex vs many simple instructions), curriculum vs direct complexity
- Failure signatures: Performance plateau despite added nodes (overfitting), win rate drop (complexity harms coherence), slow convergence (curriculum ineffective)
- First 3 experiments:
  1. Generate Tree-3, Tree-6, Tree-10 versions of 1,000 instructions and measure consistency with GPT-4
  2. Fine-tune LLaMA-13B on each complexity level and evaluate on AlpacaEval
  3. Compare Alpaca-4K (4x simple) vs Tree-10-Nodes under same token budget to test efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scaling law of instruction complexity behave beyond the tested range of 3, 6, and 10 added nodes?
- Basis in paper: [explicit] The authors observe consistent improvements in win rate as complexity increases from 3 to 6 to 10 nodes, but do not explore beyond this range
- Why unresolved: The study only tested three complexity levels, leaving open whether the scaling law continues indefinitely or plateaus
- What evidence would resolve it: Experiments testing additional complexity levels (e.g., 15, 20, 30 nodes) to determine if improvements continue or saturate

### Open Question 2
- Question: What is the optimal balance between complexity and diversity in instruction tuning?
- Basis in paper: [inferred] The authors find that complex instructions outperform diverse simple ones under the same token budget, but don't explore the full trade-off space
- Why unresolved: The study only compares extremes (few complex vs. many simple instructions), not intermediate points
- What evidence would resolve it: Experiments systematically varying both complexity and diversity while holding total tokens constant

### Open Question 3
- Question: Why does curriculum learning show limited effectiveness for instruction tuning compared to other NLP tasks?
- Basis in paper: [explicit] The authors observe that easy-to-hard curriculum learning only slightly outperforms mixed-difficulty training
- Why unresolved: The study doesn't investigate the underlying reasons for this difference from other tasks where curriculum learning is effective
- What evidence would resolve it: Analysis of model learning dynamics during curriculum training vs. mixed training, potentially using techniques like learning curve analysis or attention visualization

## Limitations

- The semantic tree complexity proxy may not generalize beyond instruction-following tasks
- The study uses a single base model (LLaMA-13B) and dataset size (1K), limiting broader applicability
- The GPT-4 evaluation introduces potential circularity since it's also used to generate complex instructions

## Confidence

- High confidence: The observation that complexity increases can improve performance within controlled experimental settings
- Medium confidence: The claim that a few complex instructions outperform many simple ones under token budget constraints
- Low confidence: The assertion that curriculum learning is ineffective

## Next Checks

1. Test the Tree-Instruct method on different base model sizes (7B vs 13B vs 70B) to determine if the complexity scaling benefits are model-dependent
2. Replicate the complexity vs. diversity trade-off experiment with varying token budgets (2x, 4x, 8x) to validate the efficiency claim across different resource constraints
3. Evaluate the curriculum learning negative result using alternative progression strategies (e.g., stage-wise training, difficulty thresholds) to determine if the failure was methodological rather than fundamental