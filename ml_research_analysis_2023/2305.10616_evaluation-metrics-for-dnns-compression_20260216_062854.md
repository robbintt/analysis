---
ver: rpa2
title: Evaluation Metrics for DNNs Compression
arxiv_id: '2305.10616'
source_url: https://arxiv.org/abs/2305.10616
tags:
- compression
- energy
- metrics
- techniques
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces NetZIP, a standardised evaluation framework
  for neural network compression techniques. The key contribution is the introduction
  of two novel metrics: CHATS (Compression and Hardware Agnostic Theoretical Speed)
  and OCS (Overall Compression Success).'
---

# Evaluation Metrics for DNNs Compression

## Quick Facts
- arXiv ID: 2305.10616
- Source URL: https://arxiv.org/abs/2305.10616
- Reference count: 31
- Primary result: Introduces NetZIP framework with two novel metrics (CHATS and OCS) for standardized DNN compression evaluation

## Executive Summary
This paper presents NetZIP, a standardized evaluation framework for neural network compression techniques that addresses the critical gap of inconsistent evaluation metrics in the field. The framework introduces two novel metrics: CHATS (Compression and Hardware Agnostic Theoretical Speed) for hardware-agnostic speed estimates, and OCS (Overall Compression Success) for combining multiple compression benefits into a single score. Through three case studies on object classification, object detection, and edge devices, the authors demonstrate that their metrics provide a more holistic evaluation compared to existing approaches.

## Method Summary
NetZIP implements a three-stage evaluation pipeline: Train (model training/pre-training), Compress (applying compression techniques), and Compare (evaluating compressed models using a comprehensive metrics suite). The framework supports various compression techniques including pruning, quantization, knowledge distillation, and tensor decomposition, and evaluates them across multiple dimensions including accuracy, size, speed, and energy consumption. The library includes baseline datasets, models, and compression techniques, with outputs including log files and plots for analysis.

## Key Results
- Introduces OCS metric that combines accuracy retention, size reduction, speed improvement, and energy efficiency into a single weighted score
- Develops CHATS metric that provides hardware-agnostic speed estimates based on operation counts rather than measured latency
- Demonstrates framework effectiveness through three case studies showing more holistic evaluation compared to existing approaches
- Standardizes evaluation process across various hardware platforms and application scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NetZIP standardizes DNN compression evaluation by providing a unified framework that combines multiple metrics across accuracy, size, speed, and energy.
- Mechanism: The framework implements a comprehensive set of evaluation metrics (Top-k accuracy, precision, recall, F1 score, mAP, inference latency, FLOPs, MACs, disk size, parameters count, CPU/GPU usage, power, energy) into a single library that allows consistent comparison across different compression techniques and hardware platforms.
- Core assumption: A single framework can meaningfully capture and compare the trade-offs between different compression techniques without introducing bias toward any particular category.
- Evidence anchors:
  - [abstract] "The framework addresses the lack of standardised evaluation metrics in the field by implementing a comprehensive set of metrics covering accuracy, size, speed, energy, and other factors."
  - [section] "Using NetZIP, one can train a model from scratch or use a pretrained model and extend its training. The trained model is saved, and then in the 'Compress' stage this saved model is used as the starting uncompressed model for every different compression technique used."

### Mechanism 2
- Claim: The OCS (Overall Compression Success) metric provides a single score that combines multiple compression benefits, enabling quick comparison between techniques.
- Mechanism: OCS aggregates multiple dimensions of compression performance (accuracy retention, size reduction, speed improvement, energy efficiency) into a single weighted score that represents overall compression success.
- Core assumption: A weighted aggregation of different metrics can meaningfully represent the "success" of a compression technique across different use cases.
- Evidence anchors:
  - [abstract] "The OCS metric combines multiple compression benefits into a single score"
  - [section] "For PTQ, QAT and GUPL1 compression techniques the accuracy is maintained more or less the same for both CIFAR10 and ImageNet1k, whilst for GUP R the drop in accuracy is clearly more significant."

### Mechanism 3
- Claim: CHATS (Compression and Hardware Agnostic Theoretical Speed) provides hardware-agnostic speed estimates that enable fair comparisons across different platforms.
- Mechanism: CHATS calculates theoretical speed improvements based on operation counts (FLOPs, MACs) rather than measured inference latency, which can vary due to hardware differences and resource utilization.
- Core assumption: Operation counts can reliably predict speed improvements independent of hardware characteristics.
- Evidence anchors:
  - [abstract] "CHATS provides hardware-agnostic speed estimates"
  - [section] "For quantisation techniques, however, only the numerical representation type changes in the neural network but the number of parameters stay the same in the model. Even though quantisation does provide improvement in speed, calculating the number of operations may not provide any insight of that."

## Foundational Learning

- Concept: Metrics standardization and benchmarking
  - Why needed here: Without standardized metrics, comparing compression techniques across different papers and implementations becomes unreliable and inconsistent
  - Quick check question: What are the key dimensions that need to be measured when evaluating neural network compression?

- Concept: Operation counting (FLOPs, MACs) vs. empirical measurement
  - Why needed here: Understanding when to use theoretical operation counts versus actual measured latency is crucial for proper evaluation of compression techniques
  - Quick check question: When would operation counts be more reliable than measured inference time for comparing compression techniques?

- Concept: Hardware platform dependencies in DNN evaluation
  - Why needed here: Different hardware platforms (CPU, GPU, edge devices) can significantly affect performance measurements, making cross-platform comparisons challenging
  - Quick check question: How might the same compression technique show different performance metrics on a Raspberry Pi versus a desktop PC?

## Architecture Onboarding

- Component map: Train -> Compress -> Compare, where each stage must complete successfully before the next can begin
- Critical path: Train → Compress → Compare, with the Compare stage generating log files and plots for analysis
- Design tradeoffs: The framework trades comprehensiveness (supporting many metrics and compression techniques) against complexity (users must understand multiple metrics and their implications)
- Failure signatures: Inconsistent results across runs may indicate resource utilization issues; missing metrics may indicate incomplete compression technique implementation; poor performance on edge devices may indicate hardware compatibility issues
- First 3 experiments:
  1. Run basic object classification with ResNet18 on CIFAR10 using all baseline compression techniques to verify the framework works
  2. Compare PTQ vs QAT on the same model to understand quantization effects
  3. Test edge device performance by running YOLOv5s on Raspberry Pi with different quantization levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the best practices for removing pruned parameters from neural network architectures without causing errors, and how can this process be automated in a model-agnostic way?
- Basis in paper: [explicit] The paper discusses the current challenge of removing zeroed parameters from pruned neural network architectures and mentions efforts like DeGraph that aim to generalize this process.
- Why unresolved: The paper states that removing pruned parameters currently relies on manual, case-dependent schemes and is an active area of research. There is no standardized method that works across different architectures.
- What evidence would resolve it: Development and demonstration of a model-agnostic technique that can automatically remove pruned parameters without causing errors across various neural network architectures.

### Open Question 2
- Question: How can metrics be developed to quantify functional equivalence pre- and post-compression for verification purposes in safety-critical applications?
- Basis in paper: [explicit] The paper identifies the need for verification metrics to ensure that compressed models maintain functional equivalence to their original versions, especially for safety-critical applications.
- Why unresolved: The paper notes that while existing metrics evaluate various aspects of compressed models, they do not directly quantify functional equivalence in the context of verification. This gap is currently overlooked by the research community.
- What evidence would resolve it: Creation and validation of metrics that can reliably measure functional equivalence between uncompressed and compressed models, ensuring they meet safety-critical requirements.

### Open Question 3
- Question: Why does quantization sometimes result in increased inference latency and energy consumption, contrary to expectations, and how can this be mitigated?
- Basis in paper: [explicit] The paper observes that in some cases, quantization using TensorFlow Lite led to increased latency and energy consumption compared to FP32 implementations, which is counter-intuitive.
- Why unresolved: The paper speculates that this may be due to configuration issues in the operating system or hardware that are not optimized for quantized models, but does not provide a definitive explanation or solution.
- What evidence would resolve it: Investigation and demonstration of hardware and software configurations that optimize the performance of quantized models, explaining the factors that lead to increased latency and energy consumption.

## Limitations

- OCS metric weighting scheme may not align with specific application requirements, potentially making the single score misleading
- CHATS theoretical speed estimates may not capture hardware-specific optimizations, particularly for quantization techniques
- The framework's comprehensiveness comes at the cost of complexity, requiring users to understand multiple metrics and their implications

## Confidence

- High confidence in the framework's ability to standardize evaluation processes and provide comprehensive metric coverage
- Medium confidence in the OCS metric's universal applicability, as weighting schemes require domain-specific validation
- Medium confidence in CHATS's theoretical speed estimates, particularly for quantization techniques where real-world performance can deviate significantly

## Next Checks

1. Validate OCS weighting scheme by surveying practitioners to determine if the aggregated scores align with their compression technique selection criteria across different application domains.

2. Compare CHATS theoretical speed estimates against measured inference times on five diverse hardware platforms (CPU, GPU, edge TPU, microcontroller, ASIC) to quantify prediction accuracy.

3. Test NetZIP's reproducibility by having three independent research groups run identical compression experiments on different hardware configurations and comparing metric consistency.