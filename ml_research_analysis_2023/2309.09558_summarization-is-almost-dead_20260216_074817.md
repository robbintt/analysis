---
ver: rpa2
title: Summarization is (Almost) Dead
arxiv_id: '2309.09558'
source_url: https://arxiv.org/abs/2309.09558
tags:
- summaries
- summarization
- llms
- human
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a comprehensive human evaluation to compare
  the performance of large language models (LLMs) and traditional methods in generating
  summaries across five distinct tasks. Using new datasets built from post-2021 data,
  human evaluators compare summaries generated by LLMs, human-written references,
  and fine-tuned models.
---

# Summarization is (Almost) Dead

## Quick Facts
- arXiv ID: 2309.09558
- Source URL: https://arxiv.org/abs/2309.09558
- Reference count: 10
- One-line primary result: LLM-generated summaries strongly outperform both human-written and fine-tuned model summaries across multiple tasks

## Executive Summary
This study conducts comprehensive human evaluations comparing LLM-generated summaries against human-written references and fine-tuned model outputs across five distinct summarization tasks. Using newly constructed post-2021 datasets to avoid training data contamination, the research finds that human evaluators strongly prefer LLM-generated summaries. The analysis reveals that LLMs exhibit better factual consistency and fewer extrinsic hallucinations compared to human-written summaries. The paper concludes that conventional text summarization research focused on improving metric scores is no longer necessary given the high quality of LLM-generated summaries.

## Method Summary
The study builds new post-2021 datasets for five summarization tasks (single-news, multi-news, dialogue, code, cross-lingual) by updating existing dataset sources. It generates summaries using zero-shot GPT-3.5 and GPT-4, along with fine-tuned models (BART, Pegasus, T5, Codet5, MT5, MBART) for each task. Human evaluators conduct pairwise comparisons between summaries, with two annotators per task determining preferences. The evaluation includes factual consistency analysis, distinguishing between intrinsic and extrinsic hallucinations. Preference rates and Cohen's kappa scores measure agreement across all tasks.

## Key Results
- Human evaluators consistently prefer LLM-generated summaries over both human-written and fine-tuned model summaries
- LLM summaries show better factual consistency with notably fewer extrinsic hallucinations (40% vs 62% in multi-news task)
- Fine-tuned models tend to produce summaries of fixed length and miss less salient topics compared to LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs achieve superior factual consistency compared to human-written summaries by reducing extrinsic hallucinations
- Mechanism: LLMs rely on explicit conditioning from source documents, whereas human writers often inject unverified external knowledge
- Core assumption: Human annotators have domain knowledge beyond the provided source, which they unintentionally include
- Evidence anchors:
  - "LLM-generated summaries exhibit better factual consistency and fewer instances of extrinsic hallucinations"
  - "notably higher occurrence of extrinsic hallucinations in tasks where human-written summaries demonstrate poor factual consistency"
- Break condition: If source documents are extremely ambiguous, LLMs might hallucinate internally rather than extrinsically

### Mechanism 2
- Claim: LLMs produce more fluent and coherent summaries than fine-tuned models due to adaptive length and multi-topic coverage
- Mechanism: LLMs dynamically adjust output length based on input information volume and can capture multiple topics
- Core assumption: Fine-tuned models are trained to produce summaries of fixed length and may miss less salient topics
- Evidence anchors:
  - "summaries generated by fine-tuned models... tend to have a fixed and rigid length"
  - "when the input contains multiple topics... LLMs can capture all the topics when generating summaries"
- Break condition: If task requires strict length constraints, LLM flexibility might be penalized

### Mechanism 3
- Claim: Human evaluators strongly prefer LLM-generated summaries because they better meet practical summarization needs
- Mechanism: LLMs produce summaries that are more readable, factually consistent, and comprehensive
- Core assumption: Human evaluators prioritize fluency, factual accuracy, and completeness over other factors
- Evidence anchors:
  - "human evaluators prefer LLM-generated summaries over human-written summaries"
  - "summaries generated by the LLMs consistently outperform both human and summaries generated by fine-tuned models"
- Break condition: If evaluation criteria change to prioritize brevity or specific stylistic elements

## Foundational Learning

- Concept: Hallucination types (intrinsic vs extrinsic)
  - Why needed here: Critical for understanding why LLM summaries show better factual consistency
  - Quick check question: What distinguishes an intrinsic hallucination from an extrinsic hallucination in text summarization?

- Concept: Zero-shot vs fine-tuned model capabilities
  - Why needed here: Explains why LLMs outperform fine-tuned models despite not being task-specific trained
  - Quick check question: How does zero-shot generation capability of LLMs compare to fine-tuned models on summarization tasks?

- Concept: Human evaluation methodology for NLP tasks
  - Why needed here: Understanding pairwise comparison and inter-annotator agreement is essential for interpreting results
  - Quick check question: What does an inter-annotator agreement score of 0.558 indicate about the reliability of human evaluations?

## Architecture Onboarding

- Component map: Data collection -> Model selection (LLMs, fine-tuned models, human) -> Human evaluation pipeline -> Analysis (WinRate, hallucination analysis)
- Critical path: Dataset creation -> Model generation -> Human evaluation -> Analysis -> Interpretation
- Design tradeoffs: Cost of human evaluation vs. automated metrics; using post-2021 data vs. larger datasets
- Failure signatures: Low inter-annotator agreement; inconsistent WinRate patterns; hallucination rates exceeding expected thresholds
- First 3 experiments:
  1. Replicate WinRate analysis on a smaller subset to verify consistency
  2. Conduct hallucination annotation on a subset to validate counting methodology
  3. Test LLM performance on a different summarization task to check generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on summarization tasks involving specialized domains or technical content compared to general news summarization?
- Basis in paper: The paper mentions that most current summarization datasets focus on news articles, scientific articles, or Wikipedia, and calls for incorporating diverse genres and longer documents
- Why unresolved: The experiments only evaluated LLMs on five common text summarization tasks (single-news, multi-news, dialogue, source codes, and cross-lingual)
- What evidence would resolve it: Experiments evaluating LLM performance on specialized domain summarization tasks with domain-specific datasets, comparing against domain experts and domain-specific fine-tuned models

### Open Question 2
- Question: What is the impact of LLM-generated summaries on downstream task performance compared to human-written summaries?
- Basis in paper: The paper mentions "extrinsic evaluation" as a future direction, suggesting measuring summary effectiveness by using them as input to other tasks
- Why unresolved: The study only conducted intrinsic evaluation of summary quality through human preference and factuality checks
- What evidence would resolve it: Experiments measuring task performance (e.g., accuracy on QA tasks, decision-making effectiveness) when using LLM-generated summaries versus human-written summaries as input

### Open Question 3
- Question: How do different LLM architectures and training approaches compare in their summarization capabilities?
- Basis in paper: The paper only evaluated GPT-3, GPT-3.5, and GPT-4, and explicitly notes it couldn't include other popular LLMs due to lack of training data cutoff information
- Why unresolved: The study was limited to OpenAI's models, preventing comparison with other architectures like LLaMA, Claude, or open-source models
- What evidence would resolve it: Comparative evaluation of multiple LLM architectures (including open-source models) on the same summarization tasks with controlled experimental conditions

## Limitations
- Human evaluation, while showing reasonable inter-annotator agreement (0.558), remains subjective and potentially inconsistent across different evaluator pools
- Post-2021 dataset construction may introduce domain shift issues that affect model performance differently
- The analysis focuses on specific LLM models (GPT-3.5 and GPT-4) without exploring the full landscape of available models or prompting strategies

## Confidence
- High Confidence: The finding that human evaluators strongly prefer LLM-generated summaries over both human-written and fine-tuned model summaries
- Medium Confidence: The mechanism explaining superior factual consistency in LLMs (reduced extrinsic hallucinations)
- Medium Confidence: The conclusion that conventional summarization research is "dead" based on these results

## Next Checks
1. Conduct ablation studies varying the number of human evaluators and their domain expertise to test the robustness of preference rates
2. Replicate the hallucination analysis using automated factual consistency checkers to validate human annotations
3. Test additional LLM models and prompting strategies to determine if results generalize beyond GPT-3.5 and GPT-4