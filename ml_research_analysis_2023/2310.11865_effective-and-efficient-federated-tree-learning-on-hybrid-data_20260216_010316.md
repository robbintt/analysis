---
ver: rpa2
title: Effective and Efficient Federated Tree Learning on Hybrid Data
arxiv_id: '2310.11865'
source_url: https://arxiv.org/abs/2310.11865
tags:
- hybridtree
- tree
- data
- guests
- split
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HybridTree, the first federated gradient
  boosting decision tree algorithm designed for hybrid federated learning settings,
  where parties differ in both features and samples. The key insight is that meta-rules
  widely exist in trees, enabling lossless reordering of split features.
---

# Effective and Efficient Federated Tree Learning on Hybrid Data

## Quick Facts
- arXiv ID: 2310.11865
- Source URL: https://arxiv.org/abs/2310.11865
- Reference count: 40
- Key outcome: First federated gradient boosting decision tree algorithm for hybrid federated learning settings that achieves up to 8x speedup while maintaining centralized training accuracy

## Executive Summary
This paper introduces HybridTree, a novel federated gradient boosting decision tree algorithm designed for hybrid federated learning settings where parties differ in both features and samples. The key insight is that meta-rules widely exist in trees, enabling lossless reordering of split features. Based on this observation, HybridTree adopts a layer-level training approach that incorporates knowledge from guests by appending layers to the tree structure, avoiding frequent communication and cryptographic overhead required by node-level solutions. Experiments on both simulated and real-world datasets show that HybridTree achieves comparable accuracy to centralized training while significantly reducing communication and computational costs.

## Method Summary
HybridTree addresses federated tree learning in hybrid settings where parties differ in both features and samples. The method leverages meta-rules in gradient boosted decision trees, enabling lossless reordering of split features. The host trains upper tree layers independently, then guests process lower layers using encrypted gradients. Additively homomorphic encryption enables secure gradient aggregation without data exposure. The layer-level approach reduces communication overhead compared to node-level aggregation while maintaining accuracy comparable to centralized training.

## Key Results
- Achieves up to 8 times speedup compared to baseline federated tree learning approaches
- Maintains comparable accuracy to centralized training across multiple datasets
- Demonstrates stable performance even as the number of guest parties increases
- Particularly effective for tabular data where knowledge can be expressed through meta-rules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Meta-rules enable lossless reordering of split features in gradient boosted decision trees
- Mechanism: When a split condition from guest data becomes a meta-rule, it can be moved to the last layer of the tree without changing prediction outcomes because the prediction becomes deterministic once the meta-rule is satisfied
- Core assumption: P(y|x ∈ S) = P(y|x ∈ (S ∩ Fk)) for any Fk not in split rule S
- Evidence anchors: [abstract] theoretical showing of knowledge incorporation into lower layers, [section 3.2] Theorem proving equivalence between trees with and without meta-rule reordering, [corpus] weak evidence of similar transformation techniques

### Mechanism 2
- Claim: Layer-level training reduces communication overhead compared to node-level aggregation
- Mechanism: Instead of aggregating statistics at each node requiring constant communication, the host trains upper layers independently, then guests only process lower layers once per tree using encrypted gradients
- Core assumption: Tree structure can be segmented such that host handles upper layers and guests handle lower layers without accuracy loss
- Evidence anchors: [abstract] layer-level solution avoiding frequent communication, [section 4.1] design of layer-level solution over node-level aggregation, [corpus] weak evidence of similar communication pattern comparisons

### Mechanism 3
- Claim: Additively homomorphic encryption enables secure gradient aggregation without data exposure
- Mechanism: Host encrypts gradients before sending to guests, who can sum encrypted values using public key operations, then encrypt leaf values for host to decrypt and aggregate
- Core assumption: Homomorphic properties allow mathematical operations on encrypted data without revealing underlying values
- Evidence anchors: [section 4.1] AHE application for gradient protection, [section 4.4] description of encrypted data exchange, [corpus] moderate evidence of established homomorphic encryption in vertical FL literature

## Foundational Learning

- Concept: Gradient boosting decision trees and their deterministic training process
  - Why needed here: Understanding how GBDT builds trees sequentially and uses gradient information to make split decisions is crucial for grasping why meta-rules exist and how layer-level training works
  - Quick check question: In GBDT, what determines the split value at each node and how is the leaf value calculated?

- Concept: Homomorphic encryption and its additively homomorphic properties
  - Why needed here: The security of gradient and leaf value exchange between parties relies on AHE allowing mathematical operations on encrypted data without revealing the underlying values
  - Quick check question: What mathematical operation does additively homomorphic encryption support that makes it suitable for aggregating gradients in this setting?

- Concept: Vertical federated learning data partitioning and privacy constraints
  - Why needed here: Understanding the hybrid FL setting where parties have different features and samples is essential for appreciating why traditional node-level approaches fail and why this layer-level solution is novel
  - Quick check question: In vertical FL, why can't parties simply share their raw data or gradients without encryption?

## Architecture Onboarding

- Component map: Host (upper layers, gradient computation, encryption/decryption, final prediction aggregation) -> Guests (lower layers, encrypted gradient processing, leaf value encryption) -> Communication (two rounds per tree: gradients from host to guests, leaf values from guests to host) -> Security (additively homomorphic encryption for all inter-party data exchange)

- Critical path: 1. Host trains upper layers using local data and labels 2. Host encrypts and sends gradients for last-layer instances to relevant guests 3. Guests decrypt with public key, compute leaf values, re-encrypt 4. Guests send encrypted leaf values back to host 5. Host decrypts and aggregates leaf values into final predictions

- Design tradeoffs: Communication vs computation (layer-level design trades some computational overhead for significantly reduced communication), Security vs efficiency (AHE provides strong security but adds computational cost compared to plaintext operations), Accuracy vs privacy (approach maintains accuracy comparable to centralized training while preserving privacy)

- Failure signatures: Accuracy degradation (could indicate meta-rules don't exist as strongly as assumed or layer separation is suboptimal), High communication overhead (may suggest too many instances require guest processing or encryption/decryption overhead is excessive), Training instability (could indicate issues with encrypted gradient aggregation or leaf value computation)

- First 3 experiments: 1. Run with single guest and single host to verify basic layer-level training works before scaling 2. Compare accuracy and communication costs against FedTree baseline with varying tree depths 3. Test with different numbers of guests (5, 10, 25) to validate scalability claims and identify communication bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HybridTree perform when applied to non-tabular data such as images, text, or graphs where knowledge cannot be easily expressed through meta-rules?
- Basis in paper: [explicit] The paper mentions that the method is well-suited for tabular data as it allows for the representation of knowledge through meta-rules, but when dealing with image, text, and graph data, the knowledge inherent in these types of data often cannot be easily captured by rule-based expressions, rendering the method less applicable in those cases.
- Why unresolved: The paper does not provide any empirical results or theoretical analysis for non-tabular data, and the authors acknowledge this as a limitation.
- What evidence would resolve it: Empirical results comparing HybridTree's performance on non-tabular data versus other federated learning approaches, or theoretical analysis explaining why the meta-rule approach fails for these data types.

### Open Question 2
- Question: What is the impact of varying the depth of trees in HybridTree on model performance and computational efficiency?
- Basis in paper: [explicit] The paper mentions that the depth of trees is set to 7 for the baselines, and the maximum depth for the host is set to 5 and for guests is set to 2 for HybridTree, but does not explore the impact of varying these depths.
- Why unresolved: The paper does not provide a sensitivity analysis or experimental results showing how different tree depths affect the performance and efficiency of HybridTree.
- What evidence would resolve it: Experimental results showing the performance and computational efficiency of HybridTree with different tree depths, and an analysis of the trade-offs between depth, accuracy, and efficiency.

### Open Question 3
- Question: How does HybridTree handle the scenario where guests have overlapping samples and heterogeneous features?
- Basis in paper: [explicit] The paper mentions that the default setting assumes guests share the same feature spaces and have no overlapping samples, but also states that the algorithm does not impose any specific requirements regarding feature and sample spaces across guests.
- Why unresolved: The paper does not provide any experimental results or theoretical analysis for the case where guests have overlapping samples and heterogeneous features.
- What evidence would resolve it: Experimental results showing the performance of HybridTree in scenarios with overlapping samples and heterogeneous features among guests, or theoretical analysis explaining how the algorithm adapts to such scenarios.

## Limitations

- The paper's core claim about lossless meta-rule reordering lacks direct empirical validation through ablation studies
- Communication cost analysis doesn't account for substantial encryption/decryption overhead for large gradient values
- Assumes guests have sufficient data to meaningfully train lower layers without exploring sparse or highly imbalanced guest data scenarios

## Confidence

- **High confidence**: The layer-level training approach reduces communication compared to node-level aggregation, as this follows directly from architectural design
- **Medium confidence**: Accuracy claims are valid for datasets where meta-rules exist strongly, but may not generalize to all tabular data
- **Low confidence**: The scalability analysis for increasing guest numbers doesn't account for encryption computational overhead growth

## Next Checks

1. **Ablation on meta-rule strength**: Run experiments with artificially weakened meta-rules to quantify the accuracy degradation threshold and validate the lossless reordering claim
2. **Encryption overhead measurement**: Instrument the implementation to measure actual AHE computation time vs plaintext operations across different gradient value ranges
3. **Guest data sparsity stress test**: Evaluate performance with varying guest sample sizes (10%, 1%, 0.1% of host data) to identify minimum viable guest data requirements