---
ver: rpa2
title: 'ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End
  Temporal Action Detection'
arxiv_id: '2311.00729'
source_url: https://arxiv.org/abs/2311.00729
tags:
- action
- video
- temporal
- embeddings
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ZEETAD, a novel zero-shot temporal action detection
  method that leverages pretrained vision-language models. The key idea is to integrate
  dual-localization and zero-shot proposal classification modules.
---

# ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection

## Quick Facts
- arXiv ID: 2311.00729
- Source URL: https://arxiv.org/abs/2311.00729
- Reference count: 40
- Key outcome: Achieves 43.2% mAP on THUMOS14 and 32.5% mAP on ActivityNet-1.3 in zero-shot temporal action detection

## Executive Summary
This paper introduces ZEETAD, a novel zero-shot temporal action detection (TAD) framework that leverages pretrained vision-language models to detect and classify action instances from untrimmed videos without requiring training on the target action classes. The key innovation is integrating dual-localization and zero-shot proposal classification modules to jointly optimize action proposal generation and classification in an end-to-end manner. The method significantly outperforms existing zero-shot TAD approaches on THUMOS14 and ActivityNet-1.3 datasets.

## Method Summary
ZEETAD employs a two-stage approach: first, a dual-localization module uses a Deformable DETR encoder-decoder to generate class-agnostic action proposals with boundary regression and actionness prediction; second, a zero-shot proposal classification module generates semantic embeddings from text and frame inputs for each temporal unit using a CLIP-based framework. Lightweight adapters are used to finetune the CLIP text encoder, enhancing discriminative capability on unseen classes. The model is trained end-to-end with boundary regression, actionness prediction, and classification losses.

## Key Results
- Achieves 43.2% average mAP on THUMOS14 dataset
- Achieves 32.5% average mAP on ActivityNet-1.3 dataset
- Outperforms existing state-of-the-art zero-shot TAD methods by significant margins on both datasets

## Why This Works (Mechanism)
The dual-localization module effectively identifies action regions in videos and generates proposals with precise temporal boundaries, while the zero-shot proposal classification module leverages semantic embeddings from vision-language models to classify actions without training on the target classes. The adapter-based finetuning of the CLIP text encoder allows the model to adapt to domain-specific action descriptions, improving classification accuracy for unseen classes.

## Foundational Learning
- **Zero-shot learning**: Why needed - to classify actions without training on target classes; Quick check - verify semantic embeddings align with target action descriptions
- **Vision-language models (CLIP)**: Why needed - to generate semantic embeddings from text and visual inputs; Quick check - ensure CLIP embeddings capture action semantics accurately
- **Temporal action detection**: Why needed - to locate and classify actions in untrimmed videos; Quick check - validate proposal boundaries align with ground truth action segments
- **Adapter-based finetuning**: Why needed - to adapt vision-language models to domain-specific tasks with minimal parameter updates; Quick check - compare performance with and without adapter finetuning
- **Dual-localization**: Why needed - to generate precise action proposals with boundary regression and actionness prediction; Quick check - evaluate proposal quality using IoU metrics
- **Soft-NMS**: Why needed - to remove duplicate proposals and improve detection accuracy; Quick check - verify duplicate removal does not eliminate valid proposals

## Architecture Onboarding
**Component map**: Video frames -> CLIP visual embeddings -> Dual-localization module -> Action proposals -> Semantic embedding generator -> Class probabilities -> Soft-NMS -> Final detections
**Critical path**: CLIP embeddings -> dual-localization (proposal generation) -> semantic embedding generation (classification) -> Soft-NMS (duplicate removal)
**Design tradeoffs**: Using pretrained CLIP models enables zero-shot classification but may introduce domain shift; dual-localization provides precise boundaries but requires careful actionness score calibration
**Failure signatures**: Poor localization from low actionness scores; classification errors from misaligned semantic embeddings; duplicate detections from inadequate Soft-NMS
**First experiments**: 1) Verify CLIP visual embeddings capture action semantics; 2) Test dual-localization module on action proposal generation; 3) Validate semantic embedding generation for zero-shot classification

## Open Questions the Paper Calls Out
### Open Question 1
How does the dual-localization module handle actions with complex hierarchical structures (e.g., sports with multiple sub-actions) compared to simpler actions?
Basis: The paper mentions that actions like "high jump" and "long jump" comprise striding and jumping, and equal attention to these sub-actions could lead to classification uncertainty.
Why unresolved: The paper does not provide quantitative or qualitative comparisons of the model's performance on actions with varying levels of hierarchical complexity.
What evidence would resolve it: Ablation studies or experiments comparing the model's performance on simple actions (e.g., "running") versus complex actions (e.g., "gymnastics routine") with multiple sub-actions would clarify the module's effectiveness.

### Open Question 2
What is the impact of different 3D video encoders (e.g., I3D vs. TSP) on the overall performance of ZEETAD, particularly in terms of action localization accuracy?
Basis: The paper mentions that I3D features are used for THUMOS14 and TSP features for ActivityNet-1.3, but it does not explore the impact of using different encoders on the same dataset or compare their performance directly.
Why unresolved: The choice of 3D video encoder is crucial for capturing temporal and spatial features, but the paper does not investigate whether the performance differences are due to the encoder or other factors.
What evidence would resolve it: Experiments using the same 3D video encoder across both datasets or a direct comparison of I3D vs. TSP on a single dataset would provide insights into the encoder's impact.

### Open Question 3
How does the proposed adapter-based finetuning method compare to other parameter-efficient finetuning techniques (e.g., LoRA, prefix tuning) in terms of zero-shot generalization?
Basis: The paper mentions that adapters are used to finetune the CLIP text encoder and claims they improve performance compared to text prompt tuning. However, it does not compare adapters to other parameter-efficient finetuning methods.
Why unresolved: While adapters are shown to be effective, there may be other finetuning techniques that could yield similar or better results with fewer parameters or faster training times.
What evidence would resolve it: Comparative experiments between adapters, LoRA, prefix tuning, and other parameter-efficient methods on the same zero-shot tasks would clarify their relative effectiveness.

## Limitations
- Adapter-based CLIP text encoder tuning details are not fully specified, affecting reproducibility
- Performance depends heavily on the quality of actionness scores and boundary predictions
- Reliance on pretrained CLIP embeddings may introduce domain shift for fine-grained temporal action detection
- Dual-localization approach assumes consistent temporal coherence within proposals, which may not hold for complex action sequences

## Confidence
- High confidence in the overall framework design and reported performance gains over baselines on THUMOS14 and ActivityNet-1.3
- Medium confidence in the adapter-based CLIP text encoder integration details
- Low confidence in the exact segment generator mask formulation and its impact on semantic embedding generation

## Next Checks
1. Implement ablation studies to isolate the contribution of each module (dual-localization, semantic embedding generation, adapter tuning) to performance gains
2. Test model robustness on videos with complex action sequences, overlapping events, and varying temporal durations to validate generalization beyond benchmark datasets
3. Compare performance against other vision-language models (e.g., BLIP, ALIGN) to verify CLIP's effectiveness for zero-shot temporal action detection