---
ver: rpa2
title: 'Analyzing FOMC Minutes: Accuracy and Constraints of Language Models'
arxiv_id: '2304.10164'
source_url: https://arxiv.org/abs/2304.10164
tags:
- sentiment
- negative
- fomc
- positive
- finbert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed FOMC meeting minutes using advanced language
  models (VADER, FinBERT, GPT-4) to evaluate their accuracy in sentiment detection.
  FinBERT outperformed others in predicting negative sentiment, achieving an AUC of
  0.73 for negative class and 0.68 for neutral/positive classes.
---

# Analyzing FOMC Minutes: Accuracy and Constraints of Language Models

## Quick Facts
- arXiv ID: 2304.10164
- Source URL: https://arxiv.org/abs/2304.10164
- Reference count: 0
- This study analyzed FOMC meeting minutes using advanced language models (VADER, FinBERT, GPT-4) to evaluate their accuracy in sentiment detection.

## Executive Summary
This study evaluates the performance of three language models (VADER, FinBERT, and GPT-4) in analyzing sentiment from FOMC meeting minutes. The research finds that domain-specific fine-tuning significantly improves sentiment detection, with FinBERT achieving an AUC of 0.73 for negative sentiment classification. The study reveals that FOMC texts' complexity and low emotional content pose challenges for existing NLP models, highlighting the need for enhanced models and alternative approaches for financial text analysis.

## Method Summary
The study analyzed 1,065 sentences from FOMC minutes (2006-2014), manually labeled for sentiment across three aspects: Growth, Employment, and Inflation. Three sentiment analysis models were applied: VADER (rule-based), FinBERT (domain-specific fine-tuning), and GPT-4 (prompt-based). Model performance was evaluated using confusion matrices, ROC curves, and AUC scores, comparing predictions against human-labeled ground truth.

## Key Results
- FinBERT achieved the highest accuracy, with AUC of 0.73 for negative sentiment detection
- VADER showed moderate performance with balanced but less accurate sentiment detection
- GPT-4 struggled with context-dependent interpretation, often defaulting to neutral classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FinBERT outperforms VADER and GPT-4 in negative sentiment detection for FOMC minutes due to domain-specific fine-tuning on financial text.
- Mechanism: FinBERT's architecture leverages BERT's bidirectional transformer with pre-training on financial corpora, enabling it to capture nuanced economic terminology and sentiment patterns absent in general-purpose models.
- Core assumption: Financial text has distinct linguistic features (e.g., technical terms, cautious phrasing) that general models misinterpret.
- Evidence anchors:
  - [abstract]: "FinBERT outperformed others in predicting negative sentiment, achieving an AUC of 0.73 for negative class"
  - [section 4.2]: "FinBERT is a pre-trained language model based on the BERT architecture... designed to capture the nuances and complexities of financial language"
  - [corpus]: "average neighbor FMR=0.381" (moderate relevance, suggests financial domain overlap)
- Break condition: If FOMC text diverges significantly from training corpus (e.g., new economic terminology), FinBERT's performance degrades.

### Mechanism 2
- Claim: VADER's rule-based approach fails on FOMC minutes due to low emotional content and complex sentence structures.
- Mechanism: VADER relies on lexical sentiment dictionaries and heuristics suited for social media, not dense economic prose with template-driven language.
- Core assumption: FOMC statements intentionally minimize emotional language, violating VADER's design assumptions.
- Evidence anchors:
  - [abstract]: "VADER showed moderate performance with balanced but less accurate sentiment detection"
  - [section 4.1]: "VADER employs a lexicon of sentiment-related words... effective in analyzing sentiments conveyed through social media posts"
  - [section 3.1]: "the FOMC is careful to avoid expressing emotion in their sentences"
- Break condition: If FOMC minutes adopt more expressive language, VADER's performance might improve.

### Mechanism 3
- Claim: GPT-4 struggles with FOMC sentiment analysis due to lack of interpretability and context-dependent interpretation.
- Mechanism: GPT-4's large language model architecture defaults to neutral classifications when uncertain, lacking transparency in decision-making for specialized financial texts.
- Core assumption: GPT-4's training data insufficiently covers FOMC-specific linguistic patterns.
- Evidence anchors:
  - [abstract]: "GPT-4 struggled with context-dependent interpretation, often defaulting to neutral classification"
  - [section 5.4]: "GPT-4 struggles to accurately discern sentiment in intricate sentences"
  - [corpus]: No direct corpus evidence for GPT-4's training data composition (explicitly weak)
- Break condition: If GPT-4 is fine-tuned on FOMC data, performance may improve but interpretability remains an issue.

## Foundational Learning

- Concept: ROC curve and AUC interpretation
  - Why needed here: Evaluating model performance across different sentiment thresholds
  - Quick check question: What does an AUC of 0.73 indicate about a model's ability to distinguish negative sentiment?

- Concept: Confusion matrix analysis
  - Why needed here: Understanding model classification accuracy across sentiment categories
  - Quick check question: How do true positives and false negatives differ in sentiment analysis context?

- Concept: Domain-specific fine-tuning
  - Why needed here: Adapting pre-trained models to specialized financial text analysis
  - Quick check question: Why might general-purpose sentiment models underperform on FOMC minutes?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline (sentence splitting, stopword removal, filtering) -> Three sentiment analysis models (VADER, FinBERT, GPT-4) -> Labeling system for ground truth sentiment -> Evaluation framework (ROC curves, confusion matrices, AUC scores)

- Critical path:
  1. Preprocess FOMC text into sentences
  2. Apply each sentiment model
  3. Compare predictions against labeled data
  4. Generate evaluation metrics

- Design tradeoffs:
  - VADER: Fast, interpretable, but domain-mismatched
  - FinBERT: Accurate for negatives, moderate for others, computationally heavier
  - GPT-4: Potentially powerful but lacks transparency and over-neutralizes

- Failure signatures:
  - VADER: Skewed toward neutral due to low emotional content
  - FinBERT: Misclassifies positive economic indicators as negative
  - GPT-4: Default neutral classifications for complex sentences

- First 3 experiments:
  1. Compare FinBERT's performance on FOMC vs. general financial text
  2. Test VADER's sensitivity to sentence complexity vs. emotional content
  3. Evaluate GPT-4's performance when provided with sentence context vs. isolation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features or sentence structures in FOMC minutes cause FinBERT and other language models to misclassify sentiment, and can these be systematically identified?
- Basis in paper: [explicit] The paper notes that complex sentence structures and lower emotional content make FOMC minutes challenging for NLP models, and provides examples where FinBERT assigned incorrect sentiment scores (e.g., sentence 2 in Table 1).
- Why unresolved: While the paper identifies general challenges, it doesn't provide a detailed analysis of specific linguistic patterns that consistently cause misclassification.
- What evidence would resolve it: A systematic analysis of misclassified sentences identifying common linguistic patterns (e.g., nested clauses, passive voice, economic jargon) and their correlation with prediction errors.

### Open Question 2
- Question: How would incorporating domain-specific financial knowledge or context improve sentiment analysis accuracy for FOMC minutes?
- Basis in paper: [inferred] The paper suggests exploring alternative approaches such as combining NLP techniques with statistical models or expert opinions, and notes that GPT-4 struggled with context-dependent interpretation.
- Why unresolved: The paper acknowledges the need for enhanced models but doesn't test approaches that integrate financial domain knowledge or contextual understanding.
- What evidence would resolve it: Comparative analysis of sentiment analysis models that incorporate financial domain knowledge (e.g., economic indicators, policy frameworks) versus standard models.

### Open Question 3
- Question: What is the optimal sentence length and structure for accurate sentiment analysis of FOMC minutes?
- Basis in paper: [explicit] The paper describes pre-processing steps that filtered out sentences with fewer than 8 words or 43 characters, suggesting that sentence length affects analysis quality.
- Why unresolved: The paper only mentions filtering criteria but doesn't systematically test how different sentence lengths and structures affect model performance.
- What evidence would resolve it: Empirical testing of sentiment analysis accuracy across sentences of varying lengths and structures to identify optimal characteristics for reliable classification.

## Limitations
- The study relies on a specific, manually labeled dataset of 1,065 FOMC sentences that is not publicly available
- The moderate average neighbor FMR (0.381) suggests the findings may not generalize across all financial text domains
- The study does not account for potential temporal shifts in FOMC communication style between 2006-2014 and current practices

## Confidence
- High Confidence: FinBERT's superior performance in negative sentiment detection (AUC=0.73) is well-supported by the data and aligns with established research on domain-specific fine-tuning.
- Medium Confidence: VADER's poor performance is clearly demonstrated, though the study could benefit from comparing it against more modern rule-based approaches.
- Medium Confidence: GPT-4's limitations are evident, but the study lacks exploration of prompt engineering or fine-tuning strategies that might improve its performance.

## Next Checks
1. Test FinBERT's performance on FOMC minutes from 2015-present to assess temporal generalization.
2. Compare VADER's performance against more recent rule-based models like TextBlob or Pattern on the same dataset.
3. Evaluate whether aggregating aspect-level sentiment scores (Growth, Employment, Inflation) improves overall classification accuracy compared to sentence-level analysis alone.