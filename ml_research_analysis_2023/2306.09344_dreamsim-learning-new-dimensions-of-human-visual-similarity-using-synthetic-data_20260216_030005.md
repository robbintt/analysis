---
ver: rpa2
title: 'DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic
  Data'
arxiv_id: '2306.09344'
source_url: https://arxiv.org/abs/2306.09344
tags:
- image
- similarity
- metric
- dataset
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of measuring perceptual image
  similarity beyond low-level features like color and texture, aiming to capture mid-level
  variations in layout, object pose, and semantic content. To achieve this, the authors
  create a new dataset called NIGHTS, consisting of 20,000 synthetic image triplets
  generated using Stable Diffusion, with human judgments collected through two-alternative
  forced choice (2AFC) and just noticeable difference (JND) tasks.
---

# DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data

## Quick Facts
- arXiv ID: 2306.09344
- Source URL: https://arxiv.org/abs/2306.09344
- Reference count: 40
- Primary result: DreamSim achieves 96.16% accuracy in predicting human similarity judgments on synthetic test set

## Executive Summary
DreamSim is a novel perceptual image similarity metric that goes beyond traditional low-level features to capture mid-level variations in layout, object pose, and semantic content. The authors create a synthetic dataset called NIGHTS using Stable Diffusion, collecting human judgments through 2AFC and JND tasks to ensure cognitively impenetrable, consistent ratings. By fine-tuning an ensemble of large vision models (CLIP, DINO, OpenCLIP) with LoRA on this dataset, DreamSim achieves state-of-the-art alignment with human perceptual similarity judgments while maintaining strong generalization to real images.

## Method Summary
The authors generate 20,000 synthetic image triplets using Stable Diffusion with prompts from various image categories, filtering for cognitively impenetrable triplets with unanimous human judgments. DreamSim is trained by concatenating features from CLIP, DINO, and OpenCLIP models, then applying LoRA fine-tuning on the NIGHTS dataset using hinge loss. The resulting metric achieves high accuracy on human similarity judgments while demonstrating strong performance on real image retrieval and reconstruction tasks.

## Key Results
- DreamSim achieves 96.16% accuracy on the NIGHTS test set, outperforming LPIPS, DISTS, and off-the-shelf embeddings
- The metric is particularly sensitive to foreground objects and semantic content while remaining responsive to color and layout
- DreamSim generalizes well to real images, demonstrating strong performance on COCO retrieval and ImageNet-R tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The synthetic dataset captures cognitively impenetrable similarity judgments that are stable across humans.
- **Mechanism:** By using stable diffusion to generate image triplets from the same prompt and iteratively filtering for unanimous human judgments, the dataset isolates mid-level variations (pose, layout, semantics) while removing low-level noise and high-level categorical differences.
- **Core assumption:** Humans can make automatic, consistent similarity judgments on synthetic images that vary in mid-level attributes but share high-level semantics.
- **Evidence anchors:** [abstract], [section 3.2]
- **Break condition:** If human judgments on synthetic triplets are inconsistent or heavily influenced by low-level or high-level cues rather than mid-level attributes.

### Mechanism 2
- **Claim:** Fine-tuning large vision models with LoRA on the synthetic dataset aligns their perceptual similarity judgments with human perception.
- **Mechanism:** LoRA allows efficient adaptation of massive transformer models (CLIP, DINO, OpenCLIP) by tuning a small subset of parameters, enabling the models to learn human-like similarity judgments without catastrophic forgetting.
- **Core assumption:** The synthetic dataset captures the relevant mid-level similarity structure that these models can learn to predict with minimal parameter changes.
- **Evidence anchors:** [section 4.2], [section 5.1]
- **Break condition:** If the LoRA-tuned models overfit to synthetic data and fail to generalize to real image domains.

### Mechanism 3
- **Claim:** Ensembling multiple large vision models and fine-tuning the ensemble captures complementary perceptual dimensions, improving human alignment.
- **Mechanism:** By concatenating features from DINO, CLIP, and OpenCLIP before LoRA fine-tuning, the ensemble metric leverages each model's strengths (e.g., DINO's disentangled structure, CLIP's semantic understanding) while the fine-tuning adapts them jointly to human judgments.
- **Core assumption:** Different vision models encode complementary information about perceptual similarity, and joint fine-tuning can harmonize these signals.
- **Evidence anchors:** [section 4.2], [section 5.1]
- **Break condition:** If the ensemble does not provide complementary information or fine-tuning degrades individual model performance.

## Foundational Learning

- **Concept: Cognitive penetrability**
  - Why needed here: Understanding why the dataset aims for "cognitively impenetrable" judgments is key to grasping the design goal of automatic, consistent human similarity ratings.
  - Quick check question: Why would cognitive penetrability be important for collecting perceptual similarity data?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: LoRA is the key fine-tuning method used; understanding how it efficiently adapts large models is crucial for the architecture.
  - Quick check question: How does LoRA differ from full fine-tuning in terms of parameter efficiency?

- **Concept: 2AFC (Two-Alternative Forced Choice)**
  - Why needed here: The 2AFC task is the primary experimental paradigm for collecting similarity judgments; understanding its structure is essential.
  - Quick check question: What advantage does 2AFC have over rating scales for measuring perceptual similarity?

## Architecture Onboarding

- **Component map:** Image triplets -> CLIP/DINO/OpenCLIP feature extractors -> Feature concatenation -> LoRA adapters -> Cosine distance -> Similarity score

- **Critical path:**
  1. Load triplet
  2. Extract features from each model
  3. Concatenate features
  4. Apply LoRA projections
  5. Compute cosine distances
  6. Predict which distortion is more similar

- **Design tradeoffs:**
  - Ensemble vs single model: More parameters but better performance
  - LoRA vs full fine-tuning: Much fewer parameters but requires compatible backbones
  - Synthetic vs real data: Cleaner judgments but potential domain gap

- **Failure signatures:**
  - Low 2AFC accuracy indicates poor human alignment
  - Overfitting to synthetic data shows as poor performance on real image retrieval
  - Large gap between LoRA and MLP tuning suggests LoRA is critical

- **First 3 experiments:**
  1. Evaluate baseline CLIP/DINO/OpenCLIP on NIGHTS test set
  2. Train LoRA on ensemble and compare to individual model LoRA
  3. Test generalization on COCO retrieval task

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the improved alignment of DreamSim with human judgments on mid-level perceptual similarity generalize to other domains beyond synthetic images, such as natural scenes, medical imaging, or remote sensing?
- **Basis in paper:** [explicit] The authors demonstrate that DreamSim generalizes to real images and shows strong performance on retrieval and reconstruction tasks, but the evaluation is limited to ImageNet-R and COCO datasets.
- **Why unresolved:** The paper does not explore the performance of DreamSim on other image domains that may have different characteristics or require specialized perceptual judgments.
- **What evidence would resolve it:** Testing DreamSim on diverse datasets from various domains and comparing its alignment with human judgments to other perceptual metrics would provide evidence for its generalization ability.

### Open Question 2
- **Question:** How sensitive is DreamSim to domain shift or distribution mismatch between the synthetic training data and real-world images, and what are the implications for its robustness and reliability in practical applications?
- **Basis in paper:** [inferred] The authors mention that DreamSim is trained on synthetic data and may inherit biases from the pre-trained backbones, but they do not explicitly address the issue of domain shift or its impact on the model's performance.
- **Why unresolved:** The paper does not provide a thorough analysis of DreamSim's robustness to domain shift or distribution mismatch, which is crucial for understanding its limitations and potential failure modes.
- **What evidence would resolve it:** Evaluating DreamSim's performance on datasets with varying degrees of domain shift from the training data, and analyzing its behavior in terms of alignment with human judgments and task-specific performance, would shed light on its robustness and reliability.

### Open Question 3
- **Question:** Can the proposed approach of using synthetic data and human judgments to train perceptual metrics be extended to other perceptual tasks beyond image similarity, such as visual saliency prediction, aesthetic quality assessment, or image captioning?
- **Basis in paper:** [explicit] The authors propose a novel approach for collecting human judgments on image similarity using synthetic data, which could potentially be adapted to other perceptual tasks that involve human preferences or judgments.
- **Why unresolved:** The paper focuses solely on the task of image similarity and does not explore the applicability of the proposed approach to other perceptual tasks.
- **What evidence would resolve it:** Applying the proposed approach to other perceptual tasks, such as visual saliency prediction or aesthetic quality assessment, and comparing the performance of the resulting metrics to existing methods would demonstrate the versatility and effectiveness of the approach.

## Limitations

- The NIGHTS dataset is synthetic and may not fully capture the diversity of real-world perceptual similarity judgments
- The study does not report ablation studies on the individual contributions of each model in the ensemble
- No comparison against recent learned metrics beyond LPIPS and DISTS is provided
- The generalization claims to real images are based on only two datasets (COCO and ImageNet-R)

## Confidence

- **High confidence**: DREAMSIM achieves superior performance on the NIGHTS test set compared to baseline metrics
- **Medium confidence**: DREAMSIM generalizes well to real image retrieval tasks
- **Low confidence**: The synthetic dataset captures "cognitively impenetrable" judgments that are truly automatic and shared across all humans

## Next Checks

1. **Cross-dataset validation**: Test DREAMSIM on established perceptual similarity benchmarks like BAPPS, TID2013, and CSIQ to verify generalization beyond synthetic and curated datasets

2. **Ablation studies**: Evaluate DREAMSIM variants with individual models (CLIP-only, DINO-only, etc.) and different ensemble compositions to quantify the contribution of each component

3. **Human validation**: Conduct new human studies comparing DREAMSIM predictions against judgments on real image triplets, particularly focusing on cases where the metric might disagree with human perception