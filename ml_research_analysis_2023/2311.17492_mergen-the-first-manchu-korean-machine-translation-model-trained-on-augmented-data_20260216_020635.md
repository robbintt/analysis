---
ver: rpa2
title: 'Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented
  Data'
arxiv_id: '2311.17492'
source_url: https://arxiv.org/abs/2311.17492
tags:
- manchu
- translation
- data
- language
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first Manchu-Korean machine translation
  model, Mergen, addressing the critical need for language preservation of the endangered
  Manchu language. The authors tackle the challenge of limited parallel data by augmenting
  their dataset through word replacement using GloVe embeddings trained on both monolingual
  and parallel texts.
---

# Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data

## Quick Facts
- arXiv ID: 2311.17492
- Source URL: https://arxiv.org/abs/2311.17492
- Reference count: 7
- Introduces the first Manchu-Korean machine translation model, achieving 20-30 point BLEU score improvements through data augmentation

## Executive Summary
This paper presents Mergen, the first machine translation model for the critically endangered Manchu language to Korean. The authors address the challenge of extremely limited parallel data (only 12,000 sentences) by developing a data augmentation approach using GloVe embeddings for word replacement. Their experiments demonstrate significant improvements in translation quality, with BLEU scores increasing by 20-30 points compared to training on original data alone. The work represents an important contribution to language preservation efforts for Manchu, an indigenous language of northeastern China with fewer than 20 native speakers.

## Method Summary
The authors tackle the low-resource challenge by augmenting parallel text through synonym replacement using GloVe embeddings trained on both parallel and monolingual Manchu texts. The model employs an encoder-decoder neural machine translation architecture with a bi-directional GRU layer, enhanced with packed sequences and masking techniques. Manchu texts are romanized using Abkai Latin transliteration, while Korean uses Yale romanization. The data augmentation strategy replaces words in training sentences with semantically similar words identified by the GloVe embeddings, effectively expanding the training dataset size and vocabulary coverage.

## Key Results
- BLEU scores exceed 38 on the Manwen Laodang test set after augmentation
- 20-30 point improvement in BLEU scores compared to baseline models
- Significant reduction in perplexity, indicating improved model confidence
- Dictionary corpus performance remains low despite augmentation, suggesting dataset characteristics matter

## Why This Works (Mechanism)

### Mechanism 1: Data Augmentation via Synonym Replacement
- **Claim**: Data augmentation via synonym replacement improves translation performance by expanding vocabulary coverage and reducing data sparsity.
- **Mechanism**: The model replaces words in parallel text with semantically similar words identified by GloVe embeddings, effectively increasing training data size and diversity.
- **Core assumption**: Synonyms found by GloVe embeddings maintain semantic equivalence in context and are valid replacements for training.
- **Evidence anchors**: 
  - "we expand our data by employing word replacement guided by GloVe embeddings"
  - "For each word in the training dataset, we gather the most similar word predicted by each individual GloVe embedding"
  - "BLEU scores exceed 38 on the Manwen Laodang test set"

### Mechanism 2: Typological Similarity Between Manchu and Korean
- **Claim**: Using bilingual corpora with typological similarity (Manchu-Korean) improves translation quality due to structural alignment.
- **Mechanism**: Shared syntactic features like word order and grammatical particles create more predictable translation patterns, making learning easier for the model.
- **Core assumption**: Typological similarities between Manchu and Korean translate to better alignment in parallel sentences.
- **Evidence anchors**: 
  - "The word order of Manchu and Korean mostly coincide, including the order of 'noun-particle,' 'modifier-modified,' and 'object-verb'"
  - "Results on the Manchu-Korean dictionary are consistently very low" (suggests structure matters)

### Mechanism 3: Bi-directional GRU Architecture with Optimizations
- **Claim**: Bi-directional GRU architecture with packed sequences and masking improves translation by better handling variable-length sequences and reducing noise from padding.
- **Mechanism**: Bi-directional processing captures context from both directions; packed sequences prevent padding from affecting RNN computations; masking ignores padding in attention.
- **Core assumption**: The GRU architecture is sufficiently powerful for this low-resource task and benefits from these optimizations.
- **Evidence anchors**: 
  - "Our model is based on the encoder-decoder structure of the NMT (Bahdanau et al., 2016), implemented with bi-directional Gated Recurrent Unit (GRU) layer"
  - Performance metrics show improvement over baseline

## Foundational Learning

- **Word embeddings and similarity metrics**
  - Why needed here: Understanding how GloVe embeddings identify synonyms for data augmentation
  - Quick check question: How does cosine similarity between word vectors indicate semantic similarity?

- **Encoder-decoder architecture and sequence-to-sequence learning**
  - Why needed here: Core understanding of how the translation model processes input and generates output
  - Quick check question: What role does the attention mechanism play in connecting encoder and decoder states?

- **Evaluation metrics for machine translation (BLEU, perplexity)**
  - Why needed here: Interpreting experimental results and understanding model performance
  - Quick check question: What does a BLEU score of 0.0 indicate about model predictions?

## Architecture Onboarding

- **Component map**: Input Manchu sentence → Bi-directional GRU encoder with packed sequences → Attention mechanism → GRU decoder with masking → Output Korean sentence → Augmented parallel corpora pipeline

- **Critical path**: Data augmentation → Model training → Evaluation
  - Data augmentation must complete before training can begin
  - Model training depends on both original and augmented data
  - Evaluation requires trained model and test set

- **Design tradeoffs**:
  - Simple GRU vs. transformer: GRU is lighter but may be less powerful
  - Full vs. half augmentation: More data vs. more noise
  - Romanization: Standardization vs. potential information loss

- **Failure signatures**:
  - BLEU score of 0.0: Model not learning, likely vocabulary/alignment issues
  - High perplexity: Model uncertainty, possibly due to augmented data quality
  - Low performance on dictionary corpus: Dataset characteristics mismatch

- **First 3 experiments**:
  1. Train on original Manchu-Korean dictionary only, evaluate on same set
  2. Train on original Manwen Laodang only, evaluate on same set
  3. Compare full vs. half augmentation on combined dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed data augmentation method compare to other low-resource MT techniques like transfer learning or multilingual pre-training in terms of effectiveness and efficiency?
- **Basis in paper**: The paper mentions other techniques like transfer learning with XLM and mBART, and multilingual models like XLM-RoBERTa, but does not directly compare them to the proposed method.
- **Why unresolved**: The authors focused on their specific data augmentation approach and did not conduct experiments to compare it with other established methods for low-resource MT.
- **What evidence would resolve it**: A comparative study evaluating the proposed method against other low-resource MT techniques on the same Manchu-Korean translation task would provide insights into its relative effectiveness and efficiency.

### Open Question 2
- **Question**: How does the quality of the augmented data impact the translation performance, and what are the potential drawbacks of using synonym replacement for data augmentation?
- **Basis in paper**: The paper mentions that the augmented data expands the vocabulary and increases translation performance, but it does not discuss the potential issues of using synonyms that may not be contextually appropriate.
- **Why unresolved**: The authors did not investigate the quality of the augmented data or analyze the potential negative impacts of synonym replacement on translation accuracy.
- **What evidence would resolve it**: A detailed analysis of the augmented data quality, including examples of both successful and problematic synonym replacements, would help understand the trade-offs of this data augmentation approach.

### Open Question 3
- **Question**: How can the proposed model be extended to handle other low-resource language pairs, and what are the challenges in applying this approach to different language families?
- **Basis in paper**: The authors mention the potential of their model for addressing NLP challenges in other low-resource scenarios, but do not provide specific insights on adapting it to different language pairs.
- **Why unresolved**: The paper focuses on Manchu-Korean translation and does not explore the generalizability of the approach to other language pairs or families.
- **What evidence would resolve it**: Experiments applying the proposed model to different low-resource language pairs, along with an analysis of the challenges and adaptations required for each language family, would demonstrate the approach's broader applicability.

## Limitations
- The quality of GloVe-based synonym replacement and its impact on semantic preservation is not thoroughly validated
- The relative contribution of each architectural component to overall performance is unclear due to lack of ablation studies
- The 20-30 point BLEU improvement may be domain-specific, as dictionary corpus performance remains low

## Confidence

**High Confidence**: The experimental methodology and data collection process are well-documented and reproducible. The baseline BLEU scores and improvement metrics are clearly reported with specific numbers.

**Medium Confidence**: The core claim that data augmentation improves translation quality in low-resource scenarios is supported by quantitative results, though the mechanism requires further investigation. The typological similarity claims are plausible but under-supported.

**Low Confidence**: The relative contribution of each architectural component to overall performance is unclear. The quality of GloVe-based synonym replacement and its impact on semantic preservation is not thoroughly validated.

## Next Checks

1. **Synonym Quality Analysis**: Conduct a human evaluation study where native speakers (or linguistic experts) rate the semantic equivalence of 100 randomly selected original-augmented sentence pairs to assess whether GloVe-based replacements maintain meaning.

2. **Ablation Study on Architecture**: Train three additional models: (a) unidirectional GRU, (b) GRU without packed sequences, and (c) GRU without masking. Compare performance to isolate the contribution of each architectural component.

3. **Domain Adaptation Experiment**: Train separate models on the dictionary corpus and the Manwen Laodang corpus, then test on both domains. This would reveal whether the 20-30 point improvement generalizes across different text styles or is domain-specific.