---
ver: rpa2
title: Who's Harry Potter? Approximate Unlearning in LLMs
arxiv_id: '2310.02238'
source_url: https://arxiv.org/abs/2310.02238
tags:
- harry
- potter
- steps
- text
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method for unlearning specific content from
  a large language model (LLM) without full retraining. The method uses reinforcement
  bootstrapping and anchored terms to create alternative training labels that approximate
  what a model without exposure to the target content would predict.
---

# Who's Harry Potter? Approximate Unlearning in LLMs

## Quick Facts
- arXiv ID: 2310.02238
- Source URL: https://arxiv.org/abs/2310.02238
- Reference count: 16
- Key outcome: Over 80% reduction in Harry Potter content completions with 1 GPU hour of fine-tuning while preserving general capabilities

## Executive Summary
This paper introduces a novel method for approximate unlearning in large language models that avoids full retraining. The approach uses reinforcement bootstrapping and anchored term substitution to create alternative training labels that approximate what a model without exposure to target content would predict. Applied to Llama2-7b, the technique successfully reduced Harry Potter-related completions by over 80% while maintaining general language abilities (Winogrande 0.665, PIQA 0.762) with just 1 GPU hour of fine-tuning.

## Method Summary
The method operates in three phases: (1) create a reinforced model by fine-tuning on the unlearn target content, (2) generate generic predictions by replacing anchored terms with generic counterparts and using the model's own predictions, and (3) fine-tune the baseline model on the original text with generic predictions as targets. This approach identifies content-specific tokens through logit comparison and redirects the model from content-specific to generic associations through fine-tuning on alternative labels.

## Key Results
- Over 80% reduction in Harry Potter content completions achieved
- General capabilities preserved (Winogrande 0.665, PIQA 0.762)
- Computational efficiency: 1 GPU hour of fine-tuning required
- Method demonstrates effective unlearning without catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
The reinforced model identifies tokens most related to the unlearning target by comparing its logits with the baseline model's logits. When trained further on the target data, it develops stronger associations with content-specific tokens. The logit difference (with ReLU) focuses only on increased probabilities, assuming these directly indicate target-related content.

### Mechanism 2
Generic predictions are created by replacing anchored terms with generic counterparts and using the baseline model's own predictions on transformed text. This systematically replaces idiosyncratic expressions (like "Hogwarts" → "Mystic Academy") to obtain predictions that approximate what a model without target content knowledge would predict.

### Mechanism 3
Fine-tuning on alternative labels effectively erases original text from the model's memory when prompted with its context. The process uses original text as input tokens but generic predictions as target tokens, training the model to associate context with generic rather than content-specific completions.

## Foundational Learning

- **Concept**: Reinforcement learning through additional training on target data
  - Why needed: To create the reinforced model that can identify content-specific tokens through logit comparison
  - Quick check: What happens to a model's token probabilities when it's trained further on specific content?

- **Concept**: Cross-entropy loss and gradient-based fine-tuning
  - Why needed: The method relies on standard fine-tuning techniques to overwrite content-specific associations with generic ones
  - Quick check: How does gradient descent update model parameters when target labels differ from current predictions?

- **Concept**: Tokenization and context window management
  - Why needed: The method must handle tokenized text and maintain consistent mappings between original and transformed versions
  - Quick check: How does tokenization affect the ability to consistently replace multi-token expressions?

## Architecture Onboarding

- **Component map**: Baseline model → Reinforced model (trained on unlearn target) → Anchor term extraction (via GPT-4) → Generic prediction generation → Fine-tuning dataset creation → Fine-tuned model
- **Critical path**: The entire pipeline must work sequentially; failure at any stage prevents successful unlearning
- **Design tradeoffs**: Using GPT-4 for anchor extraction trades accuracy for convenience; could be replaced with frequency analysis or manual curation
- **Failure signatures**: 
  - Reinforced model doesn't show clear logit differences
  - Generic predictions still contain content-specific elements
  - Fine-tuning doesn't reduce content-specific completions
- **First 3 experiments**:
  1. Train reinforced model on Harry Potter and verify it assigns higher probabilities to content-specific tokens compared to baseline
  2. Test anchor term replacement by manually verifying that generic counterparts don't reveal original content
  3. Fine-tune baseline on a small synthetic dataset and measure reduction in content-specific completions

## Open Questions the Paper Calls Out

### Open Question 1
Can the unlearning technique be generalized to non-fiction or textbook content? The authors speculate that non-fiction content, which lacks the idiosyncratic expressions of fiction, may present challenges for their technique. The paper only demonstrates unlearning on the fictional Harry Potter series, leaving the applicability to non-fiction untested.

### Open Question 2
How effective is the technique against adversarial methods of extracting unlearned knowledge? The authors acknowledge that their evaluation method may not detect knowledge extraction through non-traditional methods. The paper relies on black-box testing and community evaluation but doesn't explore white-box attacks like probing internal model representations.

### Open Question 3
Can the unlearning process be made more efficient by optimizing the dictionary of anchored terms? The authors manually curated a dictionary of ~1,500 anchored terms with GPT-4's help, suggesting potential inefficiency in the current approach. The paper doesn't explore automated or optimized methods for identifying the most impactful terms to anchor.

## Limitations

- The approach relies heavily on the quality of anchor term extraction and the effectiveness of the generic prediction generation process
- The evaluation methodology, while comprehensive, uses fixed prompts that may not represent all possible probing methods
- The method's effectiveness depends on the assumption that reinforced model's logit differences accurately identify content-related tokens

## Confidence

**High Confidence Claims:**
- The three-phase methodology is technically sound and reproducible
- The reported benchmark scores demonstrate that general capabilities are preserved
- The computational efficiency claim (1 GPU hour for fine-tuning) is verifiable

**Medium Confidence Claims:**
- The >80% reduction in content-specific completions is likely achieved but depends on evaluation prompts
- The anchored term substitution approach effectively generates generic predictions
- The logit difference method reliably identifies content-specific tokens

**Low Confidence Claims:**
- The method generalizes to other unlearn targets beyond Harry Potter content
- The approach works equally well for models larger than 7B parameters
- The unlearning is permanent and cannot be reversed through fine-tuning

## Next Checks

1. **Cross-target validation**: Apply the same methodology to unlearn a different well-defined text corpus (e.g., political speeches, technical documentation) and measure whether the >80% reduction in content-specific completions is consistently achieved across different content types.

2. **Adversarial prompting test**: Design a battery of adversarial prompts that attempt to elicit the unlearn target content through indirect means (paraphrasing, semantic similarity, contextual inference) to test whether the unlearning is semantically robust or only works for direct references.

3. **Transfer learning assessment**: Fine-tune the unlearned model on a small amount of the original unlearn target content and measure whether the content-specific knowledge can be quickly relearned, which would indicate whether the unlearning is superficial (masking) or fundamental (erasure).