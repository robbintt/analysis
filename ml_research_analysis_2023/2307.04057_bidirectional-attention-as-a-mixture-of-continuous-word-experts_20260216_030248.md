---
ver: rpa2
title: Bidirectional Attention as a Mixture of Continuous Word Experts
arxiv_id: '2307.04057'
source_url: https://arxiv.org/abs/2307.04057
tags:
- attention
- bidirectional
- word
- cbow
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper shows that a single-layer single-head bidirectional attention
  model is statistically equivalent to a continuous bag-of-words (CBOW) model with
  mixture-of-experts (MoE) weights. This equivalence is extended to multi-head attention
  (equivalent to stacked MoEs) and multi-layer attention (equivalent to a mixture
  of MoEs).
---

# Bidirectional Attention as a Mixture of Continuous Word Experts

## Quick Facts
- arXiv ID: 2307.04057
- Source URL: https://arxiv.org/abs/2307.04057
- Reference count: 40
- Key outcome: Single-layer single-head bidirectional attention is statistically equivalent to CBOW with mixture-of-experts weights, with extensions to multi-head/multi-layer architectures

## Executive Summary
This paper establishes a theoretical equivalence between bidirectional attention mechanisms and continuous bag-of-words (CBOW) models with mixture-of-experts (MoE) weights. Through reparameterization, the authors show that the masked language model objective of bidirectional attention decomposes into attention weights and similarity scores that correspond exactly to MoE components. This statistical viewpoint explains bidirectional attention's effectiveness on heterogeneous data and suggests a natural extension to categorical tabular data. The paper also characterizes conditions for linear word analogies in embeddings, revealing that bidirectional attention requires stronger assumptions than CBOW or skip-gram models for such analogies to hold.

## Method Summary
The authors prove that single-layer single-head bidirectional attention is statistically equivalent to CBOW with mixture-of-experts weights through reparameterization of attention weight matrices into attention weights and similarity scores. They extend this equivalence to multi-head attention (equivalent to stacked MoEs) and multi-layer attention (equivalent to a mixture of MoEs). For tabular data, they treat each feature as an expert using feature encodings instead of positional encodings. Theoretical analysis characterizes when linear word analogies appear in embeddings, showing that bidirectional attention requires stronger assumptions than CBOW models for analogies to hold.

## Key Results
- Single-layer single-head bidirectional attention is statistically equivalent to CBOW with MoE weights
- Multi-head bidirectional attention equals stacked MoEs, and multi-layer attention equals a mixture of MoEs
- Tabular extension using feature encodings as experts improves out-of-distribution generalization compared to existing methods
- Bidirectional attention requires stronger assumptions than CBOW for linear word analogies to hold

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-layer single-head bidirectional attention is equivalent to CBOW with mixture-of-experts (MoE) weights.
- Mechanism: Upon reparameterization, the masked language model (MLM) objective decomposes into attention weights θ(j,m) and similarity scores χ(j,m,k), corresponding exactly to CBOW MoE weights and similarities.
- Core assumption: Reparameterization from attention matrices to MoE components preserves the loss landscape.
- Evidence anchors: Abstract and section 2.2 equivalence claims, though corpus contains no direct citations about this specific equivalence.

### Mechanism 2
- Claim: Multi-head bidirectional attention equals stacked MoEs, and multi-layer attention equals a mixture of MoEs.
- Mechanism: Each attention head independently computes expert contributions (stacked structure), while multiple layers create hierarchical mixtures.
- Core assumption: Attention heads operate independently as separate experts, with layer-wise composition preserving MoE structure.
- Evidence anchors: Abstract and section 2.2 claims about multi-head/multi-layer equivalence, with moderate corpus support for related transformer architecture concepts.

### Mechanism 3
- Claim: Bidirectional attention's MoE structure explains effectiveness on heterogeneous data and enables tabular extension.
- Mechanism: Mixture-of-experts naturally handles heterogeneous patterns by allowing different positions/experts to specialize; for tabular data, each feature becomes an expert with learned encodings.
- Core assumption: Heterogeneous patterns can be captured by expert specialization, which transfers to tabular feature contexts.
- Evidence anchors: Abstract and section 3.1 claims about MoE perspective and tabular extension, with moderate corpus evidence for tabular transformer approaches.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) models
  - Why needed here: Core insight is that bidirectional attention can be re-expressed as an MoE model, explaining effectiveness and suggesting extensions.
  - Quick check question: What is the key advantage of MoE models over single-expert approaches in handling heterogeneous data?

- Concept: Masked Language Model (MLM) objective
  - Why needed here: Understanding MLM is crucial for seeing how bidirectional attention learns and how it can be extended to tabular data.
  - Quick check question: How does the MLM objective differ from standard language modeling objectives in terms of information flow?

- Concept: Attention mechanism and positional encodings
  - Why needed here: These fundamental components get re-expressed in the MoE framework and enable the tabular extension.
  - Quick check question: Why are positional encodings necessary in bidirectional attention, and how are they repurposed in the tabular extension?

## Architecture Onboarding

- Component map: One-hot encoding → Embedding + Encoding → Attention weights computation → Expert combination → Prediction
- Critical path: One-hot encoding → Embedding + Encoding → Attention weights computation → Expert combination → Prediction
- Design tradeoffs:
  - Single-head vs multi-head: Single-head simpler but less expressive; multi-head enables parallel expert processing
  - Number of layers: More layers enable deeper hierarchical mixtures but increase computational cost
  - Embedding dimension: Higher dimensions capture more nuance but require more data
- Failure signatures:
  - Poor performance on heterogeneous data suggests inadequate expert specialization
  - Overfitting on small tabular datasets indicates excessive model capacity
  - Inability to generalize OOD suggests feature encodings aren't capturing relevant invariances
- First 3 experiments:
  1. Verify single-layer single-head equivalence by training both CBOW+MoE and bidirectional attention on same data and comparing learned parameters
  2. Test tabular extension on simple synthetic dataset with known feature interactions to verify expert specialization
  3. Evaluate OOD generalization by training on correlated features and testing on decorrelated features, comparing against baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the equivalence between bidirectional attention and CBOW + MoE hold for architectures with residual connections and feedforward layers?
- Basis in paper: [explicit] The paper explicitly states this as a limitation, noting that the linear word analogy argument ignores residual connections and only considers architectures using linear layers.
- Why unresolved: The analysis excludes these components to maintain mathematical tractability, but they are standard in modern transformer architectures.
- What evidence would resolve it: Extending theoretical analysis to include residual connections and feedforward layers, verifying whether CBOW + MoE equivalence still holds or requires modification.

### Open Question 2
- Question: What are the fundamental differences between static word embeddings (CBOW, Word2Vec) and contextual embeddings (BERT) in their ability to form linear analogies?
- Basis in paper: [explicit] The paper notes it would be useful to provide theoretical justifications for the observed robustness of bidirectional attention to covariate shifts and understand differences between static and contextual word embeddings in forming linear analogies.
- Why unresolved: While the paper characterizes conditions for linear analogies in both CBOW and bidirectional attention, it doesn't fully explain why contextual embeddings might perform worse than static embeddings on analogy tasks despite superior performance on other NLP benchmarks.
- What evidence would resolve it: Comparative theoretical analysis of how static vs. contextual embeddings handle paraphrasing assumptions and conditional independence conditions required for linear analogies.

### Open Question 3
- Question: Under what conditions does the tabular extension of bidirectional attention achieve better out-of-distribution generalization compared to traditional supervised learning methods?
- Basis in paper: [explicit] The paper states the approach is applicable to multiple datasets with partially overlapping features, where learned feature encodings bring all features into the same embedding space.
- Why unresolved: The paper demonstrates empirical superiority of the tabular extension but doesn't provide theoretical conditions under which joint modeling of p(X,Y) outperforms conditional modeling of p(Y|X) for OOD generalization.
- What evidence would resolve it: Formal characterization of data distributions and covariate shift scenarios where joint modeling provides theoretical guarantees of improved OOD performance over conditional modeling approaches.

## Limitations
- Theoretical scope limited to architectures without residual connections or feedforward layers
- Empirical validation relies heavily on synthetic data rather than real-world datasets
- Practical implementation challenges of the reparameterization scheme are not fully addressed

## Confidence
- High Confidence: The theoretical proof for single-layer single-head bidirectional attention equivalence with CBOW+MoE is mathematically rigorous and well-supported.
- Medium Confidence: The extension to multi-head and multi-layer architectures follows logically but lacks formal proof and may not hold under all parameter configurations.
- Low Confidence: The empirical validation of the tabular extension is limited to synthetic data, and claimed OOD generalization benefits would need verification on real-world datasets.

## Next Checks
1. **Direct Equivalence Validation**: Train identical CBOW+MoE and single-layer single-head bidirectional attention models on the same dataset, then compare their learned parameters and performance metrics to verify the claimed statistical equivalence.

2. **Real-World Tabular Testing**: Apply the tabular extension to established benchmark datasets (such as Adult, Credit, or Higgs) with varying feature correlations, comparing against both traditional tabular methods and modern transformer approaches under OOD settings.

3. **Layer-wise Analysis**: For multi-layer attention, conduct ablation studies to determine at which layer the MoE equivalence breaks down, and characterize the conditions (layer depth, head count, embedding dimension) under which the stacked MoE interpretation remains valid.