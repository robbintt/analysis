---
ver: rpa2
title: 'ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated
  Chain-of-Thought'
arxiv_id: '2310.17342'
source_url: https://arxiv.org/abs/2310.17342
tags:
- table
- number
- text
- concert
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose ACT-SQL, an in-context learning approach for text-to-SQL
  that automatically generates chain-of-thought prompts to improve LLMs' reasoning
  ability. Our method uses schema linking to create auto-CoT exemplars without manual
  labeling, requiring only one LLM API call per SQL query.
---

# ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought

## Quick Facts
- arXiv ID: 2310.17342
- Source URL: https://arxiv.org/abs/2310.17342
- Reference count: 40
- Key outcome: State-of-the-art in-context learning performance on Spider dataset with 62.7% exact match accuracy using automatically-generated chain-of-thought prompts

## Executive Summary
ACT-SQL introduces an in-context learning approach for text-to-SQL that automatically generates chain-of-thought (CoT) prompts to enhance large language models' reasoning capabilities. The method leverages schema linking to create auto-CoT exemplars without manual labeling, requiring only one LLM API call per SQL query. By automatically generating reasoning steps and selecting contextually relevant exemplars, ACT-SQL achieves superior performance on both single-turn and multi-turn text-to-SQL benchmarks while maintaining efficiency through its single-call design.

## Method Summary
ACT-SQL addresses the challenge of few-shot text-to-SQL learning by automatically generating chain-of-thought exemplars through schema linking. The approach identifies relevant database tables and columns by computing similarity scores between question tokens and schema elements, then constructs step-by-step reasoning prompts that guide the LLM through the SQL generation process. A hybrid exemplar selection strategy combines static exemplars with dynamic ones selected based on semantic similarity to the current question. The entire process requires only a single LLM API call per query, making it efficient while maintaining high accuracy across different database schema formats and text-to-SQL datasets.

## Key Results
- Achieves 62.7% exact match accuracy and 71.4% test-suite accuracy on Spider dev set using GPT-3.5-turbo
- Outperforms baseline in-context learning approaches across all database schema styles
- Demonstrates strong performance on multi-turn datasets (SParC, CoSQL) with 38.2% and 33.4% interaction match accuracy respectively

## Why This Works (Mechanism)

### Mechanism 1: Schema Linking for Table/Column Identification
Schema linking enables LLMs to identify relevant database tables and columns during SQL generation by computing similarity scores between question slices and schema elements. The system automatically links question tokens to schema items when they exactly or partially match schema item names. This mechanism assumes that natural language questions contain tokens that can be matched to schema item names through string similarity, with the core assumption being that direct string matching is sufficient for most text-to-SQL scenarios.

### Mechanism 2: Chain-of-Thought Reasoning Steps
Chain-of-thought prompting improves LLM reasoning by breaking down SQL generation into intermediate steps. The system generates step-by-step reasoning prompts that guide the LLM through schema linking before producing the final SQL query. This mechanism assumes that LLMs can follow logical reasoning steps when provided in the prompt format, with the core assumption being that structured reasoning leads to better SQL generation quality.

### Mechanism 3: Dynamic Exemplar Selection
Dynamic exemplar selection improves few-shot learning by providing contextually relevant examples. The system selects additional exemplars based on similarity between the current question and training questions, assuming that questions with similar semantic content will benefit from the same exemplar SQL queries. This mechanism assumes that semantic similarity between questions correlates with the usefulness of exemplar SQL patterns.

## Foundational Learning

- Concept: Database schema structure and relationships
  - Why needed here: Understanding tables, columns, primary keys, and foreign keys is essential for generating correct SQL queries
  - Quick check question: What is the difference between a primary key and a foreign key in database design?

- Concept: SQL syntax and grammar rules
  - Why needed here: The LLM must generate syntactically valid SQL queries that follow proper structure
  - Quick check question: How does the SQL SELECT statement structure differ from the WHERE clause?

- Concept: Natural language processing and semantic parsing
  - Why needed here: Translating natural language questions into structured SQL requires understanding language semantics
  - Quick check question: What is the role of schema linking in text-to-SQL systems?

## Architecture Onboarding

- Component map: Database schema → Question parsing → Similarity scoring → Exemplar selection → CoT generation → LLM prompt → SQL output
- Critical path: Schema → Question parsing → Similarity scoring → Exemplar selection → CoT generation → LLM prompt → SQL output
- Design tradeoffs: Single API call vs. multiple calls for better accuracy, manual CoT labeling vs. automatic generation, static vs. dynamic exemplar selection strategies
- Failure signatures: Incorrect schema linking leading to wrong table/column selection, poor exemplar selection resulting in irrelevant SQL patterns, CoT generation producing confusing rather than helpful reasoning steps
- First 3 experiments:
  1. Test zero-shot performance with different database prompt styles (Table(Column) vs Create(EoT))
  2. Compare static vs. dynamic exemplar selection on a small dataset
  3. Evaluate CoT generation quality by manually checking reasoning steps against gold SQL queries

## Open Questions the Paper Calls Out

### Open Question 1: Optimal Exemplar Count
What is the optimal number of static and dynamic exemplars (ns and nd) for different database schema styles and dataset complexities? The paper mentions that these are hyperparameters that need manual determination but doesn't provide a systematic method for optimization.

### Open Question 2: Auto-CoT vs Manual-CoT Quality
How does the quality of automatically generated CoT prompts impact LLM performance compared to manual CoT labeling? The paper acknowledges that manually labeled CoT exemplars are fixed but doesn't directly compare the two approaches.

### Open Question 3: Multi-turn Task Limitations
What are the limitations of the two-phase approach for multi-turn text-to-SQL tasks, and how can they be addressed? The paper identifies quality issues with rewritten questions but doesn't propose alternative strategies for handling multi-turn tasks.

## Limitations

- Performance degrades significantly on robustness variants like Spider-Syn and Spider-DK, indicating limitations with synonym handling and missing explicit column mentions
- The two-phase approach for multi-turn tasks shows concerning performance, suggesting fundamental limitations in the current methodology
- Scalability to larger, more complex databases with hundreds of tables is not demonstrated, raising questions about practical applicability

## Confidence

**High Confidence**: The core mechanism of using schema linking for automatic CoT generation is well-supported by empirical results and aligns with established principles of semantic parsing.

**Medium Confidence**: The effectiveness of dynamic exemplar selection shows promise but requires more rigorous ablation studies to validate the approach.

**Low Confidence**: Robustness claims for variants like Spider-Syn and Spider-DK are concerning, as performance degrades significantly on these benchmarks.

## Next Checks

1. **Schema Linking Robustness Test**: Evaluate performance when questions contain synonyms, paraphrases, or domain-specific terminology that doesn't directly match schema item names. Test with a manually curated set of schema-linked examples showing both successful and failed linking scenarios.

2. **CoT Generation Quality Audit**: Manually review 50 randomly selected auto-CoT exemplars to assess whether the reasoning steps are logically sound and helpful for SQL generation. Rate each exemplar on clarity, correctness, and usefulness on a 3-point scale.

3. **Exemplar Selection Ablation**: Run controlled experiments comparing different similarity metrics (cosine, dot product, learned embeddings) for exemplar selection, measuring the impact on SQL generation accuracy across different query types and database schemas.