---
ver: rpa2
title: 'UMD: Unsupervised Model Detection for X2X Backdoor Attacks'
arxiv_id: '2305.18651'
source_url: https://arxiv.org/abs/2305.18651
tags:
- class
- backdoor
- attacks
- trigger
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'UMD is the first unsupervised backdoor model detector for X2X
  attacks, which can involve arbitrary numbers of source classes, each with an arbitrary
  target class. It introduces a novel transferability (TR) statistic to identify backdoor
  class pairs and uses a two-step inference procedure: (1) select putative backdoor
  class pairs via clustering based on TR, and (2) perform robust, unsupervised anomaly
  detection on aggregated trigger statistics.'
---

# UMD: Unsupervised Model Detection for X2X Backdoor Attacks

## Quick Facts
- arXiv ID: 2305.18651
- Source URL: https://arxiv.org/abs/2305.18651
- Reference count: 40
- Key outcome: UMD outperforms all SOTA baselines (even supervised ones) by 17%, 4%, and 8% in average detection accuracy on CIFAR-10, GTSRB, and Imagenette respectively

## Executive Summary
UMD introduces the first unsupervised approach for detecting X2X backdoor attacks, which can involve arbitrary numbers of source classes each with arbitrary target classes. The method uses a novel transferability (TR) statistic to identify backdoor class pairs through clustering, followed by robust anomaly detection on aggregated trigger statistics. UMD achieves state-of-the-art performance without requiring any labeled attack data, outperforming even supervised baselines across multiple benchmark datasets.

## Method Summary
UMD operates through a two-step inference procedure: first selecting putative backdoor class pairs via clustering based on a transferability (TR) statistic that measures misclassification rates when applying triggers reverse-engineered for one class pair to samples from another class pair; then performing robust, unsupervised anomaly detection on aggregated trigger statistics for the selected class pairs. The method requires only clean validation samples from each class and a trained classifier, making it applicable in real-world scenarios where attack data is unavailable.

## Key Results
- UMD outperforms all five SOTA baselines (including supervised methods) by 17%, 4%, and 8% in average model inference accuracy on CIFAR-10, GTSRB, and Imagenette respectively
- UMD demonstrates strong detection performance against adaptive attacks including WaNet, Blended triggers, and N2N attacks
- The method successfully identifies backdoor class pairs and enables mitigation by allowing "unlearning" of the backdoor through fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UMD's TR statistic effectively distinguishes backdoor from non-backdoor class pairs
- Mechanism: The TR statistic measures how often a trigger reverse-engineered for class pair (s,t) causes misclassification to class a when applied to samples from class i, creating a distinctive pattern for backdoor pairs
- Core assumption: TR from a backdoor class pair to another backdoor class pair is guaranteed to be no less than TR from a backdoor class pair to a non-backdoor class pair in ideal cases
- Evidence anchors:
  - [abstract]: "UMD outperforms SOTA baselines (even with supervision) by 17%, 4%, and 8%..."
  - [section]: "we calculate a transferability (TR) statistic for each ordered pair of class pairs... UMD selects a subset of putative backdoor class pairs using the TR statistics"
  - [corpus]: Weak. The corpus contains papers on backdoor attacks but lacks specific evidence about UMD's TR statistic outperforming baselines.

### Mechanism 2
- Claim: UMD achieves high detection accuracy even against adaptive attacks
- Mechanism: UMD incorporates different trigger reverse-engineering algorithms (PT-RED for perturbation triggers, NC for patch triggers, and intermediate-layer technique for sample-specific triggers) to handle various attack types
- Core assumption: The trigger reverse-engineering algorithms can accurately approximate the actual triggers used in these advanced attacks
- Evidence anchors:
  - [abstract]: "We also show the strong detection performance of UMD against several strong adaptive attacks."
  - [section]: "We also evaluate UMD against four adaptive attacks, including the invisible sample-specific backdoor attack (ISSBA)... and the O2O attack..."
  - [corpus]: Weak. The corpus contains papers on various backdoor attacks but lacks specific evidence about UMD's effectiveness against adaptive attacks.

### Mechanism 3
- Claim: UMD enables backdoor mitigation by detecting backdoor class pairs
- Mechanism: Once backdoor class pairs are detected, reverse-engineered triggers can be embedded into clean samples from source classes without changing labels, and fine-tuning on these samples helps the model learn correct predictions even with triggers
- Core assumption: The detected backdoor class pairs are accurate enough to enable effective mitigation
- Evidence anchors:
  - [abstract]: "It also demonstrates strong performance against adaptive attacks and enables backdoor mitigation."
  - [section]: "Moreover, for the 10 N2N attacks, the generalized UMD that selects 5 clusters of candidate backdoor class pairs correctly identifies 28 out of the 10x3 triggers..."
  - [corpus]: Weak. The corpus contains papers on backdoor mitigation but lacks specific evidence about UMD's effectiveness in enabling mitigation.

## Foundational Learning

- Concept: Transferability in backdoor attacks
  - Why needed here: UMD's core innovation is the TR statistic, which measures transferability of triggers between class pairs. Understanding transferability is crucial for grasping how UMD identifies backdoor class pairs.
  - Quick check question: How does the TR statistic differ from traditional notions of transferability in adversarial machine learning?

- Concept: Reverse-engineering triggers in backdoor detection
  - Why needed here: UMD relies on reverse-engineering triggers for each class pair to compute the TR statistic and perform anomaly detection. Familiarity with trigger reverse-engineering techniques is essential for understanding UMD's approach.
  - Quick check question: What are the main challenges in reverse-engineering triggers for backdoor detection, and how does UMD address them?

- Concept: Unsupervised anomaly detection
  - Why needed here: UMD uses an unsupervised anomaly detector to assess the atypicality of aggregated trigger statistics over selected class pairs. Knowledge of unsupervised anomaly detection techniques is necessary for understanding UMD's inference procedure.
  - Quick check question: How does UMD's anomaly detector differ from traditional MAD-based approaches, and why is it more suitable for backdoor detection?

## Architecture Onboarding

- Component map:
  Trigger reverse-engineering module -> TR computation module -> Clustering module -> Anomaly detection module -> Model inference

- Critical path: Trigger reverse-engineering → TR computation → Clustering → Anomaly detection → Model inference

- Design tradeoffs:
  - Tradeoff between accuracy and computational cost in trigger reverse-engineering
  - Balancing sensitivity and specificity of the TR statistic
  - Choosing appropriate number of class pairs to select in clustering step

- Failure signatures:
  - High false positive rate: UMD detects attacks on benign classifiers
  - High false negative rate: UMD fails to detect actual backdoor attacks
  - Low pair detection rate: UMD correctly detects attack but fails to identify all backdoor class pairs

- First 3 experiments:
  1. Implement trigger reverse-engineering for simple perturbation-based backdoor attack on CIFAR-10 and visualize reversed triggers
  2. Compute TR statistic for small set of class pairs and analyze ability to distinguish backdoor from non-backdoor pairs
  3. Implement clustering approach to select putative backdoor class pairs and evaluate performance on known backdoor attack

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does UMD's transferability (TR) statistic perform in distinguishing backdoor class pairs from non-backdoor class pairs in the presence of advanced backdoor triggers like sample-specific or physical triggers?
- Basis in paper: [explicit] The paper states that UMD is evaluated against advanced triggers like WaNet and Blended triggers, but does not provide detailed analysis of TR's performance specifically against these triggers.
- Why unresolved: The evaluation focuses on overall detection accuracy rather than specific effectiveness of TR in distinguishing backdoor class pairs under different trigger types.
- What evidence would resolve it: Detailed analysis of TR values for backdoor vs. non-backdoor class pairs when using advanced trigger types, showing clear separation between the two groups.

### Open Question 2
- Question: Can UMD be adapted to detect backdoor attacks in other domains beyond image classification, such as natural language processing or graph neural networks?
- Basis in paper: [inferred] The paper focuses on image classification tasks, but backdoor attacks have been extended to other domains as mentioned in related work section.
- Why unresolved: The paper does not explore applicability of UMD to other data domains or learning paradigms.
- What evidence would resolve it: Empirical evaluation of UMD's performance on backdoor detection in other domains, demonstrating its effectiveness or identifying domain-specific challenges.

### Open Question 3
- Question: How does performance of UMD scale with number of classes in dataset, particularly for datasets with large number of classes like ImageNet?
- Basis in paper: [explicit] The paper evaluates UMD on datasets with 10, 43, and 10 classes, but does not explore performance on datasets with larger number of classes.
- Why unresolved: Scalability of UMD to larger class sets is not investigated, leaving uncertainty about performance on more complex datasets.
- What evidence would resolve it: Empirical results showing UMD's detection accuracy and computational efficiency on datasets with varying number of classes, particularly those with hundreds or thousands of classes.

### Open Question 4
- Question: What is impact of using different trigger reverse-engineering algorithms on UMD's detection performance, and is there way to automatically select most appropriate algorithm for given classifier?
- Basis in paper: [explicit] The paper mentions that UMD is not limited to any particular trigger reverse-engineering algorithm and uses different algorithms for different trigger types, but does not explore impact of algorithm choice on performance.
- Why unresolved: Relationship between choice of reverse-engineering algorithm and UMD's effectiveness is not studied, leaving open question of optimal algorithm selection.
- What evidence would resolve it: Comparative analysis of UMD's performance using different reverse-engineering algorithms, along with method for selecting most appropriate algorithm based on characteristics of classifier being inspected.

## Limitations

- Limited empirical evidence on UMD's effectiveness in enabling backdoor mitigation
- Weak corpus evidence supporting claims of superiority over SOTA baselines
- Limited ablation studies on clustering approach and sensitivity to number of selected class pairs

## Confidence

- UMD's overall detection performance: Medium
- Effectiveness against adaptive attacks: Medium
- Ability to enable mitigation: Low

## Next Checks

1. Conduct ablation studies varying the number of class pairs selected in clustering step to understand tradeoff between detection accuracy and false positives
2. Implement comprehensive evaluation comparing UMD's computational cost against all baseline methods on identical hardware
3. Test UMD on classifiers trained with data augmentation techniques known to create natural class similarities to assess false positive rates in realistic scenarios