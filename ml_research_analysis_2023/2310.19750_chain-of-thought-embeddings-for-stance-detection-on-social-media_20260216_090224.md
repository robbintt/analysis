---
ver: rpa2
title: Chain-of-Thought Embeddings for Stance Detection on Social Media
arxiv_id: '2310.19750'
source_url: https://arxiv.org/abs/2310.19750
tags:
- stance
- detection
- tweet
- chatgpt
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Chain-of-Thought (COT) Embeddings, a novel
  approach that improves stance detection on social media by leveraging the reasoning
  outputs of large language models (LLMs) like ChatGPT. The method addresses the challenge
  of implicit stance detection in social media text, which often contains emerging
  slang and colloquial language.
---

# Chain-of-Thought Embeddings for Stance Detection on Social Media

## Quick Facts
- arXiv ID: 2310.19750
- Source URL: https://arxiv.org/abs/2310.19750
- Reference count: 12
- Key outcome: Introduces Chain-of-Thought Embeddings that improve stance detection on social media by leveraging LLM reasoning outputs, achieving state-of-the-art performance on Tweet-Stance (F1: 76.3) and Presidential-Stance datasets.

## Executive Summary
This paper introduces Chain-of-Thought (COT) Embeddings, a novel approach that improves stance detection on social media by leveraging the reasoning outputs of large language models like ChatGPT. The method addresses the challenge of implicit stance detection in social media text, which often contains emerging slang and colloquial language. COT Embeddings work by feeding the reasoning text from LLMs into a transformer encoder, which is then used as an additional feature in a stance detection pipeline. The paper demonstrates that this approach mitigates two key issues in LLM-based stance detection: implicit stance confusion and stance label hallucinations.

## Method Summary
The Chain-of-Thought Embeddings approach involves generating COT reasoning using LLMs (like ChatGPT, Llama-2-7b) and embedding this reasoning using a transformer encoder (Twitter-RoBERTa). The method processes social media tweets along with their stance topics to generate reasoning text that is then embedded and used as features for stance classification. Three variants of the model are trained: TR-Tweet (fine-tuned with only tweet information), TR-COT (fine-tuned using only COT reasoning), and TR-Tweet+COT (fine-tuned with both tweet and COT reasoning as input). The approach is evaluated on two stance detection datasets: Tweet-Stance and Presidential-Stance.

## Key Results
- Achieved F1 score of 76.3 on Tweet-Stance dataset, outperforming baseline models
- Outperformed baseline models on Presidential-Stance dataset
- Demonstrated robustness to minor errors in LLM reasoning and ability to overlook misleading reasoning when predictions depend on domain-specific patterns

## Why This Works (Mechanism)

### Mechanism 1
Text encoders can leverage COT reasonings with minor errors or hallucinations that would otherwise distort the COT output label. When COT reasoning contains correct logic but wrong stance labels, the transformer encoder focuses on the semantic content and reasoning structure rather than the incorrect label, allowing it to extract useful features for stance classification.

### Mechanism 2
Text encoders can overlook misleading COT reasoning when a sample's prediction heavily depends on domain-specific patterns. When the COT reasoning is incorrect but the tweet itself contains clear domain-specific indicators of stance, the transformer encoder prioritizes these explicit signals over the misleading reasoning.

### Mechanism 3
COT reasonings can inject world knowledge into a text encoder. COT reasoning texts often contain explanations grounded in world knowledge not present in the tweet itself, providing the encoder with additional context that improves stance classification.

## Foundational Learning

- Concept: Implicit stance detection
  - Why needed here: Social media text often contains implicit references to stance topics that require additional context to interpret correctly.
  - Quick check question: Why is implicit stance detection particularly challenging in social media text compared to formal text?

- Concept: Chain-of-thought prompting
  - Why needed here: COT prompting helps LLMs break down complex reasoning tasks into intermediate steps, potentially revealing implicit stance information.
  - Quick check question: How does chain-of-thought prompting differ from direct prompting in terms of the reasoning process it elicits?

- Concept: Text encoder robustness to label noise
  - Why needed here: The approach relies on text encoders being able to extract useful features even when COT reasoning contains incorrect stance labels.
  - Quick check question: What properties of transformer encoders might make them robust to certain types of label noise in input text?

## Architecture Onboarding

- Component map: Tweet + Topic -> LLM (COT reasoning) -> Text Encoder (Twitter-RoBERTa) -> Classification Head -> Stance Prediction

- Critical path:
  1. Tweet and topic are fed to LLM for COT reasoning generation
  2. COT reasoning text is extracted and optionally combined with tweet
  3. Text encoder (Twitter-RoBERTa) processes the input to create embeddings
  4. Classification head predicts stance from embeddings
  5. Model is trained on stance-labeled examples

- Design tradeoffs:
  - Using only COT reasoning vs. combining with tweet: Using only COT reasoning simplifies the model but may lose some tweet-specific information; combining both provides more context but increases complexity
  - Different LLM choices: Higher-quality LLMs like ChatGPT produce better COT reasoning but are more expensive and slower; open-source alternatives like Llama-2 are cheaper but may produce lower-quality reasoning
  - Fine-tuning strategy: Fine-tuning on tweet information, COT reasoning, or both affects performance and computational requirements

- Failure signatures:
  - Low performance improvement over baseline: May indicate COT reasoning isn't providing useful information or the text encoder isn't effectively leveraging it
  - High variance across different LLM models: Suggests the approach is sensitive to the quality of COT reasoning
  - Model performs well on explicit stance but poorly on implicit stance: May indicate the text encoder isn't effectively injecting world knowledge from COT reasoning

- First 3 experiments:
  1. Baseline comparison: Train Twitter-RoBERTa on Tweet-Stance without COT reasoning to establish baseline performance
  2. COT reasoning only: Train Twitter-RoBERTa using only COT reasoning as input to measure the value of reasoning alone
  3. Combined approach: Train Twitter-RoBERTa using both tweet and COT reasoning as pairwise input to test the full proposed method

## Open Questions the Paper Calls Out

### Open Question 1
How sensitive is the performance of Chain-of-Thought Embeddings to the quality and structure of the Chain-of-Thought prompts used? The paper mentions that stance detection using COT reasoning is sensitive to the prompt provided to ChatGPT and that future works may explore better prompts to reduce stance label hallucinations.

### Open Question 2
Can Chain-of-Thought Embeddings be effectively applied to stance detection tasks in domains other than social media where colloquial language is constantly changing? The paper mentions that the core takeaway of COT Embeddings reducing implicit stance confusion may only be applicable to popular social media platforms where colloquial language is constantly changing.

### Open Question 3
What is the impact of Chain-of-Thought Embeddings on the computational efficiency of stance detection models? The paper mentions that the addition of COT embeddings may impact the computational efficiency of the model and that if COT reasonings had to be computed at inference time, there may be noticeable inference speed degradation depending on the efficiency of the LLM used.

## Limitations

- The specific COT prompt used to generate reasoning from the LLM is not provided, making faithful reproduction difficult
- Hyperparameters for model training are not fully specified, which could significantly impact performance
- The mechanism explanations are primarily supported by manual inspection of a limited number of samples rather than systematic quantitative analysis

## Confidence

- **High confidence**: The overall approach of using COT reasoning as additional features for stance detection is technically sound and aligns with established NLP practices
- **Medium confidence**: The three proposed mechanisms for why COT embeddings work are reasonable but not rigorously proven
- **Low confidence**: Claims about the robustness of text encoders to COT reasoning errors are difficult to verify without more detailed error analysis

## Next Checks

1. Conduct a comprehensive error analysis across all samples to quantify how often text encoders successfully leverage correct reasoning while ignoring incorrect labels, and identify failure patterns

2. Experiment with different COT prompt templates and quality thresholds to measure their impact on downstream stance detection performance and identify the minimum quality requirements for effective reasoning

3. Design experiments to isolate the contribution of different elements in COT reasoning (e.g., reasoning steps vs. final conclusions vs. world knowledge) to determine which aspects are most valuable for stance detection