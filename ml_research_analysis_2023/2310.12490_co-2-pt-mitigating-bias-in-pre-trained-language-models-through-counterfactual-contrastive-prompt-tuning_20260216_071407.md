---
ver: rpa2
title: 'Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual
  Contrastive Prompt Tuning'
arxiv_id: '2310.12490'
source_url: https://arxiv.org/abs/2310.12490
tags:
- bias
- co2pt
- computational
- downstream
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Co2PT, a debiasing method for pre-trained language
  models via counterfactual contrastive prompt tuning. The core idea is to construct
  counterfactual pairs from training data by replacing demographic terms and use contrastive
  learning to learn debiased continuous prompts while keeping the PLM frozen.
---

# Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning

## Quick Facts
- arXiv ID: 2310.12490
- Source URL: https://arxiv.org/abs/2310.12490
- Reference count: 26
- Key outcome: Reduces average absolute difference in similarity scores from 0.321 to 0.058 on Bias-STS-B compared to vanilla prompt tuning

## Executive Summary
This paper proposes Co2PT, a debiasing method for pre-trained language models via counterfactual contrastive prompt tuning. The method constructs counterfactual pairs by replacing demographic terms in training data, then uses contrastive learning to train debiased continuous prompts while keeping the PLM frozen. Experiments on three bias benchmarks show effective bias mitigation while maintaining task performance, with the approach being parameter-efficient since only prompts are tuned. Co2PT can also be combined with existing debiased models for enhanced performance.

## Method Summary
Co2PT mitigates bias in pre-trained language models by learning debiased continuous prompts through contrastive learning on counterfactual pairs. The method freezes the PLM parameters and adds tunable continuous prompts to each layer. For each training example, counterfactual pairs are generated by replacing demographic terms with opposite-group terms. The model optimizes both a prompt tuning loss for task performance and a contrastive loss that pulls semantically similar original-counterfactual pairs closer in representation space. This approach requires no external corpus and can be applied to any PLM or integrated with existing debiased models.

## Key Results
- Reduces average absolute difference in similarity scores from 0.321 to 0.058 on Bias-STS-B compared to vanilla prompt tuning
- Outperforms existing debiased models when combined with them
- Maintains model performance on downstream tasks while mitigating bias
- Ablation studies confirm importance of both counterfactual and contrastive components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Co2PT mitigates bias by learning debiased continuous prompts through contrastive learning on counterfactual pairs.
- Mechanism: Constructs counterfactual pairs by replacing demographic terms, creating semantically similar pairs with different bias directions. Contrastive learning pulls these pairs closer in representation space while maintaining task performance through prompt tuning.
- Core assumption: Semantic similarity between original and counterfactual pairs ensures learning to treat them similarly removes bias while preserving task knowledge.
- Evidence anchors: Abstract states construction of counterfactual pairs and use of contrastive learning; section describes eliminating need for external corpora and bringing semantically similar neighbors closer.

### Mechanism 2
- Claim: Co2PT achieves parameter efficiency by only tuning continuous prompts while keeping the PLM frozen.
- Mechanism: Continuous prompts added to every layer are tuned through optimization of prompt tuning and contrastive losses, while original PLM parameters remain unchanged.
- Core assumption: Continuous prompts have sufficient capacity to capture both task-specific knowledge and debiasing information without modifying the underlying PLM.
- Evidence anchors: Abstract emphasizes parameter efficiency of tuning only prompts; section states parameters of PLMs are frozen throughout training.

### Mechanism 3
- Claim: Co2PT can be integrated with existing debiased models to further reduce bias on downstream tasks.
- Mechanism: Since Co2PT only modifies prompts, it can be applied on top of any pre-trained or debiased model, helping bridge the gap between upstream debiasing and downstream task performance.
- Core assumption: Debiasing capability learned in prompts can complement and enhance existing debiasing methods applied to PLM parameters.
- Evidence anchors: Abstract mentions Co2PT can be extended to existing debiased language models; section discusses easy integration with upstream debiasing methods.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To pull semantically similar counterfactual pairs closer in representation space while pushing dissimilar pairs apart, creating bias-invariant representations
  - Quick check question: What happens to the similarity between (he is a nurse, she is a nurse) representations after contrastive learning with Co2PT?

- Concept: Counterfactual data augmentation
  - Why needed here: To create training pairs that differ only in demographic terms but are semantically equivalent, enabling bias measurement and removal
  - Quick check question: How would you generate a counterfactual pair for "The man is playing the piano" in Co2PT?

- Concept: Prompt tuning
  - Why needed here: To efficiently add task-specific and debiasing information without modifying the large PLM parameters
  - Quick check question: What is the advantage of adding continuous prompts to every layer versus only the input embedding layer?

## Architecture Onboarding

- Component map: Input layer → BERT model (frozen) → Continuous prompts (tunable) → Output layer → Counterfactual pair generator → Contrastive loss module → Prompt optimization loop → Task-specific loss module → Combined optimization objective

- Critical path:
  1. Load pre-trained BERT and freeze parameters
  2. Initialize continuous prompts for each layer
  3. For each training example, generate counterfactual counterpart
  4. Compute task loss on original and counterfactual pairs
  5. Compute contrastive loss between original and counterfactual representations
  6. Update only prompt parameters using combined loss
  7. Evaluate on bias benchmarks

- Design tradeoffs:
  - Parameter efficiency vs. performance: Only tuning prompts saves computation but may underperform fine-tuning on small models
  - Task-specific vs. general debiasing: Prompts can be task-specific or combined for multi-bias mitigation
  - Quality of counterfactual pairs: Relies on accurate demographic term replacement without semantic drift

- Failure signatures:
  - Bias scores remain high: Likely issues with counterfactual generation or contrastive learning temperature
  - Task performance drops significantly: Prompts may be over-debiasing or contrastive loss is too strong
  - Training instability: Temperature τ or coefficient α may need adjustment

- First 3 experiments:
  1. Run Co2PT on STS-B with default hyperparameters and verify bias reduction from 0.321 to ~0.058
  2. Test with only prompt tuning (no contrastive loss) to confirm the importance of the contrastive component
  3. Apply Co2PT to a pre-debiased model like Context-Debias and measure improvement on Bias-STS-B

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is Co2PT at mitigating non-gender and intersectional biases compared to gender biases?
- Basis in paper: The paper discusses flexibility to apply to different bias dimensions such as gender, race, and religion, and mentions potential to mitigate intersectional bias by combining corresponding prompts.
- Why unresolved: The paper primarily focuses on gender bias and does not provide experimental results or comparisons for non-gender and intersectional biases.
- What evidence would resolve it: Conducting experiments on non-gender and intersectional bias benchmarks, comparing performance to existing methods, and analyzing effectiveness in mitigating these biases.

### Open Question 2
- Question: What is the impact of different hyperparameter settings on the performance and bias mitigation of Co2PT?
- Basis in paper: The paper includes an ablation study on impact of different components and hyperparameters such as prompt length, temperature τ, and coefficient α on Co2PT's performance and bias mitigation.
- Why unresolved: The paper only explores a limited range of hyperparameter values and does not provide comprehensive analysis of impact of various settings on Co2PT's performance.
- What evidence would resolve it: Conducting more extensive hyperparameter search, analyzing impact of different settings on performance and bias mitigation, and providing recommendations for optimal hyperparameter choices.

### Open Question 3
- Question: How does Co2PT compare to other debiasing methods in terms of computational efficiency and memory usage?
- Basis in paper: The paper claims Co2PT is efficient and does not require access to external corpus or retraining entire model, unlike some other debiasing methods.
- Why unresolved: The paper does not provide direct comparison of computational efficiency and memory usage between Co2PT and other debiasing methods.
- What evidence would resolve it: Conducting experiments to measure computational efficiency and memory usage of Co2PT and comparing to other debiasing methods, analyzing trade-offs between performance and resource requirements.

## Limitations
- Limited testing on non-gender and intersectional biases, with experiments primarily focused on gender bias
- Unclear handling of multi-word demographic terms and verification of counterfactual pair quality
- Uncertain scalability to smaller PLMs and different model architectures beyond BERT-based models

## Confidence

- High confidence: Parameter efficiency claims and basic contrastive learning mechanism are well-supported by experimental results showing bias reduction on all three benchmarks
- Medium confidence: Integration capability with existing debiased models is demonstrated but lacks extensive testing across diverse debiasing methods
- Low confidence: Generalizability to other bias types and model architectures beyond BERT-based PLMs remains uncertain without additional experiments

## Next Checks

1. **Counterfactual Quality Validation**: Implement systematic evaluation of counterfactual pair quality by measuring semantic similarity scores (e.g., using sentence transformers) between original and counterfactual pairs to ensure replacement process preserves meaning.

2. **Scaling Study**: Test Co2PT on smaller PLMs (e.g., DistilBERT) and measure both bias reduction and performance trade-offs to better understand parameter efficiency claims across different model sizes.

3. **Bias Type Generalization**: Apply Co2PT to datasets with different bias types (e.g., racial, religious, or intersectional biases) beyond gender-focused benchmarks to evaluate method's broader applicability.