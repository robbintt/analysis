---
ver: rpa2
title: 'SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation'
arxiv_id: '2307.01646'
source_url: https://arxiv.org/abs/2307.01646
tags:
- data
- distribution
- graph
- graphs
- pdata
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the hardness of learning permutation-invariant
  denoising diffusion models for graph generation. It shows that invariant models
  have more modes in their effective target distribution, making them harder to learn.
---

# SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation

## Quick Facts
- arXiv ID: 2307.01646
- Source URL: https://arxiv.org/abs/2307.01646
- Reference count: 40
- Key outcome: SwinGNN achieves state-of-the-art performance on graph generation tasks by using a non-invariant diffusion model with edge-to-edge 2-WL message passing and shifted window self-attention, surpassing existing models by several orders of magnitude in most metrics.

## Executive Summary
This paper investigates the learning challenges of permutation-invariant denoising diffusion models for graph generation. Through theoretical analysis, the authors show that invariant models have more modes in their effective target distribution, making them harder to learn. To address this, they propose SwinGNN, a non-invariant diffusion model that uses an efficient edge-to-edge 2-WL message passing network and shifted window self-attention. SwinGNN achieves state-of-the-art performance on both synthetic and real-world graph datasets, including molecules and proteins, while providing a simple post-processing trick to convert any non-invariant model to a permutation-invariant one.

## Method Summary
SwinGNN is a non-permutation-invariant diffusion model that generates graphs by denoising noisy adjacency matrices. The model treats each edge representation as a token and applies shifted window-based self-attention to restrict interactions to local representations, reducing computational complexity from O(n⁴) to O(n²M²) while maintaining high-order interactions. The architecture employs an edge-to-edge 2-WL message passing network and utilizes hierarchical graph representations. Training involves stochastic differential equation modeling with noise scheduling and network preconditioning, while sampling uses a stochastic sampler with 2nd-order correction.

## Key Results
- SwinGNN achieves state-of-the-art performance on synthetic graph datasets (Ego-small, Community-small, Grid) with superior MMD scores for node degrees, clustering coefficients, and orbit counts.
- On molecule datasets (QM9, ZINC250k), SwinGNN significantly outperforms existing models in validity without correction, uniqueness, FCD, and NSPDK MMD metrics.
- The post-processing trick of randomly permuting generated graphs provably converts any non-invariant model to a permutation-invariant one while maintaining performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Invariant diffusion models face greater learning challenges because their effective target distributions have more modes.
- Mechanism: The optimal score estimator for invariant models learns the score function of a Gaussian mixture model (GMM) with O(n!m) components, while non-invariant models learn a GMM with only O(m) components.
- Core assumption: The data distribution assigns non-zero probability to only one graph per isomorphism class, but the invariant model must assign equal probability to all isomorphic graphs.
- Evidence anchors:
  - [abstract] "we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes"
  - [section] "Our analysis reveals that this is likely caused by 1) their effective target distributions having more modes"
  - [corpus] Missing relevant corpus evidence; claim appears to be novel to this work.

### Mechanism 2
- Claim: SwinGNN achieves superior performance by using edge-to-edge message passing via shifted window self-attention.
- Mechanism: The model treats each edge representation as a token and applies window-based partitioning to restrict self-attention to local representations, reducing computational complexity from O(n⁴) to O(n²M²) while maintaining high-order interactions.
- Core assumption: Edge representations need to interact with each other across the entire adjacency matrix for effective denoising.
- Evidence anchors:
  - [abstract] "employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention"
  - [section] "Our model treats each edge representation in the input matrix as a token, apply transformers with self-attention to update the token representation"
  - [corpus] Missing direct corpus evidence; mechanism appears to be novel to this work.

### Mechanism 3
- Claim: The post-processing trick of randomly permuting generated graphs provably converts any non-invariant model to a permutation-invariant one.
- Mechanism: Applying a uniform random permutation matrix Pr to generated adjacency matrix Ar yields a distribution qθ that assigns equal probability to all isomorphic graphs of Ar.
- Core assumption: The original non-invariant model generates graphs from some distribution pθ(A) that may not be invariant.
- Evidence anchors:
  - [abstract] "we introduce a simple post-processing trick, i.e., randomly permuting the generated graphs, which provably converts any graph generative model to a permutation-invariant one"
  - [section] "Let A be a random adjacency matrix distributed according to any graph distribution on n vertices. Let Pr ~ Unif(Sn) be uniform over the set of permutation matrices. Then, the induced distribution of the random matrix PrAPr^T, denoted as qθ, is permutation invariant"
  - [corpus] Missing direct corpus evidence; mechanism appears to be novel to this work.

## Foundational Learning

- Concept: Diffusion models and score-based generative modeling
  - Why needed here: The paper builds on denoising diffusion models and their connection to score-based models for graph generation
  - Quick check question: What is the relationship between the denoising network Dθ and the score network sθ in diffusion models?

- Concept: Graph isomorphism and automorphism
  - Why needed here: The paper's theoretical analysis relies on understanding isomorphism classes and automorphism numbers of graphs
  - Quick check question: How does the automorphism number of a graph affect the size of its isomorphism class?

- Concept: Graph neural networks and their expressivity
  - Why needed here: The paper compares different GNN architectures (PPGN, SwinGNN) and discusses their theoretical expressivity
  - Quick check question: What is the relationship between the WL test and the function approximation capacity of GNNs?

## Architecture Onboarding

- Component map: Noisy adjacency matrix + noise level → SwinGNN backbone (shifted window self-attention + hierarchical representations) → Denoised adjacency matrix
- Critical path: Noisy adjacency matrix → Window partitioning → Multi-head self-attention → MLP layers → Skip connections → Readout → Denoised matrix
- Design tradeoffs: Window size vs. computational complexity, hierarchical vs. flat architecture, permutation invariance vs. learning difficulty
- Failure signatures: Poor recall scores indicating inability to generate isomorphic graphs, mode collapse in generated distributions, high computational cost for large graphs
- First 3 experiments:
  1. Run SwinGNN on ego-small dataset and compare recall scores with PPGN and DiGress
  2. Test the effect of window size on memory usage and generation quality using grid dataset
  3. Validate the permutation invariance post-processing by checking if generated graphs have isomorphic counterparts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sample complexity lower bound derived for non-permutation-equivariant networks extend to permutation-equivariant networks?
- Basis in paper: [explicit] The paper provides a sample complexity lower bound for non-permutation-equivariant networks but leaves the analysis for permutation-equivariant networks as future work.
- Why unresolved: The theoretical analysis is more challenging for permutation-equivariant networks due to the need to account for the increased number of modes in the effective target distribution.
- What evidence would resolve it: A rigorous proof showing the sample complexity lower bound for permutation-equivariant networks, potentially involving a more complex analysis of the GMM with a larger number of components.

### Open Question 2
- Question: What is the impact of different graph isomorphism testing algorithms on the recall metric used to evaluate the toy dataset experiments?
- Basis in paper: [inferred] The paper uses the networkx package for isomorphism testing in the toy dataset experiments, but the choice of algorithm could affect the recall results.
- Why unresolved: The networkx package offers multiple isomorphism testing algorithms with varying computational complexities and accuracies, and the paper does not specify which one was used.
- What evidence would resolve it: A comparison of recall results using different isomorphism testing algorithms, such as VF2, Bliss, or Nauty, to assess the impact on the evaluation metric.

### Open Question 3
- Question: How does the proposed SwinGNN model perform on graph generation tasks involving weighted graphs or graphs with node/edge attributes beyond simple categorical values?
- Basis in paper: [inferred] The paper focuses on generating unweighted, undirected graphs with binary adjacency matrices, but real-world applications often involve weighted graphs or graphs with more complex attributes.
- Why unresolved: The paper does not provide experimental results or theoretical analysis on the model's ability to handle weighted graphs or graphs with more complex attributes.
- What evidence would resolve it: Experiments evaluating SwinGNN's performance on datasets with weighted graphs or graphs with continuous node/edge attributes, comparing against existing methods designed for such tasks.

## Limitations
- The theoretical analysis assumes uniform or near-uniform distribution over isomorphism classes, which may not hold for many real-world graph datasets.
- The paper lacks ablation studies isolating the impact of individual architectural components on performance gains.
- The evaluation focuses heavily on specific metrics (MMD, FCD, NSPDK) that may not capture all aspects of graph generation quality.

## Confidence
- High Confidence: The mathematical analysis of mode complexity in diffusion models is rigorous and the experimental results on synthetic datasets are reproducible. The post-processing trick for converting non-invariant models to invariant ones is clearly proven.
- Medium Confidence: The SwinGNN architecture and its implementation details are sufficiently specified for reproduction, though some implementation specifics (particularly around the shifted window mechanism) require careful interpretation.
- Low Confidence: The claim that SwinGNN's specific design choices are necessary for its performance is not well-supported. Alternative architectural decisions could potentially achieve similar results.

## Next Checks
1. **Ablation Study**: Run experiments systematically removing shifted window self-attention while keeping other components constant to isolate its contribution to performance gains.

2. **Distribution Analysis**: Analyze the actual distribution of generated graphs to verify whether they exhibit the predicted mode structure, particularly for invariant vs non-invariant models.

3. **Alternative Architecture Comparison**: Implement and test a non-invariant diffusion model using standard GNN architectures (like PPGN) with the same training procedure to determine if performance gains are due to invariance or other factors.