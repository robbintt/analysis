---
ver: rpa2
title: 'ModelScope-Agent: Building Your Customizable Agent System with Open-source
  Large Language Models'
arxiv_id: '2309.00986'
source_url: https://arxiv.org/abs/2309.00986
tags:
- agent
- llms
- tool
- apis
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ModelScope-Agent provides a customizable agent framework based
  on open-source LLMs. It features a user-friendly system library that supports training
  multiple open-source LLMs while enabling seamless integration with model APIs and
  common APIs.
---

# ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models

## Quick Facts
- arXiv ID: 2309.00986
- Source URL: https://arxiv.org/abs/2309.00986
- Authors: ModelScope Research Team
- Reference count: 12
- Key outcome: Customizable agent framework with open-source LLMs supporting 598k dialogue dataset, weighted LM training, and integration with 1000+ public AI models

## Executive Summary
ModelScope-Agent is an open-source framework that enables building customizable agent systems using open-source large language models. It provides a comprehensive library for training LLMs with tool-use capabilities, featuring a tool library, automatic API retrieval, memory control, and evaluation framework. The system supports seamless integration with model APIs and common APIs, making it suitable for real-world applications. The framework introduces MSAgent-Bench, a large-scale tool-enhanced dataset, and a Weighted LM training strategy to improve API parameter generation accuracy.

## Method Summary
The framework combines open-source LLMs with a tool-use module and memory module to accomplish complex tasks. It uses synthetic data generation to create MSAgent-Bench (598k dialogues) covering various API categories in multiple languages. The Weighted LM training strategy emphasizes API-related token generation during fine-tuning. Tool retrieval uses dense vector methods based on API descriptions and instruction prompts. The memory module maintains conversation context through knowledge retrieval and prompt assembly. The system supports multi-turn conversations and integrates with over 1000 public AI models through ModelScopeGPT.

## Key Results
- Introduces MSAgent-Bench with 598k dialogues across multiple API categories and languages
- Achieves state-of-the-art performance on tool-use benchmarks with Action EM and Argument F1 metrics
- Demonstrates strong generalization capabilities on unseen APIs and multilingual scenarios
- Successfully integrates open-source LLMs with over 1000 public AI models in ModelScopeGPT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The agent system achieves robust tool-use capabilities through multilingual synthetic data generation with weighted loss optimization
- Mechanism: MSAgent-Bench provides diverse tool-use scenarios while Weighted LM increases loss weight for API tokens during fine-tuning
- Core assumption: Synthetic data captures realistic tool-use patterns
- Evidence anchors: MSAgent-Bench dataset creation, Weighted LM training description
- Break condition: Poor synthetic data quality or ineffective Weighted LM optimization

### Mechanism 2
- Claim: Memory module handles complex multi-turn tasks through context maintenance
- Mechanism: Knowledge retrieval and prompt generator assemble contextual information for LLM input
- Core assumption: Conversation context is essential for multi-turn tool-use
- Evidence anchors: Memory module architecture description
- Break condition: Failure to maintain or utilize conversation context

### Mechanism 3
- Claim: Tool retrieval module effectively selects appropriate tools using dense vector retrieval
- Mechanism: Dense vector retrieval based on API descriptions and instruction prompts
- Core assumption: Vector similarity identifies relevant tools
- Evidence anchors: Tool retrieval module implementation
- Break condition: Inaccurate tool identification or suboptimal top-3 selection

## Foundational Learning

- Concept: Tool-use in LLMs
  - Why needed here: Fundamental to ModelScope-Agent's capability to interact with external APIs
  - Quick check question: What challenges arise when training LLMs to use external tools?

- Concept: Fine-tuning strategies for LLMs
  - Why needed here: Framework relies on Weighted LM strategy for tool-use tasks
  - Quick check question: How does Weighted LM differ from standard fine-tuning?

- Concept: Dense vector retrieval
  - Why needed here: Tool retrieval module uses this for API selection
  - Quick check question: How does dense vector retrieval work for tool selection?

## Architecture Onboarding

- Component map: User instruction → Tool Retrieval → Memory Assembly → LLM Processing → Tool Execution → Response Generation
- Critical path: User instruction flows through tool retrieval, memory assembly, LLM processing, tool execution, and response generation
- Design tradeoffs: Open-source vs closed-source LLMs, synthetic vs real API data, single-step vs multi-step tool-use
- Failure signatures: LLM tool selection errors, tool retrieval failures, memory context issues, pipeline execution problems
- First 3 experiments: 1) Evaluate open-source LLMs on MSAgent-Bench with Weighted LM, 2) Assess tool retrieval relevance for diverse prompts, 3) Test memory module on multi-turn scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is Weighted LM compared to standard fine-tuning for API parameter generation?
- Basis in paper: [explicit] Introduces Weighted LM with higher weights for API tokens
- Why unresolved: No direct comparisons with standard fine-tuning on identical datasets
- What evidence would resolve it: Direct comparison experiments on same datasets and metrics

### Open Question 2
- Question: What's optimal balance between retrieved APIs and agent-generated selection with very large tool libraries?
- Basis in paper: [inferred] Uses dense vector retrieval but doesn't explore scalability
- Why unresolved: Limited testing with small API sets, no scalability discussion
- What evidence would resolve it: Experiments varying tool library size (100, 1000, 10000 APIs)

### Open Question 3
- Question: How does performance degrade with semantically similar but functionally different tools?
- Basis in paper: [explicit] Claims generalization capability but lacks systematic evaluation
- Why unresolved: No testing of edge cases with similar tool descriptions
- What evidence would resolve it: Test suites with semantically similar but different APIs

## Limitations
- Synthetic dataset may not fully capture real-world tool-use complexity
- Dense vector retrieval may struggle with ambiguous or novel tool descriptions
- Weighted LM training strategy lacks complete specification for exact replication

## Confidence

**High confidence**: Framework architecture and modular design are well-documented and implementable. LLM integration follows established patterns.

**Medium confidence**: Weighted LM strategy and synthetic data generation lack complete technical details. Implementation specifics are partially specified.

**Low confidence**: Scalability with 1000+ APIs and real-world memory module performance in complex scenarios remain unverified.

## Next Checks

1. **Dataset Quality Validation**: Analyze MSAgent-Bench for diversity and realism across API categories and languages

2. **Training Strategy Replication**: Replicate Weighted LM on smaller scale with public tool-use datasets to verify API parameter generation improvement

3. **Real-World Deployment Test**: Deploy framework with actual APIs to evaluate performance, error handling, and user experience versus synthetic benchmarks