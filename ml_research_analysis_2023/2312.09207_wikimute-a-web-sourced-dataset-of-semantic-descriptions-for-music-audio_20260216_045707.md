---
ver: rpa2
title: 'WikiMuTe: A web-sourced dataset of semantic descriptions for music audio'
arxiv_id: '2312.09207'
source_url: https://arxiv.org/abs/2312.09207
tags:
- music
- data
- audio
- text
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces WikiMuTe, a new dataset of music audio paired
  with rich textual descriptions mined from Wikipedia. The dataset was created using
  a three-stage pipeline: data selection from Wikipedia, text-mining to extract music-related
  descriptions, and cross-modal relevance filtering to remove irrelevant pairs.'
---

# WikiMuTe: A web-sourced dataset of semantic descriptions for music audio

## Quick Facts
- arXiv ID: 2312.09207
- Source URL: https://arxiv.org/abs/2312.09207
- Reference count: 40
- This paper introduces WikiMuTe, a new dataset of music audio paired with rich textual descriptions mined from Wikipedia.

## Executive Summary
This paper introduces WikiMuTe, a new dataset of music audio paired with rich textual descriptions mined from Wikipedia. The dataset was created using a three-stage pipeline: data selection from Wikipedia, text-mining to extract music-related descriptions, and cross-modal relevance filtering to remove irrelevant pairs. Experiments using WikiMuTe for text-to-music retrieval and music classification show competitive performance compared to state-of-the-art models, especially after applying relevance filtering. The dataset offers a valuable resource for training multi-modal deep learning models in music information retrieval.

## Method Summary
WikiMuTe was created through a three-stage pipeline: (1) data selection from Wikipedia, extracting audio samples and associated text sources; (2) text mining using fine-tuned DistilBERT models to extract music-relevant descriptions at sentence and aspect levels; and (3) cross-modal relevance filtering using a pretrained text-to-audio alignment model to remove semantically mismatched pairs. The resulting dataset was used to train a bi-encoder model with MusicTaggingTransformer audio encoder and SentenceTransformers text encoder, optimized with NT-Xent contrastive loss for text-to-music retrieval and zero-shot classification tasks.

## Key Results
- WikiMuTe enables competitive text-to-music retrieval performance compared to existing datasets like MusicCaps
- Cross-modal relevance filtering significantly improves downstream task performance
- Zero-shot music classification using WikiMuTe embeddings achieves strong results on multiple benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WikiMuTe's three-stage pipeline (data selection → text-mining → relevance filtering) improves semantic alignment between music audio and descriptions.
- Mechanism: The pipeline progressively filters and refines raw Wikipedia content, first extracting candidate pairs, then mining relevant text spans, and finally removing semantically mismatched pairs via cross-modal scoring.
- Core assumption: Wikipedia captions and surrounding text contain enough music-relevant semantic cues to train a text-mining model that can extract meaningful descriptions.
- Evidence anchors:
  - [abstract] "three-stage pipeline: data selection from Wikipedia, text-mining to extract music-related descriptions, and cross-modal relevance filtering"
  - [section] "We have chosen the online encyclopedia Wikipedia as our source of data since it contains a vast array of articles on music pieces and popular songs"
  - [corpus] Weak evidence: Corpus shows similar approaches but does not directly validate the three-stage pipeline.
- Break condition: If Wikipedia content lacks sufficient music-relevant descriptions or contains too much noise, the text-mining model cannot learn effectively.

### Mechanism 2
- Claim: Text-mining with fine-tuned DistilBERT models captures both sentence-level and aspect-level music descriptions.
- Mechanism: Two separate fine-tuned DistilBERT models perform binary token classification to extract full sentences (long-form) and individual aspects (short-form) from raw text, covering different granularity levels of musical information.
- Core assumption: Music descriptions naturally decompose into meaningful sentences and aspects that can be independently detected by a classifier.
- Evidence anchors:
  - [section] "Pretrained Transformer models are a state-of-the-art solution for this task and have shown promising results in text mining for other fields [11]"
  - [section] "Both systems are trained by fine-tuning a pretrained DistilBERT [7,24] model on a binary token classification task"
  - [corpus] No direct evidence: Corpus neighbors focus on different data collection approaches without BERT-based text mining.
- Break condition: If the text-mining model fails to generalize beyond training captions, it will not extract relevant descriptions from broader article text.

### Mechanism 3
- Claim: Cross-modal relevance filtering using text-to-audio alignment models removes semantically irrelevant pairs, improving downstream retrieval performance.
- Mechanism: A pretrained text-to-audio alignment model scores each text-audio pair using cosine similarity of embeddings, removing pairs with negative similarity scores.
- Core assumption: Cross-modal alignment models can effectively distinguish semantically relevant from irrelevant text-music pairs.
- Evidence anchors:
  - [section] "we rate the semantic relevance of all music-text pairs and discard texts with low relevance from the final dataset"
  - [section] "we employ a model for text-to-audio alignment as a scoring function of how well a description matches a music piece"
  - [corpus] Weak evidence: Similar filtering approaches exist in the corpus but not specifically for Wikipedia-sourced data.
- Break condition: If the alignment model itself is poorly trained or biased, it may incorrectly filter out relevant pairs or retain irrelevant ones.

## Foundational Learning

- Concept: Cross-modal contrastive learning
  - Why needed here: The paper uses NT-Xent loss to train a bi-encoder model that maps text and audio into a shared embedding space for retrieval tasks.
  - Quick check question: What is the purpose of using NT-Xent loss instead of a standard classification loss in this context?

- Concept: Text mining with fine-tuned transformers
  - Why needed here: The pipeline relies on fine-tuned DistilBERT models to extract music-relevant descriptions from unstructured Wikipedia text.
  - Quick check question: Why is a binary token classification approach used instead of sequence labeling for extracting sentences and aspects?

- Concept: Zero-shot evaluation in multi-modal learning
  - Why needed here: The model is evaluated on downstream music classification tasks without fine-tuning, using similarity scores between audio and class label text embeddings.
  - Quick check question: How does zero-shot evaluation differ from supervised fine-tuning in terms of model generalization?

## Architecture Onboarding

- Component map: Wikipedia audio samples + text sources -> Fine-tuned DistilBERT models -> Cross-modal relevance filtering -> Bi-encoder with NT-Xent loss -> Text-to-music retrieval and classification

- Critical path:
  1. Extract audio-text pairs from Wikipedia
  2. Fine-tune text-mining models on annotated captions
  3. Apply text-mining to full Wikipedia text corpus
  4. Filter pairs using cross-modal relevance scoring
  5. Train bi-encoder model on filtered dataset
  6. Evaluate on retrieval and classification tasks

- Design tradeoffs:
  - Using Wikipedia provides large-scale data but introduces noise requiring extensive filtering
  - Fine-tuning DistilBERT balances model complexity with performance
  - Cross-modal filtering improves quality but depends on alignment model quality

- Failure signatures:
  - Poor retrieval performance indicates text-mining or relevance filtering failures
  - Low classification scores suggest embedding space misalignment
  - High variance across runs points to training instability

- First 3 experiments:
  1. Evaluate text-mining model precision/recall on annotated validation set
  2. Measure cross-modal filtering effectiveness by comparing filtered vs. unfiltered datasets
  3. Test bi-encoder training convergence and embedding space quality on a small validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of WikiMuTe compare to proprietary datasets like eCALS when scaled to larger dataset sizes?
- Basis in paper: [explicit] The paper notes that WikiMuTe has a smaller vocabulary and fewer tracks compared to eCALS, and suggests that increasing audio data could improve results.
- Why unresolved: The paper does not experiment with scaling WikiMuTe to larger sizes or directly comparing it to eCALS at comparable scales.
- What evidence would resolve it: A study comparing WikiMuTe and eCALS at similar dataset sizes, measuring performance on downstream tasks.

### Open Question 2
- Question: How robust is the text-mining pipeline to different languages and Wikipedia structures?
- Basis in paper: [explicit] The authors limit themselves to English Wikipedia and note that Wikipedia content is dynamic, but do not test the pipeline on other languages or time periods.
- Why unresolved: The paper does not explore the generalization of the pipeline to other languages or changes in Wikipedia over time.
- What evidence would resolve it: Applying the pipeline to multiple language editions of Wikipedia and comparing results over different database dumps.

### Open Question 3
- Question: What is the optimal balance between sentence-level and aspect-level annotations for training the text-mining system?
- Basis in paper: [inferred] The authors use both sentence and aspect annotations but do not experiment with varying their proportions or analyzing their individual contributions to performance.
- Why unresolved: The paper does not provide an ablation study or analysis of the relative importance of sentence vs. aspect annotations.
- What evidence would resolve it: An experiment varying the ratio of sentence to aspect annotations in training and measuring impact on text-mining accuracy.

### Open Question 4
- Question: How does the relevance filtering mechanism compare to alternative filtering strategies, such as human annotation or automatic music tagging?
- Basis in paper: [explicit] The authors use cross-modal relevance filtering and find it beneficial, but do not compare it to other filtering methods.
- Why unresolved: The paper does not benchmark the cross-modal filtering against alternative filtering approaches.
- What evidence would resolve it: A comparison of cross-modal filtering with human-annotated relevance scores and automatic music tagging-based filtering on the same dataset.

## Limitations
- Dataset quality depends on Wikipedia's variable music content quality and coverage
- Text mining approach lacks validation on diverse Wikipedia text beyond training captions
- Relevance filtering effectiveness not fully analyzed for potential false positives/negatives

## Confidence
- Dataset Construction Pipeline: Medium confidence
- Text Mining Performance: Low confidence
- Downstream Task Performance: Medium confidence

## Next Checks
1. Apply the fine-tuned text mining models to a diverse sample of Wikipedia articles not used in training and measure precision/recall of extracted descriptions against human annotations.
2. Compare downstream task performance using datasets with different relevance filtering thresholds to determine the optimal balance between dataset size and quality.
3. Evaluate the same text-to-music retrieval models on alternative music datasets (e.g., AudioCaps or other publicly available music description datasets) to test robustness beyond the MusicCaps benchmark.