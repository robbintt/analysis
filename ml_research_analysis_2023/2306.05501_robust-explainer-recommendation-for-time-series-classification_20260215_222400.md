---
ver: rpa2
title: Robust Explainer Recommendation for Time Series Classification
arxiv_id: '2306.05501'
source_url: https://arxiv.org/abs/2306.05501
tags:
- explanation
- time
- series
- methods
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of evaluating and comparing different
  explanation methods for time series classification, which is critical in domains
  like healthcare and finance. The proposed solution, AMEE, is a model-agnostic framework
  that quantifies the informativeness of explanation methods by measuring the impact
  of explanation-based perturbations on classification accuracy.
---

# Robust Explainer Recommendation for Time Series Classification

## Quick Facts
- arXiv ID: 2306.05501
- Source URL: https://arxiv.org/abs/2306.05501
- Reference count: 40
- Primary result: AMEE framework quantifies explanation informativeness by measuring perturbation impact on classification accuracy

## Executive Summary
This paper addresses the critical challenge of evaluating and comparing explanation methods for time series classification, which is essential in domains like healthcare and finance where model interpretability is paramount. The proposed AMEE framework provides a model-agnostic approach that quantifies the informativeness of explanation methods by measuring how perturbing discriminative parts of time series (identified by saliency maps) affects classification accuracy. By using multiple classifiers and perturbation strategies, AMEE produces a standardized "Explanation Power" metric that enables objective ranking and comparison of explanation methods across different datasets and scenarios.

## Method Summary
AMEE is a model-agnostic framework that evaluates explanation methods for time series classification by quantifying how perturbing discriminative regions affects classifier accuracy. The framework trains multiple diverse referee classifiers, applies each explanation method to generate saliency maps, and then perturbs time series data based on these maps using four strategies (local/global mean or Gaussian perturbations). For each explanation-referee-perturbation combination, it measures accuracy loss across different perturbation thresholds to compute Explanation AUC. These scores are standardized using min-max scaling within each (referee, perturbation) pair, then aggregated to produce the final Explanation Power metric in [0,1] range, where higher values indicate more informative explanations.

## Key Results
- AMEE effectively identifies the most informative explanation methods, with top methods achieving Explanation Power scores of 0.90+ on synthetic datasets
- The framework successfully ranks explanation methods differently across datasets, showing its ability to capture dataset-specific properties
- Real-world dataset experiments demonstrate AMEE's practical applicability, with top methods achieving scores of 0.84+ on HAR and LSST datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating perturbation impact across multiple classifiers and perturbation strategies yields a stable and comparable evaluation metric.
- Mechanism: The framework computes explanation AUC for each (perturbation, classifier, explanation) triple, then standardizes these scores using min-max rescaling within each row (classifier, perturbation pair) to normalize for classifier difficulty and perturbation severity. The final Explanation Power is the inverse of the average standardized score across all perturbations and classifiers.
- Core assumption: Different classifiers and perturbation strategies have comparable baseline performance and perturbation impact distributions.
- Evidence anchors:
  - [section]: "To be robust to different types of perturbations and different types of classifiers, we aggregate the accuracy loss across perturbations and classifiers."
  - [abstract]: "Experimental results on synthetic and real-world datasets show that AMEE can effectively identify the most informative explanation methods..."
  - [corpus]: Weak evidence; no direct comparison of multi-classifier aggregation methods in related work.
- Break condition: If classifiers have vastly different accuracies or perturbation strategies produce non-overlapping impact distributions, standardization will fail and Explanation Power will become unreliable.

### Mechanism 2
- Claim: Perturbing discriminative parts of the time series reduces classification accuracy more than perturbing non-discriminative parts, enabling quantitative comparison of explanation informativeness.
- Mechanism: Given a saliency map, the framework identifies the top-k percentile of time steps as discriminative, applies four different perturbation strategies (local/global mean or Gaussian), and measures the drop in accuracy for each referee classifier. Explanations that identify truly discriminative regions cause larger accuracy drops.
- Core assumption: The saliency map correctly identifies discriminative vs non-discriminative time steps.
- Evidence anchors:
  - [section]: "Perturbation of discriminative subsequences of the time series results in a reduced accuracy of classifiers."
  - [abstract]: "Perturbing discriminative parts of the time series leads to significant changes in classification accuracy, which can be used to evaluate each explanation."
  - [corpus]: No direct evidence; relies on assumption that saliency maps align with true discriminative regions.
- Break condition: If saliency maps are noisy or highlight irrelevant regions, perturbation will not correlate with actual discriminative information, making the evaluation metric meaningless.

### Mechanism 3
- Claim: Standardizing Explanation AUC across different referees and perturbations enables fair comparison of explanation methods.
- Mechanism: The framework applies min-max rescaling separately for each (referee, perturbation) pair, computes average scaled scores, then inverts to obtain Explanation Power in [0,1] range. This normalization accounts for varying baseline accuracies and perturbation strengths.
- Core assumption: Min-max scaling within each (referee, perturbation) pair is sufficient to make scores comparable across different experimental conditions.
- Evidence anchors:
  - [section]: "Metric Standardization & Explanation Power.AMEE employs multiple perturbation strategies and multiple referee classifiers. As the EAUC measures depend on the choices of referees and the perturbation strategies, they are not directly comparable. Thenextsteps(Figure4:Step2-5)standardizeandaggregate the EAUC to compute the final output of the framework: theExplanation Power."
  - [abstract]: "The framework provides a standardized way to evaluate explanations, helping users select appropriate methods for their specific applications."
  - [corpus]: No direct evidence of similar standardization approaches in related work.
- Break condition: If the distribution of EAUC values is very different across (referee, perturbation) pairs, min-max scaling may compress meaningful differences or amplify noise.

## Foundational Learning

- Concept: Time series classification and saliency-based explanation methods
  - Why needed here: The framework evaluates explanation methods that produce saliency maps for time series classification, so understanding both components is essential.
  - Quick check question: What is the difference between intrinsic and post-hoc explanation methods in the context of time series classification?

- Concept: Model-agnostic evaluation frameworks
  - Why needed here: AMEE is designed to evaluate explanation methods without relying on the internals of the classifiers, so understanding model-agnostic evaluation principles is crucial.
  - Quick check question: How does a model-agnostic evaluation framework differ from a model-specific one in terms of flexibility and potential biases?

- Concept: Perturbation-based evaluation strategies
  - Why needed here: The framework uses perturbation of time series based on saliency maps to measure explanation informativeness, so understanding perturbation strategies is essential.
  - Quick check question: What are the advantages and disadvantages of local vs global perturbation strategies in time series explanation evaluation?

## Architecture Onboarding

- Component map: Data pipeline -> Referee training -> Explanation generation -> Perturbation engine -> Evaluation engine -> Output module
- Critical path: Train referees → Generate explanations → Apply perturbations → Measure accuracy drops → Standardize and aggregate → Output Explanation Power
- Design tradeoffs:
  - Multiple referees increase robustness but add computational cost
  - Four perturbation strategies provide comprehensive evaluation but increase runtime
  - Standardization enables comparison but may mask meaningful differences
- Failure signatures:
  - Explanation Power close to random for all methods → Referees may be too weak or perturbations ineffective
  - Large variance across perturbations/referees → Explanation methods may be inconsistent or dataset too difficult
  - One explanation method consistently outperforms others by large margin → Possible bias in referees or perturbations
- First 3 experiments:
  1. Run AMEE with synthetic data and known ground truth to verify framework correctness
  2. Test with single referee and single perturbation to understand baseline behavior
  3. Compare results with different combinations of referees to assess robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number and diversity of referee classifiers to ensure robust evaluation of explanation methods?
- Basis in paper: [explicit] The paper discusses using a committee of referee classifiers to improve robustness, but does not provide specific guidelines on the optimal number or diversity of classifiers.
- Why unresolved: The paper shows that using multiple classifiers reduces bias, but the trade-off between computational cost and robustness is not explored. The impact of using fewer or more diverse classifiers is also not investigated.
- What evidence would resolve it: Experiments comparing explanation evaluation results using different numbers and types of referee classifiers, with analysis of the trade-offs in accuracy, computational cost, and robustness.

### Open Question 2
- Question: How does the AMEE framework perform on multivariate time series data, and are there modifications needed for optimal evaluation?
- Basis in paper: [inferred] The paper focuses on univariate time series, but many real-world applications involve multivariate time series. The perturbation strategies and explanation methods discussed may not directly translate to multivariate cases.
- Why unresolved: The paper does not address the challenges specific to multivariate time series, such as handling correlations between dimensions or the increased complexity of saliency maps.
- What evidence would resolve it: Extension of AMEE to multivariate time series datasets, with comparative analysis of evaluation results and any necessary modifications to the framework.

### Open Question 3
- Question: Can the AMEE framework be adapted to evaluate explanation methods for time series forecasting tasks, not just classification?
- Basis in paper: [inferred] The framework is designed for classification tasks, but forecasting is another critical application of time series analysis where explanation methods are equally important.
- Why unresolved: The paper does not discuss how the perturbation-based approach would work for continuous-valued forecasts or how to measure the impact on forecasting accuracy.
- What evidence would resolve it: Development and testing of an AMEE variant for forecasting, with case studies demonstrating its effectiveness in evaluating explanation methods for time series forecasting models.

### Open Question 4
- Question: How does the choice of perturbation strategy (local vs. global, mean vs. Gaussian) affect the evaluation of explanation methods in different domains?
- Basis in paper: [explicit] The paper introduces four perturbation strategies but does not provide domain-specific recommendations or analyze their relative effectiveness.
- Why unresolved: The impact of perturbation strategy choice on evaluation results is not explored across different types of time series data (e.g., physiological signals vs. financial data).
- What evidence would resolve it: Systematic comparison of AMEE results using different perturbation strategies across diverse domain-specific time series datasets, with analysis of which strategies are most effective for each domain.

## Limitations
- The framework's standardization approach may not adequately handle cases where different referees or perturbations produce vastly different EAUC distributions
- AMEE's effectiveness depends critically on the assumption that saliency maps accurately identify truly discriminative regions, which is not empirically validated
- The framework focuses on univariate time series and may require significant modifications for multivariate time series applications

## Confidence
- High Confidence: The core mechanism of using perturbation-based accuracy loss to evaluate explanation informativeness is theoretically sound and well-established in the literature.
- Medium Confidence: The multi-classifier aggregation approach and standardization methodology are reasonable but lack extensive validation across diverse experimental conditions.
- Low Confidence: The claim that AMEE can effectively rank explanation methods in real-world scenarios is not fully supported, as experimental results show significant variability in rankings across different datasets and referee combinations.

## Next Checks
1. Conduct ablation studies comparing AMEE's performance with single-referee vs. multi-referee setups to quantify the benefit of aggregation and identify when it breaks down.
2. Test the framework on synthetic datasets with known ground truth explanations to verify that higher Explanation Power scores correctly identify more accurate explanation methods.
3. Evaluate the sensitivity of Explanation Power rankings to different perturbation strengths and verify that standardization appropriately normalizes across these variations.