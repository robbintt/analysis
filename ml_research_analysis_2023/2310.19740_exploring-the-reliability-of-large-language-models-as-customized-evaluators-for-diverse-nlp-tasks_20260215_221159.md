---
ver: rpa2
title: Exploring the Reliability of Large Language Models as Customized Evaluators
  for Diverse NLP Tasks
arxiv_id: '2310.19740'
source_url: https://arxiv.org/abs/2310.19740
tags:
- evaluation
- criteria
- human
- task
- evaluators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the reliability of large language models (LLMs)
  as evaluators for diverse natural language processing (NLP) tasks. The study addresses
  limitations in current LLM evaluators, including fairness, scope, and accuracy,
  by exploring their alignment with human annotators.
---

# Exploring the Reliability of Large Language Models as Customized Evaluators for Diverse NLP Tasks

## Quick Facts
- arXiv ID: 2310.19740
- Source URL: https://arxiv.org/abs/2310.19740
- Reference count: 40
- Large language models (LLMs) can serve as reliable evaluators for NLP tasks when combined with human oversight in a collaborative pipeline.

## Executive Summary
This paper investigates the reliability of large language models (LLMs) as evaluators for diverse natural language processing (NLP) tasks. The study addresses key limitations in current LLM evaluators, including fairness, scope, and accuracy, by proposing a collaborative evaluation pipeline called COEVAL. Through experiments on three open-ended NLP tasks—story generation, instruction following, and long-form question answering—the authors demonstrate that LLM-generated evaluation criteria are generally comprehensive but may miss crucial criteria or include unnecessary ones. The study finds that COEVAL reduces annotation outliers by 62% and saves significant time compared to pure human evaluation while maintaining reliability. Human involvement remains essential, particularly for complex criteria, with around 20% of LLM evaluation scores revised during scrutiny. The collaborative approach improves inter-annotator agreement, highlighting the potential of LLMs as scalable evaluators when combined with human oversight.

## Method Summary
The paper proposes COEVAL, a collaborative evaluation pipeline that leverages LLM ideation and human scrutiny to evaluate open-ended NLP tasks. The process involves: (1) LLM ideation generates task-specific evaluation criteria based on task descriptions, input/output fields, and candidate model outputs; (2) human evaluators refine these criteria through approval, deletion, revision, or addition; (3) LLM evaluates each instance using the refined criteria with explanations; and (4) human evaluators scrutinize these assessments and make corrections as needed. The pipeline was tested on three tasks (story generation, instruction following, and long-form question answering) with 50 instances each, using GPT-3.5-turbo-0301 for all LLM evaluations.

## Key Results
- COEVAL reduces annotation outliers by 62% compared to pure human evaluation while maintaining reliability
- LLM-generated criteria are generally comprehensive but sometimes omit crucial criteria or include unnecessary ones
- Human evaluators revise approximately 20% of LLM evaluation scores during scrutiny, particularly for complex criteria requiring numerical reasoning
- Inter-annotator agreement improves from Krippendorff's α of 0.64 to 0.71 with the collaborative COEVAL approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM ideation reduces the time and effort required for human evaluation while maintaining reliability.
- Mechanism: LLM generates preliminary evaluation criteria and instance-level assessments, which humans then scrutinize and correct. This collaborative approach leverages LLM's efficiency and human judgment to refine outputs.
- Core assumption: LLM-generated outputs are sufficiently comprehensive and accurate to serve as a starting point for human evaluation.
- Evidence anchors:
  - [abstract]: "COEVAL reduces annotation outliers by 62% and saves significant time compared to pure human evaluation, while maintaining reliability."
  - [section]: "Results show that, by utilizing LLMs, COEVAL effectively evaluates lengthy texts, saving significant time and reducing human evaluation outliers."
  - [corpus]: "Average neighbor FMR=0.46, average citations=0.0." (Weak corpus evidence, but indicates related work exists)
- Break condition: If LLM-generated criteria or evaluations are consistently inaccurate or incomplete, the collaborative approach would fail to improve reliability.

### Mechanism 2
- Claim: Human scrutiny ensures the reliability of LLM-generated evaluations by correcting errors and inconsistencies.
- Mechanism: Human evaluators review and revise LLM-generated criteria and evaluations, ensuring they align with task requirements and are free from errors.
- Core assumption: Human evaluators can effectively identify and correct errors in LLM-generated outputs.
- Evidence anchors:
  - [abstract]: "Human involvement remains essential, particularly for complex criteria, with around 20% of LLM evaluation scores revised during scrutiny."
  - [section]: "Human scrutiny still plays a role, revising around 20% of LLM evaluation scores for ultimate reliability."
  - [corpus]: "Average neighbor FMR=0.46, average citations=0.0." (Weak corpus evidence, but indicates related work exists)
- Break condition: If human evaluators are unable to effectively identify or correct errors in LLM-generated outputs, the reliability of the collaborative approach would be compromised.

### Mechanism 3
- Claim: The collaborative approach improves inter-annotator agreement by aligning human evaluators with LLM-generated outputs.
- Mechanism: By involving LLM in the evaluation process, the collaborative approach provides a common reference point for human evaluators, reducing subjectivity and improving agreement.
- Core assumption: LLM-generated outputs serve as a useful reference point for human evaluators, reducing individual biases and improving agreement.
- Evidence anchors:
  - [abstract]: "With the collaborative design of COEVAL, the evaluation agreement of Krippendorff’s α notably improves from 0.64 to 0.71."
  - [section]: "Human evaluators have access to four distinct actions (aapv, adel, arevise, aadd) to scrutinize the reliability and consistency of EM with respect to the information available for the given task and instance."
  - [corpus]: "Average neighbor FMR=0.46, average citations=0.0." (Weak corpus evidence, but indicates related work exists)
- Break condition: If the collaborative approach fails to improve inter-annotator agreement, it would suggest that the LLM-generated outputs are not effectively aligning human evaluators.

## Foundational Learning

- Concept: Large Language Models (LLMs) as Evaluators
  - Why needed here: Understanding how LLMs can be used as evaluators for NLP tasks is crucial for implementing the collaborative evaluation pipeline.
  - Quick check question: How do LLMs generate evaluation criteria and instance-level assessments for NLP tasks?
- Concept: Human Evaluation in NLP
  - Why needed here: Human evaluation is essential for ensuring the reliability and accuracy of LLM-generated evaluations.
  - Quick check question: What are the key challenges and limitations of human evaluation in NLP tasks?
- Concept: Inter-Annotator Agreement
  - Why needed here: Measuring inter-annotator agreement is important for assessing the reliability and consistency of the collaborative evaluation approach.
  - Quick check question: How is inter-annotator agreement measured, and what are the implications of different agreement levels?

## Architecture Onboarding

- Component map: LLM ideation -> Human scrutiny (criteria refinement) -> LLM evaluation -> Human scrutiny (evaluation refinement)
- Critical path: The critical path involves LLM ideation for criteria and evaluations, followed by human scrutiny to refine and correct the outputs.
- Design tradeoffs: The main tradeoff is between the efficiency of LLM-generated evaluations and the reliability ensured by human scrutiny. The collaborative approach aims to balance these factors.
- Failure signatures: Failure signatures include consistently inaccurate or incomplete LLM-generated criteria or evaluations, and human evaluators' inability to effectively identify or correct errors.
- First 3 experiments:
  1. Evaluate the effectiveness of LLM-generated criteria by comparing them to human-generated criteria on a small set of tasks.
  2. Assess the reliability of LLM-generated evaluations by comparing them to human evaluations on a small set of instances.
  3. Measure inter-annotator agreement among human evaluators using the collaborative approach and compare it to agreement in traditional human evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of COEVAL compare to other collaborative human-AI evaluation frameworks beyond pure human evaluation?
- Basis in paper: [explicit] The paper compares COEVAL to conventional human evaluation (HUMAN EVAL) but does not explore comparisons with other collaborative frameworks.
- Why unresolved: The study focuses on demonstrating COEVAL's effectiveness relative to traditional human evaluation, leaving open questions about its performance against alternative collaborative methods.
- What evidence would resolve it: Empirical results comparing COEVAL to other human-AI collaborative evaluation pipelines in terms of time efficiency, reliability, and inter-annotator agreement.

### Open Question 2
- Question: Can COEVAL be effectively adapted to evaluate tasks in domains outside of NLP, such as computer vision or multimodal tasks?
- Basis in paper: [inferred] The paper mentions that COEVAL is designed for open-ended NLG tasks and does not involve other modalities, suggesting potential limitations in scope.
- Why unresolved: The study is confined to text-based NLP tasks, and the adaptability of COEVAL to other domains or multimodal tasks remains unexplored.
- What evidence would resolve it: Experimental results demonstrating COEVAL's effectiveness in evaluating tasks from other domains, such as image captioning or video summarization, with adapted evaluation criteria.

### Open Question 3
- Question: How does the choice of LLM (e.g., ChatGPT vs. other models like GPT-4 or LLAMA-2) impact the quality and reliability of COEVAL's evaluations?
- Basis in paper: [explicit] The paper uses ChatGPT (gpt-3.5-turbo-0301) but acknowledges that other LLMs could theoretically be used, without exploring their impact.
- Why unresolved: The study does not compare the performance of COEVAL when using different LLMs, leaving questions about the generalizability of its results.
- What evidence would resolve it: Comparative analysis of COEVAL's performance using different LLMs, focusing on evaluation quality, time efficiency, and human agreement rates.

## Limitations
- The study is limited to three NLP tasks, restricting generalizability to other domains and task types.
- Exact prompt formats and interface details for human evaluation are not fully specified, making exact replication challenging.
- The study relies on GPT-3.5-turbo-0301 without comparing other LLM variants or exploring parameter sensitivity.

## Confidence

**High Confidence**: The COEVAL pipeline's time-saving benefits (62% reduction in outliers) and general effectiveness in improving inter-annotator agreement (Krippendorff's α improvement from 0.64 to 0.71) are well-supported by the experimental results across all three tasks.

**Medium Confidence**: The claim that LLM evaluators struggle with complex criteria like numerical reasoning and factuality is supported by specific examples but lacks comprehensive quantification across all criteria types. The 20% revision rate for human scrutiny is reported but not deeply analyzed for consistency across different tasks.

**Low Confidence**: The assertion that LLM-generated criteria are "generally comprehensive" lacks systematic comparison metrics against human-generated criteria. The claim about "significant time savings" is qualitative without specific time measurements provided.

## Next Checks

1. **Cross-Domain Validation**: Test COEVAL on additional NLP tasks including text summarization, machine translation, and dialogue systems to assess generalizability beyond the three tasks examined.

2. **Parameter Sensitivity Analysis**: Systematically vary LLM temperature settings, prompt engineering strategies, and model variants (GPT-4, Claude) to identify optimal configurations for different evaluation criteria types.

3. **Human Expert Benchmarking**: Conduct a controlled study comparing COEVAL's reliability against evaluations from domain experts working independently, measuring both accuracy and efficiency trade-offs.