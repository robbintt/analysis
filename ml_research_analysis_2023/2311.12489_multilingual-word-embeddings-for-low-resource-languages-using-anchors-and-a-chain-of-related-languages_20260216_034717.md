---
ver: rpa2
title: Multilingual Word Embeddings for Low-Resource Languages using Anchors and a
  Chain of Related Languages
arxiv_id: '2311.12489'
source_url: https://arxiv.org/abs/2311.12489
tags:
- languages
- language
- embeddings
- low-resource
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CHAIN MWEs, a novel method for improving multilingual
  word embeddings for very low-resource languages by incorporating intermediate related
  languages to bridge the gap between distant source and target languages. The approach
  extends the anchor-based bilingual method ANCHOR BWEs to multiple languages by building
  embeddings step-by-step, starting from the source language and adding each intermediate
  language until reaching the target.
---

# Multilingual Word Embeddings for Low-Resource Languages using Anchors and a Chain of Related Languages

## Quick Facts
- arXiv ID: 2311.12489
- Source URL: https://arxiv.org/abs/2311.12489
- Reference count: 18
- Key outcome: CHAIN MWEs improves bilingual lexicon induction performance for very low-resource languages by using intermediate related languages, achieving higher precision at 1, 5, and 10 compared to baseline methods.

## Executive Summary
This paper introduces CHAIN MWEs, a novel method for improving multilingual word embeddings for very low-resource languages by leveraging a chain of intermediate related languages to bridge the gap between distant source and target languages. The approach extends the anchor-based bilingual method ANCHOR BWEs to multiple languages by building embeddings step-by-step, starting from the source language and adding each intermediate language until reaching the target. Experiments on 4 language families involving 4 moderately low-resource (≤50M tokens) and 4 very low-resource (≤5M tokens) target languages demonstrate that CHAIN MWEs outperforms both bilingual and multilingual mapping-based baselines, as well as the ANCHOR BWEs baseline, achieving higher precision at 1, 5, and 10 in bilingual lexicon induction tasks.

## Method Summary
CHAIN MWEs builds multilingual embeddings sequentially by starting with a high-resource source language (English) and iteratively adding intermediate related languages using an anchor-based approach. At each step, the method accumulates seed lexicons from all previous language pairs to improve alignment quality. The process uses Word2Vec for monolingual embedding training and pivots through bilingual dictionaries obtained from Wiktionary. The chain construction is based on linguistic relatedness, with intermediate languages selected to minimize the linguistic distance between adjacent languages in the chain. The final multilingual space is evaluated using bilingual lexicon induction precision at 1, 5, and 10.

## Key Results
- CHAIN MWEs achieves higher P@1, P@5, and P@10 than both mapping-based baselines (VecMap, UMWE) and the ANCHOR BWEs baseline across all 8 target languages
- The accumulation of anchor points from all previous languages in the chain provides sizable performance improvements
- Adding moderately low-resource intermediate languages does not always improve results, highlighting the importance of good quality embeddings for intermediate languages
- Performance gains are particularly pronounced for very low-resource languages with limited training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using intermediate related languages reduces the linguistic distance between source and target languages, leading to better alignment quality.
- Mechanism: The method builds multilingual embeddings step-by-step, starting from the source and adding each intermediate language sequentially. This minimizes the distance between any two languages being aligned at each step.
- Core assumption: The linguistic gap between adjacent languages in the chain is smaller than the gap between source and target directly.
- Evidence anchors:
  - [abstract] "we propose to leverage a chain of intermediate related languages to overcome the large language gap"
  - [section 2] "Recent work showed that good cross-lingual performance can be achieved if a source language is related to the low-resource target language"
  - [corpus] Found 25 related papers; average neighbor FMR=0.443 indicating moderate relatedness among language embedding approaches

### Mechanism 2
- Claim: The anchor-based training method transfers quality properties from source to target embeddings during the alignment process.
- Mechanism: Instead of random initialization, target words in the seed dictionary are initialized using their related anchor points from the source space. The training process then follows unchanged CBOW/Skip-gram algorithms.
- Core assumption: Good quality source embeddings have properties (like similar embeddings for similar meaning words) that can be transferred to target space through anchor points.
- Evidence anchors:
  - [section 3.1] "the properties of the good quality source space, such as similar embeddings for words with similar meaning, is transferred to the target space"
  - [section 2] "Eder et al. (2021) showed that CBOW outperforms SG in ANCHOR BWE S"
  - [corpus] Related work shows mapping-based approaches struggle with very low-resource languages due to poor quality monolingual embeddings

### Mechanism 3
- Claim: Accumulating anchor points from all previous languages in the chain improves embedding quality at each step.
- Mechanism: At each iteration, the method concatenates seed lexicons from all language pairs involving the current language and languages already in the multilingual space.
- Core assumption: More anchor points lead to better alignment quality, as shown in previous work.
- Evidence anchors:
  - [section 3.2] "our goal is to take all available anchor points already in Mi−1" and "Eder et al. (2021) showed that ANCHOR BWE S performs better as the number of available anchor points increase"
  - [section 5.2] Ablation study shows sizable performance impact when turning dictionary accumulation off
  - [corpus] Found 25 related papers with moderate FMR scores, suggesting anchor-based approaches are active research area

## Foundational Learning

- Concept: Static word embeddings vs contextualized representations
  - Why needed here: The paper uses static word embeddings (Word2Vec) rather than contextualized representations (BERT, etc.) due to larger data requirements of the latter
  - Quick check question: Why did the authors choose static embeddings over contextualized ones for this low-resource task?

- Concept: Bilingual Lexicon Induction (BLI) as evaluation metric
  - Why needed here: BLI is the de facto task for evaluating cross-lingual word embeddings quality
  - Quick check question: What evaluation metric does the paper use to assess cross-lingual embedding quality?

- Concept: Mapping-based vs joint learning approaches for MWEs
  - Why needed here: The paper contrasts its anchor-based semi-joint approach with mapping and joint learning methods
  - Quick check question: What are the two main approaches to obtain multilingual word embeddings mentioned in the paper?

## Architecture Onboarding

- Component map: Source language embedding space → Sequential addition of intermediate languages → Target language embedding space, with anchor-based alignment at each step
- Critical path: Monolingual embedding training → Anchor dictionary creation → Sequential anchor-based alignment → Evaluation via BLI
- Design tradeoffs: Sequential processing allows controlled alignment but may be slower than direct mapping; anchor-based approach requires quality seed dictionaries
- Failure signatures: Poor performance when intermediate languages are not sufficiently related; degradation when anchor dictionaries are low quality or insufficient
- First 3 experiments:
  1. Test with closely related intermediate languages (e.g., Norwegian→Swedish→Faroese) to verify basic chain mechanism works
  2. Compare performance with and without dictionary accumulation to validate mechanism 3
  3. Test with distant intermediate languages to identify break conditions for linguistic relatedness assumptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of intermediate language embeddings impact the final multilingual word embeddings?
- Basis in paper: [explicit] The paper discusses the importance of good quality embeddings for intermediate languages and shows that adding moderately low-resource languages does not always improve results.
- Why unresolved: The paper provides some evidence but does not systematically study the impact of intermediate language quality on the final embeddings.
- What evidence would resolve it: Experiments varying the quality of intermediate language embeddings and measuring the impact on final multilingual embeddings.

### Open Question 2
- Question: What is the optimal ordering of languages in the chain for building multilingual word embeddings?
- Basis in paper: [inferred] The paper uses a manually selected ordering of languages based on linguistic proximity, but does not explore all possible orderings.
- Why unresolved: The paper only explores one ordering for each language family and does not compare different orderings.
- What evidence would resolve it: Experiments comparing different orderings of languages in the chain and measuring the impact on final multilingual embeddings.

### Open Question 3
- Question: How does the size of the intermediate language corpora affect the quality of the final multilingual word embeddings?
- Basis in paper: [explicit] The paper selects intermediate languages based on their corpus size, but does not systematically study the impact of corpus size.
- Why unresolved: The paper does not provide a systematic study of the relationship between intermediate language corpus size and final embedding quality.
- What evidence would resolve it: Experiments varying the size of intermediate language corpora and measuring the impact on final multilingual embeddings.

## Limitations
- Limited evaluation scope with only 8 target languages across 4 language families may not generalize to other low-resource language contexts
- Reliance on Wiktionary dictionaries via pivoting introduces potential noise and coverage limitations, particularly for very low-resource languages
- Sequential chain approach may be computationally inefficient compared to direct mapping methods for longer chains

## Confidence
- High Confidence: The improvement over ANCHOR BWEs baseline and the importance of accumulating anchor points from all languages are well-supported by ablation studies and consistent experimental results across multiple language families.
- Medium Confidence: The claim that intermediate related languages reduce linguistic distance is supported by empirical results but lacks quantitative analysis of linguistic distances or systematic comparison of different chain lengths and orders.
- Low Confidence: The general applicability of the approach to any low-resource language scenario is not fully established due to limited evaluation scope and the assumption that suitable intermediate languages and quality dictionaries are always available.

## Next Checks
1. **Generalization Test:** Apply CHAIN MWEs to additional low-resource languages outside the Uralic and Austronesian families to assess cross-family generalization, particularly testing with language pairs that have minimal linguistic overlap.

2. **Chain Optimization Analysis:** Conduct systematic experiments varying the number and order of intermediate languages in chains, measuring how chain length and linguistic distance between adjacent languages affect final embedding quality.

3. **Dictionary Quality Impact:** Perform controlled experiments with synthetic dictionaries of varying quality (precision/recall) to quantify the impact of dictionary noise on CHAIN MWEs performance versus baseline methods, isolating the effect of dictionary quality from the chain mechanism.