---
ver: rpa2
title: 'EDAC: Efficient Deployment of Audio Classification Models For COVID-19 Detection'
arxiv_id: '2309.05357'
source_url: https://arxiv.org/abs/2309.05357
tags:
- pruning
- size
- covid-19
- time
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study explores the feasibility of deploying deep learning
  models for COVID-19 detection using cough audio recordings on edge devices. Two
  baseline models were recreated: a CNN model and a CNN-LSTM attention model.'
---

# EDAC: Efficient Deployment of Audio Classification Models For COVID-19 Detection

## Quick Facts
- arXiv ID: 2309.05357
- Source URL: https://arxiv.org/abs/2309.05357
- Reference count: 40
- Two baseline models (CNN and CNN-LSTM) can be compressed significantly without large performance loss for COVID-19 detection from cough audio.

## Executive Summary
This study explores the feasibility of deploying deep learning models for COVID-19 detection using cough audio recordings on edge devices. Two baseline models were recreated: a CNN model and a CNN-LSTM attention model. Network pruning and quantization were applied to compress these models while maintaining predictive performance. The results showed that both models could be compressed significantly without large performance loss. The CNN model achieved an ~105.76x reduction in model size with a 0.018 AUC point reduction, while the CNN-LSTM model achieved a ~19.34x reduction in model size with a 0.01 AUC point reduction. Inference times were also reduced by ~1.37x and ~1.71x, respectively. The study demonstrates that network compression techniques can increase the feasibility of deploying COVID-19 detection models on edge devices.

## Method Summary
The study recreated two baseline models: a CNN model and a CNN-LSTM attention model. These models were trained on the Coswara and CoughVid datasets, respectively. Network pruning using magnitude-based pruning with constant and polynomial decay schedules was applied to both models at various sparsity levels (10% to 99.9%). Post-training quantization was then applied to both the baseline models and the pruned models, comparing performance metrics (AUC, model size, inference time). The study focused on compressing the models while maintaining predictive performance for COVID-19 detection.

## Key Results
- CNN model: ~105.76x reduction in model size with 0.018 AUC point reduction
- CNN-LSTM model: ~19.34x reduction in model size with 0.01 AUC point reduction
- Inference time reductions: ~1.37x for CNN and ~1.71x for CNN-LSTM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Network pruning removes redundant weights while maintaining predictive performance for COVID-19 detection.
- Mechanism: Magnitude-based pruning identifies and eliminates weights with smallest norms, reducing model size without significant loss in AUC.
- Core assumption: Low-magnitude weights contribute minimally to the model's predictive capability.
- Evidence anchors:
  - [abstract] "Through applying network pruning and quantisation, we were able to compress these two architectures without reducing the model's predictive performance."
  - [section 5.1] "In the case of both the CNN and CNN-LSTM models, we see that both models can be pruned to 90% without affecting model performance."
  - [corpus] Weak evidence - no direct citation, but general literature supports this mechanism.
- Break condition: Performance degradation occurs beyond 90% pruning, indicating that too many weights are critical for maintaining model accuracy.

### Mechanism 2
- Claim: Quantization reduces computational requirements and model size with minimal performance loss.
- Mechanism: Post-training quantization reduces precision of weights (e.g., from 32-bit to 8-bit), decreasing memory usage and inference time.
- Core assumption: Lower precision weights can approximate original weights closely enough to maintain model accuracy.
- Evidence anchors:
  - [abstract] "Specifically, we were able to achieve an ∼ 105.76× and an ∼ 19.34× reduction in the compressed model file size... with corresponding ∼ 1.37× and ∼ 1.71× reductions in the inference times of the two models."
  - [section 5.2] "At 90% sparsity, there was a ∼ 19.34× reduction in model size for the 8-bit model relative to the baseline for the CNN-LSTM, whilst losing 0.01 AUC points."
  - [corpus] Weak evidence - general literature supports this mechanism but no specific citation provided.
- Break condition: Excessive quantization (e.g., very low bit precision) leads to significant performance degradation.

### Mechanism 3
- Claim: Combining pruning and quantization amplifies the benefits of each technique.
- Mechanism: Pruning first reduces the number of weights, then quantization further compresses the remaining weights, leading to greater reductions in model size and inference time.
- Core assumption: The benefits of pruning and quantization are additive when applied sequentially.
- Evidence anchors:
  - [abstract] "We were able to compress these two architectures without reducing the model's predictive performance... with corresponding ∼ 1.37× and ∼ 1.71× reductions in the inference times of the two models."
  - [section 5.2] "At 95% sparsity, the 8-bit CNN model had a ∼ 1.37× reduction in inference time relative to the baseline, whilst the 8-bit CNN-LSTM model had a ∼ 1.71× reduction in inference speed relative to its baseline."
  - [corpus] Weak evidence - general literature supports this mechanism but no specific citation provided.
- Break condition: Diminishing returns or increased complexity in model compression pipeline.

## Foundational Learning

- Concept: Neural Network Pruning
  - Why needed here: To reduce model size and computational requirements for deployment on edge devices.
  - Quick check question: What is the primary criterion used in magnitude-based pruning to determine which weights to eliminate?

- Concept: Quantization
  - Why needed here: To further reduce model size and inference time by lowering the precision of weights.
  - Quick check question: What is the trade-off when reducing the precision of weights in a neural network?

- Concept: Spectrogram and MFCC Feature Extraction
  - Why needed here: To convert audio recordings into a format suitable for input into deep learning models for COVID-19 detection.
  - Quick check question: What are the key differences between spectrograms and MFCCs as features for audio classification tasks?

## Architecture Onboarding

- Component map: Data preprocessing (spectrogram/MFCC extraction) -> Model architecture (CNN/CNN-LSTM) -> Compression techniques (pruning and quantization) -> Evaluation on test set
- Critical path: Data preprocessing → Model training → Pruning → Quantization → Evaluation on test set
- Design tradeoffs: Balancing model compression (pruning and quantization) with maintaining predictive performance (AUC score)
- Failure signatures: Significant drop in AUC score, excessive increase in inference time, or inability to deploy on target edge devices
- First 3 experiments:
  1. Reproduce baseline models (CNN and CNN-LSTM) on their respective datasets and evaluate performance.
  2. Apply pruning to baseline models at various sparsity levels and measure impact on model size, inference time, and AUC score.
  3. Apply quantization to pruned models and evaluate the compounded effect on model size, inference time, and AUC score.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different pruning schemes (e.g., movement pruning) compare to magnitude-based pruning in terms of model compression and performance for COVID-19 detection models?
- Basis in paper: [inferred] The paper mentions that exploring other pruning schemes like movement pruning could be an avenue for future research, but does not compare different schemes.
- Why unresolved: The paper only used magnitude-based pruning and did not investigate other pruning schemes.
- What evidence would resolve it: Experiments comparing magnitude-based pruning with other pruning schemes (e.g., movement pruning) on the same COVID-19 detection models and datasets, reporting compression ratios, inference times, and AUC scores for each scheme.

### Open Question 2
- Question: What is the impact of quantization-aware training on the performance and compression of COVID-19 detection models compared to post-training quantization?
- Basis in paper: [explicit] The paper mentions that quantization-aware training could be explored as an alternative to post-training quantization.
- Why unresolved: The paper only used post-training quantization and did not investigate quantization-aware training.
- What evidence would resolve it: Experiments comparing post-training quantization with quantization-aware training on the same COVID-19 detection models and datasets, reporting compression ratios, inference times, and AUC scores for each approach.

### Open Question 3
- Question: How does the deployment of compressed COVID-19 detection models on edge devices with sparse matrix computation support compare to deployment without such support in terms of inference time and resource usage?
- Basis in paper: [inferred] The paper mentions that benchmarking experiments on edge devices that support sparse matrix computation could be an extension of the study, but does not provide such results.
- Why unresolved: The paper does not report on the performance of compressed models on actual edge devices with sparse matrix computation support.
- What evidence would resolve it: Benchmarking experiments deploying the compressed COVID-19 detection models on representative edge devices, both with and without sparse matrix computation support, measuring inference times, memory usage, and energy consumption.

## Limitations
- The study relies on two specific datasets which may not fully represent real-world diversity.
- Compression techniques were applied to only two baseline architectures, limiting generalizability.
- Evaluation focuses on AUC scores and inference metrics without comprehensive robustness assessment.

## Confidence
- **High confidence**: Network pruning and quantization can significantly reduce model size and inference time while maintaining predictive performance.
- **Medium confidence**: Specific compression ratios and performance metrics may vary with different datasets or model architectures.
- **Low confidence**: Long-term stability and reliability of compressed models in real-world deployment scenarios are not sufficiently evidenced.

## Next Checks
1. **Cross-dataset validation**: Evaluate compressed models on additional COVID-19 cough audio datasets to assess generalization across different recording conditions and populations.
2. **Ablation studies**: Systematically vary pruning percentages and quantization precision levels to identify optimal compression configurations for different performance requirements.
3. **Robustness testing**: Conduct comprehensive testing of compressed models against noise injection, data drift scenarios, and adversarial examples to assess real-world reliability.