---
ver: rpa2
title: 'Stable Normative Explanations: From Argumentation to Deontic Logic'
arxiv_id: '2307.05156'
source_url: https://arxiv.org/abs/2307.05156
tags:
- argumentation
- theory
- defeasible
- logic
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the notion of stable explanation from defeasible
  logic to the setting of formal argumentation, where arguments are built from defeasible
  rules. The key result is a semantic characterisation of stable normative explanation
  in terms of neighbourhood models for deontic logic.
---

# Stable Normative Explanations: From Argumentation to Deontic Logic

## Quick Facts
- arXiv ID: 2307.05156
- Source URL: https://arxiv.org/abs/2307.05156
- Reference count: 34
- One-line primary result: A semantic characterization of stable normative explanations using neighborhood models for deontic logic, showing stability checking is co-NP-complete

## Executive Summary
This paper extends the concept of stable explanation from defeasible logic to formal argumentation frameworks built from defeasible rules. The key innovation is establishing a semantic characterization of stable normative explanations in terms of neighborhood models for deontic logic. A stable normative explanation for a conclusion φ is defined as one that remains valid under all possible extensions of the set of facts, provided they are consistent with the rules. The authors prove that determining whether a given explanation is stable is co-NP-complete, while determining non-stability is NP-complete. They also demonstrate how to construct a neighborhood model from an argumentation framework such that the stability of an explanation corresponds to its persistence across all possible extensions of the model.

## Method Summary
The paper constructs a semantic characterization of stable normative explanations by mapping argumentation frameworks to neighborhood models for deontic logic. Given a defeasible theory (F, R, >) where F is a consistent set of facts, R is a set of defeasible rules, and > is a superiority relation, the method builds an argumentation framework and then constructs a canonical neighborhood D-model. The stability of a normative explanation Expl(φ, AF(D)) is verified by checking if it remains valid across all extensions F' where F ⊆ F' ⊆ Lit(R). The approach uses generated submodels to isolate relevant arguments and verify persistence of the explanation under added facts.

## Key Results
- Stable normative explanations can be semantically characterized using neighborhood models for deontic logic
- The problem of determining stability is co-NP-complete, while determining non-stability is NP-complete
- A construction exists that maps argumentation frameworks to neighborhood models where explanation stability corresponds to persistence across all extensions
- The framework provides a logical characterization of argument resilience applicable to legal reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stable normative explanations in defeasible logic can be mapped to persistence of obligations across all extensions in a neighborhood model.
- Mechanism: The construction from argumentation frameworks to neighborhood models ensures that a normative explanation for φ is stable if, in every possible extension of the theory, the corresponding argument for φ remains justified. This is encoded by requiring that the set of worlds supporting φ is closed under the neighborhood relation across all extensions.
- Core assumption: The superiority relation in the argumentation framework is acyclic, and the dependency graph is also acyclic, ensuring well-defined extensions.
- Evidence anchors:
  - [abstract] "a stable normative explanation for a conclusion φ is one that remains valid under all possible extensions of the set of facts"
  - [section 3] "arg = Expl(φ , AF(D)) is a stable normative explanation for φ in AF(D) iff for all AF(D′) = ( A ′, ≫ ′) where D ′ = ( F ′, R, >) s.t. F ⊆ F ′ ⊆ Lit(R), we have that arg = Expl(φ , AF(D′))"
  - [corpus] Weak evidence; neighboring papers discuss stability in argumentation but not this specific neighborhood semantics mapping.
- Break condition: If the superiority relation or dependency graph contains cycles, the extension may not be well-defined, breaking the correspondence between stable explanations and persistence in the model.

### Mechanism 2
- Claim: The truth lemma ensures that justified arguments correspond exactly to the presence of the corresponding obligation in the neighborhood model.
- Mechanism: By constructing a canonical neighborhood D-model where each world represents a maximal consistent extension of the defeasible theory, the truth lemma shows that an argument for φ is justified iff the model forces OBL φ in all relevant worlds.
- Core assumption: The canonical model construction preserves the justification relation of the argumentation framework.
- Evidence anchors:
  - [section 4.2] "Lemma 2 (Truth Lemma). If M = (W, N , v) is canonical for S, where S ⊇ EL , then for any w ∈ W and for any formula p, p ∈ w iff M , w |= φ ."
  - [section 4] "D ⊢ +∂ φ if and only if M , w |= OBLφ for some world w in some model M"
  - [corpus] Moderate evidence; neighboring papers discuss argumentation semantics but not this exact canonical model construction.
- Break condition: If the model construction does not preserve the justification relation (e.g., due to inconsistency in extensions), the correspondence breaks.

### Mechanism 3
- Claim: The generated submodel isolates the arguments in a normative explanation, allowing stability to be checked by verifying persistence in a restricted model.
- Mechanism: By generating a submodel that contains only the worlds relevant to the explanation, one can verify that adding new facts does not introduce new worlds that invalidate the explanation, thus ensuring stability.
- Core assumption: The generated submodel preserves the truth of modal formulas and the justification relation for the arguments in the explanation.
- Evidence anchors:
  - [section 5] "Proposition 6 (Generated D-submodel for a normative explanation). Let D = (F, R, >) be an argumentation theory... X = Expl(ψ, AF(D)) = {A1, . . . , An} iff WX = W − X where X = {w |w ∈ W, ∀φ ∈ w : φ ∈ F & Ax ∈ A , Ax ̸∈ X and Ax :⇒ F φ }"
  - [section 5] "Corollary 3 (Stable normative explanation in neighbourhood D-models). Let D = (F, R, >) be an argumentation theory... If X = Expl(ψ, AF(D)) = {A1, . . . , An} is a stable normative explanation for ψ in AF(D)..."
  - [corpus] Weak evidence; neighboring papers discuss submodels but not this specific generated submodel construction for stability.
- Break condition: If the generated submodel does not preserve the necessary truth conditions, stability cannot be verified in the submodel.

## Foundational Learning

- Concept: Defeasible logic and argumentation frameworks
  - Why needed here: The paper builds on defeasible logic to define argumentation frameworks and uses them to reason about stable normative explanations.
  - Quick check question: What are the three types of rules in defeasible logic, and how do they differ in their use for argumentation?

- Concept: Neighborhood semantics for deontic logic
  - Why needed here: The paper maps argumentation frameworks to neighborhood models to characterize stable explanations semantically.
  - Quick check question: How does the neighborhood semantics for deontic logic differ from Kripke semantics, and why is it suitable for this application?

- Concept: Stable explanations and their persistence
  - Why needed here: The core contribution is defining and characterizing stable normative explanations in terms of persistence across all extensions.
  - Quick check question: What is the difference between a normative explanation and a stable normative explanation in the context of this paper?

## Architecture Onboarding

- Component map:
  Defeasible theory (F, R, >) -> Argumentation framework (A, ≫) -> Neighborhood D-model (W, N, v) -> Stable explanation check
  Key components: rule sets, superiority relation, attack relation, world generation, neighborhood construction, truth lemma, submodel generation.

- Critical path:
  Parse defeasible theory -> Build argumentation framework -> Construct canonical neighborhood model -> Generate submodel for explanation -> Check stability by verifying persistence across extensions.

- Design tradeoffs:
  Using neighborhood semantics allows for a direct mapping of obligations but requires careful construction of the model to ensure well-defined extensions.
  The generated submodel approach simplifies stability checking but may lose information about the full theory.

- Failure signatures:
  Cycles in the superiority relation or dependency graph lead to undefined extensions.
  Inconsistencies in the defeasible theory prevent well-defined neighborhood models.
  Failure to preserve the justification relation in the submodel leads to incorrect stability checks.

- First 3 experiments:
  1. Implement a parser for defeasible theories and generate the corresponding argumentation framework.
  2. Construct a simple neighborhood D-model for a small defeasible theory and verify the truth lemma.
  3. Implement the generated submodel construction and test it on a stable explanation to verify persistence across extensions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the notion of stable normative explanation be effectively applied to develop symbolic models for explainable AI in legal contexts?
- Basis in paper: [explicit] The paper mentions that the idea of stability can pave the way to develop symbolic models for XAI when applied to the law.
- Why unresolved: While the paper establishes the theoretical foundation for stable normative explanations, it does not provide concrete examples or methods for applying this concept to real-world XAI systems in legal domains.
- What evidence would resolve it: Empirical studies demonstrating the effectiveness of stable normative explanations in improving the transparency and interpretability of AI systems used in legal decision-making processes.

### Open Question 2
- Question: How can the revision operations (expansion, contraction, and revision) defined in the paper be effectively implemented and utilized in argumentation frameworks?
- Basis in paper: [explicit] The paper briefly mentions these revision operations in the context of stability but does not elaborate on their implementation or potential applications.
- Why unresolved: The paper does not provide a detailed framework or algorithms for implementing these revision operations within argumentation frameworks, leaving open questions about their practical feasibility and impact on argument stability.
- What evidence would resolve it: A comprehensive framework or algorithm that demonstrates how to perform these revision operations in argumentation frameworks, along with case studies showing their effects on argument stability and legal outcomes.

### Open Question 3
- Question: What are the computational implications of extending the stable normative explanation framework to more complex legal domains with a larger number of rules and facts?
- Basis in paper: [inferred] The paper establishes that determining the stability of an explanation is co-NP-complete and determining non-stability is NP-complete, suggesting potential computational challenges in more complex scenarios.
- Why unresolved: While the paper provides complexity results for the basic framework, it does not explore the computational scalability of the approach when applied to larger and more complex legal domains.
- What evidence would resolve it: Empirical studies comparing the computational performance of the stable normative explanation framework across different scales of legal domains, identifying bottlenecks and proposing optimizations for large-scale applications.

## Limitations
- The semantic characterization relies heavily on the absence of cycles in both the superiority relation and dependency graph.
- Computational complexity results (co-NP-complete for stability, NP-complete for non-stability) suggest practical scalability challenges.
- The paper does not address concrete performance bounds or optimization strategies for real-world applications.

## Confidence

**High confidence**: The mapping between stable explanations and persistence across extensions in neighbourhood models is well-supported by the abstract and section 3's formal definition. The truth lemma in section 4.2 provides solid theoretical grounding for the correspondence.

**Medium confidence**: The generated submodel approach for isolating explanations appears promising but requires careful verification that it preserves necessary truth conditions. The construction from argumentation frameworks to neighbourhood models is theoretically sound but the practical implementation details are not fully specified.

**Low confidence**: The practical scalability of the approach given the computational complexity results, and how the framework handles inconsistencies in defeasible theories, remain unclear from the abstract and section descriptions.

## Next Checks

1. **Cycle Detection Implementation**: Implement automated checks for cycles in the superiority relation and dependency graph of given argumentation frameworks, with clear error handling when cycles are detected.

2. **Model Construction Validation**: Build and verify the neighbourhood D-model construction for small, non-trivial examples (3-5 rules with dependencies), ensuring the S_r relations are correctly defined and capture all possible extensions.

3. **Stability Verification on Generated Submodels**: Test the generated submodel approach on a stable explanation case, systematically adding new facts and verifying that the explanation persists across all extensions, documenting any cases where the submodel fails to preserve necessary truth conditions.