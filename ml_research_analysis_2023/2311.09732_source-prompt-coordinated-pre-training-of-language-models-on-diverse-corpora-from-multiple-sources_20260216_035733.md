---
ver: rpa2
title: 'Source Prompt: Coordinated Pre-training of Language Models on Diverse Corpora
  from Multiple Sources'
arxiv_id: '2311.09732'
source_url: https://arxiv.org/abs/2311.09732
tags:
- pre-training
- source
- corpora
- data
- plms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies the challenge of pre-training language models
  on diverse corpora from multiple sources, which can lead to performance degradation
  due to heterogeneous data distributions. To address this, the authors propose source
  prompts (SP), which explicitly indicate the data source during pre-training and
  fine-tuning.
---

# Source Prompt: Coordinated Pre-training of Language Models on Diverse Corpora from Multiple Sources

## Quick Facts
- arXiv ID: 2311.09732
- Source URL: https://arxiv.org/abs/2311.09732
- Reference count: 12
- Primary result: Source prompts improve pre-training on diverse corpora, yielding 2-4% average score gains across BERT, T5, and OpenLLaMA-3b models

## Executive Summary
This paper addresses the challenge of pre-training language models on heterogeneous corpora from multiple sources, which can degrade performance due to inconsistent data distributions. The authors propose Source Prompts (SP), a method that explicitly indicates data source during pre-training and fine-tuning by prepending source tokens to input text. Experiments across three model architectures and multiple benchmarks demonstrate significant performance improvements compared to models without SP, validating the effectiveness of learning source-specific language styles.

## Method Summary
The approach involves prepending source indicator tokens to each input during pre-training, allowing models to learn source-specific language patterns. Masked Source Prediction (MSP) is introduced as an auxiliary objective where SP tokens are randomly masked, requiring the model to predict them from context. During fine-tuning, models can use either manual SP assignment (using known source labels) or auto SP assignment (letting the model predict the most relevant source for each sample). The method is designed to work across different model architectures including encoder-only (BERT), encoder-decoder (T5), and decoder-only models.

## Key Results
- SP significantly improves performance across diverse corpora, with average score gains of 2-4% on downstream tasks
- Models pre-trained with SP and MSP outperform those without SP across all tested architectures (BERT, T5, OpenLLaMA-3b)
- Auto SP assignment during fine-tuning is effective, allowing the model to dynamically select relevant sources for downstream samples
- SP is robust to different source naming policies and works with both domain-specific and general corpora

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Source prompts (SP) improve performance by enabling models to learn source-specific language styles.
- Mechanism: By prepending a source indicator token to each input, the model conditions its predictions on the origin of the text, learning style patterns unique to each corpus.
- Core assumption: The source token embeddings are distinct enough to capture source-specific linguistic features.
- Evidence anchors:
  - [abstract] "Results of extensive experiments demonstrate that PLMs pre-trained with SP on diverse corpora gain significant improvement in various downstream tasks."
  - [section] "Therefore, the PLMs can predict masked words using the SP’s assistance, enabling them to learn language styles dependent on varying sources."
- Break condition: If source tokens are not sufficiently discriminative or if corpora share too much linguistic overlap, the benefit of SP diminishes.

### Mechanism 2
- Claim: Masked Source Prediction (MSP) enhances source distinction by forcing the model to infer the source from context.
- Mechanism: During training, SP tokens are randomly masked, requiring the model to predict them from surrounding text, reinforcing source-aware representations.
- Core assumption: The context provides enough signal to disambiguate sources when SP tokens are masked.
- Evidence anchors:
  - [section] "In this process, the source prompts are randomly masked with a certain probability, requiring the PLMs to predict the masked source based on the contexts."
  - [section] "These validate the effectiveness of our MSP objective, and suggest that higher SP masking probability encourage the model to better distinguish texts from different sources and learn source related features."
- Break condition: If masking probability is too low to enforce learning or too high to disrupt MLM training, MSP becomes ineffective.

### Mechanism 3
- Claim: Auto SP assignment improves fine-tuning by dynamically selecting the most relevant source per sample.
- Mechanism: The model predicts the source for each downstream sample, and the predicted SP is prepended for fine-tuning and inference.
- Core assumption: The model has learned reliable source-specific features during pre-training that can generalize to downstream data.
- Evidence anchors:
  - [section] "We then ask PLMs to predict each sample’s text source, including both training and testing data."
  - [section] "This conclusion is further validated by rows C and D in Table 5 and Table 6."
- Break condition: If downstream data distribution diverges significantly from pre-training sources, source prediction may become unreliable.

## Foundational Learning

- Concept: Transformer architecture
  - Why needed here: SP is applied across encoder-only, encoder-decoder, and decoder-only models, requiring understanding of how each handles input tokens.
  - Quick check question: How does token position and attention differ between BERT (encoder-only) and GPT-style models (decoder-only)?

- Concept: Masked Language Modeling (MLM)
  - Why needed here: SP is integrated with MLM by masking both tokens and SP tokens during pre-training.
  - Quick check question: In BERT, what is the probability of masking a token, and how does that compare to the SP masking probability used in MSP?

- Concept: Causal Language Modeling (CLM)
  - Why needed here: Decoder-only models use next-token prediction, so SP must be positioned to aid or follow the text accordingly.
  - Quick check question: How does SP placement (prefix vs suffix) affect next-token prediction in decoder-only models?

## Architecture Onboarding

- Component map: Pre-training loop -> SP injection -> MSP masking -> Model embedding layer -> Fine-tuning loop -> Auto/manual SP assignment -> Downstream task evaluation
- Critical path: 1. Pre-training with SP and MSP 2. Source prediction for downstream samples 3. Fine-tuning with predicted/assigned SP 4. Inference with same SP strategy
- Design tradeoffs:
  - SP granularity: too specific (e.g., per article) vs too coarse (e.g., per corpus)
  - SP masking rate: balance between MSP learning and MLM stability
  - Auto SP vs manual SP: automation vs control
- Failure signatures:
  - No performance gain: SP tokens not distinctive enough
  - Degraded performance: SP disrupts token semantics or attention
  - Unstable training: SP masking too aggressive
- First 3 experiments:
  1. Baseline: pre-train without SP, fine-tune without SP
  2. Pre-train with SP, fine-tune without SP (test SP pre-training effect)
  3. Pre-train with SP and MSP (p=0.15), fine-tune with auto SP (test full pipeline)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does source prompt (SP) effectiveness scale with model size and training token count?
- Basis in paper: [explicit] The authors note that due to computational constraints, they used limited model scales and training tokens, and state "It remains to be studied whether our method works for the large-scale PLMs."
- Why unresolved: The experiments were conducted on relatively small models (BERT-base, T5-base, OpenLLaMA-3b) and limited training tokens, so the method's effectiveness on truly large-scale models hasn't been verified.
- What evidence would resolve it: Experiments demonstrating SP effectiveness on models with billions of parameters and training on datasets with 100B+ tokens, comparing performance with and without SP.

### Open Question 2
- Question: What is the optimal masking probability for masked source prediction (MSP) across different model architectures and corpus types?
- Basis in paper: [explicit] The authors tested three masking probabilities (0, 0.15, 0.3) and found P=0.3 slightly better than P=0.15, but note this was just one setting and the impact may vary.
- Why unresolved: The experiments only tested one model architecture (T5) on one corpus type (BBT-FinCorpus), so the optimal masking probability for other architectures and corpus types remains unknown.
- What evidence would resolve it: Systematic experiments varying masking probability across different model architectures (BERT, T5, decoder-only models) and corpus types (general vs domain-specific) to identify optimal settings for each combination.

### Open Question 3
- Question: How does source prompt performance vary with different degrees of corpus heterogeneity?
- Basis in paper: [inferred] The paper shows SP improves performance when pre-training on diverse corpora, but doesn't explore how the improvement varies with the degree of heterogeneity between sources or the similarity to downstream tasks.
- Why unresolved: The experiments used corpora with clear source distinctions but didn't systematically vary the similarity between sources or measure how SP performance changes as corpora become more or less heterogeneous.
- What evidence would resolve it: Experiments measuring SP performance across corpora with varying degrees of source similarity, from highly heterogeneous to nearly homogeneous, and correlating performance gains with source dissimilarity metrics.

## Limitations
- Experiments primarily conducted on Chinese corpora, limiting generalizability to other languages and domains
- Most results focus on T5 architecture, with limited comparative analysis across all three tested architectures
- Optimal SP masking probability appears empirically determined rather than theoretically grounded, suggesting potential sensitivity to hyperparameter choices

## Confidence
- High Confidence: Core claim that source prompts improve performance on heterogeneous corpora is well-supported by experimental results across multiple benchmarks and model architectures
- Medium Confidence: Improvements come from learning source-specific language styles rather than textual information in source names; auto SP assignment shows promise but is evaluated on limited downstream tasks
- Low Confidence: Claim that SP is "robust to different source naming policies" is based on experiments with three naming variations, which may not be sufficient to establish robustness across diverse naming conventions

## Next Checks
1. **Cross-lingual validation**: Test the SP approach on English and multilingual corpora to verify if the performance gains transfer across languages, particularly for the OpenLLaMA-3b model which was evaluated on Chinese corpora.

2. **Scaling analysis**: Conduct experiments varying the number of sources from 2 to 10+ to determine how SP performance scales with source diversity, and identify the point of diminishing returns or potential degradation.

3. **Domain transfer evaluation**: Assess how well auto SP assignment generalizes when fine-tuning on domains not seen during pre-training by creating controlled experiments where pre-training and fine-tuning domains have partial overlap.