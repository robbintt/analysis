---
ver: rpa2
title: Transformer Training Strategies for Forecasting Multiple Load Time Series
arxiv_id: '2306.10891'
source_url: https://arxiv.org/abs/2306.10891
tags:
- load
- time
- training
- forecasting
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a global Transformer training strategy for
  forecasting the load of multiple clients. Instead of training a separate local model
  for each client, or a single multivariate model that predicts all clients simultaneously,
  they train one global model on all the load data.
---

# Transformer Training Strategies for Forecasting Multiple Load Time Series

## Quick Facts
- **arXiv ID:** 2306.10891
- **Source URL:** https://arxiv.org/abs/2306.10891
- **Reference count:** 30
- **Primary result:** Global Transformer training reduces forecasting errors by 21.8% and 12.8% compared to local and multivariate strategies across 1-day to 1-month horizons.

## Executive Summary
This paper investigates three training strategies for forecasting multiple electrical load time series using Transformers: local (separate model per client), multivariate (single model for all series), and global (single model trained on all clients' data). The global strategy significantly outperforms the others by pooling training data across clients, leading to lower Mean Absolute Error (MAE) on two datasets. Experiments show the global Transformer also surpasses linear models, MLPs, and LSTMs, establishing Transformers as effective for load forecasting when trained with the right strategy.

## Method Summary
The authors propose a global Transformer training strategy for forecasting electrical load across multiple clients. Instead of training separate local models or a single multivariate model, they train one global model on all clients' time series data. The Transformer architecture uses an encoder-decoder structure with self-attention and cross-attention layers to capture temporal dependencies and generate multi-step forecasts. Inputs include load values and 9 time/calendar features. The model is trained with AdamW optimizer, MSE loss, batch size 128, learning rate 0.0001, and early stopping. Three strategies are compared: local (separate model per client), multivariate (single model predicting all series), and global (single model trained on pooled data).

## Key Results
- Global Transformer reduces forecasting errors by 21.8% and 12.8% compared to local and multivariate strategies
- Outperforms linear models, MLPs, and LSTMs across forecasting horizons from one day to one month
- Pooling training data across 321 clients (2.1 years each) provides substantial performance gains over local models with limited per-client data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Global training reduces forecasting error by pooling more training data across clients.
- **Mechanism:** A single Transformer model is trained on all clients' time series, so the effective training set size is multiplied by the number of clients. This increases the diversity of patterns the model can learn, leading to better generalization and lower MAE.
- **Core assumption:** The underlying load patterns across clients share enough similarity that a single model can generalize well across them.
- **Evidence anchors:** [abstract] "The global training strategy results in 21.8% and 12.8% lower forecasting errors than the two other strategies." [section] "Table 1: Training strategy details... Global: 321 * 2.1 years training data."
- **Break Condition:** If client load patterns are too heterogeneous, the global model may underfit specific clients, and local or clustered models would outperform it.

### Mechanism 2
- **Claim:** Multivariate modeling fails because the model must predict all clients simultaneously, increasing output complexity and overfitting.
- **Mechanism:** The multivariate strategy produces a high-dimensional output (h×321 values) and input (L×330), causing the model to struggle with both learning temporal patterns and inter-client relationships. The simpler univariate outputs in local and global strategies avoid this.
- **Core assumption:** The additional output complexity in multivariate modeling degrades performance more than any benefit from joint prediction.
- **Evidence anchors:** [section] "Table 2: MAE... Transformer (ours) multivariate 1 week 0.365 0.382 0.381" vs. "Transformer (ours) global 2 weeks 0.184 0.225 0.312"
- **Break Condition:** If strong inter-client correlations exist, multivariate modeling could outperform univariate approaches, but current data does not support this.

### Mechanism 3
- **Claim:** Local models underperform because each model is trained on limited data per client, leading to higher variance and less stable predictions.
- **Mechanism:** With only one time series per model, the amount of training data is small. The global model shares data across all clients, reducing variance and stabilizing the learned patterns.
- **Core assumption:** The per-client data volume is too small for effective local model training.
- **Evidence anchors:** [section] "Table 1: Training strategy details... Local: 2.1 years training data (per model) vs. Global: 321 * 2.1 years."
- **Break Condition:** If clients have very long, diverse histories, local models may match or exceed global performance.

## Foundational Learning

- **Concept:** Transformer encoder-decoder architecture
  - **Why needed here:** The paper uses a Transformer with self-attention and cross-attention layers to capture temporal dependencies and generate multi-step forecasts.
  - **Quick check question:** What is the role of the decoder's masked self-attention vs. cross-attention in this forecasting setup?

- **Concept:** Transfer learning through global training
  - **Why needed here:** Global training shares knowledge across clients, acting as a form of transfer learning without fine-tuning.
  - **Quick check question:** How does the amount of training data scale between local and global strategies in this paper?

- **Concept:** Multi-step direct forecasting
  - **Why needed here:** The model predicts h future steps in one pass rather than iterating, which is critical for efficiency and accuracy in long horizons.
  - **Quick check question:** What is the difference between direct and iterative multi-step forecasting, and why is direct chosen here?

## Architecture Onboarding

- **Component map:** Load values + 9 calendar features → Linear embedding (128-dim) → Encoder (3 layers, 8 heads) → Decoder (3 layers, 8 masked self-attn + 8 cross-attn) → Output Linear (h×1 or h×321)
- **Critical path:** Load values + 9 calendar features → Encoder → Decoder → Forecast. The decoder uses zero-loaded inputs to generate independent future steps.
- **Design tradeoffs:** 
  - Global vs. local: More data vs. overfitting risk
  - Direct multi-step vs. iterative: Efficiency vs. potential error accumulation
  - Univariate vs. multivariate: Simpler output vs. potential joint modeling benefits
- **Failure signatures:** 
  - High MAE on test set → Underfitting (too simple model or insufficient data)
  - Training divergence → Learning rate too high or batch size too small
  - Overfitting → Validation loss increases while training loss decreases
- **First 3 experiments:**
  1. Train a local Transformer with 1-week lookback; compare MAE to persistence baseline
  2. Train a global Transformer with 2-week lookback; measure improvement over local
  3. Swap direct forecasting for iterative forecasting; compare performance and training time

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How much training data is needed for the global Transformer model to surpass local models?
- **Basis in paper:** [explicit] The authors hypothesize that the global model's superiority is due to larger training data but suggest future experiments with varying data amounts.
- **Why unresolved:** The paper does not experimentally vary the amount of training data to determine the threshold where the global model outperforms local models.
- **What evidence would resolve it:** Experiments comparing local vs. global Transformer performance across datasets with different amounts of training data per client.

### Open Question 2
- **Question:** Does incorporating weather data improve forecasting performance, and how does it affect the relative performance of different models?
- **Basis in paper:** [inferred] The authors mention that weather data could improve forecasts due to dependencies like electrical heating, but do not test this.
- **Why unresolved:** The experiments did not include weather data as an input feature, leaving its impact on performance and model comparison unknown.
- **What evidence would resolve it:** Experiments training and evaluating the same models with and without weather data to measure performance changes and shifts in relative model rankings.

### Open Question 3
- **Question:** Would pre-training on other tasks or datasets, followed by fine-tuning, improve the global Transformer's performance?
- **Basis in paper:** [explicit] The authors mention that pre-training and fine-tuning could be tested as part of transfer learning, but do not conduct these experiments.
- **Why unresolved:** The paper only explores training a global model directly on the target data, not leveraging transfer learning from other domains or tasks.
- **What evidence would resolve it:** Experiments comparing the performance of a global model trained from scratch versus one pre-trained on related tasks (e.g., other building load data) and fine-tuned on the target dataset.

## Limitations
- Does not explore the heterogeneity of client load patterns, which could limit global model effectiveness in real-world deployments
- Uses fixed lookback windows that may not be optimal for all clients
- Does not address computational costs or scalability for much larger numbers of clients

## Confidence

- **High confidence** in the empirical finding that global training reduces forecasting errors compared to local and multivariate strategies on the tested datasets
- **Medium confidence** in the explanation that data pooling is the primary driver of improved accuracy, as the paper does not provide direct evidence isolating this mechanism
- **Low confidence** in the claim that multivariate modeling is inherently inferior, as the paper does not explore alternative architectures or feature engineering to mitigate its complexity

## Next Checks

1. **Test model robustness:** Evaluate the global Transformer on a dataset with highly heterogeneous client load patterns to determine if performance degrades
2. **Compare adaptive vs. fixed lookback:** Implement and test an adaptive lookback window strategy to assess its impact on forecasting accuracy
3. **Explore multivariate alternatives:** Experiment with dimensionality reduction or hierarchical modeling to reduce the complexity of multivariate forecasting and compare results to the global strategy