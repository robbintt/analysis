---
ver: rpa2
title: 'GBT: Two-stage transformer framework for non-stationary time series forecasting'
arxiv_id: '2307.08302'
source_url: https://arxiv.org/abs/2307.08302
tags:
- forecasting
- time
- series
- prediction
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GBT addresses the over-fitting problem in time series forecasting
  Transformers (TSFTs) caused by zero-initialization of decoder inputs, particularly
  when handling non-stationary time series. It proposes a two-stage Transformer framework
  with "Good Beginning" that decouples the prediction process into Auto-Regression
  and Self-Regression stages.
---

# GBT: Two-stage transformer framework for non-stationary time series forecasting

## Quick Facts
- arXiv ID: 2307.08302
- Source URL: https://arxiv.org/abs/2307.08302
- Reference count: 5
- Primary result: Two-stage Transformer framework outperforms state-of-the-art TSFTs on seven benchmark datasets with reduced time and space complexity

## Executive Summary
GBT addresses over-fitting in time series forecasting Transformers (TSFTs) caused by zero-initialization of decoder inputs, particularly for non-stationary time series. The framework decouples the prediction process into Auto-Regression and Self-Regression stages, where the first stage generates a "Good Beginning" initialization for the second stage. An Error Score Modification module enhances forecasting capability by dynamically adjusting attention scores. Extensive experiments demonstrate superior performance compared to state-of-the-art models while maintaining lower computational complexity.

## Method Summary
GBT proposes a two-stage Transformer framework that addresses over-fitting in TSFTs through decoupled prediction stages. The Auto-Regression stage generates predictions using encoders and a ConvBlock replacement for FFN layers, producing a "Good Beginning" initialization. The Self-Regression stage then refines these predictions using masked self-attention decoders with an Error Score Modification (ESM) module. ESM injects learnable Gaussian noise into attention scores to bias attention toward earlier, more reliable predictions. The architecture removes redundant components like start tokens and cross-attention to reduce computation. Training occurs in two phases: first training the Auto-Regression stage, then freezing its parameters while training the Self-Regression stage with ESM active.

## Key Results
- GBT outperforms state-of-the-art TSFTs and other forecasting models on seven benchmark datasets
- Model achieves lower time and space complexity compared to existing TSFTs
- Only requires canonical attention and convolution mechanisms, avoiding complex operations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage decoupling addresses the distribution shift between input and prediction windows by using Auto-Regression to generate a better initialization for the Self-Regression stage.
- Mechanism: The first stage (Auto-Regression) predicts a partial sequence that has statistical properties closer to the ground truth. This "Good Beginning" is then used as input for the second stage (Self-Regression), which only needs to refine the predictions internally, avoiding the problematic zero-initialization.
- Core assumption: The prediction results from the first stage will have more similar statistical properties to the ground truth than zero vectors, especially under non-stationary conditions.
- Evidence anchors:
  - [abstract]: "Prediction results of Auto-Regression stage serve as a Good Beginning, i.e., a better initialization for inputs of Self-Regression stage."
  - [section]: "Prediction results of Auto-Regression stage are trained with the truth of prediction windows, so they will have more similar statistical properties with those of truth compared with former initialization methods."
- Break condition: If the Auto-Regression stage fails to produce predictions that resemble the true distribution, the benefit of the Good Beginning is lost.

### Mechanism 2
- Claim: The Error Score Modification (ESM) module dynamically adjusts attention scores to compensate for error accumulation in later prediction steps.
- Mechanism: ESM injects learnable Gaussian noise centered at zero into the query-key matching matrix. This biases attention towards earlier, more reliable predictions, helping to mitigate error propagation in later steps.
- Core assumption: Earlier predictions are more reliable and should contribute more to the inference of later steps.
- Evidence anchors:
  - [abstract]: "We also propose Error Score Modification module to further enhance the forecasting capability of the Self-Regression stage in GBT."
  - [section]: "more masked self-attention scores will be allocated to earlier and more trusted prediction elements."
- Break condition: If the learnable parameters of ESM fail to adapt to the dataset's characteristics, the module may not effectively reduce error accumulation.

### Mechanism 3
- Claim: Removing redundant components (start token, cross-attention) in the Self-Regression stage reduces computation and avoids interference with the Good Beginning.
- Mechanism: By eliminating start token and cross-attention, the second stage focuses purely on refining the internal structure of the prediction sequence, relying entirely on the initialization provided by the first stage.
- Core assumption: The Good Beginning from the first stage is sufficient, and the second stage does not need additional context from input sequences or artificial initialization.
- Evidence anchors:
  - [abstract]: "it employs decoders which are only composed of masked self-attention modules and abandon modules including start token and cross-attention to reduce reduplicate computations."
  - [section]: "Cross-attention module in decoder are Auto-Regression processes which aim to seek connections between input and prediction windows. Meanwhile, masked self-attention module in decoder belongs to Self-Regression process."
- Break condition: If the Good Beginning is insufficiently informative, removing cross-attention may degrade performance.

## Foundational Learning

- Concept: Z-score standardization and its assumptions
  - Why needed here: Understanding why zero-initialization works for stationary series but fails for non-stationary ones.
  - Quick check question: If a time series has a moving average that changes over time, will Z-score standardization make its subsequences 0-mean?

- Concept: Transformer decoder input initialization strategies
  - Why needed here: To grasp why common methods like start token or trend decomposition are insufficient for non-stationary data.
  - Quick check question: In a Transformer decoder, if all input tokens are zero, what will the initial attention scores between them be?

- Concept: Error propagation in autoregressive models
  - Why needed here: To understand why later predictions in a sequence tend to drift from the truth and how ESM addresses this.
  - Quick check question: If each step in an autoregressive model has a small bias, will the cumulative error grow or shrink over many steps?

## Architecture Onboarding

- Component map:
  - Auto-Regression stage: Encoders + ConvBlock + FC layer → produces Good Beginning
  - Self-Regression stage: Masked self-attention decoders + ESM + linear projection → refines predictions
  - ESM: Learnable Gaussian noise added to attention scores
  - ConvBlock: Replaces FFN to reduce sequence length and increase locality

- Critical path:
  1. Train Auto-Regression stage on input sequences to predict target windows.
  2. Freeze Auto-Regression parameters.
  3. Train Self-Regression stage using Good Beginning as input, with ESM active.
  4. Combine outputs from both stages for final forecast.

- Design tradeoffs:
  - Two-stage adds complexity but solves initialization and error accumulation.
  - ESM adds parameters but improves accuracy; removing it hurts performance.
  - ConvBlock reduces sequence length, lowering complexity but potentially losing global context.

- Failure signatures:
  - Auto-Regression stage predictions look flat or random → Good Beginning is poor.
  - Self-Regression stage ignores Good Beginning → ESM not effective or parameters not learned.
  - Training diverges in second stage → forgetting or instability in ESM parameters.

- First 3 experiments:
  1. Train only Auto-Regression stage; evaluate MSE on prediction windows to confirm it produces a usable Good Beginning.
  2. Train GBT with ESM disabled; compare MSE to full GBT to quantify ESM contribution.
  3. Swap ConvBlock for standard FFN in Auto-Regression; measure impact on both accuracy and inference time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which the Error Score Modification (ESM) module improves forecasting accuracy in the Self-Regression stage of GBT?
- Basis in paper: [explicit] The paper mentions that ESM employs a group of learnable Gaussian distributions added to the masked query-key matching matrix in the second stage to guide the model to allocate more attention scores to earlier and more trusted prediction elements.
- Why unresolved: While the paper explains the concept of ESM, it does not provide a detailed analysis of how this mechanism specifically contributes to improved forecasting accuracy. Further investigation is needed to understand the impact of ESM on the model's performance.
- What evidence would resolve it: Conducting ablation studies where ESM is removed or modified, and comparing the forecasting accuracy of GBT with and without ESM, would provide evidence of its effectiveness.

### Open Question 2
- Question: How does the two-stage architecture of GBT compare to other multi-stage forecasting methods, such as those based on representation learning or reconstruction strategies?
- Basis in paper: [explicit] The paper briefly mentions that some self-supervised time series forecasting methods and models with reconstruction strategies use two-stage architectures, but it does not provide a detailed comparison between GBT and these methods.
- Why unresolved: The paper does not explore the similarities and differences between GBT and other two-stage forecasting methods, making it unclear how GBT's approach differs and whether it offers any advantages over existing methods.
- What evidence would resolve it: Conducting a comprehensive comparison between GBT and other two-stage forecasting methods, including experiments on the same datasets and evaluation of their forecasting accuracy, would provide insights into the strengths and weaknesses of GBT's architecture.

### Open Question 3
- Question: What is the impact of the pyramid architecture on the performance of GBT, and how does it contribute to the model's ability to handle non-stationary time series?
- Basis in paper: [explicit] The paper mentions that GBT employs a pyramid architecture similar to Informer in the Auto-Regression stage to extract hierarchical feature maps. However, it does not provide a detailed analysis of how this architecture specifically contributes to the model's performance.
- Why unresolved: While the paper suggests that the pyramid architecture enhances the locality of attention modules and smooths turbulence, it does not explore the impact of this architecture on the model's ability to handle non-stationary time series or its contribution to overall forecasting accuracy.
- What evidence would resolve it: Conducting ablation studies where the pyramid architecture is removed or modified, and comparing the forecasting accuracy of GBT with and without the pyramid architecture, would provide evidence of its effectiveness in handling non-stationary time series and improving overall performance.

## Limitations
- Heavy reliance on the assumption that Auto-Regression stage consistently produces "Good Beginnings" with statistical properties similar to ground truth
- ESM module introduces additional learnable parameters whose effectiveness depends on proper training dynamics
- ConvBlock replacement for FFN layers may limit the model's ability to capture long-range dependencies

## Confidence
- High confidence: Core observation that zero-initialization causes over-fitting in non-stationary time series
- Medium confidence: Proposed two-stage solution's general effectiveness across diverse datasets
- Medium confidence: Specific implementation details of ESM and ConvBlock components

## Next Checks
1. Perform controlled experiments with stationary vs. non-stationary synthetic data to isolate the impact of zero-initialization and verify the Good Beginning hypothesis.
2. Implement ablation studies removing ESM and ConvBlock components to quantify their individual contributions to performance improvements.
3. Test GBT on datasets with varying degrees of non-stationarity to establish the threshold at which the two-stage approach becomes beneficial compared to single-stage Transformers.