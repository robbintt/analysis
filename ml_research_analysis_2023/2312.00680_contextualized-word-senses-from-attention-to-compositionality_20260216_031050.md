---
ver: rpa2
title: 'Contextualized word senses: from attention to compositionality'
arxiv_id: '2312.00680'
source_url: https://arxiv.org/abs/2312.00680
tags:
- attention
- words
- compositional
- meaning
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a transparent, interpretable approach to contextualized
  word sense modeling using dependency-based semantic compositionality. Unlike black-box
  neural architectures such as Transformers, which rely on attention mechanisms without
  explicit compositional rules, this method uses syntactic dependencies and selection
  preferences to dynamically build contextualized senses.
---

# Contextualized word senses: from attention to compositionality

## Quick Facts
- arXiv ID: 2312.00680
- Source URL: https://arxiv.org/abs/2312.00680
- Reference count: 13
- Dependency-based compositional model (DepFunc) achieves Spearman correlations of 42-57 (average 50) on sentence similarity tasks, outperforming BERT's 47

## Executive Summary
This paper proposes a transparent, interpretable approach to contextualized word sense modeling using dependency-based semantic compositionality. Unlike black-box neural architectures such as Transformers, which rely on attention mechanisms without explicit compositional rules, this method uses syntactic dependencies and selection preferences to dynamically build contextualized senses. Selectional preferences are computed from paradigmatic classes—sets of semantically related words—and combined with static word embeddings to produce contextualized vectors. Experiments on sentence similarity tasks in four languages (English, Portuguese, Spanish, Galician) show that the compositional dependency-based model (DepFunc) achieves competitive results with BERT, reaching Spearman correlations between 42-57, with an average of 50 versus BERT's 47. The difference is statistically significant (p ≤ 0.005), demonstrating that linguistically motivated models can rival complex neural architectures while offering greater interpretability.

## Method Summary
The paper proposes a dependency-based compositional approach to contextualized word sense modeling that explicitly builds contextualized vectors through syntactic dependencies and selectional preferences. The method starts with static word embeddings, then uses dependency parsing to identify relationships between words. For each dependency relation, paradigmatic classes (sets of semantically related words that co-occur in that relation) are computed from parsed corpora. These selectional preferences are combined with static embeddings through component-wise multiplication/addition to produce contextualized vectors. The process is applied incrementally up the dependency tree, contextualizing each word based on its syntactic partners. The root vector of the tree serves as the sentence representation for similarity tasks.

## Key Results
- DepFunc achieves Spearman correlations between 42-57 on sentence similarity tasks across four languages
- Average performance of 50 vs BERT's 47, with statistical significance (p ≤ 0.005)
- The dependency-based compositional approach rivals complex neural architectures while providing greater interpretability
- Performance differences are statistically significant but relatively narrow, suggesting competitive viability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextualized word senses can be built through explicit dependency-based composition rather than implicit attention patterns.
- Mechanism: Syntactic dependencies define how word vectors are combined with paradigmatic class vectors to generate contextualized senses.
- Core assumption: Semantic compositionality follows from syntactic dependencies and selection preferences.
- Evidence anchors:
  - [abstract] The paper proposes a transparent, interpretable approach using dependency-based semantic compositionality.
  - [section] Section 3.1 defines selection preferences in dependency trees and how paradigmatic classes restrict meaning.
  - [corpus] Neighbor paper "Graph-based Clustering for Detecting Semantic Change Across Time and Languages" supports dependency-based clustering methods.
- Break condition: If paradigmatic class construction fails (e.g., sparse parsed corpora), contextualized senses degrade sharply.

### Mechanism 2
- Claim: Selectional preferences act as dynamic filters that disambiguate polysemous words based on context.
- Mechanism: The paradigmatic class of frequent co-occurring words in a dependency relation forms a selection vector that multiplies with the target word's static vector.
- Core assumption: The paradigmatic class adequately represents the selectional constraints for a given dependency relation.
- Evidence anchors:
  - [abstract] Selectional preferences are computed from paradigmatic classes and combined with static embeddings.
  - [section] Figure 2 shows how "catch" and "ball" disambiguate each other via paradigmatic classes in the obj dependency.
  - [corpus] Weak: No direct corpus evidence for paradigmatic class selection in neighbor works.
- Break condition: If paradigmatic classes are too sparse or noisy, selection preferences fail to disambiguate.

### Mechanism 3
- Claim: A compositional dependency tree can incrementally build contextualized vectors for all words, not just functional heads.
- Mechanism: Starting from leaf dependencies, each word's sense is contextualized by combining with the paradigmatic class of its syntactic partner; this repeats up the tree.
- Core assumption: Co-compositional hypothesis holds—both words in a dependency influence each other's sense.
- Evidence anchors:
  - [abstract] Dependency relations and selection preferences are used to build contextualized vectors incrementally.
  - [section] Section 3.2 describes the incremental interpretation of dependency trees for contextualization.
  - [corpus] Neighbor "ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding" supports context-aware vectorization but not dependency-specific composition.
- Break condition: In highly ambiguous or long-range dependencies, incremental composition may lose signal.

## Foundational Learning

- Concept: Syntactic dependencies (e.g., subj, obj, prep)
  - Why needed here: They define the relational structure over which selectional preferences are applied.
  - Quick check question: What dependency relation links "dog" to "bark" in "The dog barks loudly"?
- Concept: Paradigmatic classes
  - Why needed here: They represent the selectional preferences—sets of semantically related words in the same syntactic role.
  - Quick check question: If "catch" appears with {ball, fish, thief} as direct objects, what is the paradigmatic class for obj of "catch"?
- Concept: Static vs contextualized embeddings
  - Why needed here: Static embeddings provide the base sense; contextualized ones refine it via composition.
  - Quick check question: In "bank" (financial) vs "bank" (river), which part of the vector changes during contextualization?

## Architecture Onboarding

- Component map: Parsed sentence -> Static embedding layer -> Paradigmatic class builder -> Composition engine -> Contextualized vector output
- Critical path: Parse → Static embedding → Selection preference lookup → Composition → Output
- Design tradeoffs:
  - Memory vs accuracy: Storing all paradigmatic classes is heavy; pruning to top-K balances both.
  - Dependency granularity: Fine-grained UD vs coarse PTB affects paradigmatic class quality.
  - Static vs contextualized space alignment: Must keep POS-specific spaces compatible.
- Failure signatures:
  - Missing paradigmatic class → Zero or random contextualized vector.
  - Parse errors → Wrong dependency arcs → Incorrect composition.
  - Corpus sparsity → Poor selection preferences → Overfitting to few examples.
- First 3 experiments:
  1. Parse a fixed sentence, show static embeddings per word.
  2. Manually construct paradigmatic class for one dependency, compute contextualized vector.
  3. Compare similarity of root vectors for paraphrases using DepFunc vs static baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do global discourse-level meanings (e.g., knowledge about situations and circumstances) interact with the local/lexical meaning produced by the dependency-based compositional approach?
- Basis in paper: [explicit] The authors note that their approach and the attention mechanism "only address the local lexical meaning, leaving out the global one referring to knowledge about situations, circumstances and state of affairs" and propose this as a crucial challenge to address.
- Why unresolved: The paper focuses on modeling contextualized senses of words through syntactic dependencies and selectional preferences, but does not address how to represent or integrate global discourse-level meaning into this framework.
- What evidence would resolve it: A proposed solution for representing global meaning that can interact with the local/lexical meaning produced by the dependency-based compositional approach, along with experimental validation showing improved performance on tasks requiring both local and global semantic understanding.

### Open Question 2
- Question: How can the dependency-based compositional model be extended to handle non-compositional or weakly compositional expressions like idioms, compounds, and collocations?
- Basis in paper: [explicit] The authors identify this as a future challenge, stating "we need other non-compositional or weakly compositional semantic mechanisms, other than that based on selection preferences, to account for this type of expressions."
- Why unresolved: The current model relies on selectional preferences and syntactic dependencies, which may not adequately capture the semantics of expressions where the meaning is not predictable from the constituent words or their syntactic relationships.
- What evidence would resolve it: A modified or extended compositional model that can handle both compositional and non-compositional expressions, validated through improved performance on benchmark datasets containing such expressions.

### Open Question 3
- Question: Can explicit linguistic information be effectively integrated into the attention mechanism of Transformers to make them more transparent and interpretable?
- Basis in paper: [explicit] The authors propose exploring "the possibility of elaborating neuro-symbolic strategies by integrating explicit linguistic information into the attention mechanism of Transformers" as a way to make these models more interpretable.
- Why unresolved: While the authors demonstrate the potential of linguistically motivated models, they do not explore how to combine these insights with the powerful pattern recognition capabilities of neural architectures like Transformers.
- What evidence would resolve it: A proposed method for incorporating linguistic knowledge into Transformer architectures, along with experimental results showing improved interpretability without sacrificing performance on relevant NLP tasks.

## Limitations

- Dependency on high-quality syntactic parses and sufficiently large paradigmatic class corpora
- Narrow comparison with BERT without detailed hyperparameter specifications
- Limited testing across only four languages with similar but not identical datasets
- Potential issues with static embedding space alignment across POS tags

## Confidence

- **High confidence** in the claim that dependency-based composition can produce contextualized senses, supported by direct evidence in Section 3.1-3.2.
- **Medium confidence** in the claim that DepFunc rivals BERT's performance, given statistical significance but narrow margin (50 vs 47 Spearman).
- **Low confidence** in the scalability claim across languages, as only four languages were tested with similar but not identical datasets.

## Next Checks

1. **Corpus sparsity test**: Intentionally remove paradigmatic class members from a held-out subset of the corpus and measure DepFunc's performance degradation to quantify robustness to sparse selection preferences.

2. **Parse error simulation**: Inject controlled syntactic parse errors (e.g., swap subj/obj dependencies) and measure how contextualized vector quality degrades, establishing sensitivity to parsing accuracy.

3. **Cross-language alignment verification**: Train DepFunc on parallel corpora across two languages and measure whether contextualized vectors preserve semantic similarity across language pairs, testing the universality of the compositionality approach.