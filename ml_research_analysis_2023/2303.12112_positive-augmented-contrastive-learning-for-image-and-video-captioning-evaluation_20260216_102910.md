---
ver: rpa2
title: Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation
arxiv_id: '2303.12112'
source_url: https://arxiv.org/abs/2303.12112
tags:
- image
- pac-s
- clip-s
- captioning
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PAC-S, a new image and video captioning evaluation
  metric based on positive-augmented contrastive learning. The method addresses the
  problem of caption evaluation by training a shared embedding space that unifies
  real and synthetically generated image-text pairs from a cleaned dataset, using
  generated images and captions as additional positive samples in a contrastive learning
  framework.
---

# Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation

## Quick Facts
- **arXiv ID**: 2303.12112
- **Source URL**: https://arxiv.org/abs/2303.12112
- **Reference count**: 40
- **Primary result**: PAC-S achieves higher correlation with human judgment than CIDEr, SPICE, and CLIP-Score on multiple image and video datasets

## Executive Summary
This paper introduces PAC-S, a new image and video captioning evaluation metric based on positive-augmented contrastive learning. The method addresses caption evaluation by training a shared embedding space that unifies real and synthetically generated image-text pairs from a cleaned dataset, using generated images and captions as additional positive samples in a contrastive learning framework. The authors show that PAC-S achieves higher correlation with human judgment than existing reference-based metrics like CIDEr and SPICE, as well as reference-free metrics like CLIP-Score, on multiple image and video datasets. The approach also demonstrates superior sensitivity to object hallucinations and improved system-level correlation with human preferences.

## Method Summary
PAC-S is built on the CLIP architecture, starting with pre-trained CLIP ViT-B/32 visual and textual encoders and fine-tuning only the projection heads. The training procedure uses positive-augmented contrastive learning with three cross-modal pairs: real image-text pairs, generated images with real text, and real images with generated text. Synthetic data is generated using Stable Diffusion for images (ViT-L/14 model fine-tuned on LAION-Aesthetics) and BLIP for captions (ViT-L/14). The model is trained on COCO dataset with the three-term InfoNCE loss, using temperature parameter τ = 0.07, λv = 0.05, λt = 0.1, for 1,500 iterations until validation loss plateaus. The evaluation score is computed as Score(t,v) = 21 × max(cos(t,v), 0), with a reference-based version using harmonic mean with max similarity to reference captions.

## Key Results
- PAC-S achieves higher Kendall's tau-b correlation with human judgment than CIDEr, SPICE, and CLIP-Score on Flickr8k-Expert and Flickr8k-CF datasets
- The method shows improved sensitivity to object hallucinations, outperforming CLIP-Score on FOIL and ActivityNet-FOIL datasets
- PAC-S demonstrates superior system-level correlation with human preferences across all tested scenarios, including video captioning datasets

## Why This Works (Mechanism)

### Mechanism 1
PAC-S improves correlation with human judgment by incorporating synthetic positive pairs during training. By generating synthetic images and captions from real pairs, the model receives additional contrastive supervision signals that regularize the embedding space, reducing the influence of noisy web-scale data and improving alignment with human preferences. This works under the assumption that synthetic image-caption pairs generated by diffusion models and BLIP maintain semantic alignment with their real counterparts, thus acting as valid positive samples.

### Mechanism 2
PAC-S achieves better sensitivity to object hallucinations by learning fine-grained cross-modal matching. The model learns to match individual word embeddings to video frame embeddings, enabling detection of object-level mismatches when captions contain hallucinated content. This fine-grained alignment allows the metric to detect when captions mention objects not present in the corresponding visual content.

### Mechanism 3
PAC-S outperforms CLIP-Score by leveraging curated data instead of relying solely on web-scale noisy data. Training on cleaned COCO data with additional synthetic pairs provides better visual-textual alignment than CLIP-Score's web-scale pre-training alone. The curated data provides cleaner supervision than alt-text web data, and synthetic pairs can compensate for limited training size while maintaining high quality.

## Foundational Learning

- **Concept: Contrastive learning and InfoNCE loss**
  - Why needed here: PAC-S is built on contrastive learning framework to align visual and textual embeddings
  - Quick check question: What is the difference between symmetric and asymmetric InfoNCE loss in multimodal contrastive learning?

- **Concept: Cross-modal embedding spaces**
  - Why needed here: PAC-S requires learning a shared embedding space where images and text can be compared via cosine similarity
  - Quick check question: How does the choice of backbone architecture (ViT vs ResNet) affect cross-modal embedding quality?

- **Concept: Synthetic data generation for augmentation**
  - Why needed here: PAC-S uses generated images and captions as additional positive samples to regularize training
  - Quick check question: What are the key failure modes when using synthetic data as positive samples in contrastive learning?

## Architecture Onboarding

- **Component map**: CLIP ViT-B/32 visual encoder -> projection head -> cosine similarity -> InfoNCE loss; CLIP textual encoder -> projection head -> cosine similarity -> InfoNCE loss; Synthetic data generators (Stable Diffusion, BLIP) -> generated samples -> three-term loss

- **Critical path**: 1) Load real image-caption pairs from COCO; 2) Generate synthetic images from captions and synthetic captions from images; 3) Compute pairwise cosine similarities between all modalities; 4) Apply symmetric InfoNCE loss across three cross-modal combinations; 5) Update projection heads only (freeze CLIP encoders)

- **Design tradeoffs**: Using projection heads only vs fine-tuning full CLIP encoders (faster training but potentially less adaptation); Synthetic data quality vs quantity (higher quality reduces noise but may limit augmentation diversity); Symmetric vs asymmetric loss (symmetric provides balanced supervision but increases computational cost)

- **Failure signatures**: Poor hallucination detection indicates weak fine-grained word-frame alignment; Low correlation with human judgment suggests synthetic data quality issues or insufficient curated data; Mode collapse in generated data indicates generator instability affecting positive sample quality

- **First 3 experiments**: 1) Compare PAC-S vs CLIP-Score correlation on Flickr8k-Expert with varying synthetic data ratios; 2) Test hallucination detection accuracy on FOIL dataset with different word-frame matching granularities; 3) Evaluate impact of using different CLIP backbones (ViT-B/16, ViT-L/14) on metric performance

## Open Questions the Paper Calls Out

### Open Question 1
How does PAC-S perform when evaluated on datasets with significantly different image distributions than COCO, such as medical imaging or satellite imagery? The paper demonstrates PAC-S's effectiveness on various datasets but primarily uses COCO for training and evaluation, suggesting potential generalization concerns to other domains.

### Open Question 2
What is the impact of using different text generation models (besides BLIP) on the quality of PAC-S, and how does it affect the sensitivity to object hallucinations? The paper uses BLIP for generating text descriptions but acknowledges that other text generation models could be employed.

### Open Question 3
How does PAC-S handle captions that describe abstract concepts or emotions, which are not directly observable in the image? The paper focuses on evaluating the correspondence between captions and visual content, implying a potential limitation in assessing abstract or emotional descriptions.

## Limitations

- **Synthetic data quality dependency**: The approach heavily relies on generated image-caption pairs maintaining semantic alignment with real pairs. If Stable Diffusion or BLIP models hallucinate or create mismatched content, these synthetic pairs could degrade the embedding space rather than improve it.

- **Dataset bias concerns**: Training on COCO data (focused on common objects and scenes) and evaluating on similar datasets may overstate the metric's general applicability. The performance gains over CLIP-Score might reflect domain adaptation to COCO-style content rather than fundamental improvements in cross-modal alignment.

- **Limited evaluation scope**: While the paper shows improvements on multiple datasets, it primarily evaluates on captioning-specific datasets. The method's effectiveness for broader vision-language tasks remains unclear.

## Confidence

- **High confidence**: Claims about PAC-S achieving higher correlation with human judgment than reference-based metrics (CIDEr, SPICE) and reference-free metrics (CLIP-Score) on tested datasets
- **Medium confidence**: Claims about improved sensitivity to object hallucinations based on controlled foil-correct datasets
- **Low confidence**: Claims about PAC-S outperforming previous metrics "across all tested scenarios" based on limited direct comparison in related work

## Next Checks

1. **Synthetic data quality audit**: Conduct a systematic evaluation of the generated image-caption pairs to measure hallucination rates, semantic drift, and distribution mismatch compared to real data

2. **Cross-dataset generalization test**: Evaluate PAC-S on non-captioning vision-language tasks (visual question answering, image retrieval) to determine if the metric's improvements generalize beyond the captioning domain

3. **Ablation study on synthetic augmentation**: Compare PAC-S performance with and without synthetic positive pairs, and with different ratios of synthetic to real data, to quantify the actual contribution of the positive-augmented learning approach