---
ver: rpa2
title: 'Evaluating Large Language Models on Graphs: Performance Insights and Comparative
  Analysis'
arxiv_id: '2308.11224'
source_url: https://arxiv.org/abs/2308.11224
tags:
- graph
- llms
- node
- prompting
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates large language models on graph data comprehension
  and reasoning. It uses 50 randomly generated graphs with 20-50 nodes and 50-500
  edges to test four models: GPT-3.5-turbo, GPT-4, CalderaAI/30B-Lazarus, and TheBloke/Wizard-Vicuna-13B.'
---

# Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis

## Quick Facts
- arXiv ID: 2308.11224
- Source URL: https://arxiv.org/abs/2308.11224
- Reference count: 37
- Primary result: GPT models significantly outperform open-source models on graph reasoning tasks, with GPT-4 achieving 91% accuracy on simple path tasks versus 4% for open-source models

## Executive Summary
This paper evaluates four large language models on graph data comprehension and reasoning tasks using 50 randomly generated graphs with 20-50 nodes and 50-500 edges. The evaluation tests GPT-3.5-turbo, GPT-4, and two open-source models across four metrics: Comprehension, Correctness, Fidelity, and Rectification. GPT models demonstrate superior performance in structural reasoning and accuracy, particularly on simple path and shortest path tasks. However, the study reveals significant limitations including high rates of erroneous multi-answer responses from GPT models and poor self-correction capabilities, especially when the models exhibit high confidence in incorrect answers.

## Method Summary
The study generates 50 random graphs (20-50 nodes, 50-500 edges) and evaluates four LLMs using three prompting techniques: zero-shot, zero-shot chain-of-thought, and few-shot prompting. The models are tested on five graph tasks: connectivity (finding simple paths), node neighbor classification (1-hop, 2-hop, 3-hop), node degree determination, pattern matching (wedges and triangles), and finding all shortest paths. Evaluation metrics include Comprehension (understanding graph structure), Correctness (answer accuracy), Fidelity (confidence and trustworthiness of multi-answer responses), and Rectification (ability to correct errors). The methodology uses natural language representations of graphs through node lists and edge lists, with the code available at https://github.com/Ayame1006/LLMtoGraph.

## Key Results
- GPT models achieved 72% and 91% accuracy on simple path tasks versus only 4% for open-source models
- GPT-3.5-turbo produced 7.57 times more wrong than correct answers on shortest path tasks due to spurious multi-answer responses
- GPT-4 demonstrated limited error rectification ability, discarding 26% of correct outputs while attempting to correct errors
- Chain-of-thought prompting showed inconsistent benefits and sometimes increased hallucination in model responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT models can effectively comprehend graph data presented as natural language text
- Mechanism: The models leverage their strong language understanding capabilities to parse node and edge lists and reconstruct the underlying graph topology
- Core assumption: Graph structure information is sufficiently encoded in sequential text representation
- Evidence anchors:
  - [abstract] "LLMs effectively comprehend graph data in natural language and reason with graph topology"
  - [section] "GPT models shows proficiency in structural reasoning over the provided edges"
- Break condition: If graph becomes too large for context window or if graph encoding loses critical structural information

### Mechanism 2
- Claim: Chain-of-thought prompting can improve reasoning but may also increase hallucination
- Mechanism: By forcing models to articulate reasoning steps, CoT helps them organize their approach to graph problems, but may also lead them down incorrect reasoning paths
- Core assumption: Verbalizing reasoning improves logical coherence but doesn't guarantee correctness
- Evidence anchors:
  - [abstract] "zero-shot chain-of-thought and few-shot prompting showing diminished efficacy"
  - [section] "such methods may potentially lead the models to generate fallacious responses, indicative of hallucination"
- Break condition: When CoT prompts cause the model to commit to incorrect reasoning paths that it then struggles to correct

### Mechanism 3
- Claim: GPT models exhibit high confidence even in incorrect answers, limiting self-correction ability
- Mechanism: The models generate responses with high certainty regardless of accuracy, making them resistant to self-revision
- Core assumption: Model confidence is not well-calibrated with actual accuracy
- Evidence anchors:
  - [abstract] "GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities"
  - [section] "GPT-3.5-turbo displays high confidence in such logic, even in instances where the responses are incorrect"
- Break condition: When model is presented with explicit contradictory evidence that it must acknowledge

## Foundational Learning

- Concept: Graph theory fundamentals (nodes, edges, paths, connectivity, degrees)
  - Why needed here: Understanding the graph problems being tested requires knowledge of basic graph concepts
  - Quick check question: What is the difference between a simple path and a shortest path in a graph?

- Concept: Large language model capabilities and limitations
  - Why needed here: Understanding how LLMs process information differently than traditional graph algorithms
  - Quick check question: How do LLMs represent graph data when given as text rather than as structured data?

- Concept: Prompt engineering techniques (zero-shot, few-shot, chain-of-thought)
  - Why needed here: Different prompting approaches significantly impact LLM performance on graph tasks
  - Quick check question: What is the key difference between zero-shot and few-shot prompting?

## Architecture Onboarding

- Component map:
  - Graph generator → LLM interface → Response parser → Evaluation metrics (Comprehension, Correctness, Fidelity, Rectification)
  - Separate pipelines for each prompting technique and model variant

- Critical path:
  - Generate graph → Format as natural language text → Apply prompting technique → Send to LLM → Parse response → Evaluate against ground truth

- Design tradeoffs:
  - Single vs. multiple answers: Limiting to single answers reduces enumeration complexity but may not capture all correct solutions
  - Graph size vs. context window: Larger graphs provide better testing but may exceed model context limits
  - Synthetic vs. real graphs: Synthetic graphs ensure controlled testing but may miss real-world complexities

- Failure signatures:
  - High Comprehension but low Correctness: Model understands graph structure but makes logical errors in solving
  - High Correctness but low Fidelity: Model finds correct answers but also generates many incorrect ones
  - High confidence in incorrect answers: Indicates poor calibration that limits self-correction

- First 3 experiments:
  1. Test basic connectivity (find simple path) with small graphs across all prompting techniques
  2. Evaluate neighbor classification with varying hop distances to assess multi-hop reasoning
  3. Compare performance on dense vs. sparse graphs of similar size to identify sparsity effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of open-source models be improved for graph topological reasoning tasks?
- Basis in paper: [explicit] The paper shows that two open-source models (CalderaAI/30B-Lazarus and TheBloke/Wizard-Vicuna-13B) exhibit substantially lower performance compared to GPT models in correctness, with accuracy rates of only 4% and 0% on simple path tasks versus 72% and 91% for GPT models.
- Why unresolved: The paper identifies the performance gap but does not explore methods to enhance open-source models' graph reasoning capabilities.
- What evidence would resolve it: Comparative experiments testing various architectural modifications, fine-tuning strategies, or novel prompting techniques specifically designed for graph data on open-source models, showing improved accuracy rates approaching those of GPT models.

### Open Question 2
- Question: What specific factors cause GPT models to produce erroneous multi-answer responses in graph tasks?
- Basis in paper: [explicit] The paper observes that GPT models often generate spurious responses in multi-answer scenarios, with GPT-3.5-turbo producing 7.57 times more wrong than correct answers on shortest path tasks.
- Why unresolved: While the paper documents the phenomenon of high error rates in multi-answer tasks, it does not investigate the underlying mechanisms or specific factors contributing to this behavior.
- What evidence would resolve it: Detailed analysis of GPT model outputs identifying patterns in error generation, such as confidence miscalibration, reasoning breakdowns, or limitations in handling combinatorial complexity, along with experiments testing targeted interventions to reduce error rates.

### Open Question 3
- Question: How can GPT models be improved to better self-rectify incorrect responses in graph reasoning tasks?
- Basis in paper: [explicit] The paper finds that GPT-3.5-turbo has difficulty self-filtering incorrect responses, while GPT-4 can rectify some errors but also discards 26% of correct outputs, indicating imperfect rectification capabilities.
- Why unresolved: The paper demonstrates limited rectification abilities but does not explore methods to enhance this capacity without sacrificing correct answers.
- What evidence would resolve it: Experiments testing various prompting strategies, model architectures, or post-processing techniques that improve the precision of GPT model self-correction in graph tasks, achieving higher rectification accuracy while maintaining retention of correct responses.

## Limitations
- The use of synthetic graphs may not capture real-world graph complexity and patterns
- The evaluation focuses exclusively on English language inputs and specific text representations
- Open-source models tested may not represent state-of-the-art capabilities for their respective sizes

## Confidence

**High Confidence**: The observation that GPT models significantly outperform open-source models on correctness metrics (72% vs 4% on simple path tasks) is well-supported by the empirical results.

**Medium Confidence**: The finding that GPT models exhibit high confidence in incorrect answers, limiting their rectification capacity, is supported by the data but requires careful interpretation due to measurement uncertainty.

**Low Confidence**: The claim that chain-of-thought prompting may increase hallucination and is generally ineffective requires more nuanced interpretation as the study shows inconsistent benefits across different tasks.

## Next Checks
1. Test with real-world graph datasets to assess whether synthetic graph findings generalize to practical scenarios
2. Implement confidence calibration by adding explicit confidence scoring to model responses and testing its impact on rectification rates
3. Test with alternative graph representations (adjacency matrices, visual graph descriptions, graph schema definitions) to determine if performance differences are specific to node/edge list format