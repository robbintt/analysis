---
ver: rpa2
title: GPT detectors are biased against non-native English writers
arxiv_id: '2304.02819'
source_url: https://arxiv.org/abs/2304.02819
tags:
- detectors
- essays
- non-native
- detection
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reveals significant bias in widely-used GPT detectors
  against non-native English writers. Seven popular detectors misclassified over 60%
  of TOEFL essays from non-native speakers as AI-generated, while showing near-perfect
  accuracy for native English college essays.
---

# GPT detectors are biased against non-native English writers

## Quick Facts
- arXiv ID: 2304.02819
- Source URL: https://arxiv.org/abs/2304.02819
- Reference count: 40
- Key outcome: GPT detectors misclassify over 60% of non-native English writing as AI-generated, while nearly perfectly identifying native writing

## Executive Summary
This study reveals significant bias in widely-used GPT detectors against non-native English writers. Seven popular detectors misclassified over 60% of TOEFL essays from non-native speakers as AI-generated, while showing near-perfect accuracy for native English college essays. The bias stems from lower linguistic diversity and perplexity in non-native writing, which detectors interpret as AI-generated. Simple interventions like enhancing word choices to sound more native reduced misclassification by 49.45%. The study also demonstrates that basic prompts can easily bypass detectors by increasing perplexity in AI-generated text, dropping detection rates from 100% to as low as 13%. These findings highlight critical fairness issues in current GPT detection methods and caution against their use in educational settings where they may unfairly penalize non-native speakers.

## Method Summary
The study evaluated seven widely-used GPT detectors (Originality.AI, Quil.org, Sapling, OpenAI, Crossplag, GPTZero, ZeroGPT) using three human-written corpora: 91 TOEFL essays from Chinese educational forum, 70 US college admission essays from PrepScholar, and 145 scientific abstracts from Stanford CS224n project reports. The researchers measured false positive rates (misclassifying human writing as AI-generated), calculated perplexity using GPT-2 XL, and performed paired t-tests to assess statistical significance. They also tested bypass strategies by applying GPT-4 enhancement prompts to both human and AI-generated text and re-evaluating with detectors.

## Key Results
- GPT detectors misclassified 61.32% to 99.14% of non-native TOEFL essays as AI-generated, while only 2.2% to 3.1% of native college essays were misclassified
- Simple prompts to enhance linguistic complexity reduced non-native essay misclassification by 49.45%
- Basic "elevate the text" prompts could bypass detectors entirely, dropping detection rates from 100% to as low as 13% for AI-generated content
- Non-native authors' scientific abstracts showed significantly lower perplexity even in peer-reviewed academic contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT detectors misclassify non-native writing as AI-generated due to lower perplexity.
- Mechanism: Perplexity-based detectors interpret limited linguistic diversity (lexical richness, syntactic complexity) as signs of AI authorship.
- Core assumption: Lower perplexity directly correlates with AI-generated text in current detector models.
- Evidence anchors:
  - [abstract] "GPT detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified."
  - [section] "GPT detectors may penalize non-native writers with limited linguistic expressions" and "non-native English writers produce lower perplexity text"
  - [corpus] Weak - no direct corpus evidence provided for perplexity correlation with detector output
- Break condition: If perplexity is not the primary feature used for detection, or if detectors incorporate additional linguistic features beyond perplexity.

### Mechanism 2
- Claim: Simple prompts can bypass GPT detectors by increasing perplexity.
- Mechanism: Second-round self-edit prompts ("Elevate the provided text by employing literary/advanced technical language") increase linguistic diversity and perplexity, evading detection.
- Core assumption: Detectors rely primarily on perplexity thresholds and are not robust to controlled linguistic variation.
- Evidence anchors:
  - [abstract] "simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors"
  - [section] "a straightforward second-round self-edit prompt can drastically reduce detection rates for both college essays and scientific abstracts"
  - [corpus] Weak - no corpus evidence showing distribution of bypassed vs non-bypassed samples
- Break condition: If detectors use additional features beyond perplexity, or if they adapt to recognize prompt-based linguistic enhancements.

### Mechanism 3
- Claim: Bias persists even in professional writing contexts (ICLR papers).
- Mechanism: Non-native authors' abstracts show lower perplexity even in peer-reviewed academic contexts, suggesting systematic linguistic constraints.
- Core assumption: Academic writing quality (as measured by review ratings) does not fully compensate for native language advantages in perplexity.
- Evidence anchors:
  - [section] "authors based in non-native English-speaking countries wrote significantly lower text perplexity abstracts compared to those based in native English-speaking countries"
  - [corpus] Weak - no corpus evidence showing detector performance on these ICLR abstracts
- Break condition: If perplexity differences in academic writing do not translate to detector misclassification, or if other features dominate detection in technical writing.

## Foundational Learning

- Concept: Perplexity as a language model metric
  - Why needed here: Core mechanism of detector bias and bypass strategies
  - Quick check question: What does lower perplexity indicate about text predictability and linguistic diversity?

- Concept: Linguistic variability metrics
  - Why needed here: Explains why non-native writing has lower perplexity (lexical richness, syntactic complexity)
  - Quick check question: How do lexical richness and syntactic complexity typically differ between native and non-native writing?

- Concept: Prompt engineering for text generation
  - Why needed here: Explains how simple prompts can manipulate text properties to bypass detectors
  - Quick check question: What linguistic features are typically enhanced by "elevate the text" style prompts?

## Architecture Onboarding

- Component map: Input text → Perplexity calculation (GPT-2 XL) → Detector classification (7 different models) → Output (AI/human label)
- Critical path: Text → Perplexity measurement → Classification threshold → Output label
- Design tradeoffs: Perplexity-based detection offers simplicity but introduces bias; more sophisticated methods exist but are computationally expensive
- Failure signatures: High false positive rates for non-native writing; dramatic detection rate reduction with simple prompts
- First 3 experiments:
  1. Test detector performance on TOEFL essays vs college essays to reproduce bias finding
  2. Apply GPT-4 enhancement prompt to TOEFL essays and measure perplexity change and detection rate change
  3. Generate ChatGPT essays with and without self-edit prompts and compare detection rates and perplexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do GPT detectors perform when evaluating non-native English writing in specialized technical domains (e.g., scientific abstracts, legal documents) compared to general TOEFL essays?
- Basis in paper: [explicit] The study tested TOEFL essays and US college admission essays, but noted that detectors performed poorly on scientific abstracts as well, suggesting domain-specific variations.
- Why unresolved: The study only tested TOEFL essays and US college admission essays, leaving open how detectors perform on other specialized writing domains where non-native English speakers are prevalent.
- What evidence would resolve it: Systematic evaluation of GPT detectors across multiple specialized domains (scientific, legal, medical) comparing non-native vs native English writing performance.

### Open Question 2
- Question: Can detection methods be developed that account for linguistic diversity differences between native and non-native English writers without introducing bias?
- Basis in paper: [inferred] The study demonstrated that non-native writers' texts have lower perplexity due to constrained linguistic expressions, leading to systematic misclassification, but didn't explore alternative detection methods.
- Why unresolved: The study identified the bias problem but didn't propose or test solutions that could account for natural linguistic differences while still detecting AI-generated content.
- What evidence would resolve it: Development and validation of detection methods that incorporate linguistic diversity metrics or adaptive thresholds for non-native writing.

### Open Question 3
- Question: What are the long-term implications of using GPT detectors in educational settings for non-native English speakers' academic performance and participation?
- Basis in paper: [explicit] The study explicitly cautioned against using GPT detectors in educational settings due to potential harm to non-native speakers.
- Why unresolved: The study was focused on technical evaluation rather than longitudinal studies of educational impact on student outcomes.
- What evidence would resolve it: Longitudinal studies tracking non-native students' grades, participation, and academic progression in settings where GPT detectors are implemented versus those where they aren't.

### Open Question 4
- Question: How effective are more advanced detection methods (like DetectGPT) compared to perplexity-based approaches when accounting for non-native English writing?
- Basis in paper: [explicit] The study mentioned DetectGPT as a potentially more robust method but noted it was computationally expensive and not widely deployed.
- Why unresolved: The study only evaluated widely-used detectors based on perplexity and supervised learning, leaving advanced methods untested on non-native writing samples.
- What evidence would resolve it: Direct comparison of advanced detection methods (DetectGPT, watermarking techniques) against traditional detectors using the same non-native English writing datasets.

## Limitations

- The TOEFL essay corpus (91 samples from a Chinese educational forum) and college admission essays (70 samples from PrepScholar) may not generalize to other academic or professional writing scenarios
- The ICLR abstracts analysis shows lower perplexity in non-native writing but doesn't directly test detector misclassification in peer-reviewed contexts
- Bypass strategies were tested only on a subset of data, and their effectiveness may vary across different writing styles and detector models

## Confidence

- **High Confidence**: The core finding that GPT detectors misclassify non-native TOEFL essays as AI-generated at rates exceeding 60% is well-supported by the experimental data and statistical analysis
- **Medium Confidence**: The mechanism explanation (lower perplexity → detector misclassification) is plausible but relies on assumptions about detector internals that cannot be fully verified
- **Low Confidence**: The extrapolation of bias findings to professional academic writing (ICLR papers) is suggestive but not directly tested with detector performance metrics

## Next Checks

1. Test the same seven detectors on additional non-native English writing datasets (e.g., IELTS essays, academic papers from non-native authors) to assess whether the 60%+ misclassification rate generalizes beyond the TOEFL corpus

2. Investigate whether detectors that use features beyond perplexity (e.g., syntactic patterns, semantic coherence) show reduced bias against non-native writing, or whether perplexity remains the dominant factor across all tested models

3. Systematically evaluate the effectiveness of different linguistic enhancement strategies (beyond simple "elevate the text" prompts) in reducing detector bias while preserving the original meaning and voice of non-native writers