---
ver: rpa2
title: Make Deep Networks Shallow Again
arxiv_id: '2309.08414'
source_url: https://arxiv.org/abs/2309.08414
tags:
- layers
- deep
- parallel
- layer
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether deep neural networks can be effectively
  replaced by shallow, parallel architectures without sacrificing performance. The
  key insight stems from the observation that residual connections, commonly used
  to mitigate the vanishing gradient problem in deep networks, can be mathematically
  decomposed into a structure analogous to a Taylor series expansion.
---

# Make Deep Networks Shallow Again

## Quick Facts
- arXiv ID: 2309.08414
- Source URL: https://arxiv.org/abs/2309.08414
- Reference count: 5
- Key outcome: Deep neural networks can be effectively replaced by shallow, parallel architectures without sacrificing performance, challenging the prevailing notion that depth is inherently superior.

## Executive Summary
This study challenges the conventional wisdom that deep neural networks are inherently superior to shallow ones. The authors propose that residual connections, which enable deep networks to avoid vanishing gradients, can be mathematically decomposed into a structure analogous to a Taylor series expansion. By truncating this expansion to its first two terms, a single-layer, parallel architecture emerges as a potential substitute for the original deep network. Through extensive computational experiments on MNIST and CIFAR10, the authors demonstrate that shallow and deep architectures can achieve comparable performance, with shallow networks showing superior robustness to overfitting on validation sets.

## Method Summary
The authors conducted computational experiments comparing deep (sequential) and shallow (parallel) residual convolutional architectures on image classification tasks. They trained models on MNIST and CIFAR10 datasets using 6,912 configurations varying the number of convolutional layers, filters, kernel sizes, and other hyperparameters. Both architectures were designed with identical parameter counts for fair comparison. Models were trained for 100 epochs using RMSprop optimizer with a learning rate of 1e-4 and batch size of 512, using identical random initialization for corresponding layers.

## Key Results
- Deep and shallow architectures achieve similar training and validation losses across extensive experiments
- Shallow architectures demonstrate superior performance on validation sets despite slightly inferior training set performance
- The overdetermination ratio Q provides a useful heuristic for predicting when shallow and deep models perform similarly
- Results challenge the prevailing notion that deep networks are inherently superior to shallow ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Residual connections allow deep networks to avoid vanishing gradients by providing identity shortcuts.
- Mechanism: Identity mappings parallel to nonlinear layers enable gradients to flow directly backward, bypassing the nonlinearities that cause gradient decay.
- Core assumption: The skip connection preserves the gradient magnitude during backpropagation.
- Evidence anchors:
  - [abstract] "A breakthrough has been achieved by the concept of residual connections—an identity mapping parallel to a conventional layer"
  - [section] "The identity function plays the role of a bridge—or 'highway'—transferring the gradient w.r.t. layer output into that of the input with unmodified size"
  - [corpus] Weak evidence - no direct mention of residual connections or gradient flow in neighbor papers
- Break condition: If identity mappings fail to preserve gradient flow (e.g., due to numerical instability or architectural constraints).

### Mechanism 2
- Claim: Stacking residual layers can be mathematically decomposed into a Taylor series expansion.
- Mechanism: Each residual layer contributes a term to an expansion analogous to a Taylor series; truncating higher-order terms yields a shallow approximation.
- Core assumption: The network mapping can be locally linearized around the minimum using gradient information.
- Evidence anchors:
  - [section] "A stack of residual connection layers can be expressed as an expansion of terms similar to the Taylor expansion"
  - [section] "In an analogy to Taylor expansion, it can be hypothesized that the first two terms may be a reasonable approximation of the whole mapping"
  - [corpus] Weak evidence - no neighbor papers discuss Taylor-like expansions of residual networks
- Break condition: If the expansion diverges or the higher-order terms are essential for capturing nonlinearities.

### Mechanism 3
- Claim: Shallow parallel architectures match deep sequential ones when overdetermination ratio Q is sufficiently high.
- Mechanism: With more constraints (data points) than free parameters, both architectures can represent the same function; shallow networks may generalize better due to smoother responses.
- Core assumption: Performance depends on the balance between representational capacity and data constraints, not depth per se.
- Evidence anchors:
  - [section] "With P < KM, the system is overdetermined, and not all constraints can be satisfied... fitting to the additive noise... is reduced to the fraction 1/Q"
  - [section] "shallow architectures show better performance on the validation set despite the slightly inferior loss on the training set"
  - [corpus] Weak evidence - no neighbor papers analyze overdetermination or parameter/data constraints in depth
- Break condition: If data distribution or noise structure changes such that the overdetermination ratio no longer predicts performance.

## Foundational Learning

- Concept: Residual connections
  - Why needed here: Core building block enabling both deep and shallow architectures to learn effectively without vanishing gradients.
  - Quick check question: What is the role of the identity mapping in a residual block, and how does it differ from a standard layer?

- Concept: Taylor series approximation
  - Why needed here: Provides the mathematical justification for why a shallow parallel architecture might approximate a deep sequential one.
  - Quick check question: How does truncating a Taylor expansion to its first two terms relate to replacing a stack of residual layers with a single parallel layer?

- Concept: Overdetermination ratio (Q)
  - Why needed here: Explains the interplay between model capacity (number of parameters) and data constraints, predicting when shallow and deep models perform similarly.
  - Quick check question: How does the overdetermination ratio Q influence the risk of overfitting, and why might shallow architectures be more robust?

## Architecture Onboarding

- Component map: Input image → preprocessing → (sequential: conv → residual → conv → residual → ... → flatten → dense) OR (parallel: conv A + conv B + conv C + ... → flatten → dense)
- Critical path:
  1. Input image → preprocessing
  2. Sequential path: conv → residual → conv → residual → ... → flatten → dense
  3. Parallel path: conv A + conv B + conv C + ... (summed) → flatten → dense
- Design tradeoffs:
  - Depth vs. width: Deep networks may fit training data better but risk overfitting; shallow networks may generalize better but need sufficient filters.
  - Parameter count: Both architectures should have comparable parameters for fair comparison.
  - Kernel size and filter count: Larger kernels or more filters increase capacity but also computational cost.
- Failure signatures:
  - Validation loss increases with more filters (overfitting)
  - Deep architecture outperforms shallow on training but not validation (memorization)
  - Both architectures plateau at similar loss values (equivalent capacity)
- First 3 experiments:
  1. Single filter, single layer: Both architectures identical; establish baseline equivalence.
  2. Single filter, varying depth (1, 2, 4, 8, 16, 32 layers): Compare training/validation loss as depth increases.
  3. Multiple filters (2, 4, 8, 16, 32), fixed depth (e.g., 16 layers): Observe overfitting as parameter count grows.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do shallow networks with parallel architectures exhibit superior performance over deep networks on more complex datasets beyond MNIST and CIFAR10?
- Basis in paper: [explicit] The authors suggest testing the hypothesis on more complex datasets to validate the generalizability of the findings.
- Why unresolved: The current study only tested on two relatively simple datasets, limiting the scope of conclusions.
- What evidence would resolve it: Conducting experiments on more complex datasets such as ImageNet or medical imaging datasets to compare performance metrics.

### Open Question 2
- Question: How do second-order optimization algorithms affect the performance of shallow networks compared to first-order methods?
- Basis in paper: [explicit] The authors mention that future research could explore second-order algorithms for potentially more pronounced results.
- Why unresolved: The study used first-order optimization, which lacks guaranteed convergence properties.
- What evidence would resolve it: Implementing and comparing second-order optimization algorithms like Newton's method or L-BFGS with first-order methods on both shallow and deep architectures.

### Open Question 3
- Question: What is the impact of network depth and width on the robustness of models to overfitting?
- Basis in paper: [explicit] The authors observed that shallow networks might be more robust to overfitting due to their inherent smoothness.
- Why unresolved: The study did not systematically investigate the trade-offs between depth and width in relation to overfitting.
- What evidence would resolve it: Conducting controlled experiments varying the depth and width of networks while measuring overfitting indicators such as generalization gap and validation loss.

## Limitations
- The Taylor-like expansion assumption lacks strong empirical validation and rigorous mathematical proof
- Experiments are limited to two relatively simple datasets (MNIST and CIFAR10)
- The overdetermination ratio Q's predictive power is not validated across diverse scenarios
- The claim that shallow networks are inherently more robust to overfitting is speculative

## Confidence
- Medium confidence: The equivalence of deep and shallow architectures in terms of training and validation loss is well-supported by the experimental results
- Low confidence: The assertion that shallow networks are inherently smoother and thus more robust to overfitting is speculative
- Medium confidence: The overdetermination ratio Q provides a useful heuristic for predicting performance

## Next Checks
1. Replicate the experiments on additional datasets (e.g., CIFAR100, ImageNet) to test the robustness of the deep-shallow equivalence across different data complexities and class distributions
2. Conduct ablation studies to isolate the effects of residual connections, overdetermination ratio, and architectural depth on overfitting and generalization
3. Experimentally verify the Taylor-like expansion hypothesis by analyzing the contribution of higher-order terms in residual layers