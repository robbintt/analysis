---
ver: rpa2
title: Leveraging Explainable AI to Analyze Researchers' Aspect-Based Sentiment about
  ChatGPT
arxiv_id: '2308.11001'
source_url: https://arxiv.org/abs/2308.11001
tags:
- sentiment
- chatgpt
- figure
- text
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of performing Aspect-Based Sentiment
  Analysis (ABSA) on long research articles, where traditional ABSA methods struggle
  due to the lack of labeled datasets and the length of text data. The authors propose
  a methodology leveraging Explainable AI, specifically using SHAP, to interpret the
  outputs of pre-trained sentiment analysis models like nlptown/bert-base-multilingual-uncased-sentiment.
---

# Leveraging Explainable AI to Analyze Researchers' Aspect-Based Sentiment about ChatGPT

## Quick Facts
- arXiv ID: 2308.11001
- Source URL: https://arxiv.org/abs/2308.11001
- Reference count: 0
- Primary result: Proposed Explainable AI approach successfully identifies positive overall sentiment (75.5%) and nuanced aspect-level sentiments in 868 arXiv papers about ChatGPT

## Executive Summary
This work addresses the challenge of performing Aspect-Based Sentiment Analysis (ABSA) on long research articles, where traditional ABSA methods struggle due to the lack of labeled datasets and the length of text data. The authors propose a methodology leveraging Explainable AI, specifically using SHAP, to interpret the outputs of pre-trained sentiment analysis models like nlptown/bert-base-multilingual-uncased-sentiment. By visualizing which words or phrases influence model predictions, they can extract aspect-level sentiments even without labeled aspect terms. Applied to a dataset of 868 arXiv papers on ChatGPT, the method successfully identifies positive overall sentiment (75.5%) and nuanced aspect-level sentiments, such as negative views on ChatGPT's truthfulness. The approach demonstrates that Explainable AI can extend ABSA capabilities to long, unlabeled texts, offering valuable insights into sentiment analysis beyond short reviews.

## Method Summary
The methodology combines pre-trained sentiment models with Explainable AI to perform ABSA on long research abstracts. The process uses nlptown/bert-base-multilingual-uncased-sentiment for overall sentiment classification, applies SHAP to interpret model predictions and extract aspect-level sentiment indicators, and validates results using yangheng/deberta-v3-base-absa-v1.1 for aspect sentiment classification. The approach processes 868 arXiv abstracts containing "chatgpt" by first classifying overall sentiment (1-5 stars), then using SHAP visualizations to identify influential words and phrases for aspect extraction, and finally validating aspect-level sentiment through additional model inference.

## Key Results
- Overall positive sentiment identified at 75.5% across 868 arXiv papers about ChatGPT
- Successful extraction of nuanced aspect-level sentiments, including negative views on ChatGPT's truthfulness
- Demonstrated feasibility of using Explainable AI to perform ABSA on long, unlabeled research texts without manual aspect term labeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explainable AI (SHAP) can compensate for missing aspect-level labels in long text ABSA.
- Mechanism: SHAP generates feature importance scores for individual words/phrases, enabling extraction of aspect terms and sentiment polarity without manual labeling.
- Core assumption: Pre-trained sentiment models retain sufficient linguistic nuance to produce interpretable feature attributions.
- Evidence anchors: Abstract states SHAP enables extraction of aspect-level sentiments without labeled aspect terms; section 5.1 describes SHAP visualization methodology.
- Break condition: If model representations are too entangled or text is too long, SHAP attributions may not isolate meaningful aspects.

### Mechanism 2
- Claim: Joint use of nlptown and yangheng models yields complementary aspect and sentiment signals.
- Mechanism: nlptown provides coarse sentiment scores; yangheng provides fine-grained aspect-level sentiment. SHAP interprets nlptown to bridge the gap.
- Core assumption: Sentiment and aspect extraction are separable tasks that can be sequentially composed.
- Evidence anchors: Section 5.1 applies yangheng model to extract aspect-level sentiment; section 5.2 finds yangheng labels aspects like education as positive.
- Break condition: If yangheng model's aspect extraction fails on unlabeled text, pipeline collapses.

### Mechanism 3
- Claim: Long abstracts contain shifting aspect sentiments that require interpretable explanations to detect.
- Mechanism: SHAP highlights text spans driving neutral vs. negative polarity, revealing sentiment shifts missed by overall document-level scores.
- Core assumption: Sentiment shifts are signaled by localized lexical cues that SHAP can identify.
- Evidence anchors: Section 5.1 shows ChatGPT's failure to provide truthful answers is consistently highlighted in red; section 5.2 highlights potential negative aspects in blue.
- Break condition: If sentiment shifts are expressed through complex discourse structures rather than lexical cues, SHAP may not detect them.

## Foundational Learning

- Concept: Aspect-Based Sentiment Analysis (ABSA)
  - Why needed here: Core task of extracting sentiment toward specific aspects in long research abstracts.
  - Quick check question: Can you list the four sentiment elements in ABSA (aspect category, aspect term, opinion term, sentiment polarity)?

- Concept: Explainable AI (SHAP)
  - Why needed here: Enables unsupervised aspect extraction by revealing feature importance in sentiment model outputs.
  - Quick check question: How does SHAP assign importance to words in a sentence for a given model prediction?

- Concept: Transfer learning with pre-trained transformers
  - Why needed here: Allows sentiment classification on unlabeled research text without training from scratch.
  - Quick check question: What is the difference between fine-tuning and zero-shot inference for a pre-trained BERT model?

## Architecture Onboarding

- Component map: arXiv metadata ingestion → nlptown sentiment scoring → SHAP interpretation → yangheng aspect sentiment extraction → aggregation
- Critical path: Data collection → nlptown inference → SHAP visualization → yangheng inference → result synthesis
- Design tradeoffs: Using pre-trained models avoids labeling cost but limits control over domain-specific aspects; SHAP adds interpretability but increases inference latency
- Failure signatures: Neutral overall score with mixed SHAP attributions; yangheng failing to detect any aspect; SHAP attributions dominated by stop words or irrelevant phrases
- First 3 experiments:
  1. Run nlptown on a sample of 10 abstracts; verify 1-5 star output distribution
  2. Apply SHAP to the same samples; check that key sentiment words are highlighted consistently
  3. Feed SHAP-identified aspect terms into yangheng; confirm aspect-level sentiment alignment with human judgment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can aspect terms be detected with higher precision in long research abstracts using Explainable AI?
- Basis in paper: Authors note as future work: "We plan to investigate as to how we can detect the aspect terms as precisely as possible" and discuss challenges in extracting aspect terms from unlabeled long texts.
- Why unresolved: Current method relies on interpreting SHAP visualizations, which is qualitative and may miss subtle or implicit aspect terms.
- What evidence would resolve it: Development of a quantitative metric or automated pipeline that improves aspect term detection accuracy beyond manual SHAP interpretation.

### Open Question 2
- Question: Can alternative pre-trained sentiment models outperform nlptown/bert-base-multilingual-uncased-sentiment for aspect-based sentiment analysis in research texts?
- Basis in paper: Authors plan to "apply and evaluate more models to improve the performance of Aspect-Based Sentiment Analysis."
- Why unresolved: Only one sentiment model (nlptown) and one ABSA model (yangheng) were tested; no comparative evaluation was performed.
- What evidence would resolve it: Systematic comparison of multiple models (e.g., different BERT variants, RoBERTa, DeBERTa) on the same dataset with standardized metrics.

### Open Question 3
- Question: Does the proposed Explainable AI approach scale effectively to datasets larger than 868 papers or to different domains (e.g., biomedical, social sciences)?
- Basis in paper: Authors focus on a specific dataset of 868 arXiv CS papers and do not test scalability or domain transfer.
- Why unresolved: No experiments were conducted on larger datasets or non-CS domains; performance on long documents is only discussed qualitatively.
- What evidence would resolve it: Experiments showing consistent aspect extraction and sentiment accuracy on datasets 10x larger and from multiple academic disciplines.

## Limitations

- The approach relies on assumptions about pre-trained model interpretability that are not empirically validated on the specific dataset
- Validation using yangheng model outputs and ChatGPT agreement lacks transparency in methodology and could introduce circularity
- Sample size for validation is not specified, making it difficult to assess generalizability
- The reliance on SHAP assumes sentiment shifts are lexically cued, which may not hold for complex discourse structures in academic writing

## Confidence

- High confidence: The overall positive sentiment (75.5%) is directly reported from model outputs and is straightforward to verify
- Medium confidence: The claim that SHAP can compensate for missing aspect-level labels is plausible but depends on model interpretability quality, which is not empirically demonstrated
- Medium confidence: The assertion that the joint use of nlptown and yangheng models yields complementary signals is inferred from model descriptions rather than validated performance metrics

## Next Checks

1. Conduct a manual annotation study on a subset of abstracts to verify whether SHAP-identified words/phrases accurately correspond to aspect terms and sentiment polarity
2. Perform ablation tests by comparing SHAP interpretations with and without domain-specific fine-tuning of the sentiment model to assess robustness
3. Validate the consistency of aspect sentiment extraction by cross-checking yangheng model outputs against multiple independent human raters and ChatGPT responses