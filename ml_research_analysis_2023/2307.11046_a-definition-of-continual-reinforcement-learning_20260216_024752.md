---
ver: rpa2
title: A Definition of Continual Reinforcement Learning
arxiv_id: '2307.11046'
source_url: https://arxiv.org/abs/2307.11046
tags:
- agent
- learning
- basis
- agents
- definition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a formal definition of the continual reinforcement
  learning (CRL) problem. The key insight is that CRL problems are those in which
  the best agents never stop learning.
---

# A Definition of Continual Reinforcement Learning

## Quick Facts
- arXiv ID: 2307.11046
- Source URL: https://arxiv.org/abs/2307.11046
- Authors: 
- Reference count: 40
- Key outcome: This paper develops a formal definition of the continual reinforcement learning (CRL) problem, defining it as any RL instance where optimal agents never stop learning by implicitly searching over a basis of behaviors.

## Executive Summary
This paper provides a formal mathematical definition of continual reinforcement learning (CRL) by introducing two key operators: "generates" and "reaches." The generates operator captures how agents can be understood as searching over a space of behaviors, while the reaches operator determines whether an agent ever stops this search. CRL is then defined as any reinforcement learning problem in which all optimal agents never stop their implicit search over a basis of behaviors. The framework reveals important properties about agent design spaces and provides examples of CRL problems including multi-task reinforcement learning and continual supervised learning.

## Method Summary
The paper introduces a formal RL framework with histories, agents, environments, and performance functions. It then defines the "generates" operator to model how agents search over basis behaviors, and the "reaches" operator to determine if agents stop their search. CRL is verified by checking if all optimal agents never stop reaching the basis. The approach requires implementing these operators and testing them against the CRL definition, though specific details of the performance function and learning rules are left abstract.

## Key Results
- Introduces generates and reaches operators to formalize implicit search processes of agents
- Defines CRL as RL problems where all optimal agents never stop their implicit search over basis behaviors
- Proves CRL necessarily involves restricted agent spaces and constrained bases with inherent redundancies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Every agent can be understood as implicitly searching over a space of behaviors
- Mechanism: The generates operator formalizes how any agent set can be understood as switching between a basis of behaviors according to learning rules. This provides a behavioral signature of learning as an implicit search process.
- Core assumption: Agent behavior can be decomposed into sequences of history-conditioned switches over a basis of behaviors
- Evidence anchors:
  - [abstract] "we can understand every agent as implicitly searching over a set of behaviors"
  - [section] "any set of agents generates (Definition 3.4) another set of agents, and (2) a given agent reaches (Definition 3.5) an agent set"

### Mechanism 2
- Claim: CRL problems are those where the best agents never stop learning
- Mechanism: The reaches operator formalizes when an agent stops its search and settles on a fixed behavior. CRL is defined as the setting where all optimal agents never stop their implicit search.
- Core assumption: Learning is an endless adaptation process rather than finding a final solution
- Evidence anchors:
  - [abstract] "CRL is then defined as any instance of the reinforcement learning problem in which all optimal agents never stop their implicit search"
  - [section] "every agent will either continue this search forever, or eventually stop"

### Mechanism 3
- Claim: The CRL framework reveals necessary properties of agent design spaces
- Mechanism: CRL problems necessarily involve restricted spaces of agents and constrained bases, creating inherent redundancies in the design space
- Core assumption: Bounded agents with constrained learning rules are central to continual learning
- Evidence anchors:
  - [section] "Î› âŠ‚ Î›" and "there exists an agent set, Î›â—¦, such that |Î›â—¦ | < |Î›| and Î›â—¦ âŠ¢ð‘’ Î›"
  - [corpus] "bound agents" and "constrained set of learning rules"

## Foundational Learning

- Concept: Operator theory in agent analysis
  - Why needed here: The generates and reaches operators provide the mathematical foundation for defining and analyzing continual learning agents
  - Quick check question: Can you explain how the generates operator differs from traditional notions of agent composition?

- Concept: Behavioral equivalence and history suffixes
- Why needed here: The reaches operator depends on comparing agent behaviors across realizable histories and their suffixes to determine when search stops
  - Quick check question: How does the definition of realizable histories impact when an agent is said to reach a basis?

- Concept: Uniform generation and basis minimality
  - Why needed here: Understanding when bases can universally generate agent sets helps identify minimal, non-redundant representations of agent behaviors
  - Quick check question: Why can't a singleton basis generate any agent set other than itself?

## Architecture Onboarding

- Component map:
  - Core operators: generates (âŠ¢ð‘’) and reaches (â‡ð‘’)
  - Agent basis: Î›B âŠ‚ Î›
  - Learning rules: Î£ mapping histories to basis agents
  - Performance function: ð‘£ mapping histories to rewards
  - Realizable histories: Â¯â„‹ and their suffixes Â´â„‹

- Critical path:
  1. Define agent basis Î›B
  2. Identify learning rules Î£ that generate desired agent set Î›
  3. Verify generation relationship Î›B âŠ¢ð‘’ Î›
  4. Check if optimal agents satisfy ðœ† â‡/ð‘’ Î›B
  5. Confirm CRL properties hold

- Design tradeoffs:
  - Rich vs minimal bases: Universal bases have infinite rank but may be less interpretable
  - Computational vs expressive power: Constrained bases limit search space but may miss optimal solutions
  - Environment dependence: Generation and reaching are environment-specific properties

- Failure signatures:
  - Generation fails: No learning rules can produce target agent set from basis
  - Reaching fails: All agents converge to basis, violating CRL definition
  - Non-minimal basis: Basis contains redundant agents that can be removed

- First 3 experiments:
  1. Implement generates operator for simple MDP with 2-3 basis policies
  2. Test reaches operator on switching MDP environment with convergent vs continual agents
  3. Verify CRL properties hold for toy problem with known optimal solutions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we define a "minimal" agent basis for a given agent set, and what properties does it have?
- Basis in paper: [explicit] The paper introduces the concept of a minimal basis as one that cannot be made smaller while retaining the same expressive power.
- Why unresolved: The paper defines minimal bases but doesn't explore their properties or how to find them efficiently.
- What evidence would resolve it: Characterizing the properties of minimal bases, and developing algorithms to find them for various agent sets.

### Open Question 2
- Question: How does the choice of agent basis impact the performance of continual reinforcement learning agents?
- Basis in paper: [explicit] The paper discusses how the choice of basis is constrained by practical considerations like computational resources.
- Why unresolved: The paper doesn't investigate how different basis choices affect agent performance in CRL problems.
- What evidence would resolve it: Empirical studies comparing CRL agent performance across various basis choices.

### Open Question 3
- Question: Are there specific families of learning rules that guarantee the resulting agents will be continual learners?
- Basis in paper: [explicit] The paper presents a family of model-based learning rules and shows that in CRL problems, optimal agents must replan infinitely often.
- Why unresolved: The paper only considers one family of learning rules; a broader characterization is needed.
- What evidence would resolve it: Identifying and characterizing other learning rule families that ensure continual learning behavior.

## Limitations
- Limited empirical validation across diverse RL environments
- Abstractness of the basis concept may not hold for all agent types
- Assumes agent behaviors can be decomposed into switches over a basis

## Confidence
- High Confidence: The formal definition of CRL using generates and reaches operators is mathematically sound and provides a clear theoretical framework
- Medium Confidence: The examples provided adequately illustrate the concept, but more diverse examples would strengthen the claims
- Low Confidence: The claim that CRL necessarily involves restricted spaces of agents and constrained bases needs more empirical support

## Next Checks
1. Implement the generates and reaches operators in a simple MDP environment and verify that they correctly identify whether agents are continual learners according to the CRL definition.

2. Apply the CRL framework to diverse RL environments (e.g., Atari games, continuous control tasks) to test if the theoretical properties hold across different problem domains.

3. Conduct experiments to test the assumption that agent behaviors can be decomposed into switches over a basis, examining cases where this might fail or be computationally intractable.