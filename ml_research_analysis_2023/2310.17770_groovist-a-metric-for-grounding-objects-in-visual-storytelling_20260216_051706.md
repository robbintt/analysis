---
ver: rpa2
title: 'GROOViST: A Metric for Grounding Objects in Visual Storytelling'
arxiv_id: '2310.17770'
source_url: https://arxiv.org/abs/2310.17770
tags:
- groovist
- story
- visual
- grounding
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating visual grounding
  in generated stories for image sequences, a task known as visual storytelling. Existing
  metrics have critical shortcomings in capturing this aspect.
---

# GROOViST: A Metric for Grounding Objects in Visual Storytelling

## Quick Facts
- arXiv ID: 2310.17770
- Source URL: https://arxiv.org/abs/2310.17770
- Reference count: 21
- Key outcome: GROOViST is a novel modular metric that leverages noun phrases, vision-language alignment, and concreteness weighting to assess visual grounding in visual storytelling, outperforming existing metrics with higher correlation to human judgments.

## Executive Summary
This paper addresses the challenge of evaluating visual grounding in generated stories for image sequences, a task known as visual storytelling. Existing metrics have critical shortcomings in capturing this aspect. The authors propose GROOViST, a novel modular metric that leverages noun phrases, vision-language alignment, and concreteness weighting to assess grounding. GROOViST is robust to temporal misalignments and correlates well with human intuitions. Experimental results on various datasets demonstrate that GROOViST outperforms existing metrics, with higher correlations to human judgments and better ability to distinguish between original and randomly paired stories. The modular design allows for interpretability and component-level analysis.

## Method Summary
GROOViST evaluates visual grounding in visual storytelling by extracting noun phrases from stories, computing CLIP similarity scores between each noun phrase and bounding boxes from corresponding images, and aggregating these scores with penalization and concreteness weighting. The method involves preprocessing stories to extract noun phrases, computing CLIP alignment for each noun phrase to image regions, selecting maximum alignments, applying penalization based on dataset thresholds, weighting by concreteness ratings, and normalizing by noun phrase count. This modular approach allows for component-wise interpretability and addresses limitations of existing metrics.

## Key Results
- GROOViST achieves higher correlation with human judgments than RoViST-VG, VSR, and CLIPScore on VIST and VWP datasets
- GROOViST handles temporal misalignments better than sentence-by-sentence metrics by considering the entire story holistically
- The modular design allows for interpretability and component-level analysis, enabling diagnosis of metric behavior

## Why This Works (Mechanism)

### Mechanism 1
GROOViST achieves higher alignment with human grounding judgments than RoViST-VG by incorporating concreteness weighting instead of IDF weighting. IDF weighting can penalize frequent but visually grounded nouns and boost abstract infrequent ones. Concreteness weighting preferentially weights visually grounded nouns higher.

### Mechanism 2
GROOViST handles temporal misalignments better than CLIPScore by considering the entire story holistically rather than sentence-by-sentence. By computing grounding scores for all NPs in the story and selecting the max alignment per NP across all images, GROOViST captures entities mentioned out-of-order relative to their visual appearance.

### Mechanism 3
GROOViST's modular design allows component-level analysis and interpretability. By extracting NPs, computing alignment scores, penalizing poorly grounded NPs, and applying concreteness weighting as separate steps, GROOViST enables diagnosis of which components contribute most to the final score.

## Foundational Learning

- **Noun phrase extraction**: GROOViST builds on NPs to capture visually informative words and their modifiers. Quick check: How does GROOViST handle multi-word expressions like "parking lot" differently from RoViST-VG?
- **Concreteness ratings**: Concreteness weighting preferentially boosts scores of visually grounded nouns. Quick check: What percentage of NPs in the VIST test set have available concreteness ratings?
- **Vision-language alignment**: GROOViST uses CLIP to compute alignment scores between NPs and image regions. Quick check: How does GROOViST select the alignment score for each NP across all image regions?

## Architecture Onboarding

- **Component map**: Input (image sequence + story) -> NP extraction (spaCy) -> Vision-language alignment (CLIP) -> Concreteness weighting (Brysbaert et al.) -> Penalization (threshold-based) -> Aggregation (weighted sum normalized by NP count)
- **Critical path**: 1) Extract NPs from story 2) Compute CLIP alignment scores for each NP-image pair 3) Select max alignment per NP 4) Apply penalization based on threshold 5) Apply concreteness weighting 6) Aggregate and normalize
- **Design tradeoffs**: NP-based vs word-based (NPs capture compound nouns and modifiers but may miss verbs), Concreteness vs IDF weighting (Concreteness more theoretically motivated but requires ratings), Holistic vs sentence-by-sentence (Holistic handles temporal misalignment but loses local coherence)
- **Failure signatures**: Low scores on stories with high temporal misalignment (check if max alignment selection is working), Inconsistent scores across datasets (check if threshold and concreteness ratings are appropriate), Poor correlation with human judgments (check if NP extraction or alignment computation is failing)
- **First 3 experiments**: 1) Ablation study: Remove penalization component and compare scores on VIST 2) Replacement study: Swap concreteness weighting for IDF weighting and measure Flickr8k-Expert correlation 3) Temporal misalignment analysis: Bin stories by misalignment and compute Kendall tau with human judgments

## Open Questions the Paper Calls Out

### Open Question 1
How does GROOViST perform on languages other than English? The paper mentions that GROOViST could potentially be adapted for multilingual use by switching to models like multilingual-CLIP, but does not provide any experimental results or analysis for non-English languages. This leaves open questions about its generalizability and effectiveness across different linguistic contexts.

### Open Question 2
How does GROOViST handle stories with abstract or metaphorical language? While the paper discusses the use of concreteness weighting in GROOViST, it does not explicitly address how the metric performs on stories containing abstract or metaphorical language. The reliance on noun phrases and visual grounding may pose challenges for such cases.

### Open Question 3
How does GROOViST perform on visual storytelling datasets with longer or more complex narratives? The paper evaluates GROOViST on datasets like VIST, AESOP, and VWP, which contain stories of varying lengths. However, it does not provide a detailed analysis of how the metric performs on datasets with significantly longer or more complex narratives, such as those found in literature or film.

## Limitations
- Reliance on concreteness ratings from Brysbaert et al. (2014) may limit applicability to non-English domains or specialized vocabularies
- Assumption that temporal misalignment can be adequately captured through holistic story-level evaluation may not hold for extreme cases
- CLIP-based alignment may struggle with abstract concepts or culturally-specific entities that lack clear visual representations

## Confidence

- **High confidence**: GROOViST's superior correlation with human judgments compared to existing metrics (RoViST-VG, VSR, CLIPScore) on VIST and VWP datasets
- **Medium confidence**: The claim that concreteness weighting is superior to IDF weighting for visual grounding tasks, based on theoretical reasoning rather than extensive empirical comparison
- **Low confidence**: The generalizability of GROOViST to other visual storytelling datasets beyond VIST, AESOP, and VWP, given the limited evaluation scope

## Next Checks

1. Conduct a cross-linguistic evaluation of GROOViST by testing it on visual storytelling datasets in languages other than English to assess the robustness of the concreteness weighting component
2. Perform a controlled experiment introducing varying degrees of temporal misalignment to quantify GROOViST's sensitivity and compare it against sentence-by-sentence metrics
3. Evaluate GROOViST's performance on visual storytelling tasks involving abstract or culturally-specific content to test the limits of CLIP-based alignment and concreteness weighting assumptions