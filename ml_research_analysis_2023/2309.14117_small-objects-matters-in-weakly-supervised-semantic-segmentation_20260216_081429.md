---
ver: rpa2
title: Small Objects Matters in Weakly-supervised Semantic Segmentation
arxiv_id: '2309.14117'
source_url: https://arxiv.org/abs/2309.14117
tags:
- segmentation
- baseline
- instances
- loss
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel evaluation metric, Instance-Aware Mean
  Intersection-over-Union (IA-mIoU), to address the limitation of the conventional
  metric (mIoU) in capturing small objects in weakly-supervised semantic segmentation
  (WSSS). The authors also construct a size-balanced evaluation dataset, PASCAL-B,
  to complement the existing PASCAL VOC dataset, which suffers from an imbalanced
  distribution of object sizes.
---

# Small Objects Matters in Weakly-supervised Semantic Segmentation

## Quick Facts
- arXiv ID: 2309.14117
- Source URL: https://arxiv.org/abs/2309.14117
- Reference count: 40
- Key outcome: Proposes IA-mIoU metric and size-balanced training strategy to improve small object detection in WSSS

## Executive Summary
This paper addresses the critical limitation of conventional mIoU in weakly-supervised semantic segmentation, which fails to adequately capture small object performance due to dominance by larger objects. The authors propose a novel Instance-Aware Mean Intersection-over-Union (IA-mIoU) metric that evaluates each instance individually, preventing large objects from masking poor small object performance. They construct a size-balanced PASCAL-B dataset and introduce a size-balanced cross-entropy loss coupled with Elastic Weight Consolidation (EWC) training strategy to improve small object capture while preserving large object performance.

## Method Summary
The method involves three main components: (1) constructing a size-balanced PASCAL-B dataset from LVIS and COCO to complement the imbalanced PASCAL VOC dataset, (2) implementing IA-mIoU metric that splits predictions and ground-truths into individual instances for separate IoU computation, and (3) training with size-balanced cross-entropy loss that weights pixels based on instance size, combined with EWC regularization to preserve large object performance during fine-tuning. The training follows a two-stage approach: first learning large objects with standard CE loss, then fine-tuning with size-balanced loss while regularizing important parameters.

## Key Results
- IA-mIoU metric reveals significant performance gaps in existing WSSS methods for small objects that mIoU fails to capture
- Size-balanced cross-entropy loss (Lsb) with EWC achieves consistent improvements across ten WSSS methods on PASCAL VOC, PASCAL-B, and MS COCO
- PASCAL-B dataset provides more reliable evaluation for small object performance compared to imbalanced PASCAL VOC
- The proposed method achieves comprehensive performance boost particularly in capturing small objects while maintaining large object accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IA-mIoU addresses mIoU's limitation by evaluating each instance individually
- Mechanism: Splits predictions and ground-truths into instances and computes IoU separately, then averages
- Core assumption: Instance masks can be approximated by connected components or additional instance annotation
- Evidence anchors: Abstract mentions comprehensive assessment across object sizes; section 2.1 details instance splitting methodology
- Break condition: If instance splitting fails due to overlapping objects or ambiguous boundaries

### Mechanism 2
- Claim: Size-balanced cross-entropy loss improves small object capture through pixel weighting
- Mechanism: Assigns higher weights to pixels from smaller instances during training
- Core assumption: Pseudo ground-truth masks can identify connected components for size estimation
- Evidence anchors: Abstract mentions size-balanced loss; section 4.2 describes pixel weighting approach; section 5.2 shows IA-mIoU improvements
- Break condition: If instance size estimation is inaccurate due to poor pseudo masks

### Mechanism 3
- Claim: EWC training preserves large object performance while learning small objects
- Mechanism: Two-task training with parameter regularization using Fisher information matrix
- Core assumption: Parameters important for large object segmentation can be identified and preserved
- Evidence anchors: Abstract mentions proper training strategy; section 4.2 explains EWC's continual learning benefits; section 5.3 shows ablation results
- Break condition: If regularization is too strong (prevents learning) or too weak (loses large object performance)

## Foundational Learning

- Concept: Instance-aware evaluation metrics
  - Why needed here: Conventional metrics like mIoU don't capture per-instance performance, masking weaknesses in small object segmentation
  - Quick check question: How would you modify mIoU to account for individual instances rather than aggregated pixels?

- Concept: Size-weighted loss functions
  - Why needed here: Standard cross-entropy treats all pixels equally, causing models to focus on large objects with more pixels
  - Quick check question: What are the trade-offs of weighting small object pixels more heavily during training?

- Concept: Continual learning with EWC
  - Why needed here: Training strategy needs to balance learning small objects without forgetting large object performance
  - Quick check question: How does EWC prevent catastrophic forgetting when fine-tuning on a new task?

## Architecture Onboarding

- Component map: PASCAL-B dataset construction -> IA-mIoU implementation -> Lsw/Lsb loss functions -> EWC training strategy -> Performance validation
- Critical path: Dataset → Metric evaluation → Loss function selection → EWC training strategy → Performance validation
- Design tradeoffs:
  - Instance splitting accuracy vs. computational overhead
  - Weight scaling for small objects vs. potential gradient instability
  - EWC regularization strength vs. flexibility for learning new features
- Failure signatures:
  - Poor IA-mIoU improvement despite mIoU gains (indicates large object bias)
  - Training instability with high weight values (suggests need for weight clipping)
  - Degradation in large object performance (indicates excessive EWC regularization)
- First 3 experiments:
  1. Implement IA-mIoU metric and compare against mIoU on existing WSSS methods
  2. Test Lsw loss function on a simple segmentation model with synthetic size-imbalanced data
  3. Validate EWC regularization by training on task A then fine-tuning with Lsw while monitoring both task performances

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed size-balanced cross-entropy loss (Lsb) perform on fully supervised semantic segmentation methods compared to weakly supervised ones?
- Basis in paper: The paper mentions that the findings can be applied to fully supervised semantic segmentation methods, but the experiments were limited due to computational constraints.
- Why unresolved: The paper does not provide experimental results for fully supervised methods, only mentioning the potential applicability.
- What evidence would resolve it: Conducting experiments on fully supervised methods and comparing their performance with weakly supervised methods using Lsb.

### Open Question 2
- Question: How does the performance of the proposed method vary with different object size distributions in the evaluation dataset?
- Basis in paper: The paper highlights the importance of a balanced dataset for accurate evaluation, suggesting that performance might vary with different object size distributions.
- Why unresolved: The paper only evaluates the method on three datasets with varying object size distributions, not systematically exploring the impact of different distributions.
- What evidence would resolve it: Systematically evaluating the method on datasets with different object size distributions and analyzing the performance variations.

### Open Question 3
- Question: How does the proposed method handle objects with irregular shapes or complex boundaries compared to traditional methods?
- Basis in paper: The paper focuses on improving the capture of small objects, which often have irregular shapes or complex boundaries, but does not explicitly discuss how the method handles such cases.
- Why unresolved: The paper does not provide specific experiments or analysis on objects with irregular shapes or complex boundaries.
- What evidence would resolve it: Conducting experiments on datasets with objects of irregular shapes or complex boundaries and comparing the performance of the proposed method with traditional methods.

## Limitations

- IA-mIoU metric relies on accurate instance segmentation for evaluation, introducing additional annotation overhead
- Size-balanced loss function is sensitive to quality of pseudo ground-truth masks for instance size estimation
- EWC regularization introduces hyperparameters requiring careful tuning for different datasets and architectures

## Confidence

- High confidence: The core observation that mIoU fails to capture small object performance is well-supported by experimental evidence
- Medium confidence: Effectiveness of Lsw and Lsb loss functions is demonstrated empirically but may require dataset-specific tuning
- Low confidence: Generalizability of EWC training strategy to other WSSS architectures needs further validation

## Next Checks

1. **Instance splitting robustness test**: Evaluate IA-mIoU performance across varying levels of instance boundary quality in pseudo masks to establish sensitivity to annotation noise
2. **Cross-dataset generalization**: Apply the size-balanced loss and EWC strategy to WSSS methods on MS COCO with different backbone architectures to test scalability
3. **Hyperparameter sensitivity analysis**: Systematically vary the weight scaling factor α and EWC regularization λ to identify optimal ranges and potential failure modes