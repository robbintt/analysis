---
ver: rpa2
title: Model-tuning Via Prompts Makes NLP Models Adversarially Robust
arxiv_id: '2303.07320'
source_url: https://arxiv.org/abs/2303.07320
tags:
- adversarial
- robustness
- clean
- language
- mlp-ft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that fine-tuning pretrained language models
  using prompts (MVP) yields significantly more robust models against adversarial
  word substitutions compared to standard fine-tuning with an MLP head. Across three
  text classification datasets, MVP improves robust accuracy by an average of 8% over
  MLP-FT and outperforms adversarial training baselines by 3.5%.
---

# Model-tuning Via Prompts Makes NLP Models Adversarially Robust

## Quick Facts
- arXiv ID: 2303.07320
- Source URL: https://arxiv.org/abs/2303.07320
- Reference count: 40
- Primary result: MVP improves robust accuracy by 8% over MLP-FT and outperforms adversarial training by 3.5%

## Executive Summary
This paper demonstrates that fine-tuning pretrained language models using prompts (MVP) yields significantly more robust models against adversarial word substitutions compared to standard fine-tuning with an MLP head. Across three text classification datasets, MVP improves robust accuracy by an average of 8% over MLP-FT and outperforms adversarial training baselines by 3.5%. Combining MVP with single-step adversarial training further improves robustness by over 10%. The authors argue that the robustness gains stem from better alignment with pretraining objectives and reduced vulnerability from randomly initialized MLP parameters.

## Method Summary
The paper compares two fine-tuning approaches: standard MLP fine-tuning (MLP-FT) which adds a randomly initialized MLP head to a pretrained model, and Masked Prompt Tuning (MVP) which fine-tunes via prompt templates using the language model head. MVP appends prompt templates containing [MASK] tokens to inputs and uses the LM head to predict masked tokens, with class logits computed by averaging over templates and taking max over candidate answers. The authors evaluate both methods on three datasets (AG News, SST-2, BoolQ) under clean and adversarial conditions using TextFooler and TextBugger attacks.

## Key Results
- MVP improves robust accuracy by 8% over MLP-FT on average across three datasets
- MVP outperforms single-step adversarial training baselines by 3.5%
- Combining MVP with single-step adversarial training improves robustness by over 10%
- Removing the randomly initialized MLP head improves robustness by ~8%
- Untrained RoBERTa models show similar robust performance for MVP and MLP-FT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding a randomly initialized MLP head to a pretrained model is a source of adversarial vulnerability.
- Mechanism: Randomly initialized parameters are not aligned with the pretraining objective, causing feature distortion and making the model sensitive to small input perturbations.
- Core assumption: Pretrained representations are well-optimized for the pretraining task, but a randomly initialized MLP head breaks the continuity between pretraining and fine-tuning.
- Evidence anchors:
  - [abstract] "the randomly initialized MLP parameters" are a cause of vulnerability
  - [section] "removing the dense layer of weights (768×768 parameters) from the standard MLP architecture" improves robustness by ~8%
  - [corpus] Weak: no corpus evidence directly supporting random parameter vulnerability in NLP.

### Mechanism 2
- Claim: Fine-tuning via prompts (MVP) aligns better with the pretraining objective, leading to improved adversarial robustness.
- Mechanism: The mask-infilling task in MVP matches the masked language modeling objective used during pretraining, preserving the structure of learned representations and reducing sensitivity to adversarial perturbations.
- Core assumption: The pretraining objective shapes the model's feature space in a way that makes mask-infilling a natural extension, while appending an MLP head is a mismatch.
- Evidence anchors:
  - [abstract] "misalignment between pre-training and fine-tuning tasks" is a key cause of vulnerability
  - [section] "untrained RoBERTa model: in the absence of pre-training, MVP and MLP-FT have similar robust performance"
  - [corpus] Weak: no direct corpus evidence linking pretraining-task alignment to robustness gains.

### Mechanism 3
- Claim: The choice of semantically similar candidate answers is not critical for robustness in MVP.
- Mechanism: With sufficient fine-tuning, the model learns to associate any candidate word with a class label, making semantic similarity irrelevant for robustness.
- Core assumption: The model's ability to associate arbitrary candidate answers with classes is robust to semantic content after fine-tuning.
- Evidence anchors:
  - [abstract] "the choice of candidate answers is inconsequential" in the fine-tuning regime
  - [section] "random proper nouns ('jack', 'john', 'ann', 'ruby') result in similar robustness gains" as semantically related candidates
  - [corpus] Weak: no corpus evidence directly addressing semantic candidate impact.

## Foundational Learning

- Concept: Adversarial robustness in NLP
  - Why needed here: Understanding how models fail under small input perturbations is central to evaluating MVP's effectiveness.
  - Quick check question: What is the difference between clean accuracy and robust accuracy in the context of adversarial attacks?

- Concept: Masked language modeling (MLM) and its pretraining objective
  - Why needed here: MVP leverages the MLM objective during fine-tuning, which is key to its robustness gains.
  - Quick check question: How does the MLM objective differ from a standard classification objective?

- Concept: Fine-tuning vs. prompting in NLP
  - Why needed here: MVP modifies the input with prompts instead of the model, which is a fundamental shift from standard fine-tuning.
  - Quick check question: What is the key architectural difference between MVP and standard fine-tuning with an MLP head?

## Architecture Onboarding

- Component map:
  - Input text -> Prompt template (with [MASK]) -> Pretrained LM -> LM head -> Candidate answers -> Class logits

- Critical path:
  1. Construct prompt input by appending template(s) to raw input
  2. Pass prompt input through pretrained LM
  3. Use LM head to get [MASK] token probabilities for each candidate answer
  4. Compute class logits by averaging over templates and taking max over candidates
  5. Predict class with highest logit

- Design tradeoffs:
  - More prompt templates → better robustness but higher computation
  - More candidate answers → slight robustness gain but increased memory usage
  - Single template vs. multiple templates → trade-off between simplicity and robustness

- Failure signatures:
  - Poor clean accuracy: likely due to suboptimal prompt templates or candidate answers
  - Low robust accuracy: may indicate model is overfitting to clean data or prompt templates are too weak
  - High variance across runs: could be due to small batch size or insufficient training epochs

- First 3 experiments:
  1. Compare clean and robust accuracy of MVP vs. MLP-FT on a small dataset (e.g., BoolQ)
  2. Ablate number of prompt templates (1 vs. 4) and measure impact on robust accuracy
  3. Test MVP with semantically random vs. semantically related candidate answers to confirm insignificance of semantic choice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms in MVP lead to the observed robustness gains beyond the random parameter vulnerability and pretraining task alignment?
- Basis in paper: [explicit] The paper discusses random parameter vulnerability and pretraining task alignment as two hypotheses, but also mentions conducting ablations to investigate the mechanism underlying the robustness gains.
- Why unresolved: The ablations study in the paper shows that the number of candidate answers and prompt templates have some impact on robustness, but the paper does not provide a definitive answer on the specific mechanisms that lead to the robustness gains.
- What evidence would resolve it: A detailed analysis of the learned representations and decision boundaries in MVP models compared to MLP-FT models could provide insights into the specific mechanisms leading to robustness gains.

### Open Question 2
- Question: How do the robustness gains of MVP translate to other types of adversarial attacks beyond word substitution attacks?
- Basis in paper: [inferred] The paper primarily focuses on word substitution attacks and does not extensively explore other types of adversarial attacks.
- Why unresolved: The paper does not provide evidence on how MVP performs against other types of adversarial attacks, such as character-level misspellings or imperceptible homoglyphs.
- What evidence would resolve it: Conducting experiments with different types of adversarial attacks and comparing the performance of MVP and MLP-FT models would provide insights into the generalizability of the robustness gains.

### Open Question 3
- Question: How does the sample efficiency of MVP change with different dataset sizes and complexities?
- Basis in paper: [explicit] The paper discusses the sample efficiency of MVP compared to MLP-FT in low-data regimes but does not explore how it changes with different dataset sizes and complexities.
- Why unresolved: The paper provides evidence on the sample efficiency of MVP in specific low-data regimes but does not provide a comprehensive analysis of how it changes with varying dataset sizes and complexities.
- What evidence would resolve it: Conducting experiments with datasets of different sizes and complexities and comparing the sample efficiency of MVP and MLP-FT models would provide insights into the scalability of the robustness gains.

## Limitations
- The mechanisms explaining robustness gains rely heavily on indirect evidence rather than direct causal experiments
- The claim about semantic candidate answers being inconsequential is supported by weak anecdotal evidence
- The paper does not systematically test whether pretraining-task alignment is necessary for MVP's robustness gains

## Confidence
- High Confidence: MVP improves robust accuracy over MLP-FT by ~8% (directly measured and reproducible)
- Medium Confidence: Removing randomly initialized MLP head improves robustness (supported by ablation but lacks theoretical justification)
- Low Confidence: Proposed mechanisms (pretraining alignment, semantic candidate inconsequence) rely on theoretical reasoning rather than direct empirical validation

## Next Checks
1. Conduct controlled study where MLP head is initialized with meaningful weights to test if random initialization is the true causal mechanism
2. Remove pretraining entirely and train from scratch with MVP vs MLP-FT to test necessity of pretraining-task alignment
3. Systematically test MVP with candidate answers spanning from semantically related to completely random words to determine if there's any threshold effect rather than complete inconsequentiality