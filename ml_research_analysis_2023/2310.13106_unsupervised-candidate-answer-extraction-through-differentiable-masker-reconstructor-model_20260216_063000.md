---
ver: rpa2
title: Unsupervised Candidate Answer Extraction through Differentiable Masker-Reconstructor
  Model
arxiv_id: '2310.13106'
source_url: https://arxiv.org/abs/2310.13106
tags:
- tokens
- candidate
- answer
- methods
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of unsupervised candidate answer
  extraction for question generation systems, where traditional methods rely on linguistic
  rules or supervised learning with partially annotated data. The authors propose
  a Differentiable Masker-Reconstructor (DMR) model that leverages self-consistency
  learning to identify information tokens as candidate answers without relying on
  annotations or external tools.
---

# Unsupervised Candidate Answer Extraction through Differentiable Masker-Reconstructor Model

## Quick Facts
- arXiv ID: 2310.13106
- Source URL: https://arxiv.org/abs/2310.13106
- Authors: Multiple authors
- Reference count: 12
- One-line primary result: DMR outperforms unsupervised methods and matches supervised methods on candidate answer extraction

## Executive Summary
This paper addresses unsupervised candidate answer extraction for question generation systems, proposing a Differentiable Masker-Reconstructor (DMR) model that identifies information tokens as candidate answers without annotations. The model uses self-consistency learning to distinguish between backbone tokens (easily recoverable) and information tokens (hard to recover) through a differentiable Masker-Reconstructor architecture. Evaluated on exhaustively-annotated datasets from SQuAD and WikiHow cooking texts, DMR outperforms other unsupervised methods and achieves comparable performance to supervised approaches, demonstrating effectiveness in extracting candidate answers in an unsupervised manner.

## Method Summary
The DMR model consists of a Masker module that classifies tokens for preservation/masking and a Reconstructor module that recovers masked tokens using MLM. The key innovation is using the Straight-Through Gumbel-Softmax estimator to make the Masker's binary classification differentiable, enabling self-consistency learning where the model learns to mask tokens that the Reconstructor struggles to recover. A length penalty ensures effective masking, and the model is trained without any annotated data. The approach leverages the inherent structure of domain-specific text to identify information tokens as candidate answers.

## Key Results
- DMR outperforms other unsupervised methods by +0.05 F1 on exhaustively-annotated SQuAD and +1.13 F1 on WikiHow cooking texts
- DMR achieves comparable performance to supervised methods despite being fully unsupervised
- Performance gains are more substantial on specialized domains (WH-C) compared to open-domain data (SQuAD)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DMR identifies information tokens by exploiting the inherent structure of context passages through differentiable self-consistency learning
- Mechanism: Masker learns to mask tokens that Reconstructor can readily recover, with differentiable learning enabled by Straight-Through Gumbel-Softmax estimator
- Core assumption: Domain-specific text has inherent structure where some tokens are more recoverable than others
- Evidence anchors: [abstract] leverages inherent structure through DMR with self-consistency; [section] passage tokens categorized into backbone (easily recoverable) and information (hard to recover) tokens
- Break condition: If domain lacks sufficient structure or backbone tokens are not significantly more recoverable

### Mechanism 2
- Claim: Differentiable self-consistency learning enables model to learn without annotated data
- Mechanism: Gumbel-Softmax estimator makes Masker's binary classification differentiable, allowing gradients from reconstruction loss to flow back
- Core assumption: Reconstruction loss can effectively guide Masker to identify information tokens without labels
- Evidence anchors: [abstract] differentiable learning enabled by Straight-Through Gumbel-Softmax estimator; [section] addresses non-differentiability with Gumbel-Softmax
- Break condition: If reconstruction loss is not informative or Gumbel-Softmax approximation is poor

### Mechanism 3
- Claim: DMR performance is domain-dependent with better results on specialized domains
- Mechanism: Specialized domains have clearer structure that DMR can capture through self-consistency learning
- Core assumption: Domains with clearer templates yield better DMR performance
- Evidence anchors: [abstract] demonstrates effectiveness; [section] performance gain more substantial on WH-C (specialized cooking domain)
- Break condition: If domain lacks clear structure or model cannot capture domain-specific patterns

## Foundational Learning

- Concept: Self-consistency learning
  - Why needed here: Enables model to learn without annotated data by enforcing consistency between original and reconstructed inputs
  - Quick check question: Can you explain how self-consistency learning differs from traditional supervised learning?

- Concept: Differentiable approximation of discrete operations
  - Why needed here: Allows gradients to flow through binary mask decisions made by Masker module
  - Quick check question: What is the role of the Straight-Through Gumbel-Softmax estimator in making Masker's decisions differentiable?

- Concept: Masked Language Modeling (MLM)
  - Why needed here: Provides foundation for Reconstructor module's ability to predict masked tokens
  - Quick check question: How does MLM enable bidirectional context understanding compared to traditional language models?

## Architecture Onboarding

- Component map:
  Input passage → Masker module → Intermediate masked passage → Reconstructor module → Reconstructed passage → Loss calculation → Gradient update

- Critical path: Input → Masker → Intermediate → Reconstructor → Loss calculation → Gradient update

- Design tradeoffs:
  - Token-level vs. constituent-level masking: Token-level is simpler but may fragment answers; constituent-level could preserve answer integrity but is more complex
  - Mask percentage: Too few masks yield weak signals; too many masks make reconstruction impossible
  - Model size: Larger models may capture more complex patterns but require more resources

- Failure signatures:
  - Low recall: Model is too conservative in masking (λ too low)
  - Low precision: Model masks non-information tokens (λ too high or reconstructor is too capable)
  - No learning: Gradients not flowing through Masker (Gumbel-Softmax implementation issue)
  - Mode collapse: Model always masks same tokens (insufficient diversity in training data)

- First 3 experiments:
  1. Ablation study: Remove Gumbel-Softmax and use hard binary decisions to confirm differentiability is necessary
  2. Lambda sweep: Test different λ values to find optimal mask/reconstruction balance
  3. Domain transfer: Test DMR trained on SQuAD on new domain (e.g., news articles) to measure generalization

## Open Questions the Paper Calls Out

- Question: What specific mechanisms within the DMR model could be improved to increase precision of candidate answer extraction?
  - Basis in paper: [explicit] Paper discusses low precision and suggests exploring alternative encoding mechanisms
  - Why unresolved: Paper identifies issue but doesn't provide specific mechanisms or methods to address it
  - What evidence would resolve it: Empirical results showing improved precision after implementing alternative encoding mechanisms

- Question: How does DMR model's performance vary across different domains and what factors contribute to this variation?
  - Basis in paper: [inferred] Paper mentions WH-C dataset's higher F1 gain due to specialized cooking domain structure
  - Why unresolved: Paper doesn't provide detailed analysis of cross-domain performance or contributing factors
  - What evidence would resolve it: Comparative performance data across multiple domains with analysis of domain-specific factors

- Question: What are potential effects of incorporating partially annotated answers into DMR model's training process?
  - Basis in paper: [explicit] Paper acknowledges availability of partially annotated answers and suggests exploring incorporation with DMR model
  - Why unresolved: Paper doesn't explore or provide results on effects of incorporating partially annotated answers
  - What evidence would resolve it: Experimental results demonstrating impact of incorporating partially annotated answers on performance

## Limitations

- Performance on open-domain data may not reflect real-world question generation scenarios where only gold answers are annotated
- Model's effectiveness across diverse domains is uncertain due to heavy reliance on domain-specific patterns
- Token-level masking may fragment answers, impacting downstream question generation quality

## Confidence

- **High confidence**: Technical implementation with Straight-Through Gumbel-Softmax estimator is well-defined and core mechanism is sound
- **Medium confidence**: Comparative performance against other unsupervised methods is supported by experimental results
- **Low confidence**: Claims about effectiveness across diverse real-world domains and practical utility for production systems

## Next Checks

1. **Cross-domain transfer test**: Train DMR on SQuAD and evaluate on completely different domain (e.g., news articles or scientific papers) without fine-tuning. Measure F1 drop compared to domain-specific training to quantify domain dependency.

2. **Answer fragmentation analysis**: For each predicted answer span, calculate percentage of tokens correctly recovered versus total span length. Compare this fragmentation metric between DMR and other methods to quantify practical impact of token-level masking.

3. **Semi-supervised learning comparison**: Create experiments with 1%, 5%, and 10% labeled data to compare DMR against semi-supervised approaches. This would establish whether fully unsupervised nature provides sufficient advantage over methods leveraging small amounts of labeled data.