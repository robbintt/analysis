---
ver: rpa2
title: Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation
  Synergy
arxiv_id: '2305.15294'
source_url: https://arxiv.org/abs/2305.15294
tags:
- retrieval
- knowledge
- question
- generation
- iteration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Iter-RetGen, a method that synergizes retrieval
  and generation in an iterative manner to enhance retrieval-augmented large language
  models (LLMs). The key idea is that a model's output provides informative context
  for retrieving more relevant knowledge, which in turn helps generate a better output
  in the next iteration.
---

# Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy

## Quick Facts
- **arXiv ID**: 2305.15294
- **Source URL**: https://arxiv.org/abs/2305.15294
- **Reference count**: 27
- **Primary result**: Iter-RetGen achieves superior or competitive performance on multi-hop QA, fact verification, and commonsense reasoning tasks by iteratively synergizing retrieval and generation.

## Executive Summary
This paper presents Iter-RetGen, a method that enhances retrieval-augmented large language models through iterative synergy between retrieval and generation. The key innovation is using a model's complete output as informative context for retrieving more relevant knowledge in subsequent iterations, rather than interleaving retrieval with partial generation. The method processes all retrieved knowledge as a whole, preserving generation flexibility while avoiding structural constraints. Iter-RetGen is evaluated on six datasets across three task types, showing consistent performance improvements over state-of-the-art retrieval-augmented baselines. The paper also demonstrates that exact match metrics can significantly underestimate LLM performance in question answering tasks.

## Method Summary
Iter-RetGen implements an iterative retrieval-generation loop where each iteration uses the complete model output from the previous step concatenated with the original question to retrieve more relevant knowledge. The method uses dense retrieval (Contriever-MSMARCO) with Chain-of-Thought prompting, processing all retrieved paragraphs together rather than interleaving retrieval with generation. An optional generation-augmented retrieval adaptation stage distills knowledge from a re-ranker (TART) with access to model generations into the dense retriever, improving its ability to capture semantic gaps. The approach is evaluated under a 3-shot setting across six datasets using exact match, F1, accuracy, and LLM-evaluated correctness metrics.

## Key Results
- Iter-RetGen consistently outperforms state-of-the-art retrieval-augmented baselines on multi-hop QA, fact verification, and commonsense reasoning tasks
- The second iteration provides the greatest performance boost, with diminishing returns in subsequent iterations
- Generation-augmented retrieval adaptation enables higher accuracy with fewer iterations
- Exact match metrics can significantly underestimate model performance, with improvements in EM not always reflecting improvements in model generations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative retrieval-generation synergy improves relevance modeling by leveraging model outputs as informative retrieval contexts
- Mechanism: Complete model outputs from previous iterations are concatenated with questions to retrieve more relevant knowledge, bridging semantic gaps
- Core assumption: A complete model output provides more informative context for retrieval than partial generation or forward-looking sentences
- Evidence anchors: [abstract] "A model output shows what might be needed to finish a task"; [section 3.2] "LLM output from the previous iteration...shows what might be needed to synthesize an answer"

### Mechanism 2
- Claim: Processing all retrieved knowledge as a whole preserves generation flexibility compared to interleaved retrieval-generation
- Mechanism: Bulk processing of retrieved knowledge maintains the model's ability to reason holistically rather than being constrained by incremental retrieval
- Core assumption: Processing knowledge in bulk rather than incrementally preserves effective synthesis of information
- Evidence anchors: [abstract] "processes all retrieved knowledge as a whole and largely preserves the flexibility in generation"; [section 1] "fail to process all retrieved knowledge as a whole during the generation process"

### Mechanism 3
- Claim: Generation-augmented retrieval adaptation improves performance while reducing iteration count
- Mechanism: Knowledge distillation transfers relevance modeling capabilities from a re-ranker with model generation access to a dense retriever with only question access
- Core assumption: A re-ranker with generation access can better capture relevance than a retriever with question-only access
- Evidence anchors: [section 3.4] "distill knowledge from a re-ranker with access to model generations to a dense retriever"; [section 4.5] "retrieval adaptation enables ITER-RETGEN to achieve significantly higher Acc† with fewer iterations"

## Foundational Learning

- **Dense retrieval with dual encoders**: Why needed here - The paper uses Contriever-MSMARCO which encodes queries and documents separately and computes similarity via inner product. Quick check: How does the similarity score between a query and document get calculated in dense retrieval?

- **Knowledge distillation in retrieval**: Why needed here - Generation-augmented retrieval adaptation uses knowledge distillation from re-ranker to dense retriever. Quick check: What is the training objective for distilling knowledge from a re-ranker to a retriever?

- **Chain-of-Thought prompting**: Why needed here - The paper uses CoT prompting except that it prepends retrieved knowledge to the question. Quick check: How does Chain-of-Thought prompting differ from direct prompting in terms of output format?

## Architecture Onboarding

- **Component map**: Dense retriever (Contriever-MSMARCO) -> LLM (text-davinci-003) -> Re-ranker (TART) -> Knowledge distillation module
- **Critical path**: 1) Initial retrieval using question only; 2) First generation with retrieved knowledge (CoT prompting); 3) Iterative retrieval using previous generation + question; 4) Subsequent generations with updated retrieved knowledge; 5) (Optional) Generation-augmented retrieval adaptation
- **Design tradeoffs**: More iterations improve performance but increase computational cost; bulk processing preserves flexibility but may miss fine-grained retrieval needs; generation-augmented adaptation improves performance but adds complexity
- **Failure signatures**: Performance plateaus or degrades with additional iterations; low answer recall in retrieved paragraphs; high variance in EM vs Acc†
- **First 3 experiments**: 1) Implement single-iteration baseline (CoT prompting with retrieval); 2) Add second iteration to measure performance gains; 3) Implement generation-augmented retrieval adaptation to evaluate performance-cost tradeoffs

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the iterative retrieval-augmented generation method's performance scale with the number of iterations beyond what was tested in the paper? The paper only tested up to 7 iterations and does not provide evidence of performance improvements beyond this point.

- **Open Question 2**: Can the generation-augmented retrieval adaptation be effectively applied to other types of retrievers, such as sparse retrievers, in addition to the dense retriever used in the paper? The paper demonstrates effectiveness for dense retrievers but does not explore applicability to other retriever types.

- **Open Question 3**: How does the quality of the in-context demonstrations affect the performance of the iterative retrieval-augmented generation method? The paper mentions annotated demonstrations are used but does not explore how demonstration quality impacts performance.

## Limitations

- The method relies on model outputs as retrieval context, which have "no guarantee of correctness" and could create feedback loops reinforcing incorrect retrievals
- Limited direct comparison with state-of-the-art interleaved retrieval-generation methods makes it difficult to fully assess claimed advantages
- Knowledge distillation approach lacks detailed validation of re-ranker effectiveness compared to standard dense retrieval

## Confidence

- **High Confidence**: The core mechanism of using model outputs to improve retrieval relevance is well-supported by theoretical reasoning and consistent empirical results across multiple datasets
- **Medium Confidence**: The advantage of bulk knowledge processing over interleaved approaches is plausible but under-validated due to limited direct comparisons
- **Medium Confidence**: The effectiveness of generation-augmented retrieval adaptation is demonstrated empirically but lacks sufficient detail on the distillation process and re-ranker contribution

## Next Checks

1. **Feedback Loop Analysis**: Implement a controlled experiment injecting synthetic noise into model generations to measure effects on subsequent retrieval quality and final performance, validating robustness to incorrect intermediate outputs.

2. **Ablation of Knowledge Processing Strategy**: Compare the current bulk processing approach against a hybrid method that interleaves retrieval for some iterations and processes knowledge as a whole for others to quantify the true cost-benefit tradeoff.

3. **Re-ranker Contribution Isolation**: Run experiments comparing the generation-augmented retriever against both the original dense retriever and a strong re-ranker baseline to isolate the specific contribution of the knowledge distillation approach.