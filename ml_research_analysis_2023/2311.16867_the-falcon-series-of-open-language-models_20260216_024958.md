---
ver: rpa2
title: The Falcon Series of Open Language Models
arxiv_id: '2311.16867'
source_url: https://arxiv.org/abs/2311.16867
tags:
- data
- language
- arxiv
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Falcon series comprises three large-scale causal decoder-only
  language models (7B, 40B, and 180B parameters) pretrained on a diverse high-quality
  corpus predominantly sourced from web data. The largest model, Falcon-180B, was
  trained on over 3.5 trillion tokens of text, the largest openly documented pretraining
  run.
---

# The Falcon Series of Open Language Models

## Quick Facts
- arXiv ID: 2311.16867
- Source URL: https://arxiv.org/abs/2311.16867
- Reference count: 33
- One-line primary result: Falcon-180B is one of the top three language models globally, alongside GPT-4 and PaLM-2-Large, while being openly released under a permissive license

## Executive Summary
The Falcon series comprises three large-scale causal decoder-only language models (7B, 40B, and 180B parameters) pretrained on a diverse high-quality corpus predominantly sourced from web data. The largest model, Falcon-180B, was trained on over 3.5 trillion tokens of text, the largest openly documented pretraining run. It significantly outperforms models such as PaLM and Chinchilla, and improves upon concurrently developed models like LLaMA 2 and Inflection-1. Falcon-180B nears the performance of PaLM-2-Large at reduced pretraining and inference costs, making it one of the top three language models globally, alongside GPT-4 and PaLM-2-Large. The models are released under a permissive license to foster open-science and accelerate the development of an open ecosystem of large language models.

## Method Summary
The Falcon models use a causal decoder-only architecture based on PaLM with multigroup attention, rotary positional embeddings, and no biases in linear layers. Training employs a distributed strategy combining 3D parallelism with ZeRO optimizer sharding on up to 4,096 A100 GPUs. The models are trained on a dataset of 3.5 trillion tokens created from web data processed through the RefinedWeb pipeline with stringent filtering and deduplication, supplemented with curated data including books, conversations, technical sources, and code.

## Key Results
- Falcon-180B achieves performance close to PaLM-2-Large while reducing pretraining and inference costs
- The RefinedWeb dataset approach (filtered web data) outperforms both curated corpora and other web-based datasets like C4 and OSCAR
- Multigroup attention reduces KV cache size and improves inference scalability without significant performance loss
- Falcon models are released under a permissive license to foster open-science and ecosystem development

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using multigroup attention instead of vanilla multi-head attention reduces inference memory footprint and improves scalability without significant performance loss.
- **Mechanism:** In multigroup, keys and values are shared within each tensor-parallel group instead of globally, reducing KV cache size from O(âˆšN log n) to O(log N). This enables training on cheaper hardware with less interconnect.
- **Core assumption:** The reduction in parameters from sharing KV pairs is not fully compensated for in the architecture, but the benefits of improved inference scalability outweigh this small degradation.
- **Evidence anchors:**
  - [abstract]: "We report on our custom distributed training codebase, allowing us to efficiently pretrain these models on up to 4,096 A100s on cloud AWS infrastructure with limited interconnect."
  - [section]: "Table 9: Even without controlling for the reduction in parameters, multiquery only comes at a limited zero-shot performance cost...Multigroup with KV=8 consistently performs close to the vanilla baseline."
  - [corpus]: "Weak evidence. The corpus contains references to multiquery and multigroup attention, but no explicit evaluation of memory footprint reduction or inference speedup."
- **Break condition:** If the reduction in parameters causes a significant degradation in performance, or if the hardware does not benefit from the reduced KV cache size (e.g., if inference is not a bottleneck).

### Mechanism 2
- **Claim:** Training on filtered and deduplicated web data alone can match or outperform models trained on curated corpora.
- **Mechanism:** Stringent filtering removes low-quality and machine-generated content, while deduplication eliminates redundancy. This results in a high-quality dataset that is orders of magnitude larger than curated datasets, enabling better generalization.
- **Core assumption:** The filtering and deduplication process does not remove too much relevant information, and the remaining data is representative of the language distribution.
- **Evidence anchors:**
  - [abstract]: "Falcon-180B significantly outperforms models such as PaLM or Chinchilla, and improves upon concurrently developed models such as LLaMA 2 or Inflection-1."
  - [section]: "Table 4: Curation is not a silver bullet for zero-shot generalization: small-scale models trained on RefinedWeb outperform models trained on web data (C4, OSCAR), and on curated corpora (The Pile)."
  - [corpus]: "Weak evidence. The corpus mentions the RefinedWeb dataset and its filtering/deduplication process, but does not provide explicit comparisons with curated datasets."
- **Break condition:** If the filtering and deduplication process removes too much relevant information, or if the remaining data is not representative of the language distribution.

### Mechanism 3
- **Claim:** The Falcon architecture, with its focus on scalability and efficiency, enables training on a massive scale without sacrificing performance.
- **Mechanism:** The architecture incorporates multigroup attention for improved inference scalability, rotary positional embeddings for better performance, and custom Triton kernels for efficient computation. The distributed training strategy combines 3D parallelism with ZeRO optimizer sharding to reduce memory consumption and improve scalability.
- **Core assumption:** The architectural tweaks do not significantly degrade performance, and the distributed training strategy is effective on the target hardware.
- **Evidence anchors:**
  - [abstract]: "Falcon-180B nears the performance of PaLM-2-Large at a reduced pretraining and inference cost."
  - [section]: "Table 14: Our data recipe, predominantly based on our work on RefinedWeb (Penedo et al., 2023), significantly improves upon The Pile and other models from the state-of-the-art. Because multiquery makes models smaller (and we do not control for that effect), our architecture comes with a small zero-shot performance degradation."
  - [corpus]: "Weak evidence. The corpus mentions the Falcon architecture and its components, but does not provide explicit evaluations of scalability or performance."
- **Break condition:** If the architectural tweaks cause a significant degradation in performance, or if the distributed training strategy is not effective on the target hardware.

## Foundational Learning

- **Concept:** Transformer architecture and attention mechanism
  - Why needed here: Understanding the core building blocks of the Falcon models and how they process input data.
  - Quick check question: What is the difference between multi-head attention and multigroup attention, and how does this affect the model's performance and scalability?

- **Concept:** Distributed training and parallelism strategies
  - Why needed here: Understanding how the Falcon models are trained on large clusters of GPUs and how this affects performance and efficiency.
  - Quick check question: What are the different types of parallelism used in the Falcon training, and how do they contribute to scalability?

- **Concept:** Data preprocessing and filtering techniques
  - Why needed here: Understanding how the RefinedWeb dataset is created and how this affects the quality and diversity of the training data.
  - Quick check question: What are the key steps in the RefinedWeb pipeline, and how do they ensure high-quality data for training?

## Architecture Onboarding

- **Component map:** Falcon architecture -> PaLM architecture with modifications -> Multigroup attention, Rotary positional embeddings, GeLU activation, Parallel attention and MLP blocks, Tied embeddings, Vocabulary size of 65,024

- **Critical path:** The critical path for training a Falcon model involves: 1. Data preprocessing and filtering to create the RefinedWeb dataset 2. Model architecture definition and hyperparameter tuning 3. Distributed training on a large cluster of GPUs 4. Evaluation on a range of tasks to assess performance

- **Design tradeoffs:** The Falcon architecture makes several tradeoffs to balance performance, scalability, and efficiency: - Multigroup attention reduces KV cache size but also reduces the number of parameters - Rotary positional embeddings are slightly more computationally expensive than ALiBi but offer better performance - GeLU activation is less memory-intensive than SwiGLU but may have slightly lower performance

- **Failure signatures:** Common failure modes in Falcon training include: - Spikes in loss during training, which can be addressed by resuming from a pre-spike checkpoint and skipping a few billion tokens - Hardware failures, which can be mitigated by running tests on startup and monitoring for NaNs during training - Communication issues, which can be addressed by optimizing rank placement and using scatter-gather optimizations

- **First 3 experiments:**
  1. Train a small-scale model (1B or 3B parameters) on a subset of RefinedWeb to validate the architecture and hyperparameters
  2. Evaluate the model on a range of tasks to assess performance and identify areas for improvement
  3. Scale up the model size and training data to match the full Falcon recipe and evaluate performance again

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Falcon series' performance improvement primarily come from data quality or architectural modifications?
- Basis in paper: Explicit comparison in Section 4.5 between data recipe and architecture validation experiments
- Why unresolved: The paper acknowledges architecture improvements mainly deliver hardware scalability benefits, while data improvements significantly uplift downstream performance. However, a clear quantitative separation of each factor's contribution is missing.
- What evidence would resolve it: A controlled experiment isolating architecture changes while keeping data constant, and vice versa, would quantify each factor's impact on final model performance.

### Open Question 2
- Question: How would increasing the fraction of code in pretraining data affect Falcon's downstream code generation capabilities?
- Basis in paper: Implicit suggestion in Section 7.2 about conservative code fraction (3%) and potential for 10-30% code data
- Why unresolved: The paper mentions Falcon-180B nearly matches specialized code models despite limited code pretraining, but doesn't explore what higher code fractions would achieve.
- What evidence would resolve it: Training models with varying code fractions (5%, 15%, 25%) and evaluating on code-specific benchmarks would show the relationship between code pretraining percentage and code generation performance.

### Open Question 3
- Question: What is the optimal pretraining duration for Falcon models when considering both upstream and downstream performance?
- Basis in paper: Explicit discussion in Section 7.2 about decoupling training and inference compute, and recommendation for 4,000-15,000B tokens
- Why unresolved: The paper trained models close to Hoffmann et al.'s (2022) pretraining optimality, but acknowledges this makes deployment challenging. The trade-off between upstream performance gains and downstream inference costs isn't fully explored.
- What evidence would resolve it: Systematic training of models with varying pretraining lengths (1,000B, 4,000B, 10,000B, 20,000B tokens) and measuring both upstream language modeling performance and downstream task performance would identify the sweet spot.

## Limitations

- The multigroup attention architecture comes with a "small zero-shot performance degradation" that isn't fully controlled for in comparisons with other models
- Performance comparisons with GPT-4 and PaLM-2-Large rely on potentially non-aligned evaluation protocols
- The exact implementation details of the custom distributed training codebase and its optimizations for memory efficiency and throughput are not fully detailed in the public material

## Confidence

- Performance claims: Medium confidence - strong results but comparisons rely on potentially non-aligned evaluation protocols
- Data methodology: High confidence - well-documented RefinedWeb pipeline and filtering approach
- Scalability claims: Medium confidence - theoretically sound but exact memory optimizations not fully detailed

## Next Checks

1. Replicate the multigroup attention implementation and measure actual KV cache size reduction and inference speedup on target hardware
2. Train a small-scale model using only the RefinedWeb data (no curated data) and compare directly against the same model trained on curated corpora to validate the dataset claims
3. Implement the described 3D parallelism strategy on a smaller GPU cluster and measure memory efficiency and training throughput compared to standard approaches