---
ver: rpa2
title: On the Impact of Calibration Data in Post-training Quantization and Pruning
arxiv_id: '2311.09755'
source_url: https://arxiv.org/abs/2311.09755
tags:
- calibration
- data
- compression
- quantization
- frantar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first empirical study on the impact of
  calibration data on the performance of large language models (LLMs) under post-training
  quantization and pruning. The authors systematically investigate how the choice
  of calibration data affects downstream task performance across various quantization
  and pruning techniques, datasets, tasks, and models.
---

# On the Impact of Calibration Data in Post-training Quantization and Pruning

## Quick Facts
- arXiv ID: 2311.09755
- Source URL: https://arxiv.org/abs/2311.09755
- Reference count: 13
- One-line primary result: Calibration data significantly impacts post-training quantization and pruning performance across tasks and models

## Executive Summary
This paper presents the first empirical study on how calibration data affects the performance of large language models under post-training quantization and pruning. The authors systematically investigate how different calibration data choices influence downstream task performance across various compression techniques, datasets, tasks, and model sizes. Surprisingly, the results show substantial variations in accuracy across different calibration sets, challenging existing assumptions about calibration data robustness. The study reveals that certain tasks are more sensitive to calibration data choices, and that larger models tend to be more robust to these variations.

## Method Summary
The study applies four model compression methods (GPTQ and SpQR for quantization; SparseGPT and Wanda for pruning) to LLaMA and Vicuna models (7B and 13B variants) using calibration sets of 128 examples with 2,048 tokens each. The calibration sets are sampled from five different source datasets: C4, CNN-DM, RedPajama, RefinedWeb, and Wikipedia. The compressed models are evaluated on nine downstream tasks using the EleutherAI Evaluation Harness. Performance is measured by mean accuracy and standard deviation across ten distinct calibration sets for each configuration.

## Key Results
- Calibration data significantly impacts model performance under post-training quantization and pruning
- Larger models (13B) show greater robustness to calibration data variations than smaller models (7B)
- Performance variations across calibration sets can be substantial, with some tasks showing higher sensitivity than others
- Certain data sources consistently perform better as calibration data across multiple compression methods and tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Calibration data directly influences layer activation distributions used in post-training compression, thereby affecting model performance on downstream tasks.
- Mechanism: During post-training quantization and pruning, calibration data generates layer activations that inform the compression method how to best quantize or prune weights. Different calibration data leads to different activation distributions, causing variations in the compressed model's behavior and downstream task accuracy.
- Core assumption: The layer-wise compression problem relies heavily on the statistics derived from calibration data activations, and small changes in these distributions can propagate through the network to affect task performance.
- Evidence anchors:
  - [abstract] "They rely upon calibration data, a small set of unlabeled examples that are used to generate layer activations."
  - [section] "Post-training compression considers the scenario where the model must be compressed without any further training, instead relying upon a small amount of calibration data (Nagel et al., 2020; Hubara et al., 2021)."
- Break condition: If calibration data is sufficiently large and diverse to cover the activation space comprehensively, or if the model architecture has sufficient redundancy that small variations in activation distributions do not affect downstream task performance.

### Mechanism 2
- Claim: Larger models exhibit lower sensitivity to calibration data variations due to increased parameter redundancy.
- Mechanism: As model size increases, the number of parameters provides more degrees of freedom, allowing the model to maintain performance despite variations in calibration data-induced activation distributions. Smaller models have less redundancy and are therefore more sensitive to such variations.
- Core assumption: The additional parameters in larger models act as a buffer, absorbing the impact of different activation distributions without significantly affecting downstream task performance.
- Evidence anchors:
  - [section] "We observe that 7B parameter models exhibit a lower level of dispersion than their 13B counterparts... This suggests that larger models may be more robust to the exact calibration data."
- Break condition: If the calibration data variations are extreme enough to affect the fundamental activation patterns that the model relies upon, or if the model lacks sufficient redundancy due to architectural constraints.

### Mechanism 3
- Claim: Different source datasets for calibration data lead to varying performance levels because they capture different statistical properties relevant to downstream tasks.
- Mechanism: Calibration data from different sources (e.g., web text, news articles, encyclopedic text) has distinct statistical properties. These properties influence how well the compressed model generalizes to specific downstream tasks, leading to performance variations across different calibration data sources.
- Core assumption: The statistical properties of calibration data that are most relevant for compression optimization align differently with the requirements of various downstream tasks.
- Evidence anchors:
  - [section] "Certain data sources perform better overall. Table 1 presents the mean accuracy and standard deviation of ten distinct calibration sets sampled from each source dataset."
- Break condition: If the downstream tasks are sufficiently diverse that no single calibration data source can be optimal for all of them, or if the compression method is robust enough to handle variations in calibration data source statistics.

## Foundational Learning

- Concept: Layer-wise compression problem
  - Why needed here: Understanding how calibration data affects post-training quantization and pruning requires knowledge of the layer-wise compression problem, where compressed weights are selected to function closely to original weights with respect to calibration data.
  - Quick check question: What is the objective of the layer-wise compression problem in post-training quantization and pruning?

- Concept: Activation distribution statistics
  - Why needed here: The mechanism by which calibration data influences model performance relies on how activation distribution statistics from different data sources affect the compression process.
  - Quick check question: How do activation distribution statistics from calibration data influence the quantization or pruning process?

- Concept: Model redundancy and robustness
  - Why needed here: Understanding why larger models are less sensitive to calibration data variations requires knowledge of how model redundancy contributes to robustness against small changes in activation distributions.
  - Quick check question: Why might larger models with more parameters be less sensitive to variations in calibration data?

## Architecture Onboarding

- Component map: Calibration data generation -> Model compression methods (quantization/pruning) -> Evaluation on downstream tasks
- Critical path: Generate calibration data → Apply compression method → Evaluate on downstream tasks. This path is critical because each step directly influences the next, and variations at any stage can affect the final performance.
- Design tradeoffs: Using more diverse calibration data may improve robustness but increases computational cost. Choosing between quantization and pruning involves tradeoffs between compression ratio and performance retention.
- Failure signatures: High variance in performance across different calibration sets indicates sensitivity to calibration data. Poor performance on specific tasks suggests that the calibration data may not capture relevant statistical properties for those tasks.
- First 3 experiments:
  1. Compare performance of a compression method using calibration data from different sources (e.g., C4 vs. Wikipedia) to identify sensitivity to data source.
  2. Vary the size of calibration data (e.g., 64, 128, 256 examples) to determine the point of diminishing returns and optimal calibration set size.
  3. Test the same compression method on models of different sizes (e.g., 7B vs. 13B) to observe the impact of model size on sensitivity to calibration data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the sensitivity to calibration data vary across different model architectures beyond the LLaMA and Vicuna families?
- Basis in paper: [inferred] The paper focuses on LLaMA and Vicuna models but does not explore whether other architectures (e.g., GPT, BERT, or newer models) exhibit similar calibration data sensitivity.
- Why unresolved: The study is limited to a specific set of models, leaving uncertainty about generalizability to other architectures.
- What evidence would resolve it: Systematic experiments testing calibration data sensitivity across diverse model architectures, including both decoder-only and encoder-decoder models, would clarify whether this phenomenon is universal or model-specific.

### Open Question 2
- Question: How does the choice of calibration data affect the robustness of post-training quantization and pruning methods in real-world deployment scenarios with noisy or adversarial inputs?
- Basis in paper: [explicit] The paper highlights sensitivity to calibration data but focuses on controlled zero-shot evaluation tasks, not real-world deployment conditions.
- Why unresolved: The study does not address how calibration data choices impact model robustness under practical, noisy, or adversarial conditions.
- What evidence would resolve it: Experiments evaluating compressed models on noisy or adversarial datasets, alongside varying calibration data, would reveal whether calibration data choices impact robustness in deployment scenarios.

### Open Question 3
- Question: Can advanced techniques, such as domain adaptation or meta-learning, be used to create more robust calibration sets that minimize sensitivity across tasks and models?
- Basis in paper: [inferred] The paper identifies sensitivity to calibration data but does not explore methods to mitigate this issue, such as domain adaptation or meta-learning.
- Why unresolved: The study does not investigate whether techniques like domain adaptation or meta-learning can reduce sensitivity to calibration data.
- What evidence would resolve it: Comparative experiments testing advanced calibration techniques (e.g., domain adaptation or meta-learning) against traditional random sampling would determine if these methods can create more robust calibration sets.

## Limitations
- The study is limited to English-language tasks and models, limiting generalizability to other languages.
- The calibration set size (128 examples) and token count (2,048 tokens) are fixed, though these parameters could affect results.
- The evaluation is restricted to 7B and 13B parameter models, leaving uncertainty about how findings scale to smaller or larger architectures.

## Confidence
High confidence: The empirical findings regarding calibration data sensitivity across different sources and tasks are well-supported by systematic experiments.

Medium confidence: The recommendation that calibration data should be released with compressed models is reasonable but depends on practical constraints around data licensing and privacy.

## Next Checks
1. Test the sensitivity findings on multilingual models and tasks to determine if calibration data effects generalize across languages.
2. Systematically vary calibration set sizes beyond 128 examples (e.g., 32, 256, 512) to identify the precise scaling relationship between calibration data volume and performance stability.
3. Evaluate additional compression methods not included in this study (e.g., QLoRA, AWQ) to assess whether calibration sensitivity is consistent across the broader compression method landscape.