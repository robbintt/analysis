---
ver: rpa2
title: Accelerated Rates between Stochastic and Adversarial Online Convex Optimization
arxiv_id: '2303.03272'
source_url: https://arxiv.org/abs/2303.03272
tags:
- regret
- convex
- bound
- stochastic
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies online convex optimization in a setting that
  interpolates between stochastic i.i.d. and fully adversarial losses.
---

# Accelerated Rates between Stochastic and Adversarial Online Convex Optimization

## Quick Facts
- arXiv ID: 2303.03272
- Source URL: https://arxiv.org/abs/2303.03272
- Authors: 
- Reference count: 40
- Key outcome: Achieves regret bounds that interpolate between stochastic and adversarial regimes by exploiting smoothness of expected loss functions, replacing dependence on maximum gradient norm with gradient variance and adversarial variation

## Executive Summary
This paper bridges the gap between stochastic and adversarial online convex optimization by developing optimistic algorithms that achieve accelerated rates. The key insight is that when expected loss functions are smooth, the regret can be bounded in terms of the variance of gradients and adversarial variation rather than the maximum gradient norm. This yields regret bounds of the form O(D(σ̄T + Σ̄T)√T + LD²) for convex functions, which interpolate between the O(Dσ̄T√T) stochastic rate and the O(DG√T) adversarial rate. The authors extend these results to strongly convex functions and dynamic regret settings.

## Method Summary
The paper analyzes optimistic variants of Follow-the-Regularized-Leader (FTRL) and Mirror Descent (MD) for online convex optimization. The key innovation is using adaptive step-size tuning that automatically adjusts to the difficulty of the learning task. For convex functions, the OFTRL algorithm uses an adaptive step size ηt that depends on observed gradient differences, allowing it to automatically balance between stochastic and adversarial regimes. For strongly convex functions, the paper employs a meta-learning framework that maintains a distribution over worker algorithms with different step sizes, updating this distribution based on regret performance. The optimistic prediction mechanism (using mt = gt-1) reduces uncertainty in gradient estimates, particularly effective when gradients have low variance.

## Key Results
- Achieves regret bound O(D(σ̄T + Σ̄T)√T + LD²) for convex functions, interpolating between stochastic and adversarial rates
- Extends results to strongly convex functions with regret O((D(σ̄T + Σ̄T) + D²/μ)√T)
- Provides dynamic regret bounds for drifting distributions
- Demonstrates applicability to random order models and adversarially corrupted stochastic data
- Shows that only knowledge of diameter D is required, with all other parameters (L, σ, Σ) being adaptively estimated

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Smoothness of expected loss functions allows replacing dependence on maximum gradient norm with dependence on gradient variance and adversarial variation.
- **Mechanism:** When expected losses are L-smooth, the negative Bregman divergence term in the regret analysis can be exploited to cancel terms involving the square of differences between consecutive iterates, allowing the regret bound to depend on σ² (variance of gradients) and Σ² (adversarial variation) instead of G² (maximum gradient norm).
- **Core assumption:** The expected loss functions Ft(x) = E[ξ~Dt][f(x,ξ)] are L-smooth, i.e., their gradients are L-Lipschitz continuous.
- **Evidence anchors:**
  - [abstract] states "By exploiting smoothness of the expected losses, these bounds replace a dependence on the maximum gradient length by the variance of the gradients"
  - [section 3.1] shows the regret bound for optimistic FTRL: E[RT(u)] ⩽ (27 + 6√2)LD² + (2 + 4√2)D(σ̄T + Σ̄T)√T
  - [corpus] shows related work "Optimistic Online Mirror Descent for Bridging Stochastic and Adversarial Online Convex Optimization" which likely uses similar smoothness-based techniques
- **Break condition:** If expected losses are not smooth (L = 0), the improvement from G to σ and Σ disappears, and the algorithm degrades to the standard worst-case regret bound.

### Mechanism 2
- **Claim:** Optimistic prediction of gradients (using mt = gt-1) enables tighter regret bounds by reducing the impact of gradient uncertainty.
- **Mechanism:** By using an optimistic guess of the next gradient (the previous gradient), the algorithm can better anticipate the direction of descent, reducing the "uncertainty" term in the regret analysis. This is particularly effective when gradients have low variance, as the optimistic guess is close to the true gradient.
- **Core assumption:** The sequence of distributions Dt doesn't change too rapidly, so that gt-1 is a reasonable predictor of gt.
- **Evidence anchors:**
  - [section 3.1] uses mt = gt-1 in the OFTRL update: xt = argmin{x∈X}⟨x, mt + ∑s=1t-1 gs⟩ + ||x||²/(2ηt)
  - [abstract] mentions "optimistic online algorithms" and shows the regret bound depends on both stochastic variance and adversarial variation
  - [corpus] shows related work on optimistic methods, suggesting this is a known effective technique
- **Break condition:** If the distributions Dt change too rapidly (large Σ), the optimistic prediction becomes poor, and the benefit of optimism diminishes.

### Mechanism 3
- **Claim:** Adaptive step-size tuning (via AdaHedge-style method) automatically adjusts to the difficulty of the learning task, balancing between stochastic and adversarial regimes.
- **Mechanism:** The algorithm doesn't need to know the values of L, σ, or Σ in advance. Instead, it adaptively tunes the step-size ηt based on observed gradient differences, effectively learning the appropriate learning rate for the current environment (stochastic vs. adversarial).
- **Core assumption:** The algorithm can observe gradients gt = ∇f(xt,ξt) in each round and adjust accordingly.
- **Evidence anchors:**
  - [section 3.1] shows the adaptive step-size formula: ηt = D²/((∑s=1t-1 ηs||gs - ms||²) ∧ D||gs - ms||) - 1
  - [abstract] states "The algorithm needs only the knowledge of D" and doesn't require prior knowledge of constants
  - [corpus] shows related work on adaptive methods, suggesting this is a known effective technique
- **Break condition:** If the environment changes too rapidly or unpredictably, the adaptive method may not have enough time to adjust, leading to suboptimal performance.

## Foundational Learning

- **Concept: Online Convex Optimization (OCO)**
  - Why needed here: This paper extends OCO to interpolate between stochastic and adversarial settings, so understanding the standard OCO framework is essential.
  - Quick check question: What is the goal of an OCO algorithm, and how is it measured?

- **Concept: Smoothness of functions**
  - Why needed here: The key mechanism for improving regret bounds relies on L-smoothness of expected loss functions.
  - Quick check question: What does it mean for a function to be L-smooth, and how does this property relate to the Lipschitz continuity of its gradient?

- **Concept: Optimistic algorithms in online learning**
  - Why needed here: The paper analyzes optimistic variants of mirror descent and FTRL, which are crucial for achieving the improved regret bounds.
  - Quick check question: How does an optimistic algorithm differ from a standard online learning algorithm, and what is the intuition behind using optimistic predictions?

## Architecture Onboarding

- **Component map:**
  - Loss functions f(·,ξt) → Gradient observation gt = ∇f(xt,ξt) → Optimistic prediction mt (typically gt-1) → Adaptive step-size ηt → Next iterate xt+1 (via OFTRL or OMD) → Regret accumulation

- **Critical path:**
  1. Receive loss function f(·,ξt) and observe gradient gt = ∇f(xt,ξt)
  2. Make optimistic prediction mt (typically mt = gt-1)
  3. Update step-size ηt adaptively based on observed gradient differences
  4. Compute next iterate xt+1 using OFTRL or OMD update rule
  5. Repeat until T rounds are completed

- **Design tradeoffs:**
  - Optimistic prediction vs. stability: More aggressive optimism can lead to faster convergence in stochastic regimes but may be unstable in highly adversarial settings
  - Adaptive step-size vs. computational overhead: Adaptive methods require more computation per round but automatically adjust to the environment
  - Smoothness assumption vs. generality: The smoothness-based improvement only applies when expected losses are smooth, which may not hold in all applications

- **Failure signatures:**
  - If expected losses are not smooth (L = 0), the algorithm degrades to standard worst-case bounds
  - If distributions Dt change too rapidly (large Σ), the optimistic prediction becomes poor and regret increases
  - If gradients have high variance (large σ), the improvement from σ to G diminishes

- **First 3 experiments:**
  1. **Synthetic stochastic test:** Generate i.i.d. losses with small gradient variance σ and verify the O(Dσ√T) regret bound
  2. **Synthetic adversarial test:** Generate fully adversarial losses and verify the O(DG√T) regret bound
  3. **Mixed setting test:** Generate losses with moderate stochastic variance and adversarial variation, and verify the interpolation between the two regimes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the regret bounds for random order models be improved under weaker assumptions, such as those in Sherman et al. (2021), which relax the requirement of convexity for individual losses?
- Basis in paper: [inferred] The paper notes that their results are complementary to those of Garber et al. (2020) and Sherman et al. (2021), who focus on relaxing the convexity assumption for individual losses.
- Why unresolved: The paper only considers the case where individual losses are convex, and does not explore whether their results can be extended to the weaker assumptions used in Sherman et al. (2021).
- What evidence would resolve it: A proof that the regret bounds can be extended to the weaker assumptions, or a counterexample showing that this is not possible.

### Open Question 2
- Question: Can stronger dynamic regret bounds be obtained for distribution drifts in the case of strongly convex functions?
- Basis in paper: [inferred] The paper discusses dynamic regret bounds for drifting distributions, but notes that these bounds do not provide tracing guarantees and suggests that combining their work with that of Zhao and Zhang (2020) could be interesting.
- Why unresolved: The paper only provides dynamic regret bounds for convex functions and does not explore the case of strongly convex functions.
- What evidence would resolve it: A proof of dynamic regret bounds for strongly convex functions under distribution drift, or a counterexample showing that this is not possible.

### Open Question 3
- Question: How do the regret bounds for the adversarially corrupted stochastic model compare to those obtained using other methods, such as those in Ito (2021) or Amir et al. (2020)?
- Basis in paper: [explicit] The paper derives regret bounds for the adversarially corrupted stochastic model and compares them to the results of Ito (2021), but does not compare them to other methods.
- Why unresolved: The paper does not provide a comprehensive comparison of regret bounds for the adversarially corrupted stochastic model.
- What evidence would resolve it: A comparison of regret bounds for the adversarially corrupted stochastic model obtained using different methods, or a proof that one method is superior to others in terms of regret bounds.

## Limitations
- The analysis relies heavily on the smoothness assumption of expected loss functions, which may not hold in many practical applications
- The algorithm requires access to two samples per round (ξt and ξ̂t-1), which may be impractical in some settings
- The strongly convex extension requires knowledge of the strong convexity parameter μ or uses a computationally expensive meta-learning framework

## Confidence
- **High confidence:** The regret bounds in terms of D, σ̄T, and Σ̄T are mathematically correct and rigorously proven
- **Medium confidence:** The adaptive step-size tuning procedure works well in practice, though its theoretical guarantees may be sensitive to parameter choices
- **Medium confidence:** The optimistic prediction mechanism provides consistent improvements across different regimes, though the benefit diminishes as adversarial variation increases

## Next Checks
1. **Smoothness sensitivity test:** Systematically evaluate algorithm performance as the smoothness parameter L varies from 0 to L*, quantifying the degradation when the smoothness assumption is violated
2. **Sample complexity validation:** Test the algorithm with varying numbers of samples per round (1 vs 2) to verify the claimed sample complexity requirements
3. **Cross-over point analysis:** Identify the precise transition point between stochastic and adversarial regimes where optimistic methods outperform pessimistic approaches, and validate this empirically across different problem instances