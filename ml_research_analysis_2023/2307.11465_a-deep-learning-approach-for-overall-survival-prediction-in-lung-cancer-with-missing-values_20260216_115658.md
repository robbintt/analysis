---
ver: rpa2
title: A Deep Learning Approach for Overall Survival Prediction in Lung Cancer with
  Missing Values
arxiv_id: '2307.11465'
source_url: https://arxiv.org/abs/2307.11465
tags:
- time
- survival
- data
- patients
- missing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of overall survival (OS) prediction
  in non-small cell lung cancer (NSCLC) with missing clinical data. The authors propose
  a novel transformer-based approach that can handle missing features without requiring
  any imputation strategy, unlike previous methods.
---

# A Deep Learning Approach for Overall Survival Prediction in Lung Cancer with Missing Values

## Quick Facts
- arXiv ID: 2307.11465
- Source URL: https://arxiv.org/abs/2307.11465
- Reference count: 34
- Primary result: Transformer-based survival model with missing-value masking achieves Ct-index scores of 71.97 (1mo), 77.58 (1yr), 80.72 (2yr) on NSCLC data

## Executive Summary
This paper addresses overall survival prediction in non-small cell lung cancer with missing clinical data using a transformer-based approach. The key innovation is adapting transformer encoders to tabular data with a novel positional encoding and masking mechanism that handles missing features without requiring imputation. The model is trained with a composite loss function designed for survival analysis that accounts for both censored and uncensored patients. Experimental results show significant improvements over state-of-the-art survival analysis models across different time granularities.

## Method Summary
The proposed method adapts transformer encoders to tabular clinical data by using a d × (d+1) one-hot positional encoding matrix where the first d columns encode feature identity and the last column contains actual feature values. Missing features are represented as empty vectors and masked in the self-attention mechanism by setting their weights to negative infinity. The model is trained using a composite loss function combining temporal loss L1 (capturing death and censoring times) and concordance-based loss L2 (enforcing correct risk ordering). The architecture consists of 12 transformer layers, 19 attention heads, and 3072-dimensional feed-forward networks.

## Key Results
- Transformer-based model achieves Ct-index scores of 71.97, 77.58, and 80.72 for time units of 1 month, 1 year, and 2 years respectively
- Outperforms state-of-the-art survival analysis models across different time granularities and imputation strategies
- Handles missing clinical data without requiring imputation, unlike previous methods
- Validated on CLARO NSCLC dataset with 297 patients and 8 clinical descriptors

## Why This Works (Mechanism)

### Mechanism 1
Masking missing features in self-attention avoids imputation while preserving model expressiveness. The transformer encoder uses a masking vector in attention computation where missing features are assigned −∞ weights, forcing the attention mechanism to ignore them entirely. Core assumption: attention's weighted sum remains valid when some weights are zeroed via masking. Evidence: draws from padding mask technique in NLP to prevent illegal connections. Break condition: if positional encoding fails to distinguish features or mask is incorrectly applied.

### Mechanism 2
Composite loss function L = L1 + L2 enables learning from both censored and uncensored patients while enforcing correct risk ordering. L1 maximizes predicted risk at event time for uncensored patients and minimizes cumulative incidence for censored ones. L2 uses pairwise ranking loss to penalize incorrect risk orderings. Core assumption: both patient types contain valuable signal and pairwise comparison suffices for C-index optimization. Evidence: L1 evaluates output and cumulative incidence at patient-specific times while L2 employs concordance-based ranking. Break condition: if event distribution is highly skewed or censoring is informative.

### Mechanism 3
Positional encoding without value imputation preserves feature identity while allowing transformer to focus on available information. Uses d × (d+1) one-hot encoding where first d columns represent positional encoding and last column reports feature vector, with missing features as empty vectors. Core assumption: transformer's attention can learn to rely on positional encoding to identify present features even when value vector is missing. Evidence: explores one-hot encoding vector representing feature position. Break condition: if missingness is very high or feature set is large, positional encoding may become too sparse.

## Foundational Learning

- Concept: Transformer attention with masking
  - Why needed here: Core innovation adapts transformer's masked self-attention to ignore missing values without imputation
  - Quick check question: In masked attention computation, what value is assigned to missing features to ensure they are ignored?
    - Answer: −∞ (negative infinity) in attention logits before softmax

- Concept: C-index and time-dependent concordance (Ct-index)
  - Why needed here: Model evaluated on ability to correctly order patients by risk over time, not just at fixed time point
  - Quick check question: What is the difference between C-index and Ct-index in survival analysis?
    - Answer: C-index evaluates risk ordering at fixed time, while Ct-index evaluates ordering over entire time span using acceptable pairs

- Concept: Cumulative incidence function vs survival function
  - Why needed here: Loss function uses both to handle censored and uncensored data appropriately
  - Quick check question: How does cumulative incidence function differ from survival function?
    - Answer: Cumulative incidence F(t) is probability of event occurrence by time t, while S(t) = 1 - F(t) is probability of surviving past time t

## Architecture Onboarding

- Component map: Input → Positional encoding → Masked self-attention → Feed-forward → Output vector → Loss computation
- Critical path: Input → Positional encoding → Masked self-attention → Feed-forward → Output vector → Loss computation
- Design tradeoffs:
  - Masking vs imputation: Masking avoids bias from imputation but requires learning from incomplete signals; imputation may introduce bias but gives denser gradients
  - Positional encoding size: Larger d+1 encoding increases model capacity but also parameter count and overfitting risk on small datasets
  - Time granularity: Finer time units increase output vector size and task complexity; coarser units simplify but lose temporal resolution
- Failure signatures:
  - Low Ct-index despite high training loss reduction suggests overfitting or incorrect mask application
  - Similar performance to mean imputation baselines indicates transformer may not learn useful feature interactions
  - Slow/unstable convergence suggests learning rate or batch size mismatch to dataset size
- First 3 experiments:
  1. Verify mask application: Run with small synthetic dataset with known missing patterns and inspect attention weights
  2. Ablation of positional encoding: Replace one-hot encoding with zero-padding and measure impact on Ct-index
  3. Loss term isolation: Train with only L1 and only L2 separately to confirm each contributes to final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed transformer-based model for handling missing data in survival analysis be effectively extended to other medical domains beyond lung cancer?
- Basis in paper: Authors mention future directions include exploring performance in other task domains like classification and regression
- Why unresolved: Current study only evaluates on NSCLC data, so generalizability to other medical domains is unknown
- What evidence would resolve it: Testing model on survival analysis datasets from other cancer types or medical conditions with missing data

### Open Question 2
- Question: What is the impact of different missing data mechanisms (e.g., MCAR, MAR, MNAR) on performance of proposed transformer-based approach compared to traditional imputation methods?
- Basis in paper: Authors note missing data imputation poses challenges in selecting appropriate approaches and their method "smoothly copes" by learning from available features
- Why unresolved: Study does not explicitly analyze how different missing data mechanisms affect proposed model's performance compared to imputation methods
- What evidence would resolve it: Experiments where data is artificially made missing under different mechanisms and comparing transformer-based approach to various imputation strategies

### Open Question 3
- Question: How does performance of proposed transformer-based model scale with increasing dimensionality and complexity of input features in survival analysis?
- Basis in paper: Authors adapt transformer architecture for tabular data and use positional encoding for feature identification, suggesting potential scalability
- Why unresolved: Study uses relatively small dataset with 8 clinical descriptors, so model's performance on high-dimensional data is unknown
- What evidence would resolve it: Evaluating model on larger datasets with more features, such as combining clinical data with imaging or genomic data

## Limitations
- Exact positional encoding mechanism for feature identity without explicit value imputation is not fully detailed
- Performance claims rely on single clinical dataset (CLARO) with 297 patients, limiting generalizability
- Interaction between L1 and L2 loss terms and their relative weighting is not explored

## Confidence

- High: Transformer masking effectively handles missing values without imputation (supported by ablation vs mean imputation)
- High: Composite loss captures both censored and uncensored patient signals (explicit design and ablation results)
- Medium: Positional encoding without value imputation preserves feature identity (mechanism plausible but implementation details sparse)
- Medium: Ct-index improvements are statistically significant (reported but not validated with multiple datasets)

## Next Checks

1. Replicate the ablation study comparing missing-value handling strategies (masking vs mean imputation vs feature deletion) on an independent NSCLC dataset to confirm Ct-index gains

2. Perform sensitivity analysis on positional encoding dimension and masking threshold to identify optimal configurations for datasets with varying missingness rates

3. Conduct error analysis on the hardest-to-order patient pairs (lowest pairwise concordance) to identify whether model fails due to missingness patterns or inherent risk ambiguity