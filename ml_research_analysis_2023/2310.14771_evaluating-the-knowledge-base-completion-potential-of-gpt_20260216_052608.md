---
ver: rpa2
title: Evaluating the Knowledge Base Completion Potential of GPT
arxiv_id: '2310.14771'
source_url: https://arxiv.org/abs/2310.14771
tags:
- knowledge
- gpt-3
- language
- precision
- work
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the potential of large language models (LLMs)
  to complete the Wikidata knowledge base (KB). The authors evaluate GPT models (GPT-3,
  GPT-4, and ChatGPT) using a dataset of randomly sampled facts from Wikidata.
---

# Evaluating the Knowledge Base Completion Potential of GPT

## Quick Facts
- arXiv ID: 2310.14771
- Source URL: https://arxiv.org/abs/2310.14771
- Reference count: 12
- Key outcome: GPT-3 can extend Wikidata by 27 million facts at 90% precision through simple thresholding

## Executive Summary
This work investigates the potential of large language models (LLMs) to complete the Wikidata knowledge base (KB). The authors evaluate GPT models (GPT-3, GPT-4, and ChatGPT) using a dataset of randomly sampled facts from Wikidata. While these models do not achieve the high precision typically required for KB completion out-of-the-box, the authors show that with simple thresholding, GPT-3 can extend Wikidata by 27 million facts at 90% precision. This represents a significant improvement over earlier approaches with smaller LMs. The authors also find that GPT-3 performs particularly well on language-related and socio-demographic relations.

## Method Summary
The study evaluates GPT models for knowledge base completion using the WD-KNOWN dataset (4M statements for 3M subjects in 41 relations). The approach involves prompting GPT models with few-shot examples, evaluating predictions using retain-all and precision-thresholding settings, and manually verifying novel predictions. The key innovation is using confidence-based thresholding to achieve high-precision predictions, sorting model outputs by first token probability and retaining only those above 90-95% precision thresholds.

## Key Results
- GPT-3 can extend Wikidata by 27 million facts at 90% precision through thresholding
- Socio-demographic relations (nativeLanguage, citizenship) show particularly strong performance
- Context augmentation negatively impacts high-precision predictions
- Cost per retained statement (~0.7 cents) is lower than manual curation ($2 per statement)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3 can extend Wikidata by millions of facts when precision is prioritized over recall through thresholding.
- Mechanism: By sorting predictions by model confidence (first token probability) and retaining only those above a high-precision threshold, the system achieves 90%+ precision on selected relations while still recovering millions of facts.
- Core assumption: High-precision thresholding filters out the noise without discarding too many true facts, especially for relations with strong surface correlations.
- Evidence anchors:
  - [abstract]: "with simple thresholding, GPT-3 can extend Wikidata by 27 million facts at 90% precision"
  - [section]: "In a second step, the precision-thresholding setting, we therefore sort predictions by confidence and evaluate by recall at precision 95% and 90% (R@P95 and R@P90)"
  - [corpus]: Weak - no direct supporting evidence found in corpus.

### Mechanism 2
- Claim: Socio-demographic relations (e.g., nativeLanguage, citizenship) are easier for GPT-3 to complete accurately.
- Mechanism: These relations exhibit strong surface correlations where entity names often directly indicate properties (e.g., a person's name suggests their nationality), making it easier for the model to predict accurately.
- Core assumption: The model can leverage name-based heuristics to predict these relations with high confidence.
- Evidence anchors:
  - [section]: "Notably, the best-performing relations are mostly related to socio-demographic properties (languages, citizenship)"
  - [section]: "In line with previous results (Veseli et al., 2023), we find that GPT can do well on relations that exhibit high surface correlations"
  - [corpus]: Weak - no direct supporting evidence found in corpus.

### Mechanism 3
- Claim: LM prompting for KBC is cost-effective compared to traditional knowledge base construction methods.
- Mechanism: The cost per retained statement through LM prompting (~0.7 cents after filtering) is lower than manual curation (~2 dollars per statement) or even automated infobox scraping (~1 cent per statement), while offering higher recall potential.
- Core assumption: The monetary cost calculation is accurate and the quality of generated statements is sufficient for practical use.
- Evidence anchors:
  - [section]: "Based on our prompt size (avg. 174 tokens), the cost of one query is about 0.35 ct., with filtering increasing the cost per retained statement to about 0.7 ct."
  - [section]: "Previous works have estimated the cost of KB statement construction at 1 ct. (highly automated infobox scraping) to $2 (manual curation)"
  - [corpus]: Weak - no direct supporting evidence found in corpus.

## Foundational Learning

- Concept: Precision-Recall Trade-off in Knowledge Base Completion
  - Why needed here: The work focuses on high-precision KB completion, which requires understanding how precision and recall interact and how to optimize for precision when needed.
  - Quick check question: Why did the authors choose to focus on high-precision thresholding rather than balanced F1 scores?

- Concept: Few-shot Learning with Language Models
  - Why needed here: The experiments use few-shot prompting to query GPT-3 for knowledge base completion, requiring understanding of how example selection affects model performance.
  - Quick check question: How does the number of few-shot examples affect the cost and performance of KB completion?

- Concept: Knowledge Graph Embeddings and Link Prediction
  - Why needed here: The work compares LM-based approaches to traditional KBC methods like knowledge graph embeddings, requiring understanding of different KBC paradigms.
  - Quick check question: What are the main differences between LM-based KBC and traditional embedding-based approaches?

## Architecture Onboarding

- Component map: WD-KNOWN dataset sampling -> Prompt generation -> GPT-3 API queries -> Confidence sorting -> Thresholding -> Manual verification
- Critical path: Subject-Relation pair -> Prompt generation -> GPT-3 query -> Confidence score -> Threshold filter -> (Manual verification) -> KB insertion
- Design tradeoffs:
  - High-precision vs. high-recall: The system prioritizes precision (90%+) over recall, which limits coverage but ensures quality
  - Automated vs. manual evaluation: Automated evaluation is limited to existing facts, requiring costly manual verification for novel facts
  - Few-shot examples: More examples increase cost but may improve performance; the study found minimal difference beyond 4 examples
- Failure signatures:
  - Low precision across all relations indicates fundamental model limitations for KBC
  - High precision but very low recall suggests overly conservative thresholding
  - Inconsistent performance across relations indicates domain-specific limitations
- First 3 experiments:
  1. Test precision-thresholding on a small set of relations (e.g., nativeLanguage, citizenOf) to verify the mechanism works
  2. Compare different few-shot example sizes (1, 4, 8) to confirm the cost-performance tradeoff
  3. Run context augmentation experiments on select relations to verify the negative impact on high-precision predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GPT-4 and ChatGPT outperform GPT-3 in knowledge base completion tasks with high precision requirements?
- Basis in paper: [explicit] The paper states that none of the GPT models, including GPT-4, achieve high enough accuracy for KB completion out-of-the-box.
- Why unresolved: The paper focuses primarily on GPT-3's performance and does not provide a detailed comparison of GPT-4 and ChatGPT in the context of high-precision KB completion.
- What evidence would resolve it: Conducting a comprehensive evaluation of GPT-4 and ChatGPT's performance in high-precision KB completion tasks and comparing their results with GPT-3's performance.

### Open Question 2
- Question: How can the reliability of language models' confidence estimates be improved for knowledge base completion?
- Basis in paper: [inferred] The paper mentions that the standard prompting approach does not consistently produce high-confidence predictions, and the impact of "Don't know" prompting is unsystematic.
- Why unresolved: The paper highlights the need for further research on calibrating model confidences but does not provide a definitive solution.
- What evidence would resolve it: Developing and evaluating methods to improve the reliability of language models' confidence estimates in knowledge base completion tasks.

### Open Question 3
- Question: How can the addition of novel knowledge by language models be balanced with the need for accurate and scrutable referencing in knowledge bases?
- Basis in paper: [explicit] The paper notes that statement generation is at the core of KB completion, but for a complete KBC pipeline, critical components such as entity disambiguation and scrutable referencing are still missing.
- Why unresolved: The paper acknowledges the importance of these components but does not provide a solution for integrating them into the language model-based KB completion process.
- What evidence would resolve it: Developing and evaluating methods to integrate entity disambiguation and scrutable referencing into language model-based KB completion pipelines.

## Limitations

- Weak evidence anchors for key mechanisms, particularly the cost-effectiveness calculations
- Focus on high-precision thresholding limits coverage and recall
- Manual verification only covers a small sample of millions of predicted facts

## Confidence

**High Confidence Claims:**
- GPT models can achieve high precision (90%+) on KB completion for specific relations when using precision-thresholding
- Socio-demographic relations (nativeLanguage, citizenship) show particularly strong performance
- The approach offers cost advantages over manual curation methods

**Medium Confidence Claims:**
- GPT-3 can extend Wikidata by 27 million facts at 90% precision
- Context augmentation negatively impacts high-precision predictions
- Few-shot example size has minimal impact beyond 4 examples

**Low Confidence Claims:**
- Cost-effectiveness calculations relative to automated infobox scraping
- Generalization of performance across all 41 Wikidata relations
- Scalability of manual verification approach to millions of predictions

## Next Checks

1. **Precision-Recall Tradeoff Validation**: Run controlled experiments varying the precision threshold from 80% to 95% to empirically map the precision-recall curve for key relations, verifying whether the claimed 27 million facts at 90% precision is achievable in practice.

2. **Cost-Effectiveness Verification**: Independently calculate the actual costs of prompting, filtering, and manual verification for a representative sample of predictions, comparing these to the stated costs of alternative KB construction methods.

3. **Generalization Testing**: Evaluate the approach on a broader set of relations beyond socio-demographic properties, particularly focusing on relations that require deep contextual knowledge rather than surface correlations, to assess the limits of the method's applicability.