---
ver: rpa2
title: Fast Approximation of the Shapley Values Based on Order-of-Addition Experimental
  Designs
arxiv_id: '2309.08923'
source_url: https://arxiv.org/abs/2309.08923
tags:
- value
- shapley
- page
- estimates
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the computational challenge of estimating Shapley
  values for large cooperative games by proposing a sampling-based approximation method
  using structured designs from the field of experimental design. The core method
  replaces simple random sampling with Latin squares and component orthogonal arrays
  to sample permutations in a balanced manner, yielding unbiased estimates with smaller
  variance than simple random sampling.
---

# Fast Approximation of the Shapley Values Based on Order-of-Addition Experimental Designs

## Quick Facts
- **arXiv ID**: 2309.08923
- **Source URL**: https://arxiv.org/abs/2309.08923
- **Reference count**: 10
- **Primary result**: Sampling-based approximation using Latin squares and component orthogonal arrays yields unbiased Shapley value estimates with smaller variance than simple random sampling.

## Executive Summary
This paper addresses the computational challenge of estimating Shapley values for large cooperative games by proposing a sampling-based approximation method using structured designs from experimental design. The core innovation replaces simple random sampling with Latin squares and component orthogonal arrays to sample permutations in a balanced manner. This approach yields unbiased estimates with smaller variance than simple random sampling and is faster than existing methods, especially when the number of players is large. The method achieves both variance reduction and computational speed gains through structured sampling.

## Method Summary
The method estimates Shapley values by sampling permutations using component orthogonal arrays (COAs) or Latin squares (LSs) instead of simple random sampling (SRS). For a game with d players, COAs ensure each player appears equally often in each position and each pair of players occupies each pair of positions equally often. The algorithm generates a COA or LS design, computes marginal contributions for each player in each permutation, and averages these contributions to estimate Shapley values. The method is unbiased and has smaller variance than SRS due to the balanced structure of the designs. When d is not a prime power, a null player augmentation trick enables COA construction.

## Key Results
- Unbiased estimation of Shapley values with smaller variance than simple random sampling
- Computational speed gains through reduced sampling overhead (one sampling step vs. d(d-1) steps)
- Deterministic recovery of true Shapley values for certain classes of value functions
- Superior performance compared to existing methods in real data applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Balanced sampling across positions reduces variance by minimizing covariance among marginal contributions.
- Mechanism: COA and LS designs ensure each player appears equally often in each position, and COA additionally ensures each pair of players occupies each pair of positions equally often, reducing covariance among samples.
- Core assumption: Value function regularity allows covariance structure to be exploited for variance reduction.
- Evidence anchors: Abstract claims of smaller variance; section 2.2 on balanced positional structure; corpus evidence is weak.
- Break condition: High discontinuity or non-smoothness in value function can overwhelm balanced design benefits.

### Mechanism 2
- Claim: Computational speed gains from reducing sampling operations.
- Mechanism: COA requires one sampling step to generate d(d-1) permutations, while SRS requires d(d-1) separate random draws.
- Core assumption: Structured design generation cost is comparable to or less than individual SRS permutation generation.
- Evidence anchors: Section 3.1 on sampling efficiency; Table 1 showing COA faster than SRS; corpus evidence limited.
- Break condition: Small d or extremely expensive value function evaluation may make sampling overhead negligible.

### Mechanism 3
- Claim: Deterministic recovery of true Shapley values for certain value functions.
- Mechanism: When marginal contributions depend only on position or pairwise ordering, balanced design ensures sample mean equals population mean exactly.
- Core assumption: Value function structure satisfies conditions in Theorem 2 (marginal contributions depend on position or pairwise ordering).
- Evidence anchors: Theorem 2 on exact recovery; section 2.1 examples of symmetric voting and linear models; corpus lacks direct evidence.
- Break condition: If marginal contributions depend on full set of predecessors rather than positions or pairwise ordering, exact recovery fails.

## Foundational Learning

- **Concept**: Shapley value definition and axioms
  - Why needed: Method builds on estimating Shapley values, so understanding their definition, axioms, and permutation/combination formulations is foundational.
  - Quick check: For a 3-player game, how many marginal contributions are needed to compute a single Shapley value exactly using the permutation formulation?

- **Concept**: Design of experiments (DOE), especially order-of-addition designs
  - Why needed: Proposed method replaces random sampling with structured experimental designs (COA, LS) from DOE; understanding how these designs ensure balance is crucial.
  - Quick check: What property does a Latin square guarantee about the frequency of each element in each row and column?

- **Concept**: Variance reduction via balanced sampling
  - Why needed: Key advantage is reduced variance; knowing how balance affects covariance among samples explains why method works.
  - Quick check: In simple random sampling without replacement, how does the covariance between two sampled observations relate to the population variance?

## Architecture Onboarding

- **Component map**: Value function evaluator -> Design generator (COA/LS) -> Marginal contribution calculator -> Estimator aggregator -> Null player handler (for arbitrary d) -> Monte Carlo fallback (for intractable value functions)

- **Critical path**:
  1. Generate COA or LS design
  2. For each permutation, compute all d marginal contributions
  3. Accumulate contributions per player
  4. Normalize by number of permutations to get Shapley estimates

- **Design tradeoffs**:
  - COA vs LS: COA gives lower variance and exact recovery in more cases but requires d to be prime power; LS works for any d but may have higher variance
  - SRS vs structured designs: SRS is simple and flexible but higher variance; structured designs reduce variance but may be less flexible in design construction
  - Exact vs Monte Carlo value functions: Exact evaluation is precise but may be intractable; Monte Carlo is approximate but scalable

- **Failure signatures**:
  - High variance estimates despite using COA/LS: possible if value function violates balancedness assumption or is extremely noisy
  - Slow performance: may occur if d is large and COA construction is expensive or if value function evaluation is costly
  - Non-convergence: could happen if sample size is too small relative to d or if design is degenerate

- **First 3 experiments**:
  1. Implement a simple voting game (Example 1) and verify that COA yields zero variance estimates for all players
  2. Compare SRS vs COA vs LS on a small linear model (Example 2) to observe variance differences
  3. Test the null player augmentation trick by constructing COA for a non-prime-power d (e.g., d=10) and checking unbiasedness

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the proposed COA and LS-based sampling methods be extended to estimate Shapley values for non-additive value functions or games with more complex dependency structures among players?
  - Basis in paper: Paper primarily focuses on additive or symmetric games and does not extensively explore non-additive scenarios
  - Why unresolved: Theoretical properties and simulations are mostly limited to additive and symmetric cases
  - What evidence would resolve it: Numerical experiments and theoretical analysis for non-additive or complex dependency structures

- **Open Question 2**: How do computational efficiency and estimation accuracy scale with extremely large numbers of players (e.g., d > 1000)?
  - Basis in paper: Shows COA and LS outperform SRS for moderate d, but does not address scalability for very large d
  - Why unresolved: Does not provide empirical results or theoretical bounds for very large d
  - What evidence would resolve it: Empirical studies or theoretical analysis showing performance and computational cost as d grows to thousands

- **Open Question 3**: Can the design-based sampling approach be adapted for online or streaming settings where value function evaluations are expensive or arrive sequentially?
  - Basis in paper: Focuses on batch estimation, but does not consider dynamic or sequential estimation scenarios
  - Why unresolved: Does not address handling new permutations or updates to value function in online setting
  - What evidence would resolve it: Development and testing of online variant of COA or LS method with convergence and efficiency analysis

- **Open Question 4**: What is the impact of choice of specific LS or COA construction method on variance and bias of Shapley value estimates?
  - Basis in paper: Uses specific constructions but does not systematically compare different construction methods
  - Why unresolved: Does not explore how alternative LS or COA constructions might affect estimation performance
  - What evidence would resolve it: Systematic comparison of multiple construction methods including effects on variance and bias

## Limitations
- Method requires d to be a prime power for COA construction, necessitating null player augmentation trick for general d
- Variance reduction benefit most pronounced when value function is sufficiently regular; highly irregular or discontinuous functions may diminish effectiveness
- Performance depends on relative cost of value function evaluation versus sampling overhead

## Confidence
- **High**: Variance reduction claims (supported by both theoretical analysis and experimental results)
- **High**: Computational speed gains (supported by both theoretical analysis and experimental results)
- **Medium**: Exact recovery conditions (applies to specific subclasses of value functions)

## Next Checks
1. Verify COA construction produces balanced designs by checking that each player appears equally often in each position
2. Compare variance of estimates from COA/LS vs SRS on a simple voting game to confirm variance reduction
3. Test null player augmentation trick by constructing COA for d=10 and verifying unbiasedness of estimates