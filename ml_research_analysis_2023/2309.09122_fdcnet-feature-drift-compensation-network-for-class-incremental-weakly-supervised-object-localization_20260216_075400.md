---
ver: rpa2
title: 'FDCNet: Feature Drift Compensation Network for Class-Incremental Weakly Supervised
  Object Localization'
arxiv_id: '2309.09122'
source_url: https://arxiv.org/abs/2309.09122
tags:
- network
- localization
- object
- classes
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the novel problem of class-incremental weakly
  supervised object localization (CI-WSOL), which aims to incrementally learn object
  localization for new classes using only image-level annotations while retaining
  the ability to localize previously learned classes. The proposed method, FDCNet,
  is a strong baseline that adapts strategies from class-incremental classifiers to
  mitigate catastrophic forgetting, including knowledge distillation, maintaining
  a small dataset from previous tasks, and using cosine normalization.
---

# FDCNet: Feature Drift Compensation Network for Class-Incremental Weakly Supervised Object Localization

## Quick Facts
- arXiv ID: 2309.09122
- Source URL: https://arxiv.org/abs/2309.09122
- Reference count: 40
- Primary result: FDCNet outperforms baseline methods on class-incremental weakly supervised object localization across ImageNet-100 and CUB-200 datasets

## Executive Summary
FDCNet addresses the novel problem of class-incremental weakly supervised object localization (CI-WSOL), where a model must learn to localize new object classes over time while retaining the ability to localize previously learned classes. The proposed method combines standard class-incremental learning strategies (knowledge distillation, exemplar sets, cosine normalization) with a novel feature drift compensation network. The FDC network consists of two modules that estimate and compensate for the effects of feature drift on both classification scores and localization maps. Experiments demonstrate that FDCNet establishes strong baseline performance on two benchmark datasets.

## Method Summary
FDCNet builds upon a standard weakly supervised object localization (WSOL) architecture with feature extractor, classifier, and localizer components. For class-incremental learning, it employs four knowledge distillation losses (classification, localization, and feature maps for both), maintains an exemplar set using the herding algorithm, and applies cosine normalization to classifier weights. The novel component is the feature drift compensation (FDC) network, which contains two modules that estimate the effects of feature drift on classification and localization outputs. These modules are trained to compensate for the differences between the current and previous network outputs, helping to preserve knowledge of previously learned classes while adapting to new ones.

## Key Results
- FDCNet achieves higher average incremental accuracy (Accavg) compared to baseline methods on both ImageNet-100 and CUB-200 datasets
- The method shows consistent improvements in last incremental accuracy (Acclast) across all evaluation metrics
- Performance gains are observed for top-1, top-5, and GT-known localization metrics, demonstrating effectiveness for both classification and localization tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The feature drift compensation network mitigates catastrophic forgetting by estimating and compensating for the differences in feature distributions between previous and current tasks.
- Mechanism: FDCNet uses two modules - one for classification and one for localization. These modules map feature maps from the current network to the difference between outputs of the current and previous networks, effectively compensating for the effects of feature drift on both class scores and localization maps.
- Core assumption: The feature drift between tasks is predictable and can be learned by a neural network module.
- Evidence anchors:
  - [abstract] "compensate for the effects of feature drifts on class scores and localization maps"
  - [section] "we propose two modules that estimate the effects of the drifts on classification scores and localization maps, and compensate for them"
- Break condition: If feature drift becomes too complex or non-linear to be captured by the compensation modules, the compensation would fail.

### Mechanism 2
- Claim: Maintaining an exemplar set from previous tasks helps prevent catastrophic forgetting.
- Mechanism: A small subset of representative data from previous tasks is preserved and used during training for the current task, along with the new dataset. This helps maintain knowledge of previously learned classes.
- Core assumption: A small subset of data can effectively represent the full distribution of previously learned classes.
- Evidence anchors:
  - [abstract] "maintaining a small data set from previous tasks"
  - [section] "we also preserve a subset of representative data from previous tasks using the herding algorithm"
- Break condition: If the exemplar set becomes too small or unrepresentative, it may not effectively preserve knowledge of previous classes.

### Mechanism 3
- Claim: Knowledge distillation losses help preserve knowledge of previously learned classes.
- Mechanism: Multiple distillation losses are computed not only using class scores but also localization maps and feature maps. These losses encourage the updated network to produce outputs similar to the previous model.
- Core assumption: The previous model's outputs are good representations of the learned knowledge that should be preserved.
- Evidence anchors:
  - [abstract] "These strategies include applying knowledge distillation, maintaining a small data set from previous tasks"
  - [section] "Different from the networks for classification, a WSOL network needs an additional constraint to less forget localization"
- Break condition: If the new task is too different from previous tasks, forcing the network to match previous outputs might hinder learning new information.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why incremental learning is challenging is crucial for appreciating the need for techniques like FDCNet.
  - Quick check question: What happens to the performance on previously learned tasks when a neural network is trained on new tasks without any special techniques to prevent forgetting?

- Concept: Knowledge distillation
  - Why needed here: Knowledge distillation is a key technique used in FDCNet to preserve knowledge of previously learned classes.
  - Quick check question: How does knowledge distillation encourage a new model to behave similarly to an old model?

- Concept: Weakly supervised learning
  - Why needed here: WSOL is the specific learning paradigm that FDCNet is designed for, where only image-level labels are available instead of detailed bounding box annotations.
  - Quick check question: What is the main challenge in weakly supervised object localization compared to fully supervised object localization?

## Architecture Onboarding

- Component map:
  - WSOL Network: Feature extractor, classifier, localizer
  - FDC Network: Two compensation modules (one for classification, one for localization)
  - Losses: WSOL loss, knowledge distillation losses, FDC losses

- Critical path: Training flow involves first training the WSOL network on each task, then training the FDC network to compensate for feature drift, and finally using both networks together for inference.

- Design tradeoffs:
  - Adding FDC modules increases model complexity but improves retention of previous knowledge
  - Maintaining an exemplar set requires additional memory but helps prevent forgetting
  - Using multiple knowledge distillation losses increases training time but better preserves knowledge

- Failure signatures:
  - If the model forgets previous classes: Check if the exemplar set is too small or unrepresentative
  - If the model fails to learn new classes well: The knowledge distillation might be too strong, preventing adaptation
  - If localization performance degrades: Check if the localization-specific distillation and compensation are effective

- First 3 experiments:
  1. Train the baseline WSOL network on a single task and evaluate its performance on both classification and localization tasks.
  2. Implement the exemplar set and knowledge distillation techniques, then train on two tasks incrementally and compare performance to single-task training.
  3. Add the FDC network and evaluate the performance on previously learned classes after training on new tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FDCNet vary with different sizes of the exemplar set used for knowledge retention across tasks?
- Basis in paper: [explicit] The paper mentions using an exemplar set selected by the herding algorithm to mitigate catastrophic forgetting, but does not explore the impact of different exemplar set sizes.
- Why unresolved: The paper fixes the exemplar set size but does not provide an analysis of how varying this size affects performance.
- What evidence would resolve it: Experiments showing FDCNet performance with different exemplar set sizes (e.g., 5%, 10%, 20% of previous data) would clarify the trade-off between memory usage and performance.

### Open Question 2
- Question: Can the feature drift compensation approach be extended to work with transformer-based architectures for WSOL?
- Basis in paper: [inferred] The paper focuses on CNN-based architectures (MobileNetV1, InceptionV3) but does not explore transformer-based models which are becoming increasingly popular in computer vision.
- Why unresolved: The proposed FDC modules are designed for convolutional feature maps, and their applicability to transformer attention maps and patch embeddings is unexplored.
- What evidence would resolve it: Implementing FDCNet with a transformer-based WSOL architecture (e.g., ViT-based) and comparing performance would demonstrate generalizability.

### Open Question 3
- Question: How does FDCNet perform when object categories across tasks have semantic overlap or hierarchical relationships?
- Basis in paper: [explicit] The paper assumes disjoint class sets between tasks (Ci ∩ Cj = ∅ if i ≠ j), but real-world scenarios often involve semantically related or hierarchical categories.
- Why unresolved: The evaluation assumes completely disjoint categories, which may not reflect practical applications where new tasks include subclasses or related concepts.
- What evidence would resolve it: Experiments on datasets with hierarchical or semantically related class structures (e.g., fine-grained species classification) would reveal how FDCNet handles semantic drift between related categories.

## Limitations

- The paper lacks specific architectural details for the feature extractor and exact hyperparameter values, making direct replication challenging
- The claim that FDCNet is a "strong baseline" is primarily supported by comparison with limited baselines on only two datasets
- While results show improvements, the generalization to other datasets and larger-scale scenarios remains uncertain

## Confidence

- **High Confidence**: The core concept of feature drift compensation and its integration with class-incremental learning is well-explained and theoretically sound
- **Medium Confidence**: The experimental results showing performance improvements over baseline methods are convincing, though limited in scope
- **Low Confidence**: The claim that FDCNet establishes a strong baseline is not fully supported due to limited comparisons with state-of-the-art methods

## Next Checks

1. Reproduce the experiments on a third dataset (e.g., Places365) to verify generalization
2. Implement and compare with more recent CI-WSOL methods to establish true state-of-the-art status
3. Conduct ablation studies to quantify the individual contributions of the FDC modules, knowledge distillation, and exemplar set maintenance