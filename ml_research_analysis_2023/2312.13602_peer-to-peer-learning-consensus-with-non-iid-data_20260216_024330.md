---
ver: rpa2
title: Peer-to-Peer Learning + Consensus with Non-IID Data
arxiv_id: '2312.13602'
source_url: https://arxiv.org/abs/2312.13602
tags:
- local
- performance
- consensus
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Peer-to-peer deep learning algorithms enable distributed edge devices
  to collaboratively train neural networks without exchanging raw data or relying
  on a central server. The study identifies that interleaving local training with
  distributed consensus steps leads to model parameter drift/divergence, causing significant
  oscillations in test performance.
---

# Peer-to-Peer Learning + Consensus with Non-IID Data

## Quick Facts
- arXiv ID: 2312.13607
- Source URL: https://arxiv.org/abs/2312.13607
- Reference count: 18
- Key outcome: P2PL with Affinity dampens test performance oscillations by introducing bias terms during local training and consensus phases, without additional communication costs.

## Executive Summary
This paper addresses the challenge of model drift and performance oscillations in peer-to-peer deep learning with non-IID data. The authors identify that interleaving local training with consensus steps causes model parameters to diverge, leading to significant oscillations in test performance. They propose P2PL with Affinity, which introduces bias terms to regularize local training and consensus phases, effectively dampening these oscillations while maintaining competitive accuracy.

## Method Summary
The study focuses on peer-to-peer deep learning on MNIST digit classification with non-IID data distribution. The P2PL with Affinity algorithm uses multilayer perceptrons (2NN) with local gradient updates and bias terms. The method involves splitting MNIST training data non-IID between two devices, implementing bias terms d (local learning) and b=0 (consensus), and training with local gradient steps followed by consensus. The approach tracks test accuracy on both seen and unseen classes to evaluate performance.

## Key Results
- Interleaving local training with consensus causes model parameter drift and oscillations in test performance
- Oscillations are amplified by non-IID data, more local gradient steps, and higher task complexity
- P2PL with Affinity successfully dampens oscillations without additional communication costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interleaving local gradient updates with consensus steps causes model parameters to diverge (drift apart) during local training, then re-converge during consensus.
- Mechanism: Local training optimizes each device's local objective independently, leading to different parameter trajectories. Consensus averages parameters across neighbors, pulling them back toward each other. This back-and-forth creates oscillations in test performance.
- Core assumption: Local training with non-IID data causes different local objectives to have different minima, leading to divergent parameter updates.
- Evidence anchors:
  - [abstract] "interleaving epochs of training with distributed consensus steps. This process leads to model parameter drift/divergence amongst participating devices"
  - [section] "During local learning, devices independently extract knowledge from their local datasets by performing multiple gradient descent updates according to their local loss functions. This process causes different devices' model parameters to 'drift' apart."
- Break condition: If consensus is performed after every gradient step (DSGD instead of local DSGD), drift is minimized and oscillations reduced.

### Mechanism 2
- Claim: Model drift during local training amplifies oscillations in test performance, especially with non-IID data, more local steps, and complex tasks.
- Mechanism: Local training on non-IID data causes devices to overfit to their local data and "forget" knowledge about other classes. This creates sharp drops in performance on unseen classes during local training, which are partially recovered during consensus when neighbors share their parameters.
- Evidence anchors:
  - [abstract] "We observe that model drift results in significant oscillations in test performance evaluated after local training and consensus phases."
  - [section] "Local training leads to a sharp drop in performance on unseen classes for local DSGD – even to 0% accuracy early in training. Unlike the IID setting...overfitting to local non-IID data leads to devices forgetting how to classify the unseen classes."
- Break condition: If data is IID or if fewer local gradient steps are used between consensus phases.

### Mechanism 3
- Claim: P2PL with Affinity dampens oscillations by introducing bias terms that pull local parameters toward neighbors' parameters during training and toward pre-consensus parameters during consensus.
- Mechanism: The bias terms d and b act as regularization forces that prevent parameters from drifting too far during local training and help maintain knowledge about other classes during consensus.
- Evidence anchors:
  - [section] "Our proposed method successfully dampens oscillations on unseen data by reducing forgetting during local training and improving performance after consensus to match DSGD's performance."
  - [section] "We attribute the improved performance to the local learning bias term d that encourages device A's parameters to be close to its neighbors' parameters."
- Break condition: If bias terms are not properly tuned or if communication graph is sparse.

## Foundational Learning

- Concept: Distributed optimization with consensus
  - Why needed here: Understanding how devices collaborate to optimize a global objective while maintaining local computation
  - Quick check question: What is the difference between DSGD and local DSGD in terms of communication frequency?

- Concept: Non-IID data distributions
  - Why needed here: Heterogeneous data at devices causes different local objectives, leading to model drift and performance oscillations
  - Quick check question: How does non-IID data affect the convergence of distributed learning algorithms compared to IID data?

- Concept: Model drift and its impact on generalization
  - Why needed here: Local training on non-IID data causes devices to forget knowledge about classes they don't see locally
  - Quick check question: Why does local training on non-IID data lead to worse generalization performance on unseen classes?

## Architecture Onboarding

- Component map: Local training -> Consensus -> Repeat until convergence
- Critical path: Local training → Consensus → Repeat until convergence
- Design tradeoffs:
  - More local steps: Less communication but larger oscillations
  - Fewer local steps: More communication but smaller oscillations
  - Non-IID data: Better personalization but harder consensus
  - Complex tasks: Higher accuracy potential but larger oscillations
- Failure signatures:
  - Oscillations don't decay over time: May indicate insufficient communication or poor mixing weights
  - Performance plateaus below target: May indicate excessive model drift or poor initialization
  - Some devices consistently underperform: May indicate poor graph connectivity or initialization
- First 3 experiments:
  1. Run local DSGD on IID MNIST with different numbers of local steps (T=1,5,10) and observe oscillation amplitude
  2. Run local DSGD on non-IID MNIST (two devices, disjoint classes) and measure performance on seen vs unseen classes
  3. Implement P2PL with Affinity and compare oscillation damping vs baseline local DSGD

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the P2PL with Affinity algorithm perform on other deep learning tasks beyond MNIST classification, such as image segmentation or natural language processing?
- Basis in paper: [inferred] The paper focuses on MNIST classification but suggests the algorithm could be applicable to other tasks.
- Why unresolved: The paper only provides empirical results for MNIST classification, leaving the performance on other tasks unexplored.
- What evidence would resolve it: Experiments applying P2PL with Affinity to various deep learning tasks and comparing its performance to other algorithms.

### Open Question 2
- Question: What is the theoretical guarantee for the convergence of P2PL with Affinity, especially in the presence of non-IID data?
- Basis in paper: [inferred] The paper proposes P2PL with Affinity and shows its effectiveness empirically, but does not provide a theoretical analysis.
- Why unresolved: The paper focuses on empirical results and does not delve into the theoretical aspects of the algorithm's convergence.
- What evidence would resolve it: A theoretical analysis of the convergence properties of P2PL with Affinity, including conditions for convergence and rates of convergence.

### Open Question 3
- Question: How does the choice of bias terms in P2PL with Affinity affect the performance, and are there optimal ways to select these terms?
- Basis in paper: [explicit] The paper introduces bias terms in both the learning and consensus phases but does not explore the impact of different choices.
- Why unresolved: The paper presents one possible choice for the bias terms but does not investigate the effect of different choices or provide guidelines for selection.
- What evidence would resolve it: A study comparing the performance of P2PL with Affinity using different bias term choices and an analysis of the factors influencing the optimal selection of these terms.

## Limitations

- The analysis focuses on a pathological non-IID case (disjoint class distributions) which may not generalize to more realistic non-IID scenarios with label distribution skew
- Performance metrics are limited to MNIST classification, lacking evaluation on more complex tasks or real-world datasets
- The bias term implementation details are not fully specified, potentially affecting reproducibility

## Confidence

- **High Confidence**: The core mechanism of model drift causing oscillations during local training is well-established through multiple experiments and theoretical analysis
- **Medium Confidence**: The effectiveness of P2PL with Affinity in dampening oscillations is demonstrated but requires further validation on diverse datasets and architectures
- **Medium Confidence**: The claim that oscillations are amplified by non-IID data, more local steps, and higher task complexity is supported but needs broader empirical validation

## Next Checks

1. Test P2PL with Affinity on CIFAR-10 with realistic non-IID data splits (label distribution skew) to verify generalization beyond the pathological case
2. Evaluate the algorithm's performance on different neural network architectures (CNNs, ResNets) to assess architecture independence
3. Implement alternative bias term formulations to verify that the proposed method's success isn't tied to specific implementation choices