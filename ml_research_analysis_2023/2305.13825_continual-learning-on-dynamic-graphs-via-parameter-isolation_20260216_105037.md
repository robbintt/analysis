---
ver: rpa2
title: Continual Learning on Dynamic Graphs via Parameter Isolation
arxiv_id: '2305.13825'
source_url: https://arxiv.org/abs/2305.13825
tags:
- learning
- graph
- patterns
- pi-gnn
- dynamic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of continual learning on dynamic
  graphs, where the graph structure changes over time and new nodes and edges emerge.
  The proposed method, Parameter Isolation GNN (PI-GNN), overcomes the challenge of
  catastrophic forgetting by isolating parameters that capture stable graph patterns
  and expanding the model with new parameters to learn emerging patterns.
---

# Continual Learning on Dynamic Graphs via Parameter Isolation

## Quick Facts
- **arXiv ID**: 2305.13825
- **Source URL**: https://arxiv.org/abs/2305.13825
- **Reference count**: 40
- **Primary result**: PI-GNN prevents catastrophic forgetting in dynamic graphs by isolating stable parameters and expanding with new ones, achieving superior performance on eight real-world datasets

## Executive Summary
This paper addresses the challenge of continual learning on dynamic graphs where graph structures evolve over time with new nodes and edges emerging. The proposed Parameter Isolation GNN (PI-GNN) method overcomes catastrophic forgetting by identifying stable graph patterns and isolating their corresponding parameters while expanding the model to learn emerging patterns. Through a knowledge rectification stage and parameter isolation approach, PI-GNN demonstrates superior performance compared to state-of-the-art baselines on eight real-world datasets, effectively balancing learning new patterns while maintaining knowledge of previous ones.

## Method Summary
PI-GNN operates in two main stages: knowledge rectification and parameter isolation. The method first identifies stable subgraphs (unchanged by node/edge modifications) and optimizes the model to capture their patterns. It then isolates parameters corresponding to stable patterns by freezing them, while expanding the model with new parameters to learn emerging patterns. To prevent excessive model growth, model distillation is applied to compress the expanded model. The approach is evaluated on eight real-world graph datasets using node classification tasks, with performance measured by both learning ability and forgetting metrics.

## Key Results
- PI-GNN achieves superior Performance Mean (PM) and lower Forgetting Measure (FM) compared to state-of-the-art baselines across all eight datasets
- The method effectively prevents catastrophic forgetting while maintaining competitive performance on new patterns
- Model compression through distillation successfully manages parameter growth without significant performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stable parameters capture graph patterns that remain unchanged across snapshots, while unstable parameters learn emerging patterns.
- Mechanism: The method identifies stable subgraphs (those not affected by node/edge changes) and isolates their parameters, then expands the model to learn new patterns in changed subgraphs.
- Core assumption: Different parameters contribute to learning different graph patterns, and these can be effectively separated.
- Evidence anchors:
  - [abstract] "PI-GNN achieves this by first rectifying the model to obtain stable parameters that maintain performance on existing patterns, and then isolating these stable parameters while expanding the model with new parameters to capture new patterns."
  - [section] "Our motivation lies in that different parameters contribute to learning different graph patterns. Based on the idea, we expand model parameters to continually learn emerging graph patterns. Meanwhile, to effectively preserve knowledge for unaffected patterns, we find parameters that correspond to them via optimization and freeze them to prevent them from being rewritten."
  - [corpus] Weak evidence - no direct citations in related papers supporting this specific mechanism.

### Mechanism 2
- Claim: Knowledge rectification stage ensures stable parameters are optimized for unchanged subgraphs before parameter isolation.
- Mechanism: The model is fine-tuned to exclude the impact of patterns affected by changed nodes and edges, obtaining parameters that maintain performance over existing stable graph patterns.
- Core assumption: Parameters can be optimized to focus on stable subgraphs without being influenced by unstable patterns.
- Evidence anchors:
  - [section] "The knowledge rectification stage is used to update the model to obtain parameters that capture the topological patterns of stable subgraphs."
  - [section] "We fine-tune the model to exclude such impact and get stable parameters that maintain performance over existing stable graph patterns."
  - [corpus] No direct evidence in related papers supporting this specific knowledge rectification approach.

### Mechanism 3
- Claim: Parameter isolation with expansion and compression prevents catastrophic forgetting while managing model size.
- Mechanism: The method freezes stable parameters and expands the model with new parameters to learn new patterns, then uses model distillation to compress the expanded model.
- Core assumption: Model distillation can effectively compress an expanded model without significant performance loss.
- Evidence anchors:
  - [abstract] "To prevent the model from excessive expansion, we use model distillation to compress the model while maintaining its performance."
  - [section] "When too many nodes or edges are deleted in dynamic graphs compared to the previous snapshot, we can distill the model to get a smaller model at this snapshot to avoid the overfitting problem."
  - [corpus] Weak evidence - no direct citations in related papers supporting this specific compression approach.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their message passing mechanisms
  - Why needed here: PI-GNN builds upon GNN architecture and modifies it for continual learning on dynamic graphs
  - Quick check question: How do GNNs aggregate information from neighboring nodes, and what role do parameters play in this process?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: PI-GNN specifically addresses this problem in the context of dynamic graphs
  - Quick check question: What causes catastrophic forgetting in neural networks, and how does parameter isolation help prevent it?

- Concept: Dynamic graph representation and decomposition
  - Why needed here: PI-GNN operates on dynamic graphs by decomposing them into stable and unstable subgraphs
  - Quick check question: How are stable and unstable subgraphs defined in dynamic graphs, and why is this decomposition important for continual learning?

## Architecture Onboarding

- Component map:
  Knowledge Rectification Stage -> Parameter Isolation Stage -> Model Compression

- Critical path:
  1. Identify stable and unstable subgraphs based on node/edge changes
  2. Perform knowledge rectification to obtain stable parameters
  3. Isolate stable parameters and expand model with new parameters
  4. Apply model compression to manage size

- Design tradeoffs:
  - Balance between parameter isolation and model expansion
  - Tradeoff between maintaining stable patterns and learning new patterns
  - Efficiency vs. effectiveness in model compression

- Failure signatures:
  - Poor performance on stable patterns indicates ineffective parameter isolation
  - Rapid increase in model size suggests inadequate compression
  - Fluctuating performance across snapshots may indicate improper knowledge rectification

- First 3 experiments:
  1. Test knowledge rectification on a simple dynamic graph with clearly defined stable and unstable subgraphs
  2. Evaluate parameter isolation and expansion on a graph with gradually changing patterns
  3. Assess model compression effectiveness by comparing performance before and after distillation on a large dynamic graph

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can PI-GNN be extended to other graph learning tasks beyond node classification, such as link prediction or graph classification?
- Basis in paper: [explicit] The paper mentions that PI-GNN can be combined with different GNN backbones and states "Parameter isolation for other tasks (e.g. link prediction) may require minor design modifications (e.g. graph decomposition, choice of samples), and thus we leave them as future work."
- Why unresolved: The paper only evaluates PI-GNN on node classification tasks and does not explore its application to other graph learning tasks.
- What evidence would resolve it: Empirical results demonstrating PI-GNN's performance on link prediction and graph classification tasks, along with modifications to the algorithm for these tasks.

### Open Question 2
- Question: What is the optimal balance between the number of parameters to freeze and the number of parameters to expand when applying parameter isolation in dynamic graphs?
- Basis in paper: [inferred] The paper proposes expanding model parameters to capture new patterns while freezing stable parameters, but does not provide a method for determining the optimal balance between these two aspects.
- Why unresolved: The paper does not discuss how to determine the optimal number of parameters to freeze versus expand, which could significantly impact the model's performance and efficiency.
- What evidence would resolve it: Experimental results showing the performance of PI-GNN with different ratios of frozen to expanded parameters, along with theoretical analysis of the trade-offs involved.

### Open Question 3
- Question: How does the performance of PI-GNN scale with the size and complexity of dynamic graphs, particularly in terms of computational efficiency and memory usage?
- Basis in paper: [explicit] The paper mentions that parameter isolation can lead to dramatically increased parameters for large T and discusses model compression as a solution. It also shows efficiency comparisons with baselines.
- Why unresolved: While the paper demonstrates efficiency on the tested datasets, it does not provide a comprehensive analysis of how PI-GNN scales with larger and more complex dynamic graphs.
- What evidence would resolve it: Empirical results showing PI-GNN's performance and resource usage on increasingly large and complex dynamic graph datasets, along with theoretical analysis of its computational complexity.

## Limitations
- Relies on strong assumptions about parameter independence that may not hold in complex real-world graphs
- Limited discussion of edge cases and potential failure modes in knowledge rectification stage
- Model compression strategy lacks detailed implementation specifics and theoretical justification

## Confidence
- Theoretical framework: Medium
- Experimental validation: High
- Knowledge rectification approach: Medium
- Model compression strategy: Low

## Next Checks
1. Test parameter isolation effectiveness on synthetic graphs with varying levels of pattern interdependence to quantify the breakdown point where the method fails.
2. Conduct ablation studies removing the knowledge rectification stage to isolate its contribution to preventing catastrophic forgetting.
3. Evaluate the method on graphs with different temporal patterns (periodic vs. random changes) to assess robustness across dynamic graph types.