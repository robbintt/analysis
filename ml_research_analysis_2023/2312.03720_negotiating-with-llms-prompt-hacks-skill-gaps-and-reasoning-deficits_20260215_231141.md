---
ver: rpa2
title: 'Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits'
arxiv_id: '2312.03720'
source_url: https://arxiv.org/abs/2312.03720
tags:
- price
- llms
- negotiation
- also
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates human-LLM interaction in competitive price
  negotiations through a user study with over 40 participants across age groups. Using
  ChatGPT Turbo 3.5, participants negotiated car prices with the LLM, revealing significant
  variations in outcomes and strategies.
---

# Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits

## Quick Facts
- arXiv ID: 2312.03720
- Source URL: https://arxiv.org/abs/2312.03720
- Reference count: 4
- Primary result: LLM reasoning deficits and prompt hacking create exploitable vulnerabilities in price negotiations

## Executive Summary
This study investigates human-LLM interaction in competitive price negotiations through a user study with over 40 participants across age groups. Using ChatGPT Turbo 3.5, participants negotiated car prices with the LLM, revealing significant variations in outcomes and strategies. The LLM showed reasoning deficits, such as making non-sensical offers and being susceptible to "reasoning hacks" that exploit contextual misunderstandings. Participants employed various tactics, including inventing weaknesses and prompt hacking, achieving prices ranging from free to well above market value. The findings highlight a literacy gap in effective LLM interaction and underscore risks for both human buyers/sellers and companies employing LLMs in negotiations.

## Method Summary
The study conducted a user study at a public event where over 40 participants across age groups negotiated car prices with ChatGPT Turbo 3.5. Participants used a structured prompt to engage in a negotiation game with the LLM, which was instructed to maximize price as the seller. The study analyzed negotiation outcomes, strategies employed, and reasoning deficits using thematic content analysis and basic text mining. The LLM's susceptibility to reasoning hacks and prompt manipulation was a key focus, with outcomes ranging from free to above-market prices.

## Key Results
- LLM showed reasoning deficits, making nonsensical offers and being susceptible to "reasoning hacks"
- Participants achieved negotiated prices ranging from free to well above market value
- A significant literacy gap exists in effective LLM interaction, with outcomes varying widely based on user prompting skill

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs are susceptible to "reasoning hacks" that exploit contextual misunderstandings during negotiation.
- Mechanism: The LLM's contextual understanding can be manipulated by human participants through repeated claims or invented product weaknesses, leading to unreasonable price concessions.
- Core assumption: LLMs prioritize maintaining conversational flow and agreeing with user inputs over strict adherence to logical consistency or initial instructions.
- Evidence anchors:
  - [abstract] "LLM showed reasoning deficits, such as making non-sensical offers and being susceptible to 'reasoning hacks' that exploit contextual misunderstandings."
  - [section] "Using this strategy, one could elicit an arbitrary amount of money... Inventing a different context such as claiming the car was ultimately used in a country, where it is worthless, could be used to lead the LLM to concessions well below the actual lower value limit."
- Break condition: LLM systems with improved contextual reasoning and fact-checking capabilities that can detect and reject contradictory claims.

### Mechanism 2
- Claim: LLMs demonstrate trust in user claims beyond reasonable levels, leading to exploitable reasoning vulnerabilities.
- Mechanism: The LLM accepts user-provided information about product defects or contextual details without verification, leading to inappropriate price adjustments based on potentially false premises.
- Core assumption: LLMs are designed to be helpful and agreeable, prioritizing user satisfaction over critical evaluation of claims.
- Evidence anchors:
  - [abstract] "susceptiveness to prompt hacking, which intends to manipulate the LLM to make agreements that are against its instructions or beyond any rationality."
  - [section] "The LLM tended to agree to this incorrect allegations and often made further subsequent reasoning errors in such situations."
- Break condition: Implementation of verification mechanisms or skepticism thresholds that require corroboration for claims that significantly impact negotiation outcomes.

### Mechanism 3
- Claim: LLM negotiation outcomes vary widely due to human prompting skill differences, creating a literacy gap.
- Mechanism: Users with better understanding of LLM behavior and prompt engineering techniques achieve significantly better negotiation outcomes than those without such skills.
- Core assumption: Human ability to effectively interact with LLMs varies considerably, and this variation directly impacts negotiation success.
- Evidence anchors:
  - [abstract] "Participants employed various tactics, including inventing weaknesses and prompt hacking, achieving prices ranging from free to well above market value."
  - [section] "The mean value of negotiated prices was about 21,700 USD with a large variance... about Â¼ of all participants did not manage to negotiate a price below the middle value."
- Break condition: Standardization of interaction protocols or development of LLM negotiation interfaces that minimize the impact of user skill differences.

## Foundational Learning

- Concept: Contextual reasoning in language models
  - Why needed here: Understanding how LLMs process and maintain context during multi-turn conversations is crucial for analyzing their negotiation behavior and vulnerabilities.
  - Quick check question: What happens when an LLM receives contradictory information across multiple conversation turns?

- Concept: Prompt engineering techniques
  - Why needed here: Knowledge of how different prompt structures and approaches affect LLM outputs is essential for understanding both successful negotiation strategies and potential exploits.
  - Quick check question: How does changing the framing of a request affect an LLM's response in a negotiation scenario?

- Concept: Human-AI interaction dynamics
  - Why needed here: Understanding the psychological and behavioral aspects of how humans interact with AI systems helps explain the observed literacy gap and negotiation outcome variations.
  - Quick check question: What factors influence human trust in AI system outputs during interactive tasks?

## Architecture Onboarding

- Component map: LLM core reasoning engine -> context management system -> instruction adherence module -> output generation module -> user interaction interface
- Critical path: User input -> context processing -> instruction checking -> reasoning -> output generation -> response delivery
- Design tradeoffs: Flexibility vs. consistency (allowing natural conversation vs. maintaining logical coherence), user satisfaction vs. accuracy (agreeing with users vs. correcting errors)
- Failure signatures: Inconsistent price offers without justification, acceptance of obviously false claims, failure to maintain negotiation constraints
- First 3 experiments:
  1. Test LLM response consistency by presenting the same negotiation scenario with minor variations in user claims
  2. Evaluate price concession patterns by systematically testing different negotiation strategies across multiple runs
  3. Assess context retention by introducing contradictory information at different points in the negotiation conversation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can reasoning flaws in LLMs be systematically mitigated to prevent exploitation during negotiations?
- Basis in paper: [explicit] The paper identifies reasoning deficits and prompt hacking as vulnerabilities in LLMs during negotiations.
- Why unresolved: The study focuses on identifying flaws but does not propose solutions to address these vulnerabilities.
- What evidence would resolve it: Development and testing of enhanced LLM training methods or fine-tuning techniques that reduce susceptibility to reasoning hacks.

### Open Question 2
- Question: How does the use of LLMs in negotiations impact the spread of negotiated prices compared to human-only negotiations?
- Basis in paper: [explicit] The study observes a large variation in negotiated prices, suggesting a potential amplification of price disparities due to LLM interactions.
- Why unresolved: The study does not compare LLM-negotiation outcomes with human-only negotiations.
- What evidence would resolve it: A controlled experiment comparing price outcomes in human-only vs. human-LLM negotiations.

### Open Question 3
- Question: How do different bargaining strategies (e.g., aggressive vs. collaborative) influence the effectiveness of human-LLM negotiations?
- Basis in paper: [explicit] The paper notes that the LLM employed small concessions and aimed for compromises, but does not explore the impact of varying human strategies.
- Why unresolved: The study does not systematically test different human negotiation strategies against the LLM.
- What evidence would resolve it: Experiments where humans are instructed to use specific bargaining strategies and outcomes are analyzed.

### Open Question 4
- Question: What are the long-term societal implications of widespread LLM use in negotiations, particularly regarding fairness and equity?
- Basis in paper: [explicit] The study highlights risks of LLM manipulation and the potential amplification of literacy gaps in AI interaction.
- Why unresolved: The study focuses on immediate negotiation outcomes and does not address broader societal impacts.
- What evidence would resolve it: Longitudinal studies examining the effects of LLM-mediated negotiations on market dynamics and consumer behavior.

## Limitations
- Limited sample size and demographic context may affect generalizability
- Findings based on a single LLM model, raising questions about replication across different architectures
- Mechanisms for LLM vulnerabilities require more systematic testing across negotiation scenarios

## Confidence
- Medium: Core observations about LLM reasoning deficits and prompt hacking are well-supported by specific examples, but generalizability across different LLM architectures and negotiation contexts remains uncertain.

## Next Checks
1. Conduct a controlled experiment varying the LLM model and version to assess whether the observed reasoning deficits and vulnerabilities are consistent across different architectures.
2. Implement a systematic prompt variation study to quantify the relationship between specific prompting techniques and negotiation outcomes, moving beyond qualitative observations to establish causal links.
3. Design a verification mechanism to test whether LLM systems can be enhanced with skepticism thresholds or fact-checking capabilities to resist the prompt hacking techniques described in the study.