---
ver: rpa2
title: 'SANIA: Polyak-type Optimization Framework Leads to Scale Invariant Stochastic
  Algorithms'
arxiv_id: '2312.17369'
source_url: https://arxiv.org/abs/2312.17369
tags:
- sania
- newton
- polyak
- methods
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SANIA, a framework for Polyak-type optimization
  methods that leads to scale invariant stochastic algorithms. The main idea is to
  incorporate preconditioning and Polyak step-size into existing adaptive optimization
  methods like Adam and AdaGrad, making them invariant to scaling transformations.
---

# SANIA: Polyak-type Optimization Framework Leads to Scale Invariant Stochastic Algorithms

## Quick Facts
- **arXiv ID**: 2312.17369
- **Source URL**: https://arxiv.org/abs/2312.17369
- **Reference count**: 40
- **Primary result**: SANIA-based methods achieve scale invariance and outperform standard adaptive optimizers on classification tasks without manual step-size tuning

## Executive Summary
SANIA is a novel optimization framework that integrates Polyak step-size with preconditioning techniques to create scale-invariant stochastic optimization algorithms. The framework modifies existing adaptive methods like Adam and AdaGrad by removing square roots from their preconditioning matrices and incorporating Polyak step-size computation. This combination addresses two key challenges in deep learning optimization: the need for manual learning rate tuning and sensitivity to data scaling. SANIA demonstrates superior performance on binary classification tasks, achieving 100% accuracy on the colon-cancer dataset after 10 epochs without hyperparameter tuning.

## Method Summary
SANIA builds upon existing adaptive optimization methods by modifying their preconditioning matrices and incorporating Polyak step-size. The framework removes square roots from preconditioners like AdaGrad and Adam (creating AdaGrad-SQR and Adam-SQR), making them diagonal and symmetric. The Polyak step-size is computed as (f(wt) - f*) / ||∇f(wt)||², where f* is the optimal function value. The update rule combines these elements: wt+1 = wt - (1/Lt) * B⁻¹t ∇f(wt), where Lt is the Polyak step-size and Bt is the preconditioner. This approach maintains invariance to diagonal scaling transformations while eliminating the need for manual learning rate tuning.

## Key Results
- SANIA achieves 100% accuracy on colon-cancer dataset after 10 epochs without manual step-size tuning
- SANIA Adam-SQR and SANIA AdaGrad-SQR show identical convergence behavior on original and scaled versions of datasets
- SANIA methods outperform standard adaptive methods (Adam, AdaGrad, Adadelta) on multiple binary classification tasks from LibSVM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SANIA methods achieve scale invariance by using diagonal preconditioners that transform consistently under scaling
- Mechanism: When data is scaled by diagonal matrix V, preconditioner B transforms as B → V^T B V, maintaining consistent update directions
- Core assumption: Preconditioner B is diagonal and positive definite; scaling transformation V is diagonal
- Evidence anchors:
  - SANIA-based methods outperform other adaptive methods on various classification tasks
  - Scale invariance shown in Figure 2 for both original and scaled datasets
  - Weak corpus evidence for scale invariance claims
- Break condition: Preconditioner is not diagonal or scaling transformation is not diagonal

### Mechanism 2
- Claim: Polyak step-size eliminates manual tuning by adapting based on function value and gradient
- Mechanism: Step-size computed as (f(wt) - f*) / ||∇f(wt)||², adjusting automatically to problem geometry
- Core assumption: Function f has known or estimable minimal value f*
- Evidence anchors:
  - 100% accuracy on colon-cancer without manual tuning
  - Outperformance on binary classification tasks
  - Weak corpus evidence for Polyak step-size claims
- Break condition: Minimal value f* is unknown or difficult to estimate

### Mechanism 3
- Claim: Combined preconditioning and Polyak step-size improves convergence for ill-conditioned problems
- Mechanism: Preconditioner captures curvature while Polyak step-size adapts to function landscape
- Core assumption: Preconditioner B is positive definite and captures relevant curvature
- Evidence anchors:
  - SANIA addresses poorly scaled or ill-conditioned problems
  - Outperformance on binary classification tasks
  - Weak corpus evidence for combined approach
- Break condition: Preconditioner B is not positive definite or captures irrelevant curvature

## Foundational Learning

- Concept: Scale invariance
  - Why needed here: Ensures consistent performance across datasets with different scaling, crucial for real-world applications
  - Quick check question: How does a diagonal preconditioner like AdaGrad-SQR achieve scale invariance?

- Concept: Polyak step-size
  - Why needed here: Eliminates manual tuning, leading to faster convergence and better performance
  - Quick check question: How is the Polyak step-size computed, and what assumptions are required for it to be effective?

- Concept: Preconditioning
  - Why needed here: Improves convergence rates for ill-conditioned problems by incorporating curvature information
  - Quick check question: What is the role of preconditioner B in the SANIA update rule?

## Architecture Onboarding

- Component map: Gradient computation -> Preconditioner update -> Polyak step-size computation -> Parameter update
- Critical path: 1) Compute gradient and function value, 2) Update preconditioner B, 3) Compute Polyak step-size, 4) Update parameters using combined approach
- Design tradeoffs:
  - Diagonal preconditioners ensure scale invariance but may miss some curvature information
  - Estimating f* for Polyak step-size can be challenging for non-convex problems
- Failure signatures:
  - Poor convergence: Preconditioner B may not be positive definite or capture relevant curvature
  - Numerical instabilities: May arise from improper scaling or ill-conditioned datasets
- First 3 experiments:
  1. Compare SANIA with other adaptive methods on synthetic dataset with varying scaling factors
  2. Evaluate impact of different preconditioners (AdaGrad, Adam, Hutchinson) on SANIA performance
  3. Test SANIA on real-world datasets with known optimal solutions

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but leaves several areas for future research implicit in the discussion.

## Limitations

- Scale invariance demonstrated primarily through synthetic experiments with controlled scaling factors
- Performance claims rely heavily on binary classification tasks with limited validation on multi-class or regression problems
- Computational overhead of SANIA methods compared to standard optimizers not thoroughly analyzed for large-scale applications

## Confidence

- Scale invariance mechanism: Medium - theoretical framework strong but empirical validation limited
- Polyak step-size effectiveness: Medium - shows promise in controlled experiments but needs more non-convex validation
- Combined approach benefits: Medium - theoretical justification solid but practical advantages need broader benchmarking

## Next Checks

1. Evaluate SANIA methods on large-scale multi-class classification datasets (CIFAR-10, ImageNet) to assess scalability and performance across diverse problem types

2. Conduct ablation studies isolating contributions of preconditioning and Polyak step-size to quantify individual and combined effects on convergence

3. Perform computational complexity analysis comparing SANIA methods with standard adaptive optimizers, measuring wall-clock time and memory usage across different problem sizes