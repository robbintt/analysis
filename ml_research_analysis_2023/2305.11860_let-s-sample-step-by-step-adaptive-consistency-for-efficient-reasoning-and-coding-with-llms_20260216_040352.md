---
ver: rpa2
title: 'Let''s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and
  Coding with LLMs'
arxiv_id: '2305.11860'
source_url: https://arxiv.org/abs/2305.11860
tags:
- accuracy
- adaptive-consistency
- stopping
- samples
- average
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Adaptive-Consistency, a cost-efficient method
  for improving the correctness of outputs from large language models (LLMs) by dynamically
  adjusting the number of samples per question using a lightweight stopping criterion.
  The method builds upon Self-Consistency and employs a stopping criterion based on
  the stability of the majority element in the set of samples.
---

# Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs

## Quick Facts
- **arXiv ID**: 2305.11860
- **Source URL**: https://arxiv.org/abs/2305.11860
- **Reference count**: 40
- **Key outcome**: Adaptive-Consistency achieves comparable accuracy to Self-Consistency while significantly reducing the required sample budget by up to 6.0 times, with an average drop in accuracy of less than 0.1%.

## Executive Summary
This paper introduces Adaptive-Consistency, a cost-efficient method for improving the correctness of outputs from large language models (LLMs) by dynamically adjusting the number of samples per question using a lightweight stopping criterion. The method builds upon Self-Consistency and employs a stopping criterion based on the stability of the majority element in the set of samples. The experiments demonstrate that Adaptive-Consistency achieves comparable accuracy to Self-Consistency while significantly reducing the required sample budget, making it a promising approach for more efficient and effective reasoning in large language models.

## Method Summary
Adaptive-Consistency is a model-agnostic technique that dynamically adjusts the number of samples (n) for each input based on a lightweight stopping criterion. The method employs a stopping criterion that monitors the stability of the majority element in the set of samples, stopping the sampling process early if a clear majority is established with high confidence after sampling fewer than k answers (n < k). This approach is built upon the theoretical foundation of Dirichlet distributions as conjugate priors for multinomial distributions, which provides a framework for modeling the probability distribution over unique samples based on observed counts.

## Key Results
- Adaptive-Consistency reduces sample budget by up to 6.0 times compared to Self-Consistency
- Average accuracy drop of less than 0.1% when using Adaptive-Consistency
- Computational efficiency improved by ~1200x using Beta approximation vs Dirichlet computation

## Why This Works (Mechanism)

### Mechanism 1
Dynamic sampling stops early when majority confidence exceeds threshold, reducing computational cost without harming accuracy. Uses Dirichlet distribution over sample counts to estimate probability that current majority remains dominant after additional sampling. Core assumption: Sample counts follow a multinomial distribution parameterized by an underlying probability distribution that can be modeled by a Dirichlet prior.

### Mechanism 2
Beta approximation of Dirichlet integral enables fast stopping criterion computation. Approximates the multi-dimensional Dirichlet integral with a Beta distribution by only comparing the top two probabilities. Core assumption: The probability that the current top answer remains the majority depends primarily on its lead over the second-best answer, not the full distribution.

### Mechanism 3
Adaptive sampling achieves comparable accuracy to fixed-budget Self-Consistency while using fewer samples. By allocating samples adaptively based on agreement, the method spends fewer resources on easy questions and more on hard ones. Core assumption: Questions vary in difficulty, and the agreement among samples correlates with the likelihood of finding the correct answer.

## Foundational Learning

- **Dirichlet distribution as conjugate prior for multinomial**
  - Why needed here: Provides the theoretical foundation for modeling the probability distribution over unique samples based on observed counts
  - Quick check question: If we observe 9 out of 10 samples with the same answer, what does the Dirichlet distribution tell us about the probability this remains the majority after 40 samples?

- **Beta distribution as special case of Dirichlet**
  - Why needed here: Enables efficient computation of stopping criterion by reducing multi-dimensional integration to a one-dimensional Beta CDF
  - Quick check question: How does the Beta distribution with parameters (v1+1, v2+1) approximate the probability that answer 1 remains majority over answer 2?

- **Chinese Restaurant Process for non-parametric clustering**
  - Why needed here: Provides an alternative stopping criterion that doesn't require knowing the number of possible unique answers in advance
  - Quick check question: How does the CRP concentration parameter Î± influence the probability of generating a new answer cluster?

## Architecture Onboarding

- **Component map**: LLM -> Sampling Loop -> Stopping Criterion -> Majority Vote
- **Critical path**: For each question, sample answers iteratively, evaluate stopping criterion after each sample, return majority when criterion triggers
- **Design tradeoffs**: Beta stopping is ~1200x faster than Dirichlet but slightly less precise; CRP handles unknown answer spaces but is computationally expensive
- **Failure signatures**: Early stopping on hard questions (accuracy drops), late stopping on easy questions (wasted computation), failure to trigger on questions with diverse correct answers
- **First 3 experiments**:
  1. Implement basic adaptive sampling with random stopping to establish baseline resource usage
  2. Add Beta stopping criterion and verify computational speedup vs Dirichlet
  3. Compare accuracy and sample budget against fixed Self-Consistency baseline on a small dataset

## Open Questions the Paper Calls Out
- **Open Question 1**: How do task-specific adaptations of Adaptive-Consistency compare to the task-agnostic version in terms of performance?
- **Open Question 2**: What is the impact of using approximate matching techniques instead of exact matches for determining the majority answer in Adaptive-Consistency?
- **Open Question 3**: How do alternative stopping criteria, such as entropy-based or majority-based stopping, compare to the Dirichlet and Beta stopping criteria used in Adaptive-Consistency?

## Limitations
- The theoretical guarantees of the stopping criterion are not rigorously proven
- Experimental evaluation focuses primarily on accuracy metrics without comprehensive analysis of failure cases
- Limited validation that the stopping criterion maintains correctness guarantees across diverse question types and difficulty levels

## Confidence
- **Medium Confidence**: Computational efficiency claims are well-supported by the ~1200x speedup from Beta vs Dirichlet computation
- **Medium Confidence**: Accuracy preservation claim (average drop < 0.1%) is demonstrated across multiple datasets, though statistical significance is not rigorously established
- **Low Confidence**: Theoretical guarantees of the stopping criterion are not rigorously proven

## Next Checks
1. Evaluate Adaptive-Consistency on adversarial question sets designed to trigger premature stopping, including questions with multiple equally valid answers or deceptive near-majorities
2. Conduct formal statistical tests across all 13 datasets to verify that the < 0.1% accuracy drop is statistically significant
3. Test the method on non-reasoning tasks (e.g., creative writing, summarization) where answer diversity is inherent to correctness