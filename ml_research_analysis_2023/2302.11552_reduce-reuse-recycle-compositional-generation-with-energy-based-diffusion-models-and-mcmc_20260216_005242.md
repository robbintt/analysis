---
ver: rpa2
title: 'Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion
  Models and MCMC'
arxiv_id: '2302.11552'
source_url: https://arxiv.org/abs/2302.11552
tags:
- diffusion
- sampling
- mcmc
- compositional
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Diffusion models have emerged as the leading approach for generative\
  \ modeling, offering impressive scalability and sample quality. While they excel\
  \ at conditional generation through guidance techniques, their potential for compositional\
  \ generation\u2014combining models to create new ones\u2014remains underexplored."
---

# Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC

## Quick Facts
- arXiv ID: 2302.11552
- Source URL: https://arxiv.org/abs/2302.11552
- Reference count: 26
- Key outcome: Diffusion models can be composed beyond standard guidance methods using MCMC sampling and energy-based parameterizations, achieving better compositional generation across diverse domains.

## Executive Summary
Diffusion models excel at conditional generation but their potential for compositional generation remains underexplored. This work investigates how diffusion models can be composed using products, mixtures, and negations to generate novel distributions without retraining. The authors identify that standard reverse diffusion fails to sample correctly from composed distributions and propose MCMC-based sampling methods inspired by annealed MCMC. They also introduce an energy-based parameterization enabling explicit log-likelihood estimation and use of Metropolis-adjusted samplers like HMC and MALA, significantly improving compositional generation across 2D densities, ImageNet, and text-to-image synthesis.

## Method Summary
The method combines energy-based diffusion model parameterization with MCMC sampling for compositional generation. Diffusion models are trained to estimate either scores (via ϵθ) or energies (via fθ), where the energy parameterization enables explicit log-likelihood computation. For composition, operators like products, mixtures, and negations are applied to pre-trained models. Sampling from composed distributions uses MCMC transitions (ULA, MALA, HMC) on annealed distributions that interpolate from a tractable prior to the target composed distribution. This approach corrects the failure of standard reverse diffusion to sample from the intended composed distribution.

## Key Results
- MCMC sampling with annealed distributions correctly generates from composed distributions where standard reverse diffusion fails
- Energy-based parameterization enables mixture composition and improves sample quality with Metropolis-corrected samplers
- Significant improvements in compositional generation across 2D densities, CLEVR cube placement, ImageNet generation, and text-to-image synthesis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Naïve composition of diffusion models using standard reverse diffusion fails because it uses the wrong intermediate distribution.
- Mechanism: When composing distributions, combining score functions produces the score of the diffusion of the composed distribution, not the composed distribution of the diffusion. Standard reverse diffusion expects the latter but gets the former.
- Core assumption: The time-marginal distributions of a composed model are not equal to the composition of the time-marginal distributions of individual models.
- Evidence anchors: Abstract states naı̈ve composition fails; Section 4 shows incorrect samples from composed distributions.
- Break condition: If composed distributions have special structure where diffusion of composition equals composition of diffusions.

### Mechanism 2
- Claim: MCMC sampling with annealed distributions correctly samples from composed distributions by targeting the right intermediate distributions.
- Mechanism: Running MCMC transitions on a sequence of distributions interpolating from a tractable distribution (e.g., N(0,I)) to the target composed distribution draws samples from the correct composed model.
- Core assumption: MCMC transition kernels are invariant to intermediate distributions and ergodic, ensuring convergence to target.
- Evidence anchors: Abstract mentions MCMC methods inspired by annealed MCMC; Section 4.1 explores ULA and HMC transitions.
- Break condition: If MCMC transition kernels have poor mixing due to step sizes, high dimensionality, or complex energy landscapes.

### Mechanism 3
- Claim: Energy-based parameterization enables explicit log-likelihood estimation and use of Metropolis-corrected samplers, leading to better compositional generation.
- Mechanism: Parameterizing diffusion models as potential functions fθ(x,t) where ϵθ(x,t) = ∇xfθ(x,t) enables log pθ(x,t) computation and use of MALA/HMC with Metropolis corrections.
- Core assumption: Energy function can be accurately learned and differentiated, and Metropolis acceptance probability can be computed.
- Evidence anchors: Abstract states energy-based parameterization enables log-likelihood estimation and Metropolis-adjusted samplers; Section 4.2 discusses energy-based parameterizations.
- Break condition: If energy function is poorly estimated, leading to inaccurate gradients or acceptance probabilities, or if computational cost is prohibitive.

## Foundational Learning

- Concept: Score-based generative modeling
  - Why needed here: Diffusion models learn the score (gradient of log-probability) of a sequence of distributions, which is the core mechanism for sampling and composition.
  - Quick check question: What is the relationship between the score function and the gradient of the log-probability density function?

- Concept: Markov Chain Monte Carlo (MCMC) sampling
  - Why needed here: MCMC methods are used to sample from complex distributions, and in this work, they are adapted to sample from composed distributions in diffusion models.
  - Quick check question: How does the Metropolis-Hastings acceptance probability ensure that MCMC samples converge to the target distribution?

- Concept: Energy-based models (EBMs)
  - Why needed here: EBMs parameterize unnormalized probability distributions, which is necessary for certain types of composition (e.g., mixtures) and enables the use of Metropolis-corrected samplers.
  - Quick check question: What is the difference between an energy-based model and a standard probabilistic model in terms of the probability density function?

## Architecture Onboarding

- Component map: Diffusion model → MCMC sampler → Composed distribution
- Critical path:
  1. Train individual diffusion models on data
  2. Compose diffusion models using desired operator (product, mixture, negation)
  3. Sample from composed distribution using MCMC with annealed distributions
  4. Evaluate sample quality and likelihood using appropriate metrics
- Design tradeoffs:
  - Score vs. energy parameterization: Energy enables more compositional operators and Metropolis corrections but requires additional backward pass and may be harder to train
  - MCMC sampler choice: Unadjusted samplers (ULA, U-HMC) are cheaper but may have poor convergence; Metropolis-corrected samplers (MALA, HMC) are more accurate but more expensive
  - Annealing schedule: Number of MCMC steps and step sizes affect sample quality and computational cost
- Failure signatures:
  - Poor sample quality: Indicates issues with MCMC mixing, energy function estimation, or compositional operator implementation
  - Low likelihood estimates: Suggests problems with energy parameterization or Metropolis acceptance probabilities
  - High variance in metrics: May indicate instability in MCMC sampling or energy function learning
- First 3 experiments:
  1. Train two diffusion models on simple 2D synthetic datasets (e.g., Gaussian mixtures) and compose them using a product operator. Sample using ULA and compare to ground truth.
  2. Implement energy-based parameterization and train a diffusion model on ImageNet. Compare unconditional sample quality (FID) to score-based model.
  3. Compose two text-to-image diffusion models using a mixture operator and sample using HMC. Evaluate sample diversity and faithfulness to prompts.

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- Scalability concerns for extremely high-dimensional data like high-resolution images
- Sensitivity to MCMC hyperparameters (step sizes, number of steps, annealing schedule)
- Potential for mode collapse or poor mixing in complex energy landscapes

## Confidence
- Core claims: Medium
- MCMC-based sampling effectiveness: Medium
- Energy-based parameterization benefits: Medium
- Scalability to high-resolution images: Low

## Next Checks
1. Perform ablation studies on MCMC hyperparameters (step sizes, number of steps) across different datasets to assess sensitivity and robustness.
2. Compare the proposed method against other compositional approaches on a larger scale image dataset (e.g., FFHQ) to evaluate scalability.
3. Investigate the effect of energy function accuracy on sample quality by intentionally corrupting the energy estimates and measuring the impact on Metropolis acceptance rates and sample fidelity.