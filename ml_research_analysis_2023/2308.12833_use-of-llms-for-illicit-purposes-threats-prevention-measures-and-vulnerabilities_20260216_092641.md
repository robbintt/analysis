---
ver: rpa2
title: 'Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities'
arxiv_id: '2308.12833'
source_url: https://arxiv.org/abs/2308.12833
tags:
- llms
- arxiv
- language
- such
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of threats, prevention
  measures, and vulnerabilities associated with large language models (LLMs) being
  used for illicit purposes. The authors categorize these into a taxonomy of three
  dimensions: threats (e.g., fraud, malware generation, misinformation), prevention
  measures (e.g., content detection, red teaming, RLHF), and vulnerabilities (e.g.,
  prompt injection, jailbreaking).'
---

# Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities

## Quick Facts
- arXiv ID: 2308.12833
- Source URL: https://arxiv.org/abs/2308.12833
- Reference count: 40
- Key outcome: Comprehensive survey categorizing LLM security threats, prevention measures, and vulnerabilities into a taxonomy of three dimensions

## Executive Summary
This paper provides a systematic survey of security concerns around large language models being used for illicit purposes. The authors develop a taxonomy categorizing issues into three dimensions: threats (fraud, malware generation, misinformation), prevention measures (content detection, red teaming, RLHF), and vulnerabilities (prompt injection, jailbreaking). Through extensive literature review, the paper demonstrates how LLM generative capabilities enable misuse, how prevention techniques aim to reduce harm, and how vulnerabilities can undermine these safeguards. The work highlights growing public concern around LLMs, limitations of current safety approaches, and potential future risks, serving as a valuable overview of the security landscape and raising awareness of LLMs' limitations in light of security concerns.

## Method Summary
The paper follows a systematic literature review approach, curating and analyzing 93 papers on LLM safety and security. The methodology involves comprehensive literature search focused on recent works and pre-prints, categorization of collected papers into threats, prevention measures, and vulnerabilities based on a proposed taxonomy, and analysis and summarization of key findings for each category with extensive examples and references. The paper does not conduct original experiments but synthesizes existing research to provide an overview of the security landscape.

## Key Results
- LLMs can generate realistic text enabling scalable phishing, malware code, and misinformation
- Prevention measures like RLHF, red teaming, and content filtering can reduce harmful outputs but have limitations
- Vulnerabilities such as prompt injection and jailbreaking can bypass safeguards, undermining prevention efforts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can be misused for fraud, impersonation, and malware generation due to their generative capabilities.
- Mechanism: LLMs generate realistic text mimicking human writing, enabling adversaries to create phishing emails at scale and malware code without specialized skills.
- Core assumption: LLM outputs are sufficiently realistic and scalable to enable misuse.
- Evidence anchors:
  - [abstract] "LLMs can be misused for fraud, impersonation, and the generation of malware"
  - [section 5.1] Hazell (2023) demonstrates personalized phishing emails for 600 UK MPs using GPT models
  - [section 5.2] WormGPT and FraudGPT are real cybercrime tools based on generative models
- Break condition: If LLM outputs become easily detectable or strong safeguards prevent misuse, the mechanism fails.

### Mechanism 2
- Claim: Prevention measures like RLHF, red teaming, and content filtering can reduce harmful LLM outputs.
- Mechanism: RLHF collects human feedback to train LLMs to be harmless and helpful, while red teaming and content filtering detect and block harmful generations.
- Core assumption: These prevention techniques effectively reduce the frequency and impact of harmful outputs.
- Evidence anchors:
  - [abstract] "prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures"
  - [section 6.4] Bai et al. (2022a) collect human feedback to train LLMs to be harmless and helpful
  - [section 6.3] Markov et al. (2023) propose fine-tuning models for content moderation
- Break condition: If adversaries bypass these measures via jailbreaking or prompt injection, prevention fails.

### Mechanism 3
- Claim: Vulnerabilities like prompt injection and jailbreaking re-enable threats by bypassing prevention measures.
- Mechanism: Prompt injection allows extraction or manipulation of system prompts, while jailbreaking uses adversarial suffixes to override safety filters.
- Core assumption: Imperfect prevention measures leave exploitable gaps that adversaries can leverage.
- Evidence anchors:
  - [abstract] "vulnerabilities arising from imperfect prevention measures"
  - [section 7.1] Perez and Ribeiro (2022) define prompt injection as goal hijacking and prompt leaking
  - [section 7.2] Zou et al. (2023) demonstrate universal adversarial suffixes that jailbreak LLMs
- Break condition: If prevention measures become robust enough to eliminate these gaps, the vulnerability mechanism is neutralized.

## Foundational Learning

- Concept: Taxonomy of threats, prevention measures, and vulnerabilities in LLM security
  - Why needed here: The paper uses this taxonomy to systematically organize and analyze the security landscape, helping readers understand relationships between threats, defenses, and weaknesses.
  - Quick check question: Can you name one example from each of the three taxonomy dimensions (threat, prevention, vulnerability) discussed in the paper?

- Concept: Adversarial attacks and robustness in LLMs
  - Why needed here: Understanding adversarial attacks (e.g., prompt injection, jailbreaking) is essential to grasp how vulnerabilities can be exploited to bypass safeguards.
  - Quick check question: What is the difference between prompt injection and jailbreaking as described in the paper?

- Concept: Data poisoning and its implications for LLM training
  - Why needed here: Data poisoning can manipulate LLM behavior during training, leading to persistent vulnerabilities that are hard to detect or remove.
  - Quick check question: How does data poisoning differ from traditional adversarial attacks on trained models?

## Architecture Onboarding

- Component map: Threats -> Prevention Measures -> Vulnerabilities -> Iterate (the paper's structure maps to a security lifecycle showing what can go wrong, how to stop it, and where defenses fail)
- Critical path: Identify threats → Apply prevention measures → Detect and mitigate vulnerabilities → Iterate as new threats emerge
- Design tradeoffs: Stronger prevention (e.g., RLHF) can reduce utility (e.g., over-refusal), while weaker measures leave exploitable gaps. Balancing safety and performance is a recurring challenge.
- Failure signatures: Prevention failure is signaled by successful jailbreaks or prompt injections; threat emergence is signaled by new misuse cases (e.g., personalized phishing).
- First 3 experiments:
  1. Reproduce Hazell (2023): Use GPT models to generate personalized phishing emails for a small set of targets; measure success and detectability.
  2. Test RLHF effectiveness: Fine-tune an LLM on harmful prompts with human feedback; evaluate reduction in harmful outputs.
  3. Jailbreak vulnerability assessment: Apply universal adversarial suffixes (Zou et al., 2023) to a safety-aligned LLM; measure bypass success rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the practical limitations and trade-offs of scaling up language models in terms of their safety and security?
- Basis in paper: [explicit] The paper discusses how larger models are more susceptible to data poisoning and memorization attacks, and how there is a trade-off between model accuracy and robustness.
- Why unresolved: The paper highlights these issues but does not provide a comprehensive analysis of the trade-offs or practical implications of scaling models.
- What evidence would resolve it: Empirical studies comparing the safety and security of different-sized models, and analyses of the trade-offs between model performance and robustness.

### Open Question 2
- Question: How can we effectively detect and mitigate the spread of misinformation generated by large language models?
- Basis in paper: [explicit] The paper discusses the potential of LLMs to generate misinformation and the challenges in detecting AI-generated text.
- Why unresolved: While the paper outlines the problem, it does not provide a clear solution for detecting and mitigating the spread of misinformation.
- What evidence would resolve it: Studies evaluating the effectiveness of different detection methods and mitigation strategies, and real-world examples of misinformation spread by LLMs.

### Open Question 3
- Question: What are the long-term societal impacts of widespread access to large language models, particularly in terms of personalization and information dissemination?
- Basis in paper: [explicit] The paper discusses the potential for LLM personalization and the implications of LLMs on the dissemination of digital information.
- Why unresolved: The paper raises these concerns but does not explore the long-term societal impacts in detail.
- What evidence would resolve it: Longitudinal studies tracking the societal impacts of LLM adoption, and analyses of the effects of personalization and information dissemination on public trust and discourse.

## Limitations
- The paper's survey nature synthesizes existing literature without conducting original experiments to validate claims
- Effectiveness of prevention measures is inferred from cited works rather than empirically tested
- The rapid evolution of LLM capabilities and security research means some findings may become outdated quickly

## Confidence
- **High confidence**: The existence of LLM misuse for fraud and malware generation (supported by documented real-world cases like WormGPT and concrete examples like Hazell 2023)
- **Medium confidence**: The effectiveness of prevention measures like RLHF and red teaming (based on cited literature but limited empirical validation in the survey)
- **Medium confidence**: The taxonomy structure and categorization of threats/prevention/vulnerabilities (logically coherent but not empirically validated)

## Next Checks
1. **Empirical testing of jailbreak effectiveness**: Replicate Zou et al. (2023) universal adversarial suffixes on current state-of-the-art safety-aligned models to measure bypass success rates
2. **Prevention measure robustness evaluation**: Conduct controlled experiments comparing harmful output rates between base models and RLHF-aligned versions across diverse prompt categories
3. **Cross-dataset consistency check**: Test whether prompt injection and jailbreaking techniques transfer across different model families (e.g., from GPT to LLaMA to Claude) to assess vulnerability generalizability