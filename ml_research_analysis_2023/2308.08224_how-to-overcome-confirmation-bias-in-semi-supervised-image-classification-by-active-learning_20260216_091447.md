---
ver: rpa2
title: How To Overcome Confirmation Bias in Semi-Supervised Image Classification By
  Active Learning
arxiv_id: '2308.08224'
source_url: https://arxiv.org/abs/2308.08224
tags:
- learning
- data
- uni00000013
- active
- mnist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies whether active learning (AL) can improve semi-supervised
  learning (SSL) performance in realistic data scenarios. While recent work shows
  SSL alone can match or exceed AL performance on benchmark datasets, such benchmarks
  may overestimate real-world applicability due to unrealistic data characteristics.
---

# How To Overcome Confirmation Bias in Semi-Supervised Image Classification By Active Learning

## Quick Facts
- arXiv ID: 2308.08224
- Source URL: https://arxiv.org/abs/2308.08224
- Reference count: 40
- Primary result: Active learning methods can significantly improve semi-supervised learning performance in realistic data scenarios by mitigating confirmation bias through diverse sample selection.

## Executive Summary
This paper investigates whether active learning can enhance semi-supervised learning performance in realistic data scenarios where traditional benchmarks may overestimate real-world applicability. The authors identify three common real-world data challenges - between-class imbalance, within-class imbalance, and between-class similarity - that introduce confirmation bias in semi-supervised learning. Through experiments on constructed MNIST variants, they demonstrate that active learning methods, particularly coverage-based sampling, can effectively mitigate these issues by selecting more informative and diverse samples, leading to significant improvements over random sampling and sometimes matching supervised learning performance.

## Method Summary
The study constructs three MNIST variants to simulate real-world data challenges: BCI-MNIST (imbalanced class distribution), BCS-MNIST (high inter-class similarity with ambiguous labels), and WCI-MNIST (imbalanced intra-class variation). Using a LeNet backbone, the authors evaluate three semi-supervised learning methods (pseudo-labeling, FixMatch, and FlexMatch) combined with four active learning strategies (uncertainty, representativeness, coverage, and balanced sampling). Experiments run across labeling budgets from 20 to 250 samples with five random seeds, comparing active learning performance against random sampling baselines on both the constructed datasets and original MNIST.

## Key Results
- Random sampling sometimes performs worse than supervised learning on BCI and BCS datasets due to confirmation bias
- Coverage-based sampling is particularly effective across all three data challenges
- Representative sampling is beneficial early in training, while uncertainty sampling performs better in later stages
- Balanced sampling effectively addresses between-class imbalance by ensuring equal representation across classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Active learning mitigates confirmation bias in semi-supervised learning by diversifying the labeled pool with instances that expose the model to underrepresented or ambiguous concepts.
- Mechanism: When confirmation bias causes a model to repeatedly select and reinforce incorrect pseudo-labels, especially in imbalanced or ambiguous datasets, AL methods like coverage and representativeness sampling proactively select samples that introduce new, informative, or diverse data points. This breaks the cycle of reinforcing incorrect assumptions and helps the model generalize better.
- Core assumption: The labeled pool's diversity directly impacts the model's ability to learn robust representations; confirmation bias occurs because pseudo-labels in underrepresented regions are systematically incorrect.
- Evidence anchors:
  - [abstract] "These challenges can hurt SSL performance due to confirmation bias... coverage-based sampling is particularly effective across all challenges."
  - [section] "We demonstrate that each of these real-world challenges introduces confirmation bias reinforcing biased or misleading concepts toward SSL."
- Break condition: If the model's confidence estimates are unreliable in early training stages, uncertainty-based AL may select misleading samples, exacerbating confirmation bias rather than mitigating it.

### Mechanism 2
- Claim: Coverage-based sampling is particularly effective because it explicitly maximizes diversity in the labeled set, counteracting the effects of within-class imbalance (WCI) and between-class similarity (BCS).
- Mechanism: By selecting instances that are far apart in feature space, coverage sampling ensures that rare subclasses and ambiguous class boundaries are represented in the labeled pool. This prevents the model from overfitting to the majority subclass or confusing similar classes.
- Core assumption: The feature space adequately captures the diversity needed to distinguish subclasses and resolve class ambiguity; distance in this space correlates with informativeness.
- Evidence anchors:
  - [abstract] "Coverage-based sampling is particularly effective across all challenges."
  - [section] "Especially coverage-based sampling seems to be a viable choice... coverage-based sampling on WCI-MNIST is even equally good as the performance on the original MNIST."
- Break condition: If the feature space is poorly structured or if the distance metric fails to capture semantic differences, coverage sampling may select uninformative or redundant samples.

### Mechanism 3
- Claim: Representative sampling is effective early in training because it selects instances that are prototypical of their class, providing a stable foundation before the model's predictions become more reliable.
- Mechanism: By choosing the most central instance in each cluster (e.g., via k-means), representative sampling ensures that the labeled pool contains clear, unambiguous examples. This helps the model establish correct class boundaries before introducing more challenging samples.
- Core assumption: Early-stage predictions are unreliable; selecting clear, central examples reduces the risk of reinforcing incorrect pseudo-labels.
- Evidence anchors:
  - [section] "Representative sampling is beneficial for Fixmatch... This method promotes instances representative of a certain class or region and probably selects instances that are less ambiguous for training."
  - [section] "In the early stages, representative sampling is often beneficial."
- Break condition: If the clustering algorithm fails to identify true class centers (e.g., due to overlapping classes or noise), representative sampling may select misleading examples, reinforcing confirmation bias.

## Foundational Learning

- Concept: Semi-supervised learning (SSL) leverages both labeled and unlabeled data to train models when labeled data is scarce.
  - Why needed here: The paper's core premise is that SSL suffers from confirmation bias in realistic data scenarios, and understanding SSL's mechanisms is crucial to grasping why AL helps.
  - Quick check question: What is the primary difference between consistency regularization and pseudo-labeling in SSL?

- Concept: Confirmation bias in SSL occurs when a model's incorrect predictions are repeatedly used as pseudo-labels, reinforcing errors.
  - Why needed here: The paper's main argument is that confirmation bias is the root cause of SSL's degraded performance in real-world data, and AL mitigates this bias.
  - Quick check question: Why does confirmation bias have a larger impact in SSL compared to supervised learning?

- Concept: Active learning (AL) intelligently selects which unlabeled instances to label next, aiming to maximize model performance with minimal labeling effort.
  - Why needed here: The paper evaluates how different AL strategies (uncertainty, representativeness, coverage, balance) affect SSL performance in the presence of confirmation bias.
  - Quick check question: What is the key difference between instance-level and distribution-level AL acquisition?

## Architecture Onboarding

- Component map: LeNet backbone -> SSL methods (pseudo-labeling, FixMatch, FlexMatch) -> AL strategies (uncertainty, representativeness, coverage, balance) -> MNIST variants (BCI, BCS, WCI) -> Evaluation on original MNIST test set
- Critical path: Data preparation → Initial random labeling (20 instances) → SSL training → AL acquisition (selecting next batch) → Repeat until labeling budget exhausted → Evaluation on test set
- Design tradeoffs: Using a simple LeNet backbone allows isolating the effects of data challenges and AL strategies, but limits generalizability to more complex tasks. The focus on MNIST variants enables controlled experiments but may not fully capture real-world complexity.
- Failure signatures: If AL consistently underperforms random sampling, it may indicate that the AL strategy is not well-suited to the data challenge or that the model's confidence estimates are unreliable. If SSL performance stagnates early, it may indicate severe confirmation bias or insufficient diversity in the labeled pool.
- First 3 experiments:
  1. Run SSL with random sampling on BCI-MNIST to establish baseline performance degradation due to confirmation bias.
  2. Apply coverage-based AL to the same SSL setup and compare performance to random sampling.
  3. Repeat experiment 2 with uncertainty-based AL to assess its effectiveness in later iterations when model predictions are more reliable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do active learning methods perform on real-world datasets with multiple simultaneous data challenges (e.g., both BCI and WCI present)?
- Basis in paper: [explicit] The authors state they intend to extend experiments to "a broader range of datasets, with a strong focus on real-world examples" and mention evaluating multiple challenges simultaneously.
- Why unresolved: The current experiments only test each challenge in isolation on MNIST variants, not combined real-world scenarios.
- What evidence would resolve it: Empirical comparison of active learning methods on real-world datasets exhibiting multiple data challenges simultaneously, showing performance relative to benchmark datasets.

### Open Question 2
- Question: Which specific hybrid active learning methods combining multiple acquisition strategies perform best for semi-supervised learning under real-world data challenges?
- Basis in paper: [explicit] The authors note that "there is an abundance of hybrid methods combining two or more of the described concepts" and state they "aim to include existing hybrid AL methods in our evaluation."
- Why unresolved: The current experiments only use single acquisition strategies (uncertainty, representativeness, coverage, balance) rather than hybrid approaches.
- What evidence would resolve it: Comparative evaluation of hybrid active learning methods against single-strategy approaches on real-world datasets with data challenges.

### Open Question 3
- Question: Can the confirmation bias problem in semi-supervised learning be fully mitigated through active learning, or are there fundamental limitations?
- Basis in paper: [inferred] The authors demonstrate that AL can "overcome confirmation bias" but do not explore whether this mitigation is complete or has limitations.
- Why unresolved: The experiments show performance improvements but don't establish whether AL completely eliminates confirmation bias or if some residual bias remains.
- What evidence would resolve it: Systematic analysis of whether AL methods can achieve performance levels matching or exceeding supervised learning with full labels, or identifying scenarios where confirmation bias persists despite AL.

## Limitations
- Experiments limited to MNIST variants, which may not fully capture complexity of real-world data challenges
- Lack of external validation through comparison with recent literature on confirmation bias mitigation
- Computational overhead and scalability of active learning methods to larger datasets not explored

## Confidence
- **High Confidence**: AL methods (particularly coverage and balanced sampling) improve SSL performance across all three data challenges. The mechanism of breaking confirmation bias through diverse sample selection is well-supported by experimental results.
- **Medium Confidence**: The specific effectiveness of representativeness sampling in early training stages and uncertainty sampling in later stages. While results show trends, the interaction between AL strategy timing and SSL method convergence could benefit from additional experiments.
- **Low Confidence**: The claim that coverage-based sampling performs equally well on WCI-MNIST as on original MNIST. This extraordinary result would benefit from additional statistical analysis and replication.

## Next Checks
1. **External Validation**: Compare results against the recently published "Towards the Mitigation of Confirmation Bias in Semi-supervised Learning: a Debiased Training Perspective" to assess consistency of findings across different methodological approaches.
2. **Scalability Test**: Evaluate the proposed AL strategies on CIFAR-10 with similar data challenge constructions to verify performance holds on more complex image classification tasks.
3. **Statistical Significance**: Perform pairwise t-tests between AL strategies and random sampling at each labeling budget to quantify the statistical significance of observed improvements, particularly for the coverage-based sampling results on WCI-MNIST.