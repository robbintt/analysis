---
ver: rpa2
title: Lightweight Adaptation of Neural Language Models via Subspace Embedding
arxiv_id: '2308.08688'
source_url: https://arxiv.org/abs/2308.08688
tags:
- embedding
- subspace
- language
- embeddings
- vectors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to reduce the memory footprint of
  pre-trained language models by decomposing word embeddings into multiple subspace
  embeddings. The core idea is to represent each word as a concatenation of several
  smaller vectors, each drawn from a shared subspace embedding table.
---

# Lightweight Adaptation of Neural Language Models via Subspace Embedding

## Quick Facts
- arXiv ID: 2308.08688
- Source URL: https://arxiv.org/abs/2308.08688
- Reference count: 36
- This paper proposes a method to reduce the memory footprint of pre-trained language models by decomposing word embeddings into multiple subspace embeddings.

## Executive Summary
This paper introduces a novel approach to reduce the memory footprint of pre-trained language models by decomposing word embeddings into multiple subspace embeddings. The method exploits the fact that many words share common subword components, allowing for parameter sharing across the vocabulary. By representing each word as a concatenation of several smaller vectors drawn from shared subspace embedding tables, the approach achieves compression rates beyond 99.8% while maintaining competitive performance on downstream tasks. Two assignment methods are presented: arbitrary assignment using modulo operations and cluster-based assignment using k-means on contextual embeddings from pre-trained models.

## Method Summary
The method decomposes original word embeddings into multiple subspace embeddings through Cartesian product mapping. Each original embedding vector is reconstructed as the concatenation of f smaller subspace vectors, where f is a hyperparameter determining the level of decomposition. Two assignment strategies are employed: an arbitrary method using modulo operations for direct mapping, and a cluster-based approach that uses k-means clustering on contextual embeddings from pre-trained models to group semantically similar tokens. This shared parameter approach substantially reduces the number of embedding parameters while maintaining representational capacity for downstream tasks.

## Key Results
- Achieves compression rates beyond 99.8% compared to original embeddings
- Sacrifices only up to 4% accuracy on downstream tasks like XNLI and GLUE
- Cluster-based approach with uniform cluster sizes outperforms arbitrary assignment method
- Maintains comparable performance to original embeddings on many tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subspace embeddings exploit shared subword components to reduce redundancy in embedding parameters
- Mechanism: Decomposes each word's embedding vector into multiple smaller vectors, each drawn from a shared subspace embedding table, where semantically similar words share common subspace components
- Core assumption: Words sharing common subword components or contextual relationships can effectively share embedding parameters without significant loss of semantic information
- Evidence anchors:
  - [abstract] "The embeddings vectors reconstruction follows a set of subspace embeddings and an assignment procedure via the contextual relationship among tokens from pre-trained language models"
  - [section 3.1] "Thus, the original embedding vector comprises subspace embeddings (i.e. shared between vocabularies) that play a part in employing the common learning parameters with closely situated embedding vectors"
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.441" - Weak corpus evidence for specific subspace embedding mechanism
- Break condition: If contextual relationships between words are too diverse or if semantic precision is critical for downstream tasks, shared subspace parameters may cause performance degradation

### Mechanism 2
- Claim: Cluster-based subspace embedding assignment improves contextual mapping accuracy compared to arbitrary assignment
- Mechanism: Uses k-means clustering on contextual embeddings from pre-trained models to assign tokens to subspace embeddings based on semantic similarity, ensuring tokens with similar contexts share more subspace components
- Core assumption: Pre-trained contextual embeddings capture meaningful semantic relationships that can guide optimal subspace assignment
- Evidence anchors:
  - [section 3.3] "To alleviate such context mismatching problem, we employ the pre-trained RoBERTa [14] model... Our conjecture is that all subspace embeddings are independently assigned arbitrarily, including, the tokens allocating more subspace embeddings that are anticipated to have less L2 distance"
  - [section 4.2] "Based on the k-means clustering which uses instances rather than the cluster size... the subspace embedding with uniform cluster size outperforms a small set of embedding"
  - [corpus] "Found 25 related papers (using 8)" - Weak corpus evidence for specific clustering mechanism
- Break condition: If pre-trained model embeddings don't capture relevant semantic distinctions or if clustering introduces excessive computational overhead

### Mechanism 3
- Claim: Cartesian product of subspace embeddings enables exponential reduction in parameters while maintaining representational capacity
- Mechanism: Each original embedding vector is reconstructed as the concatenation of multiple subspace vectors, allowing a small number of subspace embeddings to represent a large vocabulary through combinatorial explosion
- Core assumption: The Cartesian product of smaller embedding spaces can effectively approximate the original high-dimensional embedding space
- Evidence anchors:
  - [section 3.1] "the embeddings can be inherent only ð·1/ð‘“ subspace embeddings. This construct can substantially reduce the number of embedding parameters with log scale"
  - [section 4.1] "Our evaluation shows that our approach substantially alleviates the number of learning parameters in the embedding part with the usage of the Cartesian product"
  - [corpus] "Found 25 related papers (using 8)" - Weak corpus evidence for specific Cartesian product mechanism
- Break condition: If the vocabulary size or embedding dimensionality makes the Cartesian product approach computationally infeasible, or if the exponential parameter reduction introduces unacceptable accuracy loss

## Foundational Learning

- Concept: Embedding dimensionality and parameter sharing
  - Why needed here: Understanding how decomposing high-dimensional embeddings into shared subspaces affects model capacity and performance
  - Quick check question: If you have a 768-dimensional embedding and decompose it into 3 subspace embeddings, what is the dimension of each subspace embedding?
- Concept: k-means clustering and distance metrics
  - Why needed here: The cluster-based approach relies on k-means to group tokens based on contextual similarity using L2 distance
  - Quick check question: How does k-means clustering determine which tokens should share subspace embeddings in this approach?
- Concept: Cartesian product and combinatorial mathematics
  - Why needed here: The exponential reduction in parameters relies on understanding how the Cartesian product of smaller spaces can represent a larger space
  - Quick check question: If you have 3 subspace embeddings each with 50 vectors, how many unique original embeddings can you represent?

## Architecture Onboarding

- Component map:
  - Tokenizer â†’ Subspace embedding lookup â†’ Transformer layers â†’ CLS token â†’ Task-specific head
  - Subspace embedding tables replace original embedding matrix
  - Assignment mechanism (arbitrary or cluster-based) determines which subspace vectors combine for each token
- Critical path:
  - Tokenization â†’ Subspace embedding assignment lookup â†’ Vector concatenation â†’ Input to transformer
  - Assignment accuracy directly impacts downstream performance
- Design tradeoffs:
  - Parameter reduction vs. accuracy: More aggressive compression (fewer subspace embeddings) leads to greater parameter savings but potentially lower accuracy
  - Arbitrary vs. cluster-based assignment: Cluster-based provides better performance but requires pre-trained model and clustering computation
  - Number of subspace vectors (f): More vectors allow finer-grained representation but increase parameter count
- Failure signatures:
  - Significant accuracy drop on downstream tasks indicating insufficient representational capacity
  - Out-of-vocabulary issues if subspace assignment doesn't cover all tokens
  - Increased training instability due to altered embedding structure
- First 3 experiments:
  1. Baseline comparison: Implement arbitrary assignment with f=2 and measure accuracy drop vs original embeddings on GLUE benchmark
  2. Cluster assignment validation: Implement cluster-based assignment with uniform clusters and compare performance to arbitrary assignment
  3. Compression analysis: Systematically vary f (2, 3, 4, 6, 8) and plot parameter reduction vs accuracy tradeoff curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do subspace embeddings perform on low-resource languages compared to high-resource languages in cross-lingual transfer tasks?
- Basis in paper: [inferred] The paper mentions multilingual datasets and XNLI benchmark but does not specifically analyze performance across different resource levels of languages.
- Why unresolved: The paper focuses on general multilingual performance but does not provide a detailed analysis of how resource levels of languages affect subspace embedding performance.
- What evidence would resolve it: Comparative results showing performance differences between low-resource and high-resource languages on cross-lingual tasks using subspace embeddings.

### Open Question 2
- Question: What is the optimal number of subspace embeddings (f) for balancing compression ratio and task performance across different NLP tasks?
- Basis in paper: [explicit] The paper mentions different values of f (2, 3, 4, 6, 8) but does not provide a comprehensive analysis of optimal f values for various tasks.
- Why unresolved: The paper shows results for different f values but does not explore the relationship between f, compression ratio, and task-specific performance in depth.
- What evidence would resolve it: A systematic study showing how varying f affects performance and compression ratio across multiple NLP tasks, identifying task-specific optimal values.

### Open Question 3
- Question: How does the cluster-based subspace embedding approach scale with extremely large vocabularies (e.g., >1 million tokens)?
- Basis in paper: [inferred] The paper uses vocabularies up to 250k tokens but does not explore scalability to much larger vocabularies.
- Why unresolved: The paper's experiments are limited to moderate vocabulary sizes, and the scalability of the k-means clustering approach for very large vocabularies is not addressed.
- What evidence would resolve it: Performance and efficiency results for subspace embeddings with vocabularies exceeding 1 million tokens, including computational resource requirements and task performance.

## Limitations

- Theoretical gaps: The paper lacks rigorous mathematical justification for why subspace embeddings can maintain representational capacity with aggressive parameter reduction
- Generalization concerns: Evaluation focuses primarily on RoBERTa and XLM-R architectures, raising questions about cross-model applicability
- Implementation details: Critical implementation specifics are underspecified, particularly around the k-means clustering procedure

## Confidence

- High confidence (Theoretical foundation): The core mechanism of decomposing embeddings into shared subspaces is well-grounded in existing parameter-sharing techniques
- Medium confidence (Empirical results): The reported compression rates and accuracy trade-offs appear reasonable given the experimental setup
- Medium confidence (Practical applicability): The method shows promise for real-world deployment in resource-constrained settings

## Next Checks

- Check 1: Parameter sensitivity analysis: Systematically vary the number of subspace embeddings (f) and measure the corresponding accuracy drop across multiple GLUE tasks to establish the precise accuracy-compression tradeoff curve
- Check 2: Cross-model generalization: Apply the subspace embedding approach to architectures beyond RoBERTa (such as BERT, GPT variants, or smaller models like DistilBERT) to validate whether the method's effectiveness is architecture-dependent
- Check 3: Robustness to vocabulary size: Test the method with different vocabulary sizes and subword tokenization schemes to understand how tokenization strategy affects the effectiveness of subspace embedding assignment