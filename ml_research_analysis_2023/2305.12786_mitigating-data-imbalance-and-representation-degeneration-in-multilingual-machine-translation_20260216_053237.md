---
ver: rpa2
title: Mitigating Data Imbalance and Representation Degeneration in Multilingual Machine
  Translation
arxiv_id: '2305.12786'
source_url: https://arxiv.org/abs/2305.12786
tags:
- language
- data
- translation
- bilingual
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Bi-ACL, a method that leverages target-side
  monolingual data and bilingual dictionaries to improve multilingual neural machine
  translation (MNMT). The method addresses two key challenges: data imbalance, where
  long-tail languages lack sufficient parallel data, and representation degeneration,
  where model representations are concentrated in a small subspace.'
---

# Mitigating Data Imbalance and Representation Degeneration in Multilingual Machine Translation

## Quick Facts
- **arXiv ID:** 2305.12786
- **Source URL:** https://arxiv.org/abs/2305.12786
- **Reference count:** 28
- **Primary result:** Bi-ACL improves multilingual NMT by addressing data imbalance and representation degeneration using target-side monolingual data and bilingual dictionaries

## Executive Summary
This paper addresses two fundamental challenges in multilingual neural machine translation (MNMT): data imbalance where long-tail languages lack sufficient parallel data, and representation degeneration where model embeddings become concentrated in a small subspace. The authors propose Bi-ACL, a method that leverages target-side monolingual data and bilingual dictionaries to improve translation quality without requiring additional parallel corpora. Bi-ACL combines bidirectional autoencoding, bidirectional contrastive learning, constrained beam search for pseudo-parallel data generation, and curriculum learning sampling strategies.

## Method Summary
Bi-ACL operates by first generating pseudo-parallel data using constrained beam search with bilingual dictionaries to translate target-side monolingual sentences into source language tokens. The method then employs a bidirectional autoencoder that learns to reconstruct both source and target representations using only target-side monolingual data. Bidirectional contrastive learning with perturbed embeddings is used to create more isotropic representation spaces. A curriculum learning strategy samples training examples based on dictionary coverage scores, starting with high-coverage sentences and gradually introducing more challenging examples. The approach combines these components with a weighted loss function that balances autoencoding and contrastive learning objectives.

## Key Results
- Significant BLEU score improvements across multiple language pairs (e.g., +3.02 on en→ta, +2.62 on ta→tr)
- Enhanced embedding isotropy compared to strong baselines, as measured by I1 and I2 scores
- Demonstrated zero-shot domain and language transfer capabilities without additional parallel data
- Improved performance on long-tail languages that traditionally suffer from data scarcity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constrained beam search generates pseudo-parallel data that enables training on low-resource languages without parallel corpora.
- **Mechanism:** Uses a bilingual dictionary to constrain decoding so that target sentences are translated into source language tokens that exist in the dictionary, creating pseudo-source sentences online during training.
- **Core assumption:** The bilingual dictionary provides sufficient lexical coverage to generate meaningful pseudo-source sentences.
- **Evidence anchors:**
  - [abstract] "We define two modules, named bidirectional autoencoder and bidirectional contrastive learning, which we combine with an online constrained beam search and a curriculum learning sampling strategy."
  - [section 3.1] "We use lexically constrained decoding (i.e., constrained beam search; Post and Vilar, 2018) to generate a pseudo source language sentence Xℓs i in an online mode"
- **Break condition:** If dictionary coverage is too low, generated pseudo-source sentences will be poor quality and training will fail to learn meaningful representations.

### Mechanism 2
- **Claim:** Bidirectional autoencoder with only target-side monolingual data and dictionary can learn to reconstruct both source and target representations, improving low-resource translation.
- **Mechanism:** Backward autoencoder encodes target sentences and decodes to pseudo-source; forward autoencoder encodes pseudo-source and decodes back to target, creating a reconstruction loop that learns useful representations.
- **Core assumption:** The reconstruction objective forces the model to learn meaningful intermediate representations even without parallel data.
- **Evidence anchors:**
  - [abstract] "We define two modules, named bidirectional autoencoder and bidirectional contrastive learning, which we combine with an online constrained beam search and a curriculum learning sampling strategy."
  - [section 3.2] "Our bidirectional autoencoder contains two parts: backward autoencoder and forward autoencoder" with detailed equations showing the reconstruction process
- **Break condition:** If the autoencoder cannot learn meaningful reconstructions, the model will fail to improve translation quality and may even degrade performance.

### Mechanism 3
- **Claim:** Bidirectional contrastive learning with perturbed embeddings creates more isotropic representation spaces and improves translation quality.
- **Mechanism:** Generates positive examples by adding semantic-preserving perturbations to target embeddings and negative examples by adding dissimilar perturbations to source embeddings, then uses contrastive loss to pull positives together and push negatives apart.
- **Core assumption:** Perturbations can be designed to create semantically meaningful positive/negative pairs that improve embedding space structure.
- **Evidence anchors:**
  - [abstract] "To address representation degeneration, we use bidirectional contrastive learning"
  - [section 3.3] Detailed description of how perturbations are generated and used in contrastive loss formulation
  - [section 6.1] Ablation study shows that contrastive losses improve isotropy (I1/I2 scores) and translation quality
- **Break condition:** If perturbations are poorly designed, contrastive learning may create misleading gradients that harm representation quality.

## Foundational Learning

- **Concept:** Neural machine translation architecture and attention mechanisms
  - **Why needed here:** Understanding how MNMT models encode and decode sequences is crucial for implementing the autoencoder and contrastive learning components
  - **Quick check question:** How does the decoder in an MNMT model use encoder outputs and previous tokens to generate the next token?

- **Concept:** Contrastive learning principles
  - **Why needed here:** The bidirectional contrastive learning module relies on creating positive and negative pairs and using contrastive loss functions
  - **Quick check question:** What is the difference between supervised and self-supervised contrastive learning, and when would each be appropriate?

- **Concept:** Curriculum learning strategies
  - **Why needed here:** The sampling strategy orders training examples by dictionary coverage to gradually introduce more challenging examples
  - **Quick check question:** How does ordering training examples by difficulty affect convergence and final performance in deep learning models?

## Architecture Onboarding

- **Component map:** Target-side monolingual data → Constrained beam search → Pseudo-parallel data → Autoencoder + Contrastive learning → Improved MNMT model

- **Critical path:** Monolingual data → Constrained beam search → Pseudo-parallel pairs → Autoencoder + Contrastive learning → Improved MNMT model

- **Design tradeoffs:**
  - Using only target-side data limits the model's ability to learn source-side representations but reduces data requirements
  - Dictionary-based constraints ensure lexical consistency but may limit semantic flexibility
  - Perturbation-based contrastive learning requires careful parameter tuning to avoid misleading gradients

- **Failure signatures:**
  - Poor dictionary coverage → Low-quality pseudo-parallel data → Degraded translation performance
  - Misconfigured perturbations → Ineffective contrastive learning → Worsened embedding space structure
  - Improper curriculum ordering → Slow convergence or poor final performance

- **First 3 experiments:**
  1. Verify constrained beam search generates reasonable pseudo-source sentences by sampling and manual inspection
  2. Test autoencoder reconstruction quality on a small validation set before full training
  3. Evaluate isotropy improvement (I1/I2 scores) on a held-out set after contrastive learning training

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the quality of the bilingual dictionary impact the performance of the Bi-ACL method across different language pairs and domains?
- **Basis in paper:** Explicit - The paper investigates the impact of bilingual dictionary quality on experimental performance, comparing dictionaries from wiktionary and Panlex.
- **Why unresolved:** While the paper shows that using a higher quality dictionary (wiktionary) results in better performance, it does not explore the relationship between dictionary quality and performance across a wider range of language pairs and domains. The study only focuses on a few specific language pairs.
- **What evidence would resolve it:** Conducting experiments using dictionaries of varying quality across a broader set of language pairs and domains would provide insights into the generalizability of the dictionary quality's impact on performance.

### Open Question 2
- **Question:** Is there an optimal size for the target-side monolingual data used in the Bi-ACL method, and how does it vary across different language pairs and domains?
- **Basis in paper:** Explicit - The paper investigates the effect of monolingual corpus size on experimental results, showing that performance is not proportional to the size of the monolingual data.
- **Why unresolved:** The paper only explores a limited range of corpus sizes and does not determine if there is an optimal size for the monolingual data. Additionally, it does not investigate how this optimal size might vary across different language pairs and domains.
- **What evidence would resolve it:** Conducting experiments with a wider range of monolingual corpus sizes and across different language pairs and domains would help determine if there is an optimal size and how it varies.

### Open Question 3
- **Question:** How does the Bi-ACL method perform on other natural language processing tasks beyond machine translation, such as text generation or question answering?
- **Basis in paper:** Explicit - The paper mentions that the Bi-ACL method should work well on other NLP tasks because it relies on bilingual dictionaries and monolingual data, which are easily accessible for many language pairs.
- **Why unresolved:** The paper only evaluates the Bi-ACL method on machine translation tasks and does not provide empirical evidence of its performance on other NLP tasks.
- **What evidence would resolve it:** Conducting experiments applying the Bi-ACL method to other NLP tasks, such as text generation or question answering, and comparing its performance to existing methods would provide insights into its generalizability.

## Limitations

- **Dictionary Coverage Dependency:** Performance critically depends on bilingual dictionary quality and coverage, which may vary significantly across language pairs and could limit effectiveness for languages with poor dictionary coverage.

- **Computational Overhead:** Online pseudo-parallel data generation via constrained beam search may create computational bottlenecks during training, though this aspect is not thoroughly analyzed in the paper.

- **Limited Zero-Shot Validation:** Zero-shot domain and language transfer capabilities are claimed but not comprehensively validated with extensive experimental evidence across truly unseen language pairs and domains.

## Confidence

- **High Confidence:** Core architectural innovations are well-defined with clear mathematical formulations and standard evaluation methodology using BLEU scores and isotropy metrics.
- **Medium Confidence:** Substantial BLEU score improvements are demonstrated, though exact contribution of individual components is not fully isolated in ablation studies.
- **Low Confidence:** Effectiveness of zero-shot transfer capabilities is mentioned but not empirically validated with comprehensive experiments, and potential negative transfer effects are not addressed.

## Next Checks

1. **Dictionary Coverage Analysis:** Measure and report bilingual dictionary coverage percentages for each language pair, particularly focusing on long-tail languages, to quantify the fundamental limitation and identify which languages benefit most from Bi-ACL.

2. **Component Ablation on Resource-Constrained Languages:** Conduct detailed ablation studies on the lowest-resource languages (e.g., en→gu, en→si) to isolate which component (autoencoder, contrastive learning, or curriculum learning) contributes most to performance gains.

3. **Computational Overhead Benchmarking:** Measure and report the additional training time and GPU memory consumption introduced by online constrained beam search and bidirectional training objectives to assess scalability for production deployment.