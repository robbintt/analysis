---
ver: rpa2
title: Test-Time Self-Adaptive Small Language Models for Question Answering
arxiv_id: '2310.13307'
source_url: https://arxiv.org/abs/2310.13307
tags:
- t-sas
- self-adaptive
- tasks
- datasets
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the self-adaptive capabilities of smaller
  language models (LMs) during test-time without labeled data. It addresses the challenge
  of adapting LMs to specific tasks when fine-tuning with labeled data is infeasible.
---

# Test-Time Self-Adaptive Small Language Models for Question Answering

## Quick Facts
- **arXiv ID**: 2310.13307
- **Source URL**: https://arxiv.org/abs/2310.13307
- **Reference count**: 25
- **Key outcome**: T-SAS method achieves 74.68 average F1 on SQuAD, outperforming larger zero-shot LMs

## Executive Summary
This paper introduces T-SAS, a test-time self-adaptive method for small language models to improve question answering performance without labeled data. The approach generates multiple answer candidates using Monte Carlo dropout, applies majority voting to select pseudo-labels, and filters out low-quality samples. T-SAS significantly outperforms baseline methods and larger zero-shot models on three QA benchmarks, demonstrating that smaller models can achieve competitive results through self-adaptation. The method shows particular effectiveness when combined with external knowledge augmentation.

## Method Summary
T-SAS (Test-Time Self-Adaptive Small Language Models) enables smaller LMs to adapt during test-time without labeled data through a three-stage process. First, it generates multiple answer candidates using Monte Carlo dropout to create diverse predictions. Second, it applies majority voting to select the most plausible pseudo-label from these candidates. Third, it filters out samples with low agreement (below 0.7 threshold) to remove potentially incorrect labels. The method is evaluated on three QA datasets (NQ, TQA, SQuAD) with external Wikipedia knowledge, using instruction-finetuned FLAN-T5 models of various sizes.

## Key Results
- Achieves 74.68 average F1 score on SQuAD, outperforming larger zero-shot LMs
- Shows 11.44% relative improvement over zero-shot models on NQ
- Demonstrates higher robustness across diverse prompts compared to baselines
- Small models show significant performance gains when combined with external knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stochastic self-ensemble with MC dropout generates multiple diverse answer samples
- Mechanism: Randomly masking weights during inference creates varied model predictions, enabling sampling from different latent representations
- Core assumption: MC dropout approximates Bayesian uncertainty, producing diverse yet plausible outputs
- Evidence anchors:
  - [abstract]: "we first stochastically generate multiple answers, and then ensemble them while filtering out low-quality samples"
  - [section 3.2]: "{¯a_i,j}^n_{j=1} = {LM~M(d_i, q_i; θ ⊙ M)}^n_{j=1}, where M is a distribution of mask weights and M is a sampled mask weight"
  - [corpus]: weak - no direct corpus support for MC dropout in language models
- Break condition: If mask distribution M collapses to near-deterministic, diversity is lost

### Mechanism 2
- Claim: Majority voting selects the most plausible pseudo-label from ensemble candidates
- Mechanism: The most frequently occurring answer among stochastic samples becomes the pseudo-label for training
- Core assumption: Consistent predictions across multiple stochastic samples indicate higher confidence and correctness
- Evidence anchors:
  - [section 3.2]: "our next objective is to assign one pseudo label ¯a*_i from the set using a majority voting strategy, which selects ¯a*_i with the highest number of occurrences among {¯a_i,j}^n_{j=1}"
  - [section 4.2]: "by integrating external knowledge, the performance of all models, especially smaller ones, is largely improved"
  - [corpus]: weak - majority voting not directly supported in corpus
- Break condition: If all generated answers are equally wrong, voting amplifies error

### Mechanism 3
- Claim: Filtering removes low-agreement samples to improve training quality
- Mechanism: Samples whose pseudo-labels have low occurrence counts are excluded from training
- Core assumption: Low-agreement samples are likely to be incorrect, and removing them prevents negative training effects
- Evidence anchors:
  - [abstract]: "we further propose an automatic filtering strategy to identify and exclude samples, {(q_i, d_i, ¯a*_i)} ⊂ D test_self , that are likely to have incorrect ¯a*_i"
  - [section 4.2]: "Solely relying on the final result of the self-ensemble should be avoided with smaller LMs"
  - [corpus]: weak - filtering strategy not well-documented in corpus
- Break condition: If filtering threshold is too high, useful samples are lost; too low, noise remains

## Foundational Learning

- **Concept**: Stochastic sampling and Monte Carlo methods
  - Why needed here: Understanding MC dropout as a way to generate diverse samples from a probabilistic model
  - Quick check question: What does MC dropout approximate in Bayesian inference, and how does this relate to model uncertainty?

- **Concept**: Ensemble methods and majority voting
  - Why needed here: The core of combining multiple predictions to improve robustness
  - Quick check question: How does majority voting reduce variance in ensemble predictions, and what assumptions does it make about the independence of predictions?

- **Concept**: Filtering strategies based on agreement metrics
  - Why needed here: Identifying and removing low-quality samples to prevent negative training effects
  - Quick check question: What are common agreement metrics for filtering, and how do they relate to the quality of generated labels?

## Architecture Onboarding

- **Component map**: Input -> MC dropout layer -> Voting module -> Filtering module -> Training loop -> External knowledge retriever
- **Critical path**: Generate multiple answers via MC dropout → Apply majority voting to select pseudo-label → Filter low-agreement samples → Train model on remaining samples
- **Design tradeoffs**: Number of MC dropout samples vs. computational cost; Filtering threshold vs. sample retention; Model size vs. need for external knowledge
- **Failure signatures**: Low diversity in MC dropout samples indicates mask collapse; High filtering rate suggests model uncertainty or poor prompts; Performance degradation after filtering suggests over-filtering
- **First 3 experiments**: 1) Verify MC dropout produces diverse outputs by measuring lexical diversity; 2) Test voting accuracy by comparing majority vote to ground truth; 3) Evaluate filtering effectiveness by measuring F1 before/after filtering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of self-adaptive small language models vary across different domains and tasks beyond question answering?
- Basis in paper: [inferred] The paper primarily focuses on question answering tasks and demonstrates the effectiveness of the proposed method in this domain. However, it is unclear how well the approach generalizes to other tasks and domains.
- Why unresolved: The paper does not provide extensive experiments or analysis on the performance of self-adaptive small language models across diverse domains and tasks.
- What evidence would resolve it: Conducting experiments on a wider range of tasks and domains, such as text summarization, sentiment analysis, or machine translation, and comparing the performance of self-adaptive small language models with other baselines would provide insights into the generalizability of the approach.

### Open Question 2
- Question: How does the proposed filtering strategy impact the quality and diversity of the self-generated labels?
- Basis in paper: [explicit] The paper mentions that the filtering strategy is designed to remove potentially incorrect samples based on low agreement among self-generated labels. However, it is unclear how this filtering affects the overall quality and diversity of the labels.
- Why unresolved: The paper does not provide a detailed analysis of the impact of the filtering strategy on the self-generated labels, such as their accuracy, coverage, or diversity.
- What evidence would resolve it: Conducting experiments to measure the quality and diversity of the self-generated labels with and without the filtering strategy, and comparing them with human-annotated labels or other baselines, would provide insights into the effectiveness of the filtering strategy.

### Open Question 3
- Question: How does the proposed stochastic self-ensemble strategy compare to other ensemble methods in terms of performance and efficiency?
- Basis in paper: [explicit] The paper proposes a stochastic self-ensemble strategy that generates multiple answers using Monte Carlo dropout and selects the most plausible answer through majority voting. However, it is unclear how this strategy compares to other ensemble methods in terms of performance and efficiency.
- Why unresolved: The paper does not provide a comprehensive comparison of the proposed stochastic self-ensemble strategy with other ensemble methods, such as weighted averaging or stacking, in terms of their performance and computational efficiency.
- What evidence would resolve it: Conducting experiments to compare the proposed stochastic self-ensemble strategy with other ensemble methods on various tasks and datasets, and measuring their performance and computational efficiency, would provide insights into the relative strengths and weaknesses of different ensemble approaches.

## Limitations
- Relies heavily on MC dropout for generating diverse predictions without clear validation of its effectiveness versus alternative sampling methods
- Filtering threshold of 0.7 appears arbitrary without sensitivity analysis to determine optimal values
- Claims of outperforming larger models require careful interpretation due to instruction tuning and external knowledge augmentation

## Confidence
- **High confidence**: General framework of test-time adaptation using self-generated pseudo-labels is technically sound and well-supported by experimental results
- **Medium confidence**: Implementation details around MC dropout sampling and majority voting are plausible but lack thorough ablation studies
- **Low confidence**: Claims about outperforming larger zero-shot models require careful interpretation given experimental setup

## Next Checks
1. Conduct ablation studies comparing MC dropout sampling versus alternative methods (Top-k, Top-p) to verify that observed improvements stem from stochastic ensemble rather than sampling mechanism
2. Perform sensitivity analysis on the filtering threshold to determine its optimal value across different model sizes and datasets
3. Test method's robustness when applied to models without instruction tuning to isolate contribution of T-SAS from prompt engineering effects