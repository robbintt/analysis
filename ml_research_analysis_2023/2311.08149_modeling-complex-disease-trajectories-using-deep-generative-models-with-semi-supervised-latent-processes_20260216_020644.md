---
ver: rpa2
title: Modeling Complex Disease Trajectories using Deep Generative Models with Semi-Supervised
  Latent Processes
arxiv_id: '2311.08149'
source_url: https://arxiv.org/abs/2311.08149
tags:
- latent
- time
- generative
- trajectories
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a deep generative model for modeling complex
  disease trajectories using latent temporal processes. The key idea is to learn interpretable
  and comprehensive temporal latent representations of an underlying generative process
  that explain observed disease trajectories.
---

# Modeling Complex Disease Trajectories using Deep Generative Models with Semi-Supervised Latent Processes

## Quick Facts
- arXiv ID: 2311.08149
- Source URL: https://arxiv.org/abs/2311.08149
- Authors: 
- Reference count: 40
- Primary result: Semi-supervised deep generative model for modeling complex disease trajectories with interpretable latent representations and uncertainty quantification

## Executive Summary
This paper introduces a deep generative model for modeling complex disease trajectories using latent temporal processes. The key innovation is a semi-supervised approach that learns interpretable and comprehensive temporal latent representations while integrating established medical concepts. By combining unsupervised learning from raw clinical measurements with supervised learning from medical concept labels, the model discovers novel medically-driven patterns while maintaining interpretability. The learned temporal latent processes enable data analysis, clinical hypothesis testing, and personalized online monitoring with uncertainty quantification.

## Method Summary
The method is a deep generative temporal model that learns the joint distribution over observed clinical measurements, medical concept labels, and latent variables. The model consists of prior, likelihood, and guidance networks that model the distributions over latent variables, observed data, and medical concepts respectively. Semi-supervised disentanglement is achieved by constraining subsets of latent dimensions to represent individual medical concepts. Amortized variational inference is used for efficient learning, and the model enables online prediction with uncertainty quantification. The approach was demonstrated on systemic sclerosis data, showing improved interpretability and predictive performance compared to non-ML baselines.

## Key Results
- Outperforms non-ML baselines in predictive performance on systemic sclerosis data
- Achieves good predictive performance, calibration, and coverage for uncertainty quantification
- Provides interpretable latent space facilitating trajectory visualizations and analysis
- Enables discovery of novel medically-driven patterns while integrating domain knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semi-supervised guidance disentangles the latent space by allocating distinct dimensions to different medical concepts.
- Mechanism: The model uses a guidance network that maps subsets of latent dimensions to individual medical concepts, constraining the latent space so each concept is represented by specific dimensions.
- Core assumption: Medical concepts are conditionally independent given the latent variables and can be accurately predicted from distinct subsets of the latent space.
- Evidence anchors:
  - [abstract] "we develop a semi-supervised approach for disentangling the latent space using established medical concepts."
  - [section] "we assume a factorized Gaussian prior distribution per time and latent dimensions"

### Mechanism 2
- Claim: The generative model learns the joint distribution over all observed variables, capturing complex correlations between clinical measurements and medical concepts.
- Mechanism: By modeling p(x, y, z) instead of just p(y|x), the model learns the underlying generative process producing both raw clinical measurements and higher-level medical concepts.
- Core assumption: The underlying generative process is consistent across patients and can be learned from the data.
- Evidence anchors:
  - [abstract] "The key idea is to learn interpretable and comprehensive temporal latent representations of an underlying generative process"
  - [section] "Deep probabilistic generative models... provide a more holistic approach to modeling complex data"

### Mechanism 3
- Claim: The semi-supervised approach improves interpretability by integrating medical domain knowledge into unsupervised learning.
- Mechanism: Sparse labels for medical concepts during training guide the model to learn latent representations consistent with established medical definitions.
- Core assumption: The provided medical concept labels are accurate and representative of underlying disease processes.
- Evidence anchors:
  - [abstract] "By combining the generative approach with medical knowledge, we leverage the ability to discover novel aspects of the disease"
  - [section] "To achieve more interpretable latent temporal spaces, we propose a semi-supervised approach"

## Foundational Learning

- **Variational Inference**: Used to approximate intractable posterior distribution over latent variables; enables efficient learning and inference. Quick check: What is the key difference between variational inference and Markov Chain Monte Carlo methods?

- **Generative Models**: Learn joint distribution over observed and latent variables; capture complex relationships and generate new samples. Quick check: How does a generative model differ from a discriminative model in terms of the distributions it learns?

- **Latent Variable Models**: Use latent variables to represent underlying generative process; capture hidden patterns in data. Quick check: What is the main advantage of using latent variables in a generative model?

## Architecture Onboarding

- **Component map**: Prior network -> Encoder network -> Latent variables -> Decoder network -> Observed data; Guidance networks -> Medical concepts

- **Critical path**: 1) Encode observed data into latent variables using encoder network; 2) Sample from approximate posterior distribution over latent variables; 3) Generate predictions using decoder and guidance networks; 4) Compute loss and update parameters using backpropagation

- **Design tradeoffs**: Complexity vs interpretability (more complex models capture nuanced patterns but harder to interpret); Supervised vs unsupervised learning (semi-supervised allows domain knowledge integration but sensitive to label noise); Deterministic vs probabilistic predictions (probabilistic provides uncertainty estimates but may be less precise)

- **Failure signatures**: Poor performance on held-out data (may indicate overfitting or underfitting); Inconsistent latent representations across patients (model not capturing underlying process well); High uncertainty in predictions (model not confident due to lack of data or model limitations)

- **First 3 experiments**: 1) Train model on subset of data and evaluate performance on held-out test set; 2) Visualize latent space and check if medical concepts are well-separated; 3) Compare semi-supervised model performance to unsupervised baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the semi-supervised guidance approach impact the disentanglement of latent processes for different medical concepts?
- Basis in paper: [explicit] The paper discusses the semi-supervised approach for disentangling the latent space using established medical concepts and shows improved interpretability in the latent space.
- Why unresolved: While the paper demonstrates improved interpretability, the specific impact on disentangling different medical concepts is not fully explored.
- What evidence would resolve it: Additional experiments analyzing disentanglement of latent processes for various medical concepts, quantifying improvement in interpretability.

### Open Question 2
- Question: How does the inclusion of more organs and clinical variables affect the model's performance in capturing complex disease trajectories?
- Basis in paper: [explicit] The paper focuses on modeling involvement of three organs (lung, heart, joints) and uses limited clinical variables.
- Why unresolved: Impact of including more organs and clinical variables on capturing complex disease trajectories is not investigated.
- What evidence would resolve it: Experiments with larger set of organs and clinical variables, comparing performance with and without additional information.

### Open Question 3
- Question: How does the model handle continuous-time data and irregular sampling intervals?
- Basis in paper: [explicit] The paper mentions measurements and medical concepts are irregularly sampled but doesn't discuss handling continuous-time data.
- Why unresolved: Model's ability to handle continuous-time data and irregular sampling intervals is not explicitly addressed.
- What evidence would resolve it: Experiments with continuous-time data and irregular sampling intervals, comparing performance with different approaches for handling time.

### Open Question 4
- Question: How does the model's performance compare to other state-of-the-art models for disease trajectory modeling?
- Basis in paper: [explicit] The paper mentions model outperforms non-ML baselines but doesn't compare to other state-of-the-art models.
- Why unresolved: Relative performance compared to other state-of-the-art models is not discussed.
- What evidence would resolve it: Comparison of model's performance with other state-of-the-art models on common dataset or benchmark.

### Open Question 5
- Question: How can the model be extended to handle more complex medical concepts and incorporate additional domain knowledge?
- Basis in paper: [explicit] The paper demonstrates model's ability to handle medical concepts related to organ involvement and stage but doesn't discuss potential for handling more complex concepts.
- Why unresolved: Model's ability to handle more complex medical concepts and incorporate additional domain knowledge is not explored.
- What evidence would resolve it: Experiments with more complex medical concepts and additional domain knowledge, demonstrating ability to learn and represent these concepts in latent space.

## Limitations

- The effectiveness of semi-supervised disentanglement heavily depends on quality and representativeness of provided medical concept labels, which is not extensively validated
- The model's performance on data with different characteristics (different disease types, measurement frequencies, or patient populations) remains untested
- Computational complexity of the model and its scalability to larger datasets or more complex disease trajectories is not discussed

## Confidence

- **High Confidence**: The core generative modeling framework and variational inference approach are well-established and theoretically sound
- **Medium Confidence**: The semi-supervised disentanglement mechanism is plausible but relies on assumptions about medical concept independence that may not hold in practice
- **Medium Confidence**: The predictive performance and uncertainty quantification results are promising but are based on a single disease dataset (systemic sclerosis)

## Next Checks

1. **Robustness Testing**: Evaluate model's performance on multiple disease types with varying characteristics to assess generalizability
2. **Label Sensitivity Analysis**: Quantify impact of label quality and quantity on semi-supervised disentanglement by systematically varying supervision level
3. **Scalability Assessment**: Measure computational requirements and training/inference times on larger datasets to determine practical deployment constraints