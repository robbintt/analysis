---
ver: rpa2
title: Robust MRI Reconstruction by Smoothed Unrolling (SMUG)
arxiv_id: '2312.07784'
source_url: https://arxiv.org/abs/2312.07784
tags:
- smug
- reconstruction
- image
- uni00000013
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the vulnerability of deep learning-based MRI
  reconstruction models to input perturbations, sampling rate variations, and changes
  in unrolling steps, which can lead to unstable, aliased images. To improve robustness,
  the authors propose a novel framework called Smoothed Unrolling (SMUG) that integrates
  randomized smoothing (RS) into a deep unrolled MRI reconstruction model.
---

# Robust MRI Reconstruction by Smoothed Unrolling (SMUG)

## Quick Facts
- arXiv ID: 2312.07784
- Source URL: https://arxiv.org/abs/2312.07784
- Authors: 
- Reference count: 40
- The paper proposes SMUG, a novel framework that integrates randomized smoothing into deep unrolled MRI reconstruction models to improve robustness against input perturbations and sampling variations.

## Executive Summary
This paper addresses the vulnerability of deep learning-based MRI reconstruction models to input perturbations, sampling rate variations, and changes in unrolling steps. The authors propose a novel framework called Smoothed Unrolling (SMUG) that integrates randomized smoothing (RS) into a deep unrolled MRI reconstruction model. SMUG applies RS at each intermediate unrolling step, specifically to the denoising network, and uses a pre-training and fine-tuning approach to learn the network parameters. The authors provide a theoretical analysis showing that SMUG achieves robustness against data perturbations. They also introduce weighted smoothing as an extension to further improve image quality and sharpness. Experiments on brain and knee MRI datasets demonstrate that SMUG and its weighted variant outperform vanilla MoDL and RS-E2E methods in terms of reconstruction accuracy and robustness to various types of perturbations.

## Method Summary
The SMUG framework integrates randomized smoothing into deep unrolled MRI reconstruction models. It applies RS at each intermediate unrolling step, specifically to the denoising network, by adding Gaussian noise and averaging the outputs. The model uses a pre-training and fine-tuning approach to learn the network parameters, with a theoretical analysis showing robustness against data perturbations. Weighted smoothing is introduced as an extension to further improve image quality and sharpness. Experiments on brain and knee MRI datasets demonstrate that SMUG and its weighted variant outperform vanilla MoDL and RS-E2E methods in terms of reconstruction accuracy and robustness to various types of perturbations.

## Key Results
- SMUG and Weighted SMUG outperform vanilla MoDL and RS-E2E methods in terms of reconstruction accuracy and robustness to input perturbations.
- SMUG achieves theoretical robustness guarantees against data perturbations, as shown in Theorem 1.
- Weighted SMUG improves image quality and sharpness by adaptively weighting denoised images during aggregation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Randomized smoothing at each intermediate unrolled denoising step provides robustness against input perturbations
- Mechanism: By adding Gaussian noise at every iteration of the unrolled model and averaging the denoising network outputs, the model becomes tolerant to small perturbations in the input measurements
- Core assumption: The denoiser network output is bounded in norm (can be enforced via bounded activation functions or clipping)
- Evidence anchors:
  - [abstract]: "SMUG applies RS at each intermediate unrolling step, specifically to the denoising network"
  - [section]: Theorem 1 provides a theoretical bound showing the reconstruction error is controlled by the input perturbation size when using SMUG
  - [corpus]: Missing - no direct supporting evidence found in the corpus
- Break condition: If the denoiser network output is unbounded, the theoretical robustness guarantee no longer holds

### Mechanism 2
- Claim: Weighted randomized smoothing improves image sharpness by adaptively weighting denoised images during aggregation
- Mechanism: Instead of uniform averaging, a learned weight encoder assigns different importance to each denoised image based on its quality, leading to better reconstruction accuracy
- Core assumption: The weight encoder can effectively distinguish between high-quality and low-quality denoised images
- Evidence anchors:
  - [abstract]: "We introduce weighted smoothing as an extension to further improve image quality and sharpness"
  - [section]: "This approach involves the application of weighted RS at each denoising step, and the weighting encoder is trained in conjunction with the denoiser"
  - [corpus]: Missing - no direct supporting evidence found in the corpus
- Break condition: If the weight encoder fails to learn meaningful quality assessments, the weighted smoothing may perform worse than uniform averaging

### Mechanism 3
- Claim: The pre-training + fine-tuning approach provides a robustness-aware initialization for the denoising network
- Mechanism: Pre-training the denoiser on target images with noise ensures it learns to denoise effectively, while fine-tuning with unrolled stability loss adapts it to the reconstruction task while maintaining robustness
- Core assumption: The pre-trained denoiser weights provide a good starting point for fine-tuning that balances reconstruction accuracy and robustness
- Evidence anchors:
  - [section]: "Our rationale is that pre-training can provide a robustness-aware initialization of the DL-based denoising network for fine-tuning"
  - [section]: "We propose an unrolled stability (UStab) loss for fine-tuning Dθ" that guides denoising stability based on ground truth
  - [corpus]: Missing - no direct supporting evidence found in the corpus
- Break condition: If the pre-training phase doesn't capture the essential denoising properties needed for the reconstruction task, the fine-tuning may not converge to a robust solution

## Foundational Learning

- Concept: Compressed sensing and inverse problems in MRI reconstruction
  - Why needed here: Understanding how MRI reconstruction is formulated as an ill-posed inverse problem is crucial for grasping why SMUG's approach is effective
  - Quick check question: What is the fundamental challenge in reconstructing MRI images from undersampled k-space data?

- Concept: Deep unrolled optimization and the MoDL architecture
  - Why needed here: SMUG builds upon the MoDL framework by integrating randomized smoothing into its unrolled structure, so understanding how unrolled networks work is essential
  - Quick check question: How does MoDL combine data consistency and learned regularization in its iterative updates?

- Concept: Randomized smoothing and its application to regression tasks
  - Why needed here: SMUG adapts randomized smoothing, traditionally used for classification, to the regression setting of MRI reconstruction
  - Quick check question: How does randomized smoothing improve model robustness in classification tasks, and what modifications are needed for regression?

## Architecture Onboarding

- Component map:
  AH y → x0 → [DC + Denoised Smoothing]N times → xN

- Critical path:
  AH y → x0 → [DC + Denoised Smoothing]N times → xN

- Design tradeoffs:
  - Smoothing variance (σ): Higher values increase robustness but may reduce accuracy
  - Number of unrolling steps (N): More steps improve reconstruction but increase computation
  - Pre-training vs. fine-tuning: Pre-training provides robustness initialization but requires additional computation

- Failure signatures:
  - Poor reconstruction quality with clean data: May indicate over-smoothing or insufficient model capacity
  - Limited improvement over vanilla MoDL: Could suggest the smoothing variance is too low or the denoiser isn't learning effectively
  - High sensitivity to perturbations despite smoothing: Might indicate the denoiser output isn't properly bounded

- First 3 experiments:
  1. Verify pre-training effectiveness: Train denoiser with noise addition on target images and evaluate denoising performance on noisy-clean image pairs
  2. Test unrolled stability loss: Implement the UStab loss and verify it guides the denoiser toward robustness by comparing reconstructions with and without perturbations
  3. Validate smoothing integration: Apply SMUG to a simple MRI reconstruction problem and measure robustness improvement against input perturbations compared to vanilla MoDL

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical bound on the robustness of SMUG under perturbations, and how does it compare to other methods like RS-E2E?
- Basis in paper: [explicit] The paper provides a theorem (Theorem 1) that derives a bound on the robustness error of SMUG in the presence of data perturbations.
- Why unresolved: While the paper establishes a theoretical bound, it does not explicitly compare this bound to the robustness of other methods like RS-E2E. Such a comparison would require a similar theoretical analysis for RS-E2E, which is not provided in the paper.
- What evidence would resolve it: A theoretical analysis of the robustness of RS-E2E under perturbations, similar to the one provided for SMUG, would allow for a direct comparison of the robustness bounds.

### Open Question 2
- Question: How does the choice of noise level (σ) in the randomized smoothing operation affect the trade-off between accuracy and robustness in SMUG?
- Basis in paper: [explicit] The paper mentions that the constant Cn in the robustness bound depends on the noise level σ, and that the robustness error decreases as σ increases. However, it does not provide a detailed analysis of how σ affects the accuracy of the reconstructed images.
- Why unresolved: While the paper shows that increasing σ improves robustness, it does not quantify the impact on accuracy. A detailed analysis of the accuracy-robustness trade-off for different values of σ would be valuable.
- What evidence would resolve it: Experimental results showing the PSNR/SSIM values of SMUG for different values of σ, both with and without perturbations, would provide insights into the accuracy-robustness trade-off.

### Open Question 3
- Question: Can the weighted smoothing approach in Weighted SMUG be further improved by incorporating more sophisticated weighting strategies?
- Basis in paper: [inferred] The paper introduces weighted smoothing as an extension to SMUG, which uses an encoder to learn weights for different images during the smoothing process. However, it does not explore more advanced weighting strategies, such as those based on image content or denoising performance.
- Why unresolved: While the proposed weighted smoothing approach improves upon conventional RS, there is potential for further improvement by incorporating more sophisticated weighting strategies. The paper does not investigate this possibility.
- What evidence would resolve it: Experimental results comparing the performance of Weighted SMUG with different weighting strategies, such as those based on image content or denoising performance, would provide insights into the potential for further improvement.

## Limitations
- The theoretical robustness guarantee relies on the assumption that the denoiser network output is bounded in norm, which may not hold for all network architectures or activation functions.
- The effectiveness of the weighted smoothing approach depends on the weight encoder's ability to accurately assess denoised image quality, but this capability is not empirically validated in the paper.
- The pre-training + fine-tuning approach's benefits over alternative initialization strategies are not thoroughly explored.

## Confidence
- **High**: The general approach of integrating randomized smoothing into unrolled MRI reconstruction is sound and addresses a real need for robustness against perturbations.
- **Medium**: The theoretical analysis providing a robustness guarantee is valid under the stated assumptions, but the practical impact depends on the network architecture and training procedure.
- **Low**: The effectiveness of the weighted smoothing extension and the specific benefits of the pre-training + fine-tuning approach are not fully substantiated by experimental results.

## Next Checks
1. **Bounded output verification**: Test the denoiser network's output norm with various activation functions and input perturbations to empirically verify the boundedness assumption required for the theoretical robustness guarantee.
2. **Weight encoder ablation**: Conduct an ablation study comparing SMUG with and without the weighted smoothing extension to quantify the impact of the learned weights on reconstruction quality and sharpness.
3. **Pre-training comparison**: Compare the performance of SMUG with different initialization strategies (random initialization, pre-training on denoising, pre-training on reconstruction) to isolate the benefits of the proposed pre-training + fine-tuning approach.