---
ver: rpa2
title: '1-PAGER: One Pass Answer Generation and Evidence Retrieval'
arxiv_id: '2310.16568'
source_url: https://arxiv.org/abs/2310.16568
tags:
- answer
- retrieval
- corpus
- keywords
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents 1-PAGER, a novel approach for question answering
  that integrates document retrieval and answer generation into a single Transformer-based
  model. Unlike traditional "retrieve-and-read" systems that pipeline separate retrieval
  and answer generation modules, 1-PAGER uses a constrained decoder to iteratively
  partition the retrieval corpus by generating search paths of keywords that identify
  relevant documents and an answer string contained in at least one of these documents.
---

# 1-PAGER: One Pass Answer Generation and Evidence Retrieval

## Quick Facts
- arXiv ID: 2310.16568
- Source URL: https://arxiv.org/abs/2310.16568
- Authors: 
- Reference count: 27
- Key outcome: Presents 1-PAGER, a Transformer-based model that unifies question answering and evidence retrieval using constrained decoding to generate search paths and grounded answers

## Executive Summary
1-PAGER introduces a novel approach to question answering that integrates document retrieval and answer generation into a single Transformer-based model. Unlike traditional "retrieve-and-read" systems that pipeline separate retrieval and answer generation modules, 1-PAGER uses a constrained decoder to iteratively partition the retrieval corpus by generating search paths of keywords that identify relevant documents and an answer string contained in at least one of these documents. The paper demonstrates that 1-PAGER is competitive with retrieve-and-read alternatives on both retrieval and answer accuracy metrics, outperforming a comparable closed-book question answering model.

## Method Summary
1-PAGER reformulates the question answering task as a sequence-to-sequence problem where the model generates both a search path (sequence of keywords) and the final answer in a single pass. The approach uses a T5-XXL 1.1 encoder-decoder model with a constrained decoding mechanism that only allows tokens present in documents matching the current search path. The search paths are generated iteratively, with each keyword progressively narrowing the document corpus until the final answer is produced. The system uses an FM-index data structure to efficiently compute valid continuations during decoding and ensure that generated answers are grounded in evidence documents.

## Key Results
- 1-PAGER achieves competitive performance with retrieve-and-read systems on retrieval and answer accuracy metrics
- Outperforms a comparable closed-book question answering model
- Provides interpretable search paths that serve as reasoning chains for answer generation
- Demonstrates effective end-to-end training of a single model for both retrieval and generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constrained decoding grounds generated answers in evidence by restricting keywords to those present in documents matching previously generated keywords
- Mechanism: At each decoding step, the decoder only allows tokens that are n-grams in the current document subset, using the FM-index to efficiently compute valid continuations
- Core assumption: The FM-index can efficiently compute C(k, Dp) for arbitrary document subsets Dp during decoding
- Evidence anchors:
  - [section] "To efficiently implement these constraints, we need a data-structure that can quickly determine both C(k, Dp), the continuation tokens given a document set and P(Dp, k), the subset of documents that contain a given path."
  - [section] "For this, we extend the usage of an FM-index (Ferragina and Manzini, 2000) as described in (Bevilacqua et al., 2022)."
  - [corpus] Weak - the paper notes that arbitrary Dp requires traversal over all documents and is expensive, but claims optimizations reduce cost

### Mechanism 2
- Claim: Search paths provide interpretable reasoning chains that improve answer quality over closed-book QA
- Mechanism: By generating a sequence of keywords that progressively narrow the document corpus, the model performs implicit chain-of-thought reasoning before generating the final answer
- Core assumption: The keywords in the search path are semantically relevant and effectively partition the corpus toward the correct answer
- Evidence anchors:
  - [abstract] "1P output sequences are guaranteed to match at least one document in the evidence corpus."
  - [section] "Part of this improvement comes from the prediction of search paths themselves, reminiscent of chain-of-thought reasoning (Wei et al., 2022), and part is from 1P's constrained decoder, which forces the model to generate answers from passages that contain the keywords."
  - [corpus] Weak - the paper provides qualitative examples but doesn't systematically evaluate the interpretability or reasoning quality of search paths

### Mechanism 3
- Claim: The constrained decoder prevents the model from generating answers not supported by evidence documents
- Mechanism: By only allowing tokens that appear in documents matching the current search path, the decoder ensures the final answer is a substring of at least one retrieved document
- Core assumption: The constrained decoding strategy doesn't overly restrict the model's ability to generate correct answers
- Evidence anchors:
  - [abstract] "1P output sequences are guaranteed to match at least one document in the evidence corpus."
  - [section] "1P represents docids as keyword paths, which are arguably more interpretable, and learns a soft partition over the corpus instead of the hard partition imposed by DSI's clustering."
  - [corpus] Weak - the paper notes that constrained decoding sometimes hurts performance when passages are too short or tokenization is inconsistent

## Foundational Learning

- Concept: FM-index data structure
  - Why needed here: Provides efficient substring search and next-token computation for constrained decoding over the evidence corpus
  - Quick check question: What are the three key operations the FM-index supports that 1P relies on?

- Concept: Sequence-to-sequence modeling
  - Why needed here: 1P reformulates retrieval and answer generation as a single sequence prediction task, allowing end-to-end training with a single Transformer
  - Quick check question: How does 1P serialize the (question, search path) input and output pairs for training?

- Concept: Constrained decoding
  - Why needed here: Ensures generated search paths correspond to actual document substrings and answers are grounded in evidence
  - Quick check question: What is the formula for the set of valid continuation tokens C(k, Dp) at each decoding step?

## Architecture Onboarding

- Component map: T5-XXL 1.1 encoder-decoder model -> FM-index built over evidence corpus -> Constrained beam search decoder -> Output (answer, evidence passage)

- Critical path: Input question → encoder → constrained decoder (iteratively generates keywords and final answer) → output (answer, evidence passage)

- Design tradeoffs:
  - Single model vs. pipelined retrieval + reading: Simpler architecture but potentially lower accuracy than specialized components
  - Constrained decoding vs. unconstrained: Better grounding but slower inference and sensitivity to tokenization
  - Keyword paths vs. dense embeddings: More interpretable but potentially less effective for large-scale retrieval

- Failure signatures:
  - Poor retrieval (low Hits@1): Search paths not effectively partitioning corpus, possibly due to weak keyword selection
  - Answer not in passage: Constrained decoding too restrictive, model can't generate needed tokens
  - Slow inference: FM-index operations for arbitrary subsets too expensive

- First 3 experiments:
  1. Compare constrained vs. unconstrained decoding on a small subset to measure grounding benefits
  2. Vary beam size (1, 3, 5) to find sweet spot between performance and compute
  3. Test on out-of-domain datasets to assess generalization beyond training distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does 1-PAGER's performance compare to state-of-the-art dense retrieval techniques when using the same corpus and query set?
- Basis in paper: [inferred] The paper mentions that 1-PAGER lags behind state-of-the-art dense retrieval techniques on retrieval tasks, but does not provide a direct comparison
- Why unresolved: The paper focuses on comparing 1-PAGER to retrieve-and-read systems and closed-book QA models, but does not include a comparison to the latest dense retrieval methods
- What evidence would resolve it: Conducting experiments comparing 1-PAGER's retrieval performance to state-of-the-art dense retrieval models like ANCE or Condenser on the same corpus and query set

### Open Question 2
- Question: Can 1-PAGER be effectively adapted for low-resource languages with limited structured corpora and knowledge-seeking queries?
- Basis in paper: [inferred] The paper acknowledges limitations in generalization to corpora without rich structure and queries without central entities, and notes that experiments are limited to English
- Why unresolved: The paper does not explore the model's performance on non-English datasets or languages with different linguistic structures and query patterns
- What evidence would resolve it: Evaluating 1-PAGER on multilingual datasets with varying levels of corpus structure and query types, and comparing performance across different language families

### Open Question 3
- Question: How does the constrained decoding approach affect the diversity of generated search paths, and what impact does this have on overall performance?
- Basis in paper: [explicit] The paper discusses how constrained decoding affects search space exploration and mentions issues with poor search spaces and lack of diversity in constrained beam outputs
- Why unresolved: While the paper touches on the effects of constrained decoding, it does not provide a detailed analysis of how path diversity impacts retrieval and answer accuracy
- What evidence would resolve it: Conducting ablation studies comparing unconstrained and constrained decoding with various beam sizes, measuring diversity metrics for generated paths, and correlating path diversity with retrieval and answer accuracy scores

## Limitations
- Performance degrades with short passages or inconsistent tokenization, affecting practical deployment
- FM-index optimization claims are weak with limited runtime analysis and benchmarks
- Search path interpretability lacks systematic evaluation and user studies

## Confidence
- **High Confidence**: The core claim that 1-PAGER can perform both retrieval and generation in a single model is well-supported by experimental results showing competitive performance on standard metrics
- **Medium Confidence**: The claim about interpretability of search paths is supported by qualitative examples but lacks systematic evaluation or user studies to quantify interpretability benefits
- **Low Confidence**: The efficiency claims around FM-index operations for arbitrary document subsets are not adequately validated, with the paper noting potential performance issues but not providing detailed analysis or benchmarks

## Next Checks
1. **Runtime Benchmarking**: Conduct comprehensive timing experiments measuring the actual overhead of FM-index operations for arbitrary document subsets during constrained decoding, particularly for large corpora where the claimed optimizations become critical

2. **Tokenization Robustness Study**: Systematically evaluate 1-PAGER's performance across different tokenization schemes and document preprocessing strategies to quantify the sensitivity to formatting inconsistencies that the paper identifies as a limitation

3. **Interpretability Validation**: Design and execute user studies where human evaluators rate the interpretability and reasoning quality of search paths, comparing 1-PAGER's paths against traditional retrieval results to provide empirical evidence for the claimed interpretability benefits