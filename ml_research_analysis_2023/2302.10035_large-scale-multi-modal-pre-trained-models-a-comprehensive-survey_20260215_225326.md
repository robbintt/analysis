---
ver: rpa2
title: 'Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey'
arxiv_id: '2302.10035'
source_url: https://arxiv.org/abs/2302.10035
tags:
- arxiv
- pre-training
- image-text
- language
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of large-scale multi-modal
  pre-trained models (MM-PTMs), reviewing their development from background to current
  state-of-the-art. The authors introduce the evolution from conventional deep learning
  to pre-training in NLP, CV, and speech, and then discuss the task definition, key
  challenges, and advantages of MM-PTMs.
---

# Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey

## Quick Facts
- arXiv ID: 2302.10035
- Source URL: https://arxiv.org/abs/2302.10035
- Authors: [Not specified in source]
- Reference count: 40
- One-line primary result: Comprehensive survey of large-scale multi-modal pre-trained models covering their development, components, and future research directions

## Executive Summary
This paper provides a comprehensive survey of large-scale multi-modal pre-trained models (MM-PTMs), tracing their evolution from conventional deep learning to current state-of-the-art approaches. The authors systematically review the development of MM-PTMs across natural language processing, computer vision, and speech domains, analyzing key components including data, network architectures, pre-training objectives, and knowledge-enhanced approaches. The survey covers downstream task validation across generative, classification, and regression tasks, while also examining model parameters and hardware requirements.

## Method Summary
The survey methodology involved collecting and organizing relevant literature on MM-PTMs from 2019 to June 2022, followed by systematic analysis and categorization of models based on their components and downstream tasks. The authors conducted literature review, analyzed model components including data, objectives, architectures, and knowledge-enhanced pre-training, then evaluated downstream tasks. Key findings were visualized and summarized, including model parameters, hardware requirements, and experimental results on representative downstream tasks.

## Key Results
- MM-PTMs leverage massive unlabeled multi-modal data to learn generalizable representations without expensive manual annotation
- Transformer architecture enables effective multi-modal pre-training through self-attention mechanisms that capture complex interactions between modalities
- Knowledge-enhanced pre-training can improve reasoning capabilities by incorporating external knowledge sources like knowledge graphs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pre-training on massive unlabeled multi-modal data significantly improves downstream task performance compared to traditional supervised learning.
- **Mechanism**: Large-scale pre-training enables models to learn rich, generalizable representations from vast amounts of data without the need for expensive manual annotation. This approach leverages the inherent structure and patterns in the data to develop robust feature representations that can be fine-tuned for specific tasks.
- **Core assumption**: The scale and diversity of the pre-training data are sufficient to capture the underlying patterns and relationships necessary for good generalization.
- **Evidence anchors**: Success of single-domain pre-trained models like BERT, ViT, GPT; limitations of traditional methods despite large annotated datasets like ImageNet.
- **Break condition**: If the pre-training data lacks sufficient diversity or scale, the model may overfit to specific patterns and fail to generalize well to new tasks.

### Mechanism 2
- **Claim**: The Transformer architecture is a key enabler for effective multi-modal pre-training due to its ability to handle long-range dependencies and complex interactions.
- **Mechanism**: Transformers use self-attention mechanisms to capture relationships between different parts of the input, allowing them to model complex interactions between modalities. This is particularly useful for tasks that require understanding the relationships between visual and textual information.
- **Core assumption**: The Transformer architecture is capable of effectively modeling the interactions between different modalities in a multi-modal setting.
- **Evidence anchors**: Breakthroughs following AlexNet and subsequent deep learning models; Transformer's success on machine translation tasks.
- **Break condition**: If the input data is not properly tokenized or if the attention mechanism is not properly scaled, the Transformer may struggle to capture meaningful relationships between modalities.

### Mechanism 3
- **Claim**: Knowledge-enhanced pre-training can improve the reasoning capabilities of multi-modal models by incorporating external knowledge.
- **Mechanism**: By integrating knowledge graphs or other structured knowledge sources into the pre-training process, models can learn to reason about the relationships between entities and concepts in a more explicit way. This can lead to better performance on tasks that require understanding the semantic meaning of the input.
- **Core assumption**: The external knowledge sources are relevant and accurate enough to improve the model's reasoning capabilities.
- **Evidence anchors**: Knowledge representation learning enables neural networks to fuse knowledge and improve reasoning capabilities; early stages of knowledge-assisted pre-training.
- **Break condition**: If the external knowledge sources are not properly integrated or if they contain errors, they may actually degrade the model's performance.

## Foundational Learning

- **Concept**: Self-supervised learning
  - **Why needed here**: Self-supervised learning is the key to enabling large-scale pre-training on unlabeled data. It allows models to learn useful representations without the need for manual annotation.
  - **Quick check question**: What is the difference between self-supervised learning and supervised learning, and why is self-supervised learning particularly useful for multi-modal pre-training?

- **Concept**: Multi-modal fusion
  - **Why needed here**: Multi-modal fusion is the process of combining information from different modalities (e.g., images and text) into a unified representation. This is a crucial step in multi-modal pre-training.
  - **Quick check question**: What are the different approaches to multi-modal fusion, and how do they impact the performance of multi-modal pre-trained models?

- **Concept**: Contrastive learning
  - **Why needed here**: Contrastive learning is a self-supervised learning technique that learns representations by contrasting similar and dissimilar pairs of data. It is often used in multi-modal pre-training to learn aligned representations across different modalities.
  - **Quick check question**: How does contrastive learning work, and why is it particularly effective for learning aligned representations in multi-modal settings?

## Architecture Onboarding

- **Component map**: Large-scale multi-modal datasets -> Transformer-based architecture with modality-specific encoders and fusion modules -> Pre-training objectives (masked language modeling, contrastive learning) -> Knowledge integration (optional) -> Downstream task evaluation
- **Critical path**: Data collection and preprocessing -> Model architecture design -> Pre-training objective selection -> Knowledge integration (optional) -> Downstream task evaluation
- **Design tradeoffs**:
  - Single-stream vs. cross-stream architectures: Single-stream models are simpler but may struggle with very different modalities, while cross-stream models are more flexible but require more complex fusion mechanisms.
  - Scale of pre-training data: Larger datasets generally lead to better performance but require more computational resources.
  - Type of pre-training objectives: Different objectives (masked language modeling, contrastive learning) may be more effective for different tasks.
- **Failure signatures**:
  - Poor performance on downstream tasks: This could indicate issues with the pre-training data, model architecture, or pre-training objectives.
  - Overfitting: This could indicate that the model is too complex for the available data or that the pre-training objectives are not sufficiently diverse.
  - Slow convergence: This could indicate issues with the optimization algorithm or the model architecture.
- **First 3 experiments**:
  1. Data analysis: Analyze the pre-training data to understand its diversity, quality, and potential biases.
  2. Ablation study: Evaluate the impact of different pre-training objectives on downstream task performance.
  3. Knowledge integration: Test the effectiveness of incorporating external knowledge sources into the pre-training process.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can we effectively scale multi-modal pre-training to more than two modalities?
- **Basis in paper**: [explicit] The authors identify this as a key research direction, noting that existing large-scale PTMs are usually pre-trained on two modalities (like vision and language) and that acquiring real multi-modal data is challenging due to the scarcity of multi-modal imaging devices.
- **Why unresolved**: Current multi-modal datasets and models are primarily focused on vision-language pairs, with limited exploration of other modality combinations like audio, radar, event streams, depth images, or thermal images.
- **What evidence would resolve it**: Development and release of large-scale multi-modal datasets containing aligned samples across three or more modalities, along with pre-trained models demonstrating improved performance on downstream tasks compared to two-modal approaches.

### Open Question 2
- **Question**: How can incremental learning be effectively applied to multi-modal pre-trained big models?
- **Basis in paper**: [explicit] The authors highlight incremental learning as a research direction, noting that pre-training is expensive and current approaches don't consider incremental learning for big models, making it unclear if algorithms developed for traditional deep learning work well for big models.
- **Why unresolved**: Most existing pre-trained models are trained from scratch on massive datasets, and there's limited research on how to efficiently update these models with new data or modalities without retraining from the beginning.
- **What evidence would resolve it**: Development of efficient incremental learning algorithms that can update large multi-modal models with new data/modalities while maintaining or improving performance, with empirical comparisons showing reduced computational costs compared to full retraining.

### Open Question 3
- **Question**: How can knowledge-enhanced multi-modal pre-training be improved beyond current approaches?
- **Basis in paper**: [explicit] The authors identify this as a research direction, noting that current knowledge-assisted pre-training is in early stages and knowledge is often single-modal, independent of multi-modal data, and limited to improving data understanding for models.
- **Why unresolved**: Current approaches primarily use external knowledge graphs or bases in a limited way, without fully leveraging the potential of integrating diverse knowledge types into multi-modal representations or developing knowledge-specific evaluation tasks for pre-training.
- **What evidence would resolve it**: Development of more sophisticated knowledge fusion methods that can integrate diverse knowledge types (factual, commonsense, domain-specific) into multi-modal representations, along with new knowledge-oriented evaluation tasks that can assess the effectiveness of knowledge integration during the pre-training phase.

## Limitations
- The rapidly evolving nature of MM-PTMs means some recent developments may not be included, as the literature review was conducted up to June 2022
- The survey primarily focuses on English-language publications, potentially missing relevant work in other languages
- While comprehensive in scope, the analysis of individual model architectures and their specific performance characteristics is necessarily limited due to space constraints

## Confidence
- **High**: The general framework for understanding MM-PTM development and the categorization of key components (data, objectives, architectures) is well-supported by the literature
- **Medium**: Specific claims about the relative performance of different MM-PTMs are based on reported results, which may vary due to different evaluation protocols and datasets
- **Medium**: The proposed future research directions are plausible but depend on technological advancements and community adoption

## Next Checks
1. Conduct a follow-up analysis incorporating MM-PTM developments from mid-2022 onwards to assess how the field has evolved since this survey
2. Perform a more granular comparison of specific MM-PTM architectures using standardized benchmarks to validate relative performance claims
3. Investigate the reproducibility of key MM-PTM results by attempting to implement and evaluate select models using publicly available codebases and datasets