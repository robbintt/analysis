---
ver: rpa2
title: 'Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning'
arxiv_id: '2307.04869'
source_url: https://arxiv.org/abs/2307.04869
tags:
- learning
- task
- prompt
- fed-cprompt
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Fed-CPrompt, a rehearsal-free federated continual
  learning framework using prompt learning. It addresses catastrophic forgetting in
  distributed CL by enabling asynchronous prompt updates and inter-task knowledge
  transfer.
---

# Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning

## Quick Facts
- arXiv ID: 2307.04869
- Source URL: https://arxiv.org/abs/2307.04869
- Reference count: 40
- Primary result: ~2% higher accuracy and ~2% lower forgetting than prior methods under iid settings

## Executive Summary
Fed-CPrompt addresses catastrophic forgetting in rehearsal-free federated continual learning by using prompt learning techniques. The method introduces asynchronous prompt learning and C2Loss to handle asynchronous task arrival and heterogeneous data distributions in federated settings. By optimizing only ~4% of parameters compared to full-model methods, Fed-CPrompt achieves communication efficiency while maintaining strong performance across various federated CL scenarios.

## Method Summary
Fed-CPrompt uses prefix-tuning to attach task-specific prompts to layers of a pre-trained Vision Transformer (ViT) model. Clients train locally with C2Loss regularization that enforces task discrimination and smooths updates across heterogeneous data. The framework enables asynchronous prompt learning where clients can incorporate knowledge from future tasks while training current tasks. Server aggregates prompt updates via FedAvg and distributes them back to clients. The method is evaluated on CIFAR-100 with 10 clients and 10 tasks under iid, non-iid, and asynchronous conditions.

## Key Results
- Fed-CPrompt achieves 79.43% average accuracy under iid settings, outperforming Fed-CODAP (77.28%) while optimizing only prompts (~4% of parameters)
- C2Loss regularization improves accuracy from 71.30% to 79.30% under non-iid settings
- Asynchronous learning reduces forgetting compared to synchronous approaches, especially under non-iid conditions

## Why This Works (Mechanism)

### Mechanism 1: Asynchronous prompt learning
- Claim: Asynchronous prompt learning allows clients to leverage knowledge from future tasks while training the current task, improving prompt capacity
- Mechanism: When client c trains task Tm, it incorporates task-specific prompts from tasks Tm+1 through Tn (not yet observed by c) downloaded from the server
- Core assumption: Task arrival heterogeneity in federated settings can be exploited as an advantage rather than treated as noise
- Evidence: Table 4 shows Fed-CPrompt with asynchronous learning achieves 79.30% accuracy vs 71.30% without under non-iid settings

### Mechanism 2: C2Loss for task discrimination
- Claim: C2Loss enforces distinct task-specific prompts while smoothing local updates to handle data heterogeneity
- Mechanism: The loss has two components: (1) a margin-based contrastive term that maximizes distance between current task prompt and prompts from all other tasks, and (2) a stability term that restricts prompt change from previous round
- Core assumption: Task discrimination can be improved by explicitly maximizing inter-task prompt distances while maintaining temporal consistency
- Evidence: Table 4 shows Fed-CPrompt with C2Loss achieves 79.30% accuracy vs 71.30% without C2Loss under non-iid settings

### Mechanism 3: Prompt-based communication efficiency
- Claim: Prompt-based communication efficiency enables superior performance with only ~4% of parameters compared to full-model methods
- Mechanism: By optimizing only prompt parameters (4 million) instead of full model weights (86 million), Fed-CPrompt reduces communication overhead while maintaining or improving accuracy
- Core assumption: Prompt parameters can capture task-specific knowledge effectively enough to replace full fine-tuning in federated CL settings
- Evidence: Abstract states "optimizing only ~4% of parameters compared to full-model methods"

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper's core problem is that rehearsal-free FCL suffers from severe forgetting when learning new tasks without access to historical data
  - Quick check question: Why does standard fine-tuning on new tasks cause catastrophic forgetting in continual learning scenarios?

- Concept: Prompt learning and parameter-efficient transfer learning
  - Why needed here: Fed-CPrompt builds on prompt learning techniques to achieve task-specific adaptation without full model updates
  - Quick check question: How do prompt-based methods differ from traditional fine-tuning in terms of parameter efficiency and knowledge transfer?

- Concept: Contrastive learning and margin-based losses
  - Why needed here: C2Loss uses contrastive principles to enforce task discrimination and prompt distinctiveness
  - Quick check question: What is the role of margin values in contrastive loss functions and how do they affect learned representations?

## Architecture Onboarding

- Component map: Clients -> Train local prompts -> Upload to server -> Server aggregates via FedAvg -> Distribute updated prompts -> Next round
- Critical path: Client trains local prompt → uploads to server → server aggregates → distributes updated prompts → next round
- Design tradeoffs: Communication efficiency vs. expressivity (small prompt parameters vs. full model), stability vs. plasticity (C2Loss balance), asynchronous vs. synchronous training
- Failure signatures: Accuracy plateaus despite training (C2Loss margin too high), divergence in accuracy across clients (asynchronous learning misconfigured), poor forgetting metrics (prompt construction ineffective)
- First 3 experiments:
  1. Reproduce baseline comparison under iid settings (Table 1) to validate implementation correctness
  2. Test C2Loss ablation (Table 4) to verify its contribution to performance
  3. Evaluate asynchronous vs synchronous training (Figure 5) to confirm benefit of proposed asynchronous learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Fed-CPrompt scale with the number of clients and tasks beyond the 10 clients and 10 tasks tested in the paper?
- Basis in paper: [explicit] The paper evaluates Fed-CPrompt on CIFAR-100 with 10 clients and 10 tasks, but does not explore scalability to larger numbers of clients or tasks
- Why unresolved: The paper does not provide experimental results or theoretical analysis for scenarios with more than 10 clients or tasks
- What evidence would resolve it: Experiments testing Fed-CPrompt with varying numbers of clients (e.g., 50, 100) and tasks (e.g., 20, 50) to measure performance degradation, communication overhead, and convergence behavior

### Open Question 2
- Question: How sensitive is Fed-CPrompt's performance to the choice of hyperparameters, particularly γ and α in the C2Loss function, and how can optimal values be determined efficiently?
- Basis in paper: [explicit] The paper mentions γ > 0 and α ∈ [0, 1] as hyperparameters in C2Loss but does not provide a systematic study of their impact on performance
- Why unresolved: The paper does not conduct ablation studies or sensitivity analysis for these hyperparameters across different dataset sizes or task distributions
- What evidence would resolve it: A comprehensive hyperparameter sensitivity analysis showing how different values of γ and α affect accuracy and forgetting across various non-iid scenarios and task arrival patterns

### Open Question 3
- Question: What is the impact of task complexity and class similarity on Fed-CPrompt's ability to distinguish between tasks and prevent catastrophic forgetting?
- Basis in paper: [inferred] The paper evaluates on CIFAR-100 with 10 tasks of 10 classes each, but does not analyze how task similarity or complexity affects performance
- Why unresolved: The paper does not provide analysis of task similarity metrics or experiments with tasks of varying complexity or similarity
- What evidence would resolve it: Experiments measuring performance degradation when tasks have high class similarity or when tasks vary significantly in complexity, along with correlation analysis between task similarity metrics and forgetting rates

## Limitations
- C2Loss implementation details remain underspecified, particularly the margin hyperparameter α and its interaction with heterogeneous data distributions
- Prompt architecture specifics (length, embedding dimension, attachment points) are not fully detailed
- Long-term forgetting dynamics beyond 10 tasks are untested

## Confidence

- **High confidence**: Communication efficiency claims (parameter count and reduction), baseline accuracy improvements under iid settings
- **Medium confidence**: C2Loss effectiveness and asynchronous learning benefits, as these rely on less detailed implementation descriptions
- **Low confidence**: Claims about inter-task knowledge transfer mechanisms, as the specific prompt construction approach is not fully specified

## Next Checks
1. Implement C2Loss with varying margin values to identify optimal configuration and verify its contribution to task discrimination
2. Test Fed-CPrompt with 20+ tasks to evaluate long-term forgetting behavior and stability of asynchronous learning
3. Compare prompt-based vs full-model fine-tuning under severe non-iid conditions to establish boundaries of prompt effectiveness