---
ver: rpa2
title: Making Batch Normalization Great in Federated Deep Learning
arxiv_id: '2303.06530'
source_url: https://arxiv.org/abs/2303.06530
tags:
- learning
- training
- federated
- local
- statistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of batch normalization (BN) in
  federated learning (FL). Prior work suggested replacing BN with group normalization
  (GN) in FL due to performance issues, but the authors find BN often outperforms
  GN across various FL settings.
---

# Making Batch Normalization Great in Federated Deep Learning

## Quick Facts
- arXiv ID: 2303.06530
- Source URL: https://arxiv.org/abs/2303.06530
- Authors: 
- Reference count: 27
- Key outcome: Batch normalization (BN) often outperforms group normalization (GN) in federated learning, and FIXBN recovers centralized training accuracy by freezing BN statistics after an exploration stage

## Executive Summary
This paper challenges the conventional wisdom that batch normalization (BN) should be replaced with group normalization (GN) in federated learning. Through extensive experiments across various federated learning settings, the authors demonstrate that BN typically outperforms GN, contrary to prior work. The key insight is that BN's poor performance in federated learning stems from gradient deviation caused by local batch statistics. To address this, they propose FIXBN, a simple method that freezes BN statistics after an exploration stage, effectively mitigating both gradient deviation and statistics mismatch issues while maintaining BN's benefits.

## Method Summary
The authors investigate BN performance in federated learning by comparing it with GN across various settings. They identify that BN's dependency on mini-batch statistics causes gradient deviation in non-IID federated learning scenarios. Their solution, FIXBN, divides training into two stages: an exploration stage where standard federated averaging with BN is performed, followed by a calibration stage where BN statistics are frozen using values from the exploration phase. Additionally, they propose maintaining local SGD momentum without re-initialization after global aggregation to further stabilize training. Experiments are conducted on CIFAR-10, Tiny-ImageNet, and Cityscapes datasets with various non-IID partition schemes and communication frequencies.

## Key Results
- BN outperforms GN across most federated learning settings, contradicting prior assumptions
- FIXBN significantly improves BN performance, recovering centralized training accuracy in many cases
- Maintaining local SGD momentum further stabilizes BN gradients and improves performance
- The proposed method requires no architecture changes and adds minimal overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BN gradients depend on mini-batch statistics, causing gradient deviation in FL
- Mechanism: BN backward gradients include terms dependent on mini-batch statistics. Under non-IID settings, these statistics differ between local and global data, causing gradients to deviate from centralized training even with high communication frequency
- Core assumption: Mini-batch statistics significantly impact backward gradient computation
- Evidence anchors: Abstract states BN gradients depend on local batch statistics; section shows mini-batch dependency prevents FEDAVG from recovering centralized gradients
- Break condition: If mini-batch statistics become identical across clients (IID) or if dependency is removed (e.g., GN)

### Mechanism 2
- Claim: FIXBN mitigates gradient deviation by freezing BN statistics after exploration
- Mechanism: FIXBN uses two-stage training - exploration with standard FEDAVG to find solution space, then calibration using frozen aggregated BN statistics to avoid gradient deviation and statistics mismatch
- Core assumption: Exploration stage allows convergence to reasonable region; fixed statistics thereafter stabilize training
- Evidence anchors: Abstract states FIXBN freezes BN statistics after exploration; section describes using saved statistics from round T* + 1
- Break condition: If exploration stage too short (unreliable statistics) or too long (insufficient calibration)

### Mechanism 3
- Claim: Maintaining local SGD momentum stabilizes BN gradients
- Mechanism: Standard FEDAVG re-initializes local momentum to zero after aggregation, causing initial steps to lack momentum benefits. Maintaining momentum without re-initialization provides gradient stability, especially beneficial for BN-sensitive mini-batch sampling
- Core assumption: Local momentum provides gradient stability particularly beneficial for BN
- Evidence anchors: Section presents keeping local momentum without re-initialization; no direct corpus evidence
- Break condition: If maintained momentum causes instability or benefits don't outweigh statefulness

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: Paper investigates BN usage in FL and proposes FIXBN for FL-specific issues
  - Quick check question: What is the main challenge in FL that affects BN performance?

- Concept: Batch Normalization (BN)
  - Why needed here: BN is primary focus; understanding its mechanism is crucial for issues in FL and proposed solution
  - Quick check question: How do BN statistics (mean and variance) affect forward and backward passes?

- Concept: Group Normalization (GN)
  - Why needed here: GN serves as BN comparison; understanding differences helps explain why BN outperforms GN in most FL settings
  - Quick check question: How does GN differ from BN in normalization and gradient computation?

## Architecture Onboarding

- Component map: Federated Learning (FEDAVG) -> Normalization layers (BN/GN) -> FIXBN (two-stage training) -> Momentum maintenance
- Critical path: 1) Implement FEDAVG with BN/GN comparison, 2) Implement FIXBN with exploration/calibration, 3) Implement momentum maintenance, 4) Conduct experiments on CIFAR-10/Tiny-ImageNet/Cityscapes, 5) Analyze results
- Design tradeoffs: BN vs GN (performance vs stability), FIXBN exploration length (instability vs insufficient calibration), momentum maintenance (statefulness vs communication overhead)
- Failure signatures: BN significantly worse than GN in FL, FIXBN causes instability, momentum maintenance causes divergence
- First 3 experiments: 1) Compare BN/GN performance across FL settings, 2) Test FIXBN with different exploration lengths, 3) Compare stateful vs stateless momentum maintenance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what precise conditions does BN outperform GN in FL, and vice versa?
- Basis in paper: [explicit] Authors find BN often outperforms GN but GN excels in high-frequency communication and extreme non-IID regimes
- Why unresolved: Paper provides empirical observations but no precise mathematical conditions for dominance
- What evidence would resolve it: Systematic theoretical analysis establishing exact conditions (non-IID level, communication frequency) for BN/GN performance

### Open Question 2
- Question: Is gradient deviation fundamental to BN in FL, or can it be theoretically addressed?
- Basis in paper: [explicit] Authors show BN gradients depend on mini-batch statistics causing deviation even with high communication frequency
- Why unresolved: Empirical observation clear but lacks rigorous theoretical framework explaining deviation and potential solutions
- What evidence would resolve it: Formal mathematical proof of gradient deviation and conditions for mitigation

### Open Question 3
- Question: Are there FL-friendly normalization alternatives beyond BN and GN that could outperform both?
- Basis in paper: [explicit] Authors briefly explore alternatives like LN, IN, and FIXUP, finding FIXUP performs well
- Why unresolved: Paper only briefly explores alternatives without comprehensive comparison across diverse FL settings
- What evidence would resolve it: Extensive empirical evaluation of various normalization techniques across multiple FL benchmarks

## Limitations

- Weak corpus evidence supporting proposed mechanisms, particularly BN gradient deviation and FIXBN effectiveness
- Arbitrary exploration stage length (50% of training) without rigorous justification or adaptive methods
- Claim of "no additional costs" may be misleading due to statefulness from maintaining local momentum
- Limited ablation studies on exploration stage timing and performance with different architectures

## Confidence

- High confidence: BN outperforms GN in FL settings (supported by extensive empirical results)
- Medium confidence: BN gradient deviation mechanism (theoretical analysis presented but lacks comprehensive derivation)
- Low confidence: FIXBN generalizability (limited ablation studies, inadequate addressing of extreme non-IID distributions)

## Next Checks

1. **Ablation study on exploration stage timing**: Systematically vary T* from 10% to 90% of training rounds and measure impact on final accuracy across different non-IID degrees to determine optimal timing strategies.

2. **Gradient analysis validation**: Implement gradient tracking during training to empirically measure deviation between BN gradients with local vs global statistics, and quantify how much of performance gap is explained by this mechanism.

3. **Statefulness impact assessment**: Measure memory overhead and communication patterns when maintaining local SGD momentum across clients, and evaluate whether performance gains justify additional resource requirements in practical FL deployments.