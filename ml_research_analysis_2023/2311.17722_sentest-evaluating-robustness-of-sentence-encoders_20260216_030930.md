---
ver: rpa2
title: 'SenTest: Evaluating Robustness of Sentence Encoders'
arxiv_id: '2311.17722'
source_url: https://arxiv.org/abs/2311.17722
tags:
- sentence
- adversarial
- attacks
- language
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates the robustness of sentence encoders against
  adversarial attacks. The authors employ three types of perturbations - character-level
  substitution, word-level synonym replacement, and sentence-level word order shuffling
  - on four datasets (TREC, Emotion, IMDB, BBC News).
---

# SenTest: Evaluating Robustness of Sentence Encoders

## Quick Facts
- **arXiv ID**: 2311.17722
- **Source URL**: https://arxiv.org/abs/2311.17722
- **Reference count**: 40
- **Primary result**: Sentence encoders exhibit poor robustness to adversarial attacks, with accuracy drops up to 15% on perturbed datasets compared to unperturbed ones.

## Executive Summary
This paper evaluates the robustness of sentence encoders (MPNet and DistilRoBERTa) against three types of adversarial perturbations: character-level substitution, word-level synonym replacement, and sentence-level word order shuffling. Experiments on four datasets (TREC, Emotion, IMDB, BBC News) reveal that these encoders are highly vulnerable to character-level attacks, moderately affected by word-level attacks, and surprisingly resilient to complete sentence shuffling. The results suggest that current sentence encoders and their downstream classifiers rely heavily on n-gram patterns rather than semantic or syntactic structure.

## Method Summary
The authors fine-tune sentence transformers (MPNet-base and DistilRoBERTa-base) on four text classification datasets. They generate perturbed test sets using three attack methods: (1) character-level substitution with nearby keyboard keys, (2) word-level synonym replacement using a gensim model, and (3) sentence-level word shuffling. Model performance is evaluated by comparing accuracy, cosine similarity between embeddings of clean and perturbed text, and label overlap percentages.

## Key Results
- Character-level substitution causes accuracy drops up to 15% due to unknown token generation.
- Word-level synonym replacement has minimal impact except on the Emotion dataset, suggesting embeddings capture semantic similarity.
- Even with complete sentence shuffling, models achieve accuracy well above random chance, indicating reliance on bag-of-words semantics rather than syntactic structure.
- Sentence embeddings capture order information, but supervised classifiers fail to leverage it effectively.

## Why This Works (Mechanism)

### Mechanism 1
Character-level substitution degrades performance by introducing unseen tokens that the tokenizer maps to unknown tokens. This forces the model to handle unknown tokens during inference, leading to reduced accuracy. Core assumption: Tokenizers rely on exact character matches and have limited handling for misspellings or substitutions.

### Mechanism 2
Word-level synonym replacement has minimal impact because embeddings of synonyms are close in vector space, leading to small changes in the final sentence embedding. When a word is replaced by a synonym, the resulting word embedding is likely close to the original in the embedding space. Core assumption: Synonym embeddings cluster together and capture semantic similarity.

### Mechanism 3
Sentence-level word shuffling reduces accuracy but embeddings remain highly similar because the model relies on bag-of-words semantics rather than syntactic structure. Shuffling destroys syntactic structure but preserves word content. The encoder's pooling mechanism yields a similar embedding vector, so the downstream classifier still recognizes the topic or class from the bag of words. Core assumption: The sentence encoder is not trained to distinguish sentence structure and only learns word-level semantics.

## Foundational Learning

- **Concept**: Tokenization and unknown token handling
  - Why needed here: Character substitution creates words outside the vocabulary, causing unknown tokens that can disrupt model predictions.
  - Quick check question: What happens when a tokenizer encounters a word it has never seen before?

- **Concept**: Semantic similarity in embedding space
  - Why needed here: Word-level synonym replacement relies on embeddings of synonyms being close; if they drift apart, performance drops.
  - Quick check question: How do you measure whether two words have similar embeddings?

- **Concept**: Order sensitivity in sentence representations
  - Why needed here: Word order shuffling tests whether the encoder uses syntactic structure; if not, embeddings remain similar despite shuffled order.
  - Quick check question: What mechanism in transformer models can encode positional information?

## Architecture Onboarding

- **Component map**: Input text → Tokenizer → Sentence encoder (e.g., DistilRoBERTa-base, MPNet-base) → Sentence embedding → Downstream classifier → Prediction → Perturbation modules (character, word, sentence level) applied to input before tokenization
- **Critical path**: Perturbation → Tokenizer → Sentence encoder pooling → Classifier → Prediction. Most robustness issues stem from tokenization or pooling layer changes.
- **Design tradeoffs**: Using subword tokenization (e.g., BPE) improves robustness to misspellings but increases vocab size. Order-aware encoders (e.g., with explicit positional embeddings) better preserve sentence structure but may be slower. Bag-of-words pooling (mean/max) is order-agnostic; order-aware pooling (e.g., CLS + attention) is more robust to shuffling but computationally heavier.
- **Failure signatures**: High unknown token rate → character-level attacks degrade accuracy. High overlap in predictions between clean and shuffled sets → model is n-gram detector. High cosine similarity between clean and shuffled embeddings → encoder ignores order.
- **First 3 experiments**: 1) Character substitution on a small dataset; check tokenizer unknown token count and accuracy drop. 2) Synonym replacement with context-aware synonym selection; measure embedding cosine similarity and accuracy change. 3) Sentence shuffling on a labeled dataset; evaluate accuracy and embedding similarity, then train a simple binary classifier on embeddings to detect shuffling.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do sentence encoders' embeddings inherently capture sentence order information, or is this merely an artifact of specific model architectures like MPNet? The paper only tests MPNet and DistilRoBERTa, leaving open whether other sentence encoders would show the same behavior.

- **Open Question 2**: What is the minimum level of word shuffling required to significantly degrade model performance while still maintaining semantic coherence for human understanding? The paper uses complete sentence shuffling, which is unrealistic, but does not explore intermediate shuffling scenarios.

- **Open Question 3**: Can supervised classification strategies be improved to better leverage the sentence order information present in sentence encoder embeddings? The paper only tests simple classifiers (e.g., ANN, KNN) and does not explore more sophisticated architectures that might better utilize this information.

## Limitations
- The exact character replacement rules and synonym replacement source are not specified, making exact reproduction difficult.
- The paper assumes unknown token handling is the primary degradation mechanism for character attacks without verification.
- Only two sentence encoder architectures are tested, limiting generalizability to other models.

## Confidence
- **High Confidence**: The claim that sentence encoders show poor robustness to adversarial perturbations is well-supported by the experimental results across four datasets and two model architectures.
- **Medium Confidence**: The interpretation that models are "n-gram detectors" rather than leveraging semantic or syntactic structure is plausible given the results, but relies on indirect evidence.
- **Low Confidence**: The mechanism by which character substitution degrades performance is assumed to be unknown token handling, but the paper does not verify this.

## Next Checks
1. Verify tokenizer unknown token rate by instrumenting the tokenizer to count unknown tokens for each perturbed input and comparing with observed accuracy drop.
2. Context-aware synonym validation by manually or automatically verifying a sample of synonym replacements to ensure semantic equivalence in context.
3. Direct order sensitivity probe by designing an experiment where the encoder's output is explicitly tested for order sensitivity, such as training a classifier to distinguish original from shuffled embeddings.