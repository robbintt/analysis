---
ver: rpa2
title: Quantification of Uncertainty with Adversarial Models
arxiv_id: '2307.03217'
source_url: https://arxiv.org/abs/2307.03217
tags:
- uncertainty
- epistemic
- adversarial
- deep
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "QUAM introduces a novel method for quantifying epistemic uncertainty\
  \ by identifying \"adversarial models\"\u2014models with high posterior probability\
  \ that make different predictions than a reference model. Unlike Deep Ensembles\
  \ or MC dropout, which primarily sample from high-posterior regions, QUAM searches\
  \ for models where both the posterior and KL-divergence to the reference model are\
  \ large."
---

# Quantification of Uncertainty with Adversarial Models

## Quick Facts
- arXiv ID: 2307.03217
- Source URL: https://arxiv.org/abs/2307.03217
- Reference count: 40
- Key outcome: QUAM introduces a novel method for quantifying epistemic uncertainty by identifying "adversarial models"—models with high posterior probability that make different predictions than a reference model.

## Executive Summary
This paper presents QUAM (Quantification of Uncertainty with Adversarial Models), a novel approach for quantifying epistemic uncertainty in deep learning models. Unlike traditional methods like Deep Ensembles or MC dropout that primarily sample from high-posterior regions, QUAM explicitly searches for models where both the posterior and KL-divergence to a reference model are large. By using adversarial model search combined with mixture importance sampling, QUAM achieves superior performance in epistemic uncertainty estimation across multiple tasks including out-of-distribution detection, adversarial example detection, and selective prediction.

## Method Summary
QUAM quantifies epistemic uncertainty by searching for "adversarial models"—models with high posterior probability that make predictions differing significantly from a reference model. The method uses adversarial model search to find these models, then constructs a mixture distribution from them for use in mixture importance sampling. This approach reduces approximation error by ensuring the sampling distribution captures regions where both the posterior and KL-divergence are large, rather than just high-posterior regions. The method is evaluated on both synthetic and real-world datasets including MNIST and ImageNet.

## Key Results
- QUAM significantly outperforms Deep Ensembles and MC dropout in epistemic uncertainty estimation
- Superior performance in out-of-distribution detection tasks across multiple benchmark datasets
- Effective at detecting adversarial examples and misclassifications
- Demonstrates strong performance in selective prediction scenarios where model confidence matters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QUAM reduces epistemic uncertainty estimation error by explicitly searching for posterior modes where both the posterior and KL-divergence to the reference model are large.
- Mechanism: Instead of sampling only from high-posterior regions like Deep Ensembles or MC dropout, QUAM uses adversarial model search to find models that have high posterior probability but also make predictions that differ significantly from the reference model. These adversarial models are then used in mixture importance sampling to construct a sampling distribution that captures regions of high integrand value.
- Core assumption: The epistemic uncertainty integral can be well-approximated if the sampling distribution has components at locations where both posterior and KL-divergence are large.
- Evidence anchors: [abstract] "QUAM searches for models where both the posterior and KL-divergence to the reference model are large." [section] "Models for which the product is large correspond to adversarial models (not adversarial examples!). Adversarial models have both a high posterior as well as a high divergence between their predictions and that of a reference model."

### Mechanism 2
- Claim: Mixture importance sampling with adversarial models provides lower variance estimates of the epistemic uncertainty integral compared to standard importance sampling.
- Mechanism: QUAM constructs a mixture distribution with components at adversarial model locations. This mixture distribution is designed to match the shape of the integrand (posterior × KL-divergence), which reduces the variance of the Monte Carlo estimator compared to using a unimodal approximation of the posterior.
- Core assumption: A sampling distribution that matches the shape of the integrand (including both posterior and KL-divergence) will have lower variance than one that only matches the posterior.
- Evidence anchors: [section] "Most methods sample from a distribution q(˜w) to approximate the integral... As with Deep Ensembles or MC dropout, posterior sampling is often approximated by a sampling distribution q(˜w) that is close to p(˜w|D)." [section] "Thus, we use mixture importance sampling (MIS)... MIS utilizes a mixture distribution instead of the unimodal distribution in standard importance sampling."

### Mechanism 3
- Claim: QUAM excels at epistemic uncertainty estimation for out-of-distribution (OOD) samples because it identifies models that make different predictions for inputs far from the training distribution.
- Mechanism: For OOD samples, QUAM's adversarial model search finds models that assign different classes to the test point while still explaining the training data well. This captures the epistemic uncertainty that arises when different plausible models disagree on OOD predictions, which other methods miss by only sampling from high-posterior regions.
- Core assumption: OOD samples will have high epistemic uncertainty when different models with similar posterior probability make different predictions.
- Evidence anchors: [abstract] "Models for which the product is large correspond to adversarial models (not adversarial examples!). Adversarial models have both a high posterior as well as a high divergence between their predictions and that of a reference model." [section] "Adversarial models are plausible outcomes of model selection, while having a different prediction at the test data point than the reference model."

## Foundational Learning

- Concept: Bayesian posterior inference and its approximation in deep learning
  - Why needed here: QUAM builds on Bayesian principles to estimate epistemic uncertainty as an integral over the posterior. Understanding how to approximate this posterior (e.g., via variational inference, MCMC, or ensembles) is essential to grasp why QUAM's approach differs.
  - Quick check question: What is the difference between aleatoric and epistemic uncertainty in the Bayesian framework?

- Concept: Monte Carlo integration and importance sampling
  - Why needed here: QUAM uses Monte Carlo methods to estimate integrals over the posterior. Understanding importance sampling and its variance properties explains why mixture importance sampling with adversarial models can reduce estimation error.
  - Quick check question: Why does importance sampling with a unimodal distribution have high variance when the target distribution is multimodal?

- Concept: Adversarial attacks and their generalization to model search
  - Why needed here: QUAM uses a form of adversarial search, but instead of attacking inputs, it searches for models that are "adversarial" to a reference model's predictions. Understanding this concept helps in implementing the adversarial model search algorithm.
  - Quick check question: How does the adversarial model search in QUAM differ from traditional adversarial attacks on inputs?

## Architecture Onboarding

- Component map: Reference model -> Adversarial model search -> Mixture distribution -> Epistemic uncertainty estimator
- Critical path:
  1. Train or obtain a reference model on the training data.
  2. For each test point, run adversarial model search to find models that differ from the reference model's prediction but have high posterior.
  3. Construct a mixture distribution from the adversarial models.
  4. Sample from the mixture distribution and use these samples to estimate the epistemic uncertainty integral.

- Design tradeoffs:
  - Computational cost vs. accuracy: Adversarial model search is expensive but provides more accurate uncertainty estimates. Restricting the search to last layers (as done in ImageNet experiments) reduces cost but may miss some uncertainty.
  - Number of mixture components vs. variance: More adversarial models lead to lower variance in the uncertainty estimate but increase computational cost.
  - Temperature parameter in posterior probabilities vs. exploration: Lower temperature focuses on high-posterior models, while higher temperature allows exploration of lower-posterior but high-divergence regions.

- Failure signatures:
  - If QUAM underestimates uncertainty: The adversarial model search may be getting stuck in local optima or the constraint on posterior probability may be too restrictive.
  - If QUAM is computationally prohibitive: The adversarial model search may be too extensive (e.g., searching over all layers instead of just the last layer).
  - If QUAM doesn't improve over baselines: The mixture distribution may not be capturing the integrand's modes, possibly due to insufficient adversarial models or poor placement.

- First 3 experiments:
  1. Implement QUAM on a simple synthetic dataset (e.g., two-moons) and compare the epistemic uncertainty estimate to ground truth from HMC. This validates the core mechanism.
  2. Apply QUAM to a small-scale image classification task (e.g., MNIST) and evaluate OOD detection performance against Deep Ensembles and MC dropout.
  3. Implement QUAM on a subset of ImageNet with last-layer adversarial model search and evaluate its performance on OOD detection and misclassification detection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does QUAM perform when applied to models with different architectures (e.g., transformers, recurrent networks) compared to CNNs?
- Basis in paper: [inferred] The paper primarily evaluates QUAM on CNNs (LeNet, EfficientNet) and does not explore other architectures.
- Why unresolved: The authors did not experiment with non-CNN architectures, leaving uncertainty about QUAM's generalizability.
- What evidence would resolve it: Experiments applying QUAM to models with transformer or recurrent architectures on standard benchmarks.

### Open Question 2
- Question: What is the impact of the number of adversarial models searched on the accuracy of epistemic uncertainty estimation?
- Basis in paper: [explicit] The paper mentions that more samples reduce the approximation error but does not systematically analyze the effect of varying the number of adversarial models.
- Why unresolved: The experiments use a fixed number of adversarial models without exploring the trade-off between computational cost and estimation accuracy.
- What evidence would resolve it: A study varying the number of adversarial models and measuring the resulting epistemic uncertainty estimation accuracy.

### Open Question 3
- Question: How does QUAM handle continuous and high-dimensional output spaces beyond classification and regression?
- Basis in paper: [inferred] The paper focuses on classification and regression tasks, leaving uncertainty about performance in other domains like reinforcement learning or generative modeling.
- Why unresolved: The authors do not provide evidence or theoretical analysis for QUAM's applicability to non-standard output spaces.
- What evidence would resolve it: Experiments applying QUAM to reinforcement learning or generative modeling tasks with continuous or high-dimensional outputs.

### Open Question 4
- Question: What are the theoretical guarantees for QUAM's convergence and variance reduction compared to other uncertainty quantification methods?
- Basis in paper: [explicit] The paper provides theoretical bounds for mixture importance sampling but does not provide rigorous analysis specific to QUAM's performance.
- Why unresolved: While the paper suggests QUAM reduces approximation error, it lacks formal proofs or empirical validation of its superiority over existing methods in all scenarios.
- What evidence would resolve it: Theoretical analysis and empirical experiments comparing QUAM's convergence rates and variance reduction to other methods like Deep Ensembles or MC dropout.

## Limitations

- The computational cost of adversarial model search scales poorly with model depth, requiring restrictions to last layers for deep networks
- Performance in highly multimodal posteriors with multiple distinct explanations requires further validation
- Limited evaluation to classification and regression tasks, with unclear performance on other domains

## Confidence

**High Confidence**: The mechanism of using adversarial models to improve uncertainty estimation is well-supported by the theoretical framework and experimental results. The superiority of QUAM over baseline methods in controlled experiments (synthetic data, MNIST) is clearly demonstrated.

**Medium Confidence**: The ImageNet results showing QUAM's effectiveness when restricted to last-layer search are promising but require more extensive validation. The computational efficiency claims need independent verification, as the paper doesn't provide detailed timing comparisons.

**Low Confidence**: The claim that QUAM will generalize to arbitrary deep learning architectures and datasets without modification is not fully substantiated. The performance on highly complex, real-world distributions with severe covariate shift remains an open question.

## Next Checks

1. **Runtime Analysis**: Implement QUAM on multiple model depths (shallow, medium, deep) and measure the scaling of adversarial model search time. Compare the trade-off between computational cost and uncertainty estimation accuracy against Deep Ensembles with varying ensemble sizes.

2. **Multimodal Posterior Test**: Create a synthetic dataset with a highly multimodal posterior (e.g., using mixture of experts with distinct modes) and evaluate whether QUAM can identify all significant modes. Compare the recovered posterior modes against ground truth from HMC.

3. **Cross-Domain Generalization**: Apply QUAM to a dataset with severe domain shift (e.g., domain adaptation from synthetic to real images) and evaluate its performance in capturing epistemic uncertainty. Compare against methods that explicitly model domain shift, such as domain-adversarial neural networks.