---
ver: rpa2
title: 'BeautifulPrompt: Towards Automatic Prompt Engineering for Text-to-Image Synthesis'
arxiv_id: '2311.06752'
source_url: https://arxiv.org/abs/2311.06752
tags:
- prompts
- prompt
- images
- training
- beautifulprompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents BeautifulPrompt, a generative model for automatic
  prompt engineering in text-to-image synthesis. The model fine-tunes a decoder-only
  language model to rewrite low-quality prompts into high-quality ones, using a dataset
  of 143k prompt pairs.
---

# BeautifulPrompt: Towards Automatic Prompt Engineering for Text-to-Image Synthesis

## Quick Facts
- arXiv ID: 2311.06752
- Source URL: https://arxiv.org/abs/2311.06752
- Reference count: 16
- This paper presents BeautifulPrompt, a generative model for automatic prompt engineering in text-to-image synthesis.

## Executive Summary
This paper introduces BeautifulPrompt, a system that automatically rewrites low-quality text prompts into high-quality ones for text-to-image synthesis. The approach combines supervised fine-tuning of a language model on prompt pairs with reinforcement learning guided by visual feedback. Experiments demonstrate that BeautifulPrompt significantly outperforms baselines like MagicPrompt and ChatGPT, achieving a 57% human preference win rate while generating more aesthetically pleasing images across multiple evaluation metrics.

## Method Summary
BeautifulPrompt uses a BLOOM 1.1B decoder-only language model that is first fine-tuned on 143k paired prompts (low-quality and high-quality versions) using supervised learning. To further improve image quality, the authors implement Reinforcement Learning with Visual AI Feedback (RLVAIF) using Proximal Policy Optimization to maximize rewards from PickScore and aesthetic score models. The system is trained to generate prompts that produce visually appealing images while maintaining semantic consistency with the original input.

## Key Results
- BeautifulPrompt outperforms MagicPrompt and ChatGPT with a human preference win rate of over 57%
- The RLVAIF technique significantly improves image quality compared to supervised fine-tuning alone
- The model generates more aesthetically pleasing images across multiple evaluation metrics including PickScore, aesthetic score, HPS, and CLIP score

## Why This Works (Mechanism)

### Mechanism 1
Supervised fine-tuning on paired prompts teaches the model structural differences between low- and high-quality prompts. By maximizing likelihood of high-quality tokens given low-quality input, the model learns to transform simple descriptions into richer, more detailed prompts. This works because high-quality prompts are consistently longer and more descriptive than low-quality prompts.

### Mechanism 2
Reinforcement Learning with Visual AI Feedback improves prompt quality by directly optimizing visual outcomes. Reward models trained on PickScore and aesthetic scores provide scalar feedback to guide policy updates, encouraging prompts that generate visually appealing images. This assumes visual feedback correlates strongly with human preferences.

### Mechanism 3
Combining two reward signals (PickScore and aesthetic score) yields more stable and effective training. The two metrics capture different aspects of image quality—preference alignment and visual appeal—balancing each other during optimization. This assumes the metrics are complementary and neither dominates the other.

## Foundational Learning

- **Auto-regressive language modeling**: Enables the model to predict high-quality prompt tokens sequentially, conditioned on low-quality input. Quick check: Can you explain why the model predicts tokens one at a time instead of generating the whole prompt in parallel?

- **Reinforcement learning with policy gradient methods**: Allows the model to optimize for non-differentiable visual rewards rather than just next-token likelihood. Quick check: Why is Proximal Policy Optimization (PPO) chosen over vanilla policy gradient here?

- **Text-to-image evaluation metrics**: Provides quantitative signals (PickScore, aesthetic score, CLIP score) to assess prompt effectiveness. Quick check: Which metric would you prioritize if you had to choose one, and why?

## Architecture Onboarding

- **Component map**: BLOOM 1.1B decoder-only backbone → SFT stage → Reward model training → PPO fine-tuning → Inference service
- **Critical path**: Input prompt → Model rewrite → Stable Diffusion generation → Visual reward computation → Model update (if training)
- **Design tradeoffs**: Smaller model size (1.1B) for faster inference vs. potential quality gains from larger models; using synthetic vs. human-labeled data for efficiency
- **Failure signatures**: Prompts become overly generic or lose semantic alignment with input; reward models diverge; PPO training collapses to trivial solutions
- **First 3 experiments**:
  1. Validate that SFT improves prompt length and detail compared to raw input.
  2. Test reward model predictions against human ratings for a small set of images.
  3. Run ablation: RLVAIF vs. SFT only on a held-out prompt set.

## Open Questions the Paper Calls Out

### Open Question 1
How does BeautifulPrompt handle semantic consistency between original prompts and generated prompts when the original prompt contains multiple entities or complex scenes? The paper mentions that BeautifulPrompt sometimes ignores part of the information in the original prompts or generates meaningless prompts due to the auto-regressive nature of language models.

### Open Question 2
What is the optimal balance between PickScore and Aesthetic Score in the reward function for different types of text-to-image synthesis tasks? The paper uses α = 0.7 but doesn't explore how this parameter affects different types of images or tasks.

### Open Question 3
How does the performance of BeautifulPrompt scale with model size beyond the 1.1B parameter BLOOM model? The paper uses a 1.1B parameter model and mentions it's "sufficiently large to accomplish our task effectively," but doesn't explore larger model sizes.

## Limitations

- The effectiveness relies on the assumption that high-quality prompts are consistently longer and more descriptive than low-quality prompts
- Limited evidence that reward models accurately reflect human preferences across diverse image types
- Performance may degrade on domains not represented in the training data

## Confidence

**High confidence**: The SFT mechanism for learning prompt transformations is well-established and implementation details are sufficiently specified for reproduction.

**Medium confidence**: The effectiveness of the combined reward signal is supported by internal metrics but lacks external validation. The 57% human preference win rate is promising but limited to specific models.

**Low confidence**: The RLVAIF training stability and the assumption that reward models perfectly capture human preferences are significant concerns. The paper lacks sufficient evidence that reward models don't develop unintended biases.

## Next Checks

1. **Reward Model Alignment Test**: Generate 100 images using both original and BeautifulPrompt-optimized prompts, then conduct a blinded human preference study comparing images generated from prompts optimized by BeautifulPrompt against those from human-written high-quality prompts.

2. **Domain Generalization Test**: Apply BeautifulPrompt to prompts from domains not represented in the training data (e.g., architectural visualization, medical imaging, or highly technical subjects) and measure performance degradation.

3. **Ablation Study on Reward Components**: Conduct a systematic ablation study varying α (the weight between PickScore and aesthetic score) across the full range [0,1] and measure the resulting performance on each metric.