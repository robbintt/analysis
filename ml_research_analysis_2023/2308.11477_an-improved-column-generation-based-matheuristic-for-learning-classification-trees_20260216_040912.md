---
ver: rpa2
title: An improved column-generation-based matheuristic for learning classification
  trees
arxiv_id: '2308.11477'
source_url: https://arxiv.org/abs/2308.11477
tags:
- split
- constraints
- node
- each
- checks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes improvements to a column-generation-based heuristic
  for learning classification trees. The key modifications include a modified subproblem
  model that reduces the number of subproblems, using data-dependent constraints as
  cutting planes in the master problem, and an optimization model to generate these
  cutting planes.
---

# An improved column-generation-based matheuristic for learning classification trees

## Quick Facts
- arXiv ID: 2308.11477
- Source URL: https://arxiv.org/abs/2308.11477
- Reference count: 6
- Key outcome: Proposed modifications to column-generation-based heuristic for learning classification trees, including merged subproblems, data-dependent constraints as cutting planes, and improved scalability and accuracy over CART on small and large datasets.

## Executive Summary
This paper improves a column-generation-based heuristic for learning univariate binary classification trees of fixed depth. The key modifications include merging subproblems to reduce computational complexity, using data-dependent constraints as cutting planes to strengthen the linear relaxation, and an optimization model to generate these cutting planes. Computational results demonstrate significant accuracy gains over CART on both small and large UCI datasets, with improved scalability compared to previous approaches.

## Method Summary
The method uses column generation with a master problem (MP) and subproblems (SPs) to learn decision trees. Key innovations include merging SPs for multiclass problems to reduce the number of subproblems, proving that data-dependent constraints are implied and can be used as cutting planes, and developing a separation model to generate these cuts for unlabeled rows. The approach also includes preprocessing to remove duplicate rows and initialization with CART-generated split checks. The method is tested on 12 UCI datasets with 50% training and 25% testing splits across 5 random partitions.

## Key Results
- Significant accuracy gains over CART on both small and large datasets
- Improved scalability through reduced number of subproblems (from |Nlf| x |T| to |Nlf|)
- Data-dependent constraints as cutting planes strengthen the linear relaxation
- Preprocessing step reduces dataset size by identifying duplicate rows

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Merging SPs into a single model for each leaf reduces the number of SPs from |Nlf| x |T| to |Nlf|.
- Mechanism: Merged SP incorporates target computation directly via variables wt and constraints (3f)-(3h), allowing one SP per leaf instead of separate SPs for each target class.
- Core assumption: The objective function can be reformulated to maximize weighted correct predictions while still generating columns with positive reduced cost.
- Evidence anchors: Abstract mentions reduced number of SPs; Section 3.1 details the merged SP model.

### Mechanism 2
- Claim: Constraints (1c) in the integer MP are implied by constraints (1b), (1d), and (1f).
- Mechanism: Theorem 3.2 proves that each row follows exactly one path in any feasible solution satisfying (1b), (1d), and (1f), making (1c) redundant but useful as cutting planes.
- Core assumption: The proof's logic holds for any complete binary decision tree of depth k ≥ 1, regardless of the training dataset.
- Evidence anchors: Section 3.2 proves Theorem 3.2 showing (1c) are implied.

### Mechanism 3
- Claim: A separation model (8) can generate cutting planes for unlabeled rows, improving linear relaxation strength.
- Mechanism: Model (8) searches for rows (labeled or unlabeled) that violate implied constraints (1c) by maximizing the sum of dual values on paths, using implied relations between split checks on the same feature.
- Core assumption: Finding such rows strengthens the linear relaxation enough to justify the computational cost of solving (8).
- Evidence anchors: Section 3.3 describes the separation model for generating unlabeled rows.

## Foundational Learning

- Concept: Column generation in MIP
  - Why needed here: The master problem (1) has an exponential number of path variables; column generation generates only needed columns via SPs.
  - Quick check question: What is the role of dual variables from the RMP in generating new columns?

- Concept: Cutting plane method
  - Why needed here: Constraints (1c) are implied but strengthen the linear relaxation; adding them as cuts when violated improves the bound.
  - Quick check question: How does the separation model (8) find a row that violates an implied constraint?

- Concept: Mixed Integer Programming (MIP) formulation of decision trees
  - Why needed here: The paper models decision tree learning as a MIP to optimize accuracy, using path variables and split check consistency constraints.
  - Quick check question: Why are the variable upper bounds in (1) considered implied and droppable?

## Architecture Onboarding

- Component map:
  - Master Problem (MP): Integer MP (1) with path variables xp and split check variables ρj,a
  - Restricted Master Problem (RMP): Linear relaxation of MP restricted to generated columns
  - Subproblem (SP): Merged SP (3) generates new columns with positive reduced cost
  - Separation model: CP-SAT model (8) generates violated constraints (1c) as cuts
  - Preprocessing: Identifies duplicate rows and reduces dataset size
  - Initialization: Generates candidate split checks and initial paths via CART

- Critical path:
  1. Generate candidate split checks via CART (threshold sampling)
  2. Initialize RMP with paths from CART on full dataset
  3. Solve RMP, get duals
  4. Solve merged SP (3) to find new columns
  5. Add columns, repeat until no more or time limit
  6. Optionally solve integer MP with generated columns for final tree

- Design tradeoffs:
  - Fewer SPs (merged) vs. larger SP models
  - Using (1c) as cuts vs. including them from start (All Beta)
  - Solving separation model (8) for unlabeled rows vs. only labeled rows

- Failure signatures:
  - Column generation stalls: SP (3) fails to find positive reduced cost columns
  - Poor linear relaxation: No significant improvement from adding beta cuts
  - Slow solving: Merged SP (3) or separation model (8) takes too long

- First 3 experiments:
  1. Run with and without merged SPs on a small dataset; compare number of SPs solved and columns generated
  2. Compare "No Beta" vs. "Beta Ub" variants on a small dataset; measure linear relaxation gap and solving time
  3. Test preprocessing step 5 on a large dataset; compare model size and solving time before and after

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the linearized MIP version of the separation model for generating beta cuts compared to the CP-SAT solver approach?
- Basis in paper: [explicit] The paper mentions that the separation model can be linearized into a MIP model, but does not explore this approach.
- Why unresolved: The paper only uses the CP-SAT solver for the separation model and does not compare its performance to a linearized MIP version.
- What evidence would resolve it: Computational experiments comparing the performance of the CP-SAT solver and a linearized MIP version of the separation model on the same datasets.

### Open Question 2
- Question: How much does the diving heuristic improve solution quality for the No Beta variant compared to the current approach?
- Basis in paper: [inferred] The paper mentions that the No Beta variant has the best solving time but the worst solutions, and suggests developing a diving heuristic to improve solution quality.
- Why unresolved: The paper does not implement or evaluate the diving heuristic for the No Beta variant.
- What evidence would resolve it: Computational experiments comparing the solution quality of the No Beta variant with and without the diving heuristic on the same datasets.

### Open Question 3
- Question: How much does penalizing the number of active leaves in the integer MP improve generalization performance on out-of-sample datasets?
- Basis in paper: [explicit] The paper mentions that penalizing the number of active leaves is a suggestion to improve generalization performance, but does not evaluate this approach.
- Why unresolved: The paper does not implement or evaluate the effect of penalizing the number of active leaves on generalization performance.
- What evidence would resolve it: Computational experiments comparing the generalization performance of the current approach with and without penalizing the number of active leaves on the same datasets.

## Limitations
- Computational study relies on proprietary solver implementations (Gurobi/CP-SAT) whose parameter settings and internal heuristics significantly influence results
- Preprocessing step claims to remove duplicate rows but doesn't specify exact deduplication criteria or potential information loss
- Separation model (8) for generating unlabeled constraints is computationally expensive, and overhead versus benefit is not thoroughly characterized

## Confidence
- **High Confidence**: Theoretical foundations (Theorem 3.2 on implied constraints, correctness of merged SP formulation) are well-proven and logically sound
- **Medium Confidence**: Empirical improvements over CART are demonstrated, but extent of gains depends on specific solver implementations and parameter choices not fully disclosed
- **Low Confidence**: Practical scalability of the separation model (8) on very large datasets, as computational costs are not thoroughly characterized

## Next Checks
1. Replicate small-scale experiments: Run the complete pipeline on Tic-Tac-Toe dataset with detailed logging of subproblem counts, column generation iterations, and beta cut additions to verify claimed improvements
2. Parameter sensitivity analysis: Systematically vary solver parameters (e.g., time limits, node limits, presolve settings) to assess robustness of the method across different configurations
3. Separation model overhead measurement: Implement and measure the computational cost of solving the separation model (8) versus its impact on solution quality to quantify the tradeoff