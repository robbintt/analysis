---
ver: rpa2
title: 'Pixel to policy: DQN Encoders for within & cross-game reinforcement learning'
arxiv_id: '2308.00318'
source_url: https://arxiv.org/abs/2308.00318
tags:
- learning
- encoder
- performance
- agent
- episode
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that transfer learning can significantly
  reduce the training time and improve the performance of reinforcement learning models
  in Atari games. The authors explored three distinct methods of transfer learning,
  including within-game and cross-game transfer learning, as well as end-to-end training.
---

# Pixel to policy: DQN Encoders for within & cross-game reinforcement learning

## Quick Facts
- arXiv ID: 2308.00318
- Source URL: https://arxiv.org/abs/2308.00318
- Reference count: 7
- Primary result: Transfer learning with frozen encoders reduces DQN training time by 20x on Atari games

## Executive Summary
This work demonstrates that transfer learning can significantly reduce training time and improve performance of reinforcement learning models in Atari games. The authors explore three distinct transfer learning methods: within-game transfer (freezing encoders from same game), cross-game transfer (freezing encoders from similar games), and end-to-end training with initialized weights. The results show that pre-trained encoders from relatively complex tasks lead to faster convergence and better performance on newer tasks with fewer training episodes. Additionally, the authors develop a universal game-playing agent that achieves decent performance on previously unseen environments, with the DQN model achieving mean episode rewards of 46.16 (beating human performance) and 533.42/402.17 on Assault and Space Invader respectively.

## Method Summary
The approach uses DQN with experience replay and target networks as the base algorithm, enhanced with transfer learning techniques. Input frames are preprocessed through grayscale conversion, resizing to 84x84, normalization, and stacking 4 consecutive frames. The CNN architecture consists of three convolutional layers (32/64/64 filters) followed by fully connected layers. Transfer learning is implemented by either freezing pre-trained encoder weights while training only the policy head, initializing the policy head with pre-trained weights, or fine-tuning both encoder and policy head end-to-end. Training is performed asynchronously across multiple parallel environments to accelerate data collection.

## Key Results
- DQN with transfer learning achieved mean episode reward of 46.16, beating human-level performance with merely 20k episodes
- Transfer learning reduced training time by 20x (from 7 days to 7-8 hours) on Atari games
- Universal game-playing agent achieved mean rewards of 533.42 and 402.17 on Assault and Space Invader environments respectively
- End-to-end fine-tuning yielded the best final performance when time constraints allow

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning with frozen encoders reduces training time by 20x in Atari games.
- Mechanism: A pre-trained CNN encoder from a similar Atari game extracts useful visual features. Freezing its weights prevents costly fine-tuning of low-level filters, so only the lightweight policy head learns the new task.
- Core assumption: Visual features useful in one Atari game are largely transferable to similar games due to shared pixel-level structure.
- Evidence anchors:
  - [abstract] "using a pre-trained encoder from a relatively complex task can lead to faster convergence and better performance on a newer task with fewer training episodes."
  - [section 4.3] "Using a pretrained CNN encoder helps in reducing the training time by 20 folds (from 7 days to 7-8 hours)."
- Break condition: If the target game's visual domain shifts too much, frozen low-level features become irrelevant and performance stalls.

### Mechanism 2
- Claim: Initializing the policy head with pretrained weights accelerates learning even more than freezing alone.
- Mechanism: By copying the output layer weights from a pretrained model, the agent starts with a reasonable action-value mapping, so early episodes already exploit good policies rather than exploring randomly.
- Core assumption: Action-value relationships in similar games share enough structure that pretrained Q-values are a good starting point.
- Evidence anchors:
  - [section 4.3.2] "the policy network was initialized with the weights from the pre-trained policy network(head layer)"
  - [section 5.2.3] "end reward going as high as 590 and duration around 800."
- Break condition: If the action spaces differ, the copied softmax layer may misalign, causing initial poor performance until weights adapt.

### Mechanism 3
- Claim: End-to-end fine-tuning of both encoder and policy head yields the best final performance after modest extra episodes.
- Mechanism: The pretrained encoder provides strong feature initialization, but allowing its weights to adapt lets the network specialize to the target game's unique visual quirks, improving final rewards beyond what frozen encoders can achieve.
- Core assumption: The pretrained encoder's feature space is a good basin of attraction; small gradient steps improve rather than destroy useful features.
- Evidence anchors:
  - [section 4.3.3] "both the feature layer and head-layer weights were initialized with weights from the pre-trained encoder, and were not frozen during training."
  - [section 5.3] "if there is no time-constraint then is the best way to train the network if we have a same environment available."
- Break condition: If training budget is very small, unfreezing the encoder may cause overfitting or slow convergence, negating the speed advantage of transfer.

## Foundational Learning

- Concept: Deep Q-Networks (DQN) with experience replay and target networks.
  - Why needed here: DQN is the base algorithm being accelerated by transfer learning; understanding replay buffers and target updates explains why transfer is beneficial.
  - Quick check question: What is the purpose of the target network in DQN, and how does it differ from the policy network?

- Concept: Frame stacking and preprocessing (resize, grayscale, normalization).
  - Why needed here: The paper stacks 4 frames to capture motion; preprocessing reduces input dimensionality and stabilizes learning—critical for fair comparison of transfer vs. scratch training.
  - Quick check question: Why does stacking 4 consecutive frames help the agent learn velocity and direction?

- Concept: Universal policy generalization across multiple Atari games.
  - Why needed here: The universal agent experiment tests whether a single policy can perform decently on unseen games; understanding generalization bounds is key to interpreting its modest performance.
  - Quick check question: How does training on multiple games with the same action space help a policy generalize to a new, unseen game?

## Architecture Onboarding

- Component map:
  Input pipeline (84x84x3 RGB → grayscale → resize → normalize → frame stack) → Feature extractor (3 conv layers with ReLU) → Policy head (2 FC layers with ReLU) → Q-values → Action selection

- Critical path:
  1. Load and preprocess frames into a stacked state
  2. Forward pass through frozen/fine-tuned encoder → Q-values
  3. Select action via epsilon-greedy policy
  4. Store (state, action, reward, next_state, done) in replay buffer
  5. Sample minibatch, compute TD targets with target network, backpropagate only through policy head (if encoder frozen)

- Design tradeoffs:
  - Freezing encoder: Fast training, risk of suboptimal features for new game
  - Fine-tuning encoder: Better final performance, higher compute and risk of overfitting
  - Action-space mismatch: Requires careful handling of output layer size

- Failure signatures:
  - No reward improvement after 100 episodes → likely feature extractor mismatch or poor initialization
  - Loss oscillates → learning rate too high or replay buffer not diverse enough
  - Performance drops after unfreezing encoder → overfitting or destructive gradient updates

- First 3 experiments:
  1. Train DQN from scratch on a simple game (e.g., Breakout) to establish baseline performance and training time
  2. Apply within-game transfer: freeze encoder from same game, train only head; compare convergence speed and final reward
  3. Apply cross-game transfer: freeze encoder from a different but similar game (e.g., SpaceInvaders → Assault), train head; measure reward ceiling and episode count to match baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of a universal game-playing agent compare to specialized agents trained on individual games?
- Basis in paper: [explicit] The paper mentions that the universal agent achieved a mean reward of 300 for 100 game plays on Demon Attack, while the best-performing specialized agent (Demon Attack on Demon Attack) achieved a mean reward of 800.
- Why unresolved: The paper only provides a comparison between the universal agent and one specialized agent. More extensive comparisons across multiple games and agents are needed to fully understand the trade-offs between universal and specialized approaches.
- What evidence would resolve it: Comprehensive experiments comparing the performance of universal agents and specialized agents across a wide range of Atari games, including measures of training time, sample efficiency, and final performance.

### Open Question 2
- Question: What are the limitations of using pre-trained encoders from more complex games to improve performance on simpler games?
- Basis in paper: [explicit] The paper mentions that using encoders from more complex games led to better performance on simpler games, but also notes that the number of output nodes in the final layer of the policy network may need to be adjusted.
- Why unresolved: The paper does not provide a detailed analysis of the limitations or potential drawbacks of using pre-trained encoders from complex games. Further investigation is needed to understand the conditions under which this approach is most effective.
- What evidence would resolve it: Experiments exploring the performance of pre-trained encoders from complex games on a range of simpler games, including cases where the output node adjustment is not straightforward.

### Open Question 3
- Question: How can the training time and sample efficiency of reinforcement learning models be further improved using transfer learning techniques?
- Basis in paper: [inferred] The paper demonstrates that transfer learning can significantly reduce training time and improve performance compared to training from scratch. However, the specific techniques used in this paper (e.g., freezing the encoder, initializing the policy network with pre-trained weights) may not be optimal for all scenarios.
- Why unresolved: The paper focuses on a specific set of transfer learning techniques and does not explore other potential approaches or variations. More research is needed to identify the most effective transfer learning strategies for different types of reinforcement learning tasks.
- What evidence would resolve it: Experiments comparing the performance of various transfer learning techniques on a range of reinforcement learning tasks, with a focus on training time and sample efficiency.

## Limitations

- The lack of detailed hyperparameter specifications, particularly for pre-trained encoder weights and exact DQN configurations used in experiments
- The universal game-playing agent's performance on truly unseen games is not fully characterized, with only two game examples providing performance metrics
- The cross-game transfer results depend heavily on the similarity between source and target games, but no systematic analysis of which game pairs transfer well versus poorly is provided

## Confidence

- **High confidence**: The fundamental mechanism of transfer learning accelerating DQN training through frozen encoders is well-established and theoretically sound.
- **Medium confidence**: The specific performance numbers are likely accurate given the controlled experimental setup, but absolute comparison to state-of-the-art methods is not provided.
- **Low confidence**: The universal agent's ability to achieve "decent performance on previously unseen environments" is vaguely defined, with only two examples provided and no systematic evaluation across diverse game types.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary learning rates, batch sizes, and epsilon decay schedules to determine their impact on transfer learning effectiveness, particularly comparing frozen vs. fine-tuned encoder approaches.

2. **Cross-Game Transfer Matrix**: Conduct a comprehensive study mapping transfer performance between all pairs of Atari games in the experimental set, quantifying how visual and gameplay similarity correlates with transfer success.

3. **Long-term Stability Test**: Train the universal agent on a larger set of games (minimum 10-15) and evaluate performance on a held-out test set of truly unseen games after 1M+ training steps to assess whether initial transfer advantages persist over extended training.