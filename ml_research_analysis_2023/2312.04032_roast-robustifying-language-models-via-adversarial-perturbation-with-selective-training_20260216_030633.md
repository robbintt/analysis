---
ver: rpa2
title: 'RoAST: Robustifying Language Models via Adversarial Perturbation with Selective
  Training'
arxiv_id: '2312.04032'
source_url: https://arxiv.org/abs/2312.04032
tags:
- roast
- robustness
- adversarial
- training
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ROAST introduces a unified fine-tuning technique for improving
  multi-perspective robustness of language models by combining adversarial perturbation
  with selective parameter updates based on task-relevant importance. The method updates
  only critical parameters to minimize deviation from pre-trained knowledge while
  learning under adversarial input.
---

# RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training

## Quick Facts
- **arXiv ID:** 2312.04032
- **Source URL:** https://arxiv.org/abs/2312.04032
- **Reference count:** 34
- **Primary result:** ROAST achieves average relative improvements of 18.39% and 7.63% across four robustness metrics compared to vanilla fine-tuning on sentiment classification and entailment tasks.

## Executive Summary
ROAST introduces a unified fine-tuning technique for improving multi-perspective robustness of language models by combining adversarial perturbation with selective parameter updates based on task-relevant importance. The method updates only critical parameters to minimize deviation from pre-trained knowledge while learning under adversarial input. In experiments across sentiment classification and entailment tasks, ROAST achieves substantial improvements in distribution-shift generalization, adversarial robustness, calibration, and anomaly detection while maintaining validation accuracy. It consistently outperforms state-of-the-art fine-tuning methods and demonstrates generalizability across six different language model architectures.

## Method Summary
ROAST combines adversarial perturbation during fine-tuning with selective parameter updates based on importance scores to improve multi-perspective robustness. The method computes parameter importance using squared gradient sums over training data, then samples masks from a smooth logistic function to selectively update only high-importance parameters. During training, ROAST applies embedding-level adversarial noise using single-step gradient ascent and trains on both original and perturbed inputs with bidirectional KL divergence consistency loss. This approach balances robustness to adversarial examples with preservation of pre-trained knowledge, achieving significant improvements across multiple robustness metrics while maintaining clean accuracy.

## Key Results
- ROAST achieves average relative improvements of 18.39% and 7.63% compared to vanilla fine-tuning across four robustness metrics.
- The method consistently outperforms state-of-the-art fine-tuning methods including AdvEmbed and WConsol on sentiment classification and entailment tasks.
- ROAST demonstrates generalizability across six different language model architectures including RoBERTa, BERT, and T5 variants.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selective gradient masking based on relative importance preserves generalizable pre-trained knowledge while allowing task-specific adaptation.
- **Mechanism:** ROAST computes importance scores using squared gradients over training data, then samples masks from a smooth logistic function to selectively update only high-importance parameters.
- **Core assumption:** The squared gradient sum approximates Fisher information and correlates with parameter importance for the given task.
- **Evidence anchors:**
  - [abstract] "ROAST updates only critical parameters to minimize deviation from pre-trained knowledge"
  - [section] "To measure the relative importance score s(θ) of each model parameter θ ∈ Θ, we use the sum of the square of gradients"
  - [corpus] No direct support found for squared gradient approximation to Fisher; this is theoretical assumption
- **Break condition:** If the gradient distribution is highly non-Gaussian or importance scores are noisy, the selective update could remove critical parameters needed for robustness.

### Mechanism 2
- **Claim:** Adversarial perturbation during fine-tuning improves robustness across multiple perspectives by exposing the model to challenging inputs.
- **Mechanism:** ROAST adds embedding-level adversarial noise using single-step gradient ascent, then trains on both original and perturbed inputs with bidirectional KL divergence consistency loss.
- **Core assumption:** Embedding-level perturbations transfer well to word-level adversarial robustness and don't degrade clean accuracy significantly.
- **Evidence anchors:**
  - [abstract] "ROAST introduces adversarial perturbation during fine-tuning"
  - [section] "To constructex, we use a single-step gradient ascent with a step size δ under ℓ∞ norm"
  - [corpus] No direct evidence found supporting embedding-level vs word-level effectiveness; theoretical assumption
- **Break condition:** If adversarial perturbations become too large, they could push embeddings into semantically invalid regions, degrading both accuracy and robustness.

### Mechanism 3
- **Claim:** The combination of adversarial training and selective updating provides complementary benefits that neither approach achieves alone.
- **Mechanism:** ROAST simultaneously learns from perturbed inputs while constraining updates to preserve pre-trained knowledge, balancing robustness and generalization.
- **Core assumption:** The two sources of robustness (perturbation tolerance and pre-trained knowledge preservation) are complementary rather than competing.
- **Evidence anchors:**
  - [abstract] "ROAST effectively incorporates two important sources for the model robustness, robustness on the perturbed inputs and generalizable knowledge in pre-trained LMs"
  - [section] "The high-level idea of ROAST is effectively incorporating two important sources for the model robustness during fine-tuning"
  - [corpus] No direct evidence found for complementary benefits; assumed from experimental results
- **Break condition:** If the regularization strength is mis-specified, the methods could interfere rather than complement, leading to suboptimal performance on both fronts.

## Foundational Learning

- **Concept:** Adversarial training fundamentals
  - Why needed here: ROAST builds on adversarial training techniques but adapts them for language models
  - Quick check question: What's the difference between embedding-level and token-level adversarial perturbations?

- **Concept:** Gradient-based parameter importance
  - Why needed here: The selective update mechanism relies on computing and using parameter importance scores
  - Quick check question: How does squared gradient sum approximate Fisher information for parameter importance?

- **Concept:** Smooth approximations to step functions
  - Why needed here: The logistic function smooth approximation enables differentiable sampling for gradient masking
  - Quick check question: What happens to the logistic function as β approaches infinity?

## Architecture Onboarding

- **Component map:** Input pipeline -> Adversarial perturbation -> Backward pass -> Importance accumulation -> Mask generator -> Masked parameter update -> Consistency checker
- **Critical path:** Forward pass → Adversarial perturbation → Backward pass → Importance accumulation → Mask sampling → Masked parameter update
- **Design tradeoffs:**
  - Single-step vs multi-step adversarial perturbation (efficiency vs effectiveness)
  - Hard thresholding vs smooth approximation (simplicity vs calibration)
  - With vs without scaling term (theoretical correctness vs empirical performance)
- **Failure signatures:**
  - Training instability → Check adversarial step size and gradient accumulation
  - Degraded clean accuracy → Verify importance score computation and masking ratio
  - Poor robustness gains → Validate adversarial example construction and consistency loss
- **First 3 experiments:**
  1. Ablation: Run ROAST with and without adversarial perturbation to isolate its contribution
  2. Hyperparameter sweep: Vary α (masking ratio) and β (smoothness) to find optimal settings
  3. Comparison: Test against baseline methods (AdvEmbed, WConsol) on single LM task before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the specific masking hyper-parameters (α and β) interact with different pre-trained model architectures (e.g., encoder-only vs decoder-only) to affect multi-perspective robustness?
- **Basis in paper:** [explicit] The paper notes that relatively higher masking ratio α was effective for sentiment classification while smaller α was effective for entailment, and that different models showed varying baseline robustness.
- **Why unresolved:** The experiments only optimized hyper-parameters separately for each task and model, without systematically analyzing how masking parameters interact with model architecture characteristics.
- **What evidence would resolve it:** A controlled experiment varying α and β across multiple model architectures while holding other factors constant, with quantitative analysis of how these interactions affect each robustness metric.

### Open Question 2
- **Question:** Does ROAST's selective training approach have different effects on rare vs frequent features/classes, and how does this impact worst-group performance?
- **Basis in paper:** [inferred] The paper demonstrates improvements across multiple robustness metrics but doesn't analyze how selective training affects feature frequency distributions or worst-case performance.
- **Why unresolved:** The experiments focus on aggregate performance metrics without examining how selective training impacts different feature frequencies or specific subgroups within the data.
- **What evidence would resolve it:** Detailed analysis of feature importance distributions before and after ROAST training, with subgroup performance metrics to identify any bias introduced by the selective training process.

### Open Question 3
- **Question:** How does the trade-off between preserving pre-trained knowledge and learning task-specific features change across different fine-tuning durations with ROAST?
- **Basis in paper:** [explicit] The paper mentions that stronger adversarial training can cause larger deviation from pre-trained models, and includes a qualitative analysis of mask dynamics, but doesn't systematically study how this trade-off evolves over time.
- **Why unresolved:** The experiments use fixed training durations without analyzing how the balance between preservation and adaptation changes as training progresses.
- **What evidence would resolve it:** Longitudinal studies tracking model similarity metrics, task performance, and robustness measures across varying training durations with ROAST, identifying optimal stopping points for different tasks.

## Limitations
- The selective importance-based masking assumes squared gradient sums reliably approximate Fisher information for parameter importance, with limited empirical validation across diverse NLP tasks.
- The embedding-level adversarial perturbation strategy may not transfer optimally to word-level robustness, and lacks direct comparison with token-level perturbation methods.
- The bidirectional KL divergence consistency loss introduces computational overhead that may not scale efficiently to very large models or datasets.

## Confidence

- **High confidence:** The experimental methodology and evaluation framework are well-specified, with clear protocols for measuring multi-perspective robustness across standard NLP benchmarks.
- **Medium confidence:** The theoretical foundations of combining adversarial training with selective parameter updates are sound, but the complementary benefits claimed require more rigorous ablation studies.
- **Medium confidence:** The reported improvements (18.39% and 7.63% average relative gains) are substantial, though the specific hyperparameter settings and implementation details would benefit from more explicit documentation for reproducibility.

## Next Checks

1. Conduct ablation studies comparing ROAST with variants that use different importance scoring methods (e.g., random masking vs. gradient-based masking) to isolate the contribution of selective training.
2. Test the embedding-level adversarial perturbation approach against word-level perturbation baselines on the same tasks to quantify the effectiveness of the embedding strategy.
3. Perform scaling experiments with larger language models and more diverse NLP tasks to evaluate whether the robustness gains generalize beyond the reported SST-2 and MNLI benchmarks.