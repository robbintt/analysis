---
ver: rpa2
title: 'AutoST: Training-free Neural Architecture Search for Spiking Transformers'
arxiv_id: '2307.00293'
source_url: https://arxiv.org/abs/2307.00293
tags:
- spiking
- autost
- search
- performance
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoST introduces a training-free NAS method for Spiking Transformers,
  addressing the limitations of existing SNN architectures derived from ANNs. The
  key innovation is using Floating-Point Operations (FLOPs) as a performance metric,
  circumventing the non-differentiability and sparsity issues inherent to SNNs.
---

# AutoST: Training-free Neural Architecture Search for Spiking Transformers

## Quick Facts
- arXiv ID: 2307.00293
- Source URL: https://arxiv.org/abs/2307.00293
- Reference count: 40
- Key outcome: AutoST achieves up to 9.47% higher accuracy on Tiny-ImageNet and 53.9% energy reduction on CIFAR-100 compared to state-of-the-art SNN architectures

## Executive Summary
AutoST introduces a training-free Neural Architecture Search method for Spiking Transformers that overcomes the non-differentiability and sparsity challenges inherent to Spiking Neural Networks (SNNs). The key innovation uses Floating-Point Operations (FLOPs) as a performance metric, enabling architecture evaluation without training or gradient calculations. Additionally, AutoST leverages activation patterns during initialization to estimate energy consumption, allowing the search for energy-efficient architectures. Extensive experiments demonstrate that AutoST-discovered models outperform existing SNN architectures on both static and neuromorphic datasets.

## Method Summary
AutoST employs an evolutionary search algorithm to explore the design space of Spiking Transformers, using FLOPs as a performance predictor and activation patterns to estimate energy consumption. The method searches over architectural dimensions including embedding size, number of attention heads, MLP ratio, and depth. Rather than training each candidate architecture, AutoST evaluates models based on FLOPs (computational complexity) and activation patterns (energy proxy), combining these into an AutoST score that balances performance and efficiency. The search discovers wider, shallower architectures compared to traditional SNNs, optimized for both accuracy and energy efficiency.

## Key Results
- AutoST models achieve up to 9.47% higher accuracy on Tiny-ImageNet compared to state-of-the-art SNN architectures
- AutoST achieves 1.3% accuracy improvement while reducing energy consumption by 53.9% on CIFAR-100
- Discovered architectures are wider and shallower than traditional SNN designs, optimized for energy efficiency

## Why This Works (Mechanism)

### Mechanism 1
Using Floating-Point Operations (FLOPs) as a performance metric bypasses the non-differentiability and high sparsity issues inherent to SNNs. FLOPs measure computational complexity directly from model architecture without requiring training or gradient calculations, making it independent of spike-based dynamics. The core assumption is that model performance correlates strongly with FLOPs when comparing architectures within similar parameter ranges.

### Mechanism 2
Activation patterns during initialization can estimate energy consumption of Spiking Transformers. The frequency of spikes (activation patterns) directly correlates with synaptic operations, which determine power consumption in neuromorphic hardware. The core assumption is that spike frequency during initialization approximates the spike frequency after training.

### Mechanism 3
The AutoST score balances performance and energy efficiency through weighted combination of FLOPs and activation pattern scores. By tuning the coefficient λ, the search can prioritize either accuracy or efficiency depending on application requirements. The core assumption is that linear combination of performance and efficiency metrics adequately captures the trade-off between them.

## Foundational Learning

- **Spiking Neural Networks (SNNs) and their non-differentiability**: Understanding why traditional NAS methods fail for SNNs and why FLOPs work
  - Quick check: Why can't we use backpropagation gradients for SNNs during architecture search?

- **Transformer architecture components (Self-Attention, MLP blocks)**: AutoST modifies these components for spiking neurons and needs to calculate FLOPs accordingly
  - Quick check: How does the spiking neuron model change the computational complexity of self-attention layers?

- **Neural Architecture Search (NAS) methods and their limitations**: Understanding the evolution from RL-based to training-free NAS and why training-free is critical for SNNs
  - Quick check: What makes training-free NAS particularly advantageous for SNNs compared to traditional NAS methods?

## Architecture Onboarding

- **Component map**: Spiking Patch Embedding (SPE) → L × (Spiking Self Attention (SSA) + Spiking MLP (SMLP)) → Global Average Pooling (GAP) → Fully Connected (FC)
- **Critical path**: SPE → L × (SSA + SMLP) → GAP → FC
- **Design tradeoffs**:
  - FLOPs vs accuracy: Higher FLOPs generally indicate better performance but also higher computational cost
  - Depth vs width: AutoST favors wider but shallower architectures compared to traditional SNNs
  - Timesteps: More timesteps improve accuracy but increase latency and energy consumption
- **Failure signatures**:
  - Poor FLOPs-accuracy correlation suggests architectural differences not captured by FLOPs
  - Inconsistent activation patterns indicate unstable energy estimation
  - Suboptimal λ values lead to architectures that don't meet performance or efficiency requirements
- **First 3 experiments**:
  1. Verify FLOPs correlation by training 100 random architectures with different FLOPs and measuring accuracy
  2. Test activation pattern-based energy estimation by comparing predicted vs measured power consumption
  3. Validate AutoST search by comparing discovered architectures against manual designs on CIFAR-10

## Open Questions the Paper Calls Out

- Can AutoST be effectively implemented on specialized neuromorphic hardware to further enhance energy efficiency and reduce latency?
- How does the choice of the coefficient λ in the AutoST score affect the trade-off between performance and energy efficiency in different application scenarios?
- What are the limitations of using FLOPs as a performance metric for SNNs, and are there alternative metrics that could provide a more accurate prediction of model performance?

## Limitations

- Performance gains are measured against unspecified SNN baselines, making real-world significance difficult to assess
- The evolutionary search is limited to specific architectural dimensions, potentially missing optimal configurations outside this space
- The activation pattern-based energy model may not capture hardware-specific inefficiencies or communication overheads

## Confidence

- **High confidence**: The training-free NAS methodology and experimental validation framework are sound
- **Medium confidence**: The FLOPs correlation with SNN performance and activation-based energy estimation require external validation
- **Low confidence**: Cross-dataset generalization of AutoST-discovered architectures and scalability to larger models

## Next Checks

1. Conduct ablation studies on FLOPs correlation by testing architectures with identical FLOPs but different parameter distributions
2. Implement and verify energy estimation using multiple neuromorphic hardware platforms to test hardware dependence
3. Test AutoST-discovered architectures on out-of-distribution datasets to assess generalization beyond training domains