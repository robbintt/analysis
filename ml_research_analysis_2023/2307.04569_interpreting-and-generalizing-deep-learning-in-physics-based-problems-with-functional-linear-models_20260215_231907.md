---
ver: rpa2
title: Interpreting and generalizing deep learning in physics-based problems with
  functional linear models
arxiv_id: '2307.04569'
source_url: https://arxiv.org/abs/2307.04569
tags:
- neural
- learning
- data
- interpretable
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to enhance interpretability
  and generalization in deep learning models for physics-based problems by leveraging
  functional linear models from functional data analysis (FDA). The method involves
  creating an interpretable surrogate model for a trained neural network using integral
  equations, which can be learned through sparse regression from a library of candidate
  kernel functions.
---

# Interpreting and generalizing deep learning in physics-based problems with functional linear models

## Quick Facts
- arXiv ID: 2307.04569
- Source URL: https://arxiv.org/abs/2307.04569
- Reference count: 40
- Primary result: Functional linear models provide interpretable surrogates for deep learning that achieve comparable accuracy while improving out-of-distribution generalization in physics-based problems

## Executive Summary
This paper introduces a novel approach to enhance interpretability and generalization in deep learning models for physics-based problems by leveraging functional linear models from functional data analysis. The method creates an interpretable surrogate model for trained neural networks using integral equations, which can be learned through sparse regression from a library of candidate kernel functions. This approach allows for both post-hoc interpretation of existing neural networks and interpretable operator learning directly from training data. The framework demonstrates comparable accuracy to deep learning models while improving out-of-distribution generalization across multiple scientific domains including solid mechanics, fluid mechanics, and transport.

## Method Summary
The proposed method involves probing a trained neural network to generate input-output pairs, constructing a regression matrix by evaluating candidate integral equations on this data, and solving a sparse regression problem to find coefficients that form an interpretable functional linear model. The integral equations map input functions to output functions through kernel functions, which are selected from a predefined library using L1-regularized regression. This creates a surrogate model that maintains the accuracy of the original neural network while providing mathematical transparency about how inputs influence outputs.

## Key Results
- Functional linear models achieve comparable accuracy to deep learning models on in-distribution data
- The interpretable approach shows improved out-of-distribution generalization compared to black-box neural networks
- The method successfully interprets neural networks across diverse scientific domains including solid mechanics, fluid mechanics, and transport

## Why This Works (Mechanism)

### Mechanism 1
Integral equations provide a mathematically interpretable mapping between input and output functions, improving generalization compared to black-box neural networks. The functional linear model uses integral equations of the form $u(x) = \int \psi(x, \xi)f(\xi) d\xi$ to explicitly show how input function values at different points influence the output, making the mapping transparent. Core assumption: The underlying physics can be approximated by linear integral operators, or at least by piecewise linear operators within localized regions. Evidence anchors: [abstract] "generalized functional linear models as an interpretable surrogate for a trained deep learning model" [section] "Integral equations provide a mathematical framework that encourages the development of interpretable models by explicitly defining the relationships between variables." Break condition: The physics being modeled is highly nonlinear and cannot be approximated well by linear or piecewise linear integral operators, or when the kernel function cannot be learned accurately from limited data.

### Mechanism 2
Using a library of candidate kernel functions with sparse regression enables flexible kernel selection while maintaining interpretability. Instead of pre-specifying a single kernel function, the method creates a library of candidate kernels with different functional forms and bandwidths, then uses L1-regularized regression to select the most relevant terms, balancing accuracy and interpretability. Core assumption: The true kernel function can be well-approximated by a sparse combination of functions from the predefined library. Evidence anchors: [abstract] "A library of generalized functional linear models with different kernel functions is considered and sparse regression is used to discover an interpretable surrogate model" [section] "Instead of pre-specifying the kernels Ïˆ, we will pre-define a library of kernels and associated hyperparameters. Subsequently, we will use sparse regression to select among the library of candidate functions." Break condition: The true kernel function is not representable as a sparse combination of the library functions, or when the library is too small to capture the necessary complexity.

### Mechanism 3
The interpretable model improves out-of-distribution generalization because simpler, more constrained models are less prone to overfitting than complex neural networks. Neural networks have high capacity and can memorize training data, leading to poor generalization. The interpretable functional linear model has explicit constraints (linear in parameters, specific integral form) that regularize the solution space and improve extrapolation. Core assumption: The simpler, more constrained model can still capture the essential physics while avoiding overfitting to training data patterns. Evidence anchors: [abstract] "Our results demonstrate that our model can achieve comparable accuracy to deep learning and can improve OOD generalization" [section] "Extrapolation poses a serious challenge for black-box deep learning models... Interestingly, in certain examples, a simple linear regression model has exhibited remarkable performance in extrapolating training data" Break condition: The physics being modeled requires high model capacity that the interpretable model cannot provide, or when the training data is already representative of all possible scenarios.

## Foundational Learning

- Concept: Functional Data Analysis (FDA)
  - Why needed here: The paper uses FDA as the theoretical framework for modeling and analyzing data in the form of functions rather than discrete observations.
  - Quick check question: What is the difference between a scalar response model and a functional response model in FDA?

- Concept: Integral equations and Green's functions
  - Why needed here: The interpretable model is built upon integral equations that map input functions to output functions, inspired by Green's function theory.
  - Quick check question: How does the concept of Green's functions relate to the idea of using integral equations as interpretable surrogates?

- Concept: Sparse regression and model selection
  - Why needed here: The method uses sparse regression to select relevant kernel functions from a large library, balancing model complexity and interpretability.
  - Quick check question: What is the advantage of using L1 regularization (sparse regression) over L2 regularization in the context of model interpretability?

## Architecture Onboarding

- Component map: Trained neural network -> Input data probing -> Regression matrix construction -> Sparse regression solver -> Interpretable functional linear model
- Critical path: 1) Probe the trained neural network with input data to generate training pairs, 2) Construct the regression matrix by evaluating all candidate integral equations on the training data, 3) Apply sparse regression to find the coefficients, 4) Form the interpretable model as a linear combination of selected integral equations
- Design tradeoffs: More candidate kernels in the library increase flexibility but also computational cost and risk of overfitting; higher sparsity improves interpretability but may reduce accuracy; local interpretation provides detailed explanations but may not generalize
- Failure signatures: Poor training error indicates the library doesn't contain appropriate kernel functions; poor test error indicates overfitting or that the interpretable form cannot capture the underlying physics; dense solutions indicate the model is not sparse enough to be interpretable
- First 3 experiments:
  1. Implement a simple 1D case (like the sine wave example in the paper) to verify the interpretable model can learn basic functional mappings
  2. Test the method on a simple 2D case (like the permeability-to-velocity mapping) to verify it works with image-like inputs
  3. Compare the interpretable model's performance to the original neural network on out-of-distribution data to verify improved generalization

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the interpretable functional linear models compare to other state-of-the-art interpretable methods for scientific machine learning tasks? Basis in paper: [explicit] The paper states that their approach can achieve comparable accuracy to deep learning models and improve out-of-distribution generalization, but does not directly compare to other interpretable methods. Why unresolved: The paper focuses on comparing their approach to deep learning models rather than other interpretable methods. A direct comparison would provide insight into the relative strengths and weaknesses of different interpretable approaches. What evidence would resolve it: Conducting experiments comparing the proposed method to other interpretable methods (e.g., symbolic regression, SINDy) on the same datasets and tasks would provide a direct comparison of their performance and generalization capabilities.

### Open Question 2
How sensitive is the proposed approach to the choice of the library of candidate kernel functions and hyperparameters? Basis in paper: [explicit] The paper mentions that a library of candidate kernel functions is used and that different bandwidths are considered, but does not explore the sensitivity to these choices. Why unresolved: The performance of the interpretable models may depend on the specific kernels and hyperparameters chosen. Understanding the sensitivity to these choices would help in designing optimal libraries for different problems. What evidence would resolve it: Conducting experiments with different libraries of candidate kernels and hyperparameters, and analyzing the impact on the performance and interpretability of the resulting models, would provide insight into the sensitivity of the approach.

### Open Question 3
How does the proposed approach scale to higher-dimensional input and output spaces? Basis in paper: [inferred] The paper focuses on 2D input functions and scalar, 1D, or 2D output functions. It is not clear how the approach would perform with higher-dimensional data. Why unresolved: The integral equations and functional linear models may become more complex and computationally expensive as the dimensionality increases. Understanding the scalability of the approach is important for its practical applicability. What evidence would resolve it: Applying the proposed approach to higher-dimensional scientific machine learning tasks and analyzing the computational cost and performance compared to deep learning models would provide insight into the scalability of the method.

## Limitations
- The method assumes physics-based problems can be accurately represented by linear integral operators or their sparse combinations
- Computational cost scales poorly with problem complexity due to evaluating many candidate kernels
- Performance depends heavily on having sufficient training data to accurately learn kernel coefficients
- The interpretability gains may diminish for highly complex physical systems requiring high model capacity

## Confidence
- High confidence: The mathematical framework of functional linear models and sparse regression methodology
- Medium confidence: The generalizability of the approach across different physics domains
- Medium confidence: The claim of improved OOD generalization, though supported by numerical results

## Next Checks
1. Test the method on a physics problem known to have strong nonlinearities (e.g., turbulence modeling) to determine the limits of the linear integral approximation
2. Perform ablation studies varying the size and diversity of the kernel library to quantify the tradeoff between computational cost and model accuracy
3. Compare the interpretable model's extrapolation performance against simpler baselines (linear regression, polynomial regression) to isolate the contribution of the functional linear structure versus regularization effects