---
ver: rpa2
title: 'INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language
  Models'
arxiv_id: '2305.06677'
source_url: https://arxiv.org/abs/2305.06677
tags:
- subset
- pre-training
- data
- training
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of training large language
  models (LLMs) by proposing a framework called INGENIOUS that uses submodular optimization
  to select informative subsets of the training data. The method aims to retain up
  to 99% of the performance of fully-trained models while reducing training time and
  costs.
---

# INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models

## Quick Facts
- arXiv ID: 2305.06677
- Source URL: https://arxiv.org/abs/2305.06677
- Authors: 
- Reference count: 25
- Key outcome: Achieves up to 99% performance of fully-trained models while reducing training time and costs through submodular optimization of training data subsets

## Executive Summary
This paper addresses the computational inefficiency of training large language models (LLMs) by proposing INGENIOUS, a framework that uses submodular optimization to select informative subsets of training data. The method achieves up to 99% performance retention compared to full pre-training while significantly reducing computational costs. Through extensive experiments on BERT, GPT-2, and BioBERT, the authors demonstrate that carefully selected data subsets can replace full datasets without substantial performance loss, making LLM pre-training more accessible and sustainable.

## Method Summary
The INGENIOUS framework uses submodular optimization (specifically Facility Location function) to select representative subsets of training data. The method extracts dense feature representations from intermediate BERT layers (layer 9 found optimal), computes similarity kernels between samples, and employs a lazy greedy algorithm with probabilistic sampling to select subsets. The framework includes a warm-start phase with 2 epochs on full data, followed by periodic subset updates every R steps during training. The approach is evaluated on GLUE benchmark and LAMA probe to measure downstream performance and knowledge retention.

## Key Results
- Achieves up to 99% performance of fully-trained models on GLUE benchmark
- Reduces training costs by selecting informative subsets of data
- Layer 9 BERT embeddings yield optimal feature representations for subset selection
- Periodic subset updating maintains representativeness throughout training
- Ablation studies show impact of various design choices and parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subset selection based on submodular optimization yields highly representative training data while reducing redundancy.
- Mechanism: The framework uses Facility Location, a submodular function, to maximize representation of remaining samples. Submodular functions have diminishing returns property, so adding similar sentences yields less gain than adding diverse ones. Lazy greedy algorithm with memoization selects data points probabilistically based on submodular gains converted to probabilities via Taylor-softmax.
- Core assumption: The training corpus contains significant redundancy, and diverse, representative subsets can replace full dataset without performance loss.
- Evidence anchors:
  - [abstract]: "we can employ submodular optimization to select highly representative subsets of the training corpora"
  - [section 3.3]: "subset selection as a submodular maximization problem"
  - [corpus]: Weak evidence - paper mentions redundancies in vision datasets but doesn't directly show this for text datasets
- Break condition: If the corpus lacks redundancy or if the similarity kernel fails to capture meaningful relationships between samples, the submodular gains become unreliable and the selected subsets may not be representative.

### Mechanism 2
- Claim: Using intermediate layer representations from the language model itself as feature encoders captures semantic relationships better than shallow methods.
- Mechanism: Features are extracted by averaging token embeddings from a specific BERT layer (layer 9 found optimal). These dense representations capture linguistic information that better reflects sentence similarity than sparse TF-IDF features.
- Core assumption: Different BERT layers encode different types of information, and middle layers capture sufficient semantic content for subset selection.
- Evidence anchors:
  - [section 4.3]: "dense feature encoders yield the best results" and layer 9 features "yield the best results"
  - [section 3.4]: "dense and sparse feature encoders for obtaining the feature representation"
  - [corpus]: Strong evidence - paper provides ablation showing layer 9 outperforms other layers and TF-IDF
- Break condition: If the chosen layer doesn't capture the right semantic information for the task, or if the averaging approach loses critical information, the similarity kernel becomes ineffective.

### Mechanism 3
- Claim: Periodic subset updating with adaptive selection maintains representativeness throughout training as the model's understanding evolves.
- Mechanism: The subset is updated every R steps (25,000 in experiments) using adaptive subset selection. This ensures the model continues to see diverse, representative samples as its internal representations change during training.
- Core assumption: The model's feature representations change meaningfully during training, requiring periodic subset updates to maintain optimal representativeness.
- Evidence anchors:
  - [section 3.3]: "we update the subset after every Rth iteration... and train the model on the chosen subset for the following R steps"
  - [section 4.1]: Results show consistent performance improvement when subset is updated
  - [corpus]: Moderate evidence - paper shows performance benefits but doesn't explicitly demonstrate changing feature representations over time
- Break condition: If feature representations stabilize early in training or if subset updates are too frequent/costly relative to benefits, the periodic updating mechanism becomes inefficient.

## Foundational Learning

- Concept: Submodularity and greedy algorithms
  - Why needed here: The subset selection problem is NP-hard, but submodular functions allow approximate solutions via greedy algorithms with theoretical guarantees on performance
  - Quick check question: Why does the diminishing returns property of submodular functions make greedy selection effective for this problem?

- Concept: Feature representation and similarity metrics
  - Why needed here: The quality of subset selection depends entirely on how well the similarity kernel captures relationships between data samples, which requires understanding of embedding spaces and similarity measures
  - Quick check question: What are the trade-offs between using dense BERT embeddings versus sparse TF-IDF features for text similarity?

- Concept: Adaptive learning and curriculum effects
  - Why needed here: Periodic subset updating is a form of curriculum learning that adapts to the model's changing needs during training
  - Quick check question: How does adaptive subset selection differ from traditional curriculum learning approaches in terms of data selection criteria?

## Architecture Onboarding

- Component map: Warm-start phase -> Feature extraction -> Subset selection -> Model training -> Subset update -> Repeat

- Critical path: Feature extraction → Similarity kernel computation → Subset selection → Model training → Subset update → Repeat

- Design tradeoffs:
  - Memory vs. accuracy: Fewer partitions reduce memory but may decrease subset quality
  - Update frequency vs. efficiency: More frequent updates maintain better representativeness but increase computational overhead
  - Layer selection vs. generality: Different layers capture different information types; choice affects subset quality for specific tasks

- Failure signatures:
  - Performance plateaus early: Likely subset quality degradation or insufficient model capacity
  - Memory errors during kernel computation: Need to adjust partition size or reduce feature dimensionality
  - Inconsistent results across runs: Randomness in subset selection not properly controlled

- First 3 experiments:
  1. Run with full dataset (baseline) vs. single subset selection with different feature layers to identify optimal representation
  2. Test subset sizes (10%, 25%, 50%) with fixed layer to find sweet spot between efficiency and performance
  3. Vary update frequency (R steps) to optimize trade-off between representational quality and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of INGENIOUS scale when applied to much larger PTLMs like GPT-3, OPT, or PaLM, which were not tested due to resource constraints?
- Basis in paper: [explicit] The authors acknowledge they were unable to test on larger PTLMs like GPT-3, OPT, or PaLM due to resource limitations.
- Why unresolved: The paper only demonstrates results on BERT-Base (110M) and GPT2-Small (124M) parameters, leaving the scalability to truly large-scale models untested.
- What evidence would resolve it: Experiments applying INGENIOUS to pre-training of models with billions of parameters, measuring performance retention and cost savings compared to full pre-training.

### Open Question 2
- Question: Can INGENIOUS be effectively combined with other efficiency techniques like model pruning, knowledge distillation, or curriculum learning to achieve greater overall efficiency gains?
- Basis in paper: [explicit] The authors mention their work is complementary to existing methods like pruning, distillation, and curriculum learning, but do not explore combinations.
- Why unresolved: The paper focuses solely on data subset selection and does not investigate potential synergies with architectural or training pipeline optimizations.
- What evidence would resolve it: Comparative experiments showing performance and efficiency of combined approaches versus individual techniques, including ablation studies on which combinations are most effective.

### Open Question 3
- Question: How sensitive is INGENIOUS to the quality and characteristics of the pre-training corpus, and can it effectively handle corpora with varying levels of redundancy, noise, or domain specificity?
- Basis in paper: [inferred] The method relies on feature representations from the model itself and assumes the corpus contains redundancies that can be eliminated, but does not test on highly specialized or noisy datasets.
- Why unresolved: Experiments were conducted on standard corpora like Wikipedia, BooksCorpus, and PubMed, without exploring performance on more challenging or domain-specific datasets.
- What evidence would resolve it: Testing INGENIOUS on diverse corpora with different characteristics (highly redundant, noisy, specialized domains) and measuring performance degradation or improvement compared to full training.

## Limitations
- Effectiveness depends on assumed redundancy in training corpora, which is not empirically validated for text datasets
- Computational overhead of periodic subset updates may offset efficiency gains for smaller models
- Optimal feature extraction layer (layer 9) may be task-specific and not universally applicable
- Scalability to extremely large models (GPT-3, PaLM) remains untested due to resource constraints

## Confidence

- **High Confidence**: The submodular optimization framework and its theoretical guarantees for subset selection are well-established. The ablation studies demonstrating layer 9's superiority for feature extraction are rigorous and reproducible.
- **Medium Confidence**: The claim of 99% performance retention is supported by experimental results but relies on specific hyperparameter settings and corpus characteristics that may not generalize. The computational efficiency gains are demonstrated but require careful cost-benefit analysis for different use cases.
- **Low Confidence**: The assertion that periodic subset updating is necessary (rather than a one-time selection) lacks strong empirical justification - the paper shows benefits but doesn't prove this is superior to static subset selection.

## Next Checks

1. **Corpus Redundancy Analysis**: Conduct an empirical study measuring actual redundancy levels in the training corpora (Wikipedia, BooksCorpus, PubMed) using techniques like n-gram overlap, sentence similarity distributions, and information-theoretic measures to validate the assumption that submodular selection can effectively reduce dataset size.

2. **Layer Transferability Test**: Evaluate the framework using feature representations from different BERT layers across multiple downstream tasks beyond GLUE (e.g., relation extraction, question answering) to determine if layer 9 is universally optimal or task-specific.

3. **Update Frequency Trade-off**: Systematically vary the subset update interval R across multiple orders of magnitude (e.g., 5K, 25K, 100K, 500K steps) and measure both performance impact and computational overhead to identify the optimal balance between representational quality and efficiency.