---
ver: rpa2
title: Post Hoc Explanations of Language Models Can Improve Language Models
arxiv_id: '2305.11426'
source_url: https://arxiv.org/abs/2305.11426
tags:
- performance
- post
- explanations
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AMPLIFY, a novel framework that improves the
  performance of large language models (LLMs) on complex reasoning tasks by using
  post hoc explanations to generate automated rationales. The framework identifies
  misclassified samples from a validation set, computes feature attribution scores
  using a smaller proxy model, and constructs prompts embedding the top keywords as
  rationales to provide corrective signals to LLMs.
---

# Post Hoc Explanations of Language Models Can Improve Language Models

## Quick Facts
- arXiv ID: 2305.11426
- Source URL: https://arxiv.org/abs/2305.11426
- Reference count: 33
- Key result: Automated rationales from post hoc explanations improve LLM reasoning accuracy by 10-25% over standard prompting.

## Executive Summary
This paper introduces AMPLIFY, a novel framework that leverages post hoc explanations from smaller proxy models to construct automated rationales that improve LLM performance on complex reasoning tasks. The approach identifies misclassified samples, computes feature attribution scores, and embeds top keywords into few-shot prompts as corrective signals. Extensive experiments across seven real-world datasets demonstrate significant accuracy gains over standard and chain-of-thought prompting methods, even outperforming human-annotated rationales in some cases.

## Method Summary
AMPLIFY operates through a four-step process: (1) select a proxy model (e.g., GPT-2 or BERT) and optionally fine-tune it on the target task, (2) identify misclassified samples from the validation set and rank them using Misclassification Confidence Score (MCS), (3) compute post hoc explanations using gradient-based methods to extract top-k keywords, and (4) construct few-shot prompts embedding these keywords as rationales. The framework leverages in-context learning to provide corrective signals without requiring fine-tuning, demonstrating that automated rationales can be more effective than human-provided ones for certain reasoning tasks.

## Key Results
- AMPLIFY achieves 10-25% performance gains over standard prompting across seven real-world datasets
- Automated rationales outperform human-annotated rationales on certain tasks
- The framework works across diverse reasoning tasks including commonsense QA, causal judgment, and formal fallacies
- Performance improvements are consistent across different proxy model choices (GPT-2, BERT)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using post hoc explanations from a smaller proxy model to construct rationales improves LLM performance by providing targeted corrective signals.
- Mechanism: The framework identifies misclassified samples using a validation set, computes attribution scores via gradient-based post hoc explanation methods on a proxy model (e.g., GPT-2), and constructs natural language rationales highlighting top keywords. These rationales are embedded into few-shot prompts, guiding the LLM to focus on relevant features during inference.
- Core assumption: The proxy model's attribution scores reliably reflect feature importance for the true label, and LLMs can effectively use these signals in few-shot prompts.
- Evidence anchors:
  - [abstract]: "we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs"
  - [section]: "We then use explainability techniques to compute explanations for the selected samples with respect to their ground truth labels"
  - [corpus]: No direct evidence of this exact mechanism in related work; this appears to be a novel application.
- Break condition: If the proxy model's explanations are misaligned with the LLM's decision process or if the LLM ignores rationale signals in prompts.

### Mechanism 2
- Claim: Selecting samples with the highest Misclassification Confidence Score (MCS) yields the strongest corrective signals.
- Mechanism: After identifying misclassified samples from the validation set, the framework ranks them by MCS = f(x)ŷ - f(x)y, selecting those with the largest confidence gaps. These samples provide the clearest corrective guidance when paired with rationales.
- Core assumption: Samples with the highest MCS represent the most egregious errors and thus provide the most informative corrective examples.
- Evidence anchors:
  - [section]: "We then rank these instances using a metric we introduce called the Misclassiﬁcation Conﬁdence Score (MCS)"
  - [section]: "The samples that exhibit the highest MCS represent the most egregious misclassifications"
  - [corpus]: No explicit evidence of MCS use in related work; novel to this paper.
- Break condition: If MCS fails to correlate with actual difficulty or if other selection strategies outperform it.

### Mechanism 3
- Claim: Gradient-based post hoc explanation methods (e.g., Gradient x Input) effectively identify top keywords that improve LLM reasoning.
- Mechanism: For each selected sample, the framework computes attribution scores (e.g., ∂f(x)y/∂xi) using gradient-based methods, averages them per word, and selects top-k keywords. These keywords form the basis of rationales embedded in prompts.
- Core assumption: Gradient-based attribution scores accurately capture token-level importance for the true label prediction.
- Evidence anchors:
  - [section]: "Vanilla Gradients [22] calculates feature attributions by computing the norm of the gradients of model output with respect to each token's embedding"
  - [section]: "Gradient x Input derives attribution scores by taking the product of gradient and input embedding"
  - [corpus]: No direct evidence in related work of this specific gradient-based approach in LLM improvement.
- Break condition: If alternative explanation methods (e.g., perturbation-based) yield better keyword identification or if gradient methods are computationally prohibitive.

## Foundational Learning

- Concept: Post hoc explanation methods
  - Why needed here: They provide feature attribution scores that quantify how much each input token influences the model's prediction, enabling automated rationale generation.
  - Quick check question: What is the difference between perturbation-based and gradient-based post hoc explanation methods?

- Concept: In-context learning
  - Why needed here: It allows LLMs to adapt to new tasks by conditioning on few-shot examples, making it the framework for incorporating rationales without fine-tuning.
  - Quick check question: How does in-context learning differ from traditional fine-tuning in terms of computational cost and adaptability?

- Concept: Misclassification Confidence Score (MCS)
  - Why needed here: It quantifies the severity of a misclassification by measuring the confidence gap between the predicted and true labels, guiding sample selection for rationales.
  - Quick check question: Why might samples with high MCS provide stronger corrective signals than randomly selected misclassifications?

## Architecture Onboarding

- Component map: Proxy Model → Sample Selector → Explanation Generator → Rationale Constructor → Prompt Builder → LLM
- Critical path: Proxy Model → Sample Selector → Explanation Generator → Rationale Constructor → Prompt Builder → LLM
- Design tradeoffs:
  - Proxy model size vs. explanation quality: Smaller models are computationally efficient but may miss nuanced reasoning patterns
  - Rationale complexity vs. LLM comprehension: Too detailed rationales might overwhelm LLMs, while too simple ones may lack corrective power
  - Keyword selection vs. context preservation: Focusing on top-k keywords risks losing broader context
- Failure signatures:
  - No performance improvement: Proxy model explanations may be misaligned with LLM reasoning
  - Performance degradation: Rationale signals may confuse the LLM or introduce bias
  - Computational inefficiency: High cost of computing explanations or constructing prompts
- First 3 experiments:
  1. Baseline comparison: Measure LLM performance with and without AMPLIFY on a simple dataset (e.g., sentiment classification)
  2. Proxy model ablation: Test different proxy models (e.g., GPT-2 vs. BERT) to assess impact on LLM performance
  3. Explanation method comparison: Evaluate gradient-based vs. perturbation-based methods for keyword selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do post hoc explanation methods perform when applied to LLMs with different architectures or sizes, beyond GPT-2 and BERT?
- Basis in paper: [explicit] The paper uses GPT-2 and BERT as proxy models to generate post hoc explanations for LLMs.
- Why unresolved: The study primarily focuses on GPT-2 and BERT as proxy models, leaving the performance of other models unexplored.
- What evidence would resolve it: Experiments testing various proxy models (e.g., RoBERTa, T5) across different LLM architectures to compare the effectiveness of post hoc explanations.

### Open Question 2
- Question: What are the long-term effects of using post hoc explanations on the overall robustness and generalization of LLMs?
- Basis in paper: [inferred] The paper highlights performance improvements but does not discuss long-term impacts on robustness or generalization.
- Why unresolved: The study focuses on immediate accuracy gains without addressing potential overfitting or robustness issues.
- What evidence would resolve it: Longitudinal studies evaluating LLMs' performance on unseen tasks or adversarial examples after repeated use of post hoc explanations.

### Open Question 3
- Question: How does the choice of rationale template impact the interpretability and usability of explanations for non-expert users?
- Basis in paper: [explicit] The paper mentions using a task-independent rationale template but does not explore its interpretability for non-experts.
- Why unresolved: The study does not assess how different templates affect user understanding or trust in LLM predictions.
- What evidence would resolve it: User studies comparing different rationale templates to measure comprehension and trust levels among non-expert users.

### Open Question 4
- Question: Can post hoc explanations be effectively integrated into multi-modal LLMs that handle both text and other data types?
- Basis in paper: [inferred] The paper focuses on text-based LLMs, leaving the applicability to multi-modal models unexplored.
- Why unresolved: The study does not address the challenges of applying post hoc explanations to models handling images, audio, or other modalities.
- What evidence would resolve it: Experiments extending the AMPLIFY framework to multi-modal LLMs to assess the effectiveness of post hoc explanations in diverse data contexts.

## Limitations

- Reliance on proxy model explanations may introduce bias if the proxy model's reasoning diverges from the LLM's
- MCS selection heuristic is not compared against alternative sample selection strategies
- Assumes LLMs can effectively process and utilize rationale signals without empirical validation of this assumption

## Confidence

- Post hoc explanations improve LLM performance: Medium
- Automated rationales outperform human rationales: Medium
- Proxy model size doesn't significantly impact results: Low
- Rationale template is optimal for all tasks: Low

## Next Checks

1. Ablate proxy model size to test sensitivity to explanation quality
2. Compare gradient-based vs. perturbation-based attribution methods for keyword extraction
3. Evaluate whether human-annotated rationales yield similar or better improvements than automated ones