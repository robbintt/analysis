---
ver: rpa2
title: 'SmartPlay: A Benchmark for LLMs as Intelligent Agents'
arxiv_id: '2310.01557'
source_url: https://arxiv.org/abs/2310.01557
tags:
- arxiv
- game
- preprint
- like
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SmartPlay, a benchmark designed to evaluate
  the capabilities of large language models (LLMs) as intelligent agents. SmartPlay
  consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, and
  Minecraft, each presenting unique challenges that span multiple dimensions of intelligent
  agents, such as reasoning with object dependencies, planning ahead, spatial reasoning,
  learning from history, and understanding randomness.
---

# SmartPlay: A Benchmark for LLMs as Intelligent Agents

## Quick Facts
- arXiv ID: 2310.01557
- Source URL: https://arxiv.org/abs/2310.01557
- Reference count: 40
- Key outcome: Introduces SmartPlay benchmark to evaluate LLM capabilities as intelligent agents across 6 diverse games, revealing significant performance gaps compared to human baselines

## Executive Summary
SmartPlay is a comprehensive benchmark designed to evaluate large language models (LLMs) as intelligent agents through 6 diverse games including Rock-Paper-Scissors, Tower of Hanoi, and Minecraft. The benchmark provides a unified API with text-based observations and manuals for turn-by-turn LLM inference, enabling systematic comparison across different evaluation metrics. Experimental results reveal that while GPT-4 variants significantly outperform other LLMs, there remains a substantial gap between LLM and human performance on more challenging benchmarks, highlighting critical research areas for advancing LLM agent capabilities.

## Method Summary
SmartPlay evaluates LLM agents through a unified OpenAI Gym interface with text-based observations, manuals, and actions. The benchmark includes 6 games designed to challenge different capabilities of intelligent agents including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. Each game provides up to 20 evaluation settings and infinite environment variations to ensure robustness against data contamination. The evaluation protocol involves directly prompting LLMs with the game manual, history, and current observation to generate actions, with performance measured through reward, completion rate, and score metrics.

## Key Results
- GPT-4 variants significantly outperform other LLMs across all SmartPlay games
- Substantial performance gap exists between GPT-4 and human baseline on more challenging benchmarks
- Open-source LLMs achieve less than half the performance of GPT-4 on simple tasks and 1/8 on challenging tasks
- LLMs struggle with 3D spatial reasoning (60% of human baseline in Minecraft) and learning from interactions (lower scores in Crafter benchmark)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SmartPlay provides a standardized, automated evaluation pipeline that enables systematic comparison of LLM agents across diverse game environments.
- Mechanism: By offering a unified OpenAI Gym interface with text-based observations, manuals, and actions, SmartPlay allows LLMs to be evaluated using consistent metrics across different games. The benchmark's procedural generation and multiple evaluation settings ensure robustness against data contamination.
- Core assumption: LLMs can process and reason over text-based game descriptions and instructions effectively enough to interact with the game environment through the provided interface.
- Evidence anchors:
  - [abstract] "SmartPlay provides a unified and expandable API with text observations and guidance to perform turn-by-turn LLM inference."
  - [section 3.1] "For ease of use and wide compatibility, SmartPlay follows a unified OpenAI Gym interface (Brockman et al., 2016) for all games, with text-based observations, text-based manuals..."

### Mechanism 2
- Claim: SmartPlay's diverse game selection challenges LLMs across multiple dimensions of intelligent agency, revealing specific capability gaps.
- Mechanism: Each game in SmartPlay is designed to uniquely challenge a subset of 9 important capabilities (e.g., reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, understanding randomness). By analyzing performance across these games, researchers can identify which capabilities LLMs excel at and where they struggle.
- Core assumption: The games in SmartPlay are well-designed to isolate and challenge specific capabilities of intelligent agents, and the performance differences across games reflect the underlying capability gaps of the LLMs.
- Evidence anchors:
  - [abstract] "Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness."
  - [section 2.1] "The SmartPlay benchmark encapsulates a diverse set of challenges that evaluate various AI capabilities, as itemized in Table 2."

### Mechanism 3
- Claim: SmartPlay's procedural generation and multiple evaluation settings ensure robustness against data contamination and provide a more comprehensive evaluation of LLM agents.
- Mechanism: By offering infinite environment variations and up to 20 different evaluation settings per game, SmartPlay reduces the impact of LLMs having seen specific game states or scenarios during training. This allows for a more reliable assessment of the agents' true capabilities rather than their memorization of specific examples.
- Core assumption: Procedural generation and multiple evaluation settings are sufficient to cover the range of scenarios that an LLM agent might encounter, making it unlikely that the agent has seen all possible variations during training.
- Evidence anchors:
  - [abstract] "Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations."
  - [section 1] "Experimentally, we observe LLMs struggle to memoize intermediate states of a simple 3-disk Tower of Hanoi game."

## Foundational Learning

- Concept: Understanding of game theory and multi-armed bandit problems
  - Why needed here: The Two-armed Bandits game is based on the classic multi-armed bandit problem, where the agent must balance exploration and exploitation to maximize reward. Understanding this concept is crucial for designing and evaluating the game, as well as for interpreting the results.
  - Quick check question: What is the key challenge in the multi-armed bandit problem, and how does it relate to the Two-armed Bandits game in SmartPlay?

- Concept: Knowledge of reinforcement learning and agent evaluation metrics
  - Why needed here: SmartPlay is designed to evaluate LLM agents, and understanding reinforcement learning concepts (e.g., reward, completion rate, score) and evaluation metrics is essential for interpreting the results and comparing the performance of different agents.
  - Quick check question: What are the three evaluation metrics used in SmartPlay, and how do they differ in their focus and interpretation?

- Concept: Familiarity with large language models and their capabilities
  - Why needed here: SmartPlay is specifically designed to evaluate the capabilities of LLMs as intelligent agents. Understanding the strengths and limitations of LLMs, as well as their potential for reasoning, planning, and interacting with environments, is crucial for designing the benchmark and interpreting the results.
  - Quick check question: What are some key capabilities of LLMs that make them promising candidates for intelligent agents, and what are some limitations that SmartPlay aims to evaluate?

## Architecture Onboarding

- Component map: Game environments -> Text-based interface -> LLM inference -> Evaluation metrics -> Data collection and analysis

- Critical path:
  1. Initialize the game environment and LLM agent.
  2. Generate the game state observation and convert it to text format.
  3. Prompt the LLM with the observation, manual, and history.
  4. Parse the action from the LLM's response and execute it in the game environment.
  5. Calculate the reward and update the game state.
  6. Repeat steps 2-5 until the game ends.
  7. Calculate the final evaluation metrics and store the results.

- Design tradeoffs:
  - Text-based vs. visual interface: SmartPlay uses a text-based interface to make it compatible with LLMs that do not have vision capabilities. However, this may limit the expressiveness and richness of the game states compared to a visual interface.
  - Procedural generation vs. fixed environments: Procedural generation ensures robustness against data contamination but may introduce additional complexity in designing the game logic and ensuring a fair evaluation.
  - Standardized vs. game-specific evaluation metrics: Standardized metrics allow for easier comparison across games but may not capture the nuances and specific objectives of each game.

- Failure signatures:
  - LLM fails to understand the game rules or instructions: The agent may take invalid actions or fail to make progress in the game.
  - LLM struggles with long text inputs: The agent may have difficulty processing and reasoning over long game descriptions or histories.
  - LLM performs well on simple games but poorly on complex ones: This may indicate a lack of generalization or reasoning capabilities in the LLM.
  - Evaluation metrics show inconsistent results across games: This may suggest issues with the game design, evaluation pipeline, or the underlying capabilities being measured.

- First 3 experiments:
  1. Evaluate a simple LLM (e.g., text-davinci-003) on the Two-armed Bandits game to verify the basic functionality of the evaluation pipeline and identify any issues with the text-based interface or LLM inference.
  2. Compare the performance of different LLM variants (e.g., GPT-4, Claude, Bard) on the Rock Paper Scissors game to assess their ability to learn from interactions and understand randomness.
  3. Analyze the performance of a strong LLM (e.g., GPT-4) on the Tower of Hanoi game to investigate its planning and reasoning capabilities, as well as the impact of data contamination on its performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be improved to better handle 3D spatial reasoning tasks, as demonstrated by the performance gap in the Minecraft benchmark?
- Basis in paper: [explicit] The paper identifies 3D spatial reasoning as a challenge for LLMs, with all models achieving only 60% of human baseline performance in Minecraft creative tasks.
- Why unresolved: The paper does not provide a clear solution or approach to address this gap in 3D spatial reasoning capabilities of LLMs.
- What evidence would resolve it: Developing and testing new techniques or architectures that enhance 3D spatial reasoning in LLMs, followed by evaluating their performance on the Minecraft benchmark.

### Open Question 2
- Question: Can LLMs be trained to better learn from interactions and mistakes, as indicated by their lower performance in the Crafter benchmark?
- Basis in paper: [explicit] The paper observes that GPT-4 variants score lower on learning from interactions and error/mistake handling, with a significant performance gap in the Crafter benchmark.
- Why unresolved: The paper does not propose specific methods to improve LLMs' ability to learn from interactions and mistakes.
- What evidence would resolve it: Implementing and evaluating novel training approaches that focus on learning from interactions and mistakes, and measuring their impact on LLM performance in the Crafter benchmark.

### Open Question 3
- Question: How can open-source LLMs be improved to match the performance of proprietary models like GPT-4 in the SmartPlay benchmark?
- Basis in paper: [explicit] The paper notes that open-source LLMs achieve less than half the performance of GPT-4 variants on simple tasks and 1/8 the performance on more challenging tasks.
- Why unresolved: The paper does not provide a clear strategy for bridging the performance gap between open-source and proprietary LLMs.
- What evidence would resolve it: Conducting research to identify the key factors limiting open-source LLM performance and developing techniques to address these limitations, followed by evaluating their effectiveness on the SmartPlay benchmark.

## Limitations

- Text-based interface may not fully capture the richness of game states compared to visual interfaces
- Procedural generation may not guarantee complete coverage of all possible scenarios an LLM might encounter
- Evaluation metrics, while standardized, may not fully capture nuanced differences in agent behavior across games

## Confidence

- **High Confidence**: The benchmark's unified OpenAI Gym interface and text-based observations provide a consistent evaluation framework
- **Medium Confidence**: The procedural generation and multiple evaluation settings enhance robustness against data contamination
- **Medium Confidence**: The diverse game selection challenges LLMs across multiple dimensions of intelligent agency

## Next Checks

1. **Reproduce Results with Alternative LLMs**: Evaluate the benchmark's effectiveness by running experiments with other strong LLM models not mentioned in the original paper (e.g., Claude 2, Llama 2) to assess its generalizability
2. **Investigate Data Contamination**: Conduct a detailed analysis of the impact of data contamination on LLM performance, particularly for the Tower of Hanoi game, by testing with various amounts of training data exposure
3. **Assess Text Interface Limitations**: Compare the performance of LLM agents using the text-based interface against those using a visual interface (if available) for a subset of games to quantify any limitations in expressiveness and richness of game states