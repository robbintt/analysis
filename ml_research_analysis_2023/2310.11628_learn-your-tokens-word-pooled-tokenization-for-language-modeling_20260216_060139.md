---
ver: rpa2
title: 'Learn Your Tokens: Word-Pooled Tokenization for Language Modeling'
arxiv_id: '2310.11628'
source_url: https://arxiv.org/abs/2310.11628
tags:
- word
- language
- characters
- subword
- byte
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies language model tokenization schemes, comparing
  the standard subword approach with byte/character level models and a new 'learn
  your tokens' method that uses word boundaries to compress base units. The proposed
  method pools bytes/characters into word representations using a transformer encoder,
  feeds them into a language model, and decodes individual units per word.
---

# Learn Your Tokens: Word-Pooled Tokenization for Language Modeling

## Quick Facts
- arXiv ID: 2310.11628
- Source URL: https://arxiv.org/abs/2310.11628
- Reference count: 12
- This paper proposes a novel word-pooled tokenization method that achieves over 300% improvement in next-word prediction accuracy compared to subword and byte/character models.

## Executive Summary
This paper challenges the conventional wisdom of subword tokenization in language modeling by proposing a novel "learn your tokens" method that leverages word boundaries to compress character/byte sequences into word representations. The method uses a transformer encoder to pool characters/bytes into fixed CLS tokens per word, processes these word-level representations with a primary language model, and decodes individual characters/bytes per word using a shallow decoder. Experimental results across English, French, Russian, and numeracy datasets show substantial improvements in next-word prediction accuracy (300% overall, 30x on rare words) while offering a favorable tradeoff between efficiency and expressiveness.

## Method Summary
The proposed method introduces a hierarchical tokenization approach that first encodes character/byte sequences into word representations using a transformer encoder, then processes these word-level embeddings with a primary language model, and finally decodes back to characters/bytes using a shallow transformer decoder. The word encoder prefixes a fixed number of CLS tokens to each word, processes them through a transformer encoder, and produces a single vector representation per word. This representation is then contextualized by the primary LM (8-layer transformer decoder) and passed to the word decoder (2-layer transformer decoder) which autoregressively predicts characters/bytes. The method restricts attention to intra-word connections in the first and third stages, while allowing cross-word context only in the middle stage, achieving a balance between efficiency and expressiveness.

## Key Results
- 300% improvement in next-word prediction accuracy compared to subword and byte/character models
- 30x gain on rare word prediction tasks
- Significant efficiency gains through reduced sequence length while maintaining expressiveness

## Why This Works (Mechanism)

### Mechanism 1
The word-boundary compression preserves semantic coherence while reducing sequence length. The transformer encoder pools character/byte embeddings into fixed CLS tokens per word, capturing intra-word dependencies. This compressed representation maintains word-level meaning while reducing the number of tokens from variable-length words to a fixed count per word. Core assumption: Words are meaningful units that can be represented by their constituent characters/bytes without losing semantic information.

### Mechanism 2
The hierarchical architecture achieves a balance between efficiency and expressiveness. By restricting attention to intra-word connections in the first and third stages, and only allowing inter-word context in the middle stage, the model reduces the quadratic complexity of self-attention while maintaining the ability to capture cross-word dependencies. Core assumption: Intra-word dependencies are more important than inter-word dependencies for the first and last stages of token prediction.

### Mechanism 3
The learned pooling is more adaptive than deterministic subword segmentation. The transformer encoder learns to compress characters/bytes into word representations end-to-end with the language model, adapting to the specific distribution of the training data rather than using a fixed segmentation algorithm. Core assumption: End-to-end learning can discover better tokenization patterns than hand-engineered heuristics.

## Foundational Learning

- Concept: Transformer architecture and self-attention
  - Why needed here: The entire method relies on transformer encoders and decoders for both pooling and language modeling tasks
  - Quick check question: Can you explain why self-attention has O(n²) complexity and how restricting attention to intra-word connections reduces this?

- Concept: Subword tokenization methods (BPE, WordPiece, Unigram)
  - Why needed here: Understanding the baseline approaches being compared against is crucial for evaluating the proposed method
  - Quick check question: What are the key differences between BPE and WordPiece, and why might these matter for different languages?

- Concept: Language modeling and autoregressive prediction
  - Why needed here: The method is evaluated on next-word prediction, requiring understanding of language modeling objectives
  - Quick check question: How does perplexity relate to next-word prediction accuracy, and why can't we directly compare perplexity across different tokenization schemes?

## Architecture Onboarding

- Component map: Character/byte input -> Word encoder (transformer) -> Word-level embeddings -> Primary LM (transformer decoder, 8 layers) -> Contextualized word embeddings -> Word decoder (transformer decoder, 2 layers) -> Character/byte predictions

- Critical path:
  1. Character/byte input → Word encoder → Word-level embeddings
  2. Word embeddings → Primary LM → Contextualized word embeddings
  3. Contextualized embeddings → Word decoder → Character/byte predictions

- Design tradeoffs:
  - Fixed vs. variable number of CLS tokens per word (ablation shows 4 tokens optimal)
  - Shallow vs. deep word encoder/decoder (2 layers chosen for efficiency)
  - Character vs. byte base vocabulary (character used for cleaner datasets)
  - Context window size (192 characters fixed across all experiments)

- Failure signatures:
  - Poor rare word performance suggests insufficient representational power in CLS tokens
  - High memory usage indicates need to reduce context window or batch size
  - Low accuracy on number estimation suggests pooling loses numeric structure
  - Slow training indicates batch size too small for hardware constraints

- First 3 experiments:
  1. Verify word boundary detection works on sample text from each language
  2. Test single-word pooling with varying numbers of CLS tokens (1, 2, 4, 8) on a small dataset
  3. Compare end-to-end model with baseline character and subword models on a single language dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed end-to-end tokenization method provide benefits beyond language modeling, such as in machine translation or question answering?
- Basis in paper: The paper focuses on intrinsic language modeling metrics but acknowledges that many NLP tasks could benefit from improved tokenization.
- Why unresolved: The authors deliberately limited their scope to language modeling and did not test downstream applications.
- What evidence would resolve it: Experiments comparing the proposed tokenization method with baselines on tasks like machine translation, question answering, and text classification.

### Open Question 2
- Question: How does the proposed method perform on languages with no explicit word boundary markers, such as Chinese and Japanese?
- Basis in paper: The authors mention avoiding Chinese and Japanese corpora because they require additional segmentation steps, implying uncertainty about performance on these languages.
- Why unresolved: The authors chose to avoid these languages to prevent confounding results with segmentation heuristics.
- What evidence would resolve it: Experiments evaluating the proposed method on Chinese and Japanese corpora, with and without preprocessing.

### Open Question 3
- Question: What is the optimal number of CLS tokens per word for balancing representational power and computational efficiency?
- Basis in paper: The authors ablate the number of CLS tokens per word and find a significant improvement when increasing from 1 to 4, but do not explore further.
- Why unresolved: The ablation study only considers 1 and 4 CLS tokens, leaving the optimal number uncertain.
- What evidence would resolve it: Experiments varying the number of CLS tokens per word (e.g., 2, 3, 5, 6) and measuring performance and computational efficiency.

## Limitations

- The method's effectiveness depends on accurate word boundary detection, which is not explicitly addressed for languages without clear word segmentation (e.g., Chinese, Japanese).
- The claimed 300% improvement combines both tokenization scheme and model architecture, making it difficult to isolate the specific contribution of the learned pooling mechanism.
- The paper lacks empirical runtime measurements across different hardware configurations to validate theoretical efficiency claims.

## Confidence

- High confidence: The hierarchical architecture design with restricted attention patterns is technically sound and the efficiency-accuracy tradeoff analysis is reasonable
- Medium confidence: The claimed performance improvements are likely valid for languages with clear word boundaries and sufficient training data, but may not generalize to all language types
- Medium confidence: The theoretical efficiency analysis is sound, but empirical runtime validation across different hardware setups is missing

## Next Checks

1. **Boundary Detection Robustness**: Test the method on languages with ambiguous word boundaries (Chinese, Japanese, Thai) and compare performance when using different word segmentation tools versus character-level processing.

2. **Efficiency Validation**: Measure actual training and inference time on identical hardware across all three tokenization methods, comparing not just FLOPs but wall-clock time and memory usage for realistic batch sizes.

3. **Generalization Test**: Evaluate the method on a low-resource language dataset with limited training data to assess whether the end-to-end learning approach maintains its advantages when training data is scarce.