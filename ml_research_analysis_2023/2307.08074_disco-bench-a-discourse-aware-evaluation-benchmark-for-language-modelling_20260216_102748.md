---
ver: rpa2
title: 'Disco-Bench: A Discourse-Aware Evaluation Benchmark for Language Modelling'
arxiv_id: '2307.08074'
source_url: https://arxiv.org/abs/2307.08074
tags:
- tasks
- discourse
- translation
- chinese
- disco-bench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Disco-Bench, a benchmark for evaluating language
  models on discourse-aware tasks. Existing benchmarks primarily focus on inter-sentence
  properties, overlooking critical discourse phenomena that cross sentences.
---

# Disco-Bench: A Discourse-Aware Evaluation Benchmark for Language Modelling

## Quick Facts
- arXiv ID: 2307.08074
- Source URL: https://arxiv.org/abs/2307.08074
- Reference count: 25
- Key outcome: Introduces Disco-Bench, a benchmark evaluating language models on document-level discourse phenomena across 9 tasks in Chinese/English literature domains, showing that fine-grained pretraining improves discourse modeling.

## Executive Summary
Disco-Bench addresses a critical gap in language model evaluation by focusing on discourse phenomena that cross sentence boundaries, which existing benchmarks largely overlook. The benchmark comprises 9 document-level testsets in literature domains, covering understanding, translation, and generation tasks. Through experiments on 20 models including Transformers, pretrained models, and LLMs, the authors demonstrate that fine-grained pretraining on document-level literary data consistently improves discourse modeling capabilities. The benchmark includes a diagnostic test suite to examine whether models learn discourse knowledge, and the authors release datasets, pretrained models, and a leaderboard to support further research.

## Method Summary
Disco-Bench evaluates language models on discourse-aware tasks using 9 document-level testsets in Chinese and English literature domains. The benchmark uses existing pretrained models (BERT, RoBERTa, BART, mBART, GPT2) which are fine-tuned on the Disco-Bench tasks. A key innovation is fine-grained pretraining on a 400GB document-level literature corpus to improve discourse modeling. Evaluation employs multiple metrics including F1, EM, BLEU, PPL, BERTscore, and Dist scores. The diagnostic test suite uses contrastive pairs to probe model understanding of cohesion properties through perturbations of discourse devices.

## Key Results
- Fine-grained pretraining on document-level literary data consistently improves discourse modeling across all tasks
- Models trained on Disco-Bench corpus outperform those trained on general domain data on discourse-heavy tasks
- The diagnostic test suite effectively identifies models that learn genuine discourse knowledge versus those exploiting superficial cues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark exposes models to document-level discourse phenomena that cross sentences, unlike existing sentence-level benchmarks.
- Mechanism: By curating datasets from literature domains and designing tasks that require tracking cohesion and coherence across entire documents, models must learn to maintain context over longer spans.
- Core assumption: Rich discourse phenomena (e.g., anaphora, lexical cohesion, RST relations) are prevalent in literary texts and are not captured by sentence-level evaluations.
- Evidence anchors:
  - [abstract] "However, existing evaluation benchmarks primarily focus on the evaluation of inter-sentence properties and overlook critical discourse phenomena that cross sentences."
  - [section] "Disco-Bench consists of 9 document-level testsets in the literature domain, which contain rich discourse phenomena (e.g. cohesion and coherence) in Chinese and/or English."
- Break condition: If the literature datasets are noisier or less coherent than assumed, or if the discourse phenomena are not sufficiently diverse, models may not generalize.

### Mechanism 2
- Claim: Fine-grained pretraining on document-level discourse-rich data consistently improves modeling of discourse information.
- Mechanism: Initializing from coarse-grained pretrained models and further training on Disco-Bench's large-scale (400GB) literature corpus allows models to adapt to domain-specific discourse patterns.
- Core assumption: Pretraining on mixed-domain or sentence-level data (e.g., Wikipedia) does not adequately capture discourse phenomena present in literary texts.
- Evidence anchors:
  - [abstract] "fine-grained pretraining based on literary document-level training data consistently improves the modeling of discourse information."
  - [section] "Comparing our corpus to other commonly used datasets for pretraining models, Disco-Bench's dataset exhibits distinct attributes and advantages."
- Break condition: If the pretraining data size or domain mismatch is insufficient, or if the pretraining process overfits to specific discourse patterns, generalization may suffer.

### Mechanism 3
- Claim: The diagnostic test suite enables targeted evaluation of discourse knowledge learned by models.
- Mechanism: By creating contrastive pairs that modify discourse devices (e.g., repetition, substitution, ellipsis), the suite tests whether models can distinguish correct vs. incorrect discourse usage based on learned patterns.
- Core assumption: Models trained on Disco-Bench tasks will learn to rank correct discourse usage higher than perturbed versions.
- Evidence anchors:
  - [section] "To better measure the ability of models on discourse modeling, we handcraft a discourse-aware test suite that is complementary to general evaluation."
  - [section] "Each instance in our methodology comprises a contrastive pair, consisting of a correct and an incorrect input/hypothesis based on cohesion properties."
- Break condition: If the contrastive perturbations are too subtle or too artificial, or if models exploit superficial cues instead of true discourse understanding, the test may not be reliable.

## Foundational Learning

- Concept: Cohesion vs. Coherence
  - Why needed here: The benchmark evaluates both types of discourse properties, and understanding their distinction is critical for designing and interpreting tasks.
  - Quick check question: What is the difference between cohesion (e.g., lexical repetition) and coherence (e.g., RST relations)?

- Concept: Document-level vs. Sentence-level modeling
  - Why needed here: The novelty of Disco-Bench lies in evaluating discourse across entire documents, not isolated sentences.
  - Quick check question: Why is it important to evaluate discourse phenomena that cross sentence boundaries?

- Concept: Contrastive testing methodology
  - Why needed here: The diagnostic test suite relies on contrastive pairs to probe model understanding of discourse.
  - Quick check question: How does contrastive testing help isolate specific linguistic phenomena like cohesion?

## Architecture Onboarding

- Component map: Dataset curation pipeline -> Task definition modules -> Diagnostic test suite generator -> Pretraining module -> Evaluation framework
- Critical path:
  1. Curate and preprocess document-level datasets
  2. Define tasks and generate contrastive pairs for diagnostics
  3. Pretrain models on discourse-rich corpus
  4. Fine-tune on downstream tasks
  5. Evaluate using benchmark + diagnostic suite
- Design tradeoffs:
  - Document-level datasets are richer but harder to curate and annotate
  - Contrastive pairs provide targeted evaluation but may not reflect real-world usage
  - Fine-grained pretraining improves discourse modeling but increases training time and risk of overfitting
- Failure signatures:
  - Poor performance on diagnostic suite despite good benchmark scores (models not learning discourse)
  - High variance across tasks (inconsistent discourse modeling)
  - Overfitting to specific genres or phenomena (lack of generalization)
- First 3 experiments:
  1. Compare performance of models trained on Disco-Bench corpus vs. general corpus on a discourse-heavy task (e.g., ZPR).
  2. Ablate contrastive testing by evaluating models on both original and perturbed discourse instances.
  3. Analyze failure cases in diagnostic suite to identify which discourse phenomena are hardest to model.

## Open Questions the Paper Calls Out

- Open Question 1: What are the limitations of using Disco-Bench for evaluating discourse modeling in languages other than Chinese and English?
  - Basis in paper: [explicit] The paper states that Disco-Bench consists of 9 document-level testsets in the literature domain, which contain rich discourse phenomena in Chinese and/or English.
  - Why unresolved: The paper does not provide information on the performance of Disco-Bench in other languages or the generalizability of the benchmark to other languages.
  - What evidence would resolve it: Experiments evaluating Disco-Bench on discourse modeling tasks in other languages and comparing the results with those obtained in Chinese and English.

- Open Question 2: How does the performance of Disco-Bench compare to other existing discourse-aware evaluation benchmarks?
  - Basis in paper: [inferred] The paper introduces Disco-Bench as a benchmark for evaluating language models on discourse-aware tasks and claims that existing benchmarks primarily focus on inter-sentence properties and overlook critical discourse phenomena that cross sentences.
  - Why unresolved: The paper does not provide a direct comparison of Disco-Bench with other existing discourse-aware evaluation benchmarks or discuss the relative strengths and weaknesses of Disco-Bench.
  - What evidence would resolve it: A comprehensive comparison of Disco-Bench with other existing discourse-aware evaluation benchmarks in terms of task coverage, difficulty, and effectiveness in evaluating discourse modeling capabilities.

- Open Question 3: How does the performance of fine-grained pretraining on Disco-Bench data compare to other pretraining strategies for discourse modeling?
  - Basis in paper: [explicit] The paper states that fine-grained pretraining based on document-level literary data consistently improves discourse modeling and that Disco-Bench pretrained models generally improve the cohesion accuracies over their coarse-grained counterparts.
  - Why unresolved: The paper does not provide a direct comparison of fine-grained pretraining on Disco-Bench data with other pretraining strategies or discuss the relative effectiveness of different pretraining approaches for discourse modeling.
  - What evidence would resolve it: Experiments comparing the performance of models pretrained on Disco-Bench data with those pretrained using other strategies, such as pretraining on general domain data or using different pretraining objectives.

## Limitations
- The human evaluation covers only 8 representative models and 4 tasks, which may not fully represent the broader model landscape
- The benchmark's focus on literary texts may limit generalizability to other domains where different discourse patterns dominate
- The diagnostic test suite relies on hand-crafted perturbations that may not fully capture the complexity of natural discourse errors

## Confidence
- **High Confidence**: The benchmark's design and its coverage of document-level discourse phenomena are well-founded, supported by clear methodological descriptions and consistent results across multiple tasks.
- **Medium Confidence**: The effectiveness of fine-grained pretraining is demonstrated but would benefit from additional comparisons with other domain adaptation approaches.
- **Medium Confidence**: The diagnostic test suite provides valuable insights into model discourse understanding, though its reliability depends on the quality and representativeness of the contrastive pairs.

## Next Checks
1. Evaluate Disco-Bench-trained models on discourse-heavy tasks from non-literary domains (e.g., news, scientific articles) to test generalization beyond the benchmark's domain.
2. Conduct a systematic analysis of the diagnostic test suite's sensitivity to perturbation types and difficulty levels, including inter-annotator agreement studies for the human-created contrastive pairs.
3. Compare the impact of fine-grained pretraining corpus size on discourse modeling performance, including tests with varying percentages of the 400GB corpus to establish scalability limits.