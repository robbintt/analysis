---
ver: rpa2
title: Loss Spike in Training Neural Networks
arxiv_id: '2305.12133'
source_url: https://arxiv.org/abs/2305.12133
tags:
- loss
- training
- neural
- spike
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the phenomenon of loss spikes in neural network
  training. It identifies a smaller-loss-as-sharper (SLAS) structure in the loss landscape
  that causes unstable training and exponential loss increase when the sharpness exceeds
  a threshold.
---

# Loss Spike in Training Neural Networks

## Quick Facts
- **arXiv ID:** 2305.12133
- **Source URL:** https://arxiv.org/abs/2305.12133
- **Reference count:** 32
- **Primary result:** Loss spikes occur when training enters smaller-loss-as-sharper (SLAS) regions, causing exponential loss increase when sharpness exceeds threshold

## Executive Summary
This paper investigates the phenomenon of loss spikes during neural network training, where the loss suddenly increases exponentially before rapidly descending. The authors identify that this behavior arises when training enters regions with a smaller-loss-as-sharper (SLAS) structure in the loss landscape. The maximum eigenvalue of the Hessian (λmax) serves as a measure of sharpness, and when λmax exceeds 2/η (where η is the learning rate), training becomes unstable, causing the spike. The rapid descent is explained by frequency principle - low-frequency components dominate the sharpest direction and converge quickly. The paper also finds that loss spikes can facilitate condensation, where input weights evolve toward the same direction, potentially improving generalization by reducing effective network complexity.

## Method Summary
The study employs full-batch gradient descent on fully-connected and convolutional networks for both synthetic 1D fitting tasks (sin(x) + sin(4x)) and CIFAR-10 classification (1k subset). Key methods include computing the maximum eigenvalue of the loss Hessian (using Lanczos method for efficiency), monitoring loss trajectories, analyzing frequency distribution of output differences during spike events, and examining weight condensation patterns through PCA analysis of parameter trajectories. The research systematically varies learning rates to induce and study loss spikes across different network architectures and datasets.

## Key Results
- Loss spikes occur when training enters SLAS regions where sharpness increases as loss decreases
- λmax exceeding 2/η causes exponential loss increase and training instability
- Rapid loss descent after spikes is explained by frequency principle - low-frequency components converge quickly
- λmax measures sharpness but not generalization, as both good and bad generalization solutions can learn low-frequency well
- Loss spikes facilitate condensation, causing input weights to evolve toward the same direction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Loss spikes occur when training enters a smaller-loss-as-sharper (SLAS) structure where sharpness increases as loss decreases.
- **Mechanism:** The training trajectory moves toward increasingly sharp regions while descending in loss. Once the maximum eigenvalue of the Hessian (λmax) exceeds 2/η, the training becomes unstable and loss increases exponentially. The loss then decreases rapidly due to frequency principle - low-frequency components dominate the sharpest direction and converge quickly.
- **Core assumption:** The loss landscape has SLAS structure rather than smaller-loss-as-flatter (SLAF) structure, and low-frequency components converge faster than high-frequency components.
- **Evidence anchors:** [abstract] "When the training enters a region with a lower-loss-as-sharper (LLAS) structure, the training becomes unstable, and the loss exponentially increases once the loss landscape is too sharp"; [section] "The deviation in the first eigendirection, which can be reasonably explained by the frequency principle, as low-frequency information is captured rapidly, leading to the rapid descent"
- **Break condition:** If the loss landscape follows SLAF structure instead, or if high-frequency components dominate the sharpest direction, this mechanism would not hold.

### Mechanism 2
- **Claim:** λmax is a good measure of sharpness but not generalization because generalization depends on high-frequency components while λmax relates to low-frequency.
- **Mechanism:** In practical datasets, low-frequency information is dominant and shared by both training and test data. Therefore, solutions with good and bad generalization have similar projections on the first eigen direction (high λmax direction). Generalization depends on high-frequency components which correspond to small eigenvalues.
- **Core assumption:** Low-frequency is dominant in common datasets and is shared between training and test sets, while high-frequency components are more discriminative for generalization.
- **Evidence anchors:** [abstract] "a solution with good generalization and a solution with bad generalization can both learn low-frequency well, thus, they have little difference in the sharpest direction"; [section] "The maximum eigenvalue of the loss Hessian is a good measure of sharpness for whether the training is linearly stable but not a good measure for generalization"
- **Break condition:** If the dataset has dominant high-frequency components, or if the test set has significantly different frequency distribution than training set.

### Mechanism 3
- **Claim:** Loss spikes facilitate condensation where input weights evolve toward the same direction, improving generalization by reducing effective network complexity.
- **Mechanism:** During loss spikes with large learning rates, input weights of different neurons in the same layer evolve toward similar orientations. This condensation reduces the effective size of the network, creating lower effective complexity and potentially more descent directions, which may accelerate training and improve generalization.
- **Core assumption:** Condensed networks are equivalent to smaller networks with lower effective complexity, and this reduction in complexity improves generalization.
- **Evidence anchors:** [abstract] "loss spikes can facilitate condensation, causing input weights to evolve towards the same direction"; [section] "we find that loss spikes can facilitate condensation, that is, the input weights of different neurons in the same layer evolve towards the same"
- **Break condition:** If condensation doesn't occur with loss spikes, or if condensation doesn't improve generalization in the given context.

## Foundational Learning

- **Concept:** Linear stability analysis of gradient descent on quadratic models
  - Why needed here: Provides the foundation for understanding why λmax > 2/η causes instability
  - Quick check question: For a quadratic loss R(θ) = λθ²/2, what condition on λ and learning rate η ensures stable gradient descent training?

- **Concept:** Frequency principle in deep learning
  - Why needed here: Explains why low-frequency components converge faster, which is crucial for understanding rapid loss descent during spikes
  - Quick check question: In Fourier space, which components (low or high frequency) typically converge faster during neural network training according to the frequency principle?

- **Concept:** Hessian matrix and eigendecomposition
  - Why needed here: Essential for understanding sharpness (λmax) and how different eigen directions relate to frequency components
  - Quick check question: How does the eigendecomposition of the Hessian matrix relate to the directions of maximum and minimum curvature in the loss landscape?

## Architecture Onboarding

- **Component map:** Training loop with loss monitoring -> Hessian computation for λmax -> Frequency analysis of output differences -> Parameter trajectory tracking -> Weight condensation analysis
- **Critical path:** 1) Train model while monitoring loss and λmax, 2) Detect loss spikes (sudden loss increase), 3) Analyze frequency distribution of output differences during spike, 4) Examine eigen direction projections for generalization study, 5) Check for weight condensation patterns
- **Design tradeoffs:** Computing full Hessian is expensive; use Lanczos method for largest eigenvalues. Frequency analysis requires careful sampling. Condensation analysis needs consistent initialization for comparison.
- **Failure signatures:** No loss spikes despite large learning rates (SLAS structure absent), rapid descent without spike (different mechanism), λmax not correlating with stability (computation issues), no condensation despite spikes (activation function effects)
- **First 3 experiments:**
  1. Train a two-layer tanh FNN (20 hidden neurons) on sin(x) + sin(4x) using learning rate η = 0.05. Monitor loss and λmax to observe spike behavior when λmax > 2/η
  2. For CIFAR-10-1k, train a two-layer ReLU CNN with Max Pooling and learning rate η = 0.1. Track loss and λmax similarly to confirm spike occurrence at EoS stage
  3. Apply PCA to parameter trajectories and plot 2D loss surfaces to visualize SLAS structure and trajectory evolution during spikes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does the maximum eigenvalue of the loss Hessian correlate with condensation, and what is the underlying mechanism?
- **Basis in paper:** [explicit] The paper observes a correlation between λmax and condensation but does not explain the mechanism.
- **Why unresolved:** The paper conjectures this relationship but does not provide theoretical or empirical evidence to explain why larger λmax facilitates condensation.
- **What evidence would resolve it:** Experiments showing how varying λmax affects the evolution of input weights and whether this leads to condensation in different network architectures and training regimes.

### Open Question 2
- **Question:** What causes the eigen direction corresponding to large eigenvalues to be dominated by low-frequency components?
- **Basis in paper:** [inferred] The paper notes this phenomenon but does not explain why large eigenvalues relate to low-frequency.
- **Why unresolved:** The connection between eigenvalue magnitude and frequency dominance is observed but not theoretically justified.
- **What evidence would resolve it:** A mathematical proof or extensive empirical study showing how the loss Hessian structure inherently relates large eigenvalues to low-frequency directions across various datasets and network architectures.

### Open Question 3
- **Question:** Does loss spike always improve generalization through condensation, or are there conditions where it may harm generalization?
- **Basis in paper:** [explicit] The paper suggests loss spikes facilitate condensation which may improve generalization, but this is not proven universally.
- **Why unresolved:** The paper presents one direction of the relationship but doesn't explore cases where loss spikes might be detrimental.
- **What evidence would resolve it:** Systematic experiments varying spike frequency, magnitude, and timing across multiple datasets to identify when condensation improves vs. harms generalization.

### Open Question 4
- **Question:** How does the SLAS structure arise during training, and can it be predicted or controlled?
- **Basis in paper:** [explicit] The paper identifies SLAS as causing loss spikes but doesn't explain its origin or how to manipulate it.
- **Why unresolved:** The paper describes the structure's effect but not its formation process or controllability.
- **What evidence would resolve it:** Analysis of training dynamics showing how SLAS structures emerge from initialization and optimization choices, with methods to either encourage or prevent them.

## Limitations

- SLAS structure evidence is indirect - correlation between loss decrease and λmax increase is observed but not directly visualized or empirically verified
- Frequency principle generalization lacks systematic verification across different architectures, datasets, and training regimes
- Condensation mechanism and its relationship to improved generalization lacks rigorous quantification and controlled experiments

## Confidence

- **High Confidence:** The observation that λmax > 2/η causes training instability is well-established through linear stability analysis
- **Medium Confidence:** The explanation for rapid loss descent via frequency principle is plausible given existing literature on spectral bias
- **Low Confidence:** The condensation mechanism and its relationship to improved generalization through loss spikes is the most speculative claim

## Next Checks

1. **Direct SLAS Visualization:** Generate 2D loss surface plots around spike regions to empirically verify the smaller-loss-as-sharper structure
2. **Frequency-Eigen Direction Correlation:** Conduct systematic experiments measuring the correlation between frequency components of output differences and the eigen directions corresponding to λmax across different network architectures and datasets
3. **Controlled Condensation Experiments:** Design experiments that isolate condensation effects by comparing networks with artificially induced condensation versus natural spike-facilitated condensation, measuring generalization differences while controlling for other variables