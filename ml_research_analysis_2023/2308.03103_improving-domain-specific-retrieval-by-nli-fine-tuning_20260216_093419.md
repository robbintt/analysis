---
ver: rpa2
title: Improving Domain-Specific Retrieval by NLI Fine-Tuning
arxiv_id: '2308.03103'
source_url: https://arxiv.org/abs/2308.03103
tags:
- retrieval
- polish
- data
- language
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential of fine-tuning sentence encoders
  using natural language inference (NLI) data to improve information retrieval and
  ranking tasks. The authors employ a supervised contrastive learning method called
  SimCSE, which uses NLI data to create positive (entailment) and negative (contradiction)
  pairs for contrastive training.
---

# Improving Domain-Specific Retrieval by NLI Fine-Tuning

## Quick Facts
- arXiv ID: 2308.03103
- Source URL: https://arxiv.org/abs/2308.03103
- Reference count: 25
- Primary result: SimCSE fine-tuning with NLI data significantly improves retrieval performance (Recall@100 from 0.023 to 0.248 for HerBERT on Polish e-commerce dataset)

## Executive Summary
This paper investigates using natural language inference (NLI) data for fine-tuning sentence encoders to improve information retrieval and ranking tasks. The authors employ SimCSE, a supervised contrastive learning method that treats entailment pairs as positive examples and contradictions as hard negatives. They evaluate their approach on both English and Polish datasets using monolingual and multilingual models. Results show significant improvements in semantic textual similarity tasks and information retrieval metrics (NDCG and recall@100), demonstrating the effectiveness of NLI-based fine-tuning for retrieval tasks.

## Method Summary
The authors fine-tune sentence encoders using SimCSE contrastive learning with NLI data, treating entailment pairs as positive examples and contradictions as hard negatives. For Polish, they use machine-translated SNLI data filtered by COMET scores. Models are evaluated on semantic textual similarity tasks (STSB, SICK-R, CDS-R) and retrieval/ranking tasks (Recall@100, NDCG) using e-commerce datasets and benchmarks (KLEJ for Polish, SentEval for English). The approach works with both monolingual (HerBERT, BERT-base-uncased) and multilingual (XLM-RoBERTa) models.

## Key Results
- SimCSE fine-tuning significantly improves retrieval performance: Recall@100 improved from 0.023 to 0.248 for HerBERT on Polish e-commerce dataset
- Fine-tuning improves STS task performance across all tested models and languages
- NLI-based fine-tuning degrades monolingual model performance on KLEJ benchmark but improves multilingual model performance
- Uniformity of embeddings correlates with retrieval performance, while alignment does not

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SimCSE contrastive loss improves semantic retrieval by treating entailment pairs as positive examples and contradictions as hard negatives
- Mechanism: During training, positive pairs are pushed closer in embedding space while contradictions are pulled apart, creating discriminative representations
- Core assumption: NLI entailment pairs capture meaningful semantic similarity relevant to retrieval tasks
- Evidence anchors: [abstract] "SimCSE, which uses NLI data to create positive (entailment) and negative (contradiction) pairs for contrastive training" and [section] "we treated entailment pairs as positives and contradiction pairs as a hard negatives"

### Mechanism 2
- Claim: Machine-translated NLI data can effectively substitute for native language NLI datasets in contrastive training
- Mechanism: Translated SNLI pairs provide sufficient semantic signal despite translation noise, enabling cross-lingual contrastive learning
- Core assumption: Translation quality is adequate for preserving semantic relationships needed for contrastive training
- Evidence anchors: [abstract] "we test the feasibility of using machine translated NLI data and demonstrate this approach for Polish" and [section] "We evaluated the translations using COMET... mBart model reached score of 0.49 compared to 0.40 of m2m100"

### Mechanism 3
- Claim: Uniformity of embeddings (rather than alignment) correlates with improved retrieval performance
- Mechanism: Higher uniformity spreads representations more evenly in embedding space, improving nearest-neighbor retrieval
- Core assumption: Retrieval performance depends more on uniform coverage of embedding space than on maintaining semantic proximity
- Evidence anchors: [abstract] "we investigate uniformity and alignment of the embeddings to explain the effect of NLI-based fine-tuning... finding a link between uniformity and retrieval performance, but not alignment" and [section] "We calculated uniformity and alignment... only uniformity improved as the alignment metric increased"

## Foundational Learning

- Concept: Contrastive learning fundamentals
  - Why needed here: SimCSE relies on contrastive loss to create discriminative embeddings
  - Quick check question: What distinguishes positive pairs from negative pairs in contrastive learning?

- Concept: Natural language inference data structure
  - Why needed here: SimCSE uses NLI entailment/contradiction pairs for training
  - Quick check question: How are entailment pairs typically structured in NLI datasets?

- Concept: Embedding space properties (uniformity vs alignment)
  - Why needed here: Paper investigates these properties as explanations for retrieval performance
  - Quick check question: What's the mathematical difference between uniformity and alignment metrics?

## Architecture Onboarding

- Component map: Data preprocessing → Model fine-tuning (SimCSE) → Embedding generation → Retrieval/ranking evaluation
- Critical path: Translated NLI data → SimCSE fine-tuning → Embedding generation → Retrieval performance
- Design tradeoffs: Translation quality vs. dataset size, monolingual vs. multilingual models, STS vs. KLEJ performance
- Failure signatures: Poor STS performance but good retrieval suggests alignment issues; good STS but poor retrieval suggests domain mismatch
- First 3 experiments:
  1. Evaluate baseline model on STS tasks to establish translation quality
  2. Fine-tune with SimCSE and compare STS vs. retrieval performance
  3. Analyze uniformity/alignment metrics before and after fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of machine-translated NLI data impact the effectiveness of SimCSE fine-tuning for retrieval tasks?
- Basis in paper: [explicit] The paper discusses experiments with filtering translated sentences based on COMET scores and found that cleaning the dataset resulted in worse performance than baseline
- Why unresolved: The paper only tested one filtering approach (removing examples with scores < 0.05) and found it detrimental. The relationship between translation quality and fine-tuning effectiveness remains unclear
- What evidence would resolve it: Experiments testing different filtering thresholds, comparing multiple translation models, or evaluating the impact of translation quality on downstream tasks would provide more insight

### Open Question 2
- Question: Is there a relationship between alignment metrics and information retrieval performance in out-of-domain scenarios?
- Basis in paper: [explicit] The paper states "We did not observe a relationship between alignment and information retrieval as is reported in [7] or in the context of recommender systems [18]." They also note that previous work assessed these metrics in in-domain settings
- Why unresolved: The paper only investigated this for out-of-domain retrieval, and the impact of domain shift on the alignment-IR relationship is not well understood
- What evidence would resolve it: Systematic experiments comparing in-domain and out-of-domain settings, or controlled studies varying the domain gap between training and evaluation data

### Open Question 3
- Question: How does NLI fine-tuning affect the performance of sentence encoders on different types of downstream tasks beyond semantic textual similarity?
- Basis in paper: [explicit] The paper notes that "SimCSE fine-tuning degrades monolingual model performance on the KLEJ benchmark" which includes various NLU tasks beyond STS
- Why unresolved: The paper only tested KLEJ as a representative NLU benchmark. The broader impact on diverse task types (e.g., question answering, text classification, coreference resolution) is unknown
- What evidence would resolve it: Comprehensive evaluation of NLI fine-tuned models across multiple task families and benchmarks would clarify the scope of benefits and limitations

## Limitations

- Limited generalizability beyond tested domains and languages, particularly for the Polish translation approach
- No comprehensive analysis of domain shift between NLI data and specific retrieval tasks
- Limited explanation of the causal mechanism behind uniformity-improves-performance correlation

## Confidence

- High Confidence: The core finding that SimCSE fine-tuning with NLI data improves retrieval performance across both English and Polish datasets
- Medium Confidence: The explanation that uniformity (rather than alignment) of embeddings drives the performance improvements
- Medium Confidence: The claim that machine-translated NLI data can effectively substitute for native language data

## Next Checks

1. Test the fine-tuned models on retrieval tasks from completely different domains (e.g., medical or legal documents) to assess whether the improvements generalize beyond e-commerce and question-answering tasks

2. Conduct controlled experiments comparing models trained on different quality levels of translated NLI data (varying COMET thresholds) to quantify the impact of translation quality on retrieval performance

3. Perform an ablation study where only positive pairs (entailment) or only negative pairs (contradiction) are used in SimCSE training to determine which contributes more to the observed improvements