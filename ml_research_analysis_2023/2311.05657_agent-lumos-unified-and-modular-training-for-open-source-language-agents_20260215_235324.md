---
ver: rpa2
title: 'Agent Lumos: Unified and Modular Training for Open-Source Language Agents'
arxiv_id: '2311.05657'
source_url: https://arxiv.org/abs/2311.05657
tags:
- subgoal
- action
- actions
- agents
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LUMOS introduces a modular framework for training open-source language
  agents, addressing limitations of closed-source agents like high costs and lack
  of transparency. It features a planning module for generating high-level subgoals,
  a grounding module for translating subgoals into low-level actions, and an execution
  module for tool-based task completion.
---

# Agent Lumos: Unified and Modular Training for Open-Source Language Agents

## Quick Facts
- arXiv ID: 2311.05657
- Source URL: https://arxiv.org/abs/2311.05657
- Authors: 
- Reference count: 23
- Primary result: LUMOS achieves state-of-the-art performance on complex QA and web tasks, surpassing GPT-based agents and outperforming larger open-source agents on math tasks

## Executive Summary
LUMOS introduces a modular framework for training open-source language agents that addresses limitations of closed-source alternatives like high costs and lack of transparency. The framework consists of three core modules: planning for generating high-level subgoals, grounding for translating subgoals into low-level actions, and execution for tool-based task completion. By using unified, high-quality training annotations derived from diverse tasks via GPT-4-based conversion, LUMOS achieves state-of-the-art performance across multiple benchmarks. Its modular design and unified data format enable flexibility and cross-task generalization, making it a scalable solution for building robust open-source agents.

## Method Summary
LUMOS employs a modular architecture where a planning module generates high-level subgoals from task descriptions, a grounding module translates these subgoals into executable actions, and an execution module implements actions using external tools. The framework trains on unified, high-quality annotations converted from existing benchmarks using GPT-4, creating a shared semantic space across task types. Both planning and grounding modules are fine-tuned on Llama-2-7B using conversational format and binary masking, enabling the system to handle diverse interactive tasks while maintaining cross-task generalization capabilities.

## Key Results
- Outperforms GPT-based agents on complex QA and web tasks
- Surpasses larger open-source agents (WizardLM-30B, AgentLM-70B) on math tasks
- Generalizes effectively to unseen tasks like WebShop with new action spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular separation of planning and grounding enables cross-task generalization
- Mechanism: Planning module learns abstract subgoal generation independent of action space, while grounding module adapts to specific action interfaces. When encountering new tasks, only the grounding module needs updating.
- Core assumption: Subgoals are task-agnostic and can be expressed in natural language
- Evidence anchors:
  - [abstract] "modular design and unified data format enable flexibility and cross-task generalization"
  - [section 2.1] "planning module dissects a complex task into a series of high-level subgoals"
  - [corpus] FMR scores indicate related work on modular agents (0.55-0.63), but direct evidence for this specific separation mechanism is missing
- Break condition: If subgoals become too task-specific or tightly coupled to action space, cross-task generalization fails

### Mechanism 2
- Claim: Unified data format enables transfer learning across diverse interactive tasks
- Mechanism: Converting ground-truth reasoning steps from various domains into a common format creates a shared semantic space where knowledge transfers between tasks
- Core assumption: Intermediate reasoning steps contain transferable patterns across task types
- Evidence anchors:
  - [abstract] "unified data format that encompasses multiple task types"
  - [section 3] "convert ground-truth intermediate reasoning steps in existing benchmarks into a unified format"
  - [corpus] Missing direct evidence for unified format effectiveness, though related modular approaches exist
- Break condition: If task domains are too dissimilar, unified format may lose task-specific nuances

### Mechanism 3
- Claim: GPT-4-based annotation conversion provides higher quality than self-instruct methods
- Mechanism: Using GPT-4 to transform existing gold reasoning steps is more reliable than generating annotations from scratch
- Core assumption: GPT-4 can accurately follow conversion instructions while preserving reasoning quality
- Evidence anchors:
  - [section 3.1] "GPT-4 to execute annotation conversion on the given reasoning steps"
  - [section 4.5] "Our unified task representation along with our gathered data can be instrumental"
  - [corpus] No direct comparison with self-instruct methods in related work
- Break condition: If GPT-4 fails on complex reasoning, annotation quality degrades

## Foundational Learning

- Concept: Hierarchical task decomposition
  - Why needed here: Breaking complex tasks into subgoals enables modular training and generalization
  - Quick check question: Can a subgoal be accomplished by executing multiple low-level actions?

- Concept: Action space design
  - Why needed here: Well-defined action interfaces guide the grounding module to generate executable actions
  - Quick check question: Does each action have clear input arguments and output formats?

- Concept: Prompt engineering for instruction following
  - Why needed here: In-context examples guide LLMs to convert reasoning steps into the expected format
  - Quick check question: Do the in-context examples demonstrate both subgoal generation and action grounding?

## Architecture Onboarding

- Component map:
  - Planning Module → Grounding Module → Execution Module → Results → (back to Planning for iterative version)

- Critical path: Task → Planning → Grounding → Execution → Results → (back to Planning for iterative version)

- Design tradeoffs:
  - Modular design vs. integrated approaches: modularity enables generalization but adds complexity
  - High-level vs. low-level subgoals: abstraction enables transfer but may lose precision
  - Unified vs. task-specific training: shared knowledge vs. domain specialization

- Failure signatures:
  - Planning module outputs irrelevant or nonsensical subgoals
  - Grounding module generates invalid actions (wrong arguments, unsupported actions)
  - Execution module fails to implement valid actions due to API/tool issues

- First 3 experiments:
  1. Test planning module on a simple task from training data to verify subgoal generation
  2. Test grounding module with known subgoals to verify action generation
  3. Test end-to-end pipeline on a held-out task from training distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LUMOS scale with larger model sizes (e.g., 30B or 70B parameters) compared to its current 7B Llama-2 base model?
- Basis in paper: [explicit] The paper states LUMOS is built on Llama-2-7B and compares its performance to larger open-source agents (e.g., WizardLM-30B, AgentLM-70B), but does not explore scaling LUMOS itself to larger model sizes.
- Why unresolved: The authors focused on demonstrating LUMOS's effectiveness with a smaller, more efficient base model, but did not investigate whether larger models would provide additional benefits or diminishing returns.
- What evidence would resolve it: Training and evaluating LUMOS variants using larger open-source models (e.g., Llama-2-30B, Llama-2-70B) on the same benchmark tasks and comparing their performance metrics.

### Open Question 2
- Question: What is the impact of different grounding module architectures (e.g., encoder-decoder vs. decoder-only) on LUMOS's performance and cross-task generalization ability?
- Basis in paper: [inferred] The paper uses a decoder-only architecture for both planning and grounding modules, but does not explore alternative architectural choices that might affect grounding quality or generalization.
- Why unresolved: The authors chose a specific architecture without comparing it to alternatives, leaving open the question of whether different grounding module designs could yield better results.
- What evidence would resolve it: Implementing and evaluating LUMOS with alternative grounding module architectures (e.g., encoder-decoder models) while keeping the planning module and overall framework consistent.

### Open Question 3
- Question: How does LUMOS's performance degrade when faced with tasks requiring novel tools or APIs not seen during training?
- Basis in paper: [explicit] The paper demonstrates LUMOS's ability to generalize to unseen tasks like WebShop with new action spaces, but does not test scenarios involving entirely novel tools or APIs.
- Why unresolved: The generalization experiments focus on new tasks with different action spaces, but do not address scenarios where the required tools themselves are completely unfamiliar to the model.
- What evidence would resolve it: Creating benchmark tasks that require the use of novel tools/APIs not present in LUMOS's training data and measuring performance degradation compared to tasks with familiar tools.

## Limitations
- Relies on GPT-4 for annotation conversion, creating dependency on closed-source models
- Performance evaluation scope limited to specific benchmarks, generalizability to broader domains uncertain
- Implementation details for execution tools and APIs not fully specified

## Confidence

**Confidence Labels:**
- High confidence: The modular architecture design and unified data format are well-specified and reproducible
- Medium confidence: The claimed performance improvements over baseline agents, as these results depend on specific implementation details not fully disclosed
- Low confidence: The generalizability to truly unseen tasks beyond the WebShop experiment, as the evaluation scope is limited

## Next Checks

1. Implement the end-to-end pipeline on a simple held-out task from the training distribution to verify basic functionality
2. Test the grounding module's ability to generate valid actions when given subgoals from different task domains to assess cross-task generalization
3. Conduct ablation studies by training modules separately with and without unified data format to quantify the transfer learning benefits