---
ver: rpa2
title: 'Caseformer: Pre-training for Legal Case Retrieval Based on Inter-Case Distinctions'
arxiv_id: '2311.00333'
source_url: https://arxiv.org/abs/2311.00333
tags:
- legal
- case
- retrieval
- caseformer
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of legal case retrieval, which
  aims to help legal professionals find relevant cases related to their cases at hand.
  The key problem is that existing neural retrieval methods struggle in legal case
  retrieval due to their high demand for annotated data, which is difficult to obtain
  in legal domains.
---

# Caseformer: Pre-training for Legal Case Retrieval Based on Inter-Case Distinctions

## Quick Facts
- arXiv ID: 2311.00333
- Source URL: https://arxiv.org/abs/2311.00333
- Reference count: 40
- Key outcome: Caseformer is the first pre-trained language model method that outperforms BM25 in case retrieval datasets in zero-shot settings

## Executive Summary
This paper addresses the challenge of legal case retrieval, where existing neural methods struggle due to high demand for annotated data in legal domains. Caseformer introduces a novel pre-training framework that enables models to learn legal knowledge and domain-specific relevance information without human-labeled data. Through three unsupervised learning tasks (Legal LAnguage Modeling, Legal Judgment Prediction, and Factual Description Matching), the model captures the special language, document structure, and relevance patterns of legal case documents. Experimental results on multiple legal case retrieval datasets in both Chinese and English demonstrate that Caseformer outperforms existing methods in both zero-shot and full-data fine-tuning settings.

## Method Summary
Caseformer employs a three-stage pre-training approach using legal case documents and law books. The Legal Language Modeling (LAM) task uses masked language modeling to teach legal terminology and document structure. Legal Judgment Prediction (LJP) uses contrastive learning to teach the model to distinguish cases based on legal similarity rather than just textual similarity. Factual Description Matching (FDM) employs contrastive learning to capture factual similarity patterns between cases. The model uses a dual-encoder architecture for efficient retrieval and a cross-encoder for re-ranking. Pre-training is performed on 5M Chinese case documents and law books, with evaluation on LeCaRD, CAIL-LCR, CAIL-SCM, and COLIEE 2020 datasets.

## Key Results
- Caseformer outperforms BM25 in zero-shot settings on legal case retrieval datasets
- The model achieves state-of-the-art performance on multiple legal case retrieval benchmarks in both Chinese and English
- Caseformer demonstrates superior performance in both zero-shot and full-data fine-tuning settings compared to existing neural retrieval methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model's ability to outperform BM25 in zero-shot settings stems from its specialized legal pre-training tasks that teach domain-specific relevance judgment.
- Mechanism: Caseformer uses three unsupervised pre-training tasks (LAM, LJP, FDM) that explicitly model legal language, judgment similarity, and factual description matching, creating embeddings that better capture legal relevance than generic IR PLMs.
- Core assumption: Legal case relevance requires understanding legal language, document structure, and judgment-based similarity patterns beyond semantic similarity.
- Evidence anchors:
  - [abstract] "Through three unsupervised learning tasks, Caseformer is able to capture the special language, document structure, and relevance patterns of legal case documents"
  - [section 4.2] "an ideal legal case retrieval model should not only consider semantic similarity but also evaluate the legal-level similarity between cases"

### Mechanism 2
- Claim: The Legal Judgment Prediction (LJP) task teaches the model to distinguish cases based on legal similarity rather than just textual similarity.
- Mechanism: By grouping cases with similar factual descriptions and training the model to select cases with matching crimes and legal provisions, the model learns to prioritize legal relevance over lexical overlap.
- Core assumption: Cases with similar fact descriptions but different legal outcomes are fundamentally different for retrieval purposes.
- Evidence anchors:
  - [section 4.2] "in the context of legal case retrieval, the models should not only consider semantic similarity but also evaluate the legal-level similarity between cases"
  - [section 4.2] "we train the model to select cases with the same crimes and provisions from a series of cases with similar factual descriptions"

### Mechanism 3
- Claim: The Legal Language Modeling (LAM) task enables the model to understand specialized legal terminology and document structure.
- Mechanism: Pre-training on official law books and case documents with masked language modeling teaches the model legal vocabulary, syntax, and the typical organization of legal documents.
- Core assumption: Legal documents contain specialized terminology and structures that differ significantly from general text.
- Evidence anchors:
  - [section 4.1] "legal documents often contain specialized terminology, expressions, and content structures that are rarely observed in general domain documents"
  - [section 4.1] "We consider these specific professional terms, expressions, and writing structures within the legal field as legal language"

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is the foundational pre-training task that teaches the model to understand context and predict missing words, forming the basis for learning legal terminology in LAM.
  - Quick check question: What does the model learn when it predicts masked tokens in legal documents versus general text?

- Concept: Contrastive Learning
  - Why needed here: Contrastive learning in LJP and FDM tasks teaches the model to distinguish relevant from irrelevant cases based on legal similarity, not just semantic similarity.
  - Quick check question: How does the model learn which cases are relevant when it only sees positive and negative examples during pre-training?

- Concept: Dual-Encoder vs Cross-Encoder Architectures
  - Why needed here: Understanding the architectural tradeoffs between efficient retrieval (dual-encoder) and accurate re-ranking (cross-encoder) is crucial for implementing Caseformer's two-stage approach.
  - Quick check question: When would you choose a dual-encoder architecture over a cross-encoder for legal case retrieval?

## Architecture Onboarding

- Component map: LAM (MLM on law books) -> LJP (contrastive on judgment similarity) -> FDM (contrastive on factual similarity) -> Dual-encoder retrieval -> Cross-encoder re-ranking

- Critical path: 1. Pre-train on law books with LAM task 2. Pre-train on case corpus with LJP and FDM tasks 3. Fine-tune for specific retrieval task (dual-encoder) or re-ranking (cross-encoder)

- Design tradeoffs: Pre-training on law books vs case documents: Law books provide clean legal language but lack case-specific context; case documents provide practical examples but may contain noise; Information extraction accuracy: The quality of extracted facts, crimes, and provisions directly impacts LJP and FDM task effectiveness

- Failure signatures: Poor zero-shot performance: Indicates pre-training tasks didn't capture legal relevance patterns effectively; Degradation when fine-tuning: Suggests architectural mismatch between pre-training and downstream tasks; Sensitivity to input truncation: May indicate insufficient modeling of document structure

- First 3 experiments: 1. Compare zero-shot retrieval performance on a small legal subset using different pre-training task combinations (LAM only, LAM+LJP, LAM+LJP+FDM) 2. Evaluate information extraction quality by measuring consistency of automatically extracted facts/crimes across similar cases 3. Test dual-encoder vs cross-encoder performance on a re-ranking benchmark to validate architectural choices

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Caseformer be adapted to handle legal documents from countries with non-standardized formats?
- Basis in paper: [inferred] The paper mentions that legal documents in some countries lack standardized formats, making it difficult to automatically extract information like fact descriptions, legal provisions, and crimes. This poses a challenge for the FDM and LJP tasks which require such structured information.
- Why unresolved: The paper acknowledges this limitation but does not provide a concrete solution for handling non-standardized legal documents.
- What evidence would resolve it: Developing and testing automatic methods to extract relevant information from unstructured legal documents across different countries would resolve this open question.

### Open Question 2
- Question: How does the performance of Caseformer compare to other state-of-the-art legal-specific pre-trained models on legal case retrieval tasks?
- Basis in paper: [explicit] The paper compares Caseformer's performance to other pre-trained models like Legal-BERT, BERT-XS, and Lawformer on legal case retrieval tasks. However, it does not compare to other potentially state-of-the-art legal-specific models that may have been developed after the paper's publication.
- Why unresolved: The field of legal-specific pre-trained models is rapidly evolving, and new models may have been developed that could potentially outperform Caseformer.
- What evidence would resolve it: Conducting experiments comparing Caseformer's performance to the latest legal-specific pre-trained models on legal case retrieval tasks would resolve this open question.

### Open Question 3
- Question: How does the effectiveness of Caseformer's pre-training tasks (LAM, LJP, FDM) vary across different types of legal cases (e.g., criminal, civil, administrative)?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of Caseformer's pre-training tasks on criminal case datasets. However, it does not investigate how these tasks perform on other types of legal cases.
- Why unresolved: Different types of legal cases may have varying structures, terminology, and relevance criteria, which could impact the effectiveness of the pre-training tasks.
- What evidence would resolve it: Conducting experiments evaluating Caseformer's performance on various types of legal cases (criminal, civil, administrative, etc.) would provide insights into the generalizability and effectiveness of its pre-training tasks across different legal domains.

## Limitations
- The model's performance heavily depends on the quality and availability of structured legal information (facts, crimes, provisions) which may not be consistently available across different legal systems
- The contrastive learning approach requires careful negative sampling to avoid false negatives, and the assumption that similar fact descriptions with different judgments represent distinct legal concepts may not always hold
- The paper demonstrates strong results primarily on Chinese datasets, with limited validation on English legal corpora

## Confidence

**High**: The model's architecture and three-task pre-training framework are technically sound and well-documented

**Medium**: Performance claims on English datasets and cross-lingual generalization

**Medium**: Claims about BM25 outperformance in zero-shot settings, given limited baseline comparisons

## Next Checks

1. **Generalization Test**: Evaluate Caseformer on a diverse set of legal domains (civil, criminal, administrative) to verify claims about handling different legal contexts beyond the tested datasets

2. **Data Dependency Analysis**: Systematically measure model performance degradation when key structured elements (crimes, provisions) are missing or noisy to quantify the information extraction bottleneck

3. **Cross-lingual Robustness**: Test zero-shot transfer from Chinese to other language legal systems to validate claims about capturing universal legal relevance patterns rather than language-specific features