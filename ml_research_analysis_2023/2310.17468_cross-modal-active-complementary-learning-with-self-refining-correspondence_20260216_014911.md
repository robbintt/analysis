---
ver: rpa2
title: Cross-modal Active Complementary Learning with Self-refining Correspondence
arxiv_id: '2310.17468'
source_url: https://arxiv.org/abs/2310.17468
tags:
- learning
- noisy
- noise
- crcl
- correspondence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of noisy correspondence (NC)
  in image-text matching, a common issue in real-world scenarios where training pairs
  are not perfectly aligned. The proposed Cross-modal Robust Complementary Learning
  framework (CRCL) enhances existing methods' robustness against NC by introducing
  an Active Complementary Loss (ACL) and a Self-refining Correspondence Correction
  (SCC).
---

# Cross-modal Active Complementary Learning with Self-refining Correspondence

## Quick Facts
- arXiv ID: 2310.17468
- Source URL: https://arxiv.org/abs/2310.17468
- Reference count: 40
- This paper addresses the challenge of noisy correspondence (NC) in image-text matching, a common issue in real-world scenarios where training pairs are not perfectly aligned.

## Executive Summary
This paper addresses the challenge of noisy correspondence (NC) in image-text matching, a common issue in real-world scenarios where training pairs are not perfectly aligned. The proposed Cross-modal Robust Complementary Learning framework (CRCL) enhances existing methods' robustness against NC by introducing an Active Complementary Loss (ACL) and a Self-refining Correspondence Correction (SCC). ACL combines active and complementary learning to reduce the risk of erroneous supervision, while SCC uses momentum correction and multiple self-refining processes to achieve accurate and stable correspondence corrections. Extensive experiments on three benchmarks (Flickr30K, MS-COCO, and CC152K) demonstrate that CRCL significantly outperforms state-of-the-art methods in handling synthetic and real-world noisy correspondences, achieving substantial improvements in retrieval performance across various noise levels.

## Method Summary
The Cross-modal Robust Complementary Learning framework (CRCL) introduces two key components to handle noisy correspondences: Active Complementary Loss (ACL) and Self-refining Correspondence Correction (SCC). ACL combines weighted active learning (focusing on reliable positive pairs) with robust complementary learning (indirect learning via exponential normalization) to reduce reliance on potentially incorrect direct labels. SCC uses Momentum Correction (MC) to aggregate historical predictions, smoothing out noise and improving accuracy over time, along with Multiple Self-Refining (SR) processes that clear accumulated error by retraining from scratch. The framework creates a feedback loop where improved correspondence estimates enable better loss weighting, which in turn produces better correspondence estimates.

## Key Results
- CRCL significantly outperforms state-of-the-art methods in handling synthetic and real-world noisy correspondences
- Substantial improvements in retrieval performance across various noise levels on Flickr30K, MS-COCO, and CC152K benchmarks
- The combination of ACL and SCC creates a feedback loop that progressively improves correspondence estimates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Active Complementary Loss (ACL) reduces overfitting to noisy correspondences by balancing direct supervision with robust indirect learning.
- Mechanism: ACL combines weighted active learning (focusing on reliable positive pairs) with robust complementary learning (indirect learning via exponential normalization). This reduces reliance on potentially incorrect direct labels while still allowing the model to learn discriminative features from clean pairs.
- Core assumption: Noisy correspondences can be identified with reasonable accuracy, allowing the model to downweight them in training.
- Evidence anchors:
  - [abstract]: "ACL exploits active and complementary learning losses to reduce the risk of providing erroneous supervision"
  - [section]: "we propose a novel Active Complementary Loss (ACL) under the risk minimization theory [28, 26] to provide noise tolerance for noisy pairs while ensuring discriminative learning"
  - [corpus]: Weak - the corpus neighbors focus on related problems but don't directly validate the ACL mechanism
- Break condition: If the model cannot distinguish clean from noisy pairs early in training, the complementary component becomes ineffective and the weighted loss collapses.

### Mechanism 2
- Claim: Self-refining Correspondence Correction (SCC) achieves stable and accurate correspondence corrections by aggregating historical predictions with momentum.
- Mechanism: SCC uses Momentum Correction (MC) to aggregate historical predictions, smoothing out noise and improving accuracy over time. Multiple Self-Refining (SR) processes clear accumulated error by retraining from scratch.
- Core assumption: Historical predictions contain useful information that can be aggregated to improve current estimates, even in the presence of noise.
- Evidence anchors:
  - [abstract]: "SCC utilizes multiple self-refining processes with momentum correction to enlarge the receptive field for correcting correspondences"
  - [section]: "SCC leverages Momentum Correction (MC) to aggregate historical predictions, providing stable and accurate correspondence estimations while alleviating the over-memorization to NC"
  - [corpus]: Weak - no direct corpus evidence supporting momentum-based refinement for correspondence correction
- Break condition: If initial correspondence estimates are too inaccurate, error accumulation will persist despite momentum smoothing.

### Mechanism 3
- Claim: The combination of ACL and SCC creates a feedback loop where improved correspondence estimates enable better loss weighting, which in turn produces better correspondence estimates.
- Mechanism: As SCC refines correspondence estimates, ACL can more accurately weight active vs complementary learning. Better-weighted learning produces more accurate predictions, feeding back into SCC for further refinement.
- Core assumption: The system can bootstrap from initial noisy estimates to progressively cleaner ones through iterative refinement.
- Evidence anchors:
  - [abstract]: "Our framework introduces a novel Active Complementary Loss (ACL) that applies active and complementary learning to mutually boost robustness against NC"
  - [section]: "Through adequate cross-modal training, our CRCL will obtain a more stable and accurate soft correspondence label by smoothly evolving and expanding the receptive field of correction based on historical predictions with MC"
  - [corpus]: Weak - the corpus neighbors don't discuss this specific feedback mechanism between correction and loss weighting
- Break condition: If either component fails (ACL provides poor weighting or SCC provides poor estimates), the feedback loop breaks and performance degrades.

## Foundational Learning

- Concept: Triplet ranking loss and its limitations with noisy correspondences
  - Why needed here: Understanding why standard triplet loss fails with noisy correspondences is crucial for appreciating the need for ACL
  - Quick check question: Why does triplet ranking loss with hard negatives perform poorly under high noise rates?

- Concept: Cross-modal representation learning and shared embedding spaces
  - Why needed here: The paper assumes familiarity with how images and texts are projected into a shared space for matching
  - Quick check question: What are the typical architectures used to project images and texts into a common embedding space?

- Concept: Momentum-based aggregation in deep learning
  - Why needed here: SCC's momentum correction mechanism relies on understanding how momentum averaging works in training
  - Quick check question: How does momentum correction differ from simple moving averages in parameter updates?

## Architecture Onboarding

- Component map:
  - Cross-modal encoders (image and text) -> Similarity computation module -> Active Complementary Loss (ACL) module -> Self-refining Correspondence Correction (SCC) module with Momentum Correction (MC) -> Training loop with multiple SR pieces

- Critical path:
  1. Encode image-text pairs
  2. Compute bidirectional similarities
  3. Update correspondence estimates via SCC
  4. Compute ACL with current estimates
  5. Backpropagate and update model
  6. Repeat across SR pieces

- Design tradeoffs:
  - Tradeoff between exploration (complementary learning) and exploitation (active learning) in ACL
  - Memory vs accuracy tradeoff in SCC (storing historical predictions)
  - Computational cost vs robustness benefit of multiple SR pieces

- Failure signatures:
  - If correspondence estimates don't improve across SR pieces, check initial labeling quality
  - If performance degrades with higher momentum (β), check for error accumulation
  - If complementary loss dominates, check if active loss weighting is appropriate

- First 3 experiments:
  1. Ablation study: Remove SCC and test ACL alone vs baseline
  2. Parameter sweep: Test different momentum coefficients (β) on a validation set
  3. Noise sensitivity: Test performance across different noise rates to verify theoretical robustness bounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed CRCL framework perform on other cross-modal tasks beyond image-text matching, such as video-text retrieval or audio-image matching, under noisy correspondence conditions?
- Basis in paper: [inferred] The paper focuses on image-text matching but mentions that noisy correspondence is a general problem across different modalities and tasks.
- Why unresolved: The paper only evaluates CRCL on image-text matching datasets and does not explore its applicability to other cross-modal tasks.
- What evidence would resolve it: Experiments applying CRCL to video-text retrieval or audio-image matching datasets with synthetic or real noisy correspondences, comparing performance against state-of-the-art methods for those tasks.

### Open Question 2
- Question: What is the impact of varying the noise rate distribution (e.g., non-uniform noise) on the performance of CRCL compared to uniform noise assumptions?
- Basis in paper: [explicit] The paper assumes uniform noisy correspondence (ηij = η) in the problem statement and analysis, but real-world noise may not be uniformly distributed.
- Why unresolved: The theoretical analysis and experiments are based on uniform noise assumptions, leaving the performance under non-uniform noise conditions unexplored.
- What evidence would resolve it: Experiments introducing non-uniform noise (e.g., higher noise rates for certain image-text pairs) in benchmark datasets and comparing CRCL's performance against other methods under these conditions.

### Open Question 3
- Question: How does the proposed CRCL framework handle category-level noise in addition to instance-level noisy correspondence, and what modifications would be necessary to address both types of noise simultaneously?
- Basis in paper: [inferred] The paper focuses on instance-level noisy correspondence but mentions that other fields like visible-infrared person re-identification are also troubled by noisy correspondence, implying potential category-level noise issues.
- Why unresolved: The paper does not address category-level noise or discuss potential modifications to handle both instance-level and category-level noise simultaneously.
- What evidence would resolve it: Experiments introducing category-level noise (e.g., incorrect class labels for image-text pairs) in benchmark datasets and evaluating CRCL's performance with and without modifications to handle both noise types.

## Limitations
- The theoretical analysis of ACL's risk minimization properties lacks rigorous proof, relying instead on intuitive explanations
- The momentum correction mechanism in SCC is validated empirically but lacks theoretical justification for why momentum averaging specifically works better than alternatives
- The claim that CRCL can handle "real-world" noisy correspondences is based on limited experimental evidence without systematic characterization of real-world noise patterns

## Confidence
- **High confidence**: The experimental results showing CRCL's superiority over baselines on standard benchmarks with synthetic noise
- **Medium confidence**: The effectiveness of the Active Complementary Loss mechanism, supported by ablation studies
- **Low confidence**: The robustness claims for real-world noisy correspondences without more diverse real-world testing scenarios

## Next Checks
1. **Theoretical validation**: Prove or disprove the convergence properties of the momentum correction mechanism under various noise distributions
2. **Real-world noise characterization**: Systematically test CRCL on multiple datasets with different types of real-world noise (e.g., domain shift, semantic drift, annotation errors)
3. **Ablation of hyperparameters**: Conduct a comprehensive sensitivity analysis of the momentum coefficient β, temperature τ, and threshold ϵ across the full range of possible values