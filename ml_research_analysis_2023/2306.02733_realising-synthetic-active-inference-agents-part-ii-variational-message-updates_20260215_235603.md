---
ver: rpa2
title: 'Realising Synthetic Active Inference Agents, Part II: Variational Message
  Updates'
arxiv_id: '2306.02733'
source_url: https://arxiv.org/abs/2306.02733
tags:
- message
- variational
- passing
- then
- free
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper derives variational message updates for synthetic active
  inference agents based on a constrained factor graph framework. The key contribution
  is the derivation of message passing algorithms that minimize a generalized free
  energy objective, inducing epistemic behavior in agents.
---

# Realising Synthetic Active Inference Agents, Part II: Variational Message Updates

## Quick Facts
- arXiv ID: 2306.02733
- Source URL: https://arxiv.org/abs/2306.02733
- Reference count: 40
- Primary result: Derives variational message updates for synthetic active inference agents that minimize generalized free energy, inducing epistemic behavior in discrete-variable models

## Executive Summary
This paper presents a framework for deriving variational message updates for synthetic active inference agents using constrained factor graph notation. The key innovation is the derivation of message passing algorithms that minimize a generalized free energy (GFE) objective rather than the traditional Bethe free energy (BFE). This approach induces epistemic behavior in agents, enabling them to seek information and explore their environment. The authors apply their framework to a discrete-variable model commonly used in active inference and demonstrate superior performance in a T-maze navigation task compared to BFE-based agents.

## Method Summary
The paper introduces a Constrained Forney-style Factor Graph (CFFG) notation to visually represent optimization constraints for active inference. Using variational calculus, the authors derive stationary solutions to a local GFE Lagrangian under normalization and marginalization constraints. These solutions translate into message update rules for nodes and edges in the factor graph. The framework is applied to a discrete-variable goal-observation submodel, with message updates derived for four messages. The perception-action cycle is implemented with time-dependent constraints, and the resulting GFE-based agent is evaluated against a BFE baseline in a T-maze task.

## Key Results
- Message updates derived from GFE minimization induce epistemic behavior in synthetic active inference agents
- GFE-based agent outperforms BFE-based agent in T-maze task, successfully navigating to explore and exploit the environment
- The constrained FFG notation provides a modular framework for designing scalable active inference agents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Variational message updates derived via constrained optimization induce epistemic behavior in synthetic active inference agents.
- Mechanism: Stationary solutions to a local Generalized Free Energy (GFE) Lagrangian using variational calculus correspond to fixed-point iterations defining message passing updates. The GFE objective incorporates epistemic priors that drive information-seeking behavior.
- Core assumption: Local Lagrangian optimization under normalization and marginalization constraints yields unique stationary points corresponding to meaningful message updates for control.
- Evidence anchors:
  - [abstract]: "The current paper (part II) derives message passing algorithms that minimise (generalised) FE objectives on a CFFG by variational calculus. The resulting control algorithms then induce epistemic behaviour in synthetic AIF agents."
  - [section]: "The resulting control algorithms then induce epistemic behaviour in synthetic AIF agents."
- Break condition: Fixed-point iterations may not converge to stationary points, leading to suboptimal control policies.

### Mechanism 2
- Claim: The p-substitution constraint transforms VFE into GFE that maximizes mutual information between future observations and states, leading to exploration.
- Mechanism: Substituting the variational distribution for observations with the observation model creates a GFE objective that maximizes mutual information between expected observations and hidden states.
- Core assumption: The p-substitution constraint is properly formulated and does not introduce computational instabilities.
- Evidence anchors:
  - [abstract]: "When we substitute a factor in the expectation term of the VFE (2) we write G instead of F for clarity."
  - [section]: "Minimisation of the GFE maximises a mutual information between future observations and states [21]."
- Break condition: Numerical instabilities from p-substitution or improper expectation evaluation may prevent effective epistemic value capture.

### Mechanism 3
- Claim: The constrained FFG notation provides a visual and unambiguous representation of optimization constraints, enabling modular derivation and reuse of message updates across models.
- Mechanism: CFFG notation explicitly represents normalization, marginalization, factorization, and data constraints as visual elements, clarifying the optimization problem and guiding message update derivation.
- Core assumption: Visual representation accurately captures mathematical constraints without omitting critical details.
- Evidence anchors:
  - [abstract]: "A companion paper (part I) introduces a Constrained FFG (CFFG) notation that visually represents (generalised) FE objectives for AIF."
  - [section]: "In brief, a CFFG is constructed from an FFG according to the following rules: Beads represent normalised variational distributions..."
- Break condition: Visual notation may become too complex or ambiguous for intricate models, hindering message update derivation.

## Foundational Learning

- Concept: Variational Bayes and Free Energy Minimization
  - Why needed here: The paper builds upon variational Bayes to derive message passing algorithms for active inference. Understanding VFE minimization under constraints is crucial for grasping message update derivation.
  - Quick check question: What is the relationship between variational free energy and Bayesian surprise, and how does minimizing VFE relate to approximating the posterior distribution?

- Concept: Factor Graphs and Message Passing
  - Why needed here: The paper uses Forney-style Factor Graphs (FFGs) and Constrained FFGs (CFFGs) to represent the generative model and optimization constraints. Understanding message passing algorithms on factor graphs is essential for following message update derivation.
  - Quick check question: How do sum-product and variational message passing algorithms differ in their computation of messages on factor graphs?

- Concept: Active Inference and Expected Free Energy
  - Why needed here: The paper focuses on realizing synthetic active inference agents. Understanding active inference principles, including EFE's role in guiding exploration and exploitation, is necessary for appreciating the significance of derived message updates.
  - Quick check question: What is the difference between variational free energy and expected free energy in active inference, and how does EFE relate to epistemic behavior?

## Architecture Onboarding

- Component map:
  - Generative Model (GM) -> Constrained FFG (CFFG) -> Variational Distribution -> Message Passing Algorithm -> Perception-Action Cycle

- Critical path:
  1. Define generative model and constraints in CFFG notation
  2. Derive GFE objective and Lagrangian
  3. Compute stationary solutions using variational calculus
  4. Translate stationary solutions into message update rules
  5. Implement perception-action cycle with time-dependent constraints
  6. Evaluate agent performance on task

- Design tradeoffs:
  - GFE vs. BFE: GFE induces epistemic behavior but may be more computationally complex than BFE
  - Direct vs. Indirect Message Updates: Direct updates are simpler but may not converge; indirect updates (using Newton's method) are more stable but require additional computation
  - Discrete vs. Continuous Variables: Paper focuses on discrete variables, but extending to continuous variables may require different message update rules

- Failure signatures:
  - Divergence of GFE over iterations
  - Suboptimal policy selection due to local minima in GFE landscape
  - Computational instability due to p-substitution or importance sampling
  - Incorrect implementation of CFFG notation or message update rules

- First 3 experiments:
  1. Implement T-maze navigation task with GFE-based agent and compare performance to BFE-based agent
  2. Vary reward utility and probability in T-maze task to assess agent adaptability
  3. Introduce secondary dynamical model for goal prior and evaluate agent performance in more complex environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed constrained factor graph notation improve clarity and consistency of variational message passing algorithms for active inference compared to traditional approaches?
- Basis in paper: [explicit] The paper states that CFFG notation "visually represents (generalised) FE objectives for AIF" and helps "interpret and disambiguate variational objectives and constraints."
- Why unresolved: While the paper claims improved clarity, it does not provide direct comparison or empirical evidence showing practical benefits of CFFG over traditional methods.
- What evidence would resolve it: Comparative study demonstrating reduced errors or improved efficiency in implementing message passing algorithms using CFFG versus traditional approaches.

### Open Question 2
- Question: What are the convergence properties of the proposed message updates for generalized free energy-based control in active inference agents?
- Basis in paper: [explicit] The paper mentions a "possible convergence issue" with direct application of message update rule for message 2 and provides alternative approach using Newton's method.
- Why unresolved: The paper does not provide comprehensive analysis of convergence properties or guarantees for proposed message updates.
- What evidence would resolve it: Theoretical analysis or empirical study demonstrating convergence rate and stability of proposed message updates under various conditions.

### Open Question 3
- Question: How does the epistemic behavior induced by generalized free energy objective compare to that of Bethe free energy in more complex and realistic environments?
- Basis in paper: [explicit] The paper compares GFE-based agents to BFE-based agents in T-maze task, showing GFE agents exhibit epistemic behavior while BFE agents do not.
- Why unresolved: Comparison is limited to simple T-maze environment. It is unclear how agents would perform in more complex and realistic scenarios with higher-dimensional state spaces and more intricate reward structures.
- What evidence would resolve it: Experiments evaluating agents' performance in more complex environments, such as multi-agent settings or continuous state spaces.

## Limitations

- Scalability concerns: Framework focuses on discrete-variable cases; extending to continuous domains may require significant modifications
- Computational complexity: P-substitution constraint may introduce numerical instabilities in real-world applications
- Limited validation: Performance comparison only conducted in simple T-maze environment; generalizability to complex scenarios remains uncertain

## Confidence

- **High confidence**: Derivation of message updates from stationary solutions of GFE Lagrangian is mathematically sound and well-supported by variational calculus framework
- **Medium confidence**: Claim that GFE-based agents outperform BFE-based agents in epistemic tasks is supported by T-maze simulations, but generality across different environments is uncertain
- **Low confidence**: Assertion that constrained FFG notation provides truly modular and reusable framework for all active inference models requires further validation with diverse model architectures

## Next Checks

1. Implement the T-maze task with both GFE and BFE agents and verify the reported performance difference across multiple random seeds
2. Test the algorithm's robustness by introducing noise in the observation model A and measuring the impact on agent performance and convergence
3. Extend the framework to a continuous-variable model (e.g., Gaussian state-space model) and validate whether the message update derivations generalize as claimed