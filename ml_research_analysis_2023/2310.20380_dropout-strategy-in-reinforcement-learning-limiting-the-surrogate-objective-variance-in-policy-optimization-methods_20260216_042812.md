---
ver: rpa2
title: 'Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective
  Variance in Policy Optimization Methods'
arxiv_id: '2310.20380'
source_url: https://arxiv.org/abs/2310.20380
tags:
- uni00000013
- uni00000011
- uni00000048
- uni00000014
- uni00000018
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the issue of high variance in the surrogate
  objective of policy optimization methods like PPO and TRPO, which can affect algorithm
  stability and convergence. The authors derive an upper bound for the surrogate objective
  variance that grows quadratically with the objective value.
---

# Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods

## Quick Facts
- arXiv ID: 2310.20380
- Source URL: https://arxiv.org/abs/2310.20380
- Reference count: 40
- The paper proposes a dropout strategy to limit surrogate objective variance in policy optimization, applied to PPO (D-PPO) with significant performance improvements on Atari 2600 environments.

## Executive Summary
This paper addresses the critical issue of high variance in the surrogate objective of policy optimization methods like PPO and TRPO, which can destabilize learning and hinder convergence. The authors derive an upper bound showing that surrogate objective variance grows quadratically with the objective value. To mitigate this, they propose a dropout strategy that selectively removes data samples with small contributions to the variance-bound term. Applied to PPO as D-PPO, the method achieves significant performance improvements across multiple Atari 2600 environments while maintaining lower surrogate objective variance during training.

## Method Summary
The method builds upon PPO by introducing a variance-reduction dropout mechanism. During each training epoch, the algorithm computes a contribution term Δᵢ for each sample that estimates its impact on the variance bound. Samples with small |Δᵢ| values are filtered out, creating a reduced dataset that limits variance growth while preserving informative samples. The D-PPO algorithm maintains the standard PPO framework (GAE advantage estimation, clipping) but applies this dropout strategy before each policy update. The dropout ratio r controls the aggressiveness of sample filtering, with experiments showing r=0.2 provides optimal balance between variance reduction and performance.

## Key Results
- D-PPO achieves significant performance improvements over PPO in 5 out of 6 tested Atari 2600 environments
- Surrogate objective variance is consistently lower during D-PPO training compared to PPO
- The dropout strategy effectively limits the excessive increase of variance while maintaining sample utilization efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dropout strategy reduces surrogate objective variance by selectively removing samples with small contributions to the variance-bound term.
- Mechanism: The algorithm computes a term Δᵢ that approximates the contribution of each sample to the variance bound. Samples with small |Δᵢ| are removed from the training batch, thereby reducing the variance of the importance sampling estimate.
- Core assumption: The variance bound in Corollary 1 is tight enough that reducing Δᵢ directly reduces the true variance.
- Evidence anchors:
  - [abstract] "proposed a dropout technique to avoid the excessive increase of the surrogate objective variance caused by importance sampling"
  - [section] "we want the value of ˆΔᵢ to be as large as possible, whether it is positive or negative. This means that data X is divided into two parts based on the sign of ˆΔᵢ"
- Break condition: If the bound is loose or the variance contribution is not well-approximated by Δᵢ, the dropout may remove informative samples and hurt performance.

### Mechanism 2
- Claim: Limiting variance enables larger policy updates without violating trust-region constraints.
- Mechanism: By reducing variance, the algorithm can safely increase the KL divergence or clipping threshold between old and new policies, leading to more aggressive learning steps.
- Core assumption: Lower variance surrogate objective allows the trust-region constraint to be satisfied with larger parameter updates.
- Evidence anchors:
  - [abstract] "limiting the surrogate objective variance in policy optimization methods"
  - [section] "To enable the agent to reuse the data generated by the old policy and improve sample utilization efficiency, we can introduce importance sampling"
- Break condition: If the policy network is too sensitive, even reduced-variance updates may destabilize learning.

### Mechanism 3
- Claim: Dropout prevents overfitting to noisy importance weights by reducing reliance on high-variance samples.
- Mechanism: By filtering out samples with extreme importance weights, the algorithm avoids fitting to noise in the reward-to-go or advantage estimates.
- Core assumption: High-importance-weight samples are more likely to be noisy or outlier estimates.
- Evidence anchors:
  - [abstract] "effectively limited the excessive increase of the surrogate objective variance during training"
  - [section] "we want the value of ˆΔᵢ to be as large as possible, whether it is positive or negative"
- Break condition: If the dropout ratio is too high, the algorithm may lose diversity in the training data and converge prematurely.

## Foundational Learning

- Concept: Importance sampling in policy optimization
  - Why needed here: The paper uses importance sampling to reuse old data, which introduces variance; understanding this is critical to why dropout helps.
  - Quick check question: Why does reusing old data via importance sampling increase variance in the surrogate objective?

- Concept: Trust-region constraints in PPO/TRPO
  - Why needed here: The dropout strategy aims to enable larger updates while maintaining stability, which relies on understanding trust-region methods.
  - Quick check question: How does the KL divergence or clipping in PPO enforce a trust-region constraint?

- Concept: Monte Carlo estimation and variance bounds
  - Why needed here: The dropout strategy is based on a derived variance bound; understanding variance estimation is key to why the bound matters.
  - Quick check question: What is the difference between the expectation and variance of a Monte Carlo estimator?

## Architecture Onboarding

- Component map: Policy network and value network (shared conv layers, separate heads) -> GAE advantage estimator -> Importance sampling ratio and surrogate objective computation -> Dropout strategy module (computes Δᵢ, filters data) -> PPO update loop with dropout applied each epoch

- Critical path:
  1. Collect trajectories with current policy
  2. Compute advantages using GAE
  3. Compute importance ratios and surrogate objective
  4. Compute Δᵢ for each sample
  5. Apply dropout strategy to filter data
  6. Update policy and value networks

- Design tradeoffs:
  - Dropout ratio r: Higher r reduces variance more but risks losing informative samples
  - Bound tightness: The theoretical bound may not be tight in practice
  - Computational cost: Computing Δᵢ for all samples adds overhead

- Failure signatures:
  - If variance reduction is too aggressive, returns may plateau or degrade
  - If dropout removes too many high-return samples, exploration may suffer
  - If the bound is loose, variance may not actually decrease despite dropout

- First 3 experiments:
  1. Run D-PPO with r=0.2 on Breakout and compare return and variance to PPO
  2. Sweep r from 0.1 to 0.5 and measure impact on return and variance
  3. Compare D-PPO to PPO in a sparse-reward environment to test dropout's effect on sample efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical relationship between the dropout ratio hyperparameter r and the variance reduction of the surrogate objective in D-PPO?
- Basis in paper: [explicit] The paper conducts experiments with different dropout ratios (r = 0.1, 0.2, 0.3, 0.4, 0.5) and finds r = 0.2 performs best, but doesn't provide theoretical justification for this value.
- Why unresolved: The paper empirically tests different values but doesn't derive theoretical bounds or conditions for optimal r selection.
- What evidence would resolve it: A mathematical proof showing how r affects the variance bound derived in Theorem 1, or a principled method for selecting r based on environment characteristics.

### Open Question 2
- Question: How does the dropout strategy affect exploration in sparse reward environments?
- Basis in paper: [inferred] The paper mentions that most data in sparse reward environments isn't helpful for policy improvement, and the dropout strategy discards some data, but doesn't analyze the trade-off between variance reduction and exploration.
- Why unresolved: The paper focuses on variance reduction but doesn't investigate whether discarding data might eliminate rare but important exploration events.
- What evidence would resolve it: Experiments comparing exploration metrics (state visitation entropy, novelty measures) between D-PPO and PPO in sparse reward environments.

### Open Question 3
- Question: Can the dropout strategy be extended to other policy optimization algorithms beyond PPO?
- Basis in paper: [explicit] The paper presents a general dropout strategy framework (Algorithm 1) that works with "any policy optimization algorithm that introduces importance sampling," but only applies it to PPO.
- Why unresolved: The paper demonstrates effectiveness with PPO but doesn't test the framework with TRPO, SAC, or other algorithms that use importance sampling.
- What evidence would resolve it: Experiments applying the dropout strategy to TRPO, SAC, or other policy optimization algorithms showing consistent variance reduction and performance improvements.

## Limitations

- The effectiveness depends on the tightness of the derived variance bound, which is not empirically validated
- The method assumes that removing samples with small Δᵢ contributions will consistently reduce true variance without losing critical information
- Computational overhead of computing Δᵢ for all samples is not discussed, which could impact scalability

## Confidence

- High confidence in the mathematical derivation of the variance bound
- Medium confidence in the practical effectiveness of the dropout strategy
- Medium confidence in the experimental results showing performance improvements

## Next Checks

1. Verify the variance reduction empirically by comparing true variance of importance weights with and without dropout across multiple training epochs
2. Test D-PPO with varying dropout ratios (r=0.1, 0.3, 0.5) to identify optimal trade-off between variance reduction and sample efficiency
3. Evaluate D-PPO in sparse-reward environments to assess whether dropout improves sample efficiency beyond just variance reduction