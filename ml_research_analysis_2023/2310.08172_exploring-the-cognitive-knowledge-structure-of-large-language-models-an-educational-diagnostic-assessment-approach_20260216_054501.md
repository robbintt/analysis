---
ver: rpa2
title: 'Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational
  Diagnostic Assessment Approach'
arxiv_id: '2310.08172'
source_url: https://arxiv.org/abs/2310.08172
tags:
- knowledge
- llms
- cognitive
- assessment
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel approach to assess the cognitive
  knowledge structure of large language models (LLMs) using educational diagnostic
  assessment. The researchers employ MoocRadar, a dataset annotated with Bloom's Taxonomy,
  to evaluate models' performance across various cognitive dimensions and knowledge
  types.
---

# Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach

## Quick Facts
- arXiv ID: 2310.08172
- Source URL: https://arxiv.org/abs/2310.08172
- Reference count: 13
- Key outcome: Introduces diagnostic assessment framework to evaluate LLM knowledge structures across Bloom's Taxonomy dimensions, revealing that GPT-4 outperforms other models but still lags behind human performance, especially in social science and humanities.

## Executive Summary
This study introduces a novel approach to assess the cognitive knowledge structure of large language models (LLMs) using educational diagnostic assessment methods. The researchers employ MoocRadar, a dataset annotated with Bloom's Taxonomy, to evaluate models' performance across various cognitive dimensions and knowledge types. They investigate three research questions: performance analysis across disciplines, deficit assessment of knowledge structure compared to humans, and error assessment of answers and explanations. Results show that GPT-4 outperforms other models but still lags behind human performance, especially in social science and humanities. The study reveals that models struggle with intermediate-level cognitive tasks and exhibit errors in explanations, particularly for multiple-choice questions.

## Method Summary
The study uses MoocRadar, an 8,453-question dataset annotated with Bloom's Taxonomy (4 knowledge types, 5 cognitive dimensions, 3 question types). Three models (GPT-3.5, ChatGPT, GPT-4) undergo zero-shot evaluation with tailored prompts per question type. Context retrieval via BM25 from MOOCCubeX subtitles provides two related discussions per question. Human annotators evaluate all model outputs for correctness, computing accuracy per discipline and Bloom dimension, explanation accuracy and consistency, and Pearson correlation between model and human Bloom distributions for similarity scores.

## Key Results
- GPT-4 outperforms other models but still lags behind human performance, especially in social science and humanities
- Models struggle with intermediate-level cognitive tasks (Apply, Analyze) more than basic (Remember, Understand) or advanced (Evaluate) tasks
- Answer-explanation inconsistency is prevalent, particularly for multiple-choice questions where models may select some correct options with flawed reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The diagnostic assessment framework maps LLM knowledge structures by evaluating performance across Bloom's Taxonomy dimensions and knowledge types.
- Mechanism: By decomposing questions into specific cognitive dimensions (Remember, Understand, Apply, Analyze, Evaluate) and knowledge types (Factual, Conceptual, Procedural, Meta), the framework captures fine-grained differences in LLM performance that aggregate scores miss.
- Core assumption: LLM performance varies systematically across different cognitive dimensions and knowledge types, revealing their knowledge structure.
- Evidence anchors: Based on educational diagnostic assessment method, we conduct an evaluation using MoocRadar, a meticulously annotated human test dataset based on Bloom Taxonomy. According to Bloom's Taxonomy, questions in MoocRadar are categorized into four Knowledge-Types: Factual-knowledge, Conceptual-knowledge, Procedural-knowledge, and Meta-knowledge; and six cognitive dimensions: Remember, Understand, Apply, Analyze, Evaluate, and Create.

### Mechanism 2
- Claim: Comparing LLM performance distributions to human distributions reveals knowledge structure alignment.
- Mechanism: The similarity score (Pearson correlation coefficient) measures how closely LLM performance patterns match human patterns across the taxonomy dimensions, capturing knowledge structure beyond raw accuracy.
- Core assumption: Human-like knowledge structures produce similar performance patterns across cognitive dimensions.
- Evidence anchors: We develop a metric to measure their similarity to humans, which primarily considers knowledge structure, beyond mere performance, and estimates the extent to which their cognitive structure is proportional to that of humans. Likeness(M) = ρ(̃x, y), where ρ(̃x, y) represents the Pearson Correlation Coefficient of ̃x and y.

### Mechanism 3
- Claim: Context retrieval using BM25 improves LLM performance by simulating human-like problem-solving with relevant knowledge.
- Mechanism: Retrieving related discussions from course materials provides contextual knowledge that helps LLMs answer questions more accurately, mimicking how humans use domain knowledge.
- Core assumption: LLMs can effectively utilize retrieved context to improve performance on educational questions.
- Evidence anchors: To simulate human-like behavior that solving exercises with relevant knowledge, we leverage the BM25 algorithm to retrieve the two most related discussions from the subtitles in the corresponding courses in MOOCCubeX. Additional knowledge from context indeed enhances the performance of the models.

## Foundational Learning

- Concept: Bloom's Taxonomy cognitive dimensions
  - Why needed here: The framework relies on Bloom's Taxonomy to categorize questions and analyze LLM performance across different cognitive levels.
  - Quick check question: What are the six cognitive dimensions in Bloom's Taxonomy, and which ones are used in this study?

- Concept: Diagnostic assessment in education
  - Why needed here: The study adapts educational diagnostic assessment methods to evaluate LLM knowledge structures.
  - Quick check question: What are the two main approaches of diagnostic assessment mentioned in the paper, and how are they applied to LLMs?

- Concept: Knowledge types classification
  - Why needed here: Understanding how questions are categorized into Factual, Conceptual, Procedural, and Meta knowledge types is crucial for interpreting the results.
  - Quick check question: How are the four knowledge types defined, and why is this categorization important for the assessment?

## Architecture Onboarding

- Component map: MoocRadar dataset -> Bloom's Taxonomy categorization -> Context retrieval (BM25) -> LLM prompting -> Answer generation -> Human evaluation -> Performance analysis -> Similarity calculation
- Critical path: Dataset loading → Question categorization → Context retrieval (if enabled) → LLM prompting → Answer generation → Human evaluation → Performance analysis → Similarity calculation
- Design tradeoffs: Using human annotation ensures quality but limits scalability; context retrieval adds computational overhead but improves performance; focusing on zero-shot evaluation makes results comparable but may underestimate model capabilities
- Failure signatures: Poor performance across all dimensions suggests dataset or prompting issues; inconsistent answer-explanation pairs indicate generation problems; similarity scores that don't correlate with model capabilities suggest framework limitations
- First 3 experiments:
  1. Run baseline evaluation without context to establish performance across dimensions
  2. Enable context retrieval and measure performance improvement
  3. Calculate similarity scores and compare with raw performance metrics

Assumption: The framework assumes that educational assessment methods transfer effectively to LLM evaluation, which may not hold if LLM cognition differs fundamentally from human cognition.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do large language models' cognitive knowledge structures compare to humans across different disciplines and cognitive dimensions?
- Basis in paper: Explicit - The paper introduces a novel approach to assess the cognitive knowledge structure of LLMs using educational diagnostic assessment and MoocRadar dataset.
- Why unresolved: The study reveals that while GPT-4 outperforms other models, it still lags behind human performance, especially in social science and humanities. The reasons for these discrepancies and potential ways to improve model performance in these areas remain unclear.
- What evidence would resolve it: Further experiments comparing LLM performance across various disciplines and cognitive dimensions with human performance, along with analysis of the underlying reasons for any gaps.

### Open Question 2
- Question: What are the error patterns in large language models' answers and explanations, and how do they differ between single-choice, multiple-choice, and true/false questions?
- Basis in paper: Explicit - The paper investigates error patterns in LLM answers and explanations across different question types.
- Why unresolved: While the study finds that models perform worse in multiple-choice questions and that explanations can be less accurate than answers, the underlying causes of these errors and potential strategies to improve explanation accuracy remain unclear.
- What evidence would resolve it: Detailed analysis of error patterns in LLM responses across different question types, including examination of the reasoning process and potential sources of errors.

### Open Question 3
- Question: How can the similarity between large language models' knowledge structures and humans be quantified and improved?
- Basis in paper: Explicit - The paper develops a similarity score to measure the likeness of LLMs' knowledge structures to humans.
- Why unresolved: While the study finds that more advanced models are more aligned with human cognitive patterns, the factors contributing to this alignment and methods to further improve it are not explored.
- What evidence would resolve it: Research on the factors influencing the similarity between LLM and human knowledge structures, along with experiments testing strategies to enhance this similarity.

## Limitations

- The study's framework assumes Bloom's Taxonomy effectively maps LLM knowledge structures, but fundamental differences between human and LLM cognition may limit this approach's validity
- Human annotation quality and consistency are critical yet underspecified, with no reported inter-annotator agreement metrics
- Dataset filtering criteria for the 8,453 questions remain unspecified, raising concerns about reproducibility

## Confidence

- High confidence: GPT-4's superior performance across dimensions and disciplines is well-supported by direct measurements
- Medium confidence: The similarity score as a measure of knowledge structure alignment - while methodologically sound, its interpretability across different model architectures is uncertain
- Low confidence: The interpretation that context retrieval improves performance through human-like knowledge utilization - the mechanism may differ fundamentally from human cognition

## Next Checks

1. Test answer-explanation consistency across different option orderings to quantify primacy effects and order-dependent biases
2. Compare similarity scores with raw performance metrics across all Bloom dimensions to validate their interpretability
3. Conduct ablation studies removing context retrieval to quantify its contribution to performance improvements