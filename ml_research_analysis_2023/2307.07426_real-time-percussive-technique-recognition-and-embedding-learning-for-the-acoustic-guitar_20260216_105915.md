---
ver: rpa2
title: Real-time Percussive Technique Recognition and Embedding Learning for the Acoustic
  Guitar
arxiv_id: '2307.07426'
source_url: https://arxiv.org/abs/2307.07426
tags:
- uni00000013
- uni00000011
- guitar
- class
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents real-time guitar body percussion recognition
  and embedding learning techniques for the acoustic guitar. The proposed methods
  use convolutional neural networks (CNNs) and CNNs jointly trained with variational
  autoencoders (VAEs) to classify and embed guitar body hits according to a taxonomy
  based on hand part and location.
---

# Real-time Percussive Technique Recognition and Embedding Learning for the Acoustic Guitar

## Quick Facts
- arXiv ID: 2307.07426
- Source URL: https://arxiv.org/abs/2307.07426
- Reference count: 0
- This work presents real-time guitar body percussion recognition and embedding learning techniques for the acoustic guitar using CNNs and CNNs jointly trained with VAEs.

## Executive Summary
This paper introduces real-time guitar body percussion recognition and embedding learning techniques for the acoustic guitar. The proposed methods use convolutional neural networks (CNNs) and CNNs jointly trained with variational autoencoders (VAEs) to classify and embed guitar body hits according to a taxonomy based on hand part and location. The networks are trained on a dataset of guitar body percussion and evaluated using cross-dataset evaluation. Results show that the networks are strong classifiers, especially in a simplified 2-class recognition task, and the VAEs yield improved class separation compared to CNNs.

## Method Summary
The method employs three CNN-based architectures (TablaCNN, PercCNN, PercVAE) with an 11.6 ms input window to classify and embed guitar body percussion sounds. The networks are trained on the GPercRep dataset (3,157 examples, 52:37 audio at 44.1 kHz) using hold-out cross-validation (80% train, 20% validation), Adam optimizer for 100 epochs, batch size 128, and data augmentation (high-pass filters, distortion, phase inversion, gain changes). The VAE learns a probabilistic latent representation that encodes both class identity and continuous variations, enabling richer control beyond simple categorical output.

## Key Results
- All networks achieve near-perfect accuracy (>99% F-measure) in 2-class kick vs. non-kick classification
- VAEs show improved class separation compared to CNNs, evidenced by increased KL-Divergence across distributions
- Cross-dataset evaluation reveals significant generalization challenges between GPercRep and GPercPat datasets

## Why This Works (Mechanism)

### Mechanism 1
The VAE joint training improves class separation in embedding space compared to a CNN-only model. The VAE learns a probabilistic latent representation that encodes not only the class identity but also continuous variations (e.g., dynamics, location) that correlate with other attributes in the taxonomy. This leads to overlapping clusters rather than discrete boundaries, enabling smoother interpolation. If the VAE latent space fails to capture any continuous variation beyond the class labels, the KL-Divergence will be low and similar to the CNN baseline.

### Mechanism 2
The simplified 2-class classification (kick vs. non-kick) achieves near-perfect accuracy due to clear acoustic differences. Heel hits ("kicks") have a distinct low-frequency transient and body resonance that differ significantly from other hand parts, making them acoustically separable in short time windows. If other hand parts (e.g., thumb or fingers) produce similar low-frequency transients to heel hits, the 2-class classifier will misclassify non-kicks as kicks.

### Mechanism 3
The 11.6 ms input buffer meets real-time constraints while capturing sufficient information for classification. The buffer is short enough to keep latency below the 10 ms threshold for acceptable musical performance, and the use of FFT features provides frequency resolution sufficient for distinguishing hand parts and hit locations. If the 11.6 ms window is too short to capture the full spectral content of some hits, classification accuracy will degrade, especially for techniques with longer decay times.

## Foundational Learning

- **Concept: Variational Autoencoders (VAEs) and latent space representations**
  - Why needed here: To understand how the VAE jointly learns classification and continuous feature representation, enabling control intimacy beyond simple categorical output.
  - Quick check question: How does the VAE's reparameterization trick allow backpropagation through the sampling process in the latent space?

- **Concept: Real-time audio processing constraints and latency requirements**
  - Why needed here: To appreciate why the 11.6 ms buffer size was chosen and how it impacts the system's usability in musical performance.
  - Quick check question: What is the maximum acceptable latency for musical instruments, and how does the 11.6 ms buffer size compare to this threshold?

- **Concept: Cross-dataset evaluation and generalization challenges**
  - Why needed here: To understand the limitations of the model when applied to data from different recording setups and how data augmentation can help.
  - Quick check question: What are the potential sources of domain shift between the GPercRep dataset and the GPercPat dataset, and how might they affect model performance?

## Architecture Onboarding

- **Component map**: Input (6-channel FFT features) → CNN encoder → (Classifier head + VAE bottleneck) → Decoder (for VAE) → Output (classification + latent vector)
- **Critical path**: Input → CNN → Classification/VAE bottleneck → Output; latency dominated by CNN forward pass + input buffer filling
- **Design tradeoffs**: Shorter input buffer reduces latency but may lose information; VAE adds computational overhead but enables richer control
- **Failure signatures**: High classification error on GPercPat indicates overfitting; poor latent space separation suggests VAE not learning meaningful continuous features
- **First 3 experiments**:
  1. Measure classification accuracy and latency on GPercRep with varying input buffer sizes (e.g., 5.8 ms, 11.6 ms, 23.2 ms) to find the optimal tradeoff.
  2. Compare KL-Divergence of latent space distributions for VAE vs. CNN on GPercRep to quantify improvement in continuous feature representation.
  3. Test model performance on GPercPat with different data augmentation strategies (e.g., frequency response changes, gain variations) to improve generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific design challenges around generalisation to different datasets have been identified?
- Basis in paper: [explicit] The paper mentions "Further design challenges around generalisation to different datasets have been identified" but does not specify what these challenges are.
- Why unresolved: The paper identifies the existence of generalisation challenges but does not elaborate on their nature or potential solutions.
- What evidence would resolve it: Detailed analysis of why the networks fail to generalise to different datasets, including differences in recording conditions, equipment, or data augmentation strategies that could address these issues.

### Open Question 2
- Question: How do musicians adapt their gestures when using the HITar with different network configurations, and how does this affect the system's performance?
- Basis in paper: [explicit] The paper mentions that "There is scope to expand the training and the evaluation by involving more guitars, more players and different data augmentation techniques" and that "musicians performing in real time may adapt their gestures until they reliably produce a desired set of outcomes."
- Why unresolved: The paper suggests that musicians may adapt their gestures to the system's expectations, but this has not been formally studied or quantified.
- What evidence would resolve it: Empirical studies involving multiple guitarists using the HITar with different network configurations, measuring gesture adaptation and its impact on classification accuracy and musical expressiveness.

### Open Question 3
- Question: How can the VAE's latent space parameters be effectively mapped to control parameters of a synthesis engine for expressive musical interaction?
- Basis in paper: [explicit] The paper argues that "the V AE embedding quality could support control intimacy and rich interaction when the latent space's parameters are used to control an external synthesis engine" but does not provide specific mapping strategies.
- Why unresolved: While the paper suggests that VAE embeddings could support expressive control, it does not demonstrate how to map these embeddings to synthesis parameters in a musically meaningful way.
- What evidence would resolve it: Development and evaluation of specific mapping functions between VAE latent space parameters and synthesis engine controls, with user studies to assess the quality and expressiveness of the resulting musical interactions.

## Limitations
- The VAE's ability to capture meaningful continuous variations beyond class labels remains unproven
- Cross-dataset performance reveals significant generalization challenges between GPercRep and GPercPat datasets
- The 11.6 ms buffer size may not capture sufficient information for all percussive techniques, particularly those with longer decay times

## Confidence
- **High**: The 2-class classification (kick vs. non-kick) achieves near-perfect accuracy, supported by clear acoustic differences between heel hits and other techniques.
- **Medium**: The VAE improves class separation in embedding space compared to CNN-only models, though the practical musical relevance of this separation requires further validation.
- **Low**: Claims about the VAE enabling "control intimacy" for external synthesis engines are speculative without demonstration of actual musical applications.

## Next Checks
1. Conduct perceptual studies with musicians to evaluate whether VAE embeddings capture musically meaningful variations that can be mapped to synthesis parameters.
2. Test the 11.6 ms buffer size across a wider range of percussive techniques to determine if classification accuracy degrades for techniques with longer temporal signatures.
3. Implement ablation studies on the GPercPat dataset to quantify the specific impact of different data augmentation strategies on cross-dataset generalization.