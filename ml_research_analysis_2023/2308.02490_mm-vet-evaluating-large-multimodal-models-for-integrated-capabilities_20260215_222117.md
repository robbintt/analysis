---
ver: rpa2
title: 'MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities'
arxiv_id: '2308.02490'
source_url: https://arxiv.org/abs/2308.02490
tags:
- arxiv
- capabilities
- language
- evaluation
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MM-Vet, a new evaluation benchmark for large
  multimodal models (LMMs) that systematically examines 16 tasks integrating 6 core
  vision-language capabilities (recognition, OCR, knowledge, language generation,
  spatial awareness, math). To address the challenge of evaluating open-ended outputs,
  the authors propose an LLM-based evaluator that uses GPT-4 to score model responses
  with a unified metric.
---

# MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities

## Quick Facts
- arXiv ID: 2308.02490
- Source URL: https://arxiv.org/abs/2308.02490
- Reference count: 40
- Best current models achieve only around 50% scores (full score 100%)

## Executive Summary
This paper introduces MM-Vet, a comprehensive evaluation benchmark for large multimodal models (LMMs) that systematically examines 16 tasks integrating 6 core vision-language capabilities. The benchmark addresses the challenge of evaluating open-ended outputs by proposing an LLM-based evaluator using GPT-4 to score model responses with a unified metric. Tested across various LMMs including end-to-end tuned models and LLM-tool-using systems, MM-Vet reveals that current best models achieve only around 50% scores, indicating significant room for improvement while providing capability insights beyond simple ranking.

## Method Summary
MM-Vet consists of 200 images with 218 questions testing 16 integrated vision-language tasks derived from 6 core capabilities: recognition, OCR, knowledge, language generation, spatial awareness, and math. The evaluation uses an LLM-based approach with GPT-4, employing few-shot prompting to handle open-ended outputs across diverse question types. A unified 0-1 scoring metric enables fair comparison across different model architectures and system paradigms. The benchmark was applied to representative LMMs including OpenFlamingo, LLaVA, MiniGPT-4, and MM-ReAct, with 5 evaluations per sample to account for variance.

## Key Results
- Current best models achieve only around 50% scores on the benchmark (full score 100%)
- End-to-end tuned models and LLM-tool-using systems show different relative strengths across capability integrations
- The unified GPT-4 evaluator successfully handles diverse question types and answer styles with a single scoring metric
- Significant room for improvement identified across all tested model types and paradigms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM-based evaluator successfully handles open-ended outputs across diverse question types and answer styles.
- Mechanism: Few-shot prompting with GPT-4 allows automatic learning of scoring criteria without manually defined binary/multiple-choice formats, enabling unified evaluation of both short answers and long essays.
- Core assumption: GPT-4 can infer scoring criteria from in-context examples and apply consistent evaluation across diverse output formats.
- Evidence anchors: Abstract states unified scoring metric enabled, section 3.2 describes few-shot prompt design, though weak corpus evidence.
- Break Condition: If GPT-4 fails to maintain consistent scoring criteria across different question types.

### Mechanism 2
- Claim: The six core VL capabilities and their 16 integrations create a comprehensive evaluation framework.
- Mechanism: By testing combinations of fundamental capabilities rather than isolated abilities, the benchmark captures real-world multimodal task complexity.
- Core assumption: Real-world tasks require integration of multiple core capabilities, and evaluating these integrations provides more insight than isolated capability testing.
- Evidence anchors: Abstract describes 6 core capabilities and 16 integrations, section 3.1 defines the integration approach, though weak corpus evidence.
- Break Condition: If defined capability integrations don't map well to real-world tasks.

### Mechanism 3
- Claim: The unified scoring metric enables fair comparison across different model architectures.
- Mechanism: The 0-1 scoring system applied consistently across all samples allows direct comparison between end-to-end models, tool-using systems, and commercial solutions.
- Core assumption: A unified metric can fairly evaluate diverse model outputs regardless of generation method or architecture.
- Evidence anchors: Abstract mentions unified scoring metric, section 4.1 describes the 0-1 scoring system, though weak corpus evidence.
- Break Condition: If the unified metric fails to capture important differences between model types.

## Foundational Learning

- **Multimodal capability integration**: Understanding how different vision-language capabilities combine is crucial for interpreting MM-Vet results and designing better models. Quick check: Can you explain why a model might perform well on recognition but poorly on OCR + recognition tasks?

- **LLM-based evaluation methodology**: The evaluation approach is novel and central to the benchmark's effectiveness. Quick check: What are the key advantages of using few-shot prompting for evaluation rather than manual scoring criteria?

- **Capability decomposition**: The six core capabilities form the foundation for understanding the benchmark structure. Quick check: Which capability would be most critical for answering "What is the largest denomination on the table?" and why?

## Architecture Onboarding

- **Component map**: MM-Vet benchmark dataset (200 images, 218 questions) → 6 core VL capabilities → 16 capability integration tasks → LLM-based evaluator using GPT-4 → unified 0-1 scoring metric → evaluation pipeline comparing different model types
- **Critical path**: Data collection → Capability definition → Task integration design → Evaluation metric development → Model testing → Result analysis
- **Design tradeoffs**: Open-ended vs closed-ended questions (flexibility vs evaluation complexity), unified metric vs specialized metrics (comparability vs accuracy), few-shot vs manual evaluation criteria (scalability vs precision)
- **Failure signatures**: Inconsistent GPT-4 scoring across similar samples, inability to evaluate certain question types, benchmark questions not mapping to real-world tasks, model performance not correlating with expected capabilities
- **First 3 experiments**:
  1. Test GPT-4 evaluator consistency by running same sample multiple times and checking score variance
  2. Validate capability definitions by having multiple annotators label sample requirements and checking agreement
  3. Test benchmark coverage by running diverse model types and verifying they produce varied, interpretable results across different capability integrations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different vision encoder architectures affect the performance of LMMs on multimodal tasks requiring integrated capabilities?
- Basis in paper: Explicit discussion of various vision models (CLIP-ViT/L14, EV-A-ViT-G) but notes absence of comprehensive ablation studies
- Why unresolved: Paper compares LMMs with different vision encoders but doesn't conduct systematic ablation studies isolating vision encoder effects
- What evidence would resolve it: Controlled experiments comparing identical LMM architectures with different vision encoders on MM-Vet benchmark

### Open Question 2
- Question: What is the optimal balance between end-to-end trained LMMs and LLM-tool-using systems for different types of multimodal tasks?
- Basis in paper: Explicit evaluation of both paradigms showing different strengths across capabilities
- Why unresolved: Paper identifies relative strengths but doesn't establish framework for determining optimal approach for specific task types
- What evidence would resolve it: Empirical studies mapping task characteristics to optimal system paradigms with quantitative thresholds

### Open Question 3
- Question: How can the LLM-based evaluation methodology be improved to reduce variance and increase reliability?
- Basis in paper: Explicit acknowledgment of high variance (around 15%) in GPT-4 scoring across repeated evaluations
- Why unresolved: Paper uses GPT-4 with few-shot examples but doesn't explore alternative prompting strategies, ensemble methods, or calibration techniques
- What evidence would resolve it: Comparative studies of different evaluation methodologies against human judgments

## Limitations

- The evaluation methodology relies heavily on GPT-4's consistency, but acknowledges high variance (around 15%) in scoring across repeated evaluations
- The effectiveness depends on whether 16 capability integrations adequately capture real-world multimodal task complexity
- Human-annotated ground truths and question-answer pairs are referenced as available externally but not directly provided in the paper

## Confidence

- **High Confidence**: The existence of MM-Vet as a benchmark with 200 images and 218 questions testing 6 core VL capabilities and their 16 integrations
- **Medium Confidence**: The effectiveness of the LLM-based evaluator in handling open-ended outputs across diverse question types, given acknowledged variance and limited validation
- **Medium Confidence**: The unified scoring metric's ability to fairly compare different model architectures and system paradigms, though the paper shows this works empirically but with some limitations

## Next Checks

1. **Evaluator Consistency Test**: Run the GPT-4 evaluation pipeline on 50 randomly selected samples from MM-Vet 5 times each, measuring standard deviation of scores to verify if 15% variance is consistent across full dataset.

2. **Capability Coverage Validation**: Have 3 independent annotators categorize 30 random MM-Vet questions according to the 6 core capabilities, measuring inter-annotator agreement to validate capability definitions.

3. **Benchmark Discrimination Test**: Apply MM-Vet to evaluate 3 new LMMs not mentioned in the paper (e.g., GPT-4V, Gemini, and a smaller open-source model), verifying that the benchmark successfully discriminates between models of varying capability levels and reveals meaningful relative strengths and weaknesses.