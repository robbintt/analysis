---
ver: rpa2
title: A Novel Computational and Modeling Foundation for Automatic Coherence Assessment
arxiv_id: '2310.00598'
source_url: https://arxiv.org/abs/2310.00598
tags:
- coherence
- tasks
- task
- text
- discourse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of automatic coherence assessment
  in NLP, which lacks a formal definition and evaluation metrics. The authors propose
  a computational approach based on the linguistic theory of coherence by Reinhart
  (1980), which defines three conditions: cohesion, consistency, and relevance.'
---

# A Novel Computational and Modeling Foundation for Automatic Coherence Assessment

## Quick Facts
- **arXiv ID**: 2310.00598
- **Source URL**: https://arxiv.org/abs/2310.00598
- **Reference count**: 30
- **Primary result**: Jointly training on five coherence proxy tasks (sentence reordering, irrelevant sentence detection, discourse relation recognition, noun phrase enrichment, and natural language inference) significantly improves automatic coherence assessment accuracy (22% for GCDC, 30% for CoheSentia) compared to strong baselines.

## Executive Summary
This paper addresses the long-standing challenge of automatic coherence assessment in NLP, which lacks formal definitions and evaluation metrics. The authors propose a computational approach based on Reinhart's linguistic theory of coherence, formalizing three coherence conditions (cohesion, consistency, relevance) as five NLP tasks. By jointly training a multi-task model on these proxy tasks, the approach learns rich coherence features that outperform individual task-specific models and achieve state-of-the-art results on two coherence scoring benchmarks. The method provides a solid foundation for large-scale coherence assessment and can be integrated into text generation systems to improve output quality.

## Method Summary
The method formalizes Reinhart's three coherence conditions as five computational NLP tasks: sentence reordering (SRO), irrelevant sentence detection (ISR), discourse relation recognition (DRR), noun phrase enrichment (NPE), and natural language inference (NLI). A multi-task learning model (BERT-large or T5-large) is jointly trained on these proxy tasks using shared representations. The model learns coherence features across tasks through parameter sharing and gradient updates from multiple loss functions. After training, the model is evaluated on coherence scoring tasks using real-world text (GCDC) and generated text (CoheSentia) datasets. The approach leverages the complementary nature of the five tasks to capture different aspects of coherence, with the hypothesis that jointly trained models will outperform task-specific models.

## Key Results
- Jointly training on all five tasks leads to better performance on each individual task compared to task-specific models
- The approach achieves 22% accuracy improvement on GCDC (real-world text) and 30% accuracy improvement on CoheSentia (generated text) compared to strong baselines
- The model effectively functions as a coherence detection system, capturing all three aspects of Reinhart's coherence definition through the five proxy tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Jointly training on all five tasks leads to better performance on each task compared to individual task-specific models.
- **Mechanism**: Shared representations learned across tasks enable cross-task feature transfer, where solving one task improves the ability to solve others due to overlapping coherence requirements.
- **Core assumption**: The five proxy tasks share sufficient underlying coherence features to benefit from multi-task learning.
- **Evidence anchors**: Experiments confirm better performance on each task with joint training compared to task-specific models; all tasks show inferior performance with exclusive fine-tuning.
- **Break condition**: If tasks are too disparate or share minimal features, joint training may hurt performance through interference rather than benefit.

### Mechanism 2
- **Claim**: The model trained on proxy tasks functions effectively as a coherence detection model.
- **Mechanism**: Each task captures a distinct aspect of Reinhart's coherence conditions (cohesion, consistency, relevance), so their combination covers the full coherence space.
- **Core assumption**: Reinhart's three coherence conditions are comprehensive and can be fully operationalized through the selected five tasks.
- **Evidence anchors**: The tasks collectively cover all conditions for coherence based on Reinhart's definition; model achieves state-of-the-art coherence scoring results.
- **Break condition**: If coherence has aspects not captured by these tasks (e.g., cultural context, style), the model will miss them.

### Mechanism 3
- **Claim**: The proposed model achieves state-of-the-art results on both coherence scoring benchmarks.
- **Mechanism**: By learning rich, multi-faceted representations through the proxy tasks, the model develops superior ability to judge coherence in both real-world and generated texts.
- **Core assumption**: The proxy tasks provide better coherence signals than previous methods (e.g., sentence reordering alone).
- **Evidence anchors**: Model shows 22% accuracy improvement for GCDC and 30% for CoheSentia compared to strong baselines.
- **Break condition**: If benchmarks are not representative of real coherence challenges, or if proxy tasks overfit to specific training datasets.

## Foundational Learning

- **Concept**: Multi-task learning and parameter sharing
  - Why needed here: The model uses a shared BERT encoder across five different tasks, requiring understanding of how gradients from multiple loss functions update shared parameters
  - Quick check question: What happens to shared parameters when one task's loss dominates during training?

- **Concept**: Topological sorting for sentence ordering
  - Why needed here: The SRO task uses topological sort to reconstruct sentence order from pairwise precedence predictions
  - Quick check question: How does the algorithm handle cycles in the precedence graph?

- **Concept**: Discourse relation recognition
  - Why needed here: The DRR task requires identifying implicit discourse relations between sentence pairs using PDTB annotations
  - Quick check question: What's the difference between explicit and implicit discourse relations in PDTB?

## Architecture Onboarding

- **Component map**: Text → Shared encoder (BERT/T5) → Task head → Prediction (for each of the five tasks)
- **Critical path**: Text → Shared encoder → Task head → Prediction (for each of the five tasks)
  - The shared encoder must learn task-agnostic features that benefit all tasks
  - Task heads must be simple enough to avoid overfitting while complex enough to capture task nuances

- **Design tradeoffs**:
  - Classification vs. generation models: Classification is simpler but generation may capture more nuanced patterns
  - Task weighting in multi-task loss: Equal weighting vs. task-specific weights based on difficulty or importance
  - Dataset size imbalance: Some tasks have much larger datasets than others

- **Failure signatures**:
  - Performance degrades when adding new tasks (interference)
  - One task dominates training (gradient imbalance)
  - Overfitting on specific tasks while underfitting on others
  - Poor coherence scoring despite good task performance

- **First 3 experiments**:
  1. Train individual models for each task separately, establish baseline performance
  2. Train joint model on all tasks, compare task performance to individual models
  3. Train joint model with different task weightings, find optimal configuration for coherence scoring

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed coherence model perform on long-form text (e.g., articles, essays) compared to shorter text (e.g., stories, dialogues)?
- Basis in paper: The paper focuses on evaluating the model on short stories and real-world texts with a maximum of 333 tokens. However, the model's performance on longer text is not explicitly tested.
- Why unresolved: The paper does not provide any experiments or analysis on the model's performance on long-form text, which is a crucial aspect of coherence assessment in real-world applications.
- What evidence would resolve it: Additional experiments evaluating the model on long-form text datasets, such as academic papers, news articles, or novels, would provide insights into the model's generalizability and effectiveness in handling longer text.

### Open Question 2
- Question: How does the proposed coherence model compare to human judgment in assessing coherence?
- Basis in paper: The paper evaluates the model's performance on coherence scoring tasks using human-annotated benchmarks. However, it does not directly compare the model's coherence scores to human judgments.
- Why unresolved: The paper does not provide a direct comparison between the model's coherence scores and human judgments, which would help assess the model's alignment with human perception of coherence.
- What evidence would resolve it: Conducting a human evaluation study where participants rate the coherence of texts and comparing their judgments to the model's scores would provide insights into the model's agreement with human perception.

### Open Question 3
- Question: How does the proposed coherence model handle domain-specific text (e.g., legal, medical, technical)?
- Basis in paper: The paper evaluates the model on real-world text from various domains, including emails, answers, and reviews. However, it does not specifically test the model's performance on domain-specific text.
- Why unresolved: The paper does not provide any experiments or analysis on the model's performance on domain-specific text, which is crucial for assessing its applicability in specialized fields.
- What evidence would resolve it: Additional experiments evaluating the model on domain-specific text datasets, such as legal documents, medical reports, or technical manuals, would provide insights into the model's ability to handle specialized language and domain-specific coherence requirements.

## Limitations

- The empirical validation lacks ablation studies to determine which tasks are essential for coherence assessment
- No comparison against simpler baselines (e.g., sentence reordering alone) to quantify the benefit of the multi-task approach
- The five tasks may not comprehensively capture all aspects of coherence, with no external validation of this claim

## Confidence

- **High confidence** in the theoretical framework connecting Reinhart's coherence conditions to computational tasks
- **Medium confidence** in the multi-task learning benefits, as results show improvement but lack detailed ablations or comparison to simpler approaches
- **Low confidence** in the claim that these five tasks comprehensively capture all aspects of coherence, given the absence of external validation

## Next Checks

1. **Ablation study**: Train models using subsets of the five tasks to determine which combinations are essential for coherence assessment
2. **Cross-dataset generalization**: Test the trained model on additional coherence datasets beyond GCDC and CoheSentia to assess robustness
3. **Baseline comparison**: Implement and compare against simpler coherence models (e.g., sentence reordering alone) to quantify the benefit of the multi-task approach