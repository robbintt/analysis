---
ver: rpa2
title: Transformer-Based Deep Learning Model for Bored Pile Load-Deformation Prediction
  in Bangkok Subsoil
arxiv_id: '2312.03041'
source_url: https://arxiv.org/abs/2312.03041
tags:
- pile
- data
- load
- soil
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a transformer-based deep learning model to
  predict the load-deformation behavior of bored piles in Bangkok's complex subsoil.
  The model encodes soil profile and pile features as input tokens and generates load-deformation
  curves as output, incorporating previous sequential data to improve accuracy.
---

# Transformer-Based Deep Learning Model for Bored Pile Load-Deformation Prediction in Bangkok Subsoil

## Quick Facts
- arXiv ID: 2312.03041
- Source URL: https://arxiv.org/abs/2312.03041
- Reference count: 31
- Primary result: Transformer model achieves 5.72% MAE on pile load-deformation prediction

## Executive Summary
This study presents a transformer-based deep learning model for predicting load-deformation behavior of bored piles in Bangkok's complex subsoil. The model encodes soil profile and pile features as input tokens and generates load-deformation curves as output, incorporating previous sequential data to improve accuracy. Trained on 58 pile load test datasets, the model demonstrates strong generalization ability across varying pile sizes, lengths, and soil conditions, with a mean absolute error of 5.72% on test data.

## Method Summary
The researchers developed a transformer model with encoder-decoder architecture to predict pile load-deformation curves. The encoder processes soil profile tokens (SPT values, soil types, pile presence, dimensions) using multi-head self-attention, while the decoder generates sequential load predictions conditioned on pile features and previous deformation steps. The model was trained for 4000 epochs using MSE loss on 58 pile load test datasets from Bangkok subsoil.

## Key Results
- Achieved 5.72% mean absolute error on test data
- Successfully predicted load-deformation curves across varying pile sizes, lengths, and soil conditions
- Demonstrated strong generalization ability with fixed depth dimension of 60m
- Model is publicly available for geotechnical applications and parametric analysis

## Why This Works (Mechanism)

### Mechanism 1
Transformer architecture with multi-head attention captures sequential dependencies between soil layers and pile deformation data. The model encodes soil profile as sequence of tokens and uses attention mechanisms to weigh importance of each layer for predicting pile behavior at given depth.

Core assumption: Soil layers exhibit spatially correlated properties that influence pile response in sequential manner.

### Mechanism 2
Incorporating previous load-deformation sequence data into decoder improves prediction accuracy. Decoder receives shifted-right version of output sequence alongside encoder's latent representation and pile features.

Core assumption: Load-deformation behavior at given step depends on prior steps, and this temporal dependency can be learned.

### Mechanism 3
Normalization and residual connections stabilize training and prevent overfitting in deep transformer layers. Layer normalization applied after each attention and feed-forward sub-layer, with residual connections adding sub-layer input to output.

Core assumption: Deep networks without normalization/residual connections suffer from vanishing gradients or exploding activations.

## Foundational Learning

- **Soil stratification and pile behavior**: Understanding how different soil layers affect pile load-deformation is essential to interpret model outputs. Quick check: How does sand layer at depth influence pile's end-bearing capacity compared to soft clay layer?

- **Transformer attention mechanisms**: Model uses multi-head self-attention in encoder and cross-attention in decoder. Engineers must understand how attention weights prioritize certain soil layers or deformation steps. Quick check: What is role of scaling factor (1/√dk) in attention score computation?

- **Sequence-to-sequence modeling for regression**: Output is continuous load-deformation curve, requiring adaptation of sequence modeling concepts to regression. Quick check: How does "output shift right" mechanism help decoder generate sequential predictions?

## Architecture Onboarding

- **Component map**: Input encoder (Soil profile tokens) → Multi-head attention → Latent vector → Conditional decoder (Pile features + shifted-right load sequence + encoder latent) → Multi-head attention → MLP → Next load prediction

- **Critical path**: Soil profile → Encoder attention → Latent vector → Decoder cross-attention → Load prediction

- **Design tradeoffs**: Fixed depth dimension (60m) vs. variable depth profiles; Number of attention heads (10 in encoder, 3 in decoder); Dropout rate (0.1)

- **Failure signatures**: High MAPE (>10%) on test set indicates overfitting or insufficient model capacity; Load predictions plateau early suggests encoder not capturing deep-layer effects; NaN loss indicates improper scaling or too many attention loops

- **First 3 experiments**:
  1. Train with only encoder (no decoder conditioning) to assess standalone soil profile predictive power
  2. Remove previous load sequence from decoder input to test impact of temporal conditioning
  3. Vary number of attention loops (from 5 to 15) to find optimal balance between capacity and stability

## Open Questions the Paper Calls Out

- **Comparison to other architectures**: How does transformer model's performance compare to LSTM or CNN for predicting pile load-deformation curves? The paper chose transformer but didn't provide direct comparison.

- **Handling unseen variations**: How does the model handle variations in soil properties and pile conditions not present in training data? The paper mentions parametric analysis capability but doesn't discuss generalization to unseen conditions.

- **Computational cost**: How does computational cost of transformer model compare to other deep learning models for this task? The paper mentions high computational resources but lacks quantitative comparison.

## Limitations
- Relatively small training dataset (58 cases) limits generalization to significantly different soil profiles
- Lack of field validation against newly conducted pile load tests creates uncertainty about practical performance
- Limited interpretability of attention weight patterns restricts understanding of feature importance

## Confidence
- **High Confidence**: Core mechanism of using transformer attention to capture soil profile dependencies is well-established
- **Medium Confidence**: 5.72% MAE claim is supported but lacks detailed cross-validation or uncertainty quantification
- **Low Confidence**: Claims about applicability to "any" pile configuration exceed what can be verified from 58-case training set

## Next Checks
1. Cross-site validation: Test trained model on pile load test data from other geographic locations with different soil characteristics
2. Attention visualization analysis: Generate and analyze attention weight heatmaps across different soil layers and deformation steps
3. Incremental validation: Compare model predictions against series of new pile load tests conducted under controlled conditions