---
ver: rpa2
title: Explore Spurious Correlations at the Concept Level in Language Models for Text
  Classification
arxiv_id: '2311.08648'
source_url: https://arxiv.org/abs/2311.08648
tags:
- concept
- label
- dataset
- bias
- spurious
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses spurious correlations in language models at
  the concept level, focusing on text classification tasks. It introduces a method
  using ChatGPT to assign concept labels to texts, allowing measurement of concept
  bias in models during fine-tuning or in-context learning (ICL).
---

# Explore Spurious Correlations at the Concept Level in Language Models for Text Classification

## Quick Facts
- arXiv ID: 2311.08648
- Source URL: https://arxiv.org/abs/2311.08648
- Reference count: 14
- Primary result: A novel data rebalancing technique using ChatGPT-generated counterfactual data effectively mitigates concept-level spurious correlations in language models while maintaining utility performance.

## Executive Summary
This paper addresses spurious correlations in language models at the concept level, focusing on text classification tasks. It introduces a method using ChatGPT to assign concept labels to texts, allowing measurement of concept bias in models during fine-tuning or in-context learning. The paper finds that language models rely on shortcuts for predictions when encountering spurious correlations between a concept and a label in training or prompts. A novel data rebalancing technique is proposed, incorporating ChatGPT-generated counterfactual data to balance label distribution and mitigate spurious correlations. This method outperforms traditional token removal approaches. Experiments across multiple datasets demonstrate the effectiveness of the approach in reducing concept-level spurious correlations and improving model performance.

## Method Summary
The method involves annotating concepts in text datasets using ChatGPT, then creating concept-biased datasets to evaluate spurious correlations. Models are fine-tuned on both original and biased datasets, and in-context learning is evaluated using balanced and biased prompts. A novel data rebalancing technique using ChatGPT-generated counterfactual examples is proposed to mitigate spurious correlations. The approach is evaluated across multiple datasets including Yelp, IMDB, Amazon Shoe, and CeBaB, measuring concept bias using a Bias@C metric and robust accuracy.

## Key Results
- Language models show significant concept-level spurious correlations when training data contains imbalanced label distributions for concept-containing texts
- The proposed upsampling method with ChatGPT-generated counterfactual data effectively mitigates spurious correlations without sacrificing utility performance
- In-context learning models exhibit similar concept-level biases when demonstration examples contain imbalanced concept-label associations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Language models learn to rely on concept-level shortcuts when there is an imbalanced label distribution for texts containing a specific concept.
- **Mechanism**: When multiple expressions of a concept are semantically similar and consistently associated with one label in training data, the model forms a shortcut mapping between the concept and that label. During inference, the model uses this shortcut rather than learning the true underlying patterns.
- **Core assumption**: Concept expressions map to similar embeddings in the model's embedding space, creating a strong association between the concept and label.
- **Evidence anchors**:
  - [abstract]: "evidence shows that spurious correlations caused by imbalanced label distributions in training data (or exemplars in ICL) lead to robustness issues"
  - [section]: "We hypothesize that similar semantics embeddings of a concept lead to the shortcut learning"
  - [corpus]: No direct evidence found in related papers for this specific mechanism
- **Break condition**: When label distribution is balanced for each concept or when concept expressions are explicitly masked/removed from input.

### Mechanism 2
- **Claim**: In-context learning models use concept shortcuts present in demonstration examples to make predictions.
- **Mechanism**: When prompts contain demonstrations with biased label distributions for texts containing specific concepts, the model uses these demonstrations as evidence to form predictions, even if the concept is not actually predictive of the label.
- **Core assumption**: LLMs treat demonstration examples in prompts as evidence for label prediction, similar to training examples.
- **Evidence anchors**:
  - [abstract]: "LLMs' inference performance shows inductive bias when equipped with balanced prompts, but the concept-biased prompts enlarge the bias"
  - [section]: "Our findings suggest that LLMs' inference performance shows inductive bias when equipped with balanced prompts, but the concept-biased prompts enlarge the bias in most cases"
  - [corpus]: No direct evidence found in related papers for this specific mechanism
- **Break condition**: When prompts are balanced with respect to concept-label associations or when the number of demonstrations is insufficient to establish a pattern.

### Mechanism 3
- **Claim**: Data rebalancing through counterfactual generation effectively mitigates concept-level spurious correlations.
- **Mechanism**: By generating counterfactual examples that balance the label distribution for each concept, the model learns that the concept itself is not predictive of the label, breaking the shortcut association.
- **Core assumption**: Adding balanced examples for each concept during training allows the model to learn the true relationship between input features and labels.
- **Evidence anchors**:
  - [abstract]: "we propose a data rebalancing method to mitigate the spurious correlations by adding the LLM generated counterfactual data to make a balanced label distribution for each concept"
  - [section]: "Our proposed up-sampling method with LLM-generated counterfactual examples has a significant effect on mitigation without sacrificing the utility performance"
  - [corpus]: No direct evidence found in related papers for this specific mechanism
- **Break condition**: When generated counterfactuals fail to maintain the original sentiment/label while incorporating the concept, or when the number of generated examples is insufficient.

## Foundational Learning

- **Concept: Spurious correlations**
  - Why needed here: Understanding how models learn incorrect associations between features and labels is central to this work
  - Quick check question: What distinguishes a spurious correlation from a genuine predictive relationship?

- **Concept: Concept-level features vs token-level features**
  - Why needed here: The paper focuses on abstract concepts rather than specific words or phrases, which requires understanding the difference
  - Quick check question: How might a concept like "food" be expressed differently across texts, and why does this matter for spurious correlations?

- **Concept: In-context learning**
  - Why needed here: The paper evaluates both fine-tuning and ICL approaches, requiring understanding of how LLMs use demonstrations in prompts
  - Quick check question: How does the label distribution in demonstration examples affect the model's predictions in ICL?

## Architecture Onboarding

- **Component map**: Dataset loading -> Concept annotation (LLM) -> Dataset splitting -> Model training (Fine-tuning/ ICL) -> Bias measurement (Bias@C) -> Mitigation (Data rebalancing)

- **Critical path**:
  1. Annotate concepts in dataset using LLM
  2. Construct biased training datasets for concept-level evaluation
  3. Train models on original and biased datasets
  4. Measure concept bias using Bias@C metric
  5. Apply mitigation methods and evaluate effectiveness

- **Design tradeoffs**:
  - Using LLM for concept annotation trades human annotation cost for potential LLM annotation errors
  - Upsampling with counterfactual generation requires API calls but maintains more original data than downsampling
  - Token removal is simple but may not capture all concept expressions

- **Failure signatures**:
  - High Bias@C values indicate strong reliance on concept shortcuts
  - Performance drop on concept-containing texts vs non-concept texts
  - Mitigation methods fail to reduce Bias@C below threshold

- **First 3 experiments**:
  1. Fine-tune DistilBERT on original dataset vs concept-biased dataset for a specific concept and measure Bias@C
  2. Create balanced and biased prompts for ICL and compare model performance on concept-containing vs non-concept texts
  3. Apply upsampling mitigation method and verify reduction in Bias@C while maintaining utility performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important areas remain unexplored:
- How does the proposed upsampling method's effectiveness vary across different types of concepts?
- What is the impact of the proposed upsampling method on models' generalization to out-of-distribution (OOD) data?
- How does the choice of LLM for generating counterfactual data influence the quality and effectiveness of the upsampling method?

## Limitations

- The effectiveness of ChatGPT for concept annotation across diverse domains is not empirically validated
- The scalability of the proposed method to concepts with large vocabularies or multi-word expressions is unclear
- The counterfactual generation process using ChatGPT may introduce artifacts or inconsistencies in the generated texts

## Confidence

- **High Confidence**: The core finding that language models exhibit concept-level spurious correlations when training data contains imbalanced label distributions for concept-containing texts
- **Medium Confidence**: The proposed data rebalancing method using ChatGPT-generated counterfactuals is more effective than traditional token removal
- **Low Confidence**: The claim that similar semantic embeddings cause shortcut learning at the concept level

## Next Checks

- Evaluate annotation quality by comparing ChatGPT concept labels against human annotations on a held-out test set and calculate inter-annotator agreement scores
- Test the counterfactual generation quality by conducting human evaluation of generated examples, assessing whether they maintain the original sentiment/label while incorporating the target concept naturally
- Scale the method to multi-word concepts and concepts with extensive vocabularies to measure whether the approach maintains effectiveness or requires modifications for complex concepts