---
ver: rpa2
title: Counterfactual Prediction Under Selective Confounding
arxiv_id: '2310.14064'
source_url: https://arxiv.org/abs/2310.14064
tags:
- samples
- confounding
- treatment
- data
- confounders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of causal inference in settings\
  \ where some confounding variables are unobserved under the treatment of interest\
  \ (Selective Confounding). The authors propose leveraging dual-treatment samples\u2014\
  cases where the initial decision is later reversed\u2014to identify and adjust for\
  \ hidden confounding."
---

# Counterfactual Prediction Under Selective Confounding

## Quick Facts
- **arXiv ID**: 2310.14064
- **Source URL**: https://arxiv.org/abs/2310.14064
- **Reference count**: 40
- **Key outcome**: Novel method for causal inference when some confounders are unobserved under the treatment of interest, using dual-treatment samples and two-stage regression methods

## Executive Summary
This paper addresses the challenge of causal inference when some confounding variables are unobserved under the desired treatment (Selective Confounding). The authors propose leveraging dual-treatment samples—cases where an initial decision is later reversed—to identify and adjust for hidden confounding. By exploiting variations in decision policies across locations, they estimate potential outcomes for these samples and apply two-stage regression methods (Regression Adjustment and Doubly-Robust) to learn interpretable counterfactual predictors. The approach is validated on synthetic and real-world child placement data, showing improved performance over standard predictors.

## Method Summary
The method works by first identifying dual-treatment samples where the same unit experiences different treatments over time. These samples are assumed to be unconfounded between the original and desired treatments if reversals are due to judgment errors rather than covariate shifts. The algorithm estimates potential outcomes for these samples, then applies two-stage regression: first learning outcome regression and propensity score models on the dual-treatment samples, then using these as pseudo-outcomes to train the counterfactual predictor on the full population. Sample splitting is used to prevent overfitting and provide theoretical error bounds.

## Key Results
- The proposed methods (RA and DR) reduce failure rates compared to standard predictors on child placement data
- Theoretical error bounds are provided for both Regression Adjustment and Doubly-Robust methods under sample splitting
- Performance improves as the proportion of dual-treatment samples increases in synthetic experiments
- The approach offers better risk assessment in sensitive decision-making domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-treatment samples provide unbiased potential outcomes under selective confounding.
- Mechanism: When a decision is reversed, the reversal is attributed to initial judgment error rather than covariate shift, making the dual-treatment sample effectively unconfounded between the original treatment and the desired treatment.
- Core assumption: Decision reversals are due to judgment errors, not changes in the underlying covariates over time.
- Evidence anchors:
  - [abstract] "We relax the requirement of knowing all confounders under desired treatment, which we refer to as Selective Confounding, to enable causal inference in diverse real-world scenarios."
  - [section] "The intuition behind our solution is that dual-treatment samples are unconfounded and it is sometimes feasible to estimate potential outcome Ŷa1 for subpopulation."
- Break condition: If decision reversals are primarily due to covariate shifts rather than judgment errors, this mechanism fails.

### Mechanism 2
- Claim: Two-stage regression methods (RA and DR) can learn counterfactual predictors using pseudo-outcomes derived from dual-treatment samples.
- Mechanism: First stage learns an outcome regression on dual-treatment samples (which are unconfounded), then second stage uses these estimates as pseudo-outcomes to train the target model on the full population.
- Core assumption: The outcome regression learned on dual-treatment samples can generalize to the full population.
- Evidence anchors:
  - [section] "These two nuisance estimators with no confounding bias can then be used to train the target estimator ˆν(x) using two-step methods like Regression Adjustment (RA) and Doubly Robust (DR)."
- Break condition: If the dual-treatment samples are not representative of the full population, the pseudo-outcomes will not generalize properly.

### Mechanism 3
- Claim: Sample splitting prevents overfitting and provides theoretical error bounds for the two-stage estimation procedure.
- Mechanism: By splitting data into separate partitions for first-stage and second-stage estimation, we avoid overfitting the nuisance estimators to the target prediction task.
- Core assumption: The sample splitting strategy is properly implemented and maintains sufficient data for both stages.
- Evidence anchors:
  - [section] "Theorem 1 Under sample-splitting to learn ˆµa1(x, z), ˆν(x), the RA method has pointwise regression error that is bounded by..."
- Break condition: If sample splitting reduces either stage to insufficient data, the error bounds may not hold and overfitting may occur.

## Foundational Learning

- Concept: Potential outcomes framework
  - Why needed here: The entire causal inference approach relies on understanding what would have happened under different treatment assignments.
  - Quick check question: What is the difference between factual and counterfactual outcomes in the potential outcomes framework?

- Concept: Confounding bias and ignorability
  - Why needed here: The paper explicitly deals with confounding bias and relaxing the ignorability assumption when not all confounders are observed.
  - Quick check question: Why does confounding bias occur and how does it affect standard supervised learning approaches?

- Concept: Propensity score estimation
  - Why needed here: Doubly Robust method requires estimating propensity scores for treatment assignment, which is crucial for handling the dual-treatment samples.
  - Quick check question: What is the difference between marginal and conditional propensity scores and when would you use each?

## Architecture Onboarding

- Component map: Data preprocessing → First stage training (ˆµa1, ˆπa3|̸=a1) → Second stage training (ˆν) → Prediction

- Critical path: Data preprocessing → First stage training → Second stage training → Prediction

- Design tradeoffs:
  - Using Linear Regression for interpretability vs. potentially better performance with non-linear models
  - Sample splitting for theoretical guarantees vs. reduced data for each stage
  - Requiring dual-treatment samples vs. applicability to more general settings

- Failure signatures:
  - High variance in predictions when dual-treatment samples are scarce
  - Systematic bias if decision reversals are not primarily due to judgment errors
  - Poor generalization if dual-treatment samples are not representative

- First 3 experiments:
  1. Vary the proportion of dual-treatment samples in synthetic data and measure impact on prediction accuracy
  2. Test performance when CATE(a1, a3) is non-zero vs. negligible to validate Algorithm 1's effectiveness
  3. Compare RA vs DR performance on real-world data to identify which method is more robust to estimation errors in nuisance components

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical error bounds for counterfactual prediction when the conditional average treatment effect (CATE) between treatments a1 and a3 is non-zero?
- Basis in paper: [explicit] The paper mentions that the proposed solution works irrespective of the CATE value and provides theoretical error bounds for both Regression Adjustment (RA) and Doubly-Robust (DR) methods. However, it does not explicitly derive or state the error bounds for non-zero CATE cases.
- Why unresolved: The paper focuses on the case where CATE(a1, a3) is negligible or zero, and the theoretical error bounds are derived under this assumption. The error bounds for non-zero CATE cases are not explicitly stated or derived.
- What evidence would resolve it: Deriving and stating the theoretical error bounds for counterfactual prediction when CATE(a1, a3) is non-zero, possibly by extending the existing error bounds for RA and DR methods.

### Open Question 2
- Question: How does the proposed approach perform in real-world applications beyond child placement, ICU admission, and police arrests, where multiple decision-makers with different policies are involved and a re-evaluation mechanism exists?
- Basis in paper: [inferred] The paper discusses the applicability of the proposed approach in real-world scenarios involving child placement, ICU admission, and police arrests. However, it does not provide empirical evidence or results for other applications that satisfy the two key assumptions.
- Why unresolved: The paper focuses on child placement as the primary real-world example, and the results are not generalized to other applications. The performance of the proposed approach in other real-world applications is not evaluated or discussed.
- What evidence would resolve it: Conducting empirical evaluations of the proposed approach in other real-world applications that satisfy the two key assumptions, such as criminal justice, healthcare, or education, and comparing the results to the child placement example.

### Open Question 3
- Question: How does the proposed approach handle hidden confounders that are not only absent under the desired treatment but also vary across different locations or decision-makers?
- Basis in paper: [explicit] The paper assumes that hidden confounders are not available under the desired treatment (Selective Confounding) and proposes using dual-treatment samples to estimate potential outcomes. However, it does not explicitly discuss the case where hidden confounders vary across different locations or decision-makers.
- Why unresolved: The paper focuses on the case where hidden confounders are absent under the desired treatment, but it does not address the scenario where hidden confounders vary across different locations or decision-makers. This could potentially affect the performance of the proposed approach.
- What evidence would resolve it: Investigating the impact of varying hidden confounders across different locations or decision-makers on the performance of the proposed approach and proposing modifications or extensions to handle this scenario.

## Limitations

- The core assumption that decision reversals are due to judgment errors rather than covariate shifts lacks direct empirical validation
- All synthetic experiments use controlled data generation, providing an unrealistic best-case scenario
- Theoretical error bounds depend on assumptions about nuisance estimator convergence that aren't empirically verified
- Limited empirical validation beyond one real-world dataset (child placement)

## Confidence

- **High confidence**: The two-stage regression methodology (Regression Adjustment and Doubly-Robust) is well-established in the causal inference literature and correctly applied given the assumptions.
- **Medium confidence**: The selective confounding framework is novel and addresses a real limitation in causal inference, but the empirical validation is limited to one real-world dataset.
- **Low confidence**: The core claim that dual-treatment samples can reliably identify counterfactuals under selective confounding needs more rigorous validation, particularly the assumption about the source of decision reversals.

## Next Checks

1. **Sensitivity analysis on decision reversal mechanism**: Conduct experiments varying the proportion of decision reversals due to judgment errors vs. covariate shifts to determine at what point the method breaks down.

2. **Cross-dataset validation**: Test the method on multiple real-world datasets with known causal structures to assess generalizability beyond the child placement data.

3. **Nuisance estimator performance evaluation**: Systematically evaluate how errors in the first-stage nuisance estimators (outcome regression and propensity score) propagate to the final counterfactual predictions, particularly in high-dimensional settings.