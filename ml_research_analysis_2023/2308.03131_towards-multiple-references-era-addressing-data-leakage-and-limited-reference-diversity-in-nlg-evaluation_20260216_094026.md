---
ver: rpa2
title: Towards Multiple References Era -- Addressing Data Leakage and Limited Reference
  Diversity in NLG Evaluation
arxiv_id: '2308.03131'
source_url: https://arxiv.org/abs/2308.03131
tags:
- metrics
- references
- reference
- evaluation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of evaluating natural language
  generation (NLG) tasks, particularly the limitations of n-gram matching-based metrics
  like BLEU when assessing outputs from large language models (LLMs). The core idea
  is to use LLMs to generate multiple diverse reference candidates, which are then
  filtered using a diversity-aware selection strategy.
---

# Towards Multiple References Era -- Addressing Data Leakage and Limited Reference Diversity in NLG Evaluation

## Quick Facts
- arXiv ID: 2308.03131
- Source URL: https://arxiv.org/abs/2308.03131
- Authors: 
- Reference count: 14
- Primary result: Multi-reference generation framework improves n-gram metric accuracy by up to 7.2% on WMT22 Metrics Task and mitigates data leakage issues in LLM evaluation

## Executive Summary
This paper addresses the limitations of n-gram matching-based metrics like BLEU when evaluating outputs from large language models (LLMs). The core insight is that the performance bottleneck stems from limited reference diversity rather than the metrics themselves. The authors propose a framework that generates multiple diverse reference candidates using LLMs and applies diversity-aware selection to filter them. This approach significantly improves the consistency between automatic evaluation metrics and human judgments, achieving up to 7.2% accuracy improvement on the WMT22 Metrics Task. The method also demonstrates effectiveness in mitigating data leakage issues that plague neural-based metrics.

## Method Summary
The LLM-Ref framework generates multiple diverse reference candidates for each input using LLMs (specifically gpt-3.5-turbo), then applies a diversity-aware selection strategy based on Self-BLEU scores to filter the references. The process involves creating prompts with characterization rules, input descriptions, and optional ground truth, then generating 40 references per sentence for WMT22 datasets. References are filtered using a Self-BLEU threshold of 35 to ensure diversity. The framework evaluates both n-gram-based metrics (BLEU, chrF) and neural-based metrics (BERTScore, BLEURT, COMET) using the selected multi-references, comparing performance against single-reference baselines across multiple language pairs and tasks.

## Key Results
- Multi-reference approach improves n-gram metric accuracy by up to 7.2% on WMT22 Metrics Task compared to single-reference methods
- Outperforms neural-based metrics like BERTScore by 3.9% in correlation with human judgments
- Effectively mitigates data leakage issues in LLMs, where BLOOMz's advantage diminishes when using multiple references
- Achieves strong correlations with human judgments (ρ: 0.492-0.534, τ: 0.381-0.410) on WMT22 evaluation

## Why This Works (Mechanism)

### Mechanism 1
Multiple diverse reference candidates improve the alignment of n-gram matching metrics with human judgments by increasing the coverage of valid linguistic expressions for a given meaning. By generating multiple references via LLMs and selecting those with high diversity, the coverage of valid linguistic expressions for a given meaning increases, reducing the penalty for acceptable but non-matching n-grams.

### Mechanism 2
Multi-reference evaluation mitigates the data leakage problem in LLMs when using n-gram metrics by relaxing the strict one-to-one matching requirement. When multiple references are available, the strict one-to-one matching requirement is relaxed, reducing the impact of overfitted or leaked training data in the model output.

### Mechanism 3
Neural-based metrics gain limited benefit from multiple references due to their semantic matching nature. Neural metrics compute similarity in semantic space and pick the best score among references; they are less sensitive to lexical diversity and more to semantic overlap.

## Foundational Learning

- Concept: BLEU and chrF are n-gram precision-based metrics sensitive to reference diversity
  - Why needed here: Understanding why these metrics fail with diverse LLM outputs is core to the paper's motivation
  - Quick check question: What happens to BLEU score if the reference only contains one valid paraphrase of the output?

- Concept: Data leakage in LLMs refers to training on test data, causing overfitting to specific references
  - Why needed here: Explains why BLOOMz outperformed baselines on known test sets
  - Quick check question: How would you detect data leakage in an LLM evaluation setting?

- Concept: Self-BLEU as a diversity metric measures similarity between generated references
  - Why needed here: Used to filter out redundant reference candidates
  - Quick check question: What threshold for Self-BLEU is used to ensure diversity in this paper?

## Architecture Onboarding

- Component map: Prompt Generator -> LLM Reference Generator -> Diversity Filter -> Metric Evaluator
- Critical path: Prompt → LLM Generation → Diversity Selection → Metric Computation → Human Correlation Analysis
- Design tradeoffs:
  - More references increase diversity but risk redundancy; Self-BLEU threshold balances this
  - Ground truth inclusion boosts quality but may not always be available
  - Diversity selection adds latency but improves metric consistency
- Failure signatures:
  - Low diversity among references → poor metric improvement
  - Over-filtering → too few references to cover output variability
  - Prompt misconfiguration → irrelevant or poor-quality references
- First 3 experiments:
  1. Generate 2 references with and without ground truth; compare Self-BLEU scores
  2. Vary Self-BLEU threshold (25, 35, 45) and observe impact on metric consistency
  3. Compare single-reference vs. multi-reference n-gram metrics on a small test set

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of reference candidates to generate for maximizing evaluation performance while minimizing computational cost?
- Basis in paper: [inferred] The paper discusses how increasing the number of references improves performance but doesn't determine the optimal stopping point
- Why unresolved: The paper shows improvements with increasing reference numbers but doesn't determine the optimal stopping point where additional references no longer provide meaningful improvements
- What evidence would resolve it: Systematic experiments varying reference counts while measuring performance gains, computational costs, and diversity metrics

### Open Question 2
How does the proposed multi-reference approach perform on languages with limited parallel corpora compared to languages with abundant training data?
- Basis in paper: [inferred] The experiments focus on high-resource language pairs but mention evaluating on multilingual test sets
- Why unresolved: Current experiments don't systematically compare performance across languages with varying resource availability
- What evidence would resolve it: Evaluating the framework on low-resource language pairs and comparing results with high-resource pairs

### Open Question 3
What is the precise mechanism by which n-gram-based metrics with multiple references mitigate data leakage while neural-based metrics cannot?
- Basis in paper: [explicit] The paper states that n-gram-based metrics with multiple references can effectively mitigate data leakage issues while neural-based metrics struggle to overcome this problem
- Why unresolved: The paper provides empirical evidence of this difference but doesn't fully explain the underlying technical reasons
- What evidence would resolve it: Detailed analysis of how matching-based and semantic-based scoring methods respond differently to overlapping content

### Open Question 4
How does the diversity-aware selection threshold (Self-BLEU < 35) affect evaluation performance across different tasks and language pairs?
- Basis in paper: [explicit] The paper mentions using Self-BLEU < 35 as an empirical threshold but doesn't explore how varying this threshold affects results
- Why unresolved: The choice of 35 as a threshold appears arbitrary and may not be optimal across different contexts
- What evidence would resolve it: Experiments systematically varying the Self-BLEU threshold and measuring impact on evaluation consistency

### Open Question 5
Can the proposed framework be extended to evaluate other generation tasks beyond translation and summarization, such as dialogue systems or code generation?
- Basis in paper: [inferred] The paper focuses on translation and summarization but mentions NLG evaluation broadly
- Why unresolved: Current experiments are limited to specific NLG tasks
- What evidence would resolve it: Applying the framework to diverse NLG tasks like dialogue response generation or code completion

## Limitations

- The data leakage mitigation claims rest primarily on indirect evidence without explicit leakage detection or controlled experiments
- The diversity selection mechanism using Self-BLEU thresholds lacks detailed justification and sensitivity analysis
- The framework's performance on low-resource languages and other NLG tasks beyond translation and summarization remains unexplored

## Confidence

- High confidence: The core observation that multiple diverse references improve n-gram metric performance (WMT22 accuracy gains of up to 7.2%)
- Medium confidence: The data leakage mitigation hypothesis, consistent with observed performance changes but lacking direct validation
- Medium confidence: The claim that neural metrics benefit minimally from multiple references, would benefit from exploring alternative aggregation strategies

## Next Checks

1. Implement explicit data leakage detection to verify whether performance improvements correlate with reduced leakage rather than just increased diversity

2. Systematically vary Self-BLEU thresholds (20, 30, 35, 40, 50) and measure impact on both diversity metrics and evaluation consistency across different language pairs

3. Test whether neural metrics that aggregate scores across all references (mean, weighted average) rather than selecting max show improved sensitivity to reference diversity