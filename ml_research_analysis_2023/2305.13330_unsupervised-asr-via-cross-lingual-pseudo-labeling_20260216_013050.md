---
ver: rpa2
title: Unsupervised ASR via Cross-Lingual Pseudo-Labeling
arxiv_id: '2305.13330'
source_url: https://arxiv.org/abs/2305.13330
tags:
- language
- target
- data
- source
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method for training unsupervised automatic
  speech recognition (ASR) systems in target languages without labeled audio data
  by leveraging labeled audio data from other source languages. The core idea is to
  generate pseudo-labels for the target language using a source language acoustic
  model constrained by a target language model, and then iteratively refine these
  pseudo-labels to train an unsupervised target language acoustic model.
---

# Unsupervised ASR via Cross-Lingual Pseudo-Labeling

## Quick Facts
- arXiv ID: 2305.13330
- Source URL: https://arxiv.org/abs/2305.13330
- Reference count: 40
- The method achieves 15% absolute improvement in word error rate on LJSpeech dataset using 800 hours of labeled German data instead of 60,000 hours of unlabeled English data.

## Executive Summary
This paper introduces cross-lingual pseudo-labeling for unsupervised automatic speech recognition (ASR), enabling training of ASR models for target languages without labeled audio by leveraging labeled data from source languages. The approach generates pseudo-labels for target language audio using a source language acoustic model constrained by a target language model, then iteratively refines these pseudo-labels to train an unsupervised target language acoustic model. The method significantly outperforms existing unsupervised ASR approaches like wav2vec-U 2.0, achieving state-of-the-art results while requiring substantially less unlabeled audio. The approach demonstrates promising results for cross-family language transfer, achieving 18% word error rate for English to Swahili.

## Method Summary
The method trains unsupervised ASR models for target languages by leveraging labeled audio data from source languages through cross-lingual pseudo-labeling. First, a source language acoustic model (AM) is trained on labeled audio-text pairs using CTC loss. This source AM generates initial pseudo-labels for target language audio through beam search decoding constrained by a target language model. These pseudo-labels are then used to train a target AM, which iteratively generates improved pseudo-labels in subsequent rounds. The process alternates between pseudo-label generation and model training, progressively improving the target AM's performance. The approach uses character-level models to avoid phonetic knowledge requirements and enables truly unsupervised cross-lingual transfer.

## Key Results
- Achieves 15% absolute WER improvement on LJSpeech dataset using 800 hours of labeled German data versus 60,000 hours of unlabeled English data
- Demonstrates cross-family transfer with 18% WER for English to Swahili
- Outperforms wav2vec-U 2.0 across multiple target languages with significantly less unlabeled audio data
- Shows character-level models enable effective cross-lingual transfer without phonetic transcriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual transfer works because source language acoustic models can generate pseudo-labels in the target language that are constrained by a target language model to be linguistically plausible.
- Mechanism: The source AM generates initial pseudo-labels for target audio, which are often acoustically plausible but linguistically incorrect. A target LM rescored these pseudo-labels, selecting sequences that are both acoustically consistent and linguistically coherent in the target language.
- Core assumption: The source and target languages share enough acoustic-phonetic similarities that the source AM can produce reasonable acoustic hypotheses for the target language, even without training on target audio.
- Break condition: If the source and target languages are too phonologically distinct (e.g., completely different phoneme inventories), the source AM cannot generate useful acoustic hypotheses, making cross-lingual pseudo-labeling ineffective.

### Mechanism 2
- Claim: Iterative refinement of pseudo-labels progressively improves the target acoustic model through self-training.
- Mechanism: Initial pseudo-labels from the source AM are used to train a target AM, which then generates new pseudo-labels for the next iteration. Each iteration produces better pseudo-labels as the target AM becomes more specialized to the target language's acoustic patterns.
- Core assumption: The pseudo-labeling process is stable enough that errors don't compound catastrophically across iterations.
- Break condition: If the pseudo-labeling feedback loop amplifies errors rather than correcting them, performance will degrade with each iteration.

### Mechanism 3
- Claim: Using character-level models instead of phoneme-based models enables truly unsupervised cross-lingual transfer without requiring phonetic transcriptions or lexicons.
- Mechanism: Character-level AMs learn to map acoustic features directly to graphemes, bypassing the need for phonetic knowledge. This makes them more flexible for cross-lingual transfer since they don't rely on language-specific phonetic mappings.
- Break condition: If the character-level representation space is too coarse to capture language-specific phonetic distinctions, transfer performance will suffer.

## Foundational Learning

- Concept: Pseudo-labeling in semi-supervised learning
  - Why needed here: The entire approach relies on generating pseudo-labels for unlabeled target language audio and using them to train a model as if they were true labels
  - Quick check question: What is the primary risk when using pseudo-labels generated by a model to train itself, and how does this work address that risk?

- Concept: Cross-lingual transfer learning
  - Why needed here: The method transfers knowledge from a source language with labeled data to a target language with only unlabeled audio, requiring understanding of how acoustic representations transfer across languages
  - Quick check question: What linguistic property determines how well acoustic models transfer between languages?

- Concept: Language model integration with CTC decoding
  - Why needed here: The target language model rescored pseudo-labels to select linguistically plausible sequences, improving their quality for training
  - Quick check question: How does integrating an LM with CTC decoding change the objective function compared to greedy decoding?

## Architecture Onboarding

- Component map: Source AM → Pseudo-label generator (with target LM) → Target AM training → Improved pseudo-labels → Better target AM

- Critical path: Source AM → Pseudo-label generation (with target LM) → Target AM training → Improved pseudo-labels → Better target AM

- Design tradeoffs:
  - Character-level vs phoneme-level: Character-level avoids phonetic knowledge requirements but may be less precise for some languages
  - LM beam size: Larger beams produce better pseudo-labels but increase computation time
  - Number of iterations: More iterations can improve performance but risk error accumulation
  - Source language selection: Related languages transfer better but may limit applicability to low-resource languages

- Failure signatures:
  - Pseudo-labels consistently contain non-words or grammatically impossible sequences
  - WER improvement plateaus or degrades after a certain number of iterations
  - Target AM overfits to source language characteristics despite LM constraints
  - Training instability with exploding gradients or NaN values

- First 3 experiments:
  1. Zero-shot evaluation: Test source AM on target language audio with and without target LM to establish baseline transfer capability
  2. Single iteration pseudo-labeling: Generate pseudo-labels once and train target AM, compare WER to zero-shot baseline
  3. Multi-iteration training: Run 2-3 iterations of cross-lingual pseudo-labeling, track pseudo-label quality and WER improvement at each step

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of cross-lingual pseudo-labeling scale when using multilingual source acoustic models instead of single-language source models?
- Basis in paper: The authors state "we have purposely left using a multilingual source AM (as it is done in ASR2K) for future work" and note that ASR2K uses multilingual phone-based supervised ASR models.
- Why unresolved: The paper only experiments with single-source language transfer, leaving the potential benefits of multilingual source models unexplored.

### Open Question 2
- Question: What is the impact of using character-based language models instead of word-based language models for constraining pseudo-label generation in cross-lingual pseudo-labeling?
- Basis in paper: The authors mention that character-based approaches were previously considered less effective than phoneme-based ones, and note that French's irregular orthography poses challenges that might be mitigated by character-based approaches.
- Why unresolved: The paper primarily uses word-based language models and does not explore character-based alternatives despite acknowledging their potential benefits for languages with irregular orthographies.

### Open Question 3
- Question: What are the limitations of cross-lingual pseudo-labeling when transferring between languages from completely different language families (e.g., Indo-European to Sino-Tibetan)?
- Basis in paper: The authors demonstrate promising results for cross-family transfer (English to Swahili) but note that performance varies significantly and mention that "any Indo-European language transfers well to Swahili and somewhat well to Hausa."
- Why unresolved: While the paper shows some cross-family transfer is possible, it doesn't systematically explore the boundaries of language family dissimilarity that can still yield effective transfer.

## Limitations
- Assumes sufficient acoustic-phonetic similarity between source and target languages, which may not hold for distant language pairs
- 18% WER for English-to-Swahili, while impressive for unsupervised methods, still represents a significant error rate
- Limited analysis of error propagation across iterative pseudo-labeling iterations
- Computational cost of generating pseudo-labels with large beam sizes and training multiple iterations may limit practical applicability

## Confidence
- High confidence: Core methodology and experimental results on related languages are robust and reproducible
- Medium confidence: Cross-family transfer results are promising but based on limited language pairs
- Low confidence: Stability analysis of iterative pseudo-labeling and characterization of error accumulation is insufficient

## Next Checks
1. Conduct systematic study of pseudo-label quality degradation across 10+ iterations on multiple language pairs to identify optimal stopping points
2. Test method on additional distant language pairs (e.g., English to Mandarin, Spanish to Finnish) with varying phonological distance
3. Perform controlled ablation study comparing character-level versus phoneme-level models across same language pairs to quantify trade-offs