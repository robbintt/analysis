---
ver: rpa2
title: 'HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering
  over Heterogeneous Knowledge'
arxiv_id: '2308.11257'
source_url: https://arxiv.org/abs/2308.11257
tags:
- program
- hoppg
- question
- knowledge
- multi-hop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes HopPG, a self-iterative program generation framework
  for multi-hop question answering over heterogeneous knowledge. The framework addresses
  the limitations of existing semantic parsing-based models that generate a complete
  program before executing it, which struggles with multi-hop question answering over
  heterogeneous knowledge.
---

# HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge

## Quick Facts
- arXiv ID: 2308.11257
- Source URL: https://arxiv.org/abs/2308.11257
- Authors: Multiple authors
- Reference count: 8
- Key outcome: HopPG achieves 1.37 and 1.22 point improvements in EM and F1 scores respectively over baseline models on multi-hop questions in MMQA-T2 dataset.

## Executive Summary
This paper introduces HopPG, a self-iterative program generation framework designed to address the limitations of existing semantic parsing-based models for multi-hop question answering over heterogeneous knowledge. Traditional approaches generate complete programs before execution, which struggles with the complexity of multi-hop reasoning over tables and text. HopPG overcomes this by generating programs iteratively, using previous hop execution results to guide supporting fact retrieval and subsequent program generation. The framework is evaluated on MMQA-T2, a filtered subset of MMQA containing only table and text-based multi-hop questions, demonstrating significant improvements over baselines like Implicit-Decomp and UniRPG.

## Method Summary
HopPG implements a three-module architecture: a fact retriever that selects supporting facts based on questions and previous execution results, a program generator that creates executable programs hop-by-hop, and a program executor that derives answers. The model is trained using weak supervision through pseudo-programs constructed from defined templates based on annotated supporting facts and intermediate results. The framework processes questions iteratively, with each hop's program generation conditioned on the previous hop's execution result, allowing the model to focus on one reasoning step at a time rather than handling all supporting facts simultaneously.

## Key Results
- HopPG achieves 1.37 point improvement in Exact Match (EM) score over baseline models on single-hop questions
- HopPG achieves 1.22 point improvement in F1 score over baseline models on multi-hop questions
- The framework shows particular strength on multi-hop questions where traditional semantic parsing approaches struggle

## Why This Works (Mechanism)

### Mechanism 1
Iterative generation using previous-hop execution results improves multi-hop program accuracy by reducing input complexity and focusing on one reasoning step at a time. The core assumption is that previous hop execution results contain sufficient information to guide supporting fact selection and next program generation. Break condition: Noisy or irrelevant previous hop results could mislead subsequent retrieval and generation.

### Mechanism 2
Reducing complexity of supporting facts improves program generation accuracy by selecting only one supporting fact per hop instead of concatenating all candidates. The core assumption is that a single relevant supporting fact suffices for generating correct programs for each hop. Break condition: If the single selected fact is not the most relevant one, the generated program may be incorrect.

### Mechanism 3
Weak supervision using pseudo-programs constructed from templates enables training without manual annotations. The core assumption is that constructed pseudo-programs accurately represent the reasoning process required to answer questions. Break condition: If pseudo-programs don't match actual reasoning processes, the model may learn incorrect patterns.

## Foundational Learning

- **Semantic parsing and program execution**: Why needed - HopPG generates executable programs and executes them to derive answers. Quick check - What is the difference between semantic parsing and text-based question answering?

- **Multi-hop reasoning and knowledge retrieval**: Why needed - HopPG performs multi-hop reasoning over heterogeneous knowledge. Quick check - How does multi-hop reasoning differ from single-hop reasoning in question answering?

- **Weak supervision and pseudo-labeling**: Why needed - HopPG uses pseudo-programs constructed from templates for training. Quick check - What are the advantages and disadvantages of using weak supervision compared to manual annotations?

## Architecture Onboarding

- **Component map**: Question → Fact Retriever → Program Generator → Program Executor → Iteration Detector → Answer
- **Critical path**: Question flows through fact retriever to program generator to program executor, with iteration detector determining whether to continue or output final answer
- **Design tradeoffs**: Trades complexity of generating complete program in one step for accuracy of iterative generation; trades cost of manual program annotations for scalability of weak supervision using pseudo-programs
- **Failure signatures**: Fact retriever selecting irrelevant facts leads to incorrect programs; program generator creating wrong programs leads to wrong answers; iteration detector failing to detect reasoning end leads to unnecessary programs or missed final answers
- **First 3 experiments**: 1) Evaluate fact retrieval accuracy on held-out MMQA-T2 set; 2) Compare program generation accuracy with UniRPG on single-hop questions; 3) Analyze impact of pseudo-programs by comparing HopPG with and without weak supervision

## Open Questions the Paper Calls Out

### Open Question 1
How does performance scale with number of hops beyond two hops? Basis: Paper mentions maximum of two hops and acknowledges limited performance on three-hop questions. Why unresolved: No experimental results for questions requiring more than two hops. What evidence would resolve: Experimental results comparing performance on three or more hop questions against baseline models.

### Open Question 2
Impact of incorporating more diverse knowledge sources (images, videos) on performance? Basis: Paper focuses on tabular and textual knowledge, excludes image-requiring questions. Why unresolved: Doesn't explore integration of additional knowledge sources. What evidence would resolve: Experiments evaluating performance on multi-hop questions involving diverse knowledge sources.

### Open Question 3
How does HopPG handle errors in program generation and what strategies mitigate error propagation across hops? Basis: Paper mentions errors in previous hop programs don't directly lead to incorrect final results but lacks detailed error handling mechanisms. Why unresolved: No discussion of specific error handling strategies or their effectiveness. What evidence would resolve: Analysis of error handling mechanisms including strategies to detect and correct program generation errors.

## Limitations

- Limited evaluation on multi-hop questions beyond two hops, leaving scalability concerns unaddressed
- Reliance on single supporting fact per hop may be insufficient for complex reasoning steps requiring multiple evidence pieces
- Use of pseudo-programs introduces potential mismatch between learned programs and actual reasoning processes without thorough validation

## Confidence

- **High Confidence**: Iterative program generation mechanism is well-supported by architecture and experimental results showing improved multi-hop performance
- **Medium Confidence**: Single-fact selection approach is plausible but lacks thorough analysis of multi-fact requirement cases
- **Low Confidence**: Weak supervision effectiveness using pseudo-programs is least validated without ablation studies comparing performance with and without this approach

## Next Checks

1. Conduct ablation study removing pseudo-program training component to quantify actual contribution of weak supervision to performance gains

2. Perform detailed error analysis on fact retriever's selections, particularly for multi-hop questions, to identify insufficient single-fact retrieval cases and quantify impact on final answer accuracy

3. Evaluate HopPG on different multi-hop QA dataset (e.g., HotpotQA or 2WikiMultiHopQA) to assess generalization beyond MMQA-T2 domain and question distribution