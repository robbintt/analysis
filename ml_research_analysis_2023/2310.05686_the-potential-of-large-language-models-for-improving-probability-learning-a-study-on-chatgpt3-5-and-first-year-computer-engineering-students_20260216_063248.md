---
ver: rpa2
title: 'The potential of large language models for improving probability learning:
  A study on ChatGPT3.5 and first-year computer engineering students'
arxiv_id: '2310.05686'
source_url: https://arxiv.org/abs/2310.05686
tags:
- exercises
- chatgpt
- students
- exercise
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the performance of ChatGPT (version Feb 2023)
  against first-year computer engineering students in solving probability exercises.
  Twenty-three exercises from introductory statistics courses were administered to
  both students and ChatGPT, with responses evaluated by five experienced statistics
  professors.
---

# The potential of large language models for improving probability learning: A study on ChatGPT3.5 and first-year computer engineering students

## Quick Facts
- arXiv ID: 2310.05686
- Source URL: https://arxiv.org/abs/2310.05686
- Reference count: 32
- ChatGPT outperformed students on 16 of 23 probability exercises

## Executive Summary
This study evaluates ChatGPT's performance against first-year computer engineering students on 23 probability exercises. Five experienced statistics professors evaluated both ChatGPT and student responses using the same grading rubric. Results show ChatGPT excelled in reasoning, organization, and explanation quality, outperforming students on 16 exercises. However, ChatGPT struggled with numerical computations, often making arithmetic errors despite correct conceptual understanding. The study demonstrates that requesting ChatGPT to provide solutions in R code significantly improved numerical accuracy while maintaining its strong conceptual reasoning capabilities.

## Method Summary
The study administered 23 probability exercises from introductory statistics courses to both first-year computer engineering students and ChatGPT (Feb 2023 version). Student grades were collected from their coursework, while ChatGPT responses were generated using prompts in both Spanish and English. Five experienced statistics professors evaluated all responses using the same 0-10 grading rubric applied to students, focusing on reasoning quality, organization, and numerical accuracy. For exercises where ChatGPT showed numerical errors, the study tested whether requesting R code solutions would improve accuracy.

## Key Results
- ChatGPT outperformed students on 16 of 23 exercises, excelling in explanation quality and logical reasoning
- ChatGPT's main weakness was numerical operations, with errors appearing in 16 of 23 natural language responses
- Requesting R code solutions eliminated most numerical errors while preserving conceptual accuracy
- Instructor evaluations showed high agreement on where errors occurred in ChatGPT responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT's reasoning abilities outperform students when numerical computation is offloaded to R code.
- Mechanism: By requesting solutions in R code, ChatGPT bypasses its documented limitations in arithmetic while preserving its strong conceptual understanding and logical structuring.
- Core assumption: ChatGPT's language model can generate correct algorithmic logic that, when executed externally, produces accurate numerical results.
- Evidence anchors:
  - [abstract] "requesting ChatGPT to provide the solution in the form of an R script proved to be an effective approach for overcoming these limitations"
  - [section] "Our findings revealed that out of the 23 exercise answers and 7 alternate exercise answers, only the response to exercise number 8 exhibited a complete absence of numerical errors"
  - [corpus] Weak - corpus neighbors focus on Java and physics, not R-based statistical computation.

### Mechanism 2
- Claim: ChatGPT demonstrates consistent reasoning across multiple prompt variations, showing stable conceptual understanding.
- Mechanism: The model's internal representations of probability concepts remain stable even when prompted with slightly different phrasing, allowing reliable evaluation.
- Core assumption: The RLHF fine-tuning creates stable conceptual representations that generalize across minor prompt variations.
- Evidence anchors:
  - [section] "for 16 of the exercises, all three responses were deemed equivalent in terms of their reasoning"
  - [section] "the lecturers exhibited a high level of agreement on the sections where errors were made"
  - [corpus] Weak - corpus papers focus on coding and physics, not probability concept stability.

### Mechanism 3
- Claim: ChatGPT's ability to generate detailed explanations enhances learning when students critically analyze the responses.
- Mechanism: Students engage in deeper learning by evaluating whether ChatGPT's answers are correct, forcing them to understand the underlying concepts rather than passively receiving information.
- Core assumption: Critical evaluation of AI-generated content promotes active learning and conceptual understanding.
- Evidence anchors:
  - [section] "they must critically analyse and understand the answers, which improves their knowledge and enables them to work independently"
  - [section] "With the help of ChatGPT, they can be asked to solve a series of statistical exercises and explain why an answer is correct or incorrect"
  - [corpus] Moderate - corpus includes papers on student engagement and AI in education, supporting the pedagogical premise.

## Foundational Learning

- Concept: Probability distributions and their applications
  - Why needed here: All 23 exercises involve identifying and applying appropriate probability distributions (binomial, normal, Poisson, etc.)
  - Quick check question: Given a batch of 100 parts with 5% defect rate, what distribution models the number of defective parts found in a random sample of 20?

- Concept: Conditional probability and Bayes' theorem
  - Why needed here: Multiple exercises require calculating probabilities given certain conditions, including Bayes' theorem applications
  - Quick check question: If P(A|B) = 0.6 and P(B) = 0.3, what is P(A âˆ© B)?

- Concept: Combinatorics and counting principles
  - Why needed here: Several exercises involve counting possible outcomes and applying combinatorial formulas
  - Quick check question: How many ways can you choose 3 items from a set of 10 distinct items?

## Architecture Onboarding

- Component map: Input parser -> Prompt generator -> ChatGPT API -> Response evaluator -> R code executor -> Result comparator
- Critical path: Exercise input -> Prompt formulation -> ChatGPT response -> Lecturer evaluation -> R code generation (if requested) -> Numerical verification
- Design tradeoffs: Natural language responses provide better explanations but suffer from numerical errors; R code responses eliminate numerical errors but may be harder to interpret pedagogically
- Failure signatures:
  - Numerical errors in natural language responses
  - Inconsistent reasoning across prompt variations
  - R code that produces syntax errors or incorrect logic
- First 3 experiments:
  1. Run all 23 exercises through ChatGPT with identical prompts in both Spanish and English to verify language independence
  2. Test R code generation for exercises with known numerical answers to establish accuracy baseline
  3. Evaluate consistency by running the same exercises three times with different prompt phrasings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ChatGPT's performance compare to human students when solving probability problems that require multi-step reasoning and conditional probability?
- Basis in paper: [explicit] The paper states that ChatGPT struggled with conditional probability exercises that were not presented in a straightforward manner, and had difficulties when performing the necessary operations.
- Why unresolved: The study only evaluated ChatGPT's performance on a limited set of 23 exercises, and did not systematically explore its performance on different types of probability problems or multi-step reasoning tasks.
- What evidence would resolve it: A more comprehensive study that tests ChatGPT on a larger and more diverse set of probability problems, including those that require multi-step reasoning and conditional probability, would provide evidence to resolve this question.

### Open Question 2
- Question: How does the performance of ChatGPT compare to other large language models or AI systems when solving probability problems?
- Basis in paper: [inferred] The paper only evaluated ChatGPT's performance and did not compare it to other AI systems or large language models.
- Why unresolved: The study only focused on ChatGPT and did not explore how its performance compares to other AI systems or large language models in solving probability problems.
- What evidence would resolve it: A comparative study that evaluates the performance of multiple AI systems or large language models, including ChatGPT, on a set of probability problems would provide evidence to resolve this question.

### Open Question 3
- Question: How does the performance of ChatGPT vary across different languages when solving probability problems?
- Basis in paper: [explicit] The paper states that ChatGPT's performance in answering questions in both Spanish and English is similar, regardless of whether the answers are provided in natural language or R code.
- Why unresolved: The study only tested ChatGPT's performance in Spanish and English, and did not explore its performance in other languages.
- What evidence would resolve it: A study that tests ChatGPT's performance on probability problems in multiple languages would provide evidence to resolve this question.

## Limitations

- The study uses only ChatGPT version Feb 2023 without comparing to other contemporary models or later versions
- The 23 exercises, while comprehensive, represent a limited sample that may not generalize to all probability learning scenarios
- Instructor evaluations introduce potential subjectivity rather than using objective correctness measures
- The computational advantage assumes students have sufficient programming knowledge to understand and verify R code output

## Confidence

- High confidence: ChatGPT outperforms first-year students on reasoning quality and problem-solving approach
- Medium confidence: R code generation reliably improves numerical accuracy
- Low confidence: Critical evaluation of ChatGPT responses universally enhances student learning

## Next Checks

1. Test the R code generation approach across a broader range of probability exercises including continuous distributions, hypothesis testing, and regression analysis to establish generalizability of the numerical accuracy improvement.

2. Conduct a controlled experiment with actual students using ChatGPT responses as learning aids, measuring both immediate problem-solving performance and long-term retention compared to traditional learning methods.

3. Evaluate whether different prompt engineering strategies (chain-of-thought prompting, step-by-step breakdowns, or specific format requests) produce more reliable numerical results than the current approach without requiring R code generation.