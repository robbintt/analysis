---
ver: rpa2
title: Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization
arxiv_id: '2311.04163'
source_url: https://arxiv.org/abs/2311.04163
tags:
- truck
- frog
- ship
- auto
- plane
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a new phenomenon in neural network optimization
  arising from the interaction of depth and heavy-tailed structure in natural data.
  The authors demonstrate the prevalence of paired groups of outliers with strong
  opposing signals that dominate the network output and provide large, consistent
  gradients in opposite directions.
---

# Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization

## Quick Facts
- arXiv ID: 2311.04163
- Source URL: https://arxiv.org/abs/2311.04163
- Authors: 
- Reference count: 40
- Key outcome: This paper identifies a new phenomenon in neural network optimization arising from the interaction of depth and heavy-tailed structure in natural data. The authors demonstrate the prevalence of paired groups of outliers with strong opposing signals that dominate the network output and provide large, consistent gradients in opposite directions. Early optimization enters a narrow valley balancing these opposing groups; subsequent sharpening causes rapid loss oscillation between high on one group and then the other, eventually causing overall loss spikes. This behavior is shown to be a direct cause of the edge of stability phenomenon. The authors describe how to identify these groups, explore what sets them apart, and study their effect on optimization dynamics through experiments and theoretical analysis of a two-layer linear network model. They also highlight connections to other concepts in optimization and generalization including grokking, simplicity bias, and Sharpness-Aware Minimization. The findings enable new qualitative predictions of training behavior and provide a new lens to study stochastic optimization methods.

## Executive Summary
This paper identifies a new phenomenon in neural network optimization arising from the interaction of depth and heavy-tailed structure in natural data. The authors demonstrate the prevalence of paired groups of outliers with strong opposing signals that dominate the network output and provide large, consistent gradients in opposite directions. Early optimization enters a narrow valley balancing these opposing groups; subsequent sharpening causes rapid loss oscillation between high on one group and then the other, eventually causing overall loss spikes. This behavior is shown to be a direct cause of the edge of stability phenomenon. The authors describe how to identify these groups, explore what sets them apart, and study their effect on optimization dynamics through experiments and theoretical analysis of a two-layer linear network model. They also highlight connections to other concepts in optimization and generalization including grokking, simplicity bias, and Sharpness-Aware Minimization. The findings enable new qualitative predictions of training behavior and provide a new lens to study stochastic optimization methods.

## Method Summary
The authors identify training examples with extreme loss changes using a method that tracks per-example loss changes throughout training. They train neural networks (ResNet, VGG) on CIFAR-10 using full-batch gradient descent and monitor individual training point losses at each iteration. The 20 training points with most positive and most negative loss changes are selected and visually inspected to identify groups with consistent, large-magnitude features that correlate with the target task. The authors then monitor the losses of these identified groups over training to observe oscillatory patterns between groups with opposing signals, which correlate with overall loss spikes and the edge of stability phenomenon.

## Key Results
- Neural networks amplify large-magnitude features with strong opposing gradients during early training, creating a delicate balance that later destabilizes
- Progressive sharpening is caused by increasing sensitivity to how large-magnitude opposing features are used, not just general loss curvature growth
- Adam avoids the destabilizing effects of opposing signals by taking smaller steps along high-curvature directions and dampening recent gradients

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural networks amplify and rely on large-magnitude features with strong opposing gradients during early training, creating a delicate balance that later destabilizes.
- Mechanism: At initialization, the network has high sensitivity to input variation. Large-magnitude features that correlate with the target (but are not the "correct" signal) are amplified because they provide the fastest loss reduction. These features create opposing gradients: improving prediction for one group harms the other. The network enters a narrow valley balancing these gradients. As the network sharpens, sensitivity to how these features are used grows, causing the valley to narrow and forcing the network to oscillate between favoring one group and the other.
- Core assumption: Depth and high-dimensional inputs mean only a small fraction of input variation is useful, and steepest descent will amplify whichever large features reduce loss fastest.
- Evidence anchors:
  - [abstract]: "paired groups of outliers with strong opposing signals: consistent, large magnitude features which dominate the network output throughout training and provide gradients which point in opposite directions."
  - [section 3.1]: "small changes to how the network processes inputs become more influential" and "genuine signals which oppose each other will be retained and perhaps even further amplified by gradient descent."
  - [corpus]: Weak—no direct corpus evidence for this amplification mechanism; relies on experimental observation.
- Break condition: If the large-magnitude features are truly random noise (no correlation with target), they are downweighted. If the network learns smaller, more discriminative features early, the opposing signal amplification is reduced.

### Mechanism 2
- Claim: Progressive sharpening is caused by the network's increasing sensitivity to how it uses large-magnitude opposing features, not just by general loss curvature growth.
- Mechanism: As the network amplifies the large-magnitude features, the Hessian spectrum shifts so that most curvature concentrates in early layers and along directions corresponding to the opposing signals. This makes the network highly sensitive to small parameter perturbations that change how the opposing features are used. The network oscillates in weight space along these high-curvature directions, causing loss spikes on the opposing groups while the overall loss remains stable until the oscillation amplitude becomes large enough.
- Core assumption: The majority of sharpness in the loss landscape comes from sensitivity to the usage of a small number of large-magnitude features, not from general noise.
- Evidence anchors:
  - [section 3.2]: "small weight perturbation could massively increase loss by redirecting unhelpful noise to the subspace to which the network is most sensitive."
  - [section 3.4]: "sharpness often occurs overwhelmingly in the first few layers" and "sharpness will begin in the last layer but that early in training it will shift to the earlier layers."
  - [corpus]: Weak—no corpus evidence for this specific sharpness distribution; inferred from experiments.
- Break condition: If the network does not sharpen (e.g., with certain normalization or loss functions), the opposing signals do not cause the same destabilizing oscillation.

### Mechanism 3
- Claim: Adam and similar optimizers avoid the destabilizing effects of opposing signals by taking smaller steps along high-curvature directions and dampening recent gradients.
- Mechanism: Adam's adaptive learning rates normalize steps by curvature, so parameters along high-curvature (opposing signal) directions take smaller steps. Additionally, Adam's momentum with dampening downweights the most recent gradient, preventing the network from taking large steps that would over-rely on the current opposing signal. This keeps the optimization trajectory smooth and prevents the oscillation that causes loss spikes in SGD.
- Core assumption: The problematic behavior comes from large, directed steps along high-curvature directions that over-rely on a single opposing signal.
- Evidence anchors:
  - [section 4.1]: "Adam's normalization causes smaller steps along the top eigenvector, especially near the minimum" and "Adam's steps are more parallel to the valley floor than those of steepest descent."
  - [section 4.2]: "our findings hint at a possible benefit of BN which applies only to SGD: reducing the variance of imbalanced opposing signals across random minibatches."
  - [corpus]: Moderate—related work on Adam's success with attention models and heavy-tailed gradients supports this, but the specific connection to opposing signals is novel.
- Break condition: If the dataset lacks strong opposing signals, or if the optimizer does not properly normalize by curvature, the benefit disappears.

## Foundational Learning

- Concept: Heavy-tailed distributions in data and their effect on optimization
  - Why needed here: The paper's core phenomenon relies on the existence of a small number of large-magnitude features that dominate the network output. Understanding heavy-tailedness explains why such features exist and why they are influential.
  - Quick check question: If a dataset's feature magnitudes follow a Gaussian distribution, would we expect the same opposing signal phenomenon? (Answer: No, because there would be no extreme outliers to dominate.)

- Concept: Loss landscape curvature and its relationship to generalization
  - Why needed here: The paper connects progressive sharpening (growth in loss curvature) to the edge of stability and argues that sharpness along certain directions causes oscillation. Understanding curvature is essential to follow this argument.
  - Quick check question: If a network's loss landscape has very low curvature everywhere, would we expect to see the edge of stability phenomenon? (Answer: No, because there would be no unstable directions to oscillate along.)

- Concept: Optimization dynamics of gradient descent vs. adaptive methods
  - Why needed here: The paper contrasts SGD's behavior with Adam's, arguing that Adam's adaptive steps prevent the destabilizing oscillation caused by opposing signals. Understanding the mechanics of both methods is crucial to follow this comparison.
  - Quick check question: Why might Adam's per-parameter learning rates help it avoid the instability that SGD experiences with opposing signals? (Answer: Because Adam takes smaller steps along high-curvature directions where opposing signals create instability.)

## Architecture Onboarding

- Component map: Data preprocessing → Neural network training → Loss monitoring → Example selection → Signal identification → Oscillation visualization
- Critical path: Data → Network training → Loss monitoring → Example selection → Signal identification → Oscillation visualization
- Design tradeoffs: Full-batch GD provides clear opposing signal identification but is impractical for large datasets. SGD introduces noise that can obscure signals but is more realistic. The choice of neural architecture affects which features become dominant opposing signals.
- Failure signatures: If the monitoring system misses examples with large loss changes, the opposing signals won't be identified. If the clustering algorithm incorrectly groups examples, the oscillation pattern may not be visible. If the visualization doesn't properly align loss changes across steps, the oscillation may appear random.
- First 3 experiments:
  1. Train a small MLP on a synthetic dataset with known large-magnitude opposing features and verify the oscillation pattern appears.
  2. Train a ResNet on CIFAR-10 with full-batch GD and use the loss-change method to identify and visualize opposing signal groups.
  3. Train the same ResNet with Adam and compare the optimization trajectory and presence of oscillation to the SGD case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which opposing signals lead to progressive sharpening and the edge of stability?
- Basis in paper: [explicit] The paper provides a theoretical analysis of a two-layer linear network model and a toy example, but does not fully explain the mechanism.
- Why unresolved: The paper demonstrates the existence of opposing signals and their influence on training dynamics, but does not provide a complete mechanistic explanation for how they cause progressive sharpening and the edge of stability.
- What evidence would resolve it: Further theoretical analysis or experiments that directly link the presence of opposing signals to the growth of the loss Hessian's top eigenvalue and the subsequent oscillation in weight space.

### Open Question 2
- Question: Can the influence of opposing signals be reduced or eliminated to improve neural network optimization?
- Basis in paper: [explicit] The paper discusses how Adam and dampening can mitigate the influence of opposing signals, and proposes a variant of SGD called SplitSGD.
- Why unresolved: While the paper shows that Adam and SplitSGD can handle opposing signals better than standard SGD, it is unclear whether reducing their influence is necessary for improved optimization or simply coincides with it.
- What evidence would resolve it: Experiments comparing the performance of various optimization algorithms with and without opposing signals, and theoretical analysis of the impact of opposing signals on optimization.

### Open Question 3
- Question: What is the relationship between opposing signals and other phenomena in neural network optimization, such as grokking, simplicity bias, and Sharpness-Aware Minimization (SAM)?
- Basis in paper: [explicit] The paper highlights potential connections between opposing signals and these phenomena, but does not provide a detailed analysis.
- Why unresolved: The paper identifies possible links between opposing signals and other optimization phenomena, but does not explore these connections in depth or provide a unified framework for understanding their relationships.
- What evidence would resolve it: Further theoretical and experimental work that investigates the interplay between opposing signals and other optimization phenomena, and develops a more comprehensive understanding of their relationships.

## Limitations
- The identification of "opposing signal" groups relies heavily on manual visual inspection, introducing subjectivity into what constitutes a significant outlier
- The theoretical model of a two-layer linear network may not capture the full complexity of deep nonlinear networks where feature interactions are more intricate
- The central claims rest on empirical observations that may not generalize across all architectures or datasets

## Confidence
- **High**: The existence of paired outlier groups with opposing gradients and their correlation with loss oscillation is well-supported by experiments.
- **Medium**: The connection between these opposing signals and the edge of stability phenomenon is plausible but requires further theoretical validation.
- **Low**: The claim that Adam avoids instability primarily through adaptive step sizes along high-curvature directions needs more rigorous mathematical proof.

## Next Checks
1. **Dataset Variation Test**: Repeat the CIFAR-10 experiments on CIFAR-100 and ImageNet to verify whether opposing signal phenomena persist across datasets with different class distributions and feature complexities.

2. **Architecture Generalization**: Test the opposing signal identification and oscillation patterns on different architectures (Transformer, LSTM, and Vision Transformer) to determine if the phenomenon is architecture-specific or universal.

3. **Theoretical Formalization**: Develop a more rigorous mathematical framework connecting the empirical observations of opposing signals to formal convergence guarantees, particularly for non-linear networks beyond the two-layer linear case.