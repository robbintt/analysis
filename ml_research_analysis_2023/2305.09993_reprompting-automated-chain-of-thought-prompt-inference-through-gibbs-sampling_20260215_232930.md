---
ver: rpa2
title: 'Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling'
arxiv_id: '2305.09993'
source_url: https://arxiv.org/abs/2305.09993
tags:
- reprompting
- prompt
- recipes
- arxiv
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Reprompting introduces an iterative Gibbs sampling algorithm that
  automatically discovers effective Chain-of-Thought (CoT) recipes for language models
  without human intervention. The method frames CoT discovery as sampling from a joint
  distribution of reasoning steps, using previously sampled solutions as prompts to
  solve other training problems.
---

# Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling

## Quick Facts
- arXiv ID: 2305.09993
- Source URL: https://arxiv.org/abs/2305.09993
- Authors: 
- Reference count: 15
- Key outcome: Automated discovery of effective Chain-of-Thought recipes through Gibbs sampling, achieving up to +17 points improvement over human-written CoT prompts

## Executive Summary
Reprompting introduces an iterative Gibbs sampling algorithm that automatically discovers effective Chain-of-Thought (CoT) recipes for language models without human intervention. The method frames CoT discovery as sampling from a joint distribution of reasoning steps, using previously sampled solutions as prompts to solve other training problems. Tested on five challenging Big-Bench Hard tasks requiring multi-step reasoning, Reprompting achieved up to +17 points improvement over human-written CoT prompts and outperformed both zero-shot and few-shot baselines.

## Method Summary
Reprompting treats CoT discovery as a joint distribution sampling problem, using Gibbs sampling to iteratively refine reasoning recipes. The algorithm initializes with zero-shot solutions from an LLM, then iteratively samples new recipes by using previously sampled solutions as in-context examples to solve other training problems. This process exploits the conditional independence assumptions in in-context learning to evolve effective reasoning strategies. The method uses two LLMs: LLM1 for initialization and LLM2 for iterative sampling, with training accuracy monitoring convergence.

## Key Results
- Achieved up to +17 points improvement over human-written CoT prompts on Big-Bench Hard tasks
- Outperformed both zero-shot and few-shot baselines across all five tested reasoning tasks
- Demonstrated effective knowledge transfer from ChatGPT to InstructGPT, improving performance by 2-71 points
- Revealed that CoT recipes optimized for one model may not generalize well to others

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Reprompting uses Gibbs sampling to iteratively refine CoT recipes by treating prompt discovery as a joint distribution sampling problem
- **Mechanism**: The algorithm initializes with zero-shot solutions from an LLM, then iteratively samples new recipes by using previously sampled solutions as in-context examples to solve other training problems
- **Core assumption**: The joint distribution of CoT recipes can be approximated through iterative sampling where pLLM(z j, y j|{xi, zi, yi}i∈S j , x j, m) is approximately invariant to the choice of few-shot examples S j
- **Evidence anchors**: [abstract] "Through Gibbs sampling, Reprompting infers the CoT recipes that work consistently well for a set of training samples by iteratively sampling new recipes using previously sampled recipes as parent prompts to solve other training problems"
- **Break condition**: The algorithm converges when the average training accuracy stops increasing for 1,000 iterations or reaches maximum iterations (M = 20,000)

### Mechanism 2
- **Claim**: Knowledge transfer between models occurs when a stronger LLM initializes solutions for a weaker LLM through Reprompting
- **Mechanism**: ChatGPT generates diverse initial solutions that contain useful reasoning fragments, even if some solutions are incorrect. InstructGPT can follow these initial recipes and refine them through iterative sampling, combining fragments from different recipes to create more effective CoT prompts
- **Core assumption**: Initial recipes from a stronger model contain reasoning patterns that a weaker model can understand and improve upon through iterative refinement
- **Evidence anchors**: [abstract] "Notably, using a stronger model (ChatGPT) to initialize solutions for a weaker model (InstructGPT) significantly improved performance, demonstrating effective knowledge transfer between models"
- **Break condition**: When the training accuracy plateaus or when the weaker model fails to improve upon the initial recipes after several iterations

### Mechanism 3
- **Claim**: Reprompting generates model-specific CoT recipes that do not generalize well across different LLMs
- **Mechanism**: The algorithm optimizes recipes for the specific LLM being used as the sampling model (LLM2), creating prompts that exploit the particular reasoning capabilities and biases of that model
- **Core assumption**: Different LLMs have distinct reasoning capabilities and biases that require model-specific prompt optimization for optimal performance
- **Evidence anchors**: [abstract] "The approach also revealed that CoT recipes optimized for one model may not generalize well to others, emphasizing the need for model-specific prompt optimization"
- **Break condition**: When attempting to use model-specific recipes across different LLMs results in significant performance degradation

## Foundational Learning

- **Concept: Gibbs sampling**
  - Why needed here: The Reprompting algorithm relies on Gibbs sampling to iteratively refine CoT recipes by sampling from conditional distributions when the full joint distribution is intractable
  - Quick check question: How does Gibbs sampling approximate sampling from a high-dimensional joint distribution using only conditional distributions?

- **Concept: In-context learning assumptions**
  - Why needed here: Reprompting exploits the assumption that few-shot performance is approximately invariant to the specific choice of examples, which allows iterative refinement of prompts
  - Quick check question: What is the key statistical assumption behind in-context learning that enables Reprompting to work?

- **Concept: Chain-of-Thought prompting**
  - Why needed here: Understanding how CoT works is essential for grasping why iterative refinement through Reprompting can improve reasoning performance
  - Quick check question: How does Chain-of-Thought prompting differ from standard few-shot prompting in terms of the information provided to the model?

## Architecture Onboarding

- **Component map**: Training data -> Initialization model (LLM1) -> Sampling model (LLM2) -> Gibbs sampling algorithm -> Evaluation metric -> Testing pipeline

- **Critical path**: 
  1. Initialize recipes with LLM1 using zero-shot prompting
  2. Iteratively sample new recipes using LLM2 with previous recipes as prompts
  3. Evaluate training accuracy after each iteration
  4. Select best recipes based on training performance
  5. Test selected recipes on held-out test set

- **Design tradeoffs**:
  - Number of training examples (N) vs. computational cost
  - Number of shots (K) vs. recipe diversity and quality
  - Rejection probability (pre_j) vs. convergence speed
  - Number of iterations (M) vs. risk of overfitting to training set
  - Choice of LLM1 vs. quality of initial recipes
  - Choice of LLM2 vs. ability to refine recipes

- **Failure signatures**:
  - Low training accuracy indicates poor initial recipes or ineffective refinement
  - High training accuracy but low test accuracy suggests overfitting
  - Slow convergence indicates poor choice of rejection probability or insufficient iterations
  - Unstable accuracy across iterations suggests inappropriate model combination
  - Model-specific recipes failing on other models confirms the need for model-specific optimization

- **First 3 experiments**:
  1. Run Reprompting with default parameters (K=5, M=20000, pre_j=0.99) on a simple BBH task to verify basic functionality
  2. Compare performance of Gibbs sampling vs. greedy search variants on the same task to understand convergence behavior
  3. Test model transferability by using ChatGPT-initialized recipes with InstructGPT and vice versa to quantify knowledge transfer effects

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the sensitivity to initialization in Reprompting impact fair comparisons between different LLMs?
- **Basis in paper**: Explicit - The paper demonstrates that Reprompting's performance depends on the initialization model, with ChatGPT often producing better initial solutions than InstructGPT
- **Why unresolved**: The paper shows that using ChatGPT for initialization improves InstructGPT's performance, but does not explore how this initialization sensitivity affects overall fairness in LLM comparisons
- **What evidence would resolve it**: Experiments comparing Reprompting results across different initialization pairs (stronger → weaker models) and measuring how much performance varies based on initialization choices

### Open Question 2
- **Question**: What are the underlying reasons for the lack of transferability of CoT recipes between different LLMs?
- **Basis in paper**: Explicit - The paper shows that CoT recipes optimized for one model (e.g., InstructGPT) may perform poorly when used with another model (e.g., ChatGPT), with differences up to 18% accuracy
- **Why unresolved**: While the paper demonstrates this lack of transferability, it does not investigate the specific reasons why certain models cannot effectively execute recipes designed for others
- **What evidence would resolve it**: Detailed analysis of where and why model-specific recipes fail when transferred, potentially through attention pattern analysis or intermediate step evaluation

### Open Question 3
- **Question**: What is the optimal balance between Gibbs sampling iterations and greedy search variants in Reprompting?
- **Basis in paper**: Inferred - The paper implements both Gibbs sampling (M=20,000 iterations) and greedy search (M=10 iterations) variants but does not systematically compare their efficiency and effectiveness
- **Why unresolved**: The paper uses different iteration limits for each variant without exploring whether fewer iterations of Gibbs sampling could match greedy search performance or vice versa
- **What evidence would resolve it**: Systematic comparison of training accuracy convergence curves for both methods using identical iteration limits and computational budgets

### Open Question 4
- **Question**: How can Reprompting be extended to leverage multiple LLMs simultaneously rather than sequentially (LLM1 for initialization, LLM2 for sampling)?
- **Basis in paper**: Explicit - The paper only explores sequential use of two different LLMs (ChatGPT for initialization, InstructGPT for sampling) but does not investigate parallel or ensemble approaches
- **Why unresolved**: The paper demonstrates benefits from using different LLMs in different roles but does not explore more complex multi-LLM architectures that could potentially yield better results
- **What evidence would resolve it**: Experiments comparing Reprompting performance when using multiple LLMs in parallel for both initialization and sampling phases, versus the sequential approach

### Open Question 5
- **Question**: What is the relationship between the complexity of CoT recipes and their generalizability across tasks?
- **Basis in paper**: Explicit - The paper shows that model-generated CoT recipes differ significantly from human-written ones and that some models follow different solution strategies
- **Why unresolved**: While the paper demonstrates differences in recipe complexity and style, it does not investigate whether simpler or more complex recipes tend to generalize better across different reasoning tasks
- **What evidence would resolve it**: Analysis correlating recipe structural complexity metrics with cross-task generalization performance, potentially revealing optimal recipe complexity for broad applicability

## Limitations
- Limited comparison to alternative optimization methods like gradient-based approaches or direct few-shot optimization
- No systematic exploration of hyperparameter sensitivity (rejection probability, number of iterations, number of shots)
- Potential overfitting to training examples given the small number of training samples (N=20)

## Confidence
- **High confidence**: The observation that model-specific CoT recipes don't generalize well across different LLMs
- **Medium confidence**: The effectiveness of knowledge transfer from stronger to weaker models through Reprompting
- **Low confidence**: The necessity of Gibbs sampling versus alternative optimization approaches

## Next Checks
1. **Ablation study**: Compare Reprompting against greedy search and random sampling baselines on the same BBH tasks to determine if Gibbs sampling provides unique advantages
2. **Generalization test**: Evaluate the discovered CoT recipes on out-of-distribution reasoning problems not seen during training to assess true reasoning capability versus memorization
3. **Scaling analysis**: Test Reprompting with different numbers of training examples (N < 20 and N > 20) to understand the relationship between training data size and recipe quality