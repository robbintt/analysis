---
ver: rpa2
title: ConceptPsy:A Benchmark Suite with Conceptual Comprehensiveness in Psychology
arxiv_id: '2311.09861'
source_url: https://arxiv.org/abs/2311.09861
tags:
- knowledge
- questions
- points
- psychology
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ConceptPsy, a Chinese psychology benchmark
  for evaluating large language models (LLMs). Existing psychology benchmarks lack
  depth and breadth, covering limited concepts and not addressing the challenge of
  data leakage from public datasets.
---

# ConceptPsy:A Benchmark Suite with Conceptual Comprehensiveness in Psychology

## Quick Facts
- arXiv ID: 2311.09861
- Source URL: https://arxiv.org/abs/2311.09861
- Reference count: 6
- The paper introduces ConceptPsy, a Chinese psychology benchmark for evaluating large language models (LLMs).

## Executive Summary
ConceptPsy addresses critical gaps in existing psychology benchmarks by providing comprehensive coverage of 12 core psychology subjects and 1,383 concepts. The benchmark tackles the challenge of data leakage through a novel framework using GPT-4-generated questions reviewed by professional psychologists. Chapter-level annotations enable fine-grained performance analysis across different psychology concepts. Evaluations reveal significant performance variations among LLMs, even within the same model series.

## Method Summary
ConceptPsy uses knowledge points from the National Entrance Examination for Postgraduates (NEEP) as a foundation for question generation. GPT-4 generates multiple-choice questions for each knowledge point using diverse prompts, creating three question types: calculation, theory understanding, and case study. Professional psychologists review all generated questions to ensure quality and correctness. The benchmark includes chapter-level annotations for detailed performance analysis. LLMs are evaluated in both zero-shot and five-shot settings across all subjects.

## Key Results
- ConceptPsy covers 12 core psychology subjects and 1,383 concepts with comprehensive depth and breadth
- LLM performance varies significantly across psychology concepts, even among models from the same series
- The benchmark successfully mitigates data leakage concerns through its GPT-4 generation and review process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4-generated questions provide deeper psychology concept coverage than manual question creation alone.
- Mechanism: GPT-4 uses 1,383 manually collected knowledge points from NEEP to generate questions, each reviewed by professional psychologists. This combines automated generation with expert validation.
- Core assumption: GPT-4 can accurately generate psychology questions from provided knowledge points, and human reviewers can effectively validate them.
- Evidence anchors:
  - [abstract] "Specifically, we prompt GPT-4 to generate questions for each concept using carefully designed diverse prompts and hire professional psychologists to review these questions."
  - [section] "We utilize the knowledge points prescribed by the National Entrance Examination for Postgraduates (NEEP) for each of the subject, feeding each point individually into GPT-4, which then generates multiple-choice questions based on the given knowledge point."
  - [corpus] "Average neighbor FMR=0.407, average citations=0.0" - The related literature shows moderate conceptual overlap but lacks direct evidence of this generation mechanism's effectiveness.
- Break condition: If GPT-4 generates questions that consistently mislead or confuse human reviewers, or if reviewers cannot distinguish quality questions from poor ones.

### Mechanism 2
- Claim: The benchmark prevents data leakage through its question generation and review process.
- Mechanism: Questions are generated using GPT-4 from knowledge points not available during testing, and reviewed by psychologists. This creates questions that are unlikely to have been seen during model pretraining.
- Core assumption: GPT-4 won't generate questions that directly match any existing test set or public psychology exam content.
- Evidence anchors:
  - [abstract] "We attempt to mitigate data leakage problem: The issue of data leakage is a pressing concern for benchmarks for foundational models... we propose a novel framework that uses diverse prompts with knowledge points to generate questions based on the GPT-4 model."
  - [section] "Another urgent problem in evaluating LLMs is the issue of data leakage... While manually designing new, undisclosed questions for each knowledge point and regularly swapping them out is a potential solution, this approach can be costly. In response to these challenges, we propose a novel framework that uses diverse prompts with knowledge points to generate questions based on the GPT-4 model."
  - [corpus] Weak - no direct corpus evidence for this specific anti-leakage mechanism.
- Break condition: If GPT-4's training data includes psychology knowledge points similar to NEEP, it may generate questions too similar to existing public content.

### Mechanism 3
- Claim: The benchmark provides fine-grained performance analysis through chapter-level annotations.
- Mechanism: Each question is annotated with a chapter label, enabling analysis of model performance across different psychology concepts.
- Core assumption: Chapter-level annotations provide meaningful granularity for understanding model strengths and weaknesses.
- Evidence anchors:
  - [abstract] "To help to understand the fine-grained performances and enhance the weaknesses, we annotate each question with a chapter label and provide chapter-wise accuracy."
  - [section] "We utilize the knowledge points prescribed by the National Entrance Examination for Postgraduates (NEEP) for each of the subject, feeding each point individually into GPT-4... To enhance the diversity of the generated multiple-choice question types, we've designed three categories: (1) Calculation; (2) Theory understanding; (3) Case Study."
  - [corpus] "Alignment reduces language models' conceptual diversity" - related literature suggests that fine-grained analysis is important for understanding LLM capabilities.
- Break condition: If chapter-level granularity is too fine to show meaningful performance patterns, or if models perform similarly across all chapters.

## Foundational Learning

- Concept: Psychology domain knowledge structure
  - Why needed here: Understanding how psychology concepts are organized helps in designing appropriate questions and interpreting model performance across different areas.
  - Quick check question: What are the 12 core psychology subjects covered in ConceptPsy, and why were these specific subjects chosen?

- Concept: Large language model evaluation methodology
  - Why needed here: Understanding evaluation approaches like zero-shot vs. few-shot settings is crucial for interpreting benchmark results.
  - Quick check question: How do zero-shot and five-shot evaluation settings differ in their approach to testing LLM capabilities?

- Concept: Benchmark construction and validation
  - Why needed here: Understanding how benchmarks are created, validated, and updated is essential for maintaining benchmark quality and preventing data leakage.
  - Quick check question: What role do professional psychologists play in the validation process of ConceptPsy questions?

## Architecture Onboarding

- Component map: Knowledge point collection from NEEP → GPT-4 question generation → Professional psychologist review → Dataset assembly → Model evaluation
- Critical path: Knowledge point collection → GPT-4 generation → Psychologist review → Dataset assembly → Model evaluation
- Design tradeoffs: Automated generation with human review balances cost and quality, but relies on GPT-4's ability to generate appropriate questions and reviewers' ability to validate them effectively.
- Failure signatures: Poor question quality despite review (suggesting GPT-4 generation issues), inconsistent performance across similar models (suggesting benchmark issues), or suspiciously high performance (suggesting data leakage).
- First 3 experiments:
  1. Generate questions for a small subset of knowledge points and have multiple psychologists review them independently to test inter-rater reliability.
  2. Evaluate the same models on both ConceptPsy and existing psychology benchmarks to check for correlation and identify any leakage.
  3. Test whether models perform significantly differently on chapter-level subsets to validate the granularity of the annotations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of psychology concepts (e.g., theoretical vs. applied) impact LLM performance, and are there systematic differences in accuracy across these categories?
- Basis in paper: [inferred] The paper observes significant performance variations across different psychology concepts, even among models from the same series. It also mentions that questions are generated using diverse prompts (calculation, theory understanding, case study) to increase diversity.
- Why unresolved: The paper does not analyze whether certain types of psychology concepts (e.g., theoretical vs. applied) are more challenging for LLMs, or if there are systematic differences in accuracy across these categories.
- What evidence would resolve it: Analyzing LLM performance across different categories of psychology concepts (e.g., theoretical vs. applied) and identifying systematic differences in accuracy would help answer this question.

### Open Question 2
- Question: How does the performance of LLMs on ConceptPsy compare to their performance on other psychology benchmarks, and what factors contribute to any observed differences?
- Basis in paper: [inferred] The paper introduces ConceptPsy as a comprehensive Chinese psychology benchmark and evaluates multiple LLMs on it. It mentions that existing psychology benchmarks lack depth and breadth, covering limited concepts and not addressing the challenge of data leakage.
- Why unresolved: The paper does not compare the performance of LLMs on ConceptPsy to their performance on other psychology benchmarks, nor does it analyze the factors contributing to any observed differences.
- What evidence would resolve it: Comparing LLM performance on ConceptPsy to their performance on other psychology benchmarks and analyzing the factors contributing to any observed differences would help answer this question.

### Open Question 3
- Question: How does the performance of LLMs on ConceptPsy vary across different levels of expertise (e.g., undergraduate vs. graduate), and what implications does this have for their application in psychology education and practice?
- Basis in paper: [explicit] The paper mentions that ConceptPsy covers 12 core psychology subjects and 1,383 concepts, with questions generated by GPT-4 and reviewed by professional psychologists. It also mentions that the benchmark includes chapter-level annotations for fine-grained performance analysis.
- Why unresolved: The paper does not analyze how LLM performance on ConceptPsy varies across different levels of expertise, nor does it discuss the implications of these variations for their application in psychology education and practice.
- What evidence would resolve it: Analyzing LLM performance on ConceptPsy across different levels of expertise and discussing the implications of these variations for their application in psychology education and practice would help answer this question.

## Limitations

- Data leakage mitigation remains an open challenge, as GPT-4's training data may overlap with psychology knowledge points
- The benchmark's reliance on GPT-4 for question generation introduces potential bias in question style and difficulty distribution
- The review process by psychologists, while valuable, may not catch all subtle similarities to existing test content

## Confidence

High confidence in the benchmark's comprehensive coverage of psychology concepts (12 subjects, 1,383 concepts). The use of NEEP knowledge points provides a solid foundation, and professional review adds credibility.

Medium confidence in the data leakage prevention mechanism. The theoretical approach is sound, but practical effectiveness needs validation through correlation analysis with existing benchmarks.

Medium confidence in the chapter-level annotation granularity. While the annotation approach is reasonable, the actual utility for identifying model weaknesses requires empirical validation.

## Next Checks

1. **Leakage Correlation Analysis**: Compare ConceptPsy performance with existing psychology benchmarks on the same models. Calculate correlation coefficients to identify potential leakage. If correlations exceed 0.8 for any subject, investigate specific question overlaps.

2. **Inter-rater Reliability Test**: Have 3-5 independent psychologists review a random 10% sample of generated questions. Calculate Cohen's kappa to assess consistency in validation decisions. Kappa < 0.6 suggests the review process needs refinement.

3. **Concept Distribution Audit**: Analyze the distribution of questions across the 1,383 concepts. Use chi-square goodness-of-fit to test whether the distribution matches expected coverage based on NEEP requirements. Significant deviations (p < 0.05) indicate coverage gaps.