---
ver: rpa2
title: Low-latency Real-time Voice Conversion on CPU
arxiv_id: '2311.00873'
source_url: https://arxiv.org/abs/2311.00873
tags:
- conversion
- voice
- speech
- real-time
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LLVC, a low-latency real-time any-to-one voice
  conversion model that achieves the lowest resource usage and latency of any open-source
  voice conversion model. The model is based on the Waveformer architecture and uses
  a generative adversarial architecture as well as knowledge distillation to attain
  its performance.
---

# Low-latency Real-time Voice Conversion on CPU

## Quick Facts
- arXiv ID: 2311.00873
- Source URL: https://arxiv.org/abs/2311.00873
- Reference count: 35
- Key outcome: LLVC achieves under 20ms latency at 16kHz, runs 2.8x faster than real-time on CPU

## Executive Summary
This paper introduces LLVC, a low-latency real-time voice conversion model designed for CPU deployment. The model achieves the lowest resource usage and latency among open-source voice conversion systems by combining a Waveformer-derived architecture with causal convolutions, masked transformer decoders, and knowledge distillation. LLVC converts speech to a target speaker's voice in under 20ms at 16kHz, running nearly 2.8 times faster than real-time on consumer CPUs.

## Method Summary
LLVC employs a generator based on the Waveformer architecture with causal convolutions and a masked transformer decoder to ensure low latency. A discriminator using a multi-period architecture similar to VITS is used for adversarial training. Knowledge distillation is applied by first training a teacher model (RVC v2) on non-parallel data, then using it to generate a synthetic parallel dataset. The student model (LLVC) is trained on this dataset to mimic the teacher's output distribution. The model uses a 512-dimensional encoder and 256-dimensional decoder, with reduced depth and lookahead compared to Waveformer.

## Key Results
- Latency under 20ms at 16kHz sampling rate
- Runs 2.8x faster than real-time on consumer CPU
- Lowest resource usage among open-source voice conversion models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLVC achieves low latency by using a Waveformer-derived architecture with causal convolutions and masked transformer decoder.
- Mechanism: The causal convolutions in the encoder and masked transformer decoder ensure that the model only attends to present and past tokens, eliminating the need for future context and reducing latency.
- Core assumption: The causal nature of the encoder and decoder allows intermediate calculations to be cached for future inference passes, increasing inference speed.
- Evidence anchors:
  - [abstract] "Our resulting model, LLVC ( Low-latency Low-resource Voice Conversion), has a latency of under 20ms at a bitrate of 16kHz"
  - [section] "Based on the success of causal U-Nets for speech modeling and enhancement[ 22, 19], we prefix the model with a prenet composed of causal convolutions."
  - [corpus] Weak - no direct corpus evidence supporting the specific latency claim, but related work mentions "StreamVC: Real-Time Low-Latency Voice Conversion"
- Break condition: If the model needs to process future context for high-quality conversion, latency will increase beyond the claimed 20ms.

### Mechanism 2
- Claim: LLVC uses knowledge distillation to achieve high quality with low resource usage.
- Mechanism: A teacher model (RVC v2) is trained on non-parallel data, then used to generate a synthetic parallel dataset. A smaller student model (LLVC) is trained on this dataset to mimic the teacher's output distribution.
- Core assumption: The synthetic parallel dataset generated by the teacher model is of sufficient quality to train the student model effectively.
- Evidence anchors:
  - [abstract] "LLVC uses both a generative adversarial architecture as well as knowledge distillation in order to attain this performance."
  - [section] "Although parallel speaker datasets have historically been challenging to create, introducing additional challenges such as aligning the utterances in time[9], the quality of modern voice conversion networks is now high enough such that they can now be artificially created."
  - [corpus] Weak - no direct corpus evidence supporting the specific knowledge distillation approach, but related work mentions "Non-autoregressive real-time Accent Conversion model with voice cloning"
- Break condition: If the teacher model is not of sufficient quality, the synthetic dataset will be poor and the student model will not learn effectively.

### Mechanism 3
- Claim: LLVC achieves faster than real-time performance by using a smaller model with fewer parameters and less architectural complexity.
- Mechanism: By using a 512-dimensional encoder and 256-dimensional decoder (compared to Waveformer's larger dimensions), and decreasing encoder depth from 10 to 8 layers, LLVC reduces the computational complexity of the model.
- Core assumption: The reduced model complexity does not significantly impact the quality of the voice conversion output.
- Evidence anchors:
  - [abstract] "runs nearly 2.8x faster than real-time on a consumer CPU"
  - [section] "We adopt Waveformer's 512-dimensional encoder and 256-dimensional decoder as the base for our model, though we decrease encoder depth from 10 to 8 layers and decreasing lookahead to 16 samples for lower inference latency and computation speed."
  - [corpus] Weak - no direct corpus evidence supporting the specific model dimensions, but related work mentions "Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion"
- Break condition: If the reduced model complexity leads to a significant drop in conversion quality, the performance gains will not be worth the trade-off.

## Foundational Learning

- Concept: Voice conversion
  - Why needed here: Understanding the task of voice conversion is crucial for understanding the problem LLVC is trying to solve.
  - Quick check question: What is the difference between any-to-one and any-to-many voice conversion?
- Concept: Causal convolutions
  - Why needed here: Causal convolutions are a key component of LLVC's low latency architecture.
  - Quick check question: How do causal convolutions differ from regular convolutions in terms of their ability to process future context?
- Concept: Knowledge distillation
  - Why needed here: Knowledge distillation is the technique used by LLVC to achieve high quality with low resource usage.
  - Quick check question: What is the role of the teacher model in knowledge distillation?

## Architecture Onboarding

- Component map: Audio input -> Causal convolution prenet -> Waveformer encoder -> Masked transformer decoder -> Audio output
- Critical path: Audio input → Causal convolution prenet → Waveformer encoder → Masked transformer decoder → Audio output
- Design tradeoffs:
  - Latency vs. quality: Using causal convolutions and masked transformer decoder reduces latency but may impact quality
  - Model size vs. resource usage: Using a smaller model with fewer parameters reduces resource usage but may impact quality
  - Knowledge distillation vs. training data: Using knowledge distillation reduces the need for large amounts of parallel training data but relies on the quality of the teacher model
- Failure signatures:
  - High latency: Model is processing future context or using a large model
  - Low quality: Model is too small or knowledge distillation is not effective
  - High resource usage: Model is too large or not optimized for low-resource environments
- First 3 experiments:
  1. Test model latency with different lookahead values to find the optimal balance between latency and quality
  2. Compare model quality with different model sizes to find the optimal balance between quality and resource usage
  3. Test model quality with different teacher models to find the optimal balance between knowledge distillation and training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model perform on noisy speech or non-English languages?
- Basis in paper: [inferred] The paper mentions the dataset contains only clean English speech, and notes the model could be fine-tuned on noisy or multi-lingual speech.
- Why unresolved: The paper only evaluates the model on clean English speech from the LibriSpeech dataset.
- What evidence would resolve it: Evaluating the model's performance on noisy speech datasets or non-English speech corpora, comparing naturalness and speaker similarity scores to the current results.

### Open Question 2
- Question: How does the model's performance change when fine-tuned on a single input speaker to create a personalized voice conversion model?
- Basis in paper: [explicit] The paper suggests fine-tuning the model on a dataset comprised of only a single input speaker converted to a target voice to create a personalized voice conversion model.
- Why unresolved: The paper does not provide any results or analysis on the performance of a personalized model.
- What evidence would resolve it: Training and evaluating the model on a personalized dataset, comparing naturalness and speaker similarity scores to the current results.

### Open Question 3
- Question: How does the model's performance change with different amounts of training data or different target speakers?
- Basis in paper: [inferred] The paper uses a specific amount of training data (500,000 steps on LibriSpeech) and a specific target speaker. It's unclear how the model's performance scales with more data or different target speakers.
- Why unresolved: The paper does not explore the impact of varying the amount of training data or target speakers.
- What evidence would resolve it: Training the model with different amounts of data and different target speakers, comparing naturalness and speaker similarity scores to the current results.

## Limitations

- Dataset quality uncertainty: The synthetic parallel dataset generation relies on the quality of the RVC v2 teacher model, but no quantitative evaluation of the synthetic dataset quality is provided.
- Implementation specificity: The paper mentions using a Waveformer-derived architecture but doesn't provide exact architectural details, making direct replication challenging.
- Latency measurement methodology: While 20ms latency is claimed, the paper doesn't specify the exact measurement methodology or hardware specifications beyond "consumer CPU."

## Confidence

**High Confidence (80-100%)**
- LLVC achieves low latency through causal convolutions and masked transformer architecture
- The overall approach of using knowledge distillation for voice conversion is sound
- The combination of GAN architecture with knowledge distillation is theoretically justified

**Medium Confidence (50-80%)**
- The specific latency of under 20ms is achievable on target hardware
- The RTF of 2.8x faster than real-time is accurate
- The quality metrics (MOS, Resemblyze, WVMOS scores) are meaningful

**Low Confidence (0-50%)**
- LLVC has the "lowest resource usage of any open-source voice conversion model" (no comprehensive comparative analysis provided)
- The synthetic dataset quality is sufficient for effective training (no quantitative validation)
- The model generalizes well across diverse speakers and speaking styles

## Next Checks

1. **Synthetic dataset quality validation**: Generate parallel utterances using the RVC v2 teacher model and conduct a human evaluation comparing them to ground truth parallel data (if available) or to samples from other synthesis methods to verify the synthetic data quality is sufficient for effective knowledge distillation.

2. **Ablation study on architecture components**: Systematically remove or modify key architectural components (causal convolutions, masked attention, discriminator components) to quantify their individual contributions to latency, quality, and resource usage, validating the claimed performance benefits.

3. **Cross-speaker generalization testing**: Evaluate LLVC on speakers outside the LibriSpeech dataset with different vocal characteristics, accents, and speaking styles to assess the model's generalization capabilities and identify potential failure modes.