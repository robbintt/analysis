---
ver: rpa2
title: Backdoor Adjustment of Confounding by Provenance for Robust Text Classification
  of Multi-institutional Clinical Notes
arxiv_id: '2310.02451'
source_url: https://arxiv.org/abs/2310.02451
tags:
- confounding
- text
- adjustment
- shift
- backdoor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses confounding by provenance in clinical text
  classification, where data from multiple institutions may introduce bias due to
  differences in label distributions across sources. The authors propose using backdoor
  adjustment, a causal inference technique, to mitigate this bias when classifying
  clinical notes for substance abuse mentions.
---

# Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes

## Quick Facts
- arXiv ID: 2310.02451
- Source URL: https://arxiv.org/abs/2310.02451
- Authors: 
- Reference count: 29
- Primary result: Backdoor adjustment mitigates confounding by provenance in clinical text classification, with effectiveness varying by text representation and shift direction

## Executive Summary
This paper addresses confounding by provenance in clinical text classification, where data from multiple institutions may introduce bias due to differences in label distributions across sources. The authors propose using backdoor adjustment, a causal inference technique, to mitigate this bias when classifying clinical notes for substance abuse mentions. They evaluate this approach using the SHAC dataset with two text representations: binary unigram vectors and Sentence-BERT embeddings. Through a perturbation framework that simulates different degrees of distribution shift, they find that backdoor adjustment effectively reduces bias when the test set has higher proportions of positive examples from the minority source (MIMIC). However, it performs worse when the shift favors the majority source (UW). Binary unigrams show stronger overall robustness to provenance shift than Sentence-BERT embeddings.

## Method Summary
The study applies backdoor adjustment, a causal inference technique, to mitigate confounding by provenance in clinical text classification. The approach uses logistic regression models with provenance (source identity) as a confounder variable. Two text representations are compared: binary unigram vectors (one-hot encoding) and Sentence-BERT embeddings. A perturbation framework systematically varies the relationship between training and test distributions by controlling the proportion of positive examples from each source. The SHAC dataset, containing clinical notes from UW and MIMIC sources annotated for substance abuse mentions, serves as the evaluation corpus. Performance is measured using Area Under the Precision-Recall Curve (AUPRC) across different simulated distribution shifts.

## Key Results
- Backdoor adjustment effectively reduces bias when test set distribution favors the minority source (MIMIC)
- Binary unigram representations show stronger robustness to provenance shift than Sentence-BERT embeddings
- When shift favors majority source (UW), backdoor adjustment performs worse than non-adjusted models
- Sentence-BERT BA model performance ranges from 0.87-0.93 AUPRC, while binary unigrams achieve 0.90-0.94 AUPRC under P(z=MIMIC)=0.5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backdoor adjustment reduces confounding bias by explicitly modeling the relationship between provenance (source) and both text features and outcome labels
- Mechanism: The method adds provenance as a confounder variable in the logistic regression model, allowing the model to separate the effects of source-specific patterns from genuine signal about substance abuse
- Core assumption: The provenance variable (source identity) is both a cause of text feature distributions and a cause of label distributions
- Evidence anchors:
  - [abstract] "using backdoor adjustment, a causal inference technique, to mitigate this bias"
  - [section 3.1] "Under the simple setting shown in (Figure 1(a)), the confounding effect can be controlled for by the following summation over Z"
  - [corpus] Weak evidence - no direct corpus support found
- Break condition: If the source variable does not actually influence both text features and label distributions, the adjustment would be ineffective

### Mechanism 2
- Claim: Text representation choice affects the effectiveness of backdoor adjustment in mitigating provenance-related confounding
- Mechanism: Binary unigram vectors preserve more source-specific lexical patterns that can be adjusted for, while Sentence-BERT embeddings abstract away some of these patterns through contextual encoding
- Core assumption: The representational form determines how much provenance-related information is retained in the features
- Evidence anchors:
  - [section 3.3] "binary unigram vectors (i.e. one-hot vectors with a coordinate corresponding to each word in the vocabulary)"
  - [section 3.4] "Sentence-BERT is a BERT-based framework with modifications in training using siamese and triplet network structures"
  - [section 4] "When comparing binary unigram vectors and Sentence-BERT embeddings, in setting of P (z = MIMIC) = 0.5, Sentence-BERT BA model performance is in the range 0.87-0.93, while binary unigram representations result in an AUPRC of 0.90-0.94"
- Break condition: If the text representation method eliminates all source-specific patterns, backdoor adjustment would have no effect

### Mechanism 3
- Claim: The perturbation framework enables controlled evaluation of robustness to provenance shift by systematically varying the relationship between training and test distributions
- Mechanism: By controlling the proportion of positive examples from each source in training and test sets independently, the framework can simulate different degrees and directions of distribution shift
- Core assumption: The controlled perturbation accurately represents realistic scenarios of how provenance distributions might change between training and deployment
- Evidence anchors:
  - [section 3.2] "We proceed to describe our experiments in detail. The Z variable was assigned as 0 for UW, 1 for MIMIC. We fixed the training set size at 2,000 and testing set size at 500"
  - [section 3.2] "This simulates a setting where, in general, we only know our training set, and different degrees of shift are simulated"
  - [corpus] Weak evidence - no direct corpus support found
- Break condition: If real-world provenance shifts do not follow the patterns simulated by the perturbation framework, the evaluation may not be representative

## Foundational Learning

- Directed Acyclic Graphs (DAGs) in causal inference
  - Why needed here: DAGs provide the formal framework for understanding how confounders like provenance create spurious associations between features and outcomes
  - Quick check question: In the DAG shown in Figure 1(b), which variable serves as the confounder that affects both text features and the drug abuse label?

- Confounding by indication vs. confounding by provenance
  - Why needed here: Understanding the distinction helps clarify why traditional confounding adjustment methods need modification for text classification tasks
  - Quick check question: How does confounding by provenance differ from classic confounding by indication in clinical studies?

- Distribution shift and its relationship to model performance
  - Why needed here: The perturbation framework specifically creates controlled distribution shifts to evaluate model robustness
  - Quick check question: What happens to model performance when the proportion of positive examples from MIMIC increases in the test set compared to training?

## Architecture Onboarding

- Component map: Data pipeline (SHAC dataset → sampling and perturbation → train/test splits) → Text representation (Binary unigrams OR Sentence-BERT embeddings) → Model layer (Logistic regression with or without backdoor adjustment) → Evaluation (AUPRC calculation across perturbed distributions)

- Critical path: The perturbation framework → text representation choice → backdoor adjustment application → performance evaluation under distribution shift

- Design tradeoffs:
  - Binary unigrams preserve source-specific lexical patterns but ignore word order and context
  - Sentence-BERT captures semantic meaning but may obscure provenance-specific lexical cues
  - Backdoor adjustment adds complexity but provides robustness to specific types of distribution shift

- Failure signatures:
  - AUPRC drops sharply when test set distribution diverges from training distribution
  - No performance difference between adjusted and non-adjusted models suggests the confounder has no effect
  - High variance across repeated experiments suggests instability in the adjustment method

- First 3 experiments:
  1. Run with P(z=MIMIC)=0.5, αtrain=0.4, αtest=0.4 (no shift) using binary unigrams to establish baseline performance
  2. Run with P(z=MIMIC)=0.5, αtrain=0.4, αtest=2.0 (shift toward MIMIC) using binary unigrams to test adjustment effectiveness
  3. Run with P(z=MIMIC)=0.5, αtrain=0.4, αtest=0.2 (shift toward UW) using Sentence-BERT to compare representation effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of backdoor adjustment vary with different training set source-specific positive class distributions?
- Basis in paper: [explicit] The authors state this will be evaluated in future work, noting that "we utilized fixed training source-specific positive class rates... Since the positive rates are imbalanced for training the models, as future work, we will evaluate the BA method under different settings during training..."
- Why unresolved: The current study used fixed source-specific positive class rates (0.5 for UW, 0.2 for MIMIC) and did not explore how varying these rates would affect backdoor adjustment's effectiveness.
- What evidence would resolve it: Experiments systematically varying the training set source-specific positive class rates and measuring backdoor adjustment's performance across these variations.

### Open Question 2
- Question: How do different text representation methods (beyond binary unigrams and Sentence-BERT) perform when combined with backdoor adjustment for mitigating confounding by provenance?
- Basis in paper: [explicit] The authors note that "Normalized counts of n-grams could be another option for retaining more information" and suggest exploring "other statistical and machine learning models, SVM, XGBoost" as well as different Sentence-BERT models.
- Why unresolved: The study only tested binary unigrams and one specific Sentence-BERT model (all-MiniLM-L6-v2), leaving open questions about other representation methods.
- What evidence would resolve it: Comparative experiments testing multiple text representation methods (n-grams, different Sentence-BERT models, other embedding approaches) with backdoor adjustment.

### Open Question 3
- Question: Can backdoor adjustment be effectively integrated with deep learning models for text classification beyond logistic regression?
- Basis in paper: [explicit] The authors state "to better utilize embeddings generated from large language models, deep neural networks could be used for text classification" and note that "The application of backdoor adjustment while fine-tuning a deep learning model for text categorization remains an interesting direction for future work."
- Why evidence would resolve it: The current study applied backdoor adjustment only to logistic regression models, not exploring its integration with deep learning architectures.
- What evidence would resolve it: Experiments implementing backdoor adjustment within deep learning frameworks (fine-tuning BERT, using attention mechanisms, etc.) and comparing results to logistic regression approaches.

## Limitations

- The perturbation framework may not fully capture real-world distribution shifts between institutions
- Logistic regression as the base model limits applicability to more complex clinical NLP scenarios
- Finding that binary unigrams outperform Sentence-BERT embeddings under provenance shift contradicts typical trends favoring contextual embeddings

## Confidence

- **High Confidence**: Backdoor adjustment effectiveness when test distribution favors minority source (MIMIC)
- **Medium Confidence**: Text representation choice impact on adjustment effectiveness
- **Medium-Low Confidence**: Generalizability to other clinical classification tasks and institutions

## Next Checks

1. **Dataset Generalization**: Validate findings on additional clinical datasets from 3+ institutions to test scalability of backdoor adjustment approach
2. **Model Architecture Impact**: Compare backdoor adjustment effectiveness across different model architectures (e.g., transformer-based classifiers) to assess robustness to model choice
3. **Real-world Distribution Shift**: Implement backdoor adjustment on actual deployment data where source distributions naturally shift, rather than simulated perturbations, to evaluate practical utility