---
ver: rpa2
title: 'BLESS: Benchmarking Large Language Models on Sentence Simplification'
arxiv_id: '2310.15773'
source_url: https://arxiv.org/abs/2310.15773
tags:
- simplification
- llms
- association
- linguistics
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BLESS is a large-scale benchmarking study evaluating 44 large language
  models on sentence simplification across three datasets (Wikipedia, news, and medical
  texts) under a few-shot setting. The study assesses models based on automatic metrics,
  edit operation diversity, and manual quality annotations.
---

# BLESS: Benchmarking Large Language Models on Sentence Simplification

## Quick Facts
- **arXiv ID**: 2310.15773
- **Source URL**: https://arxiv.org/abs/2310.15773
- **Reference count**: 34
- **Primary result**: Closed-weight models outperform open-weight alternatives on text simplification, with GPT-3.5-Turbo achieving highest SARI scores

## Executive Summary
BLESS presents a comprehensive benchmarking study evaluating 44 large language models on sentence simplification across three datasets (Wikipedia, news, and medical texts) under few-shot in-context learning settings. The study assesses models using automatic metrics (SARI, BERTScore, FKGL, LENS), edit operation analysis, and manual quality annotations. Results demonstrate that closed-weight models significantly outperform open-weight alternatives, with GPT-3.5-Turbo and Davinci-003 achieving the highest scores, even surpassing human references on Wikipedia-style data. The study identifies a fundamental trade-off between simplicity and meaning preservation, with automatic metrics showing divergent preferences. Qualitative analysis reveals that closed-weight models perform more diverse simplification operations while open-weight models rely more heavily on deletion.

## Method Summary
The study evaluates 44 LLMs on sentence simplification using few-shot in-context learning with three different prompts, each containing 3 randomly sampled examples from validation sets. Models are evaluated on three datasets: ASSET (Wikipedia, 359 sentences), NEWSELA (news, 256 sentences), and MED-EASI (medical, 300 sentences). Automatic metrics include SARI (evaluating n-gram overlap), BERTScore (semantic similarity), FKGL (readability), and LENS (Levenshtein similarity). Edit operations are analyzed using the Wagner-Fischer algorithm to categorize insertions, replacements, deletions, and kept words. Manual qualitative analysis is performed on 15 outputs to validate automatic metrics and identify failure modes.

## Key Results
- Closed-weight models (GPT-3.5-Turbo, Davinci-003) achieve highest SARI scores, surpassing human references on Wikipedia-style data
- Instruction-tuned models demonstrate stronger performance in meaning preservation compared to base models
- Models exhibit a trade-off between simplicity (SARI) and meaning preservation (BERTScore), with automatic metrics showing divergent preferences
- MED-EASI medical domain poses the greatest challenge, with most models struggling in meaning preservation
- Closed-weight models perform more diverse simplification operations (splitting, paraphrasing) while open-weight models rely more on deletion

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Closed-weight models outperform open-weight models in few-shot text simplification tasks.
- **Mechanism**: Closed-weight models benefit from proprietary training data, alignment strategies, and optimization techniques that are not publicly available, giving them an advantage in generating high-quality simplifications with minimal task-specific examples.
- **Core assumption**: The superiority of closed-weight models is due to factors beyond just model size, including data quality, alignment, and task-specific fine-tuning.
- **Evidence anchors**:
  - [abstract] "Results show that closed-weight models outperform open-weight alternatives, with GPT-3.5-Turbo and Davinci-003 achieving the highest SARI scores on Wikipedia-style data, surpassing even human references."
  - [section 4] "We observe that the majority of the models consistently fail to preserve meaning... The models that do strike a reasonable balance with both SARI and BERTScore are again OpenAI's more powerful offerings and the Flan models."
  - [corpus] Weak - The corpus search found related papers but no direct evidence about training data differences.
- **Break condition**: If open-weight models are trained on similarly high-quality, aligned datasets with comparable optimization strategies, the performance gap may narrow or disappear.

### Mechanism 2
- **Claim**: Instruction-tuned models demonstrate stronger performance in meaning preservation during text simplification.
- **Mechanism**: Instruction tuning adapts pre-trained models to follow natural language instructions, improving their ability to understand and execute task-specific requirements like preserving meaning while simplifying text.
- **Core assumption**: Instruction tuning provides a more effective adaptation strategy for few-shot learning tasks compared to general pre-training alone.
- **Evidence anchors**:
  - [abstract] "Instruction-tuned models demonstrate stronger performance in meaning preservation."
  - [section 4] "We observe that training strategies such as instruction-tuning and RLHF help to deliver greater improvements, especially for meaning preservation, as measured by BERTScore."
  - [section 6] "Within model families, when comparing base models to their instruction fine-tuned counterparts, we observe that instruction-tuning typically leads to better performance in our few-shot ICL setting for TS."
- **Break condition**: If the instruction-tuning dataset does not include simplification-related tasks or if the model architecture is fundamentally incompatible with instruction-following, this advantage may not materialize.

### Mechanism 3
- **Claim**: There exists a trade-off between simplicity and meaning preservation in text simplification, with automatic metrics showing divergent preferences.
- **Mechanism**: Metrics like SARI reward extensive simplification operations (deletion, paraphrasing) while BERTScore favors minimal changes that preserve original meaning, creating an inherent tension between these evaluation objectives.
- **Core assumption**: The trade-off is not just an artifact of specific metrics but reflects a fundamental challenge in text simplification where aggressive simplification often compromises meaning.
- **Evidence anchors**:
  - [abstract] "The study also identifies a trade-off between simplicity and meaning preservation, with automatic metrics like SARI and BERTScore showing divergent preferences."
  - [section 5] "However, there is a clear trade-off between these two axes when we consider the top 5 models according to SARI and BERTScore. This agrees with earlier findings from Schwarzer and Kauchak (2018)."
  - [section 5] "We validate this by studying the relationship between BERTScore (computed between the system output and the reference sentence(s)) and Levenshtein similarity (computed between the system output and the original input sentence). Figure 4 reveals a strong positive correlation across all datasets, indicating that BERTScore tends to reward minimally simplified responses."
- **Break condition**: If new evaluation metrics or model architectures can simultaneously optimize for both simplicity and meaning preservation, or if human evaluation reveals different preferences than automatic metrics suggest.

## Foundational Learning

- **Concept**: In-context learning (ICL)
  - **Why needed here**: The paper evaluates LLMs using few-shot in-context learning rather than fine-tuning, which is crucial for understanding how well models can perform text simplification without task-specific training.
  - **Quick check question**: What is the difference between zero-shot, few-shot, and fine-tuning approaches when adapting LLMs to new tasks?

- **Concept**: Text simplification operations
  - **Why needed here**: Understanding the types of operations (deletion, paraphrasing, splitting, etc.) that models perform is essential for analyzing their simplification strategies and quality.
  - **Quick check question**: What are the key differences between lexical simplification and sentence-level simplification in terms of the operations involved?

- **Concept**: Automatic evaluation metrics for text generation
  - **Why needed here**: The study relies heavily on metrics like SARI, BERTScore, and LENS to evaluate model performance, requiring understanding of their strengths, weaknesses, and what aspects of output quality they measure.
  - **Quick check question**: How does SARI differ from BLEU in terms of what aspects of text simplification it rewards or penalizes?

## Architecture Onboarding

- **Component map**: Dataset loading → Prompt generation with few-shot examples → Model inference (44 LLMs) → Automatic metric computation (SARI, BERTScore, FKGL, LENS) → Edit operation analysis → Manual qualitative analysis
- **Critical path**: For each dataset and model combination: load dataset → generate prompt with few-shot examples → run inference → compute automatic metrics → analyze edit operations → (for subset) perform manual annotation
- **Design tradeoffs**: Few-shot learning vs. fine-tuning limits performance but evaluates out-of-the-box capabilities; 3 prompts and 3 seeds balance robustness with computational cost
- **Failure signatures**: Hallucinations, repetitions, direct copying of input sentences; poor performance on medical domain; low diversity in edit operations (over-reliance on deletion)
- **First 3 experiments**:
  1. Run inference with prompt 2 on ASSET dataset using 3 random seeds for Davinci-003 and compute SARI, BERTScore, and edit operation distributions
  2. Compare edit operation distributions between the best closed-weight model (Davinci-003) and the best open-weight model (Flan-UL2) on MED-EASI dataset
  3. Perform manual annotation on 15 outputs (5 from top SARI, 5 from top BERTScore, 5 from top LENS) to validate automatic metric findings and identify failure modes

## Open Questions the Paper Calls Out

- **Open Question 1**: How do different in-context learning strategies (e.g., number of examples, example selection methods) impact LLM performance on text simplification?
  - **Basis in paper**: [explicit] The paper mentions that few-shot examples are randomly sampled and leaves detailed investigation of optimal in-context learning strategies for future work.
  - **Why unresolved**: The paper only uses a fixed number of 3 examples and random sampling, without exploring the impact of varying these parameters.
  - **What evidence would resolve it**: Systematic experiments varying the number of examples, selection criteria (e.g., similarity to input, diversity), and prompt structure to determine optimal in-context learning strategies.

- **Open Question 2**: Why do some models perform better on meaning preservation but worse on simplicity, and vice versa?
  - **Basis in paper**: [explicit] The paper observes a trade-off between simplicity and meaning preservation, with models ranking high on SARI often having lower BERTScore and vice versa.
  - **Why unresolved**: The paper identifies the trade-off but does not explain the underlying reasons for why models exhibit these different behaviors.
  - **What evidence would resolve it**: Analysis of the specific simplification operations performed by different models and how these relate to the trade-off, potentially through manual analysis of model outputs.

- **Open Question 3**: How do the capabilities of LLMs on text simplification transfer to languages other than English?
  - **Basis in paper**: [explicit] The paper only considers English text simplification datasets and acknowledges this as a limitation.
  - **Why unresolved**: The study is limited to English datasets, so the generalizability to other languages is unknown.
  - **What evidence would resolve it**: Evaluation of the same models on text simplification datasets in other languages to assess cross-lingual performance.

## Limitations
- Reliance on automatic metrics without extensive human evaluation may not fully capture simplification quality
- Few-shot approach likely underperforms compared to fine-tuned models, limiting conclusions about model capabilities
- Focus on sentence-level simplification without exploring document-level or discourse-aware simplification

## Confidence
- **High Confidence**: Closed-weight models outperforming open-weight alternatives on Wikipedia-style data (multiple metrics, consistent across datasets)
- **Medium Confidence**: Trade-off between simplicity and meaning preservation identified through divergent metric preferences (theoretically sound but metric-dependent)
- **Low Confidence**: Claims about closed-weight models "surpassing human references" in SARI scores (requires careful interpretation of metric design)

## Next Checks
1. **Human Evaluation Validation**: Conduct extensive human evaluations on a subset of model outputs to validate automatic metric findings, particularly focusing on meaning preservation, grammatical correctness, and overall quality judgments
2. **Fine-tuning Comparison**: Compare the few-shot in-context learning results with fine-tuned models on the same datasets to establish the performance gap and determine whether current findings underestimate model capabilities
3. **Cross-dataset Generalization Test**: Evaluate the best-performing models from this study on additional text simplification datasets (e.g., Newsela, WikiLarge, or non-English datasets) to assess whether observed performance patterns generalize beyond the three studied domains