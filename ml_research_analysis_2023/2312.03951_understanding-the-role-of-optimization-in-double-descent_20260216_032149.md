---
ver: rpa2
title: Understanding the Role of Optimization in Double Descent
arxiv_id: '2312.03951'
source_url: https://arxiv.org/abs/2312.03951
tags:
- double
- descent
- error
- loss
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified optimization-based explanation for
  when double descent occurs or not. It argues that double descent appears if and
  only if the optimizer can find a sufficiently low-loss minimum.
---

# Understanding the Role of Optimization in Double Descent

## Quick Facts
- **arXiv ID**: 2312.03951
- **Source URL**: https://arxiv.org/abs/2312.03951
- **Reference count**: 40
- **Primary result**: Double descent appears if and only if the optimizer can find a sufficiently low-loss minimum, with poor conditioning and slow convergence reducing or eliminating the peak.

## Executive Summary
This paper proposes a unified optimization-based explanation for when double descent occurs in machine learning models. The authors argue that double descent is observed only when the optimizer can find a sufficiently low-loss minimum, with factors like initialization, normalization, batch size, learning rate, and optimizer choice affecting the condition number and optimization trajectory. Through experiments on random feature models and two-layer neural networks, they demonstrate that poor conditioning and slow convergence settings tend to eliminate the double descent peak, while better conditioning and faster convergence restore it. These findings suggest that double descent is unlikely to be problematic in real-world machine learning applications.

## Method Summary
The paper investigates double descent using random feature models (RFMs) with ReLU activation and two-layer neural networks on MNIST, Fashion-MNIST, and CIFAR-10 datasets. Experiments employ SGD with Nesterov momentum, varying learning rates (1e-3 to 2e-2), batch sizes (8 to full batch), normalization schemes, and optimization algorithms. Initial training runs for 1000-1500 epochs, with extended training to 10,000 epochs for specific experiments. The condition number of feature matrices is computed to measure optimization difficulty, and double descent curves are generated by varying model capacity (number of random features) relative to dataset size.

## Key Results
- Double descent peaks negatively correlate with the condition number of random feature matrices
- Slow-convergence settings (small learning rates, small batch sizes) often reduce or remove double descent peaks
- Extended training (10,000 epochs) can recover double descent in settings where it was initially absent
- Better-conditioned optimization problems and faster-convergence algorithms exhibit stronger double descent behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Double descent occurs if and only if the optimizer can find a sufficiently low-loss minimum.
- Mechanism: The optimizer's ability to find a low-loss minimum is determined by the condition number of the optimization problem. Poor conditioning (high condition number) makes optimization harder, leading to higher training loss and a more pronounced double descent peak. Conversely, good conditioning (low condition number) allows the optimizer to find lower minima, reducing or eliminating the double descent peak.
- Core assumption: The relationship between the condition number and the optimizer's ability to find a low-loss minimum is monotonic and predictable.
- Evidence anchors:
  - [abstract] "model-wise double descent is observed if and only if the optimizer can find a sufficiently low-loss minimum."
  - [section 4] "We observe that the height of the double descent peak negatively correlates with the condition number of the random feature matrices."
  - [corpus] Weak evidence: related papers focus on double descent but do not explicitly address the condition number's role in the optimizer's ability to find low-loss minima.
- Break condition: The relationship between the condition number and the optimizer's ability to find a low-loss minimum is not monotonic or predictable.

### Mechanism 2
- Claim: Slow-convergence settings often reduce or remove the peaking phenomenon, while fast-convergence settings restore it.
- Mechanism: Hyperparameters that affect the optimizer directly, such as learning rate, batch size, and optimization algorithm, influence the convergence speed. Slow-convergence settings lead to higher training loss and a less prominent double descent peak, while fast-convergence settings lead to lower training loss and a more pronounced peak.
- Core assumption: The relationship between convergence speed and the height of the double descent peak is consistent across different hyperparameters and optimization settings.
- Evidence anchors:
  - [abstract] "slow-convergence setting 2 often reduces or removes the peaking phenomenon, and fast-convergence setting (i.e., finding a lower minimum) restores the peaking phenomenon."
  - [section 5] "We observe that a faster-convergence optimization algorithm that finds a lower loss minimum exhibits double descent more strongly, and slow-convergence settings may not exhibit it at all."
  - [corpus] Weak evidence: related papers discuss double descent but do not explicitly address the impact of convergence speed on the peak height.
- Break condition: The relationship between convergence speed and the height of the double descent peak is not consistent across different hyperparameters and optimization settings.

### Mechanism 3
- Claim: Training longer can recover the double descent phenomenon in slow-convergence settings.
- Mechanism: Increasing the number of iterations allows the optimizer to reach a lower training loss, even in slow-convergence settings. This lower training loss corresponds to a more pronounced double descent peak.
- Core assumption: The relationship between the number of iterations and the training loss is monotonic, at least in the initial stages of training.
- Evidence anchors:
  - [abstract] "we can recover this phenomenon simply by running the optimization procedure longer to reach a lower training loss."
  - [section 6] "We show that for hyper-parameter setups that do not exhibit double descent, we can recover this phenomenon simply by running the optimization procedure longer to reach a lower training loss."
  - [corpus] Weak evidence: related papers do not explicitly address the impact of training length on the recovery of double descent in slow-convergence settings.
- Break condition: The relationship between the number of iterations and the training loss is not monotonic, or the training loss plateaus before reaching a sufficiently low value.

## Foundational Learning

- Concept: Condition number
  - Why needed here: The condition number of the optimization problem determines the difficulty of finding a low-loss minimum. Understanding its impact is crucial for explaining the relationship between optimization and double descent.
  - Quick check question: What is the condition number of a matrix, and how does it affect the convergence of gradient descent?

- Concept: Optimization trajectory
  - Why needed here: The optimization trajectory describes the path taken by the optimizer to reach a minimum. Understanding how different hyperparameters influence the trajectory is essential for explaining the impact of slow-convergence settings on double descent.
  - Quick check question: How do learning rate, batch size, and optimization algorithm affect the optimization trajectory?

- Concept: Training loss
  - Why needed here: Training loss is a measure of how well the model fits the training data. Understanding its relationship with the double descent peak is crucial for explaining the impact of optimization on double descent.
  - Quick check question: How does training loss relate to the height of the double descent peak?

## Architecture Onboarding

- Component map: Dataset (MNIST/Fashion-MNIST/CIFAR-10) -> Model (RFM/2-layer NN) -> Optimization (SGD with momentum) -> Hyperparameters (LR, batch size, normalization) -> Training loss/Test error
- Critical path: Feature generation/initialization -> Optimization with chosen hyperparameters -> Convergence to minimum -> Training loss computation -> Double descent peak measurement
- Design tradeoffs: The main tradeoff is between the height of the double descent peak and the ease of optimization. Poor conditioning and slow-convergence settings lead to a more pronounced peak but are harder to optimize, while good conditioning and fast-convergence settings lead to a less pronounced peak but are easier to optimize.
- Failure signatures: Failure signatures include the absence of double descent, a very high or very low double descent peak, and slow convergence or divergence during training.
- First 3 experiments:
  1. Vary the condition number of the random feature matrix by changing the scale of the features or the initialization of the random matrix, and observe the impact on the double descent peak.
  2. Vary the learning rate and observe the impact on the double descent peak and the convergence speed.
  3. Vary the batch size and observe the impact on the double descent peak and the convergence speed.

## Open Questions the Paper Calls Out

- Under what specific conditions does a low-loss minimum fail to be found by the optimizer, even when sufficient training iterations are provided?
- How do the dynamics of double descent change when using optimization algorithms with adaptive learning rates (like Adam) compared to SGD with momentum?
- Does the relationship between condition number and double descent generalize to non-convex loss landscapes beyond two-layer networks?

## Limitations
- Experimental validation limited to simple models (random features and two-layer networks) rather than deeper architectures
- Lack of theoretical grounding for the condition number-optimization relationship in non-convex neural network landscapes
- Claims about practical implications extend beyond what experiments on simplified settings directly support

## Confidence
- **High confidence**: The correlation between poor conditioning and reduced double descent peaks, the ability to recover peaks through longer training, and the consistent patterns across different experimental settings.
- **Medium confidence**: The claim that double descent occurs if and only if the optimizer finds sufficiently low-loss minima, as this is based on empirical observation rather than theoretical proof.
- **Low confidence**: The practical implications for real-world ML, as the paper's experiments use simplified settings that may not capture all complexities of practical training.

## Next Checks
1. Test the condition number hypothesis on deeper networks (3+ layers) with different architectures to verify if the optimization-difficulty explanation holds beyond two-layer models.
2. Systematically vary the initialization scale and measure the exact relationship between initialization magnitude, condition number, and double descent peak height across a wider range of hyperparameters.
3. Apply the same experimental protocol to real-world datasets with convolutional architectures to assess whether the practical implications claimed in the paper hold true for modern deep learning applications.