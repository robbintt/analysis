---
ver: rpa2
title: 'Hieros: Hierarchical Imagination on Structured State Space Sequence World
  Models'
arxiv_id: '2310.05167'
source_url: https://arxiv.org/abs/2310.05167
tags:
- world
- state
- arxiv
- learning
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hieros addresses sample efficiency in deep reinforcement learning
  by combining hierarchical policies with structured state space sequence world models.
  The method uses S5 layers for parallel training and iterative imagination, enabling
  more efficient learning than RNN or Transformer-based approaches.
---

# Hieros: Hierarchical Imagination on Structured State Space Sequence World Models

## Quick Facts
- **arXiv ID**: 2310.05167
- **Source URL**: https://arxiv.org/abs/2310.05167
- **Authors**: [List of authors from paper]
- **Reference count**: 28
- **Key outcome**: Achieves state-of-the-art performance on Atari 100k benchmark with superior sample efficiency using hierarchical policies and S5-based world models

## Executive Summary
Hieros introduces a hierarchical reinforcement learning approach that combines structured state space sequence models with multi-level policy architectures to achieve unprecedented sample efficiency on the challenging Atari 100k benchmark. The method uses S5 layers for parallel world model training and iterative imagination, enabling efficient learning that outperforms RNN and Transformer-based approaches. By employing hierarchical policies where each layer learns its own world model, actor-critic, and subgoal autoencoder, Hieros achieves superior exploration capabilities and performance across complex, multi-level environments while requiring fewer computational resources than existing methods.

## Method Summary
Hieros employs a multi-level hierarchical structure where each layer learns its own world model, actor-critic, and subgoal autoencoder, with goals passed between levels to guide exploration. The core innovation is the use of S5 layers for the world model, which enables parallel training during the learning phase and iterative prediction during environment interaction. An efficient time-balanced sampling method ensures uniform experience replay, and the entire system trains entirely in latent imagination space without direct environment interaction. This architecture allows for time-abstracted world representations and multi-scale trajectory imagination, significantly improving sample efficiency compared to single-level approaches.

## Key Results
- Achieves state-of-the-art mean and median normalized human scores on Atari 100k benchmark
- Outperforms existing methods including DreamerV3 while requiring fewer computational resources
- Demonstrates superior exploration capabilities particularly in complex, multi-level environments
- Excels at tasks requiring long-term planning and hierarchical decision-making

## Why This Works (Mechanism)

### Mechanism 1
- Claim: S5 layers enable parallel training and iterative imagination, making the world model more efficient than RNN or Transformer-based alternatives.
- Mechanism: S5 layers use a structured state space representation that allows parallel computation of sequence predictions during training while maintaining the ability to perform iterative autoregressive predictions during environment interaction. This dual capability reduces computational overhead compared to RNNs (which are inherently sequential during training) and Transformers (which are computationally expensive due to attention mechanisms).
- Core assumption: The parallel computation capability of S5 layers does not significantly compromise prediction accuracy compared to sequential models.
- Evidence anchors:
  - [abstract]: "Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction."
  - [section 2.2]: "S5 layers are advantageous over S4 layers as they can model longer term dependencies and are more efficient in terms of memory usage."
- Break condition: If the parallel computation introduces significant prediction errors or if the iterative imagination becomes computationally prohibitive in complex environments.

### Mechanism 2
- Claim: Hierarchical policies with multiple abstraction layers improve sample efficiency by learning time-abstracted world representations and imagining trajectories at multiple time scales.
- Mechanism: Each hierarchy level learns its own world model, actor-critic, and subgoal autoencoder. Higher levels propose subgoals to lower levels, which are kept constant for k steps. This allows the agent to plan at different time scales and explore more efficiently by abstracting away fine-grained details at higher levels.
- Core assumption: The subgoal representation learned by higher levels effectively captures meaningful progress indicators that lower levels can use to navigate the environment.
- Evidence anchors:
  - [abstract]: "Hieros employs a multi-level hierarchical structure where each layer learns its own world model, actor-critic, and subgoal autoencoder, with goals passed between levels to guide exploration."
  - [section 2.1]: "The subgoals represent world states that the lower layer is tasked to achieve and are kept constant for the next k steps of the lower layer."
- Break condition: If subgoals become too abstract to provide actionable guidance or if the communication overhead between levels outweighs the benefits of temporal abstraction.

### Mechanism 3
- Claim: Efficient time-balanced sampling (ETBS) improves learning by providing true uniform sampling over the experience dataset with O(1) time complexity.
- Mechanism: ETBS computes the cumulative distribution function of the sampling bias that occurs with iterative uniform sampling and uses probability integral transformation to correct it. This ensures that older experiences are not oversampled relative to newer ones, leading to more balanced training.
- Core assumption: The approximated CDF based on harmonic numbers provides a sufficiently accurate correction for the sampling bias.
- Evidence anchors:
  - [section 2.3]: "We propose an efficient time-balanced sampling method (ETBS), which produces a similar sampling distribution with O(1) time complexity."
  - [section 2.3]: "The idea is, to compute the CDF of this probability distribution between 0 and n to transform samples from the imbalanced distribution into uniform samples via probability integral transformation."
- Break condition: If the approximation of the harmonic number introduces significant sampling errors or if the temperature hyperparameter τ is not properly tuned for the specific environment.

## Foundational Learning

- Concept: World models in reinforcement learning
  - Why needed here: Hieros relies on learning accurate world models to train agents entirely in imagination, eliminating the need for direct environment interaction during training.
  - Quick check question: How does a world model differ from a policy network in reinforcement learning?

- Concept: Hierarchical reinforcement learning
  - Why needed here: The hierarchical structure allows Hieros to learn and make decisions across multiple levels of abstraction, improving exploration and sample efficiency.
  - Quick check question: What is the role of subgoals in hierarchical reinforcement learning?

- Concept: Structured state space sequence models (S4/S5)
  - Why needed here: S5 layers form the backbone of Hieros' world model, providing efficient parallel training and iterative imagination capabilities.
  - Quick check question: What are the key differences between S4, S5, and traditional RNN or Transformer architectures?

## Architecture Onboarding

- Component map:
  - Environment -> S5WM (parallel training) -> Hierarchical policy (multiple layers) -> Subgoal autoencoder -> ETBS sampling -> Actor-critic training

- Critical path:
  1. Environment interaction at lowest level
  2. Experience collection and storage
  3. ETBS sampling from experience dataset
  4. Parallel training of S5WM using sampled trajectories
  5. Subgoal proposal and communication between hierarchy levels
  6. Actor-critic training on imagined trajectories
  7. Policy execution at lowest level

- Design tradeoffs:
  - Hierarchical structure vs. communication overhead between levels
  - S5 layers vs. RNN/Transformer for world model accuracy and efficiency
  - Subgoal abstraction level vs. actionable guidance for lower levels
  - Sampling temperature τ vs. balance between new and old experiences

- Failure signatures:
  - Poor performance on games with simple, stable dynamics (e.g., Breakout, Pong)
  - Subgoals that do not provide meaningful guidance to lower levels
  - World model that fails to predict complex dynamics accurately
  - Sampling bias leading to oversampling of older experiences

- First 3 experiments:
  1. Replace S5WM with RSSM and compare performance on Krull, Breakout, Battle Zone, and Freeway
  2. Vary hierarchy depth (1, 2, 3, 4 layers) and measure impact on sample efficiency and exploration
  3. Compare uniform sampling vs. ETBS and measure impact on world model accuracy and policy performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Hieros change when using a smaller S5WM instead of the current larger model?
- Basis in paper: [inferred] The paper mentions that the S5WM performs worse than the RSSM on Breakout and Pong, which have simple dynamics and stable input distributions. It also mentions that a smaller S5WM might achieve better results for Breakout.
- Why unresolved: The paper only conducts experiments with the current larger S5WM and does not explore the performance of a smaller version.
- What evidence would resolve it: Conduct experiments with different sizes of S5WM and compare their performance on games with simple dynamics and stable input distributions.

### Open Question 2
- Question: How does the performance of Hieros change when using a hybrid model that combines S4/S5-based models and Transformer-based models?
- Basis in paper: [inferred] The paper mentions that S4/S5-based models and Transformer-based models show complementary strengths in regard to short and long-term memory recall. It also mentions that many hybrid models have been proposed and exploring these models might be a promising direction for future research.
- Why unresolved: The paper does not conduct any experiments with hybrid models and only explores the performance of S5WM and RSSM.
- What evidence would resolve it: Conduct experiments with hybrid models that combine S4/S5-based models and Transformer-based models and compare their performance on various tasks.

### Open Question 3
- Question: How does the performance of Hieros change when using a S5-based actor network instead of the current single-step actor network?
- Basis in paper: [inferred] The paper mentions that using the S5-based actor network proposed by Lu et al. (2023) would allow for a multistep imagination procedure, which could further improve the performance of Hieros. It also mentions that this would open the door to more efficient look-ahead search methods.
- Why unresolved: The paper does not conduct any experiments with the S5-based actor network and only explores the performance of the current single-step actor network.
- What evidence would resolve it: Conduct experiments with the S5-based actor network and compare its performance with the current single-step actor network on various tasks.

## Limitations
- Performance degrades on games with simple, stable dynamics where hierarchical complexity introduces unnecessary overhead
- Implementation requires precise configuration of S5 layers and hierarchical communication mechanisms
- Computational resources needed for parallel training may be prohibitive for some research settings

## Confidence
**High Confidence Claims:**
- The S5 world model provides efficient parallel training and iterative imagination capabilities compared to RNNs and Transformers
- Hierarchical structure with multiple abstraction layers improves sample efficiency through time-abstracted representations
- The time-balanced sampling method effectively addresses the oversampling of older experiences

**Medium Confidence Claims:**
- Subgoal representation learned by higher levels provides meaningful guidance to lower levels across all Atari games
- The communication overhead between hierarchy levels does not outweigh the benefits of temporal abstraction

**Low Confidence Claims:**
- Performance generalizes to continuous control tasks and real-world robotics scenarios

## Next Checks
1. **Cross-Domain Generalization Test:** Evaluate Hieros performance on continuous control benchmarks (e.g., DM Control Suite) to assess generalization beyond discrete action spaces in Atari games.

2. **Ablation Study on Hierarchy Depth:** Systematically test hierarchy configurations with 1, 2, 3, and 4 layers on the same set of games to quantify the impact of depth on sample efficiency and computational overhead.

3. **World Model Accuracy Analysis:** Compare the predictive accuracy of the S5-based world model against RNN and Transformer baselines on environments with varying complexity levels to validate the claimed efficiency gains.