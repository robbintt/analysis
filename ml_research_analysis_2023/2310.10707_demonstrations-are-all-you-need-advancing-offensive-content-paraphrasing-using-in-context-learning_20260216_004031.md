---
ver: rpa2
title: 'Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing
  using In-Context Learning'
arxiv_id: '2310.10707'
source_url: https://arxiv.org/abs/2310.10707
tags:
- demos
- instruction
- toxicity
- demonstrations
- bleu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using In-Context Learning (ICL) with large language
  models to paraphrase offensive content into less offensive alternatives while preserving
  meaning. This addresses the limitation of supervised methods requiring large training
  data and retaining significant toxicity.
---

# Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning

## Quick Facts
- **arXiv ID**: 2310.10707
- **Source URL**: https://arxiv.org/abs/2310.10707
- **Reference count**: 40
- **Primary result**: ICL achieves comparable generation quality to supervised methods on metrics like BLEU but with 76% lower toxicity

## Executive Summary
This paper proposes using In-Context Learning (ICL) with large language models to paraphrase offensive content into less offensive alternatives while preserving meaning. The approach addresses the limitations of supervised methods that require large training data and often retain significant toxicity. By providing a few demonstration pairs of offensive/inoffensive sentences as context, the LLM can generate appropriate paraphrases for new queries. The method achieves comparable generation quality to supervised approaches while significantly reducing toxicity, demonstrating robustness even with limited training data.

## Method Summary
The approach uses In-Context Learning where a few demonstration pairs (offensive/inoffensive sentences) are provided as context to guide LLM paraphrasing. Key factors explored include the number of demonstrations (0-40), selection and ordering strategies (random, least similar, most similar with ascending/descending order), and presence/absence of instruction prompts. The method was tested on three datasets (APPDIA, ParaDetox, CAPP) using models including text-davinci-003, gpt-3.5-turbo, and Vicuna-13b. Evaluation combined automated metrics (BLEU, BERT-F1, ROUGE, CIDEr) with toxicity measurement using Detoxify and human evaluation.

## Key Results
- ICL achieves comparable generation quality to supervised methods on BLEU scores while reducing toxicity by 76%
- Carefully selecting and ordering demonstrations by semantic similarity to the query improves performance
- Removing the instruction prompt achieves similar quality but higher toxicity compared to using instructions
- The approach remains robust to limited training data, showing only slight performance decrease even with 10% of the data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL with carefully selected and ordered demonstrations significantly reduces toxicity while maintaining paraphrase quality
- Mechanism: By selecting demonstrations semantically similar to the query and arranging them in descending order of similarity, the LLM generates paraphrases that closely match gold standards while inheriting less offensive styles
- Core assumption: The LLM can generalize from semantically relevant demonstrations to produce high-quality, less toxic paraphrases
- Evidence anchors:
  - [abstract] "ICL is comparable to supervised methods in generation quality, while being qualitatively better by 25% on human evaluation and attaining lower toxicity by 76%"
  - [section 3.2] "Curating the demos in decreasing order of similarity often results in better BLEU"
  - [corpus] Weak - corpus only provides related work titles

### Mechanism 2
- Claim: Removing instruction prompts achieves similar quality but higher toxicity
- Mechanism: Demonstrations alone provide task context, but without instruction as toxicity regularizer, generated paraphrases retain more offensive elements
- Core assumption: Instruction prompts act as toxicity regularizers
- Evidence anchors:
  - [abstract] "Removing the instruction prompt and relying only on demonstrations achieves similar quality but higher toxicity"
  - [section 3.3] "Certain instructions can result in lower BLEU than prompts with no instruction"
  - [corpus] Weak - corpus only provides related work titles

### Mechanism 3
- Claim: ICL is robust to limited training data with only slight performance reduction at 10% data
- Mechanism: Few-shot learning capabilities allow adaptation with small demonstration sets
- Core assumption: LLM's ability to learn from few examples extends to offensive content paraphrasing
- Evidence anchors:
  - [abstract] "ICL-based paraphrasers only show a slight reduction in performance even with just 10% training data"
  - [section 3.6] "Only minimal fall in BLEU up to 10% of the training data"
  - [corpus] Weak - corpus only provides related work titles

## Foundational Learning

- **Concept**: Semantic similarity and cosine similarity in vector embeddings
  - Why needed here: To select and order demonstrations most relevant to the query
  - Quick check question: Given two sentences, how would you compute their semantic similarity using cosine similarity on their vector embeddings?

- **Concept**: In-Context Learning (ICL) and few-shot learning
  - Why needed here: To leverage LLM's ability to learn from small example sets without large training data
  - Quick check question: What is the difference between zero-shot, one-shot, and few-shot learning in ICL?

- **Concept**: Toxicity detection and evaluation metrics
  - Why needed here: To measure effectiveness of paraphrasing in reducing offensiveness
  - Quick check question: What are common automated metrics for evaluating text toxicity, and how do they differ from human evaluation?

## Architecture Onboarding

- **Component map**: Datasets (APPDIA, ParaDetox, CAPP) -> Prompt Construction (instruction + demonstrations) -> LLM (text-davinci-003, gpt-3.5-turbo, Vicuna-13b) -> Evaluation (BLEU, BERT-F1, ROUGE, CIDEr, toxicity scores)

- **Critical path**: Prompt construction with instruction and demonstrations → LLM paraphrase generation → Automated evaluation using BLEU, BERT-F1, ROUGE, CIDEr, and toxicity measurement

- **Design tradeoffs**: Number and quality of demonstrations vs. toxicity of generated paraphrases; more demonstrations may improve quality but increase toxicity

- **Failure signatures**: Poor semantic similarity between demonstrations and queries; unclear or ineffective instructions; LLM inability to preserve meaning while reducing toxicity

- **First 3 experiments**:
  1. Test impact of different numbers of demonstrations on quality and toxicity
  2. Compare performance of demonstrations with and without instruction prompts
  3. Evaluate robustness to limited training data by varying demonstration amounts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ICL effectiveness vary across different types of offensive content (hate speech vs. rudeness vs. toxicity)?
- Basis in paper: [explicit] Paper tests on three datasets with different offensive content types but doesn't systematically analyze differences in ICL effectiveness
- Why unresolved: Paper compares performance across datasets but doesn't explicitly analyze how ICL effectiveness varies with content type
- What evidence would resolve it: Systematic analysis of ICL performance across different offensive content categories using a more granular taxonomy

### Open Question 2
- Question: What is the optimal balance between instruction and demonstrations in ICL prompts?
- Basis in paper: [explicit] Paper discusses impact of removing instruction but doesn't explore optimal balance between instruction and demonstrations
- Why unresolved: While both instruction and demonstrations are shown to be important, optimal balance for maximizing quality while minimizing toxicity is not explored
- What evidence would resolve it: Experiments varying instruction and demonstration amounts to find optimal balance

### Open Question 3
- Question: How does training dataset size impact ICL effectiveness?
- Basis in paper: [explicit] Paper shows robustness to limited data but doesn't explore detailed impact of different training dataset sizes
- Why unresolved: While robustness to limited data is demonstrated, detailed analysis of how different dataset sizes impact effectiveness is missing
- What evidence would resolve it: Experiments varying training dataset size to observe impact on ICL effectiveness

## Limitations

- **Prompt Engineering Sensitivity**: Performance varies significantly based on prompt construction, but exact formulations are not fully specified
- **Data Quality and Distribution**: Datasets were likely created with automated assistance, creating potential circularity issues in evaluation
- **Toxicity Measurement Reliability**: Reliance on Detoxify may not align with human judgment of offensiveness, with significant discrepancies between automated and human evaluation scores

## Confidence

**High Confidence**:
- ICL can generate paraphrases with comparable quality to supervised methods (BLEU scores)
- ICL achieves significantly lower toxicity scores than supervised baselines (76% reduction)
- Demonstration selection and ordering based on semantic similarity improves performance

**Medium Confidence**:
- Human evaluation confirms qualitative superiority of ICL approach (25% better)
- Performance remains robust with limited training data (only slight degradation at 10% data)
- Removal of instruction prompts affects toxicity but not quality metrics

**Low Confidence**:
- Specific prompt engineering choices leading to optimal performance
- Generalizability to truly offensive content not present in training data
- Correlation between automated toxicity scores and actual human perceptions

## Next Checks

1. **Blind Human Evaluation on Unseen Content**: Conduct human evaluation on offensive content not present in any training datasets, including content from different domains and time periods, to verify generalizability beyond specific datasets

2. **Cross-Model Transfer Testing**: Test whether demonstrations optimized for one LLM (e.g., text-davinci-003) transfer effectively to other models (gpt-3.5-turbo, Vicuna-13b), or whether each model requires separate demonstration optimization

3. **Ablation Study on Demonstration Quality**: Systematically vary the quality and toxicity levels of demonstrations themselves to determine whether ICL amplifies or mitigates poor-quality examples, establishing robustness to demonstration selection errors