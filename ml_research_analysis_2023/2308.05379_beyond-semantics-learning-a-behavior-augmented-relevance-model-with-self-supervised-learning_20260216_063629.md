---
ver: rpa2
title: 'Beyond Semantics: Learning a Behavior Augmented Relevance Model with Self-supervised
  Learning'
arxiv_id: '2308.05379'
source_url: https://arxiv.org/abs/2308.05379
tags:
- learning
- behavior
- relevance
- target
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BARL-ASe, a behavior-augmented relevance model
  that leverages neighbor queries and items to enhance target query-item semantic
  matching. The model employs multi-level co-attention to distill coarse-grained and
  fine-grained semantic representations, and utilizes neighbor-target self-supervised
  learning to improve accuracy and robustness.
---

# Beyond Semantics: Learning a Behavior Augmented Relevance Model with Self-supervised Learning

## Quick Facts
- **arXiv ID**: 2308.05379
- **Source URL**: https://arxiv.org/abs/2308.05379
- **Reference count**: 40
- **Primary result**: 2.46% gains in AUC and best F1 and FNR on evaluation dataset compared to deployable models

## Executive Summary
This paper addresses the limitation of semantic-only approaches in e-commerce search relevance modeling by proposing BARL-ASe, a behavior-augmented relevance model. The model leverages neighbor queries and items from user historical behavior data to enhance target query-item semantic matching. By employing multi-level co-attention to distill coarse-grained and fine-grained semantic representations and utilizing neighbor-target self-supervised learning, BARL-ASe achieves superior performance with low latency. The approach is validated through extensive experiments on real-world industry data and online A/B testing, demonstrating effectiveness in improving relevance modeling accuracy.

## Method Summary
The proposed BARL-ASe model consists of four key components: (1) multi-level co-attention to learn fine-grained representations from both neighbor and target views, (2) neighbor-target contrastive learning to enhance representations, (3) neighbor-target mutual learning to align representations and logits from dual views, and (4) a gating network to combine neighbor-dependent and neighbor-independent models for optimal performance. The model is trained using a hybrid objective function that combines relevance modeling and self-supervised learning tasks, allowing it to effectively leverage both semantic information and user behavior data for improved relevance modeling.

## Key Results
- BARL-ASe achieves 2.46% gains in AUC compared to deployable models
- The model achieves the best F1-score and FNR on the evaluation dataset
- Online A/B testing demonstrates effectiveness in real-world deployment with low latency (13ms average)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-level co-attention captures both coarse-grained global and fine-grained local semantic representations from dual behavior neighbors (queries and items).
- Mechanism: The model first pools neighbor representations to obtain global semantics, then applies co-attention at both neighbor-level and target-level to learn complementary representations.
- Core assumption: Behavior neighbors contain semantically relevant information that complements the target query-item pair, and co-attention can effectively fuse this information at multiple granularities.
- Evidence anchors: [abstract] "Specifically, our model builds multi-level co-attention for distilling coarse-grained and fine-grained semantic representations from both neighbor and target views."

### Mechanism 2
- Claim: Neighbor-target contrastive learning aligns representations between actual semantics and potential intent revealed by behavior neighbors.
- Mechanism: The model uses InfoNCE-based contrastive learning to maximize agreement between target query/item representations and their corresponding neighbor representations while minimizing agreement with negative samples.
- Core assumption: The semantic meaning of target queries/items should be consistent with the collective intent expressed through their behavior neighbors, and contrastive learning can effectively capture this alignment.
- Evidence anchors: [section 4.2.1] "we follow SimCLR [2] and adopt InfoNCE to maximize the agreement of positive pairs and minimize that of negative pairs"

### Mechanism 3
- Claim: Neighbor-target mutual learning enables view-level alignment by leveraging complementary information from neighbor-dependent and neighbor-independent models.
- Mechanism: The model employs a mutual learning framework where logits from neighbor-based and semantic-based views are aligned through a loss function that minimizes their squared difference.
- Core assumption: Neighbor-dependent and neighbor-independent models capture different aspects of relevance, and their combination through mutual learning can produce more robust predictions than either approach alone.
- Evidence anchors: [section 4.2.2] "Apart from the aforementioned neighbor-target contrastive learning, we perform mutual learning between dual views with the perspective of logits"

## Foundational Learning

- **Concept**: Transformer-based encoding with shared parameters for self-attention and separate feed-forward networks for different behavior neighbors
  - Why needed here: Allows efficient encoding while maintaining neighbor-specific information
  - Quick check question: Why would sharing self-attention parameters but using separate feed-forward parameters for different behavior neighbors be beneficial for this task?

- **Concept**: InfoNCE contrastive learning framework
  - Why needed here: Maximizes agreement between positive pairs (target and neighbor representations) while minimizing agreement with negative samples
  - Quick check question: What role does the temperature parameter τ play in the InfoNCE loss formulation, and how would changing it affect the learned representations?

- **Concept**: Multi-task learning with weighted combination of main task and self-supervised objectives
  - Why needed here: Combines cross-entropy loss for relevance prediction with self-supervised contrastive and mutual learning losses
  - Quick check question: How would you determine appropriate values for λ1 and λ2 in the hybrid objective function?

## Architecture Onboarding

- **Component map**: Input → Transformers (shared self-attention, separate FFNs) → Pooling → Multi-level Co-attention (neighbor-level + target-level) → Neighbor-target Contrastive Learning → Neighbor-target Mutual Learning → Gating Network → Final Relevance Score
- **Critical path**: Target query/item → Transformer encoding → Co-attention → Prediction head → Gating → Final score
- **Design tradeoffs**: Balances between neighbor-dependent accuracy (requires sufficient neighbor data) and neighbor-independent efficiency (for long-tail cases), while trading off between representation learning (contrastive) and logit alignment (mutual learning)
- **Failure signatures**: Poor AUC/F1 on neighbor-aware data but good performance on neighbor-independent data suggests issues with neighbor representation quality; degraded performance when behavior neighbors are removed indicates over-reliance on neighbor information
- **First 3 experiments**:
  1. Ablation study removing each key component (co-attention levels, contrastive learning, mutual learning) to verify their individual contributions
  2. Hyperparameter sensitivity analysis for λ1, λ2, and τ to find optimal values
  3. Performance comparison on neighbor-aware vs neighbor-independent subsets to validate the long-tail handling strategy

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- Study relies on proprietary industry datasets from Alibaba, limiting independent verification of results
- Does not adequately address potential bias introduced by the neighbor sampling strategy
- Computational efficiency claims (lower latency) are stated but not rigorously compared against baseline models

## Confidence

- **High confidence**: Core architectural contribution (multi-level co-attention with contrastive and mutual learning) as mathematical formulations are clearly specified
- **Medium confidence**: Claimed performance improvements due to lack of ablation studies isolating component contributions
- **Low confidence**: Generalizability of results beyond the specific e-commerce domain studied

## Next Checks

1. Conduct an ablation study removing each component (neighbor-level co-attention, target-level co-attention, contrastive learning, and mutual learning) to quantify their individual contributions to the final performance.

2. Perform cross-domain validation by testing the model on search datasets from different industries (e.g., web search, document retrieval) to assess generalizability beyond e-commerce.

3. Implement rigorous fairness analysis to measure how the neighbor-based approach affects representation of long-tail queries and niche products compared to traditional semantic-only methods.