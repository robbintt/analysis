---
ver: rpa2
title: Latent Diffusion Counterfactual Explanations
arxiv_id: '2310.06668'
source_url: https://arxiv.org/abs/2310.06668
tags:
- counterfactual
- diffusion
- image
- explanations
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LDCE is a model- and dataset-agnostic counterfactual explanation
  method leveraging latent diffusion models with a novel consensus guidance mechanism.
  It filters adversarial gradients using the implicit classifier of class- or text-conditional
  diffusion models to ensure semantic meaningfulness.
---

# Latent Diffusion Counterfactual Explanations

## Quick Facts
- arXiv ID: 2310.06668
- Source URL: https://arxiv.org/abs/2310.06668
- Authors: 
- Reference count: 34
- Key outcome: LDCE achieves FID scores as low as 21.0 (vs. 84.5 for ACE) and improves S3 scores by up to 0.22 on ImageNet and CelebA-HQ

## Executive Summary
LDCE is a model- and dataset-agnostic counterfactual explanation method that leverages latent diffusion models with a novel consensus guidance mechanism. By operating in a lower-dimensional latent space and filtering adversarial gradients using the implicit classifier of class- or text-conditional diffusion models, LDCE generates semantically meaningful counterfactual explanations that outperform prior methods like ACE and DVCE. The method successfully identifies and resolves model errors through targeted finetuning, demonstrating practical utility for understanding black-box model decisions.

## Method Summary
LDCE generates counterfactual explanations by first encoding the factual image to latent space, then applying a forward diffusion process to add noise. During the reverse denoising process, a consensus guidance mechanism filters out adversarial gradients that are misaligned with the gradients of the diffusion model's implicit classifier, ensuring semantic meaningfulness. The method operates in the latent space of an autoencoder to focus on important semantic details while expediting generation. Finally, the latent counterfactual is decoded back to pixel space. LDCE can be applied to any classifier independent of learning paradigm and demonstrates extensive domain coverage limited only by the foundation model's data coverage.

## Key Results
- LDCE achieves FID scores as low as 21.0 compared to 84.5 for ACE on ImageNet and CelebA-HQ
- Improves S3 scores by up to 0.22 over baseline methods
- Successfully identifies and resolves misclassification patterns in ResNet-50 through targeted finetuning
- Demonstrates superior performance on validity, closeness, and realism metrics compared to ACE and DVCE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LDCE achieves high-quality counterfactual explanations by leveraging latent diffusion models with a consensus guidance mechanism that filters out adversarial gradients.
- Mechanism: The method operates in a lower-dimensional latent space, decoupling semantic from pixel-level details. It uses the implicit classifier of diffusion models as a reference to filter gradients that are not aligned with semantically meaningful changes.
- Core assumption: The implicit classifier gradients of the diffusion model are reliable indicators of semantic meaningfulness and can effectively filter out adversarial perturbations.
- Evidence anchors:
  - [abstract] "LDCE harnesses the capabilities of recent class- or text-conditional foundation latent diffusion models to expedite counterfactual generation and focus on the important, semantic parts of the data."
  - [section] "Our novel consensus guidance mechanism ensures semantically meaningful changes during the reverse diffusion process by leveraging the implicit classifier (Ho & Salimans, 2022) of class- or text-conditional foundation diffusion models as a filter."
- Break condition: If the implicit classifier of the diffusion model is not well-aligned with semantic meaningfulness or is biased, the consensus guidance mechanism would fail to filter out adversarial gradients effectively.

### Mechanism 2
- Claim: LDCE is model- and dataset-agnostic, allowing application across diverse models, datasets, and learning paradigms.
- Mechanism: By using foundation diffusion models that are trained on large-scale datasets, LDCE can generate counterfactuals for any classifier without requiring the classifier or diffusion model to be trained on the same data.
- Core assumption: The domain coverage of the foundation diffusion model is sufficiently broad to handle the diversity of target models and datasets.
- Evidence anchors:
  - [abstract] "LDCE is the first counterfactual approach that can be applied to any classifier; independent of the learning paradigm (e.g., supervised or self-supervised) and with extensive domain coverage (as wide as the foundation model's data coverage)"
  - [section] "The foundational nature of the text-conditional (latent) diffusion model grants LDCE the versatility to be applied across diverse models, datasets (within reasonable bounds), and learning paradigms"
- Break condition: If the target model's data distribution is significantly different from what the foundation diffusion model was trained on, the generated counterfactuals may not be semantically meaningful or realistic.

### Mechanism 3
- Claim: LDCE achieves superior performance compared to previous methods in terms of validity, closeness, and realism of counterfactual explanations.
- Mechanism: By operating in the latent space and using the consensus guidance mechanism, LDCE generates counterfactuals that are valid (flip the classifier's prediction), close to the original image, and realistic.
- Core assumption: The latent space of the autoencoder is perceptually equivalent to the pixel space, and the diffusion model can effectively denoise and generate realistic images in this space.
- Evidence anchors:
  - [abstract] "LDCE outperforms prior methods like ACE and DVCE on ImageNet and CelebA-HQ, achieving FID scores as low as 21.0 (vs. 84.5 for ACE), and improves S3 scores by up to 0.22."
  - [section] "We observe that LDCE-txt can introduce local changes, e.g., see Figure 1(b), as well as global modifications, e.g., see Figure 1(a)."
- Break condition: If the autoencoder's latent space is not perceptually equivalent to the pixel space, or if the diffusion model is not capable of generating high-quality images, the performance of LDCE would degrade.

## Foundational Learning

- Concept: Diffusion models
  - Why needed here: LDCE relies on diffusion models for generating counterfactual explanations in the latent space.
  - Quick check question: How do diffusion models gradually denoise a noisy image to generate a clean one?

- Concept: Latent space
  - Why needed here: LDCE operates in the latent space of an autoencoder to decouple semantic from pixel-level details.
  - Quick check question: What is the advantage of working in a lower-dimensional latent space compared to the pixel space?

- Concept: Adversarial gradients
  - Why needed here: LDCE aims to filter out adversarial gradients that lead to semantically meaningless changes in counterfactual explanations.
  - Quick check question: Why are adversarial gradients problematic for generating semantically meaningful counterfactual explanations?

## Architecture Onboarding

- Component map: Encoder (E) and Decoder (D) of the autoencoder -> Latent diffusion model (class- or text-conditional) -> Target classifier (f) -> Consensus guidance mechanism -> Distance function (d) -> "Where" function (Ï•)

- Critical path: 1. Encode the factual image to latent space 2. Add noise through forward diffusion 3. Guide the denoising process using consensus guidance 4. Decode the latent counterfactual to pixel space

- Design tradeoffs:
  - Operating in latent space vs. pixel space: latent space allows focusing on semantic details but may introduce reconstruction artifacts
  - Consensus guidance vs. no guidance: consensus guidance ensures semantic meaningfulness but adds computational overhead
  - Class-conditional vs. text-conditional diffusion models: class-conditional models may be more precise but text-conditional models offer more flexibility

- Failure signatures:
  - Blurry or distorted counterfactual images: could indicate issues with the diffusion model or autoencoder
  - Counterfactuals that are too far from the original image: could indicate incorrect weighting of the distance function
  - Counterfactuals that do not flip the classifier's prediction: could indicate issues with the consensus guidance mechanism or classifier

- First 3 experiments:
  1. Generate counterfactuals for a simple classifier (e.g., MNIST) using a pre-trained latent diffusion model to verify the basic functionality.
  2. Compare the quality of counterfactuals generated with and without the consensus guidance mechanism to assess its impact.
  3. Test the method on a diverse set of classifiers and datasets to verify its model- and dataset-agnostic nature.

## Open Questions the Paper Calls Out
- How can we achieve both high realism and high validity in counterfactual explanations without trade-offs?
- Can the consensus guidance mechanism be extended to other diffusion-based generation tasks beyond counterfactual explanations?
- What is the optimal balance between diffusion steps and computational efficiency for practical applications?

## Limitations
- Performance heavily depends on the quality and domain coverage of the underlying foundation diffusion model
- Consensus guidance mechanism introduces additional computational overhead compared to simpler methods
- Method's reliance on high-quality pre-trained diffusion models limits applicability in resource-constrained environments

## Confidence
- **High Confidence**: The model- and dataset-agnostic nature of LDCE, demonstrated through consistent performance across multiple datasets and diverse model architectures
- **Medium Confidence**: The superiority of consensus guidance mechanism, as the paper provides strong quantitative results but doesn't extensively explore alternative gradient filtering approaches
- **Medium Confidence**: The practical utility of insights gained from counterfactual explanations, as while the paper demonstrates successful error identification and correction in ResNet-50, this was tested on a single case

## Next Checks
1. Apply LDCE to a classifier trained on a domain significantly different from ImageNet (e.g., medical imaging or satellite imagery) to verify the method's robustness to distribution shifts in the foundation model's training data.

2. Implement and compare against a version of LDCE without the consensus guidance mechanism to quantify the exact contribution of gradient filtering to overall performance.

3. Conduct a user study with domain experts to evaluate whether the counterfactual explanations provide actionable insights beyond what simpler methods offer.