---
ver: rpa2
title: Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural
  Networks
arxiv_id: '2308.09858'
source_url: https://arxiv.org/abs/2308.09858
tags:
- training
- gradient
- neural
- loss
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel back-propagation-free training framework
  for neural networks using zeroth-order optimization combined with tensor-compressed
  variance reduction. The authors propose a hybrid approach that integrates tensor-train
  decomposition to reduce gradient variance and a signSGD-based coarse training with
  coordinate-wise fine-tuning to improve efficiency.
---

# Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks

## Quick Facts
- arXiv ID: 2308.09858
- Source URL: https://arxiv.org/abs/2308.09858
- Authors: 
- Reference count: 11
- Key outcome: Novel back-propagation-free training framework using zeroth-order optimization with tensor-compressed variance reduction, achieving 96.64% MNIST accuracy with 200x parameter reduction

## Executive Summary
This paper introduces a novel back-propagation-free training framework for neural networks using zeroth-order optimization combined with tensor-compressed variance reduction. The authors propose a hybrid approach that integrates tensor-train decomposition to reduce gradient variance and a signSGD-based coarse training with coordinate-wise fine-tuning to improve efficiency. This method is applied to both standard neural networks and physics-informed neural networks (PINNs), addressing the challenge of training on resource-constrained edge devices. The framework achieves comparable accuracy to traditional first-order training on MNIST (96.64% vs. 97.16%) while reducing model parameters by over 200x. For PINNs, it successfully solves a 20-dimensional Hamiltonian-Jacobi-Bellman PDE with minimal accuracy loss. The approach is memory-efficient and scalable, making it suitable for on-device training on platforms like FPGAs and micro-controllers.

## Method Summary
The method combines tensor-train (TT) compression with hybrid zeroth-order (ZO) optimization. The weight matrix W is first compressed into a tensor-train format, reducing dimensionality from LQ(mk*n_k) to LP(rk-1*mk*nk*rk). This compressed representation is then trained using a two-stage ZO optimization: coarse training with signSGD (ZO-signRGE) followed by fine-tuning with coordinate-wise gradient estimation (ZO-CGE). For PINNs, a sparse-grid Stein estimator computes derivatives without automatic differentiation. The approach replaces backpropagation entirely, relying only on forward passes and function evaluations to estimate gradients.

## Key Results
- MNIST classification accuracy: 96.64% (BP-free) vs. 97.16% (first-order), with 200x parameter reduction
- 20-dimensional HJB PDE solved with minimal accuracy loss using sparse-grid Stein estimator
- Memory efficiency: Model compression enables training on edge devices with limited resources
- Query complexity: Sparse-grid method requires 10-30 samples for 2D/3D PDEs vs. thousands for Monte Carlo

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tensor-train decomposition reduces the dimensionality of the weight matrix, thereby dramatically lowering the variance in zeroth-order gradient estimation.
- Mechanism: By folding the weight matrix W into a 2L-way tensor and representing it with tensor-train (TT) cores, the number of trainable variables is compressed from LQ(mk*n_k) to LP(rk-1*mk*nk*rk). This compression reduces the dimensionality d in the ZO gradient estimator, which directly reduces the dimension-dependent variance factor O(d/N).
- Core assumption: The original weight matrix can be accurately approximated by a low-rank tensor-train decomposition without significant loss of expressive power.
- Evidence anchors:
  - [abstract]: "we present a tensor-compressed variance reduction approach to greatly improve the scalability of zeroth-order (ZO) optimization, making it feasible to handle a network size that is beyond the capability of previous ZO approaches."
  - [section 3.1]: "This TT representation reduces the number of unknown variables from LQ(mk*n_k) to LP(rk-1*mk*nk*rk). The compression ratio can be controlled by the TT-ranks, which can be learnt automatically..."
- Break condition: If the TT-rank is set too low, the compressed representation may lose model expressivity, leading to degraded accuracy even with low gradient variance.

### Mechanism 2
- Claim: Hybrid optimization combining signSGD-based coarse training with coordinate-wise fine-tuning improves both convergence speed and final accuracy.
- Mechanism: The signSGD variant (ZO-signRGE) uses only the sign of the ZO gradient, reducing the impact of high variance in early training. Once the coarse training reaches a plateau, switching to ZO-CGE with momentum provides more accurate gradient estimates for fine-tuning.
- Core assumption: Early-stage training can tolerate noisy gradient estimates, while late-stage training requires more accurate gradients for fine convergence.
- Evidence anchors:
  - [section 3.2]: "To enhance both the accuracy and efficiency of the whole ZO training process, we employ a hybrid ZO training scheme that involves two stages: ZO-signRGE coarse training... then switches to ZO-CGE to fine-tune the model."
  - [section 5.1]: "ZO-signRGE shows a more stable training than ZO-RGE at the beginning... With additional ZO-CGE fine-tuning, our hybrid ZO training strategy achieves a highly competitive result..."
- Break condition: If the switching threshold is poorly chosen, the optimizer may switch too early (losing efficiency) or too late (missing the accuracy benefits).

### Mechanism 3
- Claim: Sparse-grid Stein gradient estimator reduces the number of function queries needed to approximate derivatives in PINN loss evaluation.
- Mechanism: Instead of Monte Carlo sampling, the sparse-grid method constructs a weighted sum of function evaluations using a Smolyak algorithm. This approach requires significantly fewer samples than Monte Carlo for the same accuracy, especially in low to moderate dimensions.
- Core assumption: The sparse-grid quadrature rule can approximate the required integrals in the Stein estimator with sufficient accuracy using far fewer samples than Monte Carlo.
- Evidence anchors:
  - [section 4.2]: "For example, we only need n*L = 2D2 + 2D + 1 nodes to approximate a D-dimensional integral (D > 1) using a level-3 sparse Gaussian quadrature rule A*D,3."
  - [section 5.2]: "We use 1024 samples in SE and 925 samples in SG using a level-3 sparse Gaussian quadrature rule to approximate the expectations... For most 2-dim or 3-dim PDEs, our SG method only requires 10 ~ 30 samples."
- Break condition: In very high dimensions, the number of sparse-grid points grows exponentially, negating the computational advantage.

## Foundational Learning

- Concept: Zeroth-order optimization
  - Why needed here: The paper replaces backpropagation with zeroth-order methods because edge devices cannot support automatic differentiation.
  - Quick check question: How does the variance of a zeroth-order gradient estimator scale with the number of parameters d?

- Concept: Tensor-train decomposition
  - Why needed here: TT decomposition compresses the weight matrix to reduce the number of parameters and the variance in ZO gradient estimation.
  - Quick check question: What is the relationship between TT-ranks and the compression ratio in the weight matrix?

- Concept: Physics-informed neural networks
  - Why needed here: The paper extends the BP-free framework to PINNs, which require derivative terms in the loss function.
  - Quick check question: Why is computing derivatives in PINNs more challenging than in standard neural networks?

## Architecture Onboarding

- Component map:
  - TT compression module -> ZO gradient estimator -> Hybrid optimizer -> Sparse-grid Stein estimator -> Forward pass engine

- Critical path:
  1. Compress model weights using TT decomposition
  2. Initialize optimizer (start with ZO-signRGE)
  3. For each training step:
     - Perform forward pass to compute loss
     - Estimate gradient using current ZO method
     - Update parameters
     - Check convergence; if plateau, switch to ZO-CGE
  4. For PINNs, compute derivatives using sparse-grid Stein estimator

- Design tradeoffs:
  - TT-rank selection: higher ranks give better accuracy but less compression
  - Coarse vs. fine training balance: more coarse training saves computation but may hurt accuracy
  - Sparse-grid level: higher levels give better accuracy but more function evaluations

- Failure signatures:
  - Training diverges: likely due to insufficient TT compression or poorly chosen learning rate
  - Slow convergence: may indicate need to switch from coarse to fine training earlier
  - Accuracy plateau: could mean TT ranks are too low or sparse-grid level insufficient

- First 3 experiments:
  1. Train a small MLP on MNIST using only ZO-signRGE without TT compression to observe baseline performance
  2. Add TT compression with varying ranks and compare accuracy/memory usage
  3. Implement hybrid training (ZO-signRGE â†’ ZO-CGE) and measure convergence speed vs. accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed tensor-compressed hybrid ZO training method perform on very large neural networks, such as transformers, where the dimensionality is extremely high?
- Basis in paper: [inferred] The paper states that while the method is good enough for image, speech, and scientific computing problems, it still needs improvement to train very large neural networks like transformers.
- Why unresolved: The paper does not provide experimental results or theoretical analysis for transformer-scale models, which have vastly higher parameter counts and different architectural characteristics.
- What evidence would resolve it: Experimental results showing the method's performance on transformer models, including accuracy, memory usage, and convergence speed compared to standard backpropagation.

### Open Question 2
- Question: What is the optimal balance between the tensor compression ratio and the model's expressive power for different types of neural network tasks?
- Basis in paper: [explicit] The paper mentions that compressing the model too much can lose model expressive power and testing accuracy, and shows results for different compression ratios.
- Why unresolved: The paper only tests a few compression ratios and doesn't provide a systematic analysis of how the optimal compression ratio varies with task complexity, dataset size, or network architecture.
- What evidence would resolve it: A comprehensive study varying compression ratios across multiple tasks and network architectures, with guidelines for choosing optimal compression based on specific use cases.

### Open Question 3
- Question: How does the sparse-grid Stein gradient estimator compare to other BP-free methods for computing derivatives in PINNs, particularly in terms of accuracy and computational efficiency for high-dimensional PDEs?
- Basis in paper: [explicit] The paper compares the sparse-grid method to Monte Carlo-based Stein estimators and mentions it requires fewer samples, but doesn't compare it to other BP-free methods like finite difference or forward gradient.
- Why unresolved: The paper only compares the sparse-grid method to Monte Carlo-based Stein estimators and automatic differentiation, without exploring other BP-free derivative computation methods.
- What evidence would resolve it: Systematic comparison of the sparse-grid method against other BP-free derivative computation methods (finite difference, forward gradient, etc.) across various PDE problems, measuring both accuracy and computational cost.

## Limitations

- The TT-rank selection process appears to be critical but underspecified; the paper mentions automatic learning but provides limited guidance on practical implementation
- Sparse-grid method's scalability to very high dimensions (D > 10) is not thoroughly validated
- Limited ablation studies on the trade-off between compression ratio and accuracy for different network architectures

## Confidence

- High: Claims about tensor compression reducing parameters and gradient variance (supported by quantitative comparisons)
- Medium: Claims about hybrid optimization improving convergence (supported by MNIST results but less so for PINNs)
- Medium: Claims about sparse-grid Stein estimator reducing query complexity (supported by theory but limited empirical validation)

## Next Checks

1. Conduct controlled experiments varying TT-ranks systematically to quantify the compression-accuracy trade-off curve
2. Implement and test the sparse-grid Stein estimator on PDEs with dimensions 10-20 to validate scalability claims
3. Compare the full BP-free approach against first-order methods on a larger, more diverse set of PINN problems beyond the 20D HJB equation