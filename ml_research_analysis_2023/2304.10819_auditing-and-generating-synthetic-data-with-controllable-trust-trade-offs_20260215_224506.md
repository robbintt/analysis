---
ver: rpa2
title: Auditing and Generating Synthetic Data with Controllable Trust Trade-offs
arxiv_id: '2304.10819'
source_url: https://arxiv.org/abs/2304.10819
tags:
- data
- tf-gpt
- synthetic
- trust
- private
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive framework for auditing the
  trustworthiness of synthetic data, addressing concerns around bias, privacy, fidelity,
  utility, fairness, and robustness. The authors introduce a trust index that ranks
  synthetic datasets based on their compliance with desired safeguards and trade-offs
  across these dimensions.
---

# Auditing and Generating Synthetic Data with Controllable Trust Trade-offs

## Quick Facts
- arXiv ID: 2304.10819
- Source URL: https://arxiv.org/abs/2304.10819
- Reference count: 40
- One-line primary result: Framework quantifies uncertainty in auditing generative models and enables controllable trust trade-offs via trust-index-driven model selection during training.

## Executive Summary
This paper presents a comprehensive framework for auditing synthetic data trustworthiness across fidelity, privacy, utility, fairness, and robustness dimensions. The authors introduce a trust index that ranks synthetic datasets based on compliance with desired safeguards and trade-offs, enabling controllable trust trade-offs through a trust-index-driven model selection process during training. Demonstrated on TrustFormer transformer models, the framework quantifies uncertainty in auditing generative models and showcases effectiveness across diverse use cases including education, healthcare, banking, and human resources.

## Method Summary
The framework develops a holistic auditing approach for synthetic data generation that quantifies trust dimensions using interpretable metrics, aggregates them into trust indices, and enables controllable trade-offs via trust-index-driven model selection. TrustFormer models (transformer-based generative models) are trained under trust constraints like differential privacy and debiasing. The framework implements trust-index-driven cross-validation to select checkpoints optimizing validation trust indices for prescribed trade-off weights. Metrics for each trust dimension are aggregated using copula methods to form trust indices, which are then combined with trade-off weights to produce a final trust index for ranking synthetic datasets.

## Key Results
- Trust-index-driven model selection enables controllable trust trade-offs in synthetic data generation, outperforming real data on downstream utility, fairness, and robustness tasks
- Copula aggregation effectively normalizes heterogeneous metrics into comparable trust dimension indices, enabling principled trust index computation
- Uncertainty quantification via multiple data splits reveals that trust-index-driven cross-validation can lead to different conclusions than ranking based solely on mean trust index

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Trust index-driven model selection enables controllable trust trade-offs in synthetic data generation.
- **Mechanism**: The framework computes trust indices across fidelity, privacy, utility, fairness, and robustness dimensions. During training, it tracks validation trust indices for each checkpoint and selects the epoch maximizing the desired trade-off weighting.
- **Core assumption**: Validation trust indices correlate with test trust indices and downstream task performance.
- **Evidence anchors**:
  - [abstract] "trust-index-driven model selection process during training, exemplified with 'TrustFormers'"
  - [section] "trust-index-driven model selection is performed within each fold independently, i.e., using the validation set"
  - [corpus] "Privacy-Utility Trade-offs in Neural Networks for Medical Population Graphs" (supports DP-utility trade-off concept)
- **Break condition**: If validation and test distributions diverge significantly, or if trust dimensions are not well-calibrated, selected models may not generalize.

### Mechanism 2
- **Claim**: Copula aggregation normalizes heterogeneous metrics into comparable trust dimension indices.
- **Mechanism**: Individual metrics are polarity-aligned, transformed via empirical CDF to uniform [0,1] scores, then aggregated using geometric mean copula to form trust dimension indices.
- **Core assumption**: Metrics within a trust dimension are sufficiently independent or their dependencies are captured by the copula.
- **Evidence anchors**:
  - [section] "We resort to a popular method in software quality assessment, the so called copula aggregation method"
  - [section] "The index of a trust dimension T of a synthetic dataset Ds can be therefore defined using the geometric mean copula aggregation"
  - [corpus] "Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data" (supports metric aggregation concept)
- **Break condition**: Strong metric dependencies not captured by the copula lead to misleading aggregated indices.

### Mechanism 3
- **Claim**: Uncertainty quantification via multiple data splits enables robust model ranking under trust trade-offs.
- **Mechanism**: Real data is split S times; each generative model is trained on each split, producing S synthetic datasets. Trust indices and their deviations are computed across splits, and ranking uses a confidence-penalized score Rα.
- **Core assumption**: Variability across splits reflects true uncertainty in trust assessment.
- **Evidence anchors**:
  - [section] "We quantify the uncertainty in auditing synthetic data generation methods trained and evaluated on different real data splits"
  - [section] "∆τ (Dr, Ds, cfg, ω) = 1/S Σℓ=1 (τTrust(Dℓr,Dℓs, cfg, ω) − τTrust(Dr, Ds, cfg, ω))²"
  - [corpus] "VTruST: Controllable value function based subset selection for Data-Centric Trustworthy AI" (supports uncertainty-aware selection)
- **Break condition**: If splits are not representative or model performance is highly unstable, uncertainty estimates become unreliable.

## Foundational Learning

- **Concept**: Differentially Private Stochastic Gradient Descent (DP-SGD)
  - Why needed here: Enables privacy preservation in synthetic data generation while allowing utility preservation.
  - Quick check question: How does the privacy budget ε control the trade-off between privacy and utility in DP-SGD?

- **Concept**: Copula aggregation and empirical CDF transformation
  - Why needed here: Provides a principled way to combine heterogeneous metrics into comparable trust indices.
  - Quick check question: Why is geometric mean copula preferred over arithmetic mean for trust index aggregation?

- **Concept**: Adversarial debiasing and fair mixup techniques
  - Why needed here: Improve fairness of downstream classifiers trained on synthetic data.
  - Quick check question: How do adversarial debiasing and fair mixup differ in their approach to mitigating bias?

## Architecture Onboarding

- **Component map**: Data splits → Generative model training → Synthetic data sampling → Metric evaluation → Copula aggregation → Trust indices → Model selection → Audit report
- **Critical path**:
  1. Prepare real data splits
  2. Train generative model with trust constraints
  3. Sample synthetic data from checkpoints
  4. Evaluate metrics on each trust dimension
  5. Aggregate metrics to trust indices
  6. Select best checkpoint via trust-index cross-validation
  7. Generate audit report
- **Design tradeoffs**:
  - Fidelity vs privacy: Higher fidelity often requires less noise, reducing privacy
  - Utility vs fairness: Improving fairness may reduce classification accuracy
  - Computation vs precision: More data splits improve uncertainty estimates but increase cost
- **Failure signatures**:
  - Trust indices not improving with training → model capacity or optimization issue
  - High variance across splits → data leakage or unstable training
  - Privacy metrics poor despite DP training → incorrect implementation or insufficient ε
- **First 3 experiments**:
  1. Train TrustFormer GPT on Bank Marketing dataset with ε=1, evaluate fidelity and privacy metrics
  2. Implement copula aggregation on synthetic tabular data, verify metric normalization
  3. Perform trust-index driven model selection, compare selected epochs across different ω trade-offs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do trust-index-driven model selection and cross-validation perform when applied to generative models beyond TrustFormers, such as diffusion models or autoregressive transformers for text?
- Basis in paper: [explicit] The paper showcases trust-index-driven model selection on a class of transformer models called TrustFormers. The authors state: "This trust-index-driven model selection is applicable to any generative modeling method."
- Why unresolved: The experiments only demonstrate the effectiveness of trust-index-driven model selection on TrustFormers. While the authors claim generalizability, they do not provide empirical evidence for other generative model architectures.
- What evidence would resolve it: Empirical studies applying trust-index-driven model selection to a diverse set of generative models (e.g., diffusion models, autoregressive transformers for text) and comparing their performance to baseline selection methods.

### Open Question 2
- Question: How does the choice of weighting scheme ω for trust dimensions impact the trade-offs between fidelity, privacy, utility, fairness, and robustness in the resulting synthetic data?
- Basis in paper: [explicit] The authors introduce a trust index that ranks synthetic datasets based on their compliance with desired safeguards and trade-offs across trust dimensions. They define weighting schemes ω to reflect the relative importance of each trust dimension.
- Why unresolved: The paper provides examples of weighting schemes but does not thoroughly investigate the impact of different ω choices on the resulting synthetic data. It is unclear how sensitive the trust index is to changes in the weighting scheme.
- What evidence would resolve it: Systematic experiments varying the weighting scheme ω across a wide range of values and analyzing the resulting synthetic data's performance on each trust dimension. This would reveal the sensitivity of the trust index to the weighting scheme and help practitioners choose appropriate weights for their use cases.

### Open Question 3
- Question: How does the uncertainty quantification of the trust index impact the ranking of synthetic data under different trust trade-offs and data splits?
- Basis in paper: [explicit] The authors introduce a method to rank synthetic data under uncertainty using the weighted log ratio of mean trust index to its deviation across data splits (Rατ). They demonstrate that this ranking can lead to different conclusions compared to ranking based solely on mean trust index.
- Why unresolved: The paper provides a limited exploration of the impact of uncertainty quantification on ranking. It is unclear how the choice of α in Rατ affects the ranking and whether the uncertainty quantification method is robust to different data splits and trust trade-offs.
- What evidence would resolve it: Extensive experiments varying α in Rατ and analyzing the resulting rankings under different trust trade-offs and data splits. This would reveal the sensitivity of the ranking under uncertainty to the choice of α and the robustness of the method to different scenarios.

## Limitations
- The correlation between validation trust indices and true downstream performance remains partially unproven, particularly for complex multimodal scenarios
- Some specific metric implementations lack complete specification, particularly the greedy adversarial attacks on tabular data and exact parameterization of fidelity metrics
- Confidence in the framework's generalizability across diverse domains and data types is limited, as most experiments focus on specific use cases

## Confidence
- **High**: Trust-index-driven model selection and copula aggregation methods are well-established in quality assessment literature
- **Medium**: Uncertainty quantification approach using split-based variability may not capture all sources of uncertainty
- **Low**: Specific metric implementations, particularly adversarial attacks and fidelity metrics, lack complete specification

## Next Checks
1. **Cross-dataset validation**: Test the framework's trust index consistency when applied to datasets from different domains than those used in training, particularly focusing on whether trust indices maintain predictive power for downstream task performance.

2. **Adversarial robustness verification**: Implement and validate the greedy adversarial attack methodology on tabular data, ensuring the attack procedure is correctly specified and that robustness metrics are reliable.

3. **Privacy-utility trade-off calibration**: Systematically vary the differential privacy budget ε and measure the resulting trade-offs in privacy, utility, and other trust dimensions to validate that the framework can effectively navigate these competing objectives.