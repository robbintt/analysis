---
ver: rpa2
title: Achieving High Accuracy with PINNs via Energy Natural Gradients
arxiv_id: '2302.13163'
source_url: https://arxiv.org/abs/2302.13163
tags:
- gradient
- natural
- energy
- space
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes energy natural gradient descent as a highly
  accurate optimization method for training physics-informed neural networks (PINNs)
  and the deep Ritz method for solving partial differential equations. The energy
  natural gradient is defined using the Hessian of the training objective in function
  space, which corresponds to the Newton direction in function space up to an orthogonal
  projection.
---

# Achieving High Accuracy with PINNs via Energy Natural Gradients

## Quick Facts
- arXiv ID: 2302.13163
- Source URL: https://arxiv.org/abs/2302.13163
- Reference count: 16
- Primary result: Energy natural gradient descent achieves relative L2 errors as low as 10^-7 for 2D Poisson and 10^-6 for 1D heat equations, compared to 10^-3 errors from standard optimizers.

## Executive Summary
This paper introduces energy natural gradient descent as an optimization method for training physics-informed neural networks (PINNs) and the deep Ritz method. The method leverages the Hessian of the training objective in function space to define a natural gradient that corresponds to the Newton direction up to an orthogonal projection. Experiments demonstrate several orders of magnitude improvement in accuracy compared to standard optimizers like gradient descent and Adam, achieving highly accurate solutions for both PINN and deep Ritz formulations of PDEs.

## Method Summary
The method defines energy natural gradient descent using the Hessian of the training objective in function space. The update direction is computed by solving a least squares problem involving the energy Gram matrix, which can be efficiently assembled in parallel. A line search is used to select the optimal step size. The approach is applied to both PINN (residual-based) and deep Ritz (variational) formulations of PDEs, showing superior accuracy while maintaining computational tractability through parallelized Gram matrix assembly and least squares solution.

## Key Results
- Achieves relative L2 errors as low as 10^-7 for 2D Poisson equation
- Achieves relative L2 errors as low as 10^-6 for 1D heat equation
- Standard optimizers (GD, Adam) plateau at 10^-3 errors even with more computation time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Energy natural gradient descent achieves superior accuracy because it updates in the function space direction that corresponds to the Newton step (up to projection onto the tangent space).
- Mechanism: The update direction in parameter space DPθ∇EL(θ) is designed to match the function space Newton direction D2E(uθ)^{-1}∇E(uθ) projected onto the model's tangent space. For quadratic energies, this aligns with the residual uθ − u∗, directly targeting the error in function space.
- Core assumption: The energy function E has a well-defined Hessian D2E that is coercive or positive definite, and the parametrization P: Rp → X is differentiable with full-rank Jacobian in the relevant directions.
- Evidence anchors:
  - [abstract]: "the update direction in function space resulting from the energy natural gradient corresponds to the Newton direction modulo an orthogonal projection onto the model's tangent space"
  - [section]: Theorem 2 (Energy Natural Gradient in Function Space) states the formal correspondence
  - [corpus]: Weak - related works propose similar Hessian-informed methods but do not establish the exact Newton correspondence claimed here.
- Break condition: If the energy Hessian is ill-conditioned or the tangent space does not span the error direction, the projection may fail to capture the optimal descent direction, reducing effectiveness.

### Mechanism 2
- Claim: The method is computationally tractable because the energy Gram matrix GE can be assembled efficiently in parallel and solved via least squares rather than explicit matrix inversion.
- Mechanism: Instead of computing the pseudo-inverse G+E(θ), the update solves a least squares problem minψ ||GE(θ)ψ − ∇L(θ)||². This avoids O(p³) inversion cost and can be parallelized.
- Core assumption: The dimension p (number of parameters) is moderate enough that solving the least squares system remains feasible; parallelization hardware is available.
- Evidence anchors:
  - [section]: "Although naive, this can easily be parallelized and performs fast and eﬃcient in our experiments"
  - [section]: Implementation uses JAX automatic differentiation and least squares solve
  - [corpus]: Assumption - related works discuss computational cost but do not detail this specific least squares trick.
- Break condition: For very large networks where p is huge, even the least squares solve may become a bottleneck; also, if GE is rank-deficient, the least squares solution may be unstable.

### Mechanism 3
- Claim: The approach is robust to different PDE formulations (PINNs and deep Ritz) because it operates directly on the energy Hessian in function space, which is formulation-agnostic.
- Mechanism: By defining the metric via D2E(uθ), the method adapts to the geometry of the specific energy (residual or variational), ensuring the update direction is optimal for that energy's curvature.
- Core assumption: Both residual and variational energies admit meaningful Hessians in the appropriate function spaces, and the network parametrization is smooth enough for these Hessians to be well-defined.
- Evidence anchors:
  - [abstract]: Experiments include both PINN and deep Ritz formulations
  - [section]: "For a linear PDE operator L, the residual yields a quadratic energy and the energy inner product..." and "the deep Ritz method for a quadratic energy..."
  - [corpus]: Weak - other works focus on one formulation or the other, not both.
- Break condition: If the energy is highly nonlinear or non-convex, the Hessian may not be positive definite, breaking the natural gradient interpretation.

## Foundational Learning

- Concept: Riemannian geometry in function space (metrics induced by Hessians)
  - Why needed here: The method relies on moving along geodesics defined by the energy-induced Riemannian metric rather than Euclidean parameter space gradients.
  - Quick check question: What is the relationship between the natural gradient and the Riemannian gradient in the context of function space optimization?

- Concept: Newton's method and its function space analogue
  - Why needed here: The energy natural gradient is shown to correspond to a Newton step in function space, so understanding Newton's method is crucial to grasp why this approach converges faster.
  - Quick check question: How does the Newton direction in function space differ from the standard gradient descent direction for a quadratic energy?

- Concept: Sobolev spaces and weak formulations of PDEs
  - Why needed here: The method operates on function spaces (e.g., H², H¹) and leverages inner products defined on these spaces; understanding these is key to interpreting the Gram matrices.
  - Quick check question: Why is the energy inner product for the Poisson equation not coercive on H²(Ω), and what implication does this have for the method?

## Architecture Onboarding

- Component map: Loss function L(θ) -> Gram matrix GE(θ) -> Least squares solver -> Energy natural gradient ∇EL(θ) -> Line search -> Parameter update θ ← θ − η∗∇EL(θ)
- Critical path:
  1. Forward pass: compute uθ and loss L(θ)
  2. Backward pass: compute ∇L(θ) via automatic differentiation
  3. Assemble GE(θ) from D²E(∂θᵢuθ, ∂θⱼuθ)
  4. Solve least squares for ∇EL(θ)
  5. Perform line search to get η∗
  6. Update parameters: θ ← θ − η∗∇EL(θ)
- Design tradeoffs:
  - Accuracy vs. computational cost: energy natural gradient steps are more expensive per iteration but require far fewer iterations.
  - Parallelization vs. memory: assembling GE benefits from parallelization but requires storing O(p²) entries.
  - Activation function choice: smoothness (e.g., tanh) is required for well-defined Hessians; ReLU would break differentiability.
- Failure signatures:
  - If the line search fails to find a decreasing step size, the Hessian may be ill-conditioned.
  - If GE is singular or nearly singular, the least squares solve will be unstable.
  - If the network parametrization is not smooth, automatic differentiation will fail to produce meaningful gradients/Hessians.
- First 3 experiments:
  1. Verify basic functionality on a simple 1D Poisson equation with a shallow network; compare energy NG vs. vanilla GD.
  2. Test sensitivity to line search parameters on a 2D Poisson problem; observe convergence behavior.
  3. Compare H-NGD vs. E-NGD on a nonlinear deep Ritz problem to confirm the importance of Hessian information.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do energy natural gradients sometimes fail to train PINNs despite generally superior performance?
- Basis in paper: [explicit] Authors observe that E-NGD achieves high accuracy for most initializations but occasionally fails to train, noting that pre-training with GD or Adam can circumvent this issue.
- Why unresolved: The paper does not investigate the underlying causes of these rare training failures or characterize when they occur.
- What evidence would resolve it: Systematic experiments varying initialization schemes, network architectures, and problem parameters to identify failure patterns and conditions.

### Open Question 2
- Question: How does the computational cost of E-NGD scale with problem dimension and network size compared to standard optimizers?
- Basis in paper: [inferred] While the paper mentions that E-NGD is roughly 15x more expensive per iteration than GD/Adam, it does not provide systematic scaling analysis as problem complexity increases.
- Why unresolved: The experiments focus on relatively small-scale problems (shallow networks with ~250 parameters), leaving open questions about scalability to larger systems.
- What evidence would resolve it: Benchmarking E-NGD against standard methods on problems with varying dimensions, network depths, and collocation point densities.

### Open Question 3
- Question: Can the energy inner product be made coercive for problems where the standard formulation is not?
- Basis in paper: [explicit] For the 2D Poisson example, the authors note that the energy inner product is not coercive on H²(Ω), unlike the Hilbert inner product, yet E-NGD still performs well.
- Why unresolved: The paper does not explore whether modifying the energy formulation or inner product could improve theoretical guarantees while maintaining practical performance.
- What evidence would resolve it: Mathematical analysis showing conditions under which modified energy formulations yield coercive inner products, validated through numerical experiments.

## Limitations
- Computational complexity of assembling the energy Gram matrix scales quadratically with the number of parameters, limiting applicability to very deep networks.
- Method's effectiveness depends critically on the energy Hessian being well-conditioned and positive definite, which may not hold for highly nonlinear or non-convex PDE formulations.
- Paper does not provide extensive ablation studies on the impact of line search parameters or choice of neural network architecture.

## Confidence

- **High Confidence**: The theoretical correspondence between energy natural gradient and Newton's method in function space (Theorem 2) is rigorously established and the experimental results showing dramatic accuracy improvements (10^-7 vs 10^-3 error) are compelling.
- **Medium Confidence**: The computational efficiency claims rely on parallel implementation details that are not fully specified, and the method's scalability to larger problems remains untested.
- **Low Confidence**: The robustness of the method across different PDE types, network architectures, and initialization schemes is not thoroughly explored.

## Next Checks
1. Test the method on a 10x larger 2D Poisson problem to evaluate computational scaling and numerical stability of the energy Gram matrix assembly.
2. Implement an ablation study varying the line search parameters (grid spacing, range) and activation functions (ReLU vs tanh) to quantify their impact on convergence.
3. Compare against state-of-the-art PINN training methods (e.g., DeepXDE, Fourier features) on a challenging nonlinear PDE to establish practical advantages.