---
ver: rpa2
title: Synthesizing Black-box Anti-forensics DeepFakes with High Visual Quality
arxiv_id: '2312.10713'
source_url: https://arxiv.org/abs/2312.10713
tags:
- deepfake
- images
- k4s2p1
- visual
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for generating adversarial sharpening
  masks to launch black-box anti-forensics attacks on DeepFake images. Unlike existing
  methods, the proposed approach achieves high anti-forensics performance while maintaining
  visually pleasing sharpening effects.
---

# Synthesizing Black-box Anti-forensics DeepFakes with High Visual Quality

## Quick Facts
- arXiv ID: 2312.10713
- Source URL: https://arxiv.org/abs/2312.10713
- Reference count: 0
- Primary result: Novel method generates adversarial sharpening masks that disrupt DeepFake detectors while maintaining high visual quality

## Executive Summary
This paper introduces a novel method for generating adversarial sharpening masks to launch black-box anti-forensics attacks on DeepFake images. The approach achieves high anti-forensics performance while maintaining visually pleasing sharpening effects, outperforming existing methods on benchmark datasets. The method employs a two-stage GAN framework with a Forensics Disruption Network (FDN) and a Visual Enhancement Network (VEN) using MobileVit blocks to balance detector evasion with perceptual quality.

## Method Summary
The proposed method consists of a two-stage GAN framework. First, the Forensics Disruption Network (FDN) generates adversarial perturbations constrained to mimic real sharpened images using a combination of adversarial and reconstruction losses. Second, the Visual Enhancement Network (VEN) fine-tunes these perturbations using MobileVit blocks to produce visually pleasing sharpening effects while maintaining the adversarial signal. The approach operates in a black-box setting without requiring access to detector internals.

## Key Results
- Successfully disrupts state-of-the-art DeepFake detectors while improving visual quality metrics
- Achieves high undetectability even on challenging DeeperForensics dataset
- Excels in PSNR, SSIM, and face detection metrics compared to existing anti-forensics methods

## Why This Works (Mechanism)

### Mechanism 1
- Adversarial sharpening masks preserve perceptual realism while evading deepfake detectors
- FDN injects adversarial perturbations constrained to mimic real USM-processed images, then VEN refines these into visually plausible sharpening effects
- Assumes human perception is less sensitive to subtle sharpening than to adversarial noise; detectors rely heavily on statistical artifacts
- Evidence: Abstract states perturbations achieve high anti-forensics performance with pleasant sharpening effects
- Break condition: Detectors evolve to distinguish natural vs adversarial sharpening based on frequency or semantic artifacts

### Mechanism 2
- Two-stage training with frozen FDN parameters preserves forensic disruption while enabling visual refinement
- FDN trained first to maximize undetectability; VEN reuses G1 parameters and fine-tunes G2 to reconstruct images close to naturally sharpened DeepFakes
- Assumes freezing G1 prevents loss of adversarial noise learned during FDN training; MobileVit blocks provide better feature fusion
- Evidence: Section describes initializing G1 parameters in VEN with frozen FDN-trained parameters
- Break condition: Adversarial signal degrades during fine-tuning or G2 overfits to training distribution

### Mechanism 3
- Real-sharpened image distribution as target in FDN aligns adversarial perturbations with plausible sharpening artifacts
- Lre1 loss pulls adversarial mask toward statistics of USM-processed real images, ensuring perturbations resemble natural sharpening
- Assumes detectors are less sensitive to USM-style sharpening than unstructured adversarial noise; L1 norm ensures pixel-level fidelity
- Evidence: Section defines Lre1 loss comparing adversarial mask to real sharpened images
- Break condition: Forensic models detect sharpening artifacts specific to USM or perturbation magnitude exceeds perceptual thresholds

## Foundational Learning

- Concept: Adversarial perturbation generation in black-box settings
  - Why needed here: Method must evade detectors without access to their internal parameters
  - Quick check question: How does Lgan1 loss ensure adversarial robustness without querying target detector directly?

- Concept: Perceptual quality preservation in GAN-based image synthesis
  - Why needed here: Sharpening mask must not introduce visible artifacts; MobileVit blocks help maintain visual fidelity
  - Quick check question: What role does L1 reconstruction loss play in preventing visual degradation?

- Concept: GAN training dynamics with frozen subnetworks
  - Why needed here: Freezing G1 prevents collapse of adversarial signal while G2 refines visual quality
  - Quick check question: Why does paper train G1 first and freeze it rather than jointly training both subnetworks?

## Architecture Onboarding

- Component map: DeepFake input → FDN (G1 + D1) → adversarial mask m → VEN (G2 + G1 frozen) → sharpened adversarial mask m′ → output Irs
- Critical path: Input DeepFake → FDN → adversarial mask m → VEN → sharpened adversarial mask m′ → output Irs
- Design tradeoffs:
  - Undetectability vs. visual quality: Aggressive noise improves evasion but harms fidelity; sharpening balances both
  - Training complexity vs. performance: Two-stage training is slower but more stable than joint training
  - Model size vs. mobile deployment: MobileVit blocks reduce compute cost while maintaining quality
- Failure signatures:
  - Low PSNR/SSIM: Indicates visual artifacts from over-aggressive perturbations
  - High detector accuracy: Suggests adversarial signal is too weak or perceptually obvious
  - Face detection failure: Implies sharpening disrupts facial structure beyond acceptable limits
- First 3 experiments:
  1. Train FDN alone on small subset; measure detector evasion vs USM baseline
  2. Train VEN with frozen G1; compare visual metrics (PSNR, SSIM) to FDN output
  3. Combine FDN + VEN; evaluate joint performance on Celeb-DF and DeeperForensics datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does proposed method compare in terms of anti-forensics performance and visual quality to other state-of-the-art methods on datasets other than Celeb-DF, DeeperForensics, and FaceForensics++?
- Basis: Paper evaluates method only on these three datasets
- Why unresolved: No evaluation on other datasets provided
- Evidence needed: Experiments on other datasets comparing proposed method with state-of-the-art methods

### Open Question 2
- Question: How does proposed method perform in terms of anti-forensics effectiveness and visual quality when applied to videos instead of individual frames?
- Basis: Paper focuses on individual frames without addressing video performance
- Why unresolved: No information or experiments on video performance provided
- Evidence needed: Experiments on videos comparing proposed method with state-of-the-art methods on videos

### Open Question 3
- Question: How does proposed method handle different types of facial manipulations, such as facial expression changes or age progression, in terms of anti-forensics effectiveness and visual quality?
- Basis: Paper evaluates method on DeepFake images without addressing different facial manipulations
- Why unresolved: No information or experiments on different facial manipulations provided
- Evidence needed: Experiments on different facial manipulations comparing proposed method with state-of-the-art methods

## Limitations

- Claims about adversarial sharpening masks depend on assumptions about detector vulnerabilities not empirically validated against modern forensic architectures
- Critical role of frozen parameters in preserving adversarial signals remains theoretically justified but not experimentally isolated
- MobileVit integration shows promise but lacks ablation studies to confirm necessity versus standard CNN blocks

## Confidence

- High Confidence: Improved PSNR/SSIM metrics compared to existing anti-forensics methods; successful disruption of tested forensic detectors on benchmark datasets
- Medium Confidence: MobileVit blocks providing superior visual quality; two-stage training preserving adversarial signals
- Low Confidence: Mechanism by which adversarial sharpening evades detection; transferability to unseen forensic architectures

## Next Checks

1. **Ablation Study**: Train identical networks with standard CNN blocks instead of MobileVit to quantify specific contribution of MobileVit to visual quality improvements

2. **Detector Generalization Test**: Evaluate generated adversarial examples against forensic architectures not used during training to verify black-box transferability claims

3. **Perturbation Analysis**: Systematically vary magnitude of adversarial perturbations to identify threshold where visual quality degrades versus when detector evasion fails, isolating critical tradeoff boundary