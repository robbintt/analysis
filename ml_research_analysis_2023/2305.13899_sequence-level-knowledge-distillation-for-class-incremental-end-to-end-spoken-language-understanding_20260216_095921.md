---
ver: rpa2
title: Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken
  Language Understanding
arxiv_id: '2305.13899'
source_url: https://arxiv.org/abs/2305.13899
tags:
- learning
- task
- slurp
- knowledge
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of catastrophic forgetting in a
  class-incremental setting for spoken language understanding (SLU). The authors define
  a class-incremental learning scenario using the SLURP dataset and propose three
  knowledge distillation (KD) methods to mitigate forgetting in a transformer-based
  sequence-to-sequence model.
---

# Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding

## Quick Facts
- arXiv ID: 2305.13899
- Source URL: https://arxiv.org/abs/2305.13899
- Reference count: 0
- One-line primary result: Sequence-level knowledge distillation significantly reduces catastrophic forgetting in class-incremental SLU, improving intent accuracy and reducing word error rate.

## Executive Summary
This paper addresses catastrophic forgetting in class-incremental end-to-end spoken language understanding by proposing three knowledge distillation methods applied to a transformer-based sequence-to-sequence model. The authors define a class-incremental learning scenario using the SLURP dataset, where tasks are defined by scenario labels, and propose audio-KD, token-KD, and seq-KD to mitigate forgetting. Experiments show that seq-KD is the most effective, significantly improving intent accuracy and reducing word error rate, with the best results achieved by combining seq-KD and audio-KD.

## Method Summary
The authors propose a transformer-based sequence-to-sequence model for end-to-end SLU, using Wav2vec 2.0 as the encoder and a transformer decoder. They define a class-incremental learning scenario using the SLURP dataset, where tasks are defined by scenario labels. To mitigate catastrophic forgetting, they propose three knowledge distillation methods: audio-KD (applied to encoder output), token-KD (applied to decoder token-level), and seq-KD (applied to decoder sequence-level). The model is trained with cross-entropy loss on current task data and KD losses on a rehearsal buffer containing 1% of previous task data.

## Key Results
- Sequence-level knowledge distillation (seq-KD) significantly improves intent accuracy and reduces word error rate compared to fine-tuning and token-KD.
- Combining seq-KD with audio-KD yields the best performance, further decreasing WER and enhancing entity prediction.
- Catastrophic forgetting is a significant issue in the SLURP class-incremental setting, as evidenced by the baseline fine-tuning results.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequence-level knowledge distillation (seq-KD) preserves the global structure of the model's output distribution across tasks, thereby reducing catastrophic forgetting more effectively than token-level distillation.
- Mechanism: By forcing the student model to replicate the entire beam-search-derived output sequence of the teacher (instead of matching individual token probabilities), seq-KD ensures that the model's long-range dependencies and overall sequence coherence are maintained. This addresses the propagation of early-token errors that occur with token-KD.
- Core assumption: The teacher model's beam search output represents the optimal sequence for rehearsal data, and aligning the student to this sequence at the sequence level preserves semantic integrity better than local token matching.
- Evidence anchors:
  - [abstract] "we propose three knowledge distillation (KD) approaches to mitigate forgetting for a sequence-to-sequence transformer model: the first KD method is applied to the encoder output (audio-KD), and the other two work on the decoder output, either directly on the token-level (tok-KD) or on the sequence-level (seq-KD) distributions. We show that the seq-KD substantially improves all the performance metrics..."
  - [section 4] "A potential flaw of this method is that if some initial token distributions are poorly estimated, their bias will be propagated until the end of the sequence. Indeed, a predicted token might be optimal at the current position in the sequence, but as we proceed through the rest of the sentence, it might turn out not to be the optimal one, given that later predicted positions are not already available. Seq-KD is an alternative approach that trains the student to generate the same output sequence as the teacher, thus working at the sequence level."

### Mechanism 2
- Claim: Combining audio-KD and seq-KD yields the best performance because they regularize different parts of the model: audio-KD stabilizes the encoder's audio embeddings, while seq-KD preserves decoder sequence coherence.
- Mechanism: Audio-KD minimizes the Euclidean distance between encoder outputs of the current model and the frozen teacher on rehearsal data, effectively constraining the encoder to maintain stable feature extraction. Seq-KD then ensures the decoder generates sequences consistent with the teacher's beam search output. Together, they mitigate forgetting at both the feature extraction and sequence generation stages.
- Core assumption: The encoder and decoder are independently susceptible to forgetting, and regularizing both components is more effective than targeting only one.
- Evidence anchors:
  - [abstract] "we propose three knowledge distillation (KD) approaches to mitigate forgetting for a sequence-to-sequence transformer model: the first KD method is applied to the encoder output (audio-KD), and the other two work on the decoder output, either directly on the token-level (tok-KD) or on the sequence-level (seq-KD) distributions."
  - [section 4] "We point out that the KD, unless otherwise stated, is applied to the sole rehearsal data since the teacher can effectively predict only the data seen in the previous tasks. We propose three different types of KDs: audio-KD, token-KD, and seq-KD. The audio-KD works at the encoder's output level, whereas the other two KDs are applied to the output of the decoder."
  - [section 5.2] "We guess that forcing the encoder output of the current task to be similar to that of the previous task (audio-KD) favors the cross-attention layer of the decoder to attend to the most relevant part of the audio signals."

### Mechanism 3
- Claim: Using scenarios as task boundaries in the SLURP dataset ensures that each task introduces distinct high-level semantic domains, making catastrophic forgetting more pronounced and thus justifying the need for KD.
- Mechanism: By splitting the dataset according to scenario labels, the model must learn to switch between different semantic contexts (e.g., "music" vs. "weather"). This creates a clear delineation of tasks, where each new scenario introduces both new intents and entities. KD helps the model retain knowledge of previous scenarios while adapting to new ones.
- Core assumption: Scenarios in SLURP are sufficiently distinct and non-overlapping in their semantic content, making them ideal task boundaries for a class-incremental setting.
- Evidence anchors:
  - [section 3] "We have used the scenarios as a splitting criterion to define the tasks of the CIL setting... Since the intent classification is the chief metric to assess our model against, the use of scenarios as splitting criterion abides by the rule of having only intents related to scenarios available in the current task."
  - [section 3] "Each utterance is annotated with three semantics: Scenario, Action, and Entities... Since the number of scenarios is limited and each scenario provides a high-level concept associated with each utterance, we think that they can closely resemble a practical application that must adapt to new general domains."

## Foundational Learning

- Concept: Knowledge distillation (KD)
  - Why needed here: KD is used to transfer knowledge from a trained teacher model (on previous tasks) to a student model (on the current task), mitigating catastrophic forgetting by constraining the student's predictions to resemble the teacher's.
  - Quick check question: What is the primary difference between token-level and sequence-level knowledge distillation in this context?

- Concept: Catastrophic forgetting
  - Why needed here: Understanding catastrophic forgetting is essential because the core problem this paper addresses is the model's tendency to overwrite previously learned knowledge when adapting to new tasks, especially in a class-incremental learning setting.
  - Quick check question: Why does fine-tuning without any regularization lead to catastrophic forgetting in this SLU setting?

- Concept: Sequence-to-sequence (seq2seq) models
  - Why needed here: The SLU task is framed as a seq2seq problem where the model must output both transcription and semantic labels (intents/entities). This structure necessitates specialized KD methods that can handle sequence-level outputs.
  - Quick check question: How does treating intent and entity classification as a seq2seq task differ from traditional SLU pipelines?

## Architecture Onboarding

- Component map:
  - Wav2vec 2.0 encoder (frozen feature extractor, fine-tuned transformer layers)
  - Transformer decoder (6 layers, autoregressive token generation)
  - Knowledge distillation modules: audio-KD (encoder output), token-KD (decoder token-level), seq-KD (decoder sequence-level)
  - Rehearsal buffer (1% of previous task data)

- Critical path:
  1. Load teacher model (frozen) from previous task
  2. Fine-tune Wav2vec 2.0 encoder and decoder on current task data
  3. Apply KD losses on rehearsal data:
     - Audio-KD: minimize distance between encoder outputs
     - Token-KD: minimize KL divergence on token-level predictions
     - Seq-KD: minimize CE on beam-search-derived sequence
  4. Update student model
  5. Save student as teacher for next task

- Design tradeoffs:
  - Rehearsal buffer size: 1% chosen to balance memory constraints and forgetting mitigation
  - KD weight (λKD): proportional to fraction of rehearsal data in mini-batch
  - Beam search width: 20 for generating soft labels in seq-KD
  - BPE vocabulary: 1k with dropout=0.1 for robustness

- Failure signatures:
  - Seq-KD underperforms: likely due to poor teacher beam search outputs or insufficient rehearsal data
  - Audio-KD ineffective: encoder may not be the primary source of forgetting; check decoder-centric errors
  - Combined KD worse than individual: KD weights may be unbalanced; consider scaling or curriculum

- First 3 experiments:
  1. Run baseline fine-tuning without any KD to quantify catastrophic forgetting
  2. Apply seq-KD alone to isolate its effect on sequence coherence
  3. Combine audio-KD and seq-KD to test joint encoder-decoder regularization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed knowledge distillation methods perform if applied to a more complex sequence-to-sequence model, such as a transformer with more encoder/decoder layers or a different pre-trained backbone?
- Basis in paper: [explicit] The authors use a base Wav2vec 2.0 encoder and a transformer decoder with 6 layers, and suggest future work on refining the seq-KD method.
- Why unresolved: The paper only evaluates the proposed KD methods on a specific model architecture, and it is unclear if the results would generalize to more complex models.
- What evidence would resolve it: Experimental results comparing the performance of the KD methods on different model architectures, including models with more layers or different pre-trained backbones.

### Open Question 2
- Question: How does the choice of the weighting parameter λKD in the total loss function affect the performance of the proposed methods?
- Basis in paper: [explicit] The authors use a fixed weighting parameter λKD in their experiments, but mention that more experiments are necessary to find the optimal weights when multiple KDs are used.
- Why unresolved: The paper does not explore the impact of different λKD values on the performance of the proposed methods, and it is unclear how sensitive the results are to this parameter.
- What evidence would resolve it: Experimental results showing the performance of the proposed methods with different λKD values, and an analysis of the sensitivity of the results to this parameter.

### Open Question 3
- Question: How would the proposed knowledge distillation methods perform in a different class-incremental learning scenario, such as one based on intent labels instead of scenario labels?
- Basis in paper: [explicit] The authors define a class-incremental learning scenario for the SLURP dataset based on scenario labels, but mention that the order of the scenarios depends on their cardinality.
- Why unresolved: The paper only evaluates the proposed methods in a specific class-incremental learning scenario, and it is unclear if the results would generalize to other scenarios.
- What evidence would resolve it: Experimental results comparing the performance of the proposed methods in different class-incremental learning scenarios, such as one based on intent labels or a different ordering of the scenarios.

## Limitations

- The effectiveness of the proposed KD methods is demonstrated on the SLURP dataset, which is split according to scenarios. The assumption that scenarios represent truly distinct and non-overlapping semantic domains is critical and unverified.
- The paper does not provide a detailed analysis of hyperparameter choices (e.g., KD weights, rehearsal buffer size, beam search width), which could affect the reported improvements.
- The claim that seq-KD is superior due to its ability to preserve long-range dependencies is plausible but not rigorously tested through ablation studies.

## Confidence

**High Confidence**:
- The seq-KD method improves intent accuracy and reduces WER compared to fine-tuning and token-KD.
- The combination of audio-KD and seq-KD yields the best overall performance.
- Catastrophic forgetting is a real issue in the SLURP class-incremental setting, as evidenced by the baseline fine-tuning results.

**Medium Confidence**:
- The claim that seq-KD is superior because it preserves sequence coherence and prevents error propagation is plausible but not definitively proven.
- The assumption that scenarios in SLURP are sufficiently distinct to justify their use as task boundaries is reasonable but unverified.

**Low Confidence**:
- The exact contribution of audio-KD beyond seq-KD alone is unclear, as the ablation study does not isolate their individual effects in detail.
- The generalizability of the results to other SLU datasets or real-world scenarios is uncertain.

## Next Checks

1. **Ablation on Rehearsal Buffer Size**: Systematically vary the rehearsal buffer size (e.g., 0.5%, 1%, 2%) and evaluate the impact on forgetting and performance. This will clarify whether the 1% choice is optimal or if larger buffers yield diminishing returns.

2. **Teacher Model Quality Analysis**: Evaluate the quality of the teacher model's beam search outputs by measuring the accuracy of the generated sequences on rehearsal data. If the teacher's outputs are noisy, seq-KD may be reinforcing errors, which would explain any underperformance.

3. **Cross-Dataset Validation**: Apply the same KD methods to a different SLU dataset (e.g., Fluent Speech Commands or SNIPS) with a different task boundary definition (e.g., intents or domains instead of scenarios). This will test whether the proposed mechanisms are dataset-specific or more broadly applicable.