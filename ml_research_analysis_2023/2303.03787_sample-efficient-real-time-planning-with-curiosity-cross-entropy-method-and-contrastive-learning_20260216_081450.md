---
ver: rpa2
title: Sample-efficient Real-time Planning with Curiosity Cross-Entropy Method and
  Contrastive Learning
arxiv_id: '2303.03787'
source_url: https://arxiv.org/abs/2303.03787
tags:
- learning
- reward
- planning
- latent
- intrinsic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of sample-efficient reinforcement
  learning from high-dimensional image observations in continuous control tasks. The
  authors propose Curiosity Cross-Entropy Method (CCEM), an improved version of the
  Cross-Entropy Method (CEM) planning algorithm that encourages exploration through
  curiosity-based intrinsic rewards.
---

# Sample-efficient Real-time Planning with Curiosity Cross-Entropy Method and Contrastive Learning

## Quick Facts
- arXiv ID: 2303.03787
- Source URL: https://arxiv.org/abs/2303.03787
- Reference count: 40
- Key outcome: CCEM significantly outperforms previous model-based RL algorithms in sample efficiency, achieving highest average return on four out of six tasks at 100k environment steps

## Executive Summary
This paper addresses the challenge of sample-efficient reinforcement learning from high-dimensional image observations in continuous control tasks. The authors propose Curiosity Cross-Entropy Method (CCEM), an improved version of the Cross-Entropy Method (CEM) planning algorithm that encourages exploration through curiosity-based intrinsic rewards. The key insight is to train a Q-value function offline to estimate both extrinsic and intrinsic rewards, allowing the planner to maximize cumulative Q-values over the planning horizon without requiring ground truth future observations during planning. The method also incorporates contrastive representation learning for efficient latent representation learning. Experiments on six image-based continuous control tasks from the DeepMind Control Suite demonstrate that CCEM significantly outperforms previous model-based RL algorithms in sample efficiency.

## Method Summary
CCEM combines temporal difference learning, contrastive representation learning, and curiosity-based exploration for sample-efficient model-based RL from image observations. The method trains a Task-Oriented Latent Dynamics (TOLD) model that includes an encoder, latent dynamics model, reward model, value function, and policy. An inverse dynamics model computes intrinsic curiosity rewards via prediction error, while contrastive learning improves latent representations by maximizing temporal mutual information. During planning, CCEM uses a Q-value based scoring function that estimates cumulative extrinsic and intrinsic rewards over the planning horizon, encouraging exploration of novel states. The method is evaluated on six DeepMind Control Suite tasks using 3 stacked frames of 84×84 RGB images.

## Key Results
- CCEM achieves highest average return on four out of six tasks at 100k environment steps
- Significantly outperforms previous model-based RL algorithms in sample efficiency
- Compares favorably with best model-free RL methods while maintaining real-time planning capability
- Demonstrates robust performance across diverse continuous control tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Curiosity-based intrinsic rewards improve exploration in high-dimensional environments where standard CEM fails
- Mechanism: By training a Q-function to estimate both extrinsic and intrinsic rewards offline, the planner can maximize cumulative Q-values over the planning horizon without needing future observations during planning
- Core assumption: The intrinsic reward estimated offline during training remains a reliable signal for exploration during planning
- Evidence anchors:
  - [abstract] "Our proposed method maximizes the sum of state-action Q values over the planning horizon, in which these Q values estimate the future extrinsic and intrinsic reward, hence encouraging reaching novel observations."
  - [section] "To overcome this challenge, we propose to compute the intrinsic reward offline during training, as the ground truth observations are available, then train a Q value function to estimate future extrinsic and intrinsic reward."
- Break condition: If the intrinsic reward signal becomes unreliable or decoupled from actual novelty during planning, exploration may degrade

### Mechanism 2
- Claim: Contrastive representation learning improves sample efficiency by learning temporally predictive latent representations
- Mechanism: The method maximizes temporal mutual information between joint representations of current observation and action with next observation representation
- Core assumption: Temporal contrastive loss creates representations that capture task-relevant dynamics while being invariant to data augmentation
- Evidence anchors:
  - [abstract] "In addition, our model uses contrastive representation learning to efficiently learn latent representations."
  - [section] "To efficiently learn representations, we use contrastive learning in the form of maximizing the temporal mutual information between the joint representations of the current observation and action and the representation of the next observation."
- Break condition: If the contrastive learning objective dominates and loses task-specific information, planning performance may degrade

### Mechanism 3
- Claim: The Q-value based scoring function in CCEM provides better exploration than traditional CEM variants
- Mechanism: Instead of using simple reward sums or terminal value additions, CCEM uses discounted sum of Q values that incorporate both extrinsic and intrinsic rewards, encouraging exploration of novel states
- Core assumption: Q-values trained to estimate future extrinsic and intrinsic rewards provide a more effective scoring function for exploration than traditional reward-based approaches
- Evidence anchors:
  - [section] "During planning, we follow TD-MPC [2] except that, CCEM computes a discounted sum of Q values over the planning horizon as the scoring function to evaluate the sampled action sequences."
  - [section] "Since Qθ is trained by Eq. 3 to estimate extrinsic and intrinsic reward, CCEM encourages exploring novel states."
- Break condition: If Q-value estimation becomes inaccurate or overfits to training data, the exploration benefits may disappear

## Foundational Learning

- Concept: Cross-Entropy Method (CEM) optimization
  - Why needed here: CEM is the core planning algorithm that CCEM improves upon; understanding its mechanics is essential for implementing the improvements
  - Quick check question: How does CEM select and refine action sequences over iterations?

- Concept: Contrastive representation learning
  - Why needed here: The temporal contrastive loss is critical for efficient latent representation learning; engineers need to understand InfoNCE and temporal mutual information concepts
  - Quick check question: What is the difference between temporal contrastive loss and standard contrastive learning approaches?

- Concept: Intrinsic curiosity modules and prediction error
  - Why needed here: ICM provides the intrinsic reward signal that drives exploration; understanding how prediction error is computed and normalized is crucial
  - Quick check question: How does the intrinsic reward computation balance exploration with stability during training?

## Architecture Onboarding

- Component map:
  - Encoder (hθ) -> Latent dynamics (dθ) -> Reward model (Rθ) -> Q-value function (Qθ) -> Policy (πθ)
  - Inverse dynamics (Iφ) -> Intrinsic reward computation
  - Action encoder (gψ) -> Contrastive learning module
  - Target networks (h̄θ, Q̄θ) -> Exponential moving average copies

- Critical path:
  1. Sample trajectory from replay buffer
  2. Encode observations to latent states
  3. Compute intrinsic rewards via ICM
  4. Train Q-function to estimate extrinsic+intrinsic rewards
  5. Train latent models and policy
  6. During planning: Sample action sequences and evaluate using Q-values

- Design tradeoffs:
  - Intrinsic weight (C) vs exploration vs exploitation balance
  - Contrastive loss coefficient vs representation quality vs training stability
  - Planning horizon length vs computational cost vs performance
  - Target network update frequency vs stability vs responsiveness

- Failure signatures:
  - Poor exploration: Agent gets stuck in local optima, low intrinsic reward magnitude
  - Unstable training: High variance in Q-values, exploding/vanishing gradients
  - Inefficient representations: Low correlation between consecutive latent states, poor planning performance

- First 3 experiments:
  1. Validate intrinsic reward computation: Run ICM on validation data and visualize prediction errors vs actual novelty
  2. Test contrastive learning: Compare latent representations with and without contrastive loss using nearest neighbor analysis
  3. Benchmark planning performance: Compare CCEM scoring function against baseline CEM variants on a simple control task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CCEM perform in environments with very sparse rewards compared to dense rewards?
- Basis in paper: [inferred] The paper mentions that curiosity-based exploration is helpful in sparse reward settings, but does not provide specific comparisons between sparse and dense reward environments
- Why unresolved: The experiments primarily focus on continuous control tasks which may have varying reward densities, but do not isolate the effect of reward sparsity
- What evidence would resolve it: Direct comparison of CCEM performance on the same tasks with deliberately modified reward structures (sparse vs. dense) while keeping other factors constant

### Open Question 2
- Question: What is the computational overhead of CCEM compared to standard CEM during inference time?
- Basis in paper: [explicit] The paper mentions that computing intrinsic rewards offline during training is computationally efficient, but does not provide runtime comparisons between CCEM and standard CEM
- Why unresolved: While the method's design suggests potential computational benefits, actual timing measurements are not provided
- What evidence would resolve it: Detailed timing measurements of planning steps for both CCEM and standard CEM on identical hardware and tasks

### Open Question 3
- Question: How sensitive is CCEM to the choice of hyperparameters for intrinsic reward weighting and decay?
- Basis in paper: [explicit] The paper mentions that intrinsic reward is normalized and decayed during training, but does not provide sensitivity analysis
- Why unresolved: The paper uses fixed hyperparameters across tasks without exploring their impact on performance
- What evidence would resolve it: Systematic experiments varying the intrinsic weight C and decay weight α across a range of values to quantify their impact on performance

## Limitations

- Evaluation scope limited to six DeepMind Control Suite tasks, effectiveness on more complex environments untested
- Reliance on specific data augmentation techniques (±4 pixel shifts) may not generalize to all image-based control tasks
- Computational overhead of training Q-function alongside latent models may limit real-world applicability in resource-constrained settings

## Confidence

- **High confidence**: The mechanism of using offline-trained Q-values to incorporate intrinsic rewards during planning is well-supported by experimental results and theoretical justification
- **Medium confidence**: The effectiveness of contrastive representation learning for model-based RL planning, while demonstrated, lacks extensive ablation studies to isolate its specific contribution
- **Medium confidence**: The sample efficiency claims are well-supported by experimental results, though comparison against model-free methods could be more comprehensive

## Next Checks

1. **Ablation study on contrastive loss**: Systematically evaluate the impact of the contrastive representation learning component by training versions of CCEM with varying strengths of the temporal contrastive loss, including removing it entirely, to quantify its specific contribution to performance improvements

2. **Generalization test across augmentation techniques**: Evaluate CCEM's robustness to different data augmentation strategies beyond the ±4 pixel shifts, including random cropping, color jittering, and cutout, to assess the method's sensitivity to augmentation choices

3. **Computational overhead analysis**: Measure and compare the wall-clock training time and planning latency of CCEM against baseline methods to provide a more complete picture of the computational trade-offs associated with the improved sample efficiency