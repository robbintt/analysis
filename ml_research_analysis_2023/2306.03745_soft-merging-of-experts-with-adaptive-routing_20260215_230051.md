---
ver: rpa2
title: Soft Merging of Experts with Adaptive Routing
arxiv_id: '2306.03745'
source_url: https://arxiv.org/abs/2306.03745
tags:
- uni0030
- uni002e
- uni0031
- uni0032
- uni0033
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SMEAR, a method for training sparsely activated
  neural networks that use conditional computation to route inputs through different
  expert subnetworks. The key idea is to construct a single merged expert by averaging
  the parameters of all experts, weighted by the router's distribution, allowing for
  standard gradient-based training without the need for gradient estimation techniques.
---

# Soft Merging of Experts with Adaptive Routing

## Quick Facts
- arXiv ID: 2306.03745
- Source URL: https://arxiv.org/abs/2306.03745
- Reference count: 40
- Key outcome: SMEAR outperforms models with learned routing using gradient estimation, heuristic routing strategies, and state-of-the-art baselines for learning modular models.

## Executive Summary
This paper proposes SMEAR, a method for training sparsely activated neural networks that use conditional computation to route inputs through different expert subnetworks. The key idea is to construct a single merged expert by averaging the parameters of all experts, weighted by the router's distribution, allowing for standard gradient-based training without the need for gradient estimation techniques. The method is evaluated on two real-world settings: fine-tuning T5 on GLUE and fine-tuning ResNet18 on DomainNet. SMEAR outperforms models with learned routing using gradient estimation, heuristic routing strategies, and state-of-the-art baselines for learning modular models, while retaining similar computational costs.

## Method Summary
SMEAR works by using the router's distribution over experts to compute a weighted average of the parameters of the individual experts. This creates a single merged expert that can be trained using standard gradient-based methods. The method is evaluated on two real-world settings: fine-tuning T5 on GLUE and fine-tuning ResNet18 on DomainNet. SMEAR outperforms models with learned routing using gradient estimation, heuristic routing strategies, and state-of-the-art baselines for learning modular models, while retaining similar computational costs.

## Key Results
- SMEAR outperforms models with learned routing using gradient estimation, heuristic routing strategies, and state-of-the-art baselines for learning modular models
- SMEAR retains similar computational costs to discrete routing while providing better performance
- SMEAR learns effective routing strategies that outperform heuristic and gradient estimation-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SMEAR enables gradient-based training of models with learned routing without increasing computational costs.
- Mechanism: By computing a weighted average of expert parameters based on the router's output distribution, SMEAR creates a single merged expert. Activations are routed through this merged expert, making all operations differentiable and allowing standard backpropagation.
- Core assumption: All experts in a routing block share the same architecture, enabling a natural one-to-one mapping between parameters.
- Evidence anchors:
  - [abstract]: "SMEAR works by using the router's distribution over experts to compute a weighted average of the parameters of the individual experts."
  - [section]: "By averaging parameters, SMEAR implicitly assumes that all experts in the routing block share an identical architecture."
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.43, average citations=0.0. Top related titles: Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy, Retraining-Free Merging of Sparse MoE via Hierarchical Clustering, Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers."

### Mechanism 2
- Claim: SMEAR achieves similar computational costs to discrete routing while providing better performance.
- Mechanism: SMEAR computes the output of a single merged expert rather than multiple individual experts. The additional cost of averaging expert parameters is negligible compared to the main computation.
- Core assumption: The input sequence length is large enough that the overhead of averaging expert parameters is insignificant.
- Evidence anchors:
  - [section]: "We therefore focus on settings where models make a single routing choice for an entire input example... This results in a total cost of approximately (L × 4 + N × 2) × d × m for SMEAR."
  - [section]: "We find in section 4.1 that the wall-clock time required to process an example with SMEAR in real-world experiments is roughly the same as using discrete routing and significant faster than ensemble routing."
  - [corpus]: "Weak evidence from corpus - no direct mention of SMEAR's computational efficiency."

### Mechanism 3
- Claim: SMEAR learns effective routing strategies that outperform heuristic and gradient estimation-based approaches.
- Mechanism: SMEAR learns a router that produces a distribution over experts, which is used to compute the weighted average of expert parameters. This allows the model to specialize experts to different types of inputs.
- Core assumption: The router can learn to produce meaningful distributions that lead to effective specialization of experts.
- Evidence anchors:
  - [section]: "In experimental settings covering different modalities and model architectures, we found that SMEAR outperformed all models with discrete routing as well as performant heuristic routing strategies."
  - [section]: "Through qualitative analysis, we further confirmed that the experts learned in a model using SMEAR specialize to different types of inputs and that the router learns a nontrivial strategy that exploits commonalities across different examples."
  - [corpus]: "Weak evidence from corpus - no direct mention of SMEAR's routing performance."

## Foundational Learning

- Concept: Gradient estimation techniques for discrete operations
  - Why needed here: Traditional models with learned routing use gradient estimation to train routers that make discrete decisions. Understanding these techniques is crucial for appreciating SMEAR's advantage.
  - Quick check question: What are the main gradient estimation techniques used for training routers in sparsely activated models?

- Concept: Parameter averaging and merging in neural networks
  - Why needed here: SMEAR relies on averaging expert parameters to create a merged expert. Understanding how parameter averaging works is essential for grasping SMEAR's mechanism.
  - Quick check question: What are the benefits and limitations of averaging model parameters?

- Concept: Conditional computation and expert routing
  - Why needed here: SMEAR is a method for sparsely activated neural networks that use conditional computation. Understanding the basics of conditional computation and expert routing is necessary for understanding SMEAR's context.
  - Quick check question: How do sparsely activated neural networks with conditional computation differ from densely activated models?

## Architecture Onboarding

- Component map:
  - Input -> Router -> Expert parameters averaging -> Merged expert -> Output

- Critical path:
  1. Input enters routing block
  2. Router produces distribution over experts
  3. Expert parameters are averaged based on router distribution
  4. Input is processed by merged expert
  5. Output is used as routing block output

- Design tradeoffs:
  - Pro: Enables gradient-based training of learned routing without increasing computational costs
  - Pro: Allows for specialization of experts to different types of inputs
  - Con: Requires experts to have identical architectures
  - Con: May not work well if the number of experts is very large or input sequence length is very short

- Failure signatures:
  - Poor performance: Router fails to learn meaningful distributions, leading to ineffective specialization of experts
  - High computational cost: Number of experts is too large or input sequence length is too short, making parameter averaging overhead significant
  - Instability: Experts have different architectures, causing issues with parameter averaging

- First 3 experiments:
  1. Implement SMEAR in a simple setting (e.g., MNIST classification with expert routing blocks)
  2. Compare SMEAR performance to discrete routing and heuristic routing in a controlled experiment
  3. Visualize router distributions and expert specialization in a real-world setting (e.g., T5-GLUE)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SMEAR scale when the number of experts exceeds the number of tasks or domains, and what are the theoretical limits of this scaling?
- Basis in paper: [explicit] The paper shows a single experiment doubling the number of experts in ResNet-DomainNet but finds no benefit for T5-GLUE, suggesting limits to scaling.
- Why unresolved: The paper only tests a 2× increase in experts, and doesn't explore deeper scaling or analyze theoretical limits of when adding more experts becomes detrimental.
- What evidence would resolve it: Systematic scaling experiments with 4×, 8×, 16× experts, and analysis of expert utilization patterns as capacity increases.

### Open Question 2
- Question: How does SMEAR's performance compare to state-of-the-art routing methods like GShard or Switch Transformers when applied to large-scale pre-trained models?
- Basis in paper: [inferred] The paper focuses on moderate-scale models and doesn't test SMEAR against recent large-scale conditional computation methods.
- Why unresolved: The authors note interest in testing SMEAR at larger scales but don't have the computational resources to do so, leaving a gap in understanding SMEAR's competitiveness at scale.
- What evidence would resolve it: Direct comparisons of SMEAR vs. GShard/Switch Transformers on large pre-trained models like GPT-3 or T5-XXL.

### Open Question 3
- Question: What is the impact of SMEAR on model robustness to distribution shift and adversarial examples compared to discrete routing methods?
- Basis in paper: [inferred] The paper focuses on in-distribution performance but doesn't examine robustness properties of SMEAR versus discrete routing.
- Why unresolved: The authors don't test robustness properties, and the averaging of expert parameters in SMEAR might have different effects on robustness compared to discrete routing.
- What evidence would resolve it: Systematic robustness evaluation including adversarial attacks, domain generalization tests, and out-of-distribution detection across SMEAR and discrete routing baselines.

### Open Question 4
- Question: How does the choice of expert architecture (e.g., linear vs. MLP) affect SMEAR's performance and the learned routing patterns?
- Basis in paper: [explicit] The paper mentions that all experts share an identical architecture but doesn't explore how different architectures affect performance.
- Why unresolved: The paper uses a standard dense layer architecture but doesn't investigate whether alternative architectures (deeper experts, convolutional experts for vision) would perform better.
- What evidence would resolve it: Controlled experiments varying expert architecture complexity and structure across multiple tasks, measuring both performance and routing specialization.

## Limitations
- SMEAR assumes all experts share identical architectures, limiting its applicability to heterogeneous expert settings
- Performance benefits are demonstrated primarily on specific architectures (T5 and ResNet18) with relatively small expert counts (3-4 experts)
- The paper doesn't thoroughly address potential instability in router training or cases where experts fail to specialize effectively

## Confidence
- **High confidence**: SMEAR successfully enables gradient-based training of learned routing without increased computational costs compared to discrete routing
- **Medium confidence**: SMEAR outperforms heuristic routing strategies and state-of-the-art baselines in the tested settings, though results may not generalize to all architectures
- **Medium confidence**: The computational efficiency claims are supported by analysis, but real-world scaling behavior with larger expert counts remains untested

## Next Checks
1. Test SMEAR with larger numbers of experts (10+) to evaluate whether parameter averaging overhead becomes significant
2. Implement SMEAR on architectures beyond T5 and ResNet18 to assess generalizability
3. Conduct ablation studies varying router architecture and initialization to understand sensitivity to these design choices