---
ver: rpa2
title: Making RL with Preference-based Feedback Efficient via Randomization
arxiv_id: '2310.14554'
source_url: https://arxiv.org/abs/2310.14554
tags:
- have
- lemma
- where
- learning
- inequality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of reinforcement learning from preference-based
  feedback, where the agent receives feedback in the form of preferences over pairs
  of trajectories rather than per-step rewards. The goal is to design algorithms that
  are efficient in terms of statistical complexity (regret), computational complexity,
  and query complexity.
---

# Making RL with Preference-based Feedback Efficient via Randomization

## Quick Facts
- arXiv ID: 2310.14554
- Source URL: https://arxiv.org/abs/2310.14554
- Reference count: 40
- This paper proposes randomized algorithms for RL with preference-based feedback that achieve sublinear regret and polynomial computational efficiency simultaneously.

## Executive Summary
This paper addresses reinforcement learning from preference-based feedback, where agents receive comparisons between trajectories rather than explicit rewards. The authors introduce randomization as a key technique to balance exploration, exploitation, and feedback efficiency. They propose PR-LSVI for linear MDPs and PbTS for general function approximation, both achieving near-optimal tradeoffs between regret and query complexity. The algorithms use Gaussian noise injection and variance-based uncertainty measures to make computationally tractable decisions about when to query for preferences.

## Method Summary
The approach uses randomization to enable efficient RL from preference feedback. For linear MDPs, PR-LSVI learns a maximum likelihood reward model from preferences, then injects Gaussian noise into value function estimates to encourage exploration while maintaining computational tractability through standard dynamic programming oracles. The algorithm selectively queries preferences when uncertainty exceeds a threshold, estimated via sampling from the reward model distribution. For general function approximation, PbTS uses Thompson sampling by forming posterior distributions over transition and reward models, sampling from these posteriors for planning. Both methods avoid computationally expensive version space construction by using randomized estimates of uncertainty.

## Key Results
- PR-LSVI achieves regret bound of O(√(dT)) and query complexity of O(d^(4/3)T^(2/3)) for linear MDPs
- PbTS achieves Bayesian regret bound of O(T^(1-β)) and Bayesian query complexity of O(T^(2β)) for general function approximation
- Both algorithms simultaneously achieve sublinear regret and polynomial computational efficiency
- The variance-based uncertainty measure enables efficient active learning without expensive version space construction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding Gaussian noise to reward and value function estimates enables efficient exploration while preserving computational tractability.
- Mechanism: The algorithm samples noise from a Gaussian distribution centered at the MLE estimates, using covariance matrices that track trajectory-wise feature differences. This preserves the Markov property needed for dynamic programming while encouraging exploration.
- Core assumption: The Gaussian noise injection does not significantly bias the learning process and the covariance matrix accurately captures uncertainty in the reward model.
- Evidence anchors:
  - [abstract]: "The key idea is to use randomization in algorithm design... Randomization allows the algorithms to avoid computationally intractable operations and only use standard dynamic programming oracles"
  - [section 4.1]: "Given the MLE ˆθr,t , it next samples θr,t from a Gaussian distribution centered at ˆθr,t... The noise aims to encourage exploration"
- Break condition: If the Gaussian noise overwhelms the true signal or the covariance matrix fails to capture relevant uncertainty, exploration becomes ineffective.

### Mechanism 2
- Claim: The variance-style uncertainty quantification for query conditions enables computationally efficient active learning.
- Mechanism: The algorithm computes expected absolute reward differences between trajectories under sampled reward models, querying only when this uncertainty exceeds a threshold. This avoids expensive version space construction.
- Core assumption: The expected disagreement under sampled models accurately approximates true preference uncertainty and can be estimated efficiently.
- Evidence anchors:
  - [section 4.1]: "Given the trajectories τ0 t and τ1 t generated by π 0 t and π 1 t , we compute the expected absolute reward difference... This represents the uncertainty of the preference between the two trajectories"
  - [section 4.1]: "Computation-wise, we can estimate this expectation by drawing polynomially many reward models from the distribution... and computing the empirical average"
- Break condition: If the sampling-based estimation is too noisy or the variance measure poorly correlates with true uncertainty, query efficiency degrades.

### Mechanism 3
- Claim: Drawing trajectory pairs from different policies (current vs. previous greedy) maintains conditional independence needed for optimism proofs.
- Mechanism: The algorithm compares the current greedy policy with the previous episode's greedy policy rather than comparing two trajectories from the same policy. This ensures the comparator policy is conditionally independent of the current round's noise.
- Evidence anchors:
  - [abstract]: "Our theory shows that drawing two trajectories from a combination of new and older policies balances exploration and exploitation better"
  - [section 4.1]: "The comparator policy π 1 t is simply set to be the greedy policy from the previous episode, π 0 t−1"
- Break condition: If policy convergence is too rapid, the two policies become too similar, reducing exploration benefits.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The entire framework operates within finite-horizon MDPs where policies generate trajectories and rewards depend on state-action transitions.
  - Quick check question: In an MDP with states {A,B}, actions {left,right}, and transition probabilities P(B|A,right)=1, what is the next state if we start at A and take action right?

- Concept: Preference-based feedback models
  - Why needed here: The algorithms must learn from pairwise trajectory preferences rather than explicit rewards, requiring understanding of link functions and preference generation.
  - Quick check question: If trajectory A has reward 5 and trajectory B has reward 3, and Φ(x) = 1/(1+exp(-x)), what is the probability that A is preferred to B?

- Concept: Eluder dimension and statistical complexity
  - Why needed here: The theoretical analysis uses eluder dimension to characterize function approximation complexity, which determines regret and query complexity bounds.
  - Quick check question: For a linear function class in d dimensions, what is the eluder dimension with respect to ℓ2 norm?

## Architecture Onboarding

- Component map: MLE → Noise injection → Value iteration → Policy generation → Trajectory sampling → Preference query decision → Feedback incorporation
- Critical path: Each component must complete before the next can begin in each episode
- Design tradeoffs: The noise variance σr must balance exploration (larger noise) against estimation accuracy (smaller noise). The query threshold ǫ must balance feedback efficiency (higher threshold) against learning speed (lower threshold).
- Failure signatures: High regret despite many queries suggests poor noise calibration. Low regret with few queries suggests overly conservative querying. Computational slowdown suggests covariance matrix tracking issues.
- First 3 experiments:
  1. Test noise-free version on a simple linear MDP to verify baseline performance and identify exploration deficiencies.
  2. Vary σr across orders of magnitude to find the optimal exploration-exploitation balance.
  3. Compare variance-based querying against random querying to quantify query efficiency gains.

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the optimal dependence on the feature dimension d in the regret and query complexity bounds for the linear MDP setting?
  - Basis in paper: Explicit - The paper states that while the dependence on T is tight, the dependence on other parameters like d and H can be loose.
  - Why unresolved: The authors mention that further investigation of these factors is left as future work.
  - What evidence would resolve it: A refined analysis that provides tighter bounds on the dependence of regret and query complexity on d and H, or a lower bound that matches the upper bound.

- **Open Question 2**: Can the Bayesian regret bound for the Thompson sampling algorithm be improved to have a better dependence on the eluder dimension?
  - Basis in paper: Explicit - The paper mentions that the term (i) in the Bayesian regret bound improves the dependence on the eluder dimension compared to prior work, but it would be desired to derive a query complexity upper bound that scales as ~O(T^2β·dim1(˜R, ǫ/2)) which attains the favorable dependence on both T and the ℓ1-norm eluder dimension.
  - Why unresolved: The authors state that this is left as future work.
  - What evidence would resolve it: A new analysis that provides a Bayesian regret bound with a better dependence on the ℓ1-norm eluder dimension, or a lower bound that matches the improved upper bound.

- **Open Question 3**: How can the algorithm be extended to handle non-Markovian rewards?
  - Basis in paper: Explicit - The paper mentions that the algorithm can be easily extended to the case where the reward function is non-Markovian without any change in the algorithm and the analysis. However, it is unclear how to solve the planning problem efficiently for a non-Markovian reward in tabular MDPs.
  - Why unresolved: The paper states that the computation intractability in the planning phase makes non-Markovian rewards not easily applicable in practice.
  - What evidence would resolve it: A computationally efficient planning algorithm for non-Markovian rewards in tabular MDPs, or a theoretical analysis that shows the impossibility of such an algorithm.

- **Open Question 4**: Can the variance-style uncertainty measure for designing the query condition be further improved?
  - Basis in paper: Explicit - The paper mentions that the variance-style uncertainty measure is more computationally tractable compared to more standard active learning procedures that rely on constructing version space and confidence intervals.
  - Why unresolved: The paper does not provide a comparison of the performance of the variance-style uncertainty measure with other active learning procedures.
  - What evidence would resolve it: An empirical study that compares the performance of the variance-style uncertainty measure with other active learning procedures in terms of regret, query complexity, and computational efficiency.

## Limitations
- Limited empirical validation: The paper relies heavily on theoretical bounds rather than extensive experimental results
- Computational complexity assumptions: Claims about polynomial computational efficiency depend on assumptions about dynamic programming oracle efficiency
- Uncertainty measure correlation: The relationship between variance-based uncertainty and true preference uncertainty is not empirically verified

## Confidence
- Theoretical regret bounds (High): The sublinear regret analysis for PR-LSVI follows standard techniques with proper randomization arguments.
- Query complexity tradeoffs (Medium): The near-optimal tradeoff between regret and query complexity is theoretically justified but lacks empirical validation.
- Practical computational efficiency (Low): While the algorithms avoid version space construction, the actual runtime and scalability to larger problems is not demonstrated.

## Next Checks
1. **Empirical validation of query efficiency**: Implement the variance-based query selection and compare query counts against random querying baselines across multiple MDP instances to verify the claimed O(d^(4/3)T^(2/3)) scaling.

2. **Sensitivity analysis of noise parameters**: Systematically vary the Gaussian noise variance σr and query threshold ǫ across multiple problem instances to identify robust parameter settings and verify the exploration-exploitation tradeoff claims.

3. **Scalability testing**: Evaluate PR-LSVI on MDPs with increasing state and action space sizes to verify that the computational complexity remains polynomial and that dynamic programming oracles remain tractable in practice.