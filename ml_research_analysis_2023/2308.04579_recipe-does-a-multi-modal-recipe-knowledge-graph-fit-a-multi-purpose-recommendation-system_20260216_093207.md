---
ver: rpa2
title: 'RECipe: Does a Multi-Modal Recipe Knowledge Graph Fit a Multi-Purpose Recommendation
  System?'
arxiv_id: '2308.04579'
source_url: https://arxiv.org/abs/2308.04579
tags:
- recipe
- recommendation
- recipes
- image
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RECipe, a multi-purpose recipe recommendation
  framework that uses a multi-modal knowledge graph (MMKG) backbone. RECipe consists
  of three subsystems: behavior-based, review-based, and image-based recommenders,
  each relying on embedding representations of entities and relations in the graph.'
---

# RECipe: Does a Multi-Modal Recipe Knowledge Graph Fit a Multi-Purpose Recommendation System?

## Quick Facts
- arXiv ID: 2308.04579
- Source URL: https://arxiv.org/abs/2308.04579
- Authors: 
- Reference count: 40
- Primary result: Multi-modal knowledge graph embeddings outperform neural baselines for recipe recommendation, with pre-trained NLP embeddings improving cold-start performance and conditional recommendations enhancing accuracy

## Executive Summary
This paper introduces RECipe, a multi-purpose recipe recommendation framework built on a multi-modal knowledge graph (MMKG) backbone. The framework addresses challenges in recipe recommendation through three specialized subsystems: behavior-based, review-based, and image-based recommenders. By leveraging knowledge graph embeddings trained on multi-modal entities (recipes, ingredients, users, reviews, images), RECipe achieves state-of-the-art performance on two recipe datasets while addressing cold-start problems and enabling conditional recommendations.

## Method Summary
RECipe constructs a multi-modal knowledge graph from recipe datasets, then trains RotatE knowledge graph embeddings on the Person-Recipe subgraph for behavior-based recommendation. For review-based recommendation, it uses pre-trained MPNet embeddings aligned to KG space for matching queries to recipes. The image-based recommender employs a KGE-guided variational autoencoder to learn latent representations of recipe images. Conditional recommendation is implemented by clustering recipes and decoupling person nodes into person@recipe-cluster nodes, preventing confusion when users have diverse tastes across different recipe categories.

## Key Results
- KGE models achieve comparable performance to neural solutions (MLP, NeuMF, CDAE, CFGAN, JCA) with Hit@10 improvements up to 37% over non-conditional baselines
- Pre-trained NLP embeddings significantly improve cold-start recommendation, outperforming random initialization for new users
- Conditional recommendations based on recipe clusters increase Hit@10 by 37% and 30% for Food.com and Allrecipes.com datasets respectively
- Image-based recommender successfully generates relevant recipe images using KGE-guided VAE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal embeddings outperform unimodal representations for recipe recommendation
- Mechanism: Knowledge graph embeddings trained on multi-modal entities capture richer relational structure than traditional collaborative filtering, allowing cross-modal reasoning (e.g., using image similarity to recommend recipes similar to ones a user liked)
- Core assumption: Embedding space preserves semantic similarity across modalities when entities share underlying attributes (ingredients, cooking methods, user preferences)
- Evidence anchors:
  - [abstract]: "The framework addresses challenges in recipe recommendation, such as the cold start problem and the need for conditional recommendations"
  - [section 3.1]: "The behavior-based recommender relies on the links among the person and recipe entities in the graph"
  - [corpus]: Weak - no direct comparison of multi-modal vs unimodal KGE performance found in neighbors
- Break condition: When entity relationships become too sparse or when modalities don't share meaningful cross-modal attributes

### Mechanism 2
- Claim: Pre-trained NLP embeddings improve cold-start recommendation performance
- Mechanism: Initializing user embeddings with aligned MPNet representations of their reviews/profile data provides better starting points than random initialization, allowing KGE models to learn more accurate user-recipe relationships even with limited interaction data
- Core assumption: User reviews contain sufficient signal about user preferences that can be captured by language models and transferred to KG embedding space
- Evidence anchors:
  - [section 3.1]: "We first obtain (pre-trained) embedding representations of textual entities, such as reviews or ingredients, from a fine-tuned model of Microsoft's MPNet"
  - [section 4.4]: "We present pre-trained NLP embeddings to address important applications such as zero-shot inference for new users"
  - [section 4.4]: Experimental results showing RotatE+KG-aligned significantly outperforms random initialization for cold-start users
- Break condition: When user reviews are too generic or sparse to provide meaningful preference signals

### Mechanism 3
- Claim: Conditional recommendation by recipe clusters improves ranking quality
- Mechanism: Decoupling person nodes into person@recipe-cluster nodes prevents confusing recommendations when users have diverse tastes across unrelated recipe categories, allowing the model to learn cluster-specific preference patterns
- Core assumption: Users tend to have coherent preferences within recipe categories but may have very different preferences across categories
- Evidence anchors:
  - [section 3.3]: "We introduce conditional recommendation (CR) based on the clusters of recipes"
  - [section 4.5]: "We derive 44 and 52 well-separated clusters for the Food.com and Allrecipes.com recipes, respectively"
  - [section 4.6]: Experimental results showing Hit@10 increased by 37% and 30% for Food.com and Allrecipes.com respectively with conditional recommendation
- Break condition: When users have consistent preferences across all recipe categories or when cluster boundaries don't align with user preference patterns

## Foundational Learning

- Concept: Knowledge Graph Embeddings (KGE)
  - Why needed here: KGE models learn dense vector representations of entities and relations that capture complex multi-hop relationships, essential for reasoning across the recipe KG
  - Quick check question: What's the key difference between TransE and RotatE in modeling relations?

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: VAEs learn latent representations of recipe images while preserving uncertainty, crucial for the image-based recommendation subsystem
  - Quick check question: How does the KL divergence term in VAE loss function regularize the learned latent space?

- Concept: Transfer Learning with Pre-trained Language Models
  - Why needed here: MPNet embeddings provide rich semantic representations that can be aligned to KG space, solving the cold-start problem
  - Quick check question: Why might sentence-BERT be preferred over vanilla BERT for embedding reviews?

## Architecture Onboarding

- Component map: KG construction -> KGE training (RotatE) -> Entity embedding initialization -> Three subsystem training (behavior-based, review-based, image-based) -> Inference pipeline
- Critical path: KG construction → KGE training → Entity embedding initialization → Subsystem training → Inference pipeline
- Design tradeoffs: KGE provides strong relational reasoning but requires careful hyperparameter tuning; pre-trained embeddings help cold-start but may introduce domain mismatch; conditional recommendation improves accuracy but increases model complexity
- Failure signatures: Poor performance on sparse graphs, degraded results when modalities don't align semantically, overfitting when graph is small
- First 3 experiments:
  1. Test KGE performance on bipartite person-recipe subgraph alone vs full multi-modal KG
  2. Compare cold-start performance using random initialization vs MPNet-aligned embeddings
  3. Evaluate conditional vs non-conditional recommendation on a held-out test set with diverse user preferences

## Open Questions the Paper Calls Out

- Open Question 1: How does the performance of RECipe compare to other recipe recommendation systems on larger and more diverse datasets?
  - Basis in paper: [inferred] The paper mentions that the datasets used are large but highly sparse, which makes evaluating RS algorithms challenging. They filtered the datasets to keep only recipes and users with a certain number of reviews, but it is unclear how RECipe would perform on larger and more diverse datasets.
  - Why unresolved: The paper only presents experiments on two specific datasets (Food.com and Allrecipes.com) and does not provide a comparison with other recipe recommendation systems on larger and more diverse datasets.
  - What evidence would resolve it: Experiments on larger and more diverse datasets, comparing the performance of RECipe with other state-of-the-art recipe recommendation systems.

- Open Question 2: How does the inclusion of additional entity types, such as nutritional facts, affect the performance of RECipe for other purposes, such as nutritional fact prediction and ingredient replacement?
  - Basis in paper: [explicit] The paper mentions that for future work, they would like to expand the knowledge graphs by adding other entity types such as nutritional facts for other purposes, e.g., nutritional fact prediction and ingredient replacement.
  - Why unresolved: The paper does not present any experiments or results on the inclusion of additional entity types and their impact on the performance of RECipe for other purposes.
  - What evidence would resolve it: Experiments on the inclusion of additional entity types, such as nutritional facts, and their impact on the performance of RECipe for nutritional fact prediction and ingredient replacement tasks.

- Open Question 3: How does the conditional recommendation setting affect the performance of RECipe for users with diverse preferences?
  - Basis in paper: [explicit] The paper introduces conditional recommendation (CR) based on recipe clusters and mentions that it significantly improves the recommendation results for all evaluation measures. However, it does not discuss how the CR setting affects the performance for users with diverse preferences.
  - Why unresolved: The paper does not provide any analysis or experiments on the impact of the CR setting on the performance of RECipe for users with diverse preferences.
  - What evidence would resolve it: Experiments and analysis on the impact of the CR setting on the performance of RECipe for users with diverse preferences, considering different levels of diversity in user preferences.

## Limitations
- Evaluation limited to Western cuisine datasets (Food.com and Allrecipes.com) with English language content
- Cold-start performance tested on users with at least 10 reviews rather than true cold-start conditions (zero interactions)
- KG-VAE image generation lacks quantitative metrics for image quality or relevance

## Confidence
- High confidence: The core KGE-based recommendation mechanisms and their superiority over neural baselines (proven by multiple evaluation metrics across both datasets)
- Medium confidence: The cold-start improvements from pre-trained embeddings (based on limited cold-start test scenarios)
- Medium confidence: The conditional recommendation clustering approach (though the methodology is sound, the specific cluster assignments may not generalize)

## Next Checks
1. Test the framework on a more diverse recipe dataset including non-Western cuisines and multiple languages to assess cross-cultural generalization
2. Evaluate true cold-start performance (users with zero prior interactions) to validate the MPNet initialization claims
3. Conduct ablation studies removing each modality to quantify the actual contribution of multi-modal vs unimodal approaches