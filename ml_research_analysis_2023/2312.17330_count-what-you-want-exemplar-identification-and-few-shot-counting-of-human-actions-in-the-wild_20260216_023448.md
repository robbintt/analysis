---
ver: rpa2
title: 'Count What You Want: Exemplar Identification and Few-shot Counting of Human
  Actions in the Wild'
arxiv_id: '2312.17330'
source_url: https://arxiv.org/abs/2312.17330
tags:
- data
- counting
- action
- temporal
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses counting human actions of interest using wearable
  device sensor data. The proposed method leverages exemplars provided by user vocalizing
  "one", "two", "three" sounds to indicate the start of counting.
---

# Count What You Want: Exemplar Identification and Few-shot Counting of Human Actions in the Wild

## Quick Facts
- arXiv ID: 2312.17330
- Source URL: https://arxiv.org/abs/2312.17330
- Reference count: 7
- Primary result: Achieves average discrepancy of 7.47 between predicted and ground truth counts on 37 subjects and 50 action categories

## Executive Summary
This paper addresses the challenge of counting human actions of interest using sensor data from wearable devices. The method leverages user-provided exemplars through vocalized "one", "two", "three" sounds to indicate counting starts. An efficient dynamic programming algorithm detects these utterances in audio data, extracts exemplars, and uses them to generate exemplar-infused feature embeddings. The method then estimates temporal density to produce final counts. Experiments demonstrate viability in counting actions from new classes and subjects not seen during training.

## Method Summary
The proposed method follows a five-step approach: (1) Exemplar extraction using audio-based detection of predefined vocal sounds via constrained dynamic programming, (2) Sliding window feature embedding for both exemplars and sensor data, (3) Exemplar-based similarity estimation using Soft-DTW and correlation metrics, (4) Exemplar-infused feature embedding through similarity-based fusion, and (5) Density estimation using a Feature Pyramid Network to generate temporal density maps. The method includes pretraining on synthesized data and fine-tuning on real data with distance-preserving loss and squared error count loss.

## Key Results
- Achieves average discrepancy of 7.47 between predicted and ground truth counts
- Significantly outperforms frequency-based and transformer-based methods
- Demonstrates viability in counting actions from new classes and subjects not in training data

## Why This Works (Mechanism)

### Mechanism 1
Dynamic programming reliably localizes audio utterances ("one", "two", "three") even in noisy environments by maximizing classification scores while enforcing temporal ordering and proximity constraints via constrained optimization. The core assumption is that a pre-trained audio classifier provides sufficiently discriminative scores for utterance detection. If classification scores become unreliable due to overlapping speech or environmental noise, the DP will fail to find correct positions.

### Mechanism 2
Exemplar-infused embeddings improve similarity estimation between exemplars and query sequences by using a similarity map (combining Soft-DTW and correlation) to weight feature fusion, emphasizing regions matching exemplars. The core assumption is that exemplar-query similarity is meaningful for density estimation and counting. If exemplars poorly represent the target action or similarity metric fails to capture relevant patterns, density estimates become noisy.

### Mechanism 3
Distance-preserving loss stabilizes embedding learning under limited training data by enforcing that local neighborhood structure in raw data space is maintained in embedding space. The core assumption is that local patterns in raw window data are predictive of similarity relationships useful for counting. If k-NN graph construction is unstable, the loss may collapse or amplify noise.

## Foundational Learning

- **Concept**: Dynamic programming for constrained sequence optimization
  - Why needed here: Locates ordered utterances while respecting temporal distance bounds
  - Quick check question: Given utterance scores [0.9, 0.2, 0.8, 0.1, 0.7] and R=2, what positions does DP select?

- **Concept**: Multi-scale exemplar extraction
  - Why needed here: Captures actions at different temporal granularities
  - Quick check question: If action spans 30 timesteps, which scale (10, 20, or 40) best represents it?

- **Concept**: Feature pyramid networks for temporal density estimation
  - Why needed here: Extracts multi-scale temporal features before counting
  - Quick check question: How does FPN pooling size affect resolution of final density map?

## Architecture Onboarding

- **Component map**: Audio preprocessing -> DP utterance detector -> Exemplar extraction (multi-scale) -> Sliding window encoder -> Similarity estimator (Soft-DTW + corr) -> Fusion blocks -> FPN -> Density head -> Count output
- **Critical path**: Audio DP detection -> Exemplar extraction -> Similarity estimation -> Density prediction
- **Design tradeoffs**:
  - Exemplar scale selection: Wider windows capture more context but risk including irrelevant actions
  - Similarity metric: Soft-DTW captures temporal warping but is slower than correlation alone
  - Pretraining ratio: More synthesized data improves generalization but increases training time
- **Failure signatures**:
  - High MAE but low RMSE: Systematic under/overcounting common patterns
  - High RMSE: Outliers or unstable exemplar detection
  - Low MAE/RMSE but poor qualitative results: Density map misalignment with action peaks
- **First 3 experiments**:
  1. Ablation: Remove DP constraints, use naive max-scoring windows -> test impact on MAE
  2. Vary exemplar scales: Test 1-scale vs 3-scale vs 5-scale -> measure counting accuracy
  3. Modify similarity: Replace Soft-DTW with only correlation -> compare performance and runtime

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed distance-preserving loss function contribute to improving the model's performance in terms of counting accuracy? While the paper mentions this loss enforces the per-window encoder to preserve distance relationships by maintaining local patterns, it doesn't provide detailed analysis or quantitative results demonstrating its specific impact on counting accuracy. Detailed ablation studies comparing performance with and without this loss would help resolve this question.

### Open Question 2
How does the proposed exemplar-based data synthesis pipeline contribute to improving the model's robustness and ability to generalize? The paper mentions this pipeline leverages predefined vocal sounds to augment the dataset, claiming it improves robustness and generalization, but doesn't provide comprehensive quantitative results or analysis to demonstrate its effectiveness. Detailed experiments comparing performance with and without the data synthesis pipeline would help resolve this question.

### Open Question 3
How does the proposed exemplar extraction method based on predefined utterance detection in audio data contribute to the overall counting accuracy? While the paper introduces this intuitive and non-intrusive approach for specifying exemplars, it doesn't provide comprehensive quantitative results or analysis to demonstrate its specific impact on counting accuracy. Detailed experiments comparing different exemplar extraction methods would help resolve this question.

## Limitations

- Dynamic programming-based utterance detection mechanism lacks empirical validation against simpler baselines
- Multi-scale exemplar extraction approach doesn't explore optimal scale selection criteria
- Data synthesis pipeline critical for pretraining is only briefly mentioned without implementation details

## Confidence

- **High Confidence**: Overall problem formulation and dataset collection methodology
- **Medium Confidence**: Similarity-based exemplar infusion mechanism
- **Low Confidence**: Distance-preserving loss using Laplacian regularization

## Next Checks

1. **Ablation Study on Utterance Detection**: Replace the constrained DP algorithm with naive peak detection and measure the impact on MAE to validate whether temporal constraints are truly necessary.

2. **Similarity Metric Comparison**: Implement and compare Soft-DTW + correlation against simpler alternatives (correlation alone, cosine similarity) to quantify the contribution of temporal warping to counting accuracy.

3. **Scale Sensitivity Analysis**: Systematically vary the number of exemplar scales (1, 3, 5) and measure performance trade-offs to determine whether multi-scale approach provides meaningful improvements over single-scale extraction.