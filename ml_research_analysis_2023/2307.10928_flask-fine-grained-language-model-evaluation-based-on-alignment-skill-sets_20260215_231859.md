---
ver: rpa2
title: 'FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets'
arxiv_id: '2307.10928'
source_url: https://arxiv.org/abs/2307.10928
tags:
- evaluation
- figure
- logical
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FLASK, a fine-grained evaluation framework
  for assessing large language models based on alignment skill sets. The key idea
  is to decompose coarse-level scoring into instance-wise skill set-level scoring,
  enabling a more interpretable and holistic view of model performance.
---

# FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets

## Quick Facts
- arXiv ID: 2307.10928
- Source URL: https://arxiv.org/abs/2307.10928
- Reference count: 40
- Open-source LLMs significantly underperform proprietary LLMs on Logical Thinking and Background Knowledge abilities

## Executive Summary
This paper introduces FLASK, a fine-grained evaluation framework for assessing large language models based on alignment skill sets. The framework decomposes coarse-level scoring into instance-wise skill set-level scoring, enabling a more interpretable and holistic view of model performance. By defining 12 fine-grained skills across 4 primary abilities and annotating each evaluation instance with target domains and difficulty levels, FLASK provides a comprehensive assessment of LLM capabilities.

The authors demonstrate that current open-sourced LLMs significantly underperform proprietary LLMs on Logical Thinking and Background Knowledge abilities, even for state-of-the-art models. Different skills require different model sizes, with Logical Correctness benefiting more from larger models. The framework enables developers to more accurately measure model performance and identify improvement areas, while practitioners can use it to recommend suitable models for particular situations.

## Method Summary
FLASK constructs an evaluation dataset by collecting and modifying instances from diverse NLP datasets, resulting in 1,700 English instances under 2048 tokens. Each instance is annotated with essential skills (top-3 from 12 fine-grained skills), target domains (10 domains), and difficulty levels (1-5). Models are evaluated using both human-based and model-based methods, with GPT-4 serving as the model-based evaluator due to its high correlation with human judgments. Scores from 1 to 5 are assigned for each skill based on predefined criteria, and results are aggregated by skill, domain, and difficulty for analysis.

## Key Results
- Open-sourced LLMs significantly underperform proprietary LLMs on Logical Thinking and Background Knowledge abilities
- Different skills require different model sizes, with Logical Correctness benefiting more from larger models
- Proprietary LLMs struggle on a challenging subset of the evaluation set (FLASK-HARD)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing coarse-level scoring into instance-wise skill set-level scoring improves interpretability and holistic assessment of LLM alignment.
- Mechanism: By defining 12 fine-grained skills across 4 primary abilities, the evaluation captures diverse aspects of LLM performance. Each instance is annotated with a skill set, allowing granular scoring rather than a single overall score.
- Core assumption: User instructions require varying compositions of skills, and fixed metrics across instances are insufficient.
- Evidence anchors:
  - [abstract] "decomposes coarse-level scoring to a skill set-level scoring for each instruction"
  - [section 3.1] "We define 12 fine-grained skills needed for LLMs to follow open-ended user instructions"
- Break condition: If skill definitions are too broad or overlap significantly, decomposition loses discriminative power.

### Mechanism 2
- Claim: Metadata annotation (skill set, domain, difficulty) enables comprehensive analysis and comparison across models.
- Mechanism: Each instance is annotated with necessary skills, target domains, and difficulty levels. This allows filtering and analysis based on these dimensions, providing insights into model performance under specific conditions.
- Core assumption: Domain and difficulty annotations are reliable and capture meaningful variations in task requirements.
- Evidence anchors:
  - [section 3.2] "For each evaluation instance, we annotate the metadata which consists of 1) the essential skills to follow the instruction, 2) target domains, and 3) the difficulty level"
- Break condition: If metadata annotation is noisy or inconsistent, analysis results become unreliable.

### Mechanism 3
- Claim: Using both human-based and model-based evaluation provides complementary perspectives and mitigates individual evaluator limitations.
- Mechanism: Human evaluators and GPT-4 assess responses on each skill. This dual approach balances reliability and scalability while capturing different biases and strengths.
- Core assumption: GPT-4's evaluations correlate well with human judgments, and both methods capture meaningful aspects of model performance.
- Evidence anchors:
  - [section 5.2] "Overall, the tendency is similar between the two evaluation settings"
- Break condition: If correlation between human and model evaluations degrades for specific skills or domains, complementary value diminishes.

## Foundational Learning

- Concept: Skill decomposition in evaluation
  - Why needed here: Enables granular assessment of LLM capabilities beyond overall performance scores.
  - Quick check question: What are the 4 primary abilities and 12 skills defined in FLASK?

- Concept: Metadata annotation for analysis
  - Why needed here: Allows filtering and comparison across skills, domains, and difficulty levels.
  - Quick check question: How are domains and difficulty levels defined and annotated in FLASK?

- Concept: Dual evaluation methods
  - Why needed here: Balances reliability of human judgment with scalability of model-based evaluation.
  - Quick check question: What are the limitations of human-based and model-based evaluation that FLASK addresses?

## Architecture Onboarding

- Component map:
  - Evaluation dataset construction (Section 3.2) -> Skill set, domain, and difficulty annotation -> Evaluation process (Section 3.3) -> Analysis framework -> FLASK-HARD subset

- Critical path:
  1. Collect and prepare evaluation instances
  2. Annotate metadata (skills, domains, difficulty)
  3. Implement evaluation process for both human and model-based methods
  4. Aggregate and analyze results across dimensions
  5. Generate insights and recommendations

- Design tradeoffs:
  - Granularity vs. annotation burden: More skills enable finer analysis but increase annotation complexity.
  - Human vs. model evaluation: Balances reliability and scalability but requires validation of model correlations.
  - Fixed skill set vs. dynamic requirements: Predefined skills enable consistency but may miss emerging capabilities.

- Failure signatures:
  - Low inter-annotator agreement on metadata
  - Poor correlation between human and model evaluations
  - Insignificant differences between models across skills/domains
  - Model evaluations showing systematic biases (e.g., verbosity bias)

- First 3 experiments:
  1. Evaluate a small set of instances using both human and model-based methods; compare agreement and correlations.
  2. Test skill annotation process on diverse instances; measure inter-annotator agreement and refine skill definitions.
  3. Analyze a subset of results focusing on one dimension (e.g., domain performance); validate insights against known model capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the limitations of both human-based and model-based evaluators be mitigated in FLASK?
- Basis in paper: [inferred] from Section 8.1, where the authors discuss the limitations of both human and model evaluators.
- Why unresolved: The authors acknowledge the limitations but do not provide concrete solutions for mitigating them. Human evaluators suffer from central tendency bias and annotation fatigue, while model evaluators exhibit style and verbosity biases.
- What evidence would resolve it: Developing and testing methods to reduce biases in human evaluation (e.g., using document retrieval for fact verification or highlight hints) and implementing techniques to mitigate style and verbosity biases in model evaluation would provide evidence for resolving this question.

### Open Question 2
- Question: How does the performance of FLASK change when applied to multilingual, multi-turn, multi-modal, and few-shot in-context learning evaluation settings?
- Basis in paper: [explicit] from Section 8.2, where the authors mention that the current scope of FLASK is restricted to monolingual, single-turn, language-focused, and zero-shot evaluation.
- Why unresolved: The authors acknowledge the limitations of the current scope and suggest potential extensions but do not provide evidence of how FLASK would perform in these expanded settings.
- What evidence would resolve it: Applying FLASK to multilingual, multi-turn, multi-modal, and few-shot in-context learning evaluation tasks and comparing the results with the current findings would provide evidence for resolving this question.

### Open Question 3
- Question: How do different training steps affect the acquisition of various skills in language models, and what is the optimal training strategy for each skill?
- Basis in paper: [explicit] from Section A.4, where the authors explore the effect of different fine-tuning steps on the acquisition of various skills.
- Why unresolved: The authors observe that different skills require different training steps and suggest that optimizing each skill using experts might lead to better performance. However, they do not provide a concrete training strategy for each skill.
- What evidence would resolve it: Conducting experiments to determine the optimal number of training steps for each skill and developing a training strategy that incorporates expert knowledge for each skill would provide evidence for resolving this question.

## Limitations
- Reliance on manual annotation for skills, domains, and difficulty levels introduces potential subjectivity and scalability constraints
- Focus on English-language instances with input lengths under 2048 tokens limits applicability to multilingual or long-context scenarios
- Use of GPT-4 as the model-based evaluator may not be accessible to all practitioners and could introduce its own biases

## Confidence

- High Confidence: The correlation between human and model-based evaluations is well-supported by the experimental results, with consistent patterns observed across multiple models and skill categories.
- Medium Confidence: The claim that current open-source LLMs significantly underperform proprietary models on Logical Thinking and Background Knowledge abilities is supported by the data, though the performance gap may narrow as open-source models continue to evolve.
- Medium Confidence: The observation that different skills benefit from different model sizes is based on observed patterns but requires further validation across a broader range of model architectures and training approaches.

## Next Checks

1. Conduct inter-annotator reliability tests across a broader pool of evaluators to assess the consistency of skill, domain, and difficulty annotations beyond the reported high agreement.
2. Evaluate the framework's performance on multilingual instances and long-context scenarios to determine its generalizability beyond the current English, short-context focus.
3. Test the correlation between human and model-based evaluations on a subset of instances using different model-based evaluators (e.g., Claude, LLaMA) to assess the robustness of using GPT-4 as the primary model-based evaluator.