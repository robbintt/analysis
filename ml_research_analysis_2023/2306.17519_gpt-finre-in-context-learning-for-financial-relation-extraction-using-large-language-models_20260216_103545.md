---
ver: rpa2
title: 'GPT-FinRE: In-context Learning for Financial Relation Extraction using Large
  Language Models'
arxiv_id: '2306.17519'
source_url: https://arxiv.org/abs/2306.17519
tags:
- learning
- relation
- in-context
- language
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses relation extraction in the financial domain
  using large language models and in-context learning. The authors propose GPT-FinRE,
  which leverages GPT-3.5 Turbo and GPT-4 under the ICL framework to identify and
  classify relationships between entities in financial documents.
---

# GPT-FinRE: In-context Learning for Financial Relation Extraction using Large Language Models

## Quick Facts
- arXiv ID: 2306.17519
- Source URL: https://arxiv.org/abs/2306.17519
- Authors: 
- Reference count: 27
- Key outcome: GPT-FinRE achieves 0.718 F1-score on financial relation extraction using in-context learning with GPT-4 and EPR retriever

## Executive Summary
This paper presents GPT-FinRE, a framework for financial relation extraction using large language models under the in-context learning paradigm. The method employs two retrieval strategies—KNN with OpenAI embeddings and EPR with GPT-Neo-2.7B—to find relevant demonstrations for the LLM. Evaluated on the REFinD dataset with 29K instances and 22 relations, the approach achieves competitive performance, with GPT-4 and EPR combination outperforming other configurations. The results demonstrate the effectiveness of learning-based retrievers and larger language models in financial domain relation extraction tasks.

## Method Summary
GPT-FinRE uses in-context learning with GPT-3.5 Turbo and GPT-4 to perform relation extraction on financial documents. The method retrieves relevant demonstrations using two strategies: KNN with OpenAI embeddings (learning-free) and EPR with GPT-Neo-2.7B (learning-based). Prompts include task description, predefined classes, K-shot demonstrations from retrieved examples, and test input. The approach is evaluated on the REFinD dataset containing 10-X reports from publicly traded companies.

## Key Results
- Achieved best F1-score of 0.718, ranking 4th in the shared task
- GPT-4 with EPR retriever outperforms other combinations
- Learning-based retriever (EPR) outperforms learning-free retriever (KNN)
- GPT-4 performs better than GPT-3.5 Turbo in financial relation extraction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval quality directly impacts in-context learning performance in GPT-FinRE
- Mechanism: Two retrieval strategies (KNN with OpenAI embeddings and EPR with GPT-Neo-2.7B) find relevant demonstrations that help the model understand task structure and improve prediction accuracy
- Core assumption: Similar examples in embedding space provide more relevant demonstrations for the model to learn from
- Evidence anchors:
  - [abstract] "We utilized two retrieval strategies to find top K relevant in-context learning demonstrations / examples from training data for a given test example"
  - [section] "LLMs can relate to the presented 'to be predicted' data point more if the contextual examples predicted are similar to it"
  - [corpus] "Found 25 related papers... Top related titles: DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations"

### Mechanism 2
- Claim: GPT-4 outperforms GPT-3.5 Turbo in financial relation extraction tasks
- Mechanism: Larger language models have better capacity to understand complex financial domain relationships and generate more accurate predictions when provided with demonstrations
- Core assumption: Increased model capacity of GPT-4 allows it to better leverage in-context examples for financial relation extraction
- Evidence anchors:
  - [abstract] "The results show that GPT-4 with EPR outperforms other combinations"
  - [section] "We find that GPT 4 performs better than GPT 3.5 Turbo"
  - [corpus] "Found 25 related papers... Enhancing Language Models for Financial Relation Extraction with Named Entities and Part-of-Speech"

### Mechanism 3
- Claim: Learning-based retrievers (EPR) outperform learning-free retrievers (KNN) in this task
- Mechanism: EPR is trained to retrieve better in-context examples by estimating the probability of the output given the input and a candidate training example
- Core assumption: Learned retriever can better capture semantic relationships needed for financial relation extraction compared to static embeddings
- Evidence anchors:
  - [abstract] "The results show that GPT-4 with EPR outperforms other combinations"
  - [section] "We also find that learning based retriever (EPR) outperforms learning-free retriever (KNN with OpenAI embeddigs)"
  - [corpus] "Found 25 related papers... Learning To Retrieve Prompts for In-Context Learning"

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: GPT-FinRE uses ICL to perform relation extraction without fine-tuning, relying on demonstrations provided in the prompt
  - Quick check question: What is the key difference between in-context learning and traditional fine-tuning approaches?

- Concept: Dense retrieval
  - Why needed here: The paper employs dense retrievers (KNN with OpenAI embeddings) to find relevant demonstrations for in-context learning
  - Quick check question: How does dense retrieval differ from sparse retrieval methods like TF-IDF?

- Concept: Semantic similarity in embedding space
  - Why needed here: The effectiveness of KNN retrieval depends on finding examples that are close in the embedding space to the test example
  - Quick check question: Why is semantic similarity important for retrieving relevant in-context learning demonstrations?

## Architecture Onboarding

- Component map: Input text and entity pairs -> Retriever (KNN/EPR) -> LLM (GPT-3.5 Turbo/GPT-4) -> Predicted relation

- Critical path:
  1. Input text and entity pairs are processed
  2. Retrievers find top K relevant demonstrations
  3. Demonstrations are combined with test input to form prompt
  4. LLM generates prediction based on prompt
  5. Prediction is output as the relation between entities

- Design tradeoffs:
  - KNN vs EPR: KNN is faster and doesn't require training but may be less accurate; EPR requires training but can potentially find more relevant examples
  - GPT-3.5 Turbo vs GPT-4: GPT-4 is more accurate but more expensive; GPT-3.5 Turbo is cheaper but less accurate
  - Number of demonstrations: More demonstrations may improve performance but increase prompt size and cost

- Failure signatures:
  - Low F1-score: May indicate poor retrieval quality or insufficient demonstrations
  - High variance in predictions: Could suggest inconsistent demonstrations or model instability
  - Hallucinations: May indicate lack of relevant demonstrations or over-reliance on model's prior knowledge

- First 3 experiments:
  1. Test KNN with OpenAI embeddings and GPT-3.5 Turbo to establish baseline performance
  2. Test EPR with GPT-Neo-2.7B and GPT-4 to evaluate learning-based retrieval improvement
  3. Test varying numbers of demonstrations (K) to find optimal balance between performance and cost

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the methodology and results presented.

## Limitations
- Reliance on commercial API-based models (GPT-3.5 Turbo, GPT-4) raises reproducibility and cost concerns
- Exact implementation details of EPR mechanism are not fully specified, limiting replication
- No error analysis provided to understand failure modes or model hallucinations

## Confidence
- High confidence: Core claim that in-context learning can effectively perform financial relation extraction is well-supported by experimental results
- Medium confidence: Superiority of learning-based retrieval (EPR) over learning-free retrieval (KNN) is supported but exact implementation details are lacking
- Low confidence: Scalability and generalizability to other financial datasets or domains is not addressed

## Next Checks
1. Implement and compare multiple K values (e.g., K=2, 4, 8, 16) to determine optimal number of demonstrations for balancing performance and cost
2. Conduct thorough error analysis on misclassified examples to identify patterns and potential improvements in retrieval or prompting strategies
3. Test the approach on a different financial dataset or with modified entity types to assess generalizability beyond REFinD dataset