---
ver: rpa2
title: Learning Recurrent Models with Temporally Local Rules
arxiv_id: '2310.13284'
source_url: https://arxiv.org/abs/2310.13284
tags:
- data
- generative
- learning
- time
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the computational expense of fitting generative
  models to sequential data, which typically requires two recursive computations through
  time. The authors propose an alternative approach that learns the joint distribution
  over current and previous states, rather than just transition probabilities.
---

# Learning Recurrent Models with Temporally Local Rules

## Quick Facts
- arXiv ID: 2310.13284
- Source URL: https://arxiv.org/abs/2310.13284
- Reference count: 15
- Primary result: Proposes learning joint distribution over current and previous states to avoid backpropagation through time while capturing backward dependencies in sequential data

## Executive Summary
This paper addresses the computational expense of fitting generative models to sequential data, which typically requires two recursive computations through time. The authors propose an alternative approach that learns the joint distribution over current and previous states, rather than just transition probabilities. They evaluate this approach using three experiments: a probabilistic population codes (PPCs) experiment, a bouncing balls dataset, and a MovingMNIST dataset. Results show that the proposed method can learn aspects of the data typically requiring the backward pass, and it performs nearly as well as models trained with backpropagation through time (BPTT) on the PPC dataset. The approach also captures nonlinear second-order dynamics in the bouncing balls and MovingMNIST datasets.

## Method Summary
The method proposes learning the joint distribution p(ut-1, yt | xt) instead of the standard factorization p(yt | xt) × p(xt | xt-1). This joint modeling requires the generative model to predict both the current observation yt and the sufficient statistics ut-1 of the previous state given the current latent state xt. The recognition model produces sufficient statistics (posterior mean and variance) that summarize information from all past observations. By optimizing the free energy bound on the marginal cross-entropy, the model learns to propagate temporal dependencies forward in a way that implicitly captures backward information flow, eliminating the need for explicit backward passes or BPTT.

## Key Results
- The proposed method can learn aspects of the data typically requiring the backward pass, including second-order dynamics
- On PPC dataset, the method performs nearly as well as models trained with backpropagation through time (BPTT)
- The approach captures nonlinear second-order dynamics in bouncing balls and MovingMNIST datasets
- Performance is comparable to first-order Kalman filter baseline on PPC dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning the joint distribution over current and previous states enforces bidirectional information flow without explicit backward pass.
- Mechanism: By modeling p(ut-1, yt | xt) instead of just p(yt | xt), the model must encode how the current observation yt and the sufficient statistics of the previous state ut-1 jointly inform the latent state xt. This joint modeling requires the model to implicitly account for how xt depends on both current and past information, effectively learning backward dependencies during the forward pass.
- Core assumption: The sufficient statistics ut-1 contain all necessary information about the past observations relevant to the current state.
- Evidence anchors:
  - [abstract] "requiring the generative model to learn the joint distribution over current and previous states, rather than merely the transition probabilities"
  - [section] "we propose to model at each time step the joint distribution of the current and previous states, along with the current observations"
  - [corpus] No direct evidence found - corpus papers focus on backpropagation alternatives but not this specific joint modeling approach

### Mechanism 2
- Claim: The recognition model's sufficient statistics serve as a summary that enables learning of second-order dynamics without backpropagation through time.
- Mechanism: The recognition model produces sufficient statistics (posterior mean and variance) that summarize information from all past observations. By requiring the generative model to predict these sufficient statistics along with the current observation, the model learns to propagate temporal dependencies forward in a way that implicitly captures backward information flow.
- Core assumption: The sufficient statistics are indeed sufficient for inference and can be predicted by the generative model.
- Evidence anchors:
  - [section] "Intuitively, the sufficient statistics provide a 'summary' of ut-1 and yt, or more precisely of their information about the latent state"
  - [section] "the argument can be extended recursively to claim that ut summarizes all of the preceding observations as they pertain to the latent state"
  - [corpus] Weak evidence - corpus papers discuss local learning rules but not specifically this mechanism of using sufficient statistics

### Mechanism 3
- Claim: Modeling the joint distribution creates a tighter bound on the marginal cross-entropy than modeling transition probabilities alone.
- Mechanism: The joint distribution p(ut-1, yt | xt) creates a more informative objective than the standard p(yt | xt) × p(xt | xt-1) factorization. This tighter bound provides better gradient signals that capture backward dependencies during the forward pass.
- Core assumption: The joint cross-entropy bound is tighter than the marginal cross-entropy for this problem structure.
- Evidence anchors:
  - [section] "the marginal cross-entropy is upper-bounded by the joint cross entropy"
  - [section] "Since the joint cross entropy is anyway a bound on the quantity we care about, the marginal cross entropy"
  - [corpus] No direct evidence found in corpus - this appears to be a novel theoretical contribution

## Foundational Learning

- Concept: Markov chain assumption
  - Why needed here: The paper assumes observations can be explained by a latent state that evolves according to a Markov chain, which is fundamental to the temporal modeling approach
  - Quick check question: What does the Markov assumption imply about the dependence of the current state on past observations?

- Concept: Sufficient statistics
  - Why needed here: The paper uses sufficient statistics as a way to summarize information from past observations, enabling the model to capture temporal dependencies without explicit backward passes
  - Quick check question: How do sufficient statistics enable the model to summarize all relevant information from past observations?

- Concept: Free energy minimization
  - Why needed here: The learning objective is formulated as minimizing free energy, which provides the mathematical framework for training the proposed models
  - Quick check question: What is the relationship between free energy and the marginal cross-entropy in this context?

## Architecture Onboarding

- Component map: Recognition model → Sufficient statistics → Generative model prediction → Free energy loss → Parameter update
- Critical path: Recognition model takes ut-1 and yt as input, produces posterior mean and variance; Generative model takes xt as input, produces distributions over ut-1 and yt; Joint modeling combines predictions for free energy calculation
- Design tradeoffs:
  - More expressive models can capture complex dynamics but may overfit
  - Simpler models are more efficient but may miss important temporal dependencies
  - Joint modeling increases parameter count but provides better gradient signals
- Failure signatures:
  - Poor prediction of sufficient statistics indicates the model isn't capturing temporal dependencies
  - High reconstruction error on yt suggests the generative model isn't learning the data distribution well
  - Instability in training may indicate poor initialization or learning rate issues
- First 3 experiments:
  1. Train on PPC dataset with first-order dynamics to verify basic functionality
  2. Test on PPC dataset with second-order dynamics to check for learned backward dependencies
  3. Evaluate on bouncing balls dataset to verify capture of constant velocity dynamics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed approach of learning the joint distribution over current and previous states be rigorously mathematically justified?
- Basis in paper: [explicit] The paper states "Nevertheless, although intuitively plausible, our procedure lacks a rigorous mathematical justification."
- Why unresolved: The mathematical foundation for the proposed approach is not provided in the paper, leaving its theoretical validity unproven.
- What evidence would resolve it: A formal mathematical proof or derivation demonstrating the validity and effectiveness of the proposed approach in learning the joint distribution over current and previous states.

### Open Question 2
- Question: Can the proposed approach be scaled up to more challenging datasets beyond the toy datasets used in the study?
- Basis in paper: [explicit] The paper mentions "It also remains to scale the procedure up to more challenging datasets."
- Why unresolved: The paper only demonstrates the approach on simple toy datasets, leaving uncertainty about its performance on more complex real-world data.
- What evidence would resolve it: Experiments applying the proposed approach to larger and more diverse datasets, comparing its performance against existing methods and demonstrating its scalability.

### Open Question 3
- Question: How does the proposed approach compare to other methods that avoid backpropagation through time, such as those using temporal convolutions or attention mechanisms?
- Basis in paper: [inferred] The paper proposes an alternative to backpropagation through time but does not compare its performance to other methods that also avoid BPTT.
- Why unresolved: The paper focuses on the proposed approach without benchmarking it against other methods that address the same computational challenges.
- What evidence would resolve it: Comparative experiments evaluating the proposed approach against other BPTT alternatives on various sequential data tasks, measuring performance, computational efficiency, and biological plausibility.

## Limitations

- The paper lacks detailed architectural specifications for the generative and recognition models, making faithful reproduction difficult
- Evaluation relies heavily on synthetic datasets (PPC, bouncing balls, MovingMNIST) which may not capture real-world sequential data complexity
- The claim that this approach can learn "aspects of the data that typically require the backward pass" needs validation on more diverse and challenging sequential datasets

## Confidence

- **High Confidence**: The theoretical foundation linking joint distribution modeling to backward dependency capture is well-supported by the mathematical framework presented.
- **Medium Confidence**: The experimental results on synthetic datasets are promising but limited in scope and may not generalize to more complex real-world scenarios.
- **Low Confidence**: The claim that this approach can learn "aspects of the data that typically require the backward pass" needs further validation on more diverse and challenging sequential datasets.

## Next Checks

1. **Architecture Specification**: Implement the generative and recognition models with explicit architectural details (layer sizes, activation functions) and verify that the results match the paper's claims.

2. **Real-World Data Testing**: Evaluate the approach on a real-world sequential dataset (e.g., video prediction or time series forecasting) to assess generalization beyond synthetic data.

3. **Comparative Analysis**: Conduct a more comprehensive comparison with BPTT-based models across multiple metrics (e.g., long-term prediction accuracy, computational efficiency) to determine the practical advantages of the proposed method.