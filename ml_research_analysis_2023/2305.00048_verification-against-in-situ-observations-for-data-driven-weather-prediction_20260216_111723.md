---
ver: rpa2
title: Verification against in-situ observations for Data-Driven Weather Prediction
arxiv_id: '2305.00048'
source_url: https://arxiv.org/abs/2305.00048
tags:
- observations
- era5
- data
- weather
- ddwps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines the performance of data-driven weather prediction
  (DDWP) models against real-world in-situ observations, rather than against reanalysis
  datasets like ERA5. The authors create a dataset of quality-controlled observations
  from the NOAA MADIS program and use it to evaluate ERA5, FourCastNet (FCN), and
  the IFS NWP model for 2-meter temperature, dewpoint, wind speed, and precipitation.
---

# Verification against in-situ observations for Data-Driven Weather Prediction

## Quick Facts
- arXiv ID: 2305.00048
- Source URL: https://arxiv.org/abs/2305.00048
- Reference count: 4
- Key outcome: Data-driven weather prediction (DDWP) models perform nearly identically to IFS when evaluated against real-world in-situ observations, revealing a significant gap from their ERA5-verified performance and highlighting the need for observation-based validation.

## Executive Summary
This paper addresses a critical gap in DDWP evaluation by comparing model performance against real-world in-situ observations rather than reanalysis datasets like ERA5. The authors create a quality-controlled NOAA MADIS observation dataset and use it to evaluate ERA5, FourCastNet, and the IFS NWP model across multiple variables and regions. The key finding is that while DDWPs have shown strong performance when compared against ERA5, their real-world skill is much closer to traditional NWP models than previously thought. The study also reveals regional and temporal biases in ERA5 that propagate into DDWP forecasts, raising concerns about climate equity and the need for new research directions including fine-tuning on operational start states and improved precipitation metrics.

## Method Summary
The study evaluates DDWP models using quality-controlled in-situ observations from the NOAA MADIS program (Mesonet and METAR networks) for 2020. Observations include 2-meter temperature, dewpoint, wind speed, and 6-hourly precipitation. The evaluation compares ERA5 reanalysis, FourCastNet (a DDWP model), and the IFS NWP model using RMSE metrics. Forecasts are bilinearly interpolated from 0.25째 grids to observation points. The study partitions data into 10 geographical regions and analyzes performance across different times of day, separating all-weather and extreme-weather conditions.

## Key Results
- DDWP models (FourCastNet) perform nearly identically to IFS when evaluated against real-world observations, contrary to their strong performance against ERA5
- ERA5 contains regional and temporal biases that propagate into DDWP forecasts, particularly affecting data-sparse regions
- Precipitation evaluation reveals RMSE inadequacies, with naive "no rain" models showing deceptively low error scores
- The performance gap between DDWPs and IFS is much smaller in real-world settings than in simulation contexts

## Why This Works (Mechanism)

### Mechanism 1
DDWPs trained on ERA5 closely approximate IFS in simulation but diverge in real-world accuracy when evaluated against in-situ observations. ERA5 is a reanalysis product combining model forecasts with observational data. DDWPs trained to minimize error against ERA5 essentially learn to reproduce IFS behavior in a controlled environment. However, real-world observations contain unmodeled variability and biases that ERA5 smooths out, causing DDWPs to underperform relative to IFS when verified against actual data.

### Mechanism 2
Regional and temporal biases in ERA5 propagate into DDWP forecasts, leading to degraded performance in data-sparse regions. ERA5 uncertainty is higher in regions with fewer observations (e.g., global South) due to weaker constraint on the underlying NWP. DDWPs trained on this biased dataset inherit these errors, and since they lack a physical constraint mechanism, they cannot correct for them. This results in poorer real-world performance in those regions compared to IFS, which assimilates observations dynamically.

### Mechanism 3
Precipitation forecasting errors in DDWPs are underestimated when using RMSE because the metric is not suited for long-tailed, sparse events. Precipitation is a binary occurrence (rain/no rain) with a heavy-tailed distribution. RMSE penalizes small absolute errors heavily when most predictions are near zero (no rain), masking systematic underprediction. This leads to misleadingly low error scores even when the model fails to capture actual rain events.

## Foundational Learning

- **Difference between reanalysis and operational analysis**: ERA5 is a reanalysis product combining model forecasts with historical observations, while operational analysis uses real-time data assimilation. Why needed: DDWPs are trained on ERA5 but must operate with lower-quality operational start states; understanding this gap is critical for realistic evaluation. Quick check: What is the main difference between ERA5 reanalysis and operational analysis in terms of data assimilation window and resolution?

- **Interpolation from gridded output to point observations**: Real-world verification requires comparing gridded forecasts to point observations; bilinear interpolation is used but may introduce errors. Why needed: The study compares 0.25째 gridded forecasts against point observations. Quick check: How does bilinear interpolation work when mapping a 0.25째 grid forecast to an arbitrary observation location?

- **Verification metrics for deterministic vs. probabilistic forecasts**: RMSE is inadequate for precipitation; probabilistic metrics like CRPS or ensemble-based metrics are needed for longer lead times. Why needed: The paper highlights RMSE limitations for precipitation evaluation. Quick check: Why is RMSE not suitable for evaluating precipitation forecasts, and what alternative metric would better capture forecast skill?

## Architecture Onboarding

- **Component map**: NOAA MADIS observations -> Quality control (QC flags) -> Regional partitioning -> Gridded forecasts (ERA5, FCN, IFS) -> Bilinear interpolation -> RMSE calculation
- **Critical path**: 1) Ingest and quality-control MADIS observations, 2) Partition into regions and filter by QC criteria, 3) Generate gridded forecasts from ERA5, FCN, IFS, 4) Interpolate forecasts to observation points, 5) Compute RMSE per variable/region/time, 6) Analyze regional and temporal patterns
- **Design tradeoffs**: Using ensemble mean start states for FCN approximates operational conditions but sacrifices accuracy vs. HRES; filtering observations by QC level reduces noise but may bias toward data-rich regions; bilinear interpolation is fast but ignores sub-grid topography effects
- **Failure signatures**: RMSE spikes in data-sparse regions indicate ERA5 bias propagation; low precipitation RMSE with visually poor forecasts suggests metric inadequacy; sudden drops in skill at certain times of day may reflect ERA5 assimilation window effects
- **First 3 experiments**: 1) Compare FCN performance using HRES vs. ensemble mean start states on a small region/time subset, 2) Evaluate precipitation forecasts using CSI and CRPS instead of RMSE on a rainy event subset, 3) Test impact of stricter QC filtering (V-only vs. S+V) on RMSE distribution across regions

## Open Questions the Paper Calls Out

### Open Question 1
Does fine-tuning DDWPs on operational start states improve their short-term forecast accuracy compared to training solely on ERA5 reanalysis data? The paper suggests fine-tuning DDWPs on operational start states once sufficiently trained on ERA5 data to address the detrimental impact of the difference between reanalysis and operational start states on short-term forecast quality. This remains untested as the paper focuses on initial evaluations and calls for further research directions.

### Open Question 2
Can machine learning techniques like transfer learning and super-resolution effectively improve weather forecast quality in data-sparse regions of the global South by leveraging data-rich regions? The paper discusses using transfer learning and super-resolution to exploit higher quality and volume of data in other regions to improve forecasts in underserved regions, addressing regional biases in weather data. The paper identifies this as a potential research direction but does not provide experimental results or validation.

### Open Question 3
Are probabilistic metrics and NWP ensembles more appropriate than deterministic metrics like RMSE for evaluating DDWP performance at longer lead times? The paper suggests that for longer lead times, probabilistic metrics and NWP ensembles may be more appropriate baselines than deterministic metrics like RMSE, which are convenient but not suitable for all variables and longer lead times. The paper calls for the use of appropriate metrics but does not conduct experiments comparing deterministic versus probabilistic evaluations.

## Limitations

- The study uses a relatively coarse 0.25째 resolution for FCN, which may limit its ability to capture fine-scale weather features
- The evaluation period covers only one year (2020), which may not capture full seasonal and interannual variability
- The study focuses primarily on RMSE metrics, though it acknowledges their limitations for precipitation forecasting

## Confidence

- **High confidence**: The observation that IFS and FCN perform nearly identically when verified against real-world data, and that this differs from their relative performance against ERA5. The MADIS dataset creation and quality control procedures are well-documented.
- **Medium confidence**: The claim that ERA5 biases propagate into DDWP forecasts due to training data limitations. While logically sound, direct causal evidence linking specific ERA5 regional biases to DDWP performance degradation is not provided.
- **Low confidence**: The assertion that DDWPs would "exacerbate" existing biases without mitigation strategies. The paper identifies the risk but does not demonstrate this exacerbation empirically.

## Next Checks

1. **Precipitation metric validation**: Re-run the precipitation analysis using CSI and CRPS metrics on a subset of rainy events to determine if RMSE underestimates FCN skill gaps versus IFS.
2. **Start state sensitivity test**: Compare FCN performance using HRES versus ensemble mean start states on a small regional/time subset to isolate the impact of initialization quality.
3. **Regional bias quantification**: Compute RMSE ratios (FCN/IFS) stratified by observation density to quantify how data sparsity affects the relative performance of DDWPs versus physics-based models.