---
ver: rpa2
title: Matrix Compression via Randomized Low Rank and Low Precision Factorization
arxiv_id: '2310.11028'
source_url: https://arxiv.org/abs/2310.11028
tags:
- matrix
- page
- lplr
- error
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fast randomized algorithm for compressing
  matrices by obtaining a low-rank and low-precision factorization. The key idea is
  to first compute an approximate basis of the range space of the matrix by randomly
  sketching its columns, followed by quantization of this basis.
---

# Matrix Compression via Randomized Low Rank and Low Precision Factorization

## Quick Facts
- arXiv ID: 2310.11028
- Source URL: https://arxiv.org/abs/2310.11028
- Reference count: 40
- One-line primary result: Achieves aggressive compression ratios (as low as 1 bit per matrix coordinate) by combining randomized low-rank factorization with low-precision quantization

## Executive Summary
This paper introduces a randomized algorithm for compressing large matrices by exploiting both low-rank structure and low-precision quantization simultaneously. The method first computes an approximate basis of the matrix's range space using random Gaussian sketching, then quantizes this basis before projecting the original matrix onto it. This two-step process enables significant compression ratios while maintaining or surpassing the performance of traditional methods. The approach is demonstrated across image compression, nearest neighbor classification of embeddings, and compressing large language model weight matrices.

## Method Summary
The algorithm (LPLR) computes an approximate basis of a matrix's range space by randomly sketching its columns using a Gaussian matrix, then quantizes this basis. The original matrix columns are projected onto this quantized basis to obtain the low-rank factors. The method combines the low-rank approximation capabilities of randomized sketching with the compression benefits of quantization, achieving near-optimal low-rank approximation error under quantization constraints. The total number of parameters is reduced from nd to m(n+d), with each parameter stored in fewer bits.

## Key Results
- Achieves compression ratios as aggressive as one bit per matrix coordinate
- Maintains or surpasses performance of traditional compression techniques
- Demonstrated effectiveness on image compression, nearest neighbor classification, and LLM weight matrices
- Reduces computational complexity from O(nd²) to O(ndm) compared to full SVD approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gaussian sketching combined with quantization equalizes coordinate magnitudes, enabling higher precision per bit.
- Mechanism: Random Gaussian matrices act as Johnson-Lindenstrauss embeddings, spreading vector mass across coordinates uniformly. This allows uniform scalar quantizers to achieve constant ℓ2 quantization error independent of vector dimension.
- Core assumption: The matrix being compressed is approximately low-rank so that random Gaussian sketching preserves the range space well.

### Mechanism 2
- Claim: The algorithm achieves near-optimal low-rank approximation error by projecting onto a quantized basis of the approximate range space.
- Mechanism: First computes an approximate basis of the column space via Gaussian sketching, quantizes this basis, then projects the original matrix columns onto this quantized basis to obtain the second factor.
- Core assumption: The sketching dimension m ≥ k + 2 ensures sufficient oversampling to capture the k-dimensional range space with small error.

### Mechanism 3
- Claim: The algorithm achieves significant compression ratios while maintaining or surpassing traditional methods by exploiting both low-rank structure and low-precision quantization simultaneously.
- Mechanism: By factorizing the matrix into L and R where both factors are quantized, the total number of parameters is reduced from nd to m(n+d), and each parameter is stored in fewer bits.
- Core assumption: The matrix has inherent low-rank structure so that m(n+d) << nd and the quantization error is acceptable.

## Foundational Learning

- Concept: Johnson-Lindenstrauss embeddings and their equalization property
  - Why needed here: Understanding how random projections can reduce dimensionality while preserving distances and equalizing coordinate magnitudes is crucial for grasping why Gaussian sketching helps quantization
  - Quick check question: What property of Gaussian random matrices allows them to improve quantization precision compared to naive coordinate-wise quantization?

- Concept: Low-rank matrix approximation and singular value decomposition
  - Why needed here: The algorithm builds on the idea that many real-world matrices can be well-approximated by low-rank factorizations, and understanding SVD is essential for comparing with traditional methods
  - Quick check question: Why does the Eckart-Young-Mirsky theorem guarantee that truncated SVD provides the best rank-k approximation under Frobenius norm?

- Concept: Quantization theory and uniformly dithered quantization
  - Why needed here: The algorithm relies on uniform scalar quantization with dithering to achieve unbiased quantization with bounded error variance, which is crucial for the theoretical guarantees
  - Quick check question: How does uniformly dithered quantization achieve zero-mean quantization error while maintaining bounded variance?

## Architecture Onboarding

- Component map: Matrix A -> Gaussian sketching S -> Approximate basis AS -> Quantized basis Q(AS) -> Least squares projection W* -> Quantized projection Q'(W*) -> Low-rank factors L and R

- Critical path:
  1. Compute AS (matrix multiplication)
  2. Quantize AS to get Q(AS)
  3. Solve least squares problem for W*
  4. Quantize W* to get Q'(W*)
  5. Return L and R

- Design tradeoffs:
  - Sketch size m vs. approximation error: Larger m gives better approximation but more parameters
  - Quantization bit-budget vs. precision: More bits give better precision but less compression
  - Computational complexity: O(ndm) vs. O(nd²) for full SVD approaches
  - Dynamic range vs. saturation probability: Larger dynamic range reduces saturation but wastes bits

- Failure signatures:
  - High approximation error despite aggressive compression: Likely m is too small or bit-budget is insufficient
  - Saturation warnings in quantization: Dynamic range needs to be increased
  - Poor performance on high-rank matrices: Algorithm is designed for approximately low-rank matrices
  - Numerical instability: May occur if condition number κ is too large

- First 3 experiments:
  1. Image compression test: Apply to Shepp-Logan phantom image with varying bit-budgets (B=B'=8, Bnq=4) and measure Frobenius norm error and visual quality
  2. Embedding compression test: Compress CIFAR-10 embeddings using MobileNetV3, train 3-NN classifier, compare accuracy with naive quantization
  3. LLM weight compression test: Apply to LlaMa-7B weight matrices, measure compression ratio and task performance on downstream benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LPLR perform on extremely large matrices where even the O(ndm) complexity becomes prohibitive?
- Basis in paper: [inferred] from the discussion on computational complexity and memory limitations for large matrices.
- Why unresolved: The paper acknowledges that LPLR is more computationally efficient than SVD-based methods but does not explore scenarios where even O(ndm) is too expensive.
- What evidence would resolve it: Empirical results comparing LPLR with other methods on extremely large matrices (e.g., billion-scale matrices) would show if LPLR remains viable or if alternative approaches are needed.

### Open Question 2
- Question: Does LPLR's quantization provide better regularization effects on test data compared to direct-SVD or naive quantization?
- Basis in paper: [inferred] from the observation that LPLR's accuracy and F1 scores are comparable to methods with lower Frobenius norm error.
- Why unresolved: The paper notes this as an open question but does not investigate the regularization effects of different quantization methods.
- What evidence would resolve it: Controlled experiments comparing the generalization performance of LPLR, direct-SVD, and naive quantization on held-out test data would reveal if LPLR's quantization has unique regularization benefits.

### Open Question 3
- Question: How does LPLR's performance change when the underlying matrix has a less pronounced low-rank structure?
- Basis in paper: [explicit] from the discussion on how LPLR's advantage diminishes when the matrix is not inherently low-rank.
- Why unresolved: The paper focuses on matrices with strong low-rank structure and does not explore scenarios with weaker low-rank properties.
- What evidence would resolve it: Experiments on matrices with varying degrees of low-rank structure (e.g., matrices with gradually decreasing singular values) would quantify how LPLR's performance degrades as the low-rank structure weakens.

## Limitations

- The algorithm's effectiveness fundamentally depends on the low-rank assumption, which may not hold for all matrix types
- The compression benefits come at the cost of introducing quantization error, which could be significant for matrices with rapidly decaying singular values
- The computational complexity, while better than full SVD approaches, still scales as O(ndm) which could be prohibitive for extremely large matrices

## Confidence

- Gaussian sketching equalization mechanism: Medium
- Low-rank approximation guarantees: Medium
- Compression ratio claims: Medium
- Cross-domain effectiveness: High

## Next Checks

1. Verify the equalization property claim by empirically measuring coordinate variance before and after Gaussian sketching on matrices with varying spectral decay rates
2. Test algorithm robustness by applying to matrices with known high-rank structure (e.g., diagonal matrices) to establish break points
3. Implement wall-clock time measurements for each algorithmic step to validate the computational complexity claims against practical performance