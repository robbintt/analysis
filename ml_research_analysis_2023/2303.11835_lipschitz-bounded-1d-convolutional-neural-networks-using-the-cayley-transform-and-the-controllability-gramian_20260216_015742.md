---
ver: rpa2
title: Lipschitz-bounded 1D convolutional neural networks using the Cayley transform
  and the controllability Gramian
arxiv_id: '2303.11835'
source_url: https://arxiv.org/abs/2303.11835
tags:
- lipschitz
- cnns
- layers
- parameterization
- convolutional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel parameterization for 1D convolutional
  neural networks (CNNs) with built-in Lipschitz continuity guarantees. The key idea
  is to use the Cayley transform to parameterize orthogonal matrices and the controllability
  Gramian for convolutional layers, resulting in a layer-wise parameterization that
  satisfies sufficient conditions for Lipschitz continuity.
---

# Lipschitz-bounded 1D convolutional neural networks using the Cayley transform and the controllability Gramian

## Quick Facts
- arXiv ID: 2303.11835
- Source URL: https://arxiv.org/abs/2303.11835
- Authors: 
- Reference count: 26
- Key outcome: Novel parameterization for 1D CNNs with built-in Lipschitz continuity guarantees using Cayley transform and controllability Gramian, demonstrating improved robustness on heart arrhythmia classification task

## Executive Summary
This paper proposes a novel parameterization for 1D convolutional neural networks that guarantees Lipschitz continuity by construction. The approach uses the Cayley transform to parameterize orthogonal matrices for fully connected layers and the controllability Gramian for convolutional layers, satisfying sufficient conditions for Lipschitz continuity through linear matrix inequalities. The method enables unconstrained training of Lipschitz-bounded 1D CNNs and demonstrates improved robustness compared to vanilla and L2-regularized CNNs on the MIT-BIH heart arrhythmia classification dataset, maintaining high test accuracies on adversarial examples with different perturbation strengths.

## Method Summary
The method parameterizes 1D CNNs using the Cayley transform for orthogonal matrices and controllability Gramians for convolutional layers, ensuring each layer satisfies linear matrix inequalities that are sufficient for Lipschitz continuity. This allows for unconstrained optimization during training rather than post-hoc certification or projection steps. The parameterization is evaluated on the MIT-BIH arrhythmia dataset with 26,490 samples across 5 classes, comparing LipCNNs against vanilla CNNs and L2-regularized CNNs using cross-entropy loss and MOSEK solver via Julia/JuMP. The approach aims to provide a scalable and expressive way to train robust 1D CNNs with user-defined Lipschitz bounds.

## Key Results
- LipCNNs demonstrate improved robustness compared to vanilla and L2-regularized CNNs on adversarial examples
- Maintained high test accuracies across different perturbation strengths in L2 projected gradient descent attacks
- The proposed parameterization provides a scalable and expressive way to train robust 1D CNNs with user-defined Lipschitz bounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The parameterization guarantees Lipschitz continuity by construction through enforcing linear matrix inequalities (LMIs) that imply dissipativity.
- Mechanism: Each layer is parameterized so that it inherently satisfies LMIs. These LMIs are sufficient conditions for dissipativity, and the interconnection of dissipativity-certified layers ensures the overall network is Lipschitz continuous. This avoids the need for post-hoc certification or projection steps during training.
- Core assumption: The LMIs are tight enough that the feasible set includes expressive parameterizations, and the Lyapunov equations used to define the controllability Gramian have unique solutions.
- Evidence anchors:
  - [abstract] "The proposed parameterization by design fulfills linear matrix inequalities that are sufficient for Lipschitz continuity of the CNN"
  - [section] "If there exist ... then the CNN θ is ρ-Lipschitz continuous"
- Break condition: If the LMI relaxation becomes too conservative, the expressiveness of the network is severely limited and performance degrades. If the controllability Gramian fails to exist (e.g., unstable system), the parameterization is invalid.

### Mechanism 2
- Claim: The Cayley transform is used to parameterize orthogonal matrices, which ensures stability and Lipschitz bounds for linear layers.
- Mechanism: For fully connected layers, the parameterization uses the Cayley transform to map unconstrained variables into orthogonal matrices. This ensures that the resulting weight matrices have bounded spectral norms, which directly contributes to the Lipschitz bound. The structure U⊤U + V⊤V = I from the Cayley transform is exploited to satisfy the LMI conditions.
- Core assumption: The Cayley transform is invertible for the chosen parameter space, and the orthogonal matrices it generates are sufficient to capture the required expressiveness for the task.
- Evidence anchors:
  - [section] "We base our parameterization on the Cayley transform that parameterizes orthogonal matrices"
  - [section] "Lemma 4 (Cayley transform [22]): For all Y ∈ Rn×n and Z∈ Rm×n the Cayley transform ... yields matrices U∈ Rn×n and V∈ Rm×n that satisfy U⊤U +V⊤V = I"
- Break condition: If the Cayley transform maps parameters to matrices outside the desired spectral norm range, or if numerical issues arise in computing the inverse (I + M), the parameterization fails.

### Mechanism 3
- Claim: The controllability Gramian is used to parameterize Pi, ensuring the left upper block of the LMI is positive definite, which is critical for Lipschitz continuity of convolutional layers.
- Mechanism: Pi is set to the inverse of a controllability Gramian, which is computed as a sum of terms involving the state matrix Ai, input matrix Bi, and an auxiliary matrix Hi. This construction guarantees that Fi (the left upper block) is positive definite, satisfying the LMI requirement. This allows the convolutional kernel to be parameterized independently of input dimension.
- Core assumption: The system (Ai, Bi) is controllable so that the controllability Gramian exists and is positive definite. The choice of Hi ensures sufficient freedom to satisfy the LMI without overly constraining the parameterization.
- Evidence anchors:
  - [section] "We make use of the controllability Gramian of (5) to parameterize convolutional layers"
  - [section] "Lemma 8: For some ε > 0 and all Hi∈ Rnxi×nxi , the matrix Pi = X−1i with Xi = ... renders (12) feasible"
- Break condition: If the system is not controllable or if the chosen Hi leads to numerical instability in computing Pi, the parameterization is invalid.

## Foundational Learning

- Concept: Linear Matrix Inequalities (LMIs) and their use in certifying system properties.
  - Why needed here: LMIs are used to express sufficient conditions for dissipativity and Lipschitz continuity. Understanding how to formulate and solve LMIs is essential for grasping how the parameterization works.
  - Quick check question: What is the role of the Schur complement in transforming the LMI conditions for Lipschitz continuity?

- Concept: State space representation of convolutional layers.
  - Why needed here: The paper reformulates convolutions as state space systems to achieve a compact, dimension-independent parameterization. This is key to understanding the use of controllability Gramian.
  - Quick check question: How does the state space representation of a convolution differ from the traditional Toeplitz matrix formulation in terms of scalability?

- Concept: Cayley transform and orthogonal matrices.
  - Why needed here: The Cayley transform is used to parameterize orthogonal matrices, which are crucial for ensuring bounded spectral norms in linear layers. This is a core component of the Lipschitz-bounded parameterization.
  - Quick check question: Why is the Cayley transform preferred over other methods for parameterizing orthogonal matrices in this context?

## Architecture Onboarding

- Component map: Input -> Convolutional layers (with Gramian-based parameterization) -> Activation -> Pooling -> Fully connected layers (with Cayley-based parameterization) -> Output
- Critical path: Input → Convolutional layers (with Gramian-based parameterization) → Activation → Pooling → Fully connected layers (with Cayley-based parameterization) → Output
- Design tradeoffs:
  - Using state space representation vs. Fourier domain: State space is more compact and independent of input size but requires solving Lyapunov equations.
  - Enforcing dissipativity vs. layer-wise Lipschitz bounds: End-to-end approach is less conservative but requires more complex LMIs.
  - Cayley transform vs. other orthogonal parameterizations: Cayley is smooth and invertible but may have numerical issues for large matrices.
- Failure signatures:
  - Numerical instability in computing controllability Gramian or its inverse.
  - LMIs becoming infeasible due to overly conservative constraints.
  - Training divergence due to poor conditioning of parameterized matrices.
- First 3 experiments:
  1. Train a single Lipschitz-bounded convolutional layer on synthetic data to verify the Gramian parameterization.
  2. Train a small CNN with Cayley-parameterized fully connected layers on a simple classification task.
  3. Combine both parameterizations in a full CNN and evaluate robustness to adversarial examples.

## Open Questions the Paper Calls Out
- How would the proposed Lipschitz-bounded 1D CNN parameterization extend to 2D CNNs, and what modifications would be necessary?
- How does the proposed Lipschitz-bounded 1D CNN parameterization compare to other state-of-the-art methods for training robust CNNs in terms of computational efficiency and scalability?
- How sensitive is the proposed Lipschitz-bounded 1D CNN parameterization to the choice of hyperparameters, such as the Lipschitz bound ρ and the weighting factors in L2 regularization?

## Limitations
- Critical preprocessing steps for MIT-BIH dataset are referenced but not fully specified in the paper
- Exact CNN architecture parameters (layer sizes, padding, pooling specifics) are only partially described
- Numerical stability of controllability Gramian inversion for larger networks remains unverified
- Scalability of LMI-based constraints to deeper networks is not demonstrated

## Confidence

**High confidence**: The theoretical foundation linking LMIs to Lipschitz continuity is sound and mathematically rigorous.

**Medium confidence**: The parameterization approach using Cayley transforms and controllability Gramians is novel and theoretically valid, but practical implementation details could affect performance.

**Low confidence**: Empirical results showing robustness advantages over baselines are compelling but lack sufficient detail for independent verification of exact performance metrics.

## Next Checks

1. Verify the numerical stability of the controllability Gramian computation across different system dimensions and verify the positive definiteness of the resulting Pi matrices for various Hi choices.

2. Test the expressiveness of the parameterization by training LipCNNs with increasingly tight Lipschitz bounds and measuring the resulting test accuracy degradation.

3. Benchmark training time and memory usage of the proposed parameterization against standard CNN training to quantify computational overhead.