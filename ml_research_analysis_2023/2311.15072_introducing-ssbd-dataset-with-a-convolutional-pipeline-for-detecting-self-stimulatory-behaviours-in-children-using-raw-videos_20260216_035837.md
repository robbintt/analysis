---
ver: rpa2
title: Introducing SSBD+ Dataset with a Convolutional Pipeline for detecting Self-Stimulatory
  Behaviours in Children using raw videos
arxiv_id: '2311.15072'
source_url: https://arxiv.org/abs/2311.15072
tags:
- video
- authors
- dataset
- videos
- autism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel two-stage pipeline for detecting and
  classifying self-stimulatory behaviors (e.g., arm-flapping, headbanging, spinning)
  in children with autism from video data. The first stage, SSBDBinaryNet, uses a
  (2+1)D CNN with object detection and classification to identify snippets containing
  any self-stimulatory action.
---

# Introducing SSBD+ Dataset with a Convolutional Pipeline for detecting Self-Stimulatory Behaviours in Children using raw videos

## Quick Facts
- arXiv ID: 2311.15072
- Source URL: https://arxiv.org/abs/2311.15072
- Reference count: 28
- Accuracy: ~81% on augmented dataset

## Executive Summary
This paper presents a novel two-stage pipeline for detecting and classifying self-stimulatory behaviors (e.g., arm-flapping, headbanging, spinning) in children with autism from video data. The first stage, SSBDBinaryNet, uses a (2+1)D CNN with object detection and classification to identify snippets containing any self-stimulatory action. The second stage, SSBDIdentifier, employs a representative frame and pose keypoints to classify the specific behavior. The authors augment the SSBD dataset with 35 new videos and introduce a "no-class" label for snippets without stimming. Their pipeline achieves an overall accuracy of around 81% on the augmented dataset, enabling real-time, hands-free automated diagnosis. The approach addresses limitations of subjective clinical assessments and opens avenues for consistent tracking of behavioral cues in autism spectrum disorder.

## Method Summary
The authors present a two-stage pipeline for detecting and classifying self-stimulatory behaviors in children with autism from video data. The first stage, SSBDBinaryNet, uses a (2+1)D CNN with object detection and classification to identify snippets containing any self-stimulatory action. The second stage, SSBDIdentifier, employs a representative frame and pose keypoints to classify the specific behavior. The pipeline is trained and evaluated on the augmented SSBD+ dataset, which includes 35 new videos and a "no-class" label for snippets without stimming. The authors achieve an overall accuracy of around 81% on the augmented dataset.

## Key Results
- Two-stage pipeline achieves ~81% overall accuracy on augmented SSBD+ dataset
- SSBDBinaryNet effectively filters for presence of any stimming behavior before classification
- Representative frame selection via maximum keypoint displacement reduces computational load while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage pipeline reduces misclassification by first filtering for any stimming behavior, then classifying the specific type.
- Mechanism: SSBDBinaryNet acts as a coarse filter, rejecting most non-stimming snippets before SSBDIdentifier is invoked, lowering the burden on the more complex second-stage model.
- Core assumption: Most video snippets contain no stimming behavior, so early rejection improves efficiency and accuracy.
- Evidence anchors:
  - [abstract] "The pipelined architecture has major advantages including better accuracy in identifying the self-stimulatory behaviors due to the large inter-class differences in the first stage"
  - [section IV] "The dataset curated in the section II is observed to be highly unbalanced, with the ratio of about 7 no-class video snippet for every video snippet showing any self-stimulatory action"
- Break condition: If the ratio of stimming to non-stimming snippets shifts dramatically, the efficiency gain from early rejection may diminish.

### Mechanism 2
- Claim: (2+1)D convolutions capture both spatial and temporal features while reducing computation and overfitting risk compared to full 3D convolutions.
- Mechanism: Separable convolutions split spatial feature extraction (2D conv) from temporal feature extraction (1D conv), lowering parameter count and computation per frame.
- Core assumption: Spatial and temporal feature learning can be decoupled without loss of representational power for action detection.
- Evidence anchors:
  - [section IV-A] "The (2 + 1)D convolution layer allows modularizing the task of extracting features from a video to the sub-tasks of spatial feature extraction by a two-dimensional convolutional layer and temporal feature extraction by a one-dimension convolutional layer"
  - [section IV-A] "The (2 + 1)D convolution has relatively fewer computations and is less likely to overfit as compared to using a 3D convolution"
- Break condition: If action recognition requires highly correlated spatial-temporal patterns, the separable approach may miss interactions captured by full 3D convolutions.

### Mechanism 3
- Claim: Using a representative frame plus pose keypoints captures both visual context and body motion dynamics, improving action identification accuracy.
- Mechanism: ResNet-18 extracts high-level visual features from a single informative frame, while BiLSTM processes pose keypoint trajectories across all frames for motion dynamics, and concatenation merges both modalities.
- Core assumption: A single frame can sufficiently represent the spatial context for classification, and pose trajectories encode discriminative motion patterns.
- Evidence anchors:
  - [section IV-B] "Inspired by [18], the authors have exercised the concept of using a representative frame and the spatial location of keyjoints in all the frames for stimming behavior recognition"
  - [section IV-B] "The concatenation layer is followed by a fully-connected, feed-forward network that outputs the softmax probabilities for 3 classes"
- Break condition: If critical visual cues are distributed across multiple frames rather than concentrated in one, or if pose estimation fails frequently, accuracy may drop.

## Foundational Learning

- Concept: (2+1)D convolutions and separable feature extraction
  - Why needed here: Enables efficient spatiotemporal modeling with fewer parameters, crucial for real-time detection on limited hardware.
  - Quick check question: What is the key difference between a (2+1)D convolution and a standard 3D convolution in terms of computation and overfitting risk?

- Concept: Object detection + classification pipeline (YOLOv7 + VGG19)
  - Why needed here: Filters video frames to isolate child regions before behavioral classification, reducing noise and focusing the model on relevant areas.
  - Quick check question: Why might a two-stage approach (detection then classification) outperform a single-stage model for identifying children in video frames?

- Concept: Representative frame selection via maximum keypoint displacement
  - Why needed here: Ensures the model focuses on the most informative frame, reducing redundancy and computational load.
  - Quick check question: How does selecting the frame with the largest keypoint displacement between consecutive frames help in identifying meaningful actions?

## Architecture Onboarding

- Component map: Data preprocessing (10fps, 100x100, 40-frame chunks) -> Prefetch (YOLOv7 + VGG19) -> SSBDBinaryNet ((2+1)D conv + 3D BN + ReLU + GAP + Dense + Sigmoid) -> (if positive) SSBDIdentifier (ResNet-18 + BiLSTM + Concat + Dense + Softmax)
- Critical path: Prefetch -> SSBDBinaryNet -> (if positive) SSBDIdentifier
- Design tradeoffs:
  - Using (2+1)D convs reduces computation but may miss some spatiotemporal correlations
  - Single representative frame simplifies model but may lose context from other frames
  - Pose keypoints add robustness but depend on accurate detection
- Failure signatures:
  - High false negatives in BinaryNet: pipeline fails to detect present stimming
  - High false positives in BinaryNet: unnecessary invocation of Identifier, hurting efficiency
  - Poor pose estimation: BiLSTM receives noisy or missing keypoint data
- First 3 experiments:
  1. Validate that the (2+1)D conv layer improves BinaryNet accuracy over a 3D conv baseline on the same data
  2. Test BinaryNet's false negative rate with and without the prefetch child filter
  3. Compare SSBDIdentifier accuracy when using all frames vs. a single representative frame

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the pipeline's robustness be improved to handle more diverse video quality and environments, beyond the current SSBD+ dataset?
- Basis in paper: [explicit] The paper mentions the pipeline's overall accuracy of 81% on the SSBD+ dataset and proposes methods for reducing false negatives and false positives, but acknowledges the need for future work to refine these methods.
- Why unresolved: The paper does not provide specific details on how the pipeline would perform on videos with varying quality, lighting conditions, or backgrounds that are not represented in the SSBD+ dataset.
- What evidence would resolve it: Testing the pipeline on a more diverse set of videos with different quality levels, lighting, and backgrounds, and comparing the accuracy to that achieved on the SSBD+ dataset.

### Open Question 2
- Question: Can the pipeline be further optimized for low-latency deployment scenarios, especially in resource-constrained environments?
- Basis in paper: [explicit] The paper discusses the potential for reducing inference latency by omitting the Yolov7 and Pre-fetch model, but notes that this comes at the cost of a lower F1 score.
- Why unresolved: The paper does not explore alternative methods for achieving low-latency performance without significantly sacrificing accuracy.
- What evidence would resolve it: Developing and testing alternative model architectures or optimizations that maintain or improve accuracy while reducing latency, and comparing their performance to the current pipeline.

### Open Question 3
- Question: How effective is the proposed model distillation approach in reducing the model's size and computational requirements while maintaining or improving accuracy?
- Basis in paper: [explicit] The paper presents an experimental setting for distilling the teacher model into a smaller student model and notes that the student model achieved 80.89% of the teacher model's test F1-score with only 37.38% of the learnable weights.
- Why unresolved: The paper does not provide a comprehensive evaluation of the distilled model's performance on the full SSBD+ dataset or compare it to other model compression techniques.
- What evidence would resolve it: Evaluating the distilled model's performance on the full SSBD+ dataset and comparing its accuracy, latency, and resource requirements to the original pipeline and other model compression techniques.

## Limitations

- Small dataset size may limit generalizability to diverse populations and environments
- Performance metrics lack detailed statistical validation (confidence intervals, cross-validation)
- Model generalizability across different demographic groups and environmental conditions remains unproven

## Confidence

- Two-stage pipeline effectiveness: High
- (2+1)D convolution efficiency claims: Medium
- Overall accuracy (81%) on augmented dataset: Medium

## Next Checks

1. Test the complete pipeline on an independent, previously unseen video dataset to assess real-world generalizability
2. Perform k-fold cross-validation on the SSBD+ dataset to establish confidence intervals for accuracy metrics
3. Evaluate model performance across different demographic subgroups to identify potential bias or performance gaps