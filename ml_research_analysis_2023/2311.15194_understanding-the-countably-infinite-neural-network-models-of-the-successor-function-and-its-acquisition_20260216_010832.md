---
ver: rpa2
title: 'Understanding the Countably Infinite: Neural Network Models of the Successor
  Function and its Acquisition'
arxiv_id: '2311.15194'
source_url: https://arxiv.org/abs/2311.15194
tags:
- numbers
- successor
- learning
- place-value
- count
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study explores how neural network models learn the successor
  function, a foundational concept in understanding the natural numbers. Two models
  are compared: a count-list model using one-hot encoding and a place-value model
  using two-hot encoding.'
---

# Understanding the Countably Infinite: Neural Network Models of the Successor Function and its Acquisition

## Quick Facts
- arXiv ID: 2311.15194
- Source URL: https://arxiv.org/abs/2311.15194
- Reference count: 7
- The place-value model with two-hot encoding generalizes better than one-hot encoding for learning the successor function.

## Executive Summary
This study investigates how neural networks can model children's acquisition of the successor function, a fundamental concept underlying understanding of natural numbers. The authors compare two models: a count-list model using one-hot encoding and a place-value model using two-hot encoding that separates tens and ones digits. The place-value model demonstrates superior generalization ability, accurately computing successors for numbers outside its training range. The research provides insights into the mechanisms underlying children's understanding of countable infinity through the lens of neural network learning.

## Method Summary
The study employs two neural network models to learn the successor function (N → N+1) for numbers 0-98. The count-list model uses one-hot encoding with softmax output, while the place-value model uses two-hot encoding (10 units for tens, 10 for ones) with sigmoid output. Both models have 3 hidden layers of 8 units each with ReLU activation. The place-value model is trained on 80% of the data and tested on the remaining 20%, with performance evaluated through accuracy metrics, cosine similarity analysis, and MDS visualization of latent representations.

## Key Results
- The place-value model shows better generalization, accurately computing successors for numbers outside its training range.
- The model exhibits a drop in representational similarity across tens boundaries, suggesting an understanding of place-value structure.
- Vector analysis reveals that counting across tens boundaries can be understood as a 2D vector operation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The place-value model learns to represent tens and ones digits separately, enabling accurate successor computation across place-value boundaries.
- Mechanism: The model uses two-hot encoding where 10 units encode the tens place and 10 units encode the ones place. This allows the network to treat tens and ones as distinct features, making boundary-crossing operations (like 29→30) learnable through vector addition in a 2D latent space.
- Core assumption: The place-value encoding captures the recursive structure of number naming rules, making the successor function more learnable than a flat one-hot encoding.
- Evidence anchors:
  - [abstract]: "The place-value model uses a place-value encoding and corresponds to children learning the language rules for naming numbers."
  - [section]: "Unlike the count list model, learning the successor function in the place-value model corresponds to multi-label, multi-class classification."
- Break condition: If the model cannot generalize beyond training range, suggesting it memorized rather than learned the recursive structure.

### Mechanism 2
- Claim: Curriculum learning sharpens representations of smaller numbers while gradually introducing larger numbers, mimicking developmental progression.
- Mechanism: The model trains on expanding subsets of the data (first 0-19, then 0-39, etc.), allowing early-learned numbers to be reinforced while new numbers are added. This prevents interference and promotes stable representations.
- Core assumption: Smaller numbers serve as building blocks for understanding larger numbers, consistent with developmental psychology findings.
- Evidence anchors:
  - [section]: "A curriculum learning simulation shows that, in the expanding numerical environment of the developing child, representations of smaller numbers continue to be sharpened even as larger numbers begin to be learned."
- Break condition: If accuracy on small numbers degrades as larger numbers are introduced, indicating catastrophic forgetting.

### Mechanism 3
- Claim: The successor function across tens boundaries can be understood as a vector operation in 2D space, where the tens place is coded along one axis and the ones place along another.
- Mechanism: MDS analysis of hidden layer representations shows that numbers with the same tens place cluster along a line (tens axis) while numbers with the same ones place form groups (ones axis). The vector from N to N+1 has consistent direction and magnitude when crossing boundaries.
- Core assumption: The neural network learns to disentangle tens and ones in its latent representation, making boundary-crossing predictable.
- Evidence anchors:
  - [abstract]: "Counting across a tens boundary can be understood as a vector operation in 2D space, where the numbers with the same tens place are organized in a linearly separable manner, whereas those with the same ones place are grouped together."
- Break condition: If vector angles vary significantly across different tens boundaries, indicating inconsistent representation.

## Foundational Learning

- Concept: Place-value notation
  - Why needed here: The place-value model relies on understanding that numbers are composed of tens and ones digits, which are encoded separately in the input and output layers.
  - Quick check question: How many input units represent the tens place versus the ones place in the place-value model?

- Concept: Vector addition in latent space
  - Why needed here: The successor function across tens boundaries is modeled as a vector operation, requiring understanding of how vector addition works in reduced-dimensional space.
  - Quick check question: What property of the vectors connecting numbers like 9→10, 19→20, etc. indicates systematic learning?

- Concept: Curriculum learning
  - Why needed here: The simulation gradually expands the training data to mimic how children learn numbers, requiring understanding of how incremental training affects representation stability.
  - Quick check question: What happens to the accuracy on small numbers as the model is trained on larger numbers?

## Architecture Onboarding

- Component map:
  - Input layer (20 units: 10 for tens, 10 for ones) -> Hidden layers (3 layers of 8 units each with ReLU) -> Output layer (20 units: 10 for tens, 10 for ones with sigmoid)

- Critical path:
  1. Prepare place-value encoded data (tens and ones as separate one-hot vectors)
  2. Train model on full dataset (0-98 → 1-99)
  3. Analyze hidden layer representations using MDS
  4. Compare cosine similarities between numbers and their successors
  5. Evaluate boundary-crossing behavior

- Design tradeoffs:
  - Place-value encoding vs. one-hot encoding: Place-value enables better generalization but requires understanding of number structure
  - Hidden layer depth: Three layers were chosen empirically; fewer layers may not capture the recursive structure
  - Loss function: Cross-entropy chosen for multi-label classification; other losses may not converge properly

- Failure signatures:
  - Perfect training accuracy but zero test accuracy: Model memorized rather than learned the successor function
  - No drop in cosine similarity across tens boundaries: Model didn't learn place-value structure
  - Inconsistent vector angles across boundaries: Latent representation not systematic

- First 3 experiments:
  1. Train count-list model (one-hot) and verify it cannot generalize beyond training range
  2. Train place-value model and visualize hidden layer representations using MDS
  3. Compare cosine similarities between numbers and successors, especially across tens boundaries

## Open Questions the Paper Calls Out

- Question: How do the number representations learned by the place-value model change as the model is trained on increasingly larger ranges of numbers?
  - Basis in paper: [explicit] The paper mentions that the curriculum learning simulation shows that representations of smaller numbers continue to be sharpened even as larger numbers begin to be learned.
  - Why unresolved: The paper does not provide a detailed analysis of how the number representations evolve during the curriculum learning process.
  - What evidence would resolve it: A detailed analysis of the number representations learned by the place-value model at different stages of the curriculum learning simulation, showing how the representations of smaller numbers are refined while larger numbers are incorporated.

- Question: How does the sensitivity to place-value boundaries develop in children's understanding of the successor function?
  - Basis in paper: [explicit] The paper mentions that the place-value model showed a predicted drop in representational similarity across tens boundaries, suggesting an understanding of place-value structure.
  - Why unresolved: The paper does not provide a detailed analysis of how children's sensitivity to place-value boundaries develops over time.
  - What evidence would resolve it: A longitudinal study tracking children's performance on successor function tasks as they learn to count higher numbers, with a focus on their sensitivity to place-value boundaries.

- Question: How does the use of different neural network architectures, such as recurrent networks, impact the learning of the successor function and counting?
  - Basis in paper: [explicit] The paper mentions that future work will use recurrent architectures to move beyond learning the successor function to simulating the counting process more generally.
  - Why unresolved: The paper does not provide any results or analysis using recurrent network architectures.
  - What evidence would resolve it: A comparison of the performance of recurrent network models and the current models (count-list and place-value) on learning the successor function and counting, with an analysis of the differences in the representations learned by each type of model.

## Limitations

- The study relies on synthetic data rather than empirical data from actual children learning numbers, limiting ecological validity.
- The neural network models are abstractions that may not fully capture the complexity of human cognitive development.
- The focus on English number naming rules may limit generalizability to languages with different place-value structures.

## Confidence

**High Confidence** in the core finding that place-value encoding enables better generalization than one-hot encoding for learning the successor function.

**Medium Confidence** in the claim that the drop in cosine similarity across tens boundaries indicates learning of place-value structure.

**Medium Confidence** in the curriculum learning results showing sharpened representations of smaller numbers.

## Next Checks

1. **Cross-linguistic validation**: Test the models using number naming systems from languages with different place-value structures (e.g., Chinese, French) to verify that the place-value advantage is not language-specific.

2. **Developmental trajectory comparison**: Compare model learning curves with empirical data from children's number acquisition studies to assess ecological validity and identify any systematic discrepancies.

3. **Alternative architecture comparison**: Implement and test recurrent neural networks or transformer-based models on the same successor function task to determine whether the observed place-value advantages are specific to the feedforward architecture used.