---
ver: rpa2
title: Exploiting Counter-Examples for Active Learning with Partial labels
arxiv_id: '2307.07413'
source_url: https://arxiv.org/abs/2307.07413
tags:
- learning
- label
- samples
- query
- predictor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces active learning with partial labels (ALPL),
  a setting where an imperfect oracle annotates selected samples with partial labels
  rather than exact labels, reducing annotation burden. To address ALPL, the authors
  first establish a baseline using the RC loss for partial-label learning.
---

# Exploiting Counter-Examples for Active Learning with Partial labels

## Quick Facts
- arXiv ID: 2307.07413
- Source URL: https://arxiv.org/abs/2307.07413
- Reference count: 40
- This paper introduces active learning with partial labels (ALPL), a setting where an imperfect oracle annotates selected samples with partial labels rather than exact labels, reducing annotation burden. To address ALPL, the authors first establish a baseline using the RC loss for partial-label learning. They then propose WorseNet, which learns from counter-examples (CEs) constructed by inverting partial labels, guided by an inverse RC loss and Kullback-Leibler regularization. This adversarial learning pattern leverages the probability gap between WorseNet and the predictor to improve both prediction and sample selection. Experiments on four benchmark datasets (MNIST, Fashion-MNIST, SVHN, CIFAR-10) and five real-world datasets show that WorseNet achieves state-of-the-art performance, consistently outperforming ten existing AL methods and demonstrating effectiveness across different backbone architectures and partial-label generation strategies.

## Executive Summary
This paper addresses active learning with partial labels (ALPL), a scenario where an oracle provides a set of candidate labels (containing the true label) rather than a single exact label, reducing annotation effort. The authors first establish a baseline using the RC loss for partial-label learning. They then introduce WorseNet, which learns from counter-examples (CEs) constructed by inverting partial labels, guided by an inverse RC loss and Kullback-Leibler regularization. This adversarial learning pattern leverages the probability gap between WorseNet and the predictor to improve both prediction and sample selection. Experiments on four benchmark datasets and five real-world datasets show that WorseNet achieves state-of-the-art performance, consistently outperforming ten existing AL methods and demonstrating effectiveness across different backbone architectures and partial-label generation strategies.

## Method Summary
The method introduces active learning with partial labels (ALPL), where an oracle provides partial labels (sets of candidate labels containing the true label) instead of exact labels to reduce annotation burden. The approach starts with an initial labeled set and an unlabeled pool. In each round, samples are selected using a selector (e.g., MCU, MMU, EU), queried from the oracle for partial labels, and added to the labeled set. The predictor is trained with the RC loss on partial labels. WorseNet is trained with inverse RC loss and KL regularization on inverse partial labels (IPL) to learn from counter-examples. The predictor and WorseNet leverage their probability gap for improved prediction and selection.

## Key Results
- WorseNet consistently outperforms ten existing AL methods on four benchmark datasets (MNIST, Fashion-MNIST, SVHN, CIFAR-10) and five real-world datasets.
- The method demonstrates effectiveness across different backbone architectures and partial-label generation strategies.
- The probability gap between the predictor and WorseNet improves both prediction accuracy and sample selection.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constructing counter-examples by label inversion (IPL) provides supervision signals that complement the predictor's partial-label learning.
- Mechanism: The IPL labels act as false supervision, enabling WorseNet to learn patterns that are explicitly different from the predictor's focus on partial labels. The Kullback-Leibler divergence between the predictor and WorseNet is minimized only over the partial-label set, which implicitly increases the probability gap for non-partial labels.
- Core assumption: The true label always lies within the partial label set, so labels outside this set can be treated as informative negative examples.
- Evidence anchors:
  - [abstract]: "we construct CEs for the predictor by directly reversing their partial labels to the inverse version"
  - [section 5.1]: "We construct CEs for the predictor by directly reversing their partial labels to the inverse version"
  - [corpus]: No direct evidence; the corpus papers do not discuss label inversion for ALPL.
- Break condition: If the partial label generation process fails to always include the true label, the IPL construction becomes invalid and WorseNet learns incorrect patterns.

### Mechanism 2
- Claim: The probability gap between WorseNet and the predictor can be used to both improve prediction accuracy and refine sample selection.
- Mechanism: During inference, the true label is predicted as the class with the maximum difference (P - Q). For selection, the class space is narrowed to labels where P > Q, reducing uncertainty score computation to a more representative subset.
- Core assumption: The predictor's confidence on the true label exceeds WorseNet's confidence on the same label, creating a reliable gap signal.
- Evidence anchors:
  - [abstract]: "we propose to take advantage of the predicted probability gap between these two networks to separately improve the evaluating and selecting process"
  - [section 5.3]: "we assume that the true label is the class with the maximum probability distance between f and w"
  - [corpus]: No direct evidence; corpus neighbors do not cover this dual-gap mechanism.
- Break condition: If WorseNet becomes too confident or too similar to the predictor, the probability gap collapses, rendering the gap-based inference and selection ineffective.

### Mechanism 3
- Claim: Training WorseNet with inverse RC loss plus KL regularization acts as an auxiliary regularizer that pushes the predictor away from false labels.
- Mechanism: WorseNet learns high confidence on IPL labels, and KL regularization ensures its output distribution diverges from the predictor on the partial label set. This creates an implicit push for the predictor to lower confidence on non-partial labels.
- Core assumption: The inverse RC loss combined with KL divergence will create a stable adversarial learning pattern that benefits the predictor.
- Evidence anchors:
  - [section 5.2]: "we formulate this learning process, treating the IPL as the normal partial labels, to a similar PLL problem, where we propose inverse RC (IRC) loss"
  - [section 5.2]: "we further add a Kullback-Leibler divergence (KLD) regularization item for w"
  - [corpus]: No direct evidence; corpus neighbors do not describe adversarial PLL with IRC loss.
- Break condition: If the KL regularization weight is too high, WorseNet may collapse to a trivial solution; if too low, the adversarial effect is insufficient.

## Foundational Learning

- Concept: Partial-label learning (PLL)
  - Why needed here: ALPL extends active learning by using partial labels instead of exact labels, so understanding PLL is essential to handle the ambiguity in oracle feedback.
  - Quick check question: In PLL, does the candidate label set always contain the true label?

- Concept: Kullback-Leibler divergence (KLD)
  - Why needed here: KLD is used to regularize WorseNet so its output distribution diverges from the predictor on the partial label set, creating an adversarial learning signal.
  - Quick check question: In the context of WorseNet, what does minimizing KLD over S accomplish?

- Concept: Counter-examples in reasoning
  - Why needed here: The paper draws inspiration from cognitive science, where humans use counter-examples to reject false inferences. This motivates the construction of IPL labels as CEs for the predictor.
  - Quick check question: How does the IPL label set serve as a counter-example to the predictor's inference?

## Architecture Onboarding

- Component map:
  - Predictor (f) -> RC loss on partial labels
  - WorseNet (w) -> IRC loss + KLD on IPL labels
  - Selector -> MCU, MMU, EU, or WS variants
  - Oracle -> Provides partial labels for queried samples

- Critical path:
  1. Initialize labeled set L with b0 samples annotated with partial labels.
  2. Train predictor f with RC loss.
  3. Generate IPL labels for L.
  4. Train WorseNet w with IRC + KLD on IPL labels.
  5. Query b samples using selector Ψ(L, U, f).
  6. Label queried samples with partial labels, update L and W.
  7. Repeat training and querying until budget exhausted.
  8. For inference, use gap-based prediction (Eq. 9).

- Design tradeoffs:
  - Using IPL increases supervision variety but may introduce noise if the oracle's partial labels are too ambiguous.
  - The KL regularization balances between divergence and stability; too much can destabilize training.
  - Narrowing class space for selection (WS) reduces uncertainty but risks missing the true label if P ≤ Q for all labels.

- Failure signatures:
  - If WorseNet's accuracy on IPL labels is near random, the adversarial signal is weak.
  - If the selector selects mostly easy samples, the WS improvement may be negligible.
  - If KLD dominates, WorseNet may collapse to outputting uniform distributions.

- First 3 experiments:
  1. Train predictor alone with RC loss on a small partial-label dataset and measure overfitting.
  2. Add WorseNet with IRC + KLD, compare predictor performance and observe probability gap.
  3. Replace standard selector with WS variant, evaluate if selection quality improves on a validation set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of WorseNet vary when using different architectures for the predictor and WorseNet, such as transformers or graph neural networks?
- Basis in paper: [explicit] The paper mentions that WorseNet shares the same input and output space as the predictor, but does not explore different architectures.
- Why unresolved: The paper only uses identical architectures for the predictor and WorseNet.
- What evidence would resolve it: Experiments comparing WorseNet performance with different predictor and WorseNet architectures on the same datasets.

### Open Question 2
- Question: What is the impact of the regularization parameter α in the Worse loss on the final performance of the predictor?
- Basis in paper: [explicit] The paper mentions that α is a regularized parameter and is empirically set to 1, but does not explore its impact.
- Why unresolved: The paper does not perform a sensitivity analysis on the regularization parameter α.
- What evidence would resolve it: Experiments varying the value of α and analyzing its impact on the predictor's performance.

### Open Question 3
- Question: How does the proposed method perform in scenarios with more than 10 classes, such as ImageNet or COCO?
- Basis in paper: [inferred] The paper only evaluates the method on datasets with up to 10 classes, but does not explore its scalability to larger datasets.
- Why unresolved: The paper does not provide any evidence of the method's performance on datasets with more than 10 classes.
- What evidence would resolve it: Experiments evaluating the method on large-scale datasets like ImageNet or COCO.

## Limitations
- Generalizability: The approach may not scale well to extremely large-scale or noisy real-world scenarios.
- Theoretical guarantees: The paper lacks rigorous theoretical analysis of why the probability gap leads to better performance.
- Computational overhead: Training two networks increases training time and resource usage.

## Confidence
- High confidence in the empirical superiority of WorseNet variants over baseline methods.
- Medium confidence in the mechanism claims, as they are supported by experimental results but lack formal proofs.
- Medium confidence in the scalability and robustness claims, given limited testing beyond the evaluated datasets.

## Next Checks
1. **Ablation Study**: Remove the KL regularization term and measure the impact on WorseNet's performance to validate its role in stabilizing the adversarial learning pattern.
2. **Generalization Test**: Apply the framework to a large-scale dataset (e.g., ImageNet) and compare performance against the current benchmarks to assess scalability.
3. **Oracle Dependency**: Test the framework with varying levels of oracle accuracy in generating partial labels to evaluate robustness to annotation noise.