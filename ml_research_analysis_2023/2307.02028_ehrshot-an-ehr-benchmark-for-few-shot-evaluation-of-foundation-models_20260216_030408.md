---
ver: rpa2
title: 'EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models'
arxiv_id: '2307.02028'
source_url: https://arxiv.org/abs/2307.02028
tags:
- data
- dataset
- patient
- each
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of reproducibility and data access
  in machine learning for healthcare, particularly in the context of foundation models.
  It introduces EHRSHOT, a new dataset containing de-identified structured electronic
  health records (EHRs) of 6,712 patients from Stanford Medicine.
---

# EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models

## Quick Facts
- arXiv ID: 2307.02028
- Source URL: https://arxiv.org/abs/2307.02028
- Reference count: 40
- Key outcome: Introduces EHRSHOT benchmark and CLMBR foundation model for few-shot clinical prediction tasks with structured EHR data

## Executive Summary
This paper addresses reproducibility challenges in healthcare machine learning by introducing EHRSHOT, a new longitudinal EHR dataset containing 6,712 patients from Stanford Medicine. The authors present CLMBR, a 141M parameter clinical foundation model pretrained on 2.57M patients' coded EHR data. They define 15 few-shot clinical prediction tasks to evaluate sample efficiency and task adaptation. The results demonstrate that CLMBR significantly outperforms traditional supervised baselines in few-shot settings, highlighting the benefits of pretraining for sample efficiency while identifying areas for further research.

## Method Summary
The method involves three main components: (1) EHRSHOT dataset creation using de-identified structured EHRs from Stanford Medicine, (2) CLMBR foundation model pretrained on 2.57M patients using next-code prediction, and (3) a 15-task few-shot evaluation framework with k values from 1 to 128. The benchmark includes count-based GBM baselines and transformer-based CLMBR models, with evaluation metrics including AUROC and AUPRC across binary, multiclass, and multilabel tasks. The dataset and model are made available via research data use agreement.

## Key Results
- CLMBR foundation model significantly outperforms count-based GBM baseline in few-shot settings
- Pretraining provides substantial benefits for sample efficiency and task adaptation
- EHRSHOT provides more comprehensive longitudinal evaluation than existing ICU-only datasets
- Room for improvement remains on many tasks, indicating ongoing research opportunities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLMBR's autoregressive training enables effective few-shot learning through learned representations across millions of patients
- Mechanism: Pretraining on 2.57M patient timelines using next-code prediction allows the model to capture global patterns that transfer to new tasks with limited examples
- Core assumption: Next-code prediction representations generalize to diverse clinical prediction tasks
- Evidence anchors: Abstract mentions self-supervised next-code prediction training; section describes causally masked local attention for forward-only information flow
- Break condition: Task prediction windows too long relative to autoregressive training objective

### Mechanism 2
- Claim: Longitudinal data enables evaluation of foundation models on realistic clinical prediction scenarios
- Mechanism: Full patient timelines across all departments allow models to learn complete health trajectories rather than isolated acute episodes
- Core assumption: Clinical prediction benefits from complete medical history over episode-based data
- Evidence anchors: Abstract states EHRSHOT is longitudinal and not restricted to ICU/ED patients; section discusses challenges of shared pretrained models
- Break condition: Task definitions artificially constrain prediction windows to short time horizons

### Mechanism 3
- Claim: Ontology expansion improves count-based model performance by leveraging hierarchical relationships
- Mechanism: Each code occurrence is counted for itself and all parent nodes in the ontology hierarchy, increasing feature density and coverage
- Core assumption: Parent-child relationships in medical ontologies capture meaningful semantic relationships
- Evidence anchors: Section describes ontology expansion technique for increasing representation density; corpus mentions related preprocessing techniques
- Break condition: Incomplete or incorrect ontology hierarchy introduces noise rather than useful information

## Foundational Learning

- Concept: Few-shot learning
  - Why needed here: Benchmark specifically evaluates model performance with small labeled examples, common in healthcare due to labeling costs and rare conditions
  - Quick check question: What is the key difference between few-shot learning and traditional supervised learning in terms of data requirements?

- Concept: Foundation models and transfer learning
  - Why needed here: Evaluates whether pretraining on large EHR datasets provides benefits when adapting to new tasks with limited data
  - Quick check question: How does transfer learning from a foundation model differ from training a model from scratch on the target task?

- Concept: Longitudinal data analysis
  - Why needed here: Patient health trajectories over time are crucial for many clinical prediction tasks
  - Quick check question: Why might longitudinal data provide advantages over cross-sectional data for clinical prediction tasks?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline -> Count-based GBM baseline -> CLMBR foundation model -> Task definition system -> Evaluation framework

- Critical path:
  1. Load and preprocess EHR data using OMOP-CDM pipeline
  2. Generate ontology-expanded count features for GBM baseline
  3. Prepare sequential code representations for CLMBR
  4. Define task-specific prediction windows and labels
  5. Implement k-shot sampling strategy
  6. Train and evaluate both baseline models
  7. Compare performance across different k values

- Design tradeoffs:
  - Longitudinal vs. episode-based data: Longitudinal provides complete information but increases computational complexity
  - Count-based vs. sequential representations: Count-based is simpler but loses temporal ordering; sequential preserves order but requires complex models
  - Few-shot vs. full-data evaluation: Few-shot better reflects real-world constraints but may not capture model capabilities with abundant data

- Failure signatures:
  - Count-based model: Poor performance on tasks requiring temporal reasoning or complex feature interactions
  - CLMBR model: Overfitting on few-shot examples, underperformance on tasks with long prediction windows
  - Data pipeline: Missing codes, incorrect timestamp handling, or improper ontology expansion

- First 3 experiments:
  1. Verify data preprocessing by checking ontology expansion correctly maps child codes to parent codes and date jittering preserves relative ordering
  2. Test k-shot sampling by verifying balanced sampling produces equal numbers of positive and negative examples for each task
  3. Validate model implementations by running single task with k=1 and checking both models produce reasonable output probabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does CLMBR perform on EHRSHOT compared to other clinical foundation models like GatorTron or ClinicalBERT?
- Basis in paper: [explicit] CLMBR is one of the first fully released models for coded EHR data, contrasting with most prior models that only work with unstructured text
- Why unresolved: Paper does not provide direct comparisons between CLMBR and other clinical foundation models on EHRSHOT benchmark
- What evidence would resolve it: Head-to-head comparison of CLMBR and other clinical foundation models on EHRSHOT benchmark tasks

### Open Question 2
- Question: What is the impact of different pretraining strategies on clinical foundation model performance in few-shot settings?
- Basis in paper: [inferred] Paper discusses CLMBR pretraining using next code prediction and mentions benefits in few-shot settings without exploring alternative strategies
- Why unresolved: Paper does not investigate effects of different pretraining strategies on clinical foundation model performance
- What evidence would resolve it: Empirical study comparing performance of models pretrained with different strategies on few-shot learning tasks

### Open Question 3
- Question: How does clinical foundation model performance on EHRSHOT generalize to other healthcare systems and data sources?
- Basis in paper: [explicit] Paper acknowledges CLMBR performance has only been evaluated on Stanford Medicine data, unclear how well it performs at other institutions
- Why unresolved: Paper does not provide evidence of model performance on data from other healthcare systems
- What evidence would resolve it: Evaluating CLMBR performance on EHRSHOT and clinical prediction tasks using data from multiple healthcare systems

## Limitations
- Restricted access to dataset and pretrained model requiring research data use agreement limits reproducibility
- Few-shot evaluation protocol may not capture real-world clinical settings where labeled data is expensive but not as scarce as k=1 or k=2
- Focus on coded EHR data excludes valuable information from clinical notes, imaging, and other unstructured sources

## Confidence

- **High confidence**: CLMBR as one of the first fully released foundation models for coded EHR data represents genuine advance in making foundation models accessible
- **Medium confidence**: Pretraining on large EHR datasets provides significant benefits for few-shot adaptation, though magnitude varies across tasks and settings
- **Medium confidence**: EHRSHOT provides more comprehensive evaluation framework than existing benchmarks due to longitudinal nature and diverse task definitions

## Next Checks

1. **External validation**: Replicate benchmark evaluation using released CLMBR model and EHRSHOT dataset on at least 3 different clinical tasks with varying k-shot scenarios to verify reported performance improvements

2. **Clinical relevance assessment**: Conduct qualitative evaluation with clinical domain experts to assess whether 15 defined tasks and prediction windows align with realistic clinical decision-making scenarios and whether model outputs would be actionable

3. **Data access verification**: Attempt to obtain EHRSHOT dataset and CLMBR model weights through research data use agreement process to verify access requirements are reasonable and released artifacts match paper specifications