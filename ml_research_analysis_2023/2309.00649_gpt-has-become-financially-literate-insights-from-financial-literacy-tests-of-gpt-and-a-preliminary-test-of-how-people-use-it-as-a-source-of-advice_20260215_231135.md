---
ver: rpa2
title: 'GPT has become financially literate: Insights from financial literacy tests
  of GPT and a preliminary test of how people use it as a source of advice'
arxiv_id: '2309.00649'
source_url: https://arxiv.org/abs/2309.00649
tags:
- financial
- chatgpt
- will
- literacy
- year
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study assesses GPT models\u2019 ability to provide financial\
  \ advice by testing them on financial literacy questions and a savings dilemma.\
  \ GPT-3.5 scored 66-65% on the literacy test, while GPT-4 achieved near-perfect\
  \ accuracy (99%)."
---

# GPT has become financially literate: Insights from financial literacy tests of GPT and a preliminary test of how people use it as a source of advice

## Quick Facts
- **arXiv ID:** 2309.00649
- **Source URL:** https://arxiv.org/abs/2309.00649
- **Reference count:** 40
- **Primary result:** GPT-4 achieved near-perfect financial literacy (99%) while GPT-3.5 scored 66%, with participants showing high reliance on LLM advice especially when they had low subjective financial knowledge.

## Executive Summary
This study evaluates the financial literacy capabilities of GPT models and examines how people utilize their advice through a Judge-Advisor System framework. GPT-4 demonstrated near-perfect performance on standardized financial literacy tests, while GPT-3.5 showed moderate capabilities. The research also found that people exhibit high weight-of-advice (WOA) scores when receiving financial guidance from GPT, with those having lower subjective financial knowledge showing even greater reliance on the model's recommendations.

## Method Summary
The study tested GPT-3.5 (Davinci and ChatGPT) and GPT-4 on 21 financial literacy questions from established surveys, running 20 trials per model. A savings dilemma was used to measure advice utilization through the Judge-Advisor System, where participants predicted GPT's performance and then made decisions with access to GPT's advice. The weight-of-advice index was calculated to quantify how much participants adjusted their choices based on the model's recommendations.

## Key Results
- GPT-4 achieved near-perfect financial literacy scores (99.3% without pre-prompt, 97.4% with pre-prompt)
- GPT-3.5 scored 65-66% on financial literacy tests, significantly higher than random chance
- Participants showed WOA of 0.65 overall, with those having low subjective financial knowledge exhibiting WOA of 0.74
- GPT's advice reduced Mean Absolute Percentage Error by 58.6% compared to participants' initial estimates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GPT-4's near-perfect financial literacy score emerges from its ability to synthesize large-scale training data into coherent reasoning about financial concepts.
- **Mechanism:** The model maps input questions to learned representations of financial knowledge without explicit symbolic reasoning. Its 175B parameters enable statistical pattern matching across diverse financial contexts.
- **Core assumption:** Financial literacy requires retrieval and integration of factual knowledge rather than pure logical deduction.
- **Evidence anchors:** "ChatGPT based on GPT-4 achieves a near-perfect 99% score, pointing to financial literacy becoming an emergent ability of state-of-the-art models."
- **Break condition:** If GPT-4's performance degrades over time (as noted in recent studies), the emergent literacy assumption fails.

### Mechanism 2
- **Claim:** The Judge-Advisor System reveals that people over-rely on LLM advice, especially when they lack subjective financial knowledge.
- **Mechanism:** Individuals with lower self-assessed financial knowledge exhibit higher weight-of-advice (WOA) scores, indicating deference to the model. This is driven by confidence heuristicâ€”treating model certainty as competence.
- **Core assumption:** Subjective financial knowledge is a reliable proxy for actual financial literacy and correlates with advice-taking behavior.
- **Evidence anchors:** "The Judge-Advisor System was used to measure how people utilize advice from GPT in a savings problem, finding a weight of advice index (WOA) of 0.65, higher in individuals with low subjective financial knowledge (WOA=0.74)."
- **Break condition:** If subjective financial knowledge poorly predicts actual financial literacy, the correlation between WOA and subjective knowledge collapses.

### Mechanism 3
- **Claim:** Financial literacy emerges as an emergent ability in LLMs, distinct from general language understanding.
- **Mechanism:** As model size increases, non-linear performance gains occur in domain-specific tasks like financial reasoning, independent of raw parameter count.
- **Core assumption:** Emergent abilities are discontinuous and not simply a function of model scale.
- **Evidence anchors:** "ChatGPT based on GPT-4 achieves a near-perfect 99% score, pointing to financial literacy becoming an emergent ability of state-of-the-art models."
- **Break condition:** If GPT-4's performance regresses or GPT-3.5 catches up, the emergent ability claim is invalidated.

## Foundational Learning

- **Concept:** Statistical pattern matching in large language models
  - Why needed here: Understanding how LLMs generate coherent responses without explicit reasoning is key to interpreting their financial literacy.
  - Quick check question: How does a model with 175B parameters "know" the answer to a financial literacy question without symbolic reasoning?

- **Concept:** Judge-Advisor System (JAS) in human-AI interaction
  - Why needed here: The JAS framework measures how people integrate advice from non-human sources, crucial for assessing LLM advice utilization.
  - Quick check question: What does a WOA score of 0.65 indicate about participant reliance on GPT's advice?

- **Concept:** Emergent abilities in large models
  - Why needed here: Financial literacy scores for GPT-4 suggest non-linear performance gains, a hallmark of emergent abilities.
  - Quick check question: Why might GPT-4's 99% score on financial literacy be considered "emergent" rather than expected?

## Architecture Onboarding

- **Component map:** Financial literacy questions (Mitchell & Lusardi, 2022; Heinberg et al., 2014) -> GPT models (3.5 Davinci, 3.5 ChatGPT, 4 ChatGPT) -> Accuracy scoring and WOA computation -> Human-in-the-loop Judge-Advisor System

- **Critical path:** 1. Prompt LLMs with financial literacy questions (20 trials per model). 2. Collect human predictions of GPT performance and conduct savings dilemma task. 3. Compute WOA and correlate with subjective/objective financial knowledge. 4. Validate robustness via back-testing with modified prompts and answer orders.

- **Design tradeoffs:** Deterministic vs. stochastic prompting: Lower temperature (0) for factual accuracy vs. higher for diverse responses. Pre-prompting: Minimal impact on GPT-4 but reduces GPT-3.5 performance, suggesting sensitivity to prompt engineering. Single vs. multiple trials: 20 trials per question balance variance control with computational cost.

- **Failure signatures:** High variance in GPT-3.5 responses across trials indicates unreliable performance. WOA > 1 or < 0 (after winsorization) suggests extreme over- or under-reliance on advice. Back-testing accuracy < 90% signals fragility to prompt variations.

- **First 3 experiments:** 1. **Prompt sensitivity test:** Vary temperature and pre-prompting on a subset of financial literacy items to map performance boundaries. 2. **Human prediction validation:** Compare predicted vs. actual GPT performance across different demographic segments to isolate bias sources. 3. **Adversarial prompt test:** Modify financial literacy questions with subtle wording changes to probe model robustness and identify failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of GPT-4 on financial literacy tests generalize to more complex financial planning tasks beyond basic literacy questions?
- Basis in paper: [explicit] The paper states GPT-4 achieved a near-perfect score (99%) on financial literacy tests, but notes these were basic tests designed for laypeople.
- Why unresolved: The study only tested basic financial literacy, not complex planning scenarios requiring integration of multiple financial concepts.
- What evidence would resolve it: Testing GPT-4 on comprehensive financial planning tasks (retirement planning, tax optimization, investment portfolio construction) with professional financial advisor benchmarks.

### Open Question 2
- Question: How does the quality of financial advice from LLMs compare to human financial advisors when considering individual-specific factors like risk tolerance and personal circumstances?
- Basis in paper: [inferred] The paper notes that professional financial advice considers individual characteristics, but their study used generic scenarios without personalization.
- Why unresolved: The study used hypothetical scenarios without accounting for individual differences that would be present in real financial advising.
- What evidence would resolve it: Controlled experiments comparing LLM advice to human advisors using personalized client profiles with varying risk tolerances, income levels, and financial goals.

### Open Question 3
- Question: What are the long-term behavioral impacts of using LLM financial advice compared to traditional sources, and does it lead to sustained improvements in financial decision-making?
- Basis in paper: [inferred] The study found high advice utilization but was a one-time experiment; it mentioned possible benefits but didn't measure actual financial outcomes.
- Why unresolved: The study was a short-term laboratory experiment without follow-up to measure real-world financial behavior changes.
- What evidence would resolve it: Longitudinal studies tracking actual financial behaviors and outcomes of people who regularly use LLM advice versus traditional sources over 6-12 months.

## Limitations

- The study's financial literacy assessment used only 21 questions, which may not comprehensively represent real-world financial decision-making complexity.
- Subjective financial knowledge was measured through self-reporting, introducing potential measurement error and bias.
- The study did not test complex, scenario-based financial planning tasks that would better represent real-world advisory contexts.

## Confidence

- **High Confidence:** GPT-4's superior performance on standardized financial literacy questions (99% accuracy) is well-supported by the empirical results, though the limited question set tempers absolute certainty.
- **Medium Confidence:** The correlation between subjective financial knowledge and advice utilization (WOA) is statistically significant but relies on self-reported measures that may introduce bias.
- **Low Confidence:** Claims about financial literacy as an emergent ability require more extensive validation across different model architectures and task domains.

## Next Checks

1. **Temporal robustness test:** Re-run the financial literacy assessment monthly for six months to quantify performance drift and establish confidence intervals for GPT-4's claimed expertise.

2. **Ecological validity expansion:** Design and test a dynamic financial scenario battery that includes time-dependent decisions, compound interest calculations, and risk assessment tasks beyond static multiple-choice questions.

3. **Advisor comparison benchmark:** Compare GPT-4's financial advice quality against certified financial planners on identical scenarios, measuring both accuracy and the impact on decision quality in controlled experiments.