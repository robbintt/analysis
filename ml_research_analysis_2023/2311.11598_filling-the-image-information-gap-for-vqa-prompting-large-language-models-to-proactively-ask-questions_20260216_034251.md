---
ver: rpa2
title: 'Filling the Image Information Gap for VQA: Prompting Large Language Models
  to Proactively Ask Questions'
arxiv_id: '2311.11598'
source_url: https://arxiv.org/abs/2311.11598
tags:
- information
- image
- question
- questions
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework that enables Large Language Models
  (LLMs) to proactively ask relevant questions to fill information gaps in images
  for knowledge-based Visual Question Answering (VQA). The key idea is to have LLMs
  interact with Vision-Language Models (VLMs) to iteratively acquire missing details
  through generated questions and answers, which are then refined and used for reasoning.
---

# Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions

## Quick Facts
- arXiv ID: 2311.11598
- Source URL: https://arxiv.org/abs/2311.11598
- Reference count: 33
- One-line primary result: Framework enables LLMs to ask questions to fill image information gaps, improving knowledge-based VQA accuracy by 2.15% on average

## Executive Summary
This paper addresses the challenge of knowledge-based Visual Question Answering (VQA) where Large Language Models (LLMs) lack direct access to visual information. The authors propose a framework that enables LLMs to proactively ask relevant questions to uncover missing details in images, which are then answered by a Vision-Language Model (VLM). The generated information is refined and used to improve the LLM's reasoning for answering the original visual question. Experiments on OK-VQA and A-OKVQA datasets demonstrate consistent improvements over baseline methods.

## Method Summary
The proposed framework consists of three key modules: inquiry, refinement, and answering. The inquiry module prompts the LLM to generate new questions targeting missing image information that captions may not capture. These questions are answered by a VLM to provide additional visual details. The refinement module summarizes and filters the generated question-answer pairs to extract useful information while removing noise. Finally, the answering module uses the refined information to prompt the LLM to predict the final answer. The approach leverages the reasoning capabilities of LLMs and the visual understanding of VLMs to bridge the information gap between images and text descriptions.

## Key Results
- Framework consistently improves baseline performance by an average of 2.15% on OK-VQA dataset
- Gains observed across different LLM models (GPT-3.5, GPT-4, LLaMA-2)
- Refinement module crucial for performance, contributing a 2.38% accuracy boost compared to 0.07% without refinement
- Method achieves gains on both OK-VQA and A-OKVQA datasets, demonstrating generalization

## Why This Works (Mechanism)

### Mechanism 1
Proactively querying VLMs allows LLMs to recover specific missing visual details that captions fail to capture. The LLM generates targeted questions about visual features implied by the question but not described in the caption, which are answered by a VLM to provide precise visual information.

### Mechanism 2
The refinement module filters noisy generated information to retain only useful details. It summarizes question-answer pairs into narrative descriptions and uses a learned filter to score their relevance to the original question and image.

### Mechanism 3
Incorporating refined visual details into LLM prompts improves answer accuracy by providing additional context for reasoning. The refined image information is added to the LLM prompt alongside the original caption and question, giving the LLM more complete visual context.

## Foundational Learning

- **Visual Question Answering (VQA) requiring external knowledge**
  - Why needed here: The task involves answering questions that require knowledge beyond what is visually present in the image.
  - Quick check question: What distinguishes knowledge-based VQA from standard VQA?

- **Image-to-text conversion and information loss**
  - Why needed here: Understanding how converting images to text can lead to loss of subtle details critical for answering questions.
  - Quick check question: Why might a caption fail to capture all the information needed to answer a visual question?

- **In-context learning with LLMs**
  - Why needed here: The method relies on prompting LLMs with examples to perform the VQA task.
  - Quick check question: How does in-context learning allow LLMs to perform tasks without fine-tuning?

## Architecture Onboarding

- **Component map**: LLM (question generator) → VLM (question answerer) → Refinement module → LLM (answer reasoner)
- **Critical path**: LLM question generation → VLM answer generation → refinement module → LLM answer reasoning
- **Design tradeoffs**: 
  - Number of questions generated: More questions may uncover more details but increase noise
  - Refinement filter complexity: More complex filters may better identify useful information but require more training data
  - Integration of refined information: Different integration strategies may affect LLM performance
- **Failure signatures**:
  - No improvement over baseline: Could indicate issues with question generation, VLM accuracy, or refinement
  - Decreased performance: May suggest refined information is misleading or LLM is overwhelmed with context
  - High variance across examples: Could indicate sensitivity to specific types of questions or images
- **First 3 experiments**:
  1. Ablation: Compare performance with and without the refinement module to isolate its impact
  2. Ablation: Compare performance with different numbers of generated questions to find optimal balance between detail and noise
  3. Ablation: Compare performance using different types of image information (dense captions vs. general captions) to understand value of refined information

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several remain unresolved based on the content:

### Open Question 1
How does the refinement module handle conflicting or contradictory information from different generated questions and answers? The paper mentions the refinement module's existence and purpose but does not elaborate on how it resolves conflicts between different sources of information.

### Open Question 2
What is the impact of varying the number of generated questions (K) on the performance of the proposed method? The paper mentions generating 3 new questions but does not explore how different values of K affect performance.

### Open Question 3
How does the proposed method compare to other methods that incorporate external knowledge for knowledge-based VQA tasks? The paper mentions some methods that query external knowledge bases but does not provide a detailed comparison with these methods.

## Limitations

- Reliance on VLM-generated answers introduces uncertainty about the accuracy of information used for reasoning
- 2.15% average improvement, while consistent, represents a modest gain that may not justify additional complexity in all scenarios
- Method's effectiveness appears sensitive to the quality of generated questions and the refinement module's ability to filter noise

## Confidence

- **High confidence**: The core mechanism of using LLMs to generate questions about missing visual details is well-supported by evidence and logically sound
- **Medium confidence**: The claim that this approach "consistently" improves performance across different LLMs is supported by results on OK-VQA but would benefit from testing on a wider range of models and datasets
- **Low confidence**: The assertion that the method can "rectify inaccurate information and minimize ambiguity in image captions" is not directly tested or measured in the paper

## Next Checks

1. **Contradiction detection**: Test whether the method can identify and handle situations where VLM-generated answers contradict the original caption, measuring how often contradictions occur and how they affect final accuracy

2. **Noise sensitivity analysis**: Systematically vary the quality of generated questions (e.g., by introducing noise or vagueness) and measure the resulting performance degradation to quantify the method's robustness to poor question generation

3. **Cross-dataset generalization**: Evaluate the framework on datasets beyond OK-VQA and A-OKVQA, particularly those requiring different types of reasoning (temporal, spatial, causal) to assess whether gains are task-specific or represent a general improvement in knowledge-based VQA