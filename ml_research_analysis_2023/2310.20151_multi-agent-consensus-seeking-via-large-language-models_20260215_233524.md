---
ver: rpa2
title: Multi-Agent Consensus Seeking via Large Language Models
arxiv_id: '2310.20151'
source_url: https://arxiv.org/abs/2310.20151
tags:
- position
- agents
- agent
- other
- consensus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies consensus seeking among multiple LLM-driven
  agents, where each agent starts with a numerical state and negotiates with others
  to reach a consensus value. The primary finding is that when not explicitly directed,
  agents primarily use the average strategy for consensus seeking, showing cooperative
  behavior.
---

# Multi-Agent Consensus Seeking via Large Language Models

## Quick Facts
- **arXiv ID**: 2310.20151
- **Source URL**: https://arxiv.org/abs/2310.20151
- **Reference count**: 37
- **Primary result**: LLM-driven agents naturally adopt average consensus strategies and show cooperative behavior, with agent number, personality, and network topology significantly influencing outcomes.

## Executive Summary
This paper investigates consensus seeking among multiple LLM-driven agents, each starting with a numerical state and negotiating to reach a consensus value. The study reveals that agents predominantly use an average consensus strategy, even without explicit direction, reflecting cooperative behavior. Experiments demonstrate that increasing agent number reduces consensus value variance, suggesting multiple agents can mitigate LLM randomness. The paper further explores how agent personalities (stubborn vs. suggestible) and network topologies (fully connected, directed, partial connectivity) affect the negotiation process and convergence dynamics. The framework is validated on a multi-robot aggregation task, demonstrating potential for zero-shot autonomous planning in multi-robot collaboration.

## Method Summary
The method employs multiple LLM agents (GPT-3.5-turbo-0613) initialized with numerical states and personality traits. Agents iteratively negotiate, each outputting reasoning and a proposed state based on others' current states, using a connectivity matrix to define visible agents. The consensus process repeats until convergence or a maximum number of rounds. Experiments vary agent count (2, 4, 6, 8), topologies (fully connected, directed, etc.), and personalities (stubborn, suggestible, mixed), using Monte Carlo simulations to analyze consensus achievement, strategy adoption, and final consensus value distribution.

## Key Results
- LLM-driven agents primarily adopt an average consensus strategy, showing cooperative behavior.
- Increasing agent number reduces variance in final consensus values, alleviating LLM randomness.
- Stubborn agents dominate consensus outcomes and slow convergence, while suggestible agents can cause oscillations unless agent count is high.
- Network topology significantly impacts convergence speed and can produce leader-follower or clustering behaviors.
- The framework enables zero-shot autonomous planning for multi-robot aggregation tasks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLM-driven agents default to an average consensus strategy when not explicitly directed.
- **Mechanism**: Each agent calculates the mean of all agents' current states and moves toward that value in the next round, reflecting a cooperative approach.
- **Core assumption**: The LLM, given a negotiation prompt, will generate reasoning that leads to an averaging calculation rather than random or selfish behavior.
- **Evidence anchors**:
  - [abstract] "LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies."
  - [section 3.1] "The most common strategy adopted by the agents is the average strategy: the agents set their state in the next round as the average value of the current states of all agents."
  - [corpus] Weak: no related paper directly confirms this average tendency; only mentions "consensus-seeking."
- **Break condition**: If the LLM is heavily biased toward individualistic reasoning or if the prompt structure strongly encourages non-cooperative moves, the average strategy may not emerge.

### Mechanism 2
- **Claim**: Agent personality (stubborn vs. suggestible) significantly influences the final consensus outcome and convergence speed.
- **Mechanism**: Stubborn agents keep their state unchanged, forcing others to move toward them; suggestible agents move toward others' states, which can cause oscillation unless the agent count is high enough to stabilize the group.
- **Core assumption**: Personality prompts reliably alter the agent's reasoning to produce the expected movement pattern.
- **Evidence anchors**:
  - [section 3.2] "Compared to suggestible agents, stubborn agents tend to insist on their views and are less likely to change... stubborn agents have a dominant influence on the final consensus value of the group."
  - [section 3.2] "When both agents are suggestible... may lead to oscillations... increasing the number of agents may effectively stabilize group decision-making."
  - [corpus] Weak: related papers mention robustness and consensus but not personality-driven dynamics.
- **Break condition**: If personality prompt effects are overridden by hallucination or if the LLM misinterprets the personality trait, the mechanism fails.

### Mechanism 3
- **Claim**: Network topology (fully connected, directed, or partial connectivity) affects convergence speed and can produce leader-follower or clustering behaviors.
- **Mechanism**: In fully connected networks, information flows efficiently and convergence is fast; in directed or sparse topologies, information bottlenecks slow convergence and can create dominance hierarchies or clusters.
- **Core assumption**: The connectivity matrix in the prompt limits which agents' states are visible to each agent, and the LLM reasons accordingly.
- **Evidence anchors**:
  - [section 3.3] "When the network is fully connected, the exchange of information is most efficient, resulting in fast consensus convergence speed. When the network is not fully connected, the consensus convergence speed slows down."
  - [section 3.3] "In the case of directed graphs, a leader-follower hierarchical structure emerges since some agents have a dominant influence on the final consensus outcome."
  - [corpus] Weak: no neighbor paper specifically addresses topology impact in LLM consensus.
- **Break condition**: If the LLM ignores the connectivity constraints or hallucinates missing agent states, the topology effect is nullified.

## Foundational Learning

- **Concept**: Chain-of-Thought (CoT) reasoning in LLMs
  - **Why needed here**: CoT allows agents to generate explicit reasoning before choosing their next state, improving consistency and reducing random moves.
  - **Quick check question**: If you remove the "Reasoning:" part from the prompt, will agents still converge reliably?
- **Concept**: Monte Carlo simulation for variance analysis
  - **Why needed here**: Multiple random initializations show how agent count and temperature affect the stability of consensus outcomes.
  - **Quick check question**: What happens to the variance of final consensus values as the number of agents increases from 2 to 8?
- **Concept**: Multi-agent system dynamics (graph theory)
  - **Why needed here**: Understanding how network topology (adjacency matrix) impacts consensus convergence informs prompt design and experiment setup.
  - **Quick check question**: In a directed star topology with one leader, how does convergence time compare to a fully connected network?

## Architecture Onboarding

- **Component map**: LLM planner (GPT-3.5-turbo) -> Reasoning and next state generation -> State extraction -> Update via connectivity matrix -> Repeat until convergence
- **Critical path**: Prompt generation → LLM inference → State extraction → Update → Repeat until convergence or max rounds
- **Design tradeoffs**:
  - Temperature vs. stability: lower temperature reduces variance but may reduce exploration
  - Agent count vs. computation: more agents improve stability but increase LLM calls
  - Prompt length vs. coherence: longer prompts may improve reasoning but risk cutoff
- **Failure signatures**:
  - Oscillation between states (suggestible-only agents)
  - Stuck at different clusters (multiple stubborn agents)
  - Random walk (high temperature, weak reasoning)
- **First 3 experiments**:
  1. Run 2-agent fully connected case with neutral personality, record convergence and strategy used.
  2. Run 10-agent fully connected case with all stubborn personalities, observe clustering behavior.
  3. Run 5-agent directed chain topology, track leader-follower convergence speed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the specific wording of the initial prompt affect the strategies chosen by LLM-driven agents in consensus-seeking tasks?
- **Basis in paper**: [explicit] The paper uses a specific prompt format for the consensus-seeking task, but does not explore how variations in prompt wording might influence agent strategies.
- **Why unresolved**: The study focuses on analyzing the impact of factors like agent number, personality, and network topology, but does not investigate the role of prompt wording in shaping agent behavior.
- **What evidence would resolve it**: Systematic experiments varying the prompt wording while keeping other factors constant, and analyzing the resulting changes in agent strategies and consensus outcomes.

### Open Question 2
- **Question**: Can the observed "stubborn" and "suggestible" personality traits in LLM-driven agents be further refined or categorized to better capture the nuances of agent behavior in consensus-seeking tasks?
- **Basis in paper**: [explicit] The paper identifies two broad personality types - stubborn and suggestible - but does not explore the potential for more nuanced personality traits or their impact on consensus outcomes.
- **Why unresolved**: The study only examines two personality types, which may not fully capture the range of possible agent behaviors in consensus-seeking tasks.
- **What evidence would resolve it**: Additional experiments introducing more refined or diverse personality traits, and analyzing their effects on agent strategies, consensus outcomes, and the overall negotiation process.

### Open Question 3
- **Question**: How does the choice of LLM model (e.g., GPT-3.5 vs. GPT-4) affect the strategies and consensus outcomes in multi-agent consensus-seeking tasks?
- **Basis in paper**: [explicit] The study only uses GPT-3.5-turbo-0613 for the LLM-driven agents, without exploring the potential impact of using different LLM models.
- **Why unresolved**: The paper does not investigate whether the observed behaviors and consensus outcomes are specific to GPT-3.5 or if they generalize to other LLM models.
- **What evidence would resolve it**: Replication of the study using different LLM models (e.g., GPT-4, Claude, or other large language models) and comparing the resulting strategies, consensus outcomes, and the impact of factors like agent number, personality, and network topology.

## Limitations

- Exact prompt templates and personality descriptions are not fully specified, making exact reproduction challenging.
- The observed average strategy tendency lacks direct confirmation from related literature.
- Robustness of personality effects against LLM hallucinations is not fully validated.

## Confidence

- **Mechanism 1 (Average Strategy)**: Medium - supported by experiments, but prompt details missing.
- **Mechanism 2 (Personality Effects)**: Medium - plausible, but sensitivity to prompt wording unclear.
- **Mechanism 3 (Topology Impact)**: Medium - well-supported, but no external literature confirmation.
- **Multi-robot Application**: Low - limited detail on controller and motion model.

## Next Checks

1. **Prompt Sensitivity**: Test whether removing the explicit "Reasoning:" section from the prompt significantly impacts consensus reliability and strategy adoption.

2. **Agent Count Scaling**: Systematically measure the variance of final consensus values as agent number increases from 2 to 8, under both neutral and mixed personality conditions.

3. **Topology Robustness**: Compare convergence times and final states for a directed star topology (one leader) versus a fully connected network, verifying the emergence of leader-follower dynamics.