---
ver: rpa2
title: Six Lectures on Linearized Neural Networks
arxiv_id: '2308.13431'
source_url: https://arxiv.org/abs/2308.13431
tags:
- neural
- kernel
- networks
- error
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper provides a comprehensive analysis of linearized neural
  networks through six lectures, focusing on their behavior in high dimensions. It
  examines various models, including linear regression with concentrated features,
  kernel ridge regression, random feature models, and neural tangent models.
---

# Six Lectures on Linearized Neural Networks

## Quick Facts
- arXiv ID: 2308.13431
- Source URL: https://arxiv.org/abs/2308.13431
- Reference count: 0
- Key outcome: Comprehensive analysis of linearized neural networks in high dimensions revealing benign overfitting and double descent phenomena

## Executive Summary
This paper provides a theoretical framework for understanding linearized neural networks through six lectures, focusing on their behavior in high-dimensional settings. The authors analyze various models including linear regression with concentrated features, kernel ridge regression, random feature models, and neural tangent models. They demonstrate that overparametrized models exhibit surprising generalization properties such as benign overfitting where interpolation doesn't harm generalization. The infinite-width limit of neural networks can be effectively approximated by kernel ridge regression with the neural tangent kernel, providing insights into optimization and generalization in high-dimensional settings.

## Method Summary
The paper analyzes linearized neural networks through a mathematical framework combining concentration of measure techniques, reproducing kernel Hilbert space theory, and high-dimensional asymptotics. The core approach involves studying feature representations (random features and neural tangent features), ridge regression on these features, and analyzing the spectral properties of kernel matrices. The theoretical analysis covers linear regression under feature concentration assumptions, kernel ridge regression with inner-product kernels in polynomial high-dimensional scaling, and comparisons between finite-width models and their infinite-width limits. The method relies on Gaussian concentration inequalities and spectral decomposition techniques to characterize generalization error bounds.

## Key Results
- Overparametrized models exhibit benign overfitting where interpolation doesn't harm generalization in high dimensions
- The infinite-width limit of neural networks is well-approximated by kernel ridge regression with the neural tangent kernel
- Random feature models require O(n) neurons to match the generalization performance of the infinite-width limit
- Double descent behavior occurs at the interpolation threshold in random feature models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The infinite-width limit of neural networks is well-approximated by kernel ridge regression (KRR) with the neural tangent kernel (NTK).
- Mechanism: As the width N → ∞, the random feature kernel converges pointwise to its expectation, and the NT kernel converges to a deterministic kernel that depends only on the activation function and input distribution.
- Core assumption: The feature maps and NT featurization maps are sufficiently concentrated around their expectations.
- Evidence anchors:
  - [abstract]: "The authors demonstrate that the infinite-width limit of neural networks can be effectively approximated by kernel ridge regression."
  - [section 3.1]: "We draw w1, ..., wN i.i.d. from a common distribution ν on Rd. As the number of neurons goes to infinity, both kernels converge pointwise to their expectations by law of large numbers."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.345. Weak correlation evidence; no direct experimental anchors found.
- Break condition: If the activation function lacks sufficient regularity or the input distribution is highly non-isotropic, the convergence to NTK may fail.

### Mechanism 2
- Claim: Ridge regression exhibits benign overfitting in high dimensions, where interpolation does not harm generalization.
- Mechanism: In high dimensions, the kernel matrix concentrates such that the effective regularization λ* > 0 even when the explicit regularization λ = 0+, acting as a self-induced regularizer.
- Core assumption: The data distribution and target function are sufficiently smooth and the sample size is large compared to the effective dimensionality.
- Evidence anchors:
  - [abstract]: "The study reveals that overparametrized models exhibit surprising generalization properties, such as benign overfitting and double descent."
  - [section 2.4]: "These bounds have an interesting consequence. They allow us to characterize pairs Σ, β∗ (with p = ∞) for which min-norm interpolation (the λ = 0+ limit of ridge regression) is 'consistent' even if τ > 0."
  - [corpus]: Weak evidence; neighbor papers do not directly address benign overfitting in high dimensions.
- Break condition: If the target function has significant high-frequency components or the noise level is too high, benign overfitting may not occur.

### Mechanism 3
- Claim: Random feature models require O(n) neurons to match the generalization performance of the infinite-width limit.
- Mechanism: The random feature kernel matrix decomposes into a low-rank component (capturing low-degree polynomial features) and a high-rank component (approximating identity), allowing for effective dimensionality reduction.
- Core assumption: The number of neurons N and samples n are well-separated (N ≫ n or n ≫ N).
- Evidence anchors:
  - [abstract]: "The authors demonstrate that the infinite-width limit of neural networks can be effectively approximated by kernel ridge regression."
  - [section 4.2]: "Theorem 3. Assume dℓ1+δ ≤ n ≤ dℓ1+1−δ, dℓ2+δ ≤ N ≤ dℓ2+1−δ, max(N/n, n/N) ≥ dδ for some integers ℓ1, ℓ2 and constant δ > 0."
  - [corpus]: No direct evidence found in neighbor papers.
- Break condition: If N ≈ n (interpolation threshold), the random feature model suffers from double descent and poor generalization.

## Foundational Learning

- Concept: Reproducing Kernel Hilbert Space (RKHS)
  - Why needed here: RKHS provides the mathematical framework for understanding kernel methods and their connection to neural networks.
  - Quick check question: What is the reproducing property of an RKHS, and how does it relate to the kernel trick?

- Concept: Spectral decomposition of inner-product kernels
  - Why needed here: The spectral decomposition allows us to analyze the eigendecomposition of kernel matrices and understand their behavior in high dimensions.
  - Quick check question: How does the eigendecomposition of an inner-product kernel on the sphere relate to the Gegenbauer polynomials?

- Concept: Gaussian concentration and concentration of measure
  - Why needed here: Concentration inequalities are used to prove the convergence of random feature kernels and the behavior of kernel matrices in high dimensions.
  - Quick check question: What is the difference between sub-Gaussian and sub-exponential random variables, and how does this relate to concentration of measure?

## Architecture Onboarding

- Component map: Data generation -> Feature extraction -> Model training -> Kernel approximation
- Critical path:
  1. Generate data according to the isotropic model.
  2. Compute the feature representations (random or neural tangent).
  3. Perform ridge regression on the feature representations.
  4. Analyze the generalization error using the spectral decomposition of the kernel matrix.

- Design tradeoffs:
  - Random features vs. neural tangent features: Random features are simpler to analyze but may require more neurons for good generalization.
  - Ridge regularization λ: Too small λ may lead to overfitting, while too large λ may lead to underfitting.
  - Width N: Larger N improves approximation to the infinite-width limit but increases computational cost.

- Failure signatures:
  - Double descent: Test error peaks at the interpolation threshold (N ≈ n) and then decreases again.
  - Benign overfitting failure: Test error does not improve with increasing N due to high-frequency components in the target function.
  - Kernel concentration failure: The finite-width kernel does not converge to the infinite-width limit due to insufficient width or non-smooth activation function.

- First 3 experiments:
  1. Generate data with a low-degree polynomial target function and compare the generalization error of random feature regression with different numbers of neurons N.
  2. Analyze the spectral decomposition of the neural tangent kernel for different activation functions and input distributions.
  3. Investigate the effect of ridge regularization on the generalization error of kernel ridge regression in high dimensions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the self-induced regularization observed in linearized neural networks and the implicit regularization of SGD in non-linear neural networks?
- Basis in paper: [explicit] The paper discusses self-induced regularization in linearized models (Section 3.4, Remark 3.4.1) and mentions implicit regularization in the introduction (Section 1.2). However, it does not explicitly connect these two phenomena.
- Why unresolved: The paper focuses on linearized models and does not investigate the relationship between self-induced regularization and implicit regularization in non-linear networks.
- What evidence would resolve it: Experimental studies comparing the behavior of linearized and non-linear neural networks under different optimization algorithms and initialization schemes.

### Open Question 2
- Question: How does the choice of activation function affect the generalization properties of linearized neural networks in high dimensions?
- Basis in paper: [explicit] The paper mentions that the results hold for "generic" activation functions (Section 3.2), but does not explore the impact of specific activation functions in detail.
- Why unresolved: The paper focuses on a general theory and does not investigate the role of specific activation functions in detail.
- What evidence would resolve it: Theoretical analysis and experimental studies comparing the performance of linearized neural networks with different activation functions in high-dimensional settings.

### Open Question 3
- Question: What are the limitations of the mean-field approach for analyzing deep neural networks beyond two layers?
- Basis in paper: [explicit] The paper mentions that the mean-field approach can be extended to multilayer neural networks (Remark 6.2.1), but does not discuss the challenges and limitations of such an extension.
- Why unresolved: The paper focuses on two-layer networks and does not investigate the complexities of analyzing deeper networks using the mean-field approach.
- What evidence would resolve it: Theoretical analysis and numerical simulations exploring the behavior of mean-field equations for deep neural networks.

## Limitations
- The analysis heavily relies on high-dimensional asymptotics that may not hold in practical, finite-dimensional settings
- The convergence to infinite-width limits depends critically on feature concentration properties sensitive to activation function smoothness
- The paper's empirical validation is limited, with most results being theoretical

## Confidence
- **High Confidence**: The theoretical framework connecting linearized neural networks to kernel methods and the characterization of generalization behavior in high dimensions
- **Medium Confidence**: The claims about benign overfitting and double descent phenomena, as these depend on specific data and target function properties
- **Low Confidence**: The practical implications for finite-width neural networks, as the gap between theoretical infinite-width limits and practical implementations is not fully characterized

## Next Checks
1. **Finite-Width Verification**: Conduct experiments comparing finite-width neural networks with their corresponding kernel approximations across different activation functions and widths to quantify the convergence rate and identify break-down conditions.

2. **Distribution Sensitivity Analysis**: Test the concentration assumptions under non-isotropic input distributions and heavy-tailed feature distributions to identify when the theoretical bounds fail in practice.

3. **Generalization Gap Measurement**: Measure the actual generalization gap between theoretical predictions and empirical performance on real datasets, particularly focusing on the transition between under-parametrized, critically-parametrized, and over-parametrized regimes.