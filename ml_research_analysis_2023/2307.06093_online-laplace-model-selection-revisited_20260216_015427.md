---
ver: rpa2
title: Online Laplace Model Selection Revisited
arxiv_id: '2307.06093'
source_url: https://arxiv.org/abs/2307.06093
tags:
- online
- laplace
- rmse
- test
- train
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work revisits online Laplace methods for model selection in
  neural networks. These methods interleave regular NN optimization with hyperparameter
  updates targeting the Laplace-approximated evidence.
---

# Online Laplace Model Selection Revisited

## Quick Facts
- arXiv ID: 2307.06093
- Source URL: https://arxiv.org/abs/2307.06093
- Reference count: 40
- One-line primary result: Online Laplace methods can be viewed as variational inference in a tangent linear model, sharing fixed points with the exact Laplace evidence while being more computationally tractable.

## Executive Summary
This paper revisits online Laplace methods for hyperparameter optimization in neural networks, showing that they approximate the evidence of a tangent linear model rather than the exact NN posterior. The authors derive the standard online Laplace objective as a variational lower bound on this evidence, providing theoretical justification for its use. Experiments on UCI regression datasets demonstrate that the method achieves test log-likelihoods of -3.0 to -2.0 nats, outperforming validation-based early stopping while preventing overfitting through evidence maximization.

## Method Summary
The method interleaves standard neural network optimization with hyperparameter updates targeting the Laplace-approximated evidence. It performs a first-order Taylor expansion of the NN function around current parameters to create a tractable tangent linear model, then applies Laplace's method to this linear model. The standard online Laplace objective is derived as an Evidence Lower Bound (ELBO) on the evidence of this tangent model. The approach uses full-batch gradient descent with online hyperparameter updates via MacKay's analytic formulas, optimizing precision hyperparameters for both the prior and observation noise.

## Key Results
- Online Laplace methods achieve test log-likelihoods of -3.0 to -2.0 nats across UCI regression datasets
- The method outperforms validation-based early stopping while preventing overfitting
- Fixed points exist where both NN parameters and linear model parameters converge to the same MAP
- The standard online Laplace objective is a lower bound on the mode-corrected evidence of the tangent linear model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Online Laplace methods approximate the evidence of a mode-corrected tangent linear model, not the exact NN posterior evidence.
- **Mechanism:** The method performs a first-order Taylor expansion of the NN function around the current parameters and then applies Laplace's method to the resulting linear model. This creates a tractable evidence approximation that does not require stationarity.
- **Core assumption:** The NN's Jacobian remains a good linearisation point during training, and the Hessian of the linear model approximates the curvature of the NN posterior.
- **Evidence anchors:**
  - [abstract] "This work re-derives online Laplace methods, showing them to target a variational bound on a variant of the Laplace evidence which does not make stationarity assumptions."
  - [section 3] "We show that a variant of OL that includes the first-order Taylor term (and thus does not assume stationarity) corresponds to NN hyperparameter optimisation with the evidence of a tangent linear model."
  - [corpus] Weak evidence for this mechanism's practical impact; no citations yet.
- **Break condition:** If the linearisation point drifts far from a region where the first-order approximation is accurate, the evidence estimate becomes unreliable and hyperparameter updates may destabilize training.

### Mechanism 2
- **Claim:** The standard online Laplace objective is a lower bound on the mode-corrected evidence, making it safe to optimize.
- **Mechanism:** By treating the linear model's evidence as the target and bounding it via the Evidence Lower Bound (ELBO), the method guarantees that hyperparameter updates improve (or at least do not harm) the approximation.
- **Core assumption:** The variational posterior mean can be set to the linear model's MAP without loss of tightness in the bound.
- **Evidence anchors:**
  - [section 3] "Online Laplace as a variational bound on the tangent model's evidence... Thus, discarding the first-order term in the Taylor expansion of log pf (y, w; M), which seemed unjustified, actually results in a lower bound on a variant of the Laplace evidence."
  - [abstract] "We re-derive the standard OL objective as a variational lower bound on the evidence of this tangent linear model, motivating its use."
  - [corpus] No direct citations; inference based on derivation in the paper.
- **Break condition:** If the ELBO is too loose (e.g., due to poor linearisation), the bound may not guide hyperparameters toward regions of good generalization.

### Mechanism 3
- **Claim:** Fixed points exist where both the NN parameters and linear model parameters converge to the same MAP, satisfying Laplace's method assumptions.
- **Mechanism:** At convergence, the NN's parameters become a MAP of the posterior, and the linear model's evidence matches the NN's Laplace evidence. This shared stationary point motivates the online procedure.
- **Core assumption:** Full-batch gradient descent with online tuning can approach these fixed points in practice.
- **Evidence anchors:**
  - [abstract] "We show OL shares fixed points, where the ELBO is tight, with the above first-order-corrected procedure. Here, the linear model's maximum a posteriori (MAP) parameters match the NN parameters..."
  - [section 3] "Since the NN and linear model share hyperparameters M, their loss gradients match at the linearisation point..."
  - [corpus] Weak evidence; no citations yet.
- **Break condition:** If optimization gets stuck in a local stationary point that is not a true MAP, the fixed-point property fails and the approximation loses validity.

## Foundational Learning

- **Concept:** Laplace approximation and its use for Bayesian model selection
  - **Why needed here:** The method hinges on approximating the posterior with a Gaussian around a mode and using the evidence for hyperparameter optimization.
  - **Quick check question:** What role does the Hessian (or GGN) play in the Laplace approximation, and why is it replaced by the GGN in neural networks?

- **Concept:** Taylor expansion and linearisation of neural networks
  - **Why needed here:** The tangent linear model is built via a first-order Taylor expansion; understanding this is essential to see why the method works.
  - **Quick check question:** How does the Jacobian of the neural network at the current parameters define the linear model's regressor?

- **Concept:** Evidence Lower Bound (ELBO) and variational inference
  - **Why needed here:** The standard online Laplace objective is derived as an ELBO on the tangent model's evidence; knowing this links the method to broader variational inference practice.
  - **Quick check question:** Under what condition does the ELBO become tight, and what does that imply for the fixed-point analysis?

## Architecture Onboarding

- **Component map:** NN parameters w -> Jacobian J -> GGN H -> Hyperparameters M -> Evidence objectives -> Parameter updates
- **Critical path:**
  1. Forward pass through NN to get predictions f(wt)
  2. Compute Jacobian J = ∂w f(wt)
  3. Evaluate loss Hessian (GGN) H = βJᵀJ + αI
  4. Update NN parameters via gradient descent
  5. Update hyperparameters via analytic MAP formulas from MacKay (1992)
  6. Repeat until convergence

- **Design tradeoffs:**
  - Full-batch vs. mini-batch: Full-batch gives stable linearisation but is expensive; mini-batch may give noisy Jacobians.
  - Including vs. excluding first-order term: Including gives tighter bound but can cause instability; excluding is more stable but looser.
  - Analytic vs. numerical hyperparameter updates: Analytic updates are exact for linear models but assume the tangent model is accurate.

- **Failure signatures:**
  - Rapid divergence of hyperparameters (especially α or β)
  - Jacobian becoming ill-conditioned (near-singular H)
  - NN parameters drifting away from linearisation point (wt ≠ v⋆)
  - ELBO plateauing early while test error keeps improving (bound too loose)

- **First 3 experiments:**
  1. **Sanity check on synthetic data:** Train a tiny NN (e.g., 2-3 layers, <10 units) on a simple regression task and verify that the linear model's MAP converges to the NN's parameters.
  2. **ELBO tightness test:** Plot Lf, Lh, and L over training; confirm L ≤ Lh and that Lf ≤ Lh.
  3. **Hyperparameter trajectory analysis:** Run OL on a UCI dataset and plot α, β over iterations; check for stability and convergence to reasonable values.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does online Laplace model selection outperform validation-based early stopping in terms of test log-likelihood?
- Basis in paper: [explicit] The paper states that online Laplace methods "outperform validation-based early stopping" but does not specify the exact conditions or thresholds.
- Why unresolved: The paper provides experimental results showing improvements on UCI regression datasets but does not analyze the factors that determine when online methods are superior.
- What evidence would resolve it: Systematic experiments varying dataset size, model complexity, and hyperparameter dimensionality to identify the regimes where online Laplace is most effective compared to validation-based methods.

### Open Question 2
- Question: How does the inclusion of the first-order Taylor term in online Laplace methods affect training stability and convergence compared to the standard approach?
- Basis in paper: [explicit] The paper mentions that including the first-order term "leads to training instability" but does not provide detailed analysis of the trade-offs.
- Why unresolved: The paper only briefly mentions training instability without quantifying the extent or investigating mitigation strategies.
- What evidence would resolve it: Controlled experiments comparing convergence speed, hyperparameter trajectories, and final performance between online Laplace with and without the first-order term across diverse datasets and model architectures.

### Open Question 3
- Question: To what extent do the theoretical fixed points of online Laplace methods manifest in practice, and how does this depend on optimization hyperparameters?
- Basis in paper: [explicit] The paper states that "these optima are roughly attained in practise" but does not quantify the gap or analyze its sensitivity to optimization settings.
- Why unresolved: The paper provides empirical evidence of approximate convergence but lacks rigorous analysis of the approximation quality and its determinants.
- What evidence would resolve it: Detailed analysis of the distance between NN parameters and tangent model MAP throughout training, examining how this varies with learning rates, batch sizes, and convergence criteria.

## Limitations

- Theoretical fixed points and ELBO tightness are not empirically validated beyond observing hyperparameter stabilization
- Limited experimental evidence of the tangent linear model's practical impact on prediction quality
- Full-batch gradient descent approach may not scale to larger datasets or more complex architectures

## Confidence

- **High confidence**: The derivation of the ELBO for the tangent linear model and the fixed-point analysis are mathematically sound given the stated assumptions.
- **Medium confidence**: The experimental results showing improved log-likelihoods over validation-based early stopping, though limited to small UCI datasets.
- **Low confidence**: The claim that online Laplace methods prevent overfitting through evidence maximization, as this is inferred from hyperparameter stabilization rather than directly measured generalization gaps.

## Next Checks

1. **ELBO tightness verification**: Plot Lf, Lh, and L over training on a held-out dataset to empirically verify that L ≤ Lh and that the bound becomes tight at fixed points.
2. **Linearisation drift analysis**: Measure the Frobenius norm of (wt - v⋆) during training to quantify how much the NN parameters drift from the tangent model's MAP, and correlate this with prediction quality.
3. **Mini-batch scalability test**: Implement a mini-batch variant of the method and test on a larger regression dataset (e.g., year prediction MSD) to assess whether the theoretical benefits persist under realistic computational constraints.