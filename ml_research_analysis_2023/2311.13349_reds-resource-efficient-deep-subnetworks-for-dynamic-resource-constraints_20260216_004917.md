---
ver: rpa2
title: 'REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource Constraints'
arxiv_id: '2311.13349'
source_url: https://arxiv.org/abs/2311.13349
tags:
- knapsack
- layer
- reds
- macs
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Resource-Efficient Deep Subnetworks (REDS),
  a method to create nested subnetworks that adapt to dynamic resource constraints
  on edge devices. REDS uses structured sparsity based on neuron permutation invariance
  to enable hardware-specific optimizations and contiguous memory storage of weights.
---

# REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource Constraints

## Quick Facts
- arXiv ID: 2311.13349
- Source URL: https://arxiv.org/abs/2311.13349
- Reference count: 40
- Primary result: REDS achieves adaptation times under 40 microseconds on resource-constrained devices while maintaining high accuracy across multiple architectures

## Executive Summary
REDS introduces a novel approach for creating nested neural network subnetworks that can dynamically adapt to resource constraints on edge devices. The method leverages permutation invariance of neurons to reorder weights in contiguous memory regions, enabling hardware-specific optimizations. By formulating subnetwork design as an iterative knapsack problem, REDS achieves superior worst-case performance compared to top-down approaches while requiring only minutes to find solutions versus days for neural architecture search methods.

## Method Summary
REDS creates nested subnetworks through a bottom-up iterative knapsack approach. Starting from a pre-trained model, neurons are assigned importance scores based on their contribution to accuracy. The method exploits permutation invariance to reorder neurons and store weight tensors contiguously in memory, optimizing for cache performance on target devices. The framework then solves a generalized knapsack problem to determine which neurons to include at each resource constraint level, creating a family of subnetworks that can be switched between in microseconds. The approach is validated across DNN, CNN, and DS-CNN architectures on Google Speech Commands, FMNIST, and CIFAR-10 datasets.

## Key Results
- Achieves adaptation times under 40 microseconds on Arduino Nano 33 BLE Sense
- Outperforms L1 norm pruning and random selection baselines while requiring only minutes for solution discovery
- Maintains high accuracy across 25%, 50%, 75%, and 100% MAC reduction levels on multiple architectures and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Permutation invariance of neurons allows reordering without changing network function
- Mechanism: Neurons in the same layer can be permuted arbitrarily as long as incoming and outgoing connections are adjusted correspondingly
- Core assumption: Neural networks exhibit symmetry in their optimization landscape
- Evidence anchors:
  - [abstract]: "REDS leverages structured sparsity constructively by exploiting permutation invariance of neurons"
  - [section 3.1]: "The optimization landscape of neural networks contains an abundance of minima and saddle points as a result of numerous symmetries"
  - [corpus]: Weak - related papers focus on edge deployment but don't directly address permutation invariance
- Break condition: If network architecture has skip connections or layer-specific operations that depend on neuron ordering

### Mechanism 2
- Claim: Bottom-up knapsack approach yields better worst-case performance than top-down
- Mechanism: Starting with smallest subnetwork and iteratively growing ensures better quality of intermediate solutions
- Core assumption: The problem structure allows proving bounds on iterative knapsack solutions
- Evidence anchors:
  - [section 3.3.2]: "Our theoretical analysis...shows that the bottom-up approach promises a better worst-case performance"
  - [appendix A.2]: Formal proof that bottom-up yields 2/3·Opt while top-down yields 1/2·Opt
  - [corpus]: Weak - no direct evidence in related papers about knapsack approaches
- Break condition: If importance scores are not well-behaved or if MAC reduction doesn't correlate with performance

### Mechanism 3
- Claim: Contiguous memory layout optimization reduces inference time on devices with cache
- Mechanism: By storing weight tensors in contiguous memory regions and adjusting matrix multiplication order, cache hit rates improve significantly
- Core assumption: Device has cache architecture that benefits from contiguous memory access patterns
- Evidence anchors:
  - [section 5]: "We now show how by simply adapting the computational graph at compile time, we are able to optimize the computation of REDS subnetworks for devices with a cache memory architecture"
  - [section 5.2]: Empirical results showing 12-58% speed-up on Raspberry Pi Pico
  - [corpus]: Weak - related papers mention edge deployment but not specific cache optimization techniques
- Break condition: If device lacks cache or if memory constraints prevent storing weights contiguously

## Foundational Learning

- Concept: Permutation invariance in neural networks
  - Why needed here: REDS relies on this property to reorder neurons without affecting network function
  - Quick check question: If we swap neurons in a layer, what else must we adjust to maintain the same network function?

- Concept: Knapsack problem formulation for model compression
  - Why needed here: REDS uses iterative knapsack to find optimal subnetwork structures
  - Quick check question: In a knapsack problem, what are the "items" and what is the "capacity" constraint in the REDS context?

- Concept: Cache optimization for embedded systems
  - Why needed here: REDS includes specific optimizations to leverage device cache architecture
  - Quick check question: Why does storing weights in contiguous memory improve cache performance?

## Architecture Onboarding

- Component map: Pre-trained model -> Importance scoring -> Permutation -> Knapsack optimization -> Contiguous memory layout -> Deployment
- Critical path: Pre-trained model → importance scoring → knapsack optimization → contiguous memory layout → deployment
- Design tradeoffs: Contiguous memory vs. model flexibility, knapsack solution quality vs. computation time, cache optimization vs. general applicability
- Failure signatures: Degraded accuracy when switching subnetworks, unexpected inference latency, memory allocation failures
- First 3 experiments:
  1. Verify permutation invariance on a simple network by permuting neurons and checking predictions remain unchanged
  2. Compare bottom-up vs top-down knapsack performance on a small dataset with varying data availability
  3. Measure cache hit rates and inference times with/without contiguous memory layout optimization on target hardware

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does REDS performance compare when combined with quantized models?
- Basis in paper: [inferred] The paper mentions that "we do not test the efficiency of REDS when combined with quantized models, which is left for future work."
- Why unresolved: The authors explicitly state this as a limitation and future work, indicating it hasn't been tested yet.
- What evidence would resolve it: Empirical results showing accuracy and latency comparisons between REDS with and without quantization on the same benchmark architectures and datasets.

### Open Question 2
- Question: How does REDS handle more complex architectures with skip connections?
- Basis in paper: [inferred] The paper states "The support of REDS for specific layers like skip connections is not explored and should be addressed in the future to support further advanced architectures."
- Why unresolved: The authors explicitly identify this as a limitation and future work, indicating it hasn't been implemented or tested.
- What evidence would resolve it: Successful implementation and testing of REDS on architectures with skip connections, showing maintained accuracy and performance benefits.

### Open Question 3
- Question: What is the theoretical worst-case performance bound for the generalized knapsack problem for DS-CNN architectures?
- Basis in paper: [explicit] The paper states "Since our generalized problem suited for DS-CNN architectures has the classical 0-1 knapsack as its core problem, we believe that a similar result is valid for this case as well."
- Why unresolved: The authors acknowledge that while they believe a similar result to the classical knapsack holds, they haven't proven it for the generalized case.
- What evidence would resolve it: A formal mathematical proof showing the worst-case performance bound for the generalized iterative knapsack problem used in REDS for DS-CNN architectures.

## Limitations
- Permutation invariance assumption may not hold for networks with skip connections or layer-specific operations
- Cache optimization benefits are hardware-specific and may not translate to devices without cache or with different memory hierarchies
- Theoretical bounds on knapsack solutions rely on specific properties of importance scores that may not generalize across all model architectures

## Confidence
- High confidence: Experimental results demonstrating REDS performance on multiple datasets and architectures (measured accuracies and adaptation times)
- Medium confidence: Theoretical analysis of bottom-up vs top-down knapsack approaches (proof exists but relies on idealized assumptions)
- Medium confidence: Memory layout optimization benefits (empirical results show improvements but may be hardware-specific)

## Next Checks
1. Test REDS on networks with skip connections or layer-specific operations to validate permutation invariance assumptions across diverse architectures
2. Implement the iterative knapsack solver on a smaller problem instance and verify the theoretical bounds hold empirically under different data availability scenarios
3. Benchmark cache optimization benefits across multiple hardware platforms with varying cache architectures to establish generalizability of the contiguous memory layout approach