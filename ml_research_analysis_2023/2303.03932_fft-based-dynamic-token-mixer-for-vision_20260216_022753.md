---
ver: rpa2
title: FFT-based Dynamic Token Mixer for Vision
arxiv_id: '2303.03932'
source_url: https://arxiv.org/abs/2303.03932
tags:
- layer
- lter
- dynamic
- global
- shape
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Dynamic Filter, a new token-mixer for vision
  transformers, and corresponding DFFormer and CDFFormer architectures. Dynamic Filter
  dynamically generates global filters for each pair of feature channels based on
  their contents, inspired by the success of data-dependent operations in recent MLP-based
  models.
---

# FFT-based Dynamic Token Mixer for Vision

## Quick Facts
- arXiv ID: 2303.03932
- Source URL: https://arxiv.org/abs/2303.03932
- Authors: 
- Reference count: 40
- Primary result: DFFormer and CDFFormer achieve SOTA performance among non-MHSA vision models on ImageNet-1K, with CDFFormer-B36 reaching 85.0% top-1 accuracy

## Executive Summary
This paper introduces Dynamic Filter, a novel token-mixer for vision transformers that generates global filters dynamically based on input content. The approach uses an MLP to create coefficients that linearly combine basis filters in the frequency domain, enabling data-dependent token mixing. The proposed DFFormer and CDFFormer architectures achieve state-of-the-art performance among vision models on ImageNet-1K (excluding MHSA-based models) while demonstrating superior computational efficiency and memory usage for high-resolution images compared to MHSA and hybrid CNN+MHSA approaches.

## Method Summary
The Dynamic Filter module operates by first computing the mean of the input feature map across spatial dimensions, then passing this through an MLP to generate weighting coefficients. These coefficients are used to combine a set of basis filters in the frequency domain, where the input undergoes FFT transformation. The weighted filters are applied through element-wise multiplication, followed by inverse FFT to return to the spatial domain. This mechanism enables dynamic, content-adaptive global mixing of tokens while maintaining computational efficiency through FFT operations. The approach can be integrated into standard MetaFormer architectures and shows particular advantages for high-resolution image processing.

## Key Results
- CDFFormer-B36 achieves 85.0% top-1 accuracy on ImageNet-1K, setting a new state-of-the-art among non-MHSA vision models
- DFFormer and CDFFormer demonstrate higher throughput and memory efficiency than MHSA-based and hybrid CNN+MHSA architectures for high-resolution image recognition
- The models show strong performance in downstream tasks including object detection and semantic segmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic Filter improves accuracy by generating data-dependent Fourier filters for each channel pair
- Mechanism: Uses MLP-based weighting to combine basis filters dynamically based on input content
- Core assumption: Learning coefficients for basis filters is easier than learning full filters and captures input-specific frequency patterns better
- Evidence anchors: [abstract], [section 3.2]
- Break condition: MLP fails to generate meaningful coefficients or basis set is too small

### Mechanism 2
- Claim: Dynamic Filter reduces computational cost compared to MHSA for high-resolution images
- Mechanism: FFT-based operations (O(HWC log(HW))) have lower complexity than MHSA's O(HW²C + (HW)²C)
- Core assumption: FFT complexity advantage holds in practice without dynamic step becoming bottleneck
- Evidence anchors: [abstract], [section 3.1]
- Break condition: FFT implementation overhead or dynamic weighting becomes performance bottleneck

### Mechanism 3
- Claim: Dynamic Filter enables better shape bias compared to MHSA-based models
- Mechanism: FFT-based mixers capture global frequency information aligning with human shape perception
- Core assumption: Human vision relies more on low-frequency shape cues which FFTs naturally capture
- Evidence anchors: [section 5.4], [abstract]
- Break condition: Shape bias doesn't translate to better downstream performance or texture information becomes critical

## Foundational Learning

- Concept: Fast Fourier Transform (FFT) and its computational advantage over direct convolution
  - Why needed here: Enables efficient global mixing of tokens with lower theoretical complexity than self-attention
  - Quick check question: What is the computational complexity of FFT-based global filtering vs self-attention for an HxW image with C channels?

- Concept: MetaFormer architecture and token-mixer abstraction
  - Why needed here: Dynamic Filter fits within MetaFormer's token-mixer slot, replacing self-attention or MLP-based mixers
  - Quick check question: How does the MetaFormer block structure differ from standard Transformer blocks?

- Concept: Complex-valued operations and Hermitian symmetry in FFT
  - Why needed here: Dynamic Filter uses complex parameters in frequency domain and exploits Hermitian symmetry to reduce computation
  - Quick check question: Why can we store only ⌈W/2⌉ frequency components per row in 2D FFT?

## Architecture Onboarding

- Component map: Input → Channel MLP → Dynamic Filter (FFT → coefficient generation → complex multiplication → IFFT) → Channel MLP → Output
- Critical path:
  1. Compute input mean across spatial dims
  2. Pass through MLP to get weighting coefficients
  3. Apply FFT to input
  4. Combine basis filters with coefficients (element-wise product)
  5. Apply IFFT to get output
- Design tradeoffs:
  - More basis filters (N) → higher accuracy but more parameters
  - Higher expansion ratio in MLP → more expressive coefficients but more compute
  - Static vs dynamic filters → simplicity vs adaptivity
- Failure signatures:
  - Poor accuracy → check if MLP is generating meaningful coefficients
  - Slow throughput → profile FFT and dynamic weighting overhead
  - Memory issues → verify FFT implementation and basis filter storage
- First 3 experiments:
  1. Replace Dynamic Filter with static global filter (GFFormer) to measure impact of adaptivity
  2. Vary N (basis dimension) and ρ (MLP expansion) to find optimal tradeoff
  3. Compare shape vs texture bias on out-of-distribution datasets to validate perceptual alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamic filter's performance scale with increasing model depth and complexity?
- Basis in paper: [explicit] The paper mentions using N = 4 for the dimension of K in the dynamic filter basis to avoid over-computing, but suggests that different values could be explored in the ablation study
- Why unresolved: The paper does not explore the impact of varying the dimension of the dynamic filter basis on model performance or computational efficiency
- What evidence would resolve it: Experiments comparing model performance and efficiency with different values of N in the dynamic filter basis

### Open Question 2
- Question: What is the impact of using different activation functions in the dynamic filter compared to StarReLU?
- Basis in paper: [explicit] The paper mentions that StarReLU is used in the dynamic filter, but experiments with GELU and ReLU show a decrease in performance
- Why unresolved: The paper does not explore other potential activation functions or combinations that could improve performance
- What evidence would resolve it: Experiments comparing the performance of the dynamic filter with various activation functions

### Open Question 3
- Question: How does the dynamic filter's shape bias compare to other token-mixer architectures in more complex tasks?
- Basis in paper: [explicit] The paper mentions that FFT-based token-mixers, like MHSA, are token-mixers in the global domain and can capture low-frequency features, but do not necessarily learn similar representations
- Why unresolved: The paper does not explore the shape bias of the dynamic filter in more complex tasks beyond image classification
- What evidence would resolve it: Experiments comparing the shape bias of the dynamic filter to other token-mixer architectures in tasks such as object detection and semantic segmentation

## Limitations

- Computational complexity claims rely heavily on theoretical FFT advantages without sufficient empirical validation across different hardware platforms
- Shape bias superiority claims are supported by qualitative visualization but lack quantitative metrics on standard shape-texture bias benchmarks
- Performance gains over concurrent methods like RIFormer and ConvMixFormer are presented without direct comparison on the same experimental setup

## Confidence

- **High confidence**: Claims about achieving SOTA performance among non-MHSA vision models on ImageNet-1K (directly verifiable through standard benchmarks)
- **Medium confidence**: Computational efficiency claims (theoretically sound but requires platform-specific validation)
- **Low confidence**: Shape bias superiority claims (based on qualitative observations without standardized metrics)

## Next Checks

1. **Computational profiling**: Measure actual throughput and memory usage of DFFormer vs MHSA-based models on GPUs across different image resolutions to validate theoretical complexity claims
2. **Shape bias quantification**: Evaluate models on established shape-texture bias benchmarks like Geirhos et al.'s Stylized-ImageNet to provide quantitative metrics for the claimed perceptual alignment
3. **Ablation study on adaptivity**: Compare Dynamic Filter against a static global filter variant (GFFormer) with identical architecture but learned fixed coefficients to isolate the benefit of dynamic adaptation