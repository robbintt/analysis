---
ver: rpa2
title: Enhancing Cross-domain Click-Through Rate Prediction via Explicit Feature Augmentation
arxiv_id: '2312.00078'
source_url: https://arxiv.org/abs/2312.00078
tags:
- domain
- target
- feature
- knowledge
- cdanet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a cross-domain augmentation network (CDAnet)
  for click-through rate (CTR) prediction. CDAnet addresses the limitations of existing
  methods that require a super domain and have static user interest.
---

# Enhancing Cross-domain Click-Through Rate Prediction via Explicit Feature Augmentation

## Quick Facts
- **arXiv ID**: 2312.00078
- **Source URL**: https://arxiv.org/abs/2312.00078
- **Reference count**: 40
- **Key outcome**: CDAnet achieved an absolute 0.11 point CTR improvement, a relative 0.64% deal growth, and a relative 1.26% GMV increase in an online A/B test

## Executive Summary
This paper addresses cross-domain click-through rate (CTR) prediction by proposing a novel framework called CDAnet that learns explicit feature translation between domains. Unlike existing methods that require a super domain covering most users and items of the target domain, CDAnet only needs two domains with overlapping feature fields. The framework consists of a translation network that learns latent feature translation between domains using a cross-supervised feature translator, followed by an augmentation network that uses these translated features to boost target domain CTR prediction. Experiments on public benchmarks (Amazon, Taobao) and industrial production data demonstrate significant improvements over state-of-the-art methods.

## Method Summary
CDAnet is a two-stage framework for cross-domain CTR prediction that learns explicit feature translation between domains without requiring a super domain. In the first stage, a translation network is trained using data from both source and target domains, encoding inputs into latent features and learning cross-domain knowledge through a cross-supervised feature translator. The translation network is trained with three objectives: vanilla CTR prediction loss, cross-supervision loss that preserves target domain labels during translation, and an orthogonal constraint loss. In the second stage, an augmentation network reuses the learned embedding layers, feature extractors, and translator from the translation network, concatenating original and translated latent features for final CTR prediction in the target domain.

## Key Results
- CDAnet outperformed state-of-the-art cross-domain CTR methods (MLP, STAR, DDTCDR, MMOE, PLE, KEEP) on Amazon and Taobao datasets
- The model achieved an absolute 0.11 point CTR improvement in online A/B testing
- CDAnet showed a relative 0.64% deal growth and 1.26% GMV increase in production deployment

## Why This Works (Mechanism)

### Mechanism 1
The translation network learns meaningful cross-domain feature translation via cross-supervised feature translator. It encodes inputs from both domains into latent features, then uses a cross-supervised translator to map target domain latent features into source domain latent space while preserving target domain labels. This preserves semantic meaning across domains. The core assumption is that if a user favors an item in the target domain, that preference behavior is preserved when the user and item are mapped into source domain features.

### Mechanism 2
The augmentation network improves target domain CTR prediction by combining original and translated latent features. After pre-training the translation network, the augmentation network reuses the learned embedding layer, feature extractor, and translator parameters. It concatenates original target domain latent features with their translated versions as augmented features for final CTR prediction. The core assumption is that translated features contain complementary information that, when combined with original features, provides better signal for CTR prediction.

### Mechanism 3
CDAnet avoids super-domain requirement and enables context-aware user interest transfer. Unlike methods requiring a super domain that covers most users/items of target domain, CDAnet only requires two domains with overlapping feature fields. The translation happens in latent space using shared embeddings for overlapping features. The core assumption is that meaningful knowledge transfer can occur between two domains without requiring one to be a super domain containing most of the other's users/items.

## Foundational Learning

- **Cross-domain CTR prediction fundamentals**: Understanding how to leverage data from one domain to improve CTR prediction in another domain is the core problem CDAnet addresses. *Quick check*: What are the main challenges in cross-domain CTR prediction that make it different from single-domain CTR?
- **Feature translation in latent space**: CDAnet's key innovation is translating features between domains in latent space using a cross-supervised translator. *Quick check*: How does latent space translation differ from directly translating raw features between domains?
- **Knowledge transfer via parameter sharing vs explicit augmentation**: CDAnet explicitly transfers knowledge through feature translation and augmentation, rather than implicit parameter sharing used in most existing methods. *Quick check*: What are the advantages and disadvantages of explicit knowledge transfer compared to implicit parameter sharing?

## Architecture Onboarding

- **Component map**: Embedding layer → Feature extractor (MLP or MMOE) → Cross-supervised translator → Prediction tower (source/target)
- **Critical path**: Translation network training → Parameter transfer to augmentation network → Fine-tuning with augmented features
- **Design tradeoffs**: Translation network adds complexity but provides explicit knowledge transfer; parameter sharing between domains reduces model size but may limit flexibility
- **Failure signatures**: Poor translation quality (measured by correlation between original and translated features), degradation in target domain performance, mode collapse in translation
- **First 3 experiments**:
  1. Train translation network only on synthetic data where ground truth translation is known
  2. Compare target domain performance with and without augmentation using pre-trained translation network
  3. Analyze correlation between original and translated features to verify translation quality

## Open Questions the Paper Calls Out
- **Open Question 1**: How does CDAnet perform in scenarios with highly sparse data, such as new users or items with limited interaction history? The paper mentions that CDAnet can handle sparse data better than existing methods, but does not provide specific results for extremely sparse scenarios.
- **Open Question 2**: How does the performance of CDAnet compare to other state-of-the-art cross-domain CTR methods when dealing with cold-start problems? The paper mentions that CDAnet can handle sparse data better than existing methods, which implies it may perform well in cold-start scenarios. However, no direct comparison is made.
- **Open Question 3**: What is the impact of the choice of feature extractor on CDAnet's performance in different domains? The paper mentions that CDAnet is flexible in choosing feature extractors and can be combined with existing implicit knowledge transfer techniques. However, it does not provide a detailed analysis of how different feature extractors affect performance in various domains.

## Limitations
- The exact architectural details of the cross-supervised feature translator remain unspecified in the paper, making exact replication challenging
- No ablation studies are provided to isolate the contribution of individual components (translation vs augmentation)
- The orthogonal constraint's impact on performance is not empirically validated

## Confidence
- **High confidence** in the translation mechanism's theoretical soundness and the sequential training procedure
- **Medium confidence** in the empirical improvements, as results are from controlled experiments but lack extensive ablation analysis
- **Low confidence** in the scalability claims, as industrial dataset details and training times are not disclosed

## Next Checks
1. Implement an ablation study to measure the standalone contribution of the translation network and the augmentation network separately
2. Test the model on a third-party dataset with known domain overlap to verify generalizability beyond the Amazon-Taobao pair
3. Measure inference latency and memory usage to quantify the practical overhead of the translation network compared to standard cross-domain methods