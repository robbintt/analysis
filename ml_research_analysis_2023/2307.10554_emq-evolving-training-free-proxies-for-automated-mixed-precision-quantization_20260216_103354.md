---
ver: rpa2
title: 'EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization'
arxiv_id: '2307.10554'
source_url: https://arxiv.org/abs/2307.10554
tags:
- search
- quantization
- proxies
- which
- scalar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient mixed-precision
  quantization in deep neural networks, where different bit-widths are assigned to
  different layers. The authors observe that existing training-free proxies for this
  task have weak correlations with quantization accuracy and require expert knowledge
  to design.
---

# EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization

## Quick Facts
- arXiv ID: 2307.10554
- Source URL: https://arxiv.org/abs/2307.10554
- Reference count: 40
- Key outcome: Proposed EMQ framework automatically discovers superior training-free proxies for mixed-precision quantization, outperforming handcrafted proxies on ImageNet with various ResNet and MobileNet families while significantly reducing search costs.

## Executive Summary
This paper addresses the challenge of efficient mixed-precision quantization in deep neural networks by proposing EMQ, an automated search framework that discovers superior training-free proxies. The authors observe that existing handcrafted proxies have weak correlations with quantization accuracy and require expert knowledge to design. EMQ uses an evolutionary algorithm with a branched computation graph search space, employing operation sampling prioritization and compatibility screening protocol to efficiently explore the proxy design space. The searched proxy demonstrates superior performance compared to existing handcrafted proxies across various neural network architectures.

## Method Summary
EMQ uses an evolutionary algorithm to search for optimal mixed-precision quantization proxies in a branched computation graph search space. The framework employs operation sampling prioritization to reduce invalid proxy generation and a compatibility screening protocol to filter out redundant candidates. The fitness function uses Spearman@topk to focus on correlation with top-performing bit configurations rather than overall ranking consistency. The evolutionary process iterates through population initialization, fitness evaluation, selection with diversity-prompting, and offspring generation via crossover and mutation until convergence.

## Key Results
- Searched EMQ proxy outperforms existing handcrafted proxies on ImageNet across ResNet and MobileNet families
- Spearman@topk provides better correlation with top-performing bit configurations than overall rank correlation
- Operation Sampling Prioritization and Compatibility Screening Protocol significantly improve search efficiency by reducing invalid candidate generation

## Why This Works (Mechanism)

### Mechanism 1
EMQ's evolutionary algorithm discovers superior training-free proxies by exploring a large search space of primitive operations and network statistics. The framework employs diversity-prompting selection and compatibility screening to efficiently navigate the proxy design space, finding proxies that better correlate with quantization accuracy than handcrafted alternatives.

### Mechanism 2
Spearman@topk is more effective than overall rank correlation because it focuses on the ranking consistency of top-performing bit-width configurations. This aligns with the practical goal of finding optimal bit configurations for mixed-precision quantization rather than achieving overall ranking consistency.

### Mechanism 3
Operation Sampling Prioritization and Compatibility Screening Protocol improve search efficiency by reducing the generation of invalid and redundant proxies. OSP assigns different probabilities to operations based on their likelihood to cause shape-mismatch problems, while CSP filters out invalid proxies early using equivalent checking and early rejection strategies.

## Foundational Learning

- **Evolutionary algorithms and hyperparameter optimization**: Understanding how evolutionary algorithms work and their application to proxy design is crucial for grasping EMQ's search methodology.
  - Quick check: How does EMQ's diversity-prompting selection strategy differ from standard tournament selection in evolutionary algorithms?

- **Mixed-precision quantization and its challenges**: Understanding the basics of mixed-precision quantization and why training-free proxies are valuable for this task is essential.
  - Quick check: Why is mixed-precision quantization more challenging than uniform-precision quantization, and how do training-free proxies help address this challenge?

- **Spearman rank correlation and its variants**: Understanding Spearman rank correlation and why focusing on top-performing configurations is beneficial for quantization proxy evaluation.
  - Quick check: How does Spearman@topk differ from standard Spearman rank correlation, and why is this difference important for mixed-precision quantization?

## Architecture Onboarding

- **Component map**: Search space definition -> Evolutionary algorithm with OSP and CSP -> Evaluation on MQ-Bench-101
- **Critical path**: Define search space → Initialize population → Evaluate fitness with Spearman@topk → Apply OSP and CSP → Generate new population → Repeat until convergence
- **Design tradeoffs**: Tradeoff between search space expressiveness (DAG-based structure) and validity rate (sequential structure), with branched structure as a compromise
- **Failure signatures**: Premature convergence (population diversity loss), invalid proxy generation (OSP/CSP inefficiency), poor correlation with quantization accuracy (inappropriate fitness function)
- **First 3 experiments**:
  1. Replicate baseline experiments comparing EMQ to existing handcrafted proxies on MQ-Bench-101
  2. Perform ablation study on OSP and CSP components to quantify their impact on search efficiency
  3. Evaluate EMQ's performance on different neural network architectures (e.g., ResNet, MobileNet) to assess generalizability

## Open Questions the Paper Calls Out

- What are the theoretical bounds on the search space sparsity for EMQ's computation graph structures?
- How does the performance of EMQ proxies generalize across different neural network architectures beyond ResNet and MobileNet families?
- What is the impact of different Hessian approximation methods on EMQ's search efficiency and final proxy quality?

## Limitations
- Search space design may not capture all potentially useful proxy operations
- Evolutionary algorithm effectiveness depends heavily on proper hyperparameter tuning
- Evaluation focuses primarily on image classification models, leaving questions about generalizability to other domains

## Confidence
- Mechanism 1 (Evolutionary search superiority): High - The evolutionary framework is well-defined and comparisons with handcrafted proxies are provided
- Mechanism 2 (Spearman@topk effectiveness): Medium - The rationale is sound, but ablation studies on alternative fitness functions are limited
- Mechanism 3 (OSP/CSP efficiency): Low - While described conceptually, the quantitative impact of these components on search efficiency is not fully demonstrated

## Next Checks
1. Conduct ablation studies to isolate the contribution of OSP and CSP components by comparing search efficiency with and without these protocols
2. Test the searched proxy's performance on non-vision architectures (e.g., BERT, LSTM) to assess cross-domain generalizability
3. Implement and evaluate alternative fitness functions (e.g., different k values for Spearman@topk, Kendall tau) to verify the optimality of the chosen metric