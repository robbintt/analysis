---
ver: rpa2
title: Learning to Retrieve In-Context Examples for Large Language Models
arxiv_id: '2307.07164'
source_url: https://arxiv.org/abs/2307.07164
tags:
- examples
- in-context
- llm-r
- training
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-R, a novel iterative training framework
  that retrieves high-quality in-context examples for large language models. The approach
  uses LLM feedback to rank candidates and train a reward model, followed by knowledge
  distillation to learn a dense retriever.
---

# Learning to Retrieve In-Context Examples for Large Language Models

## Quick Facts
- **arXiv ID:** 2307.07164
- **Source URL:** https://arxiv.org/abs/2307.07164
- **Reference count:** 40
- **Primary result:** 7.8% average improvement over random selection on 30 NLP tasks

## Executive Summary
This paper introduces LLM-R, an iterative training framework that retrieves high-quality in-context examples for large language models. The approach uses LLM feedback to rank candidate examples and train a reward model, followed by knowledge distillation to learn a dense retriever. Experiments demonstrate that LLM-R improves in-context learning performance by 7.8% on average compared to random selection, with consistent gains across different LLMs and tasks.

## Method Summary
LLM-R follows an iterative training pipeline: initial candidates are retrieved using BM25, then ranked by an LLM to generate training data for a cross-encoder reward model. This reward model is then distilled into a bi-encoder dense retriever using KL divergence and contrastive losses. The process iterates 2-3 times, with each iteration improving retrieval quality by mining better positive and hard negative examples. The framework leverages ground-truth labels for reward model training and produces an efficient retriever for inference.

## Key Results
- 7.8% average performance improvement across 30 diverse NLP tasks
- Consistent gains across different LLMs and task categories
- Generalizes well to unseen tasks, particularly those with similar patterns to training tasks
- Converges after 2-3 iterations with diminishing returns thereafter

## Why This Works (Mechanism)

### Mechanism 1
The reward model captures fine-grained ranking signals from LLMs by using ground-truth labels as context. It takes concatenated (x, y, x+, y+) as input and produces scores that approximate the LLM's preference for positive examples over hard negatives, trained with cross-entropy loss.

### Mechanism 2
Knowledge distillation from the reward model to the bi-encoder retriever enables efficient inference while preserving ranking quality. The dense retriever minimizes KL divergence between its cosine similarity distribution and the reward model's score distribution, plus an InfoNCE contrastive loss for in-batch negatives.

### Mechanism 3
Iterative training improves retriever quality by mining better positive and hard negative examples. Each iteration uses the current retriever to retrieve candidates, which are then ranked by the LLM to generate training data for the next iteration's reward model and retriever.

## Foundational Learning

- **In-context learning and few-shot prompting**: Why needed - the entire framework improves examples used in in-context learning. Quick check - What is the difference between in-context learning and traditional fine-tuning?

- **Dense retrieval and bi-encoder architectures**: Why needed - the framework trains a bi-encoder dense retriever for efficient inference. Quick check - How does a bi-encoder architecture differ from a cross-encoder in terms of computational efficiency?

- **Knowledge distillation and teacher-student learning**: Why needed - the retriever learns from the reward model through distillation. Quick check - What is the purpose of using KL divergence in knowledge distillation?

## Architecture Onboarding

- **Component map**: BM25 retrieval → LLM ranking → Reward model training → Dense retriever training → Evaluation
- **Critical path**: Training pipeline is critical - BM25 → LLM ranking → Reward model → Dense retriever → Evaluation
- **Design tradeoffs**: Cross-encoder vs bi-encoder (accuracy vs inference speed), number of iterations (diminishing returns vs training cost), reward model supervision (labels available vs inference-only)
- **Failure signatures**: Performance worse than random (retriever learning failure), no improvement across iterations (convergence or poor mining), good training but poor generalization (overfitting)
- **First 3 experiments**: 1) Compare BM25 baseline vs random selection on held-out tasks, 2) Test single iteration vs multiple iterations to identify convergence, 3) Evaluate different retriever initializations (E5 vs BERT) to assess initialization sensitivity

## Open Questions the Paper Calls Out

- **Optimal iteration count**: The paper only evaluates up to 3 iterations, but it's unclear if more iterations would provide additional benefits or if there's a point of diminishing returns.

- **Non-overlapping task categories**: The current evaluation includes training datasets that share the same task category as held-out tasks, leaving open how well the model generalizes to completely different categories.

- **Impact of different LLM combinations**: The paper only explores limited combinations of LLMs for ranking and evaluation, raising questions about how different pairings might affect performance.

## Limitations

- Relies on access to LLM feedback and ground-truth labels, which may not be available in real-world deployment scenarios
- Shows diminishing returns after 2-3 iterations, suggesting potential convergence limitations
- Inherits computational costs of LLM inference for feedback generation, though mitigated by efficient bi-encoder retrieval at inference

## Confidence

**High Confidence**: Iterative framework structure is well-specified and validated; knowledge distillation approach is theoretically sound with measurable improvements; 7.8% average improvement is well-supported.

**Medium Confidence**: Generalization to unseen tasks based on relatively small number of task categories; architectural choices show reasonable performance differences but may be sensitive to implementation details; diminishing returns after 2-3 iterations are observed but convergence dynamics may vary.

**Low Confidence**: Specific mechanism of how reward model captures LLM ranking signals not directly validated through ablation studies; assumption that pattern similarity drives effectiveness not rigorously tested; scalability to different domains remains untested.

## Next Checks

1. **Ablation Study on Training Data Sources**: Systematically test the framework using only LLM feedback without ground-truth labels, and vice versa, to quantify each component's contribution to final performance.

2. **Cross-Domain Generalization Test**: Apply LLM-R to a completely different domain (e.g., code generation or mathematical reasoning) to validate whether the approach generalizes beyond the 9 NLP task categories used in the original experiments.

3. **Long-Tail Performance Analysis**: Analyze performance on tasks with fewer training examples to determine if the framework's improvements are consistent across the full distribution of task sizes, particularly for low-resource scenarios.