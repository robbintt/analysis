---
ver: rpa2
title: A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using
  Causal Mediation Analysis
arxiv_id: '2305.15054'
source_url: https://arxiv.org/abs/2305.15054
tags:
- effect
- prediction
- layer
- indirect
- arithmetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses causal mediation analysis to investigate how large
  language models process arithmetic reasoning. By intervening on specific layers'
  activations and measuring the effect on prediction probabilities, the authors identify
  which model components contribute most to arithmetic task performance.
---

# A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis

## Quick Facts
- arXiv ID: 2305.15054
- Source URL: https://arxiv.org/abs/2305.15054
- Reference count: 40
- Key outcome: Mid-to-late layers (particularly layer 20) have largest causal effect on arithmetic predictions

## Executive Summary
This paper investigates how large language models process arithmetic reasoning by using causal mediation analysis to identify which model components contribute most to arithmetic task performance. By intervening on specific layers' activations and measuring effects on prediction probabilities, the authors discover that mid-to-late MLP layers, especially layer 20, are responsible for computing arithmetic results. The study reveals that attention mechanisms in earlier layers route information relevant to arithmetic queries, and these patterns are consistent across different numerical representations (digits vs words), suggesting specialized computational circuits for arithmetic reasoning.

## Method Summary
The authors employ causal mediation analysis to examine how model components contribute to arithmetic predictions. They prepare arithmetic queries with operand pairs and four operators in both digit and word formats, using two-shot prompting with exemplars. The intervention procedure involves passing similar queries through the model, storing activation values for specific components, intervening on one query's activations, and measuring probability changes for correct versus incorrect results. They calculate indirect effects for MLP layers and attention mechanisms to identify which layers have the highest causal influence on arithmetic predictions.

## Key Results
- Mid-to-late MLP layers (particularly layer 20) have the largest causal effect on arithmetic predictions
- Attention mechanisms in early-to-mid layers route information relevant to arithmetic queries
- Patterns are consistent across different numerical representations (digits vs words)
- These patterns differ from those for factual knowledge prediction, suggesting arithmetic uses specialized computational circuits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mid-to-late MLP layers (particularly layer 20) are responsible for computing arithmetic results
- Mechanism: Information relevant to arithmetic queries is routed through attention mechanisms in earlier layers to the final token, then processed by specific MLP modules in layer 20 that generate result-related information incorporated into the residual stream
- Core assumption: The spike in indirect effect at layer 20 indicates these layers contain specialized computational circuits for arithmetic
- Evidence anchors:
  - [abstract] "Results show that mid-to-late layers (particularly layer 20) have the largest causal effect on arithmetic predictions"
  - [section 4.1] "we notice that the indirect effect of the MLPs spikes at earlier layers" when measuring undesired effects
- Break condition: If intervention on layer 20 doesn't significantly change probability mass between correct/incorrect results

### Mechanism 2
- Claim: Attention mechanisms in early-to-mid layers route information relevant to arithmetic queries
- Mechanism: The attention mechanism moves and copies information within the Transformer, with early-to-mid layers showing substantial influence on predictions through this routing function
- Core assumption: Attention layers are responsible for information movement rather than computation
- Evidence anchors:
  - [section 4.3] "we find that the early-to-mid layers exhibit a more substantial influence on the model's prediction" for attention effects
  - [section 4.3] "these findings align with the existing theory that attributes to the attention mechanism the responsibility of moving and copying information"
- Break condition: If attention interventions don't show information routing patterns

### Mechanism 3
- Claim: Different numerical representations (digits vs words) activate the same computational circuits
- Mechanism: The specialized MLP circuits for arithmetic are representation-agnostic, producing similar activation patterns regardless of whether numbers are represented as Arabic digits or numeral words
- Core assumption: The model implements abstract numerical concepts rather than surface-level token patterns
- Evidence anchors:
  - [abstract] "These patterns are consistent across different numerical representations (digits vs words)"
  - [section 4.2] "we observe a behavior that exhibits similarities to the experiments conducted with the Arabic representation" when using numeral words
- Break condition: If interventions on numeral words produce drastically different layer effects than digit representations

## Foundational Learning

- Concept: Causal mediation analysis framework
  - Why needed here: To identify which model components causally contribute to arithmetic predictions by intervening on activations and measuring effects
  - Quick check question: How does causal mediation analysis differ from correlation-based interpretability methods?

- Concept: Transformer architecture and residual connections
  - Why needed here: Understanding how information flows through layers via residual connections is crucial for interpreting intervention effects
  - Quick check question: What is the role of residual connections in allowing information to bypass certain layers?

- Concept: Attention mechanism and information routing
  - Why needed here: The study shows attention is responsible for moving information relevant to queries through the model
  - Quick check question: How does multi-head attention enable selective information routing in Transformers?

## Architecture Onboarding

- Component map: Input tokens → Early attention layers (1-10) → Mid attention layers (11-20) → Layer 20 MLP (primary computation) → Late layers (21-30) → Output
- Critical path: Input → Early attention layers (1-10) → Mid attention layers (11-20) → Layer 20 MLP (primary computation) → Late layers (21-30) → Output
- Design tradeoffs: Parallel placement of attention and MLP allows for efficient information routing and computation, but makes it harder to isolate which component is responsible for specific behaviors. The 4096-dimensional hidden state provides representational capacity but requires careful intervention design.
- Failure signatures: If interventions on layer 20 don't produce the expected indirect effects, this could indicate either that arithmetic computation happens elsewhere or that the intervention methodology is flawed. If attention interventions don't show routing patterns, this might suggest information flow happens through other mechanisms.
- First 3 experiments:
  1. Replicate the layer 20 MLP intervention with different operand ranges to verify consistency
  2. Test attention interventions at layer 1 vs layer 15 to confirm routing hypothesis
  3. Compare indirect effects when using single-digit vs multi-digit numbers to test representation invariance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the MLP layers identified as important for arithmetic reasoning (layers 20-21) encode specific arithmetic operations or are they general numerical processing units?
- Basis in paper: [explicit] The paper shows layer 20-21 have the largest causal effect on arithmetic predictions across all four operators (+, -, ×, ÷)
- Why unresolved: The paper identifies these layers as important but doesn't determine whether they encode specific operations or general numerical processing
- What evidence would resolve it: Neuron-level intervention experiments showing whether the same neurons are activated across different operations or if there are operation-specific subsets

### Open Question 2
- Question: How does the model's performance on arithmetic tasks change when the identified important layers (20-21) are ablated or modified?
- Basis in paper: [inferred] The paper identifies these layers as having the largest effect but doesn't test the impact of their removal
- Why unresolved: The causal mediation analysis shows these layers are important but doesn't establish necessity
- What evidence would resolve it: Experiments showing performance degradation when layers 20-21 are removed or significantly altered

### Open Question 3
- Question: What specific numerical representations or intermediate calculations are being processed in the identified MLP layers (20-21)?
- Basis in paper: [inferred] The paper shows these layers process arithmetic information but doesn't detail what intermediate representations are used
- Why unresolved: The analysis shows these layers affect arithmetic predictions but doesn't reveal the internal processing
- What evidence would resolve it: Analysis of activation patterns showing specific numerical representations or intermediate calculation steps within these layers

## Limitations

- The study relies on indirect effect measurements that could be influenced by architectural artifacts rather than true computational specialization
- The distinction between desired and undesired effects relies on arbitrary thresholds for what constitutes a "correct" prediction
- The analysis doesn't address potential confounding factors like the model's pretraining data distribution affecting arithmetic capabilities

## Confidence

- **High confidence**: The identification of mid-to-late layers having larger causal effects on arithmetic predictions
- **Medium confidence**: The specific claim about layer 20 being the primary computation site
- **Low confidence**: The assertion that arithmetic uses fundamentally different circuits than factual knowledge prediction

## Next Checks

1. **Layer ablation validation**: Remove or disable layer 20 entirely and measure the impact on arithmetic accuracy to confirm it's essential rather than just influential.

2. **Cross-architecture replication**: Apply the same causal mediation analysis to smaller models (1B-3B parameters) to determine if the layer 20 effect scales with model size or represents a fundamental architectural feature.

3. **Temporal intervention analysis**: Track how intervention effects propagate forward through subsequent layers to better understand whether layer 20 performs computation or merely amplifies existing signals.