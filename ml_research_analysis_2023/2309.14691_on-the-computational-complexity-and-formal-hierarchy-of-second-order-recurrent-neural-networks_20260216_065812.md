---
ver: rpa2
title: On the Computational Complexity and Formal Hierarchy of Second Order Recurrent
  Neural Networks
arxiv_id: '2309.14691'
source_url: https://arxiv.org/abs/2309.14691
tags:
- trnn
- neural
- state
- turing
- rnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proves that a second-order recurrent neural network (TRNN)
  with just 11 unbounded precision neurons is Turing complete and can simulate any
  Turing machine in real-time (O(T)), which is the first memory-less model to achieve
  this. It also shows that a bounded precision TRNN requires only n+1 neurons to simulate
  any deterministic finite automaton (DFA), where n is the number of DFA states.
---

# On the Computational Complexity and Formal Hierarchy of Second Order Recurrent Neural Networks

## Quick Facts
- arXiv ID: 2309.14691
- Source URL: https://arxiv.org/abs/2309.14691
- Reference count: 25
- Key outcome: Proves TRNNs with 11 neurons are Turing complete and can simulate any Turing machine in real-time (O(T)), first memory-less model to achieve this

## Executive Summary
This work establishes theoretical foundations for second-order recurrent neural networks (TRNNs) by proving their computational power across different precision regimes. The authors demonstrate that unbounded precision TRNNs can simulate any Turing machine in real-time using only 11 neurons, while bounded precision versions can simulate any deterministic finite automaton with just n+1 neurons. Experimental results on Tomita grammars show TRNNs outperform LSTMs and transformers in recognizing regular grammars, especially on longer strings, using fewer parameters. The findings bridge neural network theory with formal language theory and provide insights for explainable AI research.

## Method Summary
The research employs theoretical proofs and empirical experiments to analyze TRNN computational power. The theoretical analysis establishes bounds on neuron requirements for different computational classes (Turing machines, DFAs) under unbounded and bounded precision regimes. Experiments use 7 Tomita grammar datasets with strings up to length 50 for training/validation and 1000 samples up to length 60 and 120 for testing. The study compares TRNNs against LSTM and transformer baselines using grid search for hyperparameters, Xavier initialization for baselines, Gaussian initialization for TRNN, with 15 epochs maximum and early stopping after 5 consecutive perfect validation accuracy epochs.

## Key Results
- TRNNs with 11 unbounded precision neurons can simulate any Turing machine in real-time (O(T))
- Bounded precision TRNNs require only n+1 neurons to simulate any DFA, where n is the number of DFA states
- TRNNs outperform LSTMs and transformers on Tomita grammars, especially for longer strings, using fewer parameters
- TRNNs achieve higher success rates in extracting stable automata compared to first-order RNNs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A second-order RNN (TRNN) with only 11 unbounded precision neurons can simulate any Turing machine in real-time (O(T)).
- Mechanism: The TRNN encodes the transition table of the Turing machine directly into its recurrent weights, using tensor (second-order) synaptic connections. These tensor connections allow each neuron's update to depend on products of two other neuron states, enabling richer state transitions that can simulate the control head and tape operations simultaneously. The proof constructs a specific mapping between Turing machine configurations and the TRNN's neural activities, showing that the required number of neurons is m + n + 1, where m is the number of tape symbols and n is the number of states. For a universal Turing machine with 6 states and 4 symbols, this yields 11 neurons.
- Core assumption: The TRNN uses unbounded precision weights and neurons, and the activation function can be made arbitrarily close to binary (0 or 1) with appropriate scaling.
- Evidence anchors:
  - [abstract] "This work proves that a second-order recurrent neural network (TRNN) with just 11 unbounded precision neurons is Turing complete and can simulate any Turing machine in real-time (O(T))"
  - [section] "There exists a k-neuron unbounded precision TRNN with locally connected second-order connections, where K = m + 2n + 1 can simulate any TM in O(T 2) turns or cycles or cycles" and "Given a Turing Machine M, with m symbols and n states, there exists a k-neuron unbounded precision TRNN with two locally connected second-order connections, where K = m + n + 1 can simulate any TM in real-time or O(T 1) cycles."
- Break condition: If the precision of weights or neurons is bounded, or if the activation function cannot approximate binary values closely enough, the TRNN may no longer be Turing complete or may require more neurons.

### Mechanism 2
- Claim: A bounded precision TRNN requires only n + 1 neurons to simulate any deterministic finite automaton (DFA), where n is the number of DFA states.
- Mechanism: The TRNN uses sigmoid activation functions that converge to stable fixed points (0 or 1). The network's state at each time step can be made to match the state vector of the DFA, with the transition table encoded in the weights. The proof shows that the TRNN's dynamics, when using sigmoid activations, are equivalent to the vectorized dynamic version of a DFA, and the fixed point analysis ensures stability.
- Core assumption: The TRNN's weights are initialized from an arbitrary distribution and the sigmoid activation function ensures convergence to stable fixed points.
- Evidence anchors:
  - [abstract] "It also shows that a bounded precision TRNN requires only n+1 neurons to simulate any deterministic finite automaton (DFA), where n is the of DFA states."
  - [section] "Given a DFA M with n states and m input symbols, there exists a k-neuron bounded precision TRNN with sigmoid activation function (hH), where k = n + 1, initialized from an arbitrary distribution, that can simulate any DFA in real-time O(T)."
- Break condition: If the weights are not initialized properly, or if the activation function does not converge to stable fixed points, the TRNN may not correctly simulate the DFA.

### Mechanism 3
- Claim: TRNNs outperform LSTMs and transformers in recognizing regular grammars, especially on longer strings, using fewer parameters.
- Mechanism: TRNNs use tensor connections that allow them to capture higher-order interactions between states more efficiently than first-order connections used in LSTMs. This enables TRNNs to learn the underlying state machine more effectively, leading to better generalization on longer strings and more stable automata extraction. The experiments on Tomita grammars demonstrate that TRNNs achieve higher accuracy on longer test strings compared to LSTMs and transformers, even with fewer parameters.
- Core assumption: The tensor connections in TRNNs provide a more efficient representation of the state machine than first-order connections.
- Evidence anchors:
  - [abstract] "Experiments on Tomita grammars demonstrate that TRNNs outperform LSTMs and transformers in recognizing regular grammars, especially on longer strings, using fewer parameters."
  - [section] "In contrast, the TRNN requires only (n + 1) neurons to recognize any DFA. This is true even with weight values that are initialized using a Gaussian distribution."
- Break condition: If the tensor connections are not properly initialized or if the training process does not converge, the TRNN may not outperform LSTMs or transformers.

## Foundational Learning

- Concept: Turing completeness and its implications for neural networks
  - Why needed here: Understanding Turing completeness is crucial for grasping the significance of the TRNN's ability to simulate any Turing machine, as it establishes the model's computational power.
  - Quick check question: What is the significance of a neural network being Turing complete, and how does it relate to the TRNN's ability to simulate Turing machines?

- Concept: Formal language theory and the Chomsky hierarchy
  - Why needed here: The paper's analysis involves comparing the computational power of TRNNs with different classes of formal languages (regular, context-free, unrestricted), which are organized in the Chomsky hierarchy. Understanding this hierarchy is essential for interpreting the theoretical results.
  - Quick check question: How does the Chomsky hierarchy classify different classes of formal languages, and what is the relationship between Turing machines and Type-0 (unrestricted) grammars?

- Concept: Finite automata and state machines
  - Why needed here: The paper proves that bounded precision TRNNs can simulate deterministic finite automata (DFAs), which are fundamental models of computation for regular languages. Understanding DFAs and their properties is crucial for interpreting the bounded precision results.
  - Quick check question: What is a deterministic finite automaton (DFA), and how does it relate to regular grammars and the computational power of bounded precision TRNNs?

## Architecture Onboarding

- Component map: Input layer -> Hidden layer (n neurons with tensor connections) -> Output layer -> Activation function (saturated-linear/sigmoid) -> Weight matrices (encoded transition rules)

- Critical path:
  1. Initialize the TRNN with n neurons and randomly initialized tensor weights
  2. For each time step t:
     a. Receive input symbol x_t
     b. Update each neuron i according to: z_t+1_i = hH(W_t_ijk * z_t_j * x_t_k + b_i)
     c. Apply the activation function hH to ensure bounded outputs
  3. Repeat until a fixed point (halting state) is reached or the input sequence is processed

- Design tradeoffs:
  - Unbounded vs. bounded precision: Unbounded precision allows Turing completeness but may be impractical; bounded precision limits computational power but is more realistic
  - Number of neurons: Fewer neurons are required for TRNNs compared to first-order RNNs, but tensor connections are computationally more expensive
  - Activation function: Saturated-linear functions are simpler but sigmoid functions provide smoother gradients for training

- Failure signatures:
  - If the network does not converge to a fixed point, the weights or activation function may not be properly initialized
  - If the network's output does not match the expected DFA or Turing machine behavior, the weights may not be correctly encoding the transition rules
  - If the network's performance degrades on longer strings, it may be overfitting to the training data or struggling to learn the underlying state machine

- First 3 experiments:
  1. Implement a TRNN with 11 neurons and unbounded precision weights, and verify its ability to simulate a simple Turing machine (e.g., a binary counter)
  2. Implement a TRNN with n+1 neurons and bounded precision weights, and verify its ability to simulate a DFA for a simple regular language (e.g., strings with an even number of 0s)
  3. Train a TRNN on the Tomita grammar datasets and compare its performance with LSTMs and transformers on both short and long strings, analyzing the number of parameters required for each model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can TRNNs be scaled to handle large-scale natural language processing tasks while maintaining their computational efficiency and interpretability?
- Basis in paper: [explicit] The paper mentions limitations regarding scalability and computational expense due to tensor connections, suggesting the need for approximation approaches.
- Why unresolved: The paper does not explore methods for scaling TRNNs to larger datasets or more complex tasks.
- What evidence would resolve it: Experimental results demonstrating TRNN performance on large-scale NLP benchmarks while maintaining efficiency and interpretability.

### Open Question 2
- Question: How can the biases in the training data be mitigated to ensure ethical and unbiased outcomes when using TRNNs in sensitive domains like healthcare and law?
- Basis in paper: [explicit] The paper discusses the potential impact of TRNNs in domains where transparency and trustworthiness are crucial but acknowledges the risk of biases in training data.
- Why unresolved: The paper does not provide solutions or frameworks for addressing data biases in TRNN applications.
- What evidence would resolve it: Studies showing the effectiveness of bias mitigation techniques applied to TRNNs in sensitive applications.

### Open Question 3
- Question: What are the specific approximation approaches that can be developed to reduce the computational cost of tensor connections in TRNNs without compromising their generalization ability?
- Basis in paper: [explicit] The paper highlights the computational expense of tensor connections and suggests exploring approximation approaches as a future direction.
- Why unresolved: The paper does not propose or evaluate any specific approximation methods for tensor connections.
- What evidence would resolve it: Comparative analysis of different approximation techniques applied to TRNNs, showing trade-offs between computational cost and model performance.

## Limitations

- The Turing completeness proof relies on physically unrealizable unbounded precision weights and binary-valued neurons
- The bounded precision results assume ideal convergence to fixed points that may not occur in practice with gradient descent training
- Experimental results are limited to synthetic datasets, with unclear generalization to real-world applications

## Confidence

**High Confidence**: The theoretical bounds for DFA simulation (n+1 neurons requirement) appear well-founded given the proof structure and the properties of sigmoid activations. The experimental setup and methodology for comparing TRNNs with other architectures on Tomita grammars is clearly specified.

**Medium Confidence**: The Turing completeness proof for unbounded precision TRNNs is mathematically rigorous but relies on assumptions about weight precision and neuron behavior that cannot be realized in practice. The claim about real-time simulation (O(T)) is technically correct but the constant factors and practical implementation details are not addressed.

**Low Confidence**: The practical significance of these theoretical results for real-world applications is unclear. The experiments demonstrate advantages on synthetic datasets, but the generalization to more complex problems remains unproven. The automata extraction results, while promising, lack detailed analysis of extraction quality metrics.

## Next Checks

1. **Implementation Verification**: Implement the 11-neuron unbounded precision TRNN and verify its ability to simulate a simple Turing machine (e.g., a binary counter) through exact state tracking, comparing theoretical predictions with actual network behavior.

2. **Training Dynamics Analysis**: Conduct experiments to track the evolution of TRNN weights during training on Tomita grammars, measuring how closely the learned weights approximate the theoretical encoding required for DFA simulation.

3. **Generalization Study**: Test TRNN performance on non-synthetic regular languages from real-world datasets (e.g., simple programming language syntax or DNA sequence patterns) to assess whether the theoretical advantages translate to practical applications.