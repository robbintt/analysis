---
ver: rpa2
title: 'Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users'
arxiv_id: '2310.20479'
source_url: https://arxiv.org/abs/2310.20479
tags:
- chat
- information
- agent
- utterance
- acceptable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Multi-User MultiWOZ, a dataset of task-oriented
  dialogues among two users and one agent, extending the popular MultiWOZ 2.2 dataset.
  The dataset captures the dynamics of collaborative decision-making, including social
  chatter and deliberation.
---

# Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users

## Quick Facts
- arXiv ID: 2310.20479
- Source URL: https://arxiv.org/abs/2310.20479
- Reference count: 22
- Key outcome: Multi-User MultiWOZ dataset extends MultiWOZ 2.2 with 16,706 multi-user chats; rewriting these chats substantially improves dialogue state tracking for single-user systems (+8 points intent, +6.4 points requests).

## Executive Summary
The paper introduces Multi-User MultiWOZ, a dataset of task-oriented dialogues among two users and one agent that captures collaborative decision-making dynamics including social chatter and deliberation. The authors propose a novel task of multi-user contextual query rewriting that converts multi-user chats into concise task-oriented queries matching single-user system formats. They demonstrate that using predicted rewrites as input substantially improves dialogue state tracking accuracy for systems trained on single-user dialogues, without requiring any system modifications. The rewriting approach also generalizes to unseen domains and outperforms training medium-sized models directly on multi-user dialogues.

## Method Summary
The authors create Multi-User MultiWOZ by extending MultiWOZ 2.2 dialogues with two-user chats collected via crowdworkers, then fine-tune sequence-to-sequence models (GPT-2, BART, T5) to rewrite multi-user chats into single concise queries matching original utterances. They validate rewrite quality using ROUGE scores, slot coverage, and hallucination rates, then evaluate whether predicted rewrites improve dialogue state tracking when fed into systems trained on single-user dialogues. The approach treats multi-user chats as input and source utterances as output, learning to extract task-relevant information while filtering out social content and deliberation noise.

## Key Results
- Using predicted rewrites as input to single-user dialogue systems improves intent tracking by 8 points and request tracking by 6.4 points compared to flattened multi-user chats
- Rewriting generalizes to unseen domains (GiftFinder, NewsFinder) without retraining, achieving 9-point improvement for intent tracking
- The rewriting method outperforms training medium-sized models directly on multi-user dialogues for dialogue state tracking accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rewriting multi-user chats as single concise queries enables existing single-user dialogue systems to process multi-user inputs without modification.
- **Mechanism:** Multi-user chats contain deliberation, social chatter, and multiple speaker turns. A rewrite extracts only task-relevant information (intents, slots) into a single utterance that matches the expected format of single-user systems.
- **Core assumption:** The original user utterance in MultiWOZ can be reconstructed from the multi-user chat while preserving all task-relevant information and removing social content.
- **Evidence anchors:**
  - [abstract]: "using predicted rewrites substantially improves dialogue state tracking for dialogue systems trained on single-user dialogues"
  - [section 3.3]: "Based on the validation results, we computed the score for each criterion... (Q4–5) 0.98/1. Especially the scores of Q4–5 suggest that dialogue states are preserved between multi-user chats and their source utterances."
- **Break condition:** If multi-user chats contain deliberation where the final decision differs from the original utterance, or if social chatter masks task-relevant slots, the rewrite may fail to capture the correct information.

### Mechanism 2
- **Claim:** Model-predicted rewrites outperform simply concatenating multi-user chat utterances when input to single-user dialogue systems.
- **Mechanism:** Concatenation treats the multi-user chat as one long utterance, introducing noise and irrelevant information. Rewriting filters out non-task content and structures the query properly.
- **Core assumption:** BART-based models can effectively distinguish task-relevant from task-irrelevant content in multi-user chats.
- **Evidence anchors:**
  - [section 5.3]: "feeding predicted rewrites as input (Rewrite) substantially outperforms feeding a flattened multi-user chat (Flatten) in predicting intents (+8 points) and requests (+6.4 points)"
  - [table 7a]: Quantitative comparison showing Rewrite > Flatten for Intent and Request tracking.
- **Break condition:** If the rewriting model hallucinates or omits critical slot values (e.g., deliberation outcomes), the resulting rewrite will mislead the dialogue system.

### Mechanism 3
- **Claim:** Rewriting generalizes to unseen domains without retraining the rewriting model on those domains.
- **Mechanism:** The rewriting model learns to extract and structure task-relevant information from multi-user chats, a skill transferable across domains.
- **Core assumption:** The dynamics of collaborative decision-making (slot elicitation, deliberation) are domain-agnostic, allowing learned rewriting patterns to transfer.
- **Evidence anchors:**
  - [section 5.3]: "Rewrite still outperforms Flatten for Intent by 9 points and for Inform by 7 points" in unseen domains (GiftFinder, NewsFinder).
- **Break condition:** If unseen domains have radically different dialogue structures or slot types not represented in training data, the rewriting model may fail to generalize.

## Foundational Learning

- **Concept:** Multi-user dialogue dynamics (slot elicitation, social chatter, deliberation)
  - Why needed here: Understanding these dynamics is essential to design effective rewriting strategies and evaluate rewrite quality.
  - Quick check question: Can you identify which parts of a multi-user chat contain task-relevant information versus social content?

- **Concept:** Dialogue state tracking for task-oriented systems
  - Why needed here: The goal of rewriting is to improve dialogue state tracking accuracy; understanding DST mechanics is crucial.
  - Quick check question: What are the key outputs of dialogue state tracking in task-oriented systems?

- **Concept:** Sequence-to-sequence modeling for text rewriting
  - Why needed here: The rewriting task is framed as seq2seq, so understanding how models like BART learn to transform input to output is essential.
  - Quick check question: How does BART handle input sequences differently from autoregressive models like GPT?

## Architecture Onboarding

- **Component map:** Multi-user chat collection → Validation pipeline → Rewrite model training → Dialogue state tracking system → Evaluation framework
- **Critical path:** Multi-user chat → Rewrite model → Dialogue state tracker → System action
- **Design tradeoffs:**
  - Dataset size vs. quality: Collecting high-quality multi-user chats is expensive but necessary for training effective rewrite models
  - Model size vs. latency: Larger models (BART-large) perform better but may not be suitable for edge deployment
  - On-device vs. server processing: On-device rewriting preserves privacy but may be computationally constrained
- **Failure signatures:**
  - Low ROUGE scores indicate the rewrite doesn't match the source utterance
  - High hallucination rates (>16%) suggest the model is making up information
  - Poor dialogue state tracking accuracy indicates the rewrite failed to capture essential information
  - Model struggles with deliberation resolution (picking wrong slot values)
- **First 3 experiments:**
  1. Train BART-base on the Multi-User MultiWOZ dataset and evaluate rewrite quality using ROUGE, slot coverage, and hallucination rate
  2. Compare dialogue state tracking accuracy when using rewrites vs. flattened multi-user chats as input to a single-user DST system
  3. Test domain transfer by evaluating the rewrite model on unseen multi-user domains (e.g., gift finding, news finding) and measuring impact on DST accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we effectively automate the validation criteria used in the Multi-User MultiWOZ dataset creation, specifically the detection of user relationships and the assessment of chat realism?
- **Basis in paper:** [explicit] The paper mentions that the five validation criteria are important quality metrics and suggests that automating some of them, like detecting if dialogue participants are two customers making decisions together, is an opportunity for future work.
- **Why unresolved:** The paper does not provide details on how to implement such automation or what methods could be used.
- **What evidence would resolve it:** Development and testing of automated methods for detecting user relationships and assessing chat realism, with results showing accuracy and efficiency compared to manual validation.

### Open Question 2
- **Question:** What are the best practices for extending the Multi-User MultiWOZ dataset to include more utterances per dialogue and more challenging interactions like deliberation?
- **Basis in paper:** [explicit] The paper mentions that the dataset uses a subset of dialogues from MultiWOZ 2.2 and the first four user utterances in each dialogue, and that some interactions like deliberation are less frequent than others.
- **Why unresolved:** The paper does not provide guidance on how to effectively extend the dataset to include more utterances and interactions.
- **What evidence would resolve it:** A study comparing the performance of dialogue systems trained on different sizes and compositions of the Multi-User MultiWOZ dataset, with results showing the impact of dataset size and interaction types on system performance.

### Open Question 3
- **Question:** How does turn-level dialogue state tracking compare to contextual query rewriting in terms of effectiveness for handling multi-user dialogues?
- **Basis in paper:** [explicit] The paper mentions that an alternative approach to query rewriting is to track dialogue states after each utterance in multi-user chats, but does not compare the effectiveness of these two methods.
- **Why unresolved:** The paper does not provide a comparison of these two approaches or their relative merits.
- **What evidence would resolve it:** A study comparing the performance of dialogue systems using turn-level dialogue state tracking versus contextual query rewriting, with results showing which approach is more effective for handling multi-user dialogues.

## Limitations
- The rewriting approach's effectiveness depends heavily on the assumption that multi-user deliberation outcomes can be reliably reconstructed from the original source utterance
- The study only evaluates on MultiWOZ 2.2, limiting generalizability to other task-oriented dialogue datasets with different domain structures
- The claim that model-predicted rewrites outperform training medium-sized models directly on multi-user dialogues lacks direct experimental comparison

## Confidence
- **High Confidence:** The empirical demonstration that predicted rewrites improve dialogue state tracking (↑8 points for intent, ↑6.4 points for requests) when used as input to single-user dialogue systems trained on MultiWOZ 2.2
- **Medium Confidence:** The claim that rewriting generalizes to unseen domains without retraining, based on limited test domains and sample size
- **Low Confidence:** The assertion that model-predicted rewrites consistently outperform training a medium-sized model directly on multi-user dialogues without direct experimental comparison

## Next Checks
1. **Deliberation Outcome Testing:** Design experiments specifically testing cases where multi-user deliberation leads to different slot values than the original utterance, measuring rewrite accuracy and downstream dialogue state tracking performance in these edge cases.

2. **Cross-Dataset Generalization:** Evaluate the rewriting approach on a non-MultiWOZ task-oriented dialogue dataset (e.g., Schema-Guided Dialogue) to test true domain generalization beyond similar hotel/restaurant booking scenarios.

3. **Model Size Comparison:** Directly compare dialogue state tracking performance between (a) using BART-large rewrites as input to single-user DST, (b) training T5-base directly on multi-user dialogues, and (c) training T5-large directly on multi-user dialogues, using identical evaluation metrics and test sets.