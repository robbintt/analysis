---
ver: rpa2
title: 'Biquality Learning: a Framework to Design Algorithms Dealing with Closed-Set
  Distribution Shifts'
arxiv_id: '2308.15132'
source_url: https://arxiv.org/abs/2308.15132
tags:
- learning
- trusted
- dataset
- data
- shift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the biquality learning framework for handling
  closed-set distribution shifts in machine learning, where trusted and untrusted
  datasets are available during training. The authors propose two methods: IRBL2,
  which extends the importance reweighting approach for biquality learning to handle
  both concept drift and covariate shift using three probabilistic classifiers, and
  K-PDR, a non-parametric variant of K-DensityRatio that uses probabilistic classifiers
  to estimate class-conditional density ratios.'
---

# Biquality Learning: a Framework to Design Algorithms Dealing with Closed-Set Distribution Shifts

## Quick Facts
- arXiv ID: 2308.15132
- Source URL: https://arxiv.org/abs/2308.15132
- Reference count: 40
- Primary result: Biquality learning framework enables closed-set distribution shift correction using trusted data as anchor, with IRBL2 and K-PDR methods showing effectiveness on synthetic concept drift and class-conditional shifts respectively.

## Executive Summary
This paper introduces biquality learning, a framework for handling closed-set distribution shifts when both trusted and untrusted datasets are available during training. The authors propose two algorithms: IRBL2, which extends importance reweighting for biquality learning to handle both concept drift and covariate shift, and K-PDR, a non-parametric variant of K-DensityRatio that uses probabilistic classifiers to estimate class-conditional density ratios. Experiments on real-world datasets with synthetic concept drift and class-conditional shifts show that IRBL2 is effective for label noise while K-PDR excels at covariate shift correction, though both have limitations in certain scenarios.

## Method Summary
The biquality learning framework leverages trusted and untrusted datasets available at training time to correct closed-set distribution shifts through importance reweighting. IRBL2 extends this by decomposing the joint density ratio estimation into simpler sub-tasks using three probabilistic classifiers (PT(Y|X), PU(Y|X), and PT(X)/PU(X)), while K-PDR uses a non-parametric approach with per-class density ratio estimation. Synthetic corruption generation methods introduce controlled concept drift via label noise in decision tree leaves and class-conditional shifts via cluster-based sub-sampling, enabling fine-grained benchmarking across varying shift strengths.

## Key Results
- IRBL2 demonstrates strong performance on datasets with synthetic concept drift, effectively handling label noise through its decomposition approach
- K-PDR excels at covariate shift correction, outperforming traditional density ratio methods when class-conditional shifts are present
- Both methods show limitations when trusted data is scarce or when shifts are severe, highlighting practical constraints of the framework
- Synthetic corruption generation provides controlled benchmarking but may not fully capture real-world distribution shift patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The biquality framework enables closed-set distribution shift correction by leveraging a trusted dataset as anchor distribution reference.
- Mechanism: At training time, both trusted and untrusted datasets are available. Algorithms reweight untrusted instances so their effective distribution matches the trusted one, allowing learning without strong assumptions on the exact nature of the shift.
- Core assumption: PT(X,Y) is absolutely continuous with respect to PU(X,Y), ensuring the Radon-Nikodym derivative exists.
- Evidence anchors:
  - [abstract] "The trusted and untrusted datasets available at training time make designing algorithms dealing with any distribution shifts possible."
  - [section] Theorem 1 and Equation 3 show the equivalence of reweighted untrusted risk to trusted risk.
  - [corpus] Weak (only general ML terms, no specific biquality mention).
- Break condition: If PT(X,Y) and PU(X,Y) are not absolutely continuous (e.g., disjoint supports), the RND does not exist and reweighting fails.

### Mechanism 2
- Claim: Decomposing joint density ratio estimation into simpler sub-tasks improves estimation accuracy under complex shifts.
- Mechanism: IRBL2 estimates PT(Y|X), PU(Y|X), and PT(X)/PU(X) separately using probabilistic classifiers, then combines them to form the full reweighting factor. K-PDR does similar per-class decomposition for covariate shift.
- Core assumption: Conditional distributions PT(Y|X) and PU(Y|X) can be learned accurately from respective datasets, and PT(X)/PU(X) is estimable via discriminative learning.
- Evidence anchors:
  - [section] "IRBL2 works by defining a new supervised classification task by learning to predict if a sample is trusted or untrusted by only using its features."
  - [abstract] Mentions two methods: one inspired by label noise, another by covariate shift literature.
  - [corpus] Weak (general ML mentions, no detailed density ratio decomposition).
- Break condition: If conditional distributions are not learnable due to insufficient data or model capacity, the decomposition fails.

### Mechanism 3
- Claim: Synthetic corruption generation with controlled strength allows fine-grained benchmarking of robustness to both concept drift and class-conditional shift.
- Mechanism: Concept drift is introduced by injecting label noise in purest decision tree leaves; class-conditional shift by subsampling clusters in feature space per class. This isolates effects and allows measuring algorithm performance across varying r and ρ.
- Core assumption: The synthetic corruptions faithfully represent real-world distribution shifts and the decision tree/cluster structure captures relevant feature space regions.
- Evidence anchors:
  - [section] "We propose two novel ways to generate such shifts in real-world datasets... concept drift... class-conditional shift."
  - [abstract] "Experiments on real-world datasets with synthetic concept drift and class-conditional shifts..."
  - [corpus] Weak (no mention of synthetic corruption generation).
- Break condition: If synthetic corruption patterns do not match real shifts, benchmark results may not generalize.

## Foundational Learning

- Concept: Radon-Nikodym Derivative (RND)
  - Why needed here: Core to importance reweighting; formalizes how to adjust untrusted data to match trusted distribution.
  - Quick check question: What does it mean if PT(X,Y) is not absolutely continuous with respect to PU(X,Y) in the biquality setup?

- Concept: Density Ratio Estimation
  - Why needed here: Estimating PT(X)/PU(X) or PT(X|Y)/PU(X|Y) is essential for reweighting; many algorithms hinge on this.
  - Quick check question: How does the discriminative learning approach in IRBL2 estimate the density ratio without estimating the full densities?

- Concept: Probabilistic Classifier Calibration
  - Why needed here: IRBL2 and K-PDR rely on well-calibrated probability outputs for correct reweighting; miscalibration can severely degrade performance.
  - Quick check question: Why might well-calibrated classifiers naturally downweight samples from regions with low trusted data density?

## Architecture Onboarding

- Component map: Trusted dataset loader → probabilistic classifier trainer (PT(Y|X)) → untrusted dataset loader → probabilistic classifier trainer (PU(Y|X)) → discriminative classifier trainer (PT(X)/PU(X)) → reweighting factor combiner → final model trainer. Alternative path: K-PDR replaces discriminative classifier with per-class density ratio estimators.

- Critical path:
  1. Train PT(Y|X) and PU(Y|X) classifiers on trusted and untrusted data respectively.
  2. Train discriminative classifier on trusted/untrusted labels using only features.
  3. Combine outputs to compute per-instance weights.
  4. Train final model on union of datasets with computed weights.

- Design tradeoffs:
  - Using HGBT trees gives strong performance but requires calibration; kernels (KMM) scale poorly but are non-parametric.
  - Per-class decomposition (K-PDR) increases flexibility but adds K classifiers.
  - Synthetic corruption generation must balance realism and controllability.

- Failure signatures:
  - High variance in predicted weights → calibration or estimation issues.
  - Performance similar to no-correction baseline → RND estimation failure or insufficient trusted data.
  - Degraded performance on class-conditional shift but good on label noise → method mismatch to corruption type.

- First 3 experiments:
  1. Run IRBL2 and K-PDR on a simple two-moons dataset with synthetic concept drift; verify weight patterns match intuition.
  2. Apply both methods to a tabular dataset with synthetic class-conditional shift; compare to PDR baseline.
  3. Sweep trusted data ratio p from 0.1 to 0.9; measure AUC of Cohen's kappa vs corruption strength to assess robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does IRBL2 consistently outperform IRBL across different biquality learning scenarios, or is its advantage limited to specific types of distribution shifts?
- Basis in paper: [explicit] The authors note that "results showed no improvements brought by IRBL2 over IRBL on class-conditional shift" and that "IRBL might already be able to take into account the uncertainty about P(X) provided by PDR in the predicted probabilities P(Y|X)."
- Why unresolved: The experimental results show mixed performance, with IRBL2 not consistently outperforming IRBL. The authors suggest this might be due to calibration efficiency or the nature of the distribution shift.
- What evidence would resolve it: Additional experiments varying the type and severity of distribution shifts, along with different calibration techniques, could clarify when IRBL2 provides advantages over IRBL.

### Open Question 2
- Question: How do K-PDR and K-KMM perform when the class-conditional shift involves introducing out-of-distribution data points, rather than just under-representing sub-populations?
- Basis in paper: [inferred] The authors note that "the proposed experiments for class-conditional shifts might only reproduce some cases of dataset shifts, especially shifts with out-of-distribution data" and that "we did not introduce data points in foreign feature spaces."
- Why unresolved: The current experimental design focuses on sub-sampling clusters, which may not fully capture the challenges of out-of-distribution shifts.
- What evidence would resolve it: Experiments introducing synthetic data points in feature spaces not present in the trusted dataset would test the robustness of K-PDR and K-KMM to more extreme class-conditional shifts.

### Open Question 3
- Question: What is the impact of using different calibration techniques for the histogram-based gradient boosting trees on the performance of biquality learning algorithms?
- Basis in paper: [explicit] The authors state that "the calibration efficiency of the HGBT trees in these experiments greatly impacts the efficiency of every tested biquality-learning algorithm" and that they "have not explored different calibration techniques other than Isotonic Regression."
- Why unresolved: The choice of calibration method could significantly influence the performance of the algorithms, especially in handling uncertainty in predictions.
- What evidence would resolve it: Comparing the performance of biquality learning algorithms using various calibration techniques (e.g., Platt scaling, beta calibration) would reveal the importance of calibration in these methods.

## Limitations

- The assumption of absolute continuity between trusted and untrusted distributions is critical but not empirically validated for real-world datasets
- Performance degrades significantly when trusted data is scarce, limiting practical applicability in data-constrained scenarios
- Synthetic corruption generation, while controlled, may not faithfully represent complex real-world distribution shifts

## Confidence

- **High Confidence**: The theoretical foundation of importance reweighting and the Radon-Nikodym derivative application is well-established in the ML literature.
- **Medium Confidence**: The synthetic corruption generation methodology is novel and conceptually sound, but lacks validation against real-world shift patterns.
- **Medium Confidence**: The experimental results demonstrate the viability of biquality learning, but the generalization to unseen distribution shifts remains to be proven.

## Next Checks

1. Apply IRBL2 and K-PDR to a dataset with known, naturally occurring distribution shift (e.g., temporal data with domain adaptation) to validate the synthetic corruption patterns.
2. Systematically evaluate the calibration of the probabilistic classifiers used in IRBL2 and K-PDR, and measure the impact of miscalibration on reweighting performance.
3. Conduct a more extensive analysis of performance as a function of the trusted data ratio p, including smaller trusted set sizes, to identify the practical limits of the framework.