---
ver: rpa2
title: TMI! Finetuned Models Leak Private Information from their Pretraining Data
arxiv_id: '2306.01181'
source_url: https://arxiv.org/abs/2306.01181
tags:
- learning
- privacy
- finetuned
- attack
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel membership-inference threat model
  where an adversary with query access to a finetuned model attempts to infer whether
  individuals were included in the pretraining dataset. The proposed attack, TMI,
  leverages the influence of memorized pretraining samples on predictions in the downstream
  task by training a metaclassifier on prediction vectors from shadow models.
---

# TMI! Finetuned Models Leak Private Information from their Pretraining Data

## Quick Facts
- arXiv ID: 2306.01181
- Source URL: https://arxiv.org/abs/2306.01181
- Reference count: 40
- Primary result: Novel membership-inference attack that successfully infers whether individuals were included in pretraining data from finetuned models

## Executive Summary
This paper introduces TMI, a membership-inference attack that targets finetuned models by exploiting memorization leakage from pretraining data. Unlike previous attacks that focus on the finetuning dataset, TMI specifically aims to infer membership in the pretraining corpus. The attack works by training a metaclassifier on prediction vectors from shadow models that undergo the same pretraining and finetuning process. TMI achieves high attack success rates even when models are finetuned with differential privacy or when the downstream task differs from pretraining, demonstrating significant privacy risks in transfer learning.

## Method Summary
TMI operates by first training shadow models on random subsets of the pretraining data, then finetuning them on the downstream task. The adversary queries these shadow models and the target model with augmented versions of challenge points, collecting prediction vectors. A metaclassifier is trained on these vectors to distinguish between member and non-member samples. When applied to the target model, TMI uses the trained metaclassifier to infer membership status. The attack leverages the observation that pretraining memorization leaves fingerprints in the model that persist through finetuning and influence predictions on downstream tasks.

## Key Results
- TMI achieves AUCs up to 0.78 and TPRs up to 16.1% at 0.1% FPR
- Attack remains effective even when models are finetuned with differential privacy (ε=8, δ=1e-5)
- Success rates persist even when downstream tasks are dissimilar to pretraining data
- TMI outperforms baseline attacks that only use predicted labels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining memorization leaks through finetuned models.
- Mechanism: Individual samples from pretraining leave a "fingerprint" in the model's internal representations. When finetuned, these fingerprints influence predictions on downstream tasks, even when parameters are updated. The adversary exploits this by training shadow models that mimic the finetuning process and observing how prediction vectors differ based on membership.
- Core assumption: Memorization occurs during pretraining and is not completely overwritten during finetuning.
- Evidence anchors:
  - [abstract] "TMI constructs a dataset of prediction vectors from queries to finetuned shadow models in order to train a metaclassifier that can infer membership."
  - [section 4.3] "TMI constructs a dataset of prediction vectors from queries to finetuned shadow models in order to train a metaclassifier that can infer membership."
  - [corpus] Weak evidence. Neighbor papers discuss pretraining exposure and privacy risks, but not specific memorization leakage mechanisms.
- Break condition: If finetuning completely erases pretraining memorization, or if downstream tasks are sufficiently different to prevent meaningful feature transfer.

### Mechanism 2
- Claim: Prediction vector structure preserves membership signals.
- Mechanism: The softmax output vector contains information about how pretraining samples influence all class probabilities, not just the predicted label. By training a metaclassifier on these vectors from shadow models, the adversary can learn patterns that distinguish member vs non-member samples.
- Core assumption: The entire prediction vector, not just the top prediction, contains membership-relevant information.
- Evidence anchors:
  - [abstract] "TMI leverages the influence of memorized pretraining samples on predictions in the downstream task by observing entire prediction vectors from the finetuned model."
  - [section 4.3] "Using the prediction vectors of our shadow models on the challenge point, we generate a dataset to train a metaclassifier to determine the challenge point's membership status."
  - [corpus] Moderate evidence. Neighbor paper "One Leak Away" discusses how pretraining exposure amplifies jailbreak risks, suggesting pretraining information persists.
- Break condition: If the downstream task is completely unrelated to pretraining, or if the metaclassifier cannot learn meaningful patterns from prediction vectors.

### Mechanism 3
- Claim: Random augmentations amplify membership signals.
- Mechanism: By querying shadow models and the target model multiple times with different random augmentations of the same challenge point, the adversary creates a richer dataset that captures how the model's behavior varies with input transformations. This variation contains additional membership information.
- Core assumption: Membership status influences how the model responds to input transformations consistently across shadow models.
- Evidence anchors:
  - [section 4.3] "Because we use a relatively small number of shadow models (64 IN and 64 OUT in total), we leverage random augmentations to construct a larger metaclassifier dataset."
  - [section 4.3] "Each time we query the target model or our local shadow models, we query M times with different random augmentations of the challenge point."
  - [corpus] Weak evidence. No neighbor papers specifically discuss random augmentations in membership inference.
- Break condition: If random augmentations destroy the membership signal, or if the number of augmentations is too small to capture meaningful variation.

## Foundational Learning

- Concept: Differential Privacy
  - Why needed here: The paper evaluates TMI against models finetuned with DP-SGD, showing that even with privacy guarantees for the finetuning dataset, pretraining data remains vulnerable.
  - Quick check question: What is the difference between (ε, δ)-differential privacy for pretraining vs finetuning, and why does finetuning with DP not protect pretraining data?

- Concept: Transfer Learning and Feature Extraction
  - Why needed here: The attack works across different finetuning strategies (feature extraction, updating parameters, DP), so understanding how these affect memorization leakage is crucial.
  - Quick check question: How does freezing vs updating pretrained weights during finetuning affect the persistence of pretraining memorization?

- Concept: Metaclassifier-based Attacks
  - Why needed here: TMI uses a metaclassifier trained on shadow model outputs to infer membership, rather than direct statistical tests on model outputs.
  - Quick check question: Why does TMI use a metaclassifier instead of a likelihood ratio test, and what advantage does this provide?

## Architecture Onboarding

- Component map:
  Shadow Model Training -> Query Generation -> Metaclassifier Training -> Membership Inference -> Evaluation

- Critical path:
  1. Train shadow models (pretrain → finetune)
  2. Generate prediction vectors for challenge points using augmented queries
  3. Train metaclassifier on IN/OUT prediction vectors
  4. Query target model with challenge point
  5. Classify membership using metaclassifier

- Design tradeoffs:
  - Number of shadow models vs computational cost
  - Number of augmentations vs dataset size vs training time
  - Metaclassifier architecture (neural network vs KNN vs SVM) vs accuracy vs speed
  - Knowledge of downstream task labels vs attack feasibility

- Failure signatures:
  - Low AUC/close to 0.5: Attack is no better than random guessing
  - Large gap between IN and OUT prediction vector distributions: Suggests membership signal exists
  - Sensitivity to number of shadow models/augmentations: Indicates need for more data

- First 3 experiments:
  1. Run TMI with minimal settings (16 shadow models, 4 augmentations) on CIFAR-10 to establish baseline
  2. Compare metaclassifier architectures (NN vs KNN vs SVM) on same dataset
  3. Test effect of number of augmentations (2, 4, 8, 16) on attack success

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can property inference attacks successfully infer global properties of pretraining data from finetuned models?
- Basis in paper: [explicit] The paper discusses membership inference attacks and notes that "property inference, attribute inference, and training data extraction attacks" remain unexplored in the transfer learning setting
- Why unresolved: The paper focuses solely on membership inference attacks and explicitly states these other attack types "remain an open question"
- What evidence would resolve it: Demonstrating successful property inference attacks on finetuned models that can infer global characteristics of pretraining datasets (e.g., data distribution, class imbalance)

### Open Question 2
- Question: How does the effectiveness of privacy attacks vary across different transfer learning paradigms (e.g., domain adaptation vs. task transfer)?
- Basis in paper: [inferred] The paper evaluates TMI across tasks of varying similarity to pretraining, but only within supervised classification tasks
- Why unresolved: The paper only examines transfer learning within similar domains (vision to vision, language to language) and doesn't explore cross-domain transfers
- What evidence would resolve it: Comparative analysis of attack success rates when transferring between fundamentally different domains (e.g., vision to language, or tabular to text)

### Open Question 3
- Question: What is the relationship between memorization during pretraining and vulnerability to membership inference in finetuned models?
- Basis in paper: [explicit] The paper notes that "memorization of training data may actually be necessary to achieve optimal generalization" and that "deep learning models tend to have higher prediction confidence on training data"
- Why unresolved: While the paper observes that memorization occurs and affects attack success, it doesn't directly quantify how pretraining memorization levels correlate with MI vulnerability after finetuning
- What evidence would resolve it: Controlled experiments measuring pretraining memorization (via influence functions or other methods) and correlating with MI attack success across multiple finetuned models

## Limitations

- Attack effectiveness across diverse domains remains uncertain and untested
- Computational overhead of generating sufficient shadow models and augmentations not fully characterized
- Paper doesn't address scenarios where adversary has partial knowledge of pretraining dataset composition

## Confidence

- **High confidence**: The fundamental mechanism of memorization leakage through finetuned models is well-supported by experimental results across multiple vision and language tasks
- **Medium confidence**: The claim that TMI works even when downstream tasks are dissimilar from pretraining is supported but limited to specific task pairs tested
- **Medium confidence**: The assertion that DP-SGD finetuning doesn't protect pretraining data is demonstrated but only for specific DP parameters tested

## Next Checks

1. Test TMI on medical imaging datasets (e.g., CheXpert pretraining with dermatology downstream tasks) to evaluate cross-domain effectiveness and identify domain-specific limitations
2. Evaluate the attack's sensitivity to partial knowledge of pretraining data by simulating scenarios where the adversary knows only a subset of the pretraining corpus
3. Benchmark the computational cost of TMI in production settings by measuring shadow model generation time and metaclassifier training requirements for different dataset sizes and model architectures