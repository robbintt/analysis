---
ver: rpa2
title: 'Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking'
arxiv_id: '2311.06102'
source_url: https://arxiv.org/abs/2311.06102
tags:
- shot
- examples
- gpt-4
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance and cost-effectiveness of
  few-shot text classification methods in banking, focusing on Large Language Models
  (LLMs) like GPT-4. The authors compare fine-tuned MLMs, contrastive learning techniques
  (SetFit), and in-context learning with LLMs on the Banking77 financial intent detection
  dataset.
---

# Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking

## Quick Facts
- arXiv ID: 2311.06102
- Source URL: https://arxiv.org/abs/2311.06102
- Reference count: 40
- Key outcome: LLMs outperform fine-tuned MLMs in few-shot banking intent detection, with RAG-based inference reducing costs while maintaining performance

## Executive Summary
This paper evaluates few-shot text classification methods for banking intent detection, comparing fine-tuned MLMs, contrastive learning (SetFit), and in-context learning with LLMs. The authors find that GPT-4 can outperform fine-tuned MPNet-v2 even with fewer examples, and propose a cost-effective RAG-based inference method that reduces costs by up to 6x while maintaining or improving performance. They also demonstrate that expert-curated training examples significantly improve LLM performance, and explore data augmentation using GPT-4 to enhance results in low-resource settings.

## Method Summary
The study uses the Banking77 dataset with 77 intent labels and 13,083 examples, evaluating few-shot scenarios (1-20 examples per class) and full-data settings. Methods include fine-tuning MPNet-v2 with and without SetFit, in-context learning with GPT-3.5, GPT-4, Cohere Command-nightly, and Claude using human-curated samples, and a RAG-based retrieval approach using MPNet embeddings for similarity search. Performance is measured using Micro-F1 and Macro-F1 scores, with cost analysis based on API pricing. The study also explores data augmentation by generating additional examples with GPT-4.

## Key Results
- GPT-4 outperforms fine-tuned MPNet-v2 by over 20 percentage points in 1-shot settings
- RAG-based inference with 5 retrieved examples costs 1/6 of GPT-4's standard inference while achieving comparable performance
- Expert-curated examples improve LLM performance by up to 10 points compared to random selection
- Data augmentation with GPT-4 improves performance up to a threshold, after which quality degrades

## Why This Works (Mechanism)

### Mechanism 1
RAG-based inference reduces costs while maintaining or improving performance compared to standard few-shot LLM prompting. By retrieving only a small subset of similar training examples per query, the prompt length and computational cost are reduced without losing relevant context for classification. Evidence shows that retrieving 5 examples (2.2% of total) achieves comparable performance to full prompting at 1/6 the cost. Break condition: if retrieved examples are not semantically representative of the test instance, classification accuracy will drop.

### Mechanism 2
In-context learning with GPT-4 outperforms fine-tuned MPNet-v2 in few-shot scenarios. GPT-4 leverages its large pre-trained knowledge and instruction-following fine-tuning to generalize from a small number of examples without needing additional training. Evidence shows GPT-4 achieves competitive results and outperforms MPNet-v2 by over 20 percentage points in 1-shot settings. Break condition: if domain-specific knowledge is required beyond what GPT-4's pre-training covers, performance may degrade.

### Mechanism 3
Expert-curated training examples improve LLM performance in few-shot classification. Human selection of representative examples reduces semantic overlap and ambiguity, providing clearer signal to the LLM. Evidence shows curated samples outperform randomly selected ones with up to 10 points difference. Break condition: if human expert introduces bias or if the curated set is too small, the improvement may not materialize.

## Foundational Learning

- **Concept: Few-shot learning**
  - Why needed here: The paper focuses on scenarios with very limited labeled data (1-5 examples per class), which is common in banking domains.
  - Quick check question: What is the main difference between few-shot learning and traditional supervised learning?

- **Concept: In-context learning**
  - Why needed here: The paper uses LLMs that learn tasks from examples provided in the prompt without fine-tuning.
  - Quick check question: How does in-context learning differ from fine-tuning in terms of model adaptation?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: RAG is used to reduce costs by retrieving relevant examples instead of including all examples in each prompt.
  - Quick check question: What is the primary benefit of using RAG in the context of LLM inference?

## Architecture Onboarding

- **Component map**: Banking77 dataset -> MPNet embeddings for similarity search -> Retriever (cosine similarity) -> LLM interface (prompt construction) -> Evaluator (Micro-F1 and Macro-F1 calculation)
- **Critical path**: Load Banking77 dataset -> Generate MPNet embeddings for all training examples -> For each test query, retrieve k similar examples -> Construct prompt and send to LLM -> Parse response and calculate metrics
- **Design tradeoffs**: Number of retrieved examples (k) vs. cost and performance, human-curated vs. random examples, choice of LLM vs. cost and accuracy
- **Failure signatures**: High cost with low performance (LLM not suited for task), low performance with small k (Retriever not finding relevant examples), inconsistent results (LLM non-deterministic behavior)
- **First 3 experiments**: Test standard few-shot prompting with GPT-4 using random vs. curated examples, implement RAG with k=5 and compare cost vs. performance, test data augmentation by generating additional examples with GPT-4 and retraining with SetFit

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but suggests future work involving open-sourced LLM alternatives like LLaMA2 and Chain-of-Thought techniques, indicating that comparative performance studies with open-source models remain unexplored.

## Limitations
- Evaluation focuses exclusively on the Banking77 dataset, which may not represent the full diversity of financial text classification tasks
- Cost analysis assumes current API pricing models, which may change over time and affect economic viability
- Human curation process relies on a single expert whose selection criteria and potential biases are not fully characterized

## Confidence
- High confidence: LLMs (especially GPT-4) outperform fine-tuned MLMs in few-shot banking classification
- Medium confidence: RAG-based inference consistently reduces costs while maintaining performance across different LLMs
- Low confidence: Human-curated examples provide consistent 10-point improvements across all few-shot settings

## Next Checks
1. Test RAG performance with different embedding models (beyond MPNet) to verify retrieval quality isn't architecture-dependent
2. Conduct ablation studies on the number of retrieved examples (k) to find optimal cost-performance tradeoffs
3. Validate results on additional financial datasets to assess domain-specific vs. general LLM capabilities