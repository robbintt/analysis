---
ver: rpa2
title: Improving Plasticity in Online Continual Learning via Collaborative Learning
arxiv_id: '2312.00600'
source_url: https://arxiv.org/abs/2312.00600
tags:
- learning
- online
- plasticity
- performance
- ccl-dc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses plasticity and stability challenges in online\
  \ continual learning. The authors argue that plasticity\u2014the ability to acquire\
  \ new knowledge\u2014is as important as stability in online CL, where data is seen\
  \ only once."
---

# Improving Plasticity in Online Continual Learning via Collaborative Learning

## Quick Facts
- arXiv ID: 2312.00600
- Source URL: https://arxiv.org/abs/2312.00600
- Authors: 
- Reference count: 40
- Primary result: CCL-DC improves plasticity and stability in online continual learning through collaborative learning and entropy regularization

## Executive Summary
This paper addresses the often-overlooked importance of plasticity—the ability to acquire new knowledge—in online continual learning. While most online CL methods focus on preventing forgetting (stability), the authors argue that plasticity is equally crucial when data is seen only once. They propose Collaborative Continual Learning with Distillation Chain (CCL-DC), a strategy involving two peer learners trained collaboratively with an entropy-based distillation scheme. The method shows significant performance gains over baselines on CIFAR-10/100, TinyImageNet, and ImageNet-100, improving both final accuracy and plasticity metrics.

## Method Summary
CCL-DC involves two peer continual learners of identical architecture trained simultaneously in a peer-teaching manner. During training, networks are supervised with both ground truth labels and predictions from their peer. The method introduces a Distillation Chain (DC) that uses data augmentation strategies to generate samples with different difficulty levels. The distillation occurs from less confident predictions (harder samples) to more confident ones (easier samples), acting as entropy regularization that suppresses overconfident outputs and improves generalization. This framework is applied to existing online CL methods like ER, DER++, ER-ACE, OCM, GSA, and OnPro.

## Key Results
- CCL-DC significantly improves average accuracy and learning accuracy (plasticity metric) across multiple datasets
- The method enhances feature discrimination, as evidenced by improved NCM accuracy and t-SNE visualization
- CCL-DC successfully alleviates shortcut learning by preventing overconfident predictions
- When applied to existing online CL methods, CCL-DC consistently improves performance while maintaining stability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Collaborative learning accelerates convergence in online CL by providing more parallelism and maneuverability.
- **Mechanism**: Two peer learners train simultaneously, each learning from the other's predictions through distillation. This creates multiple optimization paths and reduces the risk of getting stuck in suboptimal minima.
- **Core assumption**: The predictions of peer learners contain useful information that can guide each other's training, even when the peers have different error patterns.
- **Evidence anchors**:
  - [abstract] "Collaborative Continual Learning (CCL) involves two peer continual learners to learn from the data stream simultaneously in a peer teaching manner, and it enables us to have more parallelism in optimization and provides more maneuverability to the continual learners."
  - [section 4.2] "CCL involves two peer continual learners of the same architecture and optimizer setting training in a peer-teaching manner. In the training phase, networks are supervised with both the ground truth label and the predictions of their peers."
- **Break condition**: If the two peer learners consistently make correlated errors, the collaborative training may reinforce these errors rather than correct them.

### Mechanism 2
- **Claim**: Entropy regularization via distillation chain prevents overconfidence and improves generalization.
- **Mechanism**: Harder augmented samples produce less confident predictions. Distilling from these less confident predictions to more confident ones creates a learned entropy regularization that suppresses overconfident outputs.
- **Core assumption**: Overconfident predictions are detrimental to generalization in online CL, similar to supervised learning scenarios.
- **Evidence anchors**:
  - [section 4.3] "DC uses data augmentation strategies to generate samples with different levels of difficulty and produces logit distribution with different confidence. The distillation from less confident predictions to more confident predictions weakens the overall confidence of the network and benefits the performance by improving the generalization capability."
  - [supplementary material] "In conventional supervised learning, it is well established that excessive confidence can harm the generalization ability of deep models."
- **Break condition**: If the augmentation chain doesn't create meaningful differences in difficulty, the entropy regularization effect will be minimal.

### Mechanism 3
- **Claim**: CCL-DC improves feature discrimination by creating more diverse feature representations.
- **Mechanism**: The collaborative learning and entropy regularization encourage the model to develop features that are more discriminative across tasks, as evidenced by improved NCM accuracy and t-SNE visualization.
- **Core assumption**: Better feature discrimination directly translates to improved continual learning performance.
- **Evidence anchors**:
  - [section 6] "Another advantage of CCL-DC is its ability to enhance the feature discrimination of continual learners. Fig. 4 illustrates the t-SNE visualization [46] of the memory data's embedding space at the end of the training. We can see that the feature representation of the method with CCL-DC is more discriminative compared with the baseline."
  - [supplementary material] "Table 6 demonstrates that CCL-DC can greatly enhance the NCM accuracy, which evidences the capability of CCL-DC in improving feature discrimination."
- **Break condition**: If the feature space becomes too fragmented across tasks, the model may lose the ability to transfer knowledge between tasks.

## Foundational Learning

- **Concept**: Catastrophic forgetting and plasticity trade-off
  - **Why needed here**: Online CL methods typically focus on preventing forgetting (stability) but neglect the ability to learn new tasks (plasticity), which CCL-DC addresses.
  - **Quick check question**: What is the difference between stability and plasticity in the context of continual learning?

- **Concept**: Knowledge distillation and entropy regularization
  - **Why needed here**: CCL-DC uses distillation between peer learners and entropy regularization via the distillation chain to improve performance.
  - **Quick check question**: How does distilling from less confident to more confident predictions act as entropy regularization?

- **Concept**: Data augmentation for curriculum learning
  - **Why needed here**: The distillation chain uses augmented samples of varying difficulty to implement entropy regularization.
  - **Quick check question**: Why might harder augmented samples produce less confident predictions?

## Architecture Onboarding

- **Component map**: Two neural networks (peer learners) -> Data augmentation pipeline (geometric distortion + RandAugment) -> Forward pass through both networks -> Compute losses (baseline + CCL + DC) -> Backward pass -> Parameter update

- **Critical path**: Data → Augmentation chain → Forward pass through both networks → Compute losses (baseline + CCL + DC) → Backward pass → Parameter update

- **Design tradeoffs**:
  - Computational cost doubles (two networks) but ensemble inference can recover some of this cost
  - More hyperparameters to tune (augmentation parameters, distillation weights)
  - Potential for correlated errors between peers if not properly initialized

- **Failure signatures**:
  - Performance worse than baseline: likely due to poor hyperparameter choices or correlated errors between peers
  - One network consistently outperforms the other: suggests imbalance in the collaborative learning
  - Degradation in stability metrics: indicates the entropy regularization may be too strong

- **First 3 experiments**:
  1. Run CCL-DC with a simple baseline (like ER) on CIFAR-10 with minimal augmentation to verify the basic collaborative learning mechanism works
  2. Test different distillation weight (λ2) values to find the optimal balance between CCL and baseline losses
  3. Evaluate the effect of different augmentation stages in the distillation chain on both performance and training stability

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions. However, based on the limitations and unresolved aspects discussed in the paper, several important open questions emerge:

1. How does the Distillation Chain (DC) mechanism specifically improve plasticity in online continual learning beyond the benefits of collaborative learning alone?
2. What is the optimal number of augmentation stages in the Distillation Chain (DC) for maximizing plasticity gains without excessive computational overhead?
3. How does the Collaborative Continual Learning (CCL) framework affect catastrophic forgetting differently across various online continual learning methods?
4. Can the principles of Collaborative Continual Learning and Distillation Chain be extended to other continual learning scenarios beyond class-incremental learning?
5. What is the relationship between the entropy of predictions during training and the final model performance in online continual learning?

## Limitations

- Computational overhead doubles due to two peer networks, though ensemble inference can recover some cost
- The exact implementation details of the Distillation Chain augmentation pipeline remain unclear
- The method doesn't extensively explore initialization strategies for peer learner diversity

## Confidence

- **High confidence**: The core mechanism of collaborative learning between two peer networks is well-established and the experimental results are convincing
- **Medium confidence**: The entropy regularization claims through distillation chain, while theoretically sound, could benefit from more ablation studies on augmentation parameters
- **Medium confidence**: The feature discrimination improvements shown through t-SNE and NCM accuracy are promising but correlational rather than causal

## Next Checks

1. Conduct ablation studies on the number of augmentation stages in the distillation chain to determine the optimal balance between regularization strength and performance
2. Test CCL-DC with different peer network initialization strategies (e.g., random vs pretrained) to assess robustness to initialization
3. Evaluate the method's performance when computational resources are constrained by using knowledge distillation from an ensemble of peers to a single network