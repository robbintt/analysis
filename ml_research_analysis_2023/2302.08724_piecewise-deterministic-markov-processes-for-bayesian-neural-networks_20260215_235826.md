---
ver: rpa2
title: Piecewise Deterministic Markov Processes for Bayesian Neural Networks
arxiv_id: '2302.08724'
source_url: https://arxiv.org/abs/2302.08724
tags:
- event
- boomerang
- these
- predictive
- posterior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes Piecewise Deterministic Markov Processes (PDMPs)
  for Bayesian Neural Networks (BNNs), which can improve predictive accuracy, MCMC
  mixing performance and provide informative uncertainty measurements when compared
  against other approximate inference schemes. The main idea is to introduce a new
  generic and adaptive thinning scheme for sampling from the Inhomogeneous Poisson
  Process (IPP) that controls the dynamics of the PDMP samplers.
---

# Piecewise Deterministic Markov Processes for Bayesian Neural Networks

## Quick Facts
- arXiv ID: 2302.08724
- Source URL: https://arxiv.org/abs/2302.08724
- Reference count: 40
- Primary result: Adaptive thinning for IPP sampling in PDMPs improves predictive accuracy, MCMC mixing, and uncertainty estimation for Bayesian neural networks

## Executive Summary
This work introduces Piecewise Deterministic Markov Processes (PDMPs) as a scalable approach for Bayesian inference in neural networks. The core innovation is an adaptive thinning scheme for efficiently sampling from inhomogeneous Poisson processes that control PDMP dynamics. The method demonstrates improved predictive accuracy, better calibration, and enhanced posterior exploration compared to standard stochastic gradient MCMC methods. Experiments across synthetic and real-world datasets validate the effectiveness of the proposed approach.

## Method Summary
The paper proposes using PDMPs for Bayesian neural network inference, introducing a novel adaptive thinning scheme for sampling event times from inhomogeneous Poisson processes. The method constructs piecewise-linear envelope functions to bound event rates, enabling efficient thinning-based sampling. The approach is applied to several PDMP samplers including the Bouncy Particle Sampler, preconditioned variants, and the Boomerang sampler. A covariance-based preconditioning strategy is employed to improve posterior exploration, with warm-up samples used to estimate the posterior covariance structure.

## Key Results
- The adaptive thinning method significantly accelerates event sampling in PDMPs for Bayesian neural networks
- Boomerang sampler with learned reference measure consistently outperforms other samplers in calibration, NLL, and effective sample size
- PDMP-based methods show competitive or improved predictive accuracy compared to SGLD across multiple benchmark datasets
- The proposed approach provides informative uncertainty measurements while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive thinning efficiently samples event times from IPPs in PDMP samplers for BNNs
- Mechanism: Constructs piecewise-linear envelope functions as approximate upper bounds for event rates, enabling efficient thinning-based sampling
- Core assumption: Envelope remains a strict upper bound on the true event rate with high probability
- Evidence anchors: Abstract and section 3.2 describe the adaptive thinning scheme and its efficiency benefits
- Break condition: Envelope failure to remain an upper bound increases computation and may bias inference

### Mechanism 2
- Claim: Covariance-based preconditioning improves posterior exploration and mixing
- Mechanism: Uses estimated posterior covariance from warm-up samples to scale gradients and velocities
- Core assumption: Posterior covariance can be reliably estimated from warm-up samples
- Evidence anchors: Section 2.2 and section 6 discuss preconditioning and its effects on mixing
- Break condition: Poor covariance estimation or rapidly changing posterior geometry degrades mixing

### Mechanism 3
- Claim: Boomerang sampler's non-linear dynamics and learned reference measure yield superior calibration
- Mechanism: Empirical Bayes reference measure tailored to posterior structure enables more effective exploration
- Core assumption: Empirical reference measure captures sufficient posterior structure
- Evidence anchors: Abstract and section 5.2 demonstrate Boomerang's superior calibration and ESS
- Break condition: Poor reference measure estimation reduces mixing advantage

## Foundational Learning

- Concept: Piecewise Deterministic Markov Processes (PDMPs)
  - Why needed here: Enable exact Bayesian inference with subsampling, avoiding high computational cost of traditional MCMC
  - Quick check question: How do PDMPs differ from standard MCMC in trajectory construction and event handling?

- Concept: Inhomogeneous Poisson Process (IPP) thinning
  - Why needed here: PDMP samplers require sampling event times from IPPs; adaptive thinning makes this feasible
  - Quick check question: Why is constructing an upper bound envelope necessary for thinning-based IPP sampling?

- Concept: Effective Sample Size (ESS) and calibration metrics
  - Why needed here: Evaluate sampling efficiency and predictive reliability for method comparison
  - Quick check question: How does PCA-based ESS estimation handle high-dimensional BNN posteriors?

## Architecture Onboarding

- Component map:
  - Sampler core (BPS, σBPS, Boomerang) → event rate computation → adaptive thinning → state update → velocity refresh (if applicable)
  - Warm-up (MAP estimate → covariance estimation) → preconditioning matrix application
  - Evaluation (predictive accuracy, NLL, ECE, ESS, ACF diagnostics)

- Critical path:
  1. MAP estimate → warm-up samples → covariance estimation
  2. Set velocity distribution (σBPS: N(0,σ²); Boomerang: N(0,Σ⋆))
  3. For each PDMP segment: compute event rate, sample via thinning, update state
  4. Periodically refresh velocity (if applicable)
  5. Collect samples for posterior inference

- Design tradeoffs:
  - Envelope tightness vs. computational cost (scaling factor α)
  - Velocity distribution variance: too small → poor mixing; too large → divergence
  - Refresh rate: too high → excessive refreshments; too low → poor exploration
  - Warm-up length: longer → better preconditioner; shorter → faster start but possibly worse mixing

- Failure signatures:
  - High rejection rates in thinning → envelope too loose or event rate mis-specified
  - Low ESS or poor mixing in trace/ACF plots → velocity/refreshment mis-configured or covariance stale
  - Poor calibration (high ECE) → sampler not exploring posterior adequately or prior misspecified
  - Memory/time blowup → mini-batch size or thinning inefficiency

- First 3 experiments:
  1. Run σBPS on small fully-connected BNN regression; tune α and σ, monitor thinning acceptance
  2. Compare BPS vs Boomerang on synthetic binary classification BNN; check calibration (ECE) and mixing (ESS, ACF)
  3. Scale up to LeNet5 on MNIST; evaluate predictive accuracy, NLL, and OOD detection; adjust refresh rate λref

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the tightness of the proposed adaptive linear envelope affect the quality of posterior inference in PDMPs?
- Basis in paper: [explicit] Paper discusses adaptive piecewise-linear envelope and its impact on efficiency
- Why unresolved: Only investigates empirically for few scaling factors; lacks theoretical bias-variance analysis
- What evidence would resolve it: Formal bias-variance decomposition under different envelope tightness levels, validated across multiple BNN architectures

### Open Question 2
- Question: What is the optimal choice of refreshment rate and velocity distribution to maximize mixing and minimize bias?
- Basis in paper: [explicit] Discusses sensitivity to these hyperparameters but lacks systematic method
- Why unresolved: Shows sensitivity through examples but no principled hyperparameter tuning method
- What evidence would resolve it: Systematic hyperparameter optimization study comparing different tuning strategies across multiple BNN tasks

### Open Question 3
- Question: How does computational complexity scale with increasing network depth and width in BNNs?
- Basis in paper: [inferred] Focuses on relatively small BNNs; doesn't investigate scalability
- Why unresolved: Doesn't explore scalability to larger architectures or provide theoretical complexity analysis
- What evidence would resolve it: Empirical studies on increasingly large BNN architectures and theoretical complexity analysis

## Limitations
- Computational overhead of adaptive envelope construction relative to standard thinning is not quantitatively compared
- Stability and sensitivity of empirical Bayes reference measure across different data regimes is not thoroughly investigated
- Optimal hyperparameter ranges for scaling factor α are not fully explored across all experimental settings

## Confidence

- High confidence in: Theoretical validity of adaptive thinning method and its implementation for PDMP-based BNN inference
- Medium confidence in: Relative performance improvements (predictive accuracy, calibration, ESS) across datasets
- Medium confidence in: Claim that adaptive thinning accelerates PDMP application for BNNs

## Next Checks

1. **Envelope tightness validation**: Systematically vary scaling factor α and measure rejection rates and computational overhead to establish optimal ranges
2. **Warm-up sensitivity analysis**: Test stability of Boomerang's empirical reference measure by varying warm-up length and initial conditions, measuring calibration and ESS sensitivity
3. **Mini-batch size ablation**: Conduct controlled experiments varying mini-batch sizes to isolate impact of adaptive thinning efficiency from subsampling benefits