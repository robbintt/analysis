---
ver: rpa2
title: Demystifying the Myths and Legends of Nonconvex Convergence of SGD
arxiv_id: '2310.12969'
source_url: https://arxiv.org/abs/2310.12969
tags:
- convergence
- nonconvex
- stochastic
- theorem
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the convergence behavior of stochastic gradient\
  \ descent (SGD) for nonconvex optimization problems. While prior work shows SGD\
  \ finds an \u03B5-stationary point somewhere among all iterations, this paper proves\
  \ such a point exists in the final \u03B7T iterations given a large enough iteration\
  \ budget T."
---

# Demystifying the Myths and Legends of Nonconvex Convergence of SGD

## Quick Facts
- arXiv ID: 2310.12969
- Source URL: https://arxiv.org/abs/2310.12969
- Reference count: 40
- This paper proves that ε-stationary points exist in the final ηT iterations of SGD for nonconvex optimization, with density approaching 1 as T increases.

## Executive Summary
This paper addresses a fundamental gap in nonconvex SGD convergence theory by proving that ε-stationary points not only exist somewhere among all iterations but are concentrated in the final ηT iterations with high probability. The authors establish that as the total iteration budget T grows, the density of ε-stationary points in the tail approaches 1, providing a stronger convergence guarantee than existing results. Through theoretical analysis and experiments on logistic regression with nonconvex penalties and neural networks, the paper demonstrates that running SGD for more iterations progressively increases the frequency of ε-stationary points in final iterates. The work also identifies open questions about practical detection of convergence and the largest class of nonconvex functions guaranteeing last-iterate convergence.

## Method Summary
The paper analyzes SGD convergence for nonconvex optimization by establishing descent inequalities under expected smoothness assumptions and bounding gradient norms across iterations. The key mechanism involves proving that a weighted sum of gradient norms over all iterations can be bounded from below by a partial sum over the last ηT iterations. This concentration analysis shows that as T increases, the density of ε-stationary points in the tail approaches 1. The authors recover classical O(1/√T) rates under various assumptions and validate their theory through experiments on logistic regression with nonconvex penalties and feed-forward neural networks with ReLU activation on MNIST.

## Key Results
- Proves existence of ε-stationary points in the final ηT iterations of SGD with high probability
- Shows density of ε-stationary points in the tail approaches 1 as T increases
- Recovers classical O(1/√T) convergence rate under various assumptions
- Experiments confirm theoretical predictions on logistic regression and neural networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SGD's final ηT iterations contain ε-stationary points with high probability
- Mechanism: By bounding a weighted sum of gradient norms over all iterations from below by a partial sum over the last ηT iterations, the paper proves that as T increases, the density of ε-stationary points in the tail approaches 1
- Core assumption: The expected smoothness assumption (Assumption 3) holds for the stochastic gradients
- Evidence anchors:
  - [abstract]: "we can measure the density of the ε-stationary points in the final iterates of SGD—A first standalone result"
  - [section 4]: "we can measure the concentration of the ε-stationary points over the tail portion for the SGD iterates, xt is almost 1 for large T"
  - [corpus]: Weak - no direct supporting evidence found
- Break condition: If the expected smoothness assumption fails or if the step size sequence doesn't satisfy the required bounds

### Mechanism 2
- Claim: Classical O(1/√T) convergence rate is recovered under various assumptions
- Mechanism: By carefully choosing the step size (constant or decreasing) and analyzing the descent inequality, the paper shows that the minimum of the expected gradient norm over the last ηT iterations approaches ε with rate O(1/√T)
- Core assumption: The step size sequence satisfies the required bounds (e.g., γt ≤ 1/LB for constant step size)
- Evidence anchors:
  - [abstract]: "we recover the classical O(1/√T) asymptotic rate under various existing assumptions"
  - [section 4]: "we can recover the classic O(1/√T) convergence rate of nonconvex SGD under no additional assumptions"
  - [corpus]: Weak - no direct supporting evidence found
- Break condition: If the step size sequence doesn't satisfy the required bounds or if the assumptions on the objective function don't hold

### Mechanism 3
- Claim: The density of ε-stationary points in the tail approaches 1 as T increases
- Mechanism: By using the descent inequality and carefully bounding the quantities on both sides, the paper shows that the number of ε-stationary points in the last ηT iterations divided by ηT approaches 1 as T increases
- Core assumption: The step size sequence satisfies the required bounds and the expected smoothness assumption holds
- Evidence anchors:
  - [abstract]: "we can measure the concentration of the ε-stationary points in the final iterates of SGD—A first standalone result"
  - [section 4]: "the density of the ε-stationary points in the top η portion of the tails approaches 1 as T increases"
  - [corpus]: Weak - no direct supporting evidence found
- Break condition: If the step size sequence doesn't satisfy the required bounds or if the assumptions on the objective function don't hold

## Foundational Learning

- Concept: Expected smoothness assumption
  - Why needed here: This assumption is crucial for bounding the second moment of the stochastic gradient and deriving the descent inequality
  - Quick check question: What is the expected smoothness assumption and why is it important for the convergence analysis of SGD?

- Concept: Descent inequality
  - Why needed here: The descent inequality is the key tool for proving the convergence of SGD and bounding the minimum of the expected gradient norm
  - Quick check question: What is the descent inequality and how is it used to prove the convergence of SGD?

- Concept: Step size sequence
  - Why needed here: The step size sequence plays a crucial role in determining the convergence rate and the existence of ε-stationary points in the final iterations
  - Quick check question: How does the choice of step size sequence affect the convergence rate and the existence of ε-stationary points in the final iterations?

## Architecture Onboarding

- Component map:
  Assumptions (Assumption 1, 2, 3) -> Descent inequality -> Step size sequence -> Concentration analysis -> Existence of ε-stationary points in final ηT iterations

- Critical path:
  1. Establish the assumptions on the objective function and the stochastic gradients
  2. Derive the descent inequality using the expected smoothness assumption
  3. Analyze the descent inequality to prove the existence of ε-stationary points in the final iterations
  4. Use the concentration analysis to show that the density of ε-stationary points in the tail approaches 1 as T increases

- Design tradeoffs:
  - Stronger assumptions on the objective function (e.g., convexity) can lead to faster convergence but may not hold in practice
  - Weaker assumptions (e.g., expected smoothness) are more general but may lead to slower convergence
  - The choice of step size sequence affects the convergence rate and the existence of ε-stationary points in the final iterations

- Failure signatures:
  - If the assumptions on the objective function or the stochastic gradients don't hold, the convergence analysis may fail
  - If the step size sequence doesn't satisfy the required bounds, the existence of ε-stationary points in the final iterations may not be guaranteed
  - If the concentration analysis is not performed correctly, the density of ε-stationary points in the tail may not approach 1 as T increases

- First 3 experiments:
  1. Verify that the assumptions on the objective function and the stochastic gradients hold for a simple nonconvex function
  2. Implement the SGD algorithm with a constant step size and verify the existence of ε-stationary points in the final iterations
  3. Implement the SGD algorithm with a decreasing step size and verify the convergence rate and the existence of ε-stationary points in the final iterations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: For SGD, without an iteration budget, T, are there practical ways to detect the first iterate, t, for which E||∇f(xt)||² ≤ ε?
- Basis in paper: [explicit] The paper poses this as an open question, noting that current nonconvex convergence analysis of SGD only guarantees the existence of ε-stationary points but doesn't provide practical detection methods.
- Why unresolved: Current theory provides asymptotic guarantees but doesn't address the practical challenge of identifying when convergence criteria are met during training.
- What evidence would resolve it: A method that can reliably predict or detect when the gradient norm falls below ε without prior knowledge of T, validated on multiple nonconvex problems.

### Open Question 2
- Question: How to terminate SGD other than the maximum number of iterations?
- Basis in paper: [explicit] The paper asks this as a follow-up to the first open question, recognizing that maximum iterations is an impractical stopping criterion.
- Why unresolved: Current practice relies on fixed iteration budgets or heuristic early stopping, but these lack theoretical justification and may waste computation or terminate prematurely.
- What evidence would resolve it: A theoretically grounded stopping criterion that balances computational efficiency with convergence guarantees, tested across different problem types.

### Open Question 3
- Question: What is the biggest class of nonconvex functions for which we can still have convergence guarantees on the last iterate of SGD?
- Basis in paper: [explicit] The paper references Yu et al. (2021) who assume dissipativity property and asks about the maximal class of functions for which similar guarantees hold.
- Why unresolved: Current theory focuses on limited function classes and there's a gap between theoretical assumptions and practical loss landscapes encountered in deep learning.
- What evidence would resolve it: A characterization of function properties that enable last-iterate convergence, validated on a broad range of nonconvex problems including deep neural networks.

## Limitations
- The expected smoothness assumption may not hold for all nonconvex functions, particularly those with irregular gradient structures
- The concentration result showing density approaches 1 may be sensitive to the choice of η and step size sequence
- The practical significance of the concentration result may not translate to meaningful performance improvements in real-world applications

## Confidence

**High Confidence**: The claim that SGD finds an ε-stationary point in the final ηT iterations, assuming the expected smoothness assumption holds and the step size sequence satisfies the required bounds.

**Medium Confidence**: The recovery of the O(1/√T) convergence rate under various assumptions, as this depends on the specific choice of step size sequence and the properties of the objective function.

**Low Confidence**: The practical significance of the concentration result, as the theoretical density approaching 1 may not translate to meaningful performance improvements in real-world applications.

## Next Checks

1. **Assumption Verification**: Rigorously verify the expected smoothness assumption for the specific nonconvex functions used in the experiments, including logistic regression with nonconvex penalties and neural networks.

2. **Hyperparameter Sensitivity**: Conduct a thorough analysis of the sensitivity of the results to the choice of η and the step size sequence, exploring a wider range of values to understand the robustness of the concentration result.

3. **Practical Impact Assessment**: Design experiments to assess the practical impact of the concentration result on the performance of SGD in real-world nonconvex optimization tasks, comparing the final iterates of SGD with different iteration budgets.