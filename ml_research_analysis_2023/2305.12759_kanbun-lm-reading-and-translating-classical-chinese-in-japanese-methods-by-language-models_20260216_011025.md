---
ver: rpa2
title: 'Kanbun-LM: Reading and Translating Classical Chinese in Japanese Methods by
  Language Models'
arxiv_id: '2305.12759'
source_url: https://arxiv.org/abs/2305.12759
tags:
- chinese
- japanese
- kanbun
- classical
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the scarcity of language resources for Kanbun
  (Japanese reading and translation methods for Classical Chinese) by constructing
  the first Classical-Chinese-to-Kanbun dataset and introducing two tasks: character
  reordering and machine translation. The authors fine-tune various language models
  on these tasks, achieving state-of-the-art results that significantly improve upon
  the baseline method.'
---

# Kanbun-LM: Reading and Translating Classical Chinese in Japanese Methods by Language Models

## Quick Facts
- arXiv ID: 2305.12759
- Source URL: https://arxiv.org/abs/2305.12759
- Reference count: 6
- Key outcome: Character reordering and machine translation models significantly outperform baseline methods for Kanbun tasks

## Executive Summary
This paper addresses the scarcity of language resources for Kanbun (Japanese reading and translation methods for Classical Chinese) by constructing the first Classical-Chinese-to-Kanbun dataset and introducing two tasks: character reordering and machine translation. The authors fine-tune various language models on these tasks, achieving state-of-the-art results that significantly improve upon the baseline method. For character reordering, models like RoBERTa-classical-chinese-char achieved 94.7% better Perfect Match Ratio and 22.5% better Kendall's Tau compared to the baseline. For machine translation, models like mT5-large outperformed the baseline across multiple automatic and manual evaluation metrics. The paper also discusses the best evaluation method for Classical-Chinese-to-Kanbun translation by comparing automatic metrics with human scores, finding BERTScore to have the highest correlation.

## Method Summary
The authors construct a Classical-Chinese-to-Kanbun dataset of 465 poems and introduce two tasks: character reordering (converting SVO Classical Chinese to SOV Japanese word order) and machine translation (adding Okurigana to produce Kanbun texts). For character reordering, they fine-tune BERT-like models to predict rank positions for each character. For machine translation, they fine-tune mT5 and mGPT models for sequence-to-sequence generation. The pipeline combines both tasks, with character reordering preceding translation. Evaluation includes automatic metrics (BLEU, RIBES, ROUGE, BERTScore) and human evaluation on relevance, accuracy, and fluency using 5-point scales.

## Key Results
- Character reordering models improved Perfect Match Ratio by 94.7% and Kendall's Tau by 22.5% over baseline
- mT5-large outperformed baseline across all automatic metrics for machine translation
- BERTScore showed highest correlation with human evaluation scores among automatic metrics
- RoBERTa-classical-chinese-char performed best on character reordering task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Character reordering models significantly improve translation accuracy by converting SVO (Classical Chinese) to SOV (Japanese) word order before translation.
- Mechanism: The character reordering module predicts the rank of each character in the sentence, effectively transforming the word order from SVO to SOV. This reordering aligns the sentence structure with Japanese syntax, making subsequent translation more accurate.
- Core assumption: The character reordering task is sufficiently well-defined and learnable from the dataset, and the reordering significantly impacts translation quality.
- Evidence anchors:
  - [abstract] "We introduce two tasks, character reordering and machine translation, both of which play a significant role in Kanbun comprehension."
  - [section] "character reordering, the goal is to transform Classical Chinese texts into Japanese reading orders, from SVO to SOV"
  - [corpus] Found 25 related papers, average neighbor FMR=0.313, suggesting moderate relatedness in the corpus, but direct evidence for this specific mechanism is limited.
- Break condition: If the dataset is too small or lacks diversity, the model may not learn effective reordering patterns, leading to poor translation performance.

### Mechanism 2
- Claim: BERTScore is the most suitable automatic evaluation metric for Classical-Chinese-to-Kanbun translation due to its higher correlation with human evaluation metrics.
- Mechanism: BERTScore uses contextual embeddings to compare candidate and reference sentences, capturing semantic similarity beyond exact n-gram matches. This makes it more robust for evaluating translations where word order and morphological changes are significant.
- Core assumption: BERTScore's embedding-based approach better captures the nuances of Classical Chinese to Kanbun translation compared to n-gram or rank-based metrics.
- Evidence anchors:
  - [abstract] "The paper also discusses the best evaluation method for Classical-Chinese-to-Kanbun translation by comparing the results with human scores, finding BERTScore to have the highest correlation."
  - [section] "BERTScore has the greatest correlation with all three human evaluation metrics. BLEU and ROUGE-L also performed well."
  - [corpus] The corpus shows moderate relatedness, but specific evidence for BERTScore's superiority in this domain is limited.
- Break condition: If the BERTScore model is not well-suited for Classical Chinese or Kanbun, or if the human evaluation criteria change, BERTScore may no longer be the best metric.

### Mechanism 3
- Claim: Pre-trained language models on Classical Chinese corpora (like RoBERTa-classical-chinese-char) perform better on Kanbun tasks than models trained on modern Chinese or Japanese corpora.
- Mechanism: Pre-training on Classical Chinese corpora exposes the model to the linguistic features and vocabulary of ancient texts, which are more relevant to Kanbun than modern language corpora. This specialized pre-training leads to better performance on both character reordering and machine translation tasks.
- Core assumption: The linguistic features and vocabulary of Classical Chinese are sufficiently similar to Kanbun to provide a strong foundation for learning, and the pre-training data is representative of the tasks.
- Evidence anchors:
  - [abstract] "We also test the current language models on these tasks and discuss the best evaluation method by comparing the results with human scores."
  - [section] "RoBERTa-classical-chinese-char, which was pre-trained on the ancient Chinese corpus, performed the best."
  - [corpus] The corpus includes papers on Classical Chinese NLP, suggesting some relevance, but direct evidence for this specific mechanism is limited.
- Break condition: If the pre-training data is not representative of the tasks or if the model overfits to the pre-training corpus, performance may suffer.

## Foundational Learning

- Concept: Word order differences between Classical Chinese (SVO) and Japanese (SOV)
  - Why needed here: Understanding the word order differences is crucial for the character reordering task and for appreciating why reordering is necessary for accurate translation.
  - Quick check question: What is the word order in Classical Chinese and Japanese, and why does this difference necessitate character reordering?

- Concept: Morphological differences between isolating languages (Classical Chinese) and agglutinative languages (Japanese)
  - Why needed here: Recognizing the morphological differences helps understand why Okurigana is needed in Kanbun and why machine translation is a complex task.
  - Quick check question: How do isolating and agglutinative languages differ in terms of morphological structure, and how does this impact the translation from Classical Chinese to Kanbun?

- Concept: Evaluation metrics for machine translation (BLEU, RIBES, ROUGE, BERTScore)
  - Why needed here: Understanding these metrics is essential for interpreting the results and comparing the performance of different models.
  - Quick check question: What are the key differences between BLEU, RIBES, ROUGE, and BERTScore, and why might BERTScore be more suitable for evaluating Classical-Chinese-to-Kanbun translation?

## Architecture Onboarding

- Component map:
  - Character Reordering Module: Uses BERT-like models to predict character ranks and reorder sentences from SVO to SOV
  - Machine Translation Module: Uses T5 or GPT models to translate reordered sentences into Kanbun by adding Okurigana
  - Pipeline: Combines the reordering and translation modules for end-to-end translation
  - Evaluation Metrics: Includes automatic metrics (BLEU, RIBES, ROUGE, BERTScore) and human evaluation for relevance, accuracy, and fluency

- Critical path:
  1. Preprocess Classical Chinese text into characters with indices
  2. Apply character reordering model to predict ranks and reorder characters
  3. Pass reordered text to machine translation model
  4. Generate Kanbun translation
  5. Evaluate using automatic and human metrics

- Design tradeoffs:
  - Character-based vs. word-based tokenization: Character-based is chosen for precise position prediction but may miss word-level context
  - Multilingual vs. monolingual models: Multilingual models offer broader coverage but may not tokenize Japanese characters effectively
  - Automatic vs. human evaluation: Automatic metrics are efficient but may not capture all nuances of translation quality

- Failure signatures:
  - Low PMR scores in character reordering indicate incorrect character positions
  - High BLEU but low human evaluation scores suggest metric misalignment
  - Repetition of characters in machine translation output indicates model generation issues

- First 3 experiments:
  1. Fine-tune a character reordering model (e.g., RoBERTa-japanese-char) on a small subset of the dataset and evaluate using Kendall's Tau and PMR
  2. Fine-tune a machine translation model (e.g., mT5-small) on the same subset and evaluate using BLEU, RIBES, and ROUGE
  3. Combine the reordering and translation models in a pipeline and compare performance to direct translation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would pre-training a character-based multilingual model (combining both Chinese and Japanese corpora) outperform the current best-performing models (RoBERTa-classical-chinese-char) for the character reordering task?
- Basis in paper: [inferred] The paper notes that the score gap between Chinese and Japanese models "originates from the pre-training corpus rather than the reading orders of the pre-training languages" and mentions this validation was left for future work.
- Why unresolved: The paper states that "the existing multilingual models cannot guarantee to tokenize an input text into characters" which prevents testing this hypothesis.
- What evidence would resolve it: Developing a truly character-based multilingual model that can handle both Chinese and Japanese characters, then testing its performance on the character reordering task against current models.

### Open Question 2
- Question: What is the upper bound of human character reordering accuracy for Classical Chinese to Japanese reading order conversion?
- Basis in paper: [explicit] The paper mentions the need to "find the upper bound of human character reordering accuracy" in future work.
- Why unresolved: The current human evaluation used annotators who are "not experts in Classical Chinese" and the paper suggests collaboration with real experts for future experiments.
- What evidence would resolve it: Conducting character reordering experiments with Classical Chinese experts and comparing their performance against the best language model (RoBERTa-classical-chinese-char) to determine the performance gap.

### Open Question 3
- Question: What is the optimal evaluation metric for Classical-Chinese-to-Kanbun translation that balances relevance, accuracy, and fluency?
- Basis in paper: [explicit] The paper discusses finding "the best evaluation method for Classical-Chinese-to-Kanbun translation" and notes that "we hope to reduce the correlation by discussing with Classical Chinese experts and reformulating the manual evaluation metrics in future work."
- Why unresolved: Current manual evaluation uses three separate metrics (relevance, accuracy, fluency) with high correlation between them, and automatic metrics only show moderate to strong correlation with human judgments.
- What evidence would resolve it: Developing a single, unified evaluation metric through expert consultation that captures all three aspects (relevance, accuracy, fluency) and testing its correlation with both human judgments and model outputs.

### Open Question 4
- Question: Does the encoder module in mT5 have a significant role in comprehending Classical Chinese compared to mGPT's architecture?
- Basis in paper: [inferred] The paper speculates that "the encoder modules in mT5 have a significant role in comprehending Classical Chinese" as a potential reason why mGPT underperforms despite having more parameters.
- Why unresolved: This remains a hypothesis stated in the paper that requires further experimental validation.
- What evidence would resolve it: Conducting ablation studies comparing mT5 with different encoder/decoder configurations against mGPT, or comparing encoder-only vs decoder-only models on the machine translation task.

## Limitations

- The dataset size (465 poems total) is relatively small for training robust language models
- The study focuses exclusively on poetic texts, which may not represent the full diversity of Kanbun applications
- Limited exploration of how performance scales with dataset size

## Confidence

- High confidence: The effectiveness of character reordering for improving translation accuracy from SVO to SOV word order, supported by strong quantitative results (94.7% improvement in PMR, 22.5% in Kendall's Tau)
- Medium confidence: The superiority of BERTScore for evaluation, as the correlation with human scores is demonstrated but the analysis is limited to this specific dataset
- Medium confidence: The advantage of Classical Chinese pre-training over modern language models, as results show RoBERTa-classical-chinese-char performing best, but the comparison includes only a limited set of pre-trained models

## Next Checks

1. **Dataset scaling experiment**: Train models on progressively larger subsets (10%, 25%, 50%, 75%, 100%) of the current dataset to empirically determine the minimum dataset size required for stable performance and identify any diminishing returns

2. **Cross-genre validation**: Test the trained models on non-poetic Classical Chinese texts (e.g., philosophical writings, historical records) to evaluate generalizability beyond the poetic domain used in training

3. **Human evaluation expansion**: Conduct a larger-scale human evaluation study with 10+ annotators and blind review conditions to verify the automatic metric correlations and assess inter-annotator agreement for relevance, accuracy, and fluency ratings