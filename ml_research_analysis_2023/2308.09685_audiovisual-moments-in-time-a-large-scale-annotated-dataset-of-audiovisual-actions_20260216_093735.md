---
ver: rpa2
title: 'Audiovisual Moments in Time: A Large-Scale Annotated Dataset of Audiovisual
  Actions'
arxiv_id: '2308.09685'
source_url: https://arxiv.org/abs/2308.09685
tags:
- uni0000004c
- audiovisual
- uni00000051
- uni0000004a
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Audiovisual Moments in Time (AVMIT), a dataset
  of 57,177 audiovisual action events annotated for the Moments in Time dataset. Eleven
  participants labelled whether the audiovisual event was present and the most prominent
  feature in each video.
---

# Audiovisual Moments in Time: A Large-Scale Annotated Dataset of Audiovisual Actions

## Quick Facts
- arXiv ID: 2308.09685
- Source URL: https://arxiv.org/abs/2308.09685
- Reference count: 34
- Key outcome: Training RNNs on audiovisual events increased top 1 accuracy by 2.71-5.94% compared to modality-agnostic training, even outweighing a three-fold increase in training data.

## Executive Summary
This paper introduces Audiovisual Moments in Time (AVMIT), a dataset of 57,177 audiovisual action events annotated for the Moments in Time dataset. The dataset includes annotations from eleven participants on whether audiovisual events are present and the most prominent feature in each video. A curated test set of 960 videos across 16 action classes was created. The authors provide pre-computed audiovisual feature embeddings using VGGish/YamNet for audio and VGG16/EfficientNetB0 for visual data. Six Recurrent Neural Networks (FRNN, GRU, LSTM) were trained on either AVMIT-filtered audiovisual events or modality-agnostic events from MIT, demonstrating that training on audiovisual events significantly improves action recognition accuracy.

## Method Summary
The authors created AVMIT by annotating videos from the existing MIT dataset with crowdsourced labels identifying audiovisual events. From this initial collection, they created a curated test set of 960 videos across 16 action classes. They provide two sets of pre-computed audiovisual feature embeddings using different CNN architectures for audio and visual modalities. Six RNN models were then trained on these embeddings using either AVMIT-filtered audiovisual events or all MIT-16 events, and evaluated on the curated test set to measure the impact of audiovisual filtering on recognition performance.

## Key Results
- Training exclusively on audiovisual events increased top 1 accuracy by 2.71-5.94% across all RNN architectures
- This improvement outweighed a three-fold increase in training data from using modality-agnostic events
- The curated test set ensures all videos contain the labeled audiovisual event as the most prominent feature

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Audiovisual correspondence improves event recognition accuracy over modality-agnostic training.
- Mechanism: By training RNNs exclusively on events where audio and visual signals are synchronized, the model learns stronger cross-modal associations than when trained on arbitrary combinations of audio and visual signals.
- Core assumption: The MIT dataset contains many modality-agnostic events where audio and visual signals do not correspond to the same event, and filtering these out improves learning.
- Evidence anchors:
  - [abstract] "In all RNNs, top 1 accuracy was increased by 2.71-5.94% by training exclusively on audiovisual events, even outweighing a three-fold increase in training data."
  - [section] "The A VMIT annotation scheme was run using videos from an existing dataset, A VMIT benefits from the quality assurances of two cleaning processes."
- Break condition: If the dataset contains too few high-quality audiovisual events, or if the filtering removes too much training data, the accuracy gain could diminish.

### Mechanism 2
- Claim: Providing pre-computed audiovisual embeddings lowers the barrier to entry for DNN research.
- Mechanism: Researchers can skip the computationally expensive feature extraction step and directly use the provided embeddings for training audiovisual models.
- Core assumption: The pre-computed embeddings are of sufficient quality and coverage for downstream tasks.
- Evidence anchors:
  - [abstract] "We also offer 2 sets of pre-computed audiovisual feature embeddings, using VGGish/YamNet for audio data and VGG16/EfficientNetB0 for visual data, thereby lowering the barrier to entry for audiovisual DNN research."
  - [section] "Finally, we provide DNN embeddings for A VMIT videos to lower the computational barriers for those who wish to train audiovisual DNNs, thereby levelling the playing field for all."
- Break condition: If the embeddings are outdated or do not match the target task, researchers may need to recompute them.

### Mechanism 3
- Claim: A controlled test set with human annotations enables comparative experiments between DNNs and human observers.
- Mechanism: The curated A VMIT test set ensures that all videos contain the labeled audiovisual event as the most prominent feature, providing a consistent benchmark for both human and model performance.
- Core assumption: The human annotation process accurately identifies videos with strong audiovisual correspondence.
- Evidence anchors:
  - [abstract] "From this initial collection, we created a curated test set of 16 distinct action classes, with 60 videos each (960 videos)."
  - [section] "With test classes obtained, we then applied video filtering... In order to ensure a level of homogeneity in the dataset, we obtained those audiovisual videos with a visual frame rate of 30fps and further cleaned them..."
- Break condition: If the test set is too small or not representative of real-world variability, it may not generalize well to other tasks.

## Foundational Learning

- Concept: Cross-modal learning and audiovisual correspondence
  - Why needed here: The paper relies on the principle that audio and visual signals from the same event provide complementary information that can improve recognition accuracy.
  - Quick check question: What is the difference between audiovisual events and modality-agnostic events in the context of this paper?

- Concept: Feature embeddings and transfer learning
  - Why needed here: The paper uses pre-trained CNNs (VGGish, YamNet, VGG16, EfficientNetB0) to extract embeddings, which are then used for training audiovisual models.
  - Quick check question: Why are pre-trained CNNs used for feature extraction instead of training from scratch?

- Concept: Recurrent Neural Networks (RNNs) and their variants (FRNN, GRU, LSTM)
  - Why needed here: The paper trains different RNN architectures on the audiovisual embeddings to perform action recognition.
  - Quick check question: What are the key differences between FRNN, GRU, and LSTM architectures, and why might one be preferred over the others?

## Architecture Onboarding

- Component map: VGGish/YamNet/EfficientNetB0 -> Multimodal squeeze unit -> RNN (FRNN/GRU/LSTM) -> Classification
- Critical path: Embedding extraction (done) -> Multimodal squeeze unit -> RNN -> Classification
- Design tradeoffs:
  - Using pre-trained CNNs saves computation but may limit adaptation to specific audiovisual tasks.
  - Freezing CNN layers prevents overfitting but also prevents fine-tuning for better performance.
  - Choosing between RNN variants involves balancing model complexity, training time, and performance.
- Failure signatures:
  - Low accuracy on the test set: Could indicate poor-quality embeddings, insufficient training data, or model overfitting.
  - High loss but low accuracy: Could indicate issues with the model architecture or hyperparameter settings.
  - Overfitting: Could indicate the need for more regularization, data augmentation, or a simpler model.
- First 3 experiments:
  1. Train an RNN on the A VMIT embeddings and evaluate on the test set to establish a baseline.
  2. Train an RNN on the MIT-16 embeddings (modality-agnostic) and compare performance to the A VMIT-trained model.
  3. Experiment with different RNN architectures (FRNN, GRU, LSTM) to determine which performs best on the audiovisual task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RNNs trained on AVMIT compare to those trained on modality-agnostic datasets when tested on audiovisual events not present in either training set?
- Basis in paper: [explicit] The paper compares RNNs trained on AVMIT-filtered audiovisual events vs. modality-agnostic events from MIT, showing a 2.71-5.94% increase in top 1 accuracy for AVMIT-trained models.
- Why unresolved: The study only tests on the AVMIT test set, which is derived from MIT. It doesn't explore generalization to completely new audiovisual events.
- What evidence would resolve it: Training and testing RNNs on entirely disjoint sets of audiovisual events, comparing AVMIT vs. modality-agnostic training approaches.

### Open Question 2
- Question: What is the optimal balance between audiovisual-specific and modality-agnostic training data for maximizing recognition performance on audiovisual events?
- Basis in paper: [inferred] The paper shows AVMIT-trained models outperform those trained on three times more modality-agnostic data, suggesting audiovisual-specific data is valuable. However, the absolute performance levels are not compared.
- Why unresolved: The study only compares pure audiovisual training vs. pure modality-agnostic training. It doesn't explore hybrid approaches.
- What evidence would resolve it: Systematically varying the ratio of audiovisual-specific to modality-agnostic training data and measuring recognition performance.

### Open Question 3
- Question: How do human observers' audiovisual integration abilities compare to the RNNs trained on AVMIT when tested on the same controlled audiovisual test set?
- Basis in paper: [explicit] The authors mention that AVMIT serves as a valuable resource for comparative experiments involving computational models and human participants, specifically when addressing research questions where audiovisual correspondence is of critical importance.
- Why unresolved: The paper only presents DNN results, not human behavioral data.
- What evidence would resolve it: Conducting psychophysical experiments with human participants using the AVMIT test set and comparing their performance to the RNNs.

## Limitations
- The 2.71-5.94% accuracy improvement is measured only on a small curated test set of 960 videos across 16 action classes, which may not reflect performance on broader or more diverse datasets.
- The pre-computed embeddings are based on specific pre-trained models (VGGish, YamNet, VGG16, EfficientNetB0) and may not capture all relevant audiovisual features for downstream tasks.
- The human annotation process for creating the test set introduces potential subjectivity in identifying "most prominent" audiovisual features.

## Confidence
- **High confidence**: The methodology for creating the AVMIT dataset through crowdsourcing and the availability of pre-computed embeddings are well-documented and reproducible.
- **Medium confidence**: The reported accuracy improvements from audiovisual filtering are promising but limited to a specific test set and may not generalize to all audiovisual recognition tasks.
- **Medium confidence**: The claim that pre-computed embeddings lower the barrier to entry is valid, but the actual impact on research accessibility requires further validation.

## Next Checks
1. Evaluate the AVMIT-trained models on additional test sets with different action classes or from different datasets to assess robustness beyond the curated 960-video test set.
2. Compare the performance of models using the provided pre-computed embeddings against models using embeddings extracted with updated or task-specific feature extractors to determine if the provided embeddings are optimal.
3. Conduct a smaller-scale human annotation study on a subset of AVMIT videos to verify the consistency and reliability of the original crowdsourcing annotations used to create the test set.