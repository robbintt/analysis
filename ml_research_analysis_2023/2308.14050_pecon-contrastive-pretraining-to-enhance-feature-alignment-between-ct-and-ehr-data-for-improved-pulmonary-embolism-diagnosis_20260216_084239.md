---
ver: rpa2
title: 'PECon: Contrastive Pretraining to Enhance Feature Alignment between CT and
  EHR Data for Improved Pulmonary Embolism Diagnosis'
arxiv_id: '2308.14050'
source_url: https://arxiv.org/abs/2308.14050
tags:
- pecon
- contrastive
- pretraining
- data
- pulmonary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PECon, a supervised contrastive pretraining
  method that enhances feature alignment between CT scans and EHR data to improve
  PE diagnosis. PECon employs a contrastive learning objective to pull together features
  from the same class while pushing away those from different classes.
---

# PECon: Contrastive Pretraining to Enhance Feature Alignment between CT and EHR Data for Improved Pulmonary Embolism Diagnosis

## Quick Facts
- arXiv ID: 2308.14050
- Source URL: https://arxiv.org/abs/2308.14050
- Reference count: 30
- F1-score of 0.913, accuracy of 0.90, and AUROC of 0.943 on RadFusion dataset

## Executive Summary
PECon introduces a supervised contrastive pretraining method to align feature representations between CT scans and EHR data for improved Pulmonary Embolism diagnosis. The method uses a contrastive learning objective to pull together features from the same class while pushing away those from different classes. When evaluated on the RadFusion dataset, PECon achieves state-of-the-art performance with F1-score of 0.913, accuracy of 0.90, and AUROC of 0.943, while also demonstrating better feature localization through class activation maps compared to existing methods.

## Method Summary
PECon employs supervised contrastive pretraining with a frozen PENet backbone to align CT and EHR features into a shared embedding space. The method processes 3D CT sub-volumes through PENet and EHR data through separate MLPs, then applies a supervised contrastive loss to align same-class samples. After pretraining, modality-specific classification heads are fine-tuned and combined using a weighted sum (λ=0.375) for final predictions. The approach uses late fusion strategy and is trained on the RadFusion dataset with 1,837 CTPA exams.

## Key Results
- Achieves F1-score of 0.913, accuracy of 0.90, and AUROC of 0.943 on RadFusion dataset
- Demonstrates improved feature localization through Grad-CAM visualizations compared to PENet baseline
- Shows superior performance across all metrics compared to existing PE diagnosis methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised contrastive pretraining aligns multimodal embeddings by pulling same-class pairs closer and pushing different-class pairs apart in a shared embedding space.
- Mechanism: For each anchor (CT or EHR embedding), the loss function encourages attraction to all embeddings of the same class and repulsion from embeddings of the opposite class. This increases inter-class separability and intra-class cohesion across modalities.
- Core assumption: Class labels are reliable and consistent between CT and EHR modalities.
- Evidence anchors:
  - [abstract]: "pull the sample features of the same class together, while pushing away those of the other class"
  - [section]: "for each anchor feature... all other features having the same label as this feature in the batch are pulled to it, and all those having the opposite label are pushed away"
  - [corpus]: Weak; no direct match to multimodal supervised contrastive alignment in the neighbor list, though "Revolutionizing Precise Low Back Pain Diagnosis via Contrastive Learning" is thematically adjacent.
- Break condition: If class labels are noisy or multimodal misalignment exists (e.g., same patient has conflicting labels in CT vs EHR), the alignment would collapse.

### Mechanism 2
- Claim: Using frozen PENet features as the visual encoder base preserves pretrained visual knowledge while allowing modality fusion learning via the contrastive stage.
- Mechanism: PENet is initialized from Kinetics pretraining and frozen during contrastive pretraining. The average of sub-volume embeddings is passed to a trainable MLP, so the contrastive alignment learns to map both modalities into a shared space without destroying the original visual semantics.
- Core assumption: Kinetics-pretrained visual features are sufficiently general for PE-relevant visual patterns.
- Evidence anchors:
  - [section]: "The PENet backbone is initialized with weights trained on RadFusion, pretrained on Kinetics dataset... and it remains frozen during the pretraining"
  - [corpus]: No direct match; PECon's use of a frozen backbone for contrastive pretraining is not represented in neighbor papers.
- Break condition: If the frozen PENet does not capture PE-relevant discriminative features, alignment cannot be effective.

### Mechanism 3
- Claim: The multimodal weighted sum (λ·ˆy_xc + (1-λ)·ˆy_xe) allows the model to leverage complementary modality strengths and improves robustness to modality-specific noise.
- Mechanism: After contrastive pretraining, separate classification heads are fine-tuned per modality. During inference, a learned weighting parameter λ balances their contributions, effectively fusing complementary discriminative signals.
- Core assumption: Both modalities contain useful and complementary information for PE diagnosis.
- Evidence anchors:
  - [abstract]: "employ both the patient's CT scans as well as the EHR data... to enhance the alignment of feature representations between the two modalities"
  - [section]: "multimodal prediction is given by: ˆyxc,xe = λˆyxc + (1-λ)ˆyxe"
  - [corpus]: No direct match; neighbor list focuses on single-modality contrastive learning, not multimodal fusion weighting.
- Break condition: If one modality is consistently misleading (e.g., poor EHR data quality), the weighted sum could degrade performance unless λ adapts.

## Foundational Learning

- Concept: Contrastive learning objective (InfoNCE-style loss)
  - Why needed here: The supervised contrastive loss (Eq. 1) is the core mechanism that drives feature alignment between CT and EHR embeddings.
  - Quick check question: In the loss term, what does P(i) represent and how does it differ from A(i)?

- Concept: Multimodal fusion strategies (late fusion)
  - Why needed here: PECon uses a late fusion approach where modality-specific embeddings are aligned during pretraining, then combined at the prediction layer.
  - Quick check question: How would early fusion (concatenating raw inputs) change the contrastive alignment objective?

- Concept: Class activation mapping (CAM)
  - Why needed here: CAM visualizations are used to interpret and compare the spatial attention of PENet vs PECon on CT slices.
  - Quick check question: What gradient-based operation is required to generate a CAM from a trained CNN?

## Architecture Onboarding

- Component map:
  Input: 3D CT sub-volumes (24 slices) + EHR vector -> Frozen PENet (generates 2048-dim per sub-volume) -> MLP(2048→128) for CT -> Projection 1: MLP(128→128) for CT
  Input: EHR vector -> MLP(1→128) for EHR -> Projection 2: MLP(128→128) for EHR
  Loss: Supervised contrastive (Eq. 1) -> Fine-tune: Separate cross-entropy heads for CT and EHR, then weighted sum -> Output: PE classification score

- Critical path:
  1. Sub-volume CT → PENet → average → MLP → z_c
  2. EHR → MLP → z_e
  3. Compute supervised contrastive loss over (z_c, z_e) batch
  4. Fine-tune classification heads separately

- Design tradeoffs:
  - Frozen PENet preserves rich visual features but limits modality-specific visual adaptation.
  - Late fusion simplifies pretraining but may miss early interaction signals.
  - Supervised loss uses labels (vs self-supervised) but requires clean labels.

- Failure signatures:
  - Poor validation loss during pretraining → modality embeddings not aligning.
  - Imbalanced λ → one modality dominates, reducing complementarity.
  - Grad-CAM shows diffuse or irrelevant regions → visual encoder not learning PE-relevant features.

- First 3 experiments:
  1. Run contrastive pretraining with λ=0.5 and monitor training loss; check if embedding clusters form in TSNE.
  2. Fine-tune CT and EHR heads separately; verify that each improves over unimodal baselines.
  3. Visualize Grad-CAM on a held-out positive CT slice; compare attention localization to PENet baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed PECon method perform on datasets other than RadFusion, and what is its generalizability across different medical imaging datasets?
- Basis in paper: [inferred] The paper mentions that the limitations of the work include testing on other datasets to assess the generalizability of the approach.
- Why unresolved: The study only evaluated the method on the RadFusion dataset, which limits the understanding of its performance and applicability to other datasets.
- What evidence would resolve it: Conducting experiments on multiple datasets from different medical imaging domains and comparing the results to establish the generalizability of PECon.

### Open Question 2
- Question: How does the contrastive pretraining loss impact the performance of early and joint fusion approaches compared to the late fusion approach used in this study?
- Basis in paper: [inferred] The paper mentions that future work will explore the impact of contrastive pretraining loss on early and joint fusion approaches.
- Why unresolved: The study only evaluated the late fusion approach, and the impact of the contrastive pretraining loss on other fusion strategies remains unexplored.
- What evidence would resolve it: Implementing and evaluating the contrastive pretraining loss on early and joint fusion approaches and comparing their performance with the late fusion approach.

### Open Question 3
- Question: How does the attention mechanism improve feature aggregation in the PECon method, and what are the potential benefits of incorporating it into the model?
- Basis in paper: [inferred] The paper mentions that attention techniques will be explored for feature aggregation in future work.
- Why unresolved: The study did not incorporate attention mechanisms, and the potential benefits of using them for feature aggregation remain unexplored.
- What evidence would resolve it: Implementing attention mechanisms in the PECon method and evaluating their impact on feature aggregation and overall model performance.

## Limitations

- The paper does not specify exact data splits for train/val/test, which could affect reproducibility and generalizability claims
- EHR MLP architecture details beyond the 128-neuron output are unspecified
- No ablation studies are provided to isolate the contribution of contrastive pretraining versus multimodal fusion

## Confidence

- High confidence in the technical mechanism of supervised contrastive pretraining for multimodal alignment
- Medium confidence in the quantitative results due to unspecified data splits and potential dataset-specific effects
- Medium confidence in the Grad-CAM interpretability claims, as no statistical comparison with baseline is provided

## Next Checks

1. Conduct ablation study comparing PECon with and without supervised contrastive pretraining on identical data splits
2. Perform cross-dataset validation to assess generalization beyond RadFusion
3. Implement statistical significance testing for the Grad-CAM localization differences between PENet and PECon