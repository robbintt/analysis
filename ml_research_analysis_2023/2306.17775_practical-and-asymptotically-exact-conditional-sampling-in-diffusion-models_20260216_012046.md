---
ver: rpa2
title: Practical and Asymptotically Exact Conditional Sampling in Diffusion Models
arxiv_id: '2306.17775'
source_url: https://arxiv.org/abs/2306.17775
tags:
- diffusion
- conditional
- particles
- scale
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the Twisted Diffusion Sampler (TDS), a sequential\
  \ Monte Carlo algorithm for asymptotically exact conditional sampling from diffusion\
  \ models. TDS addresses the problem of generating samples from a conditional distribution\
  \ p\u03B8(x|y) using an unconditional diffusion model p\u03B8(x)."
---

# Practical and Asymptotically Exact Conditional Sampling in Diffusion Models

## Quick Facts
- arXiv ID: 2306.17775
- Source URL: https://arxiv.org/abs/2306.17775
- Reference count: 40
- Key outcome: Introduces Twisted Diffusion Sampler (TDS), a sequential Monte Carlo algorithm for asymptotically exact conditional sampling from unconditional diffusion models

## Executive Summary
This paper addresses the challenge of generating samples from conditional distributions pθ(x|y) using unconditional diffusion models pθ(x). The authors introduce the Twisted Diffusion Sampler (TDS), an SMC algorithm that leverages twisting functions to incorporate heuristic approximations while maintaining asymptotic exactness. TDS is evaluated on MNIST class-conditional generation and inpainting tasks, showing improved performance over heuristic approaches. For protein motif-scaffolding problems, TDS outperforms state-of-the-art conditionally trained models on shorter scaffolds, offering greater flexibility in handling degrees of freedom in motif placement and rotation.

## Method Summary
TDS uses sequential Monte Carlo with twisting functions to approximate optimal twisting functions, enabling asymptotically exact conditional sampling. The method works by initializing particles from an unconditional diffusion model, then iteratively updating them through twisted proposals and importance weights. For tasks with degrees of freedom, TDS averages over possible configurations using a uniform prior. The approach maintains computational efficiency by bringing intermediate targets closer to the final target through twisting, reducing the number of particles needed for accurate approximation.

## Key Results
- TDS achieves asymptotically exact conditional sampling while maintaining computational efficiency
- On MNIST tasks, TDS outperforms heuristic approaches with as few as two particles
- For protein motif-scaffolding, TDS surpasses conditionally trained models on shorter scaffolds while providing flexibility for degrees of freedom

## Why This Works (Mechanism)

### Mechanism 1
TDS provides asymptotically exact conditional sampling from unconditional diffusion models using SMC with twisting functions. The twisting functions approximate optimal twisting functions that would allow exact sampling with one particle, evaluated at the denoising network output. The core assumption is that approximation error diminishes as t→0 where the denoising network converges to the true conditional distribution.

### Mechanism 2
TDS extends to inpainting and problems with degrees of freedom through modified twisting functions. For inpainting, it conditions on observed dimensions with Gaussian noise. For degrees of freedom, it averages over possible motif placements using a uniform prior on placement indices. The core assumption is that the likelihood function can be approximated by evaluating it at the denoising network output.

### Mechanism 3
TDS maintains computational efficiency while achieving asymptotic exactness through twisting. Twisting brings intermediate targets closer to the final target by modifying proposals and weights, reducing particle requirements compared to naive importance sampling. The core assumption is that twisting function approximations are sufficiently accurate that intermediate targets remain close to the true conditional distribution.

## Foundational Learning

- **Concept: Sequential Monte Carlo (SMC)**
  - Why needed here: TDS is fundamentally an SMC algorithm using weighted particles to approximate conditional distributions
  - Quick check question: How does SMC ensure that weighted particles converge to the target distribution as particle count increases?

- **Concept: Diffusion Models**
  - Why needed here: TDS operates on diffusion models which parameterize complex distributions through iterative refinement from noise
  - Quick check question: What is the relationship between the denoising network and the score function in a diffusion model?

- **Concept: Twisting in SMC**
  - Why needed here: Twisting is the key technique allowing TDS to incorporate heuristic approximations while maintaining asymptotic exactness
  - Quick check question: How do twisting functions modify proposals and weights in SMC to bring intermediate targets closer to the final target?

## Architecture Onboarding

- **Component map:** Diffusion model (pθ(x)) -> Twisting functions (˜pθ(y|xt)) -> Proposals (˜pθ(xt|xt+1,y)) -> Weights (wt(xt,xt+1)) -> Particles ({xt_k}K_k=1)

- **Critical path:**
  1. Initialize particles from unconditional diffusion model
  2. Compute twisting function values at denoising network outputs
  3. Generate new particles using twisted proposals
  4. Compute weights based on twisting functions and proposals
  5. Resample particles based on weights
  6. Repeat steps 2-5 for all time steps
  7. Return weighted particles at t=0 as approximation to conditional distribution

- **Design tradeoffs:**
  - Number of particles vs. computational cost vs. approximation accuracy
  - Quality of twisting function approximation vs. computational tractability
  - Choice of resampling strategy vs. particle diversity vs. computational efficiency

- **Failure signatures:**
  - Particle collapse (few particles receive most weight) - suggests poor twisting function approximation or too few particles
  - High variance in particle weights - suggests conditional distribution differs significantly from unconditional
  - Slow convergence to true conditional - suggests poor quality of unconditional diffusion model or twisting function approximation

- **First 3 experiments:**
  1. Implement TDS with simple unconditional diffusion model (e.g., toy Gaussian) and test on known conditional distribution to verify asymptotic exactness
  2. Compare TDS to naive importance sampling and reconstruction guidance on MNIST class-conditional generation to demonstrate particle efficiency gains
  3. Apply TDS to simple inpainting problem (e.g., half-image observed) and evaluate inpainted region quality compared to replacement method

## Open Questions the Paper Calls Out

### Open Question 1
How does the number of particles needed for accurate TDS results vary across different conditioning tasks and unconditional diffusion model architectures? The paper demonstrates TDS performance with varying particle counts but doesn't provide systematic analysis of how required particle count scales with task complexity, conditioning information type, or unconditional model capacity.

### Open Question 2
What is the theoretical limit of TDS accuracy when the unconditional diffusion model is imperfect (i.e., the score network is not perfectly trained)? While the paper provides theoretical guarantees for perfect unconditional models, it doesn't characterize how model imperfections affect TDS accuracy or establish bounds on achievable performance when the score network has finite training.

### Open Question 3
How does TDS compare to conditional training approaches in terms of sample quality and diversity when sufficient training data is available? The paper demonstrates TDS's advantages in flexibility and performance without conditional training, but doesn't directly compare sample quality and diversity to conditionally trained approaches under conditions where conditional training is feasible.

## Limitations

- TDS's performance on high-dimensional data beyond MNIST and proteins has not been demonstrated
- The tangent normal approximation for protein motif-scaffolding introduces additional errors that are not quantified
- The paper lacks finite-sample error bounds or runtime complexity guarantees

## Confidence

- **High Confidence**: The fundamental SMC framework with twisting functions is theoretically sound and asymptotic exactness claim is well-supported
- **Medium Confidence**: Practical effectiveness on MNIST and protein tasks is demonstrated, but comparisons are limited to heuristic methods
- **Low Confidence**: Extension to problems with degrees of freedom relies on several approximations (uniform prior, tangent normal) whose impact is not thoroughly analyzed

## Next Checks

1. Perform empirical studies varying particle counts K to quantify convergence rates and determine minimum particle requirements across different conditioning scenarios

2. Systematically measure and report contributions of denoising network error, twisting function approximation error, and resampling error to overall sampling quality

3. Test TDS on higher-dimensional conditional sampling tasks (e.g., class-conditional generation for CIFAR-10) to evaluate scalability and identify potential failure modes in more complex domains