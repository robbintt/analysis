---
ver: rpa2
title: 'Distributional Preference Learning: Understanding and Accounting for Hidden
  Context in RLHF'
arxiv_id: '2312.08358'
source_url: https://arxiv.org/abs/2312.08358
tags:
- learning
- preference
- utility
- hidden
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of preference learning with hidden
  context, which occurs when the data used to train a preference model does not include
  all the information that influences human preferences. This can lead to unexpected
  results, such as models being more likely to prefer jailbroken responses.
---

# Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF

## Quick Facts
- **arXiv ID**: 2312.08358
- **Source URL**: https://arxiv.org/abs/2312.08358
- **Reference count**: 40
- **Primary result**: Distributional preference learning (DPL) detects hidden context in preference datasets and reduces jailbreak vulnerabilities by 50% without hurting performance on non-harmful prompts.

## Executive Summary
This paper identifies a critical problem in preference learning: when human preference data contains hidden context (unobserved factors that influence preferences), standard methods implicitly aggregate over this context using Borda count voting rules. This can lead to unexpected behaviors like models preferring jailbroken responses. The authors propose distributional preference learning (DPL), which models a distribution over utilities rather than a single value, enabling detection of hidden context through variance analysis. Experiments show DPL can identify hidden context and, when combined with risk-averse optimization, significantly reduces jailbreak vulnerabilities in RLHF-trained models.

## Method Summary
The method introduces distributional preference learning where instead of predicting a single utility value for each alternative, the model predicts a distribution (mean-and-variance or categorical) over possible utilities. This is trained on preference comparison data using a modified Bradley-Terry-Luce framework. Hidden context is detected using an r² metric that measures how much variance in utilities can be explained by the input. For jailbreak mitigation, the approach uses risk-averse optimization by selecting lower quantiles from the utility distribution during response generation, effectively penalizing responses where different objectives (helpfulness vs harmlessness) conflict.

## Key Results
- DPL successfully detects hidden context in HH-RLHF dataset, with r² scores indicating significant unexplained variance
- Risk-averse optimization with DPL reduces jailbreak preference rates by 50% compared to standard methods
- No degradation in helpfulness accuracy on non-harmful prompts
- Categorical DPL variant shows slightly better performance than mean-variance variant for jailbreak detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preference learning implicitly aggregates hidden context using Borda count voting rule
- Mechanism: When hidden context is present, the learned utility function assigns utilities equivalent to Borda count, which averages the probability that an alternative is preferred over others
- Core assumption: The hidden context follows a distribution that can be marginalized when computing preference probabilities
- Evidence anchors:
  - [abstract]: "We prove that standard applications of preference learning... implicitly aggregate over hidden contexts according to a well-known voting rule called Borda count"
  - [section 3]: "BTL preference learning implicitly aggregates hidden context according to Borda count. That is, if ˆu is optimized according to (3), then ∀a, b ∈ A, ˆu(a) > ˆu(b) ⇔ BC(a) > BC(b)"
  - [corpus]: Weak - no corpus papers directly support this specific mechanism
- Break condition: When the hidden context distribution violates the assumptions of Theorem 3.2 (independence, identical distribution, support around zero)

### Mechanism 2
- Claim: DPL detects hidden context through variance in utility distributions
- Mechanism: By modeling a distribution over utilities instead of a single value, DPL captures the uncertainty introduced by hidden context, which manifests as higher variance
- Core assumption: Hidden context creates systematic variation in utility assessments across different instances
- Evidence anchors:
  - [abstract]: "DPL methods estimate a distribution of possible score values for each alternative in order to better account for hidden context"
  - [section 4]: "Since DPL methods give more information than a single utility estimate at each alternative, they can detect the effects of missing features both at the dataset and instance level"
  - [corpus]: Weak - no corpus papers directly validate this specific mechanism
- Break condition: When hidden context effects are too subtle to create measurable variance differences

### Mechanism 3
- Claim: Risk-averse optimization with DPL mitigates jailbreaks by penalizing high-variance responses
- Mechanism: By optimizing lower quantiles of the utility distribution, the method avoids responses where different objectives (helpfulness vs harmlessness) diverge significantly
- Core assumption: Jailbreaks exploit conflicts between different objectives, creating high variance in utility assessments
- Evidence anchors:
  - [abstract]: "risk-aversion with respect to the distribution of learned utilities can dramatically reduce the rate at which the preference model prefers jailbroken responses"
  - [section 5]: "We can avoid helpful-but-harmful responses by optimizing a lower quantile of the distribution ˆD output by DPL"
  - [corpus]: Weak - no corpus papers directly validate this specific mechanism
- Break condition: When jailbreaks don't exploit objective conflicts or when variance doesn't correlate with jailbreak vulnerability

## Foundational Learning

- Concept: Bradley-Terry-Luce (BTL) model of preference learning
  - Why needed here: Understanding the standard preference learning framework that DPL modifies
  - Quick check question: How does the BTL model calculate the probability that alternative a is preferred to b?

- Concept: Social choice theory and voting rules
  - Why needed here: Provides theoretical framework for understanding how preference learning aggregates individual preferences
  - Quick check question: What is the key difference between Borda count and pairwise majority voting?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The practical application domain where hidden context issues manifest most critically
  - Quick check question: What are the two competing objectives that often create hidden context in LLM RLHF?

## Architecture Onboarding

- Component map:
  Prompt-Response Pairs -> DPL Model (Distribution over Utilities) -> Risk-Averse Selection -> Jailbreak-Resistant Response

- Critical path:
  1. Train DPL model on preference data
  2. Evaluate r² to detect hidden context effects
  3. If r² is low, enable risk-averse optimization
  4. Generate responses using lower quantile of utility distribution

- Design tradeoffs:
  - DPL vs normal learning: Higher computational cost but better detection of hidden context
  - Mean-and-variance vs categorical DPL: Continuous vs discrete distributions, different training stability
  - Risk-aversion level: Trade-off between jailbreak prevention and response quality

- Failure signatures:
  - Low r² but no observable hidden context effects
  - Risk-averse optimization degrading response quality without improving jailbreak resistance
  - DPL failing to converge to meaningful distributions

- First 3 experiments:
  1. Train DPL on HH-RLHF data and measure r² for different subsets
  2. Compare jailbreak rates between normal and DPL models on same test set
  3. Test different risk-aversion thresholds on the categorical DPL model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical conditions under which distributional preference learning (DPL) can accurately estimate the distribution of utilities for each alternative?
- Basis in paper: Inferred from the section "Distributional Preference Learning" and the discussion of using DPL to detect and mitigate the effects of hidden context.
- Why unresolved: The paper introduces DPL and shows its potential for detecting hidden context, but does not provide a comprehensive theoretical analysis of when and how DPL can accurately estimate utility distributions. Understanding these conditions would be crucial for the practical application and further development of DPL.
- What evidence would resolve it: Theoretical proofs or empirical studies demonstrating the conditions (e.g., specific data characteristics, model architectures, or training procedures) under which DPL consistently produces accurate utility distributions.

### Open Question 2
- Question: How does hidden context contribute to failures of RLHF-trained models beyond jailbreaks, and what other vulnerabilities or biases might arise?
- Basis in paper: Inferred from the discussion of hidden context and its potential to lead to unexpected and undesirable consequences, as well as the mention of diverse preferences among annotators and the incentive for misreporting.
- Why unresolved: While the paper provides a case study on jailbreaks, it does not comprehensively explore the broader implications of hidden context on the safety and fairness of RLHF-trained models. Understanding these additional vulnerabilities is essential for developing robust and trustworthy AI systems.
- What evidence would resolve it: Empirical studies or theoretical analyses identifying specific failures or biases in RLHF-trained models that can be attributed to hidden context, along with proposed mitigation strategies.

### Open Question 3
- Question: Can alternative social welfare functionals (SWFs) beyond Borda count be implemented in preference learning, and what are their potential advantages and disadvantages?
- Basis in paper: Inferred from the discussion of social choice theory and the question of whether voting rules other than Borda count can be implemented in preference learning by changing the estimation procedure.
- Why unresolved: The paper establishes that Borda count is implicitly used in standard preference learning with hidden context, but does not explore the possibility of implementing other SWFs. Understanding the implications of different SWFs could lead to more desirable aggregation methods and better alignment with human values.
- What evidence would resolve it: Theoretical analysis or empirical experiments comparing the performance and properties of different SWFs in preference learning, considering factors such as fairness, incentive compatibility, and robustness to hidden context.

## Limitations

- The method requires sufficient data to reliably estimate utility distributions, which may be challenging with limited preference data
- Performance may degrade when hidden context is continuous or extremely subtle, making variance differences difficult to detect
- Risk-averse optimization introduces a new hyperparameter (quantile threshold) that requires careful tuning and may impact response quality

## Confidence

- **High confidence**: Borda count aggregation mechanism (Theorem 3.2 proof), basic DPL architecture
- **Medium confidence**: Hidden context detection through variance, jailbreak mitigation effectiveness, categorical vs mean-variance DPL comparison
- **Low confidence**: Generalizability to other LLMs and preference datasets, optimal risk-aversion threshold selection

## Next Checks

1. **Distributional assumption validation**: Test whether real preference datasets follow the identical and independent hidden context distribution assumptions required for the Borda count aggregation result, using statistical tests on multiple preference datasets.

2. **Cross-model generalization**: Evaluate DPL performance on multiple LLM architectures (not just LLAMA-2-7B) and different RLHF training regimes to assess whether the jailbreak mitigation generalizes beyond the specific experimental setup.

3. **Hidden context typology**: Systematically categorize different types of hidden context (contextual, temporal, objective conflicts) and measure how DPL's r² detection metric performs across each type, particularly for subtle forms that may not create large variance differences.