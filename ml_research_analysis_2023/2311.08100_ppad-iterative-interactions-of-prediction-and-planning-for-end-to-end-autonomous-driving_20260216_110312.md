---
ver: rpa2
title: 'PPAD: Iterative Interactions of Prediction and Planning for End-to-end Autonomous
  Driving'
arxiv_id: '2311.08100'
source_url: https://arxiv.org/abs/2311.08100
tags:
- agents
- planning
- driving
- arxiv
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PPAD introduces an iterative interaction mechanism between prediction
  and planning for end-to-end autonomous driving, termed Iterative Interaction of
  Prediction and Planning (PPAD). Unlike existing frameworks that perform prediction
  and planning sequentially, PPAD interleaves these processes at each timestep to
  better model dynamic interactions among ego vehicles, agents, and the environment.
---

# PPAD: Iterative Interactions of Prediction and Planning for End-to-end Autonomous Driving

## Quick Facts
- arXiv ID: 2311.08100
- Source URL: https://arxiv.org/abs/2311.08100
- Reference count: 40
- Key outcome: L2 distance errors of 0.48m at 1s, 0.96m at 2s, and 1.65m at 3s, with collision rate of 0.31%

## Executive Summary
PPAD introduces an iterative interaction mechanism between prediction and planning for end-to-end autonomous driving, termed Iterative Interaction of Prediction and Planning (PPAD). Unlike existing frameworks that perform prediction and planning sequentially, PPAD interleaves these processes at each timestep to better model dynamic interactions among ego vehicles, agents, and the environment. The method employs hierarchical dynamic key objects attention and incorporates ego-to-agent, ego-to-map, and ego-to-BEV interaction mechanisms. Experiments on the nuScenes benchmark demonstrate state-of-the-art performance.

## Method Summary
PPAD is an end-to-end autonomous driving framework that iteratively interleaves prediction and planning processes at every timestep. It uses an Expectation-Maximization framework where the Expectation process models agent behavior given the current ego state, and the Maximization process plans ego trajectory based on updated agent expectations. The system employs hierarchical dynamic key objects attention with multi-head cross attention across different distance ranges (7.5m, 15m, etc.), and includes ego-to-agent, ego-to-map, and ego-to-BEV interaction mechanisms. The model is trained end-to-end with perception loss, constraint loss (including confidence-aware collision loss), and planning loss, using noisy trajectories as predictions to improve robustness.

## Key Results
- Achieves L2 distance errors of 0.48m at 1s, 0.96m at 2s, and 1.65m at 3s
- Maintains collision rate of 0.31% on nuScenes benchmark
- Demonstrates strong performance in upstream perception and prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative interleaving of prediction and planning captures dynamic agent-environment interactions more effectively than sequential approaches.
- Mechanism: At each timestep, PPAD alternates between Expectation (modeling agent behavior given current ego state) and Maximization (planning ego trajectory based on updated agent expectations), creating a feedback loop that refines predictions and plans incrementally.
- Core assumption: Agent behavior depends on ego vehicle actions, requiring bidirectional reasoning rather than unidirectional prediction-then-planning.
- Evidence anchors:
  - [abstract]: "Unlike existing end-to-end autonomous driving frameworks, PPAD models the interactions among ego, agents, and the dynamic environment in an autoregressive manner by interleaving the Prediction and Planning processes at every timestep"
  - [section]: "DeepEMplanner consists of an expectation and maximization process. It dissects the dynamic interactions between the ego vehicle and the agents along the temporal dimension"
  - [corpus]: No direct evidence in corpus papers about iterative interleaving; most describe single-step or two-stage frameworks
- Confidence: Medium

### Mechanism 2
- Claim: Hierarchical dynamic key objects attention enables fine-grained spatial reasoning at multiple distance scales.
- Mechanism: PPAD applies multi-head cross attention to agents within different distance ranges (7.5m, 15m, etc.) and aggregates results, allowing the model to capture both local collision avoidance and global traffic context simultaneously.
- Core assumption: Different spatial scales contain distinct types of interaction information that benefit from separate processing.
- Evidence anchors:
  - [abstract]: "We design ego-to-agent, ego-to-map, and ego-to-BEV interaction mechanisms with hierarchical dynamic key objects attention"
  - [section]: "We propose to conduct a hierarchical interaction with the agents through the attention mechanism to learn coarse-to-fine context features for the ego"
  - [corpus]: Weak evidence - neighboring papers focus on global attention or fixed local regions but not hierarchical multi-scale attention
- Confidence: Low

### Mechanism 3
- Claim: Noisy trajectory training improves robustness to perception uncertainty in planning.
- Mechanism: During training, ground truth trajectories are perturbed at each timestep, forcing the planner to learn to recover from inaccurate starting positions rather than memorizing exact trajectories.
- Core assumption: Real-world perception introduces noise that must be handled by the planning system, not just the perception module.
- Evidence anchors:
  - [abstract]: "The ego is then trained to predict the original next step waypoint offset of the ego regardless of disturbance on its starting noisy positions"
  - [section]: "We introduce the noisy trajectory as the prediction to the DeepEMplanner while training. Specifically, we perturb each step of the ground truth ego trajectory by adding noise"
  - [corpus]: No evidence in corpus papers about noisy trajectory training for planning robustness
- Confidence: Medium

## Foundational Learning

- Concept: Transformer-based attention mechanisms
  - Why needed here: PPAD relies on cross-attention between ego, agents, map elements, and BEV features to model complex multi-agent interactions
  - Quick check question: What is the difference between self-attention and cross-attention in transformer architectures?

- Concept: Imitation learning with noisy supervision
  - Why needed here: The noisy trajectory training strategy requires understanding how to design robust loss functions that can handle corrupted supervision signals
  - Quick check question: How does adding noise during training differ from data augmentation in terms of learning objectives?

- Concept: Probabilistic trajectory forecasting
  - Why needed here: PPAD must handle multiple possible agent behaviors, requiring understanding of how to represent and predict distributions over future states
  - Quick check question: What are the trade-offs between deterministic and probabilistic approaches to motion prediction?

## Architecture Onboarding

- Component map: Perception Transformer (BEVFormer + vectorized feature extraction) → DeepEMplanner (Expectation-Maximization loop with hierarchical attention) → Planning output
- Critical path: Multi-camera input → BEV feature generation → Agent and map queries → Iterative EM planning → Trajectory output
- Design tradeoffs: Hierarchical attention provides better spatial reasoning but increases computational cost; iterative refinement improves accuracy but requires more training time
- Failure signatures: Poor collision avoidance indicates insufficient agent-ego interaction modeling; trajectory jitter suggests unstable EM iterations; long planning times indicate inefficient attention computation
- First 3 experiments:
  1. Compare single-step vs. multi-step EM iterations on L2 error and collision rate
  2. Evaluate hierarchical vs. flat attention on planning accuracy in complex intersection scenarios
  3. Test noisy vs. clean trajectory training on robustness to perception errors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the iterative interaction mechanism in PPAD scale to more complex driving scenarios with a larger number of agents and dynamic environments?
- Basis in paper: [explicit] The paper mentions that PPAD models interactions among ego, agents, and the dynamic environment in an autoregressive manner, but does not provide extensive analysis of its performance in scenarios with a high density of agents or complex road layouts.
- Why unresolved: The paper primarily evaluates PPAD on the nuScenes dataset, which may not fully represent the complexity of real-world driving scenarios with a large number of agents and dynamic environments.
- What evidence would resolve it: Experiments demonstrating PPAD's performance on datasets with a higher density of agents, more complex road layouts, and diverse weather conditions would provide insights into its scalability and robustness.

### Open Question 2
- Question: What is the impact of the hierarchical dynamic key objects attention mechanism on the computational efficiency of PPAD?
- Basis in paper: [explicit] The paper introduces hierarchical dynamic key objects attention to model interactions among ego, agents, and the environment, but does not provide a detailed analysis of its computational cost.
- Why unresolved: The paper does not compare the computational efficiency of PPAD with and without the hierarchical dynamic key objects attention mechanism, making it difficult to assess its impact on real-time performance.
- What evidence would resolve it: Ablation studies comparing the computational efficiency of PPAD with and without the hierarchical dynamic key objects attention mechanism would provide insights into its impact on real-time performance.

### Open Question 3
- Question: How does the noisy trajectory prediction strategy in PPAD affect its performance in scenarios with unpredictable agent behavior?
- Basis in paper: [explicit] The paper introduces a noisy trajectory prediction strategy to improve driving safety, but does not provide extensive analysis of its performance in scenarios with unpredictable agent behavior.
- Why unresolved: The paper does not evaluate PPAD's performance in scenarios with unpredictable agent behavior, such as sudden lane changes or unexpected pedestrian crossings.
- What evidence would resolve it: Experiments demonstrating PPAD's performance in scenarios with unpredictable agent behavior, such as sudden lane changes or unexpected pedestrian crossings, would provide insights into the effectiveness of the noisy trajectory prediction strategy in such situations.

## Limitations
- Limited empirical evidence for iterative interaction benefits compared to sequential approaches
- No ablation studies isolating the contribution of hierarchical attention mechanism
- No comparison of noisy trajectory training against alternative robustness techniques

## Confidence
- Iterative interaction mechanism: Medium
- Hierarchical attention mechanism: Low
- Noisy trajectory training: Medium

## Next Checks
1. **Ablation Study**: Remove the iterative EM loop and implement a sequential prediction-then-planning baseline with identical attention mechanisms to measure the specific contribution of iterative refinement.

2. **Attention Analysis**: Conduct quantitative experiments comparing hierarchical multi-scale attention against single-scale attention with equivalent parameter count to isolate the benefit of the hierarchical design.

3. **Training Robustness**: Compare noisy trajectory training against alternative robustness techniques like domain randomization or adversarial training to establish whether the specific noisy trajectory approach provides unique benefits.