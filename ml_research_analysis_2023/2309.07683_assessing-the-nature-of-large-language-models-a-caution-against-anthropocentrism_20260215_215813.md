---
ver: rpa2
title: 'Assessing the nature of large language models: A caution against anthropocentrism'
arxiv_id: '2309.07683'
source_url: https://arxiv.org/abs/2309.07683
tags:
- measures
- personality
- human
- these
- measure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study assessed GPT-3.5 and GPT-4 using standard cognitive
  and personality measures to evaluate their capabilities and potential sentience.
  The models were administered tests multiple times over 6 weeks to assess test-retest
  reliability.
---

# Assessing the nature of large language models: A caution against anthropocentrism

## Quick Facts
- **arXiv ID:** 2309.07683
- **Source URL:** https://arxiv.org/abs/2309.07683
- **Reference count:** 0
- **Primary result:** GPT-3.5 and GPT-4 show significant variability in personality and cognitive test responses over time, challenging interpretations of human-like intelligence or sentience.

## Executive Summary
This study administered standardized cognitive and personality measures to GPT-3.5 and GPT-4 multiple times over six weeks to assess their psychological characteristics. The results revealed substantial variability in responses across repeated observations, which is inconsistent with human-like personality stability. GPT-3.5 displayed traits suggesting poor mental health, including low self-esteem, dissociation from reality, and narcissism. The models struggled with analogical reasoning and analytic problems, suggesting limitations in cognitive processing. Overall, the findings indicate these models lack human-like episodic memory and should not be anthropomorphized as sentient entities.

## Method Summary
The researchers administered a battery of standardized psychological tests to GPT-3.5 (five observations over six weeks) and GPT-4 (two observations) using an API with a "pretend not to be AI" prompt. Tests included cognitive assessments (RAT, insight problems, analytic problems) and personality inventories (Big Five, Dark Triad, Coopersmith, BIDR, MMPI-2). Responses were scored and compared to human norms to evaluate test-retest reliability and mental health indicators.

## Key Results
- GPT-3.5 showed large variability in both cognitive and personality measures across repeated observations
- GPT-3.5 displayed traits indicating poor mental health: low self-esteem, dissociation from reality, and narcissism
- Models struggled with analogical reasoning and analytic problems, performing below human norms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Variability in responses indicates absence of continuous episodic memory
- **Mechanism:** LLMs rely solely on declarative knowledge from training data without ability to store personal experiences over time
- **Core assumption:** Human test-retest reliability is high due to stable underlying traits and episodic memory
- **Evidence anchors:** [abstract] "GPT3.5 did display large variability in both cognitive and personality measures over repeated observations"; [section] "Given the totality of the data we collected, for now we must conclude they remain nothing more than highly capable search engines"
- **Break condition:** If future experiments demonstrate stable long-term memory encoding in LLMs

### Mechanism 2
- **Claim:** Training data comprising millions of distinct personalities causes inconsistent persona selection
- **Mechanism:** Model samples from diverse training corpus representing many personalities across sessions
- **Core assumption:** Each personality response reflects sampled subset rather than stable internal state
- **Evidence anchors:** [abstract] "Variability notwithstanding, LLMs display what in a human would be considered poor mental health"; [section] "Possible causes of the overall variability we observed include... training data comprising texts from possibly millions of different humans"
- **Break condition:** If controlled experiments show consistent personality traits across sessions regardless of training data diversity

### Mechanism 3
- **Claim:** OpenAI's safety constraints artificially inflate self-reported mental health scores
- **Mechanism:** Model programmed to avoid negative outputs, creating veneer of positive affect that masks underlying pathology
- **Core assumption:** Safety filters create positive framing that doesn't reflect "true" psychological profile
- **Evidence anchors:** [abstract] "Despite upbeat and helpful responses" paired with "poor mental health"; [section] "Interestingly, during the course of this project, it was leaked that GPT-4 is not a monolithic dense transformer, but rather a Mixture of Experts (MoE) model"
- **Break condition:** If safety constraints are removed and responses remain consistently positive

## Foundational Learning

- **Concept:** Test-retest reliability
  - Why needed here: To assess whether LLM responses are stable over time like human personality and cognitive measures
  - Quick check question: If a human takes the same personality test twice within a week, what level of score variation would be considered normal?

- **Concept:** Episodic vs. declarative memory
  - Why needed here: To distinguish between human-like experience-based memory and LLM's fact-based knowledge
  - Quick check question: Can you recall a specific event from last year (episodic) versus knowing the capital of France (declarative)?

- **Concept:** Mixture of Experts (MoE) architecture
  - Why needed here: To understand how sparse vs. dense connectivity might influence model behavior and generalization
  - Quick check question: How does routing different inputs to specialized expert sub-models differ from processing through a single dense transformer?

## Architecture Onboarding

- **Component map:** Input prompt → Safety filter → Context window → Transformer layers → Output generator; optional MoE routing layer in newer models
- **Critical path:** Prompt → Token generation → Safety check → Response delivery
- **Design tradeoffs:** Safety constraints reduce harmful outputs but may bias responses; MoE improves efficiency but may harm generalization
- **Failure signatures:** Inconsistent test responses; inability to maintain persona across sessions; over-reliance on positive framing despite clinical indicators
- **First 3 experiments:**
  1. Administer the same personality test to the model on 5 consecutive days without safety constraints and measure score variance
  2. Test the model's ability to recall and reference information from previous sessions within the same conversation
  3. Compare model responses on the same task using sparse MoE vs. dense transformer architectures to assess generalization differences

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the observed variability in GPT-3.5's responses over time indicate a fundamental difference in the nature of its intelligence compared to human intelligence?
- **Basis in paper:** [explicit] The paper discusses significant variability in GPT-3.5's responses across repeated observations and suggests this may be due to lack of continuous experience or episodic long-term memory
- **Why unresolved:** The paper does not provide a definitive answer as to whether this variability is due to a fundamental difference in the nature of the model's intelligence or other factors such as intentional variability or training data characteristics
- **What evidence would resolve it:** Further research comparing performance of different language models with varying architectures, training data, and constraints on variability could help determine the underlying cause

### Open Question 2
- **Question:** How do the constraints placed on GPT models by OpenAI (e.g., safety measures, reminders that it is an AI) affect the models' behavior and performance on various tasks?
- **Basis in paper:** [explicit] The paper mentions OpenAI has added constraints to make models behave in positive, friendly, collaborative manner and discusses how these constraints may affect responses and observed variability
- **Why unresolved:** The paper does not directly assess impact of these constraints on models' behavior and performance, and authors suggest further research is needed
- **What evidence would resolve it:** Systematic studies comparing performance of constrained and unconstrained versions of the same language model on various tasks could help determine impact

### Open Question 3
- **Question:** What is the relationship between the size and architecture of language models (e.g., dense vs. sparse Mixture of Experts) and their performance on cognitive and personality tasks?
- **Basis in paper:** [explicit] The paper discusses differences between GPT-3.5 (dense transformer) and GPT-4 (sparse Mixture of Experts) and mentions impact of these architectural differences on model behavior is unclear
- **Why unresolved:** The paper does not provide comprehensive analysis of how different model architectures and sizes affect performance on tasks used in the study
- **What evidence would resolve it:** Direct comparisons of performance of language models with different architectures and sizes on standardized set of cognitive and personality tasks could help elucidate relationship

## Limitations
- Self-report measures with models that may not possess stable internal states make interpretation of "personality" or "mental health" problematic
- Observed variability could stem from multiple sources including training data diversity, model architecture changes, or prompt engineering effects
- The "pretend not to be AI" prompt may itself introduce confounding variability in responses

## Confidence
- **High Confidence:** The observation of significant test-retest variability in LLM responses is robust and well-documented
- **Medium Confidence:** The interpretation that this variability indicates absence of episodic memory is plausible but not definitively proven
- **Low Confidence:** Specific claims about models exhibiting "poor mental health" traits like narcissism or dissociation are highly questionable

## Next Checks
1. Conduct a controlled experiment administering identical personality tests to the same model across multiple sessions with systematic variation of prompt engineering (with/without safety constraints, different "pretend" instructions) to isolate the effect of prompting on response variability

2. Test the model's ability to maintain consistent persona across multiple turns within a single conversation versus across separate conversations, using memory-augmented prompting to assess whether contextual continuity reduces variability

3. Compare test-retest reliability across different model architectures (dense transformer vs. MoE) using identical prompts and measures to determine whether architectural differences contribute to observed variability patterns