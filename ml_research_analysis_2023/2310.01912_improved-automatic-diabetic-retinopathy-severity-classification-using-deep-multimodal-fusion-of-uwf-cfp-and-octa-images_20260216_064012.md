---
ver: rpa2
title: Improved Automatic Diabetic Retinopathy Severity Classification Using Deep
  Multimodal Fusion of UWF-CFP and OCTA Images
arxiv_id: '2310.01912'
source_url: https://arxiv.org/abs/2310.01912
tags:
- multimodal
- octa
- images
- uwf-cfp
- mixup
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of diabetic retinopathy (DR) severity
  classification using multimodal deep learning. The proposed method fuses Ultra-WideField
  Color Fundus Photography (UWF-CFP) and Optical Coherence Tomography Angiography
  (OCTA) images through a combination of ResNet50 and 3D-ResNet50 models with Squeeze-and-Excitation
  blocks.
---

# Improved Automatic Diabetic Retinopathy Severity Classification Using Deep Multimodal Fusion of UWF-CFP and OCTA Images

## Quick Facts
- arXiv ID: 2310.01912
- Source URL: https://arxiv.org/abs/2310.01912
- Reference count: 32
- Primary result: Multimodal fusion of UWF-CFP and OCTA images with Manifold Mixup achieves 0.8566 AUC for detecting mild NPDR, outperforming single-modality approaches

## Executive Summary
This paper presents a multimodal deep learning approach for diabetic retinopathy (DR) severity classification that combines Ultra-WideField Color Fundus Photography (UWF-CFP) and Optical Coherence Tomography Angiography (OCTA) images. The method employs separate ResNet50 and 3D-ResNet50 backbones with Squeeze-and-Excitation blocks to extract features from each modality, followed by feature-level fusion and classification. A multimodal extension of Manifold Mixup is applied to the concatenated features to enhance generalization. Experimental results on a clinical dataset demonstrate significant performance improvements over single-modality approaches, achieving an AUC of 0.8566 for detecting mild NPDR compared to 0.7983 for UWF-CFP alone and 0.8316 for OCTA alone.

## Method Summary
The approach processes 2D UWF-CFP images (1024×1024) through an SE-ResNet50 backbone and 3D OCTA volumes (224×224×224×2) through an SE-3D-ResNet50 backbone. Features from both modalities are concatenated and classified using Manifold Mixup regularization applied to the concatenated feature space. The model is trained with cross-entropy loss using the AdamW optimizer (learning rate 0.001, batch size 4) for 200 epochs. Six DR severity classes are used, with evaluation across four severity cutoffs (≥ mild NPDR, ≥ moderate NPDR, ≥ severe NPDR, ≥ PDR).

## Key Results
- Multimodal approach achieves 0.8566 AUC for detecting mild NPDR versus 0.7983 (UWF-CFP only) and 0.8316 (OCTA only)
- Ablation study confirms both SE blocks and Manifold Mixup contribute to improved performance
- Fusion strategy effectively captures complementary information from disparate 2D and 3D imaging modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Manifold Mixup applied to concatenated multimodal features improves generalization by creating synthetic training samples that smooth decision boundaries across modalities.
- Mechanism: The extension of Manifold Mixup operates on the joint feature representation layer, blending features from both UWF-CFP and OCTA modalities using a convex combination controlled by a Beta-distributed mixing coefficient. This forces the model to learn interpolations between modalities, reducing sensitivity to data distribution shifts.
- Core assumption: The concatenated feature space preserves modality-specific discriminative information while being amenable to interpolation without losing semantic meaning.
- Evidence anchors:
  - [abstract] "Additionally, to increase the model's generalization capabilities, a multimodal extension of Manifold Mixup, applied to concatenated multimodal features, is implemented."
  - [section] "Extending the concept of Input Mixup [26] to the hidden layers, Manifold Mixup serves as a robust regularization method that provokes neural networks to predict interpolated hidden representations with lesser confidence."
  - [corpus] Weak evidence - no directly related papers found in corpus that discuss Manifold Mixup for multimodal medical image fusion.
- Break condition: If interpolation in the concatenated feature space produces semantically invalid samples (e.g., mixing incompatible pathology features), the regularization effect could degrade rather than improve performance.

### Mechanism 2
- Claim: Feature-level fusion of UWF-CFP and OCTA images captures complementary information that single-modality approaches miss, leading to improved DR classification.
- Mechanism: Separate ResNet50 and 3D-ResNet50 backbones process each modality, extracting modality-specific features that are then concatenated and jointly classified. This allows each model to specialize in the spatial (2D) or volumetric (3D) characteristics of its input while benefiting from shared decision-making.
- Core assumption: UWF-CFP and OCTA images contain complementary pathology information that, when combined, provides a more complete representation of DR severity.
- Evidence anchors:
  - [abstract] "Our approach integrates 2D UWF-CFP images and 3D high-resolution 6x6 mm3 OCTA (both structure and flow) images using a fusion of ResNet50 and 3D-ResNet50 models..."
  - [section] "leveraging the information from both could potentially enhance the diagnosis and classification of DR [8, 24]. However, the integration of these modalities poses a significant challenge due to the disparate nature of the data they produce..."
  - [corpus] Weak evidence - corpus neighbors focus on other DR classification approaches but don't directly compare multimodal fusion performance.
- Break condition: If the modalities are not sufficiently complementary (e.g., redundant information), the fusion may not provide meaningful performance gains and could even add noise.

### Mechanism 3
- Claim: Squeeze-and-Excitation blocks improve classification by recalibrating channel-wise feature responses to emphasize more informative features for DR detection.
- Mechanism: SE blocks dynamically weight feature channels based on learned interdependencies, allowing the network to focus on features most relevant to DR pathology while suppressing less useful ones.
- Core assumption: DR pathology manifests in specific feature channels that can be identified and weighted more heavily through SE mechanisms.
- Evidence anchors:
  - [section] "Squeeze-and-Excitation (SE) blocks to amplify relevant features" and "SE blocks dynamically recalibrate channel-wise feature responses by explicitly modeling the interdependencies between channels, thus helping the model focus on more informative features."
  - [section] "Omitting the SE blocks caused a decrease in AUC scores across all DR severities."
  - [corpus] Weak evidence - no direct corpus support for SE blocks in multimodal DR classification.
- Break condition: If the optimal feature weighting changes significantly between training and deployment domains, the SE blocks could become counterproductive.

## Foundational Learning

- Concept: Convolutional Neural Networks for image feature extraction
  - Why needed here: ResNet50 and 3D-ResNet50 are CNN architectures that extract hierarchical spatial and volumetric features from UWF-CFP and OCTA images respectively
  - Quick check question: What is the difference between 2D convolution and 3D convolution in terms of input dimensions and what they capture?

- Concept: Multimodal fusion strategies
  - Why needed here: The paper employs feature-level fusion rather than input or decision-level fusion to combine information from disparate 2D and 3D imaging modalities
  - Quick check question: When would feature-level fusion be preferred over decision-level fusion for multimodal medical imaging?

- Concept: Regularization techniques in deep learning
  - Why needed here: Manifold Mixup is used to prevent overfitting and improve generalization by creating interpolated training samples in the feature space
  - Quick check question: How does Manifold Mixup differ from traditional data augmentation techniques like rotation or flipping?

## Architecture Onboarding

- Component map: UWF-CFP → SE-ResNet50 → Features; OCTA → SE-3D-ResNet50 → Features; Concatenate features → Classifier → DR severity output
- Critical path: Input images → Backbone feature extraction → Feature concatenation → Manifold Mixup regularization → Classification
- Design tradeoffs: Separate backbones allow modality-specific optimization but increase model complexity; feature-level fusion requires careful alignment of feature dimensions
- Failure signatures: Performance degradation when modalities are misaligned; overfitting if Manifold Mixup parameter α is too small; underfitting if SE blocks are too aggressive
- First 3 experiments:
  1. Train single-modality baselines (UWF-CFP only and OCTA only) to establish performance floor
  2. Implement multimodal fusion without Manifold Mixup or SE blocks to test baseline multimodal benefit
  3. Add Manifold Mixup with varying α parameters to find optimal regularization strength

## Open Questions the Paper Calls Out

- **Question**: How would the proposed multimodal approach perform on different DR severity grading systems beyond the 6-class labeling used in this study?
  - Basis in paper: [explicit] The paper mentions "each patient's eye was labeled by an ophthalmologist into one of the 6 DR classes" and evaluates performance across four DR severity cutoffs.
  - Why unresolved: The current evaluation is limited to the specific grading system used in the EviRed project. Different clinical settings may use alternative grading scales (e.g., ETDRS severity scale, International Clinical Diabetic Retinopathy Disease Severity Scale).
  - What evidence would resolve it: Testing the model on datasets using different grading systems and comparing performance metrics across these systems.

- **Question**: Would extending Manifold Mixup to additional layers beyond the concatenation layer further improve model performance?
  - Basis in paper: [explicit] "Introducing Manifold Mixup at different levels of the model, rather than solely at the concatenation layer, could provide further regularization and performance improvements."
  - Why unresolved: The current implementation applies Manifold Mixup only at the concatenation layer. The optimal layers for applying this regularization technique remain unknown.
  - What evidence would resolve it: Systematic experimentation with Manifold Mixup applied at different layers throughout the network, comparing performance metrics across these configurations.

- **Question**: How would the model performance be affected if trained on larger datasets or through data augmentation techniques?
  - Basis in paper: [inferred] The dataset included 875 eyes from 444 patients, and the paper discusses generalization capabilities and mentions that "each OCTA volume includes 2-D en-face localizer, structural, and flow 3D volumes."
  - Why unresolved: The current study uses a specific dataset size with inherent limitations, and the paper mentions hardware constraints for 3D volumes. The relationship between dataset size, augmentation strategies, and model performance remains unexplored.
  - What evidence would resolve it: Training the model on progressively larger datasets or with various augmentation techniques while monitoring performance metrics and generalization capabilities.

## Limitations
- Dataset composition and size not fully specified, limiting generalizability assessment
- Ablation study doesn't quantify individual contributions of SE blocks and Manifold Mixup separately
- No validation on independent datasets to assess real-world clinical applicability

## Confidence
- **High confidence**: The multimodal fusion approach outperforms single-modality baselines, as evidenced by direct experimental comparisons
- **Medium confidence**: The specific mechanisms of Manifold Mixup and SE blocks contributing to performance improvements, given the lack of detailed ablation results and limited corpus support
- **Low confidence**: Generalization to broader clinical populations without validation on independent datasets

## Next Checks
1. **Dataset Diversity Validation**: Test the model on an independent, multi-center dataset to assess real-world generalization beyond the original training population
2. **Ablation Study Refinement**: Conduct controlled experiments isolating the effects of SE blocks and Manifold Mixup by testing combinations (fusion only, fusion+SE, fusion+Manifold Mixup, all three) to quantify individual contributions
3. **Clinical Interpretability Analysis**: Perform feature attribution studies to verify that the model is learning clinically meaningful patterns from both UWF-CFP and OCTA modalities, not just exploiting artifacts or correlations