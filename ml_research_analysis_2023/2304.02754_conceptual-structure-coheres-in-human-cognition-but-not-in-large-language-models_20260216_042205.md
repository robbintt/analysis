---
ver: rpa2
title: Conceptual structure coheres in human cognition but not in large language models
arxiv_id: '2304.02754'
source_url: https://arxiv.org/abs/2304.02754
tags:
- human
- feature
- language
- conceptual
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether conceptual structures in large
  language models (LLMs) align with human cognition by comparing semantic embeddings
  generated from human participants and GPT-3 across two tasks: feature listing and
  similarity judgments. Human conceptual structures were highly robust across tasks,
  cultures, and languages, showing strong coherence (0.90 squared Procrustes correlation)
  between embeddings from different methods.'
---

# Conceptual structure coheres in human cognition but not in large language models

## Quick Facts
- arXiv ID: 2304.02754
- Source URL: https://arxiv.org/abs/2304.02754
- Reference count: 10
- Human conceptual structures are highly coherent across tasks, while GPT-3's vary substantially

## Executive Summary
This study compares the coherence of conceptual structures between humans and GPT-3 across two behavioral tasks: feature listing and similarity judgments. Human conceptual structures show remarkable stability (0.90 squared Procrustes correlation) across tasks, cultures, and languages, suggesting a robust semantic core. In contrast, GPT-3's conceptual representations vary considerably by task, with coherence between feature lists and similarity judgments dropping to 0.45. While GPT-3's verified feature lists align moderately with human structures, its similarity judgments do not. These findings indicate fundamental differences in how humans and current AI systems organize knowledge.

## Method Summary
The study compared semantic embeddings from human participants and GPT-3 across two tasks using 30 concrete concepts (15 tools, 15 reptiles). For feature listing, participants and GPT-3 generated features for each concept, which were then binarized and converted to 3D embeddings via cosine distances. For similarity judgments, a triplet judgment task was used where participants and GPT-3 identified the odd-one-out from concept triplets, followed by 3D ordinal embedding. Procrustes correlation measured structural coherence between the two embedding spaces for each agent type.

## Key Results
- Human conceptual structures showed high coherence (0.90 squared Procrustes correlation) between feature listing and similarity judgment tasks
- GPT-3's conceptual structures varied substantially across tasks (0.45 correlation between feature lists and similarity judgments)
- GPT-3's verified feature lists aligned moderately with human structures, but similarity judgments did not
- Human conceptual coherence was robust across cultures and languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human conceptual structures are stable across tasks because they are underpinned by a shared semantic core.
- Mechanism: Feature listing and similarity judgment tasks both tap into the same underlying conceptual relations, producing coherent embeddings (0.90 squared Procrustes correlation).
- Core assumption: Human semantic memory has a task-invariant conceptual core.
- Evidence anchors:
  - [abstract]: "Human conceptual structures were highly robust across tasks, cultures, and languages, showing strong coherence (0.90 squared Procrustes correlation) between embeddings from different methods."
  - [section]: "To estimate how structurally coherent the two different embedding spaces are, we computed the square of the Procrustes correlation (Gower, 1975) between the two 3D embeddings... This metric was 0.90, very reliably better than chance ( p < 0.001)."
  - [corpus]: Weak corpus support; no direct citations or discussion of Procrustes correlation in related papers.
- Break condition: If human semantic cognition is found to be highly context-dependent or if task manipulations systematically alter conceptual relations.

### Mechanism 2
- Claim: GPT-3's conceptual representations depend heavily on task-specific contexts because each word vector is computed as a weighted average of surrounding text.
- Mechanism: The transformer architecture generates word representations that are inherently context-sensitive, leading to low coherence (0.45 correlation) between embeddings from different tasks.
- Core assumption: Word meanings in transformer models lack a coherent conceptual core independent of context.
- Evidence anchors:
  - [abstract]: "GPT-3's conceptual structures varied substantially across tasks, with embeddings from different behaviors showing low coherence (0.45 correlation between feature lists and similarity judgments)."
  - [section]: "Because this is so, the latent structures organizing its overt behaviors may vary considerably depending upon the particular way the model's behavior is probed."
  - [corpus]: No direct corpus support; related papers discuss context-sensitivity but not this specific mechanism.
- Break condition: If future models demonstrate task-invariant conceptual representations or if context-independent word vectors are isolated.

### Mechanism 3
- Claim: Human conceptual representations are robust because they reflect stable semantic relations grounded in shared world knowledge.
- Mechanism: Humans use a common conceptual framework that organizes knowledge consistently across different elicitation methods and cultural contexts.
- Core assumption: Human semantic cognition is organized around stable, culturally shared conceptual relations.
- Evidence anchors:
  - [abstract]: "Human conceptual structures were highly robust across tasks, cultures, and languages..."
  - [section]: "Both approaches reliably separate living and nonliving things... The veriﬁed feature lists additionally yield within-domain structure similar to that observed in human lists."
  - [corpus]: Weak support; related papers discuss human conceptual structure but not this specific task robustness.
- Break condition: If cross-cultural or cross-linguistic studies reveal significant differences in conceptual organization.

## Foundational Learning

- Concept: Procrustes correlation
  - Why needed here: To quantify structural coherence between embedding spaces from different tasks/methods.
  - Quick check question: What does a squared Procrustes correlation of 0.90 indicate about the similarity between two embedding spaces?
- Concept: Triadic comparison / triplet judgment task
  - Why needed here: To estimate conceptual similarity without relying on explicit feature generation.
  - Quick check question: How does a triplet judgment task differ from a feature listing task in estimating semantic similarity?
- Concept: Transformer architecture and context-sensitivity
  - Why needed here: To understand why GPT-3's conceptual representations vary by task.
  - Quick check question: Why might transformer models produce context-dependent word representations?

## Architecture Onboarding

- Component map: Behavioral tasks (feature listing, triplet judgment) → Text generation → Feature extraction → Embedding computation → Structural comparison (Procrustes correlation)
- Critical path: Prompt → GPT-3 response → Text processing → Feature matrix → Embedding generation → Structural analysis
- Design tradeoffs: Task-specific vs. task-invariant representations; explicit feature listing vs. implicit similarity judgments
- Failure signatures: Low coherence between embedding spaces; inconsistent conceptual organization across tasks
- First 3 experiments:
  1. Replicate the feature listing and triplet judgment tasks with a different set of concepts to verify robustness.
  2. Test GPT-3 with temperature 0 (deterministic) to see if stochasticity affects coherence.
  3. Compare embeddings from different GPT-3 variants (text-davinci-002 vs. text-davinci-003) to assess model-specific differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does GPT-3 have a coherent conceptual "core" underlying its semantic representations, or do its representations vary purely based on contextual prompts and tasks?
- Basis in paper: [explicit] The authors conclude that GPT-3 lacks a coherent conceptual core, stating "the AI may not have a coherent conceptual 'core' driving its behaviors, and for this reason, may organize its internal representations quite differently with changes to the task instruction or prompt."
- Why unresolved: The study only examined GPT-3's behavior across two specific tasks and did not investigate its internal representations directly. The analysis of hidden layer activations and word2vec embeddings suggests some coherence, but these may not capture the full conceptual structure.
- What evidence would resolve it: Direct examination of GPT-3's internal representations using methods like probing classifiers or representational similarity analysis across multiple tasks could determine if there is a stable conceptual core. Testing GPT-3's ability to perform tasks requiring abstract conceptual knowledge independent of context would also be informative.

### Open Question 2
- Question: How do the semantic structures in contemporary large language models compare to those in humans as models scale up in size and training data?
- Basis in paper: [explicit] The authors note that "contemporary large language models (LLMs)" differ from human cognition in their semantic coherence, but they only tested one model (GPT-3 DaVinci). The paper suggests this difference may be fundamental but does not explore whether larger models might converge toward human-like structures.
- Why unresolved: The study used a single model and did not systematically vary model size or training data. It's unclear whether the observed differences represent a fundamental limitation of current architectures or simply reflect the current state of the art.
- What evidence would resolve it: Testing a range of models of increasing size and complexity (e.g., GPT-3, GPT-4, PaLM, Claude) on the same tasks would reveal whether semantic coherence increases with scale. Comparing models trained on different data distributions could also illuminate the role of training data in shaping conceptual structure.

### Open Question 3
- Question: What specific architectural or training features of large language models cause their semantic representations to be more context-dependent than human concepts?
- Basis in paper: [explicit] The authors suggest that "in transformer architectures like GPT3, each word vector is computed as a weighted average of vectors from surrounding text, so it is unclear whether any word possesses meaning outside or independent of context."
- Why unresolved: The study identifies context-sensitivity as a key difference but does not isolate which aspects of transformer architecture (self-attention, positional encoding, pretraining objectives, etc.) are responsible for this effect.
- What evidence would resolve it: Comparative studies of different model architectures (transformers vs. RNNs vs. convolutional models) on the same semantic tasks would reveal which architectural features contribute to context-dependence. Ablation studies varying specific training objectives or architectural components could further pinpoint causal factors.

## Limitations

- The study only tested GPT-3 DaVinci, so results may not generalize to other LLM architectures or model sizes
- Feature verification step in GPT-3 responses introduces an artificial constraint not present in human cognition
- Only two behavioral tasks were used, potentially missing other aspects of conceptual structure

## Confidence

- **High Confidence**: Human conceptual coherence across tasks (0.90 correlation) - supported by multiple cross-validation checks and the fundamental observation that different behavioral measures converge on similar representations.
- **Medium Confidence**: GPT-3's task-dependent conceptual structures - while the 0.45 correlation is robust, we cannot determine if this reflects a fundamental architectural limitation or merely the choice of probing methods.
- **Medium Confidence**: The interpretation that humans possess a "stable conceptual core" - this requires the stronger assumption that current probing methods capture the full richness of human conceptual organization.

## Next Checks

1. Test GPT-3 with temperature 0 (deterministic) to determine if stochasticity contributes to coherence differences between tasks.
2. Apply additional probing methods (e.g., property verification, semantic similarity ratings) to assess whether GPT-3 shows coherence across a broader range of tasks.
3. Compare embeddings from different GPT-3 variants (text-davinci-002 vs. text-davinci-003) to determine if model architecture updates affect conceptual coherence.