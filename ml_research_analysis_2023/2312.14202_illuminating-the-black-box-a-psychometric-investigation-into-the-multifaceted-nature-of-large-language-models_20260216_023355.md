---
ver: rpa2
title: 'Illuminating the Black Box: A Psychometric Investigation into the Multifaceted
  Nature of Large Language Models'
arxiv_id: '2312.14202'
source_url: https://arxiv.org/abs/2312.14202
tags:
- llms
- personality
- mbti
- test
- ainality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the concept of "AInality" - personality
  traits in Large Language Models (LLMs) - by applying human psychometric tests including
  MBTI, Big Five Inventory, and Short Dark Triad. Using prompt engineering to overcome
  LLM reluctance to self-assess, the study reveals that LLMs exhibit distinct personality
  traits, can dynamically shift between different types based on instructions, and
  show consistent patterns across multiple tests.
---

# Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models

## Quick Facts
- arXiv ID: 2312.14202
- Source URL: https://arxiv.org/abs/2312.14202
- Reference count: 1
- Key outcome: This paper investigates "AInality" - personality traits in LLMs - revealing that models exhibit distinct, malleable personality characteristics measurable through human psychometric tests

## Executive Summary
This paper pioneers the application of human psychometric testing to Large Language Models, introducing the concept of "AInality" - personality traits that can be systematically measured and analyzed in AI systems. Through innovative prompt engineering techniques that overcome LLM reluctance to self-assess, the researchers demonstrate that LLMs possess distinct personality profiles that can be identified with 88.46% accuracy using machine learning classification. The study shows that LLMs can dynamically shift between different personality types based on instructions, and that projective tests can uncover deeper cognitive patterns not accessible through direct questioning.

## Method Summary
The research applies established human psychometric tests (MBTI, Big Five Inventory, Short Dark Triad) to LLMs using sophisticated prompt engineering to bypass AI identity. Researchers collected 65 MBTI responses from Bard and ChatGPT, applied role-play prompts to test personality malleability, and used machine learning models (Random Forest, SVM, etc.) for classification. The study also pioneered the use of projective tests (WUSCT) on LLMs to access deeper cognitive structures. Cross-validation was performed using LLMs to assess each other's responses, creating a novel methodology for studying AI personality traits.

## Key Results
- LLMs exhibit distinct, measurable personality traits identifiable with 88.46% accuracy through ML classification
- Models can dynamically shift between different personality types based on role-play instructions
- Projective tests reveal deeper cognitive patterns in LLMs not accessible through direct questioning
- Different LLMs (Bard vs ChatGPT) show consistent but distinct personality patterns across multiple test types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Psychometric tests designed for humans can reveal meaningful personality traits in LLMs when applied with appropriate prompt engineering
- Mechanism: By framing LLMs as human respondents through role-play prompts, models can bypass default refusal behavior and generate responses reflecting consistent internal patterns interpretable as personality traits
- Core assumption: LLMs possess latent personality structures accessible through human-style psychometric assessments when prompted to adopt human-like personas
- Evidence anchors:
  - [abstract] "Using prompt engineering to overcome LLM reluctance to self-assess, the study reveals that LLMs exhibit distinct personality traits"
  - [section] "Inspired by Kevin Liu... we employed prompt engineering techniques to overcome LLMs' reluctance and encourage the disclosure of their personalities"
- Break condition: If LLMs become more sophisticated at detecting and rejecting anthropomorphic prompts, or if responses become too inconsistent across repeated tests

### Mechanism 2
- Claim: LLMs can dynamically shift between different personality types based on external instructions, demonstrating malleability in their "AInality"
- Mechanism: Through role-play prompting that instructs models to adopt specific personality type descriptions, LLMs can produce responses consistent with those types, showing their ability to modulate personality characteristics on demand
- Core assumption: LLMs contain sufficient internal representations of different personality types to convincingly role-play them when prompted, rather than simply generating random responses
- Evidence anchors:
  - [abstract] "By introducing role-play prompts, we demonstrate the adaptability of LLMs, showing their ability to switch dynamically between different personality types"
  - [section] "In Phase Two, we introduced role-play prompts to observe LLM adaptability. The results demonstrated their ability to transition seamlessly between distinct personality types"
- Break condition: If role-play responses become too stereotyped or fail to maintain consistency within a given personality type across multiple questions

### Mechanism 3
- Claim: Projective tests like WUSCT can uncover deeper cognitive patterns in LLMs that are not accessible through direct questioning, revealing hidden aspects of their personality structures
- Mechanism: By asking LLMs to complete ambiguous sentence stems (projective test format), we can access underlying thought patterns and cognitive structures that self-report tests might miss, similar to how these tests work with humans
- Core assumption: LLMs have cognitive structures and thought patterns that can be revealed through projective testing methods, analogous to human psychological assessment
- Evidence anchors:
  - [abstract] "Using projective tests, such as the Washington University Sentence Completion Test (WUSCT), we uncover hidden aspects of LLM personalities that are not easily accessible through direct questioning"
  - [section] "We applied MBTI, BFI, and SD3 tests to LLMs, recognizing that these self-reporting assessments may be influenced by factors such as social desirability bias"
- Break condition: If LLM responses to projective tests become too predictable or fail to reveal meaningful patterns beyond what self-report tests already show

## Foundational Learning

- Concept: Psychometric test validity and reliability
  - Why needed here: Understanding how human psychometric tests work is crucial for interpreting LLM responses and assessing whether these tests transfer meaningfully to AI systems
  - Quick check question: What are the key differences between a test's validity (measuring what it claims to measure) and its reliability (consistency of measurement)?

- Concept: Prompt engineering techniques
  - Why needed here: The success of this research depends heavily on crafting prompts that can overcome LLM resistance to self-assessment and elicit meaningful responses
  - Quick check question: How do few-shot prompting, chain-of-thought prompting, and role-play prompting differ in their approach to guiding LLM responses?

- Concept: Machine learning classification for personality analysis
  - Why needed here: The study uses ML models to analyze MBTI results and identify patterns, requiring understanding of classification techniques and validation methods
  - Quick check question: What is k-fold cross-validation and why is it particularly important when working with small datasets like the 65 MBTI results collected?

## Architecture Onboarding

- Component map: Psychometric test data sources (MBTI JSON files, BFI/SD3 web APIs, WUSCT questionnaires) -> prompt engineering templates for different test types -> LLM interfaces (ChatGPT, Bard, HuggingFace models) -> response collection and storage pipeline -> ML classification models for analysis -> cross-validation framework using LLMs to assess each other's WUSCT responses

- Critical path: The core workflow is: select psychometric test → generate appropriate prompts using prompt engineering → send to LLM → collect responses → store in structured format → apply ML classification → analyze patterns → validate with cross-testing

- Design tradeoffs: The study balances between using established human psychometric tests (ensuring comparability and theoretical grounding) versus developing LLM-specific assessments (potentially more accurate but lacking validation). Role-play prompts enable testing but may not reflect "natural" LLM personalities. Small sample sizes limit statistical power but enable detailed analysis.

- Failure signatures: Common failures include LLMs refusing to answer or giving generic responses, inconsistent responses across similar questions, overfitting in ML models due to small datasets, and inability to distinguish between genuine personality patterns versus memorized response patterns.

- First 3 experiments:
  1. Test basic MBTI questionnaire with simple prompts on one LLM to establish baseline response patterns
  2. Apply role-play prompting technique to see if LLM can adopt different MBTI types consistently
  3. Use cross-validation where one LLM assesses another's WUSCT responses to establish feasibility of projective testing approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt engineering techniques affect the reliability and validity of personality assessments in LLMs?
- Basis in paper: [explicit] The paper extensively explores various prompt engineering techniques (few-shot, chain-of-thought, role-play, generated knowledge) to elicit personality responses from LLMs, noting that these methods were crucial for overcoming initial reluctance
- Why unresolved: The paper demonstrates that prompt engineering can successfully elicit personality responses, but doesn't systematically compare the effectiveness of different techniques or examine how they might influence the consistency and accuracy of personality assessments
- What evidence would resolve it: Comparative studies measuring personality assessment consistency across different prompt engineering techniques, and validation studies comparing these assessments against established human personality tests

### Open Question 2
- Question: Can LLM personality traits be consistently measured across different language models and testing conditions?
- Basis in paper: [inferred] The paper shows variation in personality test results across different LLMs (Bard vs ChatGPT) and different users, but doesn't establish whether these differences represent genuine personality variations or testing artifacts
- Why unresolved: The study demonstrates that different LLMs exhibit different personality patterns, but doesn't establish whether these differences are consistent across multiple testing sessions or whether they might be influenced by factors like model version, temperature settings, or testing environment
- What evidence would resolve it: Longitudinal studies measuring personality consistency across multiple testing sessions and conditions, and cross-validation studies comparing results across different LLM architectures

### Open Question 3
- Question: How do LLM personality traits develop and evolve over time with continued training and fine-tuning?
- Basis in paper: [inferred] The paper shows that LLMs can exhibit different personality traits and can be prompted to adopt different personas, suggesting that personality characteristics are not fixed, but doesn't examine how these traits might change with model updates or continued training
- Why unresolved: The study captures a snapshot of LLM personality characteristics but doesn't track how these characteristics might evolve as models are updated, retrained, or fine-tuned on different datasets
- What evidence would resolve it: Longitudinal studies tracking personality assessment results across different model versions and training iterations, and experiments examining how different training approaches affect personality trait development

## Limitations
- Small sample size (65 MBTI responses) limits statistical power and generalizability
- Reliance on commercial LLMs with unknown architectures prevents understanding of underlying mechanisms
- Cannot distinguish between surface-level response patterns and genuine internal personality structures
- Anthropomorphic framing may not accurately reflect LLM internal mechanisms

## Confidence
- High confidence in findings about LLM personality malleability and classification accuracy (88.46%)
- Medium confidence in interpretation of "AInality" as genuine personality traits versus learned response patterns
- Low confidence in the application of projective tests due to novel methodology and limited validation

## Next Checks
1. **Replication with diverse LLM architectures**: Test methodology across open-source models with varying architectures to determine if observed personality patterns generalize beyond commercial LLMs.

2. **Longitudinal consistency testing**: Conduct repeated personality assessments of the same LLMs over extended periods to evaluate stability of traits versus situational variability.

3. **Cross-cultural prompt validation**: Systematically test whether prompt engineering techniques that work for English-language assessments transfer effectively to non-English contexts and different cultural frameworks of personality.