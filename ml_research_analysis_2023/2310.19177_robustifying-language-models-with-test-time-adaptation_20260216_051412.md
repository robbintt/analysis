---
ver: rpa2
title: Robustifying Language Models with Test-Time Adaptation
arxiv_id: '2310.19177'
source_url: https://arxiv.org/abs/2310.19177
tags: []
core_contribution: The paper addresses the vulnerability of large-scale language models
  to adversarial attacks, where imperceptible modifications to inputs cause misclassification.
  Prior work focuses on training-time robustness, but this is unrealistic for large
  foundation models.
---

# Robustifying Language Models with Test-Time Adaptation

## Quick Facts
- arXiv ID: 2310.19177
- Source URL: https://arxiv.org/abs/2310.19177
- Reference count: 8
- Key outcome: Test-time adaptation method that reverses over 65% of successful adversarial attacks while maintaining high semantic similarity

## Executive Summary
This paper addresses the vulnerability of large-scale language models to adversarial attacks by proposing a test-time adaptation method called Mask-Defense. Unlike prior work that focuses on training-time robustness, Mask-Defense leverages masked language modeling to dynamically adapt input sentences and reverse adversarial attacks without requiring additional training. The method identifies high-importance words that lead to misclassification and replaces them with semantically similar normal words, achieving success rates of 75-80% on TextFooler attacks and 65-70% on PWWS attacks.

## Method Summary
The Mask-Defense method works by using masked language modeling (MLM) to identify important words in adversarial sentences and replace them with semantically similar alternatives. For each adversarial sentence, the algorithm masks each word, computes MLM loss and top predictions, ranks words by importance (high MLM loss), and replaces up to three words with semantically similar MLM predictions based on cosine similarity of word embeddings. The method is evaluated on two text classification datasets (AG's News and Yelp Polarity) against TextFooler and PWWS adversarial attacks, measuring classification accuracy and semantic similarity between original and modified sentences.

## Key Results
- Mask-Defense reverses over 65% of successful adversarial attacks on tested datasets
- The method maintains high semantic similarity with original sentences (typically above 0.9 cosine similarity)
- Achieved 75-80% success rate on TextFooler attacks and 65-70% on PWWS attacks
- Classification accuracy improves significantly after applying the defense mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masked Language Modeling (MLM) loss serves as a proxy for word importance in adversarial contexts.
- Mechanism: Words that are perturbed by an attack will often be semantically out of place, causing the MLM to have higher uncertainty (cross-entropy loss) when predicting their masked replacements. High-loss words are prioritized for substitution.
- Core assumption: The adversarial attack introduces words that the MLM model finds unlikely or improbable in context, and that this improbability can be measured via MLM loss.
- Evidence anchors: [abstract] "By dynamically adapting the input sentence with predictions from masked words, we show that we can reverse many language adversarial attacks." [section] "The justification for using this score is simple: A masked word that only has a few potential candidates will end having a low importance... On the other hand, 'I saw a big [Mask].' could have many valid words and therefore will have a high importance and will be a good candidate for substitution."
- Break condition: If the adversarial perturbation produces words that are still contextually plausible, the MLM loss will not effectively rank them for replacement, and the defense will fail.

### Mechanism 2
- Claim: Semantic similarity filtering ensures that substituted words preserve meaning.
- Mechanism: After MLM suggests candidate replacements, cosine similarity in embedding space is used to reject words that are semantically too different from the original. This prevents semantic drift.
- Core assumption: A word embedding space that captures synonymy is available and can be used to quantify semantic similarity reliably.
- Evidence anchors: [section] "To determine whether two words are similar, we build a cosine similarity matrix C over the entire embeddings, and take the distance between the original and new word." [section] "We use word embeddings from Mrkˇsi´c et al. (2016), which were designed to capture synonymy."
- Break condition: If the embedding space does not capture the relevant semantic distinctions or the threshold is set too loosely/tightly, semantically invalid replacements may be accepted or valid ones rejected.

### Mechanism 3
- Claim: Limiting the number of word replacements controls semantic drift while maintaining attack reversal capability.
- Mechanism: The algorithm restricts the number of substitutions (e.g., n=3) to avoid over-modifying the sentence, preserving meaning while correcting adversarial perturbations.
- Core assumption: A small number of high-importance word substitutions is sufficient to reverse most adversarial attacks.
- Evidence anchors: [section] "Replacing a fraction of words in a sentence instead of a fixed number was considered for longer sentences, but experiments showed a diminishing effect." [section] "Table 2: Experimental Results... only three words were allowed to be replaced."
- Break condition: If an attack requires more than the allowed number of substitutions to reverse, the defense will fail regardless of the importance ranking or semantic filtering.

## Foundational Learning

- Concept: Adversarial examples in NLP
  - Why needed here: The entire defense targets reversing adversarial attacks on text classifiers, so understanding how attacks manipulate inputs is essential.
  - Quick check question: What is the difference between character-level and word-level adversarial attacks in NLP?

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is the core self-supervised mechanism used to rank word importance and suggest replacements.
  - Quick check question: How does MLM loss relate to the probability of a word being correctly predicted?

- Concept: Semantic similarity in embedding space
  - Why needed here: Semantic similarity filtering ensures that replacements do not change the meaning of the sentence.
  - Quick check question: Why might cosine similarity in word embeddings be preferred over simple lexical overlap for semantic filtering?

## Architecture Onboarding

- Component map: Input sentence -> MLM model (for loss ranking and candidate generation) -> Similarity filter (embedding-based) -> Modified sentence -> Classifier (output checked)
- Critical path:
  1. Calculate MLM loss for each word by masking it and observing cross-entropy.
  2. Sort words by loss (importance).
  3. For each word in order, generate top MLM candidates.
  4. Filter candidates by semantic similarity threshold.
  5. Replace up to n words; output result.
- Design tradeoffs:
  - More replacements (higher n) may improve attack reversal but risk semantic drift.
  - Lower similarity threshold increases chance of finding a replacement but may allow semantically invalid substitutions.
  - Using a large MLM vocabulary improves candidate diversity but increases computation.
- Failure signatures:
  - No words meet the similarity threshold → no replacements made.
  - MLM loss does not distinguish perturbed words → no high-loss words found.
  - Similarity threshold too high → all candidates rejected.
  - Attack requires more than n replacements → defense fails.
- First 3 experiments:
  1. Run Mask-Defense on clean sentences to confirm <2% modification rate.
  2. Run on TextFooler-attacked sentences to measure reversal rate and semantic similarity preservation.
  3. Run on PWWS-attacked sentences to test robustness across attack types and compare performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of Mask-Defense scale with different sizes and architectures of language models beyond BERT?
- Basis in paper: [inferred] The paper evaluates Mask-Defense only on BERT and does not explore its performance on other language model architectures or sizes.
- Why unresolved: The study is limited to BERT, leaving open questions about the generalizability of the method to other models like GPT, RoBERTa, or larger variants of BERT.
- What evidence would resolve it: Experiments applying Mask-Defense to various language model architectures and sizes, comparing success rates and computational efficiency across models.

### Open Question 2
- Question: What is the impact of Mask-Defense on semantic preservation in longer and more complex sentences?
- Basis in paper: [inferred] While the paper reports high semantic similarity scores, it does not extensively analyze the impact on longer or more complex sentences beyond the datasets used.
- Why unresolved: The study uses specific datasets with relatively short sentences, and the effect on more complex or longer sentences remains unexplored.
- What evidence would resolve it: Conducting experiments on datasets with longer and more complex sentences, measuring semantic similarity and classification accuracy to assess the method's robustness.

### Open Question 3
- Question: Can Mask-Defense be adapted to handle adversarial attacks that involve character-level modifications or more sophisticated sentence-level attacks?
- Basis in paper: [explicit] The paper focuses on word-level adversarial attacks and does not address character-level or advanced sentence-level attacks.
- Why unresolved: The method is tailored for word-level modifications, and its effectiveness against other types of adversarial attacks is not tested.
- What evidence would resolve it: Testing Mask-Defense against character-level and advanced sentence-level adversarial attacks to determine its adaptability and effectiveness in these scenarios.

## Limitations
- The method's effectiveness depends heavily on the MLM model's ability to detect adversarial perturbations, which may not generalize to attacks that produce contextually plausible words.
- Semantic filtering relies on static word embeddings that may not capture nuanced semantic relationships or domain-specific meanings.
- The fixed limit of three word substitutions may be insufficient for more complex or longer sentences where attacks are distributed across multiple words.

## Confidence

**High Confidence**: The Mask-Defense method can reverse a significant portion of successful adversarial attacks (65-80%) on tested datasets.

**Medium Confidence**: The approach maintains semantic similarity between original and modified sentences after defense.

**Low Confidence**: The method will generalize to more complex adversarial attacks or larger, more diverse datasets without substantial hyperparameter tuning.

## Next Checks

1. Test Mask-Defense against adaptive attacks that specifically generate words designed to have low MLM loss while still being adversarial.
2. Evaluate semantic preservation using human judgments in addition to automated embedding-based similarity metrics.
3. Assess performance on longer documents and more complex classification tasks (e.g., multi-label classification or question answering) to determine scalability.