---
ver: rpa2
title: Can Large Language Models Empower Molecular Property Prediction?
arxiv_id: '2307.07443'
source_url: https://arxiv.org/abs/2307.07443
tags:
- uni00000013
- molecular
- uni00000011
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores the use of large language models (LLMs) to
  enhance molecular property prediction. It investigates two approaches: zero/few-shot
  molecular classification and using LLM-generated text explanations as new molecular
  representations.'
---

# Can Large Language Models Empower Molecular Property Prediction?

## Quick Facts
- arXiv ID: 2307.07443
- Source URL: https://arxiv.org/abs/2307.07443
- Reference count: 12
- Primary result: LLM-generated text explanations outperform traditional graph-based and SMILES-based methods in molecular property prediction tasks

## Executive Summary
This paper explores the use of large language models (LLMs) to enhance molecular property prediction. It investigates two approaches: zero/few-shot molecular classification and using LLM-generated text explanations as new molecular representations. The authors prompt ChatGPT to classify molecules and generate detailed explanations for SMILES strings. These explanations are then used as input to fine-tune a small LM for downstream tasks. Experiments on multiple benchmark datasets show that LLM-generated text explanations outperform traditional graph-based and SMILES-based methods, achieving significant improvements in accuracy and ROC-AUC. The results highlight the potential of LLMs in advancing molecular property prediction tasks.

## Method Summary
The method involves prompting ChatGPT for in-context molecular classification and generating textual explanations for SMILES strings, which are then used to fine-tune a small-scale LM (e.g., RoBERTa) for downstream tasks. The pipeline consists of input SMILES strings, ChatGPT prompt generation, LLM text explanation generation, small LM fine-tuning, and downstream task prediction. The approach leverages ChatGPT's ability to interpret SMILES notation and produce chemically relevant natural language descriptions.

## Key Results
- LLM-generated text explanations outperform traditional graph-based and SMILES-based methods
- CaR approach achieves superior results on almost all datasets, with a 53% improvement on the PTC dataset
- ChatGPT demonstrates improved performance with increased few-shot examples

## Why This Works (Mechanism)

### Mechanism 1
- ChatGPT can generate chemically meaningful text explanations for SMILES strings that capture functional groups and chemical properties
- The LLM leverages its world knowledge and language understanding to interpret SMILES notation and produce descriptions containing chemically relevant information
- Break condition: If the LLM lacks sufficient chemical knowledge or if SMILES notation is too cryptic

### Mechanism 2
- LLM-generated text explanations serve as effective molecular representations that improve prediction accuracy over traditional methods
- The rich semantic content in LLM explanations captures molecular properties difficult to represent in SMILES or graph structures alone
- Break condition: If explanations don't contain meaningful chemical information or if small LM fine-tuning cannot extract relevant features

### Mechanism 3
- In-context learning with ChatGPT can perform molecular classification tasks with few examples
- The LLM can understand classification tasks from examples provided in the prompt and apply this understanding to classify new molecules
- Break condition: If classification requires domain-specific knowledge beyond what can be conveyed in examples, or if LLM's reasoning is insufficient

## Foundational Learning

- Concept: SMILES notation and molecular graph representations
  - Why needed here: The paper compares LLM approaches against traditional SMILES-based and graph-based methods
  - Quick check question: What are the key differences between how SMILES strings and molecular graphs represent chemical structures?

- Concept: Large Language Models and in-context learning
  - Why needed here: The paper relies on ChatGPT's ability to perform few-shot classification and generate explanations
  - Quick check question: How does in-context learning differ from traditional fine-tuning in terms of model adaptation?

- Concept: Molecular property prediction tasks
  - Why needed here: The downstream tasks (classification and regression) determine what molecular properties the representations should capture
  - Quick check question: What are the typical evaluation metrics for molecular property prediction tasks and what do they measure?

## Architecture Onboarding

- Component map: Input SMILES → ChatGPT prompt generation → LLM text explanation generation → Small LM (e.g., RoBERTa) fine-tuning → Downstream task prediction
- Critical path: The quality of LLM-generated explanations directly impacts downstream performance
- Design tradeoffs: Using ChatGPT (powerful but expensive/API-limited) vs. smaller specialized LMs; using generated text vs. direct molecular representations; prompt engineering complexity vs. model performance
- Failure signatures: Poor performance on scaffold splits, inconsistent results across different prompts, failure to converge during small LM fine-tuning
- First 3 experiments:
  1. Verify ChatGPT can generate chemically meaningful explanations for a small set of molecules
  2. Test different prompt templates on a validation set to find most effective prompt structure
  3. Compare downstream task performance using different small LMs to validate CaR is not model-specific

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLMs (e.g., GPT-4, MolReGPT) perform compared to ChatGPT on molecular property prediction tasks?
- Basis in paper: The authors note that they primarily used ChatGPT as a representative LLM, but acknowledge that the performance of other LLMs on molecular data has yet to be explored
- Why unresolved: The paper only evaluates ChatGPT, leaving a gap in understanding how other LLMs might perform
- What evidence would resolve it: Conducting experiments with various LLMs, including GPT-4 and domain-specific models like MolReGPT, and comparing their performance on molecular property prediction tasks

### Open Question 2
- Question: How can the graph structure of molecules be better incorporated into the LLM-based molecular property prediction framework to improve performance?
- Basis in paper: The authors mention that they currently model molecular prediction tasks solely as NLP tasks, but acknowledge the crucial importance of the graph structure inherent in molecules
- Why unresolved: The paper does not explore methods to incorporate graph structure information into the LLM-based framework
- What evidence would resolve it: Developing and evaluating techniques that integrate graph-structured data with LLM-generated text explanations, and comparing the performance of these hybrid models

### Open Question 3
- Question: How can LLM-based representations be extended to handle large molecules with 3D structures, such as proteins and antibodies, that cannot be represented using SMILES strings?
- Basis in paper: The authors note that their current focus is on small molecule data represented as SMILES strings, but acknowledge the need for reasonable sequential representations for large molecules with 3D structures
- Why unresolved: The paper does not address the challenge of representing and processing large molecules with 3D structures using LLMs
- What evidence would resolve it: Designing and evaluating methods for converting 3D molecular structures into sequential representations that can be processed by LLMs

## Limitations
- Reliance on ChatGPT's outputs introduces cost barriers and reproducibility challenges
- Comparison methodology doesn't fully explore whether simpler approaches could achieve similar results
- Focus on English-language explanations may limit applicability for non-English scientific communities

## Confidence

**Major Uncertainties and Limitations:**

The paper's core claims rest on several significant uncertainties. The most critical limitation is the reliance on ChatGPT's outputs, which introduces both cost barriers and reproducibility challenges since different API calls may yield varying explanations. The paper lacks transparency about the specific prompts used, making exact replication difficult. While the results show impressive improvements (up to 53% on PTC dataset), these gains need to be weighed against the substantial computational overhead of generating LLM explanations for each molecule.

The comparison methodology also presents concerns. The paper evaluates against traditional methods but doesn't fully explore whether simpler approaches (like fine-tuning LMs directly on SMILES or using pre-trained molecular models) could achieve similar results at lower cost. Additionally, the focus on English-language explanations may limit applicability for non-English scientific communities.

**Confidence Assessment:**

High confidence in the core finding that LLM-generated explanations can improve molecular property prediction performance, as evidenced by consistent improvements across multiple datasets and tasks. The methodology is sound and the experimental design is robust.

Medium confidence in the scalability and practical utility of the approach, given the lack of cost-benefit analysis and the unknown performance on larger, more diverse molecular datasets. The dependence on API access to ChatGPT also raises concerns about long-term reproducibility.

Low confidence in the generalizability across different chemical domains, as the evaluation focuses primarily on pharmaceutical datasets. The approach's effectiveness for other molecular properties (like materials science or environmental chemistry) remains untested.

## Next Checks

1. **Prompt Engineering Validation**: Systematically test different prompt templates across a validation set to determine whether the claimed improvements are robust to prompt variations, or if they depend critically on specific prompt engineering.

2. **Cost-Performance Tradeoff Analysis**: Measure the computational cost (API calls, tokens processed) required to generate explanations versus the performance gains achieved, comparing this to the cost of training specialized molecular models from scratch.

3. **Cross-Domain Generalization Test**: Evaluate the approach on non-pharmaceutical molecular datasets (such as materials science or environmental chemistry datasets) to assess whether the improvements generalize beyond the current focus area.