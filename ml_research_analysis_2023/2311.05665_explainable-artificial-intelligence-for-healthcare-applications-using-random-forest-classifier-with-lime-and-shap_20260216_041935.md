---
ver: rpa2
title: Explainable artificial intelligence for Healthcare applications using Random
  Forest Classifier with LIME and SHAP
arxiv_id: '2311.05665'
source_url: https://arxiv.org/abs/2311.05665
tags:
- explanations
- explainable
- methods
- page
- interpretable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This chapter explores explainable artificial intelligence (XAI)
  to improve transparency and interpretability in healthcare applications. The authors
  investigate post-hoc XAI methods, specifically LIME and SHAP, to interpret a Random
  Forest classifier for diabetes prediction.
---

# Explainable artificial intelligence for Healthcare applications using Random Forest Classifier with LIME and SHAP

## Quick Facts
- arXiv ID: 2311.05665
- Source URL: https://arxiv.org/abs/2311.05665
- Reference count: 38
- Primary result: Investigates post-hoc XAI methods (LIME and SHAP) to interpret Random Forest classifier for diabetes prediction, providing feature importance and local explanations

## Executive Summary
This chapter explores explainable artificial intelligence (XAI) to improve transparency and interpretability in healthcare applications. The authors investigate post-hoc XAI methods, specifically LIME and SHAP, to interpret a Random Forest classifier for diabetes prediction. They apply these methods to a publicly available diabetes dataset and demonstrate how they can provide feature importance and local explanations for individual predictions. The study highlights the importance of XAI in enhancing trust and understanding of AI models in healthcare, especially for critical applications like disease diagnosis.

## Method Summary
The study uses a Random Forest classifier as a black-box model trained on a publicly available diabetes dataset from Kaggle. The Random Forest model is used to predict diabetes, and then SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) methods are applied to provide global and local interpretability. SHAP is used to compute global feature importance and generate local explanations using Shapley values, while LIME fits locally interpretable surrogate models near individual predictions. The approach is model-agnostic, requiring no access to the internal structure of the Random Forest.

## Key Results
- SHAP and LIME successfully provide interpretable explanations for Random Forest diabetes predictions
- Global feature importance rankings identify key diabetes risk factors
- Local explanations offer insight into individual prediction decisions
- Model-agnostic XAI preserves Random Forest's predictive performance while adding transparency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SHAP and LIME provide post-hoc interpretability by approximating local model behavior around individual predictions.
- Mechanism: SHAP uses Shapley values from cooperative game theory to quantify each feature's marginal contribution across all possible feature coalitions; LIME fits a locally interpretable surrogate model (e.g., linear) near the prediction of interest.
- Core assumption: Feature independence is either negligible or handled by the model's sampling strategy, and the local neighborhood is representative of the decision boundary.
- Evidence anchors:
  - [abstract] "investigate post-hoc XAI methods, specifically LIME and SHAP, to interpret a Random Forest classifier for diabetes prediction"
  - [section] "SHapley Additive exPlanations (SHAP) dependence plot is a local, model-agnostic xAI framework which is used for image, tabular and text datasets for interpretability with Shaply values"
  - [corpus] Weak: no direct citation found in neighbors; mechanism relies on textbook definitions.
- Break condition: Severe feature correlation or adversarial perturbations outside the sampled neighborhood.

### Mechanism 2
- Claim: Global feature importance ranking via SHAP values informs domain experts about dominant predictors for disease presence.
- Mechanism: Aggregate SHAP values across the dataset to compute mean absolute contributions; higher values indicate greater global influence on the model's predictions.
- Core assumption: The model's decision boundary is stable enough that global importance rankings remain consistent across subpopulations.
- Evidence anchors:
  - [abstract] "demonstrate how they can provide feature importance and local explanations for individual predictions"
  - [section] "Global feature importance in the dataset is presented in Figure 6.5 using SHAP explainer" and "SHAP values with explanations for model output"
  - [corpus] Weak: neighbors discuss XAI broadly but lack diabetes-specific SHAP use cases.
- Break condition: Concept drift or subpopulation-specific feature relevance not captured by global averages.

### Mechanism 3
- Claim: Model-agnostic XAI preserves Random Forest's predictive performance while adding transparency.
- Mechanism: LIME and SHAP treat the trained Random Forest as a black box; no model internals are accessed, only predictions and input features are used to generate explanations.
- Core assumption: The surrogate models approximate the true decision surface accurately enough for trustworthy explanations.
- Evidence anchors:
  - [section] "random forest machine learning model is used on a publicly available diabetes dataset for prediction of the diabetes and then SHAP and LIME methods are used for its explanability"
  - [section] "Model-agnostic XAI methods, even though inner details and structure of the models cannot be accessed but it will provide an understanding for any of the AI model being used earlier"
  - [corpus] Weak: no explicit neighbor evidence of model-agnostic post-hoc XAI for Random Forest.
- Break condition: Surrogate model fidelity falls below acceptable thresholds, leading to misleading explanations.

## Foundational Learning

- Concept: Shapley value theory from cooperative game theory
  - Why needed here: Provides the theoretical foundation for SHAP's feature contribution calculations.
  - Quick check question: What property must a value allocation satisfy to fairly distribute payoff among players in a cooperative game?

- Concept: Local surrogate modeling (LIME)
  - Why needed here: Enables approximate interpretability of complex models without accessing internal structure.
  - Quick check question: How does LIME ensure that its local linear approximation remains faithful to the original model's predictions?

- Concept: Feature importance vs. feature attribution
  - Why needed here: Distinguishes between global ranking of predictors and local explanation of individual predictions.
  - Quick check question: In what scenario would a feature be globally important but locally negligible for a specific instance?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline -> Random Forest training -> Prediction generation -> SHAP explainer -> LIME explainer -> Visualization layer
- Critical path:
  Raw data -> Cleaned dataset -> Model fit -> Local explanations for high-risk predictions
- Design tradeoffs:
  SHAP: High fidelity but computationally heavier; LIME: Faster but potentially less stable across instances
  Global importance: Informative for domain experts but may mask subpopulation-specific effects
- Failure signatures:
  SHAP values with high variance across similar instances -> Poor local stability
  LIME explanations with low R² on sampled neighborhood -> Inadequate surrogate fit
- First 3 experiments:
  1. Verify SHAP mean absolute values match domain knowledge (e.g., glucose top-ranked)
  2. Compare LIME surrogate R² scores across multiple instances for consistency
  3. Test explanation stability under small input perturbations (sensitivity analysis)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LIME and SHAP compare in terms of accuracy and computational efficiency when applied to healthcare datasets of varying sizes and complexities?
- Basis in paper: [explicit] The paper compares LIME and SHAP as post-hoc XAI methods for interpreting a Random Forest classifier on a diabetes dataset, but does not provide a comprehensive comparison of their performance across different healthcare datasets.
- Why unresolved: The study focuses on a single dataset (diabetes) and does not explore the generalizability of LIME and SHAP across different healthcare applications or dataset characteristics.
- What evidence would resolve it: Empirical studies comparing LIME and SHAP on multiple healthcare datasets with varying sizes, complexities, and types of medical conditions, measuring both accuracy of explanations and computational efficiency.

### Open Question 2
- Question: What are the specific factors that influence the trustworthiness of XAI explanations in healthcare applications, and how can these be quantified?
- Basis in paper: [explicit] The paper discusses the importance of trust in XAI for healthcare applications and mentions factors like comprehensibility, accessibility, fidelity, and identity, separability, and novelty, but does not provide a detailed framework for quantifying trustworthiness.
- Why unresolved: Trust is a complex and multifaceted concept that is difficult to measure objectively, and the paper does not offer specific metrics or methods for assessing the trustworthiness of XAI explanations in healthcare.
- What evidence would resolve it: Development of a comprehensive framework for quantifying trustworthiness of XAI explanations in healthcare, incorporating factors like accuracy, reliability, fairness, and user acceptance, validated through user studies and real-world applications.

### Open Question 3
- Question: How can XAI methods be integrated with existing healthcare decision-making processes to improve patient outcomes and reduce medical errors?
- Basis in paper: [explicit] The paper highlights the potential of XAI to enhance transparency and interpretability in healthcare applications, but does not provide specific examples of how XAI can be integrated into clinical workflows or decision-making processes.
- Why unresolved: The practical implementation of XAI in healthcare settings requires careful consideration of existing workflows, regulatory requirements, and the needs of different stakeholders, which are not addressed in the paper.
- What evidence would resolve it: Case studies and pilot implementations of XAI in real healthcare settings, demonstrating how XAI explanations can be integrated into clinical decision-making processes, and evaluating the impact on patient outcomes and medical errors.

## Limitations

- Confidence is Medium for the core claim that SHAP and LIME can provide interpretable explanations for Random Forest diabetes predictions, as the evidence is primarily derived from methodological descriptions rather than empirical validation results.
- The assumption that feature independence is negligible lacks direct verification, particularly for correlated health metrics common in diabetes datasets.
- The claim that model-agnostic XAI preserves predictive performance while adding transparency remains untested - the chapter does not report whether explanation generation impacts model accuracy or training time.

## Confidence

- SHAP and LIME provide interpretable explanations for Random Forest diabetes predictions: Medium
- Feature independence assumption is negligible: Low
- Model-agnostic XAI preserves predictive performance while adding transparency: Low
- Global feature importance rankings are clinically meaningful: Low
- SHAP values accurately capture individual prediction rationales: Medium
- LIME's stability across similar instances: Low

## Next Checks

1. **Feature correlation analysis**: Compute Pearson/Spearman correlations between top-ranked features in SHAP importance scores to quantify dependency assumptions and test whether multicollinearity affects contribution calculations.

2. **Surrogate model fidelity testing**: For 10 diverse instances, calculate LIME's local R² scores and coefficient stability across multiple neighborhood samples to verify that explanations are consistent and faithful to the Random Forest predictions.

3. **Clinical knowledge validation**: Compare SHAP-derived global importance rankings against established medical literature on diabetes risk factors (e.g., glucose, BMI, age) to assess whether the model's learned importance aligns with domain expertise.