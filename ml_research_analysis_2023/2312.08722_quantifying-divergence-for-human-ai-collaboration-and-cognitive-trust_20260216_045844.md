---
ver: rpa2
title: Quantifying Divergence for Human-AI Collaboration and Cognitive Trust
arxiv_id: '2312.08722'
source_url: https://arxiv.org/abs/2312.08722
tags:
- trust
- user
- human
- users
- collaboration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a framework to measure decision-making similarity\
  \ (DMS) between humans and AI models using divergence metrics (KL, JSD) on soft\
  \ labels. A user study was conducted where participants chose the most similar model\u2019\
  s label distribution from multiple AI models (Random, TF-IDF, LSTM, RoBERTa, Davinci)\
  \ on a textual entailment task."
---

# Quantifying Divergence for Human-AI Collaboration and Cognitive Trust

## Quick Facts
- arXiv ID: 2312.08722
- Source URL: https://arxiv.org/abs/2312.08722
- Reference count: 18
- Key outcome: Decision-making similarity measured via Jensen-Shannon Divergence (JSD) is a stronger predictor of human-AI collaboration likelihood than traditional accuracy metrics, but collaboration does not necessarily imply similar cognitive trust levels.

## Executive Summary
This paper introduces a framework to measure decision-making similarity (DMS) between humans and AI models using divergence metrics (KL, JSD) on soft labels. A user study was conducted where participants chose the most similar model's label distribution from multiple AI models (Random, TF-IDF, LSTM, RoBERTa, Davinci) on a textual entailment task. The models' distances to human choices were calculated using α-KL, β-KL, and JSD. Participants were then surveyed about their collaboration likelihood and cognitive trust in their most similar model. Results show that people tend to collaborate with models with low β-KL (agreeing on same answer with high confidence) but cognitive trust requires low α-KL and JSD (avoiding overconfidence). JSD appears most suitable for measuring cognitive trust. The study reveals that collaboration likelihood does not necessarily imply similar cognitive trust levels.

## Method Summary
The study uses the SNLI dataset, filtering to 4GS pairs (90 instances, then reduced to 50) for human annotation. Five models (Random, TF-IDF, Enhanced LSTM, RoBERTa, Davinci-003) are trained on the training split excluding study instances. Participants (100 college-educated annotators) select the closest model based on soft labels, and JSD is used to find each user's aligned model. Users are then surveyed about collaboration likelihood and cognitive trust with their aligned model. α-KL, β-KL, and JSD are calculated between users and models, and correlations with survey results are analyzed.

## Key Results
- Decision-making similarity measured via JSD is a stronger predictor of human-AI collaboration likelihood than traditional accuracy metrics
- People tend to collaborate with models when their decision-making processes are similar (low β-KL)
- Cognitive trust requires avoiding model overconfidence (low α-KL) in addition to agreement on answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decision-making similarity (DMS) measured via Jensen-Shannon Divergence (JSD) is a stronger predictor of human-AI collaboration likelihood than traditional accuracy metrics.
- Mechanism: JSD captures the full probability distribution alignment between human and AI soft labels, whereas accuracy only considers the final hard label choice. When humans and AI agree on the same label distribution (high confidence on same answer), JSD is low, indicating strong DMS and higher collaboration intent.
- Core assumption: Human-AI collaboration depends on similarity of confidence distributions rather than just final label agreement.
- Evidence anchors:
  - [abstract] "We find that people tend to collaborate with their most similar models—measured via JSD—yet this collaboration does not necessarily imply a similar level of cognitive trust."
  - [section 5] "Our results show that people tend to collaborate with models when their decision-making processes are similar."
- Break condition: If human decision processes are driven primarily by final label accuracy rather than confidence distribution, JSD would be a weaker predictor than accuracy.

### Mechanism 2
- Claim: Inverse KL divergence (β-KL) specifically predicts collaboration likelihood by measuring agreement on same answers with high confidence.
- Mechanism: β-KL is high when human is confident in a label but model is uncertain, and low when both agree on same label with high confidence. Low β-KL indicates shared confidence on answers, which drives collaboration preference.
- Core assumption: Humans prefer collaborating with models that share their confidence level on answers, not just correct answers.
- Evidence anchors:
  - [section 5] "Davinci has also the lowest β-KL divergence to its aligned users. As discussed in §3, β-KL gets higher when the user predicts a label with high confidence, but the model has less confidence."
  - [section 5] "agreeing on the same answer (right or wrong) with high confidence is a critical DMS feature for collaboration."
- Break condition: If humans prioritize model accuracy over confidence alignment, β-KL would be a weaker predictor than accuracy.

### Mechanism 3
- Claim: Cognitive trust requires avoiding model overconfidence (low α-KL) in addition to agreement on answers.
- Mechanism: α-KL is high when model is overconfident in a label but human is uncertain. Low α-KL indicates model avoids overconfidence when human is uncertain, which builds cognitive trust. JSD combines both α-KL and β-KL effects.
- Core assumption: Cognitive trust requires models to exhibit appropriate uncertainty calibration matching human uncertainty.
- Evidence anchors:
  - [section 5] "Davinci suffers from high α-KL... observing this dissimilarity between themselves and models damages cognitive trust of users in the machine."
  - [section 5] "cognitive trust seems also related to low inverse-KL; however, it might also require another type of DMS with low forward-KL, i.e., avoiding being overconfident in case of uncertainty."
- Break condition: If cognitive trust is primarily driven by model accuracy rather than confidence calibration, α-KL would be a weaker predictor than accuracy.

## Foundational Learning

- Concept: Kullback-Leibler Divergence (KL)
  - Why needed here: Forms the basis for measuring DMS between human and AI soft label distributions
  - Quick check question: What does KL divergence measure when comparing two probability distributions?

- Concept: Jensen-Shannon Divergence (JSD)
  - Why needed here: Provides symmetric, bounded measure of DMS that combines forward and inverse KL effects
  - Quick check question: How does JSD differ from KL divergence in terms of symmetry and bounds?

- Concept: Soft labels vs Hard labels
  - Why needed here: Study uses soft label distributions (confidence scores) rather than just final predictions
  - Quick check question: Why might soft labels contain more information about decision-making similarity than hard labels?

## Architecture Onboarding

- Component map: Model Zoo -> Human Annotation Pipeline -> Divergence Calculator -> User Study Framework -> Analysis Engine
- Critical path: Model predictions → Human soft labels → Divergence calculation → User alignment → Survey → Analysis
- Design tradeoffs: Using aggregated human labels vs individual user divergences (individual provides more granular DMS measurement but requires more annotation effort)
- Failure signatures: Low user engagement in study, model performance degradation, divergence calculations failing on edge cases (zero probability distributions)
- First 3 experiments:
  1. Validate divergence calculations on synthetic data with known distributions
  2. Test user study interface with small pilot group to identify usability issues
  3. Compare DMS metrics against traditional accuracy-based collaboration predictions on held-out data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the inclusion of the Random baseline model affect the reliability of the user study results?
- Basis in paper: [explicit] The authors included a Random baseline model for comparison purposes.
- Why unresolved: The authors did not provide any analysis on the impact of including the Random baseline model on the study's results.
- What evidence would resolve it: An analysis comparing the study's results with and without the Random baseline model would provide clarity on its impact.

### Open Question 2
- Question: How would the study's results change if a larger and more diverse user pool was used?
- Basis in paper: [inferred] The study used 100 college-educated annotators with good command of English.
- Why unresolved: The authors acknowledge that the results are bound to the small set of experimented models, task, dataset, and user pool.
- What evidence would resolve it: Conducting the study with a larger and more diverse user pool would provide insights into how the results may vary.

### Open Question 3
- Question: What is the impact of the order in which the models are presented to the users on their decision-making?
- Basis in paper: [explicit] The authors mention that the order of the models is shuffled each time to prevent users from finding shortcuts.
- Why unresolved: The authors do not provide any analysis on how the order of model presentation may influence user decisions.
- What evidence would resolve it: An analysis comparing user decisions across different model presentation orders would provide insights into the potential impact of presentation order.

## Limitations
- Sample size and dataset scope (single NLI task) limit generalizability to other domains
- Significant performance differences between models could confound DMS measurements with accuracy effects
- Self-reported survey measures may not fully capture actual collaboration behavior in real-world settings

## Confidence
- High Confidence: Decision-making similarity can be quantified using divergence metrics on soft labels; DMS metrics can distinguish between different types of human-AI alignment; study methodology is internally consistent and executable
- Medium Confidence: Low β-KL specifically predicts collaboration likelihood; JSD is most suitable for measuring cognitive trust; humans prefer models with similar confidence distributions rather than just accurate models
- Low Confidence: DMS metrics generalize across different AI tasks and domains; survey responses accurately reflect real-world collaboration behavior; specific instance selection process does not bias results

## Next Checks
1. **Replication with Alternative Datasets**: Conduct the same user study protocol on a different NLI dataset (e.g., MNLI) and additional task types (e.g., sentiment analysis, visual question answering) to test generalizability of DMS metrics across domains and tasks.

2. **Behavioral Validation Study**: Design a follow-up experiment where participants actually collaborate with their "aligned" models on real tasks, measuring objective performance outcomes rather than self-reported likelihood and trust to validate whether DMS metrics predict actual collaboration effectiveness.

3. **Ablation Study on Model Performance**: Create controlled experiments where models have similar accuracies but different DMS profiles, and vice versa, to isolate the effects of confidence alignment from simple accuracy effects on collaboration likelihood and cognitive trust predictions.