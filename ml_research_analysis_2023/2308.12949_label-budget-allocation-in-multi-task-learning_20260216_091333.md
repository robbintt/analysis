---
ver: rpa2
title: Label Budget Allocation in Multi-Task Learning
arxiv_id: '2308.12949'
source_url: https://arxiv.org/abs/2308.12949
tags:
- task
- budget
- learning
- tasks
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of how to allocate a fixed label
  budget among tasks in multi-task learning to maximize overall performance. The authors
  propose a Task-Adaptive Budget Allocation (TABA) algorithm that estimates the informativeness
  of each label type and the reduction rate of information gain, then uses dynamic
  programming to determine an optimal budget allocation strategy.
---

# Label Budget Allocation in Multi-Task Learning

## Quick Facts
- **arXiv ID:** 2308.12949
- **Source URL:** https://arxiv.org/abs/2308.12949
- **Reference count:** 40
- **Primary result:** TABA algorithm outperforms heuristic labeling strategies by 24.7% relative performance gain on PASCAL VOC

## Executive Summary
This paper addresses the problem of optimally allocating a fixed label budget across multiple tasks in multi-task learning (MTL). The authors propose the Task-Adaptive Budget Allocation (TABA) algorithm, which estimates task-relatedness and information gain reduction rates to determine an optimal labeling strategy. TABA uses dynamic programming to solve a constrained optimization problem, maximizing overall MTL performance given budget constraints. Experiments on PASCAL VOC and Taskonomy datasets demonstrate consistent improvements over widely-used heuristic labeling strategies across various budget and cost scenarios.

## Method Summary
The TABA algorithm operates in two main steps. First, it estimates the informativeness of each label type by measuring task-relatedness from an initial seed dataset using performance changes during joint training. Second, it estimates task-specific reduction rates βi that capture diminishing returns in information gain using pseudo-labeling experiments. These estimates are then used in a dynamic programming framework to solve a constrained combinatorial optimization problem, determining the optimal number of labels to request for each task within the budget constraint. The final model is trained jointly on the combined seed and newly labeled data.

## Key Results
- TABA achieves 24.7% relative performance gain compared to 19.3-22.9% for heuristic baselines on PASCAL VOC with 6300 unit budget
- Performance improvements are consistent across different label budget and cost scenarios
- Ablation studies confirm the importance of cross-task informativeness and task-specific reduction rate estimation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** TABA's effectiveness comes from accurately estimating task-relatedness through performance changes during joint training.
- **Mechanism:** By measuring how each task's performance changes when jointly trained with another task, TABA captures the true information transfer between tasks rather than relying on loss-based approximations.
- **Core assumption:** Performance metrics are reliable proxies for information transfer and can be measured during training without excessive computational cost.
- **Evidence anchors:**
  - [abstract]: "Specifically, we estimate and then maximize the extent of new information obtained from the allocated budget as a proxy for multi-task learning performance."
  - [section]: "We propose to estimate the relative transferred information in MTL based on the task relatedness revealed in the initial 'seed' data... we probe the change of task Tj's performance (represented by its evaluation metrics) before and after jointly training with Ti"
  - [corpus]: Weak - no direct corpus evidence for this specific performance-based relatedness estimation approach
- **Break condition:** If performance metrics are unreliable or noisy, or if joint training doesn't reveal meaningful task relationships.

### Mechanism 2
- **Claim:** TABA's budget allocation strategy outperforms heuristic methods by explicitly modeling diminishing returns in information gain.
- **Mechanism:** The algorithm estimates a task-specific reduction rate βi that captures how quickly additional labels provide less new information, allowing for more optimal budget allocation.
- **Core assumption:** The reduction rate of information gain from pseudo-labels approximates that of ground-truth labels, making it feasible to estimate without full labeling.
- **Evidence anchors:**
  - [section]: "With the increasing number of labels for one task, we further consider the deductive effect of a new label to its own task as well as to other tasks... we define gi(·) with the reduction rate βi"
  - [section]: "we find that the performance curve with the increasing ground truth labels matches the performance curve with the increasing pseudo labels despite the fact that they are in different scales"
  - [corpus]: Weak - no direct corpus evidence for this specific pseudo-label based reduction rate estimation
- **Break condition:** If pseudo-labels poorly approximate ground-truth labels, or if the reduction rate varies significantly across different dataset distributions.

### Mechanism 3
- **Claim:** TABA's dynamic programming optimization ensures globally optimal budget allocation given the estimated informativeness values.
- **Mechanism:** By formulating budget allocation as a constrained combinatorial optimization problem and solving it with dynamic programming, TABA finds the allocation that maximizes total informativeness within the budget constraint.
- **Core assumption:** The estimated informativeness values are accurate enough that maximizing them leads to optimal performance, and dynamic programming is computationally feasible for the problem size.
- **Evidence anchors:**
  - [abstract]: "We propose a Task-Adaptive Budget Allocation (TABA) algorithm to robustly generate the optimal budget allocation adaptive to different multi-task learning settings"
  - [section]: "we solve the constrained combinatorial optimization problem... with dynamic programming"
  - [corpus]: Weak - no direct corpus evidence for this specific dynamic programming approach to budget allocation
- **Break condition:** If the estimated informativeness values are systematically biased, or if the problem size makes dynamic programming computationally intractable.

## Foundational Learning

- **Concept: Multi-task learning (MTL) performance optimization**
  - Why needed here: Understanding how to optimize MTL performance through budget allocation is the core problem this paper addresses.
  - Quick check question: What are the two main challenges in allocating a fixed label budget across multiple tasks in MTL?

- **Concept: Information theory and diminishing returns**
  - Why needed here: The paper's approach relies on modeling how much new information each label provides and how this decreases with more labels.
  - Quick check question: How does the concept of diminishing returns apply to labeling data in machine learning?

- **Concept: Dynamic programming for optimization**
  - Why needed here: The paper uses dynamic programming to solve the constrained optimization problem of budget allocation.
  - Quick check question: What type of optimization problem is being solved when allocating a fixed budget to maximize a sum of diminishing returns functions?

## Architecture Onboarding

- **Component map:** Task relatedness estimator -> Reduction rate estimator -> Dynamic programming solver -> MTL trainer

- **Critical path:**
  1. Estimate task relatedness from initial seed data
  2. Estimate reduction rates from pseudo-label experiments
  3. Solve dynamic programming problem for budget allocation
  4. Train final MTL model with allocated labels

- **Design tradeoffs:**
  - Accuracy vs. computation: More iterations for task relatedness estimation improve accuracy but increase computation time
  - Pseudo-label quality vs. estimation reliability: Better pseudo-labels lead to more reliable reduction rate estimates
  - Budget allocation granularity vs. dynamic programming complexity: Finer allocation granularity increases problem complexity exponentially

- **Failure signatures:**
  - Poor task relatedness estimates lead to suboptimal budget allocation
  - Inaccurate reduction rate estimates cause over/under-allocation to certain tasks
  - Dynamic programming solver fails to find optimal solution due to problem complexity

- **First 3 experiments:**
  1. Replicate the PASCAL VOC 2-task experiment to verify basic functionality
  2. Test on a synthetic dataset with known task relationships to validate the relatedness estimator
  3. Perform ablation studies on the reduction rate estimation by varying pseudo-label quality

## Open Questions the Paper Calls Out
- [explicit] The paper mentions "randomly select Ni unlabeled instances to request labels" and notes this as a limitation, suggesting future work to replace it with active learning methods.

## Limitations
- The approach relies heavily on accurate estimation of task relatedness and reduction rates, which may be sensitive to dataset characteristics and pseudo-label quality.
- The paper lacks direct comparison with more sophisticated budget allocation methods beyond basic heuristics.
- The dynamic programming solution may not scale well to problems with many tasks or fine-grained budget allocations.

## Confidence
- **High confidence** in the experimental results showing TABA outperforms heuristic baselines on the tested datasets
- **Medium confidence** in the proposed mechanisms, as the theoretical foundations are sound but practical validation is limited to specific datasets
- **Low confidence** in the generalizability of the approach to other domains and larger task sets without further empirical validation

## Next Checks
1. Test TABA on a third dataset with different task characteristics (e.g., medical imaging with classification and detection tasks) to verify generalizability beyond PASCAL VOC and Taskonomy.
2. Implement and compare against a strong baseline that uses recent multi-task learning techniques like gradient surgery or task grouping for budget allocation.
3. Conduct a thorough computational complexity analysis comparing TABA's runtime and memory requirements against heuristic methods across varying numbers of tasks and budget sizes.