---
ver: rpa2
title: Discriminatory or Samaritan -- which AI is needed for humanity? An Evolutionary
  Game Theory Analysis of Hybrid Human-AI populations
arxiv_id: '2306.17747'
source_url: https://arxiv.org/abs/2306.17747
tags:
- cooperation
- agents
- human
- social
- discriminatory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study applies evolutionary game theory to analyze how different
  AI agent strategies influence human cooperation in the Prisoner''s Dilemma. The
  researchers compared two AI types: Samaritan AI, which always cooperates, and Discriminatory
  AI, which rewards cooperative humans and punishes defectors.'
---

# Discriminatory or Samaritan -- which AI is needed for humanity? An Evolutionary Game Theory Analysis of Hybrid Human-AI populations

## Quick Facts
- arXiv ID: 2306.17747
- Source URL: https://arxiv.org/abs/2306.17747
- Reference count: 11
- Samaritan AI promotes cooperation in stable societies; Discriminatory AI in dynamic ones

## Executive Summary
This study applies evolutionary game theory to analyze how different AI agent strategies influence human cooperation in the Prisoner's Dilemma. The researchers compared two AI types: Samaritan AI, which always cooperates, and Discriminatory AI, which rewards cooperative humans and punishes defectors. Using both analytical models and agent-based simulations on well-mixed and structured populations, they found that Samaritan AI promotes higher cooperation in slow-changing societies (low intensity of selection), while Discriminatory AI is more effective in fast-changing societies (high intensity of selection). This suggests that the optimal AI strategy depends on the social environment's rate of change, with Samaritan AI being preferable in stable societies and Discriminatory AI in dynamic ones.

## Method Summary
The researchers used evolutionary game theory to model interactions between human agents and AI agents playing the Prisoner's Dilemma on various network structures. They employed replicator dynamics for infinite populations and stochastic simulations for finite populations, tracking strategy frequencies over time. The study compared three AI types (always cooperate, always defect, and intention recognition) against human agents using the Fermi rule for strategy imitation based on payoff differences.

## Key Results
- Samaritan AI agents promote higher cooperation than Discriminatory AI in slow-changing societies (low β)
- Discriminatory AI agents promote higher cooperation than Samaritan AI in fast-changing societies (high β)
- The fraction of AI agents determines whether full cooperation is the only stable state
- Results hold across well-mixed, lattice, and scale-free network structures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Samaritan AI promotes cooperation better in slow-changing societies by acting as role models that defectors can imitate, and this outweighs the payoff advantage that Discriminatory AI gives to cooperators.
- **Mechanism**: When imitation is rare (low β), the second term in the transition probability T⁺ becomes significant. Samaritan AI provides a reliable cooperative role model, increasing this term. Discriminatory AI, while offering higher payoffs to cooperators, does not directly increase this imitation probability.
- **Core assumption**: Agents base imitation on social learning, not solely payoff maximization.
- **Break condition**: If imitation becomes highly payoff-driven (β very high), or if agents can distinguish AI from humans and ignore AI strategies.

### Mechanism 2
- **Claim**: Discriminatory AI promotes cooperation better in fast-changing societies because the payoff advantage for cooperators is amplified in the Fermi function, outweighing the role model effect of Samaritan AI.
- **Mechanism**: When imitation is frequent (high β), the Fermi function in T⁺ heavily weights payoff differences. Discriminatory AI gives cooperators a higher payoff than defectors, making cooperators more likely to be imitated. Samaritan AI, while still cooperative, does not provide this differential payoff advantage.
- **Core assumption**: High imitation frequency means payoff differences dominate strategy adoption.
- **Break condition**: If the payoff structure is modified, or if agents have bounded rationality that limits payoff sensitivity.

### Mechanism 3
- **Claim**: In infinite populations, the fraction of AI agents (α) determines whether full cooperation is stable; if α is high enough, the only stable state is full cooperation regardless of AI type.
- **Mechanism**: For Samaritan AI, the evolution equation has a critical αc below which non-full cooperation fixed points exist. Above αc, the only stable fixed point is x* = 1 (full cooperation). For Discriminatory AI, the equation is simpler and may have two stable fixed points (0 and 1) depending on parameters.
- **Core assumption**: Population is infinite, allowing continuous x values, and the dynamics are governed by replicator equations.
- **Break condition**: If the population is finite, or if other strategies are introduced.

## Foundational Learning

- **Concept**: Evolutionary Game Theory (EGT) and the Replicator Dynamics
  - **Why needed here**: The paper uses EGT to model how strategies evolve in populations, and replicator dynamics to analyze infinite populations. Understanding these concepts is crucial for interpreting the results.
  - **Quick check question**: In replicator dynamics, what happens to the frequency of a strategy if its payoff is higher than the average payoff in the population?

- **Concept**: The Prisoner's Dilemma (PD) and its payoff structure
  - **Why needed here**: The study uses the PD as the interaction game. Knowing the payoff ordering (T > R > P > S) and the special case of the donation game is essential for understanding the model.
  - **Quick check question**: In the donation game, what are the payoffs T, R, P, and S in terms of benefit b and cost c?

- **Concept**: Network structures (well-mixed, lattice, scale-free)
  - **Why needed here**: The paper analyzes different network structures to see how they affect cooperation. Understanding these structures and their properties is important for interpreting the results.
  - **Quick check question**: How does the average degree of a Barabási-Albert scale-free network compare to that of a square lattice with the same number of nodes?

## Architecture Onboarding

- **Component map**: Human agents (population N) + AI agents (population M) -> play Prisoner's Dilemma on network -> calculate fitness -> imitate neighbors using Fermi rule -> update strategies -> repeat
- **Critical path**: 1. Initialize population and network. 2. For each time step: a) Each agent plays PD with neighbors. b) Each agent calculates fitness (sum of payoffs). c) Each agent selects a neighbor to imitate based on Fermi rule. d) Agent updates strategy with probability based on fitness difference. 3. Repeat until convergence.
- **Design tradeoffs**: Using a well-mixed population simplifies analysis but may not capture real-world network effects. Using agent-based simulations allows for more realistic network structures but is computationally expensive. Assuming perfect intention recognition for IR AI simplifies the model but may not be realistic.
- **Failure signatures**: If the population converges to all defectors, it suggests the AI strategy is not effective. If the population cycles between cooperation and defection, it suggests the dynamics are unstable. If the results depend heavily on initial conditions, it suggests the system is sensitive to perturbations.
- **First 3 experiments**:
  1. Run simulations with only human agents (no AI) to establish a baseline for cooperation levels.
  2. Run simulations with Samaritan AI and varying β to see how the intensity of selection affects cooperation.
  3. Run simulations with Discriminatory AI and varying β to compare its effectiveness to Samaritan AI in different regimes.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do AI agents perform in promoting human cooperation in more realistic social networks that evolve over time, rather than static networks?
- **Basis in paper**: [inferred] The paper acknowledges that real-world social networks are dynamic and evolve over time, but the study focuses on static network structures.
- **Why unresolved**: The paper's analysis is limited to static network topologies, which may not capture the full complexity of real-world social interactions.
- **What evidence would resolve it**: Simulations or experiments using dynamic network models that evolve over time, comparing the performance of different AI strategies in promoting cooperation.

### Open Question 2
- **Question**: What are the effects of varying the intensity of selection (β) for AI agents, beyond the fixed values studied in the paper?
- **Basis in paper**: [explicit] The paper discusses the impact of intensity of selection on cooperation but only considers a few fixed values of β.
- **Why unresolved**: The paper does not explore the full range of possible β values for AI agents, which could reveal different patterns of cooperation.
- **What evidence would resolve it**: Simulations varying β for AI agents across a wide range of values, analyzing the resulting levels of human cooperation.

### Open Question 3
- **Question**: How do different AI strategies, beyond Samaritan and Discriminatory, impact the evolution of cooperation in human populations?
- **Basis in paper**: [explicit] The paper mentions that the universe of AI strategies is vast and that different types of AI could have different impacts on human cooperation, but only studies three types of AI agents.
- **Why unresolved**: The study is limited to three AI strategies, leaving open the question of how other AI strategies might perform.
- **What evidence would resolve it**: Developing and testing new AI strategies in simulations or experiments, comparing their effectiveness in promoting cooperation to the strategies studied in the paper.

## Limitations
- Analytical approximations in replicator dynamics may not hold for finite populations
- Critical thresholds for AI fractions are presented numerically without analytical bounds
- Network effects are explored but generalization from well-mixed to structured populations lacks rigorous justification

## Confidence
- **High confidence**: The general direction that Samaritan AI outperforms in low β regimes and Discriminatory AI in high β regimes is well-supported by both theory and simulations
- **Medium confidence**: The specific critical values for α̃ and the precise crossover points between AI strategies depend on parameter choices and simulation details not fully specified
- **Low confidence**: Claims about mechanism dominance (imitation vs payoff effects) are qualitative; quantitative thresholds for when each mechanism dominates are not established

## Next Checks
1. Replicate the β threshold experiments with varying population sizes (N=100, 500, 1000) to test if the Samaritan/Discriminatory effectiveness crossover point shifts with system size
2. Test the critical α̃ values analytically by solving the fixed-point equations for small population sizes rather than relying on numerical simulations
3. Run simulations on heterogeneous networks with varying clustering coefficients to validate whether the network structure effect on AI strategy effectiveness is monotonic as claimed