---
ver: rpa2
title: 'MLP Fusion: Towards Efficient Fine-tuning of Dense and Mixture-of-Experts
  Language Models'
arxiv_id: '2307.08941'
source_url: https://arxiv.org/abs/2307.08941
tags:
- fusion
- language
- fine-tuning
- which
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a one-shot compression method called MLP fusion
  to reduce the computational cost of fine-tuning pre-trained language models. The
  method leverages the neural tangent kernel (NTK) to approximate the training dynamics
  of the original model while preserving its output.
---

# MLP Fusion: Towards Efficient Fine-tuning of Dense and Mixture-of-Experts Language Models

## Quick Facts
- **arXiv ID:** 2307.08941
- **Source URL:** https://arxiv.org/abs/2307.08941
- **Reference count:** 40
- **Key outcome:** MLP fusion achieves the closest NTK approximation and outperforms other one-shot compression baselines in prediction accuracy for language model fine-tuning.

## Executive Summary
MLP fusion is a one-shot compression method that reduces the computational cost of fine-tuning pre-trained language models by leveraging the neural tangent kernel (NTK) to approximate training dynamics. The approach clusters sub-MLPs within an MLP module into centroids, creating a compressed model that maintains both NTK and output approximation. Extensive experiments on natural language understanding and generation tasks demonstrate that MLP fusion outperforms other one-shot compression baselines, achieving the closest NTK approximation and superior prediction accuracy.

## Method Summary
The method works by decomposing each MLP into sub-MLPs defined by columns of weight matrices and corresponding biases, then clustering these sub-MLPs using k-means into a specified number of centroids. The centroids are restored as a compressed MLP using a one-hot clustering matrix C and scaling matrix P = CC^T, which preserves the NTK during backward propagation by maintaining gradient sign structure. After initial compression, a layer-wise task-specific tuning module minimizes MSE between teacher and student hidden states, further improving fine-tuning accuracy. The approach is evaluated on both dense models (RoBERTa) and mixture-of-experts models (Switch-Transformer) across multiple natural language understanding and generation tasks.

## Key Results
- MLP fusion achieves the closest NTK approximation compared to other one-shot compression baselines
- The method outperforms baselines in prediction accuracy on SST-2, MNLI, STS-B, QNLI, and WebNLG tasks
- Layer-wise task-specific tuning with 1 epoch further improves fine-tuning accuracy while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
Clustering sub-MLPs via k-means approximates the empirical distribution of the original MLP and preserves its NTK. The MLP is decomposed into pI sub-MLPs, each defined by a column of W1, bias b1, and corresponding row of W2. Clustering these sub-MLPs and replacing them with centroids yields a compressed MLP that retains the NTK through preserved gradient inner products. Core assumption: clustering error is small enough that fused MLP output and NTK closely approximate the original. Break condition: if clustering fails to preserve gradient structure, NTK approximation degrades and training dynamics diverge.

### Mechanism 2
Using the one-hot clustering matrix C and scaling matrix P = CC^T preserves the NTK during backward propagation. The one-hot matrix C "copies" centroids without mixing, and the diagonal scaling matrix P assigns different learning rates to clusters, maintaining the sign structure of gradients critical for Adam NTK. Core assumption: matrix C's structure allows exact gradient flow replication in forward and backward passes. Break condition: if C is not truly one-hot or P is not diagonal, gradient signs and magnitudes change, breaking NTK preservation.

### Mechanism 3
Layer-wise task-specific tuning with MSE loss on hidden states further improves fine-tuning accuracy. After initial compression, the fused MLP is tuned on unsupervised data by minimizing the MSE between teacher and student hidden states, preserving task-specific knowledge. Core assumption: hidden state alignment captures task-relevant features without overfitting to labels. Break condition: if tuning data is too small or noisy, MSE alignment may overfit or underfit, hurting generalization.

## Foundational Learning

- **Neural Tangent Kernel (NTK)**: Approximates gradient descent dynamics of neural networks, enabling compressed model to mimic training behavior of original. Quick check: What is the NTK for a two-layer MLP with ReLU activation?
- **k-means clustering on parameter vectors**: Clustering sub-MLPs into centroids allows model compression while preserving empirical parameter distribution. Quick check: How does Lloyd's algorithm ensure centroids minimize within-cluster variance?
- **Gradient sign preservation in Adam**: Adam's sign-based update rule means preserving gradient signs is critical for maintaining NTK approximation. Quick check: Why does the Hadamard product with sign(∇) appear in the Asymmetric SignGD Kernel?

## Architecture Onboarding

- **Component map**: Input preprocessing → Clustering module (k-means) → Compressed MLP (fW1, b1̃, fW2, P) → Forward pass → Loss → Backward pass (with P scaling) → Layer-wise fine-tuning (optional)
- **Critical path**: Clustering → Forward/backward gradient flow with P → NTK preservation → Fine-tuning performance
- **Design tradeoffs**: Compression ratio vs. NTK approximation accuracy (more centroids → better NTK but larger model); k-means vs. other clustering (simplicity vs. potential better structure preservation); layer-wise tuning duration (1 epoch in paper vs. longer but higher cost)
- **Failure signatures**: Training loss diverges or plateaus early → NTK approximation degraded; accuracy drops sharply vs. baselines → Clustering or scaling matrix error; Runtime increases unexpectedly → Clustering or fine-tuning overhead
- **First 3 experiments**: 1) Cluster sub-MLPs with k=768 on RoBERTa-base, measure NTK error vs. original; 2) Compare fine-tuning accuracy on SST-2 with 8-layer sketch vs. full model; 3) Evaluate effect of layer-wise fine-tuning (1 epoch) on downstream accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MLP fusion scale with varying sizes of the intermediate dimension in the MLP module? The paper explores the impact of different intermediate dimensions on training loss and prediction accuracy, finding that performance is stable when the dimension is larger than 768 but drops significantly when it is smaller. The paper does not provide a detailed analysis of the optimal intermediate dimension for different tasks or model sizes, nor does it explore the trade-offs between model size and performance. Experiments showing the performance of MLP fusion across a wider range of intermediate dimensions for various tasks and model sizes would help determine the optimal settings.

### Open Question 2
Can MLP fusion be effectively combined with other model compression techniques, such as quantization or knowledge distillation, to further enhance efficiency? The paper mentions the potential of using MLP fusion as an initialization method for distillation, suggesting that combining it with other compression techniques could be beneficial. The paper does not explore the combination of MLP fusion with other compression techniques, leaving open the question of whether such combinations could yield better results. Experiments demonstrating the performance of MLP fusion when combined with quantization or knowledge distillation would provide insights into the potential benefits of such combinations.

### Open Question 3
How does the NTK approximation property of MLP fusion impact the generalization ability of the compressed model across different tasks? The paper highlights the importance of NTK approximation in preserving the training dynamics of the original model and achieving competitive performance. While the paper shows that MLP fusion approximates the NTK, it does not provide a detailed analysis of how this property affects the model's ability to generalize across diverse tasks. Comparative studies of MLP fusion's performance on various tasks, alongside models that do not preserve NTK, would clarify the impact of NTK approximation on generalization.

## Limitations
- The theoretical and empirical foundations of NTK preservation under clustering are not rigorously proven beyond empirical demonstration
- The effectiveness of the scaling matrix P in preserving gradient signs is asserted but not deeply analyzed in terms of its interaction with Adam's adaptive learning rate mechanism
- The experimental scope uses a relatively small number of fine-tuning epochs (1 epoch for layer-wise tuning), which may limit generalizability to longer training regimes

## Confidence
- **NTK approximation claim**: Medium - supported by experiments but lacks formal proof of equivalence
- **Scaling matrix P effectiveness**: Medium - asserted based on gradient sign preservation but interaction with Adam not fully explored
- **Layer-wise fine-tuning impact**: Medium - shows promise but evaluated only in limited settings

## Next Checks
1. Test NTK approximation under varying cluster counts (k) to identify the minimum k required for acceptable NTK error
2. Compare clustering quality (e.g., silhouette score, reconstruction error) against model performance to establish correlation
3. Evaluate the impact of scaling matrix P on gradient flow by comparing with a variant where P is replaced by identity