---
ver: rpa2
title: An Investigation into Glomeruli Detection in Kidney H&E and PAS Images using
  YOLO
arxiv_id: '2307.13199'
source_url: https://arxiv.org/abs/2307.13199
tags:
- dataset
- images
- wsis
- public
- been
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a YOLO-based object detection approach for
  identifying glomeruli in kidney histopathology images. The method addresses the
  challenge of automating glomeruli detection to assist pathologists in diagnosing
  kidney diseases.
---

# An Investigation into Glomeruli Detection in Kidney H&E and PAS Images using YOLO

## Quick Facts
- arXiv ID: 2307.13199
- Source URL: https://arxiv.org/abs/2307.13199
- Reference count: 40
- Primary result: YOLO-v4 achieves 85% sensitivity and 89% specificity on PAS validation images after fine-tuning with limited University of Michigan data

## Executive Summary
This paper presents a YOLO-based object detection approach for identifying glomeruli in kidney histopathology images. The method addresses the challenge of automating glomeruli detection to assist pathologists in diagnosing kidney diseases. YOLO-v4 was trained and evaluated on two public datasets and a private dataset from the University of Michigan, using different combinations of training data. The model was validated on 20 PAS-stained and 16 H&E-stained WSIs from the University of Michigan. The best performance was achieved by fine-tuning the model with 7 PAS-stained images from the University of Michigan, resulting in an average sensitivity of 85% and specificity of 89% on PAS validation images. For H&E validation images, the model achieved 70% sensitivity and 96% specificity. The results demonstrate that YOLO-based detection outperforms segmentation methods in terms of sensitivity while maintaining high specificity, making it a promising tool for automated glomeruli detection in kidney histopathology.

## Method Summary
The study employs YOLO-v4 for glomeruli detection in kidney histopathology images. The model was trained on seven different combinations of datasets: two public datasets (31 WSIs from AIDPATH project and 8 WSIs from HubMap competition) and a private dataset from University of Michigan (7 PAS-stained WSIs for training, 20 PAS-stained and 16 H&E-stained WSIs for validation). The training configuration included batch size 40, subdivisions 16, learning rate starting at 0.001 with steps at 4800 and 5400, max batches 6000, 18 filters, and linear activation. The model's performance was evaluated using average sensitivity and specificity calculated with an IoU threshold of 0.5, and compared against a U-Net segmentation method from the HubMap competition.

## Key Results
- YOLO-v4 fine-tuned with 7 PAS-stained images from University of Michigan achieved 85% sensitivity and 89% specificity on PAS validation images
- On H&E validation images, the model achieved 70% sensitivity and 96% specificity
- YOLO-based detection outperformed segmentation methods in terms of sensitivity while maintaining high specificity
- Combining both public datasets resulted in lower accuracy compared to using only the first public dataset, likely due to domain differences between datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: YOLO-v4 outperforms segmentation methods on the same validation datasets in terms of sensitivity while maintaining high specificity.
- Mechanism: YOLO-v4 uses a single neural network to predict bounding boxes and class probabilities for objects of interest, requiring only bounding box labels instead of pixel-level annotations. This reduces annotation burden and allows the model to focus on detecting the presence and location of glomeruli rather than precisely delineating their borders.
- Core assumption: Detecting the presence and location of glomeruli is sufficient for assisting pathologists in diagnosing kidney diseases, and the trade-off of less precise borders is acceptable.
- Evidence anchors:
  - [abstract] "The results demonstrate that YOLO-based detection outperforms segmentation methods in terms of sensitivity while maintaining high specificity"
  - [section] "In detection methods, only determining the location of a given tissue pattern, the glomerulus, is required without the need to precisely delineating its borders."

### Mechanism 2
- Claim: Fine-tuning the YOLO-v4 model with limited data from the University of Michigan dataset significantly improves performance on the validation set from the same resource.
- Mechanism: Pre-training the YOLO-v4 model on public datasets and then fine-tuning it with a small number of annotated images from the target domain (University of Michigan) allows the model to adapt to the specific characteristics of the target data while leveraging the general knowledge gained from the public datasets.
- Core assumption: The public datasets and the University of Michigan dataset share sufficient similarities in terms of glomeruli appearance and characteristics to enable effective transfer learning.
- Evidence anchors:
  - [section] "The generalization of the network has been tested by training on these two public datasets, followed by the external validation on the private dataset from University of Michigan."
  - [section] "Another important point would be the difference between the results of experiments trained on the first public dataset and the second one. It has been shown that by combining both datasets, the accuracy could drop off compared to only training on the first public dataset, and the reason may be related to the difference between the images from the second public dataset and the images from the University of Michigan."

### Mechanism 3
- Claim: The performance of the YOLO-v4 model is affected by the type of tissue staining used in the training and validation datasets.
- Mechanism: The YOLO-v4 model learns to recognize glomeruli based on the visual features present in the training data. If the training data consists primarily of PAS-stained images, the model may not generalize well to H&E-stained images, which have different visual characteristics.
- Core assumption: The visual features of glomeruli in PAS-stained and H&E-stained images are sufficiently different to impact the performance of the YOLO-v4 model.
- Evidence anchors:
  - [section] "Same as in the Table 3, the results have been improved by fine-tuning the training dataset with only seven images from the University of Michigan. The difference between the outcomes of experiments trained on the first public dataset and the second is still significant. Because of the differences in images between the second dataset which are surgical biopsy images and those from the University of Michigan that are needle biopsy images, it has been demonstrated that by combining both datasets, accuracy can drop."

## Foundational Learning

- Concept: Object detection vs. segmentation
  - Why needed here: Understanding the difference between object detection and segmentation is crucial for interpreting the results and implications of the study. Object detection aims to locate objects within an image, while segmentation aims to classify each pixel in the image.
  - Quick check question: What is the main difference between object detection and segmentation in the context of histopathology image analysis?

- Concept: Transfer learning
  - Why needed here: The study employs transfer learning by pre-training the YOLO-v4 model on public datasets and then fine-tuning it with a smaller dataset from the University of Michigan. Understanding transfer learning is essential for grasping the methodology and potential benefits of the approach.
  - Quick check question: How does transfer learning help improve the performance of deep learning models when training data is limited?

- Concept: Tissue staining in histopathology
  - Why needed here: The study uses two different staining methods (PAS and H&E) for the validation datasets. Understanding the characteristics and differences between these staining methods is important for interpreting the results and potential limitations of the study.
  - Quick check question: What are the main differences between PAS and H&E staining methods in terms of the visual appearance of tissue structures?

## Architecture Onboarding

- Component map: Input images -> Pre-processing (resizing, normalization, augmentation) -> Backbone (convolutional layers) -> Neck (feature pyramid network) -> Head (bounding box regression and classification) -> Output (predicted bounding boxes and class probabilities)

- Critical path: Load and preprocess input images -> Extract features using the backbone network -> Fuse multi-scale features using the neck -> Predict bounding boxes and class probabilities using the head -> Post-process predictions (e.g., non-maximum suppression)

- Design tradeoffs: YOLO-v4 vs. segmentation methods: YOLO-v4 requires less annotation effort but provides less precise delineation of glomeruli borders. Public datasets vs. private dataset: Using public datasets allows for pre-training but may introduce domain shift, while using a private dataset allows for domain-specific fine-tuning but requires more annotation effort. PAS vs. H&E staining: PAS staining highlights basement membranes, while H&E staining provides a more general overview of tissue morphology.

- Failure signatures: Low sensitivity: The model fails to detect a significant number of glomeruli in the validation set. Low specificity: The model produces a high number of false positives (detecting non-glomerular structures as glomeruli). Domain shift: The model performs well on the public datasets but poorly on the private dataset from the University of Michigan. Staining bias: The model performs well on PAS-stained images but poorly on H&E-stained images.

- First 3 experiments:
  1. Train YOLO-v4 on the first public dataset (31 WSIs) and evaluate on the validation set from the University of Michigan (20 PAS and 16 H&E images).
  2. Fine-tune the YOLO-v4 model trained on the first public dataset with 7 PAS-stained WSIs from the University of Michigan and evaluate on the validation set.
  3. Train YOLO-v4 on the second public dataset (8 WSIs) and evaluate on the validation set from the University of Michigan.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of YOLO-based glomeruli detection vary across different kidney disease conditions or pathological states?
- Basis in paper: [explicit] The paper mentions that YOLO-v4 was trained and validated on PAS and H&E stained images from the University of Michigan, but does not specify the underlying kidney disease conditions.
- Why unresolved: The study focuses on the technical performance of YOLO-v4 in detecting glomeruli without considering the impact of different pathological states on detection accuracy.
- What evidence would resolve it: Conducting experiments on kidney images with known pathological conditions and comparing YOLO-v4's performance across these conditions would provide insights into its robustness and reliability in diverse clinical scenarios.

### Open Question 2
- Question: What is the optimal combination of training data from public and private datasets to achieve the best generalization performance of YOLO-v4 for glomeruli detection?
- Basis in paper: [explicit] The paper discusses training YOLO-v4 on different combinations of public and private datasets, but does not provide a systematic analysis of the optimal combination for generalization.
- Why unresolved: While the paper reports on various training configurations, it does not explicitly determine the best combination for maximizing generalization across different staining methods and pathological conditions.
- What evidence would resolve it: Conducting a comprehensive study with different combinations of training data, including varying the proportion of public and private datasets, and evaluating the model's performance on diverse validation sets would help identify the optimal training configuration.

### Open Question 3
- Question: How does the choice of stain type (PAS vs. H&E) affect the accuracy and reliability of YOLO-v4 in detecting glomeruli in kidney histopathology images?
- Basis in paper: [explicit] The paper compares the performance of YOLO-v4 on PAS and H&E stained images, showing differences in sensitivity and specificity between the two stain types.
- Why unresolved: The study provides a comparison of YOLO-v4's performance on different stain types but does not explore the underlying reasons for the observed differences or how to optimize the model for each stain type.
- What evidence would resolve it: Conducting experiments to analyze the impact of stain-specific features on detection accuracy and developing stain-specific training strategies or preprocessing techniques could help improve YOLO-v4's performance on different stain types.

## Limitations

- The validation set size is relatively small (20 PAS and 16 H&E images), which may not provide robust generalization estimates
- The study does not compare YOLO-v4 with other state-of-the-art object detection methods, limiting understanding of its relative performance
- The lower performance on H&E images (70% sensitivity) suggests potential domain adaptation challenges between staining methods

## Confidence

- **High Confidence**: The mechanism that YOLO-v4 requires less annotation effort than segmentation methods (bounding boxes vs. pixel-level annotations) is well-supported by the literature and methodology.
- **Medium Confidence**: The claim that fine-tuning with limited University of Michigan data improves performance is supported by the results but could benefit from more extensive validation across different domain shifts.
- **Low Confidence**: The assertion that YOLO-v4 outperforms segmentation methods is based on comparison with a single U-Net implementation from the HubMap competition, which may not represent the full range of segmentation approaches.

## Next Checks

1. **Dataset Size Validation**: Test the model on a larger, independently collected validation set from multiple institutions to verify the generalizability of the 85% sensitivity and 89% specificity on PAS images.

2. **Staining Method Robustness**: Conduct experiments training and validating the model exclusively on H&E-stained images to assess whether the lower performance on H&E images is due to staining differences or insufficient training data.

3. **Alternative Detection Methods**: Implement and compare at least two additional object detection architectures (e.g., Faster R-CNN, EfficientDet) on the same datasets to establish whether YOLO-v4's performance advantage is method-specific or a general property of object detection approaches.