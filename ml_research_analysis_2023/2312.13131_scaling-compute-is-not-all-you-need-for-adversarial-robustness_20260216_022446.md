---
ver: rpa2
title: Scaling Compute Is Not All You Need for Adversarial Robustness
arxiv_id: '2312.13131'
source_url: https://arxiv.org/abs/2312.13131
tags:
- adversarial
- robustness
- training
- scaling
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether scaling compute power can drive
  adversarial robustness advances. The authors conduct extensive experiments training
  320 models with various configurations on CIFAR-10.
---

# Scaling Compute Is Not All You Need for Adversarial Robustness

## Quick Facts
- arXiv ID: 2312.13131
- Source URL: https://arxiv.org/abs/2312.13131
- Reference count: 40
- Primary result: Scaling compute power yields diminishing returns for adversarial robustness compared to standard training.

## Executive Summary
This paper investigates whether scaling compute power can drive adversarial robustness advances. Through extensive experiments training 320 models with various configurations on CIFAR-10, the authors find that increasing compute does not yield proportionate improvements in adversarial robustness. Unlike standard training, where compute scaling follows a predictable power law with substantial gains, adversarial robustness shows a very slow growth rate. The environmental costs (CO2 emissions and electricity consumption) scale much faster than robustness gains, making compute scaling inefficient. The study also reveals that some top-performing techniques are difficult to reproduce, suggesting they are not robust to minor changes in training setup.

## Method Summary
The authors conducted a large-scale empirical study training 320 models with different configurations on CIFAR-10. They varied model architectures (WideResNet sizes), activation functions (ReLU, GELU), training losses (PGD, TRADES), attack steps (1, 2, 5, 7, 10), training epochs (100, 400), and EMA usage. Models were evaluated using AutoAttack for robustness and trained with adversarial training using PGD attacks. The study measured AutoAttack accuracy, FLOPs, electricity consumption, and CO2 emissions to analyze scaling relationships and environmental impact.

## Key Results
- Compute scaling yields diminishing returns for adversarial robustness (α = 0.01) compared to standard training
- Environmental costs (electricity, CO2) scale much faster than robustness gains, making compute scaling inefficient
- Algorithmic choices (synthetic data, loss function, model parameters) are more important than raw compute for achieving robustness
- Some top-performing techniques are difficult to reproduce, suggesting they are not robust to minor training setup changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compute scaling follows a power law relationship for adversarial robustness, but with a very low growth rate (α = 0.01).
- Mechanism: The study systematically varies model size, training configurations, and compute budgets, then fits a power law model (accuracy = C × (FLOPs)^α) to the best-performing configurations in each compute bin. The small α indicates that each unit of compute yields diminishing returns for robustness compared to standard training.
- Core assumption: The best performance envelope within each compute bin accurately represents the scaling potential of adversarial robustness.
- Evidence anchors:
  - [abstract] "We show that increasing the FLOPs needed for adversarial training does not bring as much advantage as it does for standard training in terms of performance improvements."
  - [section] "Figure 3a shows the growth in the AutoAttack accuracy as a function of the computation used in these models. Given that this is plotted on a log-log scale, a straight line indicates a polynomial growth in computing per unit of performance."
- Break condition: If future models find a way to exploit architectural innovations that decouple robustness gains from raw compute, the power law relationship would break.

### Mechanism 2
- Claim: Algorithmic choices (synthetic data, loss function, model parameters) are more important than raw compute for achieving adversarial robustness.
- Mechanism: The paper uses a gradient boosting regression model to predict AutoAttack accuracy from features like synthetic data usage, loss type, and model parameters. The model achieves high prediction accuracy, suggesting these algorithmic factors are the primary drivers of robustness improvements.
- Core assumption: The feature importance scores from the gradient boosting model accurately reflect the relative impact of each algorithmic choice on robustness.
- Evidence anchors:
  - [abstract] "Moreover, we find that some of the top-performing techniques are difficult to exactly reproduce, suggesting that they are not robust enough for minor changes in the training setup."
  - [section] "In Figure 1c, we analyze the feature importance of our predictive model... We hypothesize that the use of synthetic data, training loss, and the number of model parameters are the most important driving forces behind the adversarial robustness advances."
- Break condition: If new algorithmic innovations emerge that are not captured by the current feature set, the predictive model's accuracy would decrease.

### Mechanism 3
- Claim: The environmental cost of scaling compute for adversarial robustness is disproportionately high compared to the robustness gains achieved.
- Mechanism: The paper measures electricity consumption and CO2 emissions during training, then fits power laws to these metrics alongside AutoAttack accuracy. The high growth rates for electricity cost (α = 0.95) and CO2 emission compared to the low growth rate for accuracy demonstrate the inefficiency of compute scaling.
- Core assumption: The power law fits accurately capture the relationship between compute and environmental impact across the range of experiments.
- Evidence anchors:
  - [abstract] "The growth rate for adversarial robustness is very slow compared to high growth rates for CO2 emissions and electricity cost."
  - [section] "We find that the power law relationship also holds for electricity cost (see Figure 3b) and CO2 emission (see Figure 3c) both with R2 = 0.95."
- Break condition: If future hardware or training methods significantly reduce the energy consumption per FLOP, the power law relationship between compute and environmental impact would change.

## Foundational Learning

- Concept: Power law scaling relationships
  - Why needed here: The paper uses power laws to model how adversarial robustness scales with compute, electricity cost, and CO2 emissions.
  - Quick check question: If a power law has the form y = Cx^α, what does a small α value indicate about the relationship between x and y?

- Concept: Adversarial training and robustness evaluation
  - Why needed here: The paper conducts extensive adversarial training experiments and evaluates robustness using AutoAttack, which is essential for understanding the results.
  - Quick check question: What is the main difference between standard training and adversarial training in terms of computational cost?

- Concept: Feature importance in machine learning models
  - Why needed here: The paper uses a gradient boosting regression model to determine which algorithmic choices are most important for adversarial robustness, relying on feature importance scores.
  - Quick check question: How does a gradient boosting regression model determine the importance of each feature in predicting the target variable?

## Architecture Onboarding

- Component map: CIFAR-10 dataset -> Adversarial training pipeline -> AutoAttack evaluation -> Power law analysis -> Environmental impact measurement
- Critical path: Select model configurations → Conduct adversarial training → Evaluate with AutoAttack → Analyze scaling relationships and environmental impact
- Design tradeoffs: Balance between exploring wide range of configurations vs computational cost; balance accurate robustness evaluation vs expense of AutoAttack
- Failure signatures: Low R² values for power law fits, poor predictive model accuracy, or results contradicting existing literature
- First 3 experiments:
  1. Replicate baseline experiments using WideResNet-28-10 with standard adversarial training to establish reference point
  2. Vary number of attack steps (1, 2, 5, 7, 10) to observe impact on robustness and compute cost
  3. Compare results of training with and without synthetic data to assess its contribution to robustness gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between the number of attack steps used during adversarial training and the resulting model's robustness?
- Basis in paper: [explicit] The paper varies the number of attack iterations (1, 2, 5, 7, and 10) in their experiments and observes that increasing attack iterations leads to stronger attacks.
- Why unresolved: While the paper shows that more attack steps lead to stronger attacks, it doesn't directly analyze how this affects the scaling laws for adversarial robustness.
- What evidence would resolve it: Experiments that systematically vary the number of attack steps and measure how this affects the scaling exponent α in the power law relationship between compute and robustness.

### Open Question 2
- Question: How does the use of synthetic data affect the scaling laws for adversarial robustness compared to real data?
- Basis in paper: [explicit] The paper tests the contribution of synthetic data for adversarial training and finds that using extra data (synthetic) has significantly more advantageous scaling laws compared to models trained without extra data.
- Why unresolved: The paper doesn't quantify exactly how synthetic data changes the scaling exponent or the constants in the power law.
- What evidence would resolve it: A detailed analysis of scaling laws with varying amounts of both synthetic and real data, comparing the resulting power law parameters.

### Open Question 3
- Question: Are there specific architectural modifications that can improve the efficiency of scaling compute for adversarial robustness?
- Basis in paper: [explicit] The paper tests different model sizes (WideResNet variants) and activation functions (ReLU, GELU) but doesn't explore other architectural modifications that might improve scaling efficiency.
- Why unresolved: The paper focuses on existing architectures and only varies depth, width, and activation functions.
- What evidence would resolve it: Experiments testing novel architectural modifications specifically designed to improve scaling efficiency for adversarial robustness, measuring how these changes affect the power law parameters.

## Limitations
- Experiments limited to CIFAR-10 dataset, may not generalize to larger-scale tasks
- Power law fits based on finite range of compute budgets may not hold at extreme scales
- Reproducibility issues with top-performing techniques suggest results may be sensitive to implementation details

## Confidence
- **High confidence**: Compute scaling yields diminishing returns for adversarial robustness compared to standard training
- **Medium confidence**: Algorithmic choices are more important than raw compute for adversarial robustness
- **Medium confidence**: Environmental cost of scaling compute for adversarial robustness is disproportionately high

## Next Checks
1. Replicate experiments on a larger-scale dataset (e.g., ImageNet) to assess generalizability beyond CIFAR-10
2. Investigate impact of recent architectural innovations (vision transformers, novel regularization) on compute-robustness scaling relationship
3. Systematically vary implementation details of top-performing techniques to quantify sensitivity to training setup changes