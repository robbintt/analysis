---
ver: rpa2
title: Federated Learning with Extremely Noisy Clients via Negative Distillation
arxiv_id: '2312.12703'
source_url: https://arxiv.org/abs/2312.12703
tags:
- clients
- noisy
- client
- noise
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of federated learning with extremely
  noisy clients, where some clients have label noise ratios exceeding 90%. The authors
  propose a method called FedNed that first identifies these extremely noisy clients
  using model uncertainty, and then employs a negative distillation strategy to incorporate
  them.
---

# Federated Learning with Extremely Noisy Clients via Negative Distillation

## Quick Facts
- arXiv ID: 2312.12703
- Source URL: https://arxiv.org/abs/2312.12703
- Reference count: 13
- Key outcome: FedNed achieves 84.97% accuracy on CIFAR-10 and 53.74% on CIFAR-100 with 99% label noise, outperforming FedNoRo (82.41% and 49.36% respectively)

## Executive Summary
This paper addresses federated learning with extremely noisy clients, where some clients have label noise ratios exceeding 90%. The authors propose FedNed, a method that first identifies these extremely noisy clients using model uncertainty, then employs negative distillation to incorporate them. In negative distillation, noisy client models act as "bad teachers" to encourage the global model to diverge from their incorrect predictions. The method also uses pseudo-labeling for these clients. Experiments show FedNed consistently outperforms state-of-the-art methods on CIFAR-10 and CIFAR-100 datasets.

## Method Summary
FedNed is a federated learning algorithm designed for scenarios with extremely noisy clients. It operates in communication rounds where the server identifies extremely noisy clients using model uncertainty (via MC dropout on a public dataset). The server then performs two types of aggregation: standard aggregation of clean client models and negative distillation using noisy client models as "bad teachers." Noisy clients additionally train local models using pseudo-labels generated by the global model. The method uses ResNet-18/50 architectures with 20 clients, 100 communication rounds, and 50% client participation.

## Key Results
- FedNed achieves 84.97% accuracy on CIFAR-10 compared to FedNoRo's 82.41% with 99% label noise
- FedNed achieves 53.74% accuracy on CIFAR-100 compared to FedNoRo's 49.36% with 99% label noise
- FedNed remains stable as the number of extremely noisy clients increases, while other methods' accuracy significantly declines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model uncertainty effectively identifies extremely noisy clients
- Mechanism: Uses MC dropout to compute prediction uncertainty for each client model on a public dataset. Clients with uncertainty above threshold λ are classified as extremely noisy.
- Core assumption: Model uncertainty correlates with label noise - models trained on noisy data produce less confident predictions.
- Evidence anchors:
  - [abstract] "The server identifies the extremely noisy clients in each round (c.f. Section 3.3) and excludes their uploaded models during model aggregation"
  - [section 3.3] "we employ model uncertainty (Gal and Ghahramani 2016) to identify models with high risk, i.e., trained with noisy-labeled data"
  - [corpus] Weak - corpus papers discuss similar uncertainty-based approaches but don't directly validate this specific threshold-based identification
- Break condition: If clean clients also have high uncertainty due to data heterogeneity, or if extremely noisy clients somehow produce confident predictions

### Mechanism 2
- Claim: Negative distillation prevents global model degradation from noisy clients
- Mechanism: Instead of aggregating noisy client models directly, FedNed uses them as "bad teachers" in a knowledge distillation framework where the global model learns to diverge from their predictions.
- Core assumption: The global model can improve by actively avoiding incorrect predictions from noisy client models rather than ignoring them completely.
- Evidence anchors:
  - [abstract] "In negative distillation, the models from extremely noisy clients act as 'bad teachers' to encourage the global model to diverge from their incorrect predictions"
  - [section 3.4] "we implement this idea by the concept of negative distillation, making the global model's prediction diverge from those offered by the identified EN clients"
  - [corpus] Moderate - corpus contains papers on knowledge distillation but none specifically on negative distillation from noisy clients
- Break condition: If the noisy client models happen to make correct predictions on some samples, forcing divergence could degrade performance

### Mechanism 3
- Claim: Pseudo-labeling gradually improves noisy client model quality
- Mechanism: Identified noisy clients train additional models using pseudo-labels generated by the global model, which are expected to be more accurate than their original noisy labels.
- Core assumption: The global model, even when trained with noisy clients, maintains enough accuracy to provide useful pseudo-labels that can improve the noisy client models over time.
- Evidence anchors:
  - [section 3.5] "The first local model is trained by discarding the noisy labels entirely and updated in an unsupervised manner"
  - [section 3.5] "bDk is the local dataset with pseudo-labels assigned by the global model wt at round t"
  - [corpus] Weak - corpus papers don't explicitly discuss pseudo-labeling in the context of extremely noisy federated learning
- Break condition: If the global model's pseudo-labels are also incorrect, this could reinforce the noisy patterns rather than correct them

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Forms the basis for negative distillation, where the global model learns from noisy client models by diverging from them rather than converging
  - Quick check question: In standard knowledge distillation, what does the student model try to do with respect to the teacher model?

- Concept: Model Uncertainty Estimation
  - Why needed here: Used to identify which clients have extreme label noise based on the confidence of their model predictions
  - Quick check question: What is MC dropout and how does it relate to Bayesian uncertainty estimation?

- Concept: Federated Learning with Heterogeneous Data
  - Why needed here: The setting involves multiple clients with different data distributions and varying levels of label noise
  - Quick check question: What is the difference between IID and non-IID data distributions in federated learning?

## Architecture Onboarding

- Component map: Server (public dataset, model aggregation, negative distillation loss) -> Clients (local training with original/pseudo-labels, model upload) -> Server (updated global model)
- Critical path: 1. Server identifies noisy clients using model uncertainty 2. Server aggregates clean client models 3. Server performs negative distillation using noisy client models 4. Server sends updated global model back to clients 5. Clients update local models (with or without pseudo-labels) 6. Clients upload updated models to server
- Design tradeoffs: Public dataset vs. private data for uncertainty computation (privacy vs. accuracy); Threshold λ for identifying noisy clients (sensitivity vs. specificity); Number of MC dropout samples for uncertainty estimation (accuracy vs. computational cost)
- Failure signatures: High variance in accuracy across communication rounds; Degradation when noisy client proportion increases; Slow convergence when pseudo-labeling doesn't improve noisy client models
- First 3 experiments: 1. Verify uncertainty-based identification works by plotting uncertainty distributions for clean vs. noisy clients 2. Test negative distillation alone by comparing standard aggregation vs. negative distillation performance 3. Evaluate pseudo-labeling effectiveness by measuring accuracy improvement on noisy client validation sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold λ for identifying extremely noisy clients in different FL scenarios?
- Basis in paper: [explicit] The paper uses λ=0.12 but mentions it can be set within the range (0.12, 0.14) for easy segregation of MN and EN clients.
- Why unresolved: The optimal threshold may vary depending on the dataset, noise distribution, and other factors. The paper does not explore the impact of different threshold values on performance.
- What evidence would resolve it: Experiments comparing the performance of FedNed with different threshold values (e.g., 0.1, 0.12, 0.14, 0.16) on various datasets and noise distributions.

### Open Question 2
- Question: How does the choice of public dataset DU affect the performance of FedNed?
- Basis in paper: [explicit] The paper uses different public datasets (CIFAR-100 for CIFAR-10 and ImageNet for CIFAR-100) and mentions that using MNIST or synthetic data slightly lowers performance but still outperforms the second-best method.
- Why unresolved: The paper does not extensively explore the impact of different public dataset choices on FedNed's performance. The choice of public dataset may affect the quality of pseudo-labels and the effectiveness of negative distillation.
- What evidence would resolve it: Experiments comparing the performance of FedNed with different public datasets (e.g., CIFAR-100, ImageNet, MNIST, synthetic data) on various datasets and noise distributions.

### Open Question 3
- Question: How does the number of extremely noisy clients affect the performance of FedNed?
- Basis in paper: [explicit] The paper investigates the performance of FedNed with varying numbers of EN clients (1, 3, 5, 7, 9) and finds that FedNed remains stable while other methods' accuracy significantly declines.
- Why unresolved: The paper does not explore the upper limit of EN clients that FedNed can handle effectively. It also does not investigate the impact of different EN client noise ratios on performance.
- What evidence would resolve it: Experiments comparing the performance of FedNed with increasing numbers of EN clients (e.g., 10, 15, 20) and varying EN client noise ratios (e.g., 90%, 95%, 99%) on various datasets and noise distributions.

## Limitations
- The effectiveness of the client identification threshold (λ=0.12) is not thoroughly validated across different data distributions and noise patterns
- The assumption that model uncertainty reliably correlates with label noise may not hold when clients have heterogeneous data distributions
- The universal applicability of the uncertainty threshold for client identification across different federated learning scenarios is not established

## Confidence
- High confidence: The negative distillation mechanism's core concept and its implementation are well-specified and theoretically sound
- Medium confidence: The pseudo-labeling approach for noisy clients is reasonable but lacks extensive validation on whether the global model provides sufficiently accurate pseudo-labels
- Low confidence: The universal applicability of the uncertainty threshold for client identification across different federated learning scenarios

## Next Checks
1. Test the sensitivity of λ threshold by running experiments with varying threshold values (0.05, 0.12, 0.20) and measuring performance degradation
2. Validate the assumption that model uncertainty correlates with label noise by analyzing uncertainty distributions across different client types and data heterogeneity levels
3. Evaluate the quality of pseudo-labels generated by the global model by measuring their accuracy on a held-out validation set from noisy clients