---
ver: rpa2
title: 'Uncertainty as a Predictor: Leveraging Self-Supervised Learning for Zero-Shot
  MOS Prediction'
arxiv_id: '2312.15616'
source_url: https://arxiv.org/abs/2312.15616
tags:
- audio
- wav2vec
- prediction
- quality
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that uncertainty measures derived from
  pretrained self-supervised learning models like wav2vec correlate strongly with
  Mean Opinion Scores (MOS) for audio quality assessment. The authors show that contrastive
  wav2vec models achieve SRCC values around 70% on zero-shot MOS prediction tasks,
  outperforming wav2vec2-based models.
---

# Uncertainty as a Predictor: Leveraging Self-Supervised Learning for Zero-Shot MOS Prediction

## Quick Facts
- **arXiv ID**: 2312.15616
- **Source URL**: https://arxiv.org/abs/2312.15616
- **Reference count**: 0
- **Key outcome**: Uncertainty measures from wav2vec models correlate with MOS scores, achieving ~70% SRCC on zero-shot MOS prediction tasks.

## Executive Summary
This paper demonstrates that uncertainty measures derived from pretrained self-supervised learning models like wav2vec can effectively predict audio quality without requiring labeled MOS data. The authors show that high uncertainty in SSL model outputs corresponds to low audio quality, enabling zero-shot MOS prediction. wav2vec models outperform wav2vec2 variants, achieving SRCC values around 70% across multiple languages and tasks. The results suggest SSL models learn data distributions that capture audio quality features, providing a practical solution for audio quality assessment in low-resource settings.

## Method Summary
The method computes uncertainty measures (entropy, mean, max, std) from SSL model outputs (wav2vec and wav2vec2) without any training. These uncertainty measures are then correlated with MOS scores using Spearman correlation coefficient. The approach leverages out-of-the-box pretrained models, enabling zero-shot prediction. For wav2vec2, dropout is applied to increase uncertainty and improve calibration. The method is evaluated on VoiceMOS challenge datasets including English, Chinese, French, and singing data.

## Key Results
- wav2vec models achieve SRCC values around 70% for zero-shot MOS prediction
- wav2vec outperforms wav2vec2-based models for uncertainty-based MOS prediction
- Pre-training wav2vec on target domain audio increases performance to above 70% SRCC for several tasks
- Dropout handicapping wav2vec2 models increases correlation by up to 10%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High uncertainty in SSL model outputs corresponds to low audio quality
- Mechanism: SSL models learn a data distribution during pretraining; when presented with audio outside this distribution (poor quality), they exhibit higher uncertainty in token predictions
- Core assumption: SSL models capture the underlying distribution of high-quality audio during pretraining
- Evidence anchors:
  - [abstract] "uncertainty measures derived from out-of-the-box pretrained self-supervised learning (SSL) models, such as wav2vec, correlate with MOS scores"
  - [section 2] "audio samples that lead to significant uncertainties in the latent code/token probabilities output by SSL models such as wav2vec [9] must be noisier and/or of poorer quality"
- Break condition: If SSL models learn features unrelated to audio quality, the uncertainty-quality correlation may fail

### Mechanism 2
- Claim: Contrastive predictive probabilities from wav2vec are more directly related to audio quality than ASR-based probabilities from wav2vec2
- Mechanism: wav2vec's contrastive objective trains the model to distinguish between true and false latent tokens, making uncertainty in these predictions more reflective of audio quality
- Core assumption: The contrastive objective captures quality-relevant features in the latent space
- Evidence anchors:
  - [section 4] "we believe that wav2vec achieves better calibration for our task, in the sense that despite its lower capacity, UMs calculated using its outputs are more reflective of the model's uncertainties"
  - [section 4] "wav2vec models generate contrastive predictive probabilities, which are specifically trained to identify a latent token from a set of negatives. High uncertainties implied by these probabilities are likely to directly reflect lower audio quality"
- Break condition: If wav2vec2 models can be properly calibrated or if the contrastive objective doesn't capture quality-relevant features

### Mechanism 3
- Claim: Handicapping wav2vec2 models through dropout increases their correlation with MOS scores
- Mechanism: Introducing dropout forces the model to be less certain, improving its calibration for quality assessment by preventing overconfidence on difficult samples
- Core assumption: wav2vec2 models are overconfident on difficult/low-quality samples
- Evidence anchors:
  - [section 4] "we observe that increasing the dropout probability from zero (equivalent to the original model) in our best-performing wav2vec2 model increases the SRCC by as much as 10%"
- Break condition: If dropout degrades feature extraction too much or if the model doesn't benefit from uncertainty calibration

## Foundational Learning

- **Concept**: Self-supervised learning and contrastive predictive coding
  - Why needed here: The paper relies on SSL models (wav2vec, wav2vec2) whose pretraining objectives and representations are central to the uncertainty-MOS correlation
  - Quick check question: What is the difference between the pretraining objectives of wav2vec and wav2vec2?

- **Concept**: Uncertainty quantification in neural networks
  - Why needed here: The paper constructs uncertainty measures (entropy, mean, max, std) from SSL model outputs to predict MOS scores
  - Quick check question: How does entropy of a categorical distribution relate to model confidence?

- **Concept**: Spearman correlation as an evaluation metric
  - Why needed here: The paper uses SRCC to measure the relationship between uncertainty measures and MOS scores
  - Quick check question: Why might SRCC be preferred over Pearson correlation for this task?

## Architecture Onboarding

- **Component map**: Audio → SSL model (wav2vec/wav2vec2) → Logits → Uncertainty measures (entropy, mean, max, std) → MOS prediction
- **Critical path**: The SSL model's output logits are the critical path, as they directly determine the uncertainty measures
- **Design tradeoffs**: Using out-of-the-box SSL models enables zero-shot prediction but may limit performance compared to fine-tuned models; wav2vec offers better calibration but wav2vec2 has higher capacity
- **Failure signatures**: Poor correlation between uncertainty measures and MOS scores; high variance in predictions; degradation on out-of-domain languages/singing
- **First 3 experiments**:
  1. Verify that wav2vec uncertainty measures correlate with MOS on English speech data
  2. Compare wav2vec vs wav2vec2 performance on the same dataset
  3. Test the effect of dropout handicapping on wav2vec2 model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does wav2vec outperform wav2vec2 for MOS prediction using uncertainty measures?
- Basis in paper: [explicit] The paper states that wav2vec models achieve SRCC values of about 70% while wav2vec2-based models perform worse, and hypothesizes this may be due to the nature of contrastive predictive probabilities and better model calibration in wav2vec.
- Why unresolved: The authors only provide hypotheses about why this occurs but do not conduct experiments to definitively determine the cause.

### Open Question 2
- Question: How does pre-training wav2vec on target domain audio improve zero-shot MOS prediction performance?
- Basis in paper: [explicit] The paper shows that pre-training wav2vec on high-quality target audio increases performance to above 70% SRCC for several tasks, but does not explain the mechanism behind this improvement.
- Why unresolved: The paper demonstrates the effect but does not investigate why domain-adaptive pre-training leads to better uncertainty-based MOS prediction.

### Open Question 3
- Question: What is the relationship between SSL model uncertainty and other audio quality metrics beyond MOS scores?
- Basis in paper: [explicit] The paper mentions that intelligibility measures (WER) correlate with MOS scores and that their uncertainty measures correlate strongly with WER, but does not explore other potential quality metrics.
- Why unresolved: The paper only briefly mentions one additional metric and does not comprehensively investigate how uncertainty relates to other established audio quality assessment methods.

## Limitations
- Limited model diversity: Only wav2vec and wav2vec2 models were examined, leaving uncertainty about generalizability to other SSL architectures
- Dropout-handicapping mechanism: While dropout increases correlation, the underlying reasons remain speculative without direct empirical validation
- Domain specificity: The paper doesn't analyze which types of quality degradation are best captured by uncertainty measures

## Confidence
- **High confidence**: The core finding that SSL model uncertainty correlates with MOS scores is well-supported by empirical results across multiple datasets
- **Medium confidence**: The explanation that contrastive objectives make wav2vec better suited for quality assessment is plausible but not definitively proven
- **Low confidence**: The dropout-handicapping mechanism and its relationship to audio quality assessment lacks sufficient empirical grounding

## Next Checks
1. **Cross-architecture validation**: Test whether uncertainty-MOS correlation holds for other SSL models like HuBERT and WavLM, particularly examining if contrastive models consistently outperform masked language models for this task

2. **Error analysis by degradation type**: Systematically analyze which audio quality degradation types (noise, compression, reverberation) show the strongest uncertainty-MOS correlation, and whether different SSL models specialize in detecting different degradation types

3. **Ablation on dropout mechanism**: Conduct controlled experiments varying dropout rates systematically across different wav2vec2 model sizes to determine if the correlation improvement is due to uncertainty calibration or simply reduced model capacity