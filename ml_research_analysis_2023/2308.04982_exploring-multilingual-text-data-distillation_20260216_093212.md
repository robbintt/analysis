---
ver: rpa2
title: Exploring Multilingual Text Data Distillation
arxiv_id: '2308.04982'
source_url: https://arxiv.org/abs/2308.04982
tags:
- data
- distillation
- distilled
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes several data distillation techniques for multilingual
  text classification datasets using language-model-based learning methods. The authors
  conduct experiments to analyze their performance in terms of classification strength,
  and cross-architecture generalization.
---

# Exploring Multilingual Text Data Distillation

## Quick Facts
- arXiv ID: 2308.04982
- Source URL: https://arxiv.org/abs/2308.04982
- Authors: 
- Reference count: 4
- Primary result: Proposes four dataset distillation techniques for multilingual text classification, achieving 95% distillation ratio with 10 samples per class

## Executive Summary
This paper introduces four data distillation techniques for multilingual text classification datasets using language-model-based learning methods. The authors conduct experiments on the UMSAB multilingual sentiment analysis benchmark across 8 languages, demonstrating that their VanillaDistill method can achieve comparable classification performance using just 10 samples per class compared to training on the full dataset. The SkipLookupDistill method shows particular promise for cross-architecture generalization by learning distilled embeddings at the input level of mBERT rather than the output level.

## Method Summary
The paper proposes four dataset distillation approaches: VanillaDistill, which learns distilled embeddings at the output level of mBERT; SkipLookupDistill, which learns at the input level to improve cross-architecture generalization; and two VocabDistill variants using softmax and Gumbel-Softmax functions for vocabulary-level probability distributions. All methods optimize synthetic data by matching training trajectories on original data, then decode distilled embeddings back to text using nearest-neighbor search. The methods are evaluated on UMSAB (Unified Multilingual Sentiment Analysis Benchmark) across Arabic, English, French, German, Hindi, Italian, Portuguese, and Spanish.

## Key Results
- VanillaDistill achieves 95% distillation ratio, meaning a model trained with 10 samples per class performs comparably to one trained on the full dataset
- SkipLookupDistill demonstrates better cross-architecture generalization compared to VanillaDistill
- The methods maintain reasonable language-wise performance across the 8 languages in UMSAB
- Cross-architecture generalization F1-scores range from 0.740 to 0.758 depending on the distillation method used

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning distilled embeddings at the input level of mBERT improves cross-architecture generalization compared to learning at the output level.
- Mechanism: By generating distilled word embeddings directly at mBERT's input lookup layer rather than at the contextualized output, the distillation process avoids capturing model-specific architectural biases that would otherwise limit transferability to different architectures.
- Core assumption: The input-level embeddings are more architecture-agnostic than contextualized output embeddings because they represent fundamental word representations rather than model-specific context-dependent transformations.
- Evidence anchors: [section] "By focusing on learning distilled embeddings at the word-to-embedding level, rather than directly attempting to learn the pretrained context at the output level of mBERT, we can enhance the interpretability and generalization capability of the distillation process."

### Mechanism 2
- Claim: Using vocabulary-level distillation with probability distributions (softmax) enables more interpretable word selection than direct embedding generation.
- Mechanism: By synthesizing probability vectors over the vocabulary for each position and applying softmax, the method creates interpretable word choices through maximum probability selection, making the distillation process transparent and explainable.
- Core assumption: The probability-based synthesis captures meaningful relationships between words in context that can be recovered through nearest-neighbor search in the embedding space.
- Evidence anchors: [section] "We synthesize data at the vocabulary level and apply a Softmax function to obtain word probabilities for each position."

### Mechanism 3
- Claim: Gumbel-Softmax approximation enables discrete word selection while maintaining gradient flow during training.
- Mechanism: Replacing traditional softmax with Gumbel-Softmax allows the model to sample single words (rather than probability distributions) while still providing differentiable gradients for backpropagation, combining interpretability with trainability.
- Core assumption: The Gumbel-Softmax distribution can adequately approximate the categorical distribution over words while providing sufficient gradient information for effective training.
- Evidence anchors: [section] "In contrast to the model architecture depicted in Figure 3, the approach used here involves employing Gumbel-Softmax instead of the traditional Softmax function."

## Foundational Learning

- Concept: Text embedding spaces and continuous representations
  - Why needed here: The entire approach relies on converting discrete text into continuous embeddings that can be manipulated through gradient-based optimization methods.
  - Quick check question: Why can't we directly apply dataset distillation to raw text tokens instead of embeddings?

- Concept: Cross-architecture generalization and inductive bias
  - Why needed here: Understanding how architectural differences affect model performance is crucial for designing distillation methods that produce generalizable synthetic data.
  - Quick check question: What makes a synthetic dataset generalize well across different neural architectures?

- Concept: Multilingual model architectures (mBERT/XLM-R)
  - Why needed here: The work uses mBERT as the foundation for generating multilingual embeddings, requiring understanding of how these models handle multiple languages.
  - Quick check question: How does mBERT's architecture enable it to handle multiple languages simultaneously?

## Architecture Onboarding

- Component map: Original dataset -> Embedding layer (mBERT) -> Distillation algorithm -> Synthetic dataset -> Classification model (CNN layers)
- Core components: mBERT model (frozen), classification head (CNN layers), distillation optimizer, nearest-neighbor decoder
- Interfaces: Embedding lookup matrix, softmax/Gumbel-Softmax layers, classification head outputs

- Critical path:
  1. Embed original text data using mBERT
  2. Initialize distilled embeddings/probabilities
  3. Optimize distilled data by matching training trajectories
  4. Decode distilled embeddings back to text via nearest-neighbor
  5. Train classification model on distilled data
  6. Evaluate performance on test set

- Design tradeoffs:
  - Input-level vs output-level distillation: Input-level provides better generalization but may lose contextual information; output-level preserves context but limits transferability
  - Softmax vs Gumbel-Softmax: Softmax provides probability distributions (more information) but requires post-processing; Gumbel-Softmax gives discrete words directly but may be less stable
  - Number of distilled samples: More samples improve performance but reduce the benefits of dataset distillation

- Failure signatures:
  - Poor cross-architecture generalization: High variance in F1-scores across different model architectures
  - Convergence issues: Stagnant or oscillating loss during distillation optimization
  - Interpretability problems: Distilled embeddings that don't map cleanly to meaningful words via nearest-neighbor search

- First 3 experiments:
  1. Baseline comparison: Train classification model on original full dataset vs distilled dataset (1 sample/class) using VanillaDistill method
  2. Architecture sensitivity test: Generate distilled data using one architecture and evaluate on multiple different architectures
  3. Language distribution analysis: Compare token-level language proportions between original and distilled datasets using language detection model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed dataset distillation approach affect fairness across different language groups in the distilled dataset?
- Basis in paper: [explicit] The authors investigate language-specific fairness of the distilled data summaries and analyze the distribution of samples across different languages in the distilled data.
- Why unresolved: While the authors analyze language-wise performance and token-level language proportion, they do not explicitly evaluate fairness metrics or compare fairness outcomes between the original and distilled datasets across different language groups.
- What evidence would resolve it: Quantitative fairness metrics (e.g., demographic parity, equal opportunity) computed for each language group on both original and distilled datasets, showing any disparities or improvements.

### Open Question 2
- Question: What is the impact of using learnable labels instead of fixed labels in the dataset distillation process?
- Basis in paper: [inferred] The authors mention in their conclusion that it would be interesting to use learnable labels instead of fixed labels in future work, implying that this has not been explored yet.
- Why unresolved: The authors only experiment with fixed labels in their current study and do not investigate the potential benefits or drawbacks of using learnable labels in the dataset distillation process.
- What evidence would resolve it: Comparative experiments showing the performance of dataset distillation with both fixed and learnable labels, demonstrating any improvements or differences in distillation quality and downstream task performance.

### Open Question 3
- Question: How does the performance of the dataset distillation methods scale with larger datasets and more complex classification models?
- Basis in paper: [inferred] The authors conduct experiments on a specific multilingual sentiment analysis dataset and a relatively simple classification model, suggesting that further exploration with larger datasets and more complex models is needed.
- Why unresolved: The current study is limited to a specific dataset and model architecture, and the authors suggest experimenting with a broader range of datasets and better classification models in future work.
- What evidence would resolve it: Experiments on larger, more diverse datasets and with more complex classification models (e.g., fine-tuned language models) showing the scalability and generalizability of the proposed dataset distillation methods.

## Limitations

- Weak corpus support for cross-architecture generalization claims, with only 25 related papers and average neighbor FMR of 0.451
- Exceptional 95% distillation ratio claim may not generalize well across different classification tasks or languages beyond the UMSAB benchmark
- Exact implementation details of the text distillation algorithm are not fully specified, creating ambiguity in reproducing results

## Confidence

- High Confidence: The general methodology of using language-model-based learning for text dataset distillation is well-established, and the mathematical framework for dataset distillation is sound.
- Medium Confidence: The experimental results showing VanillaDistill achieving 95% distillation ratio are supported by the reported metrics, though the practical significance across different tasks remains uncertain.
- Low Confidence: The cross-architecture generalization improvements claimed for SkipLookupDistill are based on limited comparisons and lack robust statistical validation across diverse architectures.

## Next Checks

1. Conduct a detailed code review of the distillation algorithm implementation, focusing on the optimization procedure and gradient computation steps to ensure faithful reproduction of the methodology.

2. Systematically test the distilled datasets across a broader range of classification architectures (CNN, RNN, transformer-based) to validate the claimed cross-architecture generalization improvements, measuring performance variance and conducting statistical significance tests.

3. Evaluate the distillation methods on additional multilingual datasets beyond UMSAB, including languages with different scripts and linguistic structures, to assess whether the 95% distillation ratio claim holds across diverse language families.