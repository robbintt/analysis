---
ver: rpa2
title: 'KGLens: Towards Efficient and Effective Knowledge Probing of Large Language
  Models with Knowledge Graphs'
arxiv_id: '2312.11539'
source_url: https://arxiv.org/abs/2312.11539
tags:
- knowledge
- llms
- question
- edges
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KGLens addresses the problem of efficiently and effectively evaluating
  the factual knowledge alignment between large language models (LLMs) and curated
  knowledge graphs (KGs), which is challenging due to the size of KGs and the need
  for up-to-date, domain-specific knowledge assessment. The core method, KGLens, introduces
  a parameterized KG where each edge is augmented with a beta distribution to guide
  sampling during evaluation.
---

# KGLens: Towards Efficient and Effective Knowledge Probing of Large Language Models with Knowledge Graphs

## Quick Facts
- **arXiv ID**: 2312.11539
- **Source URL**: https://arxiv.org/abs/2312.11539
- **Reference count**: 15
- **Primary result**: Achieves 95.7% accuracy compared to human annotators while probing only 24.1% of KG edges

## Executive Summary
KGLens addresses the challenge of efficiently evaluating the factual knowledge alignment between large language models (LLMs) and curated knowledge graphs (KGs). Traditional brute-force evaluation requires probing all KG edges, which becomes computationally prohibitive for large graphs. KGLens introduces a parameterized KG with beta distributions on edges to guide sampling, combined with a graph-guided question generation strategy that converts KG edges into natural language questions. The method iteratively updates edge difficulty estimates based on LLM responses, focusing evaluation on knowledge gaps while maintaining comprehensive coverage.

## Method Summary
KGLens operates by first constructing a parameterized knowledge graph where each edge is associated with a beta distribution representing the LLM's failure probability. During evaluation, edges are sampled using Thompson sampling based on these distributions. GPT-4 generates natural language questions from KG edges using a graph-guided strategy that includes entity aliases for context. After each LLM response, the beta distributions are updated using Thompson sampling, and failure signals propagate to neighboring edges to account for correlated knowledge gaps. This iterative process continues until convergence, enabling efficient identification of LLM knowledge gaps while probing only a fraction of total KG edges.

## Key Results
- Probing efficiency: Requires only 24.1% of KG edges versus 71.7% for brute-force methods
- Accuracy: Achieves 95.7% of human annotator accuracy in knowledge assessment
- Coverage: Maintains comprehensive evaluation across entity types, relations, and temporal factors
- Scalability: Successfully evaluated on three domain-specific KGs with over 19K edges total

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The parameterized KG with beta-distributed edge weights enables efficient focus on LLM knowledge gaps
- Mechanism: Each edge has a beta distribution θ_j ~ Beta(α_j, β_j) that represents the LLM's failure probability. Edges are sampled based on these probabilities, and the distributions are updated using Thompson sampling after each LLM response
- Core assumption: Beta-Bernoulli conjugacy allows efficient posterior updates and Thompson sampling provides good exploration-exploitation balance
- Evidence anchors:
  - [abstract]: "KGLens features a graph-guided question generator for converting KGs into natural language, along with a carefully designed importance sampling strategy based on parameterized KG structure to expedite KG traversal"
  - [section 3.1]: "We use beta distribution to model θ due to the conjugacy between Bernoulli distribution and Beta distribution"
  - [corpus]: Weak evidence - related papers mention knowledge graph traversal but not this specific Thompson sampling approach
- Break condition: If LLM responses are uncorrelated with edge difficulty, beta distribution updates won't converge to meaningful values

### Mechanism 2
- Claim: Graph-guided question generation produces more natural and less ambiguous questions than cloze tasks
- Mechanism: Uses GPT-4 to convert KG edges into natural language questions with entity aliases for context, supporting both Yes/No and Wh-questions based on graph structure
- Core assumption: Including entity aliases and using natural question formats reduces ambiguity compared to cloze statements
- Evidence anchors:
  - [abstract]: "The proposed graph-guided QG strategy enables us to evaluate LLMs in a way that is more similar to human interaction"
  - [section 3.2]: "We design a graph-guided QG strategy to enhance the naturalness and reduce the ambiguity of the generated questions"
  - [corpus]: No direct evidence - related work focuses on KG integration but not this specific QG approach
- Break condition: If generated questions remain ambiguous despite aliases, evaluation accuracy will suffer

### Mechanism 3
- Claim: Signal propagation to neighboring edges enables efficient knowledge gap identification
- Mechanism: When an LLM fails on an edge, the failure signal propagates to connected edges (one degree), updating their beta parameters to reflect correlated knowledge gaps
- Core assumption: Knowledge gaps are correlated among connected edges in the KG
- Evidence anchors:
  - [section 3.1]: "In order to account for the high correlation in error probability among the connected edges, we have additionally propagate the signal to the neighboring edges"
  - [section 3.1]: "The signal gathered from pj is propagated to both the incoming and outgoing edges that are connected to node sj and oj"
  - [corpus]: No direct evidence - this specific propagation mechanism isn't mentioned in related work
- Break condition: If knowledge gaps are not correlated across edges, propagation will introduce noise

## Foundational Learning

- Concept: Thompson sampling for multi-armed bandit problems
  - Why needed here: KGLens uses Thompson sampling to balance exploration (trying uncertain edges) and exploitation (focusing on known difficult edges) during KG traversal
  - Quick check question: What distribution family is conjugate to Bernoulli for Thompson sampling updates?

- Concept: Beta-Bernoulli conjugacy
  - Why needed here: Enables efficient posterior updates of edge difficulty probabilities using simple parameter updates
  - Quick check question: Given a Beta(α, β) prior and observing a failure (Bernoulli 0), what are the updated parameters?

- Concept: Knowledge graph traversal and random walks
  - Why needed here: KG cleaning and sampling strategies rely on random walks to maintain data distribution while focusing on relevant entities
  - Quick check question: How does limiting random walk distance affect the representativeness of sampled subgraphs?

## Architecture Onboarding

- Component map: Parameterized KG -> Graph-guided QG -> QA Examiner -> Signal Propagator -> KG Cleaner
- Critical path:
  1. Initialize parameterized KG with Beta(1,1) on all edges
  2. Sample edges using Thompson sampling
  3. Generate questions via graph-guided QG
  4. Evaluate LLM responses
  5. Update beta distributions and propagate signals
  6. Repeat until convergence
- Design tradeoffs:
  - Thompson sampling vs. epsilon-greedy: Better exploration but more complex
  - One-degree vs. multi-degree signal propagation: Faster but potentially less accurate
  - Yes/No vs. Wh-questions: Simpler verification vs. richer evaluation
- Failure signatures:
  - Beta distributions not converging: Indicates LLM responses uncorrelated with edge difficulty
  - High variance in question generation: GPT-4 prompts may need adjustment
  - Signal propagation creating noise: Correlation assumption may not hold
- First 3 experiments:
  1. Test beta distribution updates with synthetic LLM responses
  2. Verify question generation quality with human evaluation
  3. Measure convergence speed on small KG with known LLM knowledge gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of KGLens vary across different domain-specific knowledge graphs, and what factors contribute to these variations?
- Basis in paper: Explicit
- Why unresolved: The paper presents results for three domain-specific knowledge graphs (country, NBA, movies) but does not provide a detailed analysis of the factors contributing to performance variations across these domains.
- What evidence would resolve it: A comparative analysis of the knowledge graphs' characteristics (e.g., size, complexity, domain-specificity) and their impact on KGLens' performance would provide insights into the factors influencing the method's effectiveness.

### Open Question 2
- Question: Can KGLens be effectively applied to multilingual knowledge graphs, and how would this impact its performance?
- Basis in paper: Inferred
- Why unresolved: The paper focuses on English-language knowledge graphs and does not explore the method's applicability to multilingual knowledge graphs.
- What evidence would resolve it: Evaluating KGLens on multilingual knowledge graphs and comparing its performance to the monolingual case would determine its effectiveness in handling multiple languages.

### Open Question 3
- Question: How does the choice of the initial beta distribution parameters in the parameterized knowledge graph affect KGLens' convergence and performance?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that the prior of each theta is set to Beta(1, 1) but does not investigate the impact of different initial parameter choices on the method's convergence and performance.
- What evidence would resolve it: Conducting experiments with various initial beta distribution parameters and analyzing their impact on convergence speed and overall performance would provide insights into the sensitivity of KGLens to initial parameter choices.

## Limitations
- The approach assumes knowledge gaps are correlated across connected edges, which may not hold for all domains
- Requires multiple LLM interactions to converge, limiting rapid assessment scenarios
- Performance may degrade on smaller KGs where brute-force evaluation is computationally feasible

## Confidence

**Medium confidence** in efficiency claims - Limited comparison scope with other sampling strategies
**Low confidence** in signal propagation effectiveness - No ablation study demonstrating marginal benefit
**Medium confidence** in question generation quality - Relies on win rate comparisons rather than direct human assessment

## Next Checks

1. **Ablation study on signal propagation**: Compare KGLens performance with and without one-degree signal propagation to quantify its contribution to efficiency gains.

2. **Cross-domain robustness test**: Evaluate KGLens on KGs from domains with different correlation structures (e.g., scientific literature vs. social networks) to assess the universality of the correlation assumption.

3. **Human evaluation of question quality**: Conduct a direct human assessment comparing graph-guided questions versus cloze statements on metrics like naturalness, ambiguity, and answerability to validate the claimed superiority of the QG approach.