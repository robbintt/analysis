---
ver: rpa2
title: Domain Generalisation with Bidirectional Encoder Representations from Vision
  Transformers
arxiv_id: '2307.08117'
source_url: https://arxiv.org/abs/2307.08117
tags:
- domain
- vision
- generalisation
- accuracy
- beit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a study of vision transformers for domain generalisation
  in computer vision. The authors compare four vision transformer architectures (ViT,
  LeViT, DeiT, BEIT) on out-of-distribution (OOD) ImageNet variants and select BEIT
  based on its superior performance.
---

# Domain Generalisation with Bidirectional Encoder Representations from Vision Transformers

## Quick Facts
- arXiv ID: 2307.08117
- Source URL: https://arxiv.org/abs/2307.08117
- Reference count: 1
- Primary result: BEIT achieves state-of-the-art domain generalization results with 0.96 validation and 0.94 test accuracy on PACS

## Executive Summary
This paper investigates vision transformers for domain generalization in computer vision, focusing on four transformer architectures (ViT, LeViT, DeiT, BEIT) evaluated on out-of-distribution ImageNet variants. The authors identify BEIT as the superior architecture based on its performance and use it to achieve state-of-the-art results on three domain generalization benchmarks (PACS, Office-Home, DomainNet). The method demonstrates remarkable ability to generalize across domains with minimal performance gap between in-distribution and out-of-distribution data, particularly excelling on PACS with a negligible 0.02 gap between validation and test accuracy.

## Method Summary
The study fine-tunes a pre-trained BEIT vision transformer (base version with 12 layers, 768 hidden units) on three domain generalization benchmarks using standard data augmentation including resizing, random horizontal flip, and normalization. BEIT was selected based on superior performance on OOD ImageNet variants including Sketch, R, Adversarial, and Corrupted datasets. The fine-tuning process maintains BEIT's core architecture while adapting it to domain-shifted data, leveraging the model's pre-trained bidirectional encoder representations from masked image modeling to achieve robust generalization across diverse visual domains.

## Key Results
- BEIT achieves 0.96 validation and 0.94 test accuracy on PACS with only 0.02 gap between IID and OOD performance
- On Office-Home, BEIT achieves 0.86 validation and 0.87 test accuracy with a negative gap (-0.0094), indicating better OOD performance
- On DomainNet, BEIT achieves 0.70 test accuracy, significantly outperforming existing domain generalization approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BEIT's self-supervised pre-training on large-scale ImageNet-21k provides rich visual representations that generalize better across domains
- Mechanism: BEIT uses masked image modeling (MIM) to learn bidirectional encoder representations by reconstructing masked patches from visible ones, forcing the model to understand global context rather than local patterns
- Core assumption: The visual tokens learned through MIM capture domain-agnostic features that transfer effectively to unseen domains
- Evidence anchors: BEIT architecture performs best and is used in further experiments; the main reason BEIT surpasses others are its properties including MIM with self-supervised learning; weak evidence from corpus papers

### Mechanism 2
- Claim: The attention mechanism in BEIT enables better handling of variable-length inputs and global context understanding compared to CNNs
- Mechanism: Self-attention allows the model to weigh relationships between all patches regardless of spatial distance, capturing long-range dependencies that are crucial for domain generalization
- Core assumption: Global attention patterns learned during pre-training remain effective when fine-tuned on different domain distributions
- Evidence anchors: Vision transformers use self-attention during learning and an attention score is computed by product of query-key terms; vision transformers are inherently more appropriate for domain generalization due to global understanding and attention mechanism

### Mechanism 3
- Claim: BEIT's denoising capability improves robustness to visual distractions and corruptions across domains
- Mechanism: The pre-training objective includes reconstructing corrupted inputs, making the model inherently robust to various image distortions and domain shifts
- Core assumption: Robustness learned through denoising during pre-training transfers to handling domain-specific corruptions in target benchmarks
- Evidence anchors: BEIT is trained with 8,192 visual tokens and ImageNet Corrupted consists of images with 75 common visual distractions; the goal was to improve and evaluate the robustness of models

## Foundational Learning

- Concept: Self-supervised learning through masked image modeling
  - Why needed here: Enables learning rich visual representations without extensive labeled data, crucial for domain generalization where labeled target data is unavailable
  - Quick check question: How does masked image modeling differ from contrastive learning approaches like SimCLR?

- Concept: Attention mechanisms in transformers
  - Why needed here: Allows capturing long-range dependencies and global context that are important for generalizing across domain shifts
  - Quick check question: What is the computational complexity of self-attention compared to convolutional layers?

- Concept: Domain generalization principles
  - Why needed here: Understanding how to learn representations that perform well on unseen distributions is central to this work
  - Quick check question: What is the difference between domain adaptation and domain generalization?

## Architecture Onboarding

- Component map: Image → Patch Embedding → 12 Transformer Layers → CLS Token → Classification Head
- Critical path: Image → Patch Embedding → 12 Transformer Layers → CLS Token → Classification Head
- Design tradeoffs: BEIT trades computational efficiency for better generalization through pre-training; the 12-layer base model is smaller than state-of-the-art vision models but achieves better domain generalization
- Failure signatures: Large gaps between validation and test accuracy indicate overfitting to source domains; poor performance on specific domains (like artwork in Office-Home) suggests pre-training data bias
- First 3 experiments:
  1. Fine-tune BEIT on PACS with standard hyperparameters to establish baseline performance
  2. Evaluate BEIT on ImageNet-OOD variants (Sketch, R, Adversarial, Corrupted) to understand generalization capability
  3. Compare BEIT with ViT and DeiT on Office-Home to quantify the benefit of BEIT's pre-training approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the BEIT model's performance on domain generalization benchmarks change when using different sizes of pre-trained models (e.g., base vs. large vs. huge)?
- Basis in paper: The paper uses a base version of BEIT (12 transformer layers, 768 hidden size) but does not explore how different model sizes affect domain generalization performance
- Why unresolved: The paper only experiments with the base version of BEIT and does not provide comparative results with larger model variants, leaving uncertainty about scalability of the approach
- What evidence would resolve it: Experiments comparing domain generalization performance across different BEIT model sizes (base, large, huge) on the same benchmarks would show how model capacity affects generalization ability

### Open Question 2
- Question: What specific properties of the BEIT architecture (compared to other vision transformers) make it particularly effective for domain generalization?
- Basis in paper: The paper mentions BEIT's properties like Mask Image Modeling (MIM), self-supervised learning, self-attention mechanism, and denoising of corrupted inputs as reasons for its superiority, but does not systematically isolate which properties contribute most to domain generalization
- Why unresolved: While the paper identifies several BEIT properties, it does not conduct ablation studies or controlled experiments to determine which architectural features are most critical for domain generalization success
- What evidence would resolve it: Systematic ablation studies comparing BEIT variants with different architectural components disabled or modified would identify which properties are essential for domain generalization

### Open Question 3
- Question: How does the BEIT model perform on domain generalization tasks with more than four domains or with highly imbalanced domain distributions?
- Basis in paper: The paper tests on three benchmarks (PACS, Office-Home, DomainNet) but does not explore scenarios with more domains or examine performance under domain imbalance conditions, which are common in real-world applications
- Why unresolved: The benchmarks used have relatively few domains (4 or 6) and balanced class distributions, leaving open questions about BEIT's scalability and robustness to domain imbalance
- What evidence would resolve it: Testing BEIT on benchmarks with larger numbers of domains or artificially created imbalanced domain distributions would reveal its limitations and scalability

## Limitations
- Lack of comparison against non-transformer state-of-the-art domain generalization methods that use different architectures
- No ablation experiments to isolate which components of BEIT's pre-training contribute most to domain generalization performance
- Substantial computational resources required for pre-training BEIT may limit practical applicability

## Confidence
- High confidence: BEIT's superior performance compared to other vision transformer variants (ViT, LeViT, DeiT) on OOD ImageNet variants
- Medium confidence: BEIT achieving state-of-the-art results on domain generalization benchmarks when compared only to other transformer-based approaches
- Low confidence: Claims about BEIT's mechanisms (MIM, attention, denoising) being the primary drivers of improved generalization, as these are not experimentally isolated or validated

## Next Checks
1. Conduct controlled ablation experiments to isolate the contribution of masked image modeling, self-attention mechanisms, and denoising capabilities to domain generalization performance
2. Directly compare BEIT against established domain generalization methods (e.g., D-MTAE, MetaReg, CrossGrad) using identical benchmarks and evaluation protocols
3. Evaluate BEIT's performance across different pre-training dataset sizes and domain distributions to determine minimum requirements for effective domain generalization