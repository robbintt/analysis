---
ver: rpa2
title: Optimization Dynamics of Equivariant and Augmented Neural Networks
arxiv_id: '2303.13458'
source_url: https://arxiv.org/abs/2303.13458
tags:
- equivariant
- augmented
- gradient
- have
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares optimization dynamics between equivariant and
  augmented neural networks on symmetric data. Under natural assumptions on loss and
  nonlinearities, it proves that the sets of equivariant stationary points are identical
  for both strategies, and that the equivariant subspace remains invariant under augmented
  gradient flow when initialized equivariantly.
---

# Optimization Dynamics of Equivariant and Augmented Neural Networks

## Quick Facts
- arXiv ID: 2303.13458
- Source URL: https://arxiv.org/abs/2303.13458
- Authors: 
- Reference count: 34
- One-line primary result: Under natural assumptions, the sets of equivariant stationary points are identical for equivariant and augmented training, but stationary points may be unstable under augmented training.

## Executive Summary
This paper analyzes the optimization dynamics of equivariant neural networks compared to standard augmented training on symmetric data. The authors prove that the sets of equivariant stationary points are identical for both strategies under natural assumptions on loss and nonlinearities. They show that the equivariant subspace remains invariant under augmented gradient flow when initialized equivariantly, but stationary points may exhibit instability under augmented training even when stable for equivariant models. Experiments on graph classification, translation invariant MNIST, and rotation equivariant segmentation confirm these theoretical findings.

## Method Summary
The study compares three training strategies (NOMINAL, AUGMENTED, EQUIVARIANT) across three tasks using multilayer perceptrons with batch normalization. Models are trained for 50 epochs with learning rate 1e-5 on synthetically generated graphs, subsampled MNIST, and synthetic shapes. The key comparison is the distance from the starting position and non-equivariance metrics between augmented models with equivariant initialization and equivariant models.

## Key Results
- The sets of equivariant stationary points are identical for both equivariant and augmented training strategies
- The equivariant subspace remains invariant under augmented gradient flow when initialized equivariantly
- Augmented models with equivariant initialization maintain closer proximity to the equivariant subspace during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Equivariant stationary points are identical for equivariant and augmented training strategies.
- Mechanism: Under Assumptions 1-3, the gradient of the augmented risk at any equivariant point equals the gradient of the equivariant risk, making their stationary sets identical.
- Core assumption: The Haar measure's invariance and the loss function's invariance under group action.
- Evidence anchors:
  - [abstract]: "under natural assumptions on the data, network, loss, and group of symmetries...the sets of equivariant stationary points are identical for the two strategies"
  - [section]: "For A∈E we have ∇Raug(A) = ΠE∇R(A) = ∇Reqv(A)" (Proposition 1)
  - [corpus]: Weak evidence - only tangentially related papers on equivariant architectures, no direct comparison to augmentation dynamics
- Break condition: If the loss function is not invariant under group action, or if the Haar measure is not used in augmentation.

### Mechanism 2
- Claim: The equivariant subspace remains invariant under augmented gradient flow when initialized equivariantly.
- Mechanism: The gradient of the augmented risk at equivariant points lies entirely within the equivariant subspace, preventing departure from it during training.
- Core assumption: The model's non-linearities are equivariant under the group action.
- Evidence anchors:
  - [abstract]: "the set of equivariant layers is even invariant under the gradient flow for augmented models"
  - [section]: "The equivariant subspace E is invariant under the gradient flow of Raug" (Corollary 1)
  - [corpus]: No direct evidence - papers discuss equivariant architectures but not their invariance properties during augmentation training
- Break condition: If non-linearities break equivariance, or if initialization is not equivariant.

### Mechanism 3
- Claim: Stationary points stable for equivariant training may be unstable for augmented training.
- Mechanism: The Hessian of the augmented risk restricted to the orthogonal complement of the equivariant subspace may have negative eigenvalues, causing instability even when the original point is stable.
- Core assumption: The augmented Hessian's restriction to E⊥ has negative eigenvalues at some stationary points.
- Evidence anchors:
  - [abstract]: "stationary points may be unstable for augmented training although they are stable for the equivariant models"
  - [section]: "Proposition 3: If A* is a strict local minimum of Raug, it is a strict local minimum of Reqv" (but not the converse)
  - [corpus]: No direct evidence - papers discuss equivariant neural networks but not stability comparisons between training strategies
- Break condition: If the augmented Hessian's restriction to E⊥ is positive definite at all stationary points.

## Foundational Learning

- Representation theory and group actions:
  - Why needed here: To formally define equivariance and understand how group actions induce representations on linear maps and tensor products
  - Quick check question: What is the relationship between a group representation on a vector space and the induced representation on the space of linear maps between vector spaces?
- Gradient flow and optimization dynamics:
  - Why needed here: To analyze how different training strategies affect the convergence behavior and stability of stationary points
  - Quick check question: How does the gradient flow of a projected risk differ from the gradient flow of the original risk?
- Tensor products and bilinear forms:
  - Why needed here: To express and analyze the Hessian of the augmented risk as a bilinear form on the space of linear layers
  - Quick check question: How does a representation on a vector space induce a representation on its tensor product with itself?

## Architecture Onboarding

- Component map: Input layer -> Hidden layers -> Output layer, where hidden layers consist of linear layers Hom(Xi, Xi+1) with induced representations and equivariant non-linearities Xi+1 → Xi+1
- Critical path:
  1. Define group action and representations on input/output spaces
  2. Construct equivariant layers using projection onto E
  3. Implement equivariant non-linearities
  4. Set up training with either equivariant or augmented strategy
  5. Monitor distance to equivariant subspace during training
- Design tradeoffs:
  - Equivariant: More parameter-efficient, computationally expensive layers, guaranteed equivariance
  - Augmented: Less parameter-efficient, computationally cheaper layers, no guaranteed equivariance
- Failure signatures:
  - Model diverges from equivariant subspace during augmented training
  - Training fails to converge to stable stationary points
  - Equivariance not maintained at test time
- First 3 experiments:
  1. Implement permutation invariant graph classification with small synthetic graphs
  2. Implement translation invariant MNIST classification with subsampled images
  3. Implement rotation equivariant image segmentation with synthetic shapes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions do augmented models exhibit instability in the equivariant subspace during training, and can this be quantified through spectral analysis of the Hessian?
- Basis in paper: [explicit] The paper mentions that stationary points may be unstable for augmented training although they are stable for the equivariant models, and observes that the stability of the equivariant subspace is completely determined by the spectrum of ∇²R_aug restricted to E⊥.
- Why unresolved: While the paper identifies the role of the Hessian spectrum in determining stability, it does not provide concrete conditions or thresholds for when instability occurs. The paper leaves the closer study of these matters to further work.
- What evidence would resolve it: Empirical studies across a range of tasks and network architectures to identify patterns in the Hessian spectrum that correlate with instability. Analytical bounds on eigenvalues of the Hessian for specific model classes.

### Open Question 2
- Question: How does the efficiency of parameter usage in equivariant models compare to augmented models in practice, particularly when the symmetry group is large or complex?
- Basis in paper: [inferred] The paper contrasts the efficient parameter usage through weight sharing in equivariant models with the inefficient use in augmented models, but does not provide empirical comparisons of parameter efficiency.
- Why unresolved: The paper focuses on theoretical comparisons of dynamics and stability, but does not quantify the practical parameter efficiency differences between the two strategies.
- What evidence would resolve it: Systematic experiments comparing model sizes, training times, and generalization performance for equivariant vs. augmented models across tasks with varying symmetry group complexities.

### Open Question 3
- Question: Can the global convergence properties of the augmented gradient flow be characterized, and under what conditions does it converge to the same minima as the equivariant model?
- Basis in paper: [explicit] The paper notes that while the dynamics on the equivariant subspace E are identical for both models, nothing is known about the dynamics away from E for the augmented model.
- Why unresolved: The paper establishes local convergence properties near E but does not analyze the global landscape of the augmented risk function.
- What evidence would resolve it: Proofs of global convergence conditions for augmented models, potentially using techniques from non-convex optimization theory. Empirical validation across diverse tasks and architectures.

### Open Question 4
- Question: How do batch normalization layers affect the equivariance properties of neural networks during training, and can they introduce instability in the equivariant subspace?
- Basis in paper: [explicit] The paper mentions incorporating batch normalization layers into models, noting it entails a slight deviation from the theoretical setting but importantly does not break invariance or equivariance at test time.
- Why unresolved: While the paper acknowledges the practical use of batch normalization, it does not analyze its impact on the theoretical results regarding equivariant dynamics.
- What evidence would resolve it: Theoretical analysis of how batch normalization interacts with group actions during training. Controlled experiments comparing models with and without batch normalization across various symmetry groups.

## Limitations

- The theoretical analysis relies on Assumptions 1-3 about loss invariance and group action properties, which may not hold for all practical applications
- The empirical validation covers only three specific tasks (graph classification, MNIST classification, and segmentation), limiting generalizability
- The comparison focuses on specific group symmetries (permutations, translations, rotations) without exploring more complex or combined symmetries

## Confidence

- High confidence: The theoretical proof that stationary points are identical for both strategies (Mechanism 1)
- Medium confidence: The invariance of the equivariant subspace under augmented gradient flow (Mechanism 2)
- Medium confidence: The experimental validation showing proximity to equivariant subspace during augmented training

## Next Checks

1. Test the theoretical findings on additional symmetry groups (e.g., dihedral groups for images) to verify generality beyond permutations, translations, and rotations
2. Evaluate model performance and optimization dynamics on larger-scale datasets to assess practical implications beyond synthetic and small-scale experiments
3. Analyze the behavior under different optimization algorithms (e.g., Adam, SGD with momentum) to determine if the theoretical results hold beyond standard gradient descent