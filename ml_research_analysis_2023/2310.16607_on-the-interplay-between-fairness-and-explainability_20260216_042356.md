---
ver: rpa2
title: On the Interplay between Fairness and Explainability
arxiv_id: '2310.16607'
source_url: https://arxiv.org/abs/2310.16607
tags:
- fairness
- methods
- fair
- bias
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the relationship between fairness and explainability
  in NLP models. The authors evaluate five fairness-promoting methods and three rationale
  extraction frameworks on two English multi-class classification datasets (BIOS and
  ECtHR).
---

# On the Interplay between Fairness and Explainability

## Quick Facts
- arXiv ID: 2310.16607
- Source URL: https://arxiv.org/abs/2310.16607
- Authors: 
- Reference count: 25
- Key outcome: Empirical fairness and explainability are orthogonal - improving one does not improve the other.

## Executive Summary
This paper investigates the relationship between fairness and explainability in NLP models through systematic evaluation of five fairness-promoting methods and three rationale extraction frameworks on English text classification tasks. The authors find that bias mitigation algorithms do not reliably lead to fairer models, and that empirical fairness and explainability are fundamentally independent dimensions of model trustworthiness. These findings suggest that achieving both fairness and explainability simultaneously remains an open challenge in building trustworthy NLP systems.

## Method Summary
The study evaluates five fairness-promoting methods (FAIR-GP, FAIR-GN, FAIR-DRO, FAIR-SD, FAIR-DFL) and three rationale extraction frameworks (REF-BASE, REF-3P, REF-R2A) on RoBERTa-base models fine-tuned for multi-class classification on BIOS and ECtHR datasets. The evaluation uses macro-F1 for overall performance, group-wise F1 scores and their absolute difference for fairness, AOPC for faithfulness, token R@k for human-model rationale alignment, and L2 norm of classification logits as a bias proxy. Synthetic versions of the BIOS dataset are used to create controlled bias scenarios for comparative analysis.

## Key Results
- Bias mitigation algorithms do not reliably lead to fairer models, as they often rely on spurious correlations rather than true causal relationships.
- Empirical fairness and explainability are orthogonal - improving one does not improve the other.
- Over-confidence in model predictions indicates reliance on spurious correlations, which can be a source of bias.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bias mitigation methods do not always lead to fairer models because they often rely on spurious correlations rather than true causal relationships.
- Mechanism: When models are trained on biased data, they learn to exploit these biases (e.g., gender correlations with occupations) rather than understanding the true task. Fairness methods that don't address the root cause of bias (like neutralizing gender information) still allow models to use superficial correlations.
- Core assumption: The presence of spurious correlations in training data leads models to learn biased decision-making patterns.
- Evidence anchors:
  - [abstract] "We find that bias mitigation algorithms do not always lead to fairer models."
  - [section] "All fairness-promoting methods improve empirical fairness compared to the baseline, but in such extreme scenarios only a direct manual intervention on the data as in FAIR-GN can sufficiently overcome spurious correlations."
  - [corpus] Weak evidence - corpus neighbors discuss fairness and explainability intersection but don't provide specific empirical evidence for this mechanism.
- Break condition: If training data contains no spurious correlations or if methods directly neutralize protected attribute information.

### Mechanism 2
- Claim: Empirical fairness and explainability are orthogonal because improving one does not necessarily improve the other.
- Mechanism: The dimensions of fairness (performance parity) and explainability (faithfulness and alignment with human rationales) are independent. Optimizing for fairness through bias mitigation doesn't inherently create more interpretable or human-aligned explanations, and vice versa.
- Core assumption: Fairness and explainability are separate dimensions of model trustworthiness that can be optimized independently.
- Evidence anchors:
  - [abstract] "we discover that empirical fairness and explainability are orthogonal."
  - [section] "we see that improving either empirical fairness or explainability does not improve the other."
  - [corpus] Weak evidence - corpus neighbors discuss the intersection but don't provide direct empirical evidence for orthogonality.
- Break condition: If there exists a method that simultaneously optimizes for both fairness and explainability in a way that creates positive correlation.

### Mechanism 3
- Claim: Over-confidence in model predictions indicates reliance on spurious correlations, which can be a source of bias.
- Mechanism: When models make highly confident predictions based on simple feature correlations (rather than true understanding), they're likely relying on biased patterns. Penalizing over-confidence can reduce bias by forcing the model to consider more nuanced features.
- Core assumption: Over-confidence in predictions reflects over-reliance on limited, potentially biased features.
- Evidence anchors:
  - [section] "Over-confident model predictions are considered an indication of bias based on the intuition that all simple feature correlations—leading to high confidence—are spurious."
  - [section] "FAIR-SD where the L2 norm of the classification logits is used as a regularization penalty."
  - [corpus] Weak evidence - corpus doesn't provide specific evidence for this mechanism.
- Break condition: If over-confidence is caused by factors other than reliance on spurious correlations, or if the regularization approach is too aggressive.

## Foundational Learning

- Concept: Spurious correlations in training data
  - Why needed here: Understanding how models can learn to exploit superficial patterns rather than true causal relationships is crucial for understanding why fairness methods fail.
  - Quick check question: If a model learns that all nurses are female in training data, what might it predict for a male nurse in testing?

- Concept: Orthogonal dimensions of model evaluation
  - Why needed here: Recognizing that fairness and explainability are separate metrics that can be optimized independently helps in designing comprehensive evaluation frameworks.
  - Quick check question: Can a model be highly fair but poorly explainable, or vice versa?

- Concept: Post-hoc vs. intrinsic explainability
  - Why needed here: Understanding the difference between explanations extracted after training versus those built into the model architecture is important for interpreting the results.
  - Quick check question: What's the key difference between LRP explanations and rationale extraction frameworks in terms of how they generate explanations?

## Architecture Onboarding

- Component map:
  - RoBERTa-base classifier with cross-entropy loss -> Fairness-promoting methods (5 variants) -> LRP for post-hoc explanation extraction OR Rationale extraction frameworks (3 variants) with built-in explanations -> Probing classifiers for bias detection

- Critical path:
  1. Fine-tune RoBERTa with chosen fairness/explainability method
  2. Extract explanations using LRP (for fairness methods) or use built-in rationales (for REF methods)
  3. Evaluate empirical fairness (group performance disparity)
  4. Evaluate explainability (faithfulness via AOPC, alignment via R@k)
  5. Assess bias through probing classifiers and confidence metrics

- Design tradeoffs:
  - Fairness methods vs. performance: Some fairness methods reduce overall accuracy
  - Intrinsic vs. post-hoc explanations: REF methods sacrifice performance for built-in explanations
  - Group fairness vs. individual fairness: Methods focus on group performance parity
  - Synthetic vs. real data: Synthetic data reveals edge cases but may not reflect real-world complexity

- Failure signatures:
  - High group disparity despite using fairness methods
  - Low faithfulness scores (AOPC) despite high R@k
  - Probing classifier accuracy > 95% for protected attribute
  - Severe performance drops for minority groups

- First 3 experiments:
  1. Run baseline RoBERTa on BIOS dataset, extract LRP explanations, measure group disparity and faithfulness
  2. Apply FAIR-SD method, compare performance and fairness metrics to baseline
  3. Apply REF-BASE method, evaluate trade-off between explainability and empirical fairness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the relationship between fairness and explainability hold across other languages and downstream tasks beyond English text classification?
- Basis in paper: [inferred] The authors acknowledge their analysis is limited to English text classification datasets and suggest extending the analysis to other languages and downstream tasks.
- Why unresolved: The study only examines two English multi-class classification datasets. Without testing on diverse languages and tasks, it's unclear if the observed orthogonality between fairness and explainability is a universal phenomenon.
- What evidence would resolve it: Empirical studies showing consistent or inconsistent relationships between fairness and explainability across multiple languages (e.g., non-European languages) and diverse downstream tasks (e.g., generation, translation, summarization).

### Open Question 2
- Question: How do different definitions of fairness and explainability affect their interplay? Are there specific fairness or explainability metrics that show stronger correlations than others?
- Basis in paper: [explicit] The authors note their analysis is limited to specific definitions of fairness, bias, and explainability for binary attributes. They also use specific metrics like group-wise performance disparity and AOPC.
- Why unresolved: The study uses a limited set of fairness and explainability definitions and metrics. Other definitions (e.g., individual fairness, counterfactual fairness) or metrics might yield different insights into their relationship.
- What evidence would resolve it: Systematic comparisons of the fairness-explainability relationship using a broader range of fairness definitions (e.g., individual fairness, counterfactual fairness) and explainability metrics (e.g., fidelity, stability, robustness).

### Open Question 3
- Question: What are the underlying mechanisms that lead to the observed orthogonality between fairness and explainability? Are there specific model architectures or training strategies that can promote both simultaneously?
- Basis in paper: [inferred] The authors find that improving either fairness or explainability does not improve the other, suggesting underlying mechanisms are at play. They also note that trustworthiness is still an open challenge.
- Why unresolved: The study identifies the orthogonality but does not delve into the underlying causes. Understanding these mechanisms could lead to new approaches for achieving both fairness and explainability.
- What evidence would resolve it: Theoretical analysis or empirical studies identifying the specific model components or training dynamics that contribute to the fairness-explainability trade-off. Additionally, demonstrating new architectures or training strategies that can improve both metrics simultaneously would provide strong evidence.

## Limitations
- Study focuses exclusively on English datasets, limiting generalizability to other languages and cultural contexts.
- Use of synthetic data for control experiments may not fully capture the complexity of real-world bias scenarios.
- Evaluation metrics may not capture all dimensions of fairness and explainability, such as intersectional fairness or more nuanced forms of explainability.

## Confidence
- High confidence: The finding that empirical fairness and explainability are orthogonal is well-supported by the experimental results showing no correlation between fairness metrics and explainability metrics across all tested methods.
- Medium confidence: The claim that bias mitigation algorithms don't always lead to fairer models is supported by the data but may be context-dependent on the specific datasets and methods used.
- Medium confidence: The assertion that post-hoc explainability methods (like LRP) are more reliable than intrinsic methods for detecting bias is supported by the data but requires further validation across different model architectures and domains.

## Next Checks
1. Replicate the study on additional diverse datasets including languages other than English and domains beyond biographies and legal cases to test generalizability.
2. Implement and test the fairness methods on more complex intersectional fairness scenarios to see if the orthogonality finding holds when considering multiple protected attributes simultaneously.
3. Conduct human evaluation studies to validate whether the orthogonality between fairness and explainability identified by the automated metrics aligns with human perceptions of model trustworthiness.