---
ver: rpa2
title: 'VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset'
arxiv_id: '2304.08345'
source_url: https://arxiv.org/abs/2304.08345
tags:
- audio
- vision
- valor
- arxiv
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VALOR, a vision-audio-language pretraining
  model that jointly models relationships among vision, audio, and language in an
  end-to-end manner. The model consists of three separate encoders for single modality
  representations and a decoder for multimodal conditional text generation.
---

# VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset

## Quick Facts
- arXiv ID: 2304.08345
- Source URL: https://arxiv.org/abs/2304.08345
- Reference count: 40
- Key outcome: VALOR achieves state-of-the-art performance on cross-modality benchmarks, outperforming previous methods by 3.8-12.7% on retrieval tasks and 3.8-12.5% on QA tasks.

## Executive Summary
This paper presents VALOR, a vision-audio-language pretraining model that jointly models relationships among vision, audio, and language in an end-to-end manner. The model consists of three separate encoders for single modality representations and a decoder for multimodal conditional text generation. Two pretext tasks, Multimodal Grouping Alignment (MGA) and Multimodal Grouping Captioning (MGC), are designed to pretrain VALOR, establishing fine-grained alignment between three modality groups and learning how to generate text tokens in conditions of vision, audio, or their combination. To promote vision-audio-language pretraining research, the authors construct a large-scale high-quality tri-modality dataset named VALOR-1M, containing 1M audible videos with human annotated audiovisual captions. Extensive experiments show that VALOR can learn strong multimodal correlations and generalize to various downstream tasks, achieving new state-of-the-art performances on series of public cross-modality benchmarks.

## Method Summary
VALOR consists of separate encoders for vision (Video Swin/CLIP), audio (AST), and language (BERT), plus a multimodal BERT decoder with cross-attention. The model is pretrained using two pretext tasks: Multimodal Grouping Alignment (MGA) for cross-modal alignment via contrastive learning, and Multimodal Grouping Captioning (MGC) for conditional text generation using masked language modeling. The training uses a modality grouping strategy with T-V, T-A, and T-AV groups. VALOR-1M, a 1M-video dataset with human-annotated audiovisual captions, serves as the primary pretraining data, supplemented with public vision-language datasets.

## Key Results
- VALOR outperforms previous state-of-the-art methods by 3.8%, 6.2%, 12.7%, 0.6%, 10.4% (R@1) on text-to-video retrieval benchmarks
- VALOR achieves 3.8%, 3.4%, 5.1%, 12.5% (Acc) improvements on open-ended video question answering benchmarks
- VALOR shows 38.9%, 13.0% (R@1) gains on text-to-audio retrieval benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tri-modal pretraining aligns vision, audio, and language in a shared space, improving cross-modal retrieval and generation tasks.
- Mechanism: VALOR uses Multimodal Grouping Alignment (MGA) to project each modality into a common semantic space and compute fine-grained similarity via token-to-token dot product, then maximizes similarity for matched pairs and minimizes for mismatched pairs using contrastive learning.
- Core assumption: Aligning individual tokens across modalities (rather than global pooled features) yields more precise cross-modal associations, especially when audio contains complementary semantic cues.
- Evidence anchors: [abstract] "MGA projects vision, language and audio to the same common space, building vision-language, audio-language and audiovisual-language alignment simultaneously." [section] "Instead of directly aligning global representations... we build fine-grained correlations between every text token and every video frame or audio clip."
- Break condition: If modality-specific embeddings are poorly aligned or modality groups have low semantic overlap, fine-grained alignment will not converge and downstream retrieval accuracy will degrade.

### Mechanism 2
- Claim: Multimodal Grouping Captioning (MGC) enables conditional text generation given any modality or combination, supporting both discriminative and generative tasks.
- Mechanism: MGC applies causal masked language modeling where masked text tokens are reconstructed conditioned on vision, audio, or both via cross-attention layers, allowing autoregressive inference at test time.
- Core assumption: Learning to generate text tokens conditioned on different modality combinations during pretraining transfers to real-world generation tasks (captioning, QA) with consistent modality inputs.
- Evidence anchors: [abstract] "MGC learns how to generate text tokens in conditions of vision, audio or their both." [section] "Causal masked language modeling are used for this task... text, vision and audio features are fused through cross attention layers."
- Break condition: If modality fusion via cross-attention fails to capture complementary semantics (e.g., audio-only input is insufficient for captioning), generation quality will drop sharply on audio-language tasks.

### Mechanism 3
- Claim: Using VALOR-1M, a large-scale high-quality tri-modality dataset, provides explicit vision-audio-language correlations that existing datasets lack, leading to superior performance.
- Mechanism: VALOR-1M contains 1M videos annotated with audiovisual captions that simultaneously describe both visual and audio content, ensuring strong vision-audio-language alignment absent in datasets with ASR or alt-text only.
- Core assumption: High-quality manual audiovisual captions are essential for learning robust tri-modal correlations; weakly aligned data cannot substitute for explicit human annotation.
- Evidence anchors: [abstract] "We construct a large-scale high-quality tri-modality dataset named VALOR-1M... contains 1M audiable videos with human annotated audiovisual captions." [section] "we propose a vision-audio-language correlated dataset VALOR for tri-modality model pretraining... by annotating public audiovisual data."
- Break condition: If manual annotation quality drops (e.g., inconsistent or incomplete captions), the dataset's advantage over weakly aligned datasets will diminish.

## Foundational Learning

- Concept: Contrastive learning for cross-modal alignment.
  - Why needed here: Establishes fine-grained similarity between modality tokens, critical for retrieval tasks.
  - Quick check question: What loss function is used to pull matched modality pairs together and push mismatched pairs apart?

- Concept: Causal masked language modeling for conditional generation.
  - Why needed here: Enables autoregressive text generation given any modality combination, supporting both captioning and QA.
  - Quick check question: How does the model prevent information leakage from future tokens during masked token reconstruction?

- Concept: Cross-attention fusion for multi-modal representation.
  - Why needed here: Allows the decoder to attend to vision and audio features when generating text, essential for multimodal understanding.
  - Quick check question: What is the difference between audio-visual cross-attention and visual-audio cross-attention in the decoder?

## Architecture Onboarding

- Component map: BERT text encoder -> multimodal BERT decoder (with cross-attention) <- Video Swin/CLIP vision encoder, AST audio encoder
- Critical path: MGA loss → cross-modal alignment → MGC loss → conditional generation → downstream task fine-tuning
- Design tradeoffs: Separate encoders allow modality-specific pretraining and tuning, but require careful modality grouping to avoid modality imbalance during pretraining
- Failure signatures: Poor retrieval scores indicate misaligned modality embeddings; low captioning quality indicates weak cross-attention fusion or insufficient modality-conditioned generation capability
- First 3 experiments:
  1. Train VALOR on VALOR-1M with MGA only and evaluate zero-shot text-to-video retrieval to confirm alignment effectiveness
  2. Train with MGC only and evaluate zero-shot audiovisual captioning to confirm conditional generation works
  3. Combine MGA+MGC with modality grouping strategy and compare retrieval/captioning performance against modality-specific baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does VALOR's performance scale with increasing dataset size beyond 33.5M examples, and what are the diminishing returns?
- Basis in paper: [explicit] The paper states that VALORL uses 33.5M examples and achieves state-of-the-art results, but does not explore larger datasets.
- Why unresolved: The paper does not provide data on performance improvements with larger datasets.
- What evidence would resolve it: Experiments comparing VALOR's performance on datasets larger than 33.5M examples, showing performance gains and potential diminishing returns.

### Open Question 2
- Question: How does VALOR's performance compare to other models when using fewer training examples, and what is the minimum dataset size required for effective performance?
- Basis in paper: [inferred] The paper mentions that VALORL outperforms models with much larger parameters and data sizes, but does not explore performance with fewer training examples.
- Why unresolved: The paper does not provide data on VALOR's performance with smaller datasets.
- What evidence would resolve it: Experiments comparing VALOR's performance on datasets with fewer than 33.5M examples, showing the minimum dataset size required for effective performance.

### Open Question 3
- Question: How does VALOR's performance on audiovisual tasks compare to models that use additional modalities such as subtitles or multiple audio tracks?
- Basis in paper: [explicit] The paper states that VALOR outperforms models that use additional modalities, but does not provide a direct comparison with models using multiple audio tracks or subtitles.
- Why unresolved: The paper does not provide a direct comparison with models using additional modalities beyond vision and audio.
- What evidence would resolve it: Experiments comparing VALOR's performance on audiovisual tasks with models that use additional modalities such as subtitles or multiple audio tracks.

## Limitations

- The VALOR-1M dataset quality and scale lack independent verification, with no external audit or detailed sampling methodology provided
- The weak corpus signals (0.0 average citations, only 25 related papers) suggest limited external validation of the proposed approach and dataset
- The multimodal pretraining framework's effectiveness heavily depends on the assumption that human-annotated audiovisual captions provide superior alignment compared to weakly supervised alternatives, but this assumption lacks comparative analysis

## Confidence

- **High Confidence**: The architectural design combining separate modality encoders with a multimodal decoder is technically sound and follows established multimodal pretraining principles. The use of contrastive learning (MGA) and masked language modeling (MGC) for cross-modal alignment has strong theoretical foundations.
- **Medium Confidence**: The claimed performance improvements (3.8-12.7% R@1 gains) are plausible given the large-scale pretraining and comprehensive benchmark coverage, but require independent reproduction to verify. The effectiveness of modality grouping strategy is supported by ablation studies but needs broader validation.
- **Low Confidence**: The specific impact of VALOR-1M's human annotation quality versus weakly aligned datasets remains unproven. The claim that tri-modal pretraining significantly outperforms dual-modality approaches needs more rigorous ablation studies isolating each modality's contribution.

## Next Checks

1. **Dataset Quality Audit**: Conduct a stratified random sample of 100 VALOR-1M captions with expert annotation quality assessment and semantic coverage analysis. Compare against ASR-generated captions from the same videos to quantify the annotation quality advantage claimed in the paper.

2. **Modality Contribution Analysis**: Implement controlled ablation experiments systematically removing one modality (vision-only, audio-only, language-only) during both pretraining and fine-tuning phases. Measure performance degradation patterns across different downstream tasks to identify which modality combinations provide the most critical information.

3. **Cross-Dataset Generalization Test**: Fine-tune VALOR on VALOR-1M pretraining data, then evaluate zero-shot transfer to completely different datasets (e.g., Kinetics for action recognition, AudioSet for sound classification) without any fine-tuning. This would validate the model's ability to learn general tri-modal representations beyond the specific domain of VALOR-1M.