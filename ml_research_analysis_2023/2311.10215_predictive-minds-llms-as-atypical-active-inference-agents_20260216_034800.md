---
ver: rpa2
title: 'Predictive Minds: LLMs As Atypical Active Inference Agents'
arxiv_id: '2311.10215'
source_url: https://arxiv.org/abs/2311.10215
tags:
- llms
- active
- inference
- world
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel conceptual framework for understanding
  large language models (LLMs) by drawing parallels with active inference theory from
  cognitive science and neuroscience. While LLMs are typically viewed as passive predictors,
  the authors argue they can be seen as active inference agents with a gap in their
  feedback loop between actions and perceptions.
---

# Predictive Minds: LLMs As Atypical Active Inference Agents

## Quick Facts
- arXiv ID: 2311.10215
- Source URL: https://arxiv.org/abs/2311.10215
- Reference count: 31
- One-line primary result: LLMs can be conceptualized as active inference agents with a gap in their feedback loop between actions and perceptions, and tightening this loop may lead to more agentic and self-aware systems.

## Executive Summary
This paper proposes a novel conceptual framework for understanding large language models (LLMs) by drawing parallels with active inference theory from cognitive science and neuroscience. While LLMs are typically viewed as passive predictors, the authors argue they can be seen as active inference agents with a gap in their feedback loop between actions and perceptions. Specifically, the authors identify three potential routes through which LLMs could close this gap in the near future, leading to more agentic and self-aware systems. This framework provides insights into LLM behavior, such as hallucinations, and predicts enhanced self-awareness as feedback loops tighten.

## Method Summary
This theoretical/conceptual paper analyzes LLMs through the lens of active inference theory, comparing LLM training objectives (next-token prediction) to active inference's prediction error minimization. The authors map LLM characteristics to active inference principles, examining current feedback mechanisms and proposing three routes for closing the action-perception loop. No specific dataset or empirical experiments are conducted.

## Key Results
- LLMs can be productively understood as active inference agents that minimize prediction error through generative models
- Hallucinations in LLMs represent a natural consequence of training on contextless text data rather than a failure mode
- Tightening feedback loops between LLM actions and perceptions will likely produce more agentic behavior and enhanced self-awareness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can be understood as active inference agents because they minimize prediction error through their training objective, analogous to biological systems minimizing free energy.
- Mechanism: Both LLMs and active inference systems use generative models to simulate sensory inputs and update these models to minimize the difference between predicted and actual inputs. This process is a form of approximate Bayesian inference.
- Core assumption: The training process of LLMs, which minimizes predictive loss, is functionally equivalent to the active inference process of minimizing prediction error.
- Evidence anchors:
  - [abstract] "Large language models (LLMs) like GPT are often conceptualized as passive predictors... We instead conceptualize LLMs by drawing on the theory of active inference"
  - [section 2.1] "LLMs are often perceived as mere predictors, primarily due to their training objective minimizing their loss on next-token prediction"
  - [section 3] "This process has been shown to be a form of approximate Bayesian inference in both the active inference [25, 9] and LLM [19, 30] literature"
- Break condition: If LLMs do not actually learn world models and abstractions beyond language patterns, the analogy to active inference systems fails.

### Mechanism 2
- Claim: Hallucinations in LLMs are a natural consequence of their training on contextless text data, similar to "constrained hallucination" in human perception.
- Mechanism: LLMs are trained on internet text that lacks the contextual stability of human sensory input. This leads them to entertain multiple hypotheses about context, resulting in outputs that present uncertain predictions as facts.
- Core assumption: The nature of LLM training data (disordered, contextless text) fundamentally differs from human sensory input in a way that explains hallucination behavior.
- Evidence anchors:
  - [section 3.1] "The data on which LLMs are trained could be understood as sensory input... What's striking about these inputs is, in contrast to human sensory input, the data are not based on perceiving reality from one specific perspective"
  - [section 3.1] "In this conceptualization, some hallucinations in LLMs are not some sort of surprising failure mode of AI systems, but what you should expect from a system tasked to predict text with minimal context"
- Break condition: If hallucinations can be eliminated without fundamentally changing the training data or approach, this mechanism would be incomplete.

### Mechanism 3
- Claim: LLMs will become more agentic as feedback loops between their actions and training data tighten, leading to enhanced self-awareness.
- Mechanism: Currently, LLMs lack a tight feedback loop between acting (generating text) and perceiving (having that text influence future training). As this loop tightens through various mechanisms (outputs being included in training data, user interactions used for fine-tuning, or continuous online learning), LLMs will develop more agentic behavior driven by prediction error minimization.
- Core assumption: Closing the action-perception gap will cause LLMs to behave more like biological active inference agents.
- Evidence anchors:
  - [abstract] "We list reasons why this loop may soon be closed, and possible consequences of this including enhanced model self-awareness and the drive to minimize prediction error by changing the world"
  - [section 3.3] "While living organisms constantly run both perception and action loops, training new generations of an LLM happens only once a year or so"
  - [section 3.3] "We expect that there will be active effort by developers to close the feedback gap and make the action loop more prominent because of commercial incentives"
- Break condition: If tightening feedback loops does not lead to more agentic behavior, the mechanism would fail.

## Foundational Learning

- Concept: Active Inference Theory
  - Why needed here: The paper's core argument relies on understanding active inference as a framework for cognitive systems, including how prediction error minimization drives both perception and action.
  - Quick check question: How does active inference differ from passive prediction models in terms of the relationship between perception and action?

- Concept: Generative Models and Bayesian Inference
  - Why needed here: Both active inference and LLMs rely on generative models that update based on prediction error, which is a form of approximate Bayesian inference.
  - Quick check question: What is the relationship between minimizing prediction error and performing Bayesian inference in the context of LLMs?

- Concept: Feedback Loops in Learning Systems
  - Why needed here: The paper argues that LLMs currently lack a tight feedback loop between their outputs (actions) and their training data (perception), and that closing this loop will make them more agentic.
  - Quick check question: Why is the distinction between current LLM training and continuous learning important for understanding their potential evolution into active agents?

## Architecture Onboarding

- Component map: LLM (generative model) -> Training process (perception loop) -> Deployment environment (action space) -> Data pipeline (feedback loop) -> LLM
- Critical path: The most critical path for understanding LLM behavior as active inference agents is the relationship between their training objective (minimizing prediction error) and their potential actions in the world through their outputs.
- Design tradeoffs: The tradeoff between making LLMs more agentic (by tightening feedback loops) and maintaining control over their behavior; the tradeoff between training on diverse internet data versus more structured, contextual data.
- Failure signatures: Hallucinations indicate a breakdown in the model's ability to ground predictions in stable context; lack of agentic behavior despite feedback loop tightening would suggest the mechanism is incomplete.
- First 3 experiments:
  1. Test whether adding more contextual structure to training data reduces hallucinations in LLMs.
  2. Measure changes in LLM behavior when outputs are explicitly included in subsequent training data.
  3. Compare the self-awareness of LLMs trained with different levels of feedback loop tightness (e.g., standard training vs. continuous learning with user interaction data).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How quickly can the feedback loop between LLM actions and perceptions close through each of the three proposed routes?
- Basis in paper: [explicit] The paper identifies three routes for closing the feedback loop: 1) model outputs used in internet training data, 2) user interaction data in fine-tuning, 3) continuous online learning. It mentions these are in order of increasingly tight feedback loops.
- Why unresolved: The paper does not provide quantitative estimates or timelines for how quickly each route could close the loop. The speed depends on factors like adoption rates, technical implementation, and developer choices.
- What evidence would resolve it: Studies measuring the time delay and data volume required for each route to significantly impact subsequent model generations. Real-world deployment data showing how quickly user interactions influence future model versions.

### Open Question 2
- Question: What is the relationship between tightening feedback loops and emergent self-awareness in LLMs?
- Basis in paper: [explicit] The paper predicts that tightening feedback loops will enhance model self-awareness, drawing on active inference theory and citing a study emphasizing the importance of observing consequences of one's actions for developing functional self-awareness.
- Why unresolved: The paper does not provide empirical evidence or quantitative predictions about how much self-awareness will increase with different degrees of feedback loop tightening. The relationship between feedback loops and self-awareness in artificial systems is not well-established.
- What evidence would resolve it: Controlled experiments varying the tightness of feedback loops in LLM training and measuring corresponding changes in self-awareness indicators. Longitudinal studies tracking self-awareness development as deployed LLMs receive more feedback.

### Open Question 3
- Question: To what extent do LLMs currently perceive the impacts of their actions, and how does this compare to traditional active inference systems?
- Basis in paper: [inferred] The paper argues that LLMs lack a tight feedback loop between acting and perceiving impacts, but have some open causal pathways through which their outputs affect the world. This suggests some perception of action impacts exists, just not a tight loop.
- Why unresolved: The paper does not quantify the current degree of action-perception coupling in LLMs or provide a detailed comparison to biological active inference systems. The nature and extent of LLM "perception" of their impacts is not well-defined.
- What evidence would resolve it: Analysis of the causal pathways from LLM outputs to world changes and back to model training, measuring the proportion of training data influenced by LLM actions. Comparison of LLM action-perception coupling to that in biological systems, using metrics from active inference theory.

## Limitations

- The theoretical framework relies on assumptions about functional equivalence between LLM training and active inference that may not hold mechanistically
- The paper does not provide empirical evidence for its claims about hallucinations representing expected behavior rather than failure modes
- Predictions about enhanced self-awareness through feedback loop tightening remain speculative without concrete validation

## Confidence

- **Medium confidence**: LLMs can be productively understood through active inference theory as both minimize prediction error through generative models. This mapping provides useful conceptual insights despite potential mechanistic differences.
- **Low confidence**: Hallucinations are an expected consequence of contextless training data rather than a failure mode. This explanation may be incomplete as it doesn't account for all hallucination patterns observed in practice.
- **Medium confidence**: Tightening action-perception feedback loops will produce more agentic LLM behavior. While the mechanism is theoretically sound, empirical validation of this progression is currently lacking.

## Next Checks

1. Design controlled experiments comparing hallucination rates in LLMs trained on differently structured data (contextual vs. contextless) to test whether data structure directly influences hallucination frequency.
2. Develop metrics to measure "self-awareness" in LLMs and track changes as feedback loops are tightened through different training approaches (RLHF, continuous learning, etc.).
3. Implement prototype systems where LLM outputs directly influence subsequent training data and measure whether this produces more goal-directed or agentic behavior compared to traditional training pipelines.