---
ver: rpa2
title: 'AdBooster: Personalized Ad Creative Generation using Stable Diffusion Outpainting'
arxiv_id: '2309.11507'
source_url: https://arxiv.org/abs/2309.11507
tags:
- creative
- user
- diffusion
- generation
- product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the task of Generative Creative Optimization
  (GCO) to align ad creative generation with user interests using generative AI models.
  AdBooster is proposed, a model that fine-tunes Stable Diffusion outpainting for
  personalized ad creatives based on user interest distributions.
---

# AdBooster: Personalized Ad Creative Generation using Stable Diffusion Outpainting

## Quick Facts
- **arXiv ID**: 2309.11507
- **Source URL**: https://arxiv.org/abs/2309.11507
- **Reference count**: 31
- **Primary result**: Personalized ad creatives using Stable Diffusion outpainting achieve up to 27.87% improvement in CLIP similarity scores over default product images

## Executive Summary
AdBooster introduces Generative Creative Optimization (GCO), a novel task for aligning ad creative generation with user interests using generative AI. The system fine-tunes Stable Diffusion outpainting to generate personalized backgrounds for product images based on user interest distributions. An automated data augmentation pipeline extracts product masks and generates captions, enabling scalable fine-tuning without manual labeling. Experiments on simulated data demonstrate that AdBooster generates more relevant creatives than default product images, successfully creating personalized backgrounds for various categories like seasons, events, and sports.

## Method Summary
AdBooster fine-tunes Stable Diffusion for outpainting tasks using an automated data augmentation pipeline that extracts product masks with U2-Net and generates captions with BLIP. The fine-tuned model generates personalized ad backgrounds conditioned on user representations (query + context) without modifying the product region. The system uses CLIP similarity scores to evaluate how well generated creatives align with user interests compared to baseline product images with white or catalog backgrounds.

## Key Results
- GCO conditioned on user representations gives 15% average increase in CLIP similarity across categories versus baselines
- Up to 27.87% improvement in CLIP scores compared to default product images
- Successfully generates personalized backgrounds for diverse categories including seasons, events, and sports

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning Stable Diffusion for outpainting improves ad background realism for fashion products. By fine-tuning on augmented data with larger background regions and product masks, the model learns to generate coherent backgrounds without distorting the product. Core assumption: Fashion product images with clean product masks and background captions are sufficient for fine-tuning.

### Mechanism 2
Personalized ad creatives generated with user context outperform generic product images. By incorporating user interest signals into both fine-tuning and generation, the model produces backgrounds that align with user intent, increasing relevance as measured by CLIP similarity. Core assumption: User interest distributions can be effectively translated into background prompts that enhance ad relevance.

### Mechanism 3
The automated data augmentation pipeline enables effective fine-tuning without manual labeling. U2-Net extracts product masks, BLIP generates captions, and these form the training data for fine-tuning, making the pipeline scalable. Core assumption: Automatic mask extraction and captioning are sufficiently accurate for the fine-tuning task.

## Foundational Learning

- **Concept**: Stable Diffusion architecture and diffusion models
  - Why needed: Understanding how latent diffusion works is essential for modifying the model for outpainting and interpreting fine-tuning results
  - Quick check: What are the key components of Stable Diffusion and how does the denoising process work?

- **Concept**: CLIP similarity as a proxy for user relevance
  - Why needed: The evaluation metric assumes that CLIP similarity between user interest vectors and generated images correlates with ad effectiveness
  - Quick check: How does CLIP encode text and images into a shared space, and why is this suitable for measuring ad relevance?

- **Concept**: Data augmentation techniques for computer vision
  - Why needed: The pipeline relies on automatic mask extraction and captioning, requiring understanding of how these models work and their limitations
  - Quick check: What are the key challenges in automatic background removal and captioning for e-commerce images?

## Architecture Onboarding

- **Component map**: Pinterest STL dataset → U2-Net mask extraction → BLIP captioning → cleaned dataset → Stable Diffusion fine-tuning → User representation conditioning → Image generation → CLIP scoring
- **Critical path**: User representation → prompt generation → image generation → CLIP scoring → selection of best image
- **Design tradeoffs**: Using automatic augmentation vs manual labeling (scalability vs quality control), using z_QC concatenation vs GPT-3.5 generated prompts (simplicity vs prompt quality), fine-tuning entire diffusion model vs just conditioning layers (performance vs computational cost)
- **Failure signatures**: Low CLIP similarity improvements despite training (prompts may not capture user intent well), artifacts around product boundaries (mask extraction or fine-tuning issues), repetitive backgrounds across different users (conditioning mechanism not working)
- **First 3 experiments**:
  1. Run baseline evaluation: generate images with both z_QC and z_P for a small user set, calculate CLIP scores against ground truth, verify uplift over baselines
  2. Ablation study: compare fine-tuned vs base Stable Diffusion on same prompts to quantify fine-tuning impact
  3. Failure case analysis: manually inspect generated images to identify patterns in quality issues (people generation, lighting, deduplication)

## Open Questions the Paper Calls Out

- How can the quality of people generation in AdBooster be improved?
- What is the optimal balance between simple concatenation (z_QC) and generated prompts (z_P) for outpainting?
- How can deduplication in generated images be prevented?
- What is the impact of AdBooster on actual user engagement versus simulated metrics?

## Limitations

- Evaluation relies entirely on simulated user data and CLIP similarity scores as proxies for real-world ad effectiveness
- Model shows limitations in generating realistic people and experiences quality issues with lighting and image duplication
- Tested only on fashion products from a single dataset, limiting generalizability to other product categories

## Confidence

- **Technical approach**: High confidence - fine-tuning methodology and data augmentation pipeline are well-specified
- **Performance claims**: Medium confidence - CLIP improvements are based on synthetic data that may not translate to real user behavior
- **Generalizability**: Low confidence - model was tested only on fashion products from a single dataset

## Next Checks

1. Deploy the model in a controlled advertising campaign with actual users to measure click-through rates and conversion improvements versus baseline creatives

2. Evaluate the fine-tuned model on non-fashion product categories (electronics, home goods, etc.) to assess cross-category performance

3. Conduct blind user studies comparing generated creatives to professional human-designed ads, measuring both relevance and aesthetic quality across diverse product types