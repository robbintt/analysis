---
ver: rpa2
title: Japanese SimCSE Technical Report
arxiv_id: '2310.19349'
source_url: https://arxiv.org/abs/2310.19349
tags:
- japanese
- simcse
- cl-tohoku
- datasets
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report addresses the lack of Japanese sentence embedding models
  by evaluating Japanese SimCSE across 24 pre-trained language models, five supervised
  and four unsupervised datasets, and multiple hyperparameters. Using fine-tuning
  with contrastive learning, models were trained and evaluated on Japanese STS tasks,
  with JSICK and JSTS datasets.
---

# Japanese SimCSE Technical Report

## Quick Facts
- arXiv ID: 2310.19349
- Source URL: https://arxiv.org/abs/2310.19349
- Reference count: 13
- Key outcome: Supervised Japanese SimCSE models achieved up to 81.91% Spearman's correlation on STS tasks, with large models outperforming base models and subword tokenization consistently better than character-level

## Executive Summary
This report addresses the lack of Japanese sentence embedding models by evaluating Japanese SimCSE across 24 pre-trained language models, five supervised and four unsupervised datasets, and multiple hyperparameters. Using fine-tuning with contrastive learning, models were trained and evaluated on Japanese STS tasks, with JSICK and JSTS datasets. The best-performing supervised models achieved Spearman's correlation scores up to 81.91% on STS benchmarks, with large models (e.g., cl-tohoku/bert-large-japanese-v2) generally outperforming base models. Unsupervised models performed comparably to existing supervised baselines. Four models were released as public baselines.

## Method Summary
The study fine-tuned 24 pre-trained Japanese or multilingual BERT-like models using SimCSE contrastive learning with both supervised (using Japanese NLI datasets like JSNLI, JaNLI, NU-NLI) and unsupervised approaches (using Wikipedia, BCCWJ, CC100). Models were evaluated on Japanese STS tasks using JSICK and JSTS datasets, measuring Spearman's rank correlation between cosine similarity scores and human-annotated similarity judgments. Training used AdamW optimizer, BF16 data type, gradient checkpointing, linear warmup, sequence length 64, temperature 0.05, with evaluation every 26 steps using 220 training examples.

## Key Results
- Supervised models achieved up to 81.91% Spearman's correlation on STS benchmarks, outperforming unsupervised approaches (77.61% on JSICK)
- Large Japanese language models consistently outperformed base models in both supervised and unsupervised settings
- Subword-level tokenization consistently outperformed character-level tokenization for Japanese sentence embeddings
- Four Japanese SimCSE models were released as public baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Supervised SimCSE with Japanese NLI datasets produces higher quality sentence embeddings than unsupervised SimCSE for Japanese STS tasks.
- **Mechanism:** Supervised SimCSE uses semantically similar sentences as positive samples and contradictions as hard negatives in contrastive learning, directly aligning embeddings with human semantic judgments captured in NLI datasets.
- **Core assumption:** The semantic relationships in Japanese NLI datasets accurately reflect the semantic similarities humans use when judging STS tasks like JSICK and JSTS.
- **Evidence anchors:** The best-performing supervised models achieved Spearman's correlation scores up to 81.91% on STS benchmarks; JSNLI demonstrated the highest performance; Tohoku University's BERT models exhibited high performance.
- **Break condition:** If Japanese NLI datasets contain annotation artifacts, mistranslations, or cultural biases that don't align with how Japanese speakers evaluate semantic similarity in the target STS datasets.

### Mechanism 2
- **Claim:** Large Japanese language models consistently outperform base models in both supervised and unsupervised SimCSE settings.
- **Mechanism:** Larger models have more parameters and capacity to capture nuanced semantic relationships, leading to better embedding representations.
- **Core assumption:** Model size correlates positively with semantic understanding capacity for Japanese text, and the training data scale matches the model capacity.
- **Evidence anchors:** Large-size models generally outperformed base-size models; Tohoku University's BERT and Waseda University's RoBERTa demonstrated superior performance even in the unsupervised setting.
- **Break condition:** If the larger models overfit to training data or if the training data is insufficient relative to model capacity, causing degraded generalization.

### Mechanism 3
- **Claim:** Subword-level tokenization consistently outperforms character-level tokenization for Japanese sentence embeddings.
- **Mechanism:** Subword tokenization better handles Japanese morphology and compound words, preserving semantic units more effectively than character-level approaches.
- **Core assumption:** Japanese semantic meaning is more accurately captured at the subword level than at the character level for the types of sentences in JSICK and JSTS datasets.
- **Evidence anchors:** Overall, we observed a trend in both supervised and unsupervised settings that subword-level models consistently outperformed character-level models.
- **Break condition:** If the target STS datasets contain many proper nouns, loanwords, or other entities where character-level representations might be more stable, or if the subword vocabulary is poorly constructed for Japanese.

## Foundational Learning

- **Concept:** Contrastive learning fundamentals (positive/negative sampling, temperature scaling)
  - Why needed here: SimCSE relies entirely on contrastive learning objectives, and understanding how temperature affects similarity scaling is critical for hyperparameter tuning
  - Quick check question: What happens to the contrastive loss if temperature is set too high vs too low?

- **Concept:** Japanese linguistic characteristics (morphology, lack of spaces, compound words)
  - Why needed here: Japanese text processing differs significantly from English, affecting tokenization choices and preprocessing requirements for both training and evaluation
  - Quick check question: How does Japanese tokenization differ from English tokenization, and why does this matter for sentence embeddings?

- **Concept:** Semantic Textual Similarity (STS) evaluation methodology
  - Why needed here: Understanding Spearman's rank correlation and cosine similarity metrics is essential for interpreting model performance and comparing against baselines
  - Quick check question: Why is Spearman's correlation preferred over Pearson correlation for STS evaluation?

## Architecture Onboarding

- **Component map:** Pre-trained Japanese/Multilingual BERT-like models → SimCSE fine-tuning (supervised/unsupervised) → Evaluation on JSICK/JSTS → Model selection and release
- **Critical path:** Data preprocessing → Model fine-tuning → Development set evaluation → Test set evaluation → Model release
- **Design tradeoffs:** 
  - Supervised vs unsupervised training data availability and quality
  - Model size vs computational cost and memory requirements
  - Subword vs character tokenization for Japanese text
  - Batch size vs learning rate optimization tradeoffs
- **Failure signatures:** 
  - Poor STS correlation scores despite high training accuracy (overfitting)
  - Large performance variance across random seeds (instability)
  - Degradation when using different Japanese datasets (lack of generalization)
  - Unexpected character-level model superiority (tokenization issues)
- **First 3 experiments:**
  1. Fine-tune cl-tohoku/bert-base-japanese-v3 with JSNLI using default hyperparameters, evaluate on JSICK validation
  2. Compare performance of cl-tohoku/bert-large-japanese-v2 vs base model on same dataset with identical settings
  3. Test different batch sizes (64, 256, 512) with fixed learning rate to identify optimal batch size for Japanese SimCSE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which Japanese sentence embedding model architecture performs best for retrieval tasks versus STS tasks?
- Basis in paper: The paper focuses on STS tasks and notes that performance trends may differ for other tasks like Dense Passage Retrieval, mentioning GLuCOSE and multilingual E5 as alternatives designed for multiple purposes.
- Why unresolved: The evaluation only covers semantic textual similarity tasks, not retrieval tasks, leaving a gap in understanding performance across different downstream applications.
- What evidence would resolve it: Systematic evaluation of Japanese sentence embedding models on retrieval benchmarks like MS MARCO or BEIR would clarify which architectures generalize best beyond STS.

### Open Question 2
- Question: How do character-level versus subword-level Japanese language models compare in performance and efficiency for sentence embeddings?
- Basis in paper: The paper observes that subword-level models consistently outperformed character-level models in their experiments.
- Why unresolved: While the trend is clear, the paper doesn't explore why this performance gap exists or whether it's consistent across different domains and tasks.
- What evidence would resolve it: Comparative analysis of model efficiency, tokenization quality, and performance across diverse Japanese text domains would explain the performance differences.

### Open Question 3
- Question: What is the optimal trade-off between model size and performance for Japanese sentence embeddings in practical applications?
- Basis in paper: The paper notes that large models generally outperformed base models but with a non-significant performance gap.
- Why unresolved: The paper doesn't quantify the cost-benefit trade-off or explore whether smaller models might be preferable in resource-constrained settings.
- What evidence would resolve it: Detailed benchmarking of model performance versus computational requirements across various deployment scenarios would clarify optimal model selection.

## Limitations

- Limited evaluation to only two Japanese STS datasets (JSICK and JSTS) without cross-domain or task generalization testing
- No quantitative analysis of subword vs character-level tokenization performance, only qualitative observations
- Uncertainty about whether NLI datasets used for supervision align with human similarity judgments in target STS datasets

## Confidence

**High Confidence:** Large models outperform base models - well-supported by consistent performance improvements across different model families and training conditions.

**Medium Confidence:** Supervised SimCSE outperforms unsupervised SimCSE - supported by correlation scores but lacks deeper analysis of why this occurs or whether it generalizes to other datasets.

**Low Confidence:** Subword-level tokenization superiority - stated but not empirically validated within the report; no specific performance numbers or controlled experiments provided.

## Next Checks

1. **Dataset Alignment Validation:** Conduct human evaluation of sentence pairs from both NLI datasets (JSNLI, JaNLI, NU-NLI) and STS datasets (JSICK, JSTS) to measure correlation between NLI labels and STS similarity scores, determining if supervised training data aligns with target evaluation task.

2. **Tokenizer Ablation Study:** Systematically compare subword and character-level tokenization using identical models and training procedures with cl-tohoku/bert-base-japanese-v3, training on same JSNLI dataset and hyperparameters, then evaluate on JSICK and JSTS to quantify exact performance difference.

3. **Cross-Domain Generalization Test:** Evaluate released models on additional Japanese STS or semantic similarity datasets not used in original study, such as domain-specific corpora (legal, medical, technical documents) or cross-lingual STS tasks, to reveal whether models generalize beyond general web text used in training.