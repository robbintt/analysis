---
ver: rpa2
title: 'Graph Prompt Learning: A Comprehensive Survey and Beyond'
arxiv_id: '2311.16534'
source_url: https://arxiv.org/abs/2311.16534
tags:
- graph
- prompt
- tasks
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey presents a comprehensive analysis of graph prompt learning
  within Artificial General Intelligence (AGI), addressing key challenges in cross-modality,
  cross-domain, and cross-task applications for graph data. The authors propose a
  unified framework to understand graph prompts, categorizing them into prompt tokens,
  token structures, and insertion patterns.
---

# Graph Prompt Learning: A Comprehensive Survey and Beyond

## Quick Facts
- **arXiv ID**: 2311.16534
- **Source URL**: https://arxiv.org/abs/2311.16534
- **Reference count**: 40
- **Primary result**: Proposes a unified framework for graph prompt learning, categorizing prompts into tokens, structures, and insertion patterns, and introduces ProG library

## Executive Summary
This survey presents a comprehensive analysis of graph prompt learning within Artificial General Intelligence (AGI), addressing key challenges in cross-modality, cross-domain, and cross-task applications for graph data. The authors propose a unified framework to understand graph prompts, categorizing them into prompt tokens, token structures, and insertion patterns. They introduce ProG, a Python library supporting graph prompting, and an accompanying website for research advancement. The survey analyzes over 100 works, offering insights into the interplay between graph prompts and models, and provides a roadmap for future research in this evolving field.

## Method Summary
The survey categorizes graph prompt learning approaches into three main components: prompt tokens (continuous/learnable vectors, discrete/semantic embeddings, random noise), token structures (single node, connected structures, subgraphs), and inserting patterns (prepending, appending, local/global injection). The ProG library provides implementations for these components, supporting various graph datasets and pre-trained GNN models. The methodology emphasizes reformulating downstream tasks into pre-training tasks using prompts, avoiding extensive model tuning while preserving node flexibility.

## Key Results
- Proposes a comprehensive categorization framework for graph prompts (tokens, structures, insertion patterns)
- Introduces ProG library as a unified framework supporting graph prompting research
- Identifies key challenges in cross-modality, cross-domain, and cross-task applications for graph data
- Analyzes over 100 works to provide insights into graph prompt effectiveness and limitations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Graph prompt learning addresses the flexibility-expressiveness trade-off in graph representation learning.
- **Mechanism**: Prompts introduce learnable token parameters into the graph, preserving node flexibility while leveraging the expressiveness of pre-trained graph models.
- **Core assumption**: Pre-trained GNNs inherently possess knowledge of both structural and semantic aspects of graphs.
- **Evidence anchors**:
  - [abstract]: "We reveal that graph prompts and graph models are interconnected on a deeper level...graph prompts offer a promising solution to address the limitations of existing graph representation learning methods, effectively balancing flexibility and expressiveness."
  - [section]: "With the above analysis, we can find that traditional fine-tuning actually seeks to further improve the expressiveness of a new task with the pre-trained graph model and can not take care of node flexibility. Unlike fine-tuning, a graph prompt usually has several tokens with free parameters...By inserting the prompt graph to the original graph, the combined graph has both nodes with constant features and tokens with free parameters."
  - [corpus]: No direct evidence in corpus; claim is novel to this paper.
- **Break condition**: If pre-trained graph models do not capture sufficient structural and semantic knowledge, prompts cannot effectively balance flexibility and expressiveness.

### Mechanism 2
- **Claim**: Graph prompts reformulate downstream tasks into pre-training tasks, avoiding extensive model tuning.
- **Mechanism**: Prompts align downstream tasks with pre-training tasks by modifying input data, enabling knowledge extraction from pre-trained models without retraining.
- **Core assumption**: Pre-training tasks can be designed to be general enough to cover a wide range of downstream tasks.
- **Evidence anchors**:
  - [abstract]: "Prompt learning is the art of designing informative prompts to manipulate input data for the pre-trained foundation models...By reformulating downstream tasks into pre-training tasks, this approach avoids the need for extensive model tuning and efficiently extracts preserved knowledge."
  - [section]: "Instead of adapting pre-trained GNNs to downstream tasks through objective engineering, this paradigm reformulates downstream tasks into those solved during the pre-training phase using a graph prompt."
  - [corpus]: No direct evidence in corpus; claim is novel to this paper.
- **Break condition**: If pre-training tasks are too specific or narrow, prompts cannot effectively reformulate diverse downstream tasks.

### Mechanism 3
- **Claim**: Graph prompts enable cross-modality, cross-domain, and cross-task generalization in AGI.
- **Mechanism**: Prompts act as a bridge between different modalities, domains, and tasks by providing a unified framework for task reformulation and knowledge transfer.
- **Core assumption**: Graph data can be perceived as a general structure encompassing other modalities like text and images.
- **Evidence anchors**:
  - [abstract]: "This survey critically evaluates the current landscape of AGI in handling graph data, highlighting the distinct challenges in cross-modality, cross-domain, and cross-task applications specific to graphs."
  - [section]: "This insight encourages us to explore the transference of successful prompting techniques from text to the graph area for similar concerns."
  - [corpus]: No direct evidence in corpus; claim is novel to this paper.
- **Break condition**: If the assumption that graph data can encompass other modalities is invalid, prompts cannot effectively enable cross-modality generalization.

## Foundational Learning

- **Concept**: Graph representation learning
  - Why needed here: Understanding the evolution from shallow embeddings to deep GNNs is crucial for grasping the limitations that graph prompts aim to address.
  - Quick check question: What are the main differences between shallow embedding methods and deep GNNs in terms of flexibility and expressiveness?

- **Concept**: Pre-training and fine-tuning paradigm
  - Why needed here: Graph prompts build upon the pre-training and fine-tuning paradigm, so understanding this workflow is essential for comprehending how prompts fit into the overall process.
  - Quick check question: How does the pre-training and fine-tuning paradigm improve model initialization and generalization compared to training from scratch?

- **Concept**: Prompt learning in NLP and CV
  - Why needed here: Graph prompts are inspired by successful prompt learning techniques in NLP and CV, so familiarity with these approaches is beneficial for understanding the motivations and design choices behind graph prompts.
  - Quick check question: What are the key differences between prompt learning in NLP/CV and graph prompt learning in terms of prompt design and task reformulation?

## Architecture Onboarding

- **Component map**: Graph data (nodes, edges, features) -> Pre-trained GNN model -> Graph prompt (tokens, token structures, inserting patterns) -> Task-specific answering function -> Downstream task

- **Critical path**:
  1. Pre-train GNN model on large-scale graph data.
  2. Design graph prompt to reformulate downstream task into pre-training task.
  3. Insert graph prompt into original graph data.
  4. Pass combined graph through frozen pre-trained GNN model.
  5. Apply task-specific answering function to generate downstream task results.

- **Design tradeoffs**:
  - Flexibility vs. expressiveness: Balancing the number and complexity of prompt tokens to preserve node flexibility while leveraging the expressiveness of pre-trained models.
  - Prompt tuning vs. task-specific tuning: Choosing between learning a general prompt across tasks or tuning prompts for specific downstream tasks.
  - Hand-crafted vs. learnable answering functions: Deciding between using pre-defined task templates or training task-specific modules to align prompts with downstream tasks.

- **Failure signatures**:
  - Poor performance on downstream tasks: Indicates ineffective prompt design or misalignment between prompts and pre-training tasks.
  - Overfitting to specific tasks or domains: Suggests insufficient generalization of prompts across tasks or domains.
  - High computational cost: Implies inefficient prompt tuning or complex prompt structures that hinder scalability.

- **First 3 experiments**:
  1. Implement a simple graph prompt (e.g., adding a learnable vector to node features) and evaluate its performance on a node classification task using a pre-trained GNN model.
  2. Design a more complex graph prompt (e.g., incorporating task-specific tokens and structure) and compare its performance against the simple prompt on the same task.
  3. Extend the graph prompt to handle multiple tasks (e.g., node classification and link prediction) and evaluate its ability to generalize across tasks using a single pre-trained model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between efficiency, generalization, and flexibility when designing graph prompts?
- Basis in paper: [explicit] The paper discusses the need to balance these factors, mentioning that non-tunable prompts are efficient but inflexible, while tunable prompts offer flexibility but require more resources.
- Why unresolved: The paper highlights the trade-offs but does not provide a definitive answer on how to achieve the optimal balance in different scenarios.
- What evidence would resolve it: Empirical studies comparing different prompt designs across various graph tasks and domains, measuring efficiency (parameter count, training time), generalization (performance on unseen data), and flexibility (adaptability to new tasks).

### Open Question 2
- Question: How can graph prompts be designed to be more intuitive and interpretable, allowing for better understanding of their inner workings?
- Basis in paper: [inferred] The paper mentions that current graph prompts are often represented as learnable tokens or augmented graphs, making them difficult to interpret compared to textual prompts in NLP.
- Why unresolved: The paper acknowledges this limitation but does not propose specific solutions for creating more interpretable graph prompt designs.
- What evidence would resolve it: Development of graph prompt formats that allow for human-readable descriptions or visualizations of the prompt's structure and content, along with studies demonstrating improved understanding and debugging capabilities.

### Open Question 3
- Question: Can large graph models (LGMs) be developed to the scale of large language models (LLMs), enabling more powerful and generalizable graph prompting?
- Basis in paper: [explicit] The paper discusses the potential of LGMs as universal tools for graph tasks, similar to how LLMs are used in NLP, but notes that current graph models are typically smaller and more specialized.
- Why unresolved: The paper presents this as a future direction but does not provide a clear path or timeline for developing LGMs of comparable scale to LLMs.
- What evidence would resolve it: Successful development and demonstration of LGMs with billions of parameters, showing improved performance on a wide range of graph tasks and domains compared to current specialized models.

## Limitations
- The survey relies heavily on categorizing existing works without providing extensive empirical validation of the proposed mechanisms
- The effectiveness of graph prompts depends on the assumption that pre-trained GNNs capture sufficient structural and semantic knowledge
- The survey does not address potential computational overhead from prompt tuning or scalability challenges for large-scale graphs

## Confidence
- **High Confidence**: The categorization framework (prompt tokens, token structures, inserting patterns) is well-defined and supported by existing literature. The basic mechanism of using prompts to reformulate tasks is theoretically sound.
- **Medium Confidence**: The claims about balancing flexibility and expressiveness through prompts are plausible but lack extensive empirical validation across diverse graph datasets and tasks.
- **Low Confidence**: The cross-modality and cross-domain generalization claims are largely speculative, as the survey does not provide concrete evidence or experiments demonstrating these capabilities beyond theoretical discussion.

## Next Checks
1. **Empirical Validation**: Conduct controlled experiments comparing graph prompt learning against traditional fine-tuning and task-specific training across multiple graph datasets (e.g., Cora, CiteSeer, Reddit) to quantify the flexibility-expressiveness trade-off.
2. **Cross-Modality Testing**: Design experiments that explicitly test graph prompt learning's ability to handle multimodal graph data (e.g., graphs with text attributes) and measure performance against unimodal approaches.
3. **Scalability Analysis**: Evaluate the computational overhead and memory requirements of graph prompt tuning on progressively larger graph datasets to assess practical scalability limits.