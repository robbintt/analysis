---
ver: rpa2
title: High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation
arxiv_id: '2304.02621'
source_url: https://arxiv.org/abs/2304.02621
tags:
- segmentation
- which
- pages
- vision
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses weakly-supervised semantic segmentation,\
  \ which aims to generate segmentation masks using only image-level labels. The authors\
  \ propose improving two existing techniques\u2014importance sampling and feature\
  \ similarity loss\u2014by reformulating them based on binomial posteriors instead\
  \ of multinomial posteriors."
---

# High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation

## Quick Facts
- arXiv ID: 2304.02621
- Source URL: https://arxiv.org/abs/2304.02621
- Authors: 
- Reference count: 40
- Primary result: Improves weakly-supervised segmentation by reformulating importance sampling and feature similarity loss using binomial posteriors instead of multinomial posteriors, achieving up to 2.0 points improvement in mIoU and 2.5 points in F-score on PASCAL VOC and MS COCO.

## Executive Summary
This paper addresses weakly-supervised semantic segmentation by improving two existing techniques - importance sampling and feature similarity loss - through reformulation using binomial posteriors instead of multinomial posteriors. The key insight is that class exclusivity assumptions at the pixel level are problematic for weakly-supervised segmentation, particularly when dealing with downsampled class activation maps (CAMs). The proposed method serves as an add-on that can boost the performance of virtually any existing weakly-supervised segmentation approach. Experiments on PASCAL VOC and MS COCO demonstrate consistent improvements across multiple state-of-the-art baselines, with region similarity (mIoU) increasing by up to 2.0 points and contour quality (F-score) by up to 2.5 points.

## Method Summary
The method reformulates importance sampling loss and feature similarity loss using binomial posteriors of multiple independent binary problems, replacing the original multinomial posterior formulation. This allows multiple classes to share the same pixel region, addressing the mutual exclusivity assumption at the pixel level. The approach introduces multi-pixel sampling to simulate global average pooling behavior, sampling multiple pixels per class and averaging their loss contributions. The feature similarity loss is adapted for likelihood modeling where classes are considered independently, preventing predictions between classes from being pushed apart at class borders. This serves as an add-on module that can be plugged into any weakly-supervised semantic segmentation method to improve pseudo-label quality.

## Key Results
- Achieves up to 2.0 points improvement in mIoU on PASCAL VOC and MS COCO datasets
- Improves contour quality (F-score) by up to 2.5 points across multiple baselines
- Consistent performance gains when applied to various state-of-the-art weakly-supervised segmentation methods
- Demonstrates that likelihood modeling generalizes and improves upon the original multinomial posterior formulation

## Why This Works (Mechanism)

### Mechanism 1
Modeling likelihoods instead of multinomial posteriors improves weakly-supervised segmentation performance by removing the mutual exclusivity assumption at the pixel level. The authors reformulate importance sampling and feature similarity losses using binomial posteriors (independent binary problems) instead of multinomial posteriors. This allows multiple classes to share the same pixel region, which is more realistic for downsampled CAMs.

Core assumption: Class exclusivity assumptions at the pixel level are problematic when multiple classes can occupy the same pixel region, especially in downsampled CAMs.

Break condition: If the downsampling scale factor is 1:1 (no downsampling), or if classes are truly mutually exclusive at the pixel level, the multinomial posterior assumption may be valid.

### Mechanism 2
Multi-pixel sampling in importance sampling loss improves segmentation by better simulating global average pooling behavior. Instead of sampling one pixel per class (analogous to global max pooling), the authors sample multiple pixels and average their loss contributions. This simulates global average pooling which is more powerful as it allows backpropagation over a larger region.

Core assumption: Global average pooling outperforms global max pooling for localization in weakly-supervised segmentation.

Break condition: If the dataset contains only single-object images where max pooling would suffice, or if computational cost becomes prohibitive with many samples.

### Mechanism 3
Feature similarity loss with likelihood modeling improves contour quality by independently handling class contours without enforcing mutual exclusivity. The feature similarity loss is adapted for likelihood modeling where classes are considered independently. This prevents the predictions between classes from being pushed apart at class borders, allowing more accurate contour delineation.

Core assumption: Object contours almost exclusively align with color edges in images, and this heuristic should be applied independently per class rather than enforcing mutual exclusivity.

Break condition: If objects don't have clear color edges separating them, or if the heuristic that contours align with color edges doesn't hold for the specific dataset or domain.

## Foundational Learning

- Concept: Global Average Pooling (GAP) vs Global Max Pooling (GMP)
  - Why needed here: The paper builds on the finding that GAP outperforms GMP for localization in weakly-supervised segmentation, which motivates the importance sampling approach that simulates GAP behavior.
  - Quick check question: Why does GAP typically outperform GMP for object localization in weakly-supervised settings?

- Concept: Class Activation Maps (CAMs)
  - Why needed here: CAMs are the core mechanism for generating pseudo-labels from image-level classification labels in weakly-supervised segmentation, and the paper improves how CAMs are used and interpreted.
  - Quick check question: How are CAMs typically normalized and thresholded to generate pseudo-labels for weakly-supervised segmentation?

- Concept: Mutual exclusivity assumption in classification
  - Why needed here: The paper explicitly challenges the assumption that classes are mutually exclusive at the pixel level, which is fundamental to understanding why their likelihood-based approach works better.
  - Quick check question: What is the mutual exclusivity assumption in multinomial classification, and why might it be problematic for weakly-supervised segmentation?

## Architecture Onboarding

- Component map: Classification network → CAM generation → Importance sampling + Feature similarity loss → Pseudo-label generation → Segmentation network training
- Critical path: Classification network → CAM generation → Importance sampling + Feature similarity loss → Pseudo-label generation → Segmentation network training
- Design tradeoffs: The paper trades computational cost (multiple pixel sampling) for better localization accuracy, and relaxes strict mutual exclusivity assumptions for more realistic multi-class pixel occupancy modeling.
- Failure signatures: If the dataset has very distinct object boundaries with clear color edges, the mutual exclusivity assumption might not hurt performance much. If objects overlap significantly or share textures, the likelihood approach might not provide as much benefit.
- First 3 experiments:
  1. Implement basic SEAM baseline and verify CAM generation works correctly
  2. Add importance sampling loss with single pixel sampling and compare to baseline
  3. Extend to multi-pixel sampling and measure improvement in contour quality specifically

## Open Questions the Paper Calls Out

### Open Question 1
How does the binomial posterior model compare to likelihood modeling in terms of performance and computational efficiency? The authors propose modeling the likelihood as opposed to the multinomial posterior, arguing that the mutual exclusivity assumption is not suitable for weakly-supervised segmentation. They also claim that their method can boost the performance of any previous WSSS method. This remains unresolved as the authors do not provide a direct comparison between the binomial posterior and likelihood modeling in terms of performance or computational efficiency.

### Open Question 2
What is the optimal number of sampled pixels for ISL in terms of balancing performance and computational cost? The authors propose sampling multiple pixels per class for ISL, and they perform an ablation study on the number of sampled pixels. They observe a boost in performance when increasing the number of samples up to 10, but little to no improvement beyond that. This remains unresolved as the authors fix the number of samples to 10 but do not provide a comprehensive analysis of the trade-off between performance and computational cost.

### Open Question 3
How does the proposed method perform on other weakly-supervised segmentation tasks, such as object detection or instance segmentation? The authors evaluate their method on weakly-supervised semantic segmentation using image-level labels. They claim that their method can be applied to virtually any WSSS method, but they do not provide any evidence for its applicability to other weakly-supervised segmentation tasks. This remains unresolved as the authors do not explore the applicability of their method to other weakly-supervised segmentation tasks.

## Limitations
- The computational overhead of multi-pixel sampling is not quantified, which could be significant for larger datasets or higher-resolution images
- The assumption that object contours align with color edges may not hold in datasets with complex scenes or overlapping objects sharing similar colors
- The paper doesn't provide sufficient evidence that the contour quality improvements are directly attributable to the independence assumption rather than other implementation factors

## Confidence

**High confidence**: The empirical improvements on PASCAL VOC and MS COCO datasets are well-documented with consistent results across multiple baselines. The mathematical formulation of binomial posteriors is clearly presented and theoretically sound.

**Medium confidence**: The mechanism explanation for why likelihood modeling outperforms multinomial posteriors is reasonable but could benefit from more ablation studies. The claim about multi-pixel sampling simulating GAP behavior is supported by theoretical arguments but lacks direct experimental validation comparing different sampling strategies.

**Low confidence**: The assertion that feature similarity loss improvements are solely due to likelihood modeling rather than other implementation changes is not fully supported. The paper doesn't provide sufficient evidence that the contour quality improvements are directly attributable to the independence assumption rather than other factors.

## Next Checks

1. **Ablation study isolation**: Create controlled experiments that isolate the effect of binomial posterior reformulation by keeping all other hyperparameters constant while switching between multinomial and binomial formulations on the same baseline.

2. **Cross-dataset validation**: Test the method on datasets with different characteristics (e.g., Cityscapes with urban scenes, or ADE20K with complex indoor/outdoor scenes) to verify that the improvements generalize beyond PASCAL VOC and MS COCO.

3. **Computational overhead measurement**: Quantify the exact computational cost of multi-pixel sampling compared to single-pixel sampling, including memory usage and training time, to determine if the performance gains justify the additional computational expense.