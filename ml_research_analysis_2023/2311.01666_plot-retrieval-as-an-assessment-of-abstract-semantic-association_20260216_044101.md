---
ver: rpa2
title: Plot Retrieval as an Assessment of Abstract Semantic Association
arxiv_id: '2311.01666'
source_url: https://arxiv.org/abs/2311.01666
tags:
- retrieval
- plot
- association
- semantic
- abstract
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes PLOTRETRIEVAL, a novel dataset for plot retrieval,
  a task that requires retrieving relevant plots from a book for a given query. Unlike
  existing IR datasets, PLOTRETRIEVAL focuses on abstract semantic association between
  query and plot, which is more challenging for current IR models.
---

# Plot Retrieval as an Assessment of Abstract Semantic Association

## Quick Facts
- arXiv ID: 2311.01666
- Source URL: https://arxiv.org/abs/2311.01666
- Reference count: 22
- Key outcome: PLOTRETRIEVAL dataset shows existing IR models struggle with abstract semantic association in plot retrieval, with humans significantly outperforming models

## Executive Summary
PLOTRETRIEVAL introduces a novel dataset for plot retrieval that challenges traditional information retrieval models by focusing on abstract semantic associations rather than lexical matching. The dataset contains 430K query-plot pairs collected from reader comments about books, requiring models to understand nuanced relationships between queries and plots. Experiments demonstrate that current IR models, including dense retrieval and cross-encoder approaches, perform substantially worse than humans on this task, highlighting the need for improved semantic understanding capabilities in IR systems.

## Method Summary
The PLOTRETRIEVAL dataset is constructed by collecting reader comments from online reading platforms, filtering out those with high word overlap or little practical meaning, and having human annotators extract abstract plot descriptions. The dataset includes weakly supervised training data derived from this process, along with a test set for evaluation. A novel N-RODCG metric is introduced that considers the distance between retrieved plots and ground truth in the original book text, better reflecting actual reading scenarios than traditional IR metrics.

## Key Results
- Existing IR models show significantly lower performance on PLOTRETRIEVAL compared to human performance
- PLOTRETRIEVAL exhibits the lowest word overlap between queries and candidate documents among common IR datasets
- Weakly supervised training data from PLOTRETRIEVAL improves model performance compared to training on existing IR datasets
- N-RODCG metric better captures the plot retrieval task requirements than traditional metrics like MRR and NDCG

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Abstract semantic association is harder for IR models than lexical matching
- Mechanism: Queries describe plots requiring understanding of abstract relationships (e.g., "saving" vs. literal word overlap)
- Core assumption: Humans naturally integrate summaries, speculations, or emotions into plot descriptions
- Evidence anchors: [abstract] shows need for abstract semantic association estimation; [section 4.3] demonstrates PLOTRETRIEVAL has lowest word overlap among IR datasets

### Mechanism 2
- Claim: Weakly supervised data can improve IR model performance on abstract semantic association
- Mechanism: Reader comments filtered for semantic association create challenging training pairs
- Core assumption: Free-form reader comments naturally contain abstract semantic associations
- Evidence anchors: [section 4.2] describes data collection process; [section 5.3] shows positive impact of weakly supervised data

### Mechanism 3
- Claim: N-RODCG is more suitable evaluation metric for plot retrieval
- Mechanism: Considers distance between retrieved plots and ground truth in original book
- Core assumption: Readers prefer plots close to ground truth in original text
- Evidence anchors: [section 3.3] explains N-RODCG motivation; [section 5.3] shows metric performance

## Foundational Learning

- Concept: Word overlap and semantic similarity
  - Why needed here: Understanding difference between lexical matching and semantic understanding is crucial for grasping why Plot Retrieval is challenging
  - Quick check question: If a query and plot have high word overlap, does it guarantee relevance? (Answer: No, as Plot Retrieval queries often have low word overlap but high semantic association)

- Concept: Weakly supervised learning
  - Why needed here: Understanding how weakly labeled data can be used for training
  - Quick check question: What's the difference between weakly and fully supervised learning? (Answer: Weakly supervised uses less precise or complete labels)

- Concept: Evaluation metrics in IR
  - Why needed here: Understanding common metrics and how N-RODCG differs
  - Quick check question: How does N-RODCG differ from NDCG? (Answer: N-RODCG considers distance in original book, NDCG only considers relevance)

## Architecture Onboarding

- Component map: Data collection -> Data filtering -> Human annotation -> Translation -> Corpus construction -> Model training -> Evaluation
- Critical path: Data collection → Data filtering → Human annotation → Translation → Corpus construction → Model training → Evaluation
- Design tradeoffs: Reader comments provide large-scale data but introduce noise; translation ensures consistency but may lose semantics; fixed-size chunks simplify retrieval but may split semantic units
- Failure signatures: Low N-RODCG scores indicate poor abstract semantic capture; high word overlap suggests insufficient filtering; low human agreement indicates subjective task
- First 3 experiments: 1) Evaluate zero-shot performance of existing IR models; 2) Fine-tune models on weakly supervised data vs existing datasets; 3) Analyze performance gap between IR models and humans

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are existing IR models at capturing abstract semantic associations in plot retrieval compared to humans?
- Basis in paper: [explicit] Current IR models struggle with abstract semantic association and are far behind humans
- Why unresolved: Paper provides some comparisons but lacks detailed analysis of specific limitations
- What evidence would resolve it: More in-depth human studies analyzing specific errors and limitations of IR models

### Open Question 2
- Question: Can the weakly supervised training data in PLOTRETRIEVAL be further improved?
- Basis in paper: [explicit] Weakly supervised data proves effective but potential for further improvement unexplored
- Why unresolved: While effectiveness is shown, investigation of improvements to training data is not conducted
- What evidence would resolve it: Experiments exploring variations and improvements to weakly supervised training data

### Open Question 3
- Question: Are there specific types of abstract semantic associations more challenging for IR models?
- Basis in paper: [explicit] Five manifestations identified but specific difficulties not analyzed
- Why unresolved: Paper identifies types but doesn't provide detailed analysis of challenges for each
- What evidence would resolve it: Experiments analyzing performance on different types of semantic associations

## Limitations

- Evaluation relies heavily on human annotation quality, introducing potential subjectivity
- Translation from Chinese to English may result in semantic information loss
- Fixed-size plot chunking (200-300 words) may artificially split semantically coherent units
- Weak supervision depends on assumption that reader comments naturally contain abstract associations

## Confidence

**High Confidence**: PLOTRETRIEVAL requires abstract semantic association rather than lexical matching (well-supported by evidence of lower word overlap and human performance gaps)

**Medium Confidence**: Weakly supervised data improves model performance (supported by experimental results but supervision quality may vary)

**Low Confidence**: N-RODCG is significantly better than traditional IR metrics (requires more extensive validation across different plot structures)

## Next Checks

1. Conduct ablation studies comparing model performance when trained on PLOTRETRIEVAL versus other IR datasets to quantify weak supervision contribution

2. Perform cross-linguistic validation testing models trained on English PLOTRETRIEVAL on Chinese book plots to assess translation impact

3. Implement alternative segmentation strategies for plot chunks and evaluate how different chunk sizes affect both model performance and N-RODCG scores to determine optimal segmentation approaches