---
ver: rpa2
title: Closed Drafting as a Case Study for First-Principle Interpretability, Memory,
  and Generalizability in Deep Reinforcement Learning
arxiv_id: '2310.20654'
source_url: https://arxiv.org/abs/2310.20654
tags:
- memory
- drafting
- closed
- cards
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper establishes first-principle benchmarks for studying
  Deep Reinforcement Learning (DRL) in closed drafting games like Sushi Go Party!
  by examining three key aspects: memory, generalizability, and interpretability.'
---

# Closed Drafting as a Case Study for First-Principle Interpretability, Memory, and Generalizability in Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2310.20654
- Source URL: https://arxiv.org/abs/2310.20654
- Authors: 
- Reference count: 2
- Key outcome: Establishes benchmarks for DRL in closed drafting games by examining memory, generalizability, and interpretability using Sushi Go Party!

## Executive Summary
This paper establishes first-principle benchmarks for studying Deep Reinforcement Learning (DRL) in closed drafting games like Sushi Go Party! by examining three key aspects: memory, generalizability, and interpretability. The authors develop a multi-agent RL environment that supports explicit memory inclusion, train DQN models with and without memory features, and measure memory influence via KL divergence between action distributions under input perturbations. They quantify generalizability across different game configurations using set distance as a similarity metric, and interpret learned strategies by fitting decision rules. Key findings include: DRL agents with explicit memory significantly outperform those without (p-value 1.4×10⁻⁶⁵, 14.08 point difference), but memory features have minimal impact on action selection (MemInfluence metric ~2×10⁻⁴); generalization performance decreases as game configurations become more dissimilar; and the learned decision rules align with human intuition while revealing novel strategic preferences. The paper demonstrates closed drafting as a valuable testbed for analyzing fundamental DRL capabilities.

## Method Summary
The authors establish benchmarks for DRL in closed drafting games by training DQN models on Sushi Go Party! with configurable card sets. They compare models with and without explicit memory features (information about other players' hands), measure memory influence using KL divergence between action distributions under input perturbations, quantify generalization across game configurations using set distance metrics, and extract interpretable decision rules using SkopeRules. The methodology includes self-play training for 50 epochs, systematic evaluation across different card configurations, and statistical analysis of performance differences.

## Key Results
- DRL agents with explicit memory significantly outperform those without (p-value 1.4×10⁻⁶⁵, 14.08 point difference)
- Memory features have minimal impact on action selection (MemInfluence metric ~2×10⁻⁴) despite performance gains
- Generalization performance decreases as game configurations become more dissimilar (measured by set distance)
- Decision rules align with human intuition while revealing novel strategic preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit memory features improve DRL agent performance in closed drafting games by providing direct access to previously observed information.
- Mechanism: The DQN model receives an additional input dimension containing information about other players' hands, allowing it to make more informed decisions based on known card distributions.
- Core assumption: The model cannot effectively infer other players' hands through game mechanics alone, and direct memory access provides a meaningful advantage.
- Evidence anchors:
  - [abstract]: "DQN models with and without memory features" and "memory influence via KL divergence between action distributions under input perturbations"
  - [section]: "We propose two methods targeting distinct aspects of what it means to learn memory... a t-test to compare whether agents given memory as a feature and agents without the memory features have a statistically significant difference in game performance"
  - [corpus]: Weak evidence - no direct citations found for this specific memory mechanism
- Break condition: If the model can successfully learn to track other players' hands through game mechanics without explicit memory features, the performance advantage would diminish.

### Mechanism 2
- Claim: The set distance metric effectively quantifies game configuration similarity for measuring DRL generalization.
- Mechanism: By calculating the symmetric difference between card sets (|(A ∪ B) \ (A ∩ B)|), the metric provides a normalized measure of how different two game configurations are.
- Core assumption: Game configurations with similar card sets will require similar strategies, making the metric a valid proxy for environmental similarity.
- Evidence anchors:
  - [abstract]: "quantify the generalizability of DRL models trained on various sets of cards, establishing a method to benchmark agent performance as a function of environment unfamiliarity"
  - [section]: "To quantify the degree of similarity between game configs, given A, B as the sets of cards in play respectively, we define the set distance defined as: EnvSim(A, B) = |(A ∪ B) \ (A ∩ B)|"
  - [corpus]: No direct citations found for this specific set distance metric
- Break condition: If the metric fails to correlate with actual performance differences when agents are evaluated on different configurations.

### Mechanism 3
- Claim: Decision rule interpretation reveals both human-aligned and novel strategic preferences in trained DRL agents.
- Mechanism: By fitting decision trees to observation-action pairs and extracting rules, the approach maps learned strategies to interpretable conditions that can be compared against human player preferences.
- Core assumption: The extracted rules accurately represent the decision-making process of the DQN model.
- Evidence anchors:
  - [abstract]: "interpret learned strategies by fitting decision rules"
  - [section]: "Learning them involves fitting a tree ensemble and selecting the most precise yet disparate collection of rules from the decision tree branches"
  - [corpus]: No direct citations found for this specific decision rule interpretation method
- Break condition: If the decision rules fail to capture significant aspects of the learned strategy or if the rules become too complex to be interpretable.

## Foundational Learning

- Concept: Partial observability in Markov Decision Processes
  - Why needed here: Closed drafting games are inherently partially observable since players cannot see other players' hands
  - Quick check question: How does partial observability affect the optimal policy in a POMDP compared to a fully observable MDP?

- Concept: KL divergence as a measure of distribution similarity
  - Why needed here: Used to quantify how much the action distribution changes when memory inputs are perturbed
  - Quick check question: What does a KL divergence of 0 between two distributions indicate about their similarity?

- Concept: Set theory and symmetric difference
  - Why needed here: The set distance metric relies on calculating the symmetric difference between card sets
  - Quick check question: What is the symmetric difference between sets {1,2,3} and {2,3,4}?

## Architecture Onboarding

- Component map: Game environment simulator -> DQN model with configurable memory input -> Memory perturbation module -> Set distance calculator -> Decision rule extraction module -> Training pipeline
- Critical path: 1. Initialize game environment with specific card configuration 2. Train DQN agent with/without memory features 3. Evaluate agent performance across different configurations 4. Calculate MemInfluence metric 5. Extract and analyze decision rules
- Design tradeoffs: Memory vs. inference: Explicit memory improves performance but may not reflect true learning capability; Rule complexity vs. interpretability: More complex rules may capture strategy better but be harder to interpret; Generalization metric simplicity vs. accuracy: Set distance is simple but may not capture all aspects of game similarity
- Failure signatures: MemInfluence remains near zero even with explicit memory → model ignores memory input; Performance doesn't decrease with increasing set distance → model doesn't generalize; Decision rules don't match human intuition → model learns fundamentally different strategy
- First 3 experiments: 1. Train identical DQNs with and without memory features on "MyFirstMeal" configuration, compare performance 2. Train agent on "MyFirstMeal", evaluate on progressively different configurations, plot performance vs. set distance 3. Extract decision rules from best-performing agent, compare with human player rankings for common situations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper limit of generalization for DRL agents in closed drafting games before performance degrades significantly?
- Basis in paper: [explicit] The paper quantifies generalization performance across game configurations using set distance, but doesn't explore the threshold where performance drops dramatically
- Why unresolved: The experiments only test configurations with set distances up to 4 cards apart, leaving unexplored the relationship between configuration distance and performance drop-off
- What evidence would resolve it: Systematic testing of DRL performance across increasingly distant game configurations (e.g., 5, 10, 15+ cards different) to identify the point where win rates become comparable to random agents

### Open Question 2
- Question: How do different neural network architectures affect the ability of DRL agents to learn and utilize memory in closed drafting games?
- Basis in paper: [explicit] The paper uses a fixed architecture (four hidden layers of 128 units each) but notes that memory features have minimal impact on action selection despite performance differences
- Why unresolved: The study doesn't explore whether different architectures (CNNs, LSTMs, transformers) could better capture memory-dependent strategic elements
- What evidence would resolve it: Comparative training of multiple architectures with varying memory capacities and analysis of their MemInfluence metrics and game performance

### Open Question 3
- Question: What specific game states or conditions maximize the utility of memory in closed drafting games?
- Basis in paper: [explicit] The paper notes that MemInfluence is highest in the second-to-last round but doesn't systematically identify all conditions where memory matters most
- Why unresolved: The analysis only examines one specific game configuration and focuses on aggregate metrics rather than state-specific memory utility
- What evidence would resolve it: Comprehensive mapping of state-action pairs where memory perturbations cause significant changes in action distributions, across multiple game configurations and round positions

### Open Question 4
- Question: How do human players' strategic preferences differ from learned DRL strategies in closed drafting games, and what explains these differences?
- Basis in paper: [explicit] The paper compares human rankings to DRL preferences and finds "intuitive common rules and intriguing new moves" but doesn't analyze the strategic reasoning behind differences
- Why unresolved: The study only provides surface-level comparison of preferences without investigating the underlying decision-making processes or potential advantages/disadvantages of each approach
- What evidence would resolve it: Detailed analysis of high-stakes game states where human and DRL strategies diverge, including win probability calculations and strategic trade-off evaluations

### Open Question 5
- Question: Can explicit memory features be integrated into DRL architectures in ways that improve both performance and memory utilization?
- Basis in paper: [explicit] The paper finds that explicit memory improves performance but has minimal impact on action selection, suggesting suboptimal memory utilization
- Why unresolved: The study only provides binary conditions (memory present vs absent) without exploring architectural modifications that could better leverage memory information
- What evidence would resolve it: Development and testing of memory-augmented architectures (attention mechanisms, memory networks) that show both improved performance and higher MemInfluence scores

## Limitations

- Limited generalizability claims due to small sample of 16 game configurations tested
- Apparent contradiction between memory feature performance gains and minimal impact on action selection (MemInfluence metric)
- Decision rule interpretability may miss complex strategic patterns not captured by tree-based rules

## Confidence

- High Confidence: The performance improvement from memory features is statistically significant and robust across multiple configurations. The methodology for measuring memory influence is sound.
- Medium Confidence: The generalization results show expected trends but require more extensive validation across diverse configurations. The set distance metric provides reasonable but not definitive evidence.
- Low Confidence: The interpretability claims rely heavily on qualitative assessment of decision rules matching human intuition, which needs more rigorous validation.

## Next Checks

1. **Extended Generalization Study**: Expand the evaluation to 50+ game configurations spanning a wider range of strategic complexities, including configurations with similar set distances but different strategic requirements to validate the metric's effectiveness.

2. **Memory Ablation Analysis**: Conduct a detailed ablation study where memory features are progressively removed or corrupted during inference to precisely measure their contribution to decision-making beyond the current MemInfluence metric.

3. **Human-AI Strategy Comparison**: Conduct controlled experiments where both the trained DQN agents and human players face identical game situations, systematically comparing their decision distributions and win rates to quantify the alignment between learned and human strategies.