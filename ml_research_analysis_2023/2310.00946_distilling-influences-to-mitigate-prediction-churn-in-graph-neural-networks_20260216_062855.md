---
ver: rpa2
title: Distilling Influences to Mitigate Prediction Churn in Graph Neural Networks
arxiv_id: '2310.00946'
source_url: https://arxiv.org/abs/2310.00946
tags:
- churn
- node
- nodes
- prediction
- influence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses prediction churn in graph neural networks (GNNs),
  where models with similar performance exhibit significant disagreement in individual
  predictions. The authors propose a novel metric called Influence Difference (ID)
  to quantify variation in features utilized by nodes across models by comparing their
  influence distributions.
---

# Distilling Influences to Mitigate Prediction Churn in Graph Neural Networks

## Quick Facts
- arXiv ID: 2310.00946
- Source URL: https://arxiv.org/abs/2310.00946
- Reference count: 8
- This work introduces Influence Difference (ID) and DropDistillation (DD) to reduce prediction churn in GNNs through influence matching.

## Executive Summary
This paper addresses prediction churn in graph neural networks, where models with similar performance exhibit significant disagreement in individual predictions. The authors propose a novel metric called Influence Difference (ID) to quantify variation in features utilized by nodes across models by comparing their influence distributions. They also introduce DropDistillation (DD), an efficient approximation that matches outputs for graphs perturbed by edge deletions, to minimize churn in Knowledge Distillation. The results show DD outperforms previous methods regarding prediction stability and overall performance in all considered Knowledge Distillation experiments.

## Method Summary
The authors propose a novel approach to mitigate prediction churn in GNNs by quantifying and aligning the feature influences utilized by different models. They introduce the Influence Difference (ID) metric, which measures the variation in context nodes' contributions to predictions between models using gradient-based influence scores. To minimize churn in Knowledge Distillation, they develop DropDistillation (DD), which approximates influence matching by removing random edges and matching outputs for perturbed graphs. The method combines DD with standard knowledge distillation loss during training, improving both prediction stability and overall performance across multiple benchmark datasets.

## Key Results
- Models utilize different features for predictions even when churn is low
- Stable and unstable nodes have similar influence differences
- DropDistillation improves accuracy by up to 3.4% and reduces churn by up to 4.1%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prediction churn occurs because models learn different subsets of features, not because they disagree on predictions.
- Mechanism: Influence Difference (ID) metric captures the variation in context nodes' contributions to predictions between models, revealing feature-level differences even when churn is low.
- Core assumption: The gradient of a node's representation with respect to input features (influence scores) reflects the importance of neighboring nodes in the prediction.
- Evidence anchors:
  - [abstract] "We propose a novel metric called Influence Difference (ID) to quantify the variation in reasons used by nodes across models by comparing their influence distribution."
  - [section 3.1] "Instead of comparing the output differences for a pair of models, we propose to compare which features each model utilized for its predictions directly."
  - [corpus] Weak correlation between ID and churn, supporting that ID captures deeper differences than churn alone.
- Break condition: If the gradient-based influence scores do not accurately reflect feature importance, the ID metric becomes unreliable.

### Mechanism 2
- Claim: Stable and unstable nodes have similar influence differences between models, indicating both rely on different features.
- Mechanism: The correlation between influence difference and node stability is low, showing that stable nodes also utilize different features across models.
- Core assumption: Nodes with the same label have access to redundant features, allowing stable predictions even when the utilized features change.
- Evidence anchors:
  - [section 3.2] "Contrary to previous investigations, we hypothesize that stable and unstable predictions exhibit similar differences in their utilized features."
  - [section 3.3] "The correlation to the entropy of neighboring labels is much higher and always negative, indicating that less variance in the labels of neighboring nodes helps the stability of a prediction."
  - [corpus] Experiments show low correlation between influence difference and stability, supporting the hypothesis.
- Break condition: If the assumption about label entropy correlating with feature redundancy is incorrect, the hypothesis about stable nodes' features may not hold.

### Mechanism 3
- Claim: DropDistillation (DD) approximates influence matching by removing random edges and matching outputs, improving knowledge transfer.
- Mechanism: Edge deletions perturb the influence distribution, and matching outputs for perturbed graphs teaches the student to mimic the teacher's reasoning.
- Core assumption: The expectation of edge perturbations is zero, allowing the approximation of influence differences through output matching.
- Evidence anchors:
  - [section 4.2] "Our DropDistillation (DD) matches the logit outputs of T and S for the perturbed inputs using the mean squared error."
  - [section 4.3] "Our proof closely follows the proof for random noise by Srinivas et al. We use the Taylor-approximation of T and S around the point (X, A) and use our assumption about zero mean for each entry of Adrop."
  - [corpus] Experimental results show DD improves both prediction stability and overall performance.
- Break condition: If the assumption about zero expectation of edge perturbations is violated, the approximation may not effectively match influence distributions.

## Foundational Learning

- Concept: Influence scores and their role in quantifying feature importance in GNNs.
  - Why needed here: Influence scores are the foundation for the ID metric and DropDistillation, enabling the quantification and matching of feature utilization between models.
  - Quick check question: How are influence scores calculated for a node in a GNN, and what do they represent?

- Concept: Graph Neural Networks (GNNs) and their smoothing properties.
  - Why needed here: Understanding GNNs is essential for grasping why edge deletions are effective perturbations and how influence distributions are formed.
  - Quick check question: What is the over-smoothing problem in GNNs, and how does it affect the propagation of high-frequency signals?

- Concept: Knowledge Distillation (KD) and its goals in model compression and knowledge transfer.
  - Why needed here: DropDistillation is an extension of KD that aims to transfer not just predictions but also the reasoning behind them, improving the student's performance.
  - Quick check question: What is the difference between traditional KD and DropDistillation, and how does the latter improve upon the former?

## Architecture Onboarding

- Component map: Input Graph -> Teacher GAT -> Influence Scores -> ID Metric -> Edge Perturbation -> DropDistillation Loss -> Student GAT -> Improved Predictions
- Critical path:
  1. Calculate influence scores for both teacher and student models
  2. Compute ID to quantify feature-level differences
  3. Apply DD by removing random edges and matching outputs
  4. Fine-tune student using combined DD and standard KD loss
- Design tradeoffs:
  - Edge perturbation probability (p*): Higher values increase approximation accuracy but may disrupt graph structure
  - Number of DD iterations: More iterations improve influence matching but increase computational cost
  - Model capacity: Higher-capacity student models may better match influence but reduce compression benefits
- Failure signatures:
  - Low accuracy improvement despite reduced churn: Indicates ineffective influence matching
  - High variance in results across runs: Suggests instability in the DD approximation
  - Convergence issues during training: May be due to inappropriate edge perturbation or loss balancing
- First 3 experiments:
  1. Verify ID metric by comparing influence scores between two randomly initialized models on a simple graph
  2. Test DD approximation by applying it to a teacher-student pair and measuring ID reduction
  3. Evaluate the impact of DD on knowledge distillation by comparing student performance with and without DD on a benchmark dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Influence Difference (ID) metric perform in scenarios with highly non-linear GNNs or deeper architectures?
- Basis in paper: [inferred] The paper evaluates ID using a 3-layer Graph Attention Network (GAT) but does not explore its behavior in deeper or highly non-linear architectures.
- Why unresolved: The study focuses on shallow networks, leaving the metric's applicability to deeper or more complex GNNs untested.
- What evidence would resolve it: Experimental results showing ID's behavior across varying depths and non-linearities in GNN architectures.

### Open Question 2
- Question: What is the impact of DropDistillation (DD) on the computational efficiency of Knowledge Distillation when scaling to larger graphs?
- Basis in paper: [explicit] The paper mentions that DD is computationally efficient but does not provide a detailed analysis of its scalability or runtime performance on large-scale graphs.
- Why unresolved: The experiments are conducted on relatively small benchmark datasets, and the paper does not address performance on larger, real-world graphs.
- What evidence would resolve it: Empirical studies comparing the runtime and memory usage of DD with other methods on large-scale graph datasets.

### Open Question 3
- Question: Can the Influence Difference (ID) metric be extended to quantify feature importance in heterogeneous or multi-relational graphs?
- Basis in paper: [inferred] The paper focuses on homogeneous graphs and does not explore the metric's applicability to heterogeneous or multi-relational graph structures.
- Why unresolved: The metric is defined for single-type node and edge relationships, leaving its generalization to more complex graph types unexplored.
- What evidence would resolve it: Theoretical extensions of ID to heterogeneous graphs and experimental validation on multi-relational graph datasets.

## Limitations
- The study focuses primarily on GAT architectures, limiting generalizability to other GNN variants
- The assumption of zero-mean edge perturbations may not hold for all graph structures
- The effectiveness of ID metric for deeper or highly non-linear GNNs remains unexplored

## Confidence

- High: Prediction churn quantification and its impact on model reliability
- Medium: Effectiveness of DropDistillation across different datasets and model sizes
- Medium: Correlation between influence differences and prediction stability

## Next Checks

1. Validate ID metric sensitivity by comparing influence scores across different random initializations of the same model architecture
2. Test DropDistillation with varying edge perturbation probabilities to identify optimal values for different graph sizes
3. Evaluate the approach on additional GNN architectures (e.g., GCN, GIN) to assess generalizability beyond GAT models