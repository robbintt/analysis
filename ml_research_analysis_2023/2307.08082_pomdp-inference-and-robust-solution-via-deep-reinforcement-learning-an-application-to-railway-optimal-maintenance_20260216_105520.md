---
ver: rpa2
title: 'POMDP inference and robust solution via deep reinforcement learning: An application
  to railway optimal maintenance'
arxiv_id: '2307.08082'
source_url: https://arxiv.org/abs/2307.08082
tags:
- pomdp
- inference
- solution
- learning
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of performing reinforcement learning
  (RL) in partially observable Markov decision processes (POMDPs) when the model is
  unknown and data is limited. The authors propose a combined framework that first
  infers the transition and observation model parameters via Markov Chain Monte Carlo
  (MCMC) sampling of a hidden Markov model (HMM), conditioned on actions.
---

# POMDP inference and robust solution via deep reinforcement learning: An application to railway optimal maintenance

## Quick Facts
- arXiv ID: 2307.08082
- Source URL: https://arxiv.org/abs/2307.08082
- Authors: 
- Reference count: 13
- Key outcome: This paper tackles the challenge of performing reinforcement learning (RL) in partially observable Markov decision processes (POMDPs) when the model is unknown and data is limited.

## Executive Summary
This paper presents a novel framework for solving POMDPs with model uncertainty using a combination of MCMC-based inference and deep reinforcement learning. The key innovation is the use of HMM inference conditioned on actions to recover full posterior distributions of POMDP model parameters from limited data. These posteriors are then incorporated into the RL solution via domain randomization, enabling the agent to learn policies robust to model uncertainty. The framework is demonstrated on a real-world railway maintenance planning problem, showing significant improvements over baseline methods.

## Method Summary
The proposed framework consists of two main components: (1) MCMC sampling of a hidden Markov model (HMM) conditioned on actions to infer posterior distributions of POMDP transition and observation parameters, and (2) deep RL with domain randomization to solve the POMDP with uncertain parameters. The HMM inference uses Dirichlet distributions for transitions and truncated Student's t processes for observations, with NUTS sampling to explore the joint parameter space. The RL solution employs a belief-input method, where beliefs are computed via Bayes' rule and used as inputs to a standard feed-forward neural network trained with PPO. Domain randomization is implemented by sampling POMDP configurations from the inferred posterior distributions at each episode.

## Key Results
- The proposed approach outperforms model-free RL methods like LSTMs and Transformers on the railway maintenance planning problem.
- Incorporating model uncertainty via domain randomization yields a more robust policy compared to training without domain randomization.
- The framework achieves significant cost savings compared to QMDP and optimal MDP baselines over a 25-year horizon.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MCMC inference of a HMM conditioned on actions recovers full posterior distributions of POMDP model parameters from real-world data.
- Mechanism: The HMM is constructed by modeling transitions via Dirichlet distributions and observations via truncated Student's t processes, conditioned on the observed actions. MCMC sampling (NUTS) explores the joint parameter space to produce posterior distributions that capture model uncertainty.
- Core assumption: The HMM structure accurately approximates the true POMDP generative process, and the chosen priors (Dirichlet for transitions, truncated Student's t for observations) are appropriate for the data characteristics.
- Evidence anchors:
  - [abstract] "First, all transition and observation model parameters are jointly inferred via Markov Chain Monte Carlo sampling of a hidden Markov model, which is conditioned on actions, in order to recover full posterior distributions from the available data."
  - [section] "In the context of discrete hidden states and actions, the transition dynamics are modelled via Dirichlet distributions: T0 ∼ Dirichlet(α0)..."
- Break condition: If the HMM assumptions (e.g., Markov property, conditional independence) are violated by the real system dynamics, the posterior distributions will be biased or uninformative.

### Mechanism 2
- Claim: Domain randomization via sampling from posterior parameter distributions during RL training yields policies robust to model uncertainty.
- Mechanism: At each episode, a POMDP configuration is sampled from the inferred posterior. The RL agent trains on this configuration, and the process repeats. This stochastic gradient ascent over the parameter space implements Bayesian decision-making in practice, optimizing expected utility across all plausible models.
- Core assumption: The posterior distributions adequately represent the true model uncertainty, and the RL algorithm can effectively optimize over the randomized environments.
- Evidence anchors:
  - [abstract] "The POMDP with uncertain parameters is then solved via deep RL techniques with the parameter distributions incorporated into the solution via domain randomization, in order to develop solutions that are robust to model uncertainty."
  - [section] "At every episode, a different POMDP configuration is sampled from the parameter distributions. The RL agent interacts with this POMDP configuration until the end of the episode."
- Break condition: If the posterior is too narrow (under-representing uncertainty) or too broad (including implausible models), the resulting policy may be over-confident or too conservative, respectively.

### Mechanism 3
- Claim: Belief-input RL outperforms model-free LSTM/GTrXL by explicitly computing beliefs via Bayes' rule and using them as inputs to standard RL networks.
- Mechanism: The belief, a sufficient statistic over the complete history of actions and observations, is computed at each step using Bayes' theorem. This belief is then fed into a standard feed-forward NN trained with RL, converting the POMDP into a belief-MDP that is easier for the RL algorithm to solve.
- Core assumption: The belief computation is tractable and accurately represents the agent's information state, and the belief-MDP is not too complex for standard RL methods.
- Evidence anchors:
  - [section] "The belief-MDP is then solved via classical deep model-free RL methods with feed-forward NNs (Andriotis & Papakonstantinou, 2019; Morato et al., 2023)."
  - [section] "For this comparison we set the POMDP parameters to the mean values of the distributions reported in Appendix A, in order to evaluate the methods without model uncertainty."
- Break condition: If the belief space becomes too high-dimensional or the belief update is computationally expensive, this method may not scale or may be outperformed by more sophisticated memory architectures.

## Foundational Learning

- Concept: Hidden Markov Models (HMMs) and their use in parameter inference
  - Why needed here: The POMDP inference relies on modeling the system as an HMM to jointly estimate transition and observation parameters via MCMC.
  - Quick check question: In an HMM, what is the key conditional independence assumption that allows tractable inference?

- Concept: Bayesian decision theory and expected utility maximization
  - Why needed here: Domain randomization implements Bayesian decision-making by optimizing expected utility over the posterior parameter distribution.
  - Quick check question: In Bayesian decision theory, what is the form of the optimal action when maximizing expected utility under parameter uncertainty?

- Concept: Belief-MDP and belief computation via Bayes' theorem
  - Why needed here: The belief-input RL method converts the POMDP into a belief-MDP by explicitly computing beliefs and using them as inputs.
  - Quick check question: What is the recursive formula for updating the belief state in a POMDP after receiving a new observation?

## Architecture Onboarding

- Component map:
  - Data preprocessing: fractal value computation from raw track geometry measurements
  - HMM inference module: Dirichlet and truncated Student's t distributions, MCMC sampling (NUTS)
  - RL training module: belief computation (Bayes' rule), PPO algorithm, domain randomization
  - Evaluation module: simulation of POMDP episodes, comparison of learned policies

- Critical path:
  1. Process raw track geometry data to compute fractal values
  2. Infer HMM parameters via MCMC sampling to obtain posterior distributions
  3. For each RL episode: sample POMDP parameters from posterior, compute beliefs, train policy via PPO
  4. Evaluate learned policies on held-out simulations

- Design tradeoffs:
  - MCMC sampling provides full posterior distributions but is computationally expensive; simpler point estimates (e.g., MAP) are faster but ignore uncertainty.
  - Belief-input RL is more sample-efficient but requires tractable belief computation; model-free LSTM/GTrXL methods are more general but may need more samples.
  - Domain randomization improves robustness but may slow down learning due to increased variance.

- Failure signatures:
  - Poor MCMC convergence (high R-hat, divergences) indicates issues with the model or data.
  - RL training instability (high variance, poor performance) may suggest overly broad domain randomization or suboptimal hyperparameters.
  - Beliefs that do not reflect the true state distribution indicate errors in the belief update equations.

- First 3 experiments:
  1. Verify HMM inference on simulated data with known parameters to check recovery accuracy.
  2. Train RL agent without domain randomization on the inferred mean parameters to establish a baseline.
  3. Implement domain randomization and compare performance and robustness to the baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed POMDP inference framework scale to continuous multi-dimensional states and actions?
- Basis in paper: [inferred] The authors acknowledge that while their methods allow for complex extensions like time-dependent dynamics and hierarchical components, the POMDP inference under continuous multi-dimensional states and actions is still to be investigated.
- Why unresolved: The paper focuses on a specific problem with discrete hidden states and continuous observations. Scaling to continuous multi-dimensional states and actions requires new methodologies and approaches that are not yet developed.
- What evidence would resolve it: Developing and demonstrating the proposed framework on a real-world problem with continuous multi-dimensional states and actions would provide evidence of its scalability and effectiveness.

### Open Question 2
- Question: What are the optimal hyperparameters for the belief-input method with domain randomization in the context of model uncertainty?
- Basis in paper: [explicit] The authors mention that the hyper-parameter tuning was restricted to a minimal grid-search due to the more challenging learning task with model uncertainty, and the results are already satisfying but can likely be further increased via a more thorough hyperparameter optimization.
- Why unresolved: The authors did not perform an exhaustive grid-search due to computational constraints and the complexity of the problem. The optimal hyperparameters may differ from those found in the initial analysis.
- What evidence would resolve it: Conducting a more thorough hyperparameter optimization, potentially using techniques like Bayesian optimization, would provide evidence of the optimal hyperparameters for the belief-input method with domain randomization.

### Open Question 3
- Question: How does the performance of the proposed framework compare to other state-of-the-art RL methods for POMDPs in terms of sample efficiency and computational complexity?
- Basis in paper: [inferred] The authors compare their proposed methods to LSTM and GTrXL, which are model-free RL solutions, and a hybrid belief-input case. However, they do not compare their framework to other state-of-the-art RL methods for POMDPs.
- Why unresolved: The authors focus on demonstrating the effectiveness of their proposed framework on a specific real-world problem. A comprehensive comparison to other state-of-the-art RL methods for POMDPs would require additional experiments and analyses.
- What evidence would resolve it: Conducting experiments comparing the proposed framework to other state-of-the-art RL methods for POMDPs, such as Recurrent Experience Replay (R2D2) or Efficient Recurrent Model-free Reinforcement Learning (ERMRL), would provide evidence of its relative performance in terms of sample efficiency and computational complexity.

## Limitations

- The approach requires sufficient historical data to infer meaningful posterior distributions; in low-data regimes, the MCMC sampling may not converge to informative posteriors.
- The HMM assumption of conditional independence between observations given the hidden state may not fully capture complex real-world dynamics.
- The evaluation is limited to a single real-world application (railway maintenance), and the comparison to other state-of-the-art RL methods for POMDPs is not exhaustive.

## Confidence

- Claim: The proposed framework effectively solves POMDPs with model uncertainty -> Medium-High
- Claim: Domain randomization improves policy robustness -> Medium-High
- Claim: Belief-input RL outperforms model-free methods -> Medium-High

## Next Checks

1. Test the framework on multiple POMDP benchmark problems with varying levels of observability and complexity to assess generalizability.
2. Compare against more recent memory-based RL architectures (e.g., Recurrent Replay Distributed DQN) that are specifically designed for partially observable environments.
3. Conduct an ablation study to quantify the individual contributions of HMM inference, belief-input method, and domain randomization to the overall performance.