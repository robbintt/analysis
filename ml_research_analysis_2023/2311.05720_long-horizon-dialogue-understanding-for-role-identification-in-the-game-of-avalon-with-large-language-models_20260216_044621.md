---
ver: rpa2
title: Long-Horizon Dialogue Understanding for Role Identification in the Game of
  Avalon with Large Language Models
arxiv_id: '2311.05720'
source_url: https://arxiv.org/abs/2311.05720
tags:
- game
- players
- player
- evil
- merlin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a new benchmark for long-horizon dialogue
  understanding in cooperative-competitive multi-party games. The task involves identifying
  hidden player roles in the game of Avalon: The Resistance, where players must deceive
  each other while cooperating to achieve their team''s objective.'
---

# Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models

## Quick Facts
- arXiv ID: 2311.05720
- Source URL: https://arxiv.org/abs/2311.05720
- Reference count: 25
- Primary result: Even state-of-the-art LLMs fall short of human performance in identifying hidden roles in the game of Avalon

## Executive Summary
This paper introduces a new benchmark for long-horizon dialogue understanding in cooperative-competitive multi-party games. The task involves identifying hidden player roles in the game of Avalon: The Resistance, where players must deceive each other while cooperating to achieve their team's objective. The authors collected a high-quality dataset of 20 human games with 2384 utterances, hand-annotated with persuasion strategies, deception labels, and player beliefs. They evaluated state-of-the-art LLMs on this task using different modalities (chat only, state only, or both) and context representations (round-based or full context). The results show that even the best models fall short of human performance, particularly in identifying evil players and Merlin.

## Method Summary
The authors collected 20 human games of Avalon with 2384 utterances, hand-annotated with persuasion strategies, deception labels, and player beliefs. They evaluated LLMs (GPT-4, GPT-3.5-turbo, Llama-2-13B) using different modalities (Chat, State, or both) and context representations (round-based or full context). The models were tested in zero-shot and fine-tuned settings, with fine-tuning performed on 14 training games. Performance was measured using F1 scores for role prediction (Good, Evil, Merlin) and Merlin identification accuracy.

## Key Results
- Combining chat and state information yields the highest F1 scores for role prediction
- GPT-4 outperforms other models in identifying good players, while fine-tuned GPT-3.5 excels at identifying Merlin
- All models fall short of human performance, especially in identifying evil players and Merlin
- Round-based context is more token-efficient than full context but sometimes misses long-term patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can leverage their inherent knowledge to understand deception in long-horizon dialogues if the game state is properly grounded.
- Mechanism: The paper grounds the conversation in structured state representations, converting game state information into linguistic statements that provide context for the players' discussions. This grounding helps the LLM track inconsistencies over time that reveal deceptive behavior.
- Core assumption: LLMs have sufficient encoded knowledge about social deduction games and deception patterns to reason about long-horizon conversations when provided with appropriate context.
- Evidence anchors:
  - [abstract]: "we discuss the multimodal integration of the chat between the players and the game's state that grounds the conversation, providing further insights into the true player identities"
  - [section 4.2]: "the shortcoming of these models is that their interactions are mostly cooperative and they assume that language does not need to be grounded in the world's state"
  - [corpus]: Weak - the paper shows LLMs perform below human level, suggesting limited effectiveness of grounding alone
- Break condition: If the LLM lacks sufficient knowledge about deception patterns or the state representation doesn't capture relevant behavioral cues.

### Mechanism 2
- Claim: Breaking conversations into rounds with carried-over beliefs helps LLMs track player identities across extended dialogue.
- Mechanism: The paper proposes representing conversations as rounds, where each round is a single discussion cycle and beliefs about player identities carry over between rounds. This structure helps the LLM track evolving player identities without being overwhelmed by the full context.
- Core assumption: LLMs can effectively track and update beliefs about player identities when provided with structured round-based context and belief states.
- Evidence anchors:
  - [section 4.1]: "We propose to break up conversations into 'rounds' r representing a single round of discussion in which each player has had the chance to speak at least once"
  - [section 5.3]: "we notice that combining game chat and game state information results in the highest F1 scores when providing the full context"
  - [corpus]: Weak - the paper shows mixed results, with round-based context sometimes outperforming full context
- Break condition: If the round structure doesn't align with natural breakpoints in the conversation or beliefs become too complex to track effectively.

### Mechanism 3
- Claim: Fine-tuning LLMs on high-quality domain-specific data significantly improves their performance in role identification tasks.
- Mechanism: The paper fine-tunes GPT-3.5 and Llama-2 models on the 14 training games from their dataset, showing improved performance especially for Merlin identification.
- Core assumption: The high-quality, hand-annotated data in the Avalon dataset provides sufficient signal for LLMs to learn effective deception detection patterns.
- Evidence anchors:
  - [section 5.3.1]: "we have fine-tuned GPT-3.5-Turbo as well as Llama-2-13b. Training data for fine-tuning was generated from the remaining 14 games"
  - [section 5.3]: "fine-tuning dramatically improves the performance of GPT-3.5 over three epochs of training"
  - [corpus]: Weak - the paper shows limited improvement for Llama-2, suggesting fine-tuning effectiveness may depend on the base model
- Break condition: If the training data quality is insufficient or the base model lacks the capacity to learn the required patterns.

## Foundational Learning

- Concept: Social deduction game mechanics and deception strategies
  - Why needed here: Understanding the game of Avalon's rules and common deception strategies is essential for interpreting player behavior and identifying roles
  - Quick check question: What are the three types of lies evil players can use according to the paper's deception strategy labels?

- Concept: Long-horizon dialogue understanding and belief tracking
  - Why needed here: The task requires reasoning over extended conversations to identify inconsistencies and deceptive patterns that only emerge over time
  - Quick check question: How does the paper propose to help LLMs handle the long-horizon nature of the task?

- Concept: Multimodal integration of text and structured state information
  - Why needed here: The paper explores combining chat dialogue with game state representations to ground the conversation and improve role identification
  - Quick check question: What are the two types of state representations the paper uses to ground the conversation?

## Architecture Onboarding

- Component map: Data collection pipeline (browser-based Avalon game) -> Annotation system (persuasion and deception strategies, player beliefs) -> LLM inference pipeline (prompt generation, structured output validation) -> Evaluation framework (F1 scoring, human baseline comparison)

- Critical path: Data collection → Annotation → Prompt engineering → LLM inference → Evaluation

- Design tradeoffs:
  - Round-based vs full context: Round-based is more token-efficient but may miss long-term patterns; full context captures more information but risks overwhelming the model
  - Chat only vs state only vs combined: Combining modalities generally improves performance but increases complexity; individual modalities may be more interpretable

- Failure signatures:
  - Models consistently confuse evil players with good players
  - Merlin identification accuracy remains significantly below human level
  - Fine-tuning shows limited improvement for some models

- First 3 experiments:
  1. Compare round-based vs full context performance for a single model and modality
  2. Test the impact of including vs excluding game state information
  3. Evaluate the effectiveness of fine-tuning on a subset of the training data

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- State representations are hand-crafted and may not capture all relevant information for deception detection
- Fine-tuning effectiveness varies significantly across models, with unclear reasons for this discrepancy
- Round-based context may miss long-term patterns that emerge over the full conversation

## Confidence
- Multimodal grounding: Medium confidence
- Round-based context: Medium confidence
- Fine-tuning effectiveness: Low confidence (varies significantly by model)

## Next Checks
1. Cross-dataset generalization: Test the best-performing models on games with different player counts or rule variations to assess whether the learned deception patterns generalize beyond the specific Avalon games in the dataset.

2. State representation ablation: Systematically remove individual elements from the state representation (e.g., player locations, mission outcomes) to identify which features are most critical for role identification performance.

3. Temporal pattern analysis: Analyze whether models that perform poorly on Merlin identification are actually tracking Merlin-specific patterns over time, or if they're making decisions based primarily on immediate conversational cues.