---
ver: rpa2
title: 'Prodigy: An Expeditiously Adaptive Parameter-Free Learner'
arxiv_id: '2306.06101'
source_url: https://arxiv.org/abs/2306.06101
tags:
- prodigy
- step
- log2
- adam
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes two modifications to the D-Adaptation method
  for learning-rate-free optimization. The first, Prodigy, improves the worst-case
  convergence rate by a factor of O(sqrt(log(D/d0))) using weighted gradient updates.
---

# Prodigy: An Expeditiously Adaptive Parameter-Free Learner

## Quick Facts
- arXiv ID: 2306.06101
- Source URL: https://arxiv.org/abs/2306.06101
- Reference count: 40
- One-line primary result: Prodigy improves D-Adaptation's convergence rate by O(sqrt(log(D/d0))) using weighted gradient updates and achieves state-of-the-art performance on convex and deep learning tasks without hand-tuned learning rates.

## Executive Summary
This paper introduces Prodigy, a parameter-free optimization method that improves upon D-Adaptation by using weighted gradient updates with λk = d²k. The method achieves an optimal O(GD log(n+1) / (sqrt(n) sqrt(log+(D/d0)))) convergence rate for convex Lipschitz functions, improving the worst-case rate by a factor of O(sqrt(log(D/d0))). The authors also propose a simpler resetting variant that achieves the same rate with periodic restarts. Extensive experiments on 12 convex logistic regression datasets and deep learning tasks demonstrate that Prodigy consistently outperforms D-Adaptation and matches the accuracy of hand-tuned Adam baselines.

## Method Summary
Prodigy modifies D-Adaptation by using weighted gradient updates with λk = d²k instead of uniform weights, which changes the step size calculation to grow more aggressively early in training. This weighted approach improves the convergence rate by O(sqrt(log(D/d0))). The method maintains a distance estimate dk that provides a lower bound on ||x0 - x*||, updated using weighted inner products. A simpler variant resets the dual averaging process when dk increases by more than a factor of 2, achieving the same rate with easier analysis. The authors also extend Prodigy to Adam-like methods using exponential weights λk = β^(-k/2) to approximate Adam's exponential moving average of squared gradients.

## Key Results
- Prodigy improves D-Adaptation's convergence rate by O(sqrt(log(D/d0))) factor
- Achieves optimal O(GD log(n+1) / (sqrt(n) sqrt(log+(D/d0)))) rate for convex Lipschitz functions
- Matches hand-tuned Adam accuracy on deep learning tasks including VGG11, ResNet-50, ViT, LSTM, DLRM, VarNet, RoBERTa, and GPT

## Why This Works (Mechanism)

### Mechanism 1
Weighted gradient updates with λk = d²k improve convergence by growing step sizes more aggressively when dk is small. This accelerates early training while maintaining stability through the weighted gradient norm denominator. The core assumption is that dk remains non-decreasing and bounded by D.

### Mechanism 2
Periodic resetting when dk increases by >2x achieves the same rate with simpler analysis. The reset clears the gradient accumulator and allows larger steps after restarts, preventing denominator growth from slowing convergence. The key assumption is that monotonic dk growth captures the true distance asymptotically.

### Mechanism 3
Exponential weights λk = β^(-k/2) approximate Adam's exponential moving average structure, enabling coordinate-wise adaptive methods. This matches Adam's update rule while maintaining Prodigy's parameter-free properties. The exponential decay must be slow enough (β close to 1) for stability.

## Foundational Learning

- Concept: Lipschitz continuity of gradients
  - Why needed here: Convergence proofs rely on gradients being bounded by G
  - Quick check question: What does it mean for a function to be G-Lipschitz continuous in terms of its gradients?

- Concept: Dual averaging vs. gradient descent
  - Why needed here: Different convergence rates for the two variants
  - Quick check question: How does dual averaging differ from standard gradient descent in terms of iterate updates?

- Concept: Exponential moving averages
  - Why needed here: Adam extension uses exponential weights for coordinate-wise adaptation
  - Quick check question: What is the difference between an exponential moving average and a simple average in terms of weight decay?

## Architecture Onboarding

- Component map: Distance estimator dk -> Gradient accumulator (sk or direct) -> Weight sequence λk -> Step size calculator -> Parameter update (xk+1 = xk - ηkgk or dual averaging) -> (Optional) Reset logic

- Critical path:
  1. Compute gradient gk
  2. Update distance estimate dk+1 using weighted inner products
  3. Compute step size ηk using current dk and λk
  4. Update parameters: xk+1 = xk - ηkgk (or dual averaging variant)
  5. (Optional) Check reset condition and restart if needed

- Design tradeoffs:
  - Weight choice λk: Uniform (simpler) vs. polynomial/exponential (faster adaptation but more complex)
  - Reset frequency: More frequent resets simplify analysis but may lose progress
  - Coordinate-wise vs. full-vector updates: Coordinate-wise adapts per-dimension but increases memory/computation

- Failure signatures:
  - Divergence: Step sizes become too large (dk >> D or λk grows too quickly)
  - Slow convergence: Step sizes become too conservative (dk grows too slowly)
  - Instability: Reset triggers too frequently or gradients become unbounded

- First 3 experiments:
  1. Compare Prodigy vs D-Adaptation on simple convex logistic regression with varying D/d0 ratios
  2. Test reset variant on problem with known D to verify reset triggers appropriately
  3. Implement Adam extension and compare against standard Adam on deep learning benchmark

## Open Questions the Paper Calls Out

### Open Question 1
Can Prodigy be extended to non-convex optimization settings while maintaining theoretical convergence guarantees? The paper focuses exclusively on convex Lipschitz functions and mentions exponential growth bounds may not hold for non-convex functions. This remains unresolved because convexity assumptions are heavily used in the analysis.

### Open Question 2
What is the practical impact of the log(D/d0) factor on convergence speed for real-world problems where D is very large? The paper emphasizes the O(sqrt(log(D/d0))) improvement but doesn't explore scenarios where D/d0 is extremely large. The practical significance for problems with very large condition numbers remains unclear.

### Open Question 3
How does Prodigy's performance compare to other parameter-free methods on federated learning scenarios with heterogeneous devices? The introduction mentions federated learning as motivation but only tests on standard benchmarks. Federated learning presents unique challenges not captured by standard datasets.

## Limitations
- The exponential moving average mechanism for Adam variants lacks detailed theoretical analysis
- Reset mechanism may lose progress on problems with many local minima
- Experiments focus on Adam variants for deep learning, not simpler gradient descent versions

## Confidence

High confidence: The O(√log(D/d0)) improvement for dual averaging variant with weighted gradients - follows from established techniques in literature.

Medium confidence: The Adam extension with exponential moving averages - mechanism described but theoretical analysis limited and experimental validation primarily empirical.

Medium confidence: The resetting variant's convergence rate - analysis is simpler but practical implications of frequent resets on complex landscapes need more exploration.

## Next Checks

1. Verify the √β2 modification in Adam variant: Implement both √β2 and β2 versions on simple deep learning task and compare convergence behavior.

2. Test reset sensitivity: Run Algorithm 3 on non-convex problem with known local minima to quantify progress loss from resets and determine optimal reset thresholds.

3. Cross-validate weight sequences: Compare convergence rates of uniform, polynomial (1/√k), and exponential (β^(-k/2)) weights on same convex problem to validate claimed O(√log(D/d0)) improvement.