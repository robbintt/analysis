---
ver: rpa2
title: Expand BERT Representation with Visual Information via Grounded Language Learning
  with Multimodal Partial Alignment
arxiv_id: '2312.01592'
source_url: https://arxiv.org/abs/2312.01592
tags:
- language
- visual
- learning
- arxiv
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of incorporating visual information
  into BERT representations through grounded language learning. The authors propose
  GroundedBERT, a method that enhances BERT with visually grounded information by
  combining the original BERT contextual representation with a visual grounding module.
---

# Expand BERT Representation with Visual Information via Grounded Language Learning with Multimodal Partial Alignment

## Quick Facts
- **arXiv ID**: 2312.01592
- **Source URL**: https://arxiv.org/abs/2312.01592
- **Reference count**: 40
- **Primary result**: GroundedBERT significantly outperforms baseline language models on GLUE and SQuAD tasks by incorporating visual information through grounded language learning.

## Executive Summary
This paper proposes GroundedBERT, a method that enhances BERT representations by incorporating visual information learned from visual-grounded datasets. The approach combines the original BERT contextual representation with a visual grounding module that captures patch-level visual features. By employing Partial Optimal Transport for multimodal alignment and training with both global image-sentence matching and local alignment objectives, the model achieves state-of-the-art performance on various language understanding tasks, demonstrating the effectiveness of grounded language learning for improving BERT representations.

## Method Summary
GroundedBERT comprises two components: the original BERT encoder for contextual text representation and a visual grounding module that processes patch embeddings from a frozen ViT encoder. The model uses Partial Optimal Transport to align textual and visual modalities, allowing fractional matching between patches and tokens. Training involves two objectives: a global image-sentence matching task and a local alignment loss based on partial OT. The visual grounding embeddings are concatenated with BERT contextual embeddings to create multimodal representations for downstream tasks.

## Key Results
- Significant performance improvements on GLUE benchmark tasks compared to baseline BERT
- State-of-the-art results on SQuAD v1.1 and v2.0 question answering datasets
- Effective incorporation of visual information demonstrates the potential of grounded language learning for language understanding

## Why This Works (Mechanism)

### Mechanism 1
The visual grounding module enriches token embeddings by adding patch-level visual features. Text tokens from BERT are passed through a small MLP (visual grounding module) to produce a "visual grounding embedding." This embedding is concatenated with the original BERT contextual embedding to create a multimodal representation. Core assumption: patch embeddings from ViT capture local visual semantics that complement textual context. Break condition: if patch embeddings do not align well with token-level meaning, concatenation will produce noisy multimodal vectors.

### Mechanism 2
Partial Optimal Transport aligns patch embeddings with token embeddings while allowing partial matches. A cost matrix based on cosine distance between each patch and token embedding is used in a POT formulation, relaxing the constraint that all mass must be matched. The resulting transport plan is used as an alignment loss. Core assumption: in image-caption pairs, not every image region is described by the caption, so full OT is too restrictive. Break condition: if the amount of partial transport is mis-specified, the alignment loss may fail to encourage meaningful cross-modal correspondence.

### Mechanism 3
The global image-sentence matching task encourages the model to learn coarse cross-modal correspondences. The CLS token's multimodal embedding (concatenation of visual-textual token embedding and global image embedding) is passed through a classifier to predict whether the image matches the sentence. BCE loss is applied. Core assumption: a global matching signal can bootstrap learning of local alignments. Break condition: if the global classifier dominates training, local patch-token alignment may be neglected.

## Foundational Learning

- **Concept: Optimal Transport**
  - Why needed here: Provides a principled distance metric for aligning probability distributions over patches and tokens
  - Quick check question: In OT, what do the constraints `T1 = a` and `T^T1 = b` enforce?

- **Concept: Vision Transformer patch embeddings**
  - Why needed here: Supply spatially local visual features that can be matched to token-level semantics
  - Quick check question: What is the shape of the patch embedding matrix output by ViT before projection?

- **Concept: Multimodal concatenation**
  - Why needed here: Merges textual context and visual grounding into a single representation for downstream tasks
  - Quick check question: In the visual-textual embedding `t_i = [h^L_i, g_i]`, what are the dimensionalities of each component before concatenation?

## Architecture Onboarding

- **Component map**: BERT base (frozen) → token embeddings → visual grounding MLP → `g_i` → ViT base (frozen) → patch embeddings → image projection MLP → `v_i` → Concatenation → multimodal embedding `t_i` → POT solver → alignment loss → CLS embedding + global image vector → binary classifier → matching loss
- **Critical path**: Forward pass through BERT and ViT → Compute `g_i` and `v_i` via MLPs → Compute POT alignment loss → Compute global matching loss → Backpropagate through MLPs only
- **Design tradeoffs**: Freezing BERT/ViT speeds training but limits fine-tuning of modality-specific features. Using POT instead of classical OT adds flexibility but requires tuning the mass fraction `s`. Low-dimensional visual grounding embeddings reduce parameters but may lose detail.
- **Failure signatures**: Alignment loss flatlines → POT not learning (check cost matrix scaling, `β` temperature). Global matching loss low but downstream performance poor → over-reliance on global signal. Gradient explosion in MLPs → check learning rate, activation scaling.
- **First 3 experiments**: 1) Train with only the global image-sentence matching loss (no POT) to confirm coarse alignment is learned. 2) Enable POT with a fixed mass fraction `s=0.5` and monitor alignment loss convergence. 3) Vary visual grounding MLP output dimension (64 vs 128) and evaluate impact on GLUE/SQuAD downstream tasks.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of GroundedBERT compare when using different vision encoders (e.g., ResNet vs. ViT) for extracting image features? The paper uses ViT for visual feature extraction but does not explore alternative vision encoders. What evidence would resolve it: Experiments comparing the performance of GroundedBERT using different vision encoders (e.g., ResNet, EfficientNet) for extracting image features.

### Open Question 2
How does the performance of GroundedBERT scale with larger amounts of visual-grounded data during pre-training? The paper mentions the gap in the distribution and quantity of word tokens between visual datasets and language corpora but does not explore the impact of increasing visual-grounded data. What evidence would resolve it: Experiments training GroundedBERT with varying amounts of visual-grounded data and evaluating the performance on downstream tasks.

### Open Question 3
How does the performance of GroundedBERT compare to other multimodal language models that incorporate visual information, such as VisualBERT or VL-BERT? The paper compares GroundedBERT to LXMERT, VisualBERT, VL-BERT, ViLBERT, and Oscar, but only reports the performance of GroundedBERT. What evidence would resolve it: Experiments comparing the performance of GroundedBERT to VisualBERT, VL-BERT, and other multimodal language models on the same downstream tasks.

## Limitations
- The reported GLUE and SQuAD improvements come from a single experimental setup with fixed hyperparameters, limiting generalizability
- The benefits of partial OT over full OT are asserted but not extensively ablated, with the impact of mass fraction `s` remaining untested
- Freezing BERT and ViT encoders could limit adaptability to downstream tasks, yet this trade-off is not explored

## Confidence
- Visual grounding reliably enriches BERT representations: **Medium** (supported by architectural consistency but lacks ablation evidence)
- Partial OT solves alignment better than full OT: **Low** (without comparative experiments)
- Global matching bootstraps local alignment: **Medium** (based on VL pretraining precedent but minimal internal validation)

## Next Checks
1. Conduct an ablation study removing the POT alignment loss while keeping the global matching task to measure its isolated contribution
2. Vary the visual grounding MLP output dimension and the mass fraction `s` in partial OT to assess sensitivity and robustness
3. Fine-tune the full model (BERT + ViT) on a subset of downstream tasks to evaluate the impact of unfreezing encoders versus the current frozen approach