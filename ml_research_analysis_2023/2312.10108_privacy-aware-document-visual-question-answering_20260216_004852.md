---
ver: rpa2
title: Privacy-Aware Document Visual Question Answering
arxiv_id: '2312.10108'
source_url: https://arxiv.org/abs/2312.10108
tags:
- privacy
- learning
- provider
- training
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces the first privacy-aware Document Visual Question\
  \ Answering (DocVQA) dataset and evaluates privacy-preserving methods for training\
  \ DocVQA models. The PFL-DocVQA dataset is based on invoice documents, where the\
  \ provider\u2019s identity is the sensitive information to protect."
---

# Privacy-Aware Document Visual Question Answering

## Quick Facts
- arXiv ID: 2312.10108
- Source URL: https://arxiv.org/abs/2312.10108
- Reference count: 40
- Primary result: Privacy-preserving methods (FL and DP) reduce membership inference attacks on DocVQA models while maintaining performance

## Executive Summary
This paper introduces PFL-DocVQA, the first dataset and framework for privacy-aware Document Visual Question Answering. The dataset, based on invoice documents, is designed to reflect real-world non-i.i.d. data distributions and is suitable for federated learning and differential privacy research. The paper demonstrates that non-private DocVQA models exhibit memorization, leaking sensitive provider information, which can be exploited through membership inference attacks. Privacy-preserving methods, including federated learning and differential privacy, are evaluated and shown to mitigate privacy leakage while maintaining reasonable DocVQA performance.

## Method Summary
The method involves training a multi-modal DocVQA model (VisualT5) on the PFL-DocVQA dataset using centralized, federated, and differential privacy approaches. The dataset is split into BLUE (training/validation/test) and RED (attack evaluation) sets, with provider identities as the sensitive information to protect. Federated learning with per-provider differential privacy is implemented using FedAvg, secure aggregation, and PRV accountant. Membership inference attacks are designed to exploit model memorization, using metrics like accuracy, normalized Levenshtein similarity, loss, and confidence. The effectiveness of privacy-preserving methods is evaluated by comparing DocVQA performance and attack accuracy.

## Key Results
- Non-private DocVQA models exhibit memorization, leaking private provider information.
- Differential privacy training significantly reduces membership inference attack accuracy while maintaining reasonable DocVQA performance.
- Federated learning reflects non-i.i.d. data distributions and contributes to privacy protection by avoiding central data aggregation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differential Privacy (DP) mitigates privacy leakage by adding calibrated noise to model updates, making it hard to infer individual provider information.
- Mechanism: In FL-PROVIDER-DP, per-provider updates are clipped to a maximum norm (C) and Gaussian noise is added proportional to C and the privacy budget (ε, δ). The noise ensures that removing or adding a single provider's data does not significantly change the output distribution.
- Core assumption: Gaussian noise with appropriate scale σ, computed via PRV accountant, ensures (ε, δ)-DP