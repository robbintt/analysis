---
ver: rpa2
title: 'UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation
  of Rerankers'
arxiv_id: '2303.00807'
source_url: https://arxiv.org/abs/2303.00807
tags:
- queries
- synthetic
- domain
- rerankers
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UDAPDR, a method for unsupervised domain
  adaptation of neural information retrieval models using large language models. The
  approach generates synthetic queries with LLMs and uses them to train multiple rerankers,
  which are then distilled into a single efficient retriever.
---

# UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers

## Quick Facts
- arXiv ID: 2303.00807
- Source URL: https://arxiv.org/abs/2303.00807
- Reference count: 19
- This paper introduces UDAPDR, a method for unsupervised domain adaptation of neural information retrieval models using large language models

## Executive Summary
UDAPDR addresses unsupervised domain adaptation for neural retrievers by generating synthetic queries using large language models, training multiple specialized rerankers on these queries, and distilling them into a single efficient ColBERTv2 retriever. The approach significantly improves zero-shot retrieval accuracy across diverse domains while maintaining competitive latency, achieving up to 10-point gains in Success@5 metrics compared to baselines. The method requires only thousands of synthetic queries rather than millions, making it computationally practical.

## Method Summary
The UDAPDR method operates in stages: (1) GPT-3 generates initial high-quality synthetic queries for a small number of passages, (2) corpus-adapted prompts are created using these initial queries and in-domain passages, (3) Flan-T5 XXL generates large-scale synthetic queries for each corpus-adapted prompt, (4) multiple DeBERTaV3-Large rerankers are trained on these synthetic queries, and (5) the rerankers are distilled into a single ColBERTv2 retriever. The approach leverages the quality of expensive LLMs for initial query generation while using cheaper models for scale, and employs multi-teacher distillation to compress diverse domain knowledge into an efficient retriever.

## Key Results
- UDAPDR achieves up to 10-point gains in Success@5 metrics compared to baselines
- The method requires only 2K-100K synthetic queries versus millions used in prior work
- Maintains competitive latency while improving accuracy across LoTTE, NaturalQuestions, and SQuAD domains

## Why This Works (Mechanism)

### Mechanism 1
Using GPT-3 to generate high-quality synthetic queries for a small number of passages, then using Flan-T5 to generate many more queries based on those examples, leads to effective unsupervised domain adaptation. The initial expensive queries serve as high-quality exemplars that guide the cheaper model to generate more diverse yet still relevant queries.

### Mechanism 2
Training multiple rerankers on queries from different corpus-adapted prompts and distilling them into a single ColBERTv2 retriever preserves accuracy while maintaining low latency. Multiple rerankers capture diverse aspects of the domain adaptation, and distillation compresses this knowledge into an efficient retriever architecture.

### Mechanism 3
Using a small number of high-quality synthetic queries (10K-100K) is sufficient for effective domain adaptation, avoiding the need for millions of queries used in prior work. Quality of synthetic queries matters more than quantity when they are well-targeted to the domain and used to train specialized rerankers.

## Foundational Learning

- **Prompt engineering and demonstration-based learning**: The method relies on carefully crafted prompts that show examples of good and bad queries to guide LLM query generation. *Why needed*: The quality of synthetic queries directly impacts downstream retrieval accuracy. *Quick check*: How does the corpus-adapted prompt template differ from a standard instruction prompt, and why is this important for domain adaptation?

- **Knowledge distillation in neural networks**: The rerankers trained on synthetic queries must transfer their knowledge to the final ColBERTv2 retriever. *Why needed*: To preserve the accuracy gains of multiple specialized rerankers while maintaining the efficiency of a single retriever. *Quick check*: What is the key difference between distillation from a single teacher vs. multiple teachers, and how might this affect the final model's performance?

- **Information retrieval evaluation metrics (Success@5, latency)**: The paper evaluates both accuracy (Success@5) and efficiency (latency), requiring understanding of these metrics. *Why needed*: To properly assess the trade-offs between accuracy and efficiency in the proposed method. *Quick check*: Why might Success@5 be a more appropriate metric than Success@1 for evaluating information retrieval systems in this context?

## Architecture Onboarding

- **Component map**: GPT-3 (text-davinci-002) → Initial high-quality query generation → Flan-T5 XXL → Large-scale query generation → DeBERTaV3-Large → Passage reranker training → ColBERTv2 → Final efficient retriever

- **Critical path**: 1) Generate initial queries with GPT-3, 2) Create corpus-adapted prompts, 3) Generate large query set with Flan-T5 XXL, 4) Train multiple rerankers, 5) Distill into ColBERTv2, 6) Evaluate on target domain

- **Design tradeoffs**: Quality vs. quantity (GPT-3 vs. Flan-T5), accuracy vs. latency (multiple rerankers vs. single retriever), complexity vs. performance (multi-reranker approach vs. single reranker)

- **Failure signatures**: Low Success@5 despite high synthetic query count (poor prompt quality or mismatched domain adaptation), high latency despite ColBERTv2 (inefficient distillation or oversized model), large variance across domains (sensitivity to domain characteristics)

- **First 3 experiments**: 1) Replicate Y=5, Z=20k reranker configuration on LoTTE Lifestyle, 2) Test impact of synthetic query count (Z=10k vs Z=100k), 3) Evaluate effect of removing GPT-3 stage

## Open Questions the Paper Calls Out

- How does the quality of synthetic queries generated by GPT-3 versus Flan-T5 XXL impact the effectiveness of the UDAPDR approach? While the paper shows both models can generate effective synthetic queries, it doesn't delve into the specific characteristics of the queries generated by each model that lead to improved retrieval accuracy.

- What is the optimal number of rerankers to use in the UDAPDR approach for maximizing retrieval accuracy while minimizing computational cost? The paper provides insights into the effectiveness of using multiple rerankers but doesn't pinpoint the optimal number for different scenarios.

- How does the choice of base model for ColBERTv2 (e.g., BERT-Base, DeBERTaV3, ELECTRA) affect the performance of the UDAPDR approach? The paper mentions the possibility of using different base models but doesn't explore their impact on effectiveness.

## Limitations

- The method's effectiveness heavily relies on specific prompt templates that are not fully specified, creating a barrier to reproducibility
- Performance on domains with different characteristics (highly technical vs. conversational) remains untested, questioning generalization
- The approach reduces query quantity needs but introduces substantial computational overhead through multiple LLM generations and reranker training

## Confidence

- **High Confidence**: The core claim that LLM-generated synthetic queries can improve zero-shot retrieval accuracy is well-supported by experimental results
- **Medium Confidence**: The assertion that the multi-LLM approach (GPT-3 → Flan-T5) is essential for quality is plausible but not rigorously tested
- **Low Confidence**: The claim that only thousands of synthetic queries are needed (vs. millions in prior work) requires more validation

## Next Checks

1. **Prompt Ablation Study**: Systematically test the contribution of each prompt template by removing or modifying components and measuring impact on synthetic query quality and final retrieval accuracy

2. **Single-Teacher Distillation Comparison**: Compare the multi-reranker distillation approach against a single reranker trained on pooled synthetic queries from all prompts

3. **Cross-Domain Transfer Analysis**: Evaluate the method on a domain with minimal textual overlap to the training domains (e.g., biomedical literature or legal documents) to test the limits of prompt transferability and synthetic query effectiveness