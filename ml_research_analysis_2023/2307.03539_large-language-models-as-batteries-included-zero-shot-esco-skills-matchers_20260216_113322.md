---
ver: rpa2
title: Large Language Models as Batteries-Included Zero-Shot ESCO Skills Matchers
arxiv_id: '2307.03539'
source_url: https://arxiv.org/abs/2307.03539
tags:
- skills
- skill
- data
- language
- potential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using large language models (LLMs) for zero-shot
  skills extraction from job postings against the ESCO taxonomy. The authors generate
  synthetic training data for all 13,890 ESCO skills using GPT-3.5 and train a classifier
  and similarity retriever to identify potential skills.
---

# Large Language Models as Batteries-Included Zero-Shot ESCO Skills Matchers

## Quick Facts
- arXiv ID: 2307.03539
- Source URL: https://arxiv.org/abs/2307.03539
- Reference count: 40
- Using synthetic data achieves RP@10 score 10 points higher than previous methods

## Executive Summary
This paper proposes using large language models (LLMs) for zero-shot skills extraction from job postings against the ESCO taxonomy. The authors generate synthetic training data for all 13,890 ESCO skills using GPT-3.5 and train a classifier and similarity retriever to identify potential skills. A second LLM (GPT-4) then re-ranks these candidates. The approach achieves significant improvements over previous methods, with RP@10 scores increasing by 10+ points using synthetic data and over 22 points with GPT-4 re-ranking.

## Method Summary
The method uses LLMs to generate synthetic training data for all ESCO skills, trains a classifier and similarity retriever on this data, then uses a second LLM (GPT-4) to re-rank candidates. The pipeline starts with GPT-3.5 generating 30-40 example sentences per skill, then trains binary logistic regression classifiers and uses E5-Large-V2 embeddings for similarity retrieval. Finally, GPT-4 re-ranks the top candidates using either natural language or programming-style prompts.

## Key Results
- Synthetic data generation achieves RP@10 score 10 points higher than previous methods
- GPT-4 re-ranking improves RP@10 by over 22 points over previous methods
- Combining classifier and similarity approaches outperforms single methods
- Programming-style prompts improve performance for weaker LLMs like GPT-3.5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large Language Models can reliably generate high-quality synthetic training data that improves skill matching performance.
- Mechanism: LLMs generate diverse example sentences for each skill label based on prompt instructions, capturing various ways skills are mentioned in job postings. This synthetic data trains classifiers and similarity models without requiring manual annotations.
- Core assumption: The training data distribution generated by LLMs is sufficiently similar to real-world job posting distributions to enable effective learning.
- Evidence anchors:
  - [abstract] "We generate synthetic training data for the entirety of ESCO skills and train a classifier to extract skill mentions from job posts."
  - [section] "We prompt GPT-3.5 to generate forty example sentences that could be used in a job posting in order to refer to the skill."
  - [corpus] Weak evidence - no direct mention of synthetic data quality or effectiveness in corpus neighbors.
- Break condition: If generated examples don't capture real-world variations or contain systematic biases, model performance would degrade.

### Mechanism 2
- Claim: Framing skill matching as a mock programming problem improves LLM reranking performance, especially for weaker models.
- Mechanism: Programming-style prompts constrain model outputs to structured formats (Python functions), reducing hallucination and improving instruction-following, particularly for GPT-3.5.
- Core assumption: Programming languages provide more rigid output constraints than natural language, making models more reliable at following instructions.
- Evidence anchors:
  - [abstract] "We also show that Framing the task as mock programming when prompting the LLM can lead to better performance than natural language prompts, especially with weaker LLMs."
  - [section] "Recent work has shown that Large Language Models can often perform better on 'reasoning' tasks when they are approached as programming exercises."
  - [corpus] Weak evidence - corpus neighbors don't mention programming framing or its benefits.
- Break condition: If the programming constraint introduces unnecessary complexity or if models can't effectively reason about ranking within programming syntax.

### Mechanism 3
- Claim: Combining multiple candidate generation approaches (classifier + similarity) improves reranking performance over single approaches.
- Mechanism: Different methods capture complementary information - classifiers identify explicit mentions while similarity captures contextual associations. Combining them provides richer candidate pools for reranking.
- Core assumption: Classifier and similarity approaches have different failure modes and strengths that complement each other.
- Evidence anchors:
  - [section] "We notice that in all cases, the performance obtained by combining potential skills generated by both the classifier and the similarity approaches is noticeably stronger than when using only one method of generating candidates."
  - [abstract] "We also employ a similarity retriever to generate skill candidates which are then re-ranked using a second LLM."
  - [corpus] No direct evidence in corpus neighbors about combining multiple approaches.
- Break condition: If the methods have significant overlap or if combining introduces noise that overwhelms the benefits.

## Foundational Learning

- Concept: Extreme Multi-Label Classification (XMLC)
  - Why needed here: ESCO has 13,890 skills, making this an XMLC problem where each job posting can be associated with multiple skills simultaneously.
  - Quick check question: Why can't we treat this as a standard multi-class classification problem with one correct answer per job posting?

- Concept: Information Retrieval Metrics (RP@10, MRR)
  - Why needed here: The evaluation framework treats skill matching like document retrieval, measuring how well the system ranks relevant skills among candidates.
  - Quick check question: What's the difference between RP@10 and MRR, and why would both be important for evaluating this system?

- Concept: Zero-Shot Learning
  - Why needed here: The system performs skill matching without any manually annotated training data, relying entirely on synthetic data and the LLM's pre-existing knowledge.
  - Quick check question: How does zero-shot learning differ from few-shot learning in the context of this skill matching pipeline?

## Architecture Onboarding

- Component map: Synthetic Data Generator -> Candidate Generators (Classifier + Similarity) -> Reranker (LLM) -> Evaluation Metrics
- Critical path: Data Generation → Candidate Generation → Reranking → Evaluation
- Design tradeoffs:
  - Using synthetic data vs. real annotations (speed/quality tradeoff)
  - Single vs. multiple candidate generation approaches (simplicity vs. performance)
  - Natural language vs. programming-style prompts (flexibility vs. reliability)
- Failure signatures:
  - Poor synthetic data quality → Classifier and similarity models learn incorrect patterns
  - Insufficient candidates → Reranker lacks relevant options to choose from
  - Hallucination in reranking → Incorrect skills appear in final rankings
  - Prompt engineering issues → Models fail to follow instructions properly
- First 3 experiments:
  1. Compare synthetic data generation quality by manually reviewing 100 generated examples for each skill type
  2. Test single candidate generation method (classifier only) vs. combined approach on a small validation set
  3. Compare natural language vs. Python-style prompts for reranking with GPT-3.5 on 50 test samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance change if different LLM architectures (e.g., open-source models like Falcon or Llama) were used for synthetic data generation instead of GPT-3.5?
- Basis in paper: [explicit] The paper mentions that other LLMs like Flan-UL2 could potentially perform the task but were not thoroughly evaluated, and suggests this as a valuable future research area.
- Why unresolved: The authors limited their experiments to GPT models due to computational constraints and API access, leaving the performance of alternative LLM architectures unexplored.
- What evidence would resolve it: Systematic comparison of synthetic data quality and downstream performance using different LLM architectures for the same skills taxonomy.

### Open Question 2
- Question: What is the optimal number of potential skills to provide to the LLM reranker, and how does this number affect performance across different domains or taxonomies?
- Basis in paper: [explicit] The authors note they used up to 60 skills from similarity approaches and all classifier-identified skills, but did not systematically experiment with varying this number.
- Why unresolved: The paper treats the number of candidates as a hyperparameter without optimization, potentially leaving performance gains unrealized.
- What evidence would resolve it: Controlled experiments varying the number of input candidates across multiple domains while measuring both precision and computational efficiency.

### Open Question 3
- Question: How would domain-specific fine-tuning of the LLMs (e.g., using JobBERT or ESCOXLM-R) impact the quality of synthetic training data and reranking performance?
- Basis in paper: [inferred] The limitations section explicitly suggests that domain-specific models could improve performance, as general domain models may not capture job posting nuances.
- Why unresolved: The authors used only general-purpose LLMs without adaptation to the job market domain, potentially missing opportunities for improved performance.
- What evidence would resolve it: Direct comparison of the full pipeline using general versus domain-specific LLMs for both synthetic data generation and reranking tasks.

## Limitations
- Synthetic data quality is not directly evaluated, raising uncertainty about whether generated examples accurately represent real job posting distributions
- Performance comparisons are limited to specific datasets (House and Tech) without broader generalization testing
- Resource requirements for generating synthetic data for 13,890 skills and running multiple LLM components may limit practical deployment

## Confidence
**High confidence** in the core finding that LLM-based synthetic data generation followed by reranking improves ESCO skill matching performance compared to previous methods.

**Medium confidence** in the programming-style prompt benefits, as the improvement is particularly noted for weaker LLMs (GPT-3.5) but not extensively validated across different prompt variations.

**Medium confidence** in the combined candidate generation approach, as the paper shows improved performance but doesn't provide detailed error analysis on when classifier vs. similarity approaches succeed or fail differently.

## Next Checks
1. **Synthetic Data Quality Audit**: Manually review 200 randomly selected synthetic examples across different skill types to assess accuracy, relevance, and diversity. Calculate precision and recall of generated examples against real job posting samples.

2. **Cross-Dataset Generalization Test**: Evaluate the complete pipeline on an additional job posting dataset (different domain or source) not used in the original evaluation to test robustness beyond the House and Tech datasets.

3. **Resource Efficiency Analysis**: Measure the computational cost (API calls, tokens processed, wall-clock time) for generating synthetic data and running the full pipeline on a representative sample, then extrapolate to full-scale deployment costs.