---
ver: rpa2
title: Large Language Models in Analyzing Crash Narratives -- A Comparative Study
  of ChatGPT, BARD and GPT-4
arxiv_id: '2308.13563'
source_url: https://arxiv.org/abs/2308.13563
tags:
- crash
- narratives
- vehicle
- interfaces
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study compared three popular LLM interfaces\u2014ChatGPT,\
  \ BARD, and GPT-4\u2014in extracting and classifying information from 100 crash\
  \ narratives from Iowa and Kansas. Five queries tested their performance: determining\
  \ fault, manner of collision, work-zone occurrence, pedestrian involvement, and\
  \ crash event sequences."
---

# Large Language Models in Analyzing Crash Narratives -- A Comparative Study of ChatGPT, BARD and GPT-4

## Quick Facts
- arXiv ID: 2308.13563
- Source URL: https://arxiv.org/abs/2308.13563
- Reference count: 27
- Key outcome: Three LLM interfaces (ChatGPT, BARD, GPT-4) showed 35%-96% similarity across five crash narrative queries, with network analysis revealing common but model-specific crash patterns.

## Executive Summary
This study compares the performance of three popular large language models—ChatGPT, BARD, and GPT-4—in extracting and classifying information from 100 crash narratives from Iowa and Kansas. Five specific queries tested their capabilities: determining fault, classifying manner of collision, identifying work-zone occurrences, detecting pedestrian involvement, and reconstructing crash event sequences. The study found that while all models could process crash narratives, their responses varied significantly, especially for complex tasks like event sequence reconstruction. Network analysis of crash sequences revealed overlapping but not identical event patterns across models, with high-centrality nodes appearing consistently.

## Method Summary
The study analyzed 100 crash narratives collected from Iowa and Kansas highway agencies between April and May 2023. Researchers queried three LLM interfaces (ChatGPT, BARD, GPT-4) using five standardized questions with structured prompts based on Iowa DOT's crash reporting categories. Responses were tabulated and compared for similarity across models. For complex sequence analysis, network diagrams were generated from event sequences with centrality measures calculated to identify influential crash patterns. The study focused on consistency among models rather than validation against ground truth crash databases.

## Key Results
- Similarity among LLM responses ranged from 35% (manner of collision) to 96% (work-zone detection)
- Network analysis revealed overlapping but distinct event patterns across models
- High-centrality nodes like "Collision with Vehicle in traffic" appeared consistently across all three models
- Complex sequence extraction showed the most variability between models
- The study recommends using multiple LLM models together for more reliable crash data analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs with clear prompt categories improve consistency over free-text responses
- Mechanism: By providing explicit category lists (e.g., Iowa DOT's official crash reporting guide), the LLM's output is constrained to known fields, reducing ambiguity and aligning responses more closely with police-reported databases
- Core assumption: Pre-trained LLMs can interpret structured prompts and select appropriate categories without additional training
- Evidence anchors:
  - [abstract]: "we have incorporated a list of categories in the prompt for the LLM to answer within"
  - [section]: "To circumvent this issue, we have incorporated a list of categories in the prompt for the LLM to answer within"
- Break condition: If the LLM misinterprets the prompt structure or the category list is incomplete, responses will deviate from expected outputs

### Mechanism 2
- Claim: Multi-model comparison reduces reliance on any single LLM's biases
- Mechanism: Using three different LLMs (ChatGPT, BARD, GPT-4) and comparing outputs highlights systematic differences and common patterns, improving overall reliability
- Core assumption: Different LLMs have different strengths and biases that can be balanced by cross-referencing
- Evidence anchors:
  - [abstract]: "This study suggests using multiple models to extract viable information from narratives"
  - [section]: "if the results are similar from multiple interfaces, it will require less human effort for quality control"
- Break condition: If all models share similar training data biases, cross-referencing may not eliminate systematic errors

### Mechanism 3
- Claim: Network analysis reveals influential crash event patterns across models
- Mechanism: Creating network diagrams from sequence of events and analyzing centrality measures (in-degree, out-degree, betweenness) identifies common and influential crash patterns
- Core assumption: Event sequences from narratives can be reliably extracted and represented as network structures
- Evidence anchors:
  - [abstract]: "To compare the responses to question 5, network diagram and centrality measures were analyzed"
  - [section]: "A network of events can help understand the overall crash scenario in a particular location"
- Break condition: If event extraction is inconsistent across models, network comparisons will be misleading

## Foundational Learning

- Concept: Natural Language Processing (NLP) and text classification
  - Why needed here: Understanding how LLMs process and classify text from crash narratives is fundamental to interpreting results
  - Quick check question: What is the difference between word-based and semantic text analysis in crash narrative classification?

- Concept: Network topology and centrality measures
  - Why needed here: Network analysis of crash events requires understanding how to measure node influence and identify patterns
  - Quick check question: How do in-degree, out-degree, and betweenness centrality differ in identifying influential events in crash sequences?

- Concept: Large Language Models (LLMs) and transformer architecture
  - Why needed here: Knowing how LLMs work helps understand their capabilities and limitations in processing crash narratives
  - Quick check question: What is the key difference between GPT-3.5 and GPT-4 in terms of token handling capacity?

## Architecture Onboarding

- Component map:
  Data Ingestion -> Prompt Engineering -> LLM Processing -> Response Collection -> Analysis Layer -> Validation

- Critical path:
  1. Load crash narratives
  2. Generate structured prompts for each query
  3. Send prompts to all three LLMs
  4. Collect and tabulate responses
  5. Calculate similarity metrics
  6. Generate network diagrams for complex queries
  7. Calculate centrality measures
  8. Compare across models

- Design tradeoffs:
  - Single model simplicity vs. multi-model robustness
  - Direct comparison simplicity vs. network analysis depth
  - Real-time API access vs. web interface scraping

- Failure signatures:
  - Inconsistent responses across models for simple binary questions
  - Missing expected categories in manner of collision responses
  - Network diagrams showing disconnected components or illogical sequences

- First 3 experiments:
  1. Test with 10 simplified crash narratives and binary queries to establish baseline similarity
  2. Compare single model vs. multi-model responses on complex sequence extraction
  3. Validate network analysis by manually checking extracted event sequences against original narratives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the reliability of LLM-based crash narrative analysis be validated against human expert analysis?
- Basis in paper: [explicit] The study acknowledges that the LLM responses were not validated against actual crash databases or ground truth values, and suggests that future research could compare LLM outputs with human expert analysis or other NLP models.
- Why unresolved: The study only compares the consistency of responses among different LLMs, but does not validate their accuracy against a gold standard.
- What evidence would resolve it: A study that compares LLM outputs to human expert analysis of the same crash narratives, measuring accuracy, precision, and recall.

### Open Question 2
- Question: What is the impact of model-specific training data and knowledge cutoff dates on LLM performance in crash narrative analysis?
- Basis in paper: [explicit] The study notes that ChatGPT and GPT-4 have knowledge cutoff dates of September 2021, while BARD has a continuous updating structure, and suggests this could influence their performance.
- Why unresolved: The study does not systematically analyze how these differences affect the quality or consistency of crash narrative analysis.
- What evidence would resolve it: A controlled experiment comparing LLM performance on crash narratives using models with different training data and knowledge cutoff dates.

### Open Question 3
- Question: How can LLM interfaces be optimized to improve reproducibility and consistency in crash narrative analysis?
- Basis in paper: [explicit] The study highlights that LLM interfaces are not consistent in providing results, which questions their reproducibility, and suggests caution in using them for safety-related issues.
- Why unresolved: The study does not explore methods to improve the consistency or reproducibility of LLM outputs.
- What evidence would resolve it: A study that tests different prompting strategies, fine-tuning approaches, or ensemble methods to improve the consistency and reproducibility of LLM outputs in crash narrative analysis.

## Limitations
- The study did not validate LLM responses against ground truth crash databases or human expert analysis
- Results may not generalize to all types of crash scenarios or narrative styles due to limited sample size
- Web interface usage rather than direct API access may introduce inconsistencies in response collection

## Confidence
- **High Confidence**: The effectiveness of using structured prompts with predefined categories for improving LLM consistency
- **Medium Confidence**: The comparative performance differences among the three LLM models (ChatGPT, BARD, GPT-4)
- **Medium Confidence**: The value of network analysis in identifying influential crash patterns, though the interpretation requires domain expertise

## Next Checks
1. **Cross-validation with additional narratives**: Test the same prompts on 50 additional crash narratives from different time periods to assess consistency of results
2. **Manual verification study**: Have traffic safety experts manually review a sample of LLM responses to quantify accuracy rates for each query type
3. **API-based replication**: Replicate the study using direct API access to the LLMs rather than web interfaces to eliminate potential interface-related variability