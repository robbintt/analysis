---
ver: rpa2
title: Fairer and More Accurate Tabular Models Through NAS
arxiv_id: '2310.12145'
source_url: https://arxiv.org/abs/2310.12145
tags:
- fairness
- accuracy
- search
- learning
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first application of Neural Architecture
  Search (NAS) and Hyperparameter Optimization (HPO) to address fairness in tabular
  data. The authors propose a multi-objective optimization framework that jointly
  optimizes model architectures (MLP, ResNet, FT-Transformer) and training hyperparameters
  to improve both accuracy and fairness metrics.
---

# Fairer and More Accurate Tabular Models Through NAS

## Quick Facts
- arXiv ID: 2310.12145
- Source URL: https://arxiv.org/abs/2310.12145
- Reference count: 40
- This work presents the first application of Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO) to address fairness in tabular data.

## Executive Summary
This paper introduces a multi-objective NAS+HPO framework that jointly optimizes model architectures and training hyperparameters to improve both accuracy and fairness in tabular data classification. The authors demonstrate that traditional single-objective optimization for accuracy produces highly unfair models, while their approach consistently discovers architectures that Pareto-dominate existing bias mitigation techniques. By searching across MLP, ResNet, and FT-Transformer architectures with Hyperband multi-fidelity optimization, the method achieves significantly fairer models while maintaining competitive accuracy across three benchmark datasets.

## Method Summary
The authors employ SMAC3 for black-box optimization using Hyperband as the intensifier algorithm (1-10 epochs, η=3) to jointly optimize architectural hyperparameters (layers, neurons, attention blocks, heads) and training hyperparameters (learning rate, weight decay, batch sizes, dropout rates) across three model classes: MLP, ResNet, and FT-Transformer. The optimization targets balanced accuracy and fairness metrics including statistical parity difference, average odds difference, and equal opportunity difference. The approach is evaluated against 12 baseline bias mitigation techniques spanning pre-processing, in-processing, and post-processing methods on three benchmark datasets: Adult Income, COMPAS, and Folktables ACS-Income.

## Key Results
- Multi-objective NAS+HPO discovers architectures with better fairness-accuracy tradeoffs than existing bias mitigation methods
- Single-objective accuracy optimization produces models that are highly accurate but deeply unfair
- Bias mitigation chaining (pre-processing + NAS+HPO) can recover accuracy lost by pre-processing techniques

## Why This Works (Mechanism)

### Mechanism 1
Multi-objective NAS+HPO discovers architectures with better fairness-accuracy tradeoffs than existing bias mitigation methods by optimizing jointly over architectural and training hyperparameters under fairness and accuracy constraints, exploring configurations that inherently encode fairness rather than patching biased models post-hoc. This works because the search space contains architectural configurations that are inherently fairer than those obtained by optimizing only accuracy. The approach produces architectures that consistently Pareto-dominate state-of-the-art bias mitigation techniques. If the search space is too constrained or biased toward high-accuracy, low-fairness regions, multi-objective optimization cannot escape this trap.

### Mechanism 2
Single-objective accuracy optimization produces models that are highly accurate but deeply unfair because when only accuracy is optimized, the search converges to configurations that exploit correlations with protected attributes, maximizing accuracy at the expense of fairness. This occurs because accuracy and fairness are not naturally aligned in tabular data without explicit fairness constraints. The authors observe that single-objective optimization on standard accuracy metrics produces highly detrimental models to unprivileged classes. If the dataset is balanced or protected attributes have minimal correlation with the target, accuracy optimization might incidentally yield fair models.

### Mechanism 3
Bias mitigation chaining (pre-processing + NAS+HPO) can recover accuracy lost by pre-processing because pre-processing techniques like Disparate Impact Remover reduce bias but hurt accuracy, and joint NAS+HPO over the entire pipeline can find configurations that mitigate both bias and accuracy loss simultaneously. This works by treating the whole pipeline as a black box and optimizing it over the target fairness and accuracy metrics. The approach still outperforms baselines even when applied after pre-processing. If the pre-processing step fundamentally removes too much information or the search space cannot compensate for this loss, chaining cannot recover accuracy.

## Foundational Learning

- Concept: Multi-objective optimization with scalarization
  - Why needed here: The paper reduces the fairness-accuracy tradeoff to a single scalar objective using weighted mean aggregation and ParEGO
  - Quick check question: Why is scalarization necessary for this NAS+HPO setup instead of directly optimizing the Pareto front?

- Concept: Hyperband multi-fidelity optimization
  - Why needed here: With hundreds of architectural combinations and infinite training hyperparameter settings, full training is prohibitive; Hyperband allows early stopping of poor configurations
  - Quick check question: How does Hyperband's successive halving schedule balance exploration vs. exploitation in this fairness-accuracy context?

- Concept: Tabular data architectural biases
  - Why needed here: Understanding how MLP, ResNet, and FT-Transformer architectures encode inductive biases that can be fairness-relevant
  - Quick check question: What architectural differences between FT-Transformer and MLP make one more likely to discover fairer configurations?

## Architecture Onboarding

- Component map: Search space design (MLP/ResNet/FT-Transformer) → Multi-fidelity HPO (Hyperband) → Multi-objective optimization (weighted mean / ParEGO) → Fairness-accuracy evaluation
- Critical path: Design search space → Configure Hyperband budgets → Set fairness/accuracy objectives → Run SMAC optimization → Evaluate Pareto front vs. baselines
- Design tradeoffs: Larger search spaces give more fairness potential but increase computational cost; multi-fidelity speeds up search but risks premature convergence; scalarization simplifies optimization but may miss non-dominated solutions
- Failure signatures: No Pareto improvement over baselines; single-objective accuracy runs consistently dominate multi-objective; optimization converges to trivial solutions (e.g., MLP on imbalanced data)
- First 3 experiments:
  1. Run single-objective accuracy optimization on FT-Transformer search space to establish baseline and confirm it produces unfair models
  2. Run multi-objective optimization with statistical parity difference to see if fairness improves without significant accuracy loss
  3. Apply Disparate Impact Remover pre-processing then run multi-objective optimization to test chaining effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the multi-objective NAS+HPO approach maintain its superiority over baseline methods when extended to larger and more complex tabular datasets with higher dimensionality and more diverse feature types?
- Basis in paper: The authors demonstrate their approach on three benchmark datasets (Adult, COMPAS, Folktables ACS-Income) and show Pareto dominance over baseline methods, but acknowledge the need to validate on larger and more complex datasets.
- Why unresolved: The current study only covers relatively small-scale tabular datasets. Real-world tabular data often contains higher dimensionality, mixed feature types, and larger sample sizes that could affect the performance and scalability of the NAS+HPO approach.
- What evidence would resolve it: Empirical validation of the multi-objective NAS+HPO approach on larger, more complex tabular datasets with varying dimensionality, feature types, and sample sizes, comparing its performance against baseline methods.

### Open Question 2
- Question: How do the discovered fair architectures generalize to unseen data distributions and temporal shifts in the protected attributes' prevalence?
- Basis in paper: The authors optimize models on specific datasets and benchmark them against existing methods, but do not address the generalization and temporal robustness of the discovered architectures to changing data distributions.
- Why unresolved: Real-world tabular data often exhibits temporal shifts and changes in the distribution of protected attributes. The current study does not investigate how well the discovered fair architectures adapt to such changes or generalize to unseen data distributions.
- What evidence would resolve it: Empirical evaluation of the discovered fair architectures on temporally shifted data and data from different distributions, assessing their fairness and accuracy performance under these conditions.

### Open Question 3
- Question: What is the computational overhead of the multi-objective NAS+HPO approach compared to traditional bias mitigation techniques, and how does it scale with the complexity of the search space?
- Basis in paper: The authors mention that their approach outperforms existing bias mitigation techniques in terms of fairness and accuracy, but do not provide a detailed comparison of the computational overhead and scalability of their method.
- Why unresolved: While the multi-objective NAS+HPO approach shows promising results, it is essential to understand its computational efficiency and scalability compared to traditional bias mitigation techniques, especially as the complexity of the search space increases.
- What evidence would resolve it: A comprehensive comparison of the computational overhead, runtime, and scalability of the multi-objective NAS+HPO approach against traditional bias mitigation techniques across various search space complexities and dataset sizes.

## Limitations

- The exact configuration of the multi-objective optimization (weighted mean aggregation vs. ParEGO, specific weight settings) is not fully specified
- Computational feasibility claims (3 days on a single GPU) lack detailed benchmarking across different hardware configurations
- Generalization claims are based on only three benchmark datasets, raising questions about broader applicability

## Confidence

**High Confidence:** The fundamental claim that joint NAS+HPO can discover fairer architectures than single-objective optimization is well-supported by the empirical results showing consistent Pareto improvements over baseline methods.

**Medium Confidence:** The claim about bias mitigation chaining (pre-processing + NAS+HPO) is supported by limited experimental evidence and lacks detailed analysis of when this approach succeeds or fails.

**Low Confidence:** The assertion that fairness constraints are "essential" for developing fair models in tabular data applications is an overstatement based on the current experimental scope.

## Next Checks

1. **Replicate single-objective vs. multi-objective comparison** on a held-out tabular dataset to verify that single-objective accuracy optimization consistently produces unfair models while multi-objective optimization achieves better fairness-accuracy tradeoffs.

2. **Test scalability limits** by measuring optimization wall-clock time and resource usage on larger tabular datasets to validate the claimed computational efficiency and identify potential bottlenecks.

3. **Validate robustness to different fairness metrics** by running the same experiments with alternative fairness definitions (e.g., equalized odds, demographic parity) to confirm the results are not metric-specific.