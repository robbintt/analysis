---
ver: rpa2
title: Emulating Human Cognitive Processes for Expert-Level Medical Question-Answering
  with Large Language Models
arxiv_id: '2310.11266'
source_url: https://arxiv.org/abs/2310.11266
tags:
- booksmed
- clinical
- medical
- evidence
- med-palm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BooksMed is a framework that simulates human cognitive processes
  to provide evidence-based medical answers. It uses a multistage approach with fine-tuned
  models for problem identification, knowledge acquisition, strategy formulation,
  monitoring, and reflection.
---

# Emulating Human Cognitive Processes for Expert-Level Medical Question-Answering with Large Language Models

## Quick Facts
- **arXiv ID:** 2310.11266
- **Source URL:** https://arxiv.org/abs/2310.11266
- **Reference count:** 40
- **Primary result:** BooksMed framework emulates human cognitive processes to deliver evidence-based medical answers, outperforming baseline models on clinical benchmarks.

## Executive Summary
BooksMed is a framework that simulates human cognitive processes to provide evidence-based medical answers using large language models. It implements a five-stage cognitive problem-solving approach (Problem Identification, Knowledge Acquisition, Strategy Formulation, Monitoring, and Reflection) with fine-tuned models for each stage. The framework quantifies evidence strength using the GRADE framework and retrieves information using a multiscale context retrieval method. BooksMed was evaluated on ExpertMedQA, a multispecialty clinical benchmark validated by medical professionals, and achieved favorable ratings for factual accuracy, adequacy, formatting, clarity, and citation relevance.

## Method Summary
BooksMed employs a five-stage cognitive problem-solving framework where each stage is implemented by a fine-tuned large language model. The framework uses multiscale context retrieval with varying token lengths and multiple embedding models to flexibly retrieve documents. Complex questions are decomposed into sub-questions using factored decomposition, with each sub-question answered within a new context. Evidence strength is assessed using the GRADE framework, and answers are constructed using in-context learning combined with chain-of-thought reasoning. The framework was evaluated on ExpertMedQA, an internally validated clinical benchmark spanning four medical specialties.

## Key Results
- BooksMed achieved favorable ratings across factual accuracy, adequacy, formatting, clarity, and citation relevance on ExpertMedQA benchmark
- Outperformed existing models like Med-PaLM 2, Almanac, and ChatGPT on various metrics including accuracy, completeness, and safety
- Demonstrated effective integration of GRADE framework for evidence quality quantification in medical recommendations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiscale context retrieval improves retrieval precision by addressing the trade-off between recall and precision in medical document retrieval.
- Mechanism: The framework employs vector databases with varying token lengths and multiple embedding models to flexibly retrieve documents at different levels of specificity. The HyDE method generates hypothetical embeddings to capture relevance patterns even with incomplete queries.
- Core assumption: Medical questions have heterogeneous complexity requiring different retrieval granularities.
- Evidence anchors:
  - [section] "To enhance document retrieval efficiency, BooksMed implements the following strategies: 1. Multiscale Context Retrieval: By deploying vector databases with varying token lengths, BooksMed ensures a more flexible and nuanced approach to document retrieval."
  - [section] "To fortify document retrieval capabilities, BooksMed incorporates the HyDE method. This method involves generating hypothetical document embeddings (HyDE) that capture relevance patterns (albeit with potential inaccuracies) and then encoding these into embedding vectors."
- Break condition: If the embedding models are poorly aligned with the medical domain, retrieval quality will degrade regardless of multiscale approach.

### Mechanism 2
- Claim: Factored decomposition improves answer accuracy by breaking complex medical questions into manageable sub-questions.
- Mechanism: Complex questions are decomposed into sub-questions, each answered within a new context, allowing the model to focus on specific aspects without being overwhelmed by the full complexity.
- Core assumption: Medical questions can be meaningfully decomposed into sub-questions that can be answered independently and then integrated.
- Evidence anchors:
  - [section] "Similarly, in LLMs, question decomposition improves the accuracy of model-generated reasoning. We implemented the factored decomposition approach. Sub-questions and answers are generated by factored decomposition, and each sub-question is answered within a new context."
  - [corpus] Weak evidence - no direct comparison between decomposed and non-decomposed performance provided in corpus neighbors.
- Break condition: If sub-questions are poorly formulated or the relationships between them are complex, integration may introduce errors.

### Mechanism 3
- Claim: GRADE framework integration ensures evidence quality quantification for medical recommendations.
- Mechanism: The framework assesses evidence strength using the GRADE framework, providing reliable scores that evaluate the quality of evidence and strength of recommendations in healthcare.
- Core assumption: Medical decision-making requires explicit evidence quality assessment, and GRADE provides a reliable framework for this purpose.
- Evidence anchors:
  - [abstract] "BooksMed uniquely emulates human cognitive processes to deliver evidence-based and reliable responses, utilizing the GRADE (Grading of Recommendations, Assessment, Development, and Evaluations) framework to effectively quantify evidence strength."
  - [section] "Strength of Evidence Assessment: Providing accurate and reliable scores based on the GRADE framework, is crucial for evaluating the quality of evidence and the strength of recommendations in healthcare."
- Break condition: If GRADE criteria are misapplied or evidence is insufficient to apply GRADE properly, the assessment becomes unreliable.

## Foundational Learning

- Concept: GRADE evidence quality framework
  - Why needed here: Medical recommendations require explicit evidence quality assessment to ensure reliability and clinical validity.
  - Quick check question: What are the main domains assessed by GRADE (risk of bias, inconsistency, indirectness, imprecision, publication bias)?

- Concept: Multiscale context retrieval and embedding techniques
  - Why needed here: Medical documents vary in length and specificity; different retrieval scales are needed to capture both broad context and specific details.
  - Quick check question: How does the HyDE method improve retrieval when queries are incomplete or ambiguous?

- Concept: Question decomposition for complex reasoning
  - Why needed here: Medical questions often involve multiple concepts and relationships that can be better addressed by breaking them into sub-questions.
  - Quick check question: What are the key principles for ensuring that decomposed sub-questions can be effectively reintegrated into a coherent answer?

## Architecture Onboarding

- Component map:
  Safety Measures (Stage I) -> Formulation Module (Stage I) -> Multiscale Retriever (Stage II) -> Factored Decomposition Engine (Stage III) -> Evidence Assessment Module (Stage IV) -> Answer Construction (Stage V)

- Critical path: Question → Safety Check → Understanding → Multiscale Retrieval → Decomposition → Sub-question Answering → Evidence Assessment → Answer Construction → Formatting

- Design tradeoffs:
  - Multiscale retrieval vs. computational cost: Multiple embedding models and vector databases increase accuracy but require more resources
  - Decomposition depth vs. integration complexity: Deeper decomposition may improve accuracy but makes answer synthesis more challenging
  - GRADE assessment vs. answer speed: Thorough evidence assessment takes time but ensures clinical reliability

- Failure signatures:
  - Retrieval failures: Missing relevant documents, retrieving irrelevant documents, poor ranking
  - Decomposition failures: Sub-questions that cannot be answered independently, integration errors
  - Evidence assessment failures: Misapplication of GRADE criteria, inappropriate evidence categorization
  - Safety failures: Model being tricked by adversarial prompts despite safety measures

- First 3 experiments:
  1. Retrieval precision test: Compare retrieval results using single-scale vs. multiscale approach on a medical query dataset
  2. Decomposition validation: Test whether decomposed sub-questions can be independently answered and successfully reintegrated
  3. Evidence assessment accuracy: Evaluate GRADE framework implementation by having clinicians rate the accuracy of evidence strength scores

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the performance of BooksMed vary across different clinical specialties and patient demographics?
  - Basis in paper: [explicit] The paper mentions that BooksMed's performance was evaluated across four specialties (Internal Medicine, Neurology, Pediatrics, and Psychiatry) and no significant differences were found between the median ratings across specialties.
  - Why unresolved: The evaluation was limited to a subset of clinical specialties and did not include a diverse range of patient demographics.
  - What evidence would resolve it: A larger-scale evaluation of BooksMed's performance across a wider range of clinical specialties and patient demographics, including different age groups, socioeconomic backgrounds, and geographical locations.

- **Open Question 2:** How can the data retrieval and search processes in BooksMed be enhanced to prioritize the latest research findings?
  - Basis in paper: [inferred] The paper mentions that in some instances, BooksMed cited older, but still valid and relevant research studies, highlighting a potential area for improvement.
  - Why unresolved: The current data retrieval and search processes may not be optimized to prioritize the latest research findings.
  - What evidence would resolve it: Comparative studies evaluating the impact of different data retrieval and search strategies on the accuracy and relevance of BooksMed's responses, particularly in terms of incorporating the latest research findings.

- **Open Question 3:** What are the potential long-term impacts of using BooksMed on patient outcomes and healthcare costs?
  - Basis in paper: [explicit] The paper mentions that BooksMed has the potential to facilitate informed decision-making and potentially enhance the quality and outcome of patient care, but does not provide evidence on its long-term impacts.
  - Why unresolved: The long-term impacts of using BooksMed on patient outcomes and healthcare costs have not been studied.
  - What evidence would resolve it: Longitudinal studies evaluating the impact of BooksMed on patient outcomes, such as mortality rates, readmission rates, and length of hospital stay, as well as its impact on healthcare costs, such as treatment costs and resource utilization.

## Limitations

- ExpertMedQA is a new, internally validated dataset with limited size (80 questions) across four specialties, potentially limiting generalizability
- Specific fine-tuning datasets and methodologies for the five cognitive stage models are not fully detailed
- Limited comparison to broader range of medical LLM approaches beyond the three mentioned baselines

## Confidence

**High Confidence Claims:**
- BooksMed successfully implements a five-stage cognitive problem-solving framework
- The framework demonstrates superior performance on the ExpertMedQA benchmark compared to baseline models
- Multiscale context retrieval and GRADE evidence assessment are effectively integrated

**Medium Confidence Claims:**
- The cognitive process emulation meaningfully improves medical reasoning over traditional approaches
- The specific performance improvements are directly attributable to the cognitive emulation methodology
- The safety measures effectively prevent adversarial prompt exploitation

**Low Confidence Claims:**
- Claims about BooksMed's ability to generalize across all medical specialties
- Assertions about the framework's scalability to real-world clinical deployment
- Specific percentage improvements without confidence intervals or statistical significance testing

## Next Checks

1. **Ablation Study Validation:** Remove individual cognitive stages (e.g., Evidence Assessment, Monitoring) and measure performance degradation to quantify each component's contribution to overall accuracy.

2. **External Dataset Testing:** Evaluate BooksMed on established medical QA benchmarks (MedQA-USMLE, PubMedQA) to assess generalizability beyond the proprietary ExpertMedQA dataset.

3. **Clinical Expert Review:** Conduct a blinded review by independent medical experts to validate the clinical accuracy and appropriateness of BooksMed's recommendations across different medical specialties.