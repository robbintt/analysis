---
ver: rpa2
title: 'CTQScorer: Combining Multiple Features for In-context Example Selection for
  Machine Translation'
arxiv_id: '2305.14105'
source_url: https://arxiv.org/abs/2305.14105
tags:
- example
- translation
- features
- examples
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CTQ Scorer, a framework that combines multiple
  features to select in-context examples for machine translation with large language
  models. It learns a regression model that estimates the translation quality of examples
  based on features such as semantic similarity, example quality, and token statistics.
---

# CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation

## Quick Facts
- arXiv ID: 2305.14105
- Source URL: https://arxiv.org/abs/2305.14105
- Reference count: 6
- Primary result: CTQ Scorer achieves over 2.5 COMET point improvement over BM25-based retrieval using multi-feature combination

## Executive Summary
This paper introduces CTQ Scorer, a framework for selecting in-context examples in few-shot machine translation with large language models. The approach learns a regression model that combines multiple features—including semantic similarity, example quality, and token statistics—to predict translation quality. Since no human judgments are available for training, the model is trained on synthetic data generated by evaluating 1-shot translations. Experiments across multiple language pairs demonstrate significant improvements over random selection and strong single-feature baselines, with COMET-QE-based similarity (particularly input source to example target) identified as the most effective feature.

## Method Summary
CTQ Scorer learns a regression model to select in-context examples for LLM-based machine translation. The method retrieves 100 candidate examples per input using BM25, generates synthetic training data by creating 1-shot translations with each candidate, and extracts multiple features including semantic similarity metrics (LaBSE, chrF, COMET-QE), example quality scores, token counts, and perplexity. A neural regression model (3-5 hidden layers, 64-512 neurons) is trained to predict a Contextual Translation Quality (CTQ) score from these features. At inference, candidates are reranked by CTQ score and top-k examples are selected for prompting the LLM. The approach is evaluated on BLOOM 7.1B and XGLM 7.5B models across multiple language pairs using COMET scores as the quality metric.

## Key Results
- CTQ Scorer achieves over 2.5 COMET point improvement over BM25-based retrieval on average
- Using COMET-QE-based similarity between input source and example target alone provides strong baseline performance
- The multi-feature approach significantly outperforms random selection and single-feature baselines
- CTQ Scorer demonstrates consistent improvements across multiple language pairs and LLM models

## Why This Works (Mechanism)

### Mechanism 1
The regression model effectively combines multiple features to predict contextual translation quality better than single-feature methods. By training on synthetic data generated through 1-shot prompting, the model learns which feature combinations correlate best with actual translation quality, assuming different features contribute differently to translation quality.

### Mechanism 2
COMET-QE based similarity, especially between input source and example target, is the most effective feature for example selection. The CTQ model learns that translation quality is better predicted when the semantic content of the input source matches the semantic content of the example target, rather than just matching source-to-source or using lexical metrics.

### Mechanism 3
Using perplexity-based features helps identify examples that the LLM is more familiar with, improving translation quality. Lower perplexity indicates the LLM has encountered similar contexts during training, making it more likely to generate accurate translations when prompted with those examples, assuming perplexity is a good proxy for this familiarity.

## Foundational Learning

- Concept: Feature engineering and selection
  - Why needed here: The model's performance depends on selecting the right features and understanding how they interact
  - Quick check question: Why might combining multiple similarity metrics (LaBSE, COMET-QE, chrF) be more effective than using just one?

- Concept: Synthetic data generation for training
  - Why needed here: Since no human judgments are available, the model must be trained on artificially created data
  - Quick check question: What could go wrong if the held-out dataset used for synthetic data generation doesn't represent the test distribution?

- Concept: Regression model optimization
  - Why needed here: The CTQ Scorer is a neural regression model that needs proper hyperparameter tuning
  - Quick check question: Why might different language pairs require different neural network architectures or hyperparameters?

## Architecture Onboarding

- Component map: Example database → BM25 retrieval → Feature extraction → CTQ scoring → Top-k selection
- Critical path: Input sentence → Feature extraction → CTQ scoring → LLM translation
- Design tradeoffs: BM25 retrieval is faster but less semantically accurate than embedding-based search; synthetic data generation is computationally expensive but necessary
- Failure signatures: Random selection baseline should perform significantly worse; if not, features aren't informative; if perplexity features dominate, might indicate overfitting to model familiarity rather than translation quality
- First 3 experiments:
  1. Implement baseline BM25 retrieval and random selection to establish performance floor
  2. Add single-feature CTQ models (CmtQE-InTgt alone) to verify it's a strong baseline
  3. Implement full multi-feature CTQ with ablation studies to identify most important features

## Open Questions the Paper Calls Out
- How do different example selection strategies impact the translation quality for low-resource language pairs?
- What is the optimal number of in-context examples (k) for different language pairs and translation directions?
- How do different prompt templates affect the translation quality of LLMs?

## Limitations
- The synthetic training data quality depends on the LLM's 1-shot translation performance, which may not generalize well to different domains or language pairs
- The approach requires computationally expensive synthetic data generation through 1-shot prompting for every candidate example
- The method's effectiveness for low-resource languages and translation out of English remains unexplored

## Confidence
- High: The core methodology of using a regression model to combine multiple features for example selection is sound and well-implemented
- Medium: The claim that COMET-QE-based similarity (particularly input source to example target) is the most effective feature
- Medium: The overall 2.5 COMET point improvement claim

## Next Checks
- Implement cross-domain validation by testing CTQ Scorer on technical documentation, social media text, and conversational dialogue to assess domain robustness
- Conduct feature interaction analysis by systematically combining features in different configurations and using SHAP values to quantify each feature's contribution
- Perform computational efficiency benchmarking by measuring the time required for synthetic data generation, feature extraction, and CTQ model inference across different dataset sizes