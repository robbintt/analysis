---
ver: rpa2
title: "R\xE9sum\xE9 Parsing as Hierarchical Sequence Labeling: An Empirical Study"
arxiv_id: '2309.07015'
source_url: https://arxiv.org/abs/2309.07015
tags:
- sequence
- each
- features
- labeling
- line
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work presents a hierarchical sequence labeling approach to\
  \ r\xE9sum\xE9 parsing, jointly predicting line-level section labels and token-level\
  \ entity labels in a single model. Experiments on seven languages (English, French,\
  \ Chinese, Spanish, German, Portuguese, and Swedish) show that multi-task architectures\
  \ outperform single-task variants for token labeling, with Transformer-based features\
  \ yielding 2.5% higher F1 on English and 1-2% gains on other languages compared\
  \ to FastText features."
---

# R√©sum√© Parsing as Hierarchical Sequence Labeling: An Empirical Study

## Quick Facts
- arXiv ID: 2309.07015
- Source URL: https://arxiv.org/abs/2309.07015
- Reference count: 33
- Primary result: Multi-task hierarchical sequence labeling achieves 90.94% F1 for entity extraction and 92.62% F1 for section classification in English

## Executive Summary
This work introduces a hierarchical sequence labeling approach to r√©sum√© parsing that jointly predicts line-level section labels and token-level entity labels using a single model. The architecture employs BiRNN+CRF layers with either FastText or Transformer-based features, showing that multi-task learning significantly improves token-level entity extraction performance. Experiments across seven languages demonstrate consistent advantages for multi-task variants and Transformer features, while maintaining engineering simplicity compared to traditional pipeline approaches.

## Method Summary
The method implements a hierarchical BiRNN+CRF architecture that processes r√©sum√©s at both line and token levels simultaneously. Token representations are generated using either FastText embeddings with handcrafted features or frozen T5/mT5 encoders, then contextualized by a token-level BiRNN. Line representations are aggregated from token embeddings and processed by a line-level BiRNN. Separate CRF layers handle line-level section classification and token-level entity extraction, with multi-task variants jointly optimizing both objectives.

## Key Results
- Multi-task architectures outperform single-task variants for token labeling across all seven languages tested
- Transformer-based features yield 2.5% higher F1 on English and 1-2% gains on other languages compared to FastText features
- The model achieves 90.94% F1 for entity extraction and 92.62% F1 for section classification in English
- BiRNN layers and CRF decoding are critical components confirmed through ablation studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning improves token-level entity extraction by leveraging shared hierarchical context
- Mechanism: The model processes both line-level section labels and token-level entity labels in a single forward pass. Line representations are aggregated from token embeddings and then concatenated with token representations for entity prediction, providing cross-level supervision
- Core assumption: Joint optimization of line and token tasks improves token prediction accuracy beyond single-task models
- Evidence anchors:
  - [abstract]: "multi-task architectures outperform single-task variants for token labeling"
  - [section]: "A second important observation is that the multi-task models generally outperform their single-task counterparts for the token sequence labeling task"
  - [corpus]: Weak evidence - corpus shows 7 languages with 1000+ r√©sum√©s each, but no direct comparison of multi-task vs single-task performance per language
- Break condition: If line-level predictions are highly inaccurate, the concatenated representations may harm rather than help token-level predictions

### Mechanism 2
- Claim: Transformer-based features provide better contextualization than FastText for token-level tasks
- Mechanism: T5/mT5 token representations encode line-level context before being fed to the BiRNN+CRF architecture, while FastText relies on local word embeddings combined with handcrafted features
- Core assumption: Pre-trained transformer models capture semantic and syntactic patterns that are transferable to entity extraction tasks
- Evidence anchors:
  - [abstract]: "Transformer-based features yielding 2.5% higher F1 on English and 1-2% gains on other languages compared to FastText features"
  - [section]: "Comparing row 1 with row 3, and also row 2 with row 4, we see that using Transformer-based embeddings yields an improvement of 2.5% in the goalsùêπ1 on English"
  - [corpus]: Weak evidence - corpus statistics show average 834 tokens per English r√©sum√©, but no direct analysis of token-level feature effectiveness
- Break condition: When transformer models are too large for inference constraints, the accuracy gains may not justify the computational cost

### Mechanism 3
- Claim: BiRNN layer is critical for capturing cross-line context in long r√©sum√© documents
- Mechanism: After obtaining token representations from either FastText or Transformer features, the BiRNN layer processes the entire token sequence across all lines, providing document-level context that transformers alone cannot capture
- Core assumption: R√©sum√© entities often depend on information spread across multiple lines or sections
- Evidence anchors:
  - [abstract]: "Ablation studies confirm that bidirectional RNN layers and CRF decoding are critical for performance"
  - [section]: "This is because the two layers capture complementary aspects of the context: the Transformer encodes tokens by exclusively considering the context of the current line, while the BiRNN layer on top contextualizes across every line"
  - [corpus]: Strong evidence - corpus shows average 73 lines per English r√©sum√©, demonstrating the need for cross-line context
- Break condition: If the r√©sum√© structure is very simple with entities confined to single lines, the BiRNN may provide diminishing returns

## Foundational Learning

- Concept: Sequence labeling with CRF layers
  - Why needed here: CRF layers capture label dependencies and ensure valid label sequences, improving accuracy over independent softmax predictions
  - Quick check question: What is the key difference between CRF and softmax in sequence labeling tasks?

- Concept: Hierarchical modeling
  - Why needed here: R√©sum√©s have natural hierarchical structure (sections ‚Üí groups ‚Üí entities) that can be modeled jointly rather than as separate tasks
  - Quick check question: How does joint modeling of hierarchical tasks differ from pipeline approaches in terms of error propagation?

- Concept: Multi-task learning
  - Why needed here: Learning multiple related tasks simultaneously can improve generalization through shared representations and regularization
  - Quick check question: What are the potential benefits and drawbacks of multi-task learning compared to single-task approaches?

## Architecture Onboarding

- Component map: Tokenized r√©sum√© text -> Feature extractor (FastText or Transformer) -> Token BiRNN -> Line aggregator -> Line BiRNN -> CRF (for lines) AND Token features + Line representations -> CRF (for tokens)

- Critical path: Token features ‚Üí Token BiRNN ‚Üí Line aggregator ‚Üí Line BiRNN ‚Üí CRF (for lines) AND Token features + Line representations ‚Üí CRF (for tokens)

- Design tradeoffs:
  - FastText vs Transformer features: Speed vs accuracy
  - Single-task vs multi-task: Simplicity vs potential performance gains
  - Frozen vs fine-tuned transformers: Faster inference vs better adaptation

- Failure signatures:
  - Poor line-level predictions: May indicate issues with the line aggregator or line BiRNN
  - Inaccurate entity extraction: Could be feature representation issues or insufficient cross-line context
  - Slow inference: Likely transformer-based features without GPU acceleration

- First 3 experiments:
  1. Compare FastText features vs frozen T5 features on a small r√©sum√© subset
  2. Test single-task vs multi-task variants to verify performance gains
  3. Replace BiRNN with just CRF on token features to confirm its importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the effects of using multilingual pre-trained models (e.g., mBERT or mT5) for initial token representations across all languages compared to language-specific models?
- Basis in paper: [inferred] The paper explores language-specific models like mT5 for different languages, suggesting potential investigation into multilingual models.
- Why unresolved: The study focuses on language-specific models and does not directly compare them with multilingual models.
- What evidence would resolve it: Experimental results comparing multilingual and language-specific models across the seven languages studied.

### Open Question 2
- Question: How does the proposed model perform on resumes from industries or locations not represented in the training data?
- Basis in paper: [explicit] The paper acknowledges that the model may not generalize well to resumes from unseen industries or locations.
- Why unresolved: The study does not test the model on resumes from outside the training distribution.
- What evidence would resolve it: Testing the model on a separate dataset containing resumes from diverse, unseen industries and locations.

### Open Question 3
- Question: What is the impact of using character-based initial features in combination with word embeddings for the FastText variants?
- Basis in paper: [inferred] The paper suggests character-based features as a potential future direction, indicating possible benefits.
- Why unresolved: The study does not explore character-based features for the FastText models.
- What evidence would resolve it: Experiments comparing models with and without character-based features for the FastText variants.

## Limitations

- The study uses relatively small test sets (50-100 r√©sum√©s per language) and cross-validation results showing performance variance across folds
- Handcrafted feature engineering components are not fully specified, making exact reproduction difficult without access to language-specific dictionaries
- The model's robustness to noise, incomplete documents, or significantly different formatting conventions is not evaluated

## Confidence

**High Confidence**: The superiority of multi-task architectures over single-task variants for token labeling is well-supported by direct comparisons across multiple languages and consistent with established multi-task learning literature.

**Medium Confidence**: The claim that Transformer-based features yield 2.5% higher F1 on English and 1-2% gains on other languages is supported by the data but shows language-dependent variation, with some languages showing minimal differences between feature types.

**Medium Confidence**: The assertion that BiRNN and CRF components are "critical" is supported by ablation studies, though the magnitude of performance drops upon removal is not quantified in detail for all architectures.

## Next Checks

1. **Cross-fold Consistency Analysis**: Replicate the cross-validation experiments for French and Chinese to quantify variance in F1 scores across folds, particularly for the FastText vs Transformer feature comparison where differences are less pronounced.

2. **Handcrafted Feature Dependency**: Implement a simplified version of the FastText + handcrafted feature pipeline using publicly available named entity recognition tools and common r√©sum√© section keywords to assess whether the performance gap with Transformer features is primarily due to the handcrafted features.

3. **Robustness to Document Noise**: Create a perturbed test set by randomly removing 10-20% of lines from r√©sum√©s and measuring degradation in both line-level and token-level performance to assess model robustness to incomplete or noisy inputs.