---
ver: rpa2
title: 'An Adversarial Example for Direct Logit Attribution: Memory Management in
  GELU-4L'
arxiv_id: '2310.07325'
source_url: https://arxiv.org/abs/2310.07325
tags:
- l0h2
- output
- residual
- stream
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents concrete evidence for a memory management mechanism
  in a 4-layer transformer model (GELU-4L). The authors identify "clean-up behavior"
  where certain attention heads and MLP layers remove information from the residual
  stream that was previously written by earlier layers.
---

# An Adversarial Example for Direct Logit Attribution: Memory Management in GELU-4L

## Quick Facts
- arXiv ID: 2310.07325
- Source URL: https://arxiv.org/abs/2310.07325
- Reference count: 9
- Key outcome: Demonstrates memory management in transformers through "clean-up behavior" that makes Direct Logit Attribution misleading

## Executive Summary
This paper presents concrete evidence for a memory management mechanism in a 4-layer transformer model (GELU-4L). The authors identify "clean-up behavior" where certain attention heads and MLP layers remove information from the residual stream that was previously written by earlier layers. This is demonstrated by tracking how the output of a specific attention head (L0H2) is consistently removed by six layer-2 attention heads across 300 prompts. The study shows that Direct Logit Attribution (DLA), a common interpretability technique, can produce misleading results because it doesn't account for this erasure phenomenon.

## Method Summary
The study uses a 4-layer transformer (GELU-4L) and analyzes 300 prompts from the training dataset. The authors track the residual stream to identify clean-up behavior by measuring projection ratios of writer node outputs (specifically L0H2) onto the residual stream before and after layer 2 attention blocks. They construct adversarial examples by selecting tokens and prompts where the model predicts these tokens with high probability, then verify that L0H2's DLA values remain high even when the input is patched with unrelated text. The correlation between DLA values of writer heads and clean-up heads is calculated using V-composition to isolate writer-dependent components.

## Key Results
- Clean-up behavior identified where layer-2 attention heads consistently remove L0H2 output from the residual stream across 300 prompts
- Strong negative correlation (-0.716) between DLA values of writer nodes and their corresponding clean-up heads
- DLA values can be high for nodes whose contributions are actively being removed from the residual stream, demonstrating DLA's vulnerability to clean-up behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The residual stream has limited bandwidth, causing later layers to overwrite earlier computations.
- Mechanism: Attention heads and MLP layers in later layers detect and remove information written by earlier layers, creating "clean-up behavior."
- Core assumption: The residual stream acts as a shared memory space with limited capacity, requiring active memory management.
- Evidence anchors: [abstract] "Prior work suggests that language models manage the limited bandwidth of the residual stream through a 'memory management' mechanism"; [section] "We characterize clean-up behavior as four steps during a forward pass" including later nodes clearing information from earlier nodes; [corpus] Weak - only 0 citations for the cited Elhage et al. [1] paper
- Break condition: If later layers don't show consistent negative projections onto earlier layer outputs, the memory management hypothesis would be weakened.

### Mechanism 2
- Claim: Direct Logit Attribution fails to account for information erasure by later layers.
- Mechanism: DLA computes attributions by applying unembedding directly to node outputs, but doesn't consider that later nodes may remove this information from the residual stream.
- Core assumption: The final residual stream state determines predictions, not individual layer outputs in isolation.
- Evidence anchors: [abstract] "Direct Logit Attribution (DLA), a common technique for interpreting the output of intermediate transformer layers, can show misleading results by not accounting for erasure"; [section] "DLA values will be high, if the node output happens to be aligned with the unembedding direction of certain tokens. In the gelu-4l model however, the output of L0H2 is largely removed from the residual stream"; [corpus] Weak - only 0 citations for the cited DLA paper [2]
- Break condition: If DLA values consistently correlated with actual prediction contributions across all layers, the critique would be invalid.

### Mechanism 3
- Claim: Clean-up heads and writer heads show negative correlation in their DLA values.
- Mechanism: Clean-up heads write negative values to cancel writer head outputs, so their DLA values have opposite signs.
- Core assumption: Linear operations in the residual stream mean clean-up is effectively a subtraction of the writer's contribution.
- Evidence anchors: [abstract] "We find a strong negative correlation (-0.716) between the DLA values of writer nodes and their corresponding clean-up heads"; [section] "applying DLA to clean-up heads will yield significant values as well, but with a flipped sign"; [corpus] Weak - no corpus papers discussing this specific negative correlation phenomenon
- Break condition: If clean-up heads showed positive correlation with writer heads in DLA values, the cancellation hypothesis would be falsified.

## Foundational Learning

- Concept: Transformer residual stream mechanics
  - Why needed here: Understanding how information flows through and accumulates in the residual stream is critical for interpreting the clean-up mechanism
  - Quick check question: If the residual stream at layer n equals the sum of all previous layer outputs, what does this imply about the relationship between early and late layer computations?

- Concept: Direct Logit Attribution methodology
  - Why needed here: DLA is the interpretability technique being critiqued, so understanding its assumptions and limitations is essential
  - Quick check question: If DLA applies unembedding directly to a layer's output, what key aspect of information flow does it potentially miss?

- Concept: Attention head composition and V-composition
  - Why needed here: The paper uses V-composition to isolate the portion of clean-up head output that depends on the writer head
  - Quick check question: How does computing the V-composition output help distinguish between general clean-up behavior and writer-specific clean-up?

## Architecture Onboarding

- Component map: GELU-4L model with 4 transformer layers, each with attention heads and MLPs. Key components are L0H2 (writer head), L2H2-L2H7 (clean-up heads), and the residual stream connecting all layers.
- Critical path: Forward pass through layers 0→1→2→3, with special attention to how L0H2 output is processed by L2 heads and eventually removed from the residual stream.
- Design tradeoffs: Using a 4-layer toy model allows precise tracking of information flow but may not generalize to deeper models where memory management mechanisms could differ.
- Failure signatures: If DLA values don't correlate with actual prediction contributions, or if clean-up behavior varies significantly across prompts, the proposed mechanisms may not be robust.
- First 3 experiments:
  1. Verify clean-up behavior by measuring projection ratios of residual stream onto L0H2 output before and after layer 2 attention blocks
  2. Test causal relationship by patching L0H2 output from layer 2 attention heads' Value inputs and observing changes in clean-up behavior
  3. Examine correlation between DLA values of writer heads and clean-up heads using V-composition to isolate writer-dependent components

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms or patterns does the cleaned-up subspace in the residual stream get reused for by later nodes in the model?
- Basis in paper: [explicit] The paper explicitly states they will address how the cleared space is used by later nodes after clean-up in future work (section 1.1).
- Why unresolved: The current study only analyzes the removal of writer node output (steps 1-3 of clean-up behavior) and defers investigation of step 4 to future work.
- What evidence would resolve it: Experiments tracking the residual stream directions after clean-up to identify what new information gets written to these cleared subspaces and how it contributes to final predictions.

### Open Question 2
- Question: How generalizable is the memory management/clean-up behavior observed in the 4-layer GELU-4L model to larger, more complex transformer models?
- Basis in paper: [inferred] The authors mention they will look for clean-up behavior in other models in future work (section 4 conclusion).
- Why unresolved: The study only examined one specific 4-layer model, and the authors acknowledge that previous research suggests early and late heads exhibit high DLA values that may have been misinterpreted.
- What evidence would resolve it: Systematic analysis of clean-up behavior across various transformer architectures (different depths, widths, attention mechanisms) to determine prevalence and patterns.

### Open Question 3
- Question: What is the precise functional role of the writer head L0H2 in the GELU-4L model, given its behavior resembles a positional information head?
- Basis in paper: [explicit] The paper states "The function of L0H2 is not totally clear, but it resembles a positional information head" (section 2.2).
- Why unresolved: Despite identifying that L0H2's output gets consistently cleaned up, the study doesn't provide a definitive explanation of what information L0H2 is encoding or why it needs to be cleared.
- What evidence would resolve it: Detailed analysis of L0H2's attention patterns, value outputs, and composition with other heads to determine what specific information it encodes and why that information becomes obsolete after layer 2.

## Limitations
- The findings are based on a 4-layer toy model, which may not generalize to deeper transformers where memory management mechanisms could be more complex
- The correlation analysis (-0.716) is based on only 30 additional samples from training data, which may not be sufficient for robust statistical claims
- The paper relies on specific attention heads (L0H2 and L2H2-L2H7) as examples, and it's unclear if this clean-up behavior is universal across all attention heads or model architectures

## Confidence
- **High confidence**: The existence of clean-up behavior in the gelu-4l model is well-demonstrated through projection ratio measurements across 300 prompts
- **Medium confidence**: The negative correlation between DLA values of writer heads and clean-up heads (-0.716) is statistically present but based on limited samples
- **Low confidence**: Claims about the generality of memory management mechanisms across transformer architectures, as the analysis is confined to a specific 4-layer model

## Next Checks
1. Scale-up validation: Test whether clean-up behavior and the associated DLA misinterpretation occurs in larger models (8+ layers) using the same projection ratio methodology to assess generalizability
2. Cross-model comparison: Apply the analysis to models with different attention mechanisms (e.g., Performer, Nyströmformer) to determine if clean-up behavior is architecture-dependent or universal
3. Statistical robustness: Expand the correlation analysis to use 100+ samples rather than 30, and test different statistical measures (Spearman vs Pearson) to confirm the negative relationship between writer and clean-up head DLA values