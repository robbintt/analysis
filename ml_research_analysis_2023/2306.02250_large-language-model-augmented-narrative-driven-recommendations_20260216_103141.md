---
ver: rpa2
title: Large Language Model Augmented Narrative Driven Recommendations
arxiv_id: '2306.02250'
source_url: https://arxiv.org/abs/2306.02250
tags:
- queries
- retrieval
- narrative
- language
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Narrative-driven recommendation (NDR) addresses the challenge of
  generating recommendations based on long, complex user queries describing preferences
  and context. The primary challenge is the lack of abundant training data for NDR
  models.
---

# Large Language Model Augmented Narrative Driven Recommendations

## Quick Facts
- arXiv ID: 2306.02250
- Source URL: https://arxiv.org/abs/2306.02250
- Reference count: 40
- Primary result: LLM-generated synthetic narrative queries enable training effective smaller NDR models that match or exceed larger LLM baselines

## Executive Summary
This paper addresses the challenge of narrative-driven recommendation (NDR), where systems must recommend items based on long, complex user queries describing preferences and context. The key innovation is using large language models (LLMs) to generate synthetic narrative queries from existing user-item interaction data, enabling training of smaller, more efficient retrieval models. The approach, called Mint, demonstrates that bi-encoder and cross-encoder models trained on synthetic data can outperform several baselines and match the performance of significantly larger LLM models, achieving strong results on a point-of-interest recommendation dataset.

## Method Summary
The method involves using a 175B InstructGPT model to generate synthetic narrative queries from user-item interaction data (reviews), then filtering items based on likelihood scores from a 3B FlanT5 model. The resulting synthetic queries and filtered reviews are used to train 110M parameter MPNet bi-encoder and cross-encoder models. The approach leverages existing textual data in user-item interaction datasets to create the narrative queries that NDR models need for training, addressing the data scarcity problem in this domain.

## Key Results
- Bi-encoder and cross-encoder models trained on synthetic data outperform several strong baselines
- BiEnc-Mint achieves 11-13% improvement over best bi-encoder baselines on precision measures
- BiEnc-Mint performs at par with a 175B LLM while offering the inference efficiency of a small-parameter bi-encoder
- Models match or exceed performance of significantly larger LLM baselines on NDCG, MAP, MRR, and recall metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated synthetic narrative queries capture user preferences that traditional interaction data misses
- Mechanism: LLMs condition on user-item interaction text (reviews) to generate long-form narrative queries describing preferences and context
- Core assumption: User reviews contain sufficient information about user preferences and context to generate realistic narrative queries
- Evidence anchors:
  - [abstract]: "classical user-item interaction datasets contain rich textual data, e.g., reviews, which often describe user preferences and context"
  - [section]: "Mint re-purposes readily available user-item interaction datasets... by using LLMs as query generation models to author narrative queries"
  - [corpus]: Weak evidence - related papers focus on retrieval-augmented LLMs but don't specifically address narrative query generation from interaction data
- Break condition: If user reviews don't contain preference information, or if LLM fails to generate coherent narrative queries from review text

### Mechanism 2
- Claim: Filtering items based on query likelihood improves training data quality
- Mechanism: Items with higher P(query|item) are retained for training, removing noisy or irrelevant items
- Core assumption: Query likelihood from pre-trained language models correlates with item relevance to the generated narrative query
- Evidence anchors:
  - [section]: "we only retain some of the items present in {ð‘‘ð‘–}ð‘ð‘¢ð‘–=1 before using it for training retrieval models... using a pre-trained language model to compute the likelihood of the query given each user item"
  - [section]: "Without this step, we found CrEnc to result in much poorer performance"
  - [corpus]: Weak evidence - related work uses query generation but doesn't specifically discuss item filtering based on query likelihood
- Break condition: If query likelihood doesn't correlate with actual relevance, or if filtering removes too many relevant items

### Mechanism 3
- Claim: Training small retrieval models on synthetic data achieves performance comparable to large LLMs
- Mechanism: Bi-encoder and cross-encoder models (110M parameters) trained on synthetic narrative queries and filtered items outperform baselines and match larger LLM performance
- Core assumption: Synthetic training data quality is sufficient to train effective smaller models without requiring large-scale real narrative data
- Evidence anchors:
  - [abstract]: "bi-encoder and cross-encoder models trained on synthetic data outperform several baselines and match the performance of significantly larger LLM baselines"
  - [section]: "BiEnc-Mint sees significant improvement compared to BM25 and outperforms the best bi-encoder baselines by 11-13% on precision measures"
  - [section]: "BiEnc-Mint also performs at par with a 175B LLM while offering the inference efficiency of a small-parameter bi-encoder"
- Break condition: If synthetic data quality is insufficient, or if smaller models cannot learn the complex relationships in narrative-driven recommendations

## Foundational Learning

- Concept: Narrative-driven recommendation (NDR) as ranking task
  - Why needed here: The entire approach frames NDR as a ranking problem where models must rank items based on narrative queries
  - Quick check question: What distinguishes NDR from standard personalized recommendation?

- Concept: Data augmentation using large language models
  - Why needed here: The core innovation relies on using LLMs to generate synthetic training data where real data is scarce
  - Quick check question: Why can't we simply use existing user-item interaction datasets without LLM augmentation?

- Concept: Bi-encoder vs cross-encoder architectures
  - Why needed here: The paper evaluates both architectures, with bi-encoders providing efficiency and cross-encoders providing better ranking quality
  - Quick check question: What are the tradeoffs between bi-encoder and cross-encoder models in terms of efficiency and ranking performance?

## Architecture Onboarding

- Component map: 175B LLM query generator -> 3B FlanT5 item filter -> 110M MPNet bi-encoder/cross-encoder models
- Critical path: Narrative query generation -> Item filtering -> Retrieval model training -> Evaluation
- Design tradeoffs: Larger LLM for query generation vs. computational cost; item filtering vs. data retention; bi-encoder efficiency vs. cross-encoder accuracy
- Failure signatures: Poor query quality from LLM, insufficient filtering removing relevant items, overfitting on synthetic data, poor generalization to real queries
- First 3 experiments:
  1. Generate synthetic queries for a small subset of users and manually evaluate query quality
  2. Test item filtering by comparing rankings with and without filtering on a validation set
  3. Train bi-encoder model on synthetic data and evaluate against simple baselines (BM25) before comparing to full baseline suite

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions but raises several implicit ones through its discussion of limitations and future work directions, particularly around the generalizability of the approach across different recommendation domains and the potential for training smaller specialized query generation models.

## Limitations
- Evaluation limited to a single dataset (Pointrec), restricting generalizability across recommendation domains
- Paper doesn't provide detailed analysis of synthetic query quality or the impact of different filtering thresholds
- Comparison with 175B LLM baseline may not be directly comparable due to different architectures or fine-tuning approaches

## Confidence
- High confidence: The effectiveness of using LLMs for synthetic query generation from user-item interaction data
- Medium confidence: The generalizability of the approach across different recommendation domains
- Medium confidence: The filtering mechanism's impact on performance and optimal filtering strategies

## Next Checks
1. **Cross-domain validation**: Test the approach on at least two additional recommendation datasets (e.g., movie or product recommendations) to assess generalizability beyond point-of-interest recommendations.

2. **Synthetic data quality analysis**: Conduct human evaluation of a sample of synthetic narrative queries to assess their coherence, relevance, and similarity to real user queries in the test collection.

3. **Filtering threshold sensitivity**: Systematically vary the item filtering threshold and analyze its impact on both training data quality and final model performance to identify optimal filtering strategies.