---
ver: rpa2
title: 'A Comparison of Personalized and Generalized Approaches to Emotion Recognition
  Using Consumer Wearable Devices: Machine Learning Study'
arxiv_id: '2308.14245'
source_url: https://arxiv.org/abs/2308.14245
tags:
- generalized
- personalized
- stress
- data
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared personalized and generalized machine learning
  models for three-class emotion classification (neutral, stress, and amusement) using
  wearable biosignal data. The researchers developed a convolutional encoder model
  and tested it on the WESAD dataset containing physiological signals from 15 subjects.
---

# A Comparison of Personalized and Generalized Approaches to Emotion Recognition Using Consumer Wearable Devices: Machine Learning Study

## Quick Facts
- arXiv ID: 2308.14245
- Source URL: https://arxiv.org/abs/2308.14245
- Reference count: 0
- Primary result: Personalized models achieved 95.06% accuracy vs 67.65% for generalized models

## Executive Summary
This study demonstrates that personalized machine learning models significantly outperform generalized approaches for emotion recognition using physiological signals from consumer wearable devices. Using the WESAD dataset with 15 subjects, the researchers compared three model variants for three-class emotion classification (neutral, stress, amusement). The personalized approach, where each subject has their own dedicated model, achieved over 95% accuracy compared to approximately 67% for generalized models. This performance gap highlights the importance of accounting for individual physiological differences in emotion recognition systems.

## Method Summary
The researchers developed a convolutional encoder model with three blocks of two 1D convolutional layers each, followed by max pooling and dropout. Data preprocessing involved normalizing eight physiological modalities (ECG, EDA, EMG, RESP, TEMP, and ACC x/y/z) to zero mean and unit variance, then partitioning into 64-point intervals with 50% overlap. The model was trained using AdamW optimizer with cross-entropy loss for 1000 epochs. Three variants were compared: subject-exclusive generalized (trained on other subjects), subject-inclusive generalized (trained on all subjects), and personalized (trained on individual subject data only).

## Key Results
- Personalized model achieved 95.06% accuracy and 91.71 F1-score for three-class emotion classification
- Subject-exclusive generalized model achieved 67.65% accuracy and 43.05 F1-score
- Subject-inclusive generalized model achieved 66.95% accuracy and 42.50 F1-score
- Personalized models showed substantial improvement over generalized approaches across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Personalized models capture individual physiological baselines, reducing inter-subject variance and improving classification accuracy.
- Mechanism: Each subject has a dedicated model trained on their own physiological data, allowing the model to learn subject-specific patterns of arousal and valence that generalize poorly across individuals.
- Core assumption: Inter-subject variability in physiological responses to emotional states is significant and consistent within subjects.
- Evidence anchors:
  - Personalized model achieved 95.06% accuracy vs 67.65% for subject-exclusive generalized model.
  - "individuals inherently experience and react to affective stimuli differently. This creates inter-subject data variance, which degrades model accuracy."

### Mechanism 2
- Claim: Sequential time-based partitioning prevents overfitting compared to random sampling by preserving temporal relationships in physiological signals.
- Mechanism: Training, validation, and testing sets are created using sequential intervals (first 70%, next 15%, final 15%) rather than random selection, ensuring that adjacent intervals with similar features remain in the same set.
- Core assumption: Physiological signal patterns exhibit temporal correlation that random sampling would disrupt.
- Evidence anchors:
  - "our partitioning of intervals according to sequential time order, rather than random selection, helped prevent overfitting by guaranteeing that two adjacent intervals with similar features would be in the same set."
  - "Simply using the first 70% of all intervals for the training data would skew the distribution of affective states given the nature of the WESAD dataset."

### Mechanism 3
- Claim: The U-Net inspired encoder architecture effectively extracts multi-domain features from physiological signals for emotion classification.
- Mechanism: The encoder uses three blocks of convolutional layers with max pooling and dropout, doubling channels between blocks, to create hierarchical feature representations from the eight input modalities.
- Core assumption: Hierarchical convolutional feature extraction is appropriate for physiological time-series data in emotion recognition.
- Evidence anchors:
  - "The encoder network had three blocks, with each block consisting of two one-dimensional convolutional layers (kernel size of three) followed by a one-dimensional max pooling (kernel size of two)."
  - "The output of the encoder was flattened and passed through two fully connected layers with SiLU activation to produce a three-class probability distribution."

## Foundational Learning

- Concept: Convolution operations on time-series data
  - Why needed here: The model uses one-dimensional convolutions to extract temporal patterns from physiological signals
  - Quick check question: What does a 1D convolution kernel of size 3 do to a time-series signal?

- Concept: Sliding window feature extraction
  - Why needed here: Data is partitioned into 64-point intervals with 50% overlap to create fixed-length input sequences
  - Quick check question: How many unique intervals can be extracted from 1000 data points using a 64-point window with 50% overlap?

- Concept: Cross-entropy loss for multi-class classification
  - Why needed here: The model predicts three emotion classes (neutral, stress, amusement) and uses cross-entropy to measure prediction error
  - Quick check question: How does cross-entropy loss behave when the model assigns high probability to the wrong class?

## Architecture Onboarding

- Component map: Input (8 physiological modalities) → Encoder (3 conv blocks with pooling) → Flatten → Fully Connected (2 layers) → Output (3-class softmax)
- Critical path: Data preprocessing → Encoder feature extraction → Classification head → Loss computation → Backpropagation
- Design tradeoffs: Personalized models require more storage and training time but achieve higher accuracy; generalized models are more scalable but less accurate
- Failure signatures: Poor generalization (high training accuracy but low test accuracy), overfitting (validation loss increasing while training loss decreases), class imbalance issues
- First 3 experiments:
  1. Train a single subject model and verify it achieves >90% accuracy on its test set
  2. Train a generalized model using subject-exclusive partitioning and compare performance to personalized model
  3. Test the model's sensitivity to different sliding window sizes (32, 64, 128 points) and overlap percentages (25%, 50%, 75%)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do personalized emotion recognition models perform when applied to different stress detection tasks or different emotion categories beyond the neutral, stress, and amusement classes tested in this study?
- Basis in paper: The authors note that their study only evaluated 15 subjects from a single dataset (WESAD) and suggest that reproducing results on additional physiological signal datasets and exploring a broader range of emotions could help better assess the benefits of personalization.
- Why unresolved: The study focused specifically on three-class emotion classification (neutral, stress, amusement) and did not test performance across different emotion categories or stress detection tasks.
- What evidence would resolve it: Conducting similar personalized vs. generalized model comparisons on datasets with different emotion categories (e.g., fear, anger, sadness) and stress detection tasks, and evaluating performance across these different conditions.

### Open Question 2
- Question: What is the minimum amount of labeled data required for a personalized model to achieve comparable performance to the results presented in this study?
- Basis in paper: The authors mention that individuals must annotate their own data for personalized models, which can be time-consuming and expensive, and suggest that self-supervised learning approaches could be explored to reduce the burden of manual labeling.
- Why unresolved: The study used the full WESAD dataset for training personalized models, but did not investigate how performance scales with the amount of labeled data available.
- What evidence would resolve it: Conducting experiments that systematically vary the amount of labeled data available for training personalized models and measuring the resulting performance to identify the minimum data requirements.

### Open Question 3
- Question: How do personalized emotion recognition models perform in real-world settings compared to controlled laboratory environments?
- Basis in paper: The authors note that data from WESAD were collected under controlled laboratory environments, which may not generalize to the real world, and suggest analyzing emotions in a real-world context through datasets containing physiological data collected during naturalistic conversations.
- Why unresolved: The study evaluated models on data collected in controlled laboratory settings, but did not test performance in real-world environments where emotions may be experienced differently.
- What evidence would resolve it: Testing personalized and generalized models on real-world datasets (e.g., K-EmoCon) that contain physiological data collected during naturalistic conversations or daily activities, and comparing performance to that achieved in laboratory settings.

## Limitations
- Small sample size of only 15 subjects limits generalizability to broader populations
- Controlled laboratory conditions in WESAD dataset may not reflect real-world emotional experiences
- No hyperparameter tuning details or comparison against recent state-of-the-art models

## Confidence
- Personalized vs. generalized model performance comparison: **High** - The 95.06% vs 67.65% accuracy difference is substantial and consistent across evaluation metrics.
- Temporal partitioning preventing overfitting: **Medium** - The mechanism is plausible but not rigorously validated through ablation studies.
- U-Net inspired architecture effectiveness: **Medium** - The architectural details are somewhat vague, and no ablation studies test the specific design choices.

## Next Checks
1. Test model performance on additional datasets with different emotional stimuli and larger subject pools to verify generalizability.
2. Conduct ablation studies to isolate the impact of temporal partitioning vs. random sampling on overfitting.
3. Compare against recent transformer-based architectures and established baselines to benchmark the proposed approach.