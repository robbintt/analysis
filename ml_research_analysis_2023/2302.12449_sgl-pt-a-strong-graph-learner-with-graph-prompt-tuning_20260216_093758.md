---
ver: rpa2
title: 'SGL-PT: A Strong Graph Learner with Graph Prompt Tuning'
arxiv_id: '2302.12449'
source_url: https://arxiv.org/abs/2302.12449
tags:
- graph
- learning
- prompt
- pre-training
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a strong and universal graph pre-training method
  SGL that combines generative and contrastive learning via an asymmetric design and
  a dynamic queue to integrate both branches. It also introduces a novel verbalizer-free
  prompting function to reformulate the downstream graph classification task in the
  same format as the pre-training task, which minimizes the training objective gap.
---

# SGL-PT: A Strong Graph Learner with Graph Prompt Tuning

## Quick Facts
- arXiv ID: 2302.12449
- Source URL: https://arxiv.org/abs/2302.12449
- Reference count: 35
- Key outcome: SGL-PT surpasses fine-tuning methods on biological datasets while achieving strong performance under unsupervised setting

## Executive Summary
SGL-PT is a self-supervised graph learning framework that combines generative and contrastive learning through an asymmetric design with dynamic queues. The method introduces a novel verbalizer-free prompting approach that reformulates downstream graph classification tasks to match the pre-training objective, minimizing the training gap between pre-training and fine-tuning. Experiments demonstrate that SGL-PT achieves state-of-the-art performance on various graph datasets, particularly excelling on biological datasets where traditional methods struggle.

## Method Summary
SGL-PT follows a "pre-train, prompt, and predict" paradigm. First, SGL pre-trains a GIN encoder using an asymmetric combination of generative (masked autoencoder) and contrastive (instance discrimination) learning objectives, integrated through a dynamic queue. During prompt tuning, the method adds a super node connected to all graph nodes and reformulates classification as masked feature reconstruction, eliminating the need for verbalizers. Class mapping is achieved through supervised prototypical contrastive learning. The approach is evaluated across multiple graph classification benchmarks and molecular property prediction tasks.

## Key Results
- SGL achieves state-of-the-art performance under unsupervised setting across multiple graph datasets
- SGL-PT significantly outperforms fine-tuning methods on biological datasets (BBBP, Tox21, ToxCast, SIDER, ClinTox, MUV, HIV, BACE)
- Ablation studies confirm the effectiveness of the dynamic queue and super node prompting approach
- The method shows improved performance when combining local (generative) and global (contrastive) branches

## Why This Works (Mechanism)

### Mechanism 1: Complementary Learning Integration
- The combination of generative (local) and contrastive (global) learning captures both intra-data and inter-data graph relationships through an asymmetric design with dynamic queues
- Core assumption: Local and global representations are complementary, and their integration leads to better graph-level representations
- Evidence: [abstract] states the method "acquires the complementary merits of generative and contrastive self-supervised graph learning"
- Break condition: If local and global branches learn conflicting representations or if the dynamic queue becomes a bottleneck

### Mechanism 2: Training Objective Alignment
- Reformulating graph classification as masked super node reconstruction minimizes the training objective gap between pre-training and downstream tasks
- Core assumption: The super node representation can effectively capture the entire graph's information, and reconstructed features can be mapped to semantic labels through prototypical contrastive learning
- Evidence: [abstract] mentions "unifying pre-training and fine-tuning by designing a novel verbalizer-free prompting function"
- Break condition: If super node representation fails to capture global graph information or prototypical contrastive learning cannot establish effective class mappings

### Mechanism 3: Dynamic Queue Consistency
- The dynamic queue with exponential moving average maintains consistency between online and target encoders, improving contrastive learning efficiency
- Core assumption: Consistent target representations are crucial for effective contrastive learning, and the dynamic queue provides sufficient negative samples without requiring large batch sizes
- Evidence: [section 3.2] describes using exponential moving average to update the target encoder and dynamic queue for negative sampling
- Break condition: If queue size is insufficient for dataset complexity or EMA momentum is poorly tuned

## Foundational Learning

- **Graph Neural Networks (GNNs)**: Understanding GNN message passing is crucial for grasping how the encoder aggregates node features for both local reconstruction and global contrastive learning. *Quick check: How does a GNN layer aggregate information from a node's neighbors, and why is this important for both local feature reconstruction and global contrastive learning?*

- **Self-supervised learning objectives**: Understanding the differences between generative (reconstruction-based) and contrastive (discrimination-based) methods is key to grasping SGL's asymmetric design. *Quick check: What are the main differences between generative and contrastive self-supervised learning, and how do these differences manifest in the local and global branches of SGL?*

- **Prompt engineering adaptation**: The concept of aligning pre-training and downstream objectives through template design in NLP is adapted for graph classification. *Quick check: In NLP prompting, how does template design help align pre-training (e.g., masked language modeling) with downstream tasks, and how is this concept adapted for graph classification in SGL-PT?*

## Architecture Onboarding

- **Component map**: Input graphs → Local branch (masking → encoding → re-masking → decoding → reconstruction loss) + Global branch (augmentation → encoding → readout → projection → contrastive loss with dynamic queue) → Combined loss → Encoder update → Super node addition → Prompt tuning (encoding → reconstruction + prototypical contrastive loss) → Prediction (encoding → graph representation → comparison with class prototypes → class prediction)

- **Critical path**: 1) Pre-training: Input graphs → Local branch + Global branch → Combined loss → Encoder update 2) Prompt Tuning: Pre-trained encoder + Input graphs with super node → Encoding → Reconstruction + Prototypical contrastive loss → Encoder update 3) Prediction: Input graph with super node → Encoding → Graph representation → Comparison with class prototypes → Class prediction

- **Design tradeoffs**: 
  - Local vs Global branch balance: λpre=0.5 for most datasets, but 0.9 for COLLAB and NCI1 to prioritize local learning
  - Dynamic queue size: Larger queues improve performance but increase memory usage
  - Masking rates: 0.5-0.75 for pre-training, but only 0.1 for prompt tuning to avoid catastrophic forgetting
  - Augmentation strategy: Simple masking and edge dropping work well, but complex augmentations weren't explored

- **Failure signatures**: 
  - Poor pre-training performance: Check local/global loss balance, dynamic queue updates, and EMA momentum
  - Negative transfer during fine-tuning: Indicates representation gap - prompt tuning should help, but verify super node usage
  - Overfitting on small datasets: Reduce model capacity, increase dropout, or use stronger regularization
  - Memory issues: Reduce dynamic queue size, decrease batch size, or use gradient checkpointing

- **First 3 experiments**: 
  1. Train with only local branch (w/o global branch) on MUTAG to confirm it learns meaningful node representations
  2. Train with only global branch (w/o local branch) on the same dataset to confirm it learns discriminative graph representations
  3. Compare SGL with and without dynamic queue on PROTEINS to quantify the queue's contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamic queue size affect the trade-off between computational efficiency and the quality of learned representations in SGL?
- Basis: The paper discusses sensitivity analysis of queue size showing performance improves with larger queues but doesn't explore computational cost implications
- Why unresolved: No detailed analysis of how queue size impacts computational resources or training time
- What evidence would resolve it: Experiments comparing training times and memory usage with varying queue sizes alongside performance metrics

### Open Question 2
- Question: What are the potential limitations of the verbalizer-free prompting approach when applied to more complex or diverse graph classification tasks?
- Basis: The paper demonstrates success on biological datasets but doesn't extensively test on more complex or varied graph structures
- Why unresolved: Limited scope of tested datasets and no discussion of potential challenges when scaling to diverse graph tasks
- What evidence would resolve it: Testing on a wider range of graph datasets with varying complexity and structure

### Open Question 3
- Question: How does the choice of GNN architecture (e.g., GIN, GCN) influence the performance of SGL and SGL-PT in different graph domains?
- Basis: The paper uses GIN as the default encoder but doesn't explore how different GNN architectures might affect results
- Why unresolved: No comparative results using other GNN architectures
- What evidence would resolve it: Experiments with multiple GNN architectures comparing their performance on the same tasks

## Limitations

- The paper's claims rely heavily on empirical results without providing theoretical guarantees for the proposed mechanisms
- The generalization of SGL-PT to domains beyond molecular and biological graphs is uncertain due to limited testing scope
- The effectiveness of the asymmetric design and dynamic queue integration is demonstrated empirically but lacks rigorous theoretical analysis

## Confidence

**High Confidence**: The empirical results showing SGL's superiority over baseline methods in pre-training and SGL-PT's effectiveness in prompt tuning are well-supported by extensive experiments across multiple datasets.

**Medium Confidence**: The claim that generative and contrastive learning are complementary and their integration captures both intra-data and inter-data relationships is plausible but not rigorously proven.

**Low Confidence**: The generalization of SGL-PT to domains beyond molecular and biological graphs is uncertain, as the paper focuses heavily on datasets with simple node features.

## Next Checks

1. **Cross-domain validation**: Test SGL-PT on a diverse set of graph classification tasks including social networks with rich node attributes, citation networks, and knowledge graphs to assess generalization beyond biological datasets.

2. **Theoretical analysis**: Develop theoretical bounds or analysis showing why the combination of generative and contrastive learning through asymmetric training provides better representations than either method alone.

3. **Scalability evaluation**: Assess the memory and computational efficiency of the dynamic queue approach on larger graph datasets and evaluate whether the FIFO queue becomes a bottleneck as dataset size increases.