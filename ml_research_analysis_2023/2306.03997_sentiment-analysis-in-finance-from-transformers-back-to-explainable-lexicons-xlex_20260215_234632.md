---
ver: rpa2
title: 'Sentiment Analysis in Finance: From Transformers Back to eXplainable Lexicons
  (XLex)'
arxiv_id: '2306.03997'
source_url: https://arxiv.org/abs/2306.03997
tags:
- sentiment
- lexicon
- shap
- value
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel methodology named eXplainable Lexicons
  (XLex) that combines the advantages of both lexicon-based methods and transformer
  models for sentiment analysis in finance. XLex utilizes transformers and SHAP for
  explainability to automatically learn financial lexicons, reducing the need for
  manual annotation efforts and expanding the vocabulary coverage of the Loughran-McDonald
  (LM) lexicon.
---

# Sentiment Analysis in Finance: From Transformers Back to eXplainable Lexicons (XLex)

## Quick Facts
- arXiv ID: 2306.03997
- Source URL: https://arxiv.org/abs/2306.03997
- Reference count: 40
- Key outcome: XLex combines transformer-derived word importance scores with lexicon-based methods to create an explainable financial sentiment lexicon that outperforms the Loughran-McDonald lexicon in accuracy while being significantly faster and smaller than transformer models.

## Executive Summary
This paper introduces eXplainable Lexicons (XLex), a methodology that bridges transformer models and lexicon-based sentiment analysis for financial text. XLex uses a pre-trained RoBERTa model and SHAP to automatically generate a financial sentiment lexicon, expanding coverage beyond manually curated resources like the Loughran-McDonald (LM) lexicon. The approach achieves higher accuracy than LM alone while maintaining the interpretability and efficiency advantages of lexicon-based methods. The resulting model is orders of magnitude smaller and faster than transformer alternatives, making it suitable for production environments requiring real-time sentiment analysis.

## Method Summary
XLex fine-tunes a RoBERTa transformer model on financial sentiment datasets (Financial PhraseBank and SemEval-2017-Task5), then uses SHAP to extract word-level importance scores from the model's predictions. These scores are aggregated to create an explainable lexicon, which is merged with the LM lexicon using weighted contributions optimized via grid search. The combined lexicon powers a simple lookup-based sentiment classifier that achieves higher accuracy than either lexicon alone while being significantly more efficient than the original transformer model.

## Key Results
- XLex expands vocabulary coverage beyond the manually curated Loughran-McDonald lexicon
- The combined XLex+LM lexicon outperforms LM alone on multiple financial datasets
- Lexicon-based models are ~1000x smaller and faster than transformer models while maintaining competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The model improves financial sentiment lexicon coverage by using transformer-derived word importance scores.
- **Mechanism:** XLex uses a pre-trained RoBERTa model to classify financial text sentiment, then applies SHAP to extract the contribution of each word to the model's prediction. These SHAP scores are aggregated and assigned to words, creating a lexicon with higher coverage than manually annotated ones like Loughran-McDonald (LM).
- **Core assumption:** Transformer models trained on financial data can identify sentiment-bearing words more comprehensively than expert-annotated lexicons.
- **Evidence anchors:** [abstract] "XLex utilizes transformers and SHAP for explainability to automatically learn financial lexicons, reducing the need for manual annotation efforts and expanding the vocabulary coverage of the Loughran-McDonald (LM) lexicon."
- **Break condition:** If the pre-trained transformer model does not generalize well to the target financial domain, or if SHAP fails to consistently identify sentiment-bearing words across contexts, the lexicon coverage gains will be limited.

### Mechanism 2
- **Claim:** Combining XLex and LM lexicons produces higher sentiment classification accuracy than either alone.
- **Mechanism:** After generating the explainable lexicon, XLex merges it with LM, assigning weighted contributions to each lexicon's features based on grid search optimization. This combined lexicon leverages the broad coverage of XLex and the domain reliability of LM.
- **Core assumption:** The features extracted by XLex (SHAP avg, SHAP ratio, count) are complementary to LM's manually curated sentiment values.
- **Evidence anchors:** [abstract] "The resulting lexicon outperforms the standard LM lexicon in sentiment analysis of financial datasets."
- **Break condition:** If the optimization coefficients (cxlp, cxlo, clmp, clmo) do not generalize across datasets, the combined lexicon may not outperform LM in all cases.

### Mechanism 3
- **Claim:** Lexicon-based models are significantly faster and smaller than transformer models, making them suitable for production environments.
- **Mechanism:** XLex generates a lexicon file that is orders of magnitude smaller than a transformer model (e.g., 363 KB vs. 1.32 GB). Sentiment classification is performed by simple lookups and weighted sums rather than complex matrix operations, resulting in much faster inference.
- **Core assumption:** The trade-off of slightly lower accuracy (if any) is acceptable given the gains in speed and model size for real-time applications.
- **Evidence anchors:** [abstract] "the lexicon-based approach is significantly more efficient in terms of model speed and size compared to transformers."
- **Break condition:** If the lexicon becomes too large due to extensive word coverage, or if the lookup-based computation is not optimized, the speed advantage may diminish.

## Foundational Learning

- **Concept:** SHAP (SHapley Additive exPlanations)
  - **Why needed here:** SHAP is used to attribute the transformer model's sentiment classification decisions to individual words, enabling the automatic creation of a sentiment lexicon.
  - **Quick check question:** What does a positive SHAP value for a word indicate about its contribution to the sentiment prediction?

- **Concept:** Lexicon-based sentiment analysis
  - **Why needed here:** The paper builds on the idea that pre-defined word lists with sentiment scores can be used for fast and interpretable sentiment classification, and aims to improve these lexicons using transformer models.
  - **Quick check question:** How does lexicon-based sentiment analysis differ from machine learning-based sentiment analysis in terms of interpretability?

- **Concept:** Grid search for hyperparameter optimization
  - **Why needed here:** The model uses grid search to find the optimal coefficients that control the contribution of XLex and LM features to the final sentiment score.
  - **Quick check question:** Why is it important to perform grid search when combining features from two different lexicons?

## Architecture Onboarding

- **Component map:** Financial text → Tokenizer → RoBERTa model → SHAP explainer → Word importance scores → Aggregated lexicon → Sentiment classification
- **Critical path:** Sentence → Tokenizer → RoBERTa model → SHAP explainer → Word importance scores → Aggregated lexicon → Sentiment classification
- **Design tradeoffs:**
  - **Speed vs. Accuracy:** Lexicon-based models are much faster but may have slightly lower accuracy than transformers
  - **Coverage vs. Precision:** XLex increases coverage but may introduce noise; LM is precise but limited in coverage
  - **Interpretability vs. Complexity:** Lexicon models are interpretable; transformer models are not
- **Failure signatures:**
  - **Low accuracy:** Indicates poor alignment between XLex and LM features, or suboptimal coefficient tuning
  - **Slow inference:** Suggests the lexicon is too large or the lookup logic is inefficient
  - **Missing words:** Indicates gaps in the source datasets used to generate XLex
- **First 3 experiments:**
  1. **Validate SHAP extraction:** Run the RoBERTa model and SHAP explainer on a small set of financial sentences; verify that positive and negative words are correctly identified
  2. **Test lexicon merging:** Create a small combined lexicon from XLex and LM; ensure that overlapping words are handled correctly and that feature alignment works
  3. **Benchmark performance:** Compare inference time and model size of the lexicon-based model vs. the RoBERTa transformer on a sample dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the XLex methodology perform when applied to domains outside of finance, such as medicine or legal document analysis?
- **Basis in paper:** [explicit] The authors state that the XLex methodology is general and adaptable, and that it has the potential to significantly impact various industries, enhancing the accuracy and interpretability of sentiment analysis results while reducing the time and cost associated with manual lexicon development.
- **Why unresolved:** The paper only presents results and discussion for financial datasets. The authors suggest that the methodology could be applied to other domains, but do not provide any concrete evidence or results to support this claim.
- **What evidence would resolve it:** Conducting experiments using the XLex methodology on datasets from other domains, such as medical or legal texts, and comparing the results to existing methods in those fields would provide evidence of the methodology's performance outside of finance.

### Open Question 2
- **Question:** What is the optimal combination of features (SHAP avg, SHAP ratio, and count) for sentiment analysis using the XLex methodology?
- **Basis in paper:** [explicit] The authors mention that they performed a grid search to identify the most effective combination of features, and found that SHAP avg had the dominant impact on accuracy. However, they do not provide a definitive answer as to the optimal combination of features.
- **Why unresolved:** The paper does not provide a clear answer as to the optimal combination of features for sentiment analysis using the XLex methodology. The authors only mention that they performed a grid search and found that SHAP avg was the most important feature, but do not provide a definitive answer as to the optimal combination.
- **What evidence would resolve it:** Conducting further experiments using different combinations of features and comparing the results to determine the optimal combination for sentiment analysis using the XLex methodology would provide evidence to answer this question.

### Open Question 3
- **Question:** How does the XLex methodology compare to other explainable AI (XAI) methods in terms of interpretability and accuracy?
- **Basis in paper:** [explicit] The authors state that the XLex methodology is inherently more interpretable than transformer models, as it relies on predefined rules and allows for better insights into the results of sentiment analysis. However, they do not compare the XLex methodology to other XAI methods.
- **Why unresolved:** The paper does not provide a comparison between the XLex methodology and other XAI methods in terms of interpretability and accuracy. The authors only mention that the XLex methodology is inherently more interpretable than transformer models, but do not provide a comparison to other XAI methods.
- **What evidence would resolve it:** Conducting experiments comparing the XLex methodology to other XAI methods, such as LIME or SHAP-based methods for transformer models, in terms of interpretability and accuracy would provide evidence to answer this question.

## Limitations

- SHAP-based lexicon generation assumes the pre-trained transformer's sentiment understanding generalizes to all financial sub-domains; performance may degrade for highly specialized or novel financial contexts not represented in the training data
- The grid-search optimization for combining XLex and LM features may overfit to specific datasets, potentially limiting robustness when applied to unseen financial corpora
- The 0.12% overlap between XLex and LM suggests the methods capture largely distinct vocabularies, but this also raises questions about whether the union truly captures the complete financial sentiment landscape

## Confidence

- **High Confidence:** Lexicon-based models are inherently faster and smaller than transformers (well-established trade-off); the mechanism for generating XLex using SHAP is clearly described
- **Medium Confidence:** Claims about XLex outperforming LM in accuracy are supported by experiments on multiple datasets, but the extent of improvement varies by dataset and the specific evaluation methodology is not fully transparent
- **Low Confidence:** The generalizability of the optimized XLex+LM combination coefficients across different financial domains and languages is not demonstrated

## Next Checks

1. Perform ablation studies to quantify the individual contribution of XLex vs. LM features to classification accuracy across all tested datasets
2. Test the XLex+LM model on a held-out financial dataset not used in any optimization step to assess true generalization
3. Evaluate the impact of different transformer architectures (e.g., BERT, DistilBERT) on the quality of the generated XLex lexicon