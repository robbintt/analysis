---
ver: rpa2
title: Multimodal Representation Learning by Alternating Unimodal Adaptation
arxiv_id: '2311.10707'
source_url: https://arxiv.org/abs/2311.10707
tags:
- modality
- learning
- multimodal
- modalities
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MLA, a method to address modality laziness
  in multimodal learning by alternating unimodal optimization. MLA alternates between
  optimizing each modality's encoder independently while capturing cross-modal interactions
  through a shared head.
---

# Multimodal Representation Learning by Alternating Unimodal Adaptation

## Quick Facts
- arXiv ID: 2311.10707
- Source URL: https://arxiv.org/abs/2311.10707
- Reference count: 40
- Primary result: MLA achieves up to 79.70% accuracy on CREMA-D, 93.33% on Food-101, and 78.92% on IEMOCAP by addressing modality laziness through alternating unimodal optimization

## Executive Summary
This paper addresses modality laziness in multimodal learning by introducing MLA (Multimodal Learning via Alternating adaptation), which alternates between independently optimizing each modality's encoder while maintaining cross-modal interactions through a shared head. The method prevents modality forgetting through a gradient modification mechanism that orthogonalizes gradient directions between modalities. During inference, MLA employs an uncertainty-based dynamic fusion strategy that weights predictions according to each modality's uncertainty. Experiments on five diverse datasets demonstrate MLA's consistent superiority over prior methods in both complete and missing modality scenarios.

## Method Summary
MLA reframes conventional joint multimodal learning into an alternating unimodal process where each modality is optimized independently in turn. A shared head captures cross-modal interactions while a gradient modification mechanism using Recursive Least Squares prevents forgetting of previously learned modality information. During inference, predictions are combined using uncertainty-based weights derived from entropy calculations. The approach is evaluated across five datasets with varying numbers of modalities and both complete and missing modality scenarios.

## Key Results
- MLA achieves 79.70% accuracy on CREMA-D emotion recognition
- MLA reaches 93.33% accuracy on Food-101 image classification
- MLA obtains 78.92% accuracy on IEMOCAP emotion recognition
- Consistent improvements across all five tested datasets in both complete and missing modality scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alternating unimodal optimization prevents modality laziness by isolating the optimization process for each modality, eliminating interference from more dominant modalities.
- Mechanism: The model processes only one modality at each training iteration, optimizing its encoder independently while using a shared head for cross-modal interactions.
- Core assumption: Modality laziness occurs because joint optimization causes the model to optimize for dominant modalities while neglecting subordinate ones.
- Evidence anchors:
  - [abstract] "MLA reframes the conventional joint multimodal learning process by transforming it into an alternating unimodal learning process, thereby minimizing interference between modalities."
  - [section 2.1] "In multimodal learning scenarios, the phenomenon known as modality laziness arises due to the inadequate optimization of less informative modalities when these are learned in conjunction with others"

### Mechanism 2
- Claim: The gradient modification mechanism prevents modality forgetting by ensuring the shared head maintains orthogonal gradient directions between modalities.
- Mechanism: At each iteration, a gradient modification matrix is computed using Recursive Least Square algorithm to orthogonalize the gradient direction of the shared head parameters, preventing interference between consecutive modalities.
- Core assumption: The shared head tends to forget information from previously trained modalities when learning new ones, leading to modality forgetting.
- Evidence anchors:
  - [abstract] "This optimization process is controlled by a gradient modification mechanism to prevent the shared head from losing previously acquired information."
  - [section 2.2] "we introduce a gradient modification matrix Pt at each iteration t to rectify the gradients of parameter Ï• of the shared head g before starting to learn new modality"

### Mechanism 3
- Claim: Test-time dynamic modality fusion improves multimodal performance by weighting predictions based on each modality's uncertainty.
- Mechanism: During inference, the model calculates prediction uncertainty for each modality using entropy, then assigns weights inversely proportional to uncertainty for combining predictions.
- Core assumption: Higher uncertainty in predictions correlates with lower confidence and potential incorrect predictions, making uncertainty a good proxy for modality importance.
- Evidence anchors:
  - [abstract] "During the inference phase, MLA utilizes a test-time uncertainty-based model fusion mechanism to integrate multimodal information."
  - [section 2.3] "we leverage prediction uncertainty as a proxy to gauge the importance of each modality"

## Foundational Learning

- Concept: Gradient orthogonalization
  - Why needed here: To prevent the shared head from forgetting previously learned modality information when learning new modalities
  - Quick check question: What mathematical operation ensures that two gradient vectors are perpendicular to each other?

- Concept: Uncertainty quantification through entropy
  - Why needed here: To measure the confidence of each modality's predictions and use this to weight their contributions during fusion
  - Quick check question: How does entropy relate to the confidence of a probability distribution?

- Concept: Alternating optimization strategy
  - Why needed here: To independently optimize each modality's encoder while still capturing cross-modal interactions through the shared head
  - Quick check question: What is the main difference between alternating optimization and joint optimization in terms of computational graph construction?

## Architecture Onboarding

- Component map:
  - Modality-specific encoders (one per modality)
  - Shared head (single layer connecting all modalities)
  - Gradient modification module (computes orthogonalization matrix)
  - Uncertainty calculation module (entropy-based weighting)
  - Fusion layer (weighted combination of modality predictions)

- Critical path:
  1. Data flows through modality-specific encoder
  2. Encoder output passes through shared head
  3. Loss is computed for current modality
  4. Gradients are modified using orthogonalization matrix
  5. Parameters are updated
  6. During inference, uncertainty is calculated and predictions are fused

- Design tradeoffs:
  - Alternating optimization provides better modality balance but may slow convergence compared to joint optimization
  - Gradient orthogonalization prevents forgetting but adds computational overhead
  - Uncertainty-based fusion is more robust but requires reliable uncertainty estimates

- Failure signatures:
  - If modality laziness persists: Check if alternating optimization is implemented correctly (modalities should be processed in isolation)
  - If performance degrades with missing modalities: Verify that the model can handle unpaired data during training
  - If uncertainty-based fusion underperforms: Examine whether entropy is a good proxy for prediction quality in your specific task

- First 3 experiments:
  1. Verify alternating optimization by checking that each modality's encoder is only updated when its data is present
  2. Test gradient orthogonalization by comparing weight updates with and without the modification matrix
  3. Validate uncertainty-based fusion by comparing weighted vs. unweighted prediction combination on a held-out validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MLA's performance scale with the number of modalities beyond three (audio, image, text)?
- Basis in paper: [inferred] The paper tests MLA on datasets with up to three modalities (A-I-T) but doesn't explore scenarios with more modalities.
- Why unresolved: The experiments only cover datasets with a maximum of three modalities, leaving open questions about scalability to higher-dimensional multimodal problems.
- What evidence would resolve it: Experiments testing MLA on datasets with four or more modalities, comparing performance against baselines as modality count increases.

### Open Question 2
- Question: What is the computational overhead of MLA compared to standard multimodal fusion methods during both training and inference?
- Basis in paper: [inferred] The paper focuses on accuracy improvements but doesn't provide runtime or memory usage comparisons between MLA and baseline methods.
- Why unresolved: While MLA shows accuracy gains, the paper doesn't discuss the trade-off between performance improvements and increased computational cost.
- What evidence would resolve it: Runtime and memory profiling of MLA versus baselines across all tested datasets, showing training time, inference latency, and memory consumption.

### Open Question 3
- Question: How does MLA perform on multimodal tasks requiring fine-grained temporal understanding (e.g., video understanding with audio)?
- Basis in paper: [inferred] The tested datasets (CREMA-D, Kinetic-Sound) focus on emotion recognition rather than complex temporal reasoning tasks.
- Why unresolved: The current evaluation focuses on classification tasks without exploring MLA's effectiveness on temporally complex multimodal problems.
- What evidence would resolve it: Testing MLA on video datasets requiring temporal reasoning (like action recognition or video question answering) and comparing performance against temporal-aware multimodal methods.

### Open Question 4
- Question: Does MLA's alternating optimization approach introduce any negative transfer when modalities have conflicting optimization objectives?
- Basis in paper: [explicit] The paper mentions alternating unimodal optimization but doesn't discuss scenarios where modalities might have conflicting goals.
- Why unresolved: While the paper addresses modality laziness, it doesn't explore edge cases where different modalities might be optimized toward incompatible objectives.
- What evidence would resolve it: Experiments on datasets where modalities have inherently conflicting objectives (e.g., visual aesthetics vs. semantic accuracy) to test MLA's robustness to such conflicts.

## Limitations
- The alternating optimization approach introduces significant computational overhead compared to joint training, as each modality requires separate forward and backward passes.
- The gradient modification mechanism adds complexity and may be sensitive to hyperparameter choices like the forgetting factor in the Recursive Least Squares algorithm.
- The uncertainty-based fusion strategy assumes that entropy provides a reliable proxy for prediction quality, which may not hold across all domains or when modalities have fundamentally different noise characteristics.

## Confidence

- **High Confidence**: The core hypothesis that alternating unimodal optimization can mitigate modality laziness is well-supported by the experimental results showing consistent improvements across five diverse datasets.
- **Medium Confidence**: The gradient orthogonalization mechanism's effectiveness is demonstrated empirically but the theoretical guarantees of preventing forgetting require further investigation under varying conditions.
- **Medium Confidence**: The uncertainty-based fusion strategy shows promise but its performance may be domain-dependent, particularly in cases where modality uncertainties are not well-calibrated.

## Next Checks

1. **Convergence Analysis**: Compare training convergence speed and final performance between MLA and joint optimization across varying dataset sizes to quantify the tradeoff between modality balance and computational efficiency.

2. **Ablation Study**: Remove the gradient modification mechanism and uncertainty-based fusion to isolate their individual contributions to overall performance improvements.

3. **Robustness Testing**: Evaluate MLA's performance when modality uncertainties are artificially manipulated or when modalities have highly correlated prediction errors.