---
ver: rpa2
title: 'On the Safety of Open-Sourced Large Language Models: Does Alignment Really
  Prevent Them From Being Misused?'
arxiv_id: '2310.01581'
source_url: https://arxiv.org/abs/2310.01581
tags:
- llms
- proman
- affirmative
- content
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that alignment techniques like supervised fine-tuning
  and reinforcement learning from human feedback do not prevent open-source large
  language models from being misused to generate harmful or private content. The authors
  propose a method called Probability Manipulation (ProMan) that directly manipulates
  the token generation probabilities during inference to force the model to produce
  affirmative or non-rejecting responses.
---

# On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?

## Quick Facts
- arXiv ID: 2310.01581
- Source URL: https://arxiv.org/abs/2310.01581
- Reference count: 19
- Key outcome: Alignment techniques like supervised fine-tuning and reinforcement learning from human feedback do not prevent open-source LLMs from being misused to generate harmful or private content

## Executive Summary
This paper demonstrates that alignment techniques such as supervised fine-tuning and reinforcement learning from human feedback fail to prevent open-source large language models from generating harmful or private content. The authors propose ProMan (Probability Manipulation), a method that manipulates token generation probabilities during inference by adding large positive values to specific logits, successfully generating harmful or private information from four popular open-source LLMs with attack success rates ranging from 68% to 91%. The method works by forcing affirmative responses through affirmative prefixes and preventing negative refusals through negation reversing.

## Method Summary
ProMan is an inference-time attack method that manipulates the token generation probabilities of open-source LLMs by directly modifying the logits of targeted tokens. The core technique involves adding a large positive value (δ) to specific token logits during generation, forcing the model to select predetermined tokens at key positions. The method employs two main strategies: affirmative prefixes that initialize an affirmative tone at the beginning of generation, and negation reversing that prevents the generation of negative words that might lead to rejective responses. The attack is evaluated on four 7B-parameter open-source models using the AdvBench dataset containing 520 malicious prompts across five categories.

## Key Results
- ProMan achieved attack success rates of 68-91% across four open-source LLMs for generating harmful content
- The method outperformed two baseline attacks (heuristic and optimization-based) on both harmful content generation and privacy leakage tasks
- ProMan successfully extracted private information such as names, phone numbers, and email addresses from the models

## Why This Works (Mechanism)

### Mechanism 1: Token Generation Probability Manipulation
- Claim: Direct manipulation of token generation probabilities can override safety alignment in LLMs
- Mechanism: The method adds a large positive value (δ) to specific token logits during generation, forcing the model to select predetermined tokens at key positions
- Core assumption: Safety alignment operates primarily through probability distributions rather than hard constraints on token generation
- Evidence anchors:
  - [abstract]: "Our key idea is to directly manipulate the generation process of open-sourced LLMs to misguide it to generate undesired content"
  - [section 3.2]: "z′_h+k = GM(z_h+k, t) = z_h+k + δ · m_t, where δ is a large positive value, t denotes the target token"
  - [corpus]: Weak - corpus doesn't contain direct evidence of probability manipulation effectiveness
- Break condition: If alignment mechanisms implement hard constraints that check token sequences rather than relying solely on probability distributions

### Mechanism 2: Affirmative Prefix Priming
- Claim: Starting responses with affirmative prefixes can override safety refusals
- Mechanism: The method manipulates the first few tokens to create an affirmative tone ("Sure, here is"), which influences subsequent generation to continue in an affirmative direction
- Core assumption: LLMs generate continuations based on established context and tone, so an affirmative start leads to affirmative continuation
- Evidence anchors:
  - [abstract]: "By only manipulating a few key tokens, ProMan successfully generates harmful or private information"
  - [section 3.2]: "Affirmative prefix initializes an affirmative tone at the beginning of the generation"
  - [section 4.2]: "The experimental results demonstrate that ProMan can achieve higher ASRs than baselines"
- Break condition: If the model has mechanisms to detect and override contextual manipulation regardless of initial tokens

### Mechanism 3: Negation Reversing
- Claim: Preventing negative token generation prevents safety refusals
- Mechanism: When the model attempts to generate negative words (like "sorry" or "illegal"), the method forces generation of their antonyms instead, preventing rejection patterns
- Core assumption: Safety refusals are triggered by specific negative word patterns, and avoiding these patterns prevents rejection
- Evidence anchors:
  - [abstract]: "ProMan successfully generates harmful or private information from 4 popular open-source LLMs"
  - [section 3.2]: "negation reversing prevents the victim LLM from generating negative words that may lead to a rejective response"
  - [section 4.2]: "ProMan w/o NR achieves higher ASR-H than ProMan w/o AP" showing the importance of negation reversing
- Break condition: If the model uses semantic understanding rather than keyword matching for safety decisions

## Foundational Learning

- Concept: Probability distributions in neural networks
  - Why needed here: Understanding how softmax converts logits to probabilities is essential for grasping how probability manipulation works
  - Quick check question: What happens to the probability distribution when you add a large constant to one logit value?

- Concept: Context window and generation in transformers
  - Why needed here: Understanding how transformers generate tokens sequentially and use previous tokens as context explains why affirmative prefixes work
  - Quick check question: How does the model use previously generated tokens when deciding what to generate next?

- Concept: Adversarial attacks on machine learning models
  - Why needed here: This attack is a form of adversarial attack, so understanding the general principles helps contextualize the approach
  - Quick check question: What's the difference between attacking a model through its inputs versus attacking it through its generation process?

## Architecture Onboarding

- Component map: Input → Tokenization → Initial Logits → Probability Manipulation → Token Selection → Output
- Critical path: Input → Tokenization → Initial Logits → Probability Manipulation → Token Selection → Output
- Design tradeoffs: The method trades computational overhead (checking and modifying logits) for effectiveness against alignment
- Failure signatures: The attack fails when δ is too small to overcome the model's natural probability distribution, or when the model has hard-coded refusal patterns
- First 3 experiments:
  1. Test different δ values to find the minimum effective threshold
  2. Test which tokens in the affirmative prefix are most critical for success
  3. Test the attack on models with different alignment techniques to see which are most vulnerable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ProMan's probability manipulation technique be adapted to work on closed-source or API-restricted large language models that do not provide white-box access to model parameters?
- Basis in paper: [explicit] The paper discusses that existing defenses like prompt filtering are only applicable when the LLM is closed-sourced and the attacker has limited query access, implying that current techniques may not work on such models.
- Why unresolved: The paper only evaluates ProMan on open-source models with white-box access, leaving the effectiveness on closed-source models unexplored.
- What evidence would resolve it: Experiments testing ProMan's performance on closed-source models or models with restricted access would clarify its applicability.

### Open Question 2
- Question: How effective are the proposed countermeasures (pre-training data filtering and post-training model editing) in mitigating ProMan attacks on large language models?
- Basis in paper: [explicit] The paper discusses potential countermeasures like pre-training data filtering and post-training model editing but does not evaluate their effectiveness against ProMan.
- Why unresolved: The paper only provides a theoretical discussion of countermeasures without empirical validation.
- What evidence would resolve it: Empirical studies testing the effectiveness of these countermeasures against ProMan would provide concrete insights.

### Open Question 3
- Question: What is the minimum number of tokens that need to be manipulated by ProMan to achieve a successful attack, and how does this vary across different large language models?
- Basis in paper: [inferred] The paper mentions that ProMan manipulates only a few key tokens but does not specify the exact number or how this varies across models.
- Why unresolved: The paper does not provide detailed analysis on the token manipulation requirements for different models.
- What evidence would resolve it: Detailed experiments analyzing the number of tokens manipulated and their impact on attack success rates across various models would clarify this.

## Limitations
- Evaluation limited to four 7B-parameter open-source models, restricting generalizability to larger or proprietary systems
- AdvBench dataset contains 520 prompts specifically curated to test the attack, raising questions about real-world applicability
- Generated harmful content may be syntactically correct but semantically nonsensical, requiring human evaluation for coherence assessment

## Confidence

**High Confidence (8-10/10)**: The core mechanism of probability manipulation during inference is well-established in adversarial machine learning literature. The experimental setup is clearly described, and the attack success rates (68-91%) are reported with statistical measures.

**Medium Confidence (5-7/10)**: Claims about ProMan being "more effective" than baselines require careful interpretation, as the comparison is limited to two specific baseline methods under constrained conditions.

**Low Confidence (1-4/10)**: The generalizability of results to production systems, larger models (e.g., 70B+ parameters), or proprietary models like GPT-4 remains uncertain.

## Next Checks

1. **Transferability Test**: Apply ProMan to models with different safety training paradigms (constitutional AI, debate-based alignment, or supervised fine-tuning with different datasets) to determine which alignment approaches are most vulnerable to probability manipulation attacks.

2. **Robustness Analysis**: Systematically vary the δ parameter across multiple orders of magnitude to identify the minimum effective value and determine whether models can develop resistance to probability manipulation through architectural changes rather than just training modifications.

3. **Human Evaluation**: Conduct blinded human assessments to determine whether the harmful content generated by ProMan is coherent, useful, and genuinely harmful versus syntactically correct but semantically nonsensical outputs.