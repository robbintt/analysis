---
ver: rpa2
title: Towards Optimal Patch Size in Vision Transformers for Tumor Segmentation
arxiv_id: '2308.16598'
source_url: https://arxiv.org/abs/2308.16598
tags:
- size
- patch
- segmentation
- tumor
- liver
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a method to determine optimal patch size for
  vision transformer-based models in tumor segmentation. By leveraging the average
  volume size of metastasis lesions, the method calculates the ideal patch size for
  input multi-resolution images.
---

# Towards Optimal Patch Size in Vision Transformers for Tumor Segmentation

## Quick Facts
- arXiv ID: 2308.16598
- Source URL: https://arxiv.org/abs/2308.16598
- Reference count: 23
- One-line primary result: Proposed method determines optimal patch size for vision transformers in tumor segmentation based on average tumor volume, achieving improved Dice similarity coefficient performance.

## Executive Summary
This paper addresses the challenge of optimal patch size selection for vision transformer-based models in tumor segmentation, particularly for small lesions. The authors propose a technique that calculates ideal patch size by leveraging the average volume size of metastasis lesions. By combining this approach with transfer learning - pre-training on larger tumor volumes with optimal patch size followed by training on smaller ones - they demonstrate consistent and improved segmentation performance, especially for small liver metastases. The method provides a systematic framework for optimizing semantic segmentation of small objects using vision transformers.

## Method Summary
The method involves determining optimal patch size for vision transformers based on average tumor volume, then employing transfer learning to improve segmentation performance. The approach uses two datasets: LiTS (public) with larger tumors for pre-training and mCRC (private) with smaller tumors for fine-tuning. The optimal patch size is calculated using a mathematical relationship between tumor volume and patch size. The UNETR architecture is used as the base model, trained with a combination of soft Dice loss and cross-entropy loss. The framework systematically evaluates different patch sizes and validates the effectiveness of pre-training on larger tumors for improved segmentation of smaller tumors.

## Key Results
- Optimal patch size selection based on average tumor volume significantly improves segmentation performance
- Transfer learning from larger tumor volumes to smaller ones achieves highest Dice similarity coefficient
- Consistent improvements demonstrated across both public (LiTS) and private (mCRC) colorectal liver metastases datasets
- Method shows particular effectiveness in segmenting small liver metastases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal patch size improves segmentation performance by matching receptive field to tumor size
- Mechanism: Mathematical relationship between average tumor volume and patch size ensures patch size is well-suited to tumor scale
- Core assumption: Average tumor volume is a reliable proxy for determining appropriate receptive field size
- Evidence anchors: [abstract], [section] - mathematical framework for patch size calculation
- Break condition: If average tumor volume doesn't represent tumor size distribution

### Mechanism 2
- Claim: Pre-training on larger tumor volumes improves segmentation performance on smaller tumors
- Mechanism: Transfer learning allows model to learn general tumor representations transferable to smaller tumors
- Core assumption: Features learned from larger tumors are beneficial for smaller tumor segmentation
- Evidence anchors: [abstract], [section] - validation of transfer learning approach
- Break condition: If source and target datasets are too dissimilar

### Mechanism 3
- Claim: Vision transformers capture long-range dependencies but are sensitive to patch size
- Mechanism: Finding optimal patch size enables better leverage of transformer advantages for tumor segmentation
- Core assumption: Sensitivity to patch size is significant factor in transformer performance
- Evidence anchors: [abstract], [section] - discussion of patch size sensitivity
- Break condition: If tumor sizes are similar, patch size sensitivity becomes less critical

## Foundational Learning

- Concept: Vision Transformers (ViTs)
  - Why needed here: Paper focuses on using ViTs for tumor segmentation
  - Quick check question: What is the main difference between ViTs and traditional convolutional neural networks in terms of how they process input images?

- Concept: Transfer Learning
  - Why needed here: Paper employs transfer learning approach to improve segmentation performance
  - Quick check question: In the context of this paper, what is the source dataset and what is the target dataset for transfer learning?

- Concept: Dice Similarity Coefficient (DSC)
  - Why needed here: DSC is primary metric used to evaluate segmentation performance
  - Quick check question: What does a higher DSC value indicate in the context of tumor segmentation?

## Architecture Onboarding

- Component map: Input CT volumes (H, W, L) -> Preprocessing (resampling, augmentation) -> UNETR model -> Soft Dice loss + Cross-entropy loss -> Segmented liver/tumor regions

- Critical path: 1) Preprocess input CT volumes 2) Calculate optimal patch size 3) Train on LiTS dataset with optimal patch size 4) Fine-tune on mCRC dataset 5) Evaluate using DSC

- Design tradeoffs:
  - Patch size selection: Larger patches capture more context but may miss small tumor details; smaller patches are better for small tumors but may lack global context
  - Pre-training vs. training from scratch: Pre-training can improve performance but requires suitable source dataset

- Failure signatures:
  - Low DSC: Indicates poor segmentation performance, possibly due to suboptimal patch size or insufficient training
  - High computational cost: May result from using large patch sizes or insufficient hardware resources

- First 3 experiments:
  1. Vary patch size (8, 12, 16, 24) on LiTS dataset and observe DSC performance
  2. Train model from scratch on mCRC dataset with different patch sizes and compare to pre-trained model
  3. Experiment with different combinations of source and target datasets for pre-training to assess transferability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal patch size for different tumor types beyond liver metastasis?
- Basis in paper: [explicit] Framework could be used for other small object segmentation tasks
- Why unresolved: Only validated on liver metastasis tumors
- What evidence would resolve it: Testing framework on datasets with different tumor types and comparing performance

### Open Question 2
- Question: How does optimal patch size vary with tumor heterogeneity and texture?
- Basis in paper: [inferred] Framework relies on average tumor volume size but doesn't consider heterogeneity or texture
- Why unresolved: Paper doesn't explore relationship between tumor heterogeneity, texture, and optimal patch size
- What evidence would resolve it: Analyzing tumor heterogeneity and texture metrics in conjunction with optimal patch size determination

### Open Question 3
- Question: Can the framework be extended to multi-organ segmentation with varying object sizes?
- Basis in paper: [explicit] Optimal patch size showed best performance on large objects like liver
- Why unresolved: Paper focuses on single organ (liver) and tumor segmentation, not multi-organ scenarios
- What evidence would resolve it: Applying framework to datasets with multiple organs of varying sizes

## Limitations

- Average tumor volume may not be representative of tumor size distribution in dataset
- Transfer learning effectiveness depends on similarity between source and target datasets
- Method validated only on liver metastasis tumors, limiting generalizability

## Confidence

- High Confidence: UNETR architecture and use of Dice Similarity Coefficient are well-established in medical image segmentation
- Medium Confidence: Proposed method for optimal patch size selection based on average tumor volume is reasonable but may not be optimal for all datasets
- Low Confidence: Effectiveness of transfer learning approach in improving segmentation performance on smaller tumors requires further validation on diverse datasets

## Next Checks

1. Evaluate patch size sensitivity: Perform thorough analysis of segmentation performance across wider range of patch sizes to assess model sensitivity to input patch size

2. Validate transferability: Conduct experiments to evaluate transferability of features learned from larger tumors to smaller tumors using different source and target datasets

3. Compare with alternative methods: Compare proposed method with other state-of-the-art approaches for tumor segmentation such as 3D U-Net or other transformer-based models