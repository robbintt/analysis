---
ver: rpa2
title: Distractor generation for multiple-choice questions with predictive prompting
  and large language models
arxiv_id: '2307.16338'
source_url: https://arxiv.org/abs/2307.16338
tags:
- distractors
- question
- distractor
- language
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates generating high-quality distractors for
  multiple-choice questions using large language models (LLMs), specifically ChatGPT.
  The proposed approach, Dynamic-Demo-ChatGPT, combines retrieval-based and generative
  methods by dynamically retrieving similar question items from a question bank and
  using them as in-context examples to guide ChatGPT in generating distractors.
---

# Distractor generation for multiple-choice questions with predictive prompting and large language models

## Quick Facts
- arXiv ID: 2307.16338
- Source URL: https://arxiv.org/abs/2307.16338
- Reference count: 40
- Key outcome: Dynamic-Demo-ChatGPT generates high-quality distractors, with 53% rated as good and only 15% as nonsense by teachers

## Executive Summary
This paper addresses the challenge of generating high-quality distractors for multiple-choice questions (MCQs) using large language models (LLMs). The authors propose Dynamic-Demo-ChatGPT, a novel approach that combines retrieval-based and generative methods by dynamically retrieving similar question items from a question bank and using them as in-context examples to guide ChatGPT in generating distractors. A user study with 10 teachers demonstrates that this approach significantly outperforms existing methods, achieving higher rates of good distractors and lower rates of nonsense distractors compared to mT5 and standard ChatGPT approaches.

## Method Summary
The Dynamic-Demo-ChatGPT approach uses a BERT-based ranking model (Q-SIM) to retrieve semantically similar question items from the Televic dataset, which contains 62K MCQs. These retrieved items are then used as in-context examples in the ChatGPT prompt, providing relevant context and examples of good distractors. The model is evaluated against mT5 (a fine-tuned baseline) and ChatGPT in zero-shot and few-shot settings using the Wezooz test dataset (300 MCQs). Human experts rate generated distractors on a four-level scale, with metrics including Good Distractor Rate (GDR@10) and Nonsense Distractor Rate (NDR@10).

## Key Results
- Dynamic-Demo-ChatGPT achieved an average of 53% of generated distractors rated as high-quality (good distractors)
- The approach significantly reduced nonsense distractors to only 15%, outperforming state-of-the-art models
- Dynamic-Demo-ChatGPT outperformed both mT5 and zero-shot/few-shot ChatGPT approaches in teacher evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic retrieval of in-context examples improves distractor quality by providing task-relevant context to the LLM.
- Mechanism: The Dynamic-Demo-ChatGPT model uses a BERT-based ranking model (Q-SIM) to retrieve question items from the Televic dataset that are semantically similar to the test question. These retrieved items are then used as in-context examples in the ChatGPT prompt, providing relevant context and examples of good distractors.
- Core assumption: Similar question items contain relevant distractors that can guide the LLM in generating appropriate distractors for the test question.
- Evidence anchors:
  - [abstract] "by prompting them with question items automatically retrieved from a question bank as well-chosen in-context examples"
  - [section] "We accomplish this by leveraging the question similarity (Q-SIM) model proposed by [3] to automatically select the top similar question items for the given test instance."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.359, average citations=0.0. Top related titles: Assessing Distractors in Multiple-Choice Tests, Generating Plausible Distractors for Multiple-Choice Questions via Student Choice Prediction, Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation. (Weak corpus support for this specific mechanism)

### Mechanism 2
- Claim: ChatGPT's ability to generate contextually appropriate distractors is enhanced by providing it with task-specific instructions and examples.
- Mechanism: The zero-shot and few-shot ChatGPT models are provided with specific instructions to generate plausible but incorrect answers for the given question. The few-shot model is further guided by dynamically retrieved in-context examples.
- Core assumption: LLMs can understand and follow instructions to generate contextually appropriate text, and providing examples further improves their performance.
- Evidence anchors:
  - [abstract] "by prompting them with question items automatically retrieved from a question bank as well-chosen in-context examples"
  - [section] "Our mT5 model's input sequence is constructed by copying the question stem and answer from the original question item and inserting the sentence 'Which of the following are incorrect answers' (or its translation depending on the language of the question item) between them."
  - [corpus] Weak corpus support for this specific mechanism

### Mechanism 3
- Claim: The combination of a ranking-based approach and an LLM improves the reliability of distractor generation by reducing the occurrence of nonsense distractors.
- Mechanism: The Dynamic-Demo-ChatGPT model combines the Q-SIM model's ability to retrieve relevant question items with ChatGPT's ability to generate contextually appropriate distractors. This combination results in a more reliable distractor generation process.
- Core assumption: The ranking-based approach can provide useful context to the LLM, and the LLM can effectively use this context to generate better distractors.
- Evidence anchors:
  - [abstract] "We also show the gains of our approach1 in generating high-quality distractors by comparing it with a zero-shot ChatGPT and a few-shot ChatGPT prompted with static examples."
  - [section] "We found that on average 53% of the generated distractors presented to the teachers were rated as high-quality, i.e., suitable for immediate use as is, outperforming the state-of-the-art model."
  - [corpus] Weak corpus support for this specific mechanism

## Foundational Learning

- Concept: Large Language Models (LLMs)
  - Why needed here: LLMs, such as ChatGPT, are used to generate distractors for multiple-choice questions. Understanding their capabilities and limitations is crucial for effectively using them in this task.
  - Quick check question: What are the key differences between zero-shot, few-shot, and fine-tuning approaches when using LLMs for a specific task?

- Concept: In-context learning
  - Why needed here: In-context learning, where the LLM is provided with examples in the prompt, is used to guide the LLM in generating appropriate distractors. Understanding how to effectively use in-context examples is important for optimizing the performance of the Dynamic-Demo-ChatGPT model.
  - Quick check question: How does the choice of in-context examples affect the performance of an LLM in a given task?

- Concept: Question similarity and retrieval
  - Why needed here: The Q-SIM model is used to retrieve similar question items from the Televic dataset, which are then used as in-context examples for the LLM. Understanding how to effectively measure and retrieve similar questions is crucial for the success of the Dynamic-Demo-ChatGPT model.
  - Quick check question: What are some common approaches for measuring the similarity between questions, and how do they differ in terms of effectiveness and computational cost?

## Architecture Onboarding

- Component map: Q-SIM model -> Televic dataset -> mT5 model -> ChatGPT -> Dynamic-Demo-ChatGPT
- Critical path: 1) Retrieve similar question items from Televic dataset using Q-SIM model. 2) Construct ChatGPT prompt with retrieved items as in-context examples. 3) Generate distractors using ChatGPT. 4) Evaluate quality through human expert assessment.
- Design tradeoffs: Using a ranking-based approach to retrieve in-context examples provides more control over quality and relevance but may be more computationally expensive than using static examples. Larger numbers of in-context examples may improve performance but increase computational cost and risk of overfitting.
- Failure signatures: Q-SIM model fails to retrieve relevant items, in-context examples are not representative of good distractors, or LLM fails to understand instructions or examples.
- First 3 experiments: 1) Evaluate Q-SIM model's performance in retrieving relevant question items. 2) Compare ChatGPT performance across zero-shot, few-shot with static examples, and few-shot with dynamically retrieved examples. 3) Analyze impact of number and quality of in-context examples on ChatGPT's distractor generation performance.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored:
- How does the performance vary when using different retrieval models for selecting in-context examples?
- What is the impact of increasing the number of in-context examples beyond 5 on distractor generation quality?
- How does distractor quality vary across different subject domains and difficulty levels?
- What is the long-term effectiveness of using LLM-generated distractors in actual educational assessments compared to human-created distractors?

## Limitations
- Evaluation relies heavily on subjective human judgment through teacher annotation, which introduces potential biases and limits generalizability
- The study does not provide inter-annotator agreement statistics or discuss how teacher expertise levels might affect results
- The claim that dynamically retrieved examples outperform static examples assumes the retrieval model consistently finds relevant distractors, but the paper doesn't explore failure cases

## Confidence
- High confidence: Dynamic-Demo-ChatGPT outperforms mT5 and zero-shot/few-shot ChatGPT approaches in generating distractors rated as "good" by teachers
- Medium confidence: The mechanism by which dynamic retrieval of in-context examples improves distractor quality
- Medium confidence: The generalizability of the approach beyond the specific educational contexts tested

## Next Checks
1. Conduct an ablation study to quantify the individual contributions of the Q-SIM retrieval component versus the ChatGPT generation component by testing variations with and without dynamic retrieval.
2. Implement a cross-domain validation using a different question corpus to test whether the Dynamic-Demo-ChatGPT approach maintains its performance advantage across varied educational subjects and difficulty levels.
3. Design an experiment comparing distractor quality when using different similarity thresholds in the Q-SIM model to determine the optimal balance between retrieval relevance and coverage.