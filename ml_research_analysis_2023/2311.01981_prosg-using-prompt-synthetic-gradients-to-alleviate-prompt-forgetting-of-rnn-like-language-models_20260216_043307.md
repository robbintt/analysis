---
ver: rpa2
title: 'ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of
  RNN-like Language Models'
arxiv_id: '2311.01981'
source_url: https://arxiv.org/abs/2311.01981
tags:
- prompt
- gradient
- which
- generation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of prompt forgetting in RNN-like
  language models during generation. The authors propose a method called ProSG (Prompt
  Synthetic Gradient) that uses synthetic gradients derived from prompt-encoded states
  to temporarily modify model parameters, thereby enhancing prompt memorization.
---

# ProSG: Using Prompt Synthetic Gradients to Alleviate Prompt Forgetting of RNN-like Language Models

## Quick Facts
- arXiv ID: 2311.01981
- Source URL: https://arxiv.org/abs/2311.01981
- Reference count: 3
- RWKV-4-0.4B with ProSG achieves perplexity of 3.161 and accuracy of 0.761, outperforming the base model

## Executive Summary
This paper addresses the critical problem of prompt forgetting in RNN-like language models during generation. The authors propose ProSG (Prompt Synthetic Gradient), a method that uses synthetic gradients derived from prompt-encoded states to temporarily modify model parameters, thereby enhancing prompt memorization. They construct a multi-stage instruction dataset (MuSI) with 22k instruction-answer pairs for training and evaluation. Experiments show that RWKV-4-0.4B with ProSG achieves strong performance metrics, outperforming the base model and approaching the performance of larger transformer-based models like Vicuna-7B and ChatGLM-6B. Human evaluation also demonstrates significant improvements in fluency, accuracy, and quality of generated outputs.

## Method Summary
ProSG uses synthetic gradients to temporarily modify model parameters during generation, hard-coding prompt information into the model. The method derives states that encode the prompt, transforms them into parameter modifications using low-rank gradient approximation, and applies these modifications during generation. A state embedding mechanism enables a single gradient synthesis module to handle different parameter types with distinct updates. The training procedure uses a two-phase approach: first fine-tuning the backbone language model, then training the gradient synthesis module while keeping the backbone frozen to enhance its memory capability.

## Key Results
- RWKV-4-0.4B with ProSG achieves perplexity of 3.161 and accuracy of 0.761
- Performance approaches that of transformer-based models (Vicuna-7B, ChatGLM-6B) despite using a much smaller 0.4B parameter model
- Human evaluation shows significant improvements in fluency, accuracy, and quality of generated outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ProSG uses synthetic gradients to temporarily modify model parameters, hard-coding prompt information into the model.
- Mechanism: The method derives states that encode the prompt, transforms them into parameter modifications using low-rank gradient approximation, and applies these modifications during generation.
- Core assumption: Minor modifications to model parameters are sufficient to achieve prompt memorization that enhances generation.
- Evidence anchors: [abstract] "We derive the states that encode the prompt, then transform it into model parameter modification using low-rank gradient approximation, which hard-codes the prompt into model parameters temporarily."

### Mechanism 2
- Claim: State embedding enables the same gradient synthesis module to handle different parameter types with distinct updates.
- Mechanism: Different learned embedding vectors are added to the input before processing, allowing a single module to generate synthetic gradients with different properties for each parameter type.
- Core assumption: Different parameters should receive different updates when calculating prompt synthetic gradient to properly encode all aspects of the prompt.
- Evidence anchors: [section] "To overcome this issue, we introduced state embedding. State embedding is a set of learned embedding vectors for each different state that store in each layer, which will be added to the input x□ before being fed to the core module"

### Mechanism 3
- Claim: Two-phase training improves memory capability for prompts.
- Mechanism: First phase optimizes the base model parameters, second phase freezes those parameters and optimizes only the gradient synthesis module to enhance its memory capacity.
- Core assumption: The gradient synthesis module can be independently optimized to better encode prompt information without degrading the base model's performance.
- Evidence anchors: [section] "We used a two-stage training approach, where the first stage involved fine-tuning the backbone language model, which a training loss, and the second stage focused on training the gradient synthesis module to further enhance its memory capability"

## Foundational Learning

- Concept: Synthetic gradients and their role in decoupling neural modules
  - Why needed here: ProSG relies on synthetic gradients to modify parameters during generation without full backpropagation
  - Quick check question: How do synthetic gradients differ from traditional backpropagation gradients, and why is this distinction important for RNN-like models?

- Concept: Low-rank adaptation (LoRA) and its computational benefits
  - Why needed here: ProSG uses low-rank approximation to reduce memory overhead when calculating prompt synthetic gradients
  - Quick check question: What is the mathematical intuition behind low-rank approximation, and how does it reduce computational complexity?

- Concept: Recurrent neural networks and their inherent forgetting problem
  - Why needed here: The paper specifically addresses prompt forgetting in RNN-like models due to fixed-length state vectors
  - Quick check question: Why do RNNs with fixed-length state vectors inherently struggle with long-term memory compared to transformers?

## Architecture Onboarding

- Component map: RWKV backbone → State embedding layer → Transforming stack (gradient synthesis module) → Output low-rank matrix → Parameter modification
- Critical path: Prompt encoding → State extraction → Gradient synthesis → Parameter modification → Generation
- Design tradeoffs: Using the same gradient synthesis module for all parameter types reduces parameters but requires state embedding; two-phase training adds complexity but improves performance
- Failure signatures: Generation still forgets prompts despite ProSG; training instability or divergence; excessive memory usage during inference
- First 3 experiments:
  1. Verify that state embedding vectors are being properly learned by examining their values after training
  2. Test synthetic gradient generation on a simple prompt to ensure it produces reasonable parameter modifications
  3. Compare generation outputs with and without ProSG on a held-out prompt to confirm the architecture is having an effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of ProSG scale with model size, particularly for larger models like 13B or 33B parameters?
- Basis in paper: [explicit] The authors explicitly state that due to resource limitations, they were unable to conduct experiments on larger language models such as 13B or 33B.
- Why unresolved: The paper only demonstrates results on a 0.4B parameter RWKV model, leaving uncertainty about performance on larger models.
- What evidence would resolve it: Experiments applying ProSG to larger language models (e.g., 13B, 33B) with performance comparisons to their non-ProSG counterparts.

### Open Question 2
- Question: How does ProSG perform on non-English languages or multilingual prompts?
- Basis in paper: [inferred] The paper focuses on English prompts and does not mention multilingual capabilities or testing on non-English datasets.
- Why unresolved: The MuSI dataset and evaluation are conducted entirely in English, with no indication of cross-lingual performance.
- What evidence would resolve it: Experiments evaluating ProSG on multilingual datasets or prompts in languages other than English, comparing performance across languages.

### Open Question 3
- Question: What is the computational overhead of ProSG during inference compared to standard RNN-like models?
- Basis in paper: [inferred] The paper describes the architecture and benefits of ProSG but does not provide quantitative data on inference time or computational cost compared to baseline models.
- Why unresolved: While the paper demonstrates performance improvements, it lacks a detailed analysis of the trade-off between performance gains and computational overhead.
- What evidence would resolve it: Benchmark results showing inference time, memory usage, and computational cost of ProSG versus standard RNN-like models, ideally with scaling analysis across different sequence lengths.

## Limitations
- ProSG introduces computational overhead during generation due to prompt encoding and gradient synthesis
- Method is specifically designed for RNN-like models and may not transfer directly to other architectures like transformers
- Two-phase training procedure adds complexity and may be difficult to scale to larger models or datasets

## Confidence

**High Confidence Claims:**
- ProSG reduces prompt forgetting in RNN-like language models during generation
- The MuSI dataset is effective for training and evaluating multi-stage instruction following
- RWKV-4-0.4B with ProSG outperforms the base model on both perplexity and accuracy metrics

**Medium Confidence Claims:**
- ProSG performance approaches that of transformer-based models like Vicuna-7B and ChatGLM-6B
- The two-phase training approach significantly improves the memory capability of the gradient synthesis module
- Human evaluation results are statistically significant and demonstrate real-world improvements

**Low Confidence Claims:**
- The specific contribution of each architectural component (state embedding, gradient synthesis module) to overall performance
- Generalization of ProSG to prompts outside the MuSI dataset domain
- Scalability of the method to larger models beyond RWKV-4-0.4B

## Next Checks

1. **Ablation Study**: Systematically remove components of the ProSG architecture (state embedding, gradient synthesis module, low-rank approximation) to quantify their individual contributions to performance improvements.

2. **Cross-Dataset Generalization**: Evaluate ProSG on established multi-task datasets like FLAN, MMLU, and HumanEval to assess whether the method generalizes beyond the MuSI dataset and maintains performance across different domains.

3. **Memory-Usage Analysis**: Measure the actual memory overhead introduced by ProSG during inference across different prompt lengths and complexities to quantify the computational trade-offs compared to baseline models.