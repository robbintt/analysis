---
ver: rpa2
title: Experimenting with Large Language Models and vector embeddings in NASA SciX
arxiv_id: '2312.14211'
source_url: https://arxiv.org/abs/2312.14211
tags:
- nasa
- user
- scix
- llms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors explore using open-source large language models (LLMs)
  to augment information retrieval in the NASA SciX digital library while avoiding
  hallucination and preserving privacy. They implement a Retrieval Augmented Generation
  (RAG) approach: user queries are transformed into context-rich prompts that include
  relevant snippets from scientific abstracts or full-text paragraphs.'
---

# Experimenting with Large Language Models and vector embeddings in NASA SciX

## Quick Facts
- arXiv ID: 2312.14211
- Source URL: https://arxiv.org/abs/2312.14211
- Reference count: 2
- Primary result: RAG approach reduces hallucination compared to LLM-only responses; full-text snippets yield richer answers

## Executive Summary
This paper presents an exploratory implementation of Retrieval Augmented Generation (RAG) using open-source LLMs to enhance information retrieval in the NASA SciX digital library. The authors developed a system that grounds LLM responses in retrieved scientific text snippets to reduce hallucination while preserving privacy. The implementation includes both traditional search-based retrieval and semantic vector similarity approaches. Subjective evaluation suggests RAG effectively reduces hallucination and that full-text snippets provide more detailed, specific responses than abstract-only snippets. However, the work remains exploratory without quantitative metrics or public deployment.

## Method Summary
The authors implemented a RAG pipeline that augments LLM queries with relevant scientific literature snippets. They created semantic embeddings for scientific paragraphs using BAAI/bge-small-en and stored them in PostgreSQL with pg_vector extension. The system retrieves context via either NASA SciX's search engine (with translated queries) or semantic vector similarity. Retrieved snippets are incorporated into carefully structured prompts with simulated conversations to guide the LLM toward using the provided context. The Zephyr 7B model is used for inference on a single GPU. Evaluation is limited to subjective human assessment comparing hallucination rates and response quality.

## Key Results
- RAG reduces hallucination compared to LLM-only responses based on subjective evaluation
- Full-text snippets produce richer, more specific answers than abstract-only snippets
- Semantic vector similarity enables efficient context retrieval for scientific text
- Carefully structured prompts with simulated conversations guide LLMs to use provided context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG reduces hallucination by grounding LLM responses in retrieved text snippets
- Mechanism: The LLM receives both the user query and relevant context from scientific literature, shifting generation from pure pattern matching to context-informed synthesis
- Core assumption: The retrieved snippets contain accurate, relevant information that the LLM can use to construct truthful responses
- Evidence anchors: Subjective human evaluation shows lower hallucination rates; retrieved snippets are incorporated into prompts
- Break condition: Retrieved snippets are irrelevant, misleading, or contain the same hallucinations the RAG system is trying to avoid

### Mechanism 2
- Claim: Semantic vector embeddings enable efficient retrieval of contextually relevant paragraphs
- Mechanism: Text paragraphs are converted to vectors using BAAI/bge-small-en model, stored in PostgreSQL/pg_vector, and queried using vector similarity to find semantically related content
- Core assumption: The embedding model captures semantic meaning effectively for scientific text retrieval
- Evidence anchors: System creates embeddings for all open access articles and stores them in PostgreSQL; no quantitative retrieval metrics reported
- Break condition: The embedding model fails to capture semantic relationships in scientific text or the vector similarity metric doesn't align with relevance

### Mechanism 3
- Claim: Structured prompt engineering with simulated conversation guides LLM to use provided context effectively
- Mechanism: The prompt includes system instructions, user query, simulated assistant interactions that reference the retrieved snippets, and finally the assistant response
- Core assumption: The LLM follows the conversational structure and prioritizes the provided snippets over its pretraining knowledge
- Evidence anchors: Multiple prompt strategies tested; simulated conversation format found most effective subjectively
- Break condition: The LLM ignores the provided context and defaults to pretraining knowledge, or the simulated conversation format confuses rather than guides the model

## Foundational Learning

- Concept: Vector similarity search and embedding models
  - Why needed here: Understanding how semantic vectors represent text meaning and how similarity metrics retrieve relevant content is crucial for implementing and debugging the semantic search approach
  - Quick check question: How does the cosine similarity between two text embeddings relate to their semantic similarity, and what distance threshold typically indicates relevance?

- Concept: Prompt engineering and few-shot learning
  - Why needed here: The RAG implementation relies on carefully structured prompts with simulated conversations to guide the LLM toward using provided context
  - Quick check question: What role does the system message play in the prompt structure, and how do the simulated assistant interactions influence the LLM's generation?

- Concept: Information retrieval fundamentals and search syntax
  - Why needed here: The traditional NASA SciX search approach uses Apache Solr syntax, requiring understanding of fielded search, Boolean operators, and query optimization
  - Quick check question: How does translating natural language queries into structured search syntax (author, abs, year fields) improve precision in scientific literature retrieval?

## Architecture Onboarding

- Component map: User interface → Query processing → Context retrieval (NASA SciX search OR vector similarity) → Prompt construction → LLM inference → Response delivery
- Critical path: User submits query → Query translated to NASA SciX syntax OR converted to vector → Relevant snippets retrieved → Prompt constructed with system message, user query, and simulated conversation including snippets → Prompt sent to LLM for generation → Response returned to user
- Design tradeoffs: Single GPU limits concurrent requests vs. deployment simplicity; subjective human evaluation vs. automated metrics; abstract-only snippets vs. full-text snippets (tradeoff between relevance and information richness); open-source models (privacy, cost) vs. potential performance limitations
- Failure signatures: LLM consistently ignores provided context and hallucinates; vector similarity retrieves irrelevant paragraphs (embedding model issues); NASA SciX query translation fails or produces invalid syntax; system becomes unresponsive due to GPU bottleneck under load
- First 3 experiments: 1) Test prompt structure variations: Compare performance when system message is removed, when simulated conversation is shortened, or when snippet placement in prompt changes 2) Evaluate embedding model effectiveness: Compare BAAI/bge-small-en against other embedding models on a sample of scientific paragraphs using human relevance judgment 3) Benchmark retrieval approaches: Run A/B tests where some queries use NASA SciX search and others use vector similarity, measuring hallucination rates and response quality subjectively

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the quantitative impact of Retrieval Augmented Generation (RAG) on hallucination reduction compared to baseline LLM responses?
- Basis in paper: The paper states "Based on a non-systematic human evaluation, the experiment shows a lower degree of hallucination" but no metrics are provided
- Why unresolved: Only subjective evaluation was performed, no systematic metrics or controlled experiments were conducted
- What evidence would resolve it: A controlled experiment measuring hallucination frequency with/without RAG across standardized question sets

### Open Question 2
- Question: How do abstract-based snippets compare to full-text paragraphs in terms of response quality and specificity?
- Basis in paper: "we observed that the use of full-text snippets improves the overall quality of the generated responses, making them more detailed and specific"
- Why unresolved: The paper only provides qualitative observations without systematic comparison or metrics
- What evidence would resolve it: Controlled experiments comparing responses generated from abstract-only vs full-text snippets across standardized metrics

### Open Question 3
- Question: What is the computational overhead of semantic vector search versus traditional NASA SciX search for retrieving relevant snippets?
- Basis in paper: The paper describes implementing both traditional search and semantic vector approaches but doesn't compare their performance characteristics
- Why unresolved: No timing or resource usage data was collected or reported for the two approaches
- What evidence would resolve it: Benchmark testing measuring query latency and resource utilization for both search methods

## Limitations
- Evaluation relies solely on subjective human assessment without quantitative metrics or systematic comparison
- No public deployment or external validation of the system's effectiveness
- Single GPU deployment suggests potential scalability limitations for concurrent requests
- Lack of controlled experiments makes it difficult to draw definitive conclusions about RAG effectiveness

## Confidence

**High Confidence:** Technical implementation details are clearly described and reproducible. The RAG architecture, prompt engineering approach, and vector embedding pipeline are well-specified with concrete technical details about the BAAI/bge-small-en model and PostgreSQL/pg_vector integration.

**Medium Confidence:** Subjective evaluation suggesting RAG reduces hallucination compared to LLM-only responses has some support but lacks quantitative backing. The observation that full-text snippets provide richer answers is plausible but not systematically verified.

**Low Confidence:** Claims about the superiority of the semantic vector approach over traditional search are weakly supported, as the paper doesn't provide comparative metrics or head-to-head retrieval performance data.

## Next Checks

1. Implement quantitative hallucination metrics: Develop an automated evaluation framework using fact-checking against ground truth scientific literature to measure hallucination rates for RAG vs. LLM-only approaches with statistical significance testing.

2. Conduct systematic retrieval quality analysis: Compare the semantic vector approach against NASA SciX traditional search using mean average precision (MAP) and normalized discounted cumulative gain (nDCG) metrics on a labeled dataset of query-relevance pairs.

3. Perform prompt engineering ablation study: Systematically test variations in prompt structure (removing system message, changing snippet placement, modifying conversation format) while measuring response quality, relevance, and hallucination rates using both human and automated evaluation methods.