---
ver: rpa2
title: 'QACHECK: A Demonstration System for Question-Guided Multi-Hop Fact-Checking'
arxiv_id: '2310.07609'
source_url: https://arxiv.org/abs/2310.07609
tags:
- claim
- question
- reasoning
- answer
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces QACHECK, a system for fact-checking complex
  claims using question-guided multi-hop reasoning. It addresses the limitation of
  existing fact-checking systems that lack transparency in their reasoning processes.
---

# QACHECK: A Demonstration System for Question-Guided Multi-Hop Fact-Checking

## Quick Facts
- arXiv ID: 2310.07609
- Source URL: https://arxiv.org/abs/2310.07609
- Authors: 
- Reference count: 14
- Primary result: QACHECK achieves macro-F1 scores of 55.67 (2-hop), 54.67 (3-hop), 52.35 (4-hop) on HOVER and 59.47 on FEVEROUS

## Executive Summary
QACHECK introduces a novel approach to multi-hop fact-checking by guiding reasoning through question decomposition. The system breaks complex claims into simpler, verifiable sub-questions, iteratively retrieving evidence and validating each step. Using in-context learning with InstructGPT, QACHECK provides transparent, step-by-step verification while achieving competitive performance on benchmark datasets. The system addresses the limitation of existing fact-checking systems that lack transparency in their reasoning processes.

## Method Summary
QACHECK is a five-module pipeline that guides fact-checking through question decomposition. Users input a claim, and the system iteratively generates relevant questions, retrieves evidence, validates QA pairs, and makes final veracity predictions. The architecture uses InstructGPT for in-context learning across modules: claim verifier (determines if context is sufficient), question generator (creates next relevant question), QA module (retrieves evidence with three implementations: retriever-reader, FLAN-T5, and GPT Reciter-Reader), QA validator (checks if new pairs add useful information), and reasoner (makes final prediction with rationale). The system defaults to 5 maximum iterations and provides comprehensive reports with the reasoning process guided by question-answer pairs.

## Key Results
- Achieves macro-F1 score of 55.67 on HOVER two-hop claims
- Achieves macro-F1 score of 54.67 on HOVER three-hop claims
- Achieves macro-F1 score of 52.35 on HOVER four-hop claims
- Achieves F1 score of 59.47 on FEVEROUS dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QACHECK breaks complex claims into simpler, verifiable sub-questions to guide reasoning
- Mechanism: The system iteratively generates questions that decompose the original claim, retrieves evidence for each, and validates their usefulness before proceeding
- Core assumption: Complex claims can be accurately verified by breaking them into simpler sub-questions that can be independently answered
- Evidence anchors:
  - [abstract] "guides the model's reasoning process by asking a series of questions critical for verifying a claim"
  - [section 3.1] "the claim verifier is a central component... determining if the current context information is sufficient to verify the claim"
- Break condition: If the claim verifier determines context is sufficient OR the system hits maximum allowed iterations (5 by default)

### Mechanism 2
- Claim: In-context learning with large language models enables few-shot reasoning without fine-tuning
- Mechanism: The system uses InstructGPT with demonstrations in prompts to perform tasks like claim verification, question generation, and QA validation
- Core assumption: Large language models can learn to perform new tasks effectively through in-context examples rather than parameter updates
- Evidence anchors:
  - [section 3.1] "We build the claim verifier based on InstructGPT (Ouyang et al., 2022), utilizing its powerful in-context learning ability"
  - [section 3.2] "we also leverage InstructGPT for in-context learning" for question generation
- Break condition: If the LLM fails to understand the task or produces inconsistent responses across demonstrations

### Mechanism 3
- Claim: Multiple QA implementations provide flexibility for different fact-checking scenarios
- Mechanism: The system offers three QA approaches: retriever-reader, FLAN-T5, and GPT Reciter-Reader, allowing adaptation based on claim type and available context
- Core assumption: Different fact-checking scenarios require different information retrieval strategies, and offering multiple approaches increases system robustness
- Evidence anchors:
  - [section 3.3] "we introduce three different implementations for the QA module" with specific descriptions of each
  - [section 4] Performance evaluation shows different results with different QA models
- Break condition: If all three QA implementations fail to retrieve relevant evidence or produce inconsistent answers

## Foundational Learning

- Concept: Multi-hop reasoning
  - Why needed here: Real-world claims often require combining information from multiple sources, not just single documents
  - Quick check question: Can you identify at least two distinct pieces of evidence needed to verify a given claim?

- Concept: Question decomposition
  - Why needed here: Complex claims must be broken into simpler sub-questions to enable step-by-step verification
  - Quick check question: Given a complex claim, can you generate 2-3 simpler questions that would help verify it?

- Concept: In-context learning
  - Why needed here: The system relies on few-shot demonstrations rather than fine-tuning to adapt LLMs to specific tasks
  - Quick check question: Can you create a prompt with 3-4 demonstrations that teaches a model a new task?

## Architecture Onboarding

- Component map:
  User Interface (Flask app) → Claims input and result visualization
  Claim Verifier (InstructGPT) → Determines if current context is sufficient
  Question Generator (InstructGPT) → Creates next relevant question
  QA Module (3 implementations) → Retrieves evidence and answers questions
  QA Validator (InstructGPT) → Checks if new QA pairs add useful information
  Reasoner (InstructGPT/FLAN-T5) → Makes final veracity prediction with rationale

- Critical path: User Input → Claim Verifier → (Question Generator → QA → Validator)* → Reasoner → Output

- Design tradeoffs:
  - Flexibility vs. consistency: Multiple QA implementations increase adaptability but may reduce consistency
  - Transparency vs. performance: Step-by-step reasoning improves explainability but may reduce speed
  - API dependency vs. control: Using InstructGPT APIs provides capability but introduces external dependencies

- Failure signatures:
  - Infinite loop in question generation (validator always rejects new questions)
  - Inconsistent answers from different QA implementations
  - Claim verifier incorrectly determines context is sufficient
  - Reasoner produces confidence in wrong answer

- First 3 experiments:
  1. Test with a simple two-hop claim using the default GPT Reciter-Reader to verify basic functionality
  2. Compare performance of all three QA implementations on the same claim to understand tradeoffs
  3. Test with a claim requiring maximum 5 iterations to ensure the system handles the iteration limit correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does QACHECK handle claims that require external knowledge beyond the provided evidence corpus?
- Basis in paper: [inferred] The paper mentions the potential integration of additional knowledge bases in the future, but does not discuss how the system currently handles claims requiring external knowledge.
- Why unresolved: The current implementation relies on a Wikipedia corpus and InstructGPT's parametric knowledge, which may not cover all necessary information for complex claims.
- What evidence would resolve it: Testing QACHECK on claims requiring external knowledge and analyzing its performance and reasoning process.

### Open Question 2
- Question: What is the impact of the maximum allowed iterations (set to 5 by default) on QACHECK's performance and ability to verify complex claims?
- Basis in paper: [explicit] The paper mentions the maximum allowed iterations in the description of the reasoner module.
- Why unresolved: The paper does not discuss the rationale behind this choice or its potential impact on the system's performance.
- What evidence would resolve it: Conducting experiments with different maximum iteration values and comparing the results to determine the optimal setting.

### Open Question 3
- Question: How does QACHECK's performance compare to human fact-checkers on complex claims requiring multi-hop reasoning?
- Basis in paper: [inferred] The paper focuses on the system's performance on benchmark datasets but does not compare it to human performance.
- Why unresolved: A comparison with human fact-checkers would provide insights into the system's strengths and limitations in real-world scenarios.
- What evidence would resolve it: Conducting a study where human fact-checkers and QACHECK verify the same set of complex claims, and comparing their performance and reasoning processes.

## Limitations

- The system achieves moderate performance (macro-F1 scores 52.35-59.47) on benchmark datasets, indicating room for improvement
- Reliance on InstructGPT APIs introduces potential scalability and cost concerns, and performance may vary with different in-context examples
- The iterative nature could lead to compounding errors if early modules make incorrect decisions in the reasoning chain

## Confidence

**High Confidence**: The claim that QACHECK guides reasoning through question decomposition is well-supported by the paper's methodology description and the iterative five-module architecture. The mechanism of using in-context learning with InstructGPT is also highly credible given the established effectiveness of this approach in recent literature.

**Medium Confidence**: The claim about achieving specific F1 scores on benchmark datasets is supported by experimental results, but the relatively modest performance suggests room for improvement. The effectiveness of the multiple QA implementations as a flexibility feature is plausible but not extensively validated through comparative analysis.

**Low Confidence**: The claim that the system can handle claims requiring up to 5 hops is theoretically supported but not thoroughly tested, as the evaluation focuses on 2-hop, 3-hop, and 4-hop claims in the HOVER dataset. The robustness of the system in real-world scenarios with noisy or ambiguous claims is not fully established.

## Next Checks

1. **Benchmark Expansion**: Evaluate QACHECK on additional fact-checking datasets beyond HOVER and FEVEROUS to test generalizability across different claim types and domains.

2. **Module Independence Testing**: Conduct ablation studies by systematically removing or replacing individual modules (e.g., using a different claim verifier) to quantify each component's contribution to overall performance.

3. **Error Analysis**: Perform detailed analysis of false positives and false negatives to identify systematic failure patterns and determine whether errors stem from specific modules or the overall reasoning chain.