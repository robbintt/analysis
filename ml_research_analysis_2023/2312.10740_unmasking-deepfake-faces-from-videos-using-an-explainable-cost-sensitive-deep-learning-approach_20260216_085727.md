---
ver: rpa2
title: Unmasking Deepfake Faces from Videos Using An Explainable Cost-Sensitive Deep
  Learning Approach
arxiv_id: '2312.10740'
source_url: https://arxiv.org/abs/2312.10740
tags:
- deepfake
- dataset
- detection
- used
- faceforensics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents a cost-sensitive deep learning framework for
  detecting deepfake faces in videos, addressing dataset imbalance and the need for
  resource-effective processing. Four pre-trained CNN models (XceptionNet, InceptionResNetV2,
  EfficientNetV2S, and EfficientNetV2M) were evaluated on two benchmark datasets:
  FaceForensics++ and CelebDf-V2.'
---

# Unmasking Deepfake Faces from Videos Using An Explainable Cost-Sensitive Deep Learning Approach

## Quick Facts
- arXiv ID: 2312.10740
- Source URL: https://arxiv.org/abs/2312.10740
- Reference count: 16
- Key outcome: Cost-sensitive deep learning framework achieves 98% accuracy on CelebDf-V2 and 94% on FaceForensics++ using key frame extraction and explainable AI

## Executive Summary
This study presents a cost-sensitive deep learning framework for detecting deepfake faces in videos, addressing dataset imbalance and resource-efficient processing needs. Four pre-trained CNN models (XceptionNet, InceptionResNetV2, EfficientNetV2S, and EfficientNetV2M) were evaluated on two benchmark datasets. The approach uses key frame extraction to reduce computational load while preserving video context, and applies cost-sensitive training to handle class imbalance. XceptionNet achieved the highest accuracy of 98% on CelebDf-V2, while InceptionResNetV2 reached 94% on FaceForensics++. Explainable AI tools (SmoothGrad, GradCAM, GradCAM++, and Faster Score-CAM) were used to interpret model predictions and improve transparency.

## Method Summary
The framework employs key frame extraction using inter-frame differences to identify local maxima representing meaningful video content changes. Four pre-trained CNN models were trained with cost-sensitive weighting to address dataset imbalance, where class weights were calculated based on label distribution to give minority classes higher importance. Explainable AI techniques were integrated to visualize model decision-making processes. The models were evaluated on FaceForensics++ and CelebDf-V2 datasets, with XceptionNet achieving 98% accuracy on CelebDf-V2 and InceptionResNetV2 reaching 94% on FaceForensics++.

## Key Results
- XceptionNet achieved highest accuracy of 98% on CelebDf-V2 dataset
- InceptionResNetV2 reached 94% accuracy on FaceForensics++ dataset
- Key frame extraction reduced computational load while maintaining detection performance
- Explainable AI tools successfully interpreted model predictions across all four CNN architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Key frame extraction reduces computational load while preserving video context.
- Mechanism: Identifies local maxima in inter-frame difference metric to select keyframes representing meaningful content changes.
- Core assumption: Local maxima in inter-frame differences correspond to meaningful video content changes.
- Evidence anchors:
  - [abstract] "To efficiently process video data, key frame extraction was used as a feature extraction technique."
  - [section] "The proposed approach used inter-frame differences during the key frame extraction step. The basic idea is simple: after loading the video, how much each pair of frames differs from one another was figured out. Then it was put in a local maximum detection process. Keyframes are specifically those for which the average inter-frame difference is at the local maximum."
- Break condition: Gradual video content changes without sharp frame differences may cause the method to miss meaningful keyframes.

### Mechanism 2
- Claim: Cost-sensitive neural networks address dataset imbalance in deepfake detection.
- Mechanism: Class weights calculated based on training data distribution, giving minority classes higher importance during training.
- Core assumption: Assigning higher weights to minority classes during training improves detection performance on those classes.
- Evidence anchors:
  - [abstract] "Furthermore, a cost-sensitive neural network method was applied to solve the dataset imbalance issue that arises frequently in deepfake detection."
  - [section] "To fix the class imbalance in the training data, class weights are first generated to give minority classes greater weight during model training."
- Break condition: Extreme imbalance may require more sophisticated techniques beyond simple class weighting.

### Mechanism 3
- Claim: Explainable AI techniques improve model interpretability and trustworthiness.
- Mechanism: Gradient-based methods (SmoothGrad, GradCAM, GradCAM++, Faster Score-CAM) visualize input image regions the model focuses on for predictions.
- Core assumption: Visual explanations correlate with the model's actual decision-making process.
- Evidence anchors:
  - [abstract] "Explainable AI tools (SmoothGrad, GradCAM, GradCAM++, and Faster Score-CAM) were used to interpret model predictions and improve transparency."
  - [section] "The prediction of the trained models is explained by the Explainable AI techniques (XAI). From Figure 4 to Figure 6 it has been proved that Explainable AI produces different results for different models."
- Break condition: Models relying on spurious correlations may produce misleading explanations.

## Foundational Learning

- Concept: Convolutional Neural Networks
  - Why needed here: Entire detection system built on pre-trained CNN models (XceptionNet, InceptionResNetV2, EfficientNetV2S, EfficientNetV2M)
  - Quick check question: What is the main architectural difference between XceptionNet and traditional CNNs?

- Concept: Imbalanced datasets and class weighting
  - Why needed here: Deepfake detection datasets inherently imbalanced (more fake than real samples), requiring cost-sensitive approaches
  - Quick check question: How do you calculate class weights for imbalanced datasets in practice?

- Concept: Explainable AI techniques
  - Why needed here: System uses multiple gradient-based explanation methods to interpret model predictions and build trust
  - Quick check question: What is the key difference between GradCAM and SmoothGrad in terms of what they visualize?

## Architecture Onboarding

- Component map: Video → Face extraction → Frame sampling (30fps) → Inter-frame difference calculation → Key frame selection → CNN feature extraction → Classification → Explanation generation

- Critical path: Video → Face extraction → Frame sampling (30fps) → Inter-frame difference calculation → Key frame selection → CNN feature extraction → Classification → Explanation generation

- Design tradeoffs:
  - Key frame extraction vs. using all frames: reduced computation but potential loss of temporal information
  - Cost-sensitive training vs. data augmentation: different approaches to handling imbalance
  - Multiple explanation methods vs. single method: redundancy for validation vs. computational overhead

- Failure signatures:
  - High accuracy but poor explanation quality: model may be relying on spurious correlations
  - Consistent misclassification of certain video types: potential bias in training data or model architecture
  - Slow inference time despite key frame extraction: possible bottleneck in face extraction or explanation generation

- First 3 experiments:
  1. Baseline test: Run all four models on full video frames (no key frame extraction) to establish upper bound performance
  2. Ablation test: Train models without cost-sensitive weighting to measure impact of handling imbalance
  3. Explanation validation: Compare explanation consistency across different XAI methods on same predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed cost-sensitive approach compare to alternative methods for handling dataset imbalance in deepfake detection, such as oversampling or weighted loss functions?
- Basis in paper: [explicit] The paper mentions that a cost-sensitive neural network method was applied to solve the dataset imbalance issue.
- Why unresolved: The paper does not provide a direct comparison between the cost-sensitive approach and other methods for handling imbalance.
- What evidence would resolve it: A comparative study showing the performance of the cost-sensitive approach against other imbalance-handling techniques would resolve this question.

### Open Question 2
- Question: What is the impact of using different video frame extraction techniques on the performance of deepfake detection models?
- Basis in paper: [explicit] The paper uses key frame extraction to reduce computational load while preserving video context.
- Why unresolved: The paper does not explore alternative frame extraction techniques or their effects on model performance.
- What evidence would resolve it: Experiments comparing the proposed key frame extraction method with other techniques would provide insights into their relative effectiveness.

### Open Question 3
- Question: How do the proposed models perform on deepfake videos generated using methods other than those included in the benchmark datasets?
- Basis in paper: [explicit] The paper evaluates the models on FaceForensics++ and CelebDf-V2 datasets, which use specific forgery methods.
- Why unresolved: The paper does not assess the models' generalizability to deepfakes generated by different methods.
- What evidence would resolve it: Testing the models on videos with deepfakes generated by other techniques would determine their robustness and generalizability.

## Limitations

- Key frame extraction may miss gradual transformations that occur between frames without sharp differences
- Cost-sensitive approach may not fully resolve extreme class imbalance scenarios
- Explainable AI methods cannot guarantee visualizations represent true decision-making process, especially if model relies on spurious correlations

## Confidence

**High Confidence:** Model performance metrics (accuracy, precision, recall) are well-documented with specific numerical results for both datasets. Framework architecture and methodology are clearly described.

**Medium Confidence:** Effectiveness of key frame extraction and cost-sensitive approaches relies on assumptions about video content and dataset characteristics that may not generalize to all scenarios.

**Low Confidence:** Reliability of Explainable AI interpretations as true representations of model decision-making processes, as these methods can be influenced by adversarial patterns.

## Next Checks

1. **Temporal Information Preservation Test:** Compare performance between key frame extraction and frame aggregation methods to quantify information loss.

2. **Imbalance Severity Analysis:** Systematically vary class imbalance ratios to determine breaking point where cost-sensitive weighting becomes insufficient.

3. **Explanation Consistency Validation:** Test whether GradCAM, GradCAM++, and Faster Score-CAM produce consistent explanations across identical predictions to assess reliability.