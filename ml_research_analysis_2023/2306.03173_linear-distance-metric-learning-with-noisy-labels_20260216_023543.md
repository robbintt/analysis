---
ver: rpa2
title: Linear Distance Metric Learning with Noisy Labels
arxiv_id: '2306.03173'
source_url: https://arxiv.org/abs/2306.03173
tags:
- noise
- data
- linear
- metric
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning a linear Mahalanobis
  distance metric from noisy pairwise labels (Close/Far). The authors propose a convex
  loss-based formulation that reduces to a standard smooth optimization problem.
---

# Linear Distance Metric Learning with Noisy Labels

## Quick Facts
- arXiv ID: 2306.03173
- Source URL: https://arxiv.org/abs/2306.03173
- Reference count: 40
- Key outcome: Proposes convex loss-based formulation for linear Mahalanobis metric learning from noisy pairwise labels, achieves >99% accuracy even with 45% mislabeled data, and provides first proof that low-rank truncation preserves accuracy

## Executive Summary
This paper addresses the problem of learning a linear Mahalanobis distance metric from noisy pairwise labels (Close/Far) by formalizing a convex loss optimization approach. The authors show their method can recover ground-truth parameters with high probability given sufficient samples, with a sample complexity bound of O(d² log d). A key theoretical contribution is proving that truncating the learned matrix to low rank preserves accuracy - the first such result in the literature. Experiments demonstrate high accuracy on both synthetic and real data, robustness to significant noise levels, and superior performance compared to prior methods.

## Method Summary
The approach learns a linear Mahalanobis distance metric M by optimizing a convex loss function over unconstrained parameters A (where M = AA⊤) using gradient descent. Given N i.i.d. observations of pairs (xi, yi) with noisy pairwise labels ℓi ∈ {Close, Far}, the method minimizes a loss function RN(M, τ) = -1/N Σ log σ(ℓi(∥zi∥²_M - τ)) where σ is the sigmoid function. After learning the full-rank solution, the matrix is truncated to low rank by rounding down the smallest d-k singular values to zero. The sample complexity bound of O(d² log d) ensures recovery of ground-truth parameters with high probability.

## Key Results
- Achieves >99% accuracy on synthetic data with ground-truth parameters
- Maintains high accuracy (>95%) even with 45% mislabeled training data
- Provides first proof that low-rank truncation preserves both loss function accuracy and parameter recovery
- Demonstrates superior performance compared to prior methods on real datasets (e.g., MNIST, GTEx)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Convexity of the loss function ensures global optimality.
- Mechanism: The loss function L is convex in (M, τ) because it is a composition of a convex function with an affine transformation of (M, τ). This allows standard convex optimization techniques to find the global minimum.
- Core assumption: The noise model must be "simple" (symmetric, continuous, non-zero everywhere, and -log ΦNoise must be convex and ζ-Lipschitz).
- Evidence anchors:
  - [abstract]: "We formalize a simple and elegant method which reduces to a general continuous convex loss optimization problem"
  - [section 3]: "Our core objective is optimizing RN or R over (M, τ) such that M ⪰ 0, τ ≥ 0. Similar to typical supervised learning problems, we consider loss functions L which for a data point (zi, ℓi) are convex functions of ℓi(∥z∥2M − τ)."
  - [corpus]: Weak evidence; corpus focuses on metric learning variants but not convexity analysis.
- Break condition: If the noise model is not "simple", the loss function may not be convex, and global optimality is not guaranteed.

### Mechanism 2
- Claim: Unconstrained optimization over A (where M = AA⊤) preserves the global minimum.
- Mechanism: By Journée et al. (2010), a local minimizer A* of the unconstrained problem provides a stationary point (global minimum) M* = A*(A*)⊤ of the constrained problem if A* is rank deficient. This allows us to use standard unconstrained optimization techniques.
- Core assumption: The optimal solution A* has rank less than k (where k is the number of columns in A).
- Evidence anchors:
  - [section 4.1]: "Theorem 6 (Journée et al. (2010)) A local minimizer A* of Problem 8 provides a stationary point (global minimum) M = A*(A*)⊤ of Problem 7 if A* is rank deficient."
  - [abstract]: "Moreover, we present an effective way to truncate the learned model to a low-rank model that can provably maintain the accuracy in loss function and in parameters – the first such results of this type."
  - [corpus]: Weak evidence; corpus mentions metric learning but not the specific unconstrained optimization approach.
- Break condition: If the optimal solution A* is full rank, the unconstrained optimization may not converge to the global minimum of the constrained problem.

### Mechanism 3
- Claim: Truncating the learned matrix to low rank preserves accuracy.
- Mechanism: By rounding down the smallest d-k singular values of the learned matrix A to 0, we obtain a low-rank approximation Ak. Under certain assumptions on the data distribution, this truncation preserves the accuracy of the loss function and the parameters.
- Core assumption: The width of the support of the data distribution is bounded, and either the support includes measure in regions which assign labels of both Close and Far, or the unbiased noise is large enough to generate some of each label.
- Evidence anchors:
  - [abstract]: "Moreover, we present an effective way to truncate the learned model to a low-rank model that can provably maintain the accuracy in loss function and in parameters – the first such results of this type."
  - [section 4.3]: "Returning a low-rank approximation M̂ k to M̂ can achieve bounded error with respect to |R(M̂ k, τ̂) − R(M*, τ*)| and ||M̂ − M*|| + |τ̂ − τ*|"
  - [corpus]: Weak evidence; corpus focuses on metric learning variants but not low-rank approximation.
- Break condition: If the assumptions on the data distribution are not met, the truncation may not preserve accuracy.

## Foundational Learning

- Concept: Convex optimization
  - Why needed here: The core optimization problem is convex, allowing us to use efficient convex optimization techniques to find the global minimum.
  - Quick check question: What are the conditions for a function to be convex, and how does this relate to the loss function in this paper?

- Concept: Matrix decomposition
  - Why needed here: The learned matrix M is decomposed into M = AA⊤, allowing us to use unconstrained optimization techniques and enabling low-rank approximation.
  - Quick check question: What are the properties of the decomposition M = AA⊤, and how does this relate to the optimization problem?

- Concept: Sample complexity
  - Why needed here: The paper provides a sample complexity bound, which determines the number of samples needed to achieve a certain level of accuracy.
  - Quick check question: What is the relationship between the sample complexity bound and the accuracy of the learned model?

## Architecture Onboarding

- Component map:
  - Data pairs (xi, yi) with labels ℓi -> Loss function L(ℓi(∥z∥²_M - τ)) -> Unconstrained optimization over A (M = AA⊤) -> Truncation to low rank Ak -> Output (M̂ k, τ̂)

- Critical path:
  1. Sample N pairs (xi, yi) from the data distribution
  2. Generate labels ℓi based on the noisy labeling process
  3. Solve the unconstrained optimization problem over A
  4. Truncate the learned matrix A to obtain a low-rank approximation
  5. Return the low-rank matrix M̂ k and threshold τ̂

- Design tradeoffs:
  - Using unconstrained optimization over A allows us to use standard optimization techniques, but may not converge to the global minimum if the optimal solution is full rank.
  - Truncating the learned matrix to low rank enables dimensionality reduction, but may not preserve accuracy if the assumptions on the data distribution are not met.

- Failure signatures:
  - If the noise model is not "simple", the loss function may not be convex, and global optimality is not guaranteed.
  - If the optimal solution A* is full rank, the unconstrained optimization may not converge to the global minimum of the constrained problem.
  - If the assumptions on the data distribution are not met, the truncation may not preserve accuracy.

- First 3 experiments:
  1. Generate synthetic data with known ground truth parameters M* and τ*.
  2. Vary the noise level and observe the effect on the accuracy of the learned model.
  3. Vary the dimension d and observe the effect on the sample complexity.

## Open Questions the Paper Calls Out
- How does the choice of noise distribution affect the optimal loss function for metric learning, and can the method be robust to misspecification of the noise model?
- Can the linear metric learning approach be extended to handle non-i.i.d. data or data with complex dependencies, such as time series or graph-structured data?
- How does the performance of the linear metric learning approach compare to state-of-the-art non-linear methods, such as neural network-based approaches, in terms of accuracy and interpretability?
- Can the low-rank approximation of the learned Mahalanobis matrix be further improved to achieve better dimensionality reduction while maintaining accuracy?
- How does the sample complexity of the linear metric learning approach scale with the dimensionality of the data and the rank of the true Mahalanobis matrix?

## Limitations
- The noise model assumptions are quite restrictive and may not capture complex real-world noise
- The assumption about singular values of A* being bounded below is critical but not extensively validated
- The approach requires O(d²) samples, which may be prohibitive for high-dimensional data

## Confidence
- **High confidence**: The convexity of the loss function and the basic optimization framework
- **Medium confidence**: The sample complexity bounds and their tightness
- **Medium confidence**: The low-rank truncation guarantees, pending verification of data distribution assumptions

## Next Checks
1. Test the algorithm on real-world datasets with varying noise patterns to assess robustness beyond the "simple" noise model assumptions
2. Systematically vary the data distribution properties (support width, label balance) to verify when the truncation theorem breaks down
3. Compare computational efficiency and scalability with existing metric learning approaches on high-dimensional datasets