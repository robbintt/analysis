---
ver: rpa2
title: Exploring Linguistic Similarity and Zero-Shot Learning for Multilingual Translation
  of Dravidian Languages
arxiv_id: '2308.05574'
source_url: https://arxiv.org/abs/2308.05574
tags:
- language
- translation
- zero-shot
- languages
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of transliteration and linguistic similarity
  to enable zero-shot translation between Dravidian languages using a single encoder-decoder
  neural machine translation model. The authors build a multilingual translation system
  for Kannada, Tamil, Telugu, and Malayalam, and compare the performance of vanilla
  zero-shot translation against pivot-based methods.
---

# Exploring Linguistic Similarity and Zero-Shot Learning for Multilingual Translation of Dravidian Languages

## Quick Facts
- arXiv ID: 2308.05574
- Source URL: https://arxiv.org/abs/2308.05574
- Reference count: 25
- Key outcome: Zero-shot translation between Kannada, Tamil, Telugu, and Malayalam achieved BLEU scores within 3 points of large-scale pivot-based models when trained on 50% of language directions.

## Executive Summary
This paper tackles the challenge of zero-shot translation between Dravidian languages by leveraging transliteration and linguistic similarity. The authors build a multilingual translation system using a single encoder-decoder transformer model, converting all languages to Devanagari script to increase subword overlap. They demonstrate that their approach achieves competitive performance with pivot-based methods while using significantly less training data. The work also challenges the assumption that morphologically rich languages require large vocabularies by successfully using VOLT optimization to reduce vocabulary size without performance loss.

## Method Summary
The authors develop a multilingual translation system for Dravidian languages using a Transformer-Base model with source and target language tags. They preprocess the Samanantar dataset by transliterating all languages to Devanagari script and applying SentencePiece BPE segmentation. Two vocabulary approaches are tested: standard 32K vocabulary and VOLT-optimized vocabulary (reduced to 10K). The model is trained on various combinations of translation directions, and zero-shot performance is evaluated against pivot-based baselines using case-insensitive BLEU scores on the WAT-2021 dataset.

## Key Results
- Zero-shot translation BLEU scores achieved within 3 points of large-scale pivot-based models when trained on 50% of language directions
- VOLT-optimized vocabulary (10K) outperformed standard 32K vocabulary while reducing model complexity
- Transliteration to Devanagari script significantly improved zero-shot performance compared to models without transliteration
- Performance improved with more translation directions in training, showing diminishing returns after 6 language pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transliteration to Devanagari increases subword overlap and improves zero-shot translation
- Mechanism: Shared script preserves similar phonemes across Dravidian languages while reducing vocabulary divergence, allowing the shared vocabulary model to leverage linguistic similarity
- Core assumption: Dravidian languages retain high phonetic similarity despite script differences, and transliteration preserves this similarity
- Evidence anchors: Abstract states transliteration helps overcome zero-shot shortcomings; section hypothesizes similar phonemes across Dravidian family; weak corpus support with average neighbor FMR=0.45
- Break condition: If transliteration introduces semantic ambiguity or if phonetic similarity is overstated, performance degrades

### Mechanism 2
- Claim: VOLT vocabulary optimization maintains performance while reducing size
- Mechanism: VOLT uses optimal transport to identify most informative subwords, reducing vocabulary size while maintaining semantic coverage
- Core assumption: Dravidian languages don't require large vocabularies despite being morphologically rich
- Evidence anchors: Abstract mentions testing vocabulary size theory; section shows 10K VOLT vocabulary outperforms 32K; no direct corpus evidence for VOLT on Dravidian languages
- Break condition: If out-of-domain performance drops or VOLT selects subwords losing critical morphological distinctions

### Mechanism 3
- Claim: Zero-shot performance improves with more training directions, with diminishing returns
- Mechanism: More language pairs provide better cross-lingual transfer learning through shared linguistic features
- Core assumption: Dravidian languages share sufficient features that multiple pairs create beneficial transfer effects
- Evidence anchors: Abstract states 3 BLEU point achievement with 50% directions; section observes accuracy increase with more directions; no direct corpus evidence for this pattern
- Break condition: If adding pairs increases training time disproportionately without BLEU improvements, or model capacity becomes limiting

## Foundational Learning

- Concept: Zero-shot translation
  - Why needed here: Core contribution demonstrates translation between Dravidian languages without pivot languages
  - Quick check question: What is the difference between implicit bridging (zero-shot) and explicit bridging (pivot-based) translation?

- Concept: Transliteration and its impact on NMT
  - Why needed here: Paper uses transliteration to convert all languages to shared Devanagari script
  - Quick check question: How does transliteration affect subword overlap and vocabulary size in multilingual NMT?

- Concept: VOLT (Vocabulary Learning via Optimal Transport)
  - Why needed here: VOLT optimizes vocabulary size, challenging assumptions about morphologically rich languages
  - Quick check question: What is the core principle behind VOLT's optimal transport approach to vocabulary selection?

## Architecture Onboarding

- Component map: Data preprocessing → Transliteration → Subword segmentation (with VOLT) → Model training → Evaluation with BLEU scoring
- Critical path: Clean and tokenize Samanantar dataset → Transliterate to Devanagari → Apply SentencePiece BPE (32K or VOLT-optimized) → Train Transformer-Base with language tags → Evaluate on WAT-2021 using beam search and sacrebleu
- Design tradeoffs:
  1. Transliteration script choice: Devanagari vs Dravidian scripts affects consistency vs potential bias
  2. Vocabulary size: 32K vs VOLT-optimized affects performance vs efficiency
  3. Number of trained pairs: Affects zero-shot performance vs training time
- Failure signatures:
  1. Low BLEU scores on zero-shot directions indicate insufficient cross-lingual transfer
  2. Inconsistent performance across transliteration scripts suggests script bias
  3. Poor out-of-domain performance indicates vocabulary selection issues
- First 3 experiments:
  1. Test transliteration impact: Train model with Devanagari vs no transliteration on same language pairs
  2. Test VOLT effectiveness: Compare 32K vocabulary model vs VOLT-optimized on same training data
  3. Test data vs accuracy tradeoff: Train models with 2, 4, 6, and 8 language pairs to measure zero-shot performance scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does zero-shot translation performance vary with different vocabulary sizes beyond 10K and 32K?
- Basis in paper: Authors test 10K VOLT vocabulary vs standard 32K, finding smaller performs better, but don't explore intermediate or larger sizes
- Why unresolved: No testing of vocabulary sizes between 10K-32K or larger than 32K, leaving uncertainty about optimal size
- What evidence would resolve it: Systematic testing of models with 5K-64K vocabularies, comparing BLEU scores and training efficiency

### Open Question 2
- Question: How does zero-shot translation performance generalize to out-of-domain data?
- Basis in paper: Authors mention lack of standardized test corpora and test only on WAT-2021 dataset
- Why unresolved: Without diverse out-of-domain testing, unclear how well zero-shot capabilities hold in real-world applications
- What evidence would resolve it: Evaluating models on social media text, technical documents, and informal speech for generalization assessment

### Open Question 3
- Question: How does pivot language choice affect explicit bridging methods vs zero-shot translation?
- Basis in paper: Authors compare zero-shot to pivot-based models using English, but don't explore other pivot languages
- Why unresolved: Performance may vary significantly depending on linguistic similarity between pivot and source/target languages
- What evidence would resolve it: Experiments with different pivot languages (Hindi, Telugu) compared to zero-shot to determine optimal pivot choice

### Open Question 4
- Question: How does transliteration script choice impact model's handling of ambiguous subwords?
- Basis in paper: Authors note Devanagari provides more objective performance than Dravidian scripts which introduce bias
- Why unresolved: Paper doesn't explore transliteration's impact on subword disambiguation in detail or test other balancing scripts
- What evidence would resolve it: Analyzing subword ambiguity in models trained with different transliteration scripts and evaluating impact on translation accuracy

## Limitations
- Transliteration approach assumes phonetic similarity across Dravidian languages without strong empirical validation
- VOLT optimization claims lack direct corpus evidence and require verification of implementation details
- 3 BLEU point margin compared to pivot-based models needs testing across more diverse test sets and language directions

## Confidence
- Transliteration improving zero-shot translation: **Medium** - supported by theoretical reasoning but limited empirical validation in Dravidian context
- VOLT vocabulary optimization maintaining performance: **Medium-Low** - lacks direct corpus evidence and requires verification of VOLT implementation details
- Achieving 3 BLEU points of pivot models with 50% language pairs: **Medium** - results are promising but need replication on larger test sets

## Next Checks
1. Conduct controlled experiments comparing Devanagari transliteration vs native scripts on same model architecture to isolate transliteration impact on performance
2. Verify VOLT implementation by comparing its vocabulary selection against random vocabulary pruning and standard frequency-based methods on the same dataset
3. Test zero-shot performance scaling across more granular increments (2, 3, 4, 5, 6 language pairs) to validate claimed diminishing returns pattern and identify optimal number of directions for best training time vs performance tradeoff