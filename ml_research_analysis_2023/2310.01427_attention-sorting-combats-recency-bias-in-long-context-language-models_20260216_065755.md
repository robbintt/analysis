---
ver: rpa2
title: Attention Sorting Combats Recency Bias In Long Context Language Models
arxiv_id: '2310.01427'
source_url: https://arxiv.org/abs/2310.01427
tags:
- context
- attention
- document
- arxiv
- sorting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the performance of long-context language
  models on retrieval-augmented generation (RAG) tasks, where models must extract
  relevant information from a large context. The authors identify that a key challenge
  is the "recency bias" - models tend to pay more attention to recent tokens in the
  context, making it harder to use relevant information located earlier.
---

# Attention Sorting Combats Recency Bias In Long Context Language Models

## Quick Facts
- arXiv ID: 2310.01427
- Source URL: https://arxiv.org/abs/2310.01427
- Authors: 
- Reference count: 39
- Key outcome: Attention sorting improves QA accuracy on long-context RAG tasks by combating recency bias, increasing WizardCoder accuracy from 13% to 35% on 30k context length.

## Executive Summary
This paper investigates the performance of long-context language models on retrieval-augmented generation (RAG) tasks, where models must extract relevant information from large contexts. The authors identify a key challenge: recency bias - models tend to pay more attention to recent tokens, making it harder to use relevant information located earlier. They propose a simple yet effective solution called "attention sorting": after one decoding step, sort the context documents by their attention weights (highest attention last), then repeat the process and finally generate the answer with the newly sorted context. This helps combat recency bias and improves QA accuracy on their synthetic SynthWiki dataset across multiple long-context models.

## Method Summary
The attention sorting method involves running one decode step on the input context, computing per-token attention weights averaged across layers and heads, sorting the documents by these attention scores (placing highest-attention documents last), repeating this process if needed, and then generating the final answer with the newly sorted context. The approach exploits the model's learned recency bias by ensuring relevant documents are positioned where the model naturally attends most strongly. The method is tested on a synthetic dataset of fictional biographies called SynthWiki, where models must answer questions based on long contexts.

## Key Results
- Attention sorting increases QA accuracy on 30k context length from 13% to 35% for WizardCoder
- The method works particularly well for models not specifically fine-tuned for long-context QA
- Models pay preferential attention to relevant documents even when failing to use them in responses
- Attention sorting shows diminishing returns after 1-5 iterations of sorting

## Why This Works (Mechanism)

### Mechanism 1
During pre-training on sequential text, models learn to attend more to recent tokens because they are more informative for predicting the next token. This learned bias persists when models are used for RAG tasks where relevant information can be located anywhere in context.

### Mechanism 2
Even when models fail to use relevant document information in their response, they still pay preferential attention to that document compared to irrelevant documents at the same position. Attention weights contain signal about document relevance that is independent of the model's ability to use that information for generation.

### Mechanism 3
Moving documents with highest attention weights to the end of context repeatedly improves model performance on RAG tasks. By sorting documents based on their attention weights and placing highest-attention documents at the end, the method exploits the model's recency bias to ensure relevant information is positioned where the model naturally attends most strongly.

## Foundational Learning

- **Self-attention mechanism in transformers**: Understanding how transformers compute attention weights between tokens is crucial for grasping why attention sorting works. *Quick check: How does the self-attention mechanism compute the weight between two tokens in a sequence?*

- **Positional encoding and its impact on attention**: The rotary positional encoding (RoPE) scheme creates an inductive bias toward reduced attention at long distances, contributing to recency bias. *Quick check: How does rotary positional encoding affect attention weights between tokens at different distances?*

- **Retrieval-augmented generation (RAG) architecture**: The paper addresses challenges specific to RAG tasks where models must extract relevant information from a large context. *Quick check: In a RAG setup, what is the typical relationship between the retriever component and the generator component?*

## Architecture Onboarding

- **Component map**: Context → First decode step → Compute attention weights → Sort documents by attention → Repeat if needed → Final generation with sorted context
- **Critical path**: The model decodes once, attention weights are computed, documents are sorted by attention, and the model generates with the sorted context
- **Design tradeoffs**: Attention sorting adds computational overhead (1-5 extra decoding steps) but improves accuracy without requiring model fine-tuning, trading inference time for better performance
- **Failure signatures**: If attention sorting degrades performance, it may indicate that attention weights don't reliably signal document relevance or that the recency bias has been mitigated through other means
- **First 3 experiments**:
  1. Run the model on SynthWiki with random document order and measure accuracy across different context lengths
  2. Apply one round of attention sorting and measure the change in document positions and accuracy
  3. Apply multiple rounds of attention sorting and determine at which point additional sorting provides diminishing returns

## Open Questions the Paper Calls Out

### Open Question 1
How does attention sorting perform on real-world retrieval tasks with semantically relevant distractors versus purely random distractors? The authors acknowledge that real RAG tasks have relevant rather than random distractors, and such "hard" distractors may actually be picked up by attention sorting.

### Open Question 2
What is the optimal granularity for attention sorting when documents aren't naturally separable (e.g., code generation tasks)? Many long-context tasks don't map neatly to permutable documents, and the granularity of permutable elements is unclear for tasks like code generation.

### Open Question 3
How does the performance of attention sorting change across different model architectures and pre-training objectives? The authors test only on Llama-2 based models and suggest that architecture and training may matter significantly for attention sorting effectiveness.

## Limitations

- The synthetic SynthWiki dataset lacks the complexity and diversity of real-world RAG scenarios
- The method's computational overhead and performance-time tradeoff isn't quantified
- Claims about pre-training inducing recency bias and attention weights containing relevance signals are largely theoretical with limited empirical evidence

## Confidence

**High Confidence**: The empirical results showing attention sorting improves accuracy on the SynthWiki dataset are well-supported by the data presented.

**Medium Confidence**: The mechanism explaining why attention sorting works (exploiting recency bias) is plausible but relies on assumptions about attention patterns learned during pre-training that aren't directly tested.

**Low Confidence**: The assertion that pre-training inherently induces recency bias is stated without empirical evidence or citations.

## Next Checks

1. **Real-world Dataset Validation**: Test attention sorting on a real-world RAG dataset (such as Natural Questions or TriviaQA) with actual web documents rather than synthetic biographies to validate generalizability.

2. **Attention Weight Analysis**: Conduct ablation studies where attention weights are artificially manipulated (e.g., randomized or reversed) to determine if observed performance gains are directly caused by the attention-based sorting mechanism.

3. **Computational Overhead Quantification**: Measure the exact inference time overhead introduced by attention sorting across different context lengths and document counts to determine when the method is computationally justified.