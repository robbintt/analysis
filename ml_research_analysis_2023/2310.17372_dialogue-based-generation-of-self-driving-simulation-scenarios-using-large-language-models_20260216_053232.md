---
ver: rpa2
title: Dialogue-based generation of self-driving simulation scenarios using Large
  Language Models
arxiv_id: '2310.17372'
source_url: https://arxiv.org/abs/2310.17372
tags:
- dist
- user
- scenario
- code
- scenic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores using large language models (LLMs) to generate
  simulation scenarios for self-driving cars from natural language descriptions. The
  key idea is to enable a multimodal dialogue where users can refine scenarios through
  iterative feedback, with the LLM mapping English utterances to domain-specific code.
---

# Dialogue-based generation of self-driving simulation scenarios using Large Language Models

## Quick Facts
- arXiv ID: 2310.17372
- Source URL: https://arxiv.org/abs/2310.17372
- Authors: 
- Reference count: 10
- Key outcome: 50% success rate in generating executable Scenic scenarios from natural language, with iterative dialogue improving results

## Executive Summary
This paper presents a system that uses large language models to generate self-driving simulation scenarios from natural language descriptions through iterative dialogue. The approach employs in-context learning with GPT-4 to map English utterances to Scenic code for the CARLA simulator, allowing users to refine scenarios through feedback. Experiments show that dialogue-based refinement improves success rates from 37.5% to 50% over two turns, though the LLM struggles with complex details and error correction beyond simple corrections.

## Method Summary
The system uses GPT-4 with few-shot prompting (32 training examples) to generate Scenic code from English scenario descriptions. Users interact through a dialogue interface where they can provide initial descriptions and follow-up refinements based on simulation results. The system automatically iterates up to 5 turns, appending error messages or user comments to the prompt for each new generation attempt. Generated code is compiled and executed in the CARLA simulator, with success measured by whether the output matches the intended scenario behavior.

## Key Results
- Single-turn generation achieves 37.5% success rate on 16 test scenarios
- Two dialogue turns improve success rate to 50% through iterative refinement
- The LLM shows persistent difficulties with relative movement directions and complex behavioral specifications
- Limited training data (16 examples) constrains performance despite in-context learning approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based in-context learning can generate executable Scenic code from English descriptions with iterative refinement.
- Mechanism: The LLM is prompted with few-shot examples and receives error messages or user feedback as additional context, enabling it to correct mistakes through dialogue.
- Core assumption: The LLM can understand domain-specific errors and modify code accordingly when provided with context.
- Evidence anchors:
  - [abstract] "We use Large Language Models (LLMs) to map the user's English utterances in this interaction into domain-specific code"
  - [section] "For each the following queries, the model output message and then the Scenic exception or the user comment are appended to the prompt"
  - [corpus] Weak - the corpus contains related dialogue systems but no direct evidence of LLM error correction in domain-specific languages
- Break condition: If the LLM fails to understand error messages or cannot map them to code corrections, the iterative process breaks down.

### Mechanism 2
- Claim: Dialogue-based interaction improves scenario generation success compared to single-turn generation.
- Mechanism: Users can identify implicit assumptions in initial code through simulation runs and make them explicit through follow-up comments.
- Core assumption: Users can recognize when generated scenarios violate their intent and provide corrective feedback.
- Evidence anchors:
  - [abstract] "the user can follow up prior instructions with refinements or revisions, in reaction to the simulations that have been generated"
  - [section] "The user can then judge whether they adhere to the requirements, and if necessary ask the assistant for further variations"
  - [corpus] Weak - the corpus shows related dialogue systems but no evidence of improved outcomes through iterative refinement in code generation
- Break condition: If the LLM cannot incorporate user feedback into meaningful code changes, dialogue turns provide no benefit.

### Mechanism 3
- Claim: Probabilistic scenario specifications allow concise description of diverse test cases.
- Mechanism: Scenic's probabilistic formulation samples implicit aspects (positions, velocities) while the LLM handles explicit behavior descriptions.
- Core assumption: The division between explicit behavior (handled by LLM) and implicit parameters (sampled by Scenic) aligns with human communication patterns.
- Evidence anchors:
  - [abstract] "Scenic scenarios... define a distribution over scenes... enabling the concise specification of a large number of test cases"
  - [section] "The probabilistic formulation enables the concise specification of a large number of test cases, in which the implicit aspects are randomly sampled"
  - [corpus] Weak - corpus lacks evidence about probabilistic scenario generation improving natural language interfaces
- Break condition: If the LLM tries to specify parameters that should be probabilistic, or if users expect to control all aspects explicitly, the approach fails.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The paper uses few-shot prompting with limited training data (32 examples) to generate code without fine-tuning
  - Quick check question: How many training examples are used per test scenario in the LLM prompt?

- Concept: Domain-specific languages for simulation
  - Why needed here: Scenic is used to specify self-driving scenarios, requiring understanding of its probabilistic constructs and simulator integration
  - Quick check question: What simulator backend does Scenic use in this work?

- Concept: Dialogue state management
  - Why needed here: The system maintains conversation history across turns to inform code generation and corrections
  - Quick check question: What happens to the prompt when a new user comment is added?

## Architecture Onboarding

- Component map:
  - User interface: Natural language input and simulation visualization
  - LLM module: OpenAI GPT-4 for code generation
  - Error handler: Captures Scenic compiler and runtime exceptions
  - Scenario executor: Runs generated Scenic code in CARLA simulator
  - Dialogue manager: Maintains conversation context and turn limits

- Critical path:
  1. User provides English scenario description
  2. LLM generates Scenic code with few-shot examples
  3. Scenic compiler validates and samples scenes
  4. CARLA simulator executes scenarios
  5. User evaluates results and provides feedback
  6. Repeat steps 2-5 with updated context

- Design tradeoffs:
  - Few-shot vs fine-tuning: Chose few-shot due to limited data but may limit performance
  - Maximum turns: Limited to 5 to control costs but may prevent convergence
  - Error message processing: Zero-shot approach saves effort but may miss domain-specific patterns

- Failure signatures:
  - Repeated generation of syntactically incorrect code
  - Failure to incorporate user feedback across turns
  - Simulator execution failures due to constraint violations
  - LLM context window exhaustion

- First 3 experiments:
  1. Test single-turn generation success rate on all 16 test scenarios
  2. Measure improvement from 1 to 2 dialogue turns
  3. Analyze error types that persist across multiple turns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the system scale with increased training data for scenario specifications?
- Basis in paper: [inferred] The paper notes the extremely low-resource nature of the problem with only 32 examples and suggests that more advanced LLM example selection techniques might lead to better performance.
- Why unresolved: The experiments were limited to 32 training examples due to data availability, making it impossible to test the impact of larger datasets.
- What evidence would resolve it: Experiments with systematically varied amounts of training data (e.g., 32, 64, 128 examples) showing success rates and generation quality.

### Open Question 2
- Question: Can the iterative refinement process be automated to reduce user expertise requirements?
- Basis in paper: [explicit] The paper states "the user still need expertise in the scenario specification language" and mentions future work on "semi-automatic scenario validation or formal verification."
- Why unresolved: The current system requires manual user intervention for refinements, and the paper only briefly mentions this as future work without concrete proposals.
- What evidence would resolve it: Implementation and evaluation of an automated refinement system that can handle common error patterns without user input.

### Open Question 3
- Question: What is the impact of different LLM prompting strategies on scenario generation quality?
- Basis in paper: [explicit] The paper mentions "more advanced prompting strategies (e.g. chain-of-thought)" as a potential improvement in the limitations section.
- Why unresolved: The experiments used a simple few-shot in-context learning setup with random example selection, without exploring alternative prompting techniques.
- What evidence would resolve it: Comparative experiments testing different prompting strategies (chain-of-thought, retrieval-augmented generation, etc.) on the same dataset and evaluation metrics.

## Limitations
- Limited training corpus (16 examples) constrains generalization and performance
- Evaluation restricted to simple scenarios without complex multi-object interactions
- Manual assessment methodology lacks quantitative behavioral fidelity metrics

## Confidence
- High confidence: The core methodology of using LLM in-context learning with iterative dialogue for code generation is sound and technically feasible
- Medium confidence: The claim that dialogue improves scenario generation success is supported by data but may not generalize to more complex scenarios
- Low confidence: The generalizability of results to real-world autonomous driving development workflows and impact on actual safety testing

## Next Checks
1. **Scale evaluation to more complex scenarios**: Test the system with scenarios involving multiple dynamic objects, varying road geometries, and weather conditions to assess robustness beyond the current limited scope.

2. **Quantitative behavioral metrics**: Implement automated comparison of generated scenarios against ground truth behavior specifications, rather than relying solely on manual assessment of visual correctness.

3. **User study with domain experts**: Evaluate the system with professional autonomous driving engineers to assess whether the natural language interface actually improves development efficiency compared to direct Scenic coding, and identify practical limitations not captured in controlled experiments.