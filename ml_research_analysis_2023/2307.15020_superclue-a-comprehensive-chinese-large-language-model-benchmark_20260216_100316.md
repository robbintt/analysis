---
ver: rpa2
title: 'SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark'
arxiv_id: '2307.15020'
source_url: https://arxiv.org/abs/2307.15020
tags:
- open
- questions
- evaluation
- question
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SuperCLUE, a comprehensive Chinese benchmark
  for large language models (LLMs) designed to evaluate user preferences in real-world
  scenarios. The benchmark consists of three sub-tasks: CArena (user-model interactions
  with ratings), OPEN (open-ended questions), and CLOSE (closed-ended questions).'
---

# SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark
## Quick Facts
- arXiv ID: 2307.15020
- Source URL: https://arxiv.org/abs/2307.15020
- Reference count: 23
- Primary result: Introduces SuperCLUE benchmark showing closed-ended questions alone are insufficient for evaluating user preferences on open-ended LLM tasks

## Executive Summary
SuperCLUE is a comprehensive Chinese benchmark for evaluating large language models (LLMs) across three complementary tasks: CArena (user-model interactions with ratings), OPEN (open-ended questions), and CLOSE (closed-ended questions). The benchmark demonstrates that closed-ended questions alone cannot adequately reflect human preferences on open-ended conversational tasks, while combining both formats provides a more reliable assessment. The study validates GPT-4 as a reliable automatic evaluator for Chinese open-ended questions, achieving 80% agreement with human judgments. When evaluated on 11 advanced LLMs, significant performance gaps were observed between Chinese models and top-performing global models like GPT-4.

## Method Summary
The SuperCLUE benchmark employs zero-shot evaluation of Chinese LLMs across three sub-tasks. CArena collects actual user queries and ratings through an LLM battle platform, establishing gold standard performance metrics. OPEN contains 30 single-turn and 30 multi-turn open-ended questions across ten capability categories, while CLOSE transforms these into objective closed-ended questions with manual verification. GPT-4 serves as the automatic judge for OPEN questions via pairwise comparison against ChatGPT, with win/tie judgments validated against human ratings. All models are evaluated without task-specific training to simulate real-world deployment scenarios.

## Key Results
- Closed-ended questions alone are insufficient to reflect human preferences on open-ended conversational tasks
- Combining closed-ended and open-ended questions provides more reliable assessment of user preferences
- GPT-4 achieves 80% agreement with human raters as an automatic evaluator for Chinese open-ended questions
- Significant performance gaps exist between Chinese models and top-performing global models like GPT-4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can serve as a reliable automatic judge for Chinese open-ended questions
- Mechanism: GPT-4 evaluates model responses via pairwise comparison against ChatGPT, providing win/tie judgments that correlate strongly with human ratings
- Core assumption: GPT-4's evaluation criteria align sufficiently with human judgment standards
- Evidence anchors: 80% Pearson correlation with human ratings; validation through pairwise comparison methodology
- Break condition: When task complexity exceeds GPT-4's reasoning capacity or when cultural/linguistic nuances require human judgment

### Mechanism 2
- Claim: Combining closed-ended and open-ended questions provides more reliable assessment than either format alone
- Mechanism: Closed-ended questions capture objective knowledge while open-ended questions reveal conversational ability; their combination correlates better with real user preferences
- Core assumption: User preferences in real applications depend on both knowledge accuracy and interactive quality
- Evidence anchors: Correlation analysis shows improved prediction of actual user preferences; complementary nature of different question formats
- Break condition: When one format dominates user needs in specific application contexts

### Mechanism 3
- Claim: Real user interaction data provides gold standard for model evaluation
- Mechanism: CArena collects actual user queries and ratings through side-by-side model comparisons, establishing ground truth performance metrics
- Core assumption: Users can reliably distinguish model quality through direct comparison
- Evidence anchors: 9.9k votes collected from users; user-reported win rates as gold standard for measuring real-life performance
- Break condition: When user sample size becomes insufficient for statistical significance

## Foundational Learning

- Concept: Elo rating system for model comparison
  - Why needed here: CArena uses Elo system to rank models based on user preferences
  - Quick check question: How does the Elo system adjust ratings after each pairwise comparison?

- Concept: Zero-shot evaluation methodology
  - Why needed here: All models are evaluated without task-specific training to simulate real-world deployment
  - Quick check question: What distinguishes zero-shot from few-shot evaluation in LLM benchmarking?

- Concept: Pearson correlation for agreement measurement
  - Why needed here: Used to validate GPT-4's agreement with human evaluators
  - Quick check question: What does an 80% Pearson correlation indicate about evaluator agreement?

## Architecture Onboarding

- Component map: LangYa Leaderboard platform (CArena) -> Question generation layer (OPEN/CLOSE) -> GPT-4 evaluation layer -> Correlation analysis layer
- Critical path: User query collection → Capability annotation → Question generation → Model evaluation → Performance analysis
- Design tradeoffs: Open vs closed-ended questions: Coverage vs evaluation simplicity; Automatic vs human evaluation: Scalability vs accuracy; Single vs multi-turn questions: Depth vs breadth of assessment
- Failure signatures: Low correlation between evaluation methods indicates benchmark design issues; GPT-4-human agreement dropping below 70% suggests evaluation methodology problems; Performance gaps exceeding 20% between similar models may indicate data quality issues
- First 3 experiments: 1) Validate GPT-4 agreement by having humans rate 100 random question pairs, 2) Test correlation between CLOSE accuracy and OPEN performance across all models, 3) Compare user preferences in CArena with model rankings from other benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4 as an automatic evaluator compare to human raters in evaluating Chinese open-ended questions across different capability categories?
- Basis in paper: The paper states that GPT-4 has an 80% agreement rate with human raters when evaluating open-ended questions in Chinese, but does not provide detailed breakdowns by capability category.
- Why unresolved: The paper only provides an overall agreement rate without analyzing whether GPT-4 performs consistently across all capability categories (e.g., semantic understanding, small talk, code generation, etc.).
- What evidence would resolve it: Detailed correlation analysis between GPT-4 and human evaluations broken down by each of the ten capability categories would reveal if GPT-4 is equally reliable across all domains or if it performs better in some areas than others.

### Open Question 2
- Question: What specific aspects of Chinese language and culture contribute to the performance gap between Chinese-oriented LLMs and top-performing global models like GPT-4?
- Basis in paper: The paper demonstrates a significant performance gap between Chinese models and GPT-4 but does not investigate the underlying reasons for this gap.
- Why unresolved: While the paper quantifies the performance difference, it doesn't explore whether the gap stems from differences in training data, architectural choices, cultural nuances in Chinese language, or other factors specific to Chinese language processing.
- What evidence would resolve it: Comparative analysis of training datasets, architectural differences, and evaluation of model performance on culturally-specific Chinese concepts versus general language tasks would help identify the root causes of the performance disparity.

### Open Question 3
- Question: How does the combination of closed-ended and open-ended questions improve prediction of real-world user preferences compared to using either format alone?
- Basis in paper: The paper suggests that closed- and open-ended questions complement each other to predict user preferences but doesn't quantify the specific improvement or identify optimal weighting ratios.
- Why unresolved: The paper demonstrates that combining formats is beneficial but doesn't provide detailed analysis of how much better this combination performs or what the optimal ratio of closed to open-ended questions should be for different use cases.
- What evidence would resolve it: Systematic experimentation with different ratios of closed-ended to open-ended questions and their correlation with actual user preferences from CArena would quantify the optimal combination for predicting real-world user satisfaction.

## Limitations
- Benchmark focus on Chinese language limits generalizability to other linguistic domains
- Reliance on user interaction data from a single platform may introduce platform-specific biases
- Automatic evaluation methodology using GPT-4 as judge, while showing 80% agreement, still allows for systematic evaluation errors

## Confidence

**High Confidence**: Finding that closed-ended questions alone cannot capture user preferences for open-ended tasks is well-supported by correlation analysis and aligns with established evaluation principles.

**Medium Confidence**: Claim that GPT-4 serves as a reliable automatic judge for Chinese open-ended questions is supported by 80% agreement with human raters but could be strengthened with larger validation samples.

**Low Confidence**: Assertion that user preferences are more accurately reflected in multi-turn versus single-turn interactions requires further validation across different user populations and application contexts.

## Next Checks

1. Conduct blind human evaluation of 200 randomly selected question pairs to independently verify GPT-4's 80% agreement rate with human judges, stratified across all capability categories.

2. Test benchmark reproducibility by having an independent research group evaluate the same 11 models using the published question sets and evaluation protocols, comparing result consistency.

3. Validate cross-cultural applicability by translating a subset of SuperCLUE questions to English and evaluating them with both Chinese and English language models to assess linguistic dependency.