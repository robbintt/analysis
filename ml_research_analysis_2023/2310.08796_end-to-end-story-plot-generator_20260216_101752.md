---
ver: rpa2
title: End-to-end Story Plot Generator
arxiv_id: '2310.08796'
source_url: https://arxiv.org/abs/2310.08796
tags:
- story
- plot
- plots
- generation
- generate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an end-to-end model for generating story plots,
  replacing the expensive and time-consuming process of calling LLMs multiple times.
  The method involves using LLaMA2 to generate high-quality training datasets, training
  an end-to-end model (E2EPlot) via supervised fine-tuning, and further improving
  it using RLHF with different reward models for various aspects of story quality.
---

# End-to-end Story Plot Generator

## Quick Facts
- arXiv ID: 2310.08796
- Source URL: https://arxiv.org/abs/2310.08796
- Reference count: 24
- Primary result: E2EPlot generates story plots of comparable quality to state-of-the-art but >10x faster

## Executive Summary
This paper introduces an end-to-end model for generating story plots that replaces the expensive multi-step pipeline requiring multiple LLM calls. The approach uses LLaMA2 to generate high-quality training datasets, then trains an end-to-end model (E2EPlot) via supervised fine-tuning. The model is further improved using RLHF with different reward models for various aspects of story quality. E2EPlot achieves comparable quality to the state-of-the-art DOC pipeline while being over 10 times faster, and the RLHF fine-tuned version (RLPlot) achieves a 60.0% winning rate against E2EPlot for suspense and surprise.

## Method Summary
The method involves three main steps: First, using LLaMA2-13B-chat with carefully designed prompts to generate approximately 13,000 high-quality story plots in parallel, replacing expensive OpenAI API calls. Second, fine-tuning LLaMA2-7B-chat via supervised fine-tuning on this dataset to create E2EPlot, which can generate complete story plots in a single forward pass. Third, applying reinforcement learning from human feedback (RLHF) using five different GPT-4 based reward models for aspects like interestingness, coherence, suspense, character identification, and ending quality to create RLPlot.

## Key Results
- E2EPlot generates story plots in ~30 seconds, over 10x faster than the multi-step DOC pipeline
- E2EPlot achieves comparable quality to OpenPlot (state-of-the-art) according to GPT-4 evaluation
- RLPlot achieves 60.0% winning rate against E2EPlot for suspense and surprise aspects based on human evaluation

## Why This Works (Mechanism)

### Mechanism 1
The end-to-end model achieves comparable quality to the multi-step pipeline by learning the mapping from premise to full story plot structure in a single forward pass. Supervised fine-tuning on 13,000 story plots generated by OpenPlot allows the model to internalize the structure and relationships between premise, characters, setting, and outline without requiring multiple LLM calls. This works because the story plot structure learned through OpenPlot's generation is consistent and high-quality enough that a single model can replicate it effectively.

### Mechanism 2
RLHF with aspect-specific reward models improves targeted quality dimensions beyond what supervised fine-tuning achieves. Training separate reward models for each quality aspect (interestingness, coherence, suspense, character identification, ending quality) allows the RLHF fine-tuning to specialize the model along each dimension using PPO-style updates. This works because GPT-4 can reliably evaluate these quality aspects, and the reward models can capture the nuances needed for effective RLHF.

### Mechanism 3
Replacing OpenAI API calls with Llama2 and careful prompt engineering eliminates rate limits and makes large-scale dataset generation feasible. The OpenPlot pipeline uses Llama2-13B-chat with specifically designed prompts that simulate completion model behavior through chat model constraints, enabling parallel batch generation of thousands of story plots. This works because Llama2-13B-chat can match the quality of the original DOC pipeline's text-davinci-002 model when given appropriately designed prompts.

## Foundational Learning

- **Concept: Supervised fine-tuning for task adaptation**
  - Why needed here: To adapt a general-purpose language model (Llama2-7B-chat) to the specific task of story plot generation using the structured training data from OpenPlot
  - Quick check question: What is the key difference between standard fine-tuning and supervised fine-tuning in the context of adapting LLMs to new tasks?

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: To further improve the model beyond what supervised fine-tuning achieves by incorporating human preferences across multiple quality dimensions
  - Quick check question: How does RLHF differ from standard reinforcement learning in terms of the reward signal and optimization objective?

- **Concept: Prompt engineering for chat models vs completion models**
  - Why needed here: The OpenPlot pipeline must adapt from using a completion model (text-davinci-002) to a chat model (Llama2) while maintaining generation quality and structure
  - Quick check question: What are the key structural differences between prompts designed for chat models versus completion models, and how do these differences affect the generation process?

## Architecture Onboarding

- **Component map**: Premise → OpenPlot generation → E2EPlot fine-tuning → RLHF with reward models → RLPlot deployment
- **Critical path**: OpenPlot pipeline generates training data → E2EPlot fine-tuned on this data → RLHF with aspect-specific reward models → RLPlot deployed for generation
- **Design tradeoffs**: The choice of Llama2-7B-chat for E2EPlot balances generation quality with inference speed, while using Llama2-13B-chat for OpenPlot generation ensures higher quality training data despite slower generation
- **Failure signatures**: If E2EPlot quality degrades, check the consistency and coverage of the OpenPlot training data; if RLHF fails to improve specific aspects, verify the accuracy of the corresponding reward models
- **First 3 experiments**:
  1. Compare E2EPlot vs OpenPlot generation quality on a small set of premises using GPT-4 evaluation
  2. Test RLHF effectiveness by comparing RLPlot vs E2EPlot on the suspense and surprise aspect with human evaluation
  3. Measure inference speed difference between E2EPlot and OpenPlot on the same hardware

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the generation speed of E2EPlot be further improved to approach real-time performance?
- Basis in paper: The paper states that E2EPlot generates story plots in about 30 seconds, which is still somewhat slow from a user experience perspective
- Why unresolved: The paper mentions this as a future direction but does not provide specific methods or results for achieving faster generation
- What evidence would resolve it: Experiments showing the effectiveness of techniques like Zhang et al. (2023) for efficient inference when applied to E2EPlot, with generation times measured and compared to the current 30-second benchmark

### Open Question 2
- Question: Can E2EPlot be extended to support variable levels of granularity in the hierarchical outline, similar to the DOC pipeline?
- Basis in paper: The paper mentions that the previous DOC pipeline has more flexibility for controlling the level of granularity in the outline, but E2EPlot currently uses a fixed two-level hierarchy
- Why unresolved: The paper identifies this as an appealing future direction but does not provide details on how to implement or evaluate such an extension
- What evidence would resolve it: Implementation of E2EPlot with configurable outline depth, followed by human evaluation of generated story plots to assess whether the variable granularity improves plot quality compared to the fixed two-level approach

### Open Question 3
- Question: What are the key factors limiting the accuracy of reward models for different aspects of story quality?
- Basis in paper: The paper shows that RLPlot achieves varying levels of improvement across different aspects, with some aspects (like Q3) showing lower validation accuracy and less improvement
- Why unresolved: The paper does not analyze why certain aspects are more difficult to learn or provide insights into improving reward model accuracy
- What evidence would resolve it: Detailed analysis of reward model performance across aspects, including correlation studies between human preference patterns and model predictions, to identify specific challenges and potential solutions for improving accuracy on difficult aspects

## Limitations

- The quality of generated story plots depends heavily on the consistency and coverage of the OpenPlot training data, which relies on specific prompt designs that are not fully detailed
- RLHF improvements depend on the accuracy of GPT-4 based reward models, introducing uncertainty about the generalizability of results across different evaluation frameworks
- The comparison against the original DOC method is not direct since it uses Llama2 instead of the original OpenAI models, though the paper claims equivalent quality

## Confidence

- **High confidence**: The end-to-end model achieves 10x speedup compared to the multi-step pipeline (directly measured and reported)
- **Medium confidence**: E2EPlot generates story plots of comparable quality to OpenPlot (based on GPT-4 evaluation, but limited human verification)
- **Medium confidence**: RLHF with reward models improves specific quality aspects (supported by automated evaluation, but human evaluation limited to suspense/surprise aspect only)
- **Low confidence**: The claim that RLPlot achieves 60% winning rate against E2EPlot (based on limited human evaluation of one aspect only)

## Next Checks

1. Conduct comprehensive human evaluation across all five quality dimensions (interestingness, coherence, suspense, character identification, ending quality) comparing E2EPlot and RLPlot to establish the generalizability of RLHF improvements
2. Perform ablation studies testing the impact of training data quality by generating story plots with different prompt designs and measuring the downstream effect on E2EPlot quality
3. Compare the end-to-end approach against the original DOC pipeline (using OpenAI models) on the same hardware and cost basis to validate the claimed efficiency improvements and quality equivalence