---
ver: rpa2
title: Corpus Synthesis for Zero-shot ASR domain Adaptation using Large Language Models
arxiv_id: '2309.10707'
source_url: https://arxiv.org/abs/2309.10707
tags:
- domain
- text
- target
- speech
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a zero-shot domain adaptation method for
  ASR that leverages large language models (LLMs) to generate synthetic text and speech
  corpora without requiring any target domain data. The authors propose an in-context
  instruction finetuning (ICIF) strategy to improve the quality and diversity of LLM-generated
  text for unseen domains.
---

# Corpus Synthesis for Zero-shot ASR domain Adaptation using Large Language Models

## Quick Facts
- arXiv ID: 2309.10707
- Source URL: https://arxiv.org/abs/2309.10707
- Reference count: 0
- This paper introduces a zero-shot domain adaptation method for ASR that leverages large language models (LLMs) to generate synthetic text and speech corpora without requiring any target domain data.

## Executive Summary
This paper proposes a novel zero-shot domain adaptation method for Automatic Speech Recognition (ASR) that leverages Large Language Models (LLMs) to generate synthetic text and speech corpora for unseen target domains. The key innovation is an in-context instruction finetuning (ICIF) strategy that improves the LLM's ability to generate domain-relevant text by relating domain names to source domain knowledge. A state-of-the-art controllable speech synthesis model then generates synthetic speech corresponding to the synthetic text. Experiments on the SLURP dataset show the proposed method achieves an average relative word error rate (WER) improvement of 28% on unseen target domains without any performance drop in source domains.

## Method Summary
The method consists of three main components: (1) an LLM finetuned using in-context instruction finetuning (ICIF) to generate synthetic text for target domains, (2) a controllable speech synthesis (CSS) model that generates synthetic speech from the synthetic text, and (3) an ASR model finetuned on the synthetic text-speech pairs plus real source domain speech to prevent overfitting. The ICIF strategy reformulates source domain text into instruction format and finetunes the LLM, then uses source domain demonstrations during inference for target domains. The CSS model learns a prior distribution to model acoustic style and samples from it to produce synthetic speech matching the target domain text. The ASR is finetuned on both synthetic target domain data and real source domain speech to maintain source domain performance while adapting to target domains.

## Key Results
- Average relative WER improvement of 28% on unseen target domains
- No performance drop in source domains after finetuning
- Outperforms baseline ASR model trained on LibriSpeech and source domain data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICIF improves the LLM's ability to generate domain-relevant text by explicitly relating domain names to source domain knowledge.
- Mechanism: ICIF reformulates source domain text into instruction format "Please generate a sentence related to [domain]: [text]" and finetunes the LLM. During inference, it prepends source domain demonstrations to the target domain prompt.
- Core assumption: The pretrained LLM has general language knowledge that can be mapped to specific domains when given explicit instructions and demonstrations.
- Evidence anchors:
  - [abstract] "We propose a simple yet effective in-context instruction finetuning strategy to increase the effectiveness of LLM in generating text corpora for new domains."
  - [section 3.1.1] "ICIF learns the structure and format of the source corpus Cs, and relates the target domain name dt to the knowledge from pretrained LLMs."
  - [corpus] Weak - no direct corpus evidence linking ICIF performance to specific domain-text mappings.
- Break condition: If the source domains in Cs are too dissimilar from target domains, or if the LLM's pretraining lacks relevant domain knowledge.

### Mechanism 2
- Claim: Controllable speech synthesis (CSS) can generate high-quality synthetic speech that matches the acoustic style of real speech when conditioned on synthetic text.
- Mechanism: CSS learns a prior distribution to model acoustic style and samples from it to produce synthetic speech in various conditions that match the target domain text.
- Core assumption: The CSS model trained on LibriTTS can generalize to new domains and maintain acoustic quality when synthesizing speech for synthetic text.
- Evidence anchors:
  - [abstract] "A state-of-the-art controllable speech synthesis model is then used to generate synthetic speech corresponding to the synthetic text."
  - [section 4.3] Describes CSS modifications and training on LibriTTS.
  - [corpus] Weak - no direct corpus evidence comparing synthetic speech quality to real speech.
- Break condition: If the synthetic text contains domain-specific vocabulary or structures not present in LibriTTS, leading to poor speech synthesis.

### Mechanism 3
- Claim: Finetuning ASR on synthetic speech with regularization prevents overfitting to synthetic artifacts while improving target domain performance.
- Mechanism: The finetuned ASR is trained on both synthetic target domain data and real source domain speech to maintain source domain performance while adapting to target domains.
- Core assumption: The synthetic speech contains enough acoustic variation to help ASR learn domain-specific patterns without introducing too many artifacts.
- Evidence anchors:
  - [abstract] "Experiments on the SLURP dataset show that the proposed method achieves an average relative word error rate improvement of 28% on unseen target domains without any performance drop in source domains."
  - [section 3.3] "To address this problem, we add real speech data (i.e., from source domains) to the synthetic speech from the target domain to regularize the ASR model finetuning."
  - [corpus] Weak - no direct corpus evidence showing regularization effects.
- Break condition: If synthetic speech artifacts are too strong, causing the ASR to learn incorrect patterns that hurt both source and target domain performance.

## Foundational Learning

- Concept: Domain adaptation in ASR
  - Why needed here: The paper addresses adapting ASR models to new domains without target domain data, which requires understanding how ASR models generalize and adapt.
  - Quick check question: What is the difference between domain adaptation and domain generalization in ASR?

- Concept: Large Language Models (LLMs) and instruction finetuning
  - Why needed here: The method relies on using LLMs to generate synthetic text, specifically using instruction finetuning to improve domain relevance.
  - Quick check question: How does instruction finetuning differ from standard pretraining in LLMs?

- Concept: Controllable speech synthesis and style modeling
  - Why needed here: The method uses CSS to convert synthetic text to synthetic speech, requiring understanding of how speech synthesis models can be controlled and conditioned.
  - Quick check question: What is the role of the prior distribution in controllable speech synthesis models?

## Architecture Onboarding

- Component map:
  LLM (LLaMA-7B with LoRA for ICIF) -> generates synthetic text
  Controllable Speech Synthesis (CSS) -> converts text to synthetic speech
  Pretrained ASR (Conformer-Transformer) -> finetuned on synthetic+real data
  Source domain data (17 SLURP domains) -> used for ICIF and ASR regularization

- Critical path:
  1. Prepare source domain text corpus for ICIF
  2. Finetune LLM with ICIF using source domain instructions
  3. Generate synthetic text for target domain using finetuned LLM
  4. Synthesize speech using CSS from synthetic text
  5. Finetune ASR on synthetic speech + real source domain speech
  6. Evaluate on target domain test set

- Design tradeoffs:
  - ICIF vs. naive prompting: ICIF improves text quality but adds finetuning complexity
  - Synthetic vs. real data: Synthetic enables zero-shot adaptation but may contain artifacts
  - Demonstration number: More demonstrations improve diversity but increase prompt size and variance

- Failure signatures:
  - ASR overfitting to synthetic speech artifacts (performance drops on source domains)
  - LLM generating irrelevant text (low JS divergence between synthetic and real target text)
  - CSS producing unintelligible speech (low intelligibility metrics)

- First 3 experiments:
  1. Implement ICIF finetuning and evaluate synthetic text diversity and similarity on a held-out domain
  2. Test CSS speech synthesis quality on synthetic text using objective metrics (e.g., MOS)
  3. Run ASR finetuning with varying amounts of synthetic data to find optimal data size for target domain improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method scale with increasing amounts of synthetic text data? Is there a point of diminishing returns?
- Basis in paper: [explicit] The paper investigates the impact of synthetic text corpus size on WER in Section 5.2, showing that more data generally improves performance but may saturate at some point due to synthetic artifacts.
- Why unresolved: The paper only examines two specific domains (Transport and Cooking) and a limited range of synthetic text sizes. It is unclear how the scaling behavior generalizes to other domains or what the optimal corpus size might be.
- What evidence would resolve it: Conducting experiments with a wider range of synthetic text corpus sizes for all target domains in the SLURP dataset would provide more comprehensive insights into the scaling behavior and potential saturation points.

### Open Question 2
- Question: What is the impact of demonstration selection and ordering on the quality of the synthetic text corpus and subsequent ASR performance?
- Basis in paper: [explicit] The paper mentions that the selection and ordering of demonstrations may impact the synthetic text quality in Section 5.2, but does not investigate this further.
- Why unresolved: The paper does not provide any empirical evidence or analysis of how different demonstration selection strategies or orderings affect the synthetic text corpus or ASR performance.
- What evidence would resolve it: Conducting experiments with different demonstration selection methods (e.g., random, diversity-based, similarity-based) and orderings would shed light on their impact on the synthetic text quality and ASR performance.

### Open Question 3
- Question: How does the proposed method compare to other zero-shot or few-shot ASR domain adaptation techniques in terms of performance and computational efficiency?
- Basis in paper: [explicit] The paper compares the proposed method to a baseline ASR model trained on LibriSpeech and the source domain data, but does not compare it to other state-of-the-art zero-shot or few-shot ASR adaptation techniques.
- Why unresolved: The paper does not provide a comprehensive comparison with other techniques that aim to adapt ASR models to unseen domains with limited or no target domain data.
- What evidence would resolve it: Conducting experiments comparing the proposed method to other zero-shot or few-shot ASR adaptation techniques (e.g., [7], [8], [9]) on the same dataset and evaluation metrics would provide a more complete picture of its performance and efficiency.

## Limitations
- The evaluation is limited to a single ASR model (Conformer-Transformer) and a single controllable speech synthesis model.
- No quantitative measures of synthetic speech quality (e.g., MOS scores) or synthetic text diversity are reported.
- The method relies on the quality of the pretrained LLM's general knowledge and the CSS model's ability to synthesize intelligible speech for unseen domains.

## Confidence
- High confidence: The ASR finetuning procedure and regularization strategy is well-established in domain adaptation literature and is likely to work as described.
- Medium confidence: The ICIF approach for improving LLM text generation is promising but relies on specific implementation details that may not generalize well.
- Low confidence: The CSS model's ability to generate high-quality synthetic speech for unseen domains is the most uncertain component.

## Next Checks
1. Conduct an ablation study on ICIF components to identify the optimal configuration for text generation quality and diversity.
2. Evaluate the intelligibility and naturalness of CSS-generated speech using objective metrics and compare to real speech from the same domains.
3. Test the zero-shot adaptation pipeline with different ASR models and CSS models to assess the method's robustness and generalization to other architectures.