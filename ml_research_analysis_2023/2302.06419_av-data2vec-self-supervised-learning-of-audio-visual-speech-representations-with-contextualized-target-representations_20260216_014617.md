---
ver: rpa2
title: 'AV-data2vec: Self-supervised Learning of Audio-Visual Speech Representations
  with Contextualized Target Representations'
arxiv_id: '2302.06419'
source_url: https://arxiv.org/abs/2302.06419
tags:
- speech
- arxiv
- data
- learning
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AV-data2vec introduces an end-to-end self-supervised learning framework
  for audio-visual speech recognition that jointly learns representations from both
  audio and visual data. The method builds on data2vec by using a shared transformer
  encoder to process masked audio and video inputs and predict contextualized target
  representations derived from multiple encoder layers.
---

# AV-data2vec: Self-supervised Learning of Audio-Visual Speech Representations with Contextualized Target Representations

## Quick Facts
- arXiv ID: 2302.06419
- Source URL: https://arxiv.org/abs/2302.06419
- Reference count: 27
- Key outcome: AV-data2vec achieves state-of-the-art audio-visual speech recognition with 1.4 WER using 1759h of pretraining data and a base model

## Executive Summary
AV-data2vec introduces a self-supervised learning framework for audio-visual speech recognition that uses a shared transformer encoder to process both audio and video inputs simultaneously. Unlike previous methods that use separate encoders for each modality, AV-data2vec fuses audio-visual features early in the pipeline and predicts contextualized target representations derived from multiple transformer layers. The method achieves state-of-the-art performance on the LRS3 dataset, demonstrating consistent improvements across various settings including different model sizes and amounts of pretraining data.

## Method Summary
AV-data2vec is a self-supervised learning framework that jointly learns audio-visual speech representations using a shared transformer encoder. The model processes masked audio and video inputs through modality-agnostic encoders, fuses the features early using an audio-visual fusion module, and predicts contextualized target representations generated by a teacher model. Pretraining involves masked prediction of these targets, followed by finetuning on labeled data using an attention-based sequence-to-sequence architecture. The method uses modality dropout scheduling, span masking, and exponential moving average updates to stabilize training and improve performance.

## Key Results
- Achieves 1.4 WER for audio-visual speech recognition with 1759h of pretraining data using the base model
- Consistently outperforms existing methods like RA Ven and AVA w2v-BERT across most experimental settings
- Joint audio-visual pretraining outperforms audio-only pretraining in almost all settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint audio-visual encoding with a shared transformer outperforms separate modality encoders.
- Mechanism: A single modality-agnostic transformer encoder processes fused audio-visual features early in the pipeline, mimicking human early fusion of sensory inputs. This allows the model to learn richer cross-modal interactions from the start.
- Core assumption: Early fusion of audio and visual modalities captures complementary information more effectively than late fusion or separate encoders.
- Evidence anchors:
  - [abstract] "The model uses a shared transformer encoder for both audio and video and can combine both modalities to improve speech recognition."
  - [section 3.2] "A V-data2vec adopts a shared modality-agnostic transformer encoder which takes as input both audio and video data, both of which are fused early on, similar to the human speech perception system."

### Mechanism 2
- Claim: Contextualized target representations from multiple transformer layers improve learning compared to final-layer targets only.
- Mechanism: Targets are constructed by averaging outputs from multiple transformer blocks, capturing both low-level and high-level features. This provides richer supervisory signals for the student model during masked prediction.
- Core assumption: Speech and visual features contain useful information at multiple granularities, and averaging across layers preserves this diversity.
- Evidence anchors:
  - [abstract] "Target representations include features of varying granularity which is achieved by averaging the outputs of multiple layers instead of only predicting high-level features produced by the final layer."
  - [section 3.3] "We then average these representations over the last K blocks and apply instance normalization similar to Baevski et al. (2022b) to derive the targets Y = IN(ΣK k=1 ¯Z (N−k+1))."

### Mechanism 3
- Claim: Audio-only targets perform better than multimodal targets for masked prediction.
- Mechanism: During pretraining, the teacher encoder uses only audio features to generate targets, while the student encoder may use audio, video, or both. This avoids ambiguity from visual-only inputs and provides cleaner supervisory signals.
- Core assumption: Audio signals contain more reliable and less ambiguous information for predicting speech content than visual cues alone.
- Evidence anchors:
  - [section 5.5.2] "Table.3 shows that using only audio inputs for the teacher model gives the best performance for all tasks (VSR, ASR, A VSR)."
  - [section 3.3] "Empirically we find that audio-only targets perform best ( §5.5) and in this setting we found it useful to predict audio targets when we have visual-only inputs even for unmasked time-steps."

## Foundational Learning

- Concept: Masked prediction in self-supervised learning
  - Why needed here: AV-data2vec uses masked prediction to learn representations without labels, similar to BERT or wav2vec 2.0.
  - Quick check question: Why does masking a portion of the input help the model learn better representations?

- Concept: Modality fusion strategies (early vs. late)
  - Why needed here: The paper explicitly contrasts early fusion (used here) with separate encoders (like RA Ven), showing the importance of fusion timing.
  - Quick check question: What is the main advantage of early fusion over late fusion in multimodal learning?

- Concept: Exponential moving average (EMA) for teacher model
  - Why needed here: The teacher model weights are an EMA of the student model, stabilizing training and improving target quality.
  - Quick check question: How does EMA of model weights help in self-supervised learning frameworks like data2vec?

## Architecture Onboarding

- Component map:
  - Audio encoder: Dense layer mapping log filterbanks to D-dimensional features
  - Video encoder: ResNet-18 variant producing 1D visual features aligned with audio frames
  - Audio-visual fusion module: Element-wise addition of audio and visual features based on modality scheduler
  - Transformer encoder: Shared across modalities, processes fused features and outputs contextualized representations
  - Masking strategy: Span masking applied synchronously to both modalities when both are present
  - Teacher-student setup: Teacher generates contextualized targets; student predicts them from masked input

- Critical path:
  1. Input audio/video → Audio encoder / Video encoder
  2. Fusion module combines features based on scheduler
  3. Masking applied to fused features
  4. Transformer encoder processes masked input
  5. Teacher encoder (unmasked) generates targets
  6. Loss computed between student output and teacher targets
  7. EMA update of teacher weights

- Design tradeoffs:
  - Shared encoder vs. separate encoders: Shared is more parameter-efficient and aligns with human perception but may struggle if modalities are very different
  - Early fusion vs. late fusion: Early fusion enables cross-modal interaction from the start but requires careful modality balancing
  - Audio-only targets vs. multimodal targets: Audio-only avoids ambiguity but may underutilize visual information

- Failure signatures:
  - Poor ASR/VSR performance despite large pretraining data: May indicate modality imbalance or ineffective fusion
  - Slow convergence or unstable training: Could be due to aggressive masking or poorly tuned EMA schedule
  - Large gap between pretraining and finetuning performance: Suggests mismatch between pretraining and downstream tasks

- First 3 experiments:
  1. Ablation: Compare shared encoder vs. separate encoders on LRS3 with Base model and 433h pretraining
  2. Ablation: Test early fusion vs. late fusion by moving fusion after transformer layers
  3. Ablation: Evaluate audio-only vs. multimodal targets for the teacher model under low-resource setting (30h labeled)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AV-data2vec scale with the amount of unlabeled pretraining data beyond 1759 hours, and what is the saturation point for the benefits of additional data?
- Basis in paper: [inferred] The paper mentions that "joint audio-visual pretraining outperforms audio-only pretraining in almost all settings" and that "larger models result in better performance" but also notes that "the benefits of increased model capacity and more pretraining data begin to diminish as can be seen in the results of the largest setting (Large model, 1759h pretraining data)."
- Why unresolved: The paper only reports results up to 1759 hours of pretraining data and does not explore further increases in data volume to determine if performance continues to improve or plateaus.
- What evidence would resolve it: Experiments with significantly larger amounts of unlabeled pretraining data (e.g., 3000h, 5000h, or more) and corresponding finetuning results to observe trends in performance improvements.

### Open Question 2
- Question: What is the impact of using a modality-agnostic encoder compared to separate modality-specific encoders on the computational efficiency and parameter efficiency of the model?
- Basis in paper: [explicit] The paper states that "separate encoders increase the number of model parameters" and contrasts this with AV-data2vec's "single encoder for both audio and vision."
- Why unresolved: The paper does not provide a direct comparison of computational costs (e.g., training time, memory usage) or parameter efficiency metrics between the joint encoder approach and separate encoders.
- What evidence would resolve it: Detailed analysis of training time, memory consumption, and parameter count for both AV-data2vec and models with separate encoders under identical conditions.

### Open Question 3
- Question: How does the performance of AV-data2vec vary with different masking strategies (e.g., random masking vs. span masking) and what is the optimal masking ratio and span length for audio-visual speech recognition?
- Basis in paper: [explicit] The paper mentions using "span masking on fused audio-visual features" with specific parameters (r% = 50% and l = 10) but does not explore alternative masking strategies.
- Why unresolved: The paper does not investigate the impact of different masking strategies or conduct an ablation study on masking parameters.
- What evidence would resolve it: Experiments comparing AV-data2vec with different masking strategies (e.g., random masking, variable span lengths) and varying masking ratios to identify optimal configurations.

## Limitations
- Heavy reliance on large amounts of unlabeled pretraining data (433h to 1759h) for effective performance
- Complex modality scheduler with three dropout probabilities that may be sensitive to hyperparameter choices
- Limited evaluation scope primarily focused on LRS3 dataset without extensive cross-dataset validation

## Confidence

**High Confidence:**
- The shared transformer encoder architecture with early fusion improves audio-visual speech recognition performance compared to separate encoders
- The contextualized target representations from multiple layers provide richer supervisory signals than single-layer targets
- Audio-only targets during pretraining yield better downstream performance than multimodal targets

**Medium Confidence:**
- The overall framework achieves state-of-the-art results on LRS3 dataset across most experimental settings
- The modality scheduler effectively balances audio and visual inputs during training
- The approach generalizes well from pretraining to finetuning across different tasks (ASR, VSR, AVSR)

**Low Confidence:**
- The model's performance would scale similarly on datasets significantly different from LRS3
- The specific hyperparameter choices (dropout schedules, masking ratios, EMA parameters) are optimal and robust
- The method would maintain performance advantages with substantially reduced pretraining data

## Next Checks

1. **Cross-Dataset Generalization Test**: Evaluate AV-data2vec on additional audio-visual speech datasets (e.g., LRW, GRID) with varying recording conditions, speaker diversity, and vocabulary sizes to assess robustness beyond LRS3.

2. **Data Efficiency Analysis**: Systematically reduce pretraining data from 433h to 100h and 50h while measuring performance degradation to understand the minimum effective pretraining requirements and identify potential breaking points.

3. **Hyperparameter Sensitivity Study**: Conduct an ablation study varying the modality dropout probabilities (pAV, pV|AV, pA|AV), masking ratios, and EMA update schedules to quantify the impact of these critical hyperparameters on final performance and identify optimal configurations.