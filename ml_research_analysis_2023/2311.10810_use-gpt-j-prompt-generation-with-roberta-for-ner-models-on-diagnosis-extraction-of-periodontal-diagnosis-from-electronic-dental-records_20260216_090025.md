---
ver: rpa2
title: Use GPT-J Prompt Generation with RoBERTa for NER Models on Diagnosis Extraction
  of Periodontal Diagnosis from Electronic Dental Records
arxiv_id: '2311.10810'
source_url: https://arxiv.org/abs/2311.10810
tags:
- prompt
- generation
- clinical
- data
- average
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the usability of prompt generation for named
  entity recognition (NER) tasks, specifically for extracting periodontal diagnoses
  from electronic dental records. The approach used GPT-J to generate prompts and
  RoBERTa with spaCy for NER modeling.
---

# Use GPT-J Prompt Generation with RoBERTa for NER Models on Diagnosis Extraction of Periodontal Diagnosis from Electronic Dental Records

## Quick Facts
- arXiv ID: 2311.10810
- Source URL: https://arxiv.org/abs/2311.10810
- Reference count: 22
- Primary result: GPT-J prompt generation combined with RoBERTa fine-tuning achieved F1 scores of 0.92-0.97 for extracting periodontal diagnoses from dental records

## Executive Summary
This study explored prompt generation for named entity recognition (NER) tasks, specifically for extracting periodontal diagnoses from electronic dental records. The approach used GPT-J to generate prompts and RoBERTa with spaCy for NER modeling. Results showed that lower ratios of negative examples with higher numbers of examples in prompts achieved the best direct test results (F1=0.72). After training with RoBERTa, performance was consistent across all settings with F1 scores ranging from 0.92-0.97. The study highlighted the importance of seed quality over quantity in feeding NER models and demonstrated an efficient method for mining clinical notes for periodontal diagnoses.

## Method Summary
The study used GPT-J-6B to generate prompts from clinical notes using combined generation type. Seeds were split 8:1:1 for training, validation, and testing. RoBERTa model (via spaCy) was trained on generated seeds and evaluated on the test set. The approach involved extracting clinical notes, tokenizing with spaCy, applying rule-based diagnosis extraction to select examples for prompt generation, and fine-tuning RoBERTa on the generated seeds.

## Key Results
- Lower ratio of negative examples with higher numbers of examples in prompts achieved best direct test results (F1=0.72)
- RoBERTa fine-tuning produced consistent performance across all settings (F1=0.92-0.97)
- Post-processing standardization was critical for accurate NER evaluation, correcting formatting inconsistencies in 10 out of 60 cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lower ratio of negative examples in prompts leads to better direct test performance.
- Mechanism: GPT-J prompt generation benefits from higher positive example density, improving the model's ability to infer correct entity boundaries without noise from irrelevant samples.
- Core assumption: Negative examples introduce ambiguity that dilutes the signal strength of positive examples.
- Evidence anchors:
  - [abstract] "lower ratio of negative examples with higher numbers of examples in prompts achieved the best direct test results (F1=0.72)"
  - [section] "a better performance was revealed with a lower ratio of negative examples"
- Break condition: If the task domain has highly ambiguous entities where negatives are essential for disambiguation, this approach may underperform.

### Mechanism 2
- Claim: Seed quality is more important than quantity for training RoBERTa NER models.
- Mechanism: High-quality, diverse prompts provide richer context for the RoBERTa model to learn entity extraction patterns, while excessive low-quality seeds may introduce noise and degrade performance.
- Core assumption: RoBERTa's fine-tuning process is sensitive to the signal-to-noise ratio in training data.
- Evidence anchors:
  - [abstract] "The study highlighted the importance of seed quality rather than quantity in feeding NER models"
  - [section] "the accuracy was consistent across all settings" after training with varying seed sizes
- Break condition: If the training data is extremely limited, quantity may start to outweigh quality due to coverage needs.

### Mechanism 3
- Claim: Post-processing standardization is critical for accurate NER evaluation.
- Mechanism: Raw NER outputs often contain formatting inconsistencies (typos, symbols) that, if uncorrected, cause false negatives during evaluation despite correct entity extraction.
- Core assumption: Evaluation metrics assume standardized label formats matching the gold standard.
- Evidence anchors:
  - [section] "it was found that in around 10 cases out of the 60 gold standard notes, the models were able to correctly extract the target information, but some marks like dots, commas, and other symbols were included"
- Break condition: If the NER model outputs perfectly standardized predictions, post-processing becomes redundant.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: Core task is extracting structured periodontal diagnosis entities (Stage, Grade, Extent) from clinical notes.
  - Quick check question: What are the three main entity types being extracted in this study?

- Concept: Prompt Engineering for LLMs
  - Why needed here: GPT-J is used to generate training seeds by formatting examples as prompts with labeled outputs.
  - Quick check question: How does the prompt format (combined vs separate generation) affect GPT-J's output quality?

- Concept: RoBERTa fine-tuning for NER
  - Why needed here: RoBERTa model is fine-tuned on GPT-J generated seeds to build the final NER system.
  - Quick check question: Why might RoBERTa perform consistently across different seed sizes after fine-tuning?

## Architecture Onboarding

- Component map: Data collection → Rule-based extraction → GPT-J prompt generation → Seed splitting (80/10/10) → RoBERTa fine-tuning (spaCy) → Post-processing → Evaluation
- Critical path: GPT-J prompt generation → Seed quality → RoBERTa training → Post-processing → Evaluation
- Design tradeoffs:
  - Prompt complexity vs. generation speed: More complex prompts may improve quality but increase processing time.
  - Seed quantity vs. quality: Larger seed sets risk noise; smaller high-quality sets may generalize better.
  - Separate vs. combined prompt generation: Separate allows individual label tuning but may suffer from hallucination.
- Failure signatures:
  - Low F1 scores despite high precision: Indicates high false negatives, possibly from prompt design or negative example ratio.
  - High variance across seed sizes: Suggests seed quality inconsistency or overfitting.
  - Post-processing errors: Raw model outputs contain unstandardized labels causing evaluation mismatches.
- First 3 experiments:
  1. Test prompt generation with different negative example ratios (1/4 vs 1/3) on a small sample and compare F1 scores.
  2. Compare separate vs. combined prompt generation types for GPT-J output consistency and hallucination rates.
  3. Validate post-processing effectiveness by injecting known label formatting errors and checking correction accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-J prompt generation compare to traditional rule-based methods for NER tasks in dental records?
- Basis in paper: [explicit] The paper states that "prompt generation proved to be an effective method for producing training seeds for NER models" and compares it favorably to "conventional approaches by regular expression, rule-based, dictionary-pair based methods."
- Why unresolved: While the paper shows that prompt generation works well, it doesn't provide a direct quantitative comparison with traditional rule-based methods on the same dataset.
- What evidence would resolve it: A head-to-head comparison of GPT-J prompt generation vs. traditional rule-based methods using the same dental record dataset, with F1 scores for both approaches.

### Open Question 2
- Question: Can the prompt generation approach be generalized to other medical conditions beyond periodontal diagnoses?
- Basis in paper: [explicit] The paper mentions that "this approach allows it to be extended to other diseases besides periodontitis" and discusses potential expansion to other dental conditions.
- Why unresolved: The study only focused on periodontal diagnoses, so the generalizability to other medical conditions remains theoretical.
- What evidence would resolve it: Applying the same prompt generation approach to extract diagnoses for other medical conditions (e.g., diabetes, hypertension) from clinical records, with performance metrics compared to the periodontal diagnosis extraction.

### Open Question 3
- Question: How does the quality of seeds generated by GPT-J compare to manually labeled seeds for training NER models?
- Basis in paper: [explicit] The paper discusses the importance of "seed quality rather than quantity in feeding NER models" and the effectiveness of GPT-J in generating seeds.
- Why unresolved: While the paper shows that GPT-J-generated seeds lead to good NER performance, it doesn't compare this to manually labeled seeds.
- What evidence would resolve it: A direct comparison of NER model performance when trained on GPT-J-generated seeds vs. manually labeled seeds, using the same validation dataset and metrics.

## Limitations

- Performance metrics based on relatively small validation set of 60 manually labeled notes
- GPT-J prompt generation process lacks detailed specification of prompt templates and examples
- Study does not provide comprehensive error analysis beyond formatting inconsistencies

## Confidence

- **High Confidence**: The finding that lower negative example ratios improve direct test performance (F1=0.72) is well-supported by the experimental results and consistent with prompt engineering principles. The post-processing step's importance for accurate evaluation is also clearly demonstrated through the 10-case analysis.

- **Medium Confidence**: The claim about seed quality being more important than quantity for RoBERTa fine-tuning is supported by consistent F1 scores (0.92-0.97) across different seed sizes, but the underlying mechanism could benefit from additional experiments varying seed diversity and quality independently.

- **Low Confidence**: The generalizability of these findings to other medical NER tasks or different LLM architectures remains uncertain due to the study's narrow focus on periodontal diagnoses and specific use of GPT-J-6B with RoBERTa.

## Next Checks

1. **Validation Set Size Impact**: Test the model's performance on progressively larger validation sets (e.g., 60 → 120 → 240 notes) to assess whether the observed F1 scores of 0.92-0.97 hold with increased data diversity.

2. **Prompt Template Robustness**: Systematically vary the prompt format (e.g., different instruction phrasing, example ordering) while keeping GPT-J and RoBERTa configurations constant to isolate the impact of prompt engineering on final NER performance.

3. **Cross-Domain Generalization**: Apply the same prompt generation and RoBERTa fine-tuning pipeline to a different medical NER task (e.g., medication extraction from clinical notes) using identical experimental parameters to evaluate the approach's broader applicability.