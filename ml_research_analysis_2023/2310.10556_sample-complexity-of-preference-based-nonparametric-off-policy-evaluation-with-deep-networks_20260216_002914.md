---
ver: rpa2
title: Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with
  Deep Networks
arxiv_id: '2310.10556'
source_url: https://arxiv.org/abs/2310.10556
tags:
- learning
- function
- human
- reward
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes off-policy evaluation (OPE) for reinforcement
  learning with human preference data, where the reward function is learned from pairwise
  comparisons rather than directly observed. The authors propose a two-stage algorithm:
  first learning the reward function via maximum likelihood estimation with deep neural
  networks, then applying fitted Q-evaluation to estimate the value function of a
  target policy.'
---

# Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with Deep Networks

## Quick Facts
- arXiv ID: 2310.10556
- Source URL: https://arxiv.org/abs/2310.10556
- Reference count: 40
- One-line primary result: OPE with human preference data can be as sample-efficient as standard OPE when reward functions have low-dimensional smooth structure

## Executive Summary
This paper analyzes off-policy evaluation (OPE) for reinforcement learning when rewards are learned from human pairwise comparisons rather than directly observed. The authors propose a two-stage algorithm that first learns the reward function via maximum likelihood estimation with deep neural networks, then applies fitted Q-evaluation to estimate the value function of a target policy. Under assumptions of low-dimensional manifold structure and smooth reward functions, they prove that the estimation error scales as Õ(HK^(-α/(2α+d))), where H is the horizon length, K is the number of transition samples, and d is the intrinsic dimension. This matches the error bounds of standard OPE with observable rewards, demonstrating that OPE with human preferences can be as sample-efficient as standard OPE.

## Method Summary
The method takes two datasets as input: transition samples from a behavior policy and human preference pairs. It first learns a reward function using neural MLE on the preference data, then applies fitted Q-evaluation using the learned reward and transition samples to estimate the value of a target policy. The key insight is that by choosing appropriate neural network architectures that adapt to the intrinsic low-dimensional structure of the preference data, the method can achieve sample efficiency comparable to standard OPE while only requiring preference data rather than direct reward observations.

## Key Results
- Proves OPE with human preference data achieves Õ(HK^(-α/(2α+d))) error, matching standard OPE bounds
- Shows preference data requirements (KHF) can be much smaller than transition data requirements (K) when reward has low intrinsic dimension
- Demonstrates neural networks can adapt to intrinsic low-dimensional manifold structure, avoiding curse of high ambient dimensionality

## Why This Works (Mechanism)

### Mechanism 1: Neural MLE Reward Learning with Preference Data
The method learns reward functions from pairwise human preferences via maximum likelihood estimation using deep neural networks, achieving sample efficiency comparable to standard OPE with observable rewards. The algorithm first learns the reward function brh(s,a) from human preference data DHF using neural MLE, then applies fitted Q-evaluation using the learned reward. By appropriately selecting the network size, the method adapts to the intrinsic low-dimensional structure of the preference data. Core assumption: The reward function can be expressed as rh(s,a) = fh(ψh(s,a)) where ψh is a smooth low-dimensional feature map and fh is a smooth function on the feature space. Break condition: If the reward function does not admit a low-dimensional smooth representation, the neural network would need excessive capacity and samples to approximate it accurately.

### Mechanism 2: Intrinsic Dimension Adaptation via ReLU Networks
By choosing appropriate ReLU network architecture, the method can leverage any low-dimensional manifold structure in the MDP to achieve sample-efficient estimation without suffering from high ambient dimensionality. The neural network size is chosen to adapt to the intrinsic dimension d rather than the ambient dimension D. The network complexity scales with d, allowing efficient function approximation on the low-dimensional manifold. Core assumption: The state-action space X is a low-dimensional Riemannian manifold embedded in high-dimensional space RD. Break condition: If the intrinsic dimension d is large or the manifold structure is not present, the network would need to scale with ambient dimension D, losing the efficiency gains.

### Mechanism 3: Distribution Shift Control via χ² Divergence
The method controls distribution shift between behavior and target policies using restricted χ² divergence, preventing error amplification in off-policy evaluation. The estimation error is bounded by terms involving κ1 = PH h=1 √(1 + χ²_Q(qπh,qπ0h)) for transition data and κ2 = PH h=1 √(1 + χ²_Q(qπh,ηh)) for preference data. Core assumption: The restricted χ² divergence between distributions is smaller than absolute density ratio, and the function class Q captures the relevant structure. Break condition: If the function class Q is not appropriate for the problem structure, or if the distributions are too dissimilar, the χ² divergence bounds may not hold or may be too loose.

## Foundational Learning

- Concept: Low-dimensional manifold learning
  - Why needed here: The method relies on the state-action space having intrinsic low dimensionality d ≪ D to achieve sample efficiency. Understanding manifold learning helps grasp how neural networks can adapt to this structure.
  - Quick check question: What is the difference between intrinsic dimension and ambient dimension, and why does this distinction matter for function approximation?

- Concept: H"older continuity and function smoothness
  - Why needed here: The theoretical guarantees require the reward function and Bellman operator to be H"older continuous on the manifold. This smoothness property is crucial for the approximation bounds.
  - Quick check question: How does H"older continuity of a function on a manifold differ from H"older continuity in Euclidean space?

- Concept: Off-policy evaluation and distribution shift
  - Why needed here: The method evaluates policies using data from different policies (behavior policy π0 vs target policy π). Understanding distribution shift and how to control it is essential for grasping the theoretical guarantees.
  - Quick check question: What is the difference between importance sampling and the χ² divergence approach used in this paper for handling distribution shift?

## Architecture Onboarding

- Component map: DHF → Neural MLE (Gh) → brh → Fitted Q-evaluation (Fh) → bQπh → bvπ
- Critical path: Human preference data flows through neural MLE to learn rewards, then transition data and learned rewards flow through fitted Q-evaluation to estimate policy value
- Design tradeoffs:
  - Network size vs approximation error: Larger networks reduce approximation error but increase generalization error
  - KHF vs K: The number of preference samples KHF can be much smaller than transition samples K if the reward has low intrinsic dimension
  - Intrinsic dimension d vs ambient dimension D: The method's efficiency depends on d being small
- Failure signatures:
  - High estimation error despite large networks: May indicate the reward function doesn't have the assumed low-dimensional smooth structure
  - Error not decreasing with more samples: Could indicate distribution shift is too large or function class is inappropriate
  - Instability in training: May suggest the MLE for reward learning is not converging
- First 3 experiments:
  1. Validate low-dimensional structure: Test if the reward function rh(s,a) can be well-approximated by rh(s,a) = fh(ψh(s,a)) with small d̃ by checking reconstruction error on held-out data
  2. Network size scaling: Sweep over different network sizes and measure the tradeoff between approximation error and generalization error on a synthetic MDP with known low-dimensional structure
  3. Distribution shift sensitivity: Vary the similarity between behavior policy π0 and target policy π, and measure how the estimation error scales with the χ² divergence between their state-action distributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform when the intrinsic dimension d of the state-action space is comparable to or larger than the ambient dimension D?
- Basis in paper: [inferred] The paper assumes d ≪ D for its theoretical analysis, but doesn't explore cases where d approaches or exceeds D.
- Why unresolved: The paper focuses on the low-dimensional manifold case and doesn't provide analysis or empirical results for high-dimensional state-action spaces.
- What evidence would resolve it: Experiments comparing the method's performance across different ratios of d to D, or theoretical analysis extending the current bounds to higher dimensional cases.

### Open Question 2
- Question: How sensitive is the algorithm to the choice of hyperparameters (network sizes, smoothness parameters) in practice?
- Basis in paper: [inferred] While the paper provides theoretical guidance on selecting network sizes, it doesn't discuss empirical sensitivity to these choices or provide practical guidelines.
- Why unresolved: The theoretical analysis assumes optimal hyperparameter selection, but real-world implementation would require understanding the algorithm's robustness to suboptimal choices.
- What evidence would resolve it: Empirical studies varying hyperparameters and measuring their impact on performance, or theoretical analysis of the algorithm's stability under parameter perturbations.

### Open Question 3
- Question: Can the method be extended to handle continuous human preferences (e.g., ratings) instead of just pairwise comparisons?
- Basis in paper: [explicit] The paper assumes pairwise comparison model (Assumption 3.1) and doesn't discuss extensions to other preference formats.
- Why unresolved: Real-world preference data might come in various formats, and extending the method could significantly increase its practical applicability.
- What evidence would resolve it: Theoretical analysis of the algorithm's extension to continuous preferences, or empirical results comparing performance across different preference data formats.

## Limitations
- Assumes reward functions have low-dimensional smooth structure, which may not hold in practical applications
- Theoretical bounds rely on specific neural network architectures and optimal hyperparameter choices
- Requires both transition data and preference data, though preference data requirements can be smaller

## Confidence

**High confidence**: The sample complexity bound scaling with K^(-α/(2α+d)) for transition data is well-established through the analysis, given the assumptions about manifold structure and H"older continuity.

**Medium confidence**: The adaptation to intrinsic dimension through neural network architecture selection is theoretically sound but requires careful implementation in practice to achieve the stated benefits.

**Low confidence**: The practical applicability of the manifold assumption in real-world preference-based RL scenarios, where human preferences may not align with a simple low-dimensional reward structure.

## Next Checks

1. **Manifold structure validation**: Test the method on synthetic MDPs where the reward function is known to have low-dimensional structure (rh(s,a) = fh(ψh(s,a))) and verify that the neural MLE successfully recovers this structure. Measure approximation error as a function of network size and compare against theoretical predictions.

2. **Distribution shift sensitivity analysis**: Create scenarios with varying degrees of similarity between behavior policy π0 and target policy π, and measure how the estimation error scales with the χ² divergence between their state-action distributions. Validate whether the theoretical bounds accurately predict performance degradation.

3. **Comparison to standard OPE**: Implement a baseline OPE method with observable rewards on the same MDPs, and compare sample efficiency between the preference-based method and standard OPE. This would validate the claim that preference-based OPE can match the sample efficiency of standard OPE when appropriate network architectures are chosen.