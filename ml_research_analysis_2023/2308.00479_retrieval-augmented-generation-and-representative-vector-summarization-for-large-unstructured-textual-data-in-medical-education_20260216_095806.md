---
ver: rpa2
title: Retrieval Augmented Generation and Representative Vector Summarization for
  large unstructured textual data in Medical Education
arxiv_id: '2308.00479'
source_url: https://arxiv.org/abs/2308.00479
tags:
- chunks
- representative
- large
- vector
- medicine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents RAG-assisted summarization for large documents
  in medical education, addressing the challenge of processing extensive textual data
  beyond typical LLM context windows. The method uses k-means clustering to select
  representative chunks from a vector database, followed by keyword extraction and
  t-SNE visualization to provide document overviews.
---

# Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education

## Quick Facts
- arXiv ID: 2308.00479
- Source URL: https://arxiv.org/abs/2308.00479
- Reference count: 7
- Primary result: RAG-assisted summarization for large medical documents using k-means clustering and representative chunk selection

## Executive Summary
This paper addresses the challenge of processing large unstructured medical documents beyond typical LLM context windows through a novel Retrieval-Augmented Generation (RAG) framework with Representative Vector Summarization (RVS). The method combines k-means clustering to select representative text chunks, keyword extraction, and t-SNE visualization to provide document overviews. Tested on clinical medicine reference books, the approach enabled retrieval of more targeted answers compared to general LLMs and produced effective document summaries with visual representations. The system demonstrates practical utility for medical knowledge retrieval and document intelligence in domain-specific applications.

## Method Summary
The method uses k-means clustering on vector embeddings of text chunks to select representative chunks from each cluster. Keywords are generated for each representative chunk and distributed to other chunks in the same cluster. The mapped summaries are then used to create a final abstractive summary. t-SNE visualization reduces high-dimensional embeddings to 2D for visual representation of chunk distribution and cluster structure. The approach addresses the "Lost in the Middle" problem by reducing context size through representative chunk selection while maintaining semantic coverage.

## Key Results
- Successfully processed clinical medicine textbooks (Kumar & Clark's Clinical Medicine 10th Edition, British National Formulary 82) using the RVS framework
- Demonstrated effective retrieval of targeted medical information compared to general LLMs
- Produced meaningful document summaries with accompanying visual representations (word clouds and t-SNE visualizations)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: K-means clustering on vector embeddings groups text chunks by semantic similarity, enabling selection of representative chunks from each cluster.
- Mechanism: Embeddings from text chunks are clustered using k-means; the chunk closest to each cluster centroid is chosen as the representative chunk.
- Core assumption: Euclidean distance in the embedding space corresponds to semantic similarity.
- Evidence anchors: [abstract] "k-means clustering to select representative chunks from a vector database"; [section] "the chunks in the high-dimension vector space are quantized using the k-means clustering algorithm... one representative chunk, which is the closest to the corresponding centroid from each cluster is extracted."

### Mechanism 2
- Claim: The "Lost in the Middle" problem motivates using representative chunk selection to reduce context size for summarization.
- Mechanism: By selecting k representative chunks (where k is chosen to fit LLM token limits), the input size to the LLM is reduced, mitigating performance degradation in large contexts.
- Core assumption: LLMs perform poorly on large contexts due to attention mechanism limitations.
- Evidence anchors: [abstract] "Performance tends to decrease with increasing context lengths even in large context window models. Furthermore, the 'Lost in the Middle' problem... becomes worse with large input contexts"; [section] "RVS selects a pre-defined number (k) of representative text chunks from the non-parametric knowledgebase and applies a combined abstractive and extractive summarizing workflow"

### Mechanism 3
- Claim: Combining extractive (keyword mapping) and abstractive summarization improves final summary quality.
- Mechanism: Keywords are generated for each representative chunk and distributed to other chunks in the same cluster, then a final abstractive summary is created from these mapped summaries.
- Core assumption: Keyword extraction preserves key information, and abstractive summarization can synthesize this into a coherent summary.
- Evidence anchors: [abstract] "A combined extractive and abstractive summarization method for large unstructured textual data using representative vectors is proposed"; [section] "Three keywords are generated for each representative chunk and the keywords are distributed among all the other members of the same cluster... Finally, the mapped summaries are used to create a final abstractive summary"

## Foundational Learning

- Concept: Vector embeddings and semantic similarity
  - Why needed here: The system relies on FAISS vector database and OpenAItext-embedding-ada-002 to represent text chunks in a high-dimensional space where semantic similarity can be computed.
  - Quick check question: What embedding model is used to convert text chunks into vectors for clustering?

- Concept: K-means clustering
  - Why needed here: K-means is used to group semantically similar text chunks and select a representative from each cluster.
  - Quick check question: How is the representative chunk selected within each cluster?

- Concept: t-SNE dimensionality reduction
  - Why needed here: t-SNE reduces high-dimensional embeddings to 2D for visualization of chunk distribution and cluster structure.
  - Quick check question: What visualization technique is used to display the clustering structure of document chunks?

## Architecture Onboarding

- Component map: Text extraction (OCR, PDF, etc.) → Document splitting → Vector embedding → FAISS vector database → K-means clustering → Representative chunk selection → Keyword extraction → Cluster-based keyword mapping → t-SNE visualization → LLM (gpt-3.5-turbo) → Abstractive summarization
- Critical path: Text extraction → Embedding → K-means → Representative selection → LLM summarization
- Design tradeoffs:
  - Choosing k affects both summary coverage and LLM context limits; higher k risks exceeding token limits but improves coverage.
  - Embedding dimensionality (1536) balances representation quality and computational cost.
  - t-SNE visualization is useful for analysis but not critical for core functionality.
- Failure signatures:
  - Poor retrieval accuracy: Check embedding quality and FAISS index configuration.
  - Ineffective summaries: Check k selection, cluster quality, and LLM prompt construction.
  - Visualization artifacts: Check t-SNE perplexity and iteration settings.
- First 3 experiments:
  1. Verify that text chunks are correctly embedded and retrievable via similarity search in FAISS.
  2. Test k-means clustering with different k values to observe cluster coherence and representative selection.
  3. Confirm that keyword extraction and mapping work as expected by inspecting intermediate outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RVS change with different values of k (number of representative chunks) and different maximum token limits?
- Basis in paper: [explicit] The paper mentions that the value of parameter k is determined based on the maximum token limit that can be afforded, but does not provide systematic testing of different k values.
- Why unresolved: The authors did not conduct experiments varying k and maximum token limits to determine optimal configurations or analyze the trade-offs between summary quality and computational efficiency.
- What evidence would resolve it: Systematic experiments comparing RVS performance across a range of k values and maximum token limits, measuring summary quality metrics (e.g., ROUGE scores) and computational efficiency (processing time, memory usage).

### Open Question 2
- Question: How does RVS compare to other summarization methods like Longformer-based approaches or hierarchical transformers for very large documents?
- Basis in paper: [inferred] The paper mentions the "Lost in the Middle" problem and the limitations of LLMs with large context windows, but does not compare RVS to alternative methods specifically designed for long documents.
- Why unresolved: The authors only compared RVS to a basic retrieve-stuff-summarize method, without benchmarking against more advanced long-document summarization techniques.
- What evidence would resolve it: Head-to-head comparison of RVS with Longformer, BigBird, and other transformer architectures designed for long sequences, using standardized evaluation datasets and metrics.

### Open Question 3
- Question: How well does the t-SNE visualization capture the semantic relationships between document chunks in high-dimensional space?
- Basis in paper: [explicit] The paper describes using t-SNE to reduce high-dimensional vectors to 2D for visualization, but does not evaluate how well this preserves semantic relationships.
- Why unresolved: The authors present the t-SNE visualization as a feature of RVS but do not validate whether it accurately represents the semantic structure of the document or if alternative dimensionality reduction techniques might be more effective.
- What evidence would resolve it: Quantitative evaluation of t-SNE's ability to preserve semantic distances (e.g., using trustworthiness or continuity metrics) compared to other dimensionality reduction methods like UMAP or PCA, using human evaluation of semantic similarity between documents.

## Limitations
- Evaluation based on only two medical textbooks, limiting generalizability across medical literature
- No quantitative metrics for summary quality (ROUGE scores, human evaluation scores) reported
- Optimal k value selection is heuristic rather than systematically optimized
- Performance on domains outside clinical medicine remains untested

## Confidence

**High Confidence:**
- The RVS framework successfully integrates k-means clustering with representative chunk selection for document summarization
- The method produces visual representations (word clouds, t-SNE plots) that effectively communicate document structure
- RAG-assisted responses outperform general LLMs in retrieving targeted medical information

**Medium Confidence:**
- The combined extractive-abstractive approach improves summary quality compared to either method alone
- The "Lost in the Middle" problem is effectively mitigated through representative chunk selection
- The method is scalable to documents of arbitrary length within hardware constraints

**Low Confidence:**
- The method generalizes well to other medical document types beyond textbooks
- The optimal k value selection is robust across different document characteristics
- Computational efficiency gains are significant compared to baseline RAG approaches

## Next Checks

1. **Quantitative Summary Quality Assessment**: Implement automatic evaluation metrics (ROUGE-1, ROUGE-2, ROUGE-L) to compare RVS summaries against reference summaries from the source documents, establishing baseline performance scores.

2. **Cross-Domain Generalizability Test**: Apply the RVS framework to medical literature from different domains (e.g., research articles, clinical guidelines, patient education materials) and evaluate summary quality and retrieval accuracy across these document types.

3. **Parameter Sensitivity Analysis**: Systematically vary the k parameter and embedding model parameters to determine their impact on summary quality, computational efficiency, and memory usage, establishing optimal configurations for different document characteristics.