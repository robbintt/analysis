---
ver: rpa2
title: Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language
  Models
arxiv_id: '2308.11103'
source_url: https://arxiv.org/abs/2308.11103
tags:
- court
- rulings
- wikipedia
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the potential of large language models (LLMs)
  to re-identify individuals in anonymized court rulings and Wikipedia pages. A proof-of-concept
  system is developed using actual legal data from the Swiss Federal Supreme Court
  and a Wikipedia dataset.
---

# Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2308.11103
- Source URL: https://arxiv.org/abs/2308.11103
- Reference count: 18
- One-line primary result: LLMs can re-identify individuals in Wikipedia pages with high accuracy but struggle with court rulings due to complexity and specificity of legal language.

## Executive Summary
This paper investigates the potential of large language models (LLMs) to re-identify individuals in anonymized court rulings and Wikipedia pages. The authors develop a proof-of-concept system using actual legal data from the Swiss Federal Supreme Court and a Wikipedia dataset. The system masks named entities in the text and uses LLMs to predict the masked names. Four novel metrics are introduced to measure re-identification performance: Partial Name Match Score (PNMS), Normalized Levenshtein Distance (NLD), Last Name Match Score (LNMS), and Weighted Partial Name Match Score (W-PNMS). The results show that while LLMs can re-identify individuals in Wikipedia pages with high accuracy, they struggle with court rulings, highlighting the challenges of re-identification in complex and domain-specific contexts.

## Method Summary
The paper uses a dataset of Swiss court rulings from 2019 (approximately 8K rulings) and a hand-picked subset of rulings connected to news articles (approximately 700 relevant articles). The Wikipedia dataset consists of a random subset of 0.6M entries, with approximately 69K entries after masking named entities using BERT fine-tuned for Named Entity Recognition (NER). The study applies various LLMs, including GPT-4, BLOOM, and mT0, to predict the masked names in both datasets. The performance is evaluated using the introduced metrics: PNMS, NLD, LNMS, and W-PNMS. The authors systematically analyze the factors influencing successful re-identifications, identifying model size, input length, and instruction tuning as the most critical determinants.

## Key Results
- LLMs can re-identify individuals in Wikipedia pages with high accuracy, particularly when using larger models and longer input sequences.
- Re-identification performance on court rulings is significantly lower due to the complexity and specificity of legal language.
- Instruction-tuned models perform much better at re-identification compared to non-instruction-tuned models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models retain more knowledge from pre-training, enabling better re-identification.
- Mechanism: As model size increases, parameter count grows, allowing storage of more factual knowledge about entities and their associations. During inference, the model retrieves this knowledge when prompted to predict masked names.
- Core assumption: Knowledge stored in model parameters is sufficient and accessible for the re-identification task without additional context.
- Evidence anchors:
  - [abstract]: "identifying model size, input length, and instruction tuning among the most critical determinants"
  - [section]: "Comparing differently sized versions of a model as shown in Figure 1, a clear performance boost is observed as model size increases, consistent with prior research suggesting better knowledge retrieval with larger models"
  - [corpus]: Weak - no direct mention of parameter size vs. knowledge retention in cited papers.
- Break condition: If the required knowledge is not present in pre-training data or is too sparse, larger models will not help.

### Mechanism 2
- Claim: Longer input sequences provide more context for re-identification.
- Mechanism: By increasing the number of characters given to the model, more surrounding information about the masked entity is available, improving the chances of correct identification.
- Core assumption: The relevant information for re-identification is contained within the provided text snippet.
- Evidence anchors:
  - [abstract]: "identifying model size, input length, and instruction tuning among the most critical determinants"
  - [section]: "Testing a selection of models (Figure 3) revealed that performance improves with increasing input size, though the degree of improvement varies among models"
  - [corpus]: Weak - no direct citation supporting the specific impact of input length on re-identification tasks.
- Break condition: If the necessary information is not present in the given text or the input exceeds the model's maximum sequence length.

### Mechanism 3
- Claim: Instruction tuning improves model understanding of the re-identification task.
- Mechanism: Models fine-tuned with instruction datasets learn to follow specific prompts and format responses accordingly, making them more likely to understand and execute the re-identification task correctly.
- Core assumption: The instruction-tuned models have been exposed to similar tasks during fine-tuning, enabling them to generalize to re-identification.
- Evidence anchors:
  - [abstract]: "identifying model size, input length, and instruction tuning among the most critical determinants"
  - [section]: "As shown in Figure 4, instruction tuned models perform much better at re-identification"
  - [corpus]: Weak - no direct citation demonstrating the impact of instruction tuning on this specific task.
- Break condition: If the instruction tuning did not include relevant examples or the model fails to generalize.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: NER is used to identify and mask entities in the text before re-identification, ensuring consistent evaluation.
  - Quick check question: What is the purpose of using NER in the dataset preparation process?

- Concept: Levenshtein Distance
  - Why needed here: Levenshtein Distance is used to measure the similarity between predicted and target names, providing a nuanced metric for evaluation.
  - Quick check question: How is Normalized Levenshtein Distance calculated and why is normalization necessary?

- Concept: Retrieval Augmented Generation (RAG)
  - Why needed here: RAG is used to augment LLMs with external information for re-identification tasks where the model lacks sufficient knowledge.
  - Quick check question: In what scenario would RAG be preferred over fine-tuning the model on additional data?

## Architecture Onboarding

- Component map: Data pipeline -> Model zoo -> Evaluation framework -> Experimentation platform
- Critical path: 1. Prepare datasets with masked entities 2. Select and configure models 3. Run inference with appropriate prompts 4. Evaluate predictions using custom metrics 5. Analyze results and identify influential factors
- Design tradeoffs:
  - Model size vs. computational resources: Larger models perform better but require more resources
  - Input length vs. context relevance: Longer inputs provide more context but may include irrelevant information
  - Instruction tuning vs. generalization: Instruction-tuned models perform better on specific tasks but may be less generalizable
- Failure signatures:
  - High NLD scores with low PNMS: Model is guessing but not accurately
  - All predictions are common names: Model lacks specific knowledge about entities
  - Model predicts blank tokens: Possible issue with tokenization or prompt understanding
- First 3 experiments:
  1. Evaluate a small, non-instruction-tuned model on the Wikipedia dataset with default settings.
  2. Increase input length and observe changes in performance for the same model.
  3. Switch to an instruction-tuned version of the model and compare results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would different prompt engineering strategies impact the re-identification performance of LLMs on court rulings?
- Basis in paper: [inferred] The paper mentions that prompt design significantly influences model responses, but only a limited number of prompt attempts were used due to resource constraints.
- Why unresolved: The study used a fixed set of prompts without extensive optimization, leaving the potential impact of more sophisticated prompt engineering unexplored.
- What evidence would resolve it: Conducting experiments with various prompt engineering techniques (e.g., chain-of-thought prompting, few-shot examples) and comparing their effects on re-identification accuracy in court rulings.

### Open Question 2
- Question: Would training LLMs specifically on legal and news data improve their ability to re-identify individuals in court rulings?
- Basis in paper: [explicit] The paper notes that vanilla LLMs cannot re-identify individuals in court rulings, and even models trained on news articles and court rulings struggle to make credible guesses.
- Why unresolved: The study used pre-trained models without task-specific fine-tuning, so the potential benefits of domain-specific training are unknown.
- What evidence would resolve it: Training LLMs from scratch or fine-tuning existing models on large corpora of legal documents and news articles, then evaluating their re-identification performance on anonymized court rulings.

### Open Question 3
- Question: How would the use of structured information (e.g., knowledge graphs) impact the re-identification capabilities of LLMs?
- Basis in paper: [explicit] The paper mentions that future work may investigate the use of more structured information, such as structured databases or knowledge graphs, but does not explore this approach.
- Why unresolved: The study focused on textual information embedded in model weights or provided through retrieval, without considering structured data sources.
- What evidence would resolve it: Developing and evaluating methods that incorporate structured information (e.g., entity linking, knowledge graph embeddings) into the re-identification pipeline and comparing their performance to pure text-based approaches.

### Open Question 4
- Question: How does the length and complexity of anonymized court rulings affect the re-identification performance of LLMs?
- Basis in paper: [inferred] The paper mentions that input length is a critical factor for re-identification performance, but does not systematically analyze how different lengths and complexities of court rulings impact results.
- Why unresolved: The study used a fixed input length for all court rulings, without considering the variability in ruling length and complexity across different cases.
- What evidence would resolve it: Conducting experiments with varying input lengths and analyzing the relationship between ruling length, complexity, and re-identification accuracy. Additionally, exploring methods to prioritize relevant information within longer rulings for improved re-identification.

## Limitations
- LLMs struggle with re-identification in court rulings due to the complexity and specificity of legal language.
- Models often generate meaningless predictions, such as single letters or common legal terms, for court rulings.
- The knowledge stored in model parameters may not be sufficient for the re-identification task in legal contexts.

## Confidence
* High Confidence: The impact of model size, input length, and instruction tuning on re-identification performance is well-supported by experimental results and consistent with prior research on LLMs.
* Medium Confidence: The proposed metrics (PNMS, NLD, LNMS, W-PNMS) effectively capture the nuances of re-identification performance, but their applicability to other domains or datasets remains to be validated.
* Low Confidence: The assumption that larger models inherently retain more knowledge relevant to re-identification tasks may not hold true in all cases, particularly for domain-specific language or rare entities.

## Next Checks
1. Evaluate the performance of instruction-tuned models on a domain-specific dataset (e.g., medical records) to assess their generalization capabilities beyond Wikipedia and court rulings.
2. Investigate the impact of data augmentation techniques, such as fine-tuning on domain-specific data or using retrieval-augmented generation, on the re-identification performance of smaller models.
3. Conduct a user study to compare the re-identification accuracy of human experts versus LLMs on the same anonymized datasets, providing insights into the practical implications and limitations of automated re-identification systems.