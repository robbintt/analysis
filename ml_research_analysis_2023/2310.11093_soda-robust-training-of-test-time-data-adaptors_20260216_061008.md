---
ver: rpa2
title: 'SODA: Robust Training of Test-Time Data Adaptors'
arxiv_id: '2310.11093'
source_url: https://arxiv.org/abs/2310.11093
tags:
- data
- soda
- adaptation
- adaptor
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving the performance
  of deployed deep learning models under distribution shifts when model parameters
  are inaccessible due to privacy concerns. The authors propose a method called SODA
  (Pseudo-Label-Robust Data Adaptation) that uses zeroth-order optimization (ZOO)
  to train a data adaptor to adapt test data to fit the deployed model.
---

# SODA: Robust Training of Test-Time Data Adaptors

## Quick Facts
- arXiv ID: 2310.11093
- Source URL: https://arxiv.org/abs/2310.11093
- Reference count: 40
- Primary result: SODA improves deployed model performance under distribution shifts using test-time data adaptation without accessing model parameters

## Executive Summary
SODA addresses the challenge of improving deployed deep learning models under distribution shifts when model parameters are inaccessible. The method uses zeroth-order optimization (ZOO) to train a data adaptor that transforms test data to better fit the deployed model. The key innovation is splitting test data into reliable and unreliable subsets based on pseudo-label confidence, training with supervised loss on reliable data and unsupervised mutual information maximization on unreliable data. Experiments on CIFAR-10-C, CIFAR-100-C, and ImageNet-C show SODA significantly outperforms baseline methods for test-time adaptation.

## Method Summary
SODA trains a data adaptor using ZOO to transform test data without accessing model parameters. It splits test data into reliable (high-confidence pseudo-labels) and unreliable (low-confidence) subsets. The adaptor is trained in a supervised manner on reliable data using cross-entropy loss and in an unsupervised manner on unreliable data using mutual information maximization. ZOO estimates gradients from function evaluations, bypassing the need for backpropagation through the model. This approach addresses the unreliability of pseudo-labels under distribution shift while maintaining adaptation effectiveness.

## Key Results
- SODA achieves significant accuracy improvements over baseline methods on CIFAR-10-C, CIFAR-100-C, and ImageNet-C datasets
- Outperforms existing test-time adaptation methods across 19 corruption types and 5 severity levels
- Demonstrates effectiveness in online settings where test data arrives sequentially
- Shows robustness to different hyperparameter combinations, particularly query number sensitivity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unreliable pseudo-labels cause gradient estimation error in ZOO.
- Mechanism: Pseudo-labels used in place of true labels introduce noise that propagates into KL divergence loss and gradient estimation, adding error to directional derivative approximation.
- Core assumption: Pseudo-label noise variance is large enough to affect gradient estimation.
- Evidence anchors: Abstract mentions unreliable pseudo-labels; section 3.2 discusses gradient estimation issues with unreliable labels.
- Break condition: If pseudo-label confidence is extremely high, gradient error becomes negligible.

### Mechanism 2
- Claim: Splitting data into reliable/unreliable subsets improves ZOO training stability.
- Mechanism: High-confidence predictions serve as ground truth for supervised training, while low-confidence predictions use unsupervised mutual information maximization to preserve input information.
- Core assumption: High-confidence predictions are sufficiently accurate for supervision.
- Evidence anchors: Section 3.3 describes supervised/unsupervised training split; section 3.4 provides theoretical analysis of gradient error bounds.
- Break condition: If confidence threshold is too low, supervision is lost due to insufficient reliable data.

### Mechanism 3
- Claim: Zeroth-order optimization enables adaptation without model gradients.
- Mechanism: ZOO estimates gradients using function-value differences, bypassing backpropagation through the model.
- Core assumption: Multi-point estimation with sufficient queries approximates true gradients adequately.
- Evidence anchors: Section 3.1 describes multi-point directional derivative approximation; section 4.3 shows robustness to query number.
- Break condition: If query number is too small, gradient estimates become too noisy for effective adaptation.

## Foundational Learning

- Concept: Zeroth-order optimization (ZOO)
  - Why needed here: Model parameters are inaccessible, preventing backpropagation
  - Quick check question: What is the role of the smoothing parameter μ in the ZOO gradient estimator?

- Concept: Mutual information maximization
  - Why needed here: Supervised loss is harmful for unreliable pseudo-labels; MI preserves input information
  - Quick check question: How does mutual information maximization encourage both global diversity and local certainty in predictions?

- Concept: Confidence-based label selection
  - Why needed here: Separates reliable from unreliable pseudo-labels to prevent gradient corruption
  - Quick check question: What two criteria are used to select reliable pseudo-labels in SODA?

## Architecture Onboarding

- Component map:
  - Deployed model (black box, only outputs accessible) -> Data adaptor (transforms test data) -> Reliable pseudo-label selector (threshold + class balance) -> ZOO estimator (multi-point directional derivative approximator) -> Training loop (CE loss on reliable set + MI loss on unreliable set)

- Critical path:
  1. Run deployed model on test batch → predictions
  2. Select reliable pseudo-labels using confidence threshold and class balance
  3. Train data adaptor on reliable set using ZOO + CE loss
  4. Train data adaptor on unreliable set using ZOO + MI loss
  5. Update data adaptor parameters
  6. Apply adaptor to test data for inference

- Design tradeoffs:
  - Reliable set size vs. supervision quality
  - Query number q vs. gradient estimation accuracy and computation time
  - Data adaptor depth vs. ZOO gradient estimation stability
  - Confidence threshold τ vs. amount of usable supervision

- Failure signatures:
  - No improvement over deployed model → pseudo-label noise too high or adaptor too weak
  - Degraded performance → incorrect confidence threshold or query number too low
  - Very slow convergence → insufficient supervision or poor ZOO gradient estimates

- First 3 experiments:
  1. Run SODA on CIFAR-10-C Gaussian noise with default hyperparameters; verify accuracy improvement over deployed model.
  2. Vary confidence threshold τ from 0.5 to 0.9; observe impact on accuracy and reliable set size.
  3. Reduce query number q to 2 or 3; measure degradation in performance to assess ZOO sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SODA compare to other methods when distribution shift is extremely severe (beyond 5 severity levels)?
- Basis in paper: Paper only evaluates on 5 severity levels, doesn't explore more extreme shifts
- Why unresolved: No experimental results or theoretical analysis on extreme distribution shifts
- What evidence would resolve it: Experiments on datasets with more severe distribution shifts or theoretical analysis of performance on extreme shifts

### Open Question 2
- Question: How do hyperparameter choices affect SODA's performance on different distribution shifts?
- Basis in paper: Paper mentions robustness to hyperparameters but suggests adaptive threshold might improve performance
- Why unresolved: No detailed analysis of hyperparameter impact on various distribution shifts
- What evidence would resolve it: Comprehensive study of performance with different hyperparameters on various distribution shifts

### Open Question 3
- Question: Can SODA be extended to handle multi-class classification with more than 100 classes?
- Basis in paper: Paper only evaluates on datasets with up to 1000 classes, doesn't explore tasks with >100 classes
- Why unresolved: No experimental results or theoretical analysis on tasks with more than 100 classes
- What evidence would resolve it: Experiments on multi-class classification tasks with more than 100 classes

## Limitations
- Reliability of pseudo-label selection depends heavily on confidence threshold, which may not generalize across corruption types
- Assumes deployed model outputs remain stable enough for reliable pseudo-label extraction, with no analysis for severe performance degradation
- Computational overhead of ZOO with multiple queries may be prohibitive for real-time applications

## Confidence

- **High Confidence**: Core mechanism of splitting data into reliable/unreliable subsets for supervised/unsupervised training is theoretically sound and experimentally supported
- **Medium Confidence**: Effectiveness of mutual information maximization for unsupervised training on unreliable subsets is demonstrated but lacks extensive ablation studies
- **Low Confidence**: Claim that SODA is "not sensitive to query number" is based on limited experiments and may not hold across all corruption types and architectures

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary confidence threshold τ across 0.5 to 0.95 and measure impact on performance across different corruption types to determine optimal threshold selection strategies

2. **Query Number Robustness**: Test SODA with even fewer queries (q=1, q=3) on additional corruption types not covered in main experiments to validate claimed robustness to query number

3. **Severe Degradation Scenario**: Evaluate SODA when deployed model's accuracy drops below 50% on test set to assess whether pseudo-label reliability selection can still provide meaningful adaptation