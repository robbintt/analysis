---
ver: rpa2
title: Learning Performance Maximizing Ensembles with Explainability Guarantees
arxiv_id: '2312.12715'
source_url: https://arxiv.org/abs/2312.12715
tags:
- allocation
- performance
- tree
- which
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for optimal allocation of observations
  between an explainable glass-box model and a black-box model in ensemble learning.
  The approach defines allocation desireability based on sufficient performance and
  absolute performance measures, creating a ranking optimal for any explainability
  level.
---

# Learning Performance Maximizing Ensembles with Explainability Guarantees

## Quick Facts
- arXiv ID: 2312.12715
- Source URL: https://arxiv.org/abs/2312.12715
- Reference count: 40
- Primary result: Achieves 84% accuracy with 64% explainability across 31 benchmark datasets

## Executive Summary
This paper introduces EEG (Ensemble Explainable Glass-box), a method for optimally allocating observations between an explainable glass-box model and a black-box model in ensemble learning. The approach defines allocation desirability based on sufficient performance and absolute performance measures, creating a ranking optimal for any explainability level. The learned allocator consistently maintains ensemble performance at very high explainability levels (74% of observations on average) while also improving explainability in some cases, achieving 76% accuracy in estimating sufficiency categories for individual observations.

## Method Summary
The EEG method learns glass-box and black-box models independently on the full training dataset, then trains an allocator using augmented features including model predictions and disagreement metrics. The allocator predicts a ranking percentile for each observation, which is combined with a feature-independent allocation rule via ensemble. This creates a monotonic ranking that determines which observations get allocated to the explainable model for any desired explainability level. The method is task-agnostic and uses gradient boosting trees or neural networks for the allocator component.

## Key Results
- Achieves 84% average accuracy with 64% explainability across 31 benchmark datasets
- Maintains 74% explainability on average while preserving ensemble performance
- Accurately estimates sufficiency categories with 76% accuracy
- In some cases outperforms both individual component models while improving explainability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Allocation desirability ranking enables optimal performance at any explainability level
- Mechanism: By computing a sufficiency-based ranking using both performance sufficiency and absolute performance differences, observations are ordered so that for any explainability level q, the top nq observations are allocated to the glass box, maximizing ensemble performance while maintaining monotonicity
- Core assumption: Sufficiency functions s'(z) accurately capture when model performance is adequate for reliable explanation
- Evidence anchors: "The learned allocator is shown to consistently maintain ensemble performance at very high explainability levels (74% of observations on average)"
- Break condition: If sufficiency thresholds are poorly chosen or the glass box model has complementary strengths to the black box in unexpected regions

### Mechanism 2
- Claim: Independent global learning of component models prevents explainability collapse
- Mechanism: Learning glass box and black box models separately on full dataset ensures glass box predictions remain explainable in global context, avoiding scenarios where the allocator's decision function subsumes the glass box's predictive role
- Core assumption: Global training provides sufficient information for both models to capture their respective strengths across the entire feature space
- Evidence anchors: "Another important consequence of separate component model learning is that glass box predictions are independently explainable in the global context, and thus immune from 'explainability collapse'"
- Break condition: If local patterns are more important than global ones for either model type

### Mechanism 3
- Claim: Ensemble allocation can outperform both individual models
- Mechanism: By identifying complementary regions of expertise between glass box and black box models, the allocator can distribute observations to leverage each model's strengths, sometimes achieving higher accuracy than either component alone
- Core assumption: The component models have complementary, non-overlapping areas of expertise
- Evidence anchors: "In some cases even outperforming both the component explainable and black box models while improving explainability"
- Break condition: If both models perform similarly across all regions of feature space

## Foundational Learning

- Concept: Sufficient performance metrics and their relationship to allocation decisions
  - Why needed here: The method relies on distinguishing between observations where each model performs adequately (sufficiency) versus observations where performance is insufficient for reliable explanation
  - Quick check question: If a glass box model has 90% accuracy but fails on critical edge cases while a black box has 95% overall accuracy, how would the sufficiency framework handle this allocation?

- Concept: Ranking optimization and percentile-based allocation
  - Why needed here: The core innovation transforms allocation into a ranking problem where observations are ordered by desirability, then allocated based on percentile thresholds
  - Quick check question: If you have 100 observations and want 70% explainability, which observations get allocated to the glass box based on the ranking mechanism?

- Concept: Monotonicity constraints in multi-objective optimization
  - Why needed here: The allocator must maintain consistent allocations across different explainability levels (if observation A is allocated to glass box at level q, it must remain there for all higher q)
  - Quick check question: What happens to the allocation if you increase the explainability level from 60% to 80% - which observations change allocation?

## Architecture Onboarding

- Component map:
  Dataset preprocessing -> Global model training (glass box, black box) -> Allocator training (augmented features) -> Evaluation framework (PPCR, PQEOM, PCFA) -> Ensemble prediction system

- Critical path:
  1. Train glass box and black box models on full training data
  2. Compute sufficiency indicators for each observation
  3. Train allocator to predict ranking percentile based on augmented features
  4. Evaluate allocator performance across explainability levels
  5. Deploy ensemble system with runtime allocation

- Design tradeoffs:
  - Global vs local model training (global provides consistency but may miss local patterns)
  - Feature augmentation complexity (adding prediction disagreement features improves performance but increases dimensionality)
  - Allocator model complexity (more complex models may overfit the ranking task)
  - Sufficiency threshold selection (too strict reduces explainability, too lenient risks poor explanations)

- Failure signatures:
  - Low PPCR values indicate allocator isn't capturing oracle performance
  - High PCFA with poor performance suggests feature-dependent allocator isn't learning useful patterns
  - Low PQEOM indicates no explainability "free lunch" exists in dataset
  - Poor sufficiency accuracy means allocator can't reliably identify which observations are safe for glass box explanation

- First 3 experiments:
  1. Run with default settings on Wine dataset - verify baseline performance meets expectations (84% accuracy at 64% explainability)
  2. Test feature set ablation - compare performance using only original features vs full augmented feature set
  3. Component model sensitivity - train allocators with different glass box/black box combinations to identify best pairing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the EEG method perform on high-dimensional datasets with hundreds or thousands of features?
- Basis in paper: The paper uses datasets with a maximum of 613 features (IsoletR regression), which is relatively low-dimensional compared to modern high-dimensional datasets.
- Why unresolved: The paper does not explore the method's performance on datasets with significantly higher feature dimensions.
- What evidence would resolve it: Experiments on benchmark datasets with hundreds to thousands of features would demonstrate the method's scalability and performance characteristics in high-dimensional settings.

### Open Question 2
- Question: How sensitive is the EEG method to the choice of the ϵ parameter for regression tasks?
- Basis in paper: The paper states that for regression tasks, ϵ is selected as "the lower of the average validation losses of g and b, as a reasonable threshold for prediction correctness," but does not explore sensitivity to different choices of ϵ.
- Why unresolved: The paper does not provide experiments varying the ϵ parameter to understand its impact on performance.
- What evidence would resolve it: Experiments showing performance across a range of ε values would reveal the method's sensitivity and guide appropriate selection of this parameter.

### Open Question 3
- Question: How does the EEG method perform when the explainable and black-box models have similar performance levels?
- Basis in paper: The paper mentions that the component models often have "high relative advantages of (g1, b1) in different segments of the feature space" but does not specifically analyze cases where their overall performance is similar.
- Why unresolved: The paper focuses on cases where models have complementary strengths rather than similar overall performance.
- What evidence would resolve it: Experiments comparing EEG performance when component models have similar vs. different overall performance levels would reveal whether the method's benefits depend on performance complementarity.

## Limitations

- The sufficiency framework assumes global patterns are sufficient, potentially missing important local structures
- The method's performance gains may not be consistent across all dataset types and distributions
- The concept of explainability collapse is mentioned but not empirically validated or compared against alternative approaches

## Confidence

**High confidence** in the core ranking-based allocation mechanism and its monotonic properties. The mathematical formulation is sound and the empirical results across multiple datasets are robust.

**Medium confidence** in the sufficiency-based performance metrics and their relationship to reliable explanations. While intuitively appealing, the connection between performance sufficiency and explanation quality needs more empirical validation.

**Low confidence** in the generalizability of the explainability collapse prevention claim, as this concept isn't empirically tested or compared against alternative approaches that might also prevent such collapse.

## Next Checks

1. **Local vs Global Performance Analysis**: Conduct experiments comparing global-only model training against approaches that incorporate local pattern detection to determine when each strategy is optimal.

2. **Sufficiency Threshold Sensitivity**: Systematically vary the sufficiency thresholds s_g and s_b across multiple datasets to quantify their impact on allocation quality and identify robust threshold selection strategies.

3. **Failure Mode Characterization**: Analyze the 31 benchmark datasets to identify common characteristics of cases where the method underperforms, and develop diagnostic metrics to predict such scenarios in advance.