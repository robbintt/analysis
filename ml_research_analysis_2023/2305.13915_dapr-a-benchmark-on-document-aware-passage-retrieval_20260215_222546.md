---
ver: rpa2
title: 'DAPR: A Benchmark on Document-Aware Passage Retrieval'
arxiv_id: '2305.13915'
source_url: https://arxiv.org/abs/2305.13915
tags:
- passage
- neural
- bm25
- retrieval
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DAPR, a new benchmark for document-aware
  passage retrieval. The key problem addressed is retrieving relevant passages from
  long documents where document-level context (e.g., coreference resolution, background
  knowledge) is critical for success.
---

# DAPR: A Benchmark on Document-Aware Passage Retrieval

## Quick Facts
- arXiv ID: 2305.13915
- Source URL: https://arxiv.org/abs/2305.13915
- Authors: 
- Reference count: 20
- Key outcome: DAPR benchmark shows hybrid BM25-neural retrieval improves document retrieval significantly but only marginally improves passage retrieval, highlighting the challenge of incorporating document context.

## Executive Summary
This paper introduces DAPR, a new benchmark for document-aware passage retrieval that addresses the challenge of retrieving relevant passages from long documents where document-level context is critical for success. The authors create a benchmark from five diverse datasets (MS MARCO, Natural Questions, MIRACL, Genomics, COLIEE) and evaluate neural passage retrievers extended with document context through prepending summaries, pooling, and hybrid retrieval with BM25. The primary finding is that while hybrid retrieval with BM25 significantly improves document retrieval, it only marginally improves passage retrieval, demonstrating the difficulty of the DAPR task and the need for better methods to incorporate document-level context.

## Method Summary
The DAPR benchmark extends neural passage retrievers with document-level context through three approaches: (1) prepending document summaries (titles, leading sentences, or keyphrases) to passages, (2) pooling over passage representations using mean, max, or sum operations, and (3) hybrid retrieval combining BM25 document retrieval with neural passage retrieval via convex combination fusion. The evaluation uses five heterogeneous datasets with zero-shot cross-domain evaluation, measuring both Query-to-Passage (Q2P) and Query-to-Document (Q2D) tasks using nDCG@10 and recall@100 metrics. Base retrievers include BM25 variants, RetroMAE, SPLADEv2, and ColBERTv2.

## Key Results
- Hybrid retrieval with BM25 significantly improves document retrieval but only marginally improves passage retrieval
- Prepending document titles improves performance on some datasets but can harm performance on others
- Error analysis shows 53.5% of errors are due to missing document context, primarily from coreference resolution and background knowledge requirements
- Simple document-context integration methods achieve only modest improvements, indicating substantial room for improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Document-level context improves passage retrieval when the passage alone lacks sufficient information to answer the query.
- Mechanism: The retriever uses context from the document (e.g., titles, preceding sentences) to resolve coreferences and provide background knowledge that bridges the gap between the query and the relevant passage.
- Core assumption: The document contains necessary context not present in the passage itself, and the retriever can effectively leverage this context.
- Evidence anchors:
  - [abstract] "We find the major errors (53.5%) are due to missing document context."
  - [section] "To find the relevant passage to the query, the retriever needs to utilize the document-level context, which in this case means coreference resolution for the noun the venue."
  - [corpus] "Average neighbor FMR=0.298" - Weak corpus signal for document-aware retrieval.
- Break condition: If the document does not contain the necessary context, or if the retriever fails to identify and utilize the relevant context.

### Mechanism 2
- Claim: Hybrid retrieval with BM25 significantly improves document retrieval but only marginally improves passage retrieval.
- Mechanism: BM25 document retrieval provides a strong signal for document-level matching, which helps rank entire documents but does not necessarily improve the ranking of passages within those documents.
- Core assumption: The BM25 document retrieval signal is strong enough to improve document ranking but not necessarily passage ranking within documents.
- Evidence anchors:
  - [abstract] "The hybrid-retrieval systems, the overall best, can only improve on the DAPR tasks marginally while significantly improving on the document-retrieval tasks."
  - [section] "We find the passage-retrieval performance on Genomics, MIRACL, and Natural Questions peaks at 0.3, 0.4, and 0.5 fusion weight, respectively."
  - [corpus] "Average neighbor FMR=0.298" - Weak corpus signal for hybrid retrieval effectiveness.
- Break condition: If the BM25 document retrieval signal is not strong enough to improve document ranking, or if the passage ranking within documents is not improved.

### Mechanism 3
- Claim: Prepending document titles improves passage retrieval performance on some datasets but can harm performance on others.
- Mechanism: The document title provides background information that helps the retriever understand the topic of the document, improving the matching of relevant passages.
- Core assumption: The document title contains relevant background information that can improve passage retrieval, and the retriever can effectively utilize this information.
- Evidence anchors:
  - [abstract] "contextualized passage representations (e.g. prepending document titles) achieve good improvement on these hard queries."
  - [section] "We find the passage-retrieval performance on Genomics, MIRACL, and Natural Questions peaks at 0.3, 0.4, and 0.5 fusion weight, respectively."
  - [corpus] "Average neighbor FMR=0.298" - Weak corpus signal for title-based improvements.
- Break condition: If the document title does not contain relevant background information, or if the retriever fails to effectively utilize the title information.

## Foundational Learning

- Concept: Document-level context
  - Why needed here: Understanding how document-level context (e.g., titles, coreferences, background knowledge) can improve passage retrieval when the passage alone lacks sufficient information.
  - Quick check question: What are some examples of document-level context that can improve passage retrieval?
- Concept: Hybrid retrieval
  - Why needed here: Understanding how to combine BM25 document retrieval with neural passage retrieval to improve document retrieval while potentially improving passage retrieval.
  - Quick check question: How does hybrid retrieval with BM25 improve document retrieval but only marginally improve passage retrieval?
- Concept: Sparse vs. dense retrieval
  - Why needed here: Understanding the differences between sparse (e.g., BM25) and dense (e.g., neural) retrieval methods and how they can be combined in hybrid retrieval.
  - Quick check question: What are the advantages and disadvantages of sparse vs. dense retrieval methods?

## Architecture Onboarding

- Component map: Document preprocessing (chunking, title extraction) -> Passage retrieval models (BM25, neural retrievers) -> Document-context integration (prepending, pooling) -> Fusion methods (convex combination, reciprocal rank fusion) -> Evaluation (nDCG@10, recall@100)
- Critical path: Document preprocessing -> Passage retrieval -> Document-context integration -> Fusion -> Evaluation
- Design tradeoffs: Choice of document-context integration method (prepending vs. pooling), fusion method (convex combination vs. reciprocal rank fusion), balance between document and passage retrieval performance
- Failure signatures: Poor passage retrieval performance on queries requiring document-level context, suboptimal fusion of document and passage retrieval signals
- First 3 experiments:
  1. Evaluate the impact of prepending document titles on passage retrieval performance.
  2. Compare different fusion methods (convex combination, reciprocal rank fusion) for combining BM25 document retrieval with neural passage retrieval.
  3. Analyze the contribution of document-level context to passage retrieval performance on different datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can document-level context be effectively incorporated into neural passage retrievers beyond simple concatenation methods like prepending titles or summaries?
- Basis in paper: [explicit] The paper shows that simple approaches like prepending document titles help in some cases but can also harm performance, and hybrid retrieval with BM25 improves document retrieval but only marginally improves passage retrieval.
- Why unresolved: The paper demonstrates that current methods of incorporating document context are insufficient, with prepending summaries sometimes harming performance and hybrid retrieval failing on hard queries requiring document-context understanding.
- What evidence would resolve it: Development and evaluation of new neural architectures or training methods that can effectively utilize document-level context for passage retrieval, particularly on queries requiring coreference resolution or background knowledge.

### Open Question 2
- Question: What specific types of document-level context are most critical for improving passage retrieval performance across different domains?
- Basis in paper: [explicit] The error analysis shows that 48.2% of errors are in self-contained queries, while coreference resolution (14.5%), background provision (20.0%), and multi-hop reasoning (8.2%) account for significant portions of cases where document context is needed.
- Why unresolved: While the paper identifies different categories of document-level context requirements, it doesn't determine which types are most important for which domains or query types.
- What evidence would resolve it: Detailed analysis of which document context types are most beneficial for specific domains (legal, biomedical, general knowledge) and query characteristics, potentially leading to domain-specific retrieval strategies.

### Open Question 3
- Question: How can retrieval systems be designed to automatically determine when document-level context is necessary versus when it might be harmful to retrieval performance?
- Basis in paper: [inferred] The paper shows that document summaries can severely harm performance on some datasets (like Genomics) while helping on others (like Natural Questions), suggesting that indiscriminate use of document context is problematic.
- Why unresolved: Current approaches apply document context uniformly, but the paper's results suggest this is suboptimal. There's no mechanism to determine when to use or ignore document context.
- What evidence would resolve it: Development of methods to predict when document context will be beneficial for a given query-document pair, possibly through query classification or learned attention mechanisms that can selectively utilize document information.

## Limitations

- Modest and inconsistent performance gains from document-context methods across datasets suggest current approaches are highly dataset-dependent
- The corpus signal analysis shows only moderate relevance (FMR=0.298) among related work, suggesting the problem space may not be as well-established as claimed
- Error analysis based on manual subset analysis (53.5% missing context) may not be representative across all query types and datasets

## Confidence

- **High confidence**: The observation that hybrid retrieval significantly improves document retrieval but only marginally improves passage retrieval is well-supported by the experimental results across all five datasets.
- **Medium confidence**: The claim that 53.5% of errors are due to missing document context is based on manual analysis of a subset, which may not be representative across all query types and datasets.
- **Low confidence**: The assertion that simple context integration methods like title prepending represent a "substantial room for improvement" - while true that performance varies, the analysis doesn't clearly demonstrate these simple methods are inferior to more complex alternatives.

## Next Checks

1. Conduct ablation studies on the document-context integration methods to quantify the exact contribution of each context type (titles vs. summaries vs. keyphrases) across different query categories.
2. Test the zero-shot generalization capability on additional domains beyond the five datasets to validate whether the observed patterns hold for truly unseen document types.
3. Perform error analysis specifically on the subset of queries where document context was supposedly critical, to verify whether the context integration methods actually addressed the identified failure modes.