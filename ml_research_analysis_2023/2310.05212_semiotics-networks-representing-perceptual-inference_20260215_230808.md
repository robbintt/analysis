---
ver: rpa2
title: Semiotics Networks Representing Perceptual Inference
arxiv_id: '2310.05212'
source_url: https://arxiv.org/abs/2310.05212
tags:
- classifier
- image
- person
- training
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Semiotics Networks (CONNs) as a computational
  model to simulate object perception and human communication. The model captures
  how individuals perceive objects ("observed" vs.
---

# Semiotics Networks Representing Perceptual Inference

## Quick Facts
- arXiv ID: 2310.05212
- Source URL: https://arxiv.org/abs/2310.05212
- Reference count: 40
- Key outcome: CONN-based classifiers outperform standard models on small training datasets by leveraging attractor representations and stochastic perturbations

## Executive Summary
This paper introduces Semiotics Networks (CONNs) as a computational model to simulate object perception and human communication. The model captures how individuals perceive objects ("observed" vs. "seen" representations) and exchange these perceptions in dialogue, using encoder-decoder operations of autoencoders to generate attractors representing internal representations. The authors demonstrate that person-to-person communication in this framework converges to bipartite orbits, forming periodic cycles of transmitted images. They apply the model to visual classification tasks, showing that CONN-based classifiers outperform standard models on small training datasets by leveraging attractor representations and stochastic perturbations.

## Method Summary
The CONN framework trains convolutional autoencoders on distinct subsets of data (e.g., odd/even digits), then uses iterative encoder-decoder operations to generate attractor representations of input images. For communication simulation, two autoencoders alternate transformations on transmitted images, forming bipartite orbits. For classification, the framework converts images to attractor representations, with a stochastic variant using random perturbations during convergence to create ensembles of attractors that improve classification performance on small datasets.

## Key Results
- CONN-based classifiers outperform standard models on small training datasets (5-50 examples per class)
- Person-to-person communication converges to bipartite orbits, forming periodic cycles of transmitted images
- Stochastic perturbations during attractor convergence enhance classification performance by creating informative ensembles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoder-decoder iterations of convolutional autoencoders converge to attractors that represent perceived images.
- Mechanism: The paper demonstrates that repeated application of encoder and decoder operations on an input image forms a sequence that converges to a fixed point, called an attractor. This attractor represents the "seen" version of the observed image, capturing the network's internal representation of perception.
- Core assumption: Overparameterized autoencoders have the property that their encoder-decoder iterations converge to attractors, and these attractors preserve the essential features of the original input.
- Evidence anchors:
  - [abstract] "using encoder-decoder operations of autoencoders to generate attractors representing internal representations"
  - [section] "It has been empirically shown that for overparameterized autoencoders, as n approaches infinity, the sequence described in Eq. 4 converges to an attractor."
  - [corpus] Weak - no direct matches found in corpus neighbors, but related to concepts of perception and neural networks.
- Break condition: If the autoencoder is not overparameterized, convergence to attractors may not occur, or the attractors may not preserve the input features adequately.

### Mechanism 2
- Claim: Alternating application of two different autoencoder functions between two "persons" leads to bipartite orbits in the sequence of transmitted images.
- Mechanism: When two entities with different autoencoder functions communicate by alternating their transformations on transmitted images, the sequence of images does not converge to a single attractor. Instead, it forms a periodic cycle (bipartite orbit) where the images alternate between two sets, each set being an attractor of one of the autoencoder functions.
- Core assumption: The autoencoder functions are continuous and satisfy certain natural conditions that allow for the formation of bipartite orbits when applied alternately.
- Evidence anchors:
  - [abstract] "demonstrate that person-to-person communication in this framework converges to bipartite orbits, forming periodic cycles of transmitted images."
  - [section] "Specifically, we will identify two types of periodicity in the sequence of transmitted images between the persons... These properties will play an important role in our subsequent exploration of the semiotic properties of interpersonal communication."
  - [corpus] Weak - no direct matches, but relates to concepts of communication and neural networks.
- Break condition: If the autoencoder functions do not satisfy the necessary conditions, or if the number of internal steps is not sufficient, the sequence may not converge to a bipartite orbit.

### Mechanism 3
- Claim: Stochastic perturbations during attractor convergence enhance the expressiveness of the attractor-based representation for classification.
- Mechanism: By introducing small random augmentations during the iterative convergence process, the classifier generates an ensemble of attractors close to the input sample. This ensemble provides a more informative representation than a single attractor, improving classification performance, especially on small training datasets.
- Core assumption: The random perturbations, when averaged, provide a richer representation of the input that captures more relevant features for classification.
- Evidence anchors:
  - [abstract] "showing that CONN-based classifiers outperform standard models on small training datasets by leveraging attractor representations and stochastic perturbations."
  - [section] "Instead of representing Im solely by a sequence of elements converging to an attractor... we introduce J > 0 sequences that start with Im and converge to attractors. These sequences are constructed in a similar manner as described in Eq. 21, but they also incorporate random augmentations."
  - [corpus] Weak - no direct matches, but relates to concepts of stochastic processes and classification.
- Break condition: If the perturbations are too large, they may lead to attractors that do not represent the input sample well, degrading classification performance.

## Foundational Learning

- Concept: Attractor Dynamics in Overparameterized Autoencoders
  - Why needed here: Understanding how encoder-decoder iterations converge to attractors is fundamental to the CONN model's representation of perception and its application to classification.
  - Quick check question: Why do overparameterized autoencoders tend to map input examples to attractors during repeated encoding-decoding operations?

- Concept: Bipartite Orbits in Dynamical Systems
  - Why needed here: The concept of bipartite orbits explains the periodic behavior observed in person-to-person communication within the CONN model and its relation to semiotic phenomena.
  - Quick check question: What conditions must two functions satisfy for their alternating application to result in a bipartite orbit?

- Concept: Stochastic Ensemble Methods in Classification
  - Why needed here: The use of stochastic perturbations to create ensembles of attractors is a key innovation that improves the classifier's performance on small datasets.
  - Quick check question: How does averaging over multiple perturbed convergence paths provide a more informative representation than a single attractor?

## Architecture Onboarding

- Component map:
  - Convolutional Autoencoders (AP1, AP2) -> Attractor Convergence Module -> Stochastic Perturbation Module (optional) -> Benchmark Classifier (M) -> Perceptualization Layer

- Critical path:
  1. Train autoencoders on respective datasets (e.g., odd/even digits)
  2. For each input image, perform iterative encoder-decoder operations to find attractors
  3. In the stochastic classifier, apply random perturbations during convergence
  4. Train the benchmark classifier on the attractor representations
  5. During inference, convert test images to attractor representations and classify

- Design tradeoffs:
  - Deterministic vs. Stochastic: Deterministic (vanilla) classifier is faster but less expressive; stochastic classifier is slower but more accurate on small datasets
  - Overparameterization: Necessary for attractor convergence but increases computational cost and risk of overfitting
  - Number of Perturbations: More perturbations improve representation but increase computation time

- Failure signatures:
  - Convergence Failure: If autoencoders are not overparameterized, sequences may not converge to attractors, breaking the perception model
  - Poor Classification: If attractor representations do not preserve class-relevant features, classification performance will degrade
  - Periodic Artifacts: If bipartite orbits do not form correctly, the communication model may not capture semiotic phenomena accurately

- First 3 experiments:
  1. Train autoencoders on a small subset of MNIST and verify convergence to attractors for individual images
  2. Simulate person-to-person communication with two autoencoders and observe formation of bipartite orbits in the transmitted image sequence
  3. Compare classification performance of vanilla and stochastic CONN classifiers on small training datasets against a standard classifier

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions do the bipartite orbits of the first type exist for arbitrary functions F1 and F2 operating in metric spaces?
- Basis in paper: [explicit] The authors state "Although the existence of such orbits was not formally proven, in our experiments with the autoencoder generated images... we empirically observed convergence to the bipartite orbits for every initial Im, nsteps1, and nsteps2"
- Why unresolved: The paper provides empirical evidence but lacks a formal mathematical proof of the conditions required for bipartite orbits of the first type to exist.
- What evidence would resolve it: A rigorous mathematical proof demonstrating the conditions (e.g., continuity, Lipschitz continuity, or other properties of F1 and F2) that guarantee convergence to bipartite orbits of the first type.

### Open Question 2
- Question: How do the properties of bipartite orbits of the second type differ from those of the first type when the internal communication parameters nsteps1 and nsteps2 tend to infinity?
- Basis in paper: [explicit] The authors state "In this subsection, similar to the previous one, we will explore the periodic properties of the converging sequences... we will consider the behavior of the 'internal' person parameters, nsteps1 and nsteps2, as they tend towards infinity in U(Im, nsteps1, nsteps2)"
- Why unresolved: The paper describes the existence of bipartite orbits of the second type but does not provide a detailed comparison of their properties with those of the first type, particularly when nsteps1 and nsteps2 approach infinity.
- What evidence would resolve it: A comprehensive analysis of the properties of bipartite orbits of the second type, including their periodicity, fixed points, and algebraic operations, compared to those of the first type, when nsteps1 and nsteps2 tend to infinity.

### Open Question 3
- Question: How does the performance of the stochastic CONN classifier change with increasing training dataset size, and at what point does it start to underperform the benchmark classifier?
- Basis in paper: [explicit] The authors state "The stochastic classifier... shows improved classification performance, making it a more suitable choice for classification tasks with small training datasets. However, as the number of training samples increases, the benchmark classifier starts to slightly outperform the stochastic one."
- Why unresolved: The paper provides limited information on the performance of the stochastic CONN classifier as the training dataset size increases, and does not specify the exact point at which it starts to underperform the benchmark classifier.
- What evidence would resolve it: A detailed analysis of the performance of the stochastic CONN classifier on training datasets of increasing sizes, including the point at which its performance starts to degrade compared to the benchmark classifier.

## Limitations

- Reliance on overparameterized autoencoders limits scalability and increases computational costs
- Stochastic perturbation approach introduces additional hyperparameters requiring careful tuning
- Limited experimental scope focused on MNIST dataset may not generalize to more complex scenarios

## Confidence

- High confidence in the attractor convergence mechanism and its theoretical foundations
- Medium confidence in the bipartite orbit formation under alternating transformations
- Medium confidence in the classification performance improvements, given the limited experimental scope

## Next Checks

1. Test the CONN framework on a larger-scale classification task (e.g., CIFAR-10) to evaluate scalability and generalization beyond MNIST
2. Conduct ablation studies on the number of internal steps (nsteps) and perturbation parameters to quantify their impact on classification performance
3. Implement adversarial robustness tests to verify the claimed resilience against attacks and compare against standard classifiers under identical conditions