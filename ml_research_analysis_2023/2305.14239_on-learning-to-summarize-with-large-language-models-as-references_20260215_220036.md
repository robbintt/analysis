---
ver: rpa2
title: On Learning to Summarize with Large Language Models as References
arxiv_id: '2305.14239'
source_url: https://arxiv.org/abs/2305.14239
tags:
- learning
- training
- evaluation
- summary
- summaries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using large language models (LLMs) as references
  to train smaller summarization models, addressing the performance gap between supervised
  models and LLMs. The authors investigate a new learning paradigm where LLMs serve
  as both oracle summary generators for standard supervised fine-tuning and quality
  evaluators for contrastive learning.
---

# On Learning to Summarize with Large Language Models as References

## Quick Facts
- arXiv ID: 2305.14239
- Source URL: https://arxiv.org/abs/2305.14239
- Reference count: 26
- One-line primary result: Smaller summarization models trained with LLMs as references achieve performance on par with or surpassing the reference LLMs when evaluated by those same LLMs

## Executive Summary
This paper proposes a new learning paradigm for text summarization where large language models (LLMs) serve as both oracle summary generators for supervised fine-tuning and quality evaluators for contrastive learning. The authors investigate how to leverage LLMs to train smaller summarization models, addressing the performance gap between supervised models and LLMs. By using LLMs to generate quasi-reference summaries and evaluate candidate summaries, they enable training techniques beyond maximum likelihood estimation, including contrastive learning that can leverage continuous quality distributions over multiple candidate summaries.

## Method Summary
The method uses LLMs to generate summaries for training data and provide quality evaluations for contrastive learning. Two approaches are explored: GPTScore, which uses LLM-predicted probabilities as quality scores, and GPTRank, which requires LLMs to rank summary quality with explanations. The training combines label-smoothed MLE with contrastive loss, where the contrastive component uses BRIO to enforce that the model assigns higher probabilities to better-ranked candidate summaries. Candidate summaries are generated using diverse beam search, and the model is fine-tuned to approximate the LLM's distribution over possible summaries rather than just learning from a single reference.

## Key Results
- Contrastive learning with LLM-as-reference significantly outperforms standard supervised fine-tuning under both low and high resource settings on CNN/DailyMail
- The fine-tuned models achieve performance on par with or surpassing the reference LLMs when evaluated by the LLMs themselves
- GPTRank and GPTScore both enable effective contrastive learning, with GPTScore showing more consistent improvements
- The approach demonstrates significant performance improvement in both LLM and human evaluations

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Learning with LLM-as-Reference Distribution
The LLM's probability distribution over candidate summaries captures meaningful quality information that can guide smaller model training. By sorting candidates using LLM-assigned quality scores and training the model to assign higher probabilities to higher-ranked candidates, the model learns to approximate the LLM's distribution over possible summaries.

### Mechanism 2: Multi-Task Learning with Label Smoothing and Contrastive Loss
Combining label-smoothed MLE with contrastive loss creates a more robust training objective that prevents the model from collapsing to a single output. The label smoothing prevents the model from assigning all probability mass to the quasi-reference summary, while the contrastive component pushes the model to differentiate between summaries of varying quality.

### Mechanism 3: GPTScore and GPTRank as Different Quality Evaluation Strategies
Different LLM-based evaluation methods provide distinct quality signals that affect contrastive learning outcomes. GPTScore uses the LLM's predicted probability as a quality score, while GPTRank requires the LLM to rank summaries, potentially capturing different aspects of summary quality.

## Foundational Learning

- Concept: Auto-regressive sequence generation and probability factorization
  - Why needed here: Understanding how the model computes pg(S|D) = ∏ pg(si|S<i, D) is fundamental to grasping how contrastive learning modifies the training objective
  - Quick check question: Why does the model factorize the probability of a summary into a product of conditional probabilities over tokens?

- Concept: Contrastive learning and margin-based objectives
  - Why needed here: The contrastive loss requires understanding how to compare two candidate summaries and enforce a margin between their assigned probabilities
  - Quick check question: What is the mathematical form of the contrastive loss that requires the model to assign a probability at least twice as large to a better candidate?

- Concept: Label smoothing and multi-task learning
  - Why needed here: The training combines MLE with contrastive learning, requiring understanding of how to balance these objectives and why label smoothing is necessary
  - Quick check question: How does label smoothing prevent the model from collapsing to a single output when training with quasi-reference summaries?

## Architecture Onboarding

- Component map: LLM summary generation -> MLE warm-starting -> Candidate generation -> Contrastive training with quality evaluation -> Model checkpoint selection
- Critical path: LLM summary generation → MLE warm-starting → Candidate generation → Contrastive training with quality evaluation → Model checkpoint selection
- Design tradeoffs: Using GPTScore vs GPTRank (simpler implementation vs potentially more diverse quality signals), number of candidate summaries (more candidates provide better quality signals but increase computational cost), weighting of contrastive vs MLE loss (higher contrastive weight may improve quality but risk instability)
- Failure signatures: Model collapses to generating only quasi-reference summaries (label smoothing too weak), contrastive loss doesn't converge (quality evaluation not meaningful or too noisy), model performance degrades compared to MLE-only training (contrastive signal not helpful)
- First 3 experiments: 1) Compare MLE training with contrastive training using GPTScore on a small subset of CNNDM to verify performance improvement, 2) Test GPTRank vs GPTScore as quality evaluation methods using the same contrastive learning framework, 3) Vary the number of candidate summaries and beam search parameters to find optimal diversity-quality tradeoff

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important ones emerge from the work:

1. How does the performance of summarization models trained with LLMs as references compare to human evaluators' preferences for LLM-generated summaries over reference summaries?

2. What is the impact of using different sampling strategies during LLM summary generation on the quality of the resulting fine-tuned models?

3. How does the performance of contrastive learning with LLMs as references scale with the size of the summarization model being fine-tuned?

## Limitations

- The evaluation methodology creates a circular problem where models are evaluated by the same system used for training supervision
- Results are limited to CNN/DailyMail dataset, restricting generalizability to other summarization domains
- The fundamental assumption that LLM-generated quality scores provide meaningful supervision signals remains empirically uncertain

## Confidence

- High confidence: Technical implementation of contrastive learning with BRIO and multi-task training objective
- Medium confidence: Performance improvements reported on CNN/DailyMail are substantial and statistically significant, but evaluation methodology raises questions
- Medium confidence: Comparison between GPTScore and GPTRank provides useful insights, though corpus evidence for GPTRank's advantages is limited

## Next Checks

1. Conduct comprehensive human evaluation comparing model outputs from standard MLE training versus contrastive learning approaches to validate LLM-based quality assessments

2. Evaluate trained models on multiple summarization datasets (XSum, Newsroom, or arXiv) to assess generalization beyond CNN/DailyMail

3. Systematically compare GPTScore versus GPTRank across different model sizes, training regimes, and quality metrics to determine context-dependent advantages