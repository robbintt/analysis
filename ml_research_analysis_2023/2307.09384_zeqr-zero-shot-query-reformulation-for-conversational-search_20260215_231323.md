---
ver: rpa2
title: 'ZeQR: Zero-shot Query Reformulation for Conversational Search'
arxiv_id: '2307.09384'
source_url: https://arxiv.org/abs/2307.09384
tags:
- conversational
- query
- search
- zeqr
- omission
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes ZeQR, a zero-shot query reformulation framework
  for conversational search that addresses coreference and omission ambiguities without
  requiring conversational search data. The method converts query reformulation into
  machine reading comprehension problems and uses language models to resolve ambiguities
  explicitly.
---

# ZeQR: Zero-shot Query Reformulation for Conversational Search

## Quick Facts
- arXiv ID: 2307.09384
- Source URL: https://arxiv.org/abs/2307.09384
- Authors: 
- Reference count: 18
- Key outcome: Zero-shot query reformulation framework achieving up to 40% NDCG@5 improvement over ConvDR-ZS and ZeCo2 baselines

## Executive Summary
ZeQR addresses the challenge of query reformulation in conversational search without requiring conversational search data. The framework explicitly resolves coreference and omission ambiguities through machine reading comprehension, converting conversational queries into machine-readable questions. Experiments on four TREC conversational datasets demonstrate consistent improvements over state-of-the-art zero-shot baselines, with the omission resolution component identified as the primary source of performance gains.

## Method Summary
ZeQR employs a two-step framework for query reformulation, first resolving coreference ambiguities and then addressing omission ambiguities. The method uses template-based questions to convert conversational queries into machine reading comprehension problems, which are solved by BERT models fine-tuned on SQuAD. The framework works with any retriever without additional adaptation, distinguishing it from model-specific contextualization approaches. ZeQR identifies important words for omission resolution using IDF scores and systematically resolves ambiguities to produce context-independent queries.

## Key Results
- NDCG@5 improvements of up to 40% over ConvDR-ZS and ZeCo2 baselines
- Omission resolution identified as primary performance driver in ablation studies
- Framework works with various retrievers (TCT-ColBERT, ANCE, BM25) with consistent performance
- Maintains performance close to supervised methods while providing greater explainability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ZeQR explicitly resolves conversational ambiguities (coreference and omission) through MRC formulation, leading to better query reformulation.
- **Mechanism:** The framework transforms conversational queries into machine reading comprehension problems by using template-based questions that target specific ambiguity types. MRC models then extract answers from context to resolve these ambiguities.
- **Core assumption:** MRC models trained on general datasets (like SQuAD) can effectively resolve domain-specific conversational ambiguities when posed with appropriate questions.
- **Evidence anchors:**
  - [abstract] "Specifically, our framework utilizes language models designed for machine reading comprehension tasks to explicitly resolve two common ambiguities: coreference and omission, in raw queries."
  - [section 3.3] "To achieve coreference resolution through machine reading comprehension, a universal template is employed to generate a coreference resolution question q'coref for each sample"
  - [corpus] Weak evidence - no direct corpus studies showing MRC effectiveness on conversational data
- **Break condition:** MRC models fail to generalize from general domain data to conversational search context, or template questions don't adequately capture ambiguity types.

### Mechanism 2
- **Claim:** The two-step framework (coreference resolution followed by omission resolution) is necessary and optimal for query reformulation.
- **Mechanism:** Coreference resolution is performed first because pronoun replacement can introduce new omission ambiguities. This ordering ensures all ambiguities are systematically resolved.
- **Core assumption:** The order of resolution matters because resolving one type of ambiguity can create or reveal another type.
- **Evidence anchors:**
  - [section 4.3.1] "One thing worth to be mentioned is that the order of the two-step framework is important, as pronoun replacement may introduce new omission ambiguities into a query"
  - [section 3.3] "To improve the efficiency of the omission resolution process, the module only resolves omission resolution for important words with IDF scores higher than a hyperparameter η"
  - [corpus] Moderate evidence - ablation studies show performance drops when order is reversed
- **Break condition:** Performance is similar regardless of resolution order, or omission resolution should come first for certain query types.

### Mechanism 3
- **Claim:** ZeQR's generality (working with any retriever) provides significant advantages over model-specific zero-shot methods.
- **Mechanism:** By reformulating queries into context-independent forms rather than embedding them directly, ZeQR can use any existing retriever without modification, allowing benefits from new retrieval methods.
- **Core assumption:** Query reformulation is more effective than query contextualization for conversational search, and this approach doesn't sacrifice retrieval quality.
- **Evidence anchors:**
  - [abstract] "In comparison to existing zero-shot methods, our approach is universally applicable to any retriever without additional adaptation or indexing"
  - [section 3.2] "Unlike existing query contextualization methods that depend on specific dense retrieval models, the ZeQR framework is method-agnostic"
  - [corpus] Strong evidence - Table 5 shows ZeQR works well with multiple retrievers (TCT-ColBERT, ANCE, BM25)
- **Break condition:** Specific retrievers require model-specific adaptations for optimal performance, or query reformulation loses information that contextualization preserves.

## Foundational Learning

- **Concept: Machine Reading Comprehension**
  - Why needed here: MRC provides the foundation for ZeQR's ambiguity resolution approach by extracting answers from context given specific questions.
  - Quick check question: How does an MRC model differ from a standard question answering model in terms of input format and answer extraction?

- **Concept: Conversational Search Ambiguities**
  - Why needed here: Understanding coreference and omission is crucial for designing the query reformulation framework.
  - Quick check question: What distinguishes coreference ambiguity from omission ambiguity in conversational queries?

- **Concept: Zero-shot Learning**
  - Why needed here: ZeQR operates without conversational search data, relying instead on general MRC datasets.
  - Quick check question: What are the key challenges of zero-shot learning compared to supervised learning in IR tasks?

## Architecture Onboarding

- **Component map:**
  Input Layer -> Coreference Resolution Module -> Omission Resolution Module -> Output Layer -> Retriever Integration

- **Critical path:**
  1. Receive raw query and context
  2. Coreference resolution (if pronouns present)
  3. Omission resolution (for important words)
  4. Pass reformulated query to retriever
  5. Return retrieved results

- **Design tradeoffs:**
  - Template-based vs. generative approaches: Templates are simpler but may miss complex ambiguities
  - Two-step vs. unified resolution: Two-step is more systematic but computationally heavier
  - MRC model choice: BERT-base provides good balance of performance and efficiency
  - Context window: Using only most recent passage limits context but improves efficiency

- **Failure signatures:**
  - Poor performance on queries with multiple overlapping ambiguities
  - Degradation when canonical passages are long and complex
  - Suboptimal results when important words have low IDF scores
  - Issues with pronoun resolution when referent is not in immediate context

- **First 3 experiments:**
  1. Ablation test: Remove coreference resolution module and measure performance drop
  2. Retrieval comparison: Test ZeQR with different retrievers (BM25, ANCE, TCT-ColBERT) on same queries
  3. Template variation: Modify MRC question templates and evaluate impact on ambiguity resolution accuracy

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: How does the performance of ZeQR vary when using different MRC datasets for training the language model?
  - Basis in paper: [explicit] The authors mention plans to explore the impact of employing different MRC datasets on ZeQR's zero-shot retrieval performance in the future work section.
  - Why unresolved: This question is explicitly stated as a future research direction in the paper, indicating that the authors have not yet conducted experiments to determine how different MRC datasets affect performance.
  - What evidence would resolve it: Conducting experiments with various MRC datasets (e.g., SQuAD, SQuAD2, QuAC) and comparing the resulting ZeQR performance would provide empirical evidence to answer this question.

- **Open Question 2**
  - Question: What strategies can be employed to reduce the computational complexity introduced by ZeQR's two-step resolution process without compromising accuracy?
  - Basis in paper: [explicit] The authors mention investigating strategies for mitigating the computational complexity introduced by ZeQR's two-step resolution process as a future research direction.
  - Why unresolved: This question is explicitly stated as a future research direction in the paper, indicating that the authors have not yet developed or tested strategies to address the computational complexity of their framework.
  - What evidence would resolve it: Developing and testing various strategies to optimize the two-step resolution process (e.g., parallel processing, model compression, or pruning techniques) and evaluating their impact on computational efficiency and accuracy would provide evidence to answer this question.

- **Open Question 3**
  - Question: How does the performance of ZeQR change when using different retrievers as the subsequent retrieval step?
  - Basis in paper: [explicit] The authors present a comparison of different retrievers (TCT-ColBERT, ANCE, BM25) in the "Impact of Using Different Retriever" section, showing that TCT-ColBERT performs best for conversational search tasks.
  - Why unresolved: While the authors provide some evidence of how different retrievers affect performance, they do not explore the full range of possible retrievers or provide a comprehensive analysis of the impact of using different retrievers.
  - What evidence would resolve it: Conducting experiments with a wider range of retrievers (e.g., BERT, RoBERTa, or other dense retrievers) and evaluating their performance when integrated with ZeQR would provide a more comprehensive understanding of how different retrievers affect the framework's overall performance.

## Limitations
- Framework's reliance on template-based MRC questions may struggle with complex or nested ambiguities
- IDF-based filtering for omission resolution (η=2.65) is dataset-specific and may require tuning for different domains
- While zero-shot in conversational search, the BERT models are fine-tuned on SQuAD, introducing weak supervision

## Confidence
- **High confidence**: ZeQR's consistent performance improvement over zero-shot baselines across all four TREC datasets, and the ablation studies showing omission resolution as the primary performance driver
- **Medium confidence**: The generality claims across different retrievers, as the evaluation covers three retrievers but doesn't exhaustively test all possible retriever architectures
- **Medium confidence**: The efficiency claims relative to generative methods, as the computational overhead of the two-step MRC process versus direct generation is not fully characterized

## Next Checks
1. **Cross-domain robustness**: Evaluate ZeQR on non-TREC conversational datasets (e.g., academic search or e-commerce) to test generalizability beyond curated TREC data
2. **Template coverage analysis**: Systematically test the framework's ability to handle edge cases with multiple, nested, or ambiguous pronouns and omissions that may not fit template patterns
3. **Human evaluation study**: Conduct user studies to assess whether ZeQR's reformulations actually improve user understanding and search effectiveness compared to other methods