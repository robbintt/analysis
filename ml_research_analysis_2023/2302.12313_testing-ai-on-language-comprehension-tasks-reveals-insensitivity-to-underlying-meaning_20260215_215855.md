---
ver: rpa2
title: Testing AI on language comprehension tasks reveals insensitivity to underlying
  meaning
arxiv_id: '2302.12313'
source_url: https://arxiv.org/abs/2302.12313
tags:
- sentence
- language
- correct
- have
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) have made significant strides in natural
  language processing, leading to claims of human-like linguistic capabilities. However,
  their performance on specialized tasks may not reflect true language understanding.
---

# Testing AI on language comprehension tasks reveals insensitivity to underlying meaning

## Quick Facts
- arXiv ID: 2302.12313
- Source URL: https://arxiv.org/abs/2302.12313
- Reference count: 34
- Large Language Models perform at chance accuracy on comprehension tasks and significantly underperform humans

## Executive Summary
This study systematically evaluates seven state-of-the-art Large Language Models on a novel benchmark testing language comprehension through targeted comprehension questions about short texts. The results reveal that LLMs perform at chance accuracy and exhibit considerable variability in their answers, significantly underperforming humans on the same tasks. The errors made by LLMs demonstrate distinctly non-human patterns in language understanding, suggesting that despite their utility in various applications, current AI models do not match human language comprehension capabilities. The study attributes this limitation to the absence of a compositional operator for regulating grammatical and semantic information in current LLM architectures.

## Method Summary
The researchers created a dataset of 26,680 data points by prompting seven state-of-the-art LLMs with comprehension questions in two settings (one-word or open-length replies) about short texts featuring high-frequency linguistic constructions. The questions targeted grammatical illusions, semantic anomalies, complex nested hierarchies, and self-embeddings. The LLM responses were compared against 400 human responses to evaluate performance on comprehension questions, focusing on accuracy and variability in answers.

## Key Results
- LLMs perform at chance accuracy on comprehension questions, significantly underperforming humans
- LLM answers exhibit considerable variability compared to human responses
- LLM errors demonstrate non-human patterns, including failure to detect grammatical violations and semantic anomalies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail on linguistic comprehension tasks involving rare or complex syntactic structures because they lack a compositional operator that regulates grammatical and semantic information.
- Mechanism: LLMs generate text by predicting the next word based on statistical patterns from their training data. When faced with constructions that deviate from common patterns, they cannot apply compositional rules to parse and interpret the structure correctly.
- Core assumption: Language understanding requires more than statistical word prediction; it requires a generative grammar system that can handle novel syntactic structures and their meanings.
- Evidence anchors:
  - [abstract] "The tested models are outperformed by humans, and qualitatively their answers showcase distinctly non-human errors in language understanding... This limitation may stem from their absence of a compositional operator for regulating grammatical and semantic information."
  - [section] "LLMs have been linked to a degree of encoding of language knowledge... However, complementary efforts have also shown that knowledge-based accounts can be substituted by statistical ones, so that it might be the case that the task construal hypothesized by the experimenters was different from what actually occurred."

### Mechanism 2
- Claim: LLMs rely on statistical heuristics rather than true language knowledge, leading to inconsistent performance and errors when linguistic rules are violated.
- Mechanism: LLMs use statistical patterns and co-occurrence frequencies to predict words. When a linguistic rule is violated in a way that does not disrupt the overall statistical pattern, the LLM may still generate a plausible output.
- Core assumption: Statistical patterns in language are sufficient for many tasks, but not for understanding the underlying grammatical rules and their interactions with meaning.
- Evidence anchors:
  - [section] "The prompt in 6.1 is ungrammatical. The returned grammaticality judgement is correct, and both of the provided alternatives are correct. It seems that when the ungrammaticality boils down to the violation of a single, salient, morphological rule that does not heavily impact the syntax-semantics interface, the model is not facing the kind of issues that force it to break down."
  - [section] "In the coordinate structure constraint, for instance, GPT-3 returns as a correction the sentence *What did Peter eat ravioli and?, indifferent to the fact that this is not a possible one in English."

### Mechanism 3
- Claim: LLMs cannot map lexical meaning onto real-world concepts, leading to failures in understanding semantic anomalies and producing nonsensical outputs.
- Mechanism: LLMs process words as tokens and generate text based on statistical patterns, but they do not have a mechanism to connect words to their real-world meanings or to reason about the implications of those meanings.
- Core assumption: Understanding language requires not just processing words but also mapping them onto concepts and reasoning about their relationships in the real world.
- Evidence anchors:
  - [section] "Failure in the detection of semantic anomalies challenges the hypothesis that relations between concepts contribute to the encoding of meaning in LLMs... The LLM output disregards both anomalies."
  - [section] "The LLM has no means of mapping representations of lexical meaning onto independent cognitive models of the world, or some manner of commonsense reasoning."

## Foundational Learning

- Concept: Compositional semantics
  - Why needed here: Understanding how meaning is built from the combination of words and grammatical structures is crucial for interpreting the LLM's failures on complex syntactic constructions.
  - Quick check question: Can you explain how the meaning of "The key to the drawers are on the table" changes when corrected to "The key to the drawers is on the table"?

- Concept: Statistical vs. rule-based language processing
  - Why needed here: Distinguishing between the LLM's statistical approach to language and the human use of grammatical rules is key to understanding the source of their different performance on linguistic tasks.
  - Quick check question: Why might an LLM incorrectly judge "More people have been to Russia than I have" as grammatical?

- Concept: Semantic anomaly detection
  - Why needed here: Recognizing how humans detect and process semantic anomalies (like burying survivors) versus how LLMs fail to do so is essential for understanding their limitations in language comprehension.
  - Quick check question: Why does the LLM fail to recognize that survivors should not be buried?

## Architecture Onboarding

- Component map: Input processing -> Tokenization -> Prediction layer -> Output generation
- Critical path: Tokenization → Prediction → Output assembly; failure occurs when the prediction layer relies solely on statistical patterns without checking for grammatical or semantic validity
- Design tradeoffs: Size vs. efficiency (larger models may capture more statistical patterns but still lack rule-based understanding); Pre-training data vs. generalization (more data may improve performance on common constructions but not on rare or novel ones)
- Failure signatures: Accepting ungrammatical sentences as grammatical; Producing semantically nonsensical outputs; Inconsistent performance on tasks requiring grammatical or semantic reasoning
- First 3 experiments:
  1. Test the LLM on a set of grammatical illusions and compare its performance to humans
  2. Evaluate the LLM's ability to detect and correct agreement attraction errors
  3. Assess the LLM's understanding of semantic anomalies by asking it to explain why certain scenarios are impossible

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic constructs or phenomena would definitively demonstrate that LLMs have achieved genuine language understanding rather than statistical pattern matching?
- Basis in paper: [explicit] The authors argue that LLMs fail on various linguistic tests requiring understanding, but don't specify what success would definitively prove understanding.
- Why unresolved: The paper shows current LLMs fail on various linguistic tests, but doesn't propose specific tests that would definitively prove understanding if passed.
- What evidence would resolve it: A clear specification of linguistic tasks that, if successfully completed by LLMs, would constitute proof of genuine language understanding rather than pattern matching.

### Open Question 2
- Question: How do LLMs handle linguistic phenomena that humans process through both linguistic competence and world knowledge, such as metaphor or pragmatics?
- Basis in paper: [explicit] The authors note LLMs' failure with semantic anomalies but don't test more complex pragmatic or metaphorical language use.
- Why unresolved: The paper focuses on grammatical and semantic anomalies but doesn't explore how LLMs handle pragmatic aspects of language that require both linguistic and real-world knowledge.
- What evidence would resolve it: Testing LLMs on tasks requiring pragmatic understanding, metaphor comprehension, or other aspects of language that require both linguistic knowledge and world knowledge.

### Open Question 3
- Question: What architectural modifications to LLMs would be necessary to enable genuine language understanding rather than pattern matching?
- Basis in paper: [inferred] The authors suggest LLMs lack "a compositional operator for regulating grammatical and semantic information" but don't specify what such an operator would look like.
- Why unresolved: While the paper identifies the lack of genuine understanding in current LLMs, it doesn't propose specific architectural changes that would enable true language understanding.
- What evidence would resolve it: Proposals for specific architectural modifications or new model designs that could enable genuine language understanding, along with empirical validation of their effectiveness.

## Limitations

- The benchmark may not fully capture the breadth of human language comprehension abilities
- The LLMs tested may not represent the full spectrum of current LLM capabilities
- The prompts and texts used in the benchmark are not fully specified, making it difficult to assess their representativeness of real-world language use

## Confidence

- **High Confidence**: The observation that LLMs perform at chance accuracy and exhibit considerable variability in their answers on the specific comprehension tasks tested
- **Medium Confidence**: The qualitative interpretation that LLM errors demonstrate a lack of genuine language understanding
- **Low Confidence**: The specific mechanism proposed for LLMs' failures, namely the absence of a compositional operator for regulating grammatical and semantic information

## Next Checks

1. Expand the benchmark to cover a wider range of language comprehension tasks, including those requiring reasoning, inference, and understanding of context beyond the sentence level
2. Evaluate the performance of newer LLM architectures with explicit mechanisms for compositional reasoning or world knowledge grounding on the same comprehension tasks
3. Conduct further analysis to explore alternative explanations for the observed LLM errors, such as differences in task construal, statistical patterns in the training data, or the influence of prompt design