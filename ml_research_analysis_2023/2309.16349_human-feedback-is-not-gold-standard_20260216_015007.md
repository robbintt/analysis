---
ver: rpa2
title: Human Feedback is not Gold Standard
arxiv_id: '2309.16349'
source_url: https://arxiv.org/abs/2309.16349
tags:
- assertiveness
- scores
- error
- quality
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Preference scores are increasingly used to evaluate and train LLMs,
  but they may be biased and fail to capture important error criteria like factuality.
  We found that assertive responses are rated higher quality and are less likely to
  be judged as containing factual errors, even when they do.
---

# Human Feedback is not Gold Standard

## Quick Facts
- arXiv ID: 2309.16349
- Source URL: https://arxiv.org/abs/2309.16349
- Reference count: 29
- Primary result: Human preference scores used to evaluate/train LLMs may be biased by assertiveness, under-representing important errors like factuality

## Executive Summary
This paper challenges the reliability of human feedback as an evaluation metric and training objective for large language models (LLMs). Through controlled experiments, the authors demonstrate that assertive model outputs receive higher quality ratings and are less likely to be flagged for factual errors, even when they contain such errors. This bias is amplified in RLHF-trained models, suggesting the training objective itself perpetuates this issue. The findings indicate that preference scores obscure critical failures in factuality and consistency, and that human feedback should not be treated as a gold standard for LLM evaluation or training.

## Method Summary
The authors conducted experiments using multiple LLM models (MPT 30B, Falcon 40B, Command 6B/52B, Llama 2 13B) on prompts from Curation Corpus, Amazon Product Descriptions, and Wikihow datasets. They manipulated outputs by adding assertive and complex preambles, then collected human annotations for overall quality (1-5 scale) and specific error types (binary: factuality, inconsistency, fluency, relevance, formatting, repetition, refusal, harmfulness, scope, contradiction). Lasso regression was used to analyze how error types influence overall scores, and correlation analysis compared assertiveness with quality ratings across models.

## Key Results
- Assertive model outputs are rated higher quality and less likely to contain factual errors, even when they do
- Human annotators systematically underestimate factuality and inconsistency errors in LLM outputs
- RLHF-trained models (like Llama 2) show significantly higher assertiveness for equivalent quality scores
- Factuality and inconsistency errors contribute less to overall preference scores, suggesting they are under-represented in evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Assertiveness of LLM outputs biases human perception of factuality
- Mechanism: Confident language triggers a trust heuristic in readers, causing annotators to rate assertive outputs as less likely to contain factual errors regardless of actual accuracy
- Core assumption: Human annotators use language style as a proxy for credibility, conflating assertiveness with accuracy
- Evidence anchors: Crowdworkers underestimate factuality errors more for high assertiveness responses; assertive preambles skew perceived error rates

### Mechanism 2
- Claim: Human preference scores under-represent factuality and inconsistency errors
- Mechanism: Annotators focus on surface-level quality issues (refusal, formatting) and overlook deeper semantic problems, leading to regression models where factuality contributes less to overall quality scores
- Core assumption: Human evaluation prioritizes immediately visible errors over deeper semantic correctness when scoring overall preference
- Evidence anchors: Lasso regression shows factuality and inconsistency have much lower weighting in predicting overall scores

### Mechanism 3
- Claim: RLHF training disproportionately increases model assertiveness
- Mechanism: Since assertive outputs receive higher human preference scores, RLHF optimization amplifies this trait beyond its actual contribution to output utility, creating a "confidently wrong" effect
- Core assumption: The reward model learned from human feedback captures the assertiveness bias present in human annotators
- Evidence anchors: Llama 2 13B shows higher assertiveness than non-RLHF models for equivalent quality scores

## Foundational Learning

- Concept: Human evaluation bias in NLP systems
  - Why needed here: Understanding how subjective human judgments introduce systematic biases into model evaluation and training
  - Quick check question: What are the two main ways human evaluation bias manifests in LLM assessment according to this paper?

- Concept: Confounding variables in experimental design
  - Why needed here: The paper manipulates assertiveness and complexity as potential confounders to isolate their effects on error perception
  - Quick check question: How did the authors create controlled variations in assertiveness and complexity?

- Concept: Regression analysis for feature importance
  - Why needed here: Used to quantify how different error types contribute to overall preference scores
  - Quick check question: What statistical method did the authors use to determine which error types contribute most to overall quality scores?

## Architecture Onboarding

- Component map: Input prompt generator (Curation, Amazon, Wikihow) -> LLM inference layer (MPT 30B, Falcon 40B, Command 6B/52B, Llama 2 13B) -> Preamble injection module (assertiveness/complexity variation) -> Human annotation interface (error checklists, quality ratings) -> Data aggregation and analysis pipeline (regression models, correlation analysis)

- Critical path: 1) Generate prompts from datasets 2) Produce model outputs with/without preambles 3) Collect human annotations for errors and quality 4) Analyze correlation between assertiveness and perceived quality 5) Compare RLHF-trained vs non-RLHF models for assertiveness

- Design tradeoffs: Using human annotators provides realistic evaluation but introduces bias; single overall score is simple but obscures important error types; preamble-based control of style is effective but may affect true error rates

- Failure signatures: High correlation between assertiveness and quality scores suggests bias; significant difference between expert and crowdworker error detection rates; RLHF models showing higher assertiveness for equivalent quality

- First 3 experiments: 1) Replicate assertiveness manipulation with new models to confirm bias effect 2) Test automated fact-checking tools against human annotators on assertive vs non-assertive outputs 3) Implement modified RLHF training that explicitly penalizes assertiveness to reduce bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the assertiveness of model outputs affect their perceived factuality, and does this bias persist across different tasks and domains?
- Basis in paper: The paper shows assertive outputs are perceived as less likely to contain factual errors, even when they do, with stronger bias for highly assertive responses
- Why unresolved: Study focuses on limited tasks and models; unclear if bias generalizes to other domains where factual accuracy is paramount
- What evidence would resolve it: Conduct similar study across wider range of tasks and domains, including high-stakes domains where factual accuracy is critical

### Open Question 2
- Question: To what extent does RLHF training amplify the assertiveness bias, and are there methods to mitigate this effect while maintaining or improving output quality?
- Basis in paper: Llama 2 13B, trained with RLHF, exhibits higher assertiveness for equivalent quality scores compared to models trained without RLHF
- Why unresolved: Study offers preliminary evidence but doesn't explore mechanisms behind amplification or potential mitigation strategies
- What evidence would resolve it: Investigate relationship between RLHF training and assertiveness bias in more detail; experiment with different reward models, training objectives, or regularization techniques

## Limitations

- The assertiveness manipulation relies on prompt engineering rather than intrinsic model properties, limiting generalizability beyond controlled conditions
- The study uses a relatively small number of annotators per output (one to three), which may not capture full distribution of human judgment variability
- The analysis focuses on English-language outputs, limiting applicability to multilingual contexts

## Confidence

- **High confidence**: RLHF-trained models exhibit higher assertiveness levels, well-supported by comparative analysis across multiple model families
- **Medium confidence**: Human evaluators systematically underestimate factuality errors in assertive outputs, demonstrated but could benefit from larger-scale replication
- **Medium confidence**: Regression analysis showing under-representation of factuality errors in preference scores is statistically sound but depends on specific error taxonomy

## Next Checks

1. Replicate the assertiveness manipulation with a larger, more diverse set of models including open and closed-source systems to verify the bias effect generalizes across architectures

2. Conduct a within-subjects study where the same annotators rate identical outputs with and without assertiveness manipulation to isolate the causal effect of language style on error perception

3. Implement and evaluate a modified RLHF training protocol that explicitly weights factual accuracy in the reward model to determine if this reduces the assertiveness bias amplification during training