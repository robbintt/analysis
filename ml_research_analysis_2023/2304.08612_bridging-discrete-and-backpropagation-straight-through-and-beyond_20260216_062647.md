---
ver: rpa2
title: 'Bridging Discrete and Backpropagation: Straight-Through and Beyond'
arxiv_id: '2304.08612'
source_url: https://arxiv.org/abs/2304.08612
tags:
- reinmax
- gradient
- approximation
- latent
- softmax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges discrete latent variables and backpropagation
  by examining the Straight-Through (ST) heuristic and proposing a novel method called
  ReinMax. The authors show that ST works as a first-order approximation of the gradient,
  providing guidance on hyper-parameter configurations.
---

# Bridging Discrete and Backpropagation: Straight-Through and Beyond

## Quick Facts
- arXiv ID: 2304.08612
- Source URL: https://arxiv.org/abs/2304.08612
- Reference count: 39
- Key outcome: ReinMax achieves second-order accuracy by integrating Heun's method, providing consistent improvements over state-of-the-art methods on structured output prediction and unsupervised generative modeling tasks with negligible computation overheads.

## Executive Summary
This paper bridges discrete latent variables and backpropagation by examining the Straight-Through (ST) heuristic and proposing a novel method called ReinMax. The authors show that ST works as a first-order approximation of the gradient, providing guidance on hyper-parameter configurations. ReinMax achieves second-order accuracy by integrating Heun's method, a second-order numerical method for solving ODEs, without requiring Hessian or other second-order derivatives. The proposed method brings consistent improvements over state-of-the-art methods on structured output prediction and unsupervised generative modeling tasks, with negligible computation overheads.

## Method Summary
ReinMax combines two first-order derivative estimates to approximate the gradient to second order, without requiring Hessian or other second-order derivatives. It integrates Heun's method for a better gradient approximation and achieves second-order accuracy. The method uses temperature scaling (tau >= 1) to stabilize gradient approximation, which differs from STGS-style methods that prefer tau <= 1. ReinMax is evaluated against baselines including Straight-Through (ST), Straight-Through Gumbel-Softmax (STGS), Gumbel-Rao Monte Carlo (GR-MCK), and Gapped Straight-Through (GST-1.0) on polynomial programming, ListOps, and MNIST-VAE tasks.

## Key Results
- ReinMax achieves second-order accuracy by integrating Heun's method
- Consistent improvements over state-of-the-art methods on structured output prediction and unsupervised generative modeling tasks
- Negligible computation overheads compared to first-order methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Straight-Through (ST) estimator works as a first-order approximation of the gradient
- Mechanism: ST approximates the gradient by treating the non-differentiable sampling process as an identity function during backpropagation, which corresponds to the forward Euler method in numerical ODE analysis
- Core assumption: The sampling process can be approximated as identity function in backpropagation
- Evidence anchors:
  - [abstract]: "we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient"
  - [section 3.1]: "we adopt a novel perspective to examine ST and show that it works as a special case of the forward Euler method that approximates the gradient with first-order accuracy"
  - [corpus]: Weak evidence - corpus papers focus on STE in quantization context, not ST gradient approximation
- Break condition: If the sampling process has discontinuities that cannot be approximated by identity function, or if the function being optimized is highly non-linear

### Mechanism 2
- Claim: ReinMax achieves second-order accuracy by integrating Heun's method
- Mechanism: ReinMax combines two first-order derivative estimates to approximate the gradient to second order, without requiring Hessian or other second-order derivatives
- Core assumption: The gradient of the function being optimized can be approximated using two first-order derivative estimates
- Evidence anchors:
  - [abstract]: "ReinMax achieves second-order accuracy by integrating Heun's method, a second-order numerical method for solving ODEs"
  - [section 3.2]: "ReinMax integrates Heun's method for a better gradient approximation and achieves second-order accuracy"
  - [corpus]: No direct evidence - corpus papers focus on STE and quantization, not second-order gradient approximation methods
- Break condition: If the function being optimized has discontinuities that make second-order approximation unreliable, or if computational overhead becomes significant for very deep networks

### Mechanism 3
- Claim: Temperature scaling serves different purposes for ST/STGS vs ReinMax
- Mechanism: For ST and ReinMax, temperature > 1 stabilizes gradient approximation, while for STGS, temperature < 1 reduces bias
- Core assumption: The temperature parameter can be tuned to optimize different aspects of gradient approximation
- Evidence anchors:
  - [section 5.2]: "ST and ReinMax prefer to set the temperature τ ≥ 1. These observations match our analyses in Section 5 that a small τ can help reduce the bias introduced by STGS-style methods"
  - [section 5.1]: "∇STGS prefers to set the temperature τ as a relatively small value"
  - [corpus]: No direct evidence - corpus papers don't discuss temperature scaling strategies for different gradient estimators
- Break condition: If temperature scaling introduces numerical instability, or if the optimal temperature range is too narrow for practical applications

## Foundational Learning

- Concept: Numerical methods for solving ODEs (Forward Euler and Heun's method)
  - Why needed here: The paper uses these methods to explain and improve gradient approximation for discrete variables
  - Quick check question: What is the order of accuracy for Forward Euler and Heun's methods respectively?

- Concept: Gradient estimators for discrete variables (REINFORCE, ST, STGS)
  - Why needed here: Understanding existing gradient estimators is crucial to appreciate the novelty of ReinMax
  - Quick check question: Why does REINFORCE have high variance compared to ST and STGS?

- Concept: Variational autoencoders with discrete latent variables
  - Why needed here: The paper benchmarks ReinMax on MNIST-VAE, a common discrete latent variable task
  - Quick check question: What is the role of the reparameterization trick in training VAEs with continuous latent variables?

## Architecture Onboarding

- Component map: Input -> Sampling -> Function evaluation -> Gradient approximation -> Update
- Critical path:
  1. Sample discrete variable D using distribution parameterized by θ
  2. Compute function value f(D)
  3. Approximate gradient using chosen method (ST/STGS/ReinMax)
  4. Update parameters θ using gradient
  5. Repeat until convergence
- Design tradeoffs:
  - Accuracy vs computation: ReinMax offers higher accuracy than ST but with negligible additional computation
  - Bias vs variance: ST has lower variance than REINFORCE but introduces bias
  - Temperature tuning: Different methods require different temperature ranges for optimal performance
- Failure signatures:
  - Poor convergence: May indicate inappropriate temperature setting or suboptimal method choice
  - High variance in gradient estimates: May suggest need for REINFORCE-style variance reduction techniques
  - Numerical instability: Could result from temperature settings that are too extreme
- First 3 experiments:
  1. Verify temperature sensitivity: Run polynomial programming with varying temperatures for ST, STGS, and ReinMax
  2. Compare gradient approximation accuracy: Use small-scale problem where true gradient can be computed exactly
  3. Benchmark on structured output prediction: Test on ListOps to verify improvements over baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical analysis of the Straight-Through (ST) heuristic and ReinMax be extended to multi-dimensional discrete variables (e.g., multinomial distributions) rather than just one-hot categorical variables?
- Basis in paper: [explicit] The paper focuses on one-hot categorical variables where the discrete latent variable D is sampled from a softmax distribution. The theorems and analyses (Theorem 3.1, Theorem 3.2, Remarks 4.1 and 4.2) are explicitly stated for this setting.
- Why unresolved: The paper does not discuss or provide theoretical extensions for multi-dimensional discrete variables, leaving open the question of whether the first-order and second-order approximations would still hold or require modification.
- What evidence would resolve it: A theoretical proof showing that the ST heuristic approximates the gradient of a multinomial distribution to first order, and that ReinMax extends to second-order accuracy for multinomial distributions, possibly with a modified baseline or approximation scheme.

### Open Question 2
- Question: How does the performance of ReinMax compare to ST and other baselines when applied to structured prediction tasks beyond ListOps, such as parsing or machine translation with discrete latent structures?
- Basis in paper: [explicit] The paper demonstrates ReinMax's effectiveness on ListOps, a structured output prediction task, showing consistent improvements over baselines. However, it only reports results on this single task.
- Why unresolved: The paper does not explore other structured prediction tasks, so the generalizability of ReinMax's performance gains to other domains with discrete latent structures remains unknown.
- What evidence would resolve it: Empirical results comparing ReinMax to ST and other baselines on a diverse set of structured prediction tasks (e.g., dependency parsing, machine translation with latent alignments, semantic parsing) would demonstrate its broader applicability and effectiveness.

### Open Question 3
- Question: What is the impact of using different baseline functions (beyond E[f(D)] and 1/n ∑ᵢ f(Iᵢ)) on the performance of ReinMax, and can an adaptive baseline selection strategy further improve its accuracy and stability?
- Basis in paper: [explicit] The paper discusses the importance of choosing E[f(D)] as the baseline for ReinMax and provides empirical evidence that it outperforms using 1/n ∑ᵢ f(Iᵢ). It also mentions that different baselines would result in different gradient approximations (Remark 4.1).
- Why unresolved: While the paper establishes that E[f(D)] is a better baseline than 1/n ∑ᵢ f(Iᵢ), it does not explore a broader space of potential baseline functions or investigate whether an adaptive strategy could further enhance ReinMax's performance.
- What evidence would resolve it: Experiments comparing ReinMax's performance using various baseline functions (e.g., moving averages of f(D), learned baselines, state-dependent baselines) and an adaptive baseline selection mechanism that adjusts the baseline during training would reveal the potential for further improvements.

## Limitations

- Theoretical analysis assumes smooth objective functions, which may not hold in all practical scenarios
- Temperature scaling recommendations are empirical and may not generalize to all problem domains
- Comparison with GR-MCK may not reflect practical performance differences due to implementation variations
- Experiments focus primarily on synthetic and moderately sized tasks, leaving uncertainty about performance on large-scale real-world applications

## Confidence

**High Confidence**: The connection between ST and forward Euler method is well-established in numerical analysis literature and the paper provides clear mathematical derivations. The first-order approximation claim is supported by both theory and empirical evidence.

**Medium Confidence**: While the integration of Heun's method for second-order accuracy is theoretically sound, the empirical evidence is limited to three specific tasks. The claim of "negligible computation overheads" is somewhat subjective and may vary with implementation details.

**Medium Confidence**: The temperature scaling observations are based on empirical results from specific tasks. While the qualitative trends are clear, the optimal temperature ranges may vary across different problem domains and architectures.

## Next Checks

1. **Large-Scale Real-World Task**: Test ReinMax on a large-scale NLP task with discrete latent variables (e.g., neural machine translation with discrete latent variables) to verify scalability and practical performance gains over existing methods.

2. **Gradient Estimation Accuracy**: Implement a controlled experiment where the true gradient can be computed exactly (e.g., a small polynomial function with discrete inputs) and measure the approximation error of ST, STGS, and ReinMax across different temperature settings.

3. **Robustness to Non-Smooth Objectives**: Evaluate the performance of all gradient estimators on non-smooth objective functions (e.g., piecewise linear functions or functions with discontinuities) to test the limitations of the first-order and second-order approximation assumptions.