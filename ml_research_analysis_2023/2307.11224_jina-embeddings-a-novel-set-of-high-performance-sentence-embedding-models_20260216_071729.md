---
ver: rpa2
title: 'Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models'
arxiv_id: '2307.11224'
source_url: https://arxiv.org/abs/2307.11224
tags:
- training
- dataset
- data
- embeddings
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Jina Embeddings, a set of high-performance
  sentence embedding models trained on a novel dataset. The models, ranging from 35
  million to 6 billion parameters, utilize contrastive fine-tuning on the T5 architecture.
---

# Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models

## Quick Facts
- arXiv ID: 2307.11224
- Source URL: https://arxiv.org/abs/2307.11224
- Reference count: 11
- Models range from 35 million to 6 billion parameters

## Executive Summary
This paper introduces Jina Embeddings, a set of high-performance sentence embedding models trained on a novel dataset. The models utilize contrastive fine-tuning on the T5 architecture and demonstrate competitive performance across various tasks, often matching or exceeding larger models while requiring less training data. A key innovation is the development of a negation dataset to improve the model's handling of grammatical negation. The training process involves two phases: initial training on pairwise data and subsequent fine-tuning on triplets, including hard negatives.

## Method Summary
The authors trained sentence embedding models using a two-phase contrastive fine-tuning approach on the T5 architecture. The first phase involved training on 385 million high-quality text pairs using InfoNCE loss. The second phase fine-tuned the model on 927,000 triplets (anchor, match, hard-negative) using a combined loss function. A novel negation dataset was constructed to improve the model's awareness of grammatical negation. The training data was rigorously filtered through de-duplication, language filtering, and consistency filtering to ensure high quality.

## Key Results
- Models demonstrate competitive performance across sentence similarity, retrieval, and reranking tasks
- The 6B parameter model matches or exceeds larger models while requiring less training data
- Models excel in tasks involving negation, showcasing improved semantic understanding
- The two-phase training approach and data filtering significantly contribute to model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-phase training approach (pairwise then triplet) allows the model to first learn general semantic representations and then specialize in distinguishing similar from dissimilar pairs.
- Mechanism: Initial pairwise training consolidates semantic meaning into single embeddings. Subsequent triplet training teaches the model to differentiate using explicit hard negatives.
- Core assumption: Learning semantic meaning first, then discrimination, is more effective than training on both simultaneously.
- Evidence anchors:
  - [section] "The training regimen is organized into two distinct phases. The first phase centers on training the model using a voluminous quantity of text pairs... The second phase engages the model with a relatively smaller set of triplets, comprising an anchor, match, and hard-negative, teaching it to differentiate between similar and dissimilar text phrases."
  - [abstract] "The training process involves two phases: initial training on pairwise data and subsequent fine-tuning on triplets, including hard negatives."
  - [corpus] Weak evidence: No direct citations to studies proving this two-phase approach is superior.
- Break condition: If hard negatives are not informative or if the model overfits during pairwise training, the benefit of the two-phase approach diminishes.

### Mechanism 2
- Claim: Data filtering (language and consistency) significantly improves model performance by reducing noise in training data.
- Mechanism: Language filtering removes non-English text, ensuring the model trains only on relevant data. Consistency filtering removes pairs with low semantic similarity, focusing training on high-quality pairs.
- Core assumption: Removing low-quality data points improves model learning more than exposing the model to a larger but noisier dataset.
- Evidence anchors:
  - [section] "The application of these preprocessing steps resulted in a reduction of the dataset from over 1.5 billion mixed-quality pairs to 385 million high-quality pairs. This reduction permits us to train our model with significantly less data than typical embedding models, without sacrificing embedding quality."
  - [abstract] "It underlines the crucial role of data cleaning in dataset preparation."
  - [corpus] Weak evidence: No direct citations to studies proving the effectiveness of this specific filtering approach.
- Break condition: If filtering removes too much data, including some high-quality pairs, or if the filtering criteria are too strict, model performance could degrade.

### Mechanism 3
- Claim: The negation dataset addresses a specific weakness in sentence embedding models, improving their ability to handle grammatical negation.
- Mechanism: By training on explicit negation examples, the model learns to embed negated and non-negated statements at appropriate distances.
- Core assumption: Explicitly training on negation examples improves the model's understanding of negation beyond what is learned from general text data.
- Evidence anchors:
  - [section] "We observed many embedding models struggle to accurately embed negations... Thus, we decided to create our own negation dataset... Our model evaluation on the negation dataset, which includes a comparative analysis with other popular open-source models, is presented in Section 4.3."
  - [abstract] "Furthermore, to increase the model's awareness of grammatical negation, we construct a novel training and evaluation dataset of negated and non-negated statements, which we make publicly available to the community."
  - [corpus] Weak evidence: No direct citations to studies proving the effectiveness of this specific negation dataset.
- Break condition: If the negation dataset is too small or not representative of real-world negation usage, the model's improvement in handling negation may be limited.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: The model uses contrastive loss functions (InfoNCE) to learn embeddings where similar texts are close and dissimilar texts are far apart.
  - Quick check question: What is the main difference between contrastive learning and traditional supervised learning?

- Concept: Data preprocessing and filtering
  - Why needed here: The model's performance heavily depends on the quality of the training data, which is improved through language and consistency filtering.
  - Quick check question: Why is it important to remove low-quality data points from the training set?

- Concept: Triplet loss
  - Why needed here: The model uses triplet loss in the second phase of training to learn to differentiate between similar and dissimilar text pairs.
  - Quick check question: How does triplet loss differ from pairwise contrastive loss?

## Architecture Onboarding

- Component map: T5 encoder -> Mean pooling layer -> Contrastive loss functions (InfoNCE, triplet margin loss)
- Critical path: Data preprocessing → Pairwise training → Triplet training → Evaluation
- Design tradeoffs:
  - Model size vs. performance: Larger models generally perform better but require more resources.
  - Training data size vs. quality: Smaller, higher-quality datasets can lead to better performance than larger, noisier datasets.
  - Two-phase training vs. single-phase: Two-phase training may be more effective but requires more training time.
- Failure signatures:
  - Overfitting: Model performs well on training data but poorly on unseen data.
  - Underfitting: Model performs poorly on both training and unseen data.
  - Poor handling of negation: Model fails to distinguish between negated and non-negated statements.
- First 3 experiments:
  1. Train a small model on unfiltered data and compare performance to a model trained on filtered data.
  2. Train a model with only pairwise training and compare performance to a model with two-phase training.
  3. Train a model without the negation dataset and evaluate its performance on negation tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the filtering techniques (de-duplication, language filtering, and consistency filtering) impact the performance of embedding models across different datasets and domains?
- Basis in paper: [explicit] The paper discusses the effectiveness of these preprocessing steps in improving model performance and reducing the amount of training data required.
- Why unresolved: While the paper provides some evidence of the effectiveness of these filtering techniques, it does not explore their impact across a wide range of datasets and domains. The results might vary depending on the specific characteristics of each dataset.
- What evidence would resolve it: Conducting a comprehensive study across various datasets and domains, systematically applying different combinations of filtering techniques and measuring the resulting model performance, would provide a clearer understanding of their impact.

### Open Question 2
- Question: How does the inclusion of a negation dataset in the training process affect the model's ability to handle negation and contradictory statements?
- Basis in paper: [explicit] The paper introduces a negation dataset and evaluates the performance of models on negation tasks, showing improvements after fine-tuning on the triplet data that includes the negation dataset.
- Why unresolved: While the paper demonstrates the effectiveness of including the negation dataset, it does not explore the extent of the improvement or the specific aspects of negation that are better captured by the models. Additionally, the impact on other tasks beyond negation is not thoroughly investigated.
- What evidence would resolve it: Conducting a detailed analysis of the model's performance on negation tasks before and after fine-tuning on the negation dataset, as well as evaluating its performance on other tasks that may involve negation, would provide insights into the specific benefits of including the negation dataset.

### Open Question 3
- Question: How does the choice of sampling rates for different datasets during training impact the model's generalization ability across various tasks?
- Basis in paper: [explicit] The paper mentions the use of sampling rates to prioritize high-quality datasets and balance training across different domains, but the selection of these rates is described as heuristic and based on intuition.
- Why unresolved: The paper does not provide a systematic exploration of the impact of different sampling rate configurations on model performance. The choice of sampling rates may significantly influence the model's ability to generalize across tasks.
- What evidence would resolve it: Conducting a systematic study with varying sampling rate configurations and evaluating the resulting model performance across a diverse set of tasks would shed light on the optimal sampling strategies and their impact on generalization.

## Limitations

- The effectiveness of the two-phase training approach relies on assumptions without direct supporting citations.
- Data filtering techniques, while described as crucial, lack empirical evidence demonstrating their optimality.
- The negation dataset's generalizability to other types of negation or negation in different contexts is unclear.
- The specific composition of the training datasets and optimal sampling rates are not fully specified.

## Confidence

- High confidence: The overall architecture and training methodology are sound and align with established practices in contrastive learning for sentence embeddings.
- Medium confidence: The specific implementation details, such as the exact composition of the training datasets and the optimal sampling rates, are not fully specified and may impact reproducibility.
- Low confidence: The claims about the superiority of the two-phase training approach and the specific data filtering methods are not strongly supported by direct evidence or citations.

## Next Checks

1. **Replicate the filtering impact**: Train two models with identical architectures but different training data - one with the described filtering pipeline and one without. Compare performance on a held-out validation set to isolate the effect of data filtering.

2. **Ablation study on training phases**: Train three models: one with only pairwise training, one with only triplet training, and one with the full two-phase approach. Evaluate all models on a comprehensive set of tasks to determine the individual and combined contributions of each training phase.

3. **Negation dataset generalization**: Evaluate the trained model's performance on a diverse set of negation datasets beyond the one provided by the authors, including datasets with different types of negation (e.g., morphological, syntactic, semantic) and in different contexts (e.g., different domains, languages). This will help assess the generalizability of the model's improved negation handling.