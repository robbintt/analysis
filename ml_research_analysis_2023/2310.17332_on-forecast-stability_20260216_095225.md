---
ver: rpa2
title: On Forecast Stability
arxiv_id: '2310.17332'
source_url: https://arxiv.org/abs/2310.17332
tags:
- forecasts
- stability
- stable
- forecast
- origin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores forecast stability in time series forecasting,
  focusing on vertical and horizontal stability. The authors propose a model-agnostic
  linear interpolation approach to stabilise forecasts produced by any base model.
---

# On Forecast Stability

## Quick Facts
- arXiv ID: 2310.17332
- Source URL: https://arxiv.org/abs/2310.17332
- Reference count: 2
- This paper proposes a linear interpolation approach to stabilize forecasts by blending adjacent forecasts, achieving better stability and/or accuracy than benchmarks.

## Executive Summary
This paper addresses the problem of forecast stability in time series forecasting, focusing on vertical stability (stability across forecast origins) and horizontal stability (stability across forecast horizons). The authors propose a simple model-agnostic linear interpolation approach that can be applied to any base forecasting model. The method combines forecasts from adjacent origins or horizons using weighted averages, with the weight parameter w_s controlling the trade-off between stability and accuracy. Experiments on four public datasets show that this approach significantly improves both stability and accuracy compared to benchmarks, including a state-of-the-art stabilization method.

## Method Summary
The proposed method uses linear interpolation to stabilize forecasts by combining forecasts from adjacent origins (vertical stability) or adjacent horizons (horizontal stability). For vertical stability, forecasts at a given origin are linearly combined with corresponding forecasts from the previous origin. For horizontal stability, forecasts at different horizons from the same origin are combined. The method can be applied as partial interpolation (combining only original forecasts) or full interpolation (chaining previously stabilized forecasts). The weight parameter w_s controls the proportion of the previous forecast used in the combination, allowing control over the stability-accuracy trade-off. The framework is tested with three base models (N-BEATS, Pooled Regression, LightGBM) on four datasets (M4, M3, Favorita, M5).

## Key Results
- The proposed linear interpolation framework achieves significantly higher stability and/or accuracy compared to benchmarks across three error metrics (sMAPE, MAE, RMSE) and six stability metrics (sMAPC, MAC, RMSC, and their "I" variants).
- The full interpolation method provides better stability than partial interpolation by chaining previously stabilized forecasts.
- The method generates a Pareto front of accuracy-stability trade-offs, enabling decision-makers to select optimal models based on their specific requirements.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear interpolation stabilizes forecasts by blending adjacent forecasts to reduce volatility.
- Mechanism: The method computes weighted averages of forecasts from consecutive origins (vertical) or consecutive horizons (horizontal) using parameter ws. This blending anchors newer forecasts to previously communicated ones, preventing arbitrary changes.
- Core assumption: Previous forecasts are already communicated and cannot be altered; newer forecasts incorporate more recent data and are more accurate.
- Evidence anchors:
  - [abstract] "We propose a simple linear-interpolation-based approach that is applicable to stabilise the forecasts provided by any base model vertically and horizontally."
  - [section] "To make the forecasts made at a given origin vertically stable, the forecasts are linearly combined with the corresponding forecasts made at the previous origin."
- Break condition: If the base forecasts are already perfectly stable, interpolation will degrade accuracy without improving stability.

### Mechanism 2
- Claim: Full interpolation provides more stable forecasts than partial interpolation by chaining prior interpolated values.
- Mechanism: Full interpolation uses previously stabilized forecasts in the combination step, propagating stability through the forecast sequence. Partial interpolation only blends original forecasts at each step.
- Core assumption: Stability improves cumulatively when each forecast step uses already stabilized values from earlier steps.
- Evidence anchors:
  - [section] "Full interpolation considers the corresponding interpolated stable forecasts at the previous origin... thus, full interpolation combines the forecasts in a chained manner where higher weights are given for the forecasts made at closer origins."
- Break condition: If the underlying forecast series is non-stationary or trending, chaining may over-smooth and remove meaningful signal.

### Mechanism 3
- Claim: Weight ws controls the trade-off between accuracy and stability, enabling Pareto-optimal solutions.
- Mechanism: ws ∈ [0,1] determines the influence of the previous forecast: ws=0 yields the base forecast (most accurate), ws=1 enforces full stability (identical to prior), intermediate values blend the two.
- Core assumption: Accuracy and stability are inversely related, and the optimal ws varies by dataset characteristics.
- Evidence anchors:
  - [section] "The proportion of the previous forecasts that is used during the interpolation to make the current forecasts is a parameter to the method that enables to easily control the trade-off between stability and accuracy."
  - [section] "Presenting the full spectrum of accuracy-stability trade-off is highly useful for decision-making of real-world applications."
- Break condition: If a dataset's variance is very low, higher ws may improve both accuracy and stability simultaneously, breaking the assumed inverse relationship.

## Foundational Learning

- Concept: Rolling origin forecasting
  - Why needed here: The method relies on generating multiple forecasts for the same target from different origins, a common real-world scenario.
  - Quick check question: In rolling origin forecasting, if origin moves forward by one time step and forecast horizon is 6, how many forecasts will be produced for the same target time point?

- Concept: Ensemble forecasting / forecast combination
  - Why needed here: Linear interpolation is a form of forecast combination applied to stabilize outputs rather than improve accuracy alone.
  - Quick check question: What is the main difference between ensembling for accuracy versus ensembling for stability?

- Concept: Pareto front analysis
  - Why needed here: The method generates multiple variants (different ws) that form a Pareto front between accuracy and stability, requiring understanding of multi-objective optimization.
  - Quick check question: If a model variant is not on the Pareto front, what does that imply about its performance relative to other variants?

## Architecture Onboarding

- Component map: Base forecasting model → Interpolation engine (vertical/horizontal, partial/full, ws parameter) → Stability/accuracy evaluation → Pareto front selection
- Critical path: Generate base forecasts → Apply interpolation per origin/horizon → Evaluate stability metrics (sMAPC, MAC, etc.) → Select optimal ws via Pareto analysis
- Design tradeoffs: ws controls stability-accuracy balance; partial vs full interpolation affects computational cost and smoothing strength; vertical vs horizontal stability targets different business needs
- Failure signatures: Over-smoothing leading to loss of trend/seasonality (horizontal stability on seasonal data); insufficient stability gain when base forecasts are already stable; instability if ws=0 is used indiscriminately
- First 3 experiments:
  1. Baseline: Run base model without interpolation, measure sMAPE and sMAPC
  2. Vertical stability: Apply full interpolation with ws=0.5, compare stability metrics
  3. Horizontal stability: Apply partial interpolation with ws=0.2 on a dataset with clear seasonality, check for over-smoothing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal weighting mechanisms for changing stability with respect to trend, seasonality, holiday effects, known promotions, and other similar effects?
- Basis in paper: [explicit] The paper suggests that weighting mechanisms could be developed to change stability in the case of horizontal stability with respect to trend, seasonality, holiday effects, known promotions, and other similar effects.
- Why unresolved: The paper acknowledges the potential for developing such weighting mechanisms but does not provide specific methods or results.
- What evidence would resolve it: Development and testing of specific weighting mechanisms for different types of effects (trend, seasonality, etc.) and evaluation of their impact on forecast stability and accuracy.

### Open Question 2
- Question: How can the amount of new information added by newly available data be measured to determine the desirability of stability in vertical stability contexts?
- Basis in paper: [explicit] The paper suggests investigating measures of how much new information the newly available data add, as this is relevant in a stability context.
- Why unresolved: The paper identifies this as a worthy area of investigation but does not provide a method or measure for determining the amount of new information.
- What evidence would resolve it: Development of a measure or method to quantify the amount of new information added by newly available data and testing its effectiveness in determining the desirability of stability.

### Open Question 3
- Question: What are the optimal values of the weight parameter (w_s) for different types of datasets and forecasting scenarios?
- Basis in paper: [explicit] The paper explores different values of w_s (0.2, 0.4, 0.5, 0.6, 0.8, and 1) but does not determine the optimal value for different scenarios.
- Why unresolved: The paper presents results for different values of w_s but does not provide a clear guideline for selecting the optimal value based on dataset characteristics or forecasting requirements.
- What evidence would resolve it: Analysis of the relationship between w_s values and forecast accuracy/stability across different types of datasets and forecasting scenarios to determine optimal values for specific situations.

## Limitations

- The effectiveness of linear interpolation may vary significantly across different types of time series data, with potential over-smoothing issues for strongly seasonal or trending series.
- The paper does not provide specific guidelines for selecting optimal weight values (w_s) based on dataset characteristics, requiring empirical tuning for each application.
- The method's performance compared to more sophisticated stabilization approaches is not extensively explored, limiting understanding of when simpler linear interpolation is preferable.

## Confidence

- **High Confidence**: The linear interpolation mechanism for combining adjacent forecasts is clearly described and theoretically sound for general time series applications.
- **Medium Confidence**: The empirical results showing improved stability and accuracy across multiple datasets and models are promising, but the lack of detailed hyperparameter specifications limits reproducibility.
- **Low Confidence**: The claim that this approach should serve as a benchmark for all forecast stabilization methods may be premature without broader testing across diverse forecasting architectures and real-world deployment scenarios.

## Next Checks

1. **Reproducibility Test**: Implement the full interpolation framework with N-BEATS on the M4 dataset using the exact weight values (0.2, 0.4, 0.5, 0.6, 0.8, 1.0) to verify the reported stability improvements match published results.

2. **Extreme Case Analysis**: Test the interpolation method on a dataset with strong seasonality (e.g., M3) to quantify over-smoothing effects and determine the maximum usable weight before forecast accuracy degrades beyond acceptable thresholds.

3. **Pareto Front Validation**: Generate the complete accuracy-stability trade-off curve for the Favorita dataset and verify that all reported model variants fall on or near the Pareto front, with non-optimal variants properly identified and excluded.