---
ver: rpa2
title: Multilingual large language models leak human stereotypes across language boundaries
arxiv_id: '2312.07141'
source_url: https://arxiv.org/abs/2312.07141
tags:
- language
- stereotypes
- languages
- leakage
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates stereotype leakage across languages in
  multilingual large language models (MLLMs). The authors define stereotype leakage
  as the phenomenon where stereotypes expressed in one language show up in a model's
  behavior in another language.
---

# Multilingual large language models leak human stereotypes across language boundaries

## Quick Facts
- arXiv ID: 2312.07141
- Source URL: https://arxiv.org/abs/2312.07141
- Reference count: 40
- Key outcome: Multilingual large language models leak human stereotypes across language boundaries, with Hindi being most susceptible and ChatGPT showing the most leakage overall.

## Executive Summary
This paper investigates stereotype leakage across languages in multilingual large language models (MLLMs). The authors define stereotype leakage as the phenomenon where stereotypes expressed in one language show up in a model's behavior in another language. Using the Agency Beliefs Communion (ABC) model to measure stereotypes for 30 social groups across four languages, they evaluate stereotype associations in mBERT, mT5, and ChatGPT. The study finds significant stereotype leakage across all languages and models, with notable differences in susceptibility based on language resources and model architecture. The findings highlight the potential for MLLMs to spread stereotypes across cultures and languages, emphasizing the importance of understanding and mitigating bias propagation in AI systems.

## Method Summary
The study measures stereotype leakage by first collecting human stereotypes for 30 social groups across four languages (English, Russian, Chinese, and Hindi) using the ABC model. The authors then evaluate stereotype associations in three MLLMs - mBERT, mT5, and ChatGPT - using different measurement approaches (ILPS for mT5, SeT for mBERT, and custom prompt method for ChatGPT). Stereotype leakage is quantified using a mixed-effect model framework that measures the effect of human stereotypes from source languages on stereotypical associations in target language models. The analysis reveals patterns of stereotype transfer between languages and identifies which models and languages are most susceptible to leakage.

## Key Results
- Significant stereotype leakage occurs across all tested languages and models
- Hindi is the most susceptible to cross-lingual stereotype influence, while Chinese is the least
- ChatGPT exhibits the most stereotype leakage overall compared to mBERT and mT5
- The study reveals both positive and negative stereotype leakages, as well as non-polar leakages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual models share learned representations across languages, allowing stereotypes from one language to influence outputs in another language.
- Mechanism: During pretraining, multilingual models build shared vocabulary and embeddings across languages. When stereotypes are learned for concepts in one language, these representations can transfer to other languages through shared embeddings and cross-lingual transfer learning.
- Core assumption: Stereotypes are learned during pretraining and stored in shared representations that are accessible across languages.
- Evidence anchors:
  - [abstract] "multilingual language models undergo the same training procedure as monolingual ones, albeit with training data sourced from various languages"
  - [section] "with the shared knowledge between languages in MLLMs, it is likely that stereotypes may also leak between languages"
  - [corpus] Weak evidence - corpus neighbors discuss gender bias and stereotype detection but don't directly address cross-lingual leakage
- Break condition: If multilingual models used completely separate embeddings per language with no cross-lingual transfer, or if stereotypes were only learned from language-specific training data without sharing.

### Mechanism 2
- Claim: Low-resource languages are more susceptible to stereotype leakage because they rely more heavily on cross-lingual transfer from high-resource languages.
- Mechanism: When training data is limited for a language (like Hindi in this study), the model must rely more on knowledge transferred from other languages. This makes the model more vulnerable to adopting stereotypes from those source languages.
- Core assumption: The degree of stereotype leakage is inversely proportional to the amount of monolingual training data available.
- Evidence anchors:
  - [section] "Hindi is the most susceptible to influence from other languages, while Chinese is the least"
  - [section] "Since Hindi is the only low-resource language we tested, this might explain why it absorbs stereotypes from other languages"
  - [corpus] Weak evidence - corpus doesn't specifically address resource levels and stereotype susceptibility
- Break condition: If all languages had sufficient monolingual data, or if the model architecture prevented cross-lingual influence regardless of data quantity.

### Mechanism 3
- Claim: ChatGPT shows more stereotype leakage because it's a more capable multilingual model that better captures nuanced cultural associations.
- Mechanism: More sophisticated multilingual models like ChatGPT have better cross-lingual understanding and representation learning, which also means they more effectively transfer subtle cultural stereotypes between languages.
- Core assumption: Better multilingual performance correlates with increased cross-lingual stereotype transfer.
- Evidence anchors:
  - [abstract] "ChatGPT exhibits the most stereotype leakage overall"
  - [section] "ChatGPT is one of the state-of-the-art MLLMs that has been popularly deployed to users"
  - [corpus] Weak evidence - corpus neighbors discuss multilingual models but don't specifically address the relationship between capability and bias leakage
- Break condition: If simpler models showed equal or greater stereotype leakage, or if stereotype transfer was purely a function of training data composition rather than model capability.

## Foundational Learning

- Concept: Stereotype measurement using group-trait associations
  - Why needed here: The paper uses the Agency Beliefs Communion (ABC) model to quantify stereotypes by measuring how strongly groups are associated with certain traits
  - Quick check question: What are the three dimensions of the ABC model used to measure stereotypes in this study?

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding how knowledge learned in one language transfers to others is crucial for understanding how stereotypes leak across languages
  - Quick check question: What architectural feature in multilingual models enables cross-lingual transfer of learned representations?

- Concept: Resource levels in language modeling
  - Why needed here: The paper distinguishes between high-resource (English) and low-resource (Hindi) languages, which affects how stereotypes transfer
  - Quick check question: How does the amount of training data available for a language affect its susceptibility to cross-lingual stereotype leakage?

## Architecture Onboarding

- Component map: Shared embedding layers -> Transformer layers -> Task-specific heads. Cross-lingual transfer occurs primarily in the shared embedding space.
- Critical path: Data collection → Human stereotype annotation → Model stereotype measurement → Statistical analysis of leakage patterns → Qualitative analysis of specific leakages
- Design tradeoffs: More cross-lingual sharing improves performance on low-resource languages but increases stereotype leakage risk; separate language modules reduce leakage but hurt multilingual performance
- Failure signatures: Unexpected trait associations appearing in languages where they shouldn't exist culturally; strong correlation between stereotypes in different languages; certain language pairs showing consistent leakage patterns
- First 3 experiments:
  1. Measure stereotype leakage between English and Russian in mBERT using the ABC model traits
  2. Compare stereotype leakage in mT5 versus mBERT for the same language pairs
  3. Test whether fine-tuning on debiased data for one language reduces leakage to other languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does stereotype leakage in MLLMs vary across different language families or language structures?
- Basis in paper: [explicit] The paper studies stereotype leakage across four languages from two different language families (Indo-European and Sino-Tibetan), but doesn't explore broader language family patterns.
- Why unresolved: The study only examined four languages. Testing with a wider range of languages from different families could reveal systematic patterns in stereotype leakage.
- What evidence would resolve it: Analyzing stereotype leakage across a larger sample of languages from multiple families would show whether certain language structures or families are more susceptible to leakage.

### Open Question 2
- Question: Can targeted interventions reduce stereotype leakage in MLLMs without significantly degrading their multilingual performance?
- Basis in paper: [inferred] The authors identify stereotype leakage as a significant issue but don't explore mitigation strategies.
- Why unresolved: The paper focuses on measurement and analysis of stereotype leakage but doesn't propose or test solutions.
- What evidence would resolve it: Testing various debiasing techniques on MLLMs and measuring both the reduction in stereotype leakage and any impact on multilingual performance would provide answers.

### Open Question 3
- Question: How does stereotype leakage evolve as MLLMs are fine-tuned on task-specific datasets?
- Basis in paper: [explicit] The paper studies stereotype leakage in pre-trained MLLMs but doesn't examine how fine-tuning affects it.
- Why unresolved: The study only looks at the base MLLMs without considering how task-specific fine-tuning might influence stereotype leakage.
- What evidence would resolve it: Fine-tuning MLLMs on various datasets and measuring stereotype leakage before and after fine-tuning would reveal if and how it changes during the fine-tuning process.

## Limitations

- The study's findings are based on a limited set of 30 social groups and 4 languages, which may not capture the full complexity of stereotype leakage across all cultural contexts.
- The measurement approaches differ across models (ILPS for mT5, SeT for mBERT, and custom prompts for ChatGPT), potentially introducing methodological inconsistencies that could affect comparability of results.
- The study focuses on Western-centric social groups that may not translate meaningfully across all cultures, particularly for Hindi and Chinese participants.

## Confidence

- **High confidence**: The observation that multilingual models exhibit stereotype leakage across languages is well-supported by the statistical analysis showing consistent effects across all tested models and language pairs. The finding that Hindi shows the most susceptibility to cross-lingual influence is also robust, supported by multiple comparisons.
- **Medium confidence**: The claim that ChatGPT exhibits the most stereotype leakage overall is supported by the data but could be influenced by the different measurement methodology used for this model. The assertion that Chinese is the least susceptible to leakage is supported but requires caution due to potential cultural interpretation differences in the social groups studied.
- **Low confidence**: The exact mechanisms explaining why certain languages are more susceptible than others remain speculative. While the paper suggests resource levels as a factor, the complex interplay of cultural context, training data composition, and model architecture is not fully explored.

## Next Checks

1. Replicate the study with a larger set of social groups that are culturally validated across all four languages to ensure the groups have equivalent meaning and relevance in each cultural context.

2. Conduct ablation studies varying the amount of monolingual training data for each language while keeping model architecture constant, to isolate the effect of data quantity on stereotype leakage susceptibility.

3. Apply the same measurement methodology (e.g., ILPS) across all three models to eliminate potential confounds from using different evaluation approaches for different models.