---
ver: rpa2
title: Few-Shot Continual Learning via Flat-to-Wide Approaches
arxiv_id: '2306.14369'
source_url: https://arxiv.org/abs/2306.14369
tags:
- learning
- base
- flower
- where
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FLOWER is a few-shot continual learning method that combines flat
  learning, wide learning, and data augmentation to address catastrophic forgetting
  and data scarcity. It uses noise-perturbed parameters to find flat-wide minima in
  the base task, maintains them via clamping, and employs a ball generator for feature-space
  augmentation in few-shot tasks.
---

# Few-Shot Continual Learning via Flat-to-Wide Approaches

## Quick Facts
- arXiv ID: 2306.14369
- Source URL: https://arxiv.org/abs/2306.14369
- Authors: 
- Reference count: 40
- FLOWER outperforms prior arts by 1-4% accuracy on average, with larger gains in small base tasks

## Executive Summary
FLOWER addresses few-shot continual learning by combining flat learning, wide learning, and data augmentation to combat catastrophic forgetting and data scarcity. The method discovers flat-wide minima during base task learning using noise-perturbed parameters, maintains these regions via clamping in few-shot tasks, and employs a ball generator for feature-space augmentation. Performance is further regularized using projection-based memory aware synapses. Experiments demonstrate FLOWER's effectiveness across CIFAR-100, miniImageNet, and CUB-200-2011 datasets.

## Method Summary
FLOWER operates in two phases: base learning and few-shot learning. During base learning, noise-perturbed parameters find flat-wide minima to prevent catastrophic forgetting. In the few-shot phase, feature extractor parameters are clamped to maintain these flat-wide regions while a ball generator creates synthetic samples in feature space to address data scarcity. The method employs MAS regularization with projection-based optimization to induce wide local optima and protect old knowledge. The overall training minimizes a joint loss combining cross-entropy, ball generator loss, and MAS regularization.

## Key Results
- FLOWER outperforms prior arts by 1-4% accuracy on average
- Larger accuracy gains observed in small base tasks (60 classes)
- Ablation studies confirm importance of each component (flat learning, wide learning, ball generator)
- Method shows robustness to hyperparameter variations

## Why This Works (Mechanism)

### Mechanism 1
Flat-wide minima found during base learning are maintained in few-shot tasks via parameter clamping. Noise-perturbed parameters minimize loss over M trials to discover flat-wide regions, which are preserved by clamping feature extractor parameters to stay within bounds. This prevents catastrophic forgetting while maintaining generalization to new tasks.

### Mechanism 2
Ball generator in feature space addresses data scarcity by creating synthetic samples. For each class, the smallest enclosing ball is found, synthetic samples are generated around the centroid, transformed to reduce bias, and pulled toward their own centroid while pushed away from others. This preserves semantic relationships better than raw pixel augmentation.

### Mechanism 3
Projection-based memory aware synapses induces wide local optima and protects old knowledge. KL divergence to uniform distribution widens the optimum, while MAS regularization with parameter importance matrix protects important parameters from being overwritten. This increases the chance of overlapping regions across tasks, improving scalability.

## Foundational Learning

- **Catastrophic forgetting in continual learning**
  - Why needed here: Method must retain performance on old tasks while learning new few-shot tasks
  - Quick check question: What happens to network parameters when training on new tasks without any protection mechanism?

- **Flat minima and their generalization properties**
  - Why needed here: Flat regions are more robust to perturbations and better generalize to new tasks
  - Quick check question: How does the curvature of the loss landscape relate to a model's generalization ability?

- **Data augmentation for few-shot learning**
  - Why needed here: Each few-shot task has very few samples, making overfitting likely without augmentation
  - Quick check question: Why might feature-space augmentation be preferable to raw pixel augmentation in few-shot scenarios?

## Architecture Onboarding

- **Component map**: Feature extractor (WF) -> Ball generator -> Transformation module (WT) -> Classifier (WC)
- **Critical path**: Base learning → Find flat-wide minima → Clamp parameters → Few-shot learning with augmentation and MAS → Evaluate on all tasks
- **Design tradeoffs**: Flat-wide clamping vs. plasticity, augmentation complexity vs. overfitting, MAS regularization vs. capacity for new learning
- **Failure signatures**: Accuracy drops on old tasks indicate forgetting; high variance in few-shot accuracy suggests overfitting; low overall accuracy suggests poor base learning
- **First 3 experiments**:
  1. Run FLOWER with only flat learning (no wide learning, no ball generator) on CIFAR-100 to measure contribution of each component
  2. Test different values of the flat-wide bound b to find optimal clamping range
  3. Compare performance with and without the transformation module in the ball generator on miniImageNet

## Open Questions the Paper Calls Out

- **Open Question 1**: How does FLOWER's performance scale when the number of classes per task (N) in the few-shot learning phase increases significantly beyond the 5-way setting tested?
- **Open Question 2**: What is the theoretical relationship between the bound parameter b and the generalization gap across tasks in FLOWER?
- **Open Question 3**: How does FLOWER perform in cross-domain few-shot continual learning where tasks come from different data distributions?

## Limitations

- The connection between flat-wide minima found during base learning and their relevance for few-shot tasks lacks empirical validation
- The ball generator's superiority over simpler augmentation methods is theoretically sound but not directly compared through ablation studies
- MAS regularization's parameter importance estimation could be sensitive to initialization and task ordering, though these factors are not thoroughly explored

## Confidence

- **High Confidence**: Experimental results showing FLOWER outperforming baselines by 1-4% accuracy, and ablation study confirming each component's contribution
- **Medium Confidence**: Theoretical framework connecting flat-wide minima to continual learning performance, and ball generator's role in addressing data scarcity
- **Low Confidence**: Robustness of method to hyperparameter choices and different task orderings, as these aspects are only briefly mentioned without comprehensive analysis

## Next Checks

1. **Flat-Wide Region Stability**: Conduct experiments where base tasks are modified after finding flat-wide regions to test whether these regions truly generalize to different task distributions

2. **Ball Generator Ablation**: Compare the feature-space ball generator against simpler augmentation techniques (e.g., mixup, random noise) in the few-shot phase to quantify its specific contribution

3. **MAS Sensitivity Analysis**: Test FLOWER's performance across different task orderings and with varying numbers of base classes to assess the robustness of the MAS regularization to task sequence variations