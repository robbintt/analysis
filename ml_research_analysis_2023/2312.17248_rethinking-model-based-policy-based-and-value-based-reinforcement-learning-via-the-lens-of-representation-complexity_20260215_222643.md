---
ver: rpa2
title: Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning
  via the Lens of Representation Complexity
arxiv_id: '2312.17248'
source_url: https://arxiv.org/abs/2312.17248
tags:
- optimal
- function
- complexity
- value
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the representation complexity gap among
  different reinforcement learning paradigms, including model-based RL, policy-based
  RL, and value-based RL. We demonstrate that the underlying model of MDPs can be
  represented by constant-depth circuits with polynomial size or MLPs with constant
  layers and polynomial hidden dimension.
---

# Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity

## Quick Facts
- **arXiv ID**: 2312.17248
- **Source URL**: https://arxiv.org/abs/2312.17248
- **Reference count**: 40
- **Primary result**: Demonstrates a representation complexity hierarchy among RL paradigms, showing model-based RL has the lowest representation complexity, followed by policy-based RL, with value-based RL being the most complex to represent.

## Executive Summary
This paper investigates the fundamental representation complexity differences between model-based, policy-based, and value-based reinforcement learning paradigms. Through theoretical analysis, the authors establish that the underlying MDP model can be represented by constant-depth circuits or MLPs with polynomial size, while optimal policies and value functions require computationally harder representations (NP-complete and P-complete respectively). The work introduces specialized MDP constructions that encode NP-complete and P-complete problems, demonstrating that these computational hardness properties transfer to the representation of optimal policies and values. This reveals a fundamental computational barrier that may explain the empirical differences in performance between different RL approaches.

## Method Summary
The paper employs theoretical complexity analysis to establish representation complexity hierarchies. The methodology involves constructing specialized MDPs (3-SAT MDP, NP MDP, CVP MDP, P MDP) that encode computational problems from complexity theory. For each construction, the authors analyze the circuit complexity of reward functions, transition kernels, and optimal policies/values. The analysis connects theoretical circuit complexity classes (AC0, TC0, P, NP) to practical neural network expressiveness, specifically examining constant-layer MLPs with polynomial hidden dimensions. Formal proofs establish the complexity class membership of each MDP component and the expressivity limitations of neural networks.

## Key Results
- The underlying MDP model can be represented by constant-depth circuits (AC0) or constant-layer MLPs with polynomial hidden dimension
- Optimal policy representation in standard MDPs is NP-complete, while optimal value function representation is P-complete
- A hierarchy exists where model representations are easiest to compute, followed by optimal policies, with optimal values being most computationally challenging
- Constant-layer MLPs with polynomial hidden dimensions can represent models but cannot represent optimal policies or values under standard complexity assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The representation complexity gap between model-based and model-free RL is demonstrated by encoding NP-complete problems into MDP structures.
- Mechanism: By constructing 3-SAT MDPs and NP MDPs, the underlying model can be represented by constant-depth circuits (AC0), while optimal policies and values require NP-complete computation.
- Core assumption: The MDP construction accurately encodes the computational hardness of the underlying NP-complete problem.
- Evidence anchors:
  - [abstract] "We demonstrate that the underlying model of MDPs can be represented by constant-depth circuits with polynomial size or MLPs with constant layers and polynomial hidden dimension."
  - [section 3.1] "Theorem 3.3 states that the representation complexity of the underlying model is in AC0, whereas the representation complexity of optimal value function and optimal policy is NP-complete."
  - [corpus] Weak evidence - no direct citations, but related work exists on model misspecification in RL.
- Break condition: If the encoding fails to preserve the computational hardness of the NP-complete problem in the MDP structure.

### Mechanism 2
- Claim: A hierarchy within model-free RL exists where policy-based RL has lower representation complexity than value-based RL.
- Mechanism: CVP MDPs and P MDPs encode P-complete problems such that both the model and optimal policy can be represented by AC0 circuits, while optimal values require P-complete computation.
- Core assumption: The circuit value problem (CVP) can be embedded into MDPs while preserving P-completeness of value computation.
- Evidence anchors:
  - [abstract] "We introduce another general class of MDPs where both the model and optimal policy can be represented by constant-depth circuits with polynomial size or constant-layer MLPs with polynomial size. In contrast, the representation of the optimal value function is P-complete."
  - [section 4.1] "Theorem 4.2... the reward function r, transition kernel P, and optimal policy π∗ of Mn can be computed by circuits with polynomial size (in n) and constant depth, falling within the circuit complexity class AC0. However, the problem of computing the optimal value function Q∗1 of Mn is P-complete."
  - [corpus] Weak evidence - no direct citations, but related work exists on distributional RL.
- Break condition: If the P-completeness of value computation cannot be preserved through the MDP encoding.

### Mechanism 3
- Claim: The representation complexity hierarchy is observable in practical neural network implementations using constant-layer MLPs.
- Mechanism: For the constructed MDP classes, models can be represented by constant-layer MLPs with polynomial hidden dimension, while optimal policies and values cannot be represented under standard complexity assumptions.
- Core assumption: Log-precision MLPs with constant layers and polynomial hidden dimension can implement TC0 functions but cannot implement NP-complete or P-complete problems.
- Evidence anchors:
  - [abstract] "we establish a connection between our previous findings and the realm of deep RL... the underlying model through a constant-layer MLP with polynomial hidden dimension, while the optimal policy and optimal value exhibit constraints in such representation."
  - [section 5] "Theorem 5.2... Theorem 5.3... Theorem 5.4... Theorem 5.5" establish MLP representation capabilities and limitations.
  - [corpus] Weak evidence - no direct citations, but related work exists on neural circuit complexity.
- Break condition: If TC0 ≠ NP or TC0 ≠ P assumptions fail, or if MLPs can implement more complex functions than TC0.

## Foundational Learning

- Concept: Computational complexity classes (P, NP, AC0, TC0)
  - Why needed here: The paper relies on comparing the computational complexity of representing models, policies, and values in different RL paradigms.
  - Quick check question: What is the relationship between AC0 and TC0 complexity classes, and why is this distinction important for the paper's arguments?

- Concept: Circuit complexity and circuit value problem (CVP)
  - Why needed here: The paper uses circuit complexity as a measure of representation complexity and constructs MDPs based on the CVP to demonstrate its P-completeness.
  - Quick check question: How does the CVP relate to the P complexity class, and why is it used as a basis for constructing MDPs in the paper?

- Concept: Markov Decision Processes (MDPs) and their components
  - Why needed here: The paper constructs specialized MDPs to encode computational problems and analyze representation complexity across different RL paradigms.
  - Quick check question: What are the key components of an MDP (state space, action space, transition kernel, reward function), and how are they modified in the paper's constructions?

## Architecture Onboarding

- Component map: Complexity analysis module -> MDP construction module -> MLP expressiveness module -> Theoretical framework
- Critical path:
  1. Define complexity classes and their relationships
  2. Construct MDPs encoding NP-complete and P-complete problems
  3. Analyze representation complexity of model, policy, and value in each MDP
  4. Connect findings to neural network expressiveness
  5. Establish hierarchy across RL paradigms
- Design tradeoffs:
  - Theoretical rigor vs. practical applicability: The paper prioritizes theoretical complexity analysis over practical implementation considerations
  - Abstract complexity classes vs. concrete neural network architectures: The paper bridges these by connecting circuit complexity to MLP expressiveness
- Failure signatures:
  - Incorrect complexity class assumptions (e.g., if TC0 = NP or TC0 = P)
  - Failure to preserve computational hardness in MDP encoding
  - Inability to establish clear separation between complexity classes in practice
- First 3 experiments:
  1. Verify AC0 representation of model components in 3-SAT MDP construction
  2. Test NP-completeness of optimal policy/value computation in NP MDP
  3. Implement constant-layer MLP representation of model vs. inability to represent optimal policy/value in CVP MDP

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the representation complexity hierarchy between model-based, policy-based, and value-based RL be further refined or extended to other complexity classes beyond AC0, TC0, P, and NP?
- Basis in paper: [explicit] The paper establishes a hierarchy based on AC0, TC0, P, and NP complexity classes, but acknowledges the existence of other complexity classes.
- Why unresolved: The paper does not explore other complexity classes or provide a comprehensive analysis of the representation complexity landscape beyond the four mentioned classes.
- What evidence would resolve it: Extending the analysis to other complexity classes and demonstrating whether the hierarchy holds or breaks down in those settings.

### Open Question 2
- Question: How does the representation complexity hierarchy change when considering stochastic MDPs or continuous state and action spaces?
- Basis in paper: [explicit] The paper mentions the extension to stochastic MDPs but does not provide a detailed analysis of the representation complexity in those settings.
- Why unresolved: The paper focuses on deterministic MDPs and discrete state and action spaces, leaving the representation complexity in stochastic and continuous settings unexplored.
- What evidence would resolve it: Analyzing the representation complexity in stochastic MDPs and continuous state and action spaces and comparing it to the hierarchy established in the paper.

### Open Question 3
- Question: What is the impact of the representation complexity hierarchy on the sample efficiency of different RL algorithms?
- Basis in paper: [explicit] The paper mentions the connection between representation complexity and sample efficiency but does not provide a detailed analysis of this relationship.
- Why unresolved: The paper focuses on establishing the representation complexity hierarchy but does not explore its implications for sample efficiency.
- What evidence would resolve it: Conducting empirical studies comparing the sample efficiency of different RL algorithms in the context of the representation complexity hierarchy established in the paper.

## Limitations
- Relies on unproven complexity theory assumptions (P ≠ NP, P ≠ P-complete not in TC0)
- Gap between theoretical circuit complexity and practical neural network expressivity remains unclear
- Analysis focuses on deterministic MDPs with discrete state and action spaces

## Confidence
- High Confidence: The AC0 representation of model components (rewards, transitions) in the constructed MDPs
- Medium Confidence: The NP-completeness and P-completeness of optimal value and policy representations
- Low Confidence: The practical implications for deep RL with MLPs

## Next Checks
1. Verify NP-completeness preservation: Construct the log-space uniform circuits for the reduction from 3-SAT to the optimal policy/value computation in the NP MDP and verify that computational hardness is preserved through the encoding.
2. Test MLP expressiveness bounds: Implement constant-layer MLPs with polynomial hidden dimensions and empirically test their ability to represent the model components versus optimal policies/values from the constructed MDPs, comparing theoretical predictions with practical results.
3. Analyze alternative MDP constructions: Design and analyze MDP variants that challenge the hierarchy (e.g., MDPs where the optimal policy is harder to represent than the value function) to test the robustness of the claimed complexity ordering.