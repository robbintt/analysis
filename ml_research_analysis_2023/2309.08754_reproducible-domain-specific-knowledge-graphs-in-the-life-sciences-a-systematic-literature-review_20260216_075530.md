---
ver: rpa2
title: 'Reproducible Domain-Specific Knowledge Graphs in the Life Sciences: a Systematic
  Literature Review'
arxiv_id: '2309.08754'
source_url: https://arxiv.org/abs/2309.08754
tags:
- knowledge
- data
- graph
- reproducibility
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically surveyed domain-specific knowledge graphs
  (KGs) in 19 domains, focusing on reproducibility. Among 250 analyzed KGs, only 8
  (3.2%) provided publicly available source code.
---

# Reproducible Domain-Specific Knowledge Graphs in the Life Sciences: a Systematic Literature Review

## Quick Facts
- arXiv ID: 2309.08754
- Source URL: https://arxiv.org/abs/2309.08754
- Reference count: 40
- Key outcome: Only 1 out of 250 surveyed domain-specific KGs (0.4%) successfully passed reproducibility assessment despite 3.2% providing open-source code

## Executive Summary
This systematic literature review examines reproducibility in domain-specific knowledge graphs across 19 scientific domains, with a particular focus on life sciences applications. The study analyzed 250 KGs and found that while machine learning approaches dominate KG construction, reproducibility practices remain severely lacking. Only 8 KGs provided publicly available source code, and of those, just one successfully passed the reproducibility assessment. The research identifies critical gaps in provenance tracking, cross-linking, and documentation that prevent knowledge graphs from being reliably reproduced and reused.

## Method Summary
The study conducted a systematic literature review of domain-specific knowledge graphs published between 2010 and 2022, focusing on papers that explicitly discussed KG construction methods. Researchers manually analyzed repositories of identified KGs to assess code availability, data accessibility, and reproducibility potential. The evaluation included attempting to execute provided code with specified data sources to verify successful regeneration of the knowledge graphs. Reproducibility criteria encompassed code/data availability, licenses, DOIs, execution environments, run instructions, online demos, SPARQL endpoints, successful regeneration, and provenance tracking.

## Key Results
- Only 3.2% (8 out of 250) of surveyed KGs provided publicly available source code
- Of those with code, only 1 KG (0.4% of all surveyed) successfully passed reproducibility assessment
- Biomedical domains had the highest concentration of KGs, followed by social sciences
- Machine learning was the most common construction method across all domains
- Most KGs lacked cross-linking to external resources and provenance tracking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reproducibility requires both code availability and executable run instructions.
- Mechanism: Code without clear execution guidance prevents successful regeneration of the KG, even if the data source is accessible.
- Core assumption: An open-source repository implies reproducibility if run instructions are provided.
- Evidence anchors:
  - [abstract] Only 8 out of 250 KGs provided open-source code, and of those, only one passed the reproducibility assessment.
  - [section] "FarseBase, MDKG, CROssBAR, and ETKG do not provide run instructions on their repository... we cannot assert their reproducibility."
  - [corpus] Weak evidence; neighboring papers discuss KG construction and reproducibility but lack direct evidence on instruction completeness.
- Break condition: Code is available but lacks run instructions or execution environment details.

### Mechanism 2
- Claim: Provenance tracking and cross-linking are critical for reproducibility but rarely implemented.
- Mechanism: Without provenance, reproducing a KG requires exact recreation of data, code, and library versions; without cross-linking, external validation is impossible.
- Core assumption: Cross-linking is part of reproducibility assessment.
- Evidence anchors:
  - [abstract] "Most KGs lacked cross-linking and provenance tracking."
  - [section] "Tracking provenance of KG construction is rarely addressed in most papers..."
  - [corpus] Weak; neighboring papers discuss KG embeddings and applications but not provenance or cross-linking specifically.
- Break condition: Provenance is omitted or cross-linking is absent.

### Mechanism 3
- Claim: Reproducibility demands not just data availability but also DOI assignment for both data and code.
- Mechanism: Persistent identifiers ensure that exact versions of data and code can be retrieved, preventing drift over time.
- Core assumption: DOI availability is part of reproducibility criteria.
- Evidence anchors:
  - [abstract] "Only one system out of seven open-source KGs (not considering RTX-KG2) could successfully run."
  - [section] "To ensure findability, the code and data should have persistent identifiers..."
  - [corpus] Weak; no direct evidence in neighboring papers about DOI assignment.
- Break condition: Data or code lacks DOI or is moved from original repository.

## Foundational Learning

- Concept: FAIR principles (Findability, Accessibility, Interoperability, Reusability)
  - Why needed here: The study references FAIR principles as an interesting direction beyond reproducibility; understanding them helps contextualize reproducibility criteria.
  - Quick check question: Which FAIR principle ensures that data and code have persistent identifiers?

- Concept: Knowledge Graph construction methods (machine learning vs. heuristic)
  - Why needed here: The survey shows machine learning is the most common method; knowing the difference helps evaluate reproducibility effort.
  - Quick check question: What is the main difference between machine learning and heuristic approaches in KG construction?

- Concept: Provenance in KG development
  - Why needed here: Provenance is rarely tracked but critical for reproducibility; understanding its scope clarifies why most KGs fail reproducibility.
  - Quick check question: What two types of provenance are mentioned as useful for KG reproducibility?

## Architecture Onboarding

- Component map:
  - Data acquisition layer (APIs, crawls, dumps)
  - Processing pipeline (NLP, ML, heuristics)
  - Knowledge Graph storage (triplestore, NoSQL)
  - Query interface (SPARQL endpoint, demo app)
  - Provenance tracker (optional but recommended)

- Critical path: Data → Processing → KG → Query Interface → Validation

- Design tradeoffs:
  - Open vs. closed data: Open data enables reproducibility but may require DOI assignment.
  - Complexity vs. usability: More sophisticated provenance tracking increases reproducibility but adds overhead.
  - Library versioning: Strict version pinning ensures reproducibility but may limit future compatibility.

- Failure signatures:
  - Missing run instructions → cannot execute code
  - Unavailable data → cannot regenerate KG
  - Lack of DOI → data/code cannot be uniquely retrieved
  - No provenance → cannot trace construction steps

- First 3 experiments:
  1. Clone an open-source KG repository and attempt to run it following provided instructions.
  2. Check for DOI assignments on data and code sources in the repository.
  3. Verify SPARQL endpoint functionality and cross-linking to external KGs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific barriers prevent the widespread adoption of reproducibility practices in knowledge graph construction across different domains?
- Basis in paper: [explicit] The paper states that only 0.4% of domain-specific KGs are reproducible and highlights challenges including lack of code/data availability, execution environments, and provenance tracking.
- Why unresolved: While the paper identifies specific criteria for reproducibility and notes that only 8 out of 250 KGs provided source code, it doesn't deeply explore the underlying cultural, technical, or institutional barriers that prevent broader adoption of these practices.
- What evidence would resolve it: A comprehensive survey of KG developers across multiple domains examining their attitudes, technical constraints, and institutional policies regarding reproducibility practices.

### Open Question 2
- Question: How does the absence of cross-linking between knowledge graphs impact their overall utility and potential for integration across domains?
- Basis in paper: [explicit] The paper notes that "most KGs did not provide cross-linkage" and only three out of eight open-source KGs provide cross-linking to external resources like Wikidata or DBpedia.
- Why unresolved: While the paper observes the lack of cross-linking as a general trend, it doesn't analyze the specific consequences this has on KG utility, integration capabilities, or the potential for creating more comprehensive knowledge networks.
- What evidence would resolve it: Case studies comparing the functionality and application scope of cross-linked versus non-cross-linked KGs, along with metrics measuring the impact on data integration and knowledge discovery.

### Open Question 3
- Question: What is the relationship between the complexity of a knowledge graph's construction method (e.g., machine learning vs. heuristic approaches) and its reproducibility?
- Basis in paper: [explicit] The paper observes that "machine learning approaches are the most popular construction method" but doesn't analyze how different construction methods correlate with reproducibility success rates.
- Why unresolved: The paper provides a general overview of construction methods used across KGs but doesn't investigate whether certain methods are inherently more or less reproducible, or how method complexity affects the ability to reproduce results.
- What evidence would resolve it: Comparative analysis of reproducibility success rates across different construction methods, examining factors like documentation quality, dependency management, and computational requirements.

## Limitations
- The reproducibility assessment sample size is small (7 KGs tested) due to limited availability of open-source code
- The evaluation was conducted by a single researcher, potentially introducing subjectivity
- The study did not assess the quality, completeness, or accuracy of regenerated KGs

## Confidence
- High confidence: The finding that only 3.2% of surveyed KGs provide open-source code is well-supported by the systematic literature review methodology and explicit counting of repositories.
- Medium confidence: The claim that only 0.4% of KGs passed reproducibility assessment is based on a small sample size (7 KGs tested) and may not be generalizable across all domains.
- Medium confidence: The observation that biomedical domains dominate KG development is supported by the data but may reflect publication venue bias rather than actual domain distribution.

## Next Checks
1. Replicate the reproducibility assessment with multiple independent reviewers to reduce subjectivity in evaluating run instructions and execution success.
2. Conduct a follow-up survey in 2-3 years to assess whether reproducibility practices have improved, particularly regarding DOI assignment and provenance tracking.
3. Test the reproducibility criteria on a broader set of KGs beyond the biomedical domain to evaluate domain-specific differences in reproducibility practices.