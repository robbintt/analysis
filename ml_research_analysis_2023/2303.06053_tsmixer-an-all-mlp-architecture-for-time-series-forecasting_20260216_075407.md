---
ver: rpa2
title: 'TSMixer: An All-MLP Architecture for Time Series Forecasting'
arxiv_id: '2303.06053'
source_url: https://arxiv.org/abs/2303.06053
tags:
- time
- forecasting
- tsmixer
- series
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TSMixer, an MLP-based architecture for time
  series forecasting. The method uses alternating time-mixing and feature-mixing MLPs
  to aggregate information efficiently, avoiding overfitting while leveraging cross-variate
  information.
---

# TSMixer: An All-MLP Architecture for Time Series Forecasting

## Quick Facts
- arXiv ID: 2303.06053
- Source URL: https://arxiv.org/abs/2303.06053
- Authors: 
- Reference count: 26
- Primary result: TSMixer achieves 0.640±0.013 WRMSSE on M5 dataset, outperforming specialized state-of-the-art models

## Executive Summary
This paper proposes TSMixer, an all-MLP architecture for time series forecasting that uses alternating time-mixing and feature-mixing operations. The key insight is that by interleaving operations that aggregate information along time and feature dimensions separately, the model can maintain time-step-dependent characteristics while avoiding overfitting. On long-term forecasting benchmarks, TSMixer is competitive with specialized state-of-the-art models and outperforms alternatives on the large-scale M5 retail dataset.

## Method Summary
TSMixer is a simple MLP-based architecture that uses alternating time-mixing and feature-mixing operations to efficiently extract information from time series data. The model applies a time-mixing MLP to the transposed input (aggregating information across time steps) followed by a feature-mixing MLP (aggregating information across features). This interleaving design allows information aggregation when stacking deeper while maintaining time-step-dependent characteristics. The architecture includes residual connections, dropout, and can incorporate static and future time-varying features. It uses a learnable normalization layer and can be trained with standard optimizers like Adam.

## Key Results
- TSMixer achieves 0.640±0.013 WRMSSE on the M5 dataset, outperforming specialized state-of-the-art models
- On long-term forecasting benchmarks (ETTm1, ETTm2, ECL, Traffic), TSMixer is competitive with specialized models
- The model maintains performance at lookback window L=720 while being parameter-efficient (O(L+C) vs O(LC))

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TSMixer's alternating time-mixing and feature-mixing MLPs maintain time-step-dependent characteristics while enabling deep stacking.
- Mechanism: By interleaving operations that aggregate information along time and feature dimensions separately, the architecture preserves the fixed weights per time step while allowing nonlinear interactions.
- Core assumption: Time series data often exhibit smoothness or periodicity that linear models can capture effectively.
- Evidence anchors:
  - [abstract] "TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently"
  - [section] "These interleaving operations allow information aggregation when stacking deeper while better maintaining the time-step-dependent characteristics of the data"
  - [corpus] Weak evidence - no direct mention of time-step-dependent mixing in corpus papers
- Break condition: If time series data lacks smoothness/periodicity or cross-variate relationships are absent, the interleaving advantage diminishes.

### Mechanism 2
- Claim: Linear models' time-step-dependent weights are more suitable for time series than data-dependent weights in RNNs/Transformers.
- Mechanism: Fixed weights per time step align with periodic/smooth temporal patterns, avoiding the difficulty of learning equivalent time-step-independent behavior in data-dependent models.
- Core assumption: Most real-world time series exhibit smoothness or periodicity that makes them predictable.
- Evidence anchors:
  - [section] "The time-step-dependent linear model is simple yet powerful for time series forecasting as we study above"
  - [section] "For data-dependent models...it is surprisingly challenging to update the data-dependent models...to be equivalent to time-step-independent models"
  - [corpus] Weak evidence - corpus lacks discussion of time-step vs data-dependent model comparisons
- Break condition: Non-smooth, non-periodic time series where the periodicity assumption breaks down.

### Mechanism 3
- Claim: Efficient parameter growth (O(L+C) vs O(LC)) prevents overfitting while allowing long lookback windows.
- Mechanism: Interleaving time and feature mixing operations keeps parameter count manageable compared to naive MLPs that mix all dimensions simultaneously.
- Core assumption: Longer lookback windows improve forecasting performance when cross-variate information is informative.
- Evidence anchors:
  - [section] "Compared to naive MLP mixing that mixes all the information, these interleaving operations allow information aggregation when stacking deeper while better maintaining the time-step-dependent characteristics of the data"
  - [section] "The interleaving design between these two operations efficiently leverages both temporal dependencies and cross-variate information with respect to both computation and model size"
  - [corpus] Weak evidence - no direct parameter efficiency comparisons in corpus papers
- Break condition: Very short time series where lookback window benefits are minimal.

## Foundational Learning

- Concept: Linear models for time series
  - Why needed here: The paper builds TSMixer based on insights from linear model effectiveness
  - Quick check question: Why do linear models perform surprisingly well on long-term forecasting benchmarks?

- Concept: Time-step-dependent vs data-dependent models
  - Why needed here: TSMixer's design philosophy stems from understanding this distinction
  - Quick check question: What is the fundamental difference between how linear models and Transformers handle time steps?

- Concept: Parameter efficiency in deep architectures
  - Why needed here: TSMixer's interleaving design trades off parameter growth against modeling capacity
  - Quick check question: How does the parameter growth of TSMixer compare to naive MLPs as lookback window size increases?

## Architecture Onboarding

- Component map: Input normalization → Time-mixing MLP → Feature-mixing MLP → (repeat K times) → Final linear projection → Output
- Critical path: Input → Time-mixing MLP → Feature-mixing MLP → (repeat K times) → Final linear projection → Output
- Design tradeoffs:
  - Time-mixing vs feature-mixing: Balancing temporal aggregation with cross-variate information
  - Single vs multi-layer perceptrons: Trade-off between simplicity and nonlinear capacity
  - Parameter efficiency vs modeling power: Interleaving design reduces parameters but may limit some interactions
- Failure signatures:
  - Poor performance on non-smooth, non-periodic time series
  - Overfitting when lookback window is too long relative to data size
  - Underperformance when cross-variate information is absent or uninformative
- First 3 experiments:
  1. Compare TSMixer vs naive MLP with same parameter count on synthetic periodic vs non-periodic data
  2. Vary lookback window size while holding other hyperparameters constant
  3. Test interleaving design by swapping time-mixing and feature-mixing order

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions but acknowledges several areas for future work. The main limitation noted is that experiments were only conducted on periodic datasets, and future work could explore non-periodic time series data.

## Limitations

- Limited experimental validation on non-periodic time series where core assumptions may break
- Only tested on a single large-scale real-world dataset (M5) with limited diversity
- No ablation studies to isolate the contribution of alternating design vs other architectural components

## Confidence

- Time-step-dependent models are superior to data-dependent models: Low confidence
- Alternating design prevents overfitting while maintaining temporal characteristics: Low confidence
- Parameter efficiency enables longer lookback windows: Medium confidence

## Next Checks

1. Test TSMixer on non-periodic synthetic datasets (e.g., chaotic time series) to validate performance when smoothness assumptions fail
2. Compare parameter efficiency claims by training naive MLPs with matched parameter counts across different lookback window sizes
3. Conduct an ablation study removing the alternating design to quantify the contribution of time-mixing vs feature-mixing operations to overall performance