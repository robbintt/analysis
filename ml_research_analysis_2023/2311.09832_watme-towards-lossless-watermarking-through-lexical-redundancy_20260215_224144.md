---
ver: rpa2
title: 'WatME: Towards Lossless Watermarking Through Lexical Redundancy'
arxiv_id: '2311.09832'
source_url: https://arxiv.org/abs/2311.09832
tags:
- text
- watermark
- x-mark
- watermarking
- list
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how watermarking affects large language
  model (LLM) capabilities and proposes a novel approach called X-Mark to address
  watermarking-induced performance degradation. X-Mark exploits lexical redundancy
  by applying mutual exclusion rules to interchangeable word clusters during decoding,
  preserving token availability and maintaining LLM expressiveness.
---

# WatME: Towards Lossless Watermarking Through Lexical Redundancy

## Quick Facts
- **arXiv ID:** 2311.09832
- **Source URL:** https://arxiv.org/abs/2311.09832
- **Reference count:** 10
- **Key outcome:** X-Mark significantly reduces watermarking-induced performance degradation on LLM capabilities while maintaining comparable watermark detection performance

## Executive Summary
This paper addresses the critical problem of watermarking-induced performance degradation in large language models (LLMs). The authors propose X-Mark, a novel watermarking approach that exploits lexical redundancy by applying mutual exclusion rules to interchangeable word clusters during decoding. This method preserves token availability and maintains LLM expressiveness while achieving effective watermark detection. X-Mark demonstrates substantial improvements over vanilla watermarking, particularly in retaining emergent abilities like knowledge recall and logical reasoning, with GSM8K accuracy dropping only 18.3% compared to 50.0% for standard watermarking methods.

## Method Summary
X-Mark addresses watermarking-induced performance degradation by exploiting lexical redundancy in language models. The method involves mining interchangeable lexical clusters from vocabulary, then during decoding applying mutual exclusion rules to partition these clusters such that if one synonym is in the red list, at least one synonym remains in the green list. This preserves token availability and maintains language model expressiveness. The approach uses two mining methods (dictionary-based and prompting-based) to identify interchangeable tokens, then applies watermark strength parameters during sampling while ensuring at least one suitable token remains available for each semantic/syntactic function.

## Key Results
- X-Mark achieves 18.3% accuracy reduction on GSM8K vs 50.0% for vanilla watermarking while maintaining AUROC around 0.8-0.9
- The method consistently outperforms vanilla watermarking across different hyperparameters and model variants
- X-Mark reduces perplexity compared to vanilla watermarking by favoring suitable tokens during decoding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: X-Mark exploits lexical redundancy by ensuring at least one interchangeable synonym remains in the green list, preserving token availability.
- Mechanism: During decoding, X-Mark partitions interchangeable tokens such that if one synonym is in the red list, at least one synonym remains in the green list. This prevents complete loss of suitable tokens for a given semantic/syntactic function.
- Core assumption: Interchangeable tokens within a lexical cluster can fully substitute for each other without altering meaning or grammatical correctness.
- Evidence anchors:
  - [abstract]: "X-Mark exploits lexical redundancy by applying mutual exclusion rules to interchangeable word clusters during decoding, preserving token availability"
  - [section 3.3]: "interchangeable tokens are mutually exclusive during partitioning...if a fraction of tokens A...is assigned to the red list, then their remaining synonyms B should be placed on the green list"
  - [corpus]: Weak - no direct citation on synonym substitution quality
- Break condition: If synonyms are not truly interchangeable (e.g., due to polysemy or grammatical constraints), the mutual exclusion rule fails to preserve quality.

### Mechanism 2
- Claim: X-Mark reduces perplexity compared to vanilla watermarking by favoring suitable tokens.
- Mechanism: By ensuring at least one suitable token remains in the green list, X-Mark increases the probability of selecting a high-quality token during decoding. This maintains the language model's expressiveness and fluency.
- Core assumption: Tokens in the green list are more likely to be selected than those in the red list when the watermark strength δ is large.
- Evidence anchors:
  - [abstract]: "Theoretical analysis shows X-Mark reduces perplexity compared to vanilla watermarking by favoring suitable tokens"
  - [section 3.4]: "Under the conditions of Assumption 3.1...X-Mark is more likely to select a suitable token compared to the vanilla watermarking method"
  - [corpus]: No direct evidence on perplexity comparison
- Break condition: If the watermark strength δ is too small, the distinction between green and red lists becomes negligible, negating the benefit.

### Mechanism 3
- Claim: X-Mark preserves emergent abilities (knowledge recall, logical reasoning, instruction following) better than vanilla watermarking.
- Mechanism: By maintaining token availability and reducing perplexity, X-Mark allows the LLM to perform tasks requiring crystallized and fluid intelligence with less degradation.
- Core assumption: The quality of text generation directly impacts the LLM's ability to perform complex tasks like reasoning and knowledge recall.
- Evidence anchors:
  - [abstract]: "Experiments on Llama2-7B and Vicuna-7B demonstrate that X-Mark significantly improves retention of emergent abilities...while maintaining comparable watermark detection performance"
  - [section 4]: "we propose to examine crystallized intelligence through an assessment of the model's knowledge capabilities, and fluid intelligence through its ability to reason and solve mathematical problems"
  - [corpus]: Weak - no direct evidence on emergent abilities preservation
- Break condition: If the watermark detection requirement is too strict (very high δ), even X-Mark may degrade performance.

## Foundational Learning

- Concept: Lexical redundancy in NLP
  - Why needed here: Understanding that interchangeable tokens exist in the vocabulary is crucial for designing X-Mark's mutual exclusion rule.
  - Quick check question: Can you give an example of a set of interchangeable tokens that could form a lexical cluster?

- Concept: Mutual exclusion in partitioning
  - Why needed here: The mutual exclusion rule is the core mechanism that ensures at least one suitable token remains available during watermarking.
  - Quick check question: How does the mutual exclusion rule differ from a simple random partitioning of the vocabulary?

- Concept: Watermark detectability vs. text quality tradeoff
  - Why needed here: Understanding this tradeoff is essential for setting the watermark strength δ and interpreting the results.
  - Quick check question: What happens to text quality and watermark detectability as the watermark strength δ increases?

## Architecture Onboarding

- Component map: Lexical cluster mining -> Mutual exclusion partitioning -> Watermark application -> Token sampling -> z-statistic detection

- Critical path:
  1. Mine lexical clusters from vocabulary
  2. During decoding, partition vocabulary into green and red lists using mutual exclusion rule
  3. Apply watermark strength δ to logits of green list tokens
  4. Sample next token from modified distribution
  5. Repeat for each decoding step
  6. Detect watermark by computing z-statistic

- Design tradeoffs:
  - Cluster quality vs. mining cost (dictionary-based is faster but less comprehensive than prompting-based)
  - Watermark strength δ vs. text quality degradation
  - Green list size γ vs. watermark detectability

- Failure signatures:
  - High perplexity or low fluency in generated text
  - Poor performance on emergent ability tasks (e.g., low accuracy on GSM8K)
  - Low AUROC in watermark detection

- First 3 experiments:
  1. Compare perplexity of X-Mark vs. vanilla watermarking on a held-out test set
  2. Evaluate accuracy on GSM8K for X-Mark vs. vanilla watermarking
  3. Measure AUROC for watermark detection for X-Mark vs. vanilla watermarking

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the identified lexical clusters vary in quality and coverage across different language models or datasets?
- Basis in paper: [inferred] The paper mentions using WordNet and Youdao Dictionary, as well as prompting LLaMA2 to infer synonyms, suggesting potential variability in cluster quality and coverage.
- Why unresolved: The paper does not provide a detailed analysis of the consistency and robustness of the identified lexical clusters across different language models or datasets.
- What evidence would resolve it: A comparative study of the lexical clusters identified using different methods and across different language models or datasets would provide insights into the variability in cluster quality and coverage.

### Open Question 2
- Question: What are the specific grammatical rules used to eliminate words with inconsistent grammatical features during the mining of interchangeable words?
- Basis in paper: [explicit] The paper mentions that grammatical factors such as tense, voice, and number are considered during the mining of interchangeable words, and explicit rules are devised to eliminate words with inconsistent grammatical features.
- Why unresolved: The paper does not provide the specific grammatical rules used in the mining process, which could be crucial for replicating the study or applying the method to other languages.
- What evidence would resolve it: Detailed documentation of the grammatical rules used in the mining process would allow for replication and application of the method to other languages or datasets.

### Open Question 3
- Question: How does the performance of X-Mark compare to other watermarking methods in terms of detection performance and preservation of LLM capabilities?
- Basis in paper: [inferred] The paper focuses on comparing X-Mark to vanilla watermarking, but does not provide a comprehensive comparison with other existing watermarking methods.
- Why unresolved: A thorough comparison with other watermarking methods would provide a better understanding of X-Mark's relative performance and potential areas for improvement.
- What evidence would resolve it: A comparative study of X-Mark against other state-of-the-art watermarking methods in terms of detection performance and preservation of LLM capabilities would provide valuable insights into its relative effectiveness.

## Limitations
- The paper does not validate the quality and interchangeability of mined lexical clusters through empirical testing or human evaluation
- Evaluation is limited to only two model architectures (Llama2-7B and Vicuna-7B) and focuses primarily on GSM8K and TruthfulQA benchmarks
- No detailed analysis of optimal watermark strength calibration or how the tradeoff varies across different settings

## Confidence
- **High Confidence**: The theoretical mechanism of mutual exclusion partitioning and its potential to preserve token availability is well-articulated. The core insight that partitioning interchangeable tokens can maintain quality while watermarking is sound and logically coherent.
- **Medium Confidence**: The experimental results showing X-Mark's superiority over vanilla watermarking on GSM8K accuracy and TruthfulQA scores. While the results are promising, the evaluation scope is limited and doesn't cover the full range of LLM capabilities or model architectures.
- **Low Confidence**: The claim about perplexity reduction in X-Mark compared to vanilla watermarking. The paper mentions theoretical analysis but provides no direct experimental evidence or quantitative comparison of perplexity values.

## Next Checks
1. **Cluster Quality Validation**: Conduct human evaluation of mined lexical clusters to assess whether tokens within clusters are truly interchangeable across different contexts. Test substitutions from X-Mark's clusters in various sentence constructions to verify semantic and grammatical preservation.

2. **Broader Capability Assessment**: Extend evaluation beyond GSM8K and TruthfulQA to include instruction following benchmarks (like AlpacaEval), code generation tasks, and creative writing prompts. This would provide a more comprehensive view of how watermarking affects different LLM capabilities.

3. **Cross-Model Generalization**: Test X-Mark on additional model architectures beyond Llama2-7B and Vicuna-7B, including larger models (13B, 70B parameters) and different model families. This would validate whether the mutual exclusion mechanism generalizes across diverse LLM architectures and training paradigms.