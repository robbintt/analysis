---
ver: rpa2
title: Skin Lesion Segmentation Improved by Transformer-based Networks with Inter-scale
  Dependency Modeling
arxiv_id: '2310.13604'
source_url: https://arxiv.org/abs/2310.13604
tags:
- segmentation
- transformer
- attention
- skin
- efficient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a transformer-based U-Net architecture for
  skin lesion segmentation, addressing the challenge of capturing long-range dependencies
  in medical image segmentation. The proposed method utilizes an efficient attention
  mechanism and an Inter-scale Context Fusion (ISCF) module to mitigate semantic gaps
  between encoder and decoder.
---

# Skin Lesion Segmentation Improved by Transformer-based Networks with Inter-scale Dependency Modeling

## Quick Facts
- arXiv ID: 2310.13604
- Source URL: https://arxiv.org/abs/2310.13604
- Reference count: 29
- Primary result: Achieves state-of-the-art performance on ISIC 2017 and ISIC 2018 skin lesion segmentation benchmarks without pre-training

## Executive Summary
This paper presents a transformer-based U-Net architecture for skin lesion segmentation that addresses the challenge of capturing long-range dependencies in medical image segmentation. The proposed method utilizes an efficient attention mechanism and an Inter-scale Context Fusion (ISCF) module to mitigate semantic gaps between encoder and decoder. The model achieves state-of-the-art results on two public skin lesion segmentation benchmarks, ISIC 2017 and ISIC 2018, outperforming existing methods without requiring pre-training weights. The ISCF module's effectiveness is demonstrated through quantitative and qualitative results, showcasing improved boundary preservation and segmentation accuracy.

## Method Summary
The method employs a pure transformer U-Net architecture with efficient attention and an Inter-scale Context Fusion (ISCF) module. The model uses patch embedding (4x4 overlapping) to extract patch tokens, followed by efficient transformer blocks organized in three stages. The encoder uses patch merging to reduce spatial resolution while expanding feature dimensions, and the decoder uses patch expanding to recover spatial resolution. The ISCF module integrates attention correlations at various scales to adaptively combine contexts from each stage, replacing traditional skip connection concatenation. The model is trained from scratch using binary cross-entropy loss for 100 epochs with Adam optimizer (learning rate 1e-4).

## Key Results
- Achieves state-of-the-art performance on ISIC 2017 and ISIC 2018 benchmarks without pre-training weights
- Demonstrates improved boundary preservation and segmentation accuracy through ISCF module
- Shows effective multi-scale attention fusion with quantitative and qualitative improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Efficient attention reduces quadratic complexity to linear in sequence length.
- Mechanism: Replaces softmax(QK^T) multiplication with softmax(Q) * (K^T V), shifting computation order.
- Core assumption: K and V are computed once per input sequence and can be reused.
- Evidence anchors:
  - [section]: "Shen et al. [20] proposed an approach called 'Efficient Attention' that takes advantage of the fact that regular self-attention creates repetitive context matrix entries. They suggested a more efficient method for computing self-attention..."
  - [section]: "Efficient attention does not first compute pairwise similarities between points. Instead, the keys are represented as dk attention maps k^T_j, with j referring to position j in the input feature."

### Mechanism 2
- Claim: Inter-scale Context Fusion (ISCF) reduces semantic gaps between encoder and decoder by recalibrating attention maps.
- Mechanism: Aggregates global context from multiple scales via global pooling, scales attention maps, and fuses them with skip connections.
- Core assumption: Attention maps from different scales encode complementary spatial information.
- Evidence anchors:
  - [abstract]: "Integrating an already calculated attention affinity within the skip connection path improves the typical concatenation process utilized in the conventional skip connection path."
  - [section]: "Instead of simply concatenating the features from the encoder and decoder layers, we devised a context fusion module to decrease the encoder-decoder semantic gap."

### Mechanism 3
- Claim: Hybrid CNN-Transformer design balances local and global feature extraction without pre-training.
- Mechanism: Uses patch embedding and efficient transformer blocks for global context, while hierarchical merging/expanding preserves spatial resolution.
- Core assumption: Skin lesions benefit from both fine-grained local texture and global shape context.
- Evidence anchors:
  - [section]: "Due to the shifting window strategy in Swin blocks, Swin-Unet captures the contextual information locally and is heavily dependent on pre-training weights."
  - [section]: "Our structure uses the patch merging and patch expanding strategies from [17,4]. The patch embedding module extracts overlapping patch tokens..."

## Foundational Learning

- Concept: Self-attention mechanism in transformers
  - Why needed here: Core to capturing long-range dependencies in skin lesion boundaries
  - Quick check question: How does self-attention compute relationships between all token pairs?

- Concept: Skip connections in encoder-decoder architectures
  - Why needed here: Required to preserve spatial resolution lost during downsampling
  - Quick check question: What information is lost if skip connections are omitted in U-Net?

- Concept: Global pooling and feature scaling
  - Why needed here: Enables multi-scale attention fusion in ISCF module
  - Quick check question: How does global pooling affect the spatial resolution of attention maps?

## Architecture Onboarding

- Component map:
  Input -> Patch embedding (4x4 overlapping) -> Efficient Transformer blocks (3 stages) -> Patch merging (encoder)
  Encoder output -> ISCF module -> Patch expanding (decoder) -> Efficient Transformer blocks -> Segmentation head
  Skip connections: fused via ISCF instead of direct concatenation

- Critical path:
  Patch embedding -> Efficient Transformer -> ISCF fusion -> Decoder expansion -> Segmentation head

- Design tradeoffs:
  - Efficient attention reduces parameters but requires careful normalization
  - ISCF adds no parameters but increases inference time due to multi-scale fusion
  - No pre-training means higher data requirements but avoids domain shift

- Failure signatures:
  - Poor boundary delineation: likely ISCF scaling factors not learned properly
  - Missing small lesions: patch embedding overlap too small or efficient attention not capturing fine details
  - Low sensitivity: attention maps not properly fused, losing critical features

- First 3 experiments:
  1. Replace efficient attention with standard multi-head self-attention, measure parameter increase and runtime
  2. Remove ISCF module, use plain skip connections, compare segmentation quality
  3. Vary patch embedding size (2x2 vs 4x4 vs 8x8), observe impact on small lesion detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Inter-scale Context Fusion (ISCF) module's performance vary with different numbers of encoder-decoder scale pairs (SLi and SL'i) in the U-shaped architecture?
- Basis in paper: [explicit] The ablation study in Table 2 shows performance improvements as the number of skip connection modules (scales) increases, supporting the efficacy of the attention module in capturing rich representation.
- Why unresolved: While the paper provides results for using one, two, and three scale pairs, it does not explore beyond three pairs or provide a theoretical explanation for the optimal number of scales.
- What evidence would resolve it: Further ablation studies with more than three scale pairs and a theoretical analysis of the optimal number of scales based on the model's capacity and the dataset's characteristics.

### Open Question 2
- Question: What is the impact of increasing the input image size beyond 384x384 pixels on the model's performance and computational cost?
- Basis in paper: [explicit] The ablation study mentions that increasing the input size to 384x384 leads to slightly improved segmentation results but incurs a higher computational cost.
- Why unresolved: The paper only explores one larger input size (384x384) and does not provide a comprehensive analysis of the trade-off between performance improvement and computational cost for even larger input sizes.
- What evidence would resolve it: A systematic study of the model's performance and computational cost for various input sizes, including sizes larger than 384x384, to determine the optimal input size for a given trade-off between accuracy and efficiency.

### Open Question 3
- Question: How does the proposed method perform on other medical image segmentation tasks beyond skin lesion segmentation, such as brain tumor segmentation or organ segmentation in CT/MRI scans?
- Basis in paper: [inferred] The paper focuses on skin lesion segmentation and demonstrates state-of-the-art results on two public skin lesion segmentation benchmarks (ISIC 2017 and ISIC 2018). However, it does not provide evidence of the method's generalizability to other medical image segmentation tasks.
- Why unresolved: The paper does not explore the application of the proposed method to other medical image segmentation tasks, which is necessary to establish its generalizability and effectiveness across different domains.
- What evidence would resolve it: Experiments on other medical image segmentation tasks, such as brain tumor segmentation or organ segmentation in CT/MRI scans, to demonstrate the method's performance and generalizability beyond skin lesion segmentation.

## Limitations

- Lack of ablation studies specifically isolating ISCF module contribution versus efficient attention mechanism
- Efficient attention implementation differs from mainstream approaches, raising questions about claimed efficiency gains
- Comparison with Swin-Unet may not be entirely fair given smaller dataset sizes used for training

## Confidence

- High confidence: Core methodology description and overall experimental setup
- Medium confidence: Claims about efficient attention complexity reduction
- Low confidence: Specific architectural details of ISCF implementation

## Next Checks

1. Implement an ablation study comparing ISCF versus standard skip connections while keeping efficient attention constant
2. Benchmark actual inference runtime against Swin-Unet to verify computational efficiency claims
3. Test model performance with varying patch embedding sizes to quantify impact on small lesion detection