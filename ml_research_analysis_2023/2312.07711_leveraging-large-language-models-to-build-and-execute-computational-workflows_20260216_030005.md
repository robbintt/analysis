---
ver: rpa2
title: Leveraging Large Language Models to Build and Execute Computational Workflows
arxiv_id: '2312.07711'
source_url: https://arxiv.org/abs/2312.07711
tags:
- code
- workflow
- function
- file
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates how large language models can be used to
  build and execute scientific workflows through natural language interfaces. The
  authors integrated OpenAI's function-calling API with Phyloflow, a phylogenetic
  analysis tool, enabling workflow execution via simple text commands.
---

# Leveraging Large Language Models to Build and Execute Computational Workflows

## Quick Facts
- arXiv ID: 2312.07711
- Source URL: https://arxiv.org/abs/2312.07711
- Reference count: 20
- Primary result: LLM function-calling API can translate natural language into executable computational workflows

## Executive Summary
This paper demonstrates how large language models can be used to build and execute scientific workflows through natural language interfaces. The authors integrated OpenAI's function-calling API with Phyloflow, a phylogenetic analysis tool, enabling workflow execution via simple text commands. They developed adapters for Parsl apps that translate natural language instructions into executable function calls. The prototype successfully executed multi-step phylogenetic workflows based on high-level user descriptions, representing an important step toward AI-driven workflow management systems. The work highlights both the potential and current limitations of LLM-based workflow automation, including error handling and token limits for complex workflows.

## Method Summary
The authors developed a system that leverages OpenAI's function-calling API to translate natural language instructions into executable Parsl applications for phylogenetic analysis. The method involves creating JSON function descriptions for Parsl apps, which the LLM uses to interpret user commands and generate appropriate function calls with arguments. Adapter functions bridge the gap between the LLM interface and Parsl execution model, handling both direct file inputs and AppFuture dependencies. The system maintains conversation context across multiple API calls to enable chaining of workflow steps. The prototype was tested using VCF files for phylogenetic analysis, executing a workflow that included vcf-transform, pyclone-vi, and spruce-phylogeny steps.

## Key Results
- Successfully executed multi-step phylogenetic workflows through natural language commands
- Demonstrated the feasibility of using LLM function-calling API for scientific workflow automation
- Identified key limitations including token limits and error handling challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM function-calling API can automatically translate natural language instructions into executable Parsl app calls
- Mechanism: The system defines function descriptions in JSON format that specify input parameters and behaviors. When a user provides a natural language instruction, the LLM selects the appropriate function and generates the required arguments as a JSON object, which is then executed by the Parsl framework.
- Core assumption: The LLM can accurately interpret natural language descriptions and map them to the correct function signatures
- Evidence anchors:
  - [abstract] "enabling workflow execution via simple text commands"
  - [section] "Based on a user-supplied description of a collection of functions, the LLM itself can infer which function to call, produce the necessary interface for the call, call the function, and process the output."
  - [corpus] Weak evidence - no direct corpus examples of this specific mechanism working in practice
- Break condition: The LLM misinterprets the user instruction or generates incorrect function arguments, leading to failed execution or wrong results

### Mechanism 2
- Claim: Context-aware conversation allows chaining multiple workflow steps through iterative API calls
- Mechanism: After each function execution, the system maintains context by passing the previous API response (specifically the chosen function and arguments) along with new user messages indicating the current workflow state. This allows the LLM to understand the current position in the workflow and execute subsequent steps appropriately.
- Core assumption: The LLM can maintain and utilize conversation context across multiple API calls to understand workflow progression
- Evidence anchors:
  - [section] "For the next API request, two new messages are added...By adding both messages, the AI understands which step it is in relative to the user's instructions and can also execute subsequent steps"
  - [abstract] "The prototype successfully executed multi-step phylogenetic workflows based on high-level user descriptions"
  - [corpus] No direct corpus evidence for this specific context-aware chaining mechanism
- Break condition: Token limits are reached in the conversation, causing the LLM to lose track of previous context and fail to maintain workflow state

### Mechanism 3
- Claim: Adapter functions bridge the gap between LLM function-calling interface and Parsl app execution
- Mechanism: For each Parsl app, two adapter functions are created - one for direct file input and another for AppFuture dependencies. These adapters handle the translation between the LLM's JSON arguments and the actual Parsl execution requirements, including future ID management and output directory handling.
- Core assumption: The adapter functions correctly translate between the LLM interface and Parsl's execution model
- Evidence anchors:
  - [section] "we created several functions that serve as adapters for Parsl apps...For each Parsl app, we created afunction_call_from_file, which receives the paths to the physical files, and a function_call_from_futures"
  - [abstract] "The authors integrated OpenAI's function-calling API with Phyloflow, a phylogenetic analysis tool"
  - [corpus] No corpus evidence for this specific adapter pattern
- Break condition: The adapter functions fail to correctly handle AppFuture dependencies or generate incorrect directory structures, causing workflow execution failures

## Foundational Learning

- Parsl framework and AppFuture concept
  - Why needed here: Understanding how Parsl manages parallel execution and futures is essential for creating the adapter functions that bridge between LLM calls and actual workflow execution
  - Quick check question: What is the difference between a DataFuture and an AppFuture in Parsl, and why does this distinction matter for the adapter functions?

- Function-calling API patterns and JSON schema design
  - Why needed here: The system relies on properly structured JSON function descriptions that the LLM can interpret. Understanding how to design these schemas is critical for the system to work correctly
  - Quick check question: What are the key components that must be included in a function description for the OpenAI function-calling API to work properly?

- Natural language to code translation limitations
- Why needed here: The system must account for the fact that LLMs can generate syntactically correct but semantically incorrect code, requiring validation and error handling mechanisms
- Quick check question: What are the primary limitations of LLM-generated code that the workflow system needs to handle?

## Architecture Onboarding

- Component map:
  User Interface (natural language input) -> OpenAI API (function calling and context management) -> Adapter layer (translates between LLM JSON and Parsl calls) -> Parsl framework (actual workflow execution) -> Context manager (maintains conversation state across API calls) -> Error handler (manages exceptions and debugging)

- Critical path:
  User instruction → LLM function selection → Adapter translation → Parsl execution → Context update → Next step

- Design tradeoffs:
  - Token limits vs. workflow complexity: Longer workflows may exceed token limits, requiring hierarchical decomposition
  - Context maintenance vs. system complexity: More context improves accuracy but increases complexity and potential for errors
  - Adapter abstraction vs. performance: More generic adapters are easier to maintain but may introduce overhead

- Failure signatures:
  - Incorrect function selection by LLM
  - Malformed JSON arguments from LLM
  - AppFuture dependency resolution failures
  - Token limit exceeded during conversation
  - Adapter function logic errors

- First 3 experiments:
  1. Test single-step workflow execution with direct file inputs to verify basic adapter functionality
  2. Test multi-step workflow with AppFuture dependencies to verify context maintenance and chaining
  3. Test error handling by providing malformed instructions to verify system robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the token limit issue be resolved for composing more complex workflows using the current function calling approach?
- Basis in paper: [explicit] The paper explicitly states: "The second limitation is that composing more complex workflows will eventually hit the token limit, for which there is no straightforward solution in the proposed scheme; we would need to invent a hierarchical schema for task decomposition."
- Why unresolved: The paper identifies the need for a hierarchical schema for task decomposition but does not propose or test any specific solution.
- What evidence would resolve it: A working implementation demonstrating successful execution of complex workflows using a hierarchical task decomposition approach that effectively manages token limits.

### Open Question 2
- Question: How can error handling be improved to allow the system to recover from incorrect function calls or execution failures?
- Basis in paper: [explicit] The paper explicitly states: "The first is that exceptions are not handled at the moment, which means that if the API executes a wrong function call, the program cannot recover from the failure. Optimally, the error should be forwarded to the API so that it can propose alternatives."
- Why unresolved: The current prototype lacks exception handling mechanisms, and the paper only suggests the ideal behavior without providing a concrete implementation.
- What evidence would resolve it: A system implementation that successfully catches errors during workflow execution and automatically proposes alternative solutions or corrective actions.

### Open Question 3
- Question: How can the debugger component in the proposed next-gen workflow engine effectively identify and resolve issues when a task fails or produces unexpected outcomes?
- Basis in paper: [explicit] The paper proposes a debugger component but does not detail its specific capabilities or implementation approach.
- Why unresolved: While the paper outlines the need for a debugger that can identify issues and suggest solutions, it does not provide details on how this component would function or what techniques it would employ.
- What evidence would resolve it: A working prototype of the debugger component demonstrating its ability to analyze failed workflow executions, identify root causes, and suggest appropriate corrective actions or plan modifications.

## Limitations
- No systematic validation beyond a single example workflow
- Token limits severely constrain complex workflow composition
- Lack of error handling prevents recovery from failed function calls

## Confidence
- **High confidence**: The basic mechanism of using function-calling API to translate natural language to executable code is well-established and demonstrated in the prototype
- **Medium confidence**: The context-aware conversation chaining mechanism works for simple multi-step workflows but may degrade with complexity
- **Low confidence**: The system's robustness to diverse workflow types, error handling capabilities, and scalability beyond the demonstrated example remain largely unverified

## Next Checks
1. **Token limit characterization**: Systematically test workflows of increasing complexity to identify the exact token threshold where context degradation occurs and evaluate strategies for hierarchical decomposition

2. **Error handling validation**: Create a comprehensive test suite with malformed instructions, conflicting dependencies, and partial failures to evaluate the robustness of the adapter layer and context management system

3. **Generalization testing**: Apply the system to workflow types beyond phylogenetic analysis (e.g., data preprocessing, machine learning pipelines) to assess the generalizability of the approach and identify domain-specific limitations