---
ver: rpa2
title: On Sampling with Approximate Transport Maps
arxiv_id: '2302.04763'
source_url: https://arxiv.org/abs/2302.04763
tags:
- sampling
- distribution
- ow-mcmc
- target
- transport
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study compares three main strategies for sampling using Normalizing
  Flows (NFs): neural Importance Sampling (neural-IS), flow-based Markov Chain Monte
  Carlo (flow-MCMC), and flow-reparametrized local MCMC (neutra-MCMC). It finds that
  flow-MCMC and neural-IS can reliably handle multimodal targets up to moderately
  high dimensions, while neutra-MCMC struggles with multimodality but is more robust
  for unimodal targets in high-dimensional settings and under poor training.'
---

# On Sampling with Approximate Transport Maps

## Quick Facts
- arXiv ID: 2302.04763
- Source URL: https://arxiv.org/abs/2302.04763
- Reference count: 40
- Primary result: Compares three NF-based sampling strategies (neural-IS, flow-MCMC, neutra-MCMC), finding flow-MCMC and neural-IS handle multimodal targets up to moderate dimensions, while neutra-MCMC is more robust for unimodal targets in high dimensions and under poor training.

## Executive Summary
This paper systematically compares three main strategies for sampling using Normalizing Flows: neural Importance Sampling (neural-IS), flow-based Markov Chain Monte Carlo (flow-MCMC), and flow-reparametrized local MCMC (neutra-MCMC). Through systematic synthetic experiments and real-world applications, the authors demonstrate that flow-MCMC and neural-IS are typically more effective for multimodal targets up to moderately high dimensions, while neutra-MCMC struggles with multimodality but shows greater robustness for unimodal targets in high-dimensional settings and under poor training conditions. The study also derives a new quantitative bound for the mixing time of Independent Metropolis-Hastings, showing that if the importance weight function is locally Lipschitz, the mixing time can be independent of dimension.

## Method Summary
The study compares three sampling strategies using normalizing flows: (1) neural-IS uses the flow as a proposal distribution for importance sampling, (2) flow-MCMC uses the flow to generate independent proposals for MCMC, and (3) neutra-MCMC uses the inverse flow to reparametrize the target and then applies local samplers. The methods are evaluated across synthetic case studies with varying dimensions and flow qualities, as well as real-world applications including molecular systems, logistic regression, and field systems. Performance is measured using metrics like sliced total variation, participation ratio, and mixing time.

## Key Results
- Flow-MCMC and neural-IS reliably handle multimodal targets up to moderately high dimensions
- Neura-MCMC struggles with multimodality but is more robust for unimodal targets in high-dimensional settings and under poor training
- New mixing time bound shows IMH can have dimension-independent mixing when importance weights are locally Lipschitz
- Performance degradation with increasing dimension affects all methods, but with different patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Flow-MCMC and neural-IS can reliably handle multimodal targets up to moderately high dimensions, while neutra-MCMC struggles with multimodality but is more robust otherwise in high-dimensional settings and under poor training.
- Mechanism: Flow-MCMC and neural-IS use the flow as a proposal distribution, enabling global jumps that can cross energy barriers between modes. This global exploration capability makes them effective for multimodal targets. However, their performance degrades with increasing dimension due to the curse of dimensionality. In contrast, neutra-MCMC uses the flow to reparametrize the space, then applies local samplers like MALA. This local exploration is more robust to poor training and high-dimensional unimodal targets but struggles to mix between modes in multimodal distributions because the flow cannot fully eliminate energy barriers.
- Core assumption: The quality of the learned transport map directly affects the performance of all three methods, but in different ways depending on the target distribution's geometry and dimension.
- Evidence anchors:
  - [abstract]: "flow-MCMC and neural-IS can reliably handle multimodal targets up to moderately high dimensions, while neutra-MCMC struggles with multimodality but is more robust otherwise in high-dimensional settings and under poor training."
  - [section]: "neural-IS and flow-MCMC are typically more effective for multimodal targets than neutra-MCMC."
  - [corpus]: Weak. Related papers focus on transport maps and normalizing flows but do not directly address the comparative robustness of these three methods.
- Break condition: If the learned flow is too poor in quality, flow-MCMC and neural-IS may fail due to vanishing acceptance rates or poor participation ratios, while neutra-MCMC may still provide reasonable exploration within modes.

### Mechanism 2
- Claim: A new quantitative bound for the mixing time of the Independent Metropolis-Hastings (IMH) sampler shows that if the importance weight function is locally Lipschitz, the mixing time can be independent of dimension.
- Mechanism: The analysis leverages a local Lipschitz condition on the log-weight function to derive a conductance lower bound for the IMH Markov kernel. This conductance bound, combined with the isoperimetric inequality of the target distribution, leads to an upper bound on the mixing time that depends only on the local quality of the proposal and not on the dimension, provided the proposal is sufficiently close to the target locally.
- Core assumption: The target distribution is strongly log-concave, and the proposal distribution is a Gaussian with variance close to the target's.
- Evidence anchors:
  - [abstract]: "The paper also derives a new quantitative bound for the mixing time of the Independent Metropolis-Hastings sampler, showing that if the importance weight function is locally Lipschitz, the mixing time can be independent of dimension."
  - [section]: "Theorem 4.3 shows that if CR is bounded by a constant independent of the dimension for R of order at least âˆšd, then the mixing time is also independent of the dimension."
  - [corpus]: Missing. Related papers do not discuss the specific mixing time bound for IMH under local Lipschitz conditions.
- Break condition: If the importance weight function is not locally Lipschitz or the proposal is too far from the target, the mixing time bound no longer holds and may depend on dimension.

### Mechanism 3
- Claim: Imperfect flow training leads to different performance impacts across the three sampling strategies, with neutra-MCMC being more robust to poor flow quality than flow-MCMC and neural-IS.
- Mechanism: When the flow imperfectly approximates the transport map, flow-MCMC and neural-IS suffer from low acceptance rates or poor importance weights, leading to inefficient sampling. However, neutra-MCMC still benefits from the preconditioning effect of the flow, which can improve the geometry of the target even if the flow is not perfect, allowing local samplers to perform better than they would on the original target.
- Core assumption: The flow provides some level of preconditioning that improves the geometry of the target distribution, even if it is not a perfect approximation.
- Evidence anchors:
  - [abstract]: "In contrast, methods relying on reparametrization struggle with multimodality but are more robust otherwise in high-dimensional settings and under poor training."
  - [section]: "More interestingly, neutra-MCMC (MALA) quickly outperforms flow-MCMC as the flow shifts away from the exact transport, both towards over-spreading and over-concentrating the mass."
  - [corpus]: Weak. Related papers discuss flow-based sampling but do not directly compare the robustness to imperfect flow training across these three methods.
- Break condition: If the flow is extremely poor, even neutra-MCMC may not provide significant benefits over standard local MCMC methods.

## Foundational Learning

- Concept: Normalizing Flows (NFs)
  - Why needed here: NFs are the core mechanism for learning transport maps between distributions, enabling the three sampling strategies compared in the paper.
  - Quick check question: What are the key properties of a normalizing flow that make it suitable for use in MCMC and importance sampling?

- Concept: Importance Sampling and Self-Normalized Importance Sampling (SNIS)
  - Why needed here: neural-IS relies on SNIS with the flow as the proposal distribution, and the mixing time analysis of IMH is closely related to the behavior of importance weights.
  - Quick check question: How does the choice of proposal distribution affect the efficiency of importance sampling, and what role does the flow play in this context?

- Concept: Markov Chain Monte Carlo (MCMC) and Mixing Time
  - Why needed here: Understanding MCMC algorithms, particularly those with global and local proposals, is crucial for interpreting the performance of flow-MCMC, neutra-MCMC, and their comparison.
  - Quick check question: What is the mixing time of an MCMC algorithm, and how does it relate to the conductance of the Markov kernel?

## Architecture Onboarding

- Component map: Normalizing Flow -> neural-IS/Flow-MCMC/Neura-MCMC -> Sampling Performance Metrics
- Critical path:
  1. Train a normalizing flow to approximate the transport map between the base and target distributions.
  2. Choose a sampling strategy (flow-MCMC, neural-IS, or neutra-MCMC) based on the target's geometry and dimension.
  3. Evaluate the performance of the chosen strategy using metrics like sliced total variation, participation ratio, and mixing time.
  4. Analyze the robustness of the strategy to imperfect flow training and high dimensionality.
- Design tradeoffs:
  - Flow-MCMC and neural-IS offer global exploration but are sensitive to dimension and flow quality.
  - Neura-MCMC is more robust to poor flow training and high-dimensional unimodal targets but struggles with multimodality.
  - The choice of local sampler in neutra-MCMC (e.g., MALA vs. ESS) affects its ability to mix between modes.
- Failure signatures:
  - Flow-MCMC: Vanishing acceptance rates or poor participation ratios indicate the flow is too far from the target.
  - Neural-IS: High variance in importance weights suggests the proposal is not well-matched to the target.
  - Neura-MCMC: Chains remaining in a single mode indicate the flow has not effectively eliminated energy barriers.
- First 3 experiments:
  1. Train a normalizing flow on a simple multimodal target (e.g., a mixture of Gaussians) and compare the performance of flow-MCMC, neural-IS, and neutra-MCMC.
  2. Vary the quality of the flow (e.g., by interpolating between a perfect flow and a poor approximation) and observe how each method's performance changes.
  3. Test the methods on a high-dimensional unimodal target (e.g., a badly conditioned Gaussian) to evaluate their robustness to dimension and poor flow training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of neutra-MCMC methods change when using mixture base distributions or mixture of normalizing flows to accommodate multimodal targets?
- Basis in paper: [explicit] The paper mentions that using mixture base distributions or mixture of normalizing flows can accommodate multimodal targets, but it's unclear how to combine them with neutra-MCMC.
- Why unresolved: The paper does not provide experimental results or theoretical analysis of neutra-MCMC with mixture models.
- What evidence would resolve it: Experimental results comparing neutra-MCMC with mixture models to neutra-MCMC with single flow and other methods like flow-MCMC and neural-IS on multimodal targets.

### Open Question 2
- Question: How does the mixing time of Independent Metropolis-Hastings scale with the dimension when the importance weight function is not uniformly bounded but only locally Lipschitz?
- Basis in paper: [explicit] The paper derives a new mixing time bound for IMH assuming locally Lipschitz importance weights, but does not provide extensive empirical validation or explore the asymptotic scaling.
- Why unresolved: The theoretical bound is derived but not extensively tested across different dimensions and target distributions.
- What evidence would resolve it: Systematic experiments measuring the actual mixing time of IMH across a range of dimensions and target distributions with varying levels of proposal-target mismatch.

### Open Question 3
- Question: What is the optimal strategy for alternating between global flow-MCMC steps and flow-preconditioned local MCMC steps to balance exploration and exploitation in sampling?
- Basis in paper: [explicit] The paper suggests that alternating between global updates (crossing energy barriers) and flow-preconditioned local steps (robust within a mode) seems promising, especially when properties of the target distributions are not known a priori.
- Why unresolved: The paper does not provide experimental results or theoretical analysis of this alternating strategy.
- What evidence would resolve it: Experimental results comparing different scheduling strategies for alternating between global and local steps, and analysis of their performance on various target distributions.

## Limitations

- Limited exploration of extremely poor flow quality scenarios and their impact on method failure
- Theoretical mixing time bound assumes strongly log-concave targets and Gaussian proposals, which may not generalize to all distributions
- Experiments do not consider the computational cost of flow training, which could be significant in practice

## Confidence

- **High Confidence**: The theoretical mixing time bound for IMH under local Lipschitz conditions, as it is derived rigorously and supported by mathematical analysis
- **Medium Confidence**: The comparative performance of the three sampling strategies on synthetic and real-world datasets, given that the experiments are well-designed and results are consistent across different settings
- **Low Confidence**: The generalizability of findings to extremely high-dimensional settings (>100 dimensions) or highly complex target distributions, as experiments are limited to moderately high dimensions and relatively simple multimodal targets

## Next Checks

1. Extend experiments to higher-dimensional settings (e.g., >100 dimensions) to assess scalability and validate the mixing time bound in very high dimensions
2. Investigate the impact of extremely poor flow quality on the performance of all three methods and identify failure thresholds
3. Test methods on more complex multimodal targets with many modes or highly correlated dimensions to evaluate robustness to challenging target geometries