---
ver: rpa2
title: 'Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer
  Transformer'
arxiv_id: '2305.16380'
source_url: https://arxiv.org/abs/2305.16380
tags:
- token
- then
- have
- tokens
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the training dynamics of a 1-layer Transformer
  model for next-token prediction, focusing on how the self-attention layer combines
  input tokens during training. The authors prove that self-attention acts as a discriminative
  scanning algorithm: it starts with uniform attention and gradually attends more
  to distinct tokens (those unique to a specific next token) while paying less attention
  to common tokens (those shared across multiple next tokens).'
---

# Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer

## Quick Facts
- arXiv ID: 2305.16380
- Source URL: https://arxiv.org/abs/2305.16380
- Reference count: 40
- This paper analyzes how 1-layer Transformer self-attention learns to combine input tokens during training, proving it acts as a discriminative scanning algorithm that progressively attends to distinct tokens while ignoring common ones.

## Executive Summary
This paper provides a rigorous theoretical analysis of training dynamics in a 1-layer Transformer model for next-token prediction. The authors prove that self-attention acts as a discriminative scanning algorithm: starting from uniform attention, it gradually attends more to distinct tokens (those unique to specific next tokens) while paying less attention to common tokens (those shared across multiple next tokens). The attention weights on distinct tokens follow an order based on their co-occurrence frequency with the query token, with lower-frequency tokens gaining attention faster. Interestingly, this process doesn't lead to winner-takes-all behavior due to a phase transition controlled by the learning rates of the decoder and self-attention layers, resulting in an (almost) fixed token combination.

## Method Summary
The paper analyzes a 1-layer Transformer with self-attention followed by a decoder layer, trained with cross-entropy loss on next-token prediction. The theoretical framework derives differential equations for attention weight dynamics, assuming sequences become infinitely long (T → +∞) and using SGD optimization with batch size 128. Experiments are conducted on synthetic datasets (Syn-Small, Syn-Medium) with controlled vocabulary sizes and sequence classes, as well as real-world data (WikiText2 and WikiText103). The analysis focuses on how attention weights evolve during training and how they combine input tokens to predict the next token.

## Key Results
- Self-attention acts as a discriminative scanning algorithm, progressively attending more to distinct tokens while ignoring common tokens
- Among distinct tokens, lower-frequency tokens gain attention faster than higher-frequency ones based on their initial co-occurrence conditions
- The scanning process doesn't lead to winner-takes-all due to a phase transition controlled by learning rate ratios, resulting in an (almost) fixed token combination

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-attention acts as a discriminative scanning algorithm that progressively attends more to distinct tokens while paying less attention to common tokens
- Mechanism: The self-attention layer starts with uniform attention and gradually shifts focus based on co-occurrence patterns between query and key tokens. Distinct tokens gain attention faster due to their unique association with specific next tokens, while common tokens lose attention as they appear across multiple contexts
- Core assumption: The decoder layer learns much faster than self-attention (ηY ≫ ηZ), and the input sequence is long (T → +∞)
- Evidence anchors:
  - [abstract] "we prove that self-attention acts as a discriminative scanning algorithm: starting from uniform attention, it gradually attends more to distinct key tokens for a specific next token to be predicted, and pays less attention to common key tokens that occur across different next tokens"
  - [section] "Theorem 2 (Fates of contextual tokens). Let GCT be the set of common tokens (CT), and GDT (n) be the set of distinct tokens (DT) that belong to next token n. Then if Assumption 2 holds, under the self-attention dynamics (Eqn. 10), we have: (a) for any distinct token l ∈ GDT (n), ˙zl > 0; (b) if |GCT | = 1, then for the single common token l ∈ GCT , ˙zl < 0"
  - [corpus] Weak evidence. The corpus contains papers on self-attention dynamics and token pruning, but none directly confirm the discriminative scanning mechanism described in this paper
- Break condition: If the decoder layer does not learn faster than self-attention (ηY ≪ ηZ), the scanning algorithm would not properly emerge. If sequences are too short, the frequency bias would not manifest clearly

### Mechanism 2
- Claim: The growth rate of distinct tokens critically depends on their initial conditions, with lower-frequency tokens gaining attention faster than higher-frequency ones
- Mechanism: During training, distinct tokens are ranked by their initial co-occurrence probability with the query token. Tokens with lower initial probability (less frequent co-occurrence) grow their attention weights faster than those with higher initial probability, following an exponential growth pattern based on their logits
- Core assumption: The initial logits zl(0) are determined by inner products of independently initialized high-dimensional vectors, which fluctuate around zero
- Evidence anchors:
  - [abstract] "Among distinct tokens, it progressively drops attention weights, following the order of low to high co-occurrence between the key and the query token in the training set"
  - [section] "Theorem 3 (Growth of distinct tokens). For a next token n and its two distinct tokens l and l′, the dynamics of the relative gain rl/l′|n(t) := f2 nl(t)/f2 nl′(t)−1 = ˜c2 l|n(t)/˜c2 l′|n(t)−1 has the following analytic form: rl/l′|n(t) = rl/l′|n(0)e2(zl(t)−zl(0)) =: rl/l′|n(0)χl(t)"
  - [corpus] Weak evidence. The corpus includes papers on attention dynamics and token pruning, but none specifically address the initial-condition-dependent growth rates of distinct tokens
- Break condition: If tokens are not independently initialized or if the initialization mechanism changes significantly, the growth pattern would be disrupted. If the sequence length is not sufficiently long, the co-occurrence statistics would not stabilize

### Mechanism 3
- Claim: A phase transition occurs during training that prevents winner-takes-all dynamics, resulting in an (almost) fixed token combination that changes slowly over time
- Mechanism: As training progresses, the growth factor for attention weights transitions from sub-linear to logarithmic growth. This phase transition is controlled by the learning rates of the decoder and self-attention layers, causing the attention pattern to "snap" into a semi-stable configuration rather than collapsing to a single dominant token
- Core assumption: The learning rate ratio ηY/ηZ controls when the phase transition occurs, with larger ηY leading to faster transition and denser attention patterns
- Evidence anchors:
  - [abstract] "Interestingly, this procedure does not lead to winner-takes-all, but decelerates due to a phase transition that is controllable by the learning rates of the two layers, leaving (almost) fixed token combination"
  - [section] "Theorem 4 (Phase Transition in Training). If the dynamics of the single common token zl satisfies ˙zl = −Kρ −4ηZγ(t)e4zl and ξn(t) = Kρ −4γ(t)e4zl, then we have: Bn(t) = { 1 4 ln(ρ4 0/K + 2(M −1)2 KM 2 ηY ηZt2) t < t ′ 0 := K ln M ηY 1 4 ln(ρ4 0/K + 2K(M −1)2 M 2 ηZ ηY ln2(M ηY t/K)) t ≥ t0 := 2(1+o(1))K ln M ηY"
  - [corpus] Weak evidence. The corpus contains papers on attention dynamics, but none specifically discuss the phase transition mechanism that prevents winner-takes-all behavior in self-attention
- Break condition: If the learning rate ratio ηY/ηZ is not properly tuned, the phase transition may not occur at the right time. If the common token dynamics change significantly, the phase transition mechanism would be disrupted

## Foundational Learning

- Concept: Cross-entropy loss and its gradient dynamics in multi-class classification settings
  - Why needed here: The paper analyzes how the cross-entropy loss drives the training dynamics of the self-attention mechanism, specifically how gradients update attention weights based on prediction errors
  - Quick check question: What is the gradient of cross-entropy loss with respect to the logits in a multi-class classification problem?

- Concept: Matrix calculus and differential equations for neural network parameter updates
  - Why needed here: The analysis requires deriving and solving differential equations for how the attention weights and decoder parameters change over time during training
  - Quick check question: How do you compute the differential of a matrix expression like d(exp(a)/1⊤exp(a))?

- Concept: Concentration inequalities and statistical convergence in high-dimensional settings
  - Why needed here: The proofs rely on showing that empirical frequencies converge to true probabilities as sequence length increases, requiring tools from probability theory
  - Quick check question: What concentration inequality would you use to show that the frequency of a token in a long sequence converges to its true probability?

## Architecture Onboarding

- Component map: Self-attention layer → Normalization → Decoder layer. The self-attention computes weighted sum of token embeddings based on pairwise similarities. The decoder predicts next token probabilities from the attended representation
- Critical path: Input sequence → Self-attention weights computation → Attended representation → Decoder output → Cross-entropy loss → Gradient computation → Parameter updates for both self-attention and decoder
- Design tradeoffs: Position-encoding-free design simplifies analysis but limits applicability to tasks requiring positional information. Single layer limits expressiveness but enables rigorous theoretical analysis. Fixed learning rate ratio assumption may not hold in practice
- Failure signatures: If decoder learns too slowly relative to self-attention, attention weights won't properly discriminate between tokens. If sequences are too short, frequency bias won't manifest. If initialization is not random, initial-condition-dependent dynamics will be disrupted
- First 3 experiments:
  1. Verify attention patterns become sparser during training by visualizing attention maps at different epochs
  2. Test how changing ηY/ηZ ratio affects sparsity and convergence speed of attention patterns
  3. Compare attention dynamics on synthetic data with controlled token distributions versus real-world data like WikiText

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the introduction of positional encoding affect the "scan and snap" dynamics and the resulting attention patterns in a 1-layer Transformer?
- Basis in paper: [inferred] The paper explicitly states that their analysis does not include positional encoding, but mentions it could be added following relative encoding schemes. It notes this could create a prior for contextual tokens in the self-attention layer
- Why unresolved: The theoretical analysis focuses on a position-encoding-free setting. The authors suggest this extension is straightforward but leave it for future work without providing concrete analysis
- What evidence would resolve it: Experiments comparing attention dynamics and final attention patterns in 1-layer Transformers with and without positional encoding on both synthetic and real-world data. Theoretical analysis of how positional encoding modifies the dynamics equations and impacts the phase transition

### Open Question 2
- Question: What is the exact impact of different optimization algorithms (e.g., Adam vs SGD) on the "scan and snap" dynamics and the resulting attention sparsity in Transformers?
- Basis in paper: [explicit] The paper mentions that using Adam instead of SGD leads to self-attention focusing on different subsets of distinct tokens, demonstrating tunable inductive bias. However, they leave analysis on Adam for future work
- Why unresolved: The paper provides empirical observations with Adam but does not offer theoretical analysis of why different optimizers lead to different attention patterns. The mechanism behind this phenomenon remains unclear
- What evidence would resolve it: Theoretical analysis of how different optimization algorithms modify the gradient dynamics and affect the growth rates of distinct and common tokens. Experiments systematically varying optimizer hyperparameters and measuring resulting attention sparsity patterns

### Open Question 3
- Question: How does the "scan and snap" dynamic generalize to multi-layer Transformers, and what role does each layer play in the overall attention mechanism?
- Basis in paper: [explicit] The paper mentions that in multi-layer Transformers, the attention of the first layer freezes and becomes sparse during training, even with constant learning rates. They provide empirical evidence but don't offer theoretical analysis
- Why unresolved: The theoretical framework developed for 1-layer Transformers doesn't directly extend to multi-layer architectures. The interaction between layers and how information flows through them to create final attention patterns is not understood
- What evidence would resolve it: Theoretical framework extending the "scan and snap" analysis to multi-layer Transformers, explaining how each layer contributes to the final attention mechanism. Experiments analyzing attention patterns at each layer during training to validate the theoretical predictions

## Limitations
- Model simplification: Analysis assumes position-encoding-free 1-layer Transformer, which may not capture full complexity of practical Transformers
- Data assumptions: Relies on sequences becoming infinitely long (T → +∞) and idealized token distributions that may not hold in finite-length practical applications
- Initialization sensitivity: Initial-condition-dependent growth rates may be sensitive to different initialization schemes commonly used in practice

## Confidence

**High Confidence**: The proof methodology for the discriminative scanning algorithm (Mechanism 1) and the basic framework for analyzing attention dynamics are sound. The mathematical derivations for attention weight evolution and the identification of distinct vs common tokens are well-established.

**Medium Confidence**: The exponential growth pattern of distinct tokens based on initial conditions (Mechanism 2) and the phase transition preventing winner-takes-all (Mechanism 3) are theoretically derived but rely on several assumptions that may not hold in practice. The WikiText experiments provide some empirical support but are limited in scope.

**Low Confidence**: The exact quantitative predictions for attention weight trajectories and the precise timing of the phase transition in practical settings remain uncertain due to the simplified assumptions.

## Next Checks

1. **Multi-layer Extension**: Test whether the scanning and snapping dynamics persist in 2-layer or 3-layer Transformers. Specifically, verify if the phase transition mechanism remains effective when the attended representation passes through additional self-attention layers before the decoder.

2. **Positional Encoding Impact**: Evaluate how adding sinusoidal or learned positional encodings affects the attention dynamics. Measure whether the discriminative scanning behavior changes and if the phase transition still occurs at similar learning rate ratios.

3. **Real-world Sequence Lengths**: Conduct experiments with varying sequence lengths (32, 128, 512, 1024 tokens) on WikiText to quantify how well the theoretical predictions hold as sequence length increases. Compare the empirical attention sparsity and growth rates against the theoretical expectations for different sequence lengths.