---
ver: rpa2
title: 'Average Token Delay: A Duration-aware Latency Metric for Simultaneous Translation'
arxiv_id: '2311.14353'
source_url: https://arxiv.org/abs/2311.14353
tags:
- latency
- translation
- output
- input
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the need for better latency evaluation metrics
  in simultaneous translation that consider the duration of translation outputs. The
  authors propose Average Token Delay (ATD), a novel latency metric that focuses on
  the duration of partial translations.
---

# Average Token Delay: A Duration-aware Latency Metric for Simultaneous Translation

## Quick Facts
- arXiv ID: 2311.14353
- Source URL: https://arxiv.org/abs/2311.14353
- Reference count: 6
- The study proposes Average Token Delay (ATD), a novel latency metric that better reflects user experience by considering the duration of translation outputs in simultaneous translation evaluation.

## Executive Summary
This paper addresses the limitations of existing latency metrics in simultaneous translation by proposing Average Token Delay (ATD), a duration-aware metric that measures the average time difference between the ending times of input and output tokens. Unlike existing metrics that focus on start times or local wait times, ATD penalizes longer translation outputs that delay comprehension and subsequent translations. Through simulations and experiments comparing ATD with metrics like Average Lagging (AL) and Differentiable Average Lagging (DAL), the authors demonstrate that ATD has the highest correlation with Ear-Voice Span (EVS), a human-interpretable latency measure, under most conditions. The results show that ATD effectively captures the impact of output length on latency, making it a more intuitive and user-focused metric for evaluating simultaneous translation systems.

## Method Summary
The paper introduces Average Token Delay (ATD) as a novel latency metric for simultaneous translation. ATD measures the average delay of output tokens against their corresponding input tokens, considering both input and output durations. The metric is calculated by dividing input and output into tokens or sub-segments, obtaining their ending timestamps, computing the time difference between corresponding tokens, and averaging these differences. The authors compare ATD with existing metrics (AL, DAL, Start Offset, End Offset) on both text-to-text and speech-to-speech simultaneous translation tasks using datasets from IWSLT 2022, WMT 2014, IWSLT 2017, and MuST-C. Experiments use text-to-text SimulMT models (wait-k, MU, ICLP, PA) and cascaded speech-to-speech models with Whisper for ASR and WhisperX for word timestamps. The correlation between each metric and EVS is measured using Spearman's ρ.

## Key Results
- ATD achieves the highest correlation with EVS (ρ=0.922) compared to other metrics on speech-to-speech translation tasks.
- ATD outperforms AL and DAL in most conditions, particularly when translation outputs are significantly longer than inputs.
- ATD effectively captures the impact of output length on latency, making it more intuitive and user-focused than existing metrics.
- ATD generalizes across both speech and text outputs and works intuitively for chunk-based outputs that are not properly handled by AL.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ATD captures the impact of translation output length on user experience by penalizing longer outputs that delay comprehension and subsequent translations.
- Mechanism: ATD measures the average time difference between the ending times of input and output tokens, where the corresponding input token is adjusted based on how much longer the previous output was compared to the previous input. This adjustment ensures that longer outputs increase the measured delay.
- Core assumption: The delay experienced by users is directly related to the duration of the translation output, not just the start time of the output.
- Evidence anchors:
  - [abstract]: "ATD is designed to better reflect user experience by penalizing longer translation outputs that delay comprehension and subsequent translations."
  - [section]: "ATD is the average delay of the output tokens against their corresponding input tokens, considering the latency required for inputs and outputs."
- Break condition: If the correspondence between input and output tokens does not accurately reflect semantic equivalence, ATD might not correctly measure user-perceived delay.

### Mechanism 2
- Claim: ATD provides a more intuitive and user-focused metric for evaluating simultaneous translation systems compared to existing metrics like AL and DAL.
- Mechanism: ATD uses the ending times of partial translations, which aligns with how users perceive delay (waiting for the full output to finish before comprehension and subsequent translation can begin). Existing metrics like AL and DAL focus on start times or local wait times, which can underestimate the impact of long outputs.
- Core assumption: Users perceive delay based on the completion of translation outputs, not just their initiation.
- Evidence anchors:
  - [abstract]: "Through simulations and experiments comparing ATD with existing metrics like Average Lagging (AL) and Differentiable Average Lagging (DAL), the authors demonstrate that ATD has the highest correlation with Ear-Voice Span (EVS), a human-interpretable latency measure, under most conditions."
  - [section]: "In Figure 1, AL is smaller in case 2, which does not agree with its mean EVS. This difference comes from the definition of AL, which counterintuitively gives a smaller latency value to a long chunk output at a time step."
- Break condition: If the assumption about user perception of delay is incorrect, or if other factors besides output length significantly impact user experience, ATD might not be the best metric.

### Mechanism 3
- Claim: ATD generalizes latency measurements for both speech and text outputs and works intuitively for chunk-based outputs.
- Mechanism: ATD calculates the delay based on the ending times of sub-segments or tokens, which can be applied to both speech (sub-segments) and text (characters or words). This allows for a consistent measure of delay regardless of the output modality.
- Core assumption: The ending time of a translation output, regardless of modality, is a relevant and comparable measure of delay.
- Evidence anchors:
  - [abstract]: "ATD generalizes latency measurements for both speech and text outputs and works intuitively for chunk-based outputs that are not properly handled by AL."
  - [section]: "To calculate ATD, we divide each speech segment into sub-segments of length τ from the beginning of the segment, assuming one word is uttered in duration τ."
- Break condition: If the assumptions about the correspondence between sub-segments/characters and user-perceived delay do not hold for certain languages or modalities, ATD might not accurately measure latency.

## Foundational Learning

- Concept: Simultaneous Machine Translation (SimulMT)
  - Why needed here: Understanding SimulMT is crucial to grasp the problem ATD addresses and how it differs from traditional machine translation.
  - Quick check question: What is the key difference between SimulMT and traditional machine translation in terms of when the translation begins?

- Concept: Latency Metrics in Machine Translation
  - Why needed here: Familiarity with existing latency metrics (AL, DAL, EVS) is necessary to understand the limitations ATD aims to overcome and how it improves upon them.
  - Quick check question: How do Average Lagging (AL) and Differentiable Average Lagging (DAL) calculate latency, and what are their main limitations according to the paper?

- Concept: Ear-Voice Span (EVS)
  - Why needed here: EVS is the human-interpretable latency measure used to validate ATD's effectiveness, so understanding its calculation and interpretation is important.
  - Quick check question: How is EVS calculated, and why is it considered a persuasive latency metric for human interpretation?

## Architecture Onboarding

- Component map: Input sequence → Tokenization → ATD calculation module → Average Token Delay score
- Critical path: The critical path for ATD calculation involves: 1) Segmenting the input and output into tokens or sub-segments, 2) Obtaining the ending timestamps for each token/sub-segment, 3) Calculating the corresponding input token for each output token based on the length of previous outputs, 4) Computing the time difference between the ending times of corresponding input and output tokens, 5) Averaging these time differences to obtain the ATD score.
- Design tradeoffs: ATD focuses on output length, which may not always be the primary factor affecting user experience. It assumes that longer outputs always lead to more delay, which might not hold true in all cases (e.g., if the longer output provides more context and improves overall translation quality).
- Failure signatures: ATD might fail to accurately measure latency if: 1) The correspondence between input and output tokens does not reflect semantic equivalence, 2) Other factors besides output length significantly impact user experience, 3) The assumptions about user perception of delay are incorrect.
- First 3 experiments:
  1. Compare ATD with AL and DAL on a dataset with varying output lengths to demonstrate how ATD penalizes longer outputs.
  2. Validate ATD's correlation with EVS on a speech-to-speech SimulMT dataset to show its effectiveness in capturing user-perceived delay.
  3. Evaluate ATD's performance on text-to-text SimulMT datasets with different tokenization schemes (word-level vs. character-level) to assess its generalization across modalities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Average Token Delay (ATD) perform compared to other latency metrics in real-world simultaneous translation scenarios beyond simulations?
- Basis in paper: [explicit] The authors conducted experiments comparing ATD with existing metrics like Average Lagging (AL) and Differentiable Average Lagging (DAL) using simulated data and real speech-to-speech translation outputs.
- Why unresolved: The experiments were limited to specific datasets and scenarios, and real-world applications may involve additional complexities not captured in the study.
- What evidence would resolve it: Conducting ATD evaluations in diverse real-world simultaneous translation settings, such as live conferences or customer service interactions, and comparing the results with other latency metrics.

### Open Question 2
- Question: Can ATD be effectively applied to languages with different word orders or token structures, such as Chinese or Arabic?
- Basis in paper: [inferred] The paper mentions that ATD focuses on the duration of partial translations and considers the output length, which may be relevant for languages with varying token structures.
- Why unresolved: The paper does not provide specific experiments or analyses for languages with significantly different word orders or token structures.
- What evidence would resolve it: Testing ATD on simultaneous translation tasks involving languages with diverse linguistic features and comparing its performance with other latency metrics.

### Open Question 3
- Question: How does the choice of token size (e.g., word, character) affect the performance of ATD in languages like Japanese or Chinese?
- Basis in paper: [explicit] The authors conducted experiments using character-level output for Japanese and found that ATD's correlation with EVS varied depending on the token size.
- Why unresolved: The study only explored a limited range of token sizes and did not provide a comprehensive analysis of the impact of token size on ATD's performance across different languages.
- What evidence would resolve it: Conducting a systematic study of ATD's performance using various token sizes (e.g., word, character, subword) for multiple languages with different linguistic characteristics.

## Limitations
- ATD's effectiveness depends heavily on accurate token-to-token correspondence between input and output sequences, which may not always reflect semantic equivalence.
- The study focuses primarily on controlled simulations and may not fully capture the complexity of real-time human interpretation in real-world scenarios.
- ATD assumes that longer outputs always lead to more delay, which might not hold true in all cases (e.g., if the longer output provides more context and improves overall translation quality).

## Confidence
- **High Confidence**: The mathematical formulation of ATD and its comparison with AL and DAL on simulated data is well-established and reproducible. The correlation results with EVS are clearly presented and statistically sound.
- **Medium Confidence**: The claim that ATD is "more intuitive and user-focused" relies on the assumption that output length is the primary factor affecting user-perceived delay. This assumption, while reasonable, requires further validation with human studies.
- **Low Confidence**: The generalization of ATD across all simultaneous translation scenarios, including edge cases with disfluencies, restarts, or highly divergent language pairs, has not been thoroughly tested.

## Next Checks
1. **Validation of Token Correspondence Robustness**: Test ATD on datasets with known alignment errors or disfluencies to evaluate how sensitive the metric is to imperfect token-to-token correspondence. This would involve introducing controlled noise into the alignment between input and output tokens and measuring the impact on ATD scores.

2. **Human Perception Study**: Conduct a user study comparing simultaneous translation outputs with identical ATD scores but different output length distributions to verify whether ATD actually predicts human-perceived latency more accurately than existing metrics.

3. **Cross-Lingual Generalization Test**: Evaluate ATD's performance on language pairs with significantly different word orders or morphological complexity (e.g., English to Japanese) to assess whether the metric maintains its correlation with EVS when the one-to-one token correspondence assumption is weaker.