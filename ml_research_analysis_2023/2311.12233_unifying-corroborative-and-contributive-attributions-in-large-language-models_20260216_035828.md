---
ver: rpa2
title: Unifying Corroborative and Contributive Attributions in Large Language Models
arxiv_id: '2311.12233'
source_url: https://arxiv.org/abs/2311.12233
tags:
- attribution
- attributions
- language
- output
- corroborative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a unified framework for two types of LLM attributions:
  corroborative (verifying outputs via external sources) and contributive (identifying
  influential training data). It defines an interaction model with six components
  (input, model, output, attributable units, attribution domain, evaluator) and introduces
  formal definitions for attribution sets, relevance functions, and properties like
  correctness, recall, and efficiency.'
---

# Unifying Corroborative and Contributive Attributions in Large Language Models

## Quick Facts
- arXiv ID: 2311.12233
- Source URL: https://arxiv.org/abs/2311.12233
- Reference count: 40
- Key outcome: Proposes a unified framework for corroborative and contributive attributions in LLMs, validated via case studies in legal and healthcare domains.

## Executive Summary
This paper introduces a unified framework for two types of attributions in large language models: corroborative (verifying outputs via external sources) and contributive (identifying influential training data). By defining a shared interaction model with six components, the framework enables both types of attributions to be generated, evaluated, and compared under a common structure. The work highlights the complementary roles of these attributions in high-stakes applications and outlines a path toward hybrid attribution systems.

## Method Summary
The authors define an interaction model with six components: input, model, output, attributable units, attribution domain, and evaluator. Attribution sets are constructed using a cutoff-based evaluator and a relevance function. Corroborative evaluators verify if external sources support or contradict the output, while contributive evaluators measure the influence of training data. The framework supports both single and hybrid attribution systems, with properties like correctness, recall, and efficiency used to evaluate performance.

## Key Results
- The framework unifies corroborative and contributive attributions under a shared interaction model.
- Six properties (correctness, coverage, precision, recall, consistency, efficiency) are defined for evaluating attribution sets.
- Case studies in legal drafting and healthcare demonstrate the complementary value of both attribution types.
- Future work focuses on hybrid systems and standardized evaluation.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The framework unifies corroborative and contributive attributions by defining them as attribution sets that differ only in their evaluator and attribution domain.
- **Mechanism**: Both types share the same six-component interaction model (input, model, output, attributable units, attribution domain, evaluator). The evaluator determines the attribution type—corroborative evaluators check if a source supports or contradicts the output, while contributive evaluators measure how influential a training source was in generating the output.
- **Core assumption**: The evaluator cutoff α and relevance function ϕ can flexibly distinguish and prioritize sources for both types without changing the underlying framework structure.
- **Evidence anchors**:
  - [abstract] "We precisely define each type of attribution and discuss different properties that are desirable in different scenarios."
  - [section] "A contributive attribution set is an attribution set (Definition 1) that draws from an attribution domain D that is restricted to training sources and relies upon a contributive evaluator."
  - [corpus] Weak evidence—corpus does not directly discuss the unification of corroborative and contributive attributions; it focuses on feature attribution methods.
- **Break condition**: If the evaluator cannot be clearly classified as corroborative or contributive, or if the attribution domain mixes sources without clear distinction, the framework collapses into ambiguity.

### Mechanism 2
- **Claim**: The framework enables hybrid attribution systems by allowing both evaluator types to be applied to the same output, providing richer explanations.
- **Mechanism**: By defining a common interaction model, the framework supports running both corroborative and contributive evaluators on the same input-output pair. This allows simultaneous verification (external sources) and influence analysis (training data), addressing use cases like model debugging and human-AI collaboration.
- **Core assumption**: Both evaluator types can be computed efficiently enough to allow real-time or near-real-time hybrid attribution generation.
- **Evidence anchors**:
  - [abstract] "We believe that this unified framework will guide the use case driven development of systems that leverage both types of attribution."
  - [section] "Both notions of attribution are relevant to use cases that improve the safety and reliability of language models as information providers, both are often simultaneously relevant in application settings."
  - [corpus] Weak evidence—corpus neighbors discuss feature attribution unification but not specifically corroborative/contributive attribution hybrids.
- **Break condition**: If computing both evaluators is too slow or resource-intensive, hybrid systems become impractical; if the evaluators conflict, the unified interpretation breaks down.

### Mechanism 3
- **Claim**: The framework standardizes evaluation across attribution types by defining common properties and metrics (correctness, recall, efficiency, consistency, relevance).
- **Mechanism**: Properties like coverage, attribution precision, and r-relevancy are defined generically for any attribution set. Metrics can then be applied uniformly to both corroborative and contributive attributions, enabling fair comparison and benchmarking.
- **Core assumption**: The properties defined are truly applicable to both attribution types and not biased toward one.
- **Evidence anchors**:
  - [abstract] "We also outline properties relevant to both types of attributions."
  - [section] "Properties of attribution sets are inherent to a single attribution set. However, some properties are instead functions of the implemented system that generates the attribution sets in the first place."
  - [corpus] Weak evidence—corpus focuses on feature attribution robustness and synergy functions, not on a unified property framework for language model attributions.
- **Break condition**: If a property cannot be meaningfully measured for one type, or if the metrics are incompatible, standardization fails.

## Foundational Learning

- **Concept**: Interaction model components (input, model, output, attributable units, attribution domain, evaluator)
  - Why needed here: Provides a shared vocabulary and structure for defining both corroborative and contributive attributions without conflating them.
  - Quick check question: Can you list all six components and explain how the evaluator determines the attribution type?

- **Concept**: Evaluator types (corroborative vs. contributive) and their formal definitions
  - Why needed here: The evaluator is the core differentiator; understanding its formal properties is essential for implementing or extending the framework.
  - Quick check question: What is the key difference between vcorr and vMcont in terms of their counterfactual assumptions?

- **Concept**: Attribution set properties (correctness, coverage, precision, recall, r-relevancy, consistency, efficiency)
  - Why needed here: These properties guide both method development and evaluation, ensuring that attributions meet practical requirements.
  - Quick check question: Which property would be most critical for GDPR compliance use cases, and why?

## Architecture Onboarding

- **Component map**:
  - Input parser -> Model -> Output generator -> Attributable unit extractor -> Evaluator selector (corroborative/contributive) -> Attribution domain filter -> Attribution set builder -> Property evaluator -> Output formatter
  - Key data structures: Attributable units (spans of output), source sets (from domain), evaluator functions, cutoff thresholds, relevance functions.

- **Critical path**:
  1. Parse input and generate model output.
  2. Extract attributable units (sentences or clauses).
  3. Select appropriate evaluator(s) based on use case.
  4. Filter attribution domain (training data vs external corpus).
  5. Apply evaluator with cutoff to build attribution set.
  6. Optionally apply relevance function and threshold.
  7. Compute property metrics for evaluation.

- **Design tradeoffs**:
  - Granularity of attributable units: sentence-level vs clause-level explicatures—higher granularity increases detail but computational cost.
  - Evaluator choice: human-based (accurate, slow) vs automated NLI (fast, potentially less accurate).
  - Attribution domain size: larger domains improve coverage but increase computation and noise.
  - Single vs hybrid evaluator: hybrid provides richer explanations but doubles computational load.

- **Failure signatures**:
  - Low coverage: evaluator cutoff too strict or domain too narrow.
  - Low precision: evaluator too permissive or domain contains irrelevant sources.
  - Inconsistent outputs: evaluator non-deterministic or source ordering unstable.
  - High latency: inefficient evaluator or unoptimized attribution domain filtering.

- **First 3 experiments**:
  1. Implement a minimal corroborative evaluator using exact match on a small Wikipedia domain; verify coverage and precision on a QA dataset.
  2. Implement a minimal contributive evaluator using leave-one-out loss change on a toy language model; test on synthetic training/test splits.
  3. Combine both evaluators on a single input-output pair; measure hybrid attribution set size and property metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can contributive attribution methods be effectively adapted to handle the large-scale training data used in modern LLMs?
- Basis in paper: [explicit] The paper discusses concerns about the high resource burdens of TDA methods and the rarity of training sources with high influences.
- Why unresolved: Current contributive attribution methods struggle with the computational complexity and potential lack of noticeable influence from individual training sources in large-scale models.
- What evidence would resolve it: Development and evaluation of contributive attribution methods that can efficiently handle large-scale training data and accurately identify influential sources, even when individual influences are subtle.

### Open Question 2
- Question: What are the key design principles for creating hybrid attribution systems that can provide both corroborative and contributive attributions for LLM outputs?
- Basis in paper: [explicit] The paper highlights the need for hybrid attribution systems to address the limitations of using either type of attribution alone.
- Why unresolved: Existing attribution methods are largely developed in isolation, and there is no established framework for integrating both types of attributions.
- What evidence would resolve it: A framework or set of design principles for developing hybrid attribution systems, along with empirical evaluations demonstrating their effectiveness in real-world applications.

### Open Question 3
- Question: How can standardized evaluation metrics and datasets be established for assessing the performance of attribution systems across different types of attributions and use cases?
- Basis in paper: [explicit] The paper observes a lack of standardization in evaluation metrics and datasets for corroborative attribution methods.
- Why unresolved: The diverse nature of attribution tasks and use cases makes it challenging to develop universally applicable evaluation metrics and datasets.
- What evidence would resolve it: A set of standardized evaluation metrics and datasets that can be used to compare the performance of different attribution systems across various tasks and use cases, along with empirical results demonstrating their effectiveness.

## Limitations
- The framework's unification claim rests on abstract definitions rather than empirical validation across both attribution types.
- Minimal quantitative results, especially for the contributive evaluator, and no demonstration of hybrid system performance.
- Reliance on human evaluators for correctness introduces scalability concerns.
- Generality of property definitions across diverse attribution domains remains untested.

## Confidence
- Mechanism 1 (unification via evaluator and domain): Medium
- Mechanism 2 (hybrid systems): Low (no empirical validation)
- Mechanism 3 (standardized evaluation): Medium (properties are formally defined but not benchmarked)

## Next Checks
1. Implement both evaluators on the same dataset and measure overlap, coverage, and efficiency trade-offs.
2. Test the framework's properties (correctness, recall, precision) on a real-world high-stakes task (e.g., legal or medical QA) with both attribution types.
3. Benchmark hybrid attribution generation latency and consistency against single-type baselines.