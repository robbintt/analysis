---
ver: rpa2
title: 'FedWon: Triumphing Multi-domain Federated Learning Without Normalization'
arxiv_id: '2306.05879'
source_url: https://arxiv.org/abs/2306.05879
tags:
- fedwon
- fedavg
- learning
- batch
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of multi-domain federated learning,
  where data on different clients come from different domains, resulting in domain
  shifts that negatively impact model performance. To address this, the authors propose
  FedWon, a method that removes all normalization layers and reparameterizes convolution
  layers using scaled weight standardization.
---

# FedWon: Triumphing Multi-domain Federated Learning Without Normalization

## Quick Facts
- arXiv ID: 2306.05879
- Source URL: https://arxiv.org/abs/2306.05879
- Reference count: 40
- Key outcome: FedWon achieves over 10% improvement in accuracy on certain domains in multi-domain federated learning by removing normalization layers and using scaled weight standardization.

## Executive Summary
FedWon addresses the challenge of domain shifts in multi-domain federated learning by removing all normalization layers and reparameterizing convolution layers using scaled weight standardization. This approach prevents domain-specific statistic mismatches that occur when using batch normalization in heterogeneous data environments. The method demonstrates significant performance improvements across multiple datasets and models, particularly in cross-device federated learning scenarios where resource constraints limit batch sizes.

## Method Summary
FedWon removes all normalization layers from the neural network architecture and replaces convolution layers with scaled weight standardized convolutions. This reparameterization standardizes weights by subtracting their mean and dividing by their standard deviation, scaled by a gain parameter. The method is trained using standard federated learning protocols with SGD optimizer, comparing against FedAvg, FedBN, and other normalization-based approaches across various datasets including Digits-Five, Office-Caltech-10, DomainNet, and CIFAR-10 using models like 6-layer CNN, AlexNet, ResNet-18, and MobileNetV2.

## Key Results
- FedWon achieves over 10% improvement in accuracy on certain domains compared to state-of-the-art methods
- The method performs well with small batch sizes (as low as B=1), making it suitable for resource-constrained devices
- FedWon demonstrates robust domain generalization capability across both cross-silo and cross-device federated learning scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing batch normalization (BN) improves performance in multi-domain federated learning by avoiding domain-specific statistic mismatches.
- Mechanism: BN layers rely on the assumption that training data come from a single distribution. In multi-domain FL, each client's data originates from a different domain, leading to domain-specific mean and variance statistics in BN layers. By removing BN layers, FedWon eliminates the dependency on these mismatched statistics, preventing domain discrepancies from degrading the global model.
- Core assumption: The performance degradation in multi-domain FL is primarily due to BN layers failing to capture multi-domain data statistics.
- Evidence anchors:
  - [abstract] "FedWon draws inspiration from the observation that batch normalization (BN) faces challenges in effectively modeling the statistics of multiple domains"
  - [section 2.1] "However, its effectiveness is based on the assumption that the training data is from the same domain, such that the mean µ and variance σ computed from a mini-batch are representative of the training data"

### Mechanism 2
- Claim: Scaled weight standardization stabilizes training and maintains signal propagation in the absence of normalization layers.
- Mechanism: After removing BN layers, convolution layers are reparameterized using scaled weight standardization. This technique standardizes the weights by subtracting their mean and dividing by their standard deviation, scaled by a gain parameter. This standardization prevents mean shift in hidden activations, stabilizing training and maintaining signal propagation through the network, similar to the effect of BN but without domain-specific statistics.
- Core assumption: Scaled weight standardization can effectively replace the stabilizing effect of BN on training dynamics without introducing domain-specific biases.
- Evidence anchors:
  - [section 3.2] "We employ the Scaled Weight Standardization technique proposed by [5] to reparameterize the convolution layers after removing BN"
  - [section 4.1] "This weight standardization technique is closely linked to Centered Weight Normalization"

### Mechanism 3
- Claim: FedWon's approach is effective for both cross-silo and cross-device federated learning scenarios.
- Mechanism: By removing BN layers and using scaled weight standardization, FedWon eliminates the need for clients to maintain stateful information (like BN statistics) across training rounds. This makes the method stateless and suitable for cross-device FL, where clients are typically stateless and only a fraction participate in each round. Additionally, the method's simplicity and lack of extra computation during inference make it versatile for various FL scenarios.
- Core assumption: The effectiveness of FedWon in cross-device FL is due to its stateless nature, which aligns with the constraints of cross-device FL scenarios.
- Evidence anchors:
  - [abstract] "FedWon is versatile for both cross-silo and cross-device FL, exhibiting robust domain generalization capability"
  - [section 1] "FedWon is versatile for both cross-silo and cross-device FL, exhibiting robust domain generalization capability, showcasing strong performance even with a batch size as small as 1, thereby catering to resource-constrained devices"

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: Understanding the basics of FL is crucial because FedWon is a method designed to improve FL in multi-domain scenarios. FL involves training models collaboratively on decentralized data while preserving privacy.
  - Quick check question: What is the primary goal of federated learning, and how does it differ from traditional centralized training?

- Concept: Batch Normalization (BN)
  - Why needed here: BN is a key component that FedWon removes. Understanding how BN works and its limitations in multi-domain settings is essential to grasp why FedWon's approach is effective.
  - Quick check question: How does batch normalization work, and what assumption does it make about the data distribution?

- Concept: Domain Generalization
  - Why needed here: FedWon aims to learn a general global model that can generalize well across different domains. Understanding domain generalization helps in appreciating the method's effectiveness in multi-domain FL.
  - Quick check question: What is domain generalization, and why is it important in the context of multi-domain federated learning?

## Architecture Onboarding

- Component map: Input layer -> Convolution layers (scaled weight standardized) -> Activation functions (ReLU) -> Pooling layers (MaxPool2D) -> Dropout layers -> Fully connected layers -> Output layer

- Critical path:
  1. Data preprocessing and transformation
  2. Client-side training with scaled weight standardization
  3. Server-side aggregation of model parameters
  4. Model evaluation on test data

- Design tradeoffs:
  - Removing BN layers simplifies the model but requires careful handling of training dynamics
  - Using scaled weight standardization instead of BN may introduce new hyperparameters to tune
  - The method's effectiveness in cross-device FL comes at the cost of potentially losing some of the benefits of BN in centralized training

- Failure signatures:
  - Training instability or divergence
  - Poor generalization across domains
  - Performance degradation in cross-device FL scenarios

- First 3 experiments:
  1. Reproduce the baseline experiments on Digits-Five dataset to verify FedWon's performance improvement over FedAvg and FedBN
  2. Test FedWon's effectiveness on small batch sizes (e.g., B=1, B=2) to validate its suitability for resource-constrained devices
  3. Evaluate FedWon's performance in cross-device FL by randomly selecting a fraction of clients to participate in each training round

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of the clipping threshold λ on FedWon's performance when using small batch sizes?
- Basis in paper: [explicit] The paper evaluates the impact of the clipping threshold λ for AGC on Office-Caltech-10 dataset using different batch sizes B.
- Why unresolved: The paper shows that the best clipping threshold for small batch sizes (B=2) is λ=1.28, but it does not explore the performance impact of different clipping threshold values within the range of 0.01 to 0.08.
- What evidence would resolve it: Experiments comparing the performance of FedWon with different clipping threshold values (e.g., 0.01, 0.02, 0.04, 0.08) on small batch sizes would provide insights into the optimal clipping threshold for this scenario.

### Open Question 2
- Question: How does FedWon's performance compare to FedBN when using different model architectures (e.g., ResNet-18, MobileNetV2)?
- Basis in paper: [explicit] The paper evaluates FedWon's performance using different model architectures, including AlexNet, ResNet-18, and MobileNetV2, on various datasets.
- Why unresolved: The paper provides a comparison of FedWon's performance with FedBN on AlexNet for the Office-Caltech-10 dataset, but it does not explore the performance comparison with other model architectures like ResNet-18 and MobileNetV2.
- What evidence would resolve it: Experiments comparing the performance of FedWon and FedBN using different model architectures (e.g., ResNet-18, MobileNetV2) on various datasets would provide insights into the effectiveness of FedWon across different architectures.

### Open Question 3
- Question: How does FedWon's performance change with different local epochs (E) in cross-device federated learning?
- Basis in paper: [explicit] The paper mentions that FedWon maintains performance and consistently outperforms FedBN under different numbers of local epochs (E) on the Office-Caltech-10 dataset.
- Why unresolved: The paper only provides a comparison of FedWon's performance with FedBN under different local epochs (E) on the Office-Caltech-10 dataset, but it does not explore the performance changes in cross-device federated learning scenarios.
- What evidence would resolve it: Experiments comparing the performance of FedWon with different local epochs (E) in cross-device federated learning scenarios would provide insights into the impact of local epochs on FedWon's performance.

## Limitations
- The paper relies heavily on ablation studies showing performance gains rather than isolating the specific contribution of each mechanism
- It's unclear whether performance gains stem from removing BN or from the scaled weight standardization technique
- The claim that removing BN is the primary mechanism for handling domain shifts may conflate correlation with causation

## Confidence
- High confidence: The empirical observation that FedWon outperforms existing methods on multiple datasets and models
- Medium confidence: The claim that scaled weight standardization provides stable training dynamics without BN
- Low confidence: The assertion that removing BN is the primary mechanism for handling domain shifts

## Next Checks
1. Ablation study with BN layers retained: Run FedWon with BN layers frozen during training to isolate whether removing BN or the weight standardization is the key factor.
2. Domain-specific BN statistics: Implement a variant where each client maintains its own BN statistics during local training but these are not aggregated, to test if this approach can match FedWon's performance.
3. Cross-dataset generalization test: Evaluate whether models trained with FedWon on one multi-domain dataset generalize better to unseen domains compared to models trained with FedAvg or FedBN.