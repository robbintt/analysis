---
ver: rpa2
title: 'CUDC: A Curiosity-Driven Unsupervised Data Collection Method with Adaptive
  Temporal Distances for Offline Reinforcement Learning'
arxiv_id: '2312.12191'
source_url: https://arxiv.org/abs/2312.12191
tags:
- cudc
- learning
- offline
- walker
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Curiosity-driven Unsupervised Data Collection
  (CUDC) method to improve the quality of task-agnostic datasets for multi-task offline
  reinforcement learning. CUDC employs a reachability module that estimates the probability
  of a k-step future state being reachable from the current state, allowing the agent
  to adaptively determine how many steps into the future the dynamics model should
  predict.
---

# CUDC: A Curiosity-Driven Unsupervised Data Collection Method with Adaptive Temporal Distances for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2312.12191
- Source URL: https://arxiv.org/abs/2312.12191
- Reference count: 40
- Primary result: Achieves 51% improvement in Quadruped domain and 10% improvement in Jaco Arm domain compared to baseline unsupervised methods

## Executive Summary
CUDC introduces a curiosity-driven unsupervised data collection method that enhances offline reinforcement learning through adaptive temporal distance prediction. The approach uses a reachability module to estimate k-step future state reachability, allowing the agent to dynamically adjust prediction horizons based on exploration needs. By combining state-action entropy maximization with prediction error-based curiosity, CUDC collects higher-quality task-agnostic datasets that enable superior downstream RL performance across multiple DeepMind control environments.

## Method Summary
CUDC employs a reachability module with state and action encoders, forward dynamics network, and contrastive reachability network to estimate k-step future state reachability probabilities. The method adaptively adjusts the temporal distance k based on curiosity weights derived from reachability scores. During exploration, CUDC uses a mixed intrinsic reward combining K-nearest neighbor state-action entropy and prediction error, with curiosity weights regularizing DDPG critic and actor updates. The approach collects 1M transition tuples per domain for downstream RL tasks using TD3/CQL.

## Key Results
- Achieves 51% improvement in Quadruped domain and 10% improvement in Jaco Arm domain compared to best baseline methods
- Shows 8-10% higher performance than baseline methods across all tested environments
- Demonstrates superior computational efficiency and learning performance across 12 downstream RL tasks from DeepMind control suite

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive temporal distance improves feature representation by expanding the feature space
- Mechanism: Reachability module estimates k-step future state reachability, adapting k based on curiosity weight to learn distant future state information
- Core assumption: Contrastive loss can quantify reachability and guide temporal distance adaptation
- Evidence anchors: Abstract and section 3.3 describe how CUDC incorporates k as a parameter for enhanced feature representation
- Break condition: If contrastive loss fails to converge or reachability estimation becomes unreliable with large k values

### Mechanism 2
- Claim: Mixed intrinsic reward encourages exploration of both diverse state-action space and under-learned states
- Mechanism: Combines state-action entropy maximization with prediction error to promote uniform exploration beyond just state space
- Core assumption: Maximizing state-action entropy in encoded space promotes meaningful exploration
- Evidence anchors: Section 3.4 describes mixed reward combining entropy and prediction error; lemma 3.1 explains KNN-based entropy estimation
- Break condition: If entropy estimation becomes degenerate or prediction errors are dominated by noise

### Mechanism 3
- Claim: Regularizing critic-actor updates with curiosity weights improves sample efficiency by prioritizing under-learned tuples
- Mechanism: Uses curiosity weight wi = 1 - li to scale Q-learning target and policy gradient, focusing on high-prediction-error tuples
- Core assumption: Under-learned transitions contain more informative content for policy improvement
- Evidence anchors: Section 3.4 explains how curiosity weights adaptively regularize DDPG updates
- Break condition: If curiosity weights become uniformly low across all transitions due to poor reachability estimation

## Foundational Learning

- Concept: Contrastive learning for self-supervised representation learning
  - Why needed here: Estimates reachability without manual labeling or expensive density estimation
  - Quick check question: How does contrastive loss ensure predicted future state feature matches true future state while distinguishing from other future states?

- Concept: Entropy maximization for exploration
  - Why needed here: Extends entropy maximization from state space to state-action space for more meaningful exploration
  - Quick check question: What is relationship between K-NN distance in state-action space and estimated entropy used for exploration?

- Concept: Temporal abstraction in reinforcement learning
  - Why needed here: Adapting temporal distance k allows learning representations at multiple time scales
  - Quick check question: Why might fixing k=3 limit diversity of learned feature representations compared to adaptively increasing k?

## Architecture Onboarding

- Component map: Environment -> Encoders -> Forward Dynamics -> Reachability Contrastive Loss -> Curiosity Weight -> Mixed Intrinsic Reward -> DDPG Update -> Next Environment Step
- Critical path: Environment → Encoders → Forward Dynamics → Reachability Contrastive Loss → Curiosity Weight → Mixed Intrinsic Reward → DDPG Update → Next Environment Step
- Design tradeoffs:
  - Fixed vs. adaptive k: Fixed k simpler but limits feature diversity; adaptive k more complex but improves representation learning
  - Pure state entropy vs. state-action entropy: State-action entropy requires additional action encoding but encourages more meaningful exploration
  - Regularization strength: Too much regularization can slow learning; too little may not effectively prioritize under-learned samples
- Failure signatures:
  - Reachability module fails to converge: Contrastive loss plateaus at high values, curiosity weights become uninformative
  - Exploration becomes degenerate: Agent gets stuck in repetitive behaviors, entropy estimates collapse
  - Learning becomes unstable: High variance in performance scores, curiosity weights fluctuate wildly
- First 3 experiments:
  1. Validate reachability module: Fix k=3 and check if contrastive loss decreases and curiosity weights correlate with prediction errors
  2. Test adaptive k mechanism: Implement k adaptation and verify it increases k when average curiosity is low
  3. Evaluate mixed reward: Compare pure entropy vs. pure prediction error vs. mixed reward on simple exploration task

## Open Questions the Paper Calls Out

- Question: How does adaptive k-step mechanism affect scalability to more complex environments with larger state spaces?
  - Basis in paper: Paper discusses scalability limitations and mentions safety-critical applications where expert data is crucial
  - Why unresolved: Experiments conducted on limited environments without exploration of larger or more complex state spaces
  - What evidence would resolve it: Empirical results demonstrating performance in environments with larger state spaces and more complex dynamics

- Question: What is optimal range for k-step adaptation and how does this range impact learning efficiency across different environments?
  - Basis in paper: Paper mentions k-step varied from 3 to 6 and provides ablation study on upper bound but doesn't explore optimal range for different environments
  - Why unresolved: Ablation study only varies upper bound without exploring lower bound or full range of k-step values
  - What evidence would resolve it: Empirical results showing performance with different ranges of k-step values across various environments

- Question: How does performance compare to supervised methods when expert data is available, and when would unsupervised data collection be preferred?
  - Basis in paper: Paper states CUDC outperforms existing unsupervised methods but doesn't compare to supervised methods when expert data is available
  - Why unresolved: Paper focuses on case where expert data is not available or costly to obtain without comparison to supervised methods
  - What evidence would resolve it: Empirical results comparing CUDC to supervised methods in environments where expert data is accessible

## Limitations

- Adaptive k-step threshold parameters (Cw, Ck) lack comprehensive sensitivity analysis, with performance heavily dependent on these hyperparameters
- State-action entropy estimation using K-NN may become computationally expensive as state-action space grows, with scalability to high-dimensional tasks unverified
- Curiosity weight reliability depends on prediction errors correlating with learning progress, but paper doesn't address cases where errors are dominated by noise

## Confidence

- High Confidence: Core mechanism of using reachability estimation to adapt temporal distances is well-supported by presented evidence and technically sound
- Medium Confidence: Mixed intrinsic reward formulation is plausible but lacks ablation studies showing individual component contributions
- Low Confidence: Performance improvement claims lack statistical significance testing and confidence intervals

## Next Checks

1. Conduct threshold sensitivity analysis by systematically varying adaptive k-step thresholds (Cw, Ck) across plausible ranges to quantify performance changes and identify optimal values for each domain

2. Perform ablation study on mixed reward components by implementing and comparing versions with only state entropy, only prediction error, and various weightings to isolate driving performance gains

3. Extend evaluation beyond 500K steps to assess long-term stability and determine whether adaptive k mechanism continues providing benefits or introduces instability in later learning stages