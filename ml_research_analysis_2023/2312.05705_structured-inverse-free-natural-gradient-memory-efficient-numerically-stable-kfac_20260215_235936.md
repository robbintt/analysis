---
ver: rpa2
title: 'Structured Inverse-Free Natural Gradient: Memory-Efficient & Numerically-Stable
  KFAC'
arxiv_id: '2312.05705'
source_url: https://arxiv.org/abs/2312.05705
tags:
- update
- kfac
- matrix
- ingd
- singd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SINGD, a memory-efficient and numerically
  stable natural gradient descent method for training large neural networks. SINGD
  addresses the limitations of KFAC, which is memory-inefficient and numerically unstable
  in low precision due to dense Kronecker factors and matrix inversions.
---

# Structured Inverse-Free Natural Gradient: Memory-Efficient & Numerically-Stable KFAC

## Quick Facts
- arXiv ID: 2312.05705
- Source URL: https://arxiv.org/abs/2312.05705
- Authors: 
- Reference count: 40
- Key outcome: Introduces SINGD, a memory-efficient and numerically stable natural gradient descent method that outperforms AdamW and KFAC on various neural network architectures while using less memory.

## Executive Summary
SINGD addresses the limitations of KFAC by formulating an inverse-free update and imposing sparse structures on Kronecker factors, resulting in a memory-efficient and numerically stable natural gradient method. By replacing matrix inversions with matrix multiplications in the matrix logarithm space and using structured sparse Kronecker factors, SINGD achieves competitive performance on CIFAR-100 and ImageWoof-10 while significantly reducing memory consumption compared to KFAC and even AdamW in some cases.

## Method Summary
SINGD extends the inverse-free natural gradient method (INGD) by imposing sparse structures on Kronecker factors to reduce memory consumption while maintaining numerical stability in mixed-precision training. The method reformulates the KFAC update using inverse-free operations in the matrix logarithm space, avoiding explicit matrix inversions that cause numerical instability in low precision. Structured sparsity patterns (block-diagonal, low-rank, Toeplitz, hierarchical) are imposed on the Kronecker factors K and C, allowing storage and computation only on non-zero entries. Experiments demonstrate that SINGD outperforms AdamW and KFAC in terms of memory efficiency and numerical stability across various neural network architectures.

## Key Results
- SINGD achieves memory efficiency by imposing structured sparsity on Kronecker factors, reducing memory usage compared to dense KFAC factors
- Numerical stability is maintained in half-precision training through inverse-free updates that replace matrix inversions with matrix subtractions in the matrix logarithm space
- SINGD achieves competitive performance with AdamW while using less memory, outperforming both AdamW and KFAC on CIFAR-100 and ImageWoof-10 across multiple architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SINGD achieves numerical stability in half-precision by replacing matrix inversions with matrix multiplications in a matrix logarithm space.
- Mechanism: SINGD reformulates the Kronecker-Factored Approximate Curvature (KFAC) update using inverse-free operations. Instead of computing matrix inverses required in KFAC, SINGD uses matrix subtractions in the matrix logarithm space and reconstructs the inverse-free update using truncated matrix exponentials.
- Core assumption: The matrix logarithm space preserves the necessary algebraic properties for optimization while avoiding numerical instability from inversions.
- Evidence anchors:
  - [abstract] "We address them by (i) formulating an inverse-free KFAC update and (ii) imposing structures in the Kronecker factors, resulting in structured inverse-free natural gradient descent (SINGD)."
  - [section 3.1] "Inspired by the INGD method, we replace matrix inversion with matrix subtraction in a matrix logarithm space and use a truncated matrix exponential map to go back to the space of the inverse matrix without explicitly inverting any matrix."
- Break condition: If the matrix logarithm space does not preserve sufficient structure for the optimization problem, or if truncation of the matrix exponential introduces unacceptable approximation error.

### Mechanism 2
- Claim: SINGD reduces memory consumption by imposing sparse structures on Kronecker factors.
- Mechanism: SINGD extends INGD by imposing structured sparsity (block-diagonal, low-rank, Toeplitz, hierarchical) on the Kronecker factors K and C. This allows storage and computation only on non-zero entries, dramatically reducing memory usage compared to dense KFAC factors.
- Core assumption: Structured sparse patterns maintain sufficient expressiveness for effective preconditioning while reducing memory and computation.
- Evidence anchors:
  - [abstract] "We impose structures in the Kronecker factors, resulting in structured inverse-free natural gradient descent (SINGD)."
  - [section 3.2] "We propose using sparse Kronecker factors K and C in INGD. In contrast, existing structured KFAC methods (Zhang et al., 2019; Grosse et al., 2023) consider (block-)diagonal structures of factors SK and SC."
- Break condition: If the imposed sparse structure is too restrictive and harms optimization performance, or if the projection maps become computationally expensive.

### Mechanism 3
- Claim: SINGD maintains competitive performance with AdamW while using less memory.
- Mechanism: By combining numerical stability (inverse-free updates) and memory efficiency (structured sparse factors), SINGD achieves optimization performance comparable to or better than AdamW. The structured factors reduce memory to levels even lower than AdamW in some cases, while the inverse-free formulation ensures stability in low-precision training.
- Core assumption: The structured Kronecker factors and inverse-free updates together preserve sufficient optimization capability.
- Evidence anchors:
  - [abstract] "Experiments show that SINGD outperforms AdamW and KFAC in terms of memory efficiency and numerical stability, while achieving competitive performance on various neural network architectures."
  - [section 4] "From Fig. 6 and 7, we can observe that SINGD including IKFAC and INGD as special cases, outperforms AdamW in many cases."
- Break condition: If the trade-off between sparsity and expressiveness becomes unfavorable for specific architectures or datasets.

## Foundational Learning

- Concept: Natural Gradient Descent (NGD)
  - Why needed here: SINGD is built upon NGD principles, using curvature information to precondition gradients. Understanding NGD helps explain why SINGD can outperform first-order methods like AdamW.
  - Quick check question: What is the key difference between NGD and standard gradient descent in terms of the metric used to measure parameter updates?

- Concept: Kronecker-Factored Approximate Curvature (KFAC)
  - Why needed here: SINGD extends and modifies KFAC. Knowing how KFAC approximates the Fisher information matrix and preconditions gradients is essential to understand SINGD's improvements.
  - Quick check question: Why does KFAC use Kronecker products to approximate the Fisher information matrix, and what computational advantage does this provide?

- Concept: Matrix logarithm and exponential maps
  - Why needed here: SINGD uses these maps to perform inverse-free updates. Understanding their properties is crucial for grasping how SINGD avoids numerical instability.
  - Quick check question: How does the matrix exponential map relate to matrix inversion in the context of SINGD's inverse-free updates?

## Architecture Onboarding

- Component map:
  Core update engine -> Memory management -> Precision handling -> Hyperparameter system -> Structure projection module

- Critical path:
  1. Compute gradients and curvature statistics (U and G matrices)
  2. Update structured Kronecker factors (K and C) using inverse-free scheme
  3. Apply preconditioned update to model parameters
  4. Repeat for each training iteration

- Design tradeoffs:
  - Sparsity level vs. optimization performance: More sparse structures reduce memory but may harm convergence
  - Structure choice: Different structures (diagonal, block-diagonal, hierarchical) suit different architectures
  - Precision vs. stability: Lower precision saves memory but requires numerical stability techniques

- Failure signatures:
  - Numerical instability: Exploding or vanishing updates, especially in low precision
  - Memory issues: Unexpected memory spikes, often from dense intermediate computations
  - Convergence problems: Slow or failed convergence, possibly due to overly restrictive structures

- First 3 experiments:
  1. Implement SINGD with diagonal Kronecker factors on a small CNN (e.g., VGG) on CIFAR-100, comparing memory usage and accuracy to AdamW.
  2. Test SINGD with hierarchical structures on a transformer model (e.g., Compact-ViT) in mixed precision, measuring stability and performance against KFAC.
  3. Benchmark SINGD against INGD with dense factors on a graph neural network to isolate the impact of structured sparsity on memory and performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between INGD's trace terms and numerical stability compared to IKFAC?
- Basis in paper: [explicit] The paper states "These trace terms together with Riemannian momentum (i.e., Î±1 > 0) are missing in KFAC and IKFAC" and "Our experiments show that these terms can contribute to stability of INGD over KFAC and IKFAC."
- Why unresolved: The paper claims trace terms improve stability but does not provide theoretical analysis of why or how much they contribute.
- What evidence would resolve it: Controlled experiments ablating trace terms in INGD vs IKFAC, and/or theoretical analysis of condition numbers or other stability metrics with/without trace terms.

### Open Question 2
- Question: What is the impact of different sparse Kronecker factor structures on downstream performance and memory efficiency trade-offs?
- Basis in paper: [explicit] The paper evaluates several sparse structures (diagonal, block-diagonal, hierarchical, triangular, Toeplitz) but states "It can be non-trivial to design such a sparse factor while maintaining downstream performance."
- Why unresolved: The paper provides empirical results but does not analyze why certain structures work better than others or provide guidelines for choosing structures.
- What evidence would resolve it: Systematic analysis of which problem properties (e.g., layer type, model size) favor which structures, and theoretical bounds on approximation quality vs sparsity.

### Open Question 3
- Question: How does SINGD scale to extremely large transformer models (e.g., >100B parameters) and what are the practical limitations?
- Basis in paper: [explicit] The paper states "We plan to exploit SINGD's low memory footprint and numerical stability to train large language models" and evaluates on models up to "Swin-ViT" and "HDVT" but not extremely large models.
- Why unresolved: The paper does not evaluate SINGD on the scale of models it aims to address (large language models).
- What evidence would resolve it: Scaling experiments on models approaching the size of current LLMs, analysis of memory bottlenecks and computation time at different scales.

## Limitations

- The paper lacks specific hyperparameter distributions for random search, which could affect reproducibility of the claimed performance improvements
- Implementation details of KFAC-reduce and numerical tricks for convolutions are referenced but not detailed, creating potential gaps in faithfully reproducing the results
- The analysis of structure projection quality is limited to heuristic validation rather than theoretical guarantees

## Confidence

- High confidence: The core inverse-free update mechanism and its numerical stability benefits are well-established from prior work (INGD) and clearly demonstrated
- Medium confidence: The memory efficiency claims depend heavily on specific implementation choices and hyperparameter tuning, which are partially specified
- Medium confidence: The competitive performance claims relative to AdamW and KFAC are supported by experiments but could vary with different hyperparameter settings

## Next Checks

1. Implement a controlled ablation study comparing SINGD variants with and without structured sparsity on the same architecture, isolating the memory vs. performance tradeoff.

2. Test SINGD's numerical stability in lower precision (FP16) compared to BFP16, measuring error rates and training stability across different model scales.

3. Benchmark SINGD on a larger, more diverse set of architectures (e.g., ViT-Huge, ResNet-200) to verify scalability claims and identify potential failure modes with very large models.