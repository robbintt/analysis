---
ver: rpa2
title: 'GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI
  Navigation'
arxiv_id: '2311.07562'
source_url: https://arxiv.org/abs/2311.07562
tags:
- gpt-4v
- screen
- arxiv
- action
- yang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MM-Navigator, a GPT-4V-based agent for smartphone
  GUI navigation tasks. The method uses Set-of-Mark prompting to anchor action outputs
  and multimodal self-summarization to maintain historical context.
---

# GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation

## Quick Facts
- arXiv ID: 2311.07562
- Source URL: https://arxiv.org/abs/2311.07562
- Authors: 
- Reference count: 14
- Key outcome: MM-Navigator achieves 91% accuracy on iOS and 52.96% on Android for zero-shot smartphone GUI navigation using GPT-4V

## Executive Summary
This paper presents MM-Navigator, a GPT-4V-based agent for zero-shot smartphone GUI navigation. The system leverages Set-of-Mark prompting for precise action localization and multimodal self-summarization for maintaining historical context across multi-step tasks. Evaluated on both iOS and Android datasets, the approach demonstrates that large multimodal models can effectively understand and navigate smartphone interfaces without task-specific training, significantly outperforming previous methods like PaLM-2 and Llama-2 on Android tasks.

## Method Summary
The method uses GPT-4V with Set-of-Mark prompting to anchor action outputs by adding numeric tags to UI elements, and multimodal self-summarization to maintain historical context through natural language summaries. The system operates in a zero-shot manner without task-specific training, processing screen images with OCR and icon detection to identify interactive elements. GPT-4V then reasons about the current screen state and historical context to generate formatted executable actions (click/scroll with location).

## Key Results
- iOS dataset: 91% accuracy in generating reasonable action descriptions, 75% accuracy in executing correct actions
- Android dataset: 52.96% overall accuracy with partial action matching, significantly outperforming previous approaches like PaLM-2 and Llama-2
- Zero-shot performance demonstrates GPT-4V's capability in screen interpretation, action reasoning, and precise action localization without task-specific training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4V can accurately understand screen contents and reason about correct actions without task-specific training.
- Mechanism: Large multimodal models (LMMs) like GPT-4V excel at screen interpretation, action reasoning, and precise action localization through zero-shot learning capabilities.
- Core assumption: GPT-4V's pre-training enables it to understand diverse GUI layouts and infer appropriate actions from natural language instructions without requiring task-specific fine-tuning.
- Evidence anchors:
  - [abstract]: "Our findings demonstrate that large multimodal models (LMMs), specifically GPT-4V, excel in zero-shot GUI navigation through its advanced screen interpretation, action reasoning, and precise action localization capabilities."
  - [section]: "Table 1 reports an accuracy of 90.9% on generating the correct intended action description, quantitatively supporting GPT-4V's capability in understanding the screen actions to perform."
  - [corpus]: Weak - No direct evidence in corpus about GPT-4V's zero-shot capabilities specifically.
- Break condition: If the GUI layout significantly deviates from training data distribution or requires domain-specific knowledge not captured in pre-training.

### Mechanism 2
- Claim: Set-of-Mark prompting enables accurate localization of actions without requiring numerical coordinate generation.
- Mechanism: The model selects from pre-identified UI elements with numeric tags rather than generating raw coordinates, reducing localization errors.
- Core assumption: GPT-4V can effectively map its high-level understanding of screen elements to the corresponding numeric tags representing UI elements.
- Evidence anchors:
  - [section]: "We add numeric tags to those elements, and present GPT-4V with the original screen I t and the screen with tags I t tags. The output text Yaction of GPT-4V will be conditioned on the two images."
  - [section]: "Specifically, at each time step, we ask GPT-4V to perform multimodal self summarization, which converts the historical actions and current step information into a concise history in the form of natural language."
  - [corpus]: Weak - No direct evidence in corpus about Set-of-Mark prompting effectiveness.
- Break condition: If the UI elements are too densely packed or the Set-of-Mark annotation fails to properly segment clickable regions.

### Mechanism 3
- Claim: Multimodal self-summarization maintains effective historical context for multi-step navigation tasks.
- Mechanism: GPT-4V generates concise natural language summaries of historical actions and current state, providing context for subsequent decisions without overwhelming the model with raw historical data.
- Core assumption: The model can effectively compress relevant historical information into a format that preserves task context while avoiding information overload.
- Evidence anchors:
  - [section]: "Specifically, at each time step, we ask GPT-4V to perform multimodal self summarization, which converts the historical actions and current step information into a concise history in the form of natural language."
  - [section]: "The key difficulty lies in devising a strategy that allows the agent to effectively determine the subsequent action at each stage of an episode, taking into account both its prior interactions with the environment and the present state of the screen."
  - [corpus]: Weak - No direct evidence in corpus about self-summarization effectiveness.
- Break condition: If the summarization fails to capture critical information needed for future actions, or if the natural language history becomes too verbose.

## Foundational Learning

- Concept: Screen grounding through OCR and icon detection
  - Why needed here: The system requires identifying UI elements on the screen to apply Set-of-Mark prompting and enable precise action localization.
  - Quick check question: How does the system identify which parts of the screen are interactive elements that should receive numeric tags?

- Concept: Multimodal reasoning and action planning
  - Why needed here: GPT-4V must integrate visual information from the screen with natural language instructions to determine appropriate actions.
  - Quick check question: What input modalities does GPT-4V process to generate action descriptions, and how are they combined?

- Concept: Historical context maintenance in sequential tasks
  - Why needed here: Multi-step GUI navigation requires the model to remember previous actions and screen states to maintain task coherence.
  - Quick check question: How does the system ensure that GPT-4V has access to relevant historical information when making decisions at each step?

## Architecture Onboarding

- Component map: Screen image + instruction → OCR/icon detection → Set-of-Mark tagging → GPT-4V reasoning → Action output
- Critical path: Screen → OCR/icon detection → Set-of-Mark tagging → GPT-4V reasoning → Action output
- Design tradeoffs:
  - Zero-shot vs. fine-tuned approaches: Zero-shot avoids training data requirements but may have lower accuracy
  - Tag-based vs. coordinate-based localization: Tags are more reliable for GPT-4V but require preprocessing
  - Raw vs. summarized history: Summaries are more efficient but may lose critical details
- Failure signatures:
  - Incorrect action selection: Model chooses wrong UI element or action type
  - Localization errors: Model selects wrong location even when choosing correct element type
  - Context loss: Model fails to maintain task state across multiple steps
- First 3 experiments:
  1. Test Set-of-Mark prompting on a simple single-step task to verify basic localization capability
  2. Test multimodal self-summarization on a multi-step task to verify context maintenance
  3. Compare zero-shot performance against a fine-tuned baseline on a representative subset of tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GPT-4V-based agents effectively handle multi-step GUI navigation tasks across different platforms and applications?
- Basis in paper: [explicit] The paper demonstrates GPT-4V's capability in single-step iOS screen navigation with 75% accuracy and shows qualitative results for multi-screen iOS navigation tasks, but does not provide quantitative multi-step performance data.
- Why unresolved: While the paper shows promising single-step results and qualitative examples of multi-step navigation, it lacks comprehensive quantitative evaluation of GPT-4V's performance on complex, multi-step GUI navigation tasks across different platforms.
- What evidence would resolve it: Comprehensive quantitative evaluation of GPT-4V-based agents on multi-step GUI navigation tasks across different platforms and applications, including success rates, completion times, and error analysis.

### Open Question 2
- Question: How can GPT-4V-based agents be improved to handle novel or ambiguous GUI elements and instructions?
- Basis in paper: [inferred] The paper mentions that GPT-4V sometimes fails to handle novel or ambiguous GUI elements and instructions, such as when the correct clickable area is not marked or when the model lacks knowledge about specific app functionalities.
- Why unresolved: The paper identifies these limitations but does not propose or evaluate specific methods to improve GPT-4V's ability to handle novel or ambiguous GUI elements and instructions.
- What evidence would resolve it: Evaluation of various methods to improve GPT-4V's ability to handle novel or ambiguous GUI elements and instructions, such as incorporating external knowledge bases, using few-shot learning techniques, or developing more robust action localization methods.

### Open Question 3
- Question: Can GPT-4V-based agents be effectively deployed in real-world scenarios, considering factors such as latency, efficiency, and user experience?
- Basis in paper: [explicit] The paper mentions that using a large-scale model like GPT-4V for GUI navigation is costly and discusses the potential of model distillation to obtain a smaller, more efficient model.
- Why unresolved: While the paper acknowledges the challenges of deploying GPT-4V-based agents in real-world scenarios, it does not provide concrete solutions or evaluate the impact of these challenges on user experience.
- What evidence would resolve it: Implementation and evaluation of GPT-4V-based agents in real-world scenarios, including measures of latency, efficiency, and user experience, as well as the effectiveness of techniques such as model distillation in addressing these challenges.

## Limitations

- Limited dataset diversity: Results may not generalize to all GUI styles and app categories beyond tested iOS and Android datasets
- Unknown model bias: Systematic bias toward "click" actions over scrolling or other interaction types
- Evaluation methodology concerns: Human evaluation introduces subjectivity, while partial action matching may inflate Android performance metrics

## Confidence

- High confidence: GPT-4V's basic screen interpretation capabilities (90.9% accuracy in action description generation)
- Medium confidence: Effectiveness of Set-of-Mark prompting (mechanism well-described but lacks direct corpus evidence)
- Medium confidence: Multimodal self-summarization's contribution to historical context maintenance (theoretically sound but lacks empirical validation)
- Low confidence: Generalization across all smartphone GUI types (performance drops significantly from iOS to Android)

## Next Checks

1. Cross-platform robustness test: Evaluate the system on diverse apps across categories (productivity, entertainment, social media) to assess generalization beyond tested datasets

2. Bias mitigation validation: Design test cases requiring non-click actions (scrolling, long-press, gestures) to quantify and address systematic bias toward "click" predictions

3. Human evaluation standardization: Implement rigorous protocol with clear scoring rubrics and inter-rater reliability measures for consistent action correctness assessment