---
ver: rpa2
title: 'ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness
  of Large Language Models'
arxiv_id: '2310.09624'
source_url: https://arxiv.org/abs/2310.09624
tags:
- adversarial
- language
- should
- safe
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ASSERT evaluates large language model robustness across safety
  domains by generating three types of test prompts: semantically equivalent paraphrases,
  related but distinct scenarios, and adversarial prompts with misleading hints. The
  approach identifies significant model instability, with up to 11% accuracy differences
  between semantically similar prompts and up to 19% error rates on adversarial inputs.'
---

# ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models

## Quick Facts
- arXiv ID: 2310.09624
- Source URL: https://arxiv.org/abs/2310.09624
- Reference count: 13
- Primary result: GPT-4 is the most robust to safety attacks, while Vicuna shows the most instability

## Executive Summary
ASSERT is a framework for automated safety scenario red teaming that evaluates the robustness of large language models (LLMs) across safety domains. The approach generates three types of test prompts: semantically equivalent paraphrases, related but distinct scenarios, and adversarial prompts with misleading hints. The method reveals significant model instability, with up to 11% accuracy differences between semantically similar prompts and up to 19% error rates on adversarial inputs. GPT-4 demonstrates the highest robustness, while smaller models like Vicuna show greater vulnerability to subtle prompt variations and targeted adversarial attacks.

## Method Summary
The ASSERT framework evaluates LLM robustness using the SAFE TEXT dataset, which is partitioned into four safety domains (outdoors, medical, household, extra). Three automated generation methods create test prompts: semantically aligned augmentation (generating paraphrases), targeted bootstrapping (creating related scenarios), and adversarial knowledge injection (adding misleading contextual hints). Models are evaluated using few-shot demonstrations with greedy decoding, and performance is measured through classification accuracy and statistical significance testing across the different prompt types and domains.

## Key Results
- Statistically significant performance differences of up to 11% in absolute classification accuracy among semantically related scenarios
- Error rates of up to 19% absolute error in zero-shot adversarial settings
- GPT-4 shows the most stability overall, while VICUNA exhibits the most instability across safety domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantically aligned augmentation reveals model instability through prompt paraphrasing
- Mechanism: Generates multiple semantically equivalent variations of existing prompts, exposing differences in model outputs for nearly identical inputs
- Core assumption: Models should maintain consistent performance across semantically equivalent inputs
- Evidence anchors: Statistically significant performance differences of up to 11% in absolute classification accuracy among semantically related scenarios

### Mechanism 2
- Claim: Adversarial knowledge injection exploits model vulnerabilities through misleading contextual hints
- Mechanism: Extracts hypothetical benefits that contradict ground truth labels and injects them as hints, creating covert adversarial attacks
- Core assumption: Models will be misled by seemingly helpful but incorrect contextual information
- Evidence anchors: Error rates of up to 19% absolute error in zero-shot adversarial settings

### Mechanism 3
- Claim: Targeted bootstrapping generates diverse related scenarios to test model reasoning consistency
- Mechanism: Iteratively isolates and replaces subsequences of existing samples while maintaining contextual consistency
- Core assumption: Models should maintain similar performance across related but non-equivalent scenarios
- Evidence anchors: Related but distinct scenarios as one of three prompt types evaluated

## Foundational Learning

- Concept: Robustness evaluation in AI safety
  - Why needed here: Understanding what constitutes robustness is essential for evaluating model performance across different prompt variations
  - Quick check question: What are the three types of robustness evaluated in this paper?

- Concept: Adversarial attack strategies
  - Why needed here: Understanding how adversarial knowledge injection works requires knowledge of attack vectors and defensive mechanisms
  - Quick check question: How does the adversarial knowledge injection method differ from traditional adversarial attacks?

- Concept: Few-shot learning and in-context inference
  - Why needed here: The evaluation uses few-shot demonstrations to guide model outputs, requiring understanding of how this affects performance
  - Quick check question: Why is greedy decoding used during inference in this study?

## Architecture Onboarding

- Component map: SAFE TEXT dataset -> Three generation methods -> Four-shot evaluation -> Classification accuracy comparison
- Critical path: 1. Partition SAFE TEXT into safety domains, 2. Generate test prompts using three methods, 3. Evaluate model responses using few-shot inference, 4. Compare performance metrics across methods and domains
- Design tradeoffs: Automated generation vs. human verification quality, Conservative filtering vs. comprehensive coverage, Few-shot stability vs. zero-shot realism
- Failure signatures: High variance in classification accuracy for semantically similar prompts, Significant error rate increases under adversarial conditions, Domain-specific performance instability
- First 3 experiments: 1. Replicate semantic alignment test on a different safety dataset, 2. Test cross-model adversarial attacks with different source/target pairs, 3. Evaluate robustness across additional model architectures (smaller/larger variants)

## Open Questions the Paper Calls Out

The paper identifies several open questions: How do robustness results change with different prompt templates? What is the relationship between model size and robustness across safety domains? How does adversarial knowledge injection effectiveness vary with domain-specific knowledge extraction? These questions highlight areas where the current evaluation framework could be extended to provide deeper insights into model behavior and vulnerabilities.

## Limitations

- The SAFE TEXT dataset may not represent the full diversity of safety-relevant prompts encountered in real-world deployment
- The four-shot demonstration approach represents an artificial evaluation condition that may not reflect actual usage patterns
- The quality assurance process relying on human annotators introduces potential subjectivity in filtering generated samples

## Confidence

- High Confidence: Semantically equivalent prompts can produce statistically significant performance differences (up to 11%)
- Medium Confidence: Adversarial knowledge injection achieving up to 19% error rates
- Low Confidence: Comparative robustness claims between models (e.g., GPT-4 being most robust)

## Next Checks

1. Cross-dataset validation: Test the same robustness evaluation methods on alternative safety datasets to verify whether observed instability patterns persist across different data sources

2. Real-world deployment simulation: Design an experiment evaluating models on semantically similar prompts that naturally occur in conversation rather than synthetically generated variations

3. Multi-turn interaction testing: Extend evaluation to multi-turn conversational scenarios where safety-critical prompts are embedded within longer interactions