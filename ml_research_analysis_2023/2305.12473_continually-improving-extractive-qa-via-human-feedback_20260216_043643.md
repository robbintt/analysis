---
ver: rpa2
title: Continually Improving Extractive QA via Human Feedback
arxiv_id: '2305.12473'
source_url: https://arxiv.org/abs/2305.12473
tags:
- feedback
- question
- learning
- round
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of human feedback to continually improve
  an extractive QA system. The authors iteratively deploy their model to real users
  who pose questions and provide feedback on the predicted answers.
---

# Continually Improving Extractive QA via Human Feedback

## Quick Facts
- arXiv ID: 2305.12473
- Source URL: https://arxiv.org/abs/2305.12473
- Reference count: 38
- Key outcome: Iterative human feedback improves extractive QA from 39 to 67 F1 over nine rounds

## Executive Summary
This paper presents a novel approach to continually improving extractive question answering (QA) systems through iterative deployment and learning from human feedback. The authors develop a two-step model that first predicts answerability and then extracts answer spans, trained using offline contextual bandit learning with inverse propensity scoring. Over nine rounds of deployment with crowdsourced user interactions, the system demonstrates significant improvement in F1 scores while maintaining the ability to learn from new data distributions.

## Method Summary
The method involves iteratively deploying a DeBERTaV3-based extractive QA model to real users who provide feedback on predicted answers. The model predicts whether a question is answerable and extracts an answer span if deemed answerable. User feedback is mapped to rewards and used to update the model parameters offline using REINFORCE policy gradient with inverse propensity scoring to correct for selection bias. The process repeats across multiple rounds, with each round's data combined with previous rounds through rehearsal.

## Key Results
- F1 score improves from 39 to 67 over nine rounds of deployment
- Model trained on SQuAD2 (512 examples) shows significant improvement with user feedback
- Answerability classification ablation shows degraded performance compared to combined model
- Domain adaptation possible when initializing with model trained on different domain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning from user feedback over multiple rounds improves F1 scores by iteratively correcting model errors.
- Mechanism: The system aggregates user feedback, maps it to reward values, and uses policy gradient REINFORCE with inverse propensity scoring to update model parameters offline. This allows the model to learn from diverse user interactions without requiring real-time training.
- Core assumption: User feedback is reliable and representative of the true answer quality.
- Evidence anchors:
  - [abstract] "Our experiments show effective improvement from user feedback of extractive QA models over time across different data regimes, including significant potential for domain adaptation."
  - [section] "Over nine rounds of deployment with over 1,800 user interactions, our approach shows an overall improvement from 39 to 67 F1 score on our newly collected Wikipedia-based data."
  - [corpus] Weak - no direct evidence in cited papers.
- Break condition: If user feedback becomes systematically biased or adversarial, the model's performance will degrade.

### Mechanism 2
- Claim: Separating answerability classification from span extraction improves learning stability.
- Mechanism: The model first predicts if a question is answerable, then extracts a span only if it's deemed answerable. This two-step process allows different reward values for each decision, improving calibration.
- Core assumption: Answerability and span extraction are independent decisions that benefit from separate modeling.
- Evidence anchors:
  - [section] "We address this by modeling question answerability separately from span extraction: our model first predicts if the question is answerable and only then extracts an answer span."
  - [section] "This model over-predicts unanswerable in the first round, and afterwards keeps showing a lower classification accuracy compared to the default setup."
  - [corpus] Weak - no direct evidence in cited papers.
- Break condition: If the correlation between answerability and span extraction becomes too strong, the separation may introduce unnecessary complexity.

### Mechanism 3
- Claim: Using offline contextual bandit learning with inverse propensity scoring prevents unbounded negative loss and debiases the learning process.
- Mechanism: IPS weights correct for selection bias in the offline data, while clipping prevents extreme updates. The entropy term regularizes the answerability classification.
- Core assumption: The propensity scores (probability of selecting an action given the context) are accurately estimated from the interaction data.
- Evidence anchors:
  - [section] "IPS de-biases the offline data (Bietti et al., 2021), and also prevents unbounded negative loss terms (Kojima et al., 2021)."
  - [section] "The entropy term regularizes the learning (Williams, 1992; Mnih et al., 2016)."
  - [corpus] Weak - no direct evidence in cited papers.
- Break condition: If propensity scores are poorly estimated (e.g., due to sparse data), the IPS weighting may introduce more bias than it corrects.

## Foundational Learning

- Concept: Contextual bandit learning
  - Why needed here: The system must learn from user feedback without access to a reward function, making contextual bandits ideal for mapping feedback to rewards.
  - Quick check question: How does inverse propensity scoring help in offline learning from bandit feedback?

- Concept: Answerability classification
  - Why needed here: Many user questions are unanswerable given the context, so the model must first determine if an answer exists before attempting span extraction.
  - Quick check question: Why might combining answerability and span extraction into a single model degrade performance?

- Concept: Policy gradient methods
  - Why needed here: The model needs to update parameters based on user feedback rewards, and policy gradients provide a way to do this in discrete action spaces.
  - Quick check question: What role does the entropy term play in the policy gradient objective?

## Architecture Onboarding

- Component map: User interface → Model prediction (answerability + span extraction) → User feedback → Offline learning (policy gradient with IPS) → Updated model
- Critical path: User question → Model prediction → User feedback → Model update → Improved predictions
- Design tradeoffs: Simple feedback (3 options) vs. more complex feedback (e.g., natural language); offline learning vs. online learning
- Failure signatures: Sudden drops in F1 score; model overfitting to specific question types; user feedback noise overwhelming learning signal
- First 3 experiments:
  1. Deploy initial model and collect baseline user feedback
  2. Test impact of answerability classification ablation
  3. Compare multi-round vs. one-round learning on same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of iterative learning compare to a single round of learning when the total amount of feedback data is held constant?
- Basis in paper: Explicit - Section 6.3 compares one-round vs multi-round learning.
- Why unresolved: While the paper shows multi-round learning performs better, it does not explore varying the total feedback data amount or learning rate across different round counts to determine the optimal balance.
- What evidence would resolve it: Experiments varying the total feedback data amount and learning rate for different numbers of iterative rounds, comparing overall F1 and learning efficiency.

### Open Question 2
- Question: How sensitive is the learning process to the initial model strength and how much can a weak initial model improve with sufficient feedback?
- Basis in paper: Explicit - Section 6.2 studies a weaker initial model trained on 128 examples vs 512 examples.
- Why unresolved: The paper only tests one weaker initial model. It's unclear how the learning curve and final performance would change for even weaker initial models or if there's a threshold below which learning becomes ineffective.
- What evidence would resolve it: Experiments testing a range of initial model strengths from very weak to strong, measuring learning curves and final performance on the test set.

### Open Question 3
- Question: How does the frequency of model updates (number of interactions per round) impact the learning dynamics and final performance?
- Basis in paper: Explicit - Section 6.2 studies fewer interactions per round.
- Why unresolved: The paper only tests one alternative update frequency. It's unclear how the learning curve and final performance would change for even more frequent updates or if there's an optimal update frequency.
- What evidence would resolve it: Experiments testing a range of update frequencies from very frequent to infrequent, measuring learning curves, final performance, and model stability over time.

## Limitations
- Improvement from 39 to 67 F1 still leaves room for state-of-the-art performance
- Reliance on crowdsourced feedback may introduce noise and bias
- Experiments limited to single domain (Wikipedia)
- Offline learning approach may miss opportunities for more rapid online adaptation

## Confidence
- **High confidence**: The core mechanism of using human feedback with offline contextual bandit learning is sound and well-implemented. The observed improvement trend is statistically significant and reproducible.
- **Medium confidence**: The specific design choices (answerability classification separation, reward mapping, hyperparameter settings) are justified but could benefit from additional ablation studies and theoretical grounding.
- **Medium confidence**: The domain adaptation results are promising but limited to one cross-domain transfer scenario.

## Next Checks
1. **Adversarial feedback robustness test**: Introduce controlled adversarial feedback patterns to assess model resilience and potential degradation modes.
2. **Multi-domain transfer experiment**: Extend domain adaptation evaluation to 3-5 diverse domains to better understand generalization capabilities and identify failure patterns.
3. **Feedback quality impact study**: Systematically vary feedback quality (noise levels, consistency) in controlled experiments to quantify its impact on learning efficiency and final performance.