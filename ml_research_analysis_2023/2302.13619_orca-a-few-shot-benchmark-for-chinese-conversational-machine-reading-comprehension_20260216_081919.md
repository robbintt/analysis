---
ver: rpa2
title: 'Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension'
arxiv_id: '2302.13619'
source_url: https://arxiv.org/abs/2302.13619
tags:
- orca
- passage
- shot
- space
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Orca, the first few-shot Chinese conversational
  machine reading comprehension (CMRC) benchmark. Unlike previous datasets that use
  static passages, Orca assigns a dynamic, response-related passage to each turn,
  making it more aligned with real-world conversation patterns.
---

# Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension

## Quick Facts
- arXiv ID: 2302.13619
- Source URL: https://arxiv.org/abs/2302.13619
- Reference count: 33
- Introduces the first few-shot Chinese conversational machine reading comprehension benchmark

## Executive Summary
This paper introduces Orca, a novel few-shot benchmark for Chinese conversational machine reading comprehension (CMRC). Unlike previous datasets that use static passages, Orca assigns a dynamic, response-related passage to each turn, making it more aligned with real-world conversation patterns. The dataset contains 831 conversations with 4,742 turns across 33 domains, with human-annotated natural responses rather than extracted spans. The authors implement three baselines: in-context learning with GPT-3, end-to-end training with T5 and BART, and a three-stage QPR framework. Experiments show Orca is challenging, with none of the models achieving over 80% on automatic metrics. The QPR system performs best in zero-shot settings, while T5 excels in few-shot scenarios.

## Method Summary
The Orca benchmark is created by first crawling topics from the internet and then collecting conversations through crowd-sourcing. Each conversation turn is assigned a passage relevant to the current question, and responses are free-form natural language rather than extracted text spans. The authors implement three baseline approaches: in-context learning with GPT-3, end-to-end training with T5 and BART, and a three-stage QPR framework. Experiments are conducted under both zero-shot and few-shot settings, with the dataset split into support (200 conversations) and test sets. Models are evaluated using automatic metrics (BLEU, ROUGE, EM) and human evaluation on relevance, completeness, and naturalness.

## Key Results
- Orca is challenging for current models, with no approach achieving over 80% on automatic metrics
- QPR framework performs best in zero-shot settings across all metrics
- T5 achieves the highest performance in few-shot scenarios
- Human evaluation shows models struggle with response relevance, completeness, and naturalness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic passage assignment improves real-world conversation modeling by mimicking how humans retrieve topic-relevant knowledge during multi-turn interactions.
- Mechanism: Each conversation turn is paired with a passage directly related to the current question, simulating dynamic knowledge retrieval rather than relying on a static document.
- Core assumption: Real conversations draw on diverse, context-specific knowledge sources, and static passages limit the ability to capture this dynamic retrieval pattern.
- Evidence anchors:
  - [abstract]: "Each turn of a conversation is assigned with a response-related passage, aiming to evaluate model's comprehension ability more reasonably."
  - [section]: "Each turn of a conversation is assigned a golden response-related passage, fitting human cognition towards CMRC."
  - [corpus]: FMR scores for related papers indicate active interest in dynamic knowledge sources in conversational QA.

### Mechanism 2
- Claim: Natural, human-annotated responses rather than extractive spans encourage models to develop genuine generative comprehension skills.
- Mechanism: Responses are free-form, natural language rather than extracted text spans, requiring the model to generate answers instead of selecting or copying.
- Core assumption: Real-world dialogue demands generation of natural, context-aware responses rather than fixed span extraction.
- Evidence anchors:
  - [abstract]: "Importantly, answers in Orca are all well-annotated natural responses rather than the specific spans or short phrase in previous datasets."
  - [section]: "Responses are free-form text with demonstrative pronouns instead of extracted answer span from the passage, making the expression more natural."
  - [corpus]: Lack of span overlap (69.3% of responses cannot be found as exact text spans) supports the generative nature.

### Mechanism 3
- Claim: Few-shot and zero-shot settings enable evaluation of models' ability to generalize across diverse domains without extensive retraining.
- Mechanism: The dataset is split into support (few samples) and test sets, with 33 domains represented, to assess cross-domain generalization.
- Core assumption: Real-world conversational QA systems must perform well on unseen topics with limited data.
- Evidence anchors:
  - [abstract]: "We further provide zero-shot/few-shot settings to evaluate model's generalization ability towards diverse domains."
  - [section]: "We provide zero-shot and few-shot settings and extract 200 conversations from Orca as the support set and treat the rest data as test set."
  - [corpus]: Related works on few-shot learning in conversational QA support this experimental design.

## Foundational Learning

- Concept: Dynamic passage retrieval in conversational QA
  - Why needed here: Models must learn to associate each turn with the most relevant passage rather than relying on a static context.
  - Quick check question: Can the model accurately retrieve or match the passage to the current conversational turn?

- Concept: Generative response modeling vs. extractive QA
  - Why needed here: The task requires producing natural, fluent answers, not just extracting spans, to reflect real conversation.
  - Quick check question: Does the model generate fluent, context-aware responses that may not appear verbatim in the passage?

- Concept: Cross-domain generalization under limited data
  - Why needed here: The benchmark tests model adaptability to 33 diverse domains with few training examples per domain.
  - Quick check question: Can the model maintain performance across domains it has not been extensively trained on?

## Architecture Onboarding

- Component map: Topic → Conversation History + Current Question + Passage → Response Generator
- Critical path: Input (Topic, History, Question, Passage) → Model Processing → Generated Response
- Design tradeoffs: Dynamic passage assignment vs. computational overhead; generative vs. extractive responses; few-shot evaluation vs. data requirements
- Failure signatures: Poor relevance to passage, inability to handle coreference, weak cross-domain generalization, short or incomplete responses
- First 3 experiments:
  1. Test passage relevance: Feed a conversation turn and verify the model retrieves or matches the correct passage.
  2. Evaluate generative capability: Generate responses for a few sample turns and check fluency and naturalness.
  3. Cross-domain zero-shot test: Run model on a held-out domain and measure performance drop.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamic passage assignment in Orca compare to static passage approaches in terms of model performance across different conversation domains?
- Basis in paper: [explicit] The paper states that "Each turn of a conversation is assigned a response-related passage" and compares this to static passage approaches in existing datasets.
- Why unresolved: The paper implements baselines but doesn't specifically analyze performance differences between dynamic and static passage assignments across domains.
- What evidence would resolve it: A controlled experiment comparing model performance on Orca using dynamic passages versus converting it to a static passage format.

### Open Question 2
- Question: What is the optimal number of conversation turns per topic needed to achieve high-quality responses in Orca?
- Basis in paper: [explicit] The paper mentions that conversations have an average of 5.71 turns, but doesn't explore whether this is optimal or how performance varies with different conversation lengths.
- Why unresolved: The paper provides statistics about conversation length but doesn't analyze how the number of turns affects response quality or model performance.
- What evidence would resolve it: An ablation study varying the number of conversation turns per topic and measuring the impact on response quality and model performance.

### Open Question 3
- Question: How does the three-stage QPR framework perform on English CMRC datasets compared to existing approaches?
- Basis in paper: [explicit] The paper introduces the QPR framework but only evaluates it on Orca, a Chinese dataset.
- Why unresolved: The framework is presented as a strong baseline but its generalizability to other languages and datasets is not tested.
- What evidence would resolve it: Applying the QPR framework to English CMRC datasets like CoQA or QuAC and comparing performance with state-of-the-art methods.

## Limitations

- The dataset size (831 conversations, 4,742 turns) is relatively small compared to other established benchmarks
- Human evaluation was conducted on only 100 samples, which may not be representative of the full dataset diversity
- The paper does not report inter-annotator agreement scores for human evaluation

## Confidence

*High Confidence Claims:*
- The Orca dataset is indeed the first few-shot Chinese conversational machine reading comprehension benchmark
- The benchmark is challenging for current models, with none achieving over 80% on automatic metrics
- The QPR framework performs best in zero-shot settings

*Medium Confidence Claims:*
- Dynamic passage assignment significantly improves real-world conversation modeling
- Natural, human-annotated responses better reflect real conversation patterns
- Few-shot and zero-shot settings effectively evaluate generalization across diverse domains

*Low Confidence Claims:*
- The dataset captures "real-world conversation patterns" comprehensively
- Automatic metrics adequately capture response quality

## Next Checks

1. Conduct ablation studies comparing static vs. dynamic passage assignment to quantify the impact on model performance.
2. Perform inter-annotator agreement analysis on the human evaluation data to establish annotation reliability.
3. Test model performance on extended samples from the same domains to assess robustness and identify performance plateaus.