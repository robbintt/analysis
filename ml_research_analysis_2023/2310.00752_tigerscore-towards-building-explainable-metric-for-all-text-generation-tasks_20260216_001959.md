---
ver: rpa2
title: 'TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks'
arxiv_id: '2310.00752'
source_url: https://arxiv.org/abs/2310.00752
tags: []
core_contribution: This paper introduces TIGERScore, a reference-free metric for evaluating
  text generation tasks that is both explainable and task-agnostic. TIGERScore is
  built by fine-tuning LLaMA-2 on MetricInstruct, a dataset of 48K examples covering
  6 text generation tasks and 23 datasets.
---

# TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks

## Quick Facts
- arXiv ID: 2310.00752
- Source URL: https://arxiv.org/abs/2310.00752
- Reference count: 24
- TIGERScore achieves highest Spearman's correlation with human ratings across 7 held-in and held-out datasets, outperforming other metrics significantly

## Executive Summary
This paper introduces TIGERScore, a reference-free metric for evaluating text generation tasks that is both explainable and task-agnostic. TIGERScore is built by fine-tuning LLaMA-2 on MetricInstruct, a dataset of 48K examples covering 6 text generation tasks and 23 datasets. The dataset is constructed by collecting system outputs from 50+ real-world systems and generating synthetic outputs using GPT-4, with error analysis generated by prompting GPT models and filtered through heuristic-based strategies. TIGERScore achieves the highest overall Spearman's correlation with human ratings across 7 held-in and held-out datasets, outperforming other metrics significantly. It even surpasses the best reference-based metrics by 5% in terms of Spearman's correlation. Human evaluation of the generated explanations found them to be 70.8% accurate.

## Method Summary
The paper proposes TIGERScore, an explainable and reference-free metric for text generation evaluation. It constructs MetricInstruct, a dataset of 48K examples collected from real-world systems and synthetic generation using GPT-4. The dataset covers 6 text generation tasks and includes error analyses with aspects like relevance, factuality, and fluency. LLaMA-2 is fine-tuned on this dataset to learn how to evaluate system outputs and generate detailed error explanations. The model is trained using instruction-tuning with task-specific aspect definitions, enabling generalization to unseen tasks through prompt instructions alone.

## Key Results
- TIGERScore achieves the highest overall Spearman's correlation with human ratings across 7 held-in and held-out datasets
- Outperforms other metrics significantly, surpassing the best reference-based metrics by 5% in Spearman's correlation
- Human evaluation shows 70.8% accuracy in the generated error explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-channel data collection (real-world + synthetic) ensures comprehensive error coverage across diverse generation tasks.
- Mechanism: Real-world outputs capture naturally occurring errors from existing systems, while synthetic outputs deliberately introduce underrepresented error types through GPT-4 prompts.
- Core assumption: Real-world systems have incomplete error distributions that can be balanced by targeted synthetic generation.
- Evidence anchors:
  - [abstract] "The dataset includes system outputs from more than 50 real-world systems, covering a wide variety of errors."
  - [section 3.2] "Real-World: We consider a wide range of systems... To assess the quality... we use BARTScore... We categorized the outputs into three groups: the top one-third with the highest BARTScore, the middle one-third with intermediate scores, and the bottom one-third with the lowest scores."
  - [corpus] Weak evidence - no related papers directly discuss multi-channel error collection for evaluation metrics.

### Mechanism 2
- Claim: Instruction-tuning with task-specific aspect definitions enables generalization to unseen text generation tasks.
- Mechanism: The model learns implicit correlations between evaluation aspects and task definitions during fine-tuning, allowing it to adapt to new tasks through instruction alone.
- Core assumption: Task definitions contain sufficient information for the model to infer appropriate evaluation aspects without explicit aspect specification.
- Evidence anchors:
  - [abstract] "The dataset consists of 42K quadruple in the form of (instruction, input, system output → error analysis)."
  - [section 2.2] "Rather than explicitly highlighting these aspects and definitions in the prompt, we aim to implicitly incorporate this aspect-specific knowledge into the model during the fine-tuning stage."
  - [corpus] Weak evidence - while InstructScore uses instruction-tuning, it doesn't demonstrate generalization to completely unseen tasks.

### Mechanism 3
- Claim: Heuristic-based filtering improves data quality by removing hallucinations and reference-dependent explanations.
- Mechanism: Multiple filtering stages eliminate anomalous JSON, unclear formats, unreasonable scores, reference-dependent justifications, and hallucinated locations using Levenshtein distance.
- Core assumption: Reference-free evaluation requires explanations that don't rely on comparing to reference outputs.
- Evidence anchors:
  - [section 3.3] "To ensure data quality and reliability, we implement a series of steps... eliminate data with unclear format or logical issues... data relying on reference outputs for justification are excluded."
  - [section 4.4] "Based on the results of the human evaluation, it was found that 64.3% of the error analyses provided by TIGERScore were deemed entirely reasonable."
  - [corpus] No direct evidence - filtering strategies for evaluation metric datasets are not discussed in related work.

## Foundational Learning

- Concept: Error analysis generation through structured prompting
  - Why needed here: The metric must produce detailed error locations, aspects, explanations, and penalties in a consistent format
  - Quick check question: What JSON structure is expected for error analysis output?

- Concept: Reference-free evaluation metrics
  - Why needed here: Many real-world applications lack reference outputs, making traditional metrics inapplicable
  - Quick check question: How does the metric compute scores without reference comparison?

- Concept: Multi-aspect evaluation framework
  - Why needed here: Different text generation tasks require different evaluation dimensions (relevance, factuality, fluency, etc.)
  - Quick check question: How are evaluation aspects defined for each task type?

## Architecture Onboarding

- Component map: LLaMA-2 base model → fine-tuning on MetricInstruct dataset → TIGERScore metric
- Critical path: Dataset construction (real-world + synthetic) → Heuristic filtering → Model fine-tuning → Evaluation
- Design tradeoffs: Reference-free vs. reference-based accuracy, synthetic vs. real error coverage, model size vs. performance
- Failure signatures: Hallucinated error locations, reference-dependent explanations, imbalanced error aspect coverage
- First 3 experiments:
  1. Evaluate TIGERScore on a held-out summarization dataset to verify correlation with human ratings
  2. Test generalization to an unseen task (e.g., story generation) to validate instruction-following capability
  3. Compare performance of models trained on only real-world vs. only synthetic data to assess contribution of each channel

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TIGERScore's performance compare when evaluated on datasets from domains significantly different from those in MetricInstruct?
- Basis in paper: [inferred] The paper mentions TIGERScore's ability to generalize to unseen text generation tasks but does not provide specific results on datasets from entirely different domains.
- Why unresolved: The paper focuses on datasets from six text generation tasks and does not explore the model's performance on datasets from domains outside these tasks.
- What evidence would resolve it: Conducting experiments on datasets from domains not covered in MetricInstruct, such as scientific writing or legal documents, and comparing TIGERScore's performance with other metrics.

### Open Question 2
- Question: What is the impact of using different base models (other than LLaMA-2) on TIGERScore's performance?
- Basis in paper: [explicit] The paper uses LLaMA-2 for fine-tuning TIGERScore but does not explore the effects of using other base models.
- Why unresolved: The choice of base model can significantly influence the performance of fine-tuned models, and the paper does not investigate this aspect.
- What evidence would resolve it: Experimenting with different base models, such as GPT-3 or T5, and comparing their performance in fine-tuning TIGERScore.

### Open Question 3
- Question: How does TIGERScore handle multilingual text generation tasks?
- Basis in paper: [inferred] The paper does not explicitly discuss TIGERScore's performance on multilingual tasks, although it mentions translation as one of the tasks in the evaluation.
- Why unresolved: The paper focuses on English datasets and does not explore TIGERScore's ability to handle multiple languages.
- What evidence would resolve it: Evaluating TIGERScore on multilingual datasets and comparing its performance with other metrics designed for multilingual tasks.

### Open Question 4
- Question: What are the computational requirements and efficiency of TIGERScore compared to other metrics?
- Basis in paper: [inferred] The paper does not provide information on the computational efficiency of TIGERScore relative to other metrics.
- Why unresolved: While TIGERScore shows high correlation with human ratings, its computational efficiency is crucial for practical deployment.
- What evidence would resolve it: Benchmarking the computational time and resources required by TIGERScore compared to other metrics across various tasks and datasets.

## Limitations
- The metric's performance on tasks outside the 6-task scope remains untested, raising questions about true task-agnostic capability
- The 70.8% accuracy in human evaluation of explanations indicates that approximately 30% of generated error analyses may contain inaccuracies or hallucinations
- The reliance on GPT-4 for synthetic error generation introduces potential biases from the base model

## Confidence

**High confidence**: The metric's strong correlation performance with human ratings (5% improvement over reference-based metrics)

**Medium confidence**: The generalization capability to unseen tasks based on instruction-following design

**Low confidence**: The completeness of error coverage across all possible text generation scenarios

## Next Checks

1. Test TIGERScore on a diverse set of text generation tasks not included in the training data (e.g., dialogue generation, poetry generation) to assess true generalization
2. Conduct ablation studies comparing models trained on only real-world data versus only synthetic data to quantify each data source's contribution
3. Evaluate the metric's robustness by testing on adversarial system outputs designed to expose specific weaknesses in error detection