---
ver: rpa2
title: Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering
  with Multimodal LLMs
arxiv_id: '2310.16033'
source_url: https://arxiv.org/abs/2310.16033
tags:
- visual
- cropping
- question
- image
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates a critical limitation of multimodal large
  language models (MLLMs) in perceiving small visual details during zero-shot visual
  question answering (VQA). The authors find that the accuracy of state-of-the-art
  MLLMs, such as BLIP2, can decline by up to 46% when answering questions about small
  visual subjects compared to larger ones.
---

# Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs

## Quick Facts
- arXiv ID: 2310.16033
- Source URL: https://arxiv.org/abs/2310.16033
- Reference count: 36
- Key outcome: MLLM accuracy can decline by up to 46% on small visual subjects compared to larger ones, but visual cropping methods can improve zero-shot VQA performance significantly.

## Executive Summary
This paper investigates a critical limitation in multimodal large language models (MLLMs) where their accuracy in zero-shot visual question answering (VQA) significantly drops when dealing with small visual subjects. Through systematic experiments, the authors demonstrate that state-of-the-art MLLMs like BLIP2 show up to 46% accuracy decline when answering questions about small visual details compared to larger ones. To address this limitation, the paper proposes five automatic visual cropping methods that leverage external localization models or the model's own decision process to improve VQA performance. The proposed methods are evaluated across four popular VQA datasets and a new fine-detail-focused dataset, showing substantial improvements in accuracy, particularly for questions requiring perception of small visual details.

## Method Summary
The paper proposes three automatic visual cropping methods to improve zero-shot VQA performance: clip-CROP uses CLIP to progressively crop images based on similarity to the question, yolo-CROP uses YOLOv8 object detection to identify relevant bounding boxes, and sam-CROP uses SAM segmentation to find relevant regions. These methods are applied as inference-time mechanisms to existing MLLM models like BLIP2. The cropping methods identify regions of interest in images and combine them with the original image tokens to provide both global context and local detail information to the model. The effectiveness of these methods is evaluated on multiple VQA datasets including VQA v2, GQA, A-OKVQA, TextVQA, and a new dataset FD-VQA specifically designed to test fine visual detail perception.

## Key Results
- Visual cropping methods improved accuracy by up to 42.29% on FD-VQA and 37.68% on TextVQA compared to baseline models
- The best performing method, clip-CROP, achieved 42.29% accuracy on FD-VQA compared to the original model's 33.94%
- Visual cropping was particularly effective for questions requiring perception of small visual details but less effective for questions requiring global image context like counting or localization
- Human visual cropping outperformed automatic methods by 8.76% accuracy on FD-VQA, indicating room for improvement in automatic cropping approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual cropping improves zero-shot VQA accuracy by focusing the model's attention on relevant visual details.
- Mechanism: Cropping narrows the input image to the region of interest, reducing the amount of visual information the model must process and increasing the relative size of the target object within the cropped frame.
- Core assumption: MLLMs struggle to perceive small visual details due to their reliance on global image representations.
- Evidence anchors:
  - [abstract] "the accuracy of state-of-the-art MLLMs, such as BLIP2, can decline by up to 46% when answering questions about small visual subjects compared to larger ones."
  - [section 3] "the accuracy of both BLIP-2 variants declines across the three groups as the answer bounding box becomes smaller."
  - [corpus] Weak - related papers focus on medical VQA and training-free methods, but do not directly address the mechanism.
- Break condition: If the region of interest is not correctly identified, cropping could remove crucial context and worsen performance.

### Mechanism 2
- Claim: Progressive cropping using CLIP similarity effectively identifies the region of interest.
- Mechanism: Iteratively crops the image from all sides at a fixed ratio and uses CLIP to score the cropped images against the question, selecting the crop with highest similarity.
- Core assumption: CLIP's text-image similarity score correlates with the relevance of a region to the question.
- Evidence anchors:
  - [section 4] "find the region of interest, given an image-question pair, we first crop the image from the four sides (top, bottom, left, and right) at a cropping ratio of 0.9 to produce four overlapping cropped images. We then use CLIP to assess the semantic similarity between these cropped images and the question."
  - [section 5] "clip-CROP achieving best improvement on FDVQA."
  - [corpus] Weak - related papers mention CLIP but do not specifically address its use for progressive cropping.
- Break condition: If CLIP's similarity score does not align with human perception of relevance, the cropping process may not converge on the correct region.

### Mechanism 3
- Claim: Combining original and cropped image tokens allows MLLMs to leverage both global context and local details.
- Mechanism: Extends the original image tokens by concatenating the visually cropped image tokens, providing the model with information from both perspectives.
- Core assumption: MLLMs can effectively attend to and integrate information from two separate sets of image tokens.
- Evidence anchors:
  - [section 4] "we utilize the fact that multimodal LLMs typically convert the image into a series of tokens. This allows us to directly extend the original image tokens by concatenating the visually cropped image tokens."
  - [section 5] "we observe that questions that require a global view of the image, i.e., localization and counting, become harder to answer as a result of visual cropping."
  - [corpus] Weak - related papers focus on different aspects of MLLM architecture and do not specifically address token concatenation.
- Break condition: If the model's attention mechanism cannot effectively integrate the two sets of tokens, performance may degrade, especially for questions requiring global context.

## Foundational Learning

- Concept: Visual Question Answering (VQA)
  - Why needed here: The paper's main focus is on improving zero-shot VQA performance by addressing the limitation of perceiving small visual details.
  - Quick check question: What is the difference between zero-shot and few-shot VQA?

- Concept: Multimodal Large Language Models (MLLMs)
  - Why needed here: The paper investigates the performance of MLLMs, specifically BLIP2, on zero-shot VQA tasks and proposes methods to improve their accuracy.
  - Quick check question: How do MLLMs process both image and text inputs?

- Concept: Visual Attention and Feature Maps
  - Why needed here: Understanding how MLLMs attend to different regions of an image is crucial for interpreting the effectiveness of visual cropping methods.
  - Quick check question: What is the role of attention mechanisms in MLLMs' visual processing?

## Architecture Onboarding

- Component map: Image and question -> Image encoder and Question encoder -> Visual cropping method (clip-CROP/yolo-CROP/sam-CROP) -> Token mapping layer -> LLM decoder -> Answer generation

- Critical path:
  1. Input image and question are processed by their respective encoders.
  2. Visual cropping method identifies the region of interest.
  3. The cropped image is encoded and its tokens are concatenated with the original image tokens.
  4. The concatenated tokens are mapped to the LLM's input format.
  5. The LLM generates an answer based on the combined visual and textual information.

- Design tradeoffs:
  - Accuracy vs. Inference Time: More complex cropping methods (e.g., sam-CROP) may yield better results but require more computational resources.
  - Global vs. Local Context: Combining original and cropped tokens aims to balance the need for global context and local detail perception.
  - Automatic vs. Human Cropping: While automatic methods are more scalable, they may not always match the accuracy of human-annotated crops.

- Failure signatures:
  - Incorrect region of interest identification leading to poor cropping.
  - Ineffective integration of original and cropped tokens resulting in performance degradation.
  - Over-reliance on cropping causing reduced accuracy for questions requiring global context.

- First 3 experiments:
  1. Evaluate the accuracy of BLIP2 on FD-VQA and TextVQA datasets without any cropping to establish a baseline.
  2. Apply clip-CROP, yolo-CROP, and sam-CROP to the same datasets and compare their accuracy gains.
  3. Analyze the performance of each cropping method across different question types (e.g., reading, object attributes, existence) to identify their strengths and weaknesses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the underlying cause of multimodal LLMs' reduced accuracy in perceiving small visual details compared to larger ones?
- Basis in paper: [explicit] The paper observes that the zero-shot accuracy of multimodal LLMs, such as BLIP2, declines by up to 46% when answering questions about small visual subjects compared to larger ones.
- Why unresolved: The paper does not provide a definitive explanation for this phenomenon, only noting that visual cropping can mitigate the issue.
- What evidence would resolve it: Experiments isolating and testing potential factors such as the resolution of visual encoding, attention mechanisms, or the model's capacity to process fine details could provide insights into the root cause.

### Open Question 2
- Question: How can visual cropping methods be improved to match the performance of human visual cropping in enhancing zero-shot VQA accuracy?
- Basis in paper: [explicit] The paper notes that while the proposed automatic visual cropping methods improve accuracy, they are still not as successful as human visual cropping.
- Why unresolved: The paper suggests that there is a need for better visual cropping methods but does not propose specific solutions or further research directions to achieve this goal.
- What evidence would resolve it: Comparative studies evaluating the effectiveness of different visual cropping strategies, including more advanced localization techniques or integration of human feedback, could identify approaches that bridge the gap with human performance.

### Open Question 3
- Question: How does the limitation in perceiving small visual details affect the performance of other multimodal LLMs beyond BLIP2?
- Basis in paper: [inferred] The paper focuses on BLIP2 models and suggests that the limitation might extend to other multimodal LLMs, but does not provide empirical evidence for this claim.
- Why unresolved: The paper does not conduct experiments with other multimodal LLMs to verify if they exhibit similar sensitivity to the size of visual subjects.
- What evidence would resolve it: Testing a variety of multimodal LLMs on the same tasks and datasets used in the study would reveal whether the limitation is a common issue across different models or specific to BLIP2.

## Limitations
- Evaluation is primarily conducted on BLIP2 models, which may not generalize to other multimodal architectures
- The effectiveness of cropping methods depends on the accuracy of external localization models (CLIP, YOLOv8, SAM)
- The study focuses on zero-shot performance without fine-tuning, limiting insights into adapted models
- Automatic cropping methods have an 8.76% accuracy gap compared to human visual cropping

## Confidence
- Medium: The empirical evidence from multiple datasets supports the core finding that MLLMs struggle with small visual subjects and that cropping can improve performance
- Medium: The observation that different cropping methods perform better on different datasets suggests that the relationship between cropping approach and question type is complex
- Low: The mechanisms underlying the improvement and optimal cropping strategies remain incompletely understood

## Next Checks
1. Test the proposed cropping methods across a broader range of multimodal architectures beyond BLIP2 to assess generalizability
2. Conduct ablation studies to determine the individual contributions of global context and local detail when combining original and cropped image tokens
3. Evaluate the cropping methods on domain-specific datasets (medical imaging, satellite imagery) where small detail perception is critical to assess robustness across visual domains