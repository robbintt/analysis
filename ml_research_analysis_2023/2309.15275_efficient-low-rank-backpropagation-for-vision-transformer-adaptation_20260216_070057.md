---
ver: rpa2
title: Efficient Low-rank Backpropagation for Vision Transformer Adaptation
arxiv_id: '2309.15275'
source_url: https://arxiv.org/abs/2309.15275
tags:
- lbp-wht
- low-rank
- training
- accuracy
- projection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational bottleneck of fine-tuning
  large Vision Transformers (ViT) for specific tasks, particularly on resource-constrained
  edge devices. The authors propose a novel Low-rank BackPropagation via Walsh-Hadamard
  Transformation (LBP-WHT) method that projects gradients into a low-rank space using
  Walsh-Hadamard Transform, performs matrix multiplications in this space, and then
  projects the results back.
---

# Efficient Low-rank Backpropagation for Vision Transformer Adaptation

## Quick Facts
- arXiv ID: 2309.15275
- Source URL: https://arxiv.org/abs/2309.15275
- Reference count: 40
- Key outcome: LBP-WHT achieves 10.4% higher accuracy than state-of-the-art baseline while requiring 9 MFLOPs less computation when adapting EfficientFormer-L1 on CIFAR100

## Executive Summary
This paper addresses the computational bottleneck of fine-tuning large Vision Transformers for specific tasks on resource-constrained edge devices. The authors propose a novel Low-rank BackPropagation via Walsh-Hadamard Transformation (LBP-WHT) method that projects gradients into a low-rank space using Walsh-Hadamard Transform, performs matrix multiplications in this space, and then projects the results back. Extensive experiments on multiple datasets and models demonstrate that LBP-WHT consistently outperforms baseline approaches in both accuracy and speed, offering a flexible trade-off between performance and computational cost by adjusting the rank of the low-rank approximation.

## Method Summary
LBP-WHT accelerates backpropagation by projecting gradients into a low-rank space where matrix multiplications are computationally cheaper. The method uses Walsh-Hadamard Transform to transform gradients into a lower-dimensional space, performs all subsequent matrix multiplications in this low-rank space, and then projects the results back using inverse WHT. The approach leverages the properties of WHT bases (containing only +1 and -1) to require only additions/subtractions with no multiplications, achieving O(n log n) complexity. Natural images' strong spatial locality concentrates gradient energy in low-frequency WHT bases, enabling accurate low-rank approximation while reducing computation.

## Key Results
- Achieves 10.4% higher accuracy than state-of-the-art baseline while requiring 9 MFLOPs less computation on EfficientFormer-L1 with CIFAR100
- Consistently outperforms baseline approaches (Full BP, LoRA, LoRA-all) across multiple datasets (CIFAR100, CIFAR10, Cars, Flowers, Food, Pets) and models (EfficientFormer, SwinV2)
- Offers flexible trade-off between performance and computational cost by adjusting the rank of low-rank approximation
- Provides significant computational savings on resource-constrained edge devices while maintaining or improving accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LBP-WHT accelerates backpropagation by projecting gradients into a low-rank space where matrix multiplications are computationally cheaper.
- **Mechanism:** The gradient with respect to the output is transformed using Walsh-Hadamard Transform into a lower-dimensional space. All subsequent matrix multiplications occur in this low-rank space, drastically reducing FLOPs from O(CxCyL) to O(CxCyr) where r << L.
- **Core assumption:** The projection preserves the most important information for training, and the inverse WHT can recover usable gradients without significant loss in accuracy.
- **Evidence anchors:** Abstract states "substantially reduces the computation needed," section 3.1 shows FLOP reduction when r << L, but no direct evidence validates WHT projection for ViT backpropagation.

### Mechanism 2
- **Claim:** WHT-based projections incur negligible overhead compared to the savings from low-rank multiplication.
- **Mechanism:** WHT bases contain only +1 and -1, so projections and reverse projections require only additions/subtractions, no multiplications. Fast WHT algorithms further reduce cost to O(n log n).
- **Core assumption:** The constant-factor savings from avoiding multiplications outweigh the O(n log n) projection cost.
- **Evidence anchors:** Section 3.1 states WHT requires only O(n log n) additions/subtractions and no multiplications, section 3.3 mentions minimal overhead for projections, but no direct evidence validates this for ViT gradients.

### Mechanism 3
- **Claim:** Natural images have strong spatial locality, which concentrates gradient energy in low-frequency WHT bases, enabling accurate low-rank approximation.
- **Mechanism:** By selecting low-frequency WHT bases (small index values), LBP-WHT captures most of the gradient energy, minimizing approximation error while reducing rank.
- **Core assumption:** The energy distribution of gradients in ViT layers mirrors that of natural images, i.e., low-frequency dominance.
- **Evidence anchors:** Section 3.2 mentions natural images' strong spatial locality and low-frequency components, section 4.5 shows WHT spectrum for gradient w.r.t. layer output with most energy in low-frequency area, but no direct evidence validates this assumption for ViT gradients.

## Foundational Learning

- **Concept:** Low-rank matrix approximation
  - **Why needed here:** Core to reducing computational cost by replacing full-rank matrix multiplications with lower-dimensional ones.
  - **Quick check question:** If you have a matrix of size 1000x1000, how many FLOPs are required for full multiplication vs. rank-10 approximation?

- **Concept:** Walsh-Hadamard Transform (WHT)
  - **Why needed here:** Provides an orthogonal, integer-only transform ideal for fast projection and reverse projection without multiplications.
  - **Quick check question:** What property of WHT bases ensures that the projection is invertible and energy-preserving?

- **Concept:** Backpropagation through linear layers
  - **Why needed here:** Understanding how gradients w.r.t. weights and inputs are computed via matrix multiplications is essential to see where savings occur.
  - **Quick check question:** In a linear layer y = x·wT, what are the shapes of gx and gw if x is (Cx,L) and y is (Cy,L)?

## Architecture Onboarding

- **Component map:** Input x: (Cx, L) → Weight w: (Cy, Cx) → Gradient gy: (Cy, L) → LBP-WHT → gx, gw
- **Critical path:** gy → WHT projection → low-rank matrix multiplication → inverse WHT → gx, gw
- **Design tradeoffs:**
  - Rank r: Higher r → better accuracy, more computation; lower r → faster, less accurate
  - Base selection: LPL1 (low-pass) vs. LHE (energy profiling); LPL1 cheaper but may be less optimal
- **Failure signatures:**
  - Accuracy collapses when r too small for task complexity
  - Overhead dominates when L small or r close to L
  - Training instability if WHT bases poorly chosen
- **First 3 experiments:**
  1. Verify computational savings: Run LBP-WHT vs. full BP on a small synthetic dataset, measure FLOPs and wall-clock time
  2. Rank sensitivity: Train on CIFAR100 with varying r (2,4,8,16), plot accuracy vs. computation
  3. Base selection comparison: Compare LPL1, LPL∞, and LHE on EfficientFormer-L1, measure both accuracy and profiling overhead

## Open Questions the Paper Calls Out
None explicitly stated in the provided text.

## Limitations
- **Limitation 1:** No empirical validation that WHT spectrum of ViT gradients mirrors that of natural images, which is fundamental to method's effectiveness
- **Limitation 2:** Computational savings claims depend heavily on specific input dimensions and rank choices, but paper doesn't provide systematic analysis across diverse model architectures
- **Limitation 3:** Core claims rely on untested assumptions about behavior of Walsh-Hadamard Transform in context of ViT backpropagation

## Confidence
- **High Confidence:** The mathematical formulation of LBP-WHT and its theoretical computational complexity reduction (O(CxCyr) vs O(CxCyL)) are sound and well-established
- **Medium Confidence:** The experimental results showing accuracy improvements and computational savings on benchmark datasets are promising, but limited to specific model-dataset combinations
- **Low Confidence:** The assumption that WHT projections preserve essential gradient information for training, and that this generalizes across diverse vision tasks and model architectures

## Next Checks
1. **WHT Spectrum Validation:** Profile the Walsh-Hadamard Transform spectrum of gradients across different ViT layers and datasets to empirically verify the low-frequency energy concentration assumption
2. **Cross-Architecture Generalization:** Test LBP-WHT on a broader range of ViT architectures (beyond EfficientFormer and SwinV2) and task types (detection, segmentation) to assess robustness
3. **Scalability Analysis:** Conduct systematic experiments varying input sequence lengths (L) and rank values (r) to determine when the method's overhead becomes prohibitive relative to savings