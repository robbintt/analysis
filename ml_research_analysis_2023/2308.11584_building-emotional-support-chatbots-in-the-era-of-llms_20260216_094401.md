---
ver: rpa2
title: Building Emotional Support Chatbots in the Era of LLMs
arxiv_id: '2308.11584'
source_url: https://arxiv.org/abs/2308.11584
tags:
- support
- emotional
- example
- dataset
- extes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of providing emotional support
  in conversational contexts by developing a large-scale emotional support dialogue
  dataset (ExTES) and fine-tuning LLaMA models using parameter-efficient techniques.
  The authors create the dataset by leveraging ChatGPT's in-context learning capabilities
  with human-curated seed dialogues across 36 diverse scenarios and 16 support strategies,
  followed by iterative human review.
---

# Building Emotional Support Chatbots in the Era of LLMs

## Quick Facts
- arXiv ID: 2308.11584
- Source URL: https://arxiv.org/abs/2308.11584
- Reference count: 29
- This paper develops ExTES, a large-scale emotional support dialogue dataset, and fine-tunes LLaMA models using parameter-efficient techniques to create effective emotional support chatbots.

## Executive Summary
This paper addresses the challenge of providing emotional support in conversational contexts by developing a large-scale emotional support dialogue dataset (ExTES) and fine-tuning LLaMA models using parameter-efficient techniques. The authors create the dataset by leveraging ChatGPT's in-context learning capabilities with human-curated seed dialogues across 36 diverse scenarios and 16 support strategies, followed by iterative human review. They explore LoRA and Adapter tuning methods on LLaMA, finding LoRA-tuning achieves the best performance. Automatic and human evaluations show the fine-tuned model outperforms baselines in emotional support tasks while maintaining low toxicity levels.

## Method Summary
The authors develop ExTES by first collecting seed dialogues from existing datasets and web sources, then using ChatGPT's in-context learning with a predefined template to generate expanded dialogues through self-chat. This process is iterated with human review, where only about 10% of conversations require adjustments. The resulting dataset contains 11,177 dialogues covering 36 scenarios and 16 support strategies. They then fine-tune LLaMA-7B using LoRA and Adapter techniques, comparing performance with DialoGPT baseline. The models are evaluated using automatic metrics (METEOR, BLEU, ROUGE-L, Vector Extrema, Distinct) and human evaluation on fluency, identification, comforting, suggestion, and overall preference, plus toxicity assessment using Perspective API.

## Key Results
- LoRA-tuned LLaMA model outperforms Adapter-tuned models and DialoGPT baseline on most automatic metrics for emotional support tasks
- ExTES dataset shows superior performance compared to existing emotional support datasets like ESConv
- Human evaluation confirms the fine-tuned model's effectiveness in providing emotional support while maintaining low toxicity levels

## Why This Works (Mechanism)

### Mechanism 1
ChatGPT's in-context learning enables effective generation of emotional support dialogues when provided with human-curated seed dialogues. By presenting ChatGPT with carefully designed seed dialogues that demonstrate proper emotional support strategies and scenarios, the model learns to generate contextually appropriate responses through few-shot learning. The core assumption is that ChatGPT's in-context learning capabilities are sufficient to capture the nuances of emotional support conversations without extensive fine-tuning.

### Mechanism 2
Parameter-efficient fine-tuning techniques like LoRA allow effective adaptation of large language models for emotional support tasks while maintaining efficiency. LoRA introduces low-rank matrices to approximate the weight updates needed for fine-tuning, reducing the number of trainable parameters while preserving model performance. The core assumption is that the intrinsic low-rank characteristics of large language models enable effective fine-tuning with significantly fewer parameters.

### Mechanism 3
Human review and iteration in the dataset collection process ensures quality while maintaining efficiency compared to traditional methods. By incorporating human review only when necessary (fewer than 10% of generated conversations require adjustments), the process balances quality control with efficiency. The core assumption is that the quality of ChatGPT-generated dialogues is generally high enough that minimal human intervention is required.

## Foundational Learning

- **In-context learning**: Understanding how ChatGPT can generate emotional support dialogues based on seed examples is crucial for grasping the dataset collection methodology. *Quick check: How does in-context learning differ from traditional fine-tuning in terms of data requirements and model adaptation?*

- **Parameter-efficient fine-tuning**: LoRA and Adapter techniques are central to adapting large models for specific tasks without full fine-tuning, which is key to the model optimization approach. *Quick check: What are the key differences between LoRA and Adapter techniques in terms of parameter efficiency and performance?*

- **Emotional support strategies**: The 16 emotional support strategies form the foundation of the dataset and guide the generation of appropriate responses. *Quick check: How do different emotional support strategies (e.g., Reflective Statements vs. Suggest Options) serve different purposes in a conversation?*

## Architecture Onboarding

- **Component map**: Seed dialogues -> ChatGPT API -> Iterative human review -> ExTES dataset -> LLaMA base model -> LoRA fine-tuning -> Evaluation

- **Critical path**: 1. Generate seed dialogues, 2. Use ChatGPT self-chat to expand dataset, 3. Apply human review and iteration, 4. Fine-tune LLaMA with LoRA, 5. Evaluate using both automatic and human metrics

- **Design tradeoffs**: Quality vs. efficiency in data collection (human review vs. fully automated), Parameter efficiency vs. model performance (LoRA vs. full fine-tuning), Strategy guidance vs. response diversity (strategies version vs. no-strategies version)

- **Failure signatures**: Low diversity in generated responses, High toxicity scores in generated dialogues, Poor performance on out-of-distribution scenarios

- **First 3 experiments**: 1. Compare LoRA vs. Adapter fine-tuning performance on ExTES, 2. Evaluate cross-dataset generalization by training on ExTES and testing on ESConv, 3. Assess the impact of strategy guidance by comparing strategies vs. no-strategies versions

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal balance between human curation and LLM generation in creating emotional support datasets? The paper discusses their iterative approach of using seed dialogues, ChatGPT generation, and human review, noting that "fewer than 10% of the generated conversations require human adjustments," but doesn't systematically explore how varying levels of human involvement affects dataset quality, model performance, or cost-effectiveness.

### Open Question 2
How do different emotional support strategies interact and combine effectively in multi-turn conversations? While the paper analyzes strategy transitions and notes "assistants usually first understand the cause of the user's distress and then say some words of comfort," it doesn't provide a comprehensive framework for understanding when and how to combine strategies for maximum effectiveness.

### Open Question 3
How generalizable are emotional support models trained on synthetic datasets like ExTES to real-world deployment? The paper notes their dataset was "synthesized" using LLMs and mentions this as a limitation, stating "it's essential to validate these findings through real-world user interactions and feedback."

## Limitations
- The study relies heavily on ChatGPT's in-context learning capabilities for dataset generation, introducing uncertainty about dataset quality and representativeness
- Limited evaluation on real-world emotional support scenarios or long-term conversation dynamics
- Insufficient details on toxicity assessment methodology and whether the Perspective API is appropriately calibrated for emotional support contexts

## Confidence

**High Confidence** in the technical implementation of LoRA fine-tuning and the comparative performance results between LoRA and Adapter methods.

**Medium Confidence** in the dataset quality and generalizability claims due to reliance on ChatGPT-generated content and limited human review.

**Low Confidence** in the toxicity assessment methodology, as the paper provides limited details about how toxicity manifests in emotional support contexts.

## Next Checks

1. **Dataset Quality Audit**: Conduct a systematic analysis of 100 randomly selected dialogues from ExTES to identify patterns in human adjustments and assess whether the 10% adjustment rate captures all quality issues.

2. **Cross-Domain Generalization Test**: Evaluate the fine-tuned models on established emotional support datasets (ESConv, ESConv-CK) and real-world emotional support forums to assess whether the models generalize beyond the ExTES training distribution.

3. **Long-term Interaction Analysis**: Design experiments to test model performance across multi-turn emotional support conversations spanning multiple sessions, as the current evaluation focuses on single-session dialogues.