---
ver: rpa2
title: 'ISLTranslate: Dataset for Translating Indian Sign Language'
arxiv_id: '2307.05440'
source_url: https://arxiv.org/abs/2307.05440
tags:
- sign
- language
- translation
- dataset
- isltranslate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ISLTranslate, a new translation dataset for
  continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase
  pairs. The dataset is created from publicly available educational videos produced
  by the Indian Sign Language Research and Training Centre (ISLRTC).
---

# ISLTranslate: Dataset for Translating Indian Sign Language

## Quick Facts
- arXiv ID: 2307.05440
- Source URL: https://arxiv.org/abs/2307.05440
- Reference count: 17
- Primary result: New dataset of 31k ISL-English pairs with BLEU-4 of 48.93 for human validation and 6.09 for baseline model

## Executive Summary
This paper introduces ISLTranslate, a new translation dataset for continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. The dataset is created from publicly available educational videos produced by the Indian Sign Language Research and Training Centre (ISLRTC). The videos cover NCERT standardized English educational content in ISL. The dataset is validated by comparing the English translations with those provided by a certified ISL signer, achieving a BLEU-4 score of 48.93 and ROUGE-L of 60.44. The authors also propose a baseline transformer-based model for ISL-to-English translation, which achieves a BLEU-4 score of 6.09, highlighting the challenging nature of the ISL translation task.

## Method Summary
The authors create ISLTranslate by extracting synchronized audio-voiceover text from educational videos and aligning it with corresponding sign segments. They use MediaPipe pose estimation to extract 3D skeletal coordinates from sign videos, then apply a transformer encoder-decoder architecture to generate English translations from these pose sequences. The model is trained using Adam optimizer with specific hyperparameters on an 80/10/10 train/validation/test split of the 31k pairs. Human validation is performed by comparing dataset translations against those of a certified ISL signer using standard metrics like BLEU and ROUGE.

## Key Results
- Dataset contains 31k ISL-English sentence/phrase pairs from NCERT educational content
- Human validation achieves BLEU-4 score of 48.93 and ROUGE-L of 60.44
- Baseline transformer model achieves BLEU-4 score of 6.09 on translation task
- Dataset split into 80% training (24,978 pairs), 10% validation (3,122 pairs), and 10% testing (3,122 pairs)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ISLTranslate dataset provides a foundational resource that enables training of sign language translation systems by pairing ISL video segments with English translations.
- Mechanism: By extracting synchronized audio-voiceover text from educational videos and aligning it with corresponding sign segments, the dataset creates a large-scale paired corpus for supervised learning of ISL-to-English translation.
- Core assumption: The alignment between audio transcriptions and sign segments is sufficiently accurate to provide valid training signal for translation models.
- Evidence anchors:
  - [abstract] "ISLTranslate, a translation dataset for continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase pairs"
  - [section] "We use the audio voice-over (by analyzing the speech and silence parts) to split the videos into multiple segments... We found automatically transcribed text to be of high quality"
  - [corpus] Weak evidence - only 5 related papers found, suggesting limited prior work in this specific area
- Break condition: If the alignment between audio and sign segments is systematically incorrect, the training data becomes unreliable and models cannot learn meaningful translation patterns.

### Mechanism 2
- Claim: The baseline transformer-based model can learn to translate ISL pose sequences to English text, establishing a performance reference for future work.
- Mechanism: The model uses Mediapipe pose estimation to extract 3D skeletal coordinates from sign videos, then applies a transformer encoder-decoder architecture to generate English translations from these pose sequences.
- Core assumption: Pose-based representations capture sufficient information about ISL signs for translation, even without direct visual appearance features.
- Evidence anchors:
  - [abstract] "We benchmark the created dataset with a transformer-based model for ISL translation... achieves a BLEU-4 score of 6.09"
  - [section] "We use the Mediapipe pose estimation pipeline... We normalize every frame's key points... We use standard BLEU and ROUGE scores to evaluate"
  - [corpus] Weak evidence - no direct mention of pose-based SLT in related papers
- Break condition: If pose features lack critical information present in full video appearance (like facial expressions or finger spelling), the model's translation performance will plateau below usable thresholds.

### Mechanism 3
- Claim: Human validation by certified ISL signers provides credibility for the dataset's translation quality, enabling trust in downstream research.
- Mechanism: A certified ISL instructor translates a random sample of videos, and the dataset's English translations are compared against these reference translations using standard metrics like BLEU and ROUGE.
- Core assumption: The certified signer's translations represent accurate ground truth for ISL content, despite potential variations in interpretation.
- Evidence anchors:
  - [abstract] "The dataset is validated by comparing the English translations with those provided by a certified ISL signer, achieving a BLEU-4 score of 48.93"
  - [section] "To verify the reliability of the sentence/phrase ISL-English pairs present in the dataset, we take the help of a certified ISL signer... We ask an ISL instructor to translate the videos"
  - [corpus] Weak evidence - no mention of human validation in related work
- Break condition: If the certified signer's interpretations differ significantly from the original audio content, the validation scores become misleading indicators of dataset quality.

## Foundational Learning

- Concept: Sign language modality and continuous communication
  - Why needed here: Understanding that ISL uses simultaneous hand shapes, facial expressions, and body movements (unlike sequential spoken languages) explains why translation is challenging and why the dataset structure matters
  - Quick check question: How does the parallel nature of sign language features differ from sequential spoken language processing?

- Concept: Machine translation evaluation metrics (BLEU, ROUGE, WER)
  - Why needed here: These metrics quantify translation quality and enable comparison between baseline models and future improvements
  - Quick check question: What does a BLEU-4 score of 6.09 versus 48.93 indicate about model vs human translation quality?

- Concept: Transformer architecture and encoder-decoder design
  - Why needed here: The baseline model uses transformer layers for sequence-to-sequence learning, requiring understanding of self-attention mechanisms and positional encoding
  - Quick check question: Why might a transformer be chosen over RNNs for sign language translation tasks?

## Architecture Onboarding

- Component map: Video preprocessing -> Face location detection -> Signer cropping -> MediaPipe pose estimation -> Normalization -> Transformer encoder -> Transformer decoder -> Text generation
- Critical path: Pose estimation accuracy -> Feature normalization -> Transformer training -> Translation output quality
- Design tradeoffs: Pose-based vs image-based features (speed vs accuracy), dataset size vs annotation cost, human validation vs automated alignment
- Failure signatures: Low BLEU scores (poor translation), high WER (word errors), inconsistent pose tracking (noisy features)
- First 3 experiments:
  1. Test pose estimation quality on sample videos to verify key point extraction works correctly
  2. Train a small transformer on a subset of data to validate the complete pipeline end-to-end
  3. Compare BLEU scores on validation set when using different pose normalization strategies

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following questions are implied by the content:

1. How does incorporating linguistic priors from ISL grammar and syntax impact the performance of translation models compared to purely data-driven approaches?
2. What is the optimal split ratio between training, validation, and test sets for maximizing model performance on ISLTranslate?
3. How does the quality of pose estimation impact translation performance compared to using raw video frames?

## Limitations
- Pose-based baseline model performs poorly (BLEU-4 of 6.09), suggesting insufficient representation of ISL features like facial expressions and finger spelling
- Human validation sample size is limited to only 50 random samples, which may not be representative of the entire dataset
- Paper does not address potential biases in the educational content or regional variations in ISL usage across India

## Confidence
- High confidence: The dataset creation methodology and basic statistics (31k pairs, 80/10/10 split) are well-documented and verifiable
- Medium confidence: The human validation results are meaningful but based on a small sample size that may not generalize
- Low confidence: The baseline model's poor performance may indicate fundamental limitations in the pose-based approach rather than just a need for more training data

## Next Checks
1. Validate pose feature completeness: Compare translation performance using pose features versus image-based features on a small subset to quantify information loss from pose-based representation
2. Expand human validation sampling: Increase the human validation sample size to 200-300 random samples and test for consistency across different content types and signers
3. Test cross-validation stability: Perform k-fold cross-validation on the dataset to assess whether the train/validation/test split performance differences are statistically significant or due to random variation in the split