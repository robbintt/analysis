---
ver: rpa2
title: Optimal Stochastic Non-smooth Non-convex Optimization through Online-to-Non-convex
  Conversion
arxiv_id: '2302.03775'
source_url: https://arxiv.org/abs/2302.03775
tags:
- have
- learning
- gradient
- stochastic
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper develops a novel online-to-non-convex conversion technique
  that reduces non-smooth non-convex optimization to online learning. The key insight
  is to use an online learning algorithm to predict update directions, using gradients
  evaluated at random points along the line segment between current and next iterates.
---

# Optimal Stochastic Non-smooth Non-convex Optimization through Online-to-Non-convex Conversion

## Quick Facts
- arXiv ID: 2302.03775
- Source URL: https://arxiv.org/abs/2302.03775
- Reference count: 40
- Primary result: Achieves optimal O(ε⁻³δ⁻¹) complexity for finding (δ,ε)-stationary points in non-smooth non-convex optimization

## Executive Summary
This paper introduces a novel online-to-non-convex conversion technique that reduces non-smooth non-convex optimization to online learning. By using an online learning algorithm to predict update directions and evaluating gradients at random points along line segments between iterates, the method achieves the optimal O(ε⁻³δ⁻¹) complexity for finding (δ,ε)-stationary points. The approach recovers optimal rates for smooth and second-order smooth objectives, extends to directional gradient oracles, and is supported by a matching lower bound.

## Method Summary
The method reduces non-smooth non-convex optimization to online learning by having an online algorithm predict update directions. At each iteration, gradients are evaluated at random points along the line segment between current and next iterates, enabling first-order analysis in non-smooth settings. The algorithm resets every T iterations and tunes parameters to ensure gradient averages satisfy the (δ,ε)-stationarity condition. This framework handles stochastic gradients with bounded variance and extends to directional gradient oracles for Lipschitz but non-differentiable functions.

## Key Results
- Achieves optimal O(ε⁻³δ⁻¹) complexity for finding (δ,ε)-stationary points
- Recovers optimal rates for smooth and second-order smooth objectives
- Extends to directional gradient oracles for Lipschitz but non-differentiable functions
- Proves matching lower bound establishing O(ε⁻³δ⁻¹) as optimal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The online-to-non-convex conversion reduces non-smooth non-convex optimization to online learning by using an online algorithm to predict update directions.
- Mechanism: An online learning algorithm A receives linear losses constructed from stochastic gradients evaluated at random points along the line segment between current and next iterates. The algorithm outputs update directions that minimize the regret, which directly translates to progress in the optimization problem.
- Core assumption: The objective function F is well-behaved, meaning it satisfies the integral condition F(y) - F(x) = ∫₀¹ ⟨∇F(x + t(y-x)), y-x⟩ dt.
- Evidence anchors:
  - [abstract] "Our primary technique is a reduction from non-smooth non-convex optimization to online learning"
  - [section] Theorem 7 establishes the key equality linking F(xM) to the regret of the online algorithm
  - [corpus] No direct evidence found in corpus neighbors
- Break condition: If F is not well-behaved, the fundamental equality linking function values to online regret breaks down, invalidating the entire reduction.

### Mechanism 2
- Claim: Randomized scaling of gradient evaluations enables first-order analysis in non-smooth settings.
- Mechanism: Instead of evaluating gradients at xn-1 or xn-1 + ∆n, gradients are evaluated at random points wn = xn-1 + sn∆n where sn ~ Uniform[0,1]. This randomization allows the well-behavedness condition to ensure F(xn) - F(xn-1) = E[⟨gn, ∆n⟩].
- Core assumption: The well-behavedness condition holds, enabling the integral representation of function differences.
- Evidence anchors:
  - [abstract] "using gradients evaluated at random points along the line segment between current and next iterates"
  - [section] Theorem 7 explicitly uses this randomized evaluation approach
  - [corpus] No direct evidence found in corpus neighbors
- Break condition: If the gradient oracle is not unbiased (E[Grad(x,z)] ≠ ∇F(x)), the expected inner product no longer equals the function difference.

### Mechanism 3
- Claim: Shifting regret bounds translate directly to (δ,ε)-stationarity guarantees through careful parameter tuning.
- Mechanism: By resetting the online algorithm every T iterations and choosing comparison vectors uk appropriately, the K-shifting regret bound ensures that the gradient averages over each block satisfy the (δ,ε)-stationarity condition.
- Core assumption: The online algorithm provides K-shifting regret bounds of the form E[RT(u1,...,uK)] ≤ DGK√T.
- Evidence anchors:
  - [abstract] "Our primary technique is a reduction from non-smooth non-convex optimization to online learning"
  - [section] Corollary 9 shows how OGD's regret bound leads to the O(ε⁻³δ⁻¹) complexity
  - [corpus] No direct evidence found in corpus neighbors
- Break condition: If the online algorithm cannot provide the required regret bounds, the complexity guarantee fails to hold.

## Foundational Learning

- Concept: Online convex optimization and regret minimization
  - Why needed here: The reduction technique fundamentally relies on viewing optimization as an online learning problem where the algorithm must predict future gradients
  - Quick check question: Can you explain how static regret relates to optimization progress in this context?

- Concept: Well-behaved functions and integral conditions
  - Why needed here: The well-behavedness assumption enables the key identity F(y) - F(x) = ∫₀¹ ⟨∇F(x+ t(y-x)), y-x⟩ dt, which is essential for the reduction
  - Quick check question: What does the well-behavedness condition mean geometrically for how the function changes along line segments?

- Concept: Stochastic gradient oracles and unbiased estimation
  - Why needed here: The algorithm relies on having access to unbiased estimates of gradients at various points, which is provided by the stochastic gradient oracle
  - Quick check question: How does the variance of the stochastic gradient oracle affect the final complexity bound?

## Architecture Onboarding

- Component map: Online learning algorithm A -> Update direction ∆n -> Next iterate xn = xn-1 + ∆n -> Random point wn = xn-1 + sn∆n -> Gradient oracle Grad(wn, zn) -> Loss gn -> Online algorithm A
- Critical path: The critical execution path is: get ∆n from A → compute xn = xn-1 + ∆n → compute wn = xn-1 + sn∆n → query Grad(wn, zn) → send gn to A. Any delay in this pipeline directly impacts convergence.
- Design tradeoffs: Using randomized gradient evaluation points provides robustness to non-smoothness but introduces additional variance. The choice between standard gradient oracles and directional gradient oracles affects the types of objectives that can be handled.
- Failure signatures: If the algorithm is not making progress, check whether the online learning algorithm is providing useful update directions, whether the randomization is working correctly, or whether the gradient oracle is providing unbiased estimates.
- First 3 experiments:
  1. Implement the basic reduction with OGD and a smooth convex test function to verify the mechanism works in the simpler setting
  2. Test with a non-smooth but well-behaved function (e.g., |x|) to verify the randomized evaluation approach handles non-smoothness
  3. Implement the directional gradient oracle variant and test on a Lipschitz but non-differentiable function to verify the generalization to directional derivatives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the O(ε⁻³δ⁻¹) complexity for finding (δ,ε)-stationary points be improved in the deterministic setting?
- Basis in paper: [explicit] The authors state that current best-known algorithm for deterministic non-smooth optimization still requires O(ε⁻³δ⁻¹) iterations, and ask if the O(ε⁻³/²δ⁻¹/²) complexity achieved in the smooth setting can also be achieved in the non-smooth setting.
- Why unresolved: The paper proves optimality of O(ε⁻³δ⁻¹) in the stochastic setting but leaves open whether deterministic case can be improved.
- What evidence would resolve it: An algorithm that finds (δ,ε)-stationary points in o(ε⁻³δ⁻¹) deterministic iterations, or a lower bound proving this is impossible.

### Open Question 2
- Question: Is it possible to improve convergence rates in the stochastic setting when σ → 0 (i.e., when variance decreases)?
- Basis in paper: [explicit] The authors note that their bounds do not appear to improve if σ → 0, and all σ dependency is dominated by the dependency on G. They highlight this as an interesting open question.
- Why unresolved: The analysis shows σ is dominated by G in complexity bounds, but it's unclear if this is fundamental or an artifact of the analysis.
- What evidence would resolve it: An algorithm with complexity bounds that explicitly improve as σ decreases, or a lower bound showing such improvement is impossible.

### Open Question 3
- Question: Can more refined online learning techniques (beyond switching regret) lead to improved non-convex optimization algorithms?
- Basis in paper: [inferred] The authors suggest their online-to-non-convex conversion technique opens new avenues for algorithm design and motivate new research directions in online learning, mentioning strongly adaptive or dynamic regret as possibilities.
- Why unresolved: The paper only uses basic online learning algorithms (OGD, optimistic OGD) with switching regret, leaving unexplored whether more advanced online learning techniques could yield better results.
- What evidence would resolve it: Development and analysis of non-convex optimization algorithms based on advanced online learning techniques like strongly adaptive or dynamic regret that achieve better complexity bounds.

## Limitations
- The theoretical framework relies heavily on the "well-behaved" assumption, which may not hold for many practical non-smooth functions.
- The randomized gradient evaluation introduces additional variance that could impact practical performance.
- The complexity bound assumes ideal conditions with access to unbiased stochastic gradients with bounded variance, which may not be realistic in all applications.

## Confidence

- **High Confidence**: The reduction framework from non-smooth non-convex optimization to online learning is theoretically sound and the O(ε⁻³δ⁻¹) complexity bound is rigorously proven.
- **Medium Confidence**: The extension to directional gradient oracles and the application to second-order smooth objectives, while theoretically justified, may face practical implementation challenges.
- **Low Confidence**: The practical performance and robustness of the algorithm on real-world non-smooth non-convex problems remains to be validated empirically.

## Next Checks

1. **Empirical Validation**: Implement and test the algorithm on benchmark non-smooth non-convex problems (e.g., L1-regularized objectives, piecewise linear functions) to verify the theoretical complexity bound holds in practice and to measure the impact of variance introduced by randomized gradient evaluation.

2. **Well-behavedness Verification**: Systematically characterize which common non-smooth non-convex functions satisfy the well-behavedness condition, and identify practical scenarios where the assumption breaks down. This will clarify the algorithm's domain of applicability.

3. **Comparison to Alternatives**: Benchmark the proposed algorithm against other state-of-the-art methods for non-smooth non-convex optimization (e.g., stochastic subgradient methods, variance-reduced methods) to quantify the practical benefits of the online-to-non-convex conversion approach.