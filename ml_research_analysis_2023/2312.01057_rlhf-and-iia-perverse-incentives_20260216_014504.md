---
ver: rpa2
title: 'RLHF and IIA: Perverse Incentives'
arxiv_id: '2312.01057'
source_url: https://arxiv.org/abs/2312.01057
tags:
- messages
- choice
- language
- reward
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a fundamental flaw in reinforcement learning
  from human feedback (RLHF) algorithms: they assume independence of irrelevant alternatives
  (IIA), which does not align with human preferences. The authors demonstrate that
  this assumption leads to perverse incentives, causing algorithms like RLPO and DPO
  to generate verbose messages when concise ones are preferred, and IL to bias toward
  longer messages as the set of equivalent alternatives grows.'
---

# RLHF and Reinforcement Learning from Human Feedback
## Quick Facts
- **arXiv ID**: 2312.01057
- **Source URL**: https://arxiv.org/abs/2312.01057
- **Reference count**: 18
- **Primary result**: RLHF algorithms assuming IIA generate verbose messages when concise ones are preferred

## Executive Summary
This paper identifies a fundamental flaw in reinforcement learning from human feedback (RLHF) algorithms: they assume independence of irrelevant alternatives (IIA), which does not align with human preferences. The authors demonstrate that this assumption leads to perverse incentives, causing algorithms like RLPO and DPO to generate verbose messages when concise ones are preferred, and IL to bias toward longer messages as the set of equivalent alternatives grows. The paper provides theoretical proofs and simulations to support these findings, highlighting the need for new algorithms that do not rely on the IIA assumption.

## Method Summary
The paper combines theoretical analysis with simulations to demonstrate how RLHF algorithms fail under IIA violations. It uses a dichotomy data model where messages come from two categories (concise and verbose) with different individual preferences. The analysis covers three algorithms: RLPO, DPO, and IL, showing how each fails when choice sets contain more than two alternatives or when the verbose message corpus grows. The theoretical framework uses logit choice models and linear reward functions to prove the perverse incentives, while simulations demonstrate these effects in practice.

## Key Results
- RLPO and DPO fail to generate desired messages when choice sets contain more than two alternatives due to IIA assumption
- IL generates verbose messages when concise ones are preferred because longer messages have more equivalent alternatives
- The reward model learned from preference data is biased toward the majority category even when less preferred by individuals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLHF algorithms based on logit models assume IIA, leading to reward estimates that ignore the impact of irrelevant alternatives
- Mechanism: When a reward model assumes IIA but human preferences do not, the presence of equivalent alternatives distorts the estimated rewards. For example, adding "feline" as an alternative reduces the estimated probability of preferring "dog" even though "feline" is semantically equivalent to "cat"
- Core assumption: Human preferences for text content violate IIA due to the existence of semantically equivalent alternatives
- Evidence anchors:
  - [abstract] "This paper identifies a fundamental flaw in reinforcement learning from human feedback (RLHF) algorithms: they assume independence of irrelevant alternatives (IIA), which does not align with human preferences"
  - [section 3.7] "Human preferences for text messages do not satisfy IIA. This is because, for any given message, others can be virtually equivalent"
  - [corpus] Weak - no direct corpus evidence found, but related papers mention "IIA assumption" and "perverse incentives"

### Mechanism 2
- Claim: RLPO and DPO fail to generate desired messages when choice sets contain more than two alternatives
- Mechanism: The reward model learned from preference data is biased toward the majority category, even when it's less preferred by individuals. This occurs because the proportion of majority-category messages in choice sets exceeds their selection probability
- Core assumption: Data generating process creates choice sets where majority-category messages are overrepresented relative to their selection probability
- Evidence anchors:
  - [section 5.4] "With larger choice sets, the language models consistently generate elements of M2" and "RLPO and DPO are designed to produce language models that generate desired messages"
  - [section 5.4] "As the data set grows, minimizing Lreward(brψ) identifies parameters bψ to match these probabilities, if possible"
  - [corpus] Weak - no direct corpus evidence found, but related papers mention "reward modeling" and "policy optimization"

### Mechanism 3
- Claim: IL generates verbose messages when concise ones are preferred due to IIA violation
- Mechanism: IL aims to match human choice probabilities but the reward (log-probability) is influenced by the number of equivalent alternatives. More verbose expressions of an idea have more equivalent alternatives, making them more likely to be generated
- Core assumption: The number of ways to express information scales quickly with message length
- Evidence anchors:
  - [section 5.5] "Because there are so many more thousand word than ten word statements, there are likely to be many more roughly equivalent expressions of the second idea than the first"
  - [section 5.5] "IL biases generation toward messages with many equivalent alternatives, the fine-tuned language model would tend to express the second idea"
  - [corpus] Weak - no direct corpus evidence found, but related papers mention "inclusive learning" and "verbose messages"

## Foundational Learning

- Concept: Independence of Irrelevant Alternatives (IIA)
  - Why needed here: IIA is the core assumption that leads to perverse incentives in RLHF algorithms
  - Quick check question: If adding an equivalent alternative to a choice set doesn't change the relative preferences between existing alternatives, does the model satisfy IIA?

- Concept: Logit choice models
  - Why needed here: RLHF algorithms use logit models that assume IIA, which is problematic for human preferences
  - Quick check question: In a logit model, how does the probability of choosing an alternative change when a semantically equivalent alternative is added?

- Concept: Reward modeling and policy optimization
  - Why needed here: RLHF algorithms first learn a reward model from preference data, then optimize a policy to maximize reward
  - Quick check question: What is the relationship between the reward model learned from preference data and the policy that generates messages?

## Architecture Onboarding

- Component map: Base language model → Preference data collection → Reward model training → Policy optimization
- Critical path: Collect preference data → Train reward model → Optimize policy → Generate aligned messages
- Design tradeoffs: IIA assumption simplifies reward modeling but leads to perverse incentives; pairwise comparisons avoid IIA issues but may be less efficient
- Failure signatures: Verbose messages when concise ones are preferred; generation of less-preferred message categories; reward estimates biased by irrelevant alternatives
- First 3 experiments:
  1. Implement RLPO with pairwise comparisons and verify it generates preferred messages
  2. Implement RLPO with choice sets of size 3+ and observe perverse incentives
  3. Implement IL and measure the correlation between message length and generation probability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RLHF algorithms be designed that do not rely on the IIA assumption while still being practical and scalable?
- Basis in paper: [explicit] The paper discusses the fundamental flaw in current RLHF algorithms based on the IIA assumption and mentions that designing algorithms that mitigate these perverse incentives remains a subject for future research.
- Why unresolved: Current RLHF algorithms like RLPO, DPO, and IL all assume IIA, leading to undesirable behaviors. Creating new algorithms that avoid this assumption while maintaining effectiveness and scalability is a significant challenge.
- What evidence would resolve it: Successful implementation and testing of new RLHF algorithms that demonstrate improved performance without relying on the IIA assumption, validated through both theoretical analysis and empirical results.

### Open Question 2
- Question: How does the bias toward verbose messages in IL relate to other factors influencing message length in RLHF?
- Basis in paper: [explicit] The paper suggests that IL's bias toward verbose messages due to IIA may be related to findings in recent work that also identifies factors contributing to longer messages in RLHF.
- Why unresolved: While the paper identifies IIA as a potential cause of verbose message generation, it acknowledges that other factors may also play a role. Understanding the interplay between these factors is necessary for a comprehensive solution.
- What evidence would resolve it: Detailed empirical studies comparing the effects of IIA and other identified factors on message length in RLHF, possibly through controlled experiments or ablation studies.

### Open Question 3
- Question: What are the trade-offs between different query formats (e.g., pairwise vs. multi-alternative) in RLHF, and how do they impact algorithm performance and bias?
- Basis in paper: [explicit] The paper discusses how different query formats can induce different behaviors in RLHF algorithms, with larger choice sets leading to more pronounced issues with IIA-based algorithms.
- Why unresolved: While the paper highlights the impact of query format on algorithm behavior, it does not provide a comprehensive analysis of the trade-offs between different formats or how they might be optimized for specific tasks or preferences.
- What evidence would resolve it: Systematic comparison of RLHF algorithms across different query formats, evaluating their performance, bias, and robustness to different preference distributions and task domains.

## Limitations

- All empirical validation comes from simulations using synthetic preference data rather than real human preference datasets
- The strength of perverse incentives in practical applications may be attenuated by other factors in real-world RLHF implementations
- The claim about IL's bias toward verbose messages depends on the assumption that message length correlates strongly with the number of equivalent alternatives

## Confidence

- **High**: Core theoretical claims about IIA violations in RLHF algorithms - mathematical proofs are rigorous
- **Medium**: Strength of perverse incentives in practical applications - may be affected by other real-world factors
- **Medium**: IL's specific bias toward verbose messages - depends on assumptions about semantic equivalence scaling

## Next Checks

1. Apply the identified theoretical concerns to a real-world RLHF dataset with human preferences to measure the actual magnitude of IIA violations and their impact on reward model training.

2. Implement and test RLHF variants that explicitly avoid the IIA assumption (such as pairwise comparison-based approaches) and compare their performance against standard RLPO/DPO on the same tasks.

3. Develop metrics to quantify the degree of semantic equivalence between alternatives in real preference data and measure how this correlates with the observed biases in reward estimation and policy optimization.