---
ver: rpa2
title: Dissecting Multimodality in VideoQA Transformer Models by Impairing Modality
  Fusion
arxiv_id: '2306.08889'
source_url: https://arxiv.org/abs/2306.08889
tags:
- video
- multimodal
- performance
- question
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether video question answering (VideoQA)
  transformer models are truly learning multimodal representations or merely exploiting
  biases and shortcuts. To probe this, the authors propose QUAG (QUadrant AveraGe),
  a non-parametric method that systematically impairs modality fusion in pretrained
  models by averaging attention weights across quadrants.
---

# Dissecting Multimodality in VideoQA Transformer Models by Impairing Modality Fusion

## Quick Facts
- arXiv ID: 2306.08889
- Source URL: https://arxiv.org/abs/2306.08889
- Reference count: 40
- Key outcome: VideoQA transformer models maintain high accuracy even under multimodal sub-optimality, suggesting performance is not indicative of genuine multimodal understanding

## Executive Summary
This paper investigates whether video question answering (VideoQA) transformer models are truly learning multimodal representations or merely exploiting biases and shortcuts. The authors propose QUAG (QUadrant AveraGe), a non-parametric method that systematically impairs modality fusion in pretrained models by averaging attention weights across quadrants. They also introduce QUAG-attention, a simpler attention variant that approximates these operations, achieving similar performance with fewer multiplications. Experiments on ActivityNet-QA, MSRVTT-QA, and NeXT-QA show that models maintain high accuracy even under multimodal sub-optimality, suggesting performance is not indicative of genuine multimodal understanding. To address this, the authors create CLAVI (Complements in LAnguage and VIdeo), a diagnostic benchmark with balanced temporal counterfactuals in both video and question domains.

## Method Summary
The authors propose QUAG as a lightweight probe to impair modality fusion by systematically block-averaging attention weights across quadrants. They also introduce QUAG-attention, a less-expressive self-attention variant that approximates QUAG operations with significantly fewer multiplications. Additionally, they create CLAVI, a diagnostic benchmark containing balanced temporal counterfactuals in both question and video domains to accurately test multimodal understanding. The method involves applying QUAG to pretrained VideoQA models, implementing QUAG-attention as a replacement for standard attention, and evaluating models on CLAVI to assess their ability to handle counterfactual instances requiring genuine temporal understanding.

## Key Results
- Models maintain high accuracy on standard benchmarks even under QUAG impairment, indicating reliance on shortcuts rather than multimodal understanding
- QUAG-attention achieves similar performance to full attention with significantly fewer multiplication operations
- Models achieve high performance on shortcut instances in CLAVI but fail on counterfactuals requiring true temporal understanding
- Consistent accuracies (CAccV, CAccT) reveal models' inability to jointly understand temporal cues in video and language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QUAG systematically impairs multimodal fusion by block-averaging attention weights to force uniform attention across tokens
- Mechanism: QUAG operates on attention matrices at inference time by replacing specific quadrants with their row-wise averages, effectively reducing cross-modal or within-modal attention interactions
- Core assumption: Multimodal understanding emerges from progressive token-level attention patterns in fusion layers
- Evidence anchors: [abstract]: "QUAG (QUadrant AveraGe), a lightweight and non-parametric probe, to conduct dataset-model combined representation analysis by impairing modality fusion"; [section]: "QUAG impairs components of modality fusion by systematic block-averaging of attention weights"
- Break condition: If the model relies on fixed positional encodings rather than learned attention patterns for multimodal understanding

### Mechanism 2
- Claim: QUAG-attention approximates QUAG operations with significantly fewer multiplication operations while maintaining performance
- Mechanism: Replaces full attention computation with averaged representations for specific modalities, reducing effective token count from (LV + LT) to (1 + LT) or (LV + 1) depending on modality
- Core assumption: Performance can be maintained when multimodal interactions are simplified to uniform attention patterns
- Evidence anchors: [abstract]: "QUAG-attention, a less-expressive replacement of self-attention with restricted token interactions. Models with QUAG-attention achieve similar performance with significantly fewer multiplication operations without any finetuning"; [section]: "QUAG-attention augmented models manage to maintain the high performance on standard benchmarks"
- Break condition: If performance drops significantly when modality interactions are simplified, indicating genuine multimodal dependencies

### Mechanism 3
- Claim: CLAVI creates a diagnostic benchmark with balanced temporal counterfactuals to identify genuine multimodal understanding
- Mechanism: Generates synthetic video-question pairs with swapped action segments and temporal question negations, creating balanced "yes"/"no" instances that require joint temporal understanding
- Core assumption: Current benchmarks contain temporal shortcuts that can be exploited without genuine multimodal reasoning
- Evidence anchors: [abstract]: "CLAVI (Counterfactual in LAnguage and VIdeo) benchmark, a diagnostic dataset for benchmarking coupled multimodal understanding in VideoQA through counterfactuals"; [section]: "CLA VI contains balanced temporal counterfactuals in both question and video domains to accurately test if the models can jointly understand temporal cues"
- Break condition: If models achieve high performance on CLAVI without demonstrating temporal understanding capabilities

## Foundational Learning

- Concept: Attention mechanism in transformers
  - Why needed here: QUAG operates directly on attention matrices by block-averaging quadrants
  - Quick check question: What does the attention matrix represent in a transformer layer?

- Concept: Multimodal fusion in video-language models
  - Why needed here: The paper investigates whether models truly learn multimodal representations or exploit biases
  - Quick check question: How do transformers typically combine visual and textual information?

- Concept: Counterfactual reasoning in benchmarks
  - Why needed here: CLAVI uses counterfactual instances to test genuine understanding vs. shortcut exploitation
  - Quick check question: Why are balanced counterfactuals important for evaluating true understanding?

## Architecture Onboarding

- Component map:
  Video encoder → Text encoder → Multimodal fusion blocks → Answer prediction
  QUAG operates on fusion blocks' attention matrices
  QUAG-attention replaces standard attention with averaged representations

- Critical path: Video/text encoding → Multimodal fusion → Answer prediction
  QUAG and QUAG-attention modify the fusion stage
  CLAVI tests the entire pipeline's temporal understanding

- Design tradeoffs:
  QUAG: Systematic ablation vs. performance preservation
  QUAG-attention: Computational efficiency vs. representational power
  CLAVI: Diagnostic precision vs. real-world complexity

- Failure signatures:
  QUAG: Minimal performance drop indicates reliance on shortcuts
  QUAG-attention: Performance maintained despite reduced computation suggests suboptimal learning
  CLAVI: High performance on control instances but low on counterfactuals indicates bias exploitation

- First 3 experiments:
  1. Apply QUAG with [VV, TT] quadrants to test unimodal vs. crossmodal dependencies
  2. Implement QUAG-attention for video modality and measure performance/operations
  3. Fine-tune models on CLAVI and evaluate consistent accuracies across control/counterfactual subsets

## Open Questions the Paper Calls Out

- Question: Does the QUAG method generalize to other multimodal tasks beyond VideoQA, such as image captioning or visual reasoning?
- Basis in paper: [explicit] The authors state "Our methods of probing the multimodal interactions and diagnosing through counterfactuals are generic and can be extended to other multimodal tasks to get valuable insights."
- Why unresolved: The paper only evaluates QUAG on VideoQA models. No experiments or analysis are provided for other multimodal tasks.
- What evidence would resolve it: Empirical evaluation of QUAG on other multimodal tasks like image captioning or visual reasoning, demonstrating similar insights into multimodal understanding.

## Limitations
- QUAG's effectiveness depends on exact implementation details of quadrant averaging and padding handling
- CLAVI's diagnostic power relies on careful construction of counterfactual instances and balanced sampling
- Findings may not generalize across different model architectures and datasets without extensive validation

## Confidence
- High Confidence: Performance maintenance under QUAG operations indicates shortcut exploitation
- Medium Confidence: QUAG-attention achieves similar performance with fewer operations due to systematic approximation of multimodal interactions
- Medium Confidence: CLAVI's balanced counterfactual design effectively distinguishes genuine multimodal understanding from bias exploitation

## Next Checks
1. Recreate QUAG with different padding strategies and quadrant boundary definitions to test sensitivity of performance drops across models and datasets
2. Apply QUAG-attention to additional VideoQA architectures beyond JustAsk and FrozenBiLM to verify consistent computational efficiency gains
3. Generate multiple CLAVI variants with varying temporal extension thresholds and negative action pools to assess stability of consistent accuracy metrics