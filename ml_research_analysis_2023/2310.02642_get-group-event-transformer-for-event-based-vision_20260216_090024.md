---
ver: rpa2
title: 'GET: Group Event Transformer for Event-Based Vision'
arxiv_id: '2310.02642'
source_url: https://arxiv.org/abs/2310.02642
tags:
- event
- group
- vision
- dataset
- event-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'GET addresses the problem of extracting effective features from
  asynchronous event data by decoupling temporal-polarity information from spatial
  information throughout the feature extraction process. The core method involves
  a new event representation called Group Token, which groups events based on timestamps
  and polarities, and two key modules: Event Dual Self-Attention block for feature
  communication in spatial and temporal-polarity domains, and Group Token Aggregation
  for information integration.'
---

# GET: Group Event Transformer for Event-Based Vision

## Quick Facts
- arXiv ID: 2310.02642
- Source URL: https://arxiv.org/abs/2310.02642
- Reference count: 40
- Key outcome: GET achieves state-of-the-art performance on four event-based classification datasets (84.8% Cifar10-DVS, 99.7% N-MNIST, 96.7% N-CARS, 97.9% DVS128Gesture) and two object detection datasets (47.9% mAP Gen1, 48.4% mAP 1Mpx)

## Executive Summary
GET introduces a novel approach to event-based vision by decoupling temporal-polarity information from spatial information during feature extraction. The method employs Group Tokens to partition events by timestamps and polarities, then uses Event Dual Self-Attention blocks to extract features separately in spatial and temporal-polarity domains before integrating them with Group Token Aggregation. This architecture achieves state-of-the-art performance across multiple event-based vision tasks while improving computational efficiency through a more effective event representation.

## Method Summary
GET processes asynchronous event streams by first converting them into Group Tokens through discretization by time and polarity, followed by spatial patch encoding using weighted binning and convolution. The Event Dual Self-Attention (EDSA) blocks extract features in both spatial and temporal-polarity domains using window partitioning with dual residual connections. Group Token Aggregation (GTA) modules integrate information across stages using overlapping group convolution with specific kernel and stride parameters. The architecture maintains group correlations throughout processing while gradually increasing receptive fields, achieving superior performance with improved parameter efficiency.

## Key Results
- State-of-the-art classification accuracy: 84.8% on Cifar10-DVS, 99.7% on N-MNIST, 96.7% on N-CARS, 97.9% on DVS128Gesture
- State-of-the-art object detection: 47.9% mAP on Gen1, 48.4% mAP on 1Mpx
- 51% reduction in runtime for Group Token generation compared to traditional preprocessing methods
- 0.2M parameter reduction and 1.3ms speed improvement through efficient architecture design

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Temporal-Polarity from Spatial Information
GET achieves state-of-the-art performance by decoupling temporal-polarity information from spatial information during feature extraction. The method introduces Group Tokens that partition events by timestamps and polarities, then uses Event Dual Self-Attention (EDSA) blocks to extract features separately in spatial and temporal-polarity domains before integrating them. This decoupling prevents interference between domains that degrades performance when fused too early.

### Mechanism 2: Efficient Group Token Representation
Group Token representation improves efficiency and effectiveness compared to traditional event representations. Events are discretized by time and polarity, then encoded into Group Tokens using weighted binning and convolution, reducing preprocessing time from seconds to milliseconds per 108 events. This discretization preserves temporal-polarity information while enabling efficient processing.

### Mechanism 3: Overlapping Group Convolution Integration
Overlapping group convolution in GTA effectively integrates and decouples information across spatial and temporal-polarity domains. GTA uses overlapping group convolution with specific kernel and stride parameters to gradually increase receptive fields in both domains while maintaining group correlations. This integration method outperforms standard approaches that destroy group correlations during downsampling.

## Foundational Learning

- **Event camera data representation**: Event cameras encode illumination changes as asynchronous events containing location, polarity, and timestamp. Standard CNNs cannot process this data directly because events are sparse and asynchronous rather than dense frames.
  - Quick check: How does an event camera encode illumination changes, and why can't standard CNNs process this data directly?

- **Transformer architecture and self-attention**: GET is built on Transformer principles, requiring understanding of how self-attention works in vision contexts. Standard self-attention has O(N²) complexity, which GET addresses through window partitioning.
  - Quick check: What is the computational complexity of standard self-attention, and how does GET address this through window partitioning?

- **Group convolution and overlapping patterns**: GTA uses overlapping group convolution, which requires understanding of how group convolutions work and their benefits. Standard group convolution reduces computation but can lose cross-group information.
  - Quick check: How does overlapping group convolution differ from standard group convolution, and what advantages does it provide for temporal-polarity information integration?

## Architecture Onboarding

- **Component map**: Raw events → Group Token Embedding (GTE) → Event Dual Self-Attention (EDSA) blocks → Group Token Aggregation (GTA) → Task-specific heads

- **Critical path**: Raw events → GTE → Group Tokens → EDSA blocks → Refined features → GTA (between stages) → Integrated tokens → Final features → Classification/Detection heads

- **Design tradeoffs**: Window size in EDSA affects spatial resolution vs. computational efficiency; group number G affects temporal-polarity granularity vs. model complexity; overlapping group convolution parameters balance integration quality vs. computational cost

- **Failure signatures**: Poor classification accuracy suggests issues with Group Token discretization or EDSA configuration; slow runtime indicates inefficient window partitioning or excessive overlap in GTA; memory issues may result from too many groups or insufficient window partitioning

- **First 3 experiments**:
  1. Test GET on N-MNIST with varying group numbers G to find optimal temporal-polarity granularity
  2. Compare EDSA vs. standard self-attention on Cifar10-DVS to validate the dual residual connection design
  3. Benchmark GTA with different overlap parameters on 1Mpx to optimize detection performance vs. speed tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
How does GET's single-pass inference performance compare to SNN-based methods on DVS128Gesture, and what specific architectural features enable this advantage? The paper shows GET achieves 97.9% accuracy with single-pass inference versus SNN methods requiring iterative processing, but lacks detailed architectural comparisons.

### Open Question 2
What is the optimal group number (G) for different event-based datasets, and how does G affect the trade-off between computational efficiency and accuracy? The paper presents results for limited G values on specific datasets without systematic analysis across diverse dataset characteristics.

### Open Question 3
How does GET's performance scale to event-based datasets with extreme temporal resolutions? The paper demonstrates effectiveness on datasets with moderate temporal characteristics but doesn't explore the limits of its temporal modeling capabilities or needed architectural modifications for extreme temporal sparsity or density.

## Limitations

- **Temporal discretization sensitivity**: Performance heavily depends on time interval discretization, with no clear guidance on optimal K values for different datasets
- **Hyperparameter dataset specificity**: Multiple hyperparameters appear dataset-specific, suggesting limited generalizability across different event camera characteristics
- **Computational complexity trade-offs**: While claiming efficiency improvements, the added complexity of EDSA and GTA modules requires verification of actual inference-time benefits

## Confidence

**High Confidence**: State-of-the-art performance on tested datasets (84.8% Cifar10-DVS, 99.7% N-MNIST, 96.7% N-CARS, 97.9% DVS128Gesture, 47.9% mAP Gen1, 48.4% mAP 1Mpx); Group Token representation provides computational efficiency improvements; Decoupling temporal-polarity information improves feature extraction quality

**Medium Confidence**: Superiority of EDSA blocks over standard self-attention through ablation studies; GTA's overlapping group convolution provides optimal integration, though parameters may be suboptimal for some applications

**Low Confidence**: Parameter efficiency claims (0.2M reduction) and speed improvements (1.3ms reduction) require verification on diverse hardware; Generalization claims to other event-based vision tasks need further validation

## Next Checks

1. **Temporal Resolution Sensitivity Analysis**: Systematically evaluate GET's performance across a range of K values (time intervals) on N-MNIST to identify the optimal temporal discretization that balances accuracy and computational efficiency.

2. **Cross-Dataset Generalization Test**: Train GET on one classification dataset (e.g., N-MNIST) and evaluate on another (e.g., Cifar10-DVS) with minimal fine-tuning to assess generalization of temporal-polarity feature extraction.

3. **Computational Complexity Benchmarking**: Measure actual inference time and memory usage of GET versus baseline methods on the same hardware platform across all tested datasets to verify claimed efficiency improvements and identify hidden computational costs.