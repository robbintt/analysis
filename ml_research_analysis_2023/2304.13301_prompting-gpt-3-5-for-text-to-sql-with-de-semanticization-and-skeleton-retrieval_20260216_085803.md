---
ver: rpa2
title: Prompting GPT-3.5 for Text-to-SQL with De-semanticization and Skeleton Retrieval
arxiv_id: '2304.13301'
source_url: https://arxiv.org/abs/2304.13301
tags:
- schema
- text-to-sql
- question
- which
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Case-Based Reasoning framework, CBR-ApSQL,
  for improving cross-domain Text-to-SQL tasks using large language models (LLMs)
  like GPT-3.5. The framework addresses limitations of current LLM prompting methods,
  which often use simplistic or random sampling approaches, leading to poor performance
  and irrelevant outputs.
---

# Prompting GPT-3.5 for Text-to-SQL with De-semanticization and Skeleton Retrieval

## Quick Facts
- arXiv ID: 2304.13301
- Source URL: https://arxiv.org/abs/2304.13301
- Reference count: 40
- Key outcome: CBR-ApSQL framework improves cross-domain Text-to-SQL performance using de-semanticization and adaptive schema retrieval with GPT-3.5

## Executive Summary
This paper introduces CBR-ApSQL, a Case-Based Reasoning framework that improves Text-to-SQL performance by retrieving and reusing similar demonstration examples when prompting large language models. The framework addresses the limitations of current LLM prompting methods that rely on simplistic or random sampling approaches. By extracting question skeletons through de-semanticization and retrieving structurally similar examples, CBR-ApSQL provides more relevant demonstrations to guide SQL generation. The framework also adapts the range of database schema in prompts to balance informativeness with prompt length, incorporating a fallback mechanism for handling generation failures.

## Method Summary
CBR-ApSQL combines Case-Based Reasoning with GPT-3.5 for cross-domain Text-to-SQL tasks. The framework uses a de-semanticization mechanism to extract question skeletons, enabling retrieval of structurally similar examples through k-NN search. It employs the Semantic Domain Relevance Evaluator (SDRE) with Poincaré detector, TextAlign, and Positector components to model relationships between questions and database schemas. The framework uses a two-stage revision process with adaptive schema range scaling, initially using a reduced schema range and falling back to full schema if SQL generation fails. The method is evaluated on three benchmark datasets (Spider, Spider-Syn, Spider-DK) with execution accuracy, valid SQL, and test-suite accuracy metrics.

## Key Results
- Achieves 3.7%, 2.5%, and 8.2% improvements in execution accuracy on Spider, Spider-Syn, and Spider-DK datasets respectively
- Outperforms state-of-the-art models including PICARD, RASAT, RESDSQL, GPT-3.5, Codex, and ChatGPT
- Demonstrates strong generalization ability across different cross-domain Text-to-SQL benchmarks
- Shows effectiveness of de-semanticization mechanism in improving retrieval of relevant examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: De-semanticization removes domain-specific tokens to improve retrieval of structurally similar examples
- Mechanism: The SDRE component identifies and masks tokens relevant to case-specific semantics while preserving structural patterns, creating "question skeletons" for retrieval
- Core assumption: Questions with similar structural patterns tend to require similar SQL generation logic, even across different domains
- Evidence anchors:
  - [abstract] "We design a de-semanticization mechanism that extracts question skeletons, allowing us to retrieve similar examples based on their structural similarity"
  - [section] "we utilize the Electra-Large-Discriminator model (ELECTRA) [33] for token sequences encoding process...we design the Semantic Domain Relevance Evaluator (SDRE) to retrieve semantic and syntactic cases"
  - [corpus] Weak - no direct corpus evidence found for this specific de-semanticization mechanism
- Break condition: If structural similarity doesn't correlate with SQL generation similarity, or if domain-specific knowledge is essential for correct SQL generation

### Mechanism 2
- Claim: Adaptive schema range balancing prevents information overload while maintaining query accuracy
- Mechanism: The framework uses a two-stage algorithm with scaled-down schema range initially, falling back to full schema if SQL generation fails. The scaled-down range is computed using relevance scores from SDRE.
- Core assumption: Most SQL queries only need a subset of available schema information, and reducing schema complexity improves LLM focus and performance
- Evidence anchors:
  - [abstract] "Our framework adapts the range of the database schema in prompts to balance length and valuable information"
  - [section] "we balance the relevance of the (case, prompt) pair against the informativeness of the prompt...allows the model to initially focus more on the current information by reducing the schema range"
  - [corpus] Weak - no direct corpus evidence found for this specific adaptive schema range mechanism
- Break condition: If SQL queries frequently require information from non-relevant schema items, or if the relevance scoring fails to identify truly relevant schema components

### Mechanism 3
- Claim: Case-based reasoning with k-NN retrieval provides better demonstrations than random sampling
- Mechanism: The framework retrieves k-nearest neighbor cases based on de-semanticized question skeletons, then uses these similar cases as demonstrations in prompts to guide SQL generation
- Core assumption: Similar questions (in structure and intent) lead to similar SQL generation patterns, making demonstrations from similar cases more effective than random examples
- Evidence anchors:
  - [abstract] "we propose an LLM-based framework for Text-to-SQL which retrieves helpful demonstration examples to prompt LLMs"
  - [section] "Questions with similar intents typically exhibit similar sentence structures, leading to similar SQL query generation"
  - [corpus] Weak - no direct corpus evidence found for this specific CBR approach in Text-to-SQL
- Break condition: If the k-NN retrieval fails to find truly similar cases, or if the demonstration examples don't improve over random sampling

## Foundational Learning

- Concept: Hyperbolic space for semantic representation
  - Why needed here: The Poincaré detector uses hyperbolic distance metrics to capture hierarchical semantic information more effectively than Euclidean space
  - Quick check question: Why would hierarchical semantic information be better represented in hyperbolic rather than Euclidean space?

- Concept: Schema linking and NLQ-schema alignment
  - Why needed here: The framework needs to understand relationships between natural language questions and database schema items to generate correct SQL queries
  - Quick check question: What are the two main types of matching strategies used in the TextAlign component?

- Concept: In-context learning with demonstrations
  - Why needed here: The framework relies on providing similar SQL examples as demonstrations to guide the LLM in generating new queries
  - Quick check question: How does the framework structure its prompts to balance demonstration examples with schema information?

## Architecture Onboarding

- Component map: NLQ → SDRE (Poincaré detector + TextAlign + Positector) → de-semanticized skeleton → FAISS k-NN retrieval → Prompt construction (examples + schema + hint words) → GPT-3.5 generation → Revision (scaled-down schema → full schema fallback) → SQL output
- Critical path: Question → De-semanticization → Retrieval → Prompt Construction → SQL Generation → Execution Validation
- Design tradeoffs: Reduced schema range improves focus but may miss necessary information; de-semanticization improves retrieval but may lose important semantic cues
- Failure signatures: High execution failure rates indicate schema range issues; poor accuracy suggests retrieval problems; syntax errors indicate prompt construction issues
- First 3 experiments:
  1. Test de-semanticization effectiveness by comparing retrieval accuracy with and without de-semanticization
  2. Validate schema range scaling by measuring performance with scaled-down vs full schema ranges
  3. Compare k-NN retrieval performance against random sampling baseline on a held-out validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the effectiveness of the de-semanticization mechanism be further improved to better capture and filter case-relevant and case-irrelevant knowledge in Text-to-SQL tasks?
- Basis in paper: [explicit] The paper discusses the design of a de-semanticization mechanism that extracts question skeletons to retrieve similar examples based on structural similarity, and models relationships between question tokens and database schema items to filter out scheme-related information
- Why unresolved: The paper presents the design of the de-semanticization mechanism but does not provide a detailed evaluation of its effectiveness or potential areas for improvement
- What evidence would resolve it: Comparative experiments evaluating the de-semanticization mechanism against alternative methods or ablations of its components, and qualitative analysis of its impact on the retrieval and generation process

### Open Question 2
- Question: How can the adaptive fallback mechanism be optimized to better balance the informativeness of the prompt and the relevance between cases and the prompt in Text-to-SQL tasks?
- Basis in paper: [explicit] The paper mentions the design of an adaptive fallback mechanism that adjusts the range of schema items in prompts to balance length and valuable information, with a fallback mechanism for providing a more detailed schema if the generated SQL query fails
- Why unresolved: The paper does not provide a detailed analysis of the fallback mechanism's performance or potential areas for optimization
- What evidence would resolve it: Comparative experiments evaluating the fallback mechanism against alternative methods or ablations of its components, and qualitative analysis of its impact on the generation process

### Open Question 3
- Question: How can the CBR-ApSQL framework be extended to handle more complex SQL queries or domain-specific knowledge in Text-to-SQL tasks?
- Basis in paper: [inferred] The paper discusses the performance of the CBR-ApSQL framework on three cross-domain Text-to-SQL benchmarks but does not address its limitations in handling more complex SQL queries or domain-specific knowledge
- Why unresolved: The paper does not provide a detailed analysis of the framework's limitations or potential areas for extension
- What evidence would resolve it: Experiments evaluating the framework's performance on more complex SQL queries or domain-specific knowledge, and qualitative analysis of its limitations and potential extensions

## Limitations

- The de-semanticization mechanism may inadvertently remove semantically important information crucial for accurate SQL generation, particularly for complex queries requiring domain-specific understanding
- The framework's dependence on GPT-3.5 introduces limitations regarding cost and accessibility compared to open-source alternatives
- The evaluation is limited to three specific benchmark datasets, which may not fully represent the diversity of real-world Text-to-SQL scenarios

## Confidence

**High Confidence**: The core mechanism of using de-semanticized question skeletons for case retrieval is well-supported by the paper's experimental results, showing consistent improvements across all three benchmark datasets. The adaptive schema range approach demonstrates clear effectiveness in balancing information content with prompt efficiency.

**Medium Confidence**: The effectiveness of the SDRE components (Poincaré detector, TextAlign, Positector) in filtering case-relevant information is supported by performance gains, but the specific contributions of each component are not isolated in the experiments. The framework's generalization to unseen domains is demonstrated but could benefit from testing on additional datasets.

**Low Confidence**: The paper provides limited analysis of failure cases and edge conditions where the framework might underperform. The specific hyperparameter values (α, τ, θ) and their sensitivity to dataset variations are not thoroughly explored, making it difficult to assess the framework's robustness across different settings.

## Next Checks

1. **Component Ablation Study**: Systematically remove or replace each major component (de-semanticization, adaptive schema range, k-NN retrieval) to quantify their individual contributions to the overall performance gains and identify which mechanisms are most critical.

2. **Cross-Domain Generalization Test**: Evaluate the framework on additional Text-to-SQL datasets beyond the three benchmarks used in the paper, particularly datasets with different domain distributions and query complexities, to assess real-world applicability.

3. **Failure Mode Analysis**: Conduct a detailed analysis of failed cases to identify patterns in queries where the framework struggles, examining whether failures are primarily due to de-semanticization errors, schema range limitations, or retrieval mismatches, and develop targeted improvements for these scenarios.