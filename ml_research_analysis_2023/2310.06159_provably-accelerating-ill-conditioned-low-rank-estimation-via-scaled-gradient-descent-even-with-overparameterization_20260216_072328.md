---
ver: rpa2
title: Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled Gradient
  Descent, Even with Overparameterization
arxiv_id: '2310.06159'
source_url: https://arxiv.org/abs/2310.06159
tags:
- matrix
- low-rank
- tensor
- scaledgd
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This chapter introduces Scaled Gradient Descent (ScaledGD), a preconditioned
  gradient descent method for low-rank matrix and tensor estimation. The key idea
  is to scale the gradient updates by preconditioners that are inexpensive to compute,
  effectively approximating a quasi-Newton method while maintaining low per-iteration
  cost.
---

# Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled Gradient Descent, Even with Overparameterization

## Quick Facts
- arXiv ID: 2310.06159
- Source URL: https://arxiv.org/abs/2310.06159
- Authors: 
- Reference count: 40
- Key outcome: Scaled Gradient Descent achieves condition-number-free linear convergence for low-rank matrix and tensor estimation at near-optimal sample complexities.

## Executive Summary
This paper introduces Scaled Gradient Descent (ScaledGD), a preconditioned gradient descent method for low-rank matrix and tensor estimation. The key innovation is scaling gradient updates by preconditioners that approximate quasi-Newton methods while maintaining low per-iteration cost. For matrix sensing, robust PCA, and completion, ScaledGD achieves linear convergence independent of the condition number. A variant, ScaledGD(λ), provides global convergence guarantees even under overparameterization when the rank is misspecified.

## Method Summary
ScaledGD optimizes over factorized representations of low-rank matrices/tensors using preconditioned gradient updates. The method scales gradients by inverses of factor Gram matrices ((RᵀR)⁻¹ and (LᵀL)⁻¹), effectively approximating quasi-Newton methods. For tensor estimation, it extends to Tucker decomposition with mode-specific preconditioning. The algorithm handles sensing, robust PCA with sparse corruption, and completion problems, achieving near-optimal sample complexities while maintaining condition-number-free convergence rates.

## Key Results
- ScaledGD achieves linear convergence for matrix sensing at O(nᵣ²) sample complexity, independent of condition number
- For matrix completion, ScaledGD converges at O(μnᵣ²) sample complexity, matching vanilla gradient descent despite requiring more samples
- ScaledGD(λ) variant provides global convergence from small random initialization under overparameterization, with convergence rate nearly independent of condition number

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ScaledGD achieves condition-number-free linear convergence by preconditioning gradient updates with inverses of factors' Gram matrices
- Mechanism: Updates L_{t+1} = L_t - η∇_L L(L_t, R_t) (R_t^T R_t)^{-1} and R_{t+1} = R_t - η∇_R L(L_t, R_t) (L_t^T L_t)^{-1} scale search directions, approximating quasi-Newton methods
- Core assumption: Preconditioners are inexpensive to compute since their dimensions scale with rank r rather than ambient dimension
- Break condition: Large rank r makes preconditioners computationally expensive, potentially negating ScaledGD's advantage

### Mechanism 2
- Claim: ScaledGD enjoys equivariance to parameterization, maintaining invariant low-rank updates under invertible transformations
- Mechanism: Update rule is covariant with respect to invertible transformations of factors, producing same low-rank update sequence regardless of parameterization
- Core assumption: Preconditioners are designed to be covariant with respect to symmetry in low-rank factorization
- Break condition: Ill-conditioned invertible transformations may break invariance property and degrade performance

### Mechanism 3
- Claim: ScaledGD implicitly balances low-rank factors without requiring explicit regularization or orthogonalization
- Mechanism: Preconditioners act as implicit regularizers promoting factor balancing, similar to explicit terms like ||L^T L - R^T R||_F^2
- Core assumption: Preconditioners approximate inverse of diagonal blocks of Hessian for population loss, promoting factor balancing
- Break condition: Poor initialization far from balanced factors may require explicit regularization despite implicit balancing

## Foundational Learning

- Concept: Low-rank matrix and tensor factorization
  - Why needed here: ScaledGD solves low-rank estimation by optimizing over factors rather than full objects
  - Quick check question: What advantage does factorizing X = LR^T offer compared to optimizing over X directly?

- Concept: Gradient descent and preconditioning
  - Why needed here: ScaledGD is preconditioned gradient descent, requiring understanding of basic gradient descent and preconditioning principles
  - Quick check question: How does preconditioning gradient descent updates improve convergence in ill-conditioned problems?

- Concept: Tensor operations and Tucker decomposition
  - Why needed here: ScaledGD extends to tensor estimation using Tucker decomposition, requiring understanding of tensor matricization and Kronecker products
  - Quick check question: What is Tucker decomposition and how does it differ from matrix SVD?

## Architecture Onboarding

- Component map: Measurements y -> Factorized representation (L,R) or (U,V,W,S) -> ScaledGD updates with preconditioners -> Convergence to ground truth

- Critical path:
  1. Initialize factors using spectral method or small random initialization
  2. Compute gradient of loss function with respect to factors
  3. Update factors using ScaledGD rule with appropriate preconditioners
  4. Check for convergence; if not converged, return to step 2

- Design tradeoffs:
  - Rank selection: Larger rank r reduces approximation error but increases condition number, potentially slowing convergence
  - Regularization: ScaledGD(λ) handles overparameterization but λ choice critically affects performance
  - Sample complexity: ScaledGD requires more samples than vanilla GD for completion but achieves faster convergence

- Failure signatures:
  - Slow convergence: Improperly scaled preconditioners or excessive rank may cause slow convergence
  - Numerical instability: Ill-conditioned preconditioners may cause numerical instability in updates
  - Suboptimal solution: Poor initialization or rank mis-specification may lead to suboptimal convergence

- First 3 experiments:
  1. Implement ScaledGD for low-rank matrix sensing and compare convergence with vanilla GD on synthetic data with varying condition numbers
  2. Extend ScaledGD to low-rank tensor estimation and evaluate performance on tensor completion using real-world dataset
  3. Investigate rank selection and regularization effects on ScaledGD(λ) performance when true rank is unknown

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical sample complexity for ScaledGD in overparameterized setting with unknown true rank?
- Basis in paper: Section 4 discusses ScaledGD(λ) for overparameterization but lacks sample complexity results
- Why unresolved: Paper focuses on known rank case, only briefly mentioning overparameterization without theoretical guarantees
- What evidence would resolve it: Proving sample complexity bounds requires analyzing interplay between λ, step size, initialization size, and sensing operator sample complexity

### Open Question 2
- Question: How does ScaledGD perform with adversarial noise or outliers beyond sparse corruption model?
- Basis in paper: Paper considers robust PCA with sparse corruption but doesn't address other noise types
- Why unresolved: Theoretical analysis and experiments limited to specific noise models; unclear how ScaledGD behaves under general noise
- What evidence would resolve it: Analyzing convergence and robustness under various noise models requires extending theoretical framework and extensive experiments

### Open Question 3
- Question: What are ScaledGD's limitations regarding rank and condition number beyond paper's bounds?
- Basis in paper: Paper provides convergence bounds for specific condition number and rank ranges, doesn't explore beyond these
- Why unresolved: Theoretical analysis focuses on specific ranges; unclear how ScaledGD performs outside these bounds
- What evidence would resolve it: Investigating performance for higher ranks/condition numbers requires experiments and potentially new theoretical tools

## Limitations
- Preconditioning approach can become numerically unstable when Gram matrices are ill-conditioned or rank is large
- Method assumes specific measurement models (Gaussian, Bernoulli) and may not generalize to arbitrary distributions
- Sample complexity bounds require strong assumptions about measurement ensemble (e.g., restricted isometry property)

## Confidence

- High confidence: Linear convergence rate claims for matrix sensing and completion under stated assumptions; equivariance property of ScaledGD updates
- Medium confidence: Extension to tensor estimation via Tucker decomposition; performance guarantees under overparameterization with ScaledGD(λ)
- Low confidence: Robustness to adversarial corruptions in robust PCA beyond sparse corruption model; generalization to non-Gaussian measurement noise

## Next Checks
1. Empirically test numerical stability by measuring condition numbers of Gram matrices throughout optimization across different rank values and adding regularization thresholds when needed
2. Validate theoretical sample complexity bounds by systematically varying measurement sparsity and noise levels against recovery success rates
3. Benchmark against state-of-the-art nonconvex methods (e.g., alternating minimization, gradient descent with momentum) on real-world datasets with varying degrees of ill-conditioning