---
ver: rpa2
title: Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale of
  Two Benchmarks
arxiv_id: '2311.09060'
source_url: https://arxiv.org/abs/2311.09060
tags:
- neurons
- localization
- sequence
- methods
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose two benchmarks to systematically evaluate the ability
  of localization methods to pinpoint LLM components responsible for memorized data.
  The INJ benchmark injects a new sequence into a small set of LLM weights, allowing
  direct evaluation of whether localization methods can identify these ground-truth
  weights.
---

# Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale of Two Benchmarks

## Quick Facts
- arXiv ID: 2311.09060
- Source URL: https://arxiv.org/abs/2311.09060
- Reference count: 11
- Primary result: Pruning-based localization methods outperform attribution methods at identifying neurons responsible for memorized sequences in LLMs

## Executive Summary
This paper proposes two benchmarks to evaluate localization methods' ability to identify neural network components responsible for memorized data in large language models. The INJ benchmark injects new information into specific weights to provide ground truth, while the DEL benchmark measures how dropping out identified neurons affects memorization. Five localization methods are evaluated across both benchmarks on GPT2-XL and Pythia models. Results show all methods exhibit promising localization ability, with pruning-based methods (SLIMMING, HARD CONCRETE) performing best, though identified neurons tend to be relevant for multiple related sequences rather than specific to individual sequences.

## Method Summary
The paper introduces two complementary benchmarks for evaluating localization methods. The INJ benchmark injects definition sentences from ECBD-2021 into small subsets of LLM weights, creating ground truth for which weights are responsible for memorization. The DEL benchmark collects naturally memorized sequences from pretrained models and evaluates localization by measuring changes in memorization scores after dropping out identified neurons. Five localization methods are tested: ACTIVATIONS, IG (Integrated Gradients), ZERO-OUT, SLIMMING, and HARD CONCRETE. Evaluation uses recall@k% for INJ and changes in accuracy, Levenshtein distance, and perplexity for DEL.

## Key Results
- All five localization methods significantly outperform random baseline in both benchmarks
- Pruning-based methods (SLIMMING, HARD CONCRETE) consistently outperform attribution methods (ACTIVATIONS, IG)
- Identified neurons are not specific to single sequences but tend to be relevant for multiple related sequences
- Localization ability decreases as the number of related memorized sequences increases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Localization methods can identify neurons responsible for memorized sequences by measuring attribution scores.
- **Mechanism**: The paper evaluates five localization methods that assign attribution scores to each neuron based on its importance to memorizing a target sequence. These methods include both existing methods like ACTIVATIONS and IG, as well as new methods adapted from network pruning like SLIMMING and HARD CONCRETE. By ranking neurons based on these scores and dropping out the top-K neurons, the paper measures how much the memorization of the target sequence is affected.
- **Core assumption**: Neurons with high attribution scores are indeed responsible for memorizing the target sequence.
- **Evidence anchors**:
  - [abstract]: "All methods exhibit promising localization ability, especially for pruning-based methods, though the neurons they identify are not necessarily specific to a single memorized sequence."
  - [section 5]: "All methods can do localization. First, we show that all five localization methods greatly outperform the RANDOM baseline..."
- **Break condition**: If neurons with high attribution scores are not actually responsible for memorization, the localization methods would fail to identify the correct neurons.

### Mechanism 2
- **Claim**: Dropping out located neurons can erase memorized sequences from LLMs.
- **Mechanism**: The DEL Benchmark evaluates localization by measuring how much dropping out identified neurons erases a memorized pretrained sequence. After dropout, a successful localization method should cleanly erase the target sequence from an LLM without hurting the memorization of other sequences. The paper defines two memorization scores: Accuracy and Levenshtein distance, to quantify the change in memorization after dropout.
- **Core assumption**: Dropping out neurons responsible for memorization will significantly reduce the memorization of the target sequence.
- **Evidence anchors**:
  - [abstract]: "In the DEL Benchmark, we evaluate localization by measuring whether dropping out located neurons erases a memorized sequence from the model."
  - [section 4.2]: "Evaluation. Similar to INJ Benchmark, we let each localization method predict top-K neurons every layer, but exclude the bottommost layer. A successful localization method should make LLMs forget the target sequence (large changes in memorization scores), but still remember the others (small changes in memorization scores) after dropping out the predicted neurons."
- **Break condition**: If dropping out located neurons does not significantly affect the memorization of the target sequence, the localization methods would not be effective in erasing memorized sequences.

### Mechanism 3
- **Claim**: INJ Benchmark provides a direct evaluation of localization methods by creating ground truth through information injection.
- **Mechanism**: The INJ Benchmark injects a piece of new information into a small subset of LLM weights, enabling direct evaluation of whether localization methods can identify these "ground truth" weights. By fine-tuning a small fraction of weight vectors to memorize the newly introduced sequence, the paper ensures that the injected weights are the only parameters responsible for memorizing the new sequence. Localization methods are then evaluated based on their ability to predict the indices of the fine-tuned weights.
- **Core assumption**: The injected weights are indeed the only parameters responsible for memorizing the new sequence.
- **Evidence anchors**:
  - [abstract]: "In our INJ Benchmark, we actively inject a piece of new information into a small subset of LLM weights and measure whether localization methods can identify these 'ground truth' weights."
  - [section 4.1]: "Data. We use the definition sentences in ECBD-2021 dataset... We inject a piece of new information (one definition sequence) into the LLM at a time. Information Injection. As prior work has found that LLMs use different weight vectors vl_i in FFNs to store different information, we restrict the weight updates for injection to these vectors, keeping the rest of the model parameters frozen."
- **Break condition**: If the injected weights are not the only parameters responsible for memorizing the new sequence, the INJ Benchmark would not provide a direct evaluation of localization methods.

## Foundational Learning

- **Concept**: Language model memorization
  - Why needed here: The paper studies the ability of localization methods to pinpoint LLM components responsible for memorized data. Understanding how LLMs memorize sequences is crucial for evaluating the effectiveness of localization methods.
  - Quick check question: What is the difference between memorization and learning in the context of LLMs?

- **Concept**: Feed-forward network (FFN) in Transformers
  - Why needed here: The paper focuses on localizing neurons in FFN layers, as prior work shows that FFN layers act as neural memories. Understanding the structure and function of FFN layers is essential for comprehending the localization methods and their evaluation.
  - Quick check question: How do FFN layers contribute to memorization in LLMs?

- **Concept**: Attribution methods for localization
  - Why needed here: The paper evaluates various localization methods that assign attribution scores to neurons based on their importance to memorizing a target sequence. Familiarity with attribution methods is necessary to understand how localization methods work and how their effectiveness is measured.
  - Quick check question: What are some common attribution methods used for localization in neural networks?

## Architecture Onboarding

- **Component map**: LLM (GPT2-XL, Pythia) -> FFN layers -> Localization methods (ACTIVATIONS, IG, SLIMMING, HARD CONCRETE, ZERO-OUT) -> INJ and DEL benchmarks
- **Critical path**: 1) Collect memorized sequences from LLMs, 2) Apply localization methods to identify important neurons, 3) Evaluate effectiveness via INJ (ground truth) or DEL (dropout experiments)
- **Design tradeoffs**: The paper uses two complementary benchmarks (INJ and DEL) to evaluate localization methods. INJ Benchmark provides a direct evaluation with ground truth, while DEL Benchmark evaluates localization in a real-world scenario. However, DEL Benchmark lacks ground truth and relies on neuron knockouts for evaluation.
- **Failure signatures**: Localization methods may fail if they cannot accurately identify neurons responsible for memorization, if dropping out located neurons does not significantly affect memorization, or if the methods are computationally expensive and not scalable to large LLMs.
- **First 3 experiments**:
  1. Implement the INJ Benchmark to directly evaluate localization methods by injecting new information into LLM weights and measuring the ability of methods to identify the injected weights.
  2. Apply the DEL Benchmark to evaluate localization methods in a real-world scenario by dropping out located neurons and measuring the change in memorization of pretrained sequences.
  3. Compare the performance of different localization methods (ACTIVATIONS, IG, SLIMMING, HARD CONCRETE, ZERO-OUT) across both benchmarks and analyze their strengths and weaknesses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do localization methods specifically identify neurons for individual sequences or do they identify neurons that are important for multiple related sequences?
- Basis in paper: [explicit] The paper shows that all methods show evidence of localization ability but struggle to balance between erasing the target sequence and retaining other memorized data, indicating neurons identified tend to also be relevant for memorizing some other sequences.
- Why unresolved: The paper does not definitively determine if methods are identifying sequence-specific neurons or neurons important for multiple sequences. Further experiments varying the number of related sequences and measuring localization performance could help resolve this.
- What evidence would resolve it: If localization performance drops significantly as the number of related sequences increases, it would suggest methods are identifying sequence-specific neurons. If performance remains stable, it would suggest methods are identifying neurons important for multiple related sequences.

### Open Question 2
- Question: Do localization methods perform differently on different types of memorized data, such as quotes, addresses, or codes?
- Basis in paper: [explicit] The paper shows a confusion matrix analyzing localization of different categories of memorized data in GPT2, finding methods often confuse data within the same category. However, it does not systematically compare localization performance across different data categories.
- Why unresolved: The paper only analyzes localization on a subset of data categories. Further experiments comparing localization performance on a wider variety of data categories could help resolve this.
- What evidence would resolve it: If localization performance varies significantly across different data categories, it would suggest methods perform differently on different types of memorized data. If performance remains stable, it would suggest methods perform similarly on different types of memorized data.

### Open Question 3
- Question: How do localization methods perform on larger language models?
- Basis in paper: [explicit] The paper evaluates localization methods on GPT2-XL and Pythia-6.9B but notes Pythia-6.9B is too large for some computationally expensive methods like ZERO-OUT.
- Why unresolved: The paper does not evaluate localization on very large language models like GPT3. Further experiments evaluating localization on larger models could help resolve this.
- What evidence would resolve it: If localization performance degrades significantly on larger models, it would suggest methods do not scale well. If performance remains stable, it would suggest methods can scale to larger models.

## Limitations
- DEL benchmark lacks ground truth for naturally memorized sequences, relying entirely on neuron knockout experiments
- Computational cost of some methods (ZERO-OUT takes ~8.5 hours per sequence) severely limits scalability
- Injection process in INJ benchmark creates artificial memorization patterns that may not generalize to natural pretraining

## Confidence
- **High confidence** in comparative ranking of methods within each benchmark
- **Medium confidence** in absolute localization ability claims due to lack of ground truth in DEL benchmark
- **Low confidence** in specificity claims - paper acknowledges neurons are not sequence-specific but doesn't quantify this

## Next Checks
1. **Cross-task generalization test**: Apply localization methods to memorized sequences from multiple tasks (e.g., code, dialogue, reasoning) beyond definitions to verify if performance degrades or remains consistent across domains.
2. **Ablation of computational costs**: Implement a scaled-down version of ZERO-OUT and HARD CONCRETE on a smaller model (e.g., GPT2-small) to verify if the computational bottleneck is the model size or the method itself.
3. **Ground truth verification**: Design an experiment where a subset of memorized sequences from pretrained models are verified through human annotation or additional training to establish partial ground truth for the DEL benchmark.