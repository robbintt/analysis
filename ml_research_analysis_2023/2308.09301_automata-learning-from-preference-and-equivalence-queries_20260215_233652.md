---
ver: rpa2
title: Automata Learning from Preference and Equivalence Queries
arxiv_id: '2308.09301'
source_url: https://arxiv.org/abs/2308.09301
tags:
- reward
- equivalence
- number
- states
- hypothesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REMAP is an L-based algorithm for learning reward machines from
  preference queries. It uses a symbolic observation table to navigate a hypothesis
  space of reward machines, leveraging unification and constraint solving to generate
  concrete hypotheses.
---

# Automata Learning from Preference and Equivalence Queries

## Quick Facts
- **arXiv ID:** 2308.09301
- **Source URL:** https://arxiv.org/abs/2308.09301
- **Reference count:** 40
- **Key outcome:** REMAP learns reward machines from preference queries using symbolic observation tables, unification, and constraint solving to navigate hypothesis spaces and guarantee minimal automaton inference under exact equivalence queries.

## Executive Summary
REMAP is an L*-based algorithm that learns reward machines through preference queries rather than membership queries. The algorithm maintains a symbolic observation table that captures constraints from preference queries and uses unification to navigate the hypothesis space of reward machines. By leveraging constraint solving, REMAP generates concrete hypotheses that are tested through equivalence queries, converging to the minimal automaton that explains the teacher's preferences.

## Method Summary
REMAP replaces membership queries in the L* algorithm with preference queries that compare sequences of observations. It maintains a symbolic observation table where entries represent constraints on the reward machine structure derived from preference feedback. The algorithm uses unification to manage equivalence classes of symbolic entries and constraint solving (via Z3) to generate concrete reward machines from the symbolic hypotheses. The learning process iterates between making preference queries, updating the symbolic table, performing unification, generating hypotheses through constraint solving, and validating them with equivalence queries until the teacher confirms correctness.

## Key Results
- REMAP guarantees correct inference of the minimal automaton with polynomial query complexity under exact equivalence queries
- The algorithm achieves PAC-identification of the minimal automaton using sampling-based equivalence queries
- Empirical evaluations show REMAP scales to large automata and effectively learns correct reward machines from consistent teachers in both exact and sampling-based settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: REMAP can learn the minimal automaton from preference queries by navigating a hypothesis space of symbolic reward machines using unification and constraint solving.
- **Mechanism**: The symbolic observation table with unification and constraint solving allows the learner to progressively eliminate hypotheses that are inconsistent with the preference constraints, converging on the minimal automaton.
- **Core assumption**: The teacher's preference queries are consistent with a reward machine structure and provide enough information to disambiguate between hypotheses.
- **Evidence anchors**:
  - [abstract] "REMAP is guaranteed to correctly infer the minimal automaton with polynomial query complexity under exact equivalence queries"
  - [section] "REMAP introduces preference queries in place of membership queries in the L* algorithm, and leverages a symbolic observation table along with unification and constraint solving to narrow the hypothesis reward machine search space."

### Mechanism 2
- **Claim**: REMAP can achieve PAC-identification of the minimal automaton using sampling-based equivalence queries.
- **Mechanism**: By testing a finite number of sample sequences per equivalence query, REMAP can probabilistically identify the minimal automaton with high confidence.
- **Core assumption**: The teacher's sampling of sequences is representative of the true distribution of sequences.
- **Evidence anchors**:
  - [abstract] "REMAP is guaranteed to correctly infer the minimal automaton with polynomial query complexity under exact equivalence queries, and achieves PAC-identification (ε-approximate, with high probability) of the minimal automaton using sampling-based equivalence queries."
  - [section] "We employQ-learning with counterfactual experiences for reward machines(CRM) to obtain optimal policies for ground truth and learned reward machines. We measured the empirical expected return of optimal policies learned from each type of reward machine."

### Mechanism 3
- **Claim**: REMAP can scale to large automata and effectively learn correct automata from consistent teachers.
- **Mechanism**: The use of a symbolic observation table and unification allows REMAP to handle large hypothesis spaces efficiently, and the constraint solver ensures the learned automaton is consistent with the preference constraints.
- **Core assumption**: The teacher's preference queries and counterexamples are consistent with a reward machine structure.
- **Evidence anchors**:
  - [abstract] "Empirical evaluations of REMAP on the task of learning reward machines for two reinforcement learning domains indicate REMAP scales to large automata and is effective at learning correct automata from consistent teachers, under both exact and sampling-based equivalence queries."
  - [section] "Our empirical evaluations of REMAP on the task of learning reward machines for two reinforcement learning domains indicate REMAP scales to large automata and is effective at learning correct automata from consistent teachers, under both exact and sampling-based equivalence queries."

## Foundational Learning

- **Concept: Active automata learning from membership and equivalence queries**
  - Why needed here: REMAP builds upon the L* algorithm for active automata learning, replacing membership queries with preference queries and leveraging a symbolic observation table.
  - Quick check question: How does REMAP differ from the standard L* algorithm in terms of the types of queries used?

- **Concept: Preference queries over sequences**
  - Why needed here: REMAP uses preference queries to compare sequences and gather constraints on the reward machine structure, rather than direct membership queries.
  - Quick check question: How does REMAP interpret the results of preference queries to update its hypothesis space?

- **Concept: Unification and constraint solving**
  - Why needed here: REMAP uses unification to navigate the hypothesis space of symbolic reward machines and constraint solving to generate concrete hypotheses from the symbolic ones.
  - Quick check question: How does REMAP use unification and constraint solving to ensure the learned automaton is consistent with the preference constraints?

## Architecture Onboarding

- **Component map**: Teacher -> Symbolic observation table -> Unification -> Constraint solver -> Hypothesis generator -> Equivalence query
- **Critical path**: Preference queries → Symbolic observation table update → Unification → Constraint solving → Concrete hypothesis generation → Equivalence query
- **Design tradeoffs**: REMAP trades off the expressiveness of preference queries for the efficiency of navigating a symbolic hypothesis space, compared to direct membership queries.
- **Failure signatures**: REMAP may fail to converge if the teacher's feedback is inconsistent, the hypothesis space is too large, or the constraint solver cannot find a satisfying solution.
- **First 3 experiments**:
  1. Verify REMAP can learn a simple reward machine from preference queries with a consistent teacher.
  2. Test REMAP's scalability by learning a larger reward machine with more states and transitions.
  3. Evaluate REMAP's PAC-identification performance by varying the number of samples per equivalence query and measuring the empirical probability of isomorphism and regret.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the choice of preference model affect the accuracy and efficiency of REMAP?
- **Basis in paper**: [explicit] The paper uses a specific preference model where preferences represent comparisons of immediate reward. However, it acknowledges that "more realistic preference models" are a potential direction for future work.
- **Why unresolved**: The paper only evaluates one specific preference model. Different preference models (e.g., comparing cumulative rewards, using different comparison operators) might yield different results in terms of accuracy, query complexity, and scalability.
- **What evidence would resolve it**: Conduct experiments comparing REMAP's performance using various preference models (e.g., immediate reward vs. cumulative reward, different comparison operators) on the same set of tasks. Measure accuracy, query complexity, and scalability for each model.

### Open Question 2
- **Question**: Can REMAP be extended to handle feedback of varying strength and informativeness from the teacher?
- **Basis in paper**: [explicit] The paper mentions that "feedback strength controls how many hypotheses per counterexample can be eliminated by the learner" and that "the amount of information provided by these constraints impacts the process of elimination efficiency of the learner." However, it only considers strong feedback (T(c) = f(c)).
- **Why unresolved**: The paper only considers one type of feedback (strong feedback). Different types of feedback (e.g., weak feedback where T(c) ≠ ˆf(c), or partial feedback) might affect the efficiency and accuracy of REMAP.
- **What evidence would resolve it**: Implement and evaluate REMAP with different types of feedback (e.g., weak feedback, partial feedback) on the same set of tasks. Measure the impact on accuracy, query complexity, and termination time.

### Open Question 3
- **Question**: How can query efficiency be improved to reduce teacher burden?
- **Basis in paper**: [explicit] The paper acknowledges that "improving query efficiency to reduce teacher burden" is a potential direction for future work. The current implementation of REMAP makes a preference query for every pair of unique sequences, which can be computationally expensive.
- **Why unresolved**: The paper does not explore techniques to reduce the number of queries made to the teacher. Strategies like active query selection, where the learner intelligently chooses which queries to ask, could potentially reduce the teacher's burden.
- **What evidence would resolve it**: Implement and evaluate query selection strategies (e.g., uncertainty sampling, query-by-committee) to reduce the number of queries made by REMAP while maintaining accuracy. Compare the query efficiency and accuracy of REMAP with and without query selection on the same set of tasks.

## Limitations

- **Assumption dependency**: REMAP's theoretical guarantees rely on consistent teacher feedback and representative sampling, which may not hold in practice with noisy or inconsistent preferences.
- **Computational overhead**: The constraint solver integration and symbolic table management may create computational bottlenecks when learning very large automata with complex transition structures.
- **Query complexity**: REMAP may require a large number of preference queries to disambiguate between hypotheses, potentially creating teacher burden in practical applications.

## Confidence

- **Mechanism soundness**: Medium - The symbolic observation table and unification approach appears sound, but the constraint solver's role in generating concrete hypotheses is not fully detailed.
- **Scalability claims**: Medium - Empirical results show scalability to larger automata, but runtime analysis and computational bottlenecks are not thoroughly characterized.
- **Theoretical guarantees**: Low - The polynomial query complexity bounds and PAC-identification claims depend on strong assumptions about teacher consistency and sampling representativeness that are not fully validated.

## Next Checks

1. Test REMAP's robustness to inconsistent teacher feedback by introducing random noise into preference queries and measuring convergence failure rates
2. Characterize the algorithm's behavior when the teacher's preference queries are insufficient to disambiguate between multiple valid hypotheses
3. Analyze the computational overhead of constraint solving as the symbolic observation table grows, particularly for larger automata with complex transition structures