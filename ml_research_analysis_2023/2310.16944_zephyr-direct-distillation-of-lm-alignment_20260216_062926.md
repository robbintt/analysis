---
ver: rpa2
title: 'Zephyr: Direct Distillation of LM Alignment'
arxiv_id: '2310.16944'
source_url: https://arxiv.org/abs/2310.16944
tags:
- dsft
- ddpo
- feedback
- chat
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a distillation approach for aligning a small
  open language model to user intent. The method uses AI feedback (AIF) from an ensemble
  of teacher models as preference data, and applies distilled direct preference optimization
  (dDPO) as the learning objective.
---

# Zephyr: Direct Distillation of LM Alignment

## Quick Facts
- arXiv ID: 2310.16944
- Source URL: https://arxiv.org/abs/2310.16944
- Authors: 
- Reference count: 17
- Primary result: Zephyr-7B sets new SOTA for 7B chat models, outperforming Llama2-Chat-70B on MT-Bench using distilled DPO with AI feedback.

## Executive Summary
This work presents a distillation approach for aligning a small open language model to user intent. The method uses AI feedback (AIF) from an ensemble of teacher models as preference data, and applies distilled direct preference optimization (dDPO) as the learning objective. The approach requires no human annotation and no sampling compared to using other approaches like proximal policy optimization (PPO). The resulting model, Zephyr-7B, sets a new state-of-the-art for 7B parameter chat models, and even outperforms Llama2-Chat-70B on MT-Bench.

## Method Summary
The Zephyr alignment pipeline consists of two stages: supervised fine-tuning (SFT) on the UltraChat dataset followed by distilled direct preference optimization (dDPO) on UltraFeedback. In the SFT stage, the base Mistral-7B model is fine-tuned for 1 epoch on UltraChat to learn conversational patterns. In the DPO stage, the SFT model is refined for 3 epochs on UltraFeedback using AI-generated preference rankings from an ensemble of teacher models, optimizing the dDPO objective without requiring sampling.

## Key Results
- Zephyr-7B sets a new state-of-the-art for 7B parameter chat models
- Outperforms Llama2-Chat-70B on MT-Bench
- Shows improvements in both academic benchmarks and conversational capabilities
- Preference learning is critical for achieving these results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The distilled DPO approach enables the 7B model to learn user intent alignment without human annotation.
- Mechanism: The method replaces human preference labels with AI-generated rankings from an ensemble of teacher models. These rankings are used as static preference data in a DPO objective, which directly optimizes the model to rank preferred responses higher without sampling.
- Core assumption: AI feedback from a strong ensemble can approximate human preferences well enough for alignment.
- Evidence anchors:
  - [abstract]: "This work presents a distillation approach for aligning a small open language model to user intent. The method uses AI feedback (AIF) from an ensemble of teacher models as preference data, and applies distilled direct preference optimization as the learning objective."
  - [section]: "The main step is to utilize AI Feedback (AIF) from an ensemble of teacher models as preference data, and apply distilled direct preference optimization as the learning objective (Rafailov et al., 2023)."
  - [corpus]: "The alignment of language models with human preferences is critical for building reliable AI systems. The problem is typically framed as optimizing an LM policy to maximize the expected reward that reflects human preferences. Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment method..."

### Mechanism 2
- Claim: Applying DPO on top of a dSFT model is necessary to achieve strong chat performance.
- Mechanism: dSFT alone produces models that respond to prompts but lack conversational nuance and alignment. The DPO step uses the preference data to refine the dSFT model's behavior, learning to rank outputs according to the teacher model's preferences.
- Core assumption: A base SFT model is needed to learn basic prompt-response patterns before applying preference optimization.
- Evidence anchors:
  - [section]: "The goal of the final step is to refine the πdSFT by maximizing the likelihood of ranking the preferred yw over yl in a preference model."
  - [section]: "Is Preference Optimization Necessary? In Table 3 we examine the impact from different steps of the alignment process... we replicate past results (Ouyang et al., 2022) and show that without an initial SFT step (-dSFT), models are not able to learn at all from feedback and perform terribly."
  - [corpus]: "Silkie: Preference Distillation for Large Visual Language Models... This paper explores preference distillation for large vision language models (LVLMs), improving their ability to generate helpful and faithful responses anchoring the visual context."

### Mechanism 3
- Claim: The dDPO objective can be optimized without sampling from the current policy, enabling fast training.
- Mechanism: The DPO formulation derives a reward function that depends only on the fixed dSFT model and the preference data. This allows direct optimization of the objective without the need for on-policy sampling or rejection sampling.
- Core assumption: The DPO formulation is stable and effective when the preference data is fixed and high-quality.
- Evidence anchors:
  - [section]: "Direct preference optimization (DPO) uses a simpler approach to directly optimize the preference model from the static data (Rafailov et al., 2023). The key observation is to derive the optimal reward function in terms of the optimal LLM policy π* and the original LLM policy πdSFT."
  - [abstract]: "The approach requires no human annotation and no sampling compared to using other approaches like proximal policy optimization (PPO) (Schulman et al., 2017)."
  - [corpus]: "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning... The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities."

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT teaches the base model to respond to user prompts in a conversational manner, providing a foundation for subsequent preference optimization.
  - Quick check question: What is the purpose of SFT in the Zephyr alignment pipeline, and how does it differ from regular supervised learning?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO refines the SFT model by optimizing it to rank preferred responses higher based on AI feedback, aligning the model to user intent without human annotation.
  - Quick check question: How does DPO differ from traditional reinforcement learning approaches like PPO in terms of training data and optimization?

- Concept: AI Feedback (AIF)
  - Why needed here: AIF replaces expensive human preference labels with rankings from an ensemble of teacher models, enabling scalable alignment without human annotation.
  - Quick check question: What is the source of preference data in the Zephyr approach, and how is it generated?

## Architecture Onboarding

- Component map: Mistral-7B base model -> dSFT stage (UltraChat dataset) -> DPO stage (UltraFeedback dataset with AIF) -> Final aligned model (Zephyr-7B)
- Critical path: dSFT must be completed before DPO can be applied, as DPO builds on the conversational patterns learned during dSFT.
- Design tradeoffs: Using AIF instead of human feedback reduces cost but may introduce bias toward the teacher models' preferences. Training without sampling speeds up training but requires high-quality static preference data.
- Failure signatures: If dSFT is skipped, the model cannot learn from preferences at all. If DPO is applied for too many epochs, the model may overfit to the preference data. If the teacher ensemble is weak, the AIF may not accurately capture human preferences.
- First 3 experiments:
  1. Train dSFT on UltraChat for 1 epoch and evaluate on a held-out conversational dataset.
  2. Apply DPO for 1 epoch on UltraFeedback and compare performance to dSFT-only model on MT-Bench.
  3. Increase DPO epochs to 3 and observe impact on MT-Bench and overfitting (training accuracy).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the DPO method scale effectively to larger language models, such as those with 70B parameters?
- Basis in paper: Inferred
- Why unresolved: The paper mentions that while Zephyr-7B performs competitively with Llama2-Chat-70B, it performs worse than WizardLM-70B and Xwin-LM-70B. This suggests that scaling the DPO method to larger models might be necessary to match performance at these scales.
- What evidence would resolve it: Conducting experiments to apply the DPO method to larger models and comparing their performance with other large models on benchmarks like MT-Bench and AlpacaEval would provide evidence.

### Open Question 2
- Question: How does the performance of Zephyr-7B compare to proprietary models in terms of safety and handling harmful outputs?
- Basis in paper: Explicit
- Why unresolved: The paper acknowledges that safety considerations, such as producing harmful outputs or providing illegal advice, are not addressed in the study. This is due to the technical challenges of curating synthetic data for these aspects.
- What evidence would resolve it: Evaluating Zephyr-7B on safety benchmarks and comparing its performance with proprietary models in terms of handling harmful outputs would provide evidence.

### Open Question 3
- Question: Is the use of GPT-4 as an evaluator for the AlpacaEval and MT-Bench benchmarks introducing bias in the results?
- Basis in paper: Explicit
- Why unresolved: The paper notes that GPT-4 is known to be biased towards models distilled from it or those that produce verbose responses, which may not necessarily be correct.
- What evidence would resolve it: Conducting evaluations using different evaluators or human raters and comparing the results with those obtained using GPT-4 would provide evidence of any bias.

### Open Question 4
- Question: How does the performance of Zephyr-7B change with different numbers of DPO training epochs?
- Basis in paper: Inferred
- Why unresolved: The paper mentions that the strongest model was obtained with one epoch of SFT followed by three epochs of DPO, but it also notes that overfitting occurs after one epoch of DPO training. The impact of varying the number of DPO epochs on performance is not fully explored.
- What evidence would resolve it: Experimenting with different numbers of DPO training epochs and evaluating the resulting models on benchmarks like MT-Bench and AlpacaEval would provide evidence.

## Limitations

- Reliance on AI feedback as a proxy for human preferences introduces uncertainty about real-world performance
- 7B model outperforming 70B Llama2-Chat may reflect benchmark-specific characteristics rather than general superiority
- Computational resources required (16xA100 GPUs) may limit reproducibility for many research groups

## Confidence

- **High Confidence**: The technical description of the dSFT and dDPO training procedures is clear and reproducible. The claim that dSFT is necessary for learning from preferences is well-supported by the ablation study. The computational setup and training hyperparameters are sufficiently specified.
- **Medium Confidence**: The claim that Zephyr-7B sets a new state-of-the-art for 7B parameter chat models is well-supported by the benchmarks presented, though the comparison to other 7B models could be more comprehensive. The mechanism by which dDPO enables faster training without sampling is theoretically sound based on the DPO formulation.
- **Low Confidence**: The claim that AI feedback can fully replace human preference data for alignment is not directly validated with human studies. The superiority over Llama2-Chat-70B may not generalize beyond the specific benchmarks tested. The long-term stability and safety implications of training without human oversight are not addressed.

## Next Checks

1. **Human Evaluation Validation**: Conduct a human preference study comparing Zephyr-7B outputs against both the teacher ensemble's rankings and human-annotated preferences on a subset of UltraFeedback prompts to quantify the gap between AI feedback and human judgment.

2. **Generalization Benchmark Expansion**: Test Zephyr-7B on additional chat benchmarks not included in the original evaluation (such as VicunaEval or Chatbot Arena) and compare against other leading chat models to verify the MT-Bench performance is not benchmark-specific.

3. **Ablation on Teacher Ensemble Composition**: Systematically vary the composition and size of the teacher ensemble (e.g., using only GPT-4, only Claude, or a mix of smaller models) to quantify how ensemble diversity affects the quality of AI feedback and downstream alignment performance.