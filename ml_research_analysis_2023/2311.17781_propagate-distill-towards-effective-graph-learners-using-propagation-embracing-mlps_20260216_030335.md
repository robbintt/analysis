---
ver: rpa2
title: 'Propagate & Distill: Towards Effective Graph Learners Using Propagation-Embracing
  MLPs'
arxiv_id: '2311.17781'
source_url: https://arxiv.org/abs/2311.17781
tags:
- graph
- teacher
- propagation
- nodes
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Propagate & Distill (P&D), a method to enhance
  knowledge distillation from GNNs to MLPs by explicitly injecting structural information
  during training. Unlike prior approaches that rely on positional embeddings or adjacency
  matrix rows as input, P&D recursively propagates the teacher GNN's output along
  the graph structure before distillation.
---

# Propagate & Distill: Towards Effective Graph Learners Using Propagation-Embracing MLPs

## Quick Facts
- arXiv ID: 2311.17781
- Source URL: https://arxiv.org/abs/2311.17781
- Reference count: 40
- Key outcome: P&D achieves up to 3.74% accuracy improvement over baseline methods by injecting structural information during knowledge distillation from GNNs to MLPs.

## Executive Summary
This paper introduces Propagate & Distill (P&D), a method that enhances knowledge distillation from Graph Neural Networks (GNNs) to Multi-Layer Perceptrons (MLPs) by explicitly injecting structural information. Unlike prior approaches that rely on positional embeddings, P&D recursively propagates the teacher GNN's output along the graph structure before distillation. This process can be interpreted as approximating inverse propagation, allowing the student MLP to learn both feature transformation and graph propagation. Experiments on real-world datasets show consistent performance improvements over baseline methods in both transductive and inductive settings.

## Method Summary
P&D improves knowledge distillation by propagating the teacher GNN's output recursively over the graph structure using the formula Pₜ^(l+1) = γÂPₜ^l + (1-γ)Pₜ^l, where γ controls propagation strength. This approximated inverse propagation operation encodes neighborhood information into the teacher's predictions before they are distilled to the student MLP. A variant called P&D-fix prevents training node predictions from being altered by propagation, maintaining their initial teacher predictions. The method is evaluated against baselines like GLNN and InvKD on multiple graph datasets.

## Key Results
- P&D achieves up to 3.74% accuracy improvement over baseline methods on standard graph datasets
- Performance gains are consistent across both transductive and inductive learning settings
- P&D outperforms InvKD in 4 out of 6 datasets, demonstrating the effectiveness of explicit structural injection

## Why This Works (Mechanism)

### Mechanism 1
Recursive propagation of teacher GNN outputs approximates inverse propagation, enabling MLP to learn structural information without expensive matrix inversions. The recursive smoothing operation encodes neighborhood influence, particularly effective when graphs have high homophily. Low homophily graphs where neighboring nodes have dissimilar labels can break this mechanism.

### Mechanism 2
P&D-fix variant improves performance by preventing training nodes from being influenced by propagation, maintaining their original teacher predictions. This assumes training nodes have more reliable predictions that should not be altered by propagation. When training nodes have unreliable predictions, fixing them prevents beneficial corrections.

### Mechanism 3
P&D provides graph signal denoising regularization during distillation, smoothing student outputs. The propagation operation acts as a graph Laplacian smoothing term, encouraging student MLP to produce predictions consistent with graph structure. When graph structure is irrelevant to the task or when features already capture necessary information, this mechanism breaks down.

## Foundational Learning

- **Knowledge Distillation (KD)**: Core framework for transferring knowledge from GNN teacher to MLP student. Quick check: What is the primary loss function used in KD between teacher and student models?

- **Graph Neural Networks (GNNs) and Message Passing**: Understanding how GNNs separate feature transformation T and propagation Π is crucial for P&D's design. Quick check: What are the two main phases in a typical GNN layer?

- **Graph Homophily**: P&D's effectiveness relies on high homophily for propagation to distribute correct label information. Quick check: How is graph homophily defined in terms of connected node labels?

## Architecture Onboarding

- **Component map**: Teacher GNN (GraphSAGE 2-layer, 128 hidden dims) -> Propagation module (recursive smoothing) -> Student MLP (same architecture as GLNN) -> Loss function (KL divergence)

- **Critical path**: 1) Forward pass through teacher GNN to get Pₜ; 2) Apply T iterations of propagation to get ¯Π(Pₜ, Â); 3) Forward pass through student MLP to get Pₛ; 4) Compute KL(Pₛ, ¯Π(Pₜ, Â)); 5) Backpropagate to update student MLP weights

- **Design tradeoffs**: T vs. performance (higher T generally improves performance but increases computation); γ vs. propagation strength (higher γ creates stronger smoothing but may oversmooth); P&D vs. P&D-fix (P&D-fix prevents training node drift but may limit learning from propagation)

- **Failure signatures**: Student MLP performance worse than GLNN baseline; performance degradation on low homophily graphs; overfitting when T is too large for the dataset size

- **First 3 experiments**: 1) Compare P&D with GLNN baseline on Cora dataset (transductive setting); 2) Test P&D-fix variant with different γ values on Citeseer; 3) Evaluate performance degradation on synthetic low homophily graph

## Open Questions the Paper Calls Out

- How does performance vary when using different GNN architectures (e.g., GAT, GCN, GraphSAGE) as teachers? The paper uses GraphSAGE but doesn't compare across different GNN architectures.

- What is the theoretical limit of the error tolerance (ϵ) for self-correction via propagation, and how does it vary with graph properties? Theorem H.1 provides a bound but uses simplifying assumptions.

- How does the recursive propagation approximation in P&D compare to exact inverse propagation in terms of computational efficiency and accuracy? The paper proposes P&D as a computationally efficient alternative but doesn't directly compare.

## Limitations

- The recursive propagation approximation lacks rigorous mathematical justification and convergence properties are unclear.
- Performance improvements are demonstrated primarily on citation networks with high homophily, limiting generalization to other graph types.
- Computational efficiency claims are not empirically validated with runtime comparisons.

## Confidence

- **High confidence**: Experimental methodology and implementation details are well-documented with publicly available code.
- **Medium confidence**: Theoretical interpretation of P&D as approximating inverse propagation is plausible but not rigorously proven.
- **Low confidence**: Claims about effectiveness on low homophily graphs are not well-supported by experiments.

## Next Checks

1. Rigorously analyze convergence properties of recursive propagation and formally establish its relationship to inverse propagation with error bounds.

2. Evaluate P&D on graphs with varying homophily levels, including synthetic graphs with controlled properties and real-world graphs from different domains.

3. Conduct systematic ablation studies varying γ and T across wider ranges to identify optimal configurations and sensitivity to hyperparameters.