---
ver: rpa2
title: Teaching Large Language Models to Self-Debug
arxiv_id: '2304.05128'
source_url: https://arxiv.org/abs/2304.05128
tags:
- number
- code
- table
- name
- returns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents Self-Debugging, which teaches large language
  models to debug their predicted programs via few-shot prompting. We show that the
  model can self-debug without human feedback on code correctness or error messages
  by investigating execution results and explaining the generated code in natural
  language.
---

# Teaching Large Language Models to Self-Debug

## Quick Facts
- arXiv ID: 2304.05128
- Source URL: https://arxiv.org/abs/2304.05128
- Authors: 
- Reference count: 40
- This work presents Self-Debugging, which teaches large language models to debug their predicted programs via few-shot prompting.

## Executive Summary
This paper introduces Self-Debugging, a method that teaches large language models to debug their own code predictions without human feedback. The approach leverages few-shot prompting to demonstrate the complete debugging workflow, including code generation, execution, natural language explanation, and iterative refinement. The method is evaluated across multiple code generation tasks including text-to-SQL, code translation, and text-to-Python, showing consistent accuracy improvements of 2-3% on average and up to 12% in some cases, while notably improving sample efficiency.

## Method Summary
Self-Debugging uses few-shot prompting with GPT-3 code-davinci-002 to teach models the iterative debugging process. The method generates initial code, executes it to obtain results, produces natural language explanations of the code's behavior, compares these explanations to problem specifications or test results, generates feedback messages, and produces revised code. This process repeats up to 10 turns. The approach works with or without unit tests, using code execution and natural language explanations as feedback signals when tests are unavailable.

## Key Results
- Consistently improves baseline accuracy by 2-3% on the Spider text-to-SQL benchmark
- Improves accuracy on hardest-level problems by 9% on Spider
- Achieves up to 12% improvement on TransCoder C++-to-Python translation and MBPP text-to-Python generation
- Matches or outperforms baseline models using more than 10x the samples through improved sample efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can identify their own code errors by generating natural language explanations of the code's behavior and comparing it to the problem specification.
- Mechanism: The model executes the generated code, then produces a detailed explanation of what the code does line-by-line. By comparing this explanation to the problem description, the model can identify mismatches that indicate bugs.
- Core assumption: The model's ability to generate accurate natural language explanations of code correlates with its ability to detect semantic errors in that code.
- Evidence anchors:
  - [abstract] "without any feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language"
  - [section 3.3] "Inspired by these observations, instead of teaching the large language model to predict error messages, we propose to teach the model to self-debug via explaining the generated code"
  - [corpus] Weak - the corpus shows related works on self-debugging but doesn't directly support the specific mechanism of code explanation for error detection
- Break condition: This mechanism breaks when the code is syntactically correct but semantically wrong in ways that don't manifest in simple execution or when the model's explanation generation is too verbose/inaccurate to allow meaningful comparison.

### Mechanism 2
- Claim: Leveraging execution results from unit tests provides richer feedback signals that enable more effective debugging than simple correctness feedback.
- Mechanism: When unit tests are available, the model receives detailed feedback about which tests passed/failed and what the actual vs expected outputs were. This information guides more targeted fixes.
- Core assumption: The model can effectively interpret and act on unit test feedback to identify and fix bugs.
- Evidence anchors:
  - [abstract] "by leveraging feedback messages and reusing failed predictions, SELF -D EBUGGING notably improves sample efficiency"
  - [section 3.2] "For code generation tasks where the problem description includes unit tests, besides utilizing code execution to check code correctness, we can also present the execution results in the feedback message"
  - [corpus] Moderate - related works mention test-based debugging but don't specifically validate the effectiveness of this approach
- Break condition: This mechanism breaks when the unit tests are insufficient to capture all bugs, or when the model fails to interpret the test feedback correctly.

### Mechanism 3
- Claim: Few-shot prompting with exemplars of successful debugging sessions teaches the model the debugging process without requiring additional training.
- Mechanism: The prompt includes demonstrations of the complete debugging workflow - code generation, execution, explanation, feedback, and revision - which teaches the model this iterative process.
- Core assumption: Large language models can learn procedural tasks from few-shot exemplars without fine-tuning.
- Evidence anchors:
  - [abstract] "we propose SELF -D EBUGGING, which teaches a large language model to debug its predicted program via few-shot demonstrations"
  - [section 2] "Few-shot prompting aims to instruct the language model to solve a task with several input-output demonstrations"
  - [corpus] Strong - the corpus includes multiple related works that validate few-shot prompting for similar tasks
- Break condition: This mechanism breaks when the exemplars are insufficient to cover the range of debugging scenarios encountered in practice.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: SELF -D EBUGGING requires the model to generate intermediate reasoning (code explanations) before producing the final output, similar to how chain-of-thought prompting elicits reasoning in language models.
  - Quick check question: Can you explain why generating code explanations as an intermediate step helps the model identify bugs?

- Concept: Execution-based code selection
  - Why needed here: The framework uses code execution to filter out incorrect predictions before debugging, improving the efficiency of the debugging process.
  - Quick check question: How does execution-based selection improve the effectiveness of SELF -D EBUGGING?

- Concept: Few-shot learning
  - Why needed here: SELF -D EBUGGING relies on few-shot prompting to teach the debugging process without additional training, making it a zero-shot or few-shot approach.
  - Quick check question: What are the advantages of using few-shot prompting for teaching debugging over fine-tuning a separate model?

## Architecture Onboarding

- Component map: Prompt generator -> Code executor -> Explanation generator -> Feedback analyzer -> Code refiner -> (loop back to executor)
- Critical path:
  1. Generate initial code
  2. Execute code to get results
  3. Generate explanation of code behavior
  4. Compare explanation to problem description/test results
  5. Generate feedback message
  6. Generate revised code
  7. Repeat until correct or max iterations reached
- Design tradeoffs:
  - Number of exemplars in prompt vs context window limits
  - Depth of code explanation vs computational cost
  - Number of debugging iterations vs performance gains
  - Use of unit tests vs reliance on code explanation alone
- Failure signatures:
  - Infinite loops in debugging when model can't find the bug
  - Explanation generation that doesn't match code behavior
  - Code revisions that don't address the identified issues
  - Degradation in performance when adding more exemplars
- First 3 experiments:
  1. Test debugging effectiveness with simple vs complex feedback messages
  2. Measure performance improvement from adding code explanation vs unit test feedback
  3. Evaluate the impact of different numbers of exemplars in the few-shot prompt

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's ability to explain code line-by-line impact its debugging performance compared to models that do not generate explanations?
- Basis in paper: [explicit] The paper states "Empirically, we observe that a large language model can also benefit from rubber duck debugging, especially when unit tests are not available."
- Why unresolved: The paper does not provide a direct comparison between debugging performance with and without code explanation.
- What evidence would resolve it: An ablation study comparing debugging accuracy with and without code explanation would provide direct evidence.

### Open Question 2
- Question: Can Self-Debugging be applied to other programming tasks beyond code generation, such as code completion or code summarization?
- Basis in paper: [inferred] The paper demonstrates Self-Debugging's effectiveness on code generation tasks, but does not explore its applicability to other programming tasks.
- Why unresolved: The paper focuses solely on code generation and does not investigate the generalizability of Self-Debugging to other programming tasks.
- What evidence would resolve it: Experiments applying Self-Debugging to code completion and code summarization tasks would demonstrate its generalizability.

### Open Question 3
- Question: How does the quality of the model's code explanations impact its debugging performance? Are there certain types of explanations that are more effective than others?
- Basis in paper: [explicit] The paper states "We consider improving the model's ability to conduct all these steps as important future work" and mentions that "better code explanation ability leads to better debugging performance."
- Why unresolved: The paper does not provide a detailed analysis of the relationship between explanation quality and debugging performance.
- What evidence would resolve it: An analysis of the correlation between explanation quality (e.g., using metrics like BLEU score) and debugging accuracy would shed light on this relationship.

## Limitations
- Effectiveness may be limited to tasks where semantic understanding is straightforward to verbalize
- Relies heavily on model's ability to generate accurate natural language explanations, which may degrade for complex code patterns
- Evaluation focuses on benchmark datasets, real-world applicability may vary depending on execution environment availability

## Confidence
- High Confidence: The sample efficiency improvements demonstrated through reusing failed predictions and leveraging execution feedback are well-supported by the empirical results
- Medium Confidence: The general debugging effectiveness across different task types is demonstrated, though the specific contribution of code explanation vs. unit test feedback is not fully isolated
- Low Confidence: The mechanism by which natural language code explanations directly lead to bug identification is not fully explained or validated independently from the overall debugging process

## Next Checks
1. **Ablation Study on Code Explanation:** Test whether the debugging improvements come primarily from code explanation generation or from the iterative refinement process itself by comparing with a version that skips code explanation and only uses test feedback.
2. **Complexity Scaling Analysis:** Evaluate how SELF-DEBUGGING performance scales with increasing code complexity and semantic error types that may not be easily captured in natural language explanations.
3. **Real-World Dataset Validation:** Test the approach on a more diverse set of real-world code generation tasks with varying requirements and execution environments to assess generalizability beyond benchmark datasets.