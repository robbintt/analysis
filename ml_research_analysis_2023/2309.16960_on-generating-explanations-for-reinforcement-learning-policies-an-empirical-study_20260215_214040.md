---
ver: rpa2
title: 'On Generating Explanations for Reinforcement Learning Policies: An Empirical
  Study'
arxiv_id: '2309.16960'
source_url: https://arxiv.org/abs/2309.16960
tags:
- policy
- search
- specification
- agent
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a method for generating explanations for reinforcement\
  \ learning policies using Linear Temporal Logic (LTL). The key idea is to represent\
  \ explanations as LTL formulae of the form F(\xB7) \u2227 G(\xB7), where F specifies\
  \ eventual objectives and G specifies global constraints."
---

# On Generating Explanations for Reinforcement Learning Policies: An Empirical Study

## Quick Facts
- arXiv ID: 2309.16960
- Source URL: https://arxiv.org/abs/2309.16960
- Authors: 
- Reference count: 23
- Key outcome: This paper presents a method for generating explanations for reinforcement learning policies using Linear Temporal Logic (LTL), validated in a simulated capture-the-flag environment where true target LTL specifications are successfully recovered.

## Executive Summary
This paper introduces a novel approach for explaining reinforcement learning (RL) policies using Linear Temporal Logic (LTL) formulae. The method represents explanations as LTL formulae of the form F(·) ∧ G(·), where F specifies eventual objectives and G specifies global constraints. A local search algorithm iteratively modifies LTL specifications and evaluates them using weighted KL divergence to find the best explanation for a given target policy. The approach is validated in a simulated capture-the-flag environment, demonstrating successful recovery of true target LTL specifications.

## Method Summary
The method centers on a local search algorithm that explores a space of LTL formulae to find explanations for RL policies. Each LTL specification is converted to a Finite State Predicate Automaton (FSPA), which is then used to construct an augmented MDP where rewards are based on transition robustness. The RL policy for each candidate LTL specification is optimized using PPO, and its similarity to the target policy is evaluated using weighted KL divergence, where weights are proportional to the entropy of the target policy's action distribution. The search iteratively modifies the LTL formula through neighborhood operations and selects the best candidates based on similarity to the target policy.

## Key Results
- Successfully recovered true target LTL specifications from optimized policies in a capture-the-flag environment
- Demonstrated the effectiveness of weighted KL divergence in emphasizing similarity where the target policy is most certain
- Showed that the local search algorithm can navigate the LTL specification space efficiently to find good explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local search over LTL formulae space efficiently finds policies that approximate the target policy by leveraging neighborhood connectivity of LTL syntax
- Mechanism: The search treats each LTL specification as a node in a connected graph where edges represent single-bit flips in the formula representation
- Core assumption: The space of LTL formulae with the chosen structure (F(·) ∧ G(·)) is fully connected under the defined neighborhood operations
- Evidence anchors:
  - [abstract]: "Our approach centers on a family of Linear Temporal Logic (LTL) formulae that is endowed with a concept of neighborhood, which in turn is amenable to a local-search algorithm"
  - [section III-C]: "It is not hard to see that the class of LTL-formulae considered in this paper (i.e., of the form F(•) ∧ G (•)) are completely connected under this notion of a neighborhood"

### Mechanism 2
- Claim: Using KL divergence weighted by policy certainty at each state provides a robust similarity metric that emphasizes alignment where the target policy is most confident
- Mechanism: The algorithm samples non-trap states and computes weighted KL divergence where weights are proportional to the entropy of the target policy's action distribution
- Core assumption: States where the target policy has low entropy (high certainty) are more important to match correctly than uncertain states
- Evidence anchors:
  - [section III-B]: "We use a weighted average to emphasize similarity between policies at states where the target policy is highly certain about what action to take"

### Mechanism 3
- Claim: FSPA-augmented MDP construction with reward shaping based on transition robustness enables RL training to satisfy complex temporal logic specifications
- Mechanism: Each LTL specification is converted to an FSPA, which is then used to construct an augmented MDP where rewards are based on transition robustness
- Core assumption: The FSPA-augmented MDP correctly captures the semantics of the LTL specification through its reward structure
- Evidence anchors:
  - [section II-B]: "A feasible LTL-specification can be transformed into a Finite State Predicate Automaton (FSPA) using ω-automaton manipulation"
  - [section III-B]: "We first synthesize the FSPA A associated with a node's specification ϕ, and construct the resulting FSPA-augmented MDP MA"

## Foundational Learning

- Concept: Linear Temporal Logic (LTL) syntax and semantics
  - Why needed here: The entire explanation method relies on representing objectives and constraints as LTL formulae of the form F(·) ∧ G(·)
  - Quick check question: What is the difference between F(φ) and G(φ) in LTL, and how would you write "eventually reach the flag and always avoid obstacles" in this notation?

- Concept: Finite State Predicate Automata (FSPA) and their relationship to LTL
  - Why needed here: LTL specifications must be converted to FSPA to construct the augmented MDP for policy optimization
  - Quick check question: Given an LTL formula F(p) ∧ G(q), describe the basic structure of the corresponding FSPA (states, transitions, accepting conditions)

- Concept: Policy entropy and its role in similarity metrics
  - Why needed here: The weighted KL divergence metric uses policy entropy to weight state importance in the similarity calculation
  - Quick check question: Why would you weight KL divergence by the entropy of the target policy rather than using uniform weights across all states?

## Architecture Onboarding

- Component map:
  - LTL specification space with bit-vector representation -> FSPA synthesis engine -> FSPA-augmented MDP construction -> RL training pipeline (PPO) -> KL divergence similarity calculator -> Local search controller

- Critical path:
  1. Initialize random LTL specification
  2. Convert to FSPA and build augmented MDP
  3. Train RL policy using PPO
  4. Compute weighted KL divergence to target policy
  5. Generate neighborhood specifications
  6. Evaluate neighbors and update current best
  7. Repeat until convergence or max steps

- Design tradeoffs:
  - Bit-vector representation vs. symbolic LTL manipulation: Bit vectors enable efficient neighborhood operations but limit expressiveness
  - Weighted vs. unweighted KL divergence: Weighted emphasizes confident decisions but may miss important uncertain regions
  - Multiple RL replicates vs. single training: Multiple replicates improve robustness but increase computational cost

- Failure signatures:
  - Training curves that never converge to positive rewards (invalid specifications)
  - Search getting stuck in local minima with poor similarity
  - Numerical instability in KL divergence calculations due to entropy normalization
  - FSPA synthesis failures for complex specifications

- First 3 experiments:
  1. Verify FSPA synthesis works correctly for simple LTL formulas by checking automaton structure matches expected semantics
  2. Test RL training on augmented MDPs with known specifications to ensure policies satisfy temporal constraints
  3. Validate KL divergence calculation by comparing similarity between identical policies (should be zero) and completely different policies (should be high)

## Open Questions the Paper Calls Out
- How does the choice of atomic predicates affect the quality and interpretability of the generated LTL explanations?
- How does the performance of the proposed method scale with larger state spaces and more complex LTL formulae?
- How does the proposed method compare to alternative approaches for explaining RL policies, such as attention mechanisms or decision trees?

## Limitations
- The method's scalability to larger state spaces and more complex LTL formulae remains unclear
- The approach is validated only in a single simulated environment, limiting generalizability
- The choice of atomic predicates is critical but lacks systematic guidelines

## Confidence
- Medium: The basic framework of using local search over LTL formulae to explain policies is well-grounded in the LTL and RL literature
- Low: Claims about the effectiveness of the weighted KL divergence metric and the connectivity of the LTL search space lack independent validation
- Medium: The FSPA-augmented MDP approach for enforcing temporal constraints through reward shaping follows established patterns but needs more empirical support

## Next Checks
1. Test the LTL neighborhood connectivity claim by attempting to navigate between randomly selected formulae in the search space and measuring path lengths
2. Compare weighted KL divergence against alternative similarity metrics (unweighted KL, cosine similarity, etc.) on the same policy pairs to assess the benefit of entropy weighting
3. Validate the FSPA-augmented MDP construction by testing whether policies trained on the augmented MDP actually satisfy the corresponding LTL specifications in the original environment