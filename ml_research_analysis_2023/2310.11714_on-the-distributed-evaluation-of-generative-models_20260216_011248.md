---
ver: rpa2
title: On the Distributed Evaluation of Generative Models
arxiv_id: '2310.11714'
source_url: https://arxiv.org/abs/2310.11714
tags:
- generative
- evaluation
- learning
- federated
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the evaluation of generative models in distributed
  settings with heterogeneous client data distributions. It analyzes two common aggregate
  evaluation metrics: FID-avg (mean of clients'' individual FID scores) and FID-all
  (FID with respect to the collective dataset).'
---

# On the Distributed Evaluation of Generative Models

## Quick Facts
- arXiv ID: 2310.11714
- Source URL: https://arxiv.org/abs/2310.11714
- Reference count: 40
- Primary result: FID-avg and FID-all can lead to inconsistent model rankings under heterogeneous client data distributions, while KID-avg and KID-all always result in consistent rankings.

## Executive Summary
This paper studies the evaluation of generative models in distributed/federated settings where client data distributions are heterogeneous (non-i.i.d.). The authors analyze two common aggregate evaluation metrics: FID-avg (mean of clients' individual FID scores) and FID-all (FID with respect to the collective dataset). They prove that these metrics can lead to inconsistent model rankings under heterogeneous data distributions, meaning the optimal model according to one metric may differ from the optimal model according to the other. In contrast, they show that KID-avg and KID-all always result in the same model rankings, despite potentially different absolute values. The results highlight the importance of choosing appropriate evaluation metrics in federated learning scenarios with non-i.i.d. data.

## Method Summary
The paper analyzes aggregate evaluation metrics for generative models in federated learning settings. It compares two forms of evaluation: client-averaged metrics (FID-avg, KID-avg) and collective-dataset metrics (FID-all, KID-all). The authors provide theoretical analysis showing that for FID, the two aggregation methods can yield inconsistent model rankings under heterogeneous data distributions, while for KID they always produce consistent rankings. The theoretical claims are validated through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-32 datasets using trained GANs and diffusion models in federated settings.

## Key Results
- FID-avg and FID-all can produce inconsistent model rankings under heterogeneous client data distributions, potentially leading to different optimal generative models.
- KID-avg and KID-all always result in the same model rankings despite potentially different absolute values, due to the convex nature of MMD.
- The inconsistency between FID-avg and FID-all is particularly pronounced in variance-limited federated datasets where intra-client variance is artificially reduced.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FID-avg and FID-all can produce inconsistent model rankings under heterogeneous client data distributions.
- Mechanism: FID-avg is the arithmetic mean of individual client FIDs, while FID-all computes FID between the generated model and the collective mixture distribution. Because the optimal covariance matrix for FID-avg does not depend on differences in individual client mean vectors, while the optimal covariance matrix for FID-all does, the two aggregations can favor different generators depending on their variance and mean properties.
- Core assumption: The FID score is convex in the data distribution, and differences in client mean vectors create meaningful covariance optimization differences.
- Evidence anchors:
  - [abstract] "We prove that the model rankings according to the FID-all and FID-avg scores could be inconsistent, which can lead to different optimal generative models according to the two aggregate scores."
  - [section 4.1] "Theorem 1 shows that the optimal covariance matrices under FID-all and FID-avg could be significantly different in heterogeneous settings with different µi's."
- Break condition: If client distributions are identical (i.i.d.), both aggregations converge to the same ranking.

### Mechanism 2
- Claim: KID-avg and KID-all always result in the same model rankings despite potentially different absolute values.
- Mechanism: KID-avg decomposes into KID-all plus a constant term (the sum over clients of KID between the average distribution and each client). Since this extra term does not depend on the generator, the relative ordering of models is preserved.
- Core assumption: KID distance is defined via maximum mean discrepancy (MMD), which is convex in the distributions and admits a barycenter interpretation.
- Evidence anchors:
  - [abstract] "In the case of KID metric, we prove that scoring a group of generative models using the clients' averaged KID score will result in the same ranking as that of a centralized KID evaluation over a collective reference set containing all the clients' data."
  - [section 4.2] "Theorem 2... proves that unlike the FID-case, the KID-all and KID-avg will result in a consistent ordering of the models and there is a monotonic relationship between the two aggregate scores."
- Break condition: If the kernel used in KID is not characteristic or loses information in the embedding space, rankings could diverge.

### Mechanism 3
- Claim: For precision/recall and density/coverage metrics, the two aggregation forms can yield inconsistent rankings, except recall where they always agree.
- Mechanism: These metrics are computed over local client datasets or the aggregated dataset, and their definitions lead to different sensitivities to data heterogeneity. Recall-all equals recall-avg because recall is defined as an average over generated data and is unaffected by client distribution changes.
- Core assumption: The aggregation formula for each metric behaves differently under mixture distributions.
- Evidence anchors:
  - [section 5.4] "In the case of Precision/Recall... we utilized the official implementation... The numerical scores indicate that in a heterogeneous data setting, the two aggregate precision scores may not consistently rank the generative models. On the other hand, based on the recall's definition... Recall-all and Recall-avg will always take the same value."
  - [corpus] Weak evidence: only one neighbor paper discusses precision/recall in federated learning, but not in the context of aggregation inconsistency.
- Break condition: If metrics are redefined to use identical underlying distributions, rankings could align.

## Foundational Learning

- Concept: Fréchet Inception Distance (FID) as 2-Wasserstein distance between Gaussian approximations of real and generated data.
  - Why needed here: FID is the primary metric under study for distributed evaluation; understanding its mathematical form is critical to grasping why its aggregations behave differently.
  - Quick check question: If two datasets have identical means but different covariances, will their FID to a fixed generator be the same?

- Concept: Kernel Inception Distance (KID) as maximum mean discrepancy (MMD) between real and generated data embeddings.
  - Why needed here: KID's MMD formulation allows the decomposition that proves consistent rankings; without this, the aggregation behavior would be opaque.
  - Quick check question: If you add a constant term to every MMD score, does the ranking of models change?

- Concept: Convexity of evaluation metrics in data distributions.
  - Why needed here: Convexity ensures that averaging over clients upper bounds the collective-data-based score, which is foundational to the paper's theoretical claims.
  - Quick check question: Is the squared L2 distance between means convex in the underlying distributions?

## Architecture Onboarding

- Component map: Data pipeline → Pre-trained InceptionV3 → Embedding space → Distance metric computation → Aggregation (avg vs all) → Model ranking
- Critical path: 1) Fetch embeddings for all real and generated samples. 2) Compute per-client statistics (mean, covariance for FID; MMD kernel matrix for KID). 3) Aggregate via chosen formula. 4) Compare across models.
- Design tradeoffs: Using per-client FID is computationally cheaper but privacy-preserving; computing FID-all requires access to all embeddings, which may be infeasible in strict FL settings.
- Failure signatures: Inconsistent rankings between FID-avg and FID-all suggest high heterogeneity; identical rankings suggest near-i.i.d. or that the metric is insensitive to the heterogeneity pattern.
- First 3 experiments:
  1. Generate two synthetic Gaussian datasets with different means, train two generators with different variances, compare FID-avg vs FID-all rankings.
  2. Simulate federated CIFAR-10 with 10 clients each holding one class, evaluate ideal "class-specific" generators and neural net generators under both FID aggregations.
  3. Create variance-limited federated datasets by keeping only nearest neighbors per class, generate variance-controlled synthetic generators, and measure the shift in FID-avg vs FID-all as intra-client variance changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the evaluation inconsistencies between FID-avg and FID-all manifest in real-world federated learning scenarios beyond image datasets?
- Basis in paper: [explicit] The paper discusses inconsistencies between FID-avg and FID-all in CIFAR-10, CIFAR-100, and ImageNet-32 datasets, but notes limitations in applying results to other data types like text and audio.
- Why unresolved: The paper's experiments are limited to image datasets, and there's no exploration of how these inconsistencies might appear in other domains.
- What evidence would resolve it: Experiments applying the same evaluation methodology to federated learning scenarios with text, audio, or other data types to compare FID-avg and FID-all behaviors.

### Open Question 2
- Question: Does the consistency between KID-avg and KID-all hold for other evaluation metrics beyond generative models, such as in general federated learning model evaluation?
- Basis in paper: [explicit] The paper proves that KID-avg and KID-all always produce consistent rankings for generative models, while FID-avg and FID-all do not.
- Why unresolved: The paper only explores this property in the context of generative model evaluation and doesn't examine whether this consistency extends to other evaluation scenarios in federated learning.
- What evidence would resolve it: Testing KID-based evaluation consistency across different types of federated learning tasks and model architectures beyond generative models.

### Open Question 3
- Question: How do non-arithmetic averaging methods affect the consistency of evaluation metrics in federated learning?
- Basis in paper: [inferred] The paper mentions that understanding "the behavior of the aggregate score using non-arithmetic averaging" could be useful for evaluating deep generative models in federated learning contexts.
- Why unresolved: The paper focuses on arithmetic averaging (mean) for FID-avg and KID-avg but doesn't explore how other aggregation methods might affect evaluation consistency.
- What evidence would resolve it: Comparative experiments testing geometric mean, weighted averages, or other non-arithmetic aggregation methods for FID and KID scores in federated learning settings.

### Open Question 4
- Question: What is the relationship between intra-client variance and the magnitude of inconsistencies between FID-avg and FID-all?
- Basis in paper: [explicit] The paper shows that FID-avg and FID-all can lead to inconsistent rankings, particularly in variance-limited federated datasets, but doesn't systematically study how the degree of inconsistency varies with intra-client variance.
- Why unresolved: While the paper demonstrates that inconsistencies exist, it doesn't quantify how different levels of intra-client variance affect the magnitude of these inconsistencies.
- What evidence would resolve it: A systematic study varying intra-client variance across multiple datasets and measuring the corresponding changes in FID-avg and FID-all discrepancies.

## Limitations

- The theoretical claims hinge on idealized Gaussian assumptions for data distributions, which may not hold in real federated settings with non-Gaussian or multimodal client distributions.
- The experimental validation focuses on standard vision datasets (CIFAR-10, CIFAR-100, ImageNet-32), leaving performance on other modalities like text or time series unexplored.
- Other evaluation metrics beyond FID and KID (precision/recall, density/coverage) are only briefly discussed, with limited theoretical and experimental coverage of their aggregation behavior.

## Confidence

- High: Claims about FID-avg and FID-all producing inconsistent rankings under heterogeneity (supported by both theory and experiments).
- High: Claims about KID-avg and KID-all producing consistent rankings (theoretically proven and experimentally validated).
- Medium: Claims about precision/recall and density/coverage metrics, due to limited theoretical and experimental coverage.

## Next Checks

1. Test the aggregation behavior of FID and KID metrics on non-Gaussian (e.g., mixture or multimodal) client distributions to assess robustness.
2. Extend experiments to other data modalities (e.g., text or time series) to verify if the observed patterns generalize beyond image data.
3. Rigorously analyze and experimentally validate the aggregation behavior of precision/recall and density/coverage metrics under heterogeneous client distributions.