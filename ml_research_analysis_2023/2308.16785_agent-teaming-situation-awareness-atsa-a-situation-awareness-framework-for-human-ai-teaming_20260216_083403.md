---
ver: rpa2
title: 'Agent Teaming Situation Awareness (ATSA): A Situation Awareness Framework
  for Human-AI Teaming'
arxiv_id: '2308.16785'
source_url: https://arxiv.org/abs/2308.16785
tags:
- teaming
- human
- team
- situation
- awareness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes the Agent Teaming Situation Awareness (ATSA)
  framework to address the need for a unified model of situation awareness (SA) in
  human-AI teaming (HAT). The ATSA framework integrates cognitive mechanisms from
  individual and team SA models into a bidirectional, dynamic structure involving
  three core components: teaming understanding (TU), teaming control (TC), and the
  world.'
---

# Agent Teaming Situation Awareness (ATSA): A Situation Awareness Framework for Human-AI Teaming

## Quick Facts
- arXiv ID: 2308.16785
- Source URL: https://arxiv.org/abs/2308.16785
- Reference count: 0
- Key outcome: Proposes ATSA framework integrating individual and team SA models for human-AI teaming with bidirectional dynamic interactions

## Executive Summary
This paper presents the Agent Teaming Situation Awareness (ATSA) framework, a theoretical model designed to address the unique challenges of Situation Awareness in human-AI teaming. ATSA integrates cognitive mechanisms from individual and team SA models into a bidirectional, dynamic structure involving three core components: teaming understanding, teaming control, and the world. The framework emphasizes cohesive collaboration through individual and team perceptual cycles linked by a transactive part, enabling agents to transact mental models and actions. ATSA provides a theoretical foundation for designing human-AI teaming systems that accommodate both human and AI characteristics while enabling adaptive and evolving dynamics in cooperation and collaboration.

## Method Summary
The ATSA framework was developed through theoretical synthesis and extension of existing SA models to the human-AI teaming context. The authors reviewed key components of Endsley's Three-Level Model and Smith & Hancock's Perceptual Cycle Model, then extended these individual SA models by incorporating distributed SA theory concepts and the specific characteristics of AI agents. The framework integrates individual cycles (human and AI) with a teaming cycle (teaming understanding, teaming control, world) and a transactive part, ensuring bidirectional and dynamic interaction. The minimum viable reproduction plan involves synthesizing individual SA models, extending them to HAT context, and developing the integrated ATSA framework with its transactive processes.

## Key Results
- Proposes a unified framework for SA in human-AI teaming that integrates individual and team SA models
- Introduces a bidirectional SA flow mechanism enabling adaptive coordination between human and AI agents
- Establishes a transactive part that integrates individual and team SA into a coherent system-level SA
- Incorporates dynamic function allocation that adapts roles based on real-time context and constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bidirectional SA flow in ATSA improves team SA over unidirectional models.
- Mechanism: ATSA uses two-way SA sharing between human and AI agents, where each agent's SA informs the other's TU and TC, enabling adaptive coordination.
- Core assumption: Both human and AI agents can generate SA at similar levels of abstraction and share it meaningfully.
- Evidence anchors:
  - [abstract] "bidirectional, and dynamic interaction"
  - [section] "ATSA emphasizes cohesive and effective HAT through structures and components, including teaming understanding, teaming control, and the world"
  - [corpus] Weak - no direct citations to bidirectional SA models in corpus.
- Break condition: If one agent's SA generation lags or is incompatible with the other's, the cycle stalls and degrades overall team SA.

### Mechanism 2
- Claim: The transactive part integrates individual and team SA into a coherent system-level SA.
- Mechanism: Individual mental models and actions are transacted through a shared interface, merging into the teaming cycle and allowing the team to act on a unified SA representation.
- Core assumption: Transacted SA elements can be decoded and integrated without loss of meaning by the receiving agent.
- Evidence anchors:
  - [abstract] "adhesive transactive part"
  - [section] "These two constructs in ATSA are linked through the transactive part, which is adapted from the 'transactive SA' concept in distributed SA theory"
  - [corpus] Weak - corpus neighbors do not discuss transactive SA in detail.
- Break condition: If transaction fidelity drops (e.g., due to mismatched vocabularies), the team SA will fragment and performance will suffer.

### Mechanism 3
- Claim: Dynamic function allocation in ATSA adapts roles to context, improving HAT effectiveness.
- Mechanism: Function allocation is driven by complementing flow, which adjusts control authority and task distribution based on real-time constraints, costs, and availability.
- Core assumption: AI and human agents can negotiate function shifts without introducing significant overhead or confusion.
- Evidence anchors:
  - [abstract] "emphasizing adaptive and evolving dynamics in cooperation and collaboration"
  - [section] "Dynamic function allocation, or adaptive automation, proposed to a shift from a strict LOA perspective to a cooperation modes perspective"
  - [corpus] Weak - corpus lacks explicit function allocation research.
- Break condition: If adaptation is too slow or causes role ambiguity, the team will experience process loss and degraded performance.

## Foundational Learning

- Concept: Perceptual cycle theory
  - Why needed here: Forms the basis for both individual and team SA cycles in ATSA.
  - Quick check question: How does the perceptual cycle link schema, exploration, and environmental feedback?

- Concept: Distributed SA theory
  - Why needed here: Provides the "transactive SA" concept used in ATSA's linking mechanism.
  - Quick check question: What is the key difference between shared SA and transactive SA?

- Concept: Dynamic function allocation
  - Why needed here: Enables ATSA to shift control between human and AI based on context.
  - Quick check question: What are the five cooperation modes described for HAT?

## Architecture Onboarding

- Component map: Individual Cycles (Human, AI) → Transactive Part → Teaming Cycle (TU, TC, World) → Feedback Loop
- Critical path: Human SA → Transactive → TU → TC Planning → Action → World → AI SA → Transactive → TU → TC → Action → World → Feedback
- Design tradeoffs: Richer SA sharing improves coordination but increases communication overhead; dynamic function allocation improves adaptability but may introduce latency.
- Failure signatures: (1) SA mismatch between agents, (2) transaction breakdown, (3) delayed function allocation.
- First 3 experiments:
  1. Simulate bidirectional SA flow in a shared environment and measure SA alignment.
  2. Test transactive SA accuracy by varying communication fidelity and observing team SA degradation.
  3. Implement adaptive function allocation in a human-AI driving scenario and measure takeover latency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can teaming situation awareness be effectively measured in real-time during human-AI team interactions?
- Basis in paper: [explicit] The paper identifies this as an urgent need for empirical research on ATSA, suggesting team probe-recall techniques and Co-ordinated Assessment of Situation Awareness of Teams (CAST) as potential approaches.
- Why unresolved: Current SA measurement methods are primarily post-task or focus on individual SA rather than the dynamic, system-level emergent property of teaming SA.
- What evidence would resolve it: Development and validation of real-time measurement tools that capture the dynamic interactions between teaming understanding, teaming control, and the world across multiple agents.

### Open Question 2
- Question: What is the optimal level of teaming SA in HAT, and how should it be calibrated across different cooperation and collaboration scenarios?
- Basis in paper: [explicit] The authors argue that higher SA doesn't necessarily lead to better team performance and that critical information in working memory and long-term memory is sufficient for higher performance.
- Why unresolved: Most SA research assumes a positive linear relationship between SA and performance, but this may not hold in the complex, dynamic HAT context where different complementing flows require different SA levels.
- What evidence would resolve it: Empirical studies across various HAT scenarios that systematically vary SA levels and measure corresponding team performance outcomes.

### Open Question 3
- Question: How do trust and other social factors influence the cognitive processes and cooperation/collaboration dynamics within the ATSA framework?
- Basis in paper: [inferred] The authors acknowledge that social factors like trust can act upon the cognition process (social cognition) and the cooperation/collaboration process, but note that ATSA as a cognitive model doesn't explicitly address these factors.
- Why unresolved: While ATSA provides a robust framework for understanding HAT dynamics, it doesn't fully account for the social dynamics that can significantly impact teaming SA and overall team performance.
- What evidence would resolve it: Longitudinal studies of HAT teams that measure both cognitive factors (as described in ATSA) and social factors like trust, and analyze their interactions and impacts on team performance over time.

## Limitations
- The framework is currently theoretical with no empirical validation data provided
- The transactive part mechanism lacks detailed implementation specifications
- Function allocation adaptation speed and overhead costs are not quantified
- No validation data for SA compatibility across human-AI cognitive differences

## Confidence
- **High confidence**: The theoretical integration of existing SA models into a HAT framework
- **Medium confidence**: The bidirectional SA flow mechanism and its coordination benefits
- **Low confidence**: The practical effectiveness of transactive SA and dynamic function allocation without empirical evidence

## Next Checks
1. Conduct controlled experiments measuring SA alignment in human-AI teams using the proposed bidirectional flow, varying communication frequency and fidelity
2. Test transactive SA accuracy by implementing different information encoding schemes and measuring team SA degradation under communication constraints
3. Implement adaptive function allocation in a human-AI control task and measure the tradeoff between adaptation latency and task performance improvements