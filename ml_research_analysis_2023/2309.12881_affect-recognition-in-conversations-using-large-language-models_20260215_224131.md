---
ver: rpa2
title: Affect Recognition in Conversations Using Large Language Models
arxiv_id: '2309.12881'
source_url: https://arxiv.org/abs/2309.12881
tags:
- llms
- llama-7b
- emotion
- iemocap
- alpaca-7b
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the capacity of large language models (LLMs)
  to recognise human affect in conversations. We evaluate and compare LLMs' performance
  on three datasets (IEMOCAP, EmoWOZ, DAIC-WOZ) covering chit-chat, task-oriented,
  and clinical dialogues.
---

# Affect Recognition in Conversations Using Large Language Models

## Quick Facts
- arXiv ID: 2309.12881
- Source URL: https://arxiv.org/abs/2309.12881
- Reference count: 14
- Key outcome: LLMs approach state-of-the-art emotion recognition performance using only 50% of training data with fine-tuning

## Executive Summary
This study investigates large language models' capacity for affect recognition in conversations across three distinct dialogue types: chit-chat, task-oriented, and clinical. The authors evaluate zero-shot, few-shot in-context learning, and fine-tuned approaches using datasets including IEMOCAP, EmoWOZ, and DAIC-WOZ. Their results demonstrate that while zero-shot performance lags behind supervised methods, instruction-following fine-tuning and few-shot learning significantly improve LLM performance, with models approaching state-of-the-art levels using only half the training data. The study also reveals that LLMs exhibit robustness to automatic speech recognition errors in emotion recognition tasks, though this robustness does not extend to depression detection.

## Method Summary
The authors employ a comprehensive evaluation framework using three dialogue datasets representing different conversational contexts. They implement zero-shot learning with carefully designed prompt templates, few-shot in-context learning with varying numbers of examples per class, and task-specific fine-tuning using LoRA adapters. ASR error impact is assessed through both ground-truth transcript comparisons and simulated transcription errors using Whisper-medium and fine-tuned LLaMA-7B models. Performance is measured using dataset-specific metrics including weighted/unweighted accuracy for IEMOCAP, F1 scores for EmoWOZ, and F1/RMSE for DAIC-WOZ depression detection.

## Key Results
- LLMs approach state-of-the-art emotion recognition performance using only 50% of training data with fine-tuning
- Zero-shot LLM performance falls short of supervised state-of-the-art methods across all datasets
- LLMs demonstrate robustness to ASR errors in emotion recognition but not in depression detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs benefit from instruction-following fine-tuning when adapting to emotion recognition tasks.
- Mechanism: Fine-tuning on instruction demonstrations helps LLMs better utilize task definitions and label descriptions in prompts.
- Core assumption: LLMs have sufficient capacity to leverage explicit task definitions when present in prompts.
- Evidence anchors: [abstract] "We explored various setups, including zero-shot learning, few-shot in-context learning, and task-specific fine-tuning, all facilitated by specially designed prompts." [section] "Fine-tuning LLMs with instruction-following demonstrations facilitates more effective utilization of the prompt."

### Mechanism 2
- Claim: LLMs are generally robust to ASR errors in emotion recognition but not in depression detection.
- Mechanism: ASR errors accumulate differently across dialogue types and affect tasks requiring longer context (depression detection) more severely.
- Core assumption: The prompt length and complexity differ between emotion recognition and depression detection tasks.
- Evidence anchors: [abstract] "Additionally, we assessed the impact of automatic speech recognition errors on LLM predictions." [section] "ASR errors have a more pronounced influence on the accuracy of depression detection."

### Mechanism 3
- Claim: Few-shot in-context learning benefits larger LLMs more than smaller ones.
- Mechanism: Larger models have more parameters to generalize from a few examples in the prompt.
- Core assumption: The few-shot examples are representative and diverse enough for the model to learn from.
- Evidence anchors: [abstract] "Few-shot learning and task-specific fine-tuning improve performance, with LLMs approaching state-of-the-art levels using only 50% of training data." [section] "Larger models tend to derive greater benefits from an increased number of ICL samples."

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: LLMs can adapt to new tasks by learning from examples in the prompt without parameter updates.
  - Quick check question: How does ICL differ from fine-tuning in terms of parameter updates and task adaptation?

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: Efficiently fine-tunes large LLMs by decomposing weight updates into low-rank matrices.
  - Quick check question: Why is LoRA preferred over full fine-tuning for large models like LLaMA-7B?

- Concept: Emotion recognition in conversations
  - Why needed here: The core task involves classifying emotions in multi-turn dialogues, which differs from single utterance classification.
  - Quick check question: What challenges arise when recognizing emotions in multi-turn conversations compared to single utterances?

## Architecture Onboarding

- Component map:
  Speech signal -> Whisper ASR -> Text transcript
  Prompt template (task definition + ICL samples + query)
  LLM (GPT-2/3.5/4 or LLaMA/Alpaca variants)
  Label prediction (token probability or regex matching)
  Optional: LoRA adapters for fine-tuning

- Critical path:
  ASR transcription -> prompt construction -> LLM inference -> label extraction

- Design tradeoffs:
  - Model size vs. inference cost: Larger models perform better but are more expensive.
  - Prompt length vs. performance: Longer prompts with more ICL samples help larger models but increase token usage.
  - ASR quality vs. task performance: Higher WER impacts depression detection more than emotion recognition.

- Failure signatures:
  - Consistently predicting one label (e.g., Neutral) -> class imbalance or poor prompt design.
  - Poor performance on depression detection with ASR -> accumulated transcription errors in long contexts.
  - No improvement from ICL samples -> examples may be unrepresentative or prompt poorly constructed.

- First 3 experiments:
  1. Test zero-shot performance on IEMOCAP with ground truth transcripts.
  2. Evaluate robustness by comparing zero-shot results with ASR-transcribed inputs.
  3. Measure ICL impact by varying the number of examples per emotion class in the prompt.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the robustness of LLMs to ASR errors vary across different types of dialogue (chit-chat, task-oriented, and clinical)?
- Basis in paper: [explicit] The paper discusses the impact of ASR errors on LLM predictions across different datasets (IEMOCAP, EmoWOZ, DAIC-WOZ) representing different dialogue types.
- Why unresolved: The paper provides general observations about robustness to ASR errors but does not perform a detailed comparative analysis across dialogue types.
- What evidence would resolve it: A systematic comparison of LLM performance degradation due to ASR errors across the three dialogue types, with statistical significance testing.

### Open Question 2
- Question: What is the optimal combination of in-context learning sample size and model size for affect recognition tasks?
- Basis in paper: [explicit] The paper notes that larger models tend to benefit more from increased ICL samples but does not explore the optimal balance.
- Why unresolved: The paper presents performance trends but does not determine the precise point of diminishing returns for sample size relative to model capacity.
- What evidence would resolve it: A detailed ablation study varying both ICL sample count and model size parameters to identify optimal configurations.

### Open Question 3
- Question: How do LLMs handle multi-label emotion recognition where multiple emotions can be present simultaneously?
- Basis in paper: [inferred] The IEMOCAP dataset allows for multiple emotion annotations, but the paper treats emotion recognition as single-label classification.
- Why unresolved: The paper does not explore multi-label classification capabilities despite the dataset supporting it.
- What evidence would resolve it: Experiments comparing single-label vs. multi-label emotion recognition performance using the same datasets and models.

## Limitations

- ASR error simulation methodology may not fully capture real-world error patterns across diverse domains
- Direct performance comparisons between GPT and LLaMA models are limited by architectural differences
- The specific benefits of instruction fine-tuning versus standard fine-tuning remain unclear

## Confidence

**High Confidence**: Zero-shot and few-shot performance results on IEMOCAP and EmoWOZ datasets, as these are directly measured and reproducible. The general finding that instruction fine-tuning improves performance is well-supported.

**Medium Confidence**: The comparison of ASR robustness between emotion recognition and depression detection tasks, as this depends on dataset-specific characteristics and the ASR simulation methodology. The claim about larger models benefiting more from few-shot learning is supported but not comprehensively validated.

**Low Confidence**: The assertion that task-specific fine-tuning approaches state-of-the-art performance using only 50% of training data, as this comparison relies on external benchmarks with different evaluation protocols and model architectures.

## Next Checks

1. **ASR Error Pattern Validation**: Conduct experiments using live ASR transcription on the original audio data across all three datasets to validate whether simulated errors accurately represent real-world performance degradation patterns.

2. **Few-Shot Sample Diversity Analysis**: Systematically vary the diversity and representativeness of few-shot examples in the prompts to determine the minimum effective sample size and optimal composition for different model sizes.

3. **Fine-Tuning Data Efficiency Study**: Perform controlled experiments comparing instruction fine-tuned models against standard fine-tuned models using identical training data volumes to isolate the specific benefits of instruction fine-tuning versus additional training exposure.