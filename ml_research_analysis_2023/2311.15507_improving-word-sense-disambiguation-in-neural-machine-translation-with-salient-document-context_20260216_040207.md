---
ver: rpa2
title: Improving Word Sense Disambiguation in Neural Machine Translation with Salient
  Document Context
arxiv_id: '2311.15507'
source_url: https://arxiv.org/abs/2311.15507
tags:
- translation
- context
- sent
- linguistics
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of translating ambiguous words in
  machine translation. To improve accuracy, the authors propose prefixing source sentences
  with a small number of salient words extracted from pseudo-documents - bags of related
  sentences constructed from related URLs.
---

# Improving Word Sense Disambiguation in Neural Machine Translation with Salient Document Context

## Quick Facts
- **arXiv ID:** 2311.15507
- **Source URL:** https://arxiv.org/abs/2311.15507
- **Reference count:** 22
- **Primary result:** Models trained with 10 salient words achieve best WSD performance (F1 score of 0.6852) on English-German test set

## Executive Summary
This paper addresses the challenge of word sense disambiguation (WSD) in neural machine translation (NMT) by incorporating document-level context. The authors propose prefixing source sentences with salient words extracted from pseudo-documents - bags of related sentences constructed from related URLs. This approach is evaluated on a newly released English-German test set for translation disambiguation, showing that models trained with salient context improve ambiguous word translation over sentence-level and document-level baselines while reducing training time. The results demonstrate that prefixing source sentences with 10 salient words yields the best performance, with an F1 score of 0.6852 on the test set.

## Method Summary
The method involves creating pseudo-documents from related sentences in ParaCrawl by grouping sentences with similar URLs. Salient words are then extracted from these pseudo-documents using either TF-IDF or YAKE! saliency functions. The source sentences are prefixed with 5 or 10 salient words, and a standard 6+6 transformer NMT model is trained on this modified data. The approach is compared against sentence-level and document-level baselines on the DOC-MUCOW test set, evaluating both WSD metrics (Precision, Recall, F1) and standard MT metrics (BLEU, COMET).

## Key Results
- Models with 10 salient words achieve best overall WSD performance (F1 score of 0.6852)
- Salient context models outperform both sentence-level and document-level baselines
- YAKE! saliency function yields better WSD results than TF-IDF
- Shorter sentences benefit more from additional context than longer sentences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prefixing source sentences with salient words from pseudo-documents improves ambiguous word translation accuracy.
- Mechanism: The model conditions on both the sentence-level context and the global topical context provided by the salient words, allowing it to disambiguate word senses more accurately.
- Core assumption: The salient words extracted from pseudo-documents are topically relevant and provide sufficient context to resolve the ambiguity in the source sentence.
- Evidence anchors:
  - [abstract] "Salient words from pseudo-documents are then encoded as a prefix to each source sentence to condition the generation of the translation."
  - [section 2] "We define a sentence si := [w1, . . . , wn] ∈ S ⊂ Wn as a sequence of n words and a pseudo-document dj := {s1, . . . , sm} ∈ D ⊂ Sm as a set of m > 1 sentences."
  - [corpus] Weak - the corpus provides related papers on word sense disambiguation but does not directly support the mechanism of using salient words from pseudo-documents.
- Break condition: If the salient words are not topically relevant or do not provide sufficient context to resolve the ambiguity, the model's performance may not improve.

### Mechanism 2
- Claim: Using fewer salient words (5) leads to better translation disambiguation than using more (10) words.
- Mechanism: With fewer salient words, the model can focus on the most important contextual information, reducing noise and improving disambiguation accuracy.
- Core assumption: The salient words with the highest weights are the most informative for disambiguation, and adding more words beyond a certain point introduces noise rather than useful context.
- Evidence anchors:
  - [section 3] "The saliency-based model with a more sophisticated saliency function, yake, yields better WSD results than tfidf."
  - [section 5] "Systems trained with fewer salient words (5) translate ambiguous words worse than systems trained with 10 salient words, regardless of the saliency function used."
  - [corpus] Weak - the corpus does not provide direct evidence for the claim that fewer salient words lead to better disambiguation.
- Break condition: If the salient words with the highest weights are not the most informative for disambiguation, using fewer words may not improve performance.

### Mechanism 3
- Claim: Shuffling the order of salient words during training improves translation quality metrics.
- Mechanism: Shuffling reduces the model's reliance on the order of salient words, making it more robust to different word orderings and improving its ability to use the contextual information effectively.
- Core assumption: The order of salient words is not crucial for disambiguation, and the model can learn to use the information effectively regardless of the order.
- Evidence anchors:
  - [section 6] "We find shuffling improves the Translation Quality metrics up marginally on average, but has interesting impact on the WSD metrics: on average, models with 5 salient words improve in both P and R (and subsequently F1) while models with 10 salient words degrade marginally."
  - [corpus] Weak - the corpus does not provide direct evidence for the claim that shuffling improves translation quality metrics.
- Break condition: If the order of salient words is crucial for disambiguation, shuffling may degrade the model's performance.

## Foundational Learning

- Concept: Pseudo-documents
  - Why needed here: Pseudo-documents are used to provide context for ambiguous words in the source sentence, as actual document context is not available for most MT training data.
  - Quick check question: What is the difference between a pseudo-document and an actual document in the context of this paper?

- Concept: Salient words
  - Why needed here: Salient words are extracted from pseudo-documents and used as a prefix to the source sentence to provide context for disambiguation.
  - Quick check question: How are salient words selected from pseudo-documents in this paper?

- Concept: Word sense disambiguation (WSD)
  - Why needed here: WSD is the task of determining the correct sense of an ambiguous word in a given context, which is the main focus of this paper.
  - Quick check question: What is the difference between a word's sense and its translation in the context of machine translation?

## Architecture Onboarding

- Component map: Source sentence -> Salient words prefix -> MT model -> Target sentence
- Critical path: Source sentence → Salient words prefix → MT model → Target sentence
- Design tradeoffs:
  - Using more salient words may provide more context but also introduce noise and increase training time.
  - Using a simpler saliency function like tfidf is faster but may not capture as much relevant information as a more sophisticated function like YAKE!.
- Failure signatures:
  - If the model's performance does not improve despite using salient words, it may indicate that the salient words are not topically relevant or do not provide sufficient context for disambiguation.
  - If the model's performance degrades when using more salient words, it may indicate that the additional words introduce noise rather than useful context.
- First 3 experiments:
  1. Train a baseline model without salient words and compare its performance to a model with salient words on a small test set.
  2. Vary the number of salient words (e.g., 5, 10, 15) and compare the performance of models with different numbers of salient words on a small test set.
  3. Compare the performance of models using different saliency functions (e.g., tfidf, YAKE!) on a small test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance gains of salient context models change when using actual document-level data instead of pseudo-documents?
- Basis in paper: [inferred] The paper uses pseudo-documents due to lack of available parallel document-level data, but suggests this approach could easily be applied to proper document-level data if available.
- Why unresolved: The paper only evaluates on pseudo-documents, so the actual performance improvement with real document context remains unknown.
- What evidence would resolve it: Training and evaluating the same salient context models on a dataset with true parallel document-level context and comparing results to the pseudo-document experiments.

### Open Question 2
- Question: Does the amount of salient context (number of words prefixed) interact with sentence length in affecting translation disambiguation performance?
- Basis in paper: [explicit] The paper finds that shorter sentences benefit more from additional context and that efficacy diminishes as sentences get longer, but does not systematically explore the interaction between prefix length and sentence length.
- Why unresolved: While trends are observed, the paper does not provide a detailed analysis of how optimal prefix length varies with sentence length.
- What evidence would resolve it: A controlled study varying both prefix length and sentence length, then analyzing the interaction effects on WSD performance metrics.

### Open Question 3
- Question: How does shuffling the order of salient words during training affect the model's ability to use context for translation disambiguation?
- Basis in paper: [explicit] The paper trains shuffled variants of saliency-based models and finds that shuffling improves translation quality metrics on average, with interesting impacts on WSD metrics and model confidence (CXMI scores).
- Why unresolved: While some effects are observed, the paper does not provide a detailed analysis of why shuffling affects performance or under what conditions it is most beneficial.
- What evidence would resolve it: Further analysis of the model's attention patterns and context utilization with and without shuffling, as well as experiments varying the degree of shuffling.

## Limitations
- Pseudo-document construction relies on URL-based grouping, which may not always capture true topical relatedness.
- The choice of 5 versus 10 salient words appears somewhat arbitrary, with conflicting patterns across different evaluation metrics.
- Ablation studies on saliency functions and shuffling strategies do not provide a complete picture of the approach's robustness.

## Confidence
- High confidence: The core claim that salient context improves WSD performance is directly supported by experimental results on DOC-MUCOW.
- Medium confidence: Claims about training efficiency gains and the effectiveness of YAKE! over TF-IDF are supported by the results but warrant further validation.
- Low confidence: The mechanism of how salient words improve disambiguation and the optimal number of salient words to use warrant lower confidence due to the complexity of the interactions involved.

## Next Checks
1. Conduct ablation studies with varying pseudo-document construction methods (e.g., semantic similarity-based grouping) to test the robustness of the approach to different context sources.
2. Evaluate the approach on additional language pairs beyond English-German to assess its generalizability across different linguistic contexts.
3. Perform human evaluation of the translated ambiguous words to validate the automatic WSD metrics and assess the practical utility of the improvements.