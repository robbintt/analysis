---
ver: rpa2
title: 'KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination
  Detection'
arxiv_id: '2310.09044'
source_url: https://arxiv.org/abs/2310.09044
tags:
- knowledge
- language
- decoding
- linguistics
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to address the hallucination problem
  in Large Language Models (LLMs) by introducing a knowledge-constrained decoding
  method called KCTS (Knowledge-Constrained Tree Search). The core idea is to guide
  a frozen LM to generate text aligned with reference knowledge at each decoding step
  using a knowledge classifier score and Monte-Carlo Tree Search (MCTS).
---

# KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection

## Quick Facts
- arXiv ID: 2310.09044
- Source URL: https://arxiv.org/abs/2310.09044
- Reference count: 34
- Key outcome: KCTS reduces hallucinations in LLMs using knowledge-constrained decoding with MCTS and RIPA token-level detection

## Executive Summary
This paper introduces KCTS (Knowledge-Constrained Tree Search), a novel decoding method that addresses the hallucination problem in Large Language Models by constraining generation to be grounded on reference knowledge. The approach combines Monte-Carlo Tree Search (MCTS) with a novel token-level hallucination detection method called RIPA (Reward Inflection Point Approximation). The method is evaluated on knowledge-grounded dialogue (Wizard of Wikipedia) and abstractive summarization (CNN/DM) tasks, demonstrating superior performance in reducing hallucinations compared to baseline methods and even ChatGPT in some metrics.

## Method Summary
KCTS guides a frozen language model to generate text aligned with reference knowledge at each decoding step using a knowledge classifier score and Monte-Carlo Tree Search. The core innovation is RIPA, which adapts sequence-level knowledge classifiers to token-level guidance by identifying hallucination starting points. The method operates as a plug-and-play, model-agnostic decoding approach that does not require updating model weights. During decoding, MCTS explores multiple token sequences while RIPA provides token-level hallucination scores, enabling the selection of knowledge-grounded responses.

## Key Results
- KCTS outperforms baseline methods (FUDGE, NADO) in knowledge-groundedness metrics (KF1, K-Copy, MFMA)
- The method maintains fluency while improving groundedness on both Wizard of Wikipedia and CNN/DM datasets
- KCTS achieves competitive performance against ChatGPT in knowledge-groundedness metrics
- Automatic and human evaluations confirm the effectiveness of the knowledge-constrained decoding approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KCTS reduces hallucinations by using MCTS to guide decoding toward knowledge-grounded responses
- Mechanism: MCTS explores multiple token sequences at each decoding step, selecting tokens based on knowledge-groundedness scores to better estimate future rewards
- Core assumption: Knowledge-groundedness scores effectively proxy for hallucination detection, and MCTS can navigate the search space efficiently
- Evidence anchors: Abstract results show effectiveness on knowledge-grounded dialogue and summarization; section describes MCTS-guided decoding
- Break condition: If knowledge-groundedness scores poorly indicate hallucinations or search space becomes too large

### Mechanism 2
- Claim: RIPA improves token-level hallucination detection by identifying hallucination starting points
- Mechanism: RIPA trains a classifier to predict where hallucinations begin in sequences, associating only hallucinated content with negative labels
- Core assumption: Identifying hallucination inflection points more effectively approximates future knowledge-groundedness than other methods
- Evidence anchors: Abstract introduces RIPA as novel token-level detection; section discusses "Hallucination Snowballing" effect
- Break condition: If snowballing effect doesn't occur in practice or other methods prove equally effective

### Mechanism 3
- Claim: KCTS outperforms FUDGE and NADO in hallucination reduction
- Mechanism: KCTS uses RIPA for superior token-level guidance compared to methods used by baseline approaches
- Core assumption: RIPA provides better guidance signal for knowledge-constrained decoding than FUDGE/NADO methods
- Evidence anchors: Abstract shows effectiveness; section confirms MCTS produces higher rewards
- Break condition: If other methods match or exceed KCTS performance or gains aren't consistent across tasks

## Foundational Learning

- Concept: Knowledge-groundedness
  - Why needed here: Central to the paper's approach of constraining generation to reference knowledge
  - Quick check question: What distinguishes knowledge-groundedness from mere faithfulness to reference knowledge?

- Concept: Monte-Carlo Tree Search (MCTS)
  - Why needed here: Core component of KCTS for exploring token sequences
  - Quick check question: How does MCTS balance exploration and exploitation, and why is this critical for knowledge-grounded decoding?

- Concept: Token-level hallucination detection
  - Why needed here: RIPA addresses this challenge for effective decoding guidance
  - Quick check question: What makes token-level hallucination detection particularly challenging, and how does RIPA address these challenges?

## Architecture Onboarding

- Component map: Language Model -> Knowledge Classifier -> RIPA -> MCTS
- Critical path:
  1. Generate token sequence using language model
  2. Evaluate knowledge-groundedness using classifier
  3. Use RIPA to predict token-level hallucination scores
  4. Use MCTS to explore and select next token based on knowledge-groundedness and hallucination scores
- Design tradeoffs:
  - MCTS exploration vs exploitation: More exploration improves groundedness but slows decoding
  - RIPA accuracy vs speed: Higher accuracy improves performance but increases training/inference time
  - Model size vs performance: Larger models provide better probabilities but require more resources
- Failure signatures:
  - Low knowledge-groundedness scores: Generation insufficiently constrained by reference knowledge
  - High hallucination scores from RIPA: Generation contains hallucinated content
  - Slow decoding speed: MCTS or RIPA components too computationally expensive
- First 3 experiments:
  1. Compare KCTS to baseline methods (FUDGE, NADO) on knowledge-grounded dialogue and summarization
  2. Ablation study: Remove MCTS or RIPA components and compare to full KCTS
  3. Human evaluation: Compare KCTS responses to baselines and ChatGPT on fluency, relevance, and groundedness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does KCD perform when applied to knowledge retrieval scenarios with non-gold reference knowledge?
- Basis in paper: [inferred] Paper mentions not considering knowledge retrieval and suggests testing with retrieved knowledge in realistic deployment scenarios
- Why unresolved: Paper focuses on gold knowledge to demonstrate constrained decoding potential
- What evidence would resolve it: Experiments comparing KCD performance with gold vs retrieved knowledge across tasks

### Open Question 2
- Question: How does RIPA compare to other token-level hallucination detection methods in training efficiency and effectiveness?
- Basis in paper: [explicit] RIPA introduced as novel method compared to random truncation and token-level labeling
- Why unresolved: Paper provides limited comparison without comprehensive analysis of training efficiency
- What evidence would resolve it: Detailed comparison including training time, sample efficiency, and detection accuracy

### Open Question 3
- Question: How does KCTS compare to fine-tuning-based knowledge-grounded generation methods in performance and computational cost?
- Basis in paper: [explicit] KCTS presented as plug-and-play alternative to computationally expensive fine-tuning methods
- Why unresolved: Paper doesn't directly compare to fine-tuning approaches
- What evidence would resolve it: Comprehensive comparison including performance metrics and computational costs

## Limitations

- Evaluation relies on novel metrics (KF1, K-Copy, MFMA) whose correlation with true knowledge-groundedness lacks thorough validation
- Human evaluation conducted on limited sample size (100 samples, 6-8 annotators) constraining statistical power
- Computational overhead and inference speed compared to standard decoding methods not reported
- Component ablations lacking to isolate contributions of MCTS, RIPA, and knowledge classifier

## Confidence

**High Confidence**: KCTS effectively reduces hallucinations as measured by knowledge-groundedness metrics compared to standard decoding methods, with moderate support from human evaluation.

**Medium Confidence**: Claims about outperforming FUDGE/NADO and ChatGPT are supported by automatic metrics but lack detailed comparative analysis and comprehensive validation.

**Low Confidence**: Claims about specific mechanisms by which RIPA and MCTS reduce hallucinations lack detailed supporting analysis and evidence.

## Next Checks

1. **Computational Efficiency Analysis**: Measure actual inference time of KCTS compared to beam search, sampling, and baseline methods on identical hardware to assess practical applicability.

2. **Component Ablation Study**: Systematically isolate contributions of MCTS, RIPA, and knowledge classifier through controlled experiments to identify most critical components.

3. **Cross-Dataset Generalization Test**: Evaluate KCTS on additional knowledge-grounded tasks (fact-checking, question answering) beyond dialogue and summarization to assess broader applicability.