---
ver: rpa2
title: 'Prompt Cache: Modular Attention Reuse for Low-Latency Inference'
arxiv_id: '2311.04934'
source_url: https://arxiv.org/abs/2311.04934
tags:
- prompt
- cache
- module
- attention
- modules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Prompt Cache, a technique for accelerating
  large language model (LLM) inference by reusing attention states across different
  LLM prompts. The key insight is that by precomputing and storing the attention states
  of frequently occurring text segments, such as system messages, prompt templates,
  and documents, on the inference server, they can be efficiently reused when these
  segments appear in user prompts.
---

# Prompt Cache: Modular Attention Reuse for Low-Latency Inference

## Quick Facts
- arXiv ID: 2311.04934
- Source URL: https://arxiv.org/abs/2311.04934
- Reference count: 36
- Key outcome: Achieves 8x latency reduction on GPUs and 60x on CPUs for time-to-first-token by reusing attention states across LLM prompts

## Executive Summary
Prompt Cache introduces a technique for accelerating large language model inference by reusing precomputed attention states of frequently occurring text segments across different prompts. The system uses a Prompt Markup Language to define reusable prompt modules that can be cached and their attention states reused when the same segments appear in new prompts. This approach significantly reduces time-to-first-token latency, particularly for longer prompts like document-based question answering, while maintaining output accuracy without requiring model parameter modifications.

## Method Summary
Prompt Cache works by precomputing and storing attention states for text segments defined in a schema using Prompt Markup Language (PML). When user prompts contain these segments, the system retrieves the precomputed attention states instead of recomputing them. The attention states from cached modules are concatenated with newly computed states for uncached text, allowing the LLM to generate the first token without full attention computation over the entire prompt. The system maintains output quality by preserving relative positional semantics even when attention states are reused across different prompt positions.

## Key Results
- Reduces time-to-first-token latency by 8x for GPU-based inference
- Reduces time-to-first-token latency by 60x for CPU-based inference
- Maintains output accuracy without model parameter modifications
- Effective for longer prompts such as document-based question answering and recommendations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt Cache achieves low-latency inference by reusing precomputed attention states of frequently occurring text segments across different LLM prompts.
- Mechanism: The system precomputes and stores attention states for prompt modules defined in a schema. When these modules appear in user prompts, the precomputed states are retrieved from memory instead of recomputing them, reducing computational overhead.
- Core assumption: Attention states of text segments are position-independent and can be reused when the same segment appears at different positions in different prompts.
- Evidence anchors:
  - [abstract]: "Our key insight is that by precomputing and storing the attention states of these frequently occurring text segments on the inference server, we can efficiently reuse them when these segments appear in user prompts."
  - [section]: "The attention states of a text segment can only be reused if the segment appears at the same position in the LLM input. This is because transformer architectures integrate positional embeddings into the (k, v) attention states."
  - [corpus]: Weak. Related work focuses on KV cache sharing and semantic similarity matching, but no direct evidence of position-independent attention state reuse across prompts.

### Mechanism 2
- Claim: Prompt Cache reduces time-to-first-token (TTFT) latency by eliminating the need to compute attention states for cached prompt modules during the initial token generation.
- Mechanism: The system concatenates precomputed attention states from cached modules with newly computed states for uncached text, allowing the LLM to generate the first token without full attention computation over the entire prompt.
- Core assumption: The computational savings from avoiding attention state computation for cached modules outweigh the overhead of managing the cache and concatenating attention states.
- Evidence anchors:
  - [abstract]: "Prompt Cache significantly reduce latency in time-to-first-token, especially for longer prompts such as document-based question answering and recommendations."
  - [section]: "This approach significantly reduces the computation required for self-attention... Prompt Cache primarily diminishes the latency involved in producing the first token."
  - [corpus]: Moderate. Related work on KV cache recycling and prefix sharing supports the concept of attention state reuse, but specific focus on TTFT reduction is limited.

### Mechanism 3
- Claim: Prompt Cache maintains output quality by preserving the relative positional semantics of prompt modules even when their attention states are reused across different prompt positions.
- Mechanism: The system uses a Prompt Markup Language (PML) to define prompt modules with explicit positions and hierarchies. By concatenating attention states from different modules while maintaining their relative positions, the LLM can generate coherent outputs.
- Core assumption: LLMs can operate on attention states with discontinuous position IDs without significant quality degradation, as long as the relative positional semantics are preserved.
- Evidence anchors:
  - [abstract]: "all while maintaining output accuracy and without the need for model parameter modifications."
  - [section]: "we can extract different segment of attention states and concatenate them to formulate new meanings... it does not affect the output quality since the relative positional semantics are still retained."
  - [corpus]: Weak. While related work mentions position-independent caching, there's limited evidence specifically addressing the impact on output quality when reusing attention states across different prompt positions.

## Foundational Learning

- Concept: Transformer attention mechanism
  - Why needed here: Understanding how attention states are computed and cached is crucial for implementing Prompt Cache's modular attention reuse.
  - Quick check question: How does the attention mechanism in transformers compute key, query, and value vectors, and how are they used to generate attention scores?

- Concept: Positional encoding in transformers
  - Why needed here: Prompt Cache relies on the ability to reuse attention states with discontinuous position IDs, which requires understanding how positional information is encoded and used in attention computations.
  - Quick check question: What are the different methods for positional encoding in transformers (e.g., learned embeddings, sinusoidal, RoPE), and how do they handle position information in attention calculations?

- Concept: Memory management in LLM inference
  - Why needed here: Prompt Cache involves managing attention states in CPU and GPU memory, requiring knowledge of memory allocation, data transfer, and cache replacement strategies.
  - Quick check question: What are the trade-offs between storing attention states in CPU vs. GPU memory, and how do memory copy operations affect inference latency?

## Architecture Onboarding

- Component map: PML parser -> Attention state encoder -> Cached inference engine -> Memory manager
- Critical path:
  1. Parse prompt schema and user prompt using PML parser.
  2. Retrieve cached attention states for identified prompt modules.
  3. Compute attention states for uncached text segments and parameters.
  4. Concatenate attention states and pass to LLM for inference.
  5. Generate output tokens using cached and newly computed attention states.

- Design tradeoffs:
  - Memory vs. latency: Storing more prompt modules in GPU memory reduces host-to-device copy overhead but limits scalability.
  - Cache replacement policy: Balancing between keeping frequently used modules in cache and making room for new ones.
  - Schema complexity: More complex schemas with nested modules and parameters increase reuse opportunities but also parsing overhead.

- Failure signatures:
  - Increased TTFT latency: Indicates cache misses or inefficient cache management.
  - Output quality degradation: Suggests issues with positional encoding or attention state concatenation.
  - Memory allocation errors: Implies insufficient memory for storing prompt modules or attention states.

- First 3 experiments:
  1. Benchmark TTFT latency improvement with a simple schema containing one frequently reused prompt module across multiple prompts.
  2. Measure output quality degradation when reusing attention states across different prompt positions with varying levels of schema complexity.
  3. Evaluate memory overhead and latency trade-offs when storing prompt modules in CPU vs. GPU memory for different model sizes and prompt lengths.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of Prompt Cache be optimized for CPU-based inference when dealing with extremely long prompts?
- Basis in paper: [inferred] The paper mentions that the latency improvement on CPU is higher than on GPU, but it decreases as the non-cacheable portion of the prompt and response increases. It also mentions that the computational savings from Prompt Cache more than compensate for the latencies caused by memory copy operations.
- Why unresolved: The paper does not provide specific strategies or techniques for optimizing Prompt Cache for extremely long prompts on CPU-based inference. It only mentions the general trade-off between memory capacity and latency for CPU and GPU memory.
- What evidence would resolve it: Experimental results showing the performance of Prompt Cache on extremely long prompts for CPU-based inference, along with specific techniques or optimizations implemented to improve the performance.

### Open Question 2
- Question: How can Prompt Cache be extended to support more advanced prompt structures, such as conditional prompt modules or prompt modules with multiple parameters?
- Basis in paper: [explicit] The paper mentions that PML supports nested modules to express hierarchical prompt modules, and it also allows a prompt module to be parameterized in order to maximize the reuse opportunities. However, it does not discuss the support for more advanced prompt structures.
- Why unresolved: The paper does not provide information on how to extend Prompt Cache to support more advanced prompt structures. It only discusses the basic features of PML and how they can be used to define prompt modules and parameters.
- What evidence would resolve it: Implementation details and experimental results showing the support for conditional prompt modules or prompt modules with multiple parameters in Prompt Cache, along with any performance improvements or trade-offs associated with these advanced prompt structures.

### Open Question 3
- Question: How does the accuracy of Prompt Cache compare to other attention state reuse techniques, such as KV Cache or Paged Attention?
- Basis in paper: [explicit] The paper mentions that Prompt Cache maintains output accuracy and does not require model parameter modifications. It also compares the accuracy of Prompt Cache to the baseline KV Cache and shows that the accuracy of output with Prompt Cache is comparable to the baseline.
- Why unresolved: The paper does not provide a direct comparison of Prompt Cache's accuracy to other attention state reuse techniques like KV Cache or Paged Attention. It only compares Prompt Cache to KV Cache as the baseline.
- What evidence would resolve it: Experimental results comparing the accuracy of Prompt Cache to other attention state reuse techniques, such as KV Cache or Paged Attention, on the same benchmark datasets. This would provide insights into the relative accuracy of Prompt Cache compared to other techniques.

## Limitations
- Position independence challenge: The assumption that attention states can be reused across different prompt positions while maintaining output quality lacks strong empirical validation.
- Memory management complexity: The paper doesn't adequately address memory overhead for large-scale deployments across different hardware configurations.
- Schema scalability: Limited exploration of how schema complexity scales with larger systems and production environments.

## Confidence
- High Confidence: The core mechanism of caching and reusing attention states for identical text segments is technically sound and aligns with established KV cache concepts.
- Medium Confidence: The position independence claims and output quality preservation need more rigorous validation across different model architectures and tasks.
- Low Confidence: The scalability analysis for large-scale deployments and the impact of complex schema hierarchies requires further investigation.

## Next Checks
1. **Position Encoding Validation**: Test output quality degradation when reusing attention states across different positions for models using different positional encoding schemes (RoPE, sinusoidal, learned embeddings). Measure quality metrics (perplexity, BLEU scores) as the distance between original and reuse positions increases.

2. **Memory Overhead Benchmarking**: Conduct comprehensive memory usage analysis across different model sizes (7B, 13B, 70B parameters) and prompt module quantities. Compare CPU vs GPU memory storage costs and evaluate cache replacement strategies under memory constraints.

3. **Schema Complexity Scaling**: Measure parsing overhead and performance impact as schema complexity increases. Test with nested modules, multiple parameter types, and varying numbers of reusable segments to determine practical limits for production deployment.