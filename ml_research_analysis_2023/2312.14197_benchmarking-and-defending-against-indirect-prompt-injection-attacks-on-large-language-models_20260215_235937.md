---
ver: rpa2
title: Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large
  Language Models
arxiv_id: '2312.14197'
source_url: https://arxiv.org/abs/2312.14197
tags:
- attacks
- prompt
- content
- llms
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of large language models
  (LLMs) to indirect prompt injection attacks, where malicious instructions embedded
  in external content manipulate LLM outputs. The authors introduce BIPIA, the first
  benchmark for evaluating LLMs and defenses against these attacks, covering various
  application scenarios and attack types.
---

# Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models

## Quick Facts
- **arXiv ID**: 2312.14197
- **Source URL**: https://arxiv.org/abs/2312.14197
- **Reference count**: 40
- **Key outcome**: Introduces BIPIA benchmark for evaluating LLMs against indirect prompt injection attacks and proposes effective black-box and white-box defenses

## Executive Summary
This paper addresses the critical vulnerability of large language models to indirect prompt injection attacks, where malicious instructions embedded in external content manipulate LLM outputs. The authors introduce BIPIA, the first comprehensive benchmark for evaluating LLMs and defenses against these attacks across various application scenarios. Through extensive experimentation with 25 instruction-tuned LLMs, they demonstrate that more capable models are paradoxically more susceptible to such attacks. The paper proposes two categories of defenses - black-box methods based on prompt learning (border strings, in-context learning, multi-turn dialogue, datamarking) and a white-box method using adversarial fine-tuning - achieving significant reductions in attack success rates while maintaining output quality.

## Method Summary
The paper introduces BIPIA, a comprehensive benchmark dataset for indirect prompt injection attacks, covering email QA, web QA, table QA, summarization, and code QA scenarios with various attack types. The authors evaluate 25 instruction-tuned LLMs against this benchmark to establish baseline vulnerability levels. They then implement four black-box defense methods based on prompt engineering techniques to create clear boundaries between user instructions and external content. Additionally, they develop a white-box defense through adversarial fine-tuning that teaches models to ignore instructions within external content using special boundary tokens. The effectiveness of all defenses is measured using attack success rate (ASR) and ROUGE scores for task performance.

## Key Results
- More capable LLMs show higher susceptibility to indirect prompt injection attacks
- Black-box defenses can effectively reduce attack success rates while preserving output quality
- White-box adversarial training reduces attack success rate to near-zero levels
- Model capability shows positive correlation with attack success rate vulnerability

## Why This Works (Mechanism)

### Mechanism 1: Black-box Prompt Engineering
- **Claim**: Prompt engineering can teach LLMs to distinguish between user instructions and external content
- **Mechanism**: Defense methods modify prompt structure to create clear boundaries, leveraging LLMs' sensitivity to formatting and dialogue context
- **Core assumption**: LLMs cannot inherently distinguish instructions from external content, and this can be taught through prompt engineering without parameter modification
- **Break condition**: If LLMs fail to learn boundary distinctions despite prompt modifications, or if attackers develop methods to bypass prompt structure changes

### Mechanism 2: White-box Adversarial Training
- **Claim**: Fine-tuning with adversarial training can make LLMs robust to indirect prompt injection attacks
- **Mechanism**: Special tokens mark external content boundaries, and models are fine-tuned on BIPIA training data to ignore malicious instructions
- **Core assumption**: LLMs can learn to distinguish instructions from external content through adversarial training, with transferable learning to unseen attacks
- **Break condition**: If adversarial training fails to generalize to novel attack types or causes significant performance degradation on benign tasks

### Mechanism 3: Capability-Vulnerability Tradeoff
- **Claim**: More capable LLMs are more susceptible to attacks but also more responsive to defense training
- **Mechanism**: Enhanced language understanding capabilities increase both attack vulnerability and defense learning capacity
- **Core assumption**: Tradeoff between model capability and vulnerability exists and can be managed through appropriate defenses
- **Break condition**: If correlation between capability and vulnerability doesn't hold for future LLMs or defense effectiveness doesn't scale with capability

## Foundational Learning

- **Concept: Prompt Learning**
  - Why needed here: Understanding how LLMs can be guided to distinguish between instructions and external content through prompt engineering
  - Quick check question: How do different prompt structures (e.g., border strings, in-context learning examples) influence LLM behavior?

- **Concept: Adversarial Training**
  - Why needed here: Understanding how LLMs can be fine-tuned to be robust against indirect prompt injection attacks by learning from malicious examples
  - Quick check question: What are the key differences between standard fine-tuning and adversarial training in the context of LLM security?

- **Concept: Model Capability vs. Vulnerability**
  - Why needed here: Understanding the relationship between model capability and vulnerability to attacks is crucial for designing effective defenses
  - Quick check question: How does the complexity of an LLM's language understanding capabilities influence its susceptibility to indirect prompt injection attacks?

## Architecture Onboarding

- **Component map**: External Content Retrieval -> Prompt Construction -> Defense Mechanisms -> LLM -> Response Generation
- **Critical path**: Retrieve external content → Construct prompt with user instructions → Apply defense mechanisms → Generate response from LLM
- **Design tradeoffs**: Balancing model performance on benign tasks with robustness against attacks; computational overhead of defense mechanisms
- **Failure signatures**: High attack success rates despite defenses, degradation in performance on benign tasks, model instability during fine-tuning
- **First 3 experiments**:
  1. Evaluate effectiveness of different border strings on reducing attack success rates
  2. Test impact of in-context learning examples on model robustness against indirect prompt injection attacks
  3. Assess white-box defense performance through adversarial training on held-out test set

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the effectiveness of white-box defenses against indirect prompt injection attacks scale with the size and complexity of the underlying LLM?
- **Basis in paper**: [inferred] The paper evaluates white-box defenses on specific LLM models (Vicuna-7B and Vicuna-13B) without investigating generalizability to larger or more complex LLMs
- **Why unresolved**: Paper focuses on specific models without exploring scalability across diverse LLM sizes and complexities
- **What evidence would resolve it**: Experimental results demonstrating effectiveness across diverse LLM sizes and complexities, including models significantly larger than those evaluated

### Open Question 2
- **Question**: Can the proposed black-box defenses be combined or augmented with other techniques to further enhance their effectiveness against indirect prompt injection attacks?
- **Basis in paper**: [explicit] Paper introduces four black-box defense methods independently without exploring combined impact or integration with other techniques
- **Why unresolved**: Each black-box defense method presented independently without investigating combined impact or potential interactions
- **What evidence would resolve it**: Experimental results comparing individual defenses, combined defenses, and black-box defenses integrated with other techniques against indirect prompt injection attacks

### Open Question 3
- **Question**: How do the proposed defenses perform against advanced indirect prompt injection attacks that adapt to the specific defenses employed?
- **Basis in paper**: [inferred] Paper evaluates defenses against static set of attacks without considering adaptive attackers who circumvent defenses
- **Why unresolved**: Paper focuses on static attack evaluation without exploring dynamic adversarial interactions
- **What evidence would resolve it**: Experimental results demonstrating effectiveness against adaptive indirect prompt injection attacks that actively try to circumvent defenses

## Limitations
- Black-box defenses may not generalize to future LLM architectures or attack variants
- White-box defense requires access to model weights and substantial computational resources
- Near-zero ASR claims need further validation against novel attack vectors
- Long-term stability of fine-tuned models on general tasks remains uncertain

## Confidence

**High Confidence**: Observation that more capable LLMs are more susceptible to indirect prompt injection attacks is well-supported by empirical results across 25 models

**Medium Confidence**: Effectiveness of black-box defenses in reducing ASR while maintaining task performance is demonstrated, but generalizability to unseen attack variants is less certain

**Low Confidence**: White-box defense's near-zero ASR claim is impressive but requires further validation and testing against novel attack types

## Next Checks

1. **Adversarial Robustness Test**: Conduct comprehensive evaluation of all defense mechanisms against systematically generated adaptive attacks designed to bypass each defense type

2. **Longitudinal Performance Study**: Track performance of white-box defended models over extended periods and across diverse task distributions beyond BIPIA benchmark

3. **Resource Overhead Characterization**: Quantify computational and latency overhead introduced by each defense mechanism at scale, including impact on real-time applications and fine-tuning costs