---
ver: rpa2
title: 'MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning'
arxiv_id: '2310.16049'
source_url: https://arxiv.org/abs/2310.16049
tags:
- story
- fact
- reasoning
- they
- deduced
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MuSR is a new reasoning benchmark consisting of 750 challenging
  multistep inference problems presented in natural language narratives across three
  domains: murder mysteries, object placements, and team allocation. The dataset is
  generated via a neurosymbolic synthetic-to-natural algorithm that recursively expands
  structured reasoning trees into realistic stories, enabling scalable creation of
  complex reasoning instances.'
---

# MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning

## Quick Facts
- arXiv ID: 2310.16049
- Source URL: https://arxiv.org/abs/2310.16049
- Reference count: 40
- Key outcome: MuSR is a new reasoning benchmark with 750 multistep inference problems across murder mysteries, object placements, and team allocation domains, showing current LLMs significantly underperform humans on complex reasoning tasks.

## Executive Summary
MuSR introduces a novel reasoning benchmark designed to test the limits of chain-of-thought prompting and multistep reasoning in language models. The benchmark consists of 750 challenging problems across three domains that require commonsense reasoning about physical and social situations. Generated through a neurosymbolic synthetic-to-natural algorithm, the dataset presents reasoning problems in natural language narratives while preserving the underlying logical structure through reasoning trees. Human evaluation shows over 90% accuracy on solving these tasks, while even strong models like GPT-4 achieve only 60-80% accuracy, indicating significant room for improvement in language model reasoning capabilities.

## Method Summary
The MuSR benchmark is constructed through a three-stage neurosymbolic generation process: Tree Template Construction, Reasoning Tree Completion, and Story Generation. The algorithm starts with domain-specific facts and constraints, builds structured reasoning trees through recursive prompting, then uses GPT-4 to expand these trees into realistic narratives while preserving core facts. The dataset spans three domains - murder mysteries requiring social and physical reasoning, object placements demanding spatial and theory-of-mind reasoning, and team allocation involving constraint satisfaction. The generation process ensures problems require multistep inference and commonsense reasoning to solve, while maintaining narrative quality and fact consistency.

## Key Results
- Human solvers achieve over 90% accuracy on MuSR tasks across all three domains
- GPT-4 and GPT-3.5 achieve only 60-80% accuracy, significantly underperforming humans
- Chain-of-thought prompting shows limited effectiveness on these complex multistep reasoning problems
- The benchmark successfully differentiates between reasoning capabilities across different domains and model types

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The neurosymbolic synthetic-to-natural generation algorithm enables scalable creation of complex reasoning problems that are both challenging for LLMs and grounded in realistic narratives.
- **Mechanism:** The algorithm recursively expands structured reasoning trees into realistic stories, preserving the logical structure while embedding it in natural language contexts that require commonsense reasoning.
- **Core assumption:** LLMs can reliably generate coherent narratives from structured reasoning trees while maintaining logical consistency.
- **Evidence anchors:**
  - [abstract] "This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4"
  - [section] "Our novel neurosymbolic dataset generation procedure starts by instantiating a set of facts encoding a reasoning problem in a domain of interest... We then use a recursive prompting strategy to expand that into a reasoning tree"
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.475" - Weak corpus support for this specific mechanism

### Mechanism 2
- **Claim:** The three distinct domains (murder mysteries, object placements, team allocation) provide complementary challenges that test different aspects of reasoning capabilities.
- **Mechanism:** Each domain requires a unique combination of reasoning types - social reasoning for murder mysteries, theory-of-mind for object placements, and constraint reasoning for team allocation.
- **Core assumption:** Different reasoning domains will reveal different strengths and weaknesses in LLM reasoning approaches.
- **Evidence anchors:**
  - [abstract] "The domains we address here, murder mysteries, object placements, and team assignment, involve commonsense reasoning about physical and social situations"
  - [section] "Murder mysteries elicit physical and social reasoning in the fact sets S(T) and C(T)... Object Placements requires spatial and physical reasoning as well as reasoning about people's awareness"
  - [corpus] "Found 25 related papers... Top related titles: Ko-MuSR, Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification" - Some related work but limited specific evidence

### Mechanism 3
- **Claim:** The inclusion of ground-truth intermediate structure (reasoning trees) enables meaningful evaluation of reasoning quality beyond just final answer correctness.
- **Mechanism:** By providing access to the gold reasoning trees used to generate the narratives, evaluators can assess whether models are following the correct reasoning path or just guessing the answer.
- **Core assumption:** Having access to intermediate reasoning structure allows for more nuanced evaluation of model reasoning capabilities.
- **Evidence anchors:**
  - [abstract] "Crucially, these types of reasoning arise naturally from text descriptions of each problem... This reasoning tree is then fed into GPT-4 in chunks to generate the narrative"
  - [section] "The reasoning tree then describes factors that contribute to these skills and relationships... However, the lack of hard constraints means that no single clue in the text easily determines the solution"
  - [corpus] Weak corpus support for this specific evaluation mechanism

## Foundational Learning

- **Concept:** Neurosymbolic AI systems that combine neural network capabilities with symbolic reasoning structures
  - Why needed here: The dataset generation relies on combining LLM text generation (neural) with structured reasoning trees (symbolic) to create challenging problems
  - Quick check question: How does the recursive expansion of reasoning trees into narratives preserve logical structure while generating realistic text?

- **Concept:** Commonsense reasoning and theory of mind in natural language processing
  - Why needed here: Each domain requires different types of commonsense reasoning - social norms for murder mysteries, physical reasoning for object placements, and social constraints for team allocation
  - Quick check question: What types of commonsense knowledge are required to solve each domain in the dataset?

- **Concept:** Evaluation metrics for reasoning tasks beyond simple accuracy
  - Why needed here: The dataset provides reasoning trees as ground truth, enabling evaluation of reasoning quality and not just final answers
  - Quick check question: How can access to reasoning trees improve evaluation of model performance compared to just checking final answers?

## Architecture Onboarding

- **Component map:** Tree Template Construction → Reasoning Tree Completion → Story Generation
- **Critical path:** Generate gold facts and reasoning trees → Use LLM to create narratives → Validate fact preservation → Evaluate human and model performance
- **Design tradeoffs:**
  - Complexity vs. scalability: More complex reasoning trees create better challenges but are harder to generate reliably
  - Narrative quality vs. logical consistency: More natural narratives may sacrifice some logical precision
  - Domain specificity vs. generalizability: Three distinct domains provide comprehensive testing but require different evaluation approaches
- **Failure signatures:**
  - Low human performance indicates narrative generation failures
  - High model performance on some domains but not others indicates domain-specific weaknesses
  - Inconsistent model performance across prompting strategies indicates fundamental reasoning limitations
- **First 3 experiments:**
  1. Generate narratives with varying tree depths to find optimal balance between complexity and solvability
  2. Test different LLM models and prompts for narrative generation to maximize fact preservation
  3. Conduct human evaluation on a subset to validate narrative quality and problem difficulty

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How deep can the reasoning trees in MuSR be made while still allowing GPT-4 to generate coherent narratives that preserve all key facts?
- **Basis in paper:** Explicit - The paper states "Although our method can create reasoning trees of varying depths, we found that shallower trees (of depth one or two) provide the best level of detail for creating a narrative" and "We believe that prompting and better LLMs may increase the depth of acceptable deductions and is an important area of future work for our method."
- **Why unresolved:** The paper only tested reasoning trees up to a depth of 2, finding that deeper trees led to generation failures or fact omission in the narratives.
- **What evidence would resolve it:** Experiments systematically varying reasoning tree depth (e.g., 1, 2, 3, 4, 5) while measuring narrative coherence, fact recall, and LLM performance on the resulting tasks would reveal the practical depth limits.

### Open Question 2
- **Question:** Would incorporating human-authored intermediate reasoning steps during dataset generation improve the quality and diversity of the generated narratives compared to fully synthetic reasoning trees?
- **Basis in paper:** Inferred - The paper uses fully synthetic reasoning trees generated by GPT-4, but notes that "Our murder mystery domain is represented in the recent True Detective (Del & Fishel, 2022) dataset, which collects human-authored murder mysteries" and suggests synthetic benchmarks have advantages for scalability.
- **Why unresolved:** The paper doesn't compare synthetic vs. human-authored reasoning trees in terms of narrative quality, fact preservation, or task difficulty.
- **What evidence would resolve it:** Generating parallel datasets where one uses GPT-4 reasoning trees and another uses human-authored reasoning trees, then comparing narrative quality metrics, fact recall rates, and human evaluation scores.

### Open Question 3
- **Question:** How do different LLM architectures (e.g., transformers vs. other neural architectures) compare on MuSR tasks, and are there architectural modifications that could improve performance on multistep soft reasoning?
- **Basis in paper:** Inferred - The paper evaluates only transformer-based LLMs (GPT-4, GPT-3.5, Llama2, Vicuna) and finds they underperform humans, suggesting the need for architectural innovations.
- **Why unresolved:** The paper doesn't explore non-transformer architectures or architectural modifications specifically targeting the reasoning challenges in MuSR.
- **What evidence would resolve it:** Benchmarking alternative architectures (e.g., graph neural networks, neurosymbolic hybrids) on MuSR and/or developing and testing architectural modifications designed to better handle multistep reasoning and commonsense inference.

## Limitations

- The neurosymbolic generation algorithm's scalability beyond the three initial domains is untested
- The 90%+ human accuracy benchmark may not reflect true difficulty if human solvers received training or had specific expertise
- The performance gap between GPT-4 and human solvers could be influenced by prompt engineering variations rather than fundamental reasoning differences

## Confidence

- **High confidence**: The benchmark construction methodology and human evaluation results
- **Medium confidence**: The claim that current LLMs have significant room for improvement in multistep reasoning
- **Low confidence**: The assertion that this benchmark will effectively differentiate between neurosymbolic and pure LLM approaches in the long term

## Next Checks

1. Test the benchmark with domain experts versus general population to validate the 90%+ human accuracy claim
2. Conduct ablation studies removing the reasoning tree ground truth to assess whether models can solve problems without this scaffold
3. Evaluate whether performance on MuSR correlates with other established reasoning benchmarks to establish construct validity