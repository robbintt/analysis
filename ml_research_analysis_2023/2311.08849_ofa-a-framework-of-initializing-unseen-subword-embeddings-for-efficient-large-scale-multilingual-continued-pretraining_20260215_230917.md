---
ver: rpa2
title: 'OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale
  Multilingual Continued Pretraining'
arxiv_id: '2311.08849'
source_url: https://arxiv.org/abs/2311.08849
tags:
- latn
- cyrl
- embeddings
- multilingual
- continued
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OFA is a framework for efficiently adapting large language models
  to new languages via vocabulary extension and continued pretraining. It addresses
  the problem of inefficient embedding initialization for unseen subwords by leveraging
  aligned multilingual static word embeddings and matrix factorization to reduce parameters.
---

# OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining

## Quick Facts
- arXiv ID: 2311.08849
- Source URL: https://arxiv.org/abs/2311.08849
- Authors: 
- Reference count: 26
- Primary result: OFA framework accelerates multilingual model adaptation by efficiently initializing unseen subword embeddings through matrix factorization and cross-lingual similarity.

## Executive Summary
OFA addresses the challenge of efficiently adapting large language models to new languages by extending vocabulary and initializing embeddings for unseen subwords. The framework leverages matrix factorization to reduce embedding dimensionality and uses cross-lingual similarity from external multilingual embeddings to initialize new subword representations. Experiments demonstrate that OFA accelerates convergence during continued pretraining and achieves competitive or better performance than random initialization baselines across diverse multilingual tasks including sentence retrieval, text classification, and sequence labeling.

## Method Summary
OFA factorizes source embeddings into primitive embeddings and source coordinates using matrix factorization (typically SVD), reducing dimensionality from 768 to 100-400. For unseen subwords, it computes cross-lingual similarity from external well-aligned multilingual embeddings and initializes target coordinates as convex combinations of source coordinates. The framework then assembles a target model with primitive embeddings and initialized coordinates, which is continued pretrained using MLM objective. The approach significantly reduces parameters while maintaining or improving performance on downstream multilingual tasks.

## Key Results
- OFA consistently outperforms random initialization baselines on sentence retrieval tasks (SR-B, SR-T) with top-10 accuracy improvements
- On text classification (Taxi1500) and sequence labeling (NER, POS), OFA achieves competitive F1 scores compared to baselines
- OFA reduces computational cost by enabling faster convergence during continued pretraining while maintaining model quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OFA's factorization reduces embedding redundancy and improves crosslingual alignment
- Mechanism: Decomposes high-dimensional embedding matrix into primitive embeddings (language-agnostic) and target coordinates, enabling parameter-efficient adaptation to new languages
- Core assumption: Multilingual embeddings exhibit redundancy due to shared semantic concepts across languages
- Evidence anchors:
  - [abstract]: "OFA applies matrix factorization and replaces the cumbersome embeddings with two lower-dimensional matrices, which significantly reduces the number of parameters"
  - [section 3]: "P can be interpreted as the embeddings of a set of D′ latent semantic concepts that are language-agnostic, serving as the basis of a semantic space in RD for all subwords"
  - [corpus]: Weak evidence - no direct corpus support for redundancy assumption
- Break condition: If multilingual embeddings are not redundant or crosslingual semantic alignment is poor, factorization may not reduce parameters or improve performance

### Mechanism 2
- Claim: OFA's initialization leverages external multilingual embeddings for better subword representation
- Mechanism: Uses cross-lingual similarity from aligned multilingual static embeddings to initialize new subword coordinates as convex combinations of source coordinates
- Core assumption: External multilingual embeddings capture crosslingual semantic relationships that can inform subword initialization
- Evidence anchors:
  - [abstract]: "OFA takes advantage of external well-aligned multilingual static word vectors and injects the alignment knowledge into the new embeddings"
  - [section 4]: "For the rest subwords...we follow WECHSEL (Minixhofer et al., 2022) to find a good initialization based on similarity"
  - [corpus]: No direct corpus evidence for effectiveness of this initialization approach
- Break condition: If external embeddings are poorly aligned or don't capture relevant semantic relationships, initialization quality degrades

### Mechanism 3
- Claim: OFA accelerates continued pretraining convergence through better initialization
- Mechanism: Starting from embeddings that already encode crosslingual knowledge reduces the number of training steps needed to adapt to new languages
- Core assumption: Initialization quality directly impacts training efficiency and convergence speed
- Evidence anchors:
  - [abstract]: "OFA not only accelerates the convergence of continued pretraining, which is friendly to a limited computation budget"
  - [section 6.1]: "OFA-mono-768 (resp. OFA-multi-768) constantly performs better than RoBERTa-rand (resp. XLM-R-rand) throughout steps for all tasks"
  - [corpus]: Weak evidence - performance improvements observed but causation not directly established
- Break condition: If initialization benefits are outweighed by other factors (e.g., catastrophic forgetting), convergence advantages may not materialize

## Foundational Learning

- Concept: Matrix factorization and low-rank approximation
  - Why needed here: Core to OFA's parameter reduction approach - understanding how decomposing matrices preserves information while reducing dimensions
  - Quick check question: How does singular value decomposition enable dimensionality reduction while preserving most variance in embedding matrices?

- Concept: Crosslingual semantic alignment and word embedding spaces
  - Why needed here: OFA relies on well-aligned external embeddings to initialize new subwords - understanding how embeddings from different languages relate in shared space is crucial
  - Quick check question: What properties make multilingual embeddings "well-aligned" and how does this alignment enable crosslingual transfer?

- Concept: Tokenization and vocabulary extension in language models
  - Why needed here: OFA operates on extended vocabularies - understanding how subwords are tokenized and how new tokens integrate with existing models is essential
  - Quick check question: How does vocabulary extension affect tokenization and what challenges arise when adding subwords from languages not in the original model?

## Architecture Onboarding

- Component map: Source model (RoBERTa or XLM-R) → Tokenizer extraction → Matrix factorization (SVD) → Primitive embeddings extraction → External embeddings processing → Similarity computation → Target coordinates initialization → Model assembly
- Critical path: Initialization pipeline must complete before any continued pretraining can begin - typically 2-4 hours depending on vocabulary size
- Design tradeoffs:
  - Embedding dimension vs performance: Lower dimensions (100-400) often perform better than full 768 due to reduced redundancy
  - External embeddings choice: Quality of crosslingual alignment directly impacts initialization quality
  - Computation budget: Matrix factorization adds preprocessing overhead but reduces continued pretraining time
- Failure signatures:
  - Poor initialization: Random-like embeddings, no performance advantage over baselines
  - Factorized embeddings collapse: Loss of semantic information during dimensionality reduction
  - Tokenizer mismatch: Subwords not properly aligned between source and target vocabularies
- First 3 experiments:
  1. Verify matrix factorization preserves variance: Compare explained variance ratios across different dimensions using SVD on source embeddings
  2. Test external embedding alignment: Compute nearest neighbor retrieval accuracy between languages in external embeddings
  3. Validate initialization quality: Measure cosine similarity between OFA-initialized embeddings and random initialization across different dimensions

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but identifies limitations including: only applying OFA to encoder-only models, not exploring the impact of different external embedding sources, and not addressing catastrophic forgetting during continued pretraining.

## Limitations

- Performance variations across tasks: While OFA shows clear improvements on sentence retrieval, results on text classification and sequence labeling are mixed, with some tasks showing minimal gains over baselines
- Heuristic parameter selection: The choice of factorization rank and embedding dimensions appears based on empirical observation rather than principled criteria, limiting reproducibility
- External dependency: OFA's effectiveness critically depends on the quality of external multilingual embeddings, but the framework doesn't address what happens with poorly aligned or low-quality external embeddings

## Confidence

- **High confidence**: OFA successfully reduces parameters through matrix factorization and demonstrates computational efficiency gains. The methodology for vocabulary extension and embedding initialization is technically sound and reproducible.
- **Medium confidence**: OFA accelerates convergence and achieves competitive performance on downstream tasks. While results are promising, the variations across tasks and limited ablation studies prevent stronger claims about universal superiority.
- **Low confidence**: The theoretical justification for why lower-dimensional factorizations outperform full-dimensional embeddings. The paper observes this empirically but lacks a compelling explanation for the phenomenon.

## Next Checks

1. **Cross-lingual alignment quality assessment**: Systematically evaluate how varying the quality of external multilingual embeddings (using embeddings with known alignment quality metrics) affects OFA's initialization performance and downstream task results.

2. **Factorization rank sensitivity analysis**: Conduct comprehensive ablation studies across different factorization ranks (not just the 100-400 range) and establish clear criteria for selecting optimal rank based on downstream performance rather than heuristic choices.

3. **Catastrophic forgetting evaluation**: Test OFA's robustness to catastrophic forgetting by evaluating source language performance after continued pretraining on target languages, addressing a critical concern for multilingual model adaptation not covered in the current evaluation.