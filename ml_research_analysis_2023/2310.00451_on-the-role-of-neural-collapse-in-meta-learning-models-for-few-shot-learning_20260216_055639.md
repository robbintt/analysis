---
ver: rpa2
title: On the Role of Neural Collapse in Meta Learning Models for Few-shot Learning
arxiv_id: '2310.00451'
source_url: https://arxiv.org/abs/2310.00451
tags:
- learning
- collapse
- training
- neural
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether neural collapse phenomena observed
  in standard classification networks generalize to meta-learning frameworks for few-shot
  learning. The authors examine Prototypical Networks (ProtoNet) on the Omniglot dataset,
  analyzing within-class (N C1) and between-class (N C2) variability collapse metrics
  during training and validation.
---

# On the Role of Neural Collapse in Meta Learning Models for Few-shot Learning

## Quick Facts
- **arXiv ID**: 2310.00451
- **Source URL**: https://arxiv.org/abs/2310.00451
- **Reference count**: 19
- **Key outcome**: Prototypical Networks show neural collapse trends during few-shot learning, but not to the same extent as standard classification networks due to their distance-based classification mechanism.

## Executive Summary
This study investigates whether neural collapse phenomena observed in standard classification networks generalize to meta-learning frameworks for few-shot learning. The authors examine Prototypical Networks on the Omniglot dataset, analyzing within-class and between-class variability collapse metrics during training and validation. Results show that while some neural collapse trends emerge—particularly with increased model size—complete collapse as defined by standard NC properties does not occur. The study observes that larger models demonstrate stronger collapse tendencies compared to smaller models, supporting prior work showing overparameterized models exhibit stronger neural collapse. However, unlike standard networks with classifiers, ProtoNets achieve near-zero training loss without forming simplex equiangular tight frames, suggesting a different learned structure despite similar classification decision boundaries.

## Method Summary
The study employs Prototypical Networks trained on the Omniglot dataset using both ConvNet and ResNet-18 backbones. The networks are trained for 100 epochs with 5-shot 5-way classification, using Adam optimizer with standard hyperparameters. Neural collapse metrics NC1 (within-class variability) and NC2 (between-class variability) are computed throughout training by extracting features from support and query sets. The authors compare these metrics across different model sizes and analyze how they correlate with training loss and accuracy. The experimental setup follows standard few-shot learning protocols with 60 support and 60 query examples per episode.

## Key Results
- Neural collapse trends emerge in Prototypical Networks during few-shot learning, though not to the same extent as in standard classification networks
- Larger models (ResNet-18) demonstrate stronger collapse tendencies compared to smaller models (ConvNet)
- Prototypical Networks achieve near-zero training loss without forming simplex equiangular tight frames, suggesting a different learned structure despite similar classification decision boundaries
- The classification mechanism in ProtoNets (distance-based) differs fundamentally from standard networks (classifier-based), which may explain the incomplete collapse

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Neural collapse trends emerge in Prototypical Networks during few-shot learning, though not to the same extent as in standard classification networks with classifier-based decision rules.
- **Mechanism**: Prototypical Networks use a distance-based classification mechanism (nearest class mean) rather than a learned classifier. This fundamentally alters how neural collapse properties manifest compared to standard networks. While class features still tend to collapse toward their respective class means (NC1 property), the absence of a classifier layer prevents complete formation of Simplex equiangular tight frames (NC2 property) and self-duality (NC3 property).
- **Core assumption**: The classification mechanism fundamentally determines whether complete neural collapse occurs, not just the optimization dynamics or network architecture.
- **Evidence anchors**: [abstract] "However these networks are seen as black-box models and understanding the representations learnt under different learning scenarios is crucial."

### Mechanism 2
- **Claim**: Larger models (ResNet-18) exhibit stronger neural collapse tendencies than smaller models (ConvNet) in Prototypical Networks, supporting the overparameterization hypothesis from standard networks.
- **Mechanism**: The degree of neural collapse in Prototypical Networks correlates with model size, similar to observations in standard classification networks. This suggests that the overparameterization effect on neural collapse is a fundamental property that persists across different network architectures and learning paradigms, even when the classification mechanism differs.
- **Core assumption**: The relationship between model size and neural collapse strength is preserved across different learning frameworks (few-shot vs. standard supervised learning).
- **Evidence anchors**: [section] "Interestingly, when using a larger model such as ResNet-18, the extent of neural collapse increases... Despite the discrepancy in the method of training, the results observed appear to support the observations made in prior work ([9]): overparameterized models lend themselves to stronger neural collapse."

### Mechanism 3
- **Claim**: The distance-based classification mechanism in Prototypical Networks forces the model to learn structures with distinct decision boundaries across all combinations of classes, even without complete neural collapse.
- **Mechanism**: While Prototypical Networks don't achieve complete neural collapse as measured by NC properties, they still learn feature representations that enable effective classification through distance metrics. The model implicitly learns to separate classes in feature space to minimize the distance-based loss, creating a structure that, while different from simplex ETF, still supports effective few-shot classification.
- **Core assumption**: The optimization objective (minimizing distance-based loss) inherently drives the model to learn separable feature representations, even without explicit neural collapse.
- **Evidence anchors**: [section] "Another interesting observation is that the classification decision in prototypical networks is identical to N C4, raising the question of whether these models 'force' the model to learn structures with distinct decision boundaries across all combinations of classes."

## Foundational Learning

- **Concept: Neural Collapse Properties**
  - Why needed here: Understanding the four NC properties (NC1: features collapse to class means, NC2: class means form simplex ETF, NC3: self-duality, NC4: nearest class center decision rule) is essential for analyzing whether and how they manifest in Prototypical Networks.
  - Quick check question: Can you explain why the formation of a simplex equiangular tight frame (NC2) is important for linear separability of classes?

- **Concept: Meta-Learning and Few-Shot Learning**
  - Why needed here: The study investigates neural collapse specifically in the context of few-shot learning frameworks, which have fundamentally different training dynamics compared to standard supervised learning.
  - Quick check question: How does the episode-based training in Prototypical Networks differ from standard mini-batch training in terms of data flow and optimization?

- **Concept: Distance-Based Classification**
  - Why needed here: Prototypical Networks use a distance-based classification mechanism (comparing query features to class prototypes) rather than a learned classifier, which is central to understanding why complete neural collapse may not occur.
  - Quick check question: What is the mathematical difference between computing distances to class prototypes versus computing outputs from a learned classifier layer?

## Architecture Onboarding

- **Component map**: Input images -> Backbone network -> Feature extraction -> Support set averaging -> Class prototypes -> Distance computation -> Classification probabilities -> Loss computation -> Backpropagation
- **Critical path**: 1. Input images pass through backbone to extract features 2. Support set features are averaged to create class prototypes 3. Query features are compared to prototypes using ℓ2 distance 4. Classification probabilities are computed using softmax over distances 5. Loss is computed and backpropagation updates backbone parameters
- **Design tradeoffs**: Model size vs. neural collapse strength (larger models show more collapse), Episode design (number of classes, shots per class) affects learning dynamics, Backbone architecture choice impacts feature representation quality, Distance metric choice (ℓ2 vs alternatives) affects classification performance
- **Failure signatures**: NC1 and NC2 metrics not decreasing during training despite decreasing loss, Large gap between training and validation performance (underfitting), Noisy NC metrics across episodes suggesting unstable learning, Metrics plateauing at non-zero values rather than approaching zero
- **First 3 experiments**: 1. Train Prototypical Network with ConvNet backbone on Omniglot and monitor NC1/NC2 metrics alongside loss 2. Repeat experiment with ResNet-18 backbone to observe effect of model size on neural collapse 3. Test on a standard classification dataset (e.g., CIFAR-10) to compare neural collapse behavior with and without the meta-learning framework

## Open Questions the Paper Calls Out

- Does neural collapse generalize to other meta-learning frameworks beyond Prototypical Networks?
- What is the theoretical explanation for why Prototypical Networks achieve near-zero training loss without forming a Simplex ETF structure?
- How do hyperparameters (learning rate, batch size, model depth) affect the degree of neural collapse in meta-learning models?
- Is there a fundamental architectural bias in models with last-layer classifiers that predisposes them to complete neural collapse?

## Limitations
- The study only examined Prototypical Networks and did not investigate other meta-learning frameworks
- The fundamental mechanism linking classification method to neural collapse properties remains theoretical without direct ablation studies
- Results may not generalize across different few-shot learning datasets beyond Omniglot

## Confidence
- **High confidence**: Neural collapse metrics decrease during training in Prototypical Networks, and larger models show stronger collapse tendencies
- **Medium confidence**: The absence of complete neural collapse is primarily due to the distance-based classification mechanism rather than other architectural factors
- **Low confidence**: The observed neural collapse patterns in few-shot learning will generalize to other meta-learning frameworks and datasets

## Next Checks
1. **Cross-dataset validation**: Test neural collapse behavior in Prototypical Networks on mini-ImageNet or tiered-ImageNet to verify generalizability beyond Omniglot
2. **Mechanism ablation**: Implement a modified Prototypical Network with an added classifier layer while maintaining the same loss function to test whether complete neural collapse emerges
3. **Framework comparison**: Compare neural collapse patterns across multiple few-shot learning frameworks (Matching Networks, Relation Networks, MAML) to determine if the distance-based classification mechanism is the primary factor