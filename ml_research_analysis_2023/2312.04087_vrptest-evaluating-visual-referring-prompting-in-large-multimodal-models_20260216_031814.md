---
ver: rpa2
title: 'VRPTEST: Evaluating Visual Referring Prompting in Large Multimodal Models'
arxiv_id: '2312.04087'
source_url: https://arxiv.org/abs/2312.04087
tags:
- visual
- prompting
- referring
- lmms
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VRPTEST, the first benchmark for evaluating
  visual referring prompting (VRP) in large multimodal models (LMMs). The authors
  introduce a dataset of 2,275 images across three tasks and propose a metamorphic
  testing framework to systematically generate new VRP combinations.
---

# VRPTEST: Evaluating Visual Referring Prompting in Large Multimodal Models

## Quick Facts
- arXiv ID: 2312.04087
- Source URL: https://arxiv.org/abs/2312.04087
- Authors: 
- Reference count: 40
- Proprietary models outperform open-source LMMs by 22.70% on average

## Executive Summary
This paper introduces VRPTEST, the first benchmark for evaluating visual referring prompting (VRP) in large multimodal models (LMMs). The authors develop an automated assessment framework based on metamorphic testing to systematically generate and evaluate VRP combinations across 2,275 images spanning three visual reasoning tasks. The study reveals that the choice of VRP strategy significantly impacts model performance, with variations ranging from -17.5% to +7.3%, and that proprietary models like GPT-4V substantially outperform open-source alternatives. The benchmark provides insights for optimizing VRP strategies and minimizing negative impacts on model performance.

## Method Summary
VRPTEST employs metamorphic testing to generate systematic variations of image/question pairs through visual interventions (color, font, shape, position) while preserving semantic meaning. The benchmark includes three tasks: IQtest for intelligence assessment, Reasoning for logical deduction, and LatexTab for mathematical expression understanding. An automated evaluation framework extracts model responses using ChatGLM3-based answer extraction and calculates performance using accuracy metrics for two tasks and BLEU-4 scoring for mathematical expressions. The framework enables systematic comparison across 12 different VRP strategy combinations without requiring human labeling.

## Key Results
- Proprietary models outperform open-source LMMs by 22.70% on average across all tasks and conditions
- Visual referring prompting strategy choice significantly affects accuracy, with performance variations ranging from -17.5% to +7.3%
- GPT-4V demonstrates superior performance with 85.34% accuracy in No-intervention conditions, while open-source models lag behind
- The automated metamorphic testing framework successfully evaluates LMM performance without human intervention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Metamorphic testing enables automated evaluation of LMMs without human labeling
- Mechanism: The paper uses metamorphic relations to generate new test cases by mutating original image/question pairs, then applies invariant checking to assess model consistency
- Core assumption: The metamorphic relations preserve the semantic meaning while changing visual presentation
- Evidence anchors:
  - [abstract]: "We develop an automated assessment framework based on software metamorphic testing techniques to evaluate the accuracy of LMMs without the need for human intervention or manual labeling"
  - [section 2.2]: "MT alleviates the need for manual labeling and human intervention, enabling automated evaluation of LMMs using our created image/question inputs and their MR-mutated counterparts"
  - [corpus]: Weak - no direct evidence found in corpus about automated evaluation effectiveness
- Break condition: If metamorphic relations change semantic meaning rather than just presentation, automated evaluation becomes unreliable

### Mechanism 2
- Claim: Visual referring prompting significantly impacts LMM performance with variations ranging from -17.5% to +7.3%
- Mechanism: Different combinations of visual interventions (color, font, shape, position) guide LMM attention differently, affecting accuracy
- Core assumption: LMMs process visual cues in a way that affects their reasoning about image content
- Evidence anchors:
  - [abstract]: "our quantitative analysis shows that the choice of prompt strategy significantly affects the accuracy of LMMs, with variations ranging from -17.5% to +7.3%"
  - [section 3.3]: "Numerical analysis reveals that different intervention strategies could result in up to a 17.5% negative gain (GPT4V, partial) or a 7.3% gain (CogVLM, full)"
  - [corpus]: Weak - no direct evidence in corpus about specific performance variations
- Break condition: If LMMs develop robust attention mechanisms that ignore visual interventions

### Mechanism 3
- Claim: Proprietary models outperform open-source models by 22.70% on average
- Mechanism: Commercial models have better training data, larger scale, and more sophisticated architectures
- Core assumption: Performance gap reflects fundamental architectural differences rather than evaluation artifacts
- Evidence anchors:
  - [abstract]: "The average accuracy of proprietary models is 22.70% higher than that of open-source models, but there is still a lot of room for improvement"
  - [section 3.3]: "On average, open-source accuracy lagged GPT-4V by 22.70% across datasets and conditions"
  - [corpus]: Weak - no direct evidence in corpus about performance gap causes
- Break condition: If evaluation methodology systematically favors proprietary models

## Foundational Learning

- Concept: Metamorphic testing
  - Why needed here: Enables systematic generation of test cases without manual labeling
  - Quick check question: What are the two components of a metamorphic relation (MR)?

- Concept: Visual referring prompting categories
  - Why needed here: Different intervention levels affect model performance differently
  - Quick check question: What are the three types of visual referring prompting and how do they differ?

- Concept: BLEU-4 scoring for LaTeX code evaluation
  - Why needed here: Standard exact match is insufficient for LaTeX code comparison
  - Quick check question: Why is BLEU-4 scoring used instead of exact match for LaTeXTab evaluation?

## Architecture Onboarding

- Component map:
  Data collection pipeline (IQtest, Reasoning, LatexTab) -> Metamorphic relation engine (color, font, shape, position mutations) -> LMM evaluation framework (response generation, answer extraction, scoring) -> Performance analysis dashboard (accuracy tracking across conditions)

- Critical path:
  1. Original image/question pair collection
  2. Metamorphic relation application to generate variants
  3. LMM response generation
  4. Answer extraction using ChatGLM3-based extractor
  5. Performance calculation and analysis

- Design tradeoffs:
  - Manual vs automated evaluation: Automated saves time but may miss nuanced errors
  - Fixed vs adaptive prompts: Adaptive optimizes per-model but adds complexity
  - Open vs proprietary models: Open models enable local testing but underperform commercially

- Failure signatures:
  - High variance across metamorphic variants suggests sensitivity to visual cues
  - Systematic underperformance on specific task types indicates architectural limitations
  - Unexpected accuracy gains from interventions may indicate bias exploitation

- First 3 experiments:
  1. Run all models on No-intervention baseline to establish performance floor
  2. Apply single-attribute metamorphic relations to isolate intervention effects
  3. Test model sensitivity by varying intervention intensity levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of visual referring prompts (No-intervention, Partial-intervention, Full-intervention) affect the performance of LMMs across various visual tasks?
- Basis in paper: [explicit] The paper introduces and categorizes three types of visual referring prompting and evaluates their impact on LMM performance.
- Why unresolved: While the paper provides initial results, it does not comprehensively explore how each type of visual referring prompt affects performance across a wide range of visual tasks.
- What evidence would resolve it: Extensive experiments testing all three types of visual referring prompts on diverse visual tasks, comparing performance metrics like accuracy and BLEU scores.

### Open Question 2
- Question: How can chain-of-thought prompting be effectively extended to visual referring prompting in LMMs?
- Basis in paper: [inferred] The paper mentions chain-of-thought (CoT) as a promising approach for improving reasoning in LLMs, but notes that extending it to visual referring prompting is still unclear.
- Why unresolved: Current CoT strategies are limited to text-based prompting, and the paper does not provide a concrete method for integrating CoT with visual referring prompts.
- What evidence would resolve it: A proposed framework or methodology for incorporating CoT into visual referring prompting, along with experimental results demonstrating its effectiveness.

### Open Question 3
- Question: How can LMMs be made more robust to variations in visual referring prompting, such as different colors, fonts, and shapes?
- Basis in paper: [explicit] The paper uses metamorphic testing to systematically generate variations in visual referring prompts and evaluates their impact on LMM performance.
- Why unresolved: While the paper identifies the sensitivity of LMMs to different prompt variations, it does not provide a solution for improving their robustness to such variations.
- What evidence would resolve it: A method or technique for enhancing LMM robustness to visual prompt variations, along with experimental results demonstrating improved performance across different prompt types.

## Limitations
- The 22.70% performance gap between proprietary and open-source models may reflect evaluation methodology rather than fundamental architectural differences
- The automated evaluation framework using ChatGLM3-based extraction may miss nuanced errors that human evaluation would catch
- The metamorphic testing framework's validity depends on whether generated variants truly preserve semantic meaning while only changing visual presentation

## Confidence
- **High confidence**: The benchmark dataset construction and basic evaluation methodology are well-documented and reproducible
- **Medium confidence**: The metamorphic testing framework's validity for semantic preservation requires further validation
- **Low confidence**: The attribution of performance gaps specifically to architectural differences versus evaluation artifacts

## Next Checks
1. Conduct human evaluation on a subset of metamorphic variants to verify that visual interventions preserve semantic meaning while only changing presentation, ensuring the automated evaluation framework's reliability

2. Evaluate the same LMMs on external visual reasoning benchmarks to determine if the 22.70% performance gap persists across different datasets and evaluation conditions

3. Systematically vary the intensity of visual interventions (e.g., font size, color contrast) to determine whether the reported -17.5% to +7.3% performance variations follow predictable patterns or show arbitrary sensitivity