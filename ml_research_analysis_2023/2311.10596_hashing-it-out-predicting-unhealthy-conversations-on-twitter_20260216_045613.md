---
ver: rpa2
title: 'Hashing it Out: Predicting Unhealthy Conversations on Twitter'
arxiv_id: '2311.10596'
source_url: https://arxiv.org/abs/2311.10596
tags:
- twitter
- tweet
- personal
- context
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a BERT-based transformer model for predicting
  personal attacks in Twitter conversations, using two prior tweets as context. The
  model outperforms an LSTM baseline (CRAFT) on their dataset, achieving 85% accuracy
  and 70% AUPR compared to 76% and 55% respectively.
---

# Hashing it Out: Predicting Unhealthy Conversations on Twitter

## Quick Facts
- arXiv ID: 2311.10596
- Source URL: https://arxiv.org/abs/2311.10596
- Reference count: 18
- The authors propose a BERT-based transformer model for predicting personal attacks in Twitter conversations, using two prior tweets as context.

## Executive Summary
This paper introduces a BERTweet-based transformer model for predicting personal attacks in Twitter conversations using two prior tweets as context. The model outperforms an LSTM baseline (CRAFT) on their dataset, achieving 85% accuracy and 70% AUPR compared to 76% and 55% respectively. The authors address class imbalance through synthetic oversampling, which improves performance by creating artificial context examples. The study demonstrates that transformer models are effective for forecasting conversational events on Twitter, particularly when fine-tuned on task-specific data.

## Method Summary
The authors fine-tuned a pre-trained BERTweet transformer model on a dataset of 5656 tweets from controversial topics (UBI, abortion, immigration), labeled as personal attacks or not. They used two prior tweets as context for each target tweet, concatenated with special tokens. To address class imbalance (1177 positive vs 4479 negative examples), they implemented synthetic oversampling using GloVe Twitter embeddings and k-nearest neighbors to create 355 artificial positive examples. The model was trained with a learning rate of 5e-5, batch size 10, and limited to 4 epochs to prevent overfitting.

## Key Results
- BERTweet achieved 85% accuracy and 70% AUPR compared to CRAFT LSTM's 76% accuracy and 55% AUPR
- Using two-tweet context provided significantly more predictive signal than single-tweet context
- Synthetic oversampling improved performance but also introduced overfitting risks
- The model showed clear advantages in performance over the existing LSTM baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERTweet transformer architecture outperforms LSTM baseline by capturing long-range dependencies and context in Twitter conversations.
- Mechanism: Self-attention mechanisms allow BERTweet to weigh the importance of all previous tweets in the conversation without information loss from sequential processing, unlike LSTMs which struggle with long-range dependencies.
- Core assumption: Twitter conversations contain patterns where earlier tweets have strong predictive power for later conversational outcomes.
- Evidence anchors:
  - [abstract] "BERT architecture, pre-trained on a large Twitter corpus and fine-tuned on our task, is efficient and effective in making such predictions. This model shows clear advantages in performance to the existing LSTM model we use as a baseline."
  - [section 4.2] "we deem that using an Attention model could allow us to curb the sequential architecture of RNNs, allowing the model to make use of previous context without information loss."

### Mechanism 2
- Claim: Synthetic oversampling of positive examples mitigates class imbalance and improves model performance.
- Mechanism: By replacing words in positive examples with similar words using GloVe Twitter embeddings and k-nearest neighbors, synthetic oversampling creates plausible new training examples that preserve conversational context while expanding the positive class representation.
- Core assumption: Word substitutions maintain the conversational intent and toxicity level of original examples.
- Evidence anchors:
  - [section 3.1] "we made use of oversampling for positive examples, which came at the expense of overfitting" and "we developed 355 synthetic positive examples."
  - [section 5.1] "BERTweet fine-tuned with synthetic oversampling (BT SOS)" achieves 0.85 accuracy vs 0.82 for standard oversampling.

### Mechanism 3
- Claim: Using two-tweet context provides significantly more predictive signal than single-tweet context for forecasting personal attacks.
- Mechanism: The combination of the immediately preceding tweet and the tweet before that captures conversational dynamics and escalation patterns that single tweets miss, as shown by dramatic performance drops when context is truncated.
- Core assumption: Derailment is a process that unfolds over multiple conversational turns, not isolated events.
- Evidence anchors:
  - [section 5.2.1] "We were surprised to see a dramatic drop in performance, with the model performing at .74 accuracy and only .50 AUPR" when using single tweet context instead of two-tweet context.
  - [section 4.1] "The two tweets prior are referred to as the 'context'. This is the input into our model."

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: Understanding how self-attention differs from RNNs is crucial for grasping why BERTweet outperforms LSTM baselines in capturing conversational context.
  - Quick check question: What is the key computational difference between self-attention and recurrent architectures that enables transformers to handle long-range dependencies better?

- Concept: Class imbalance and oversampling techniques
  - Why needed here: The dataset has 1177 positive examples vs 4479 negative examples, making understanding imbalance mitigation essential for replicating the results.
  - Quick check question: Why might synthetic oversampling using word embeddings be preferable to simple random oversampling when dealing with text data?

- Concept: Precision-recall curves and area under the curve (AUPR)
  - Why needed here: With severe class imbalance, AUPR is a more informative metric than accuracy or ROC-AUC, and understanding this is key to interpreting model performance.
  - Quick check question: In a highly imbalanced dataset, why is AUPR typically more informative than ROC-AUC for evaluating classification performance?

## Architecture Onboarding

- Component map:
  - Data collection: Twitter API → conversation ID extraction → tweet labeling (Mechanical Turk + existing dataset + manual tagging)
  - Preprocessing: Tokenization with BERTweet tokenizer → normalization for Twitter-specific elements → context concatenation with [CLS] and </s> tokens
  - Model: Pre-trained BERTweet base → classification head (linear layer + sigmoid) → binary classification
  - Training: Fine-tuning with learning rate 5e-5 → batch size 10 → max 4 epochs → synthetic oversampling for positive class
  - Evaluation: Accuracy, precision, recall, F1, AUPR → precision-recall curves

- Critical path: Data collection → preprocessing → model fine-tuning → evaluation → synthetic oversampling iteration

- Design tradeoffs:
  - Two-tweet context vs longer context: Two tweets provide optimal signal without exceeding BERTweet's 130 token limit
  - Synthetic oversampling vs data collection: Oversampling addresses imbalance but risks overfitting vs expensive manual data collection
  - BERTweet vs larger models: Base model balances performance with computational efficiency

- Failure signatures:
  - Overfitting: Spikes in validation loss while accuracy/AUPR improve, indicating overconfident wrong predictions
  - Class imbalance issues: Low recall despite reasonable precision, suggesting the model misses positive cases
  - Context misunderstanding: Performance drops when tweet separator token is removed, indicating the model relies on speaker delineation

- First 3 experiments:
  1. Baseline comparison: Train CRAFT LSTM model with standard oversampling on the Twitter dataset to establish performance baseline
  2. Transformer validation: Fine-tune BERTweet with standard oversampling to measure transformer vs LSTM performance difference
  3. Oversampling comparison: Implement synthetic oversampling and compare against standard oversampling to quantify improvement from context-preserving data augmentation

## Open Questions the Paper Calls Out
- How does the model's performance change when incorporating GIF sentiment analysis?
- What is the impact of conversation topic diversity on model generalization?
- How effective is the model at detecting nuanced language indicators of unhealthy conversations?

## Limitations
- Relatively small dataset (5656 tweets) with significant class imbalance raises concerns about model generalizability
- Dataset limited to controversial political topics (UBI, abortion, immigration) may limit model's ability to generalize to other conversational domains
- Does not address potential biases in Mechanical Turk annotations or explore how demographic factors might influence labeling decisions

## Confidence
- High Confidence: The comparative performance advantage of BERTweet over LSTM baseline (85% vs 76% accuracy)
- Medium Confidence: The effectiveness of synthetic oversampling and its improvement from 76% to 85% accuracy
- Medium Confidence: The importance of two-tweet context demonstrated through ablation studies

## Next Checks
1. Cross-dataset validation: Test the trained model on an independent Twitter conversation dataset from different topics
2. Context length ablation: Systematically evaluate model performance with varying context lengths (1, 2, 3, 4 previous tweets)
3. Robustness to synthetic data: Create a synthetic test set using the same word substitution methodology and evaluate model performance