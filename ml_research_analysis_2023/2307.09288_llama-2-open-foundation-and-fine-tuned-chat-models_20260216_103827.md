---
ver: rpa2
title: 'Llama 2: Open Foundation and Fine-Tuned Chat Models'
arxiv_id: '2307.09288'
source_url: https://arxiv.org/abs/2307.09288
tags:
- safety
- data
- llama
- reward
- chat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Llama 2 introduces open foundation and fine-tuned large language
  models (LLMs) ranging from 7B to 70B parameters, trained on 2 trillion tokens. It
  outperforms existing open-source chat models on most benchmarks and matches closed-source
  models in human evaluations for helpfulness and safety.
---

# Llama 2: Open Foundation and Fine-Tuned Chat Models

## Quick Facts
- arXiv ID: 2307.09288
- Source URL: https://arxiv.org/abs/2307.09288
- Authors: 
- Reference count: 40
- Key outcome: Llama 2 introduces open foundation and fine-tuned large language models (LLMs) ranging from 7B to 70B parameters, trained on 2 trillion tokens. It outperforms existing open-source chat models on most benchmarks and matches closed-source models in human evaluations for helpfulness and safety.

## Executive Summary
Llama 2 introduces a family of open-source large language models ranging from 7B to 70B parameters, trained on 2 trillion tokens. The models feature improved architecture with doubled context length (4k tokens) and grouped-query attention for better inference scalability. Llama 2-Chat, the fine-tuned version, is optimized for dialogue use cases through supervised fine-tuning and reinforcement learning with human feedback (RLHF), demonstrating strong performance on benchmarks and human evaluations for both helpfulness and safety.

## Method Summary
Llama 2 models are pretrained using an optimized transformer architecture with 2 trillion tokens of publicly available data, excluding Meta's own products. The models feature expanded context windows (4k tokens) and grouped-query attention for improved efficiency. Fine-tuning involves supervised fine-tuning (SFT) on high-quality instruction data, followed by RLHF using human preference data for both helpfulness and safety. Safety improvements include targeted annotations, iterative red-teaming, and continuous evaluation. The entire pipeline is designed to create models suitable for dialogue applications while maintaining open access.

## Key Results
- Llama 2 models outperform open-source chat models on most tested benchmarks
- Human evaluations show Llama 2-Chat matches closed-source models in helpfulness and safety
- The models demonstrate strong generalization, tool-use emergence, and time-awareness capabilities
- Safety improvements show measurable reduction in harmful outputs, though limitations remain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Llama 2 achieves improved performance through larger context windows (4k vs 2k tokens).
- Mechanism: Increasing the context window allows the model to retain and process more information during inference, enabling better handling of longer dialogues and documents.
- Core assumption: The model's architecture and attention mechanisms can effectively scale to handle the increased context without degradation.
- Evidence anchors:
  - [abstract] "doubled the context length of the model"
  - [section] "The longer context window enables models to process more information, which is particularly useful for supporting longer histories in chat applications, various summarization tasks, and understanding longer documents."
- Break condition: If the model's attention mechanisms become inefficient or unable to retain relevant information across the longer context, performance may degrade.

### Mechanism 2
- Claim: Llama 2's performance is enhanced by using grouped-query attention (GQA) in larger models.
- Mechanism: GQA reduces the memory footprint of the KV cache, allowing larger models to be served efficiently without sacrificing performance.
- Core assumption: Sharing key and value projections across multiple heads does not significantly impact model quality.
- Evidence anchors:
  - [section] "We compare MQA and GQA variants with an MHA baseline... From the results, we observe that the GQA variant performs comparably to the MHA baseline on most evaluation tasks"
- Break condition: If the reduction in KV projections leads to a significant loss in model expressiveness or accuracy.

### Mechanism 3
- Claim: Llama 2's fine-tuning with RLHF and human feedback significantly improves helpfulness and safety.
- Mechanism: By iteratively collecting human preference data and using it to train reward models, the fine-tuned models learn to generate responses that align with human preferences for helpfulness and safety.
- Core assumption: Human preference data accurately captures the desired model behavior and can be effectively used to guide fine-tuning.
- Evidence anchors:
  - [abstract] "Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases... based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models."
  - [section] "Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models."
- Break condition: If the human preference data is biased or fails to capture the full range of desired behaviors, the fine-tuned models may not perform as expected.

## Foundational Learning

- Concept: Transformer architecture
  - Why needed here: Llama 2 is based on an optimized transformer architecture, so understanding transformers is crucial for comprehending the model's design and capabilities.
  - Quick check question: What are the key components of a transformer and how do they interact during the forward pass?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is a core technique used in fine-tuning Llama 2-Chat, so understanding how it works is essential for grasping the model's alignment process.
  - Quick check question: How does RLHF use human preference data to train reward models and guide the fine-tuning process?

- Concept: Safety considerations in language model development
  - Why needed here: Llama 2-Chat underwent safety fine-tuning, so understanding the challenges and techniques for mitigating harmful outputs is important for responsible deployment.
  - Quick check question: What are some common risks associated with language models and what techniques can be used to mitigate them?

## Architecture Onboarding

- Component map: Pretraining -> Supervised Fine-tuning (SFT) -> Reinforcement Learning with Human Feedback (RLHF) -> Safety Fine-tuning -> Evaluation and Deployment

- Critical path: Pretraining -> SFT -> RLHF -> Safety fine-tuning -> Evaluation and deployment

- Design tradeoffs:
  - Larger context window (4k) vs. increased computational cost and potential inefficiencies
  - GQA vs. MQA vs. MHA: Balancing memory efficiency and model performance
  - Extensive human feedback collection vs. time and cost considerations

- Failure signatures:
  - Pretraining: Poor performance on downstream tasks, high perplexity on validation data
  - SFT: Hallucinations, lack of coherence, failure to follow instructions
  - RLHF: Reward hacking, failure to align with human preferences, degradation in other capabilities
  - Safety fine-tuning: Overly cautious responses, false refusals, failure to handle adversarial prompts

- First 3 experiments:
  1. Evaluate pretrained Llama 2 on a suite of benchmarks to establish a baseline
  2. Fine-tune Llama 2 with SFT using a small, high-quality dataset and evaluate performance
  3. Iteratively collect human preference data and fine-tune with RLHF, evaluating improvements in helpfulness and safety at each stage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of dataset contamination on model performance, and how can it be quantified?
- Basis in paper: [explicit] 
- Why unresolved: The paper mentions contamination analysis but does not provide a definitive answer on its impact or methods to quantify it.
- What evidence would resolve it: Detailed analysis of contamination effects on various datasets and models, along with quantification methods.

### Open Question 2
- Question: How does the performance of Llama 2 models vary across different languages, and what are the limitations in non-English languages?
- Basis in paper: [inferred]
- Why unresolved: The paper mentions that Llama 2 is primarily trained on English data and has limited proficiency in other languages, but does not provide detailed performance metrics.
- What evidence would resolve it: Comprehensive evaluation of Llama 2 models on various non-English datasets, including performance metrics and limitations.

### Open Question 3
- Question: What are the long-term effects of using Llama 2 models in real-world applications, and how can they be assessed?
- Basis in paper: [inferred]
- Why unresolved: The paper mentions that benchmarks do not capture long-term effects, but does not provide a framework for assessing them.
- What evidence would resolve it: Long-term studies of Llama 2 models in real-world applications, including user feedback and impact assessments.

## Limitations

- Performance on non-English languages remains significantly limited due to training data composition
- Safety improvements, while measurable, cannot guarantee complete robustness against all adversarial scenarios
- The reliance on vendor-based annotation introduces potential quality and consistency concerns that are difficult to quantify

## Confidence

**High Confidence Claims:**
- Pretraining methodology and architectural improvements (context length, GQA) are well-documented and reproducible
- Benchmark performance comparisons against other open-source models are technically sound
- The core RLHF pipeline follows established methodology

**Medium Confidence Claims:**
- Human evaluation results for helpfulness and safety, as these depend heavily on subjective assessment criteria
- Generalization capabilities, which were tested on limited downstream tasks
- Safety improvements, as adversarial robustness cannot be fully validated without extensive external testing

**Low Confidence Claims:**
- Claims about matching "closed-source models" performance, as specific competitor models and comprehensive comparisons are not provided
- Long-term stability of the safety alignment under diverse deployment conditions

## Next Checks

1. **Independent Safety Red-Teaming**: Commission third-party adversarial testing across diverse demographic and cultural contexts to validate safety claims beyond the authors' internal evaluations.

2. **Cross-Lingual Benchmark Validation**: Conduct systematic testing on a comprehensive suite of non-English benchmarks to quantify the actual performance gap and identify specific linguistic failure modes.

3. **Reward Model Robustness Testing**: Systematically probe the RLHF reward models for reward hacking vulnerabilities by testing with deliberately optimized adversarial prompts that might exploit learned reward patterns.