---
ver: rpa2
title: 'Utilising a Large Language Model to Annotate Subject Metadata: A Case Study
  in an Australian National Research Data Catalogue'
arxiv_id: '2310.11318'
source_url: https://arxiv.org/abs/2310.11318
tags:
- learning
- subject
- metadata
- data
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores using GPT-3.5 via in-context learning to automatically
  annotate subject metadata for datasets in an Australian research data catalogue.
  The method uses prompts with task instructions, demonstration examples, and classification
  rules to guide the model in predicting ANZSRC-FoR category labels.
---

# Utilising a Large Language Model to Annotate Subject Metadata: A Case Study in an Australian National Research Data Catalogue

## Quick Facts
- arXiv ID: 2310.11318
- Source URL: https://arxiv.org/abs/2310.11318
- Reference count: 24
- Key outcome: GPT-3.5 achieved macro- and micro-average precision of 0.62 and 0.65, respectively, for automated ANZSRC-FoR category annotation

## Executive Summary
This study explores the use of GPT-3.5 via in-context learning to automatically annotate subject metadata for datasets in an Australian research data catalogue. The method uses prompts with task instructions, demonstration examples, and classification rules to guide the model in predicting ANZSRC-FoR category labels. GPT-3.5 achieved promising precision on several categories, with overall macro- and micro-average precision of 0.62 and 0.65, respectively. However, performance was lower in multidisciplinary or complex categories due to limited contextual information and lack of training on discipline-specific annotation rules. Results show potential for cost-effective automation but also highlight challenges with nuanced or interdisciplinary datasets.

## Method Summary
The study employs in-context learning with GPT-3.5 to predict ANZSRC-FoR subject categories for dataset records from Research Data Australia. Prompts were constructed containing task instructions, demonstration examples (either random or relevant based on text embedding similarity), classification rules, and the target dataset record. The model generated predictions which were post-processed to match ANZSRC-FoR categories. Performance was evaluated using precision metrics, comparing different prompt strategies including random and relevant demonstration examples.

## Key Results
- GPT-3.5 achieved macro-average precision of 0.62 and micro-average precision of 0.65 across ANZSRC-FoR categories
- Including relevant demonstration examples improved performance for specific categories like physical sciences (02), built environment and design (12), and commerce, management, tourism, and services (15)
- Performance was notably lower for multidisciplinary or complex categories due to limited contextual information in title/description fields

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-3.5's strong performance in five subject disciplines is due to its ability to leverage contextual information from the dataset title and description to infer relevant ANZSRC-FoR categories.
- Mechanism: The in-context learning approach provides GPT-3.5 with task instructions, demonstration examples, and classification rules that guide its reasoning process. By analyzing the semantic content of the dataset's metadata, GPT-3.5 can match it to the most appropriate subject category.
- Core assumption: The dataset's title and description contain sufficient semantic information to accurately infer its subject category.
- Evidence anchors:
  - [abstract] "GPT-3.5 achieved promising precision on several categories, with overall macro- and micro-average precision of 0.62 and 0.65, respectively."
  - [section] "By analyzing the semantic content of the dataset's metadata, GPT-3.5 can match it to the most appropriate subject category."
  - [corpus] "In-Context Learning on a Budget: A Case Study in Token Classification" suggests that in-context learning can be effective for classification tasks with limited resources.
- Break condition: If the dataset's title and description are too vague, incomplete, or lack domain-specific terminology, GPT-3.5's ability to accurately infer the subject category will be compromised.

### Mechanism 2
- Claim: Including relevant demonstration examples in the prompt improves overall performance by providing GPT-3.5 with more informative and contextually similar examples to learn from.
- Mechanism: The prompt construction process involves selecting demonstration examples that are semantically similar to the target dataset. This allows GPT-3.5 to better understand the classification task and apply the learned patterns to the new dataset.
- Core assumption: Demonstration examples that are semantically similar to the target dataset will provide more relevant and useful information for GPT-3.5's inference process.
- Evidence anchors:
  - [abstract] "Additionally, including relevant demonstration examples in the prompt improves overall performance, with one high-precision discipline added and one low-precision discipline removed."
  - [section] "Incorporating relevant examples results in a significant improvement in certain categories, such as 'physical sciences (02),' 'built environment and design (12),' and 'commerce, management, tourism, and services (15).'"
  - [corpus] "PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named Entity Detection" suggests that the choice of demonstration examples can impact the performance of in-context learning models.
- Break condition: If the pool of demonstration examples is too small or lacks diversity, GPT-3.5 may not have enough varied examples to learn from, leading to suboptimal performance.

### Mechanism 3
- Claim: GPT-3.5's lower performance in multidisciplinary or complex categories is due to the limited contextual information provided in the prompt and the lack of training on discipline-specific annotation rules.
- Mechanism: The in-context learning approach relies solely on the information provided in the prompt, which may not capture the full complexity of multidisciplinary datasets. Additionally, GPT-3.5 has not been explicitly trained on the specific annotation rules and guidelines used by researchers when submitting datasets to data catalogues.
- Core assumption: Multidisciplinary datasets require more nuanced understanding and context to accurately classify them into a single subject category.
- Evidence anchors:
  - [abstract] "However, performance was lower in multidisciplinary or complex categories due to limited contextual information and lack of training on discipline-specific annotation rules."
  - [section] "The primary challenge lies in designing a prompt that is specifically tailored to a specific task, as the model may struggle with multidisciplinary datasets that span multiple subject areas."
  - [corpus] "IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models" suggests that selective annotations can improve the performance of in-context learners, but may not fully address the challenges of multidisciplinary datasets.
- Break condition: If the prompt can be enhanced to provide more comprehensive contextual information and if GPT-3.5 can be fine-tuned on discipline-specific annotation rules, its performance in multidisciplinary categories may improve.

## Foundational Learning

- Concept: ANZSRC-FoR classification system
  - Why needed here: Understanding the ANZSRC-FoR classification system is crucial for interpreting the results and evaluating the performance of GPT-3.5 in annotating subject metadata.
  - Quick check question: What are the three levels of the ANZSRC-FoR classification system, and how are they denoted?

- Concept: In-context learning
  - Why needed here: In-context learning is the core mechanism used in this study to leverage GPT-3.5 for automated subject metadata annotation.
  - Quick check question: What are the three sequential stages of the in-context learning paradigm introduced by GPT-3?

- Concept: Prompt engineering
  - Why needed here: Effective prompt engineering is essential for guiding GPT-3.5's inference process and achieving optimal performance in subject metadata annotation.
  - Quick check question: What are the four components of the prompts used in this study, and how do they contribute to the model's understanding of the task?

## Architecture Onboarding

- Component map: RDA dataset records -> Prompt construction module -> GPT-3.5 API -> Post-processing module -> Evaluation module
- Critical path: 1. Retrieve dataset records from RDA 2. Construct prompts with relevant demonstration examples 3. Send prompts to GPT-3.5 API and obtain predictions 4. Post-process the model's responses to extract predicted ANZSRC-FoR categories 5. Evaluate the performance using precision metrics and compare results across prompt strategies
- Design tradeoffs:
  - Using a smaller number of demonstration examples (3) in the prompt to reduce token usage and cost, but potentially limiting the model's exposure to diverse examples
  - Focusing on predicting the most relevant ANZSRC-FoR category for each dataset, rather than generating multiple labels, to simplify the evaluation process but potentially missing important secondary classifications
  - Relying solely on the dataset's title and description for subject inference, without incorporating other metadata fields, to maintain a consistent input format but potentially missing valuable contextual information
- Failure signatures: Low precision in certain ANZSRC-FoR categories, particularly those involving multidisciplinary or complex datasets; inconsistent performance across different prompt strategies; difficulty in distinguishing between similar ANZSRC-FoR categories, leading to misclassification of datasets
- First 3 experiments:
  1. Evaluate the impact of using different numbers of demonstration examples (e.g., 1, 3, and 5) in the prompt on GPT-3.5's performance across various ANZSRC-FoR categories
  2. Investigate the effect of incorporating additional metadata fields (e.g., data source key, temporal coverage) in the prompt on the model's ability to accurately classify multidisciplinary datasets
  3. Compare the performance of GPT-3.5 with other large language models (e.g., GPT-4, BERT) on the same subject metadata annotation task to identify potential improvements or alternative approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt engineering strategies (e.g., hard prompts vs. soft prompts) impact the performance of in-context learning models for subject metadata annotation?
- Basis in paper: [explicit] The paper discusses the importance of prompt engineering in in-context learning and mentions that researchers have explored approximately two categories of automated prompt learning techniques: discovering hard prompts and tuning soft prompts.
- Why unresolved: The paper does not directly compare the performance of hard prompts and soft prompts in the context of subject metadata annotation. It only mentions these techniques as existing approaches without evaluating their effectiveness.
- What evidence would resolve it: A comparative study that applies both hard prompt and soft prompt techniques to the same subject metadata annotation task and evaluates their performance using metrics such as precision, recall, and F1-score.

### Open Question 2
- Question: How does the performance of in-context learning models for subject metadata annotation vary across different levels of the ANZSRC-FoR hierarchy (2-digit, 4-digit, and 6-digit codes)?
- Basis in paper: [explicit] The paper mentions that the ANZSRC-FoR classification has a hierarchical structure with three levels (2-digit, 4-digit, and 6-digit codes) and that the study focuses on predicting the most relevant label at the top level (2-digit code).
- Why unresolved: The paper does not explore the performance of in-context learning models at different levels of the ANZSRC-FoR hierarchy. It only reports results for the 2-digit code level.
- What evidence would resolve it: An extension of the study that applies the in-context learning approach to predict labels at the 4-digit and 6-digit code levels and compares the performance with the 2-digit code level results.

### Open Question 3
- Question: How does the inclusion of additional metadata features (e.g., data_source_key, date_from, and date_to) impact the performance of in-context learning models for subject metadata annotation?
- Basis in paper: [explicit] The paper mentions that the current method only uses the title and description of a dataset as features and suggests that there might be other informative metadata that can be leveraged for effective categorization.
- Why unresolved: The paper does not investigate the impact of including additional metadata features on the performance of in-context learning models. It only speculates about the potential benefits of using such features.
- What evidence would resolve it: An experiment that incorporates additional metadata features into the prompt and evaluates the performance of the in-context learning model compared to the baseline model that only uses title and description.

## Limitations
- Limited contextual information in title and description fields constrains performance on multidisciplinary or complex datasets
- Absence of training data for discipline-specific annotation rules significantly impacts performance on categories requiring nuanced understanding
- Evaluation relies solely on precision metrics without exploring recall or F1 scores

## Confidence
- High confidence: GPT-3.5 can achieve reasonable precision (0.62-0.65) on clearly defined research categories with appropriate in-context examples
- Medium confidence: Relevant demonstration examples improve performance for specific categories, but effects are variable across ANZSRC-FoR fields
- Low confidence: Scalability to truly multidisciplinary datasets or categories requiring deep domain expertise

## Next Checks
1. Test the impact of incorporating additional metadata fields (keywords, temporal coverage, data source) into prompts to assess whether expanded context improves performance on multidisciplinary categories
2. Conduct detailed error analysis comparing GPT-3.5's classification decisions against actual classification rules used by human annotators
3. Apply the same in-context learning methodology to dataset catalogues from other countries or disciplines to determine generalizability beyond Australian research context