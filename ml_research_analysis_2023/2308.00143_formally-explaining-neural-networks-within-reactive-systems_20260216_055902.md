---
ver: rpa2
title: Formally Explaining Neural Networks within Reactive Systems
arxiv_id: '2308.00143'
source_url: https://arxiv.org/abs/2308.00143
tags:
- explanation
- steps
- explanations
- contrastive
- minimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of generating formal explanations
  for deep neural network (DNN) controllers within reactive systems, a significant
  challenge in explainable AI. The authors introduce novel methods for computing abductive
  explanations and contrastive examples for multi-step decision processes, leveraging
  formal verification to provide guarantees of correctness.
---

# Formally Explaining Neural Networks within Reactive Systems

## Quick Facts
- arXiv ID: 2308.00143
- Source URL: https://arxiv.org/abs/2308.00143
- Reference count: 40
- One-line primary result: Novel methods for computing formal abductive explanations and contrastive examples for multi-step DNN controllers in reactive systems, leveraging formal verification for correctness guarantees.

## Executive Summary
This paper addresses the challenge of generating formal explanations for deep neural network (DNN) controllers within reactive systems, a significant problem in explainable AI. The authors introduce novel methods for computing abductive explanations and contrastive examples for multi-step decision processes by leveraging formal verification techniques. Their approach efficiently calculates succinct explanations by exploiting system transition constraints to curtail the search space explored by the underlying verifier. Evaluated on two popular benchmarks from automated navigation, the methods demonstrate efficient computation of minimal and minimum explanations, significantly outperforming the state of the art.

## Method Summary
The paper proposes four methods for computing formal k-step explanations in reactive systems: one-shot unrolling, independent step-by-step, incremental enumeration, and reverse incremental enumeration of contrastive examples. The core insight is to exploit transition constraints between consecutive steps to reduce the number of times the network needs to be unrolled, thus circumventing potential exponential blow-up in runtime. For minimal explanations, the approach iteratively removes features from the explanation set and checks if the modified explanation still explains the execution using simplified verification queries. For minimum explanations, the method enumerates contrastive examples and calculates their minimum hitting set. The methods are implemented using the Marabou DNN verifier and evaluated on two DRL benchmarks (GridWorld and TurtleBot).

## Key Results
- The proposed methods efficiently compute minimal and minimum explanations for multi-step executions
- The approaches significantly outperform state-of-the-art heuristic XAI techniques in terms of explanation correctness
- The methods scale to multi-step executions with up to 5 steps on the evaluated benchmarks
- The incremental enumeration method finds minimal explanations faster than the one-shot unrolling approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Formal abductive explanations for multi-step reactive systems can be computed by leveraging the transition constraints to curtail the search space explored by the underlying DNN verifier.
- Mechanism: The approach exploits the system's transition constraints between consecutive steps to reduce the number of times the network needs to be unrolled, thus circumventing potential exponential blow-up in runtime. It also allows for more efficient enumeration of candidate explanations by encoding verification queries that only encompass the relevant steps, rather than all k-steps at once.
- Core assumption: The transition constraints between steps are well-defined and can be encoded as constraints in the verification query.
- Evidence anchors:
  - [abstract]: "Our approach efficiently calculates succinct explanations, by exploiting the system's transition constraints in order to curtail the search space explored by the underlying verifier."
  - [section]: "Encoding N[k] results in an input space roughly k times the size of any single-step encoding. Such an unrolling for our running example is depicted in Fig. 6. Due to the NP-completeness of DNN verification, this may cause an exponential growth in the verification time of each query."
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.49, average citations=0.0." (Weak corpus evidence, only tangentially related to the specific mechanism.)

### Mechanism 2
- Claim: Minimal and minimum explanations for multi-step executions can be found by iteratively removing features from the explanation set and checking if the modified explanation still explains the execution using a simplified verification query.
- Mechanism: The approach starts with an initial explanation set containing all features and iteratively attempts to remove features. For each feature removal attempt, a verification query is dispatched to check if the modified explanation still explains the execution. The query is simplified by only requiring the DNN to select the action in the current step, rather than all actions in the execution, based on Lemma 1.
- Core assumption: Lemma 1 holds, i.e., removing features from an explanation at step i, while fixing features in steps i+1 to k, ensures that only action ai can change.
- Evidence anchors:
  - [section]: "Lemma 1. Let E = (E1, E2, . . . Ek) be a k-step explanation for execution E, and let 1 ≤i ≤k such that ∀j > i it holds that Ej = F . Let E′be the set obtained by removing a set of features F′⊆Ei from Ei, i.e., E′= (E1, . . . , Ei−1, Ei ∖F′, Ei+1, . . . , Ek). In this case, fixing the features in E′prevents any changes in the first i−1 actions (a1, . . . , ai−1); and if any of the last k−i+1 actions (ai, . . . , ak) change, then ai must also change."
  - [abstract]: "We suggest methods for efficiently calculating succinct explanations, by exploiting the system's transition constraints in order to curtail the search space explored by the underlying verifier."

### Mechanism 3
- Claim: Minimum explanations can be efficiently found by enumerating contrastive examples and calculating their minimum hitting set (MHS).
- Mechanism: The approach first finds all contrastive examples for each step of the execution. Then, it uses reverse incremental enumeration to classify each contrastive example as independent, dependent, or spurious. Independent contrastive examples can be used directly in computing the MHS, while dependent ones are recursively enumerated to find all multi-step contrastive examples that contain them. The MHS of all contrastive examples is then calculated to obtain the minimum explanation.
- Core assumption: The duality between contrastive examples and explanations holds, i.e., minimal hitting sets of all contrastive examples are minimal explanations.
- Evidence anchors:
  - [section]: "A contrastive example for our running example appears in Fig. 3. Checking whether C is a contrastive example can be performed using the query ⟨P, N, Q⟩= ⟨(F ∖C) = v, N, Q¬c⟩: C is contrastive iff the quest is SAT. Any set containing a contrastive example is contrastive, and so we consider only contrastive examples that are minimal, i.e., which do not contain any smaller contrastive examples."
  - [abstract]: "We also demonstrate that our methods produce formal explanations that are more reliable than competing, non-verification-based XAI techniques."

## Foundational Learning

- Concept: Deep Neural Networks (DNNs) and their verification
  - Why needed here: The approach relies on DNN verification to check the validity of candidate explanations and contrastive examples. Understanding DNNs and their verification is crucial for implementing and applying the proposed methods.
  - Quick check question: What is the NP-completeness result for DNN verification, and how does it impact the efficiency of the proposed methods?

- Concept: Reactive systems and their formalization
  - Why needed here: The approach targets reactive systems where DNNs are used as controllers. Understanding the formalization of reactive systems, including states, actions, and transition relations, is essential for encoding the system constraints in the verification queries.
  - Quick check question: How are reactive systems formally defined, and what are the key components that need to be considered when encoding the system constraints for verification?

- Concept: Abductive explanations and contrastive examples
  - Why needed here: The approach aims to generate abductive explanations and contrastive examples for multi-step executions of reactive systems. Understanding the definitions and properties of these explanations is crucial for implementing the enumeration and validation methods.
  - Quick check question: What are the formal definitions of abductive explanations and contrastive examples for multi-step executions, and how do they differ from the single-step case?

## Architecture Onboarding

- Component map: DNN verification backend (e.g., Marabou) -> Reactive system formalization module -> Explanation enumeration module (minimal and minimum) -> Contrastive example enumeration module -> MHS calculation module (e.g., RC-2) -> Evaluation and comparison module

- Critical path:
  1. Formalize the reactive system and encode the transition constraints.
  2. Generate a multi-step execution of the reactive system.
  3. For minimal explanations:
     - Initialize the explanation set with all features.
     - Iteratively remove features and check validity using simplified verification queries.
  4. For minimum explanations:
     - Enumerate all contrastive examples for each step.
     - Classify contrastive examples as independent, dependent, or spurious using reverse incremental enumeration.
     - Calculate the MHS of all contrastive examples.
  5. Validate and compare the generated explanations with heuristic methods.

- Design tradeoffs:
  - Tradeoff between explanation size and computation time: Smaller explanations are more meaningful but may require more computation to find.
  - Tradeoff between guaranteed minimality and runtime: Methods that guarantee minimality (e.g., incremental enumeration) may be slower than heuristic methods.
  - Tradeoff between expressiveness and tractability: More expressive reactive system formalizations may lead to more complex verification queries and longer computation times.

- Failure signatures:
  - Verification queries taking too long or timing out: May indicate that the transition constraints are too complex or the search space is too large.
  - Generated explanations not being minimal or minimum: May indicate issues with the enumeration or validation methods.
  - Heuristic methods outperforming the formal methods: May indicate that the formal methods are not well-suited for the specific reactive system or DNN architecture.

- First 3 experiments:
  1. Implement the minimal explanation enumeration method for a simple reactive system with a small DNN and few steps. Compare the runtime and explanation size with a baseline heuristic method.
  2. Extend the implementation to handle minimum explanations using contrastive example enumeration. Evaluate the performance on a slightly more complex reactive system with a larger DNN and more steps.
  3. Apply the methods to a real-world reactive system, such as a robotic navigation system, and assess the quality and reliability of the generated explanations compared to heuristic methods.

## Open Questions the Paper Calls Out

The paper does not explicitly call out any open questions.

## Limitations

- The effectiveness of the approach heavily depends on the quality of transition constraints encoding and the tractability of DNN verification queries.
- The methods may degrade in performance on systems with more complex state spaces or longer execution traces.
- The reliance on NP-complete DNN verification means that scalability remains a concern for larger networks or longer horizons.

## Confidence

- Mechanism 1 (Transition Constraint Exploitation): **High** - Well-supported by theoretical analysis and empirical results
- Mechanism 2 (Iterative Feature Removal): **Medium** - Relies on Lemma 1, which may not hold for all reactive systems
- Mechanism 3 (Contrastive Example Enumeration): **Medium** - The duality assumption needs further validation across diverse DNN architectures

## Next Checks

1. Test the methods on reactive systems with non-linear transition dynamics to evaluate the robustness of the transition constraint encoding approach.
2. Implement a systematic ablation study to quantify the contribution of each mechanism to the overall performance.
3. Apply the methods to a real-world reactive system with human-in-the-loop evaluation to assess the practical utility of the generated explanations.