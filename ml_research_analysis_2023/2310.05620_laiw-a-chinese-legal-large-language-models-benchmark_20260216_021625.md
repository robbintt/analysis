---
ver: rpa2
title: 'LAiW: A Chinese Legal Large Language Models Benchmark'
arxiv_id: '2310.05620'
source_url: https://arxiv.org/abs/2310.05620
tags:
- legal
- llms
- tasks
- capability
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors introduce LAiW, the first Chinese legal LLMs benchmark,
  designed to evaluate models across three levels of legal capability: basic legal
  NLP, basic legal application, and complex legal application. Each level contains
  specific tasks to ensure a comprehensive evaluation.'
---

# LAiW: A Chinese Legal Large Language Models Benchmark

## Quick Facts
- arXiv ID: 2310.05620
- Source URL: https://arxiv.org/abs/2310.05620
- Reference count: 34
- Key outcome: First Chinese legal LLM benchmark with three capability levels shows performance gaps between legal LLMs and ChatGPT, especially in basic tasks

## Executive Summary
LAiW introduces the first comprehensive Chinese legal large language models benchmark designed to evaluate models across three progressive capability levels: basic legal NLP, basic legal application, and complex legal application. The benchmark combines automated evaluation metrics with human expert validation to assess model performance on real-world legal tasks. While legal LLMs show improvements over their general counterparts, they still lag behind ChatGPT in basic legal tasks, indicating the need for enhanced legal reasoning capabilities. The benchmark is publicly available and will expand to include more tasks and data.

## Method Summary
The LAiW benchmark evaluates Chinese legal LLMs through three capability levels, each containing specific tasks that progressively increase in complexity. The benchmark integrates existing legal datasets from CAIL competitions and constructs new evaluation data, using automated metrics (accuracy, F1, MCC, ROUGE scores) alongside human expert evaluation. Five tasks are defined for stage 1, with 1000 samples per task. Models are evaluated using one-shot prompting approaches, with both legal-specific LLMs and general LLMs tested to establish performance baselines.

## Key Results
- Legal LLMs show improved performance over their general counterparts but still underperform ChatGPT, particularly in basic legal tasks
- Automated metrics and human expert evaluations reveal that current LLMs require reinforcement of legal logic despite strong technical performance
- The three-level capability hierarchy effectively stratifies model performance, with clear differentiation between basic and complex task handling
- Legal LLMs demonstrate varying degrees of success across different legal domains, with some tasks showing larger performance gaps than others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The three-level capability hierarchy (basic legal NLP → basic legal application → complex legal application) mirrors the logical progression of legal reasoning and enables systematic model evaluation.
- Mechanism: By structuring tasks from foundational information retrieval and NLP tasks up to complex legal reasoning, the benchmark ensures that models must demonstrate competency at each lower level before being assessed on higher-order capabilities. This hierarchical design reflects the syllogistic reasoning structure inherent in legal practice.
- Core assumption: Legal reasoning follows a predictable, sequential skill development path where complex capabilities depend on mastery of simpler ones.
- Evidence anchors:
  - [abstract] "we divide the legal capabilities of LLMs from easy to difficult into three levels: basic information retrieval, legal foundation inference, and complex legal application"
  - [section] "The capability of basic legal application focuses on LLMs' performance in simple application tasks within the field of legal domain... The capability of complex legal applications focuses on LLMs' performance in complex tasks within the legal domain"
  - [corpus] Weak - no direct corpus evidence for hierarchical reasoning structure, though related work suggests multi-level evaluation is emerging in legal LLM research
- Break condition: If models demonstrate strong performance on complex tasks without corresponding strength in basic tasks, the hierarchical assumption breaks down, suggesting legal reasoning may not follow the expected sequential pattern.

### Mechanism 2
- Claim: Including both automated metrics and human expert evaluation provides a more reliable assessment of legal LLM capabilities than either method alone.
- Mechanism: Automated metrics (accuracy, F1, ROUGE scores) provide quantitative benchmarks for basic tasks, while human evaluation with legal experts validates whether models align with actual legal practice and reasoning. This dual approach addresses the gap between technical performance and practical legal utility.
- Core assumption: Human legal experts can identify nuances in legal reasoning that automated metrics miss, particularly regarding logical consistency and alignment with legal practice.
- Evidence anchors:
  - [abstract] "To further confirm the complex legal application capabilities of current LLMs in legal application scenarios, we also incorporate human evaluation with legal experts"
  - [section] "The results indicate that while LLMs may demonstrate strong performance, they still require reinforcement of legal logic"
  - [corpus] Moderate - corpus shows other legal LLM benchmarks also combine automated and human evaluation, suggesting this is an emerging best practice
- Break condition: If human expert evaluations consistently contradict automated metrics without clear explanation, the reliability of either evaluation method comes into question.

### Mechanism 3
- Claim: Using domain-specific data sources (CAIL competitions, specialized legal datasets) ensures the benchmark reflects real-world legal practice rather than general language understanding.
- Mechanism: The benchmark draws from established legal AI competition datasets and constructs new evaluation data specifically for legal tasks, ensuring relevance to actual legal practice. This domain specificity distinguishes it from general LLM benchmarks.
- Core assumption: Legal tasks have unique characteristics that general NLP datasets cannot adequately capture, requiring specialized evaluation data.
- Evidence anchors:
  - [section] "we integrate and construct the Legal Evaluation Dataset (LED) based on existing open-source data... The main sources of this data are the CAIL competition data from previous years"
  - [section] "When constructing the Legal Evaluation Dataset (LED), we design different prompts for various tasks to support LLM in providing better answers"
  - [corpus] Moderate - corpus shows other legal LLM work also emphasizes domain-specific data, but comparative analysis of general vs. legal datasets is limited
- Break condition: If models trained on general data perform comparably to legal-specific models on these domain-specific tasks, the necessity of specialized legal data is undermined.

## Foundational Learning

- Concept: Syllogistic reasoning in law
  - Why needed here: The benchmark is explicitly designed around legal syllogism ("To align with the thinking process of legal experts and legal practice (syllogism)"), requiring understanding of how legal conclusions follow from premises
  - Quick check question: Can you explain how the major premise, minor premise, and conclusion structure applies to a simple criminal law scenario?

- Concept: Legal element recognition
  - Why needed here: Element Recognition is a core task in the benchmark ("Element Recognition analyzes and assesses each sentence to identify the pivotal elements of the case"), requiring understanding of what constitutes legally relevant facts
  - Quick check question: What distinguishes legally relevant elements from mere factual details in a case description?

- Concept: Multi-task evaluation design
  - Why needed here: The benchmark combines classification, summarization, and generation tasks across multiple legal domains, requiring understanding of how different evaluation metrics apply to different task types
  - Quick check question: Why would you use different metrics (accuracy/F1 vs. ROUGE scores) for classification tasks versus text generation tasks in this benchmark?

## Architecture Onboarding

- Component map: Task definition layer -> Dataset construction layer -> Evaluation execution layer. The benchmark consists of three main components: (1) Task definition layer (organizing legal tasks into three capability levels), (2) Dataset construction layer (integrating existing legal datasets and creating new evaluation data), and (3) Evaluation execution layer (automated metrics plus human expert review). Each component requires collaboration between legal domain experts and AI/ML engineers.

- Critical path: Data preparation -> Task prompt design -> Automated evaluation -> Human expert validation -> Results analysis. The most time-consuming step is typically dataset construction and quality assurance, as legal data requires careful anonymization and expert review.

- Design tradeoffs: The benchmark prioritizes comprehensiveness over speed (evaluating many tasks across three levels rather than focusing on a few high-impact tasks), and domain specificity over generalizability (using legal-specific datasets rather than adapting general NLP benchmarks). This increases evaluation fidelity but requires more resources.

- Failure signatures: Common failure modes include: (1) Poor model performance on basic tasks despite strong complex task results (suggesting models are memorizing patterns rather than learning legal reasoning), (2) High variance in human evaluations (indicating unclear task definitions or subjective interpretation), (3) Data leakage between training and evaluation sets (compromising benchmark validity).

- First 3 experiments:
  1. Run a single legal LLM through all three capability levels on a small subset of tasks to identify which levels show the largest performance gaps
  2. Compare automated metric results with human expert evaluations on the same sample to quantify correlation and identify systematic discrepancies
  3. Test whether general LLMs with legal fine-tuning perform better than legal LLMs without strong baseline performance, to validate the benchmark's sensitivity to domain adaptation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the performance gap between legal LLMs and ChatGPT be attributed to differences in legal knowledge coverage or reasoning capabilities?
- Basis in paper: [explicit] The authors note that while some legal LLMs outperform their backbones, there remains a performance gap compared to ChatGPT, especially in basic tasks.
- Why unresolved: The paper does not provide a detailed analysis of the specific aspects where legal LLMs fall short compared to ChatGPT.
- What evidence would resolve it: A detailed comparative analysis of legal knowledge coverage and reasoning capabilities between legal LLMs and ChatGPT.

### Open Question 2
- Question: How does the alignment of LLMs with legal practice logic impact their practical applicability in real-world legal scenarios?
- Basis in paper: [explicit] The authors mention that the current evaluations of LLMs in LegalAI are defined by computer science experts, lacking consistency with legal practice logic.
- Why unresolved: The paper does not provide empirical evidence on how the misalignment affects practical legal applications.
- What evidence would resolve it: Case studies or real-world applications demonstrating the impact of logical alignment on LLM performance in legal tasks.

### Open Question 3
- Question: What are the key factors that contribute to the variability in performance among different Chinese legal LLMs?
- Basis in paper: [inferred] The authors observe that not all legal LLMs perform well or bring significant improvements over their backbones, suggesting variability in performance.
- Why unresolved: The paper does not delve into the specific factors contributing to the differences in LLM performance.
- What evidence would resolve it: An analysis of the training data, model architectures, and fine-tuning strategies used by different Chinese legal LLMs.

## Limitations

- Benchmark currently limited to four specific tasks in stage 1 with only 1000 samples per task, potentially missing broader legal practice complexity
- Three-level capability hierarchy lacks empirical validation that legal reasoning truly follows this sequential progression
- Human expert evaluation relies on a small number of legal professionals whose assessments may not represent full diversity of legal practice

## Confidence

- High confidence: Systematic task organization and dual evaluation approach (automated + human)
- Medium confidence: Representativeness of chosen tasks and datasets for Chinese legal practice
- Low confidence: Generalizability of three-level capability hierarchy to all legal reasoning scenarios

## Next Checks

1. Conduct correlation analysis between automated metric performance and human expert evaluation scores across all tasks to quantify the relationship between technical and practical legal reasoning capabilities
2. Test whether models that perform well on basic tasks consistently show strong performance on complex tasks, validating the hierarchical capability assumption
3. Expand evaluation to include more diverse legal domains and task types, particularly civil law scenarios, to assess the benchmark's generalizability beyond criminal law-focused tasks