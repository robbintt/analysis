---
ver: rpa2
title: Exploring Prompting Large Language Models as Explainable Metrics
arxiv_id: '2311.11552'
source_url: https://arxiv.org/abs/2311.11552
tags:
- score
- summary
- summarization
- main
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates using large language models (LLMs) as explainable
  evaluation metrics for text summarization. The authors propose zero-shot and few-shot
  prompting strategies, where LLMs score summaries based on criteria such as capturing
  main ideas, coherence, relevance, and readability.
---

# Exploring Prompting Large Language Models as Explainable Metrics

## Quick Facts
- **arXiv ID:** 2311.11552
- **Source URL:** https://arxiv.org/abs/2311.11552
- **Reference count:** 15
- **Primary result:** Zero-shot LLM prompts achieve 0.477 Kendall correlation with human evaluations for summarization scoring

## Executive Summary
This paper investigates using large language models as explainable evaluation metrics for text summarization tasks. The authors propose and test zero-shot and few-shot prompting strategies where LLMs score summaries based on explicit criteria including capturing main ideas, coherence, relevance, and readability. Using the orca_mini_v3_7b model and SummEval dataset, the study finds that zero-shot prompting outperforms few-shot approaches, with the best prompt achieving 0.477 Kendall correlation with human evaluations. The results demonstrate that LLMs can serve as interpretable metrics for summarization evaluation, though performance remains modest compared to human judgment.

## Method Summary
The authors use the pre-trained orca_mini_v3_7b model to evaluate summaries through six different prompt variants - five zero-shot and one few-shot. The prompts explicitly define evaluation criteria such as capturing main ideas, factual correctness, coherence, and readability. The system takes source text and summary as inputs, applies the prompt template, and extracts a numerical score from the LLM's output. Evaluation uses the SummEval dataset with human ratings across four dimensions (fluency, coherence, consistency, relevance). Performance is measured through Kendall, Pearson, and Spearman correlations between LLM-generated scores and human evaluations, with the primary focus on Kendall correlation.

## Key Results
- Best zero-shot prompt achieves 0.477 Kendall correlation with human evaluation scores
- Zero-shot prompting outperforms few-shot prompting (P6 achieves lowest Kendall score)
- Prompts P2, P3, and P4 show similar performance, indicating LLMs are sensitive to wording but not dramatically so
- Explicit definition of evaluation criteria is crucial for LLM-based evaluation performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit definition of evaluation criteria improves LLM-based summarization scoring performance
- **Mechanism:** When evaluation criteria are explicitly stated in the prompt, the LLM can systematically evaluate each criterion rather than relying on implicit understanding, leading to more consistent and interpretable scores
- **Core assumption:** LLMs can reliably parse and apply explicit evaluation criteria when provided in a structured format
- **Evidence anchors:**
  - [abstract]: "The performance of our best provided prompts achieved a Kendall correlation of 0.477 with human evaluations in the text summarization task on the test data"
  - [section]: "Based on the obtained results, it can be inferred that by explicitly defining evaluation metrics, language models can be utilized as an interpretable method for evaluating the summarization task"
  - [corpus]: Weak evidence - no direct citations about explicit criteria in LLM evaluation metrics
- **Break condition:** If the LLM fails to parse or apply the criteria consistently across different prompt formulations, the performance advantage disappears

### Mechanism 2
- **Claim:** Zero-shot prompting outperforms few-shot prompting for LLM-based summarization evaluation
- **Mechanism:** Zero-shot prompting with well-crafted criteria allows the LLM to apply its pre-existing knowledge of evaluation standards directly, while few-shot examples may constrain or bias the evaluation toward the specific examples shown
- **Core assumption:** The pre-trained knowledge of the LLM about summarization quality is sufficient and generalizable without needing few-shot examples
- **Evidence anchors:**
  - [abstract]: "The best-performing zero-shot prompt achieves a Kendall correlation of 0.477 with human evaluation scores on test data, outperforming few-shot approaches"
  - [section]: "Contrary to our expectations, P6 (few-shot approach) obtains the lowest score in the Kendall measure"
  - [corpus]: Weak evidence - the related papers don't provide comparative analysis of zero-shot vs few-shot for evaluation metrics
- **Break condition:** If the LLM's pre-existing knowledge about summarization evaluation is inadequate or biased, few-shot examples would become necessary to guide performance

### Mechanism 3
- **Claim:** LLMs are sensitive to prompt wording but not dramatically so
- **Mechanism:** Small changes in prompt wording produce similar performance because the LLM extracts the core evaluation intent regardless of phrasing, but the wording must still capture the essential criteria clearly
- **Core assumption:** The LLM's understanding of summarization quality is robust to minor variations in how criteria are expressed
- **Evidence anchors:**
  - [section]: "The results of P2, P3, and P4 are very close to each other. The reason for the difference observed is the variation in the way the evaluation method is expressed. In this regard, it can be said that LLMs are sensitive to manner of expression"
  - [abstract]: No direct mention of prompt sensitivity
  - [corpus]: Weak evidence - no corpus data on prompt wording sensitivity for evaluation tasks
- **Break condition:** If minor wording changes significantly alter the LLM's interpretation of evaluation criteria, prompting would become a fragile and unpredictable process

## Foundational Learning

- **Concept:** Kendall correlation coefficient
  - **Why needed here:** The paper uses Kendall's Tau as the primary evaluation metric to compare LLM-generated scores with human evaluations
  - **Quick check question:** If two summaries are ranked 1st and 2nd by humans but the LLM ranks them 2nd and 1st, what would be the Kendall correlation between these two rankings?

- **Concept:** Zero-shot vs few-shot prompting
  - **Why needed here:** The paper compares these two prompting strategies to determine which is more effective for LLM-based evaluation
  - **Quick check question:** In zero-shot prompting, what is the primary difference between providing explicit evaluation criteria versus providing few-shot examples?

- **Concept:** Explainable AI metrics
  - **Why needed here:** The paper focuses on "explainable" evaluation metrics, meaning the LLM should provide reasoning for its scores
  - **Quick check question:** Why might an explainable metric be more valuable than a black-box metric in the context of evaluating summarization quality?

## Architecture Onboarding

- **Component map:** Source text → Summary → Prompt template → LLM model → Score extraction → Correlation calculation with human scores
- **Critical path:** Prompt generation → LLM inference → Score extraction → Correlation calculation with human scores
- **Design tradeoffs:** Zero-shot prompts require careful criterion definition but avoid the need for example pairs; few-shot prompts might provide clearer guidance but require finding good examples and add prompt length
- **Failure signatures:** Low correlation with human scores despite correct prompt format suggests either inadequate evaluation criteria definition or LLM limitations for this task
- **First 3 experiments:**
  1. Run P1 with a small subset of data to verify the correlation calculation pipeline works
  2. Test P2, P3, and P4 with the same data to establish baseline sensitivity to wording
  3. Run P6 (few-shot) with the same data to compare against zero-shot performance

## Open Questions the Paper Calls Out
The paper acknowledges that the lack of fine-tuning may affect performance and suggests exploring the impact of fine-tuning the model on summarization evaluation tasks in future research.

## Limitations
- Modest correlation (0.477 Kendall) indicates significant room for improvement compared to human judgment
- Results based on single model (orca_mini_v3_7b) may not generalize across different architectures
- Test data limited to specific time window (post-July 2023 Wikipedia articles) may limit generalizability

## Confidence
- **High Confidence:** Zero-shot prompting outperforms few-shot prompting is supported by direct experimental evidence
- **Medium Confidence:** Explicit evaluation criteria improve performance is well-supported by correlation results
- **Medium Confidence:** LLMs are sensitive to prompt wording but not dramatically so is supported by similar performance of P2, P3, and P4

## Next Checks
1. **Cross-model validation:** Test the same prompt set on at least two additional LLM architectures to determine if performance patterns are model-specific
2. **Prompt ablation study:** Systematically remove individual evaluation criteria from the best-performing prompt to quantify marginal contributions
3. **Temporal generalization test:** Evaluate prompts on summarization datasets from different time periods and domains to assess correlation pattern stability