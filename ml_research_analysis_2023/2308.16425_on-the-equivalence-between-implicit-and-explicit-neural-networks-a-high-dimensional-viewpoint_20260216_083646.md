---
ver: rpa2
title: 'On the Equivalence between Implicit and Explicit Neural Networks: A High-dimensional
  Viewpoint'
arxiv_id: '2308.16425'
source_url: https://arxiv.org/abs/2308.16425
tags:
- implicit
- explicit
- neural
- matrix
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between implicit and explicit
  neural networks from a high-dimensional perspective. The authors derive asymptotic
  spectral equivalents for the conjugate kernels (CKs) and neural tangent kernels
  (NTKs) of implicit networks, using results from random matrix theory.
---

# On the Equivalence between Implicit and Explicit Neural Networks: A High-dimensional Viewpoint

## Quick Facts
- arXiv ID: 2308.16425
- Source URL: https://arxiv.org/abs/2308.16425
- Reference count: 11
- Key outcome: Establishes equivalence between implicit and explicit neural networks by deriving asymptotic spectral equivalents for their conjugate kernels (CKs) and neural tangent kernels (NTKs) using random matrix theory

## Executive Summary
This paper investigates the relationship between implicit and explicit neural networks from a high-dimensional perspective. The authors derive asymptotic spectral equivalents for the conjugate kernels (CKs) and neural tangent kernels (NTKs) of implicit networks using results from random matrix theory. They prove that the CK and NTK matrices of implicit networks can be consistently approximated in operator norm by structured matrices involving data-dependent terms and isotropic components. By matching the coefficients of these asymptotic equivalents, the authors establish equivalence between implicit networks and single-layer explicit networks with carefully designed activation functions. A concrete example shows that a single-layer explicit network with a quadratic activation can match the CK or NTK eigenspectra of a ReLU implicit network. This equivalence is verified numerically for data sampled from high-dimensional unit spheres.

## Method Summary
The authors use random matrix theory to analyze the spectral properties of conjugate kernels (CKs) and neural tangent kernels (NTKs) for implicit neural networks. They derive asymptotic spectral equivalents showing that these kernel matrices can be approximated by structured forms involving data-dependent terms and isotropic components. The equivalence between implicit and explicit networks is established by matching the coefficients of these asymptotic equivalents. The approach involves analyzing the fixed point solutions of implicit networks, reformulating them as infinite-depth weight-shared explicit networks with input-injection, and comparing their kernel spectra under high-dimensional scaling where both input dimension and sample size grow large while maintaining a constant ratio.

## Key Results
- Asymptotic spectral equivalents are derived for CK and NTK matrices of implicit networks using random matrix theory
- Implicit and explicit neural networks with carefully matched activation functions have equivalent CK and NTK eigenspectra in high dimensions
- A single-layer explicit network with quadratic activation can match the CK eigenspectra of a ReLU implicit network
- The equivalence is verified numerically on data sampled from high-dimensional unit spheres

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Implicit and explicit neural networks can have equivalent CK and NTK eigenspectra in high dimensions
- Mechanism: By using random matrix theory, the authors derive asymptotic spectral equivalents for CK and NTK matrices of implicit networks, showing these can be approximated by structured matrices with data-dependent terms and isotropic components. Matching the coefficients of these asymptotic equivalents establishes equivalence between implicit networks and single-layer explicit networks with carefully designed activation functions.
- Core assumption: High-dimensional regime where d/n → c ∈ (0, ∞) and data points are uniformly sampled from unit spheres
- Evidence anchors:
  - [abstract] "we derive, with recent advances in random matrix theory, high-dimensional (spectral) equivalents for the CKs and NTKs of implicit NNs, and establish the equivalence between implicit and explicit NNs by matching the coefficients of the corresponding asymptotic spectral equivalents"
  - [section] "Theorem 1 (Asymptotic approximation of Implicit-CKs) Let Assumptions 1 hold. As n, d → ∞, the Implicit-CK matrix G* defined in Eq. (6) can be approximated consistently in operator norm, by the matrix G, that is ∥G* − G∥2 → 0, where G = α11⊤ + βX ⊤X + μIn"
  - [corpus] Weak evidence - corpus neighbors don't directly address high-dimensional equivalence between implicit and explicit networks
- Break condition: If the high-dimensional regime assumption is violated (d/n not in (0, ∞) or non-uniform data distribution), or if the activation functions don't satisfy the smoothness conditions

### Mechanism 2
- Claim: Single-layer explicit networks with quadratic activations can match the CK eigenspectra of ReLU implicit networks
- Mechanism: By matching the coefficients α1 = α, β1 = β, μ1 = μ of the asymptotic equivalents, a specific quadratic activation function can be constructed that produces the same CK eigenspectra as a ReLU implicit network
- Core assumption: The ReLU activation of the implicit network is normalized and the explicit network uses a quadratic polynomial activation
- Evidence anchors:
  - [abstract] "Surprisingly, our results reveal that a single-layer explicit NN with carefully designed activations has the same CK or NTK eigenspectra as a ReLU implicit NN, whose depth is essentially infinite"
  - [section] "Corollary 1 We consider a quadratic polynomial activation σ(t) = a2t2 + a1t + a0. Let Assumptions 1 hold. As n, d → ∞, the Implicit-CK matrix G* defined in Eq. (6) can be approximated consistently in operator norm, by the Explicit-CK matrixΣ defined in Eq. (5), i.e.,∥G* −Σ∥2 → 0, as long as a2 = ±√(μ/2), a1 = ±√(β), a0 = ±√(α − μ/d − a2)"
  - [corpus] No direct evidence in corpus - this specific equivalence result appears novel
- Break condition: If the quadratic activation cannot satisfy the coefficient matching conditions, or if the ReLU implicit network uses a different normalization scheme

### Mechanism 3
- Claim: Implicit networks can be reformulated as infinite-depth weight-shared explicit networks with input-injection
- Mechanism: The implicit network's fixed point solution implicitly encodes the behavior of an infinite sequence of explicit network layers, allowing the same kernel properties to emerge
- Core assumption: The implicit network has a unique fixed point and the explicit network has weight sharing across layers
- Evidence anchors:
  - [section] "An implicit NN is equivalent to an infinite-depth weight-shared explicit NN with input-injection. Unlike explicit NNs, implicit NNs generate features by directly solving for the fixed point, rather than through layer-by-layer forward propagation"
  - [section] "implicit NNs have the remarkable advantage that gradients can be computed analytically only through the fixed point with implicit differentiation"
  - [corpus] Weak evidence - corpus neighbors discuss implicit networks but don't explicitly address this infinite-depth equivalence
- Break condition: If the implicit network's fixed point doesn't exist or isn't unique, or if the weight sharing assumption is violated

## Foundational Learning

- Concept: Random Matrix Theory and Asymptotic Analysis
  - Why needed here: The equivalence proof relies on showing that kernel matrices converge to structured forms in operator norm as dimensions grow, which requires understanding spectral properties of random matrices
  - Quick check question: What is the difference between the operator norm convergence and entrywise convergence of random matrices?

- Concept: Neural Tangent Kernel (NTK) and Conjugate Kernel (CK)
  - Why needed here: These kernels capture the training dynamics and generalization properties of neural networks, and the equivalence is established by showing their eigenspectra match
  - Quick check question: How do the NTK and CK differ in their definitions and what aspects of network behavior do they characterize?

- Concept: Implicit Differentiation and Fixed Point Equations
  - Why needed here: Implicit networks are defined by fixed point equations, and their gradients are computed via implicit differentiation, which is crucial for understanding their training dynamics
  - Quick check question: How does implicit differentiation differ from explicit backpropagation in computing gradients for implicit networks?

## Architecture Onboarding

- Component map: Data (uniform on unit sphere) -> Implicit network (DEQ with fixed point solver) -> Explicit network (single-layer with quadratic activation) -> Random matrix theory analysis -> Asymptotic equivalence
- Critical path: 1) Define implicit and explicit network architectures, 2) Derive CK and NTK for both, 3) Apply random matrix theory to obtain asymptotic equivalents, 4) Match coefficients to establish equivalence, 5) Verify numerically
- Design tradeoffs: Implicit networks offer constant memory usage but require solving fixed point equations, while explicit networks are simpler to implement but may need more memory for deep architectures
- Failure signatures: If coefficient matching conditions cannot be satisfied, if the high-dimensional regime assumptions are violated, or if numerical experiments don't verify the theoretical predictions
- First 3 experiments:
  1. Verify the asymptotic approximation theorem by computing the operator norm difference between actual and approximated CK matrices for varying dimensions
  2. Test coefficient matching by constructing explicit networks with quadratic activations and comparing their CK eigenspectra to ReLU implicit networks
  3. Validate the NTK equivalence by training both network types on synthetic data and comparing their learning dynamics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the equivalence results between implicit and explicit neural networks be extended to general data distributions beyond the unit sphere?
- Basis in paper: [inferred] The paper mentions that the results can potentially extend to more general distributions using techniques from [5, 7], but this is not fully explored.
- Why unresolved: The paper only considers data uniformly sampled from the unit sphere, and extending to other distributions would require additional theoretical work.
- What evidence would resolve it: Proving the asymptotic spectral equivalents for CKs and NTKs when data follows other distributions (e.g., Gaussian, uniform on hypercubes) would demonstrate the broader applicability of the equivalence.

### Open Question 2
- Question: How do the training dynamics of implicit and explicit neural networks compare when their kernels have equivalent eigenspectra?
- Basis in paper: [explicit] The paper establishes equivalence in eigenspectra but does not analyze the training dynamics.
- Why unresolved: While the kernels may be equivalent, the iterative nature of implicit networks and the layer-wise updates in explicit networks could lead to different convergence behaviors.
- What evidence would resolve it: Empirical and theoretical studies comparing the gradient descent trajectories and convergence rates of implicit and explicit networks with matched kernels would clarify this.

### Open Question 3
- Question: Can the equivalence between implicit and explicit networks be generalized to implicit networks with arbitrary activation functions?
- Basis in paper: [explicit] The paper notes that extending results to general activations is possible using techniques from [10] but is deferred to future work.
- Why unresolved: The current analysis relies on the homogeneity of ReLU, and other activations may introduce additional complexity in the kernel expressions.
- What evidence would resolve it: Deriving the high-dimensional spectral equivalents for implicit networks with non-homogeneous activations (e.g., sigmoid, tanh) and matching them with explicit networks would demonstrate the generality of the equivalence.

## Limitations
- The theoretical results rely on strong high-dimensional assumptions that may not hold in practical finite-dimensional settings
- The analysis is limited to specific data distributions (uniform on unit spheres) and may not generalize to real-world data
- The coefficient matching conditions for activation functions may be sensitive to numerical precision and optimization dynamics

## Confidence
- High confidence: The asymptotic analysis framework and operator norm convergence proofs are mathematically rigorous
- Medium confidence: The numerical verification on synthetic high-dimensional data supports the theoretical claims
- Low confidence: Generalization to non-uniform data distributions and finite-dimensional regimes not covered by the theory

## Next Checks
1. Test the equivalence on real-world datasets with varying dimensionalities and data distributions to assess robustness beyond the theoretical assumptions
2. Conduct ablation studies on the activation function coefficient matching by perturbing parameters to quantify sensitivity
3. Evaluate the practical impact by comparing training dynamics and generalization performance between matched implicit and explicit networks on benchmark tasks