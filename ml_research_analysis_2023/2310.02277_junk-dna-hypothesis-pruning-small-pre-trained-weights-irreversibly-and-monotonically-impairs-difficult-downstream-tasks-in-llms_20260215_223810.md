---
ver: rpa2
title: 'Junk DNA Hypothesis: Pruning Small Pre-Trained Weights Irreversibly and Monotonically
  Impairs "Difficult" Downstream Tasks in LLMs'
arxiv_id: '2310.02277'
source_url: https://arxiv.org/abs/2310.02277
tags:
- task
- weights
- transfer
- sparsity
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the "Junk DNA Hypothesis" which challenges
  the conventional belief that low-magnitude weights in pre-trained large language
  models (LLMs) are redundant and can be pruned without significant performance impact.
  The authors argue that small-magnitude weights encode crucial knowledge for solving
  difficult downstream tasks, and removing them leads to irreversible performance
  degradation.
---

# Junk DNA Hypothesis: Pruning Small Pre-Trained Weights Irreversibly and Monotonically Impairs "Difficult" Downstream Tasks in LLMs

## Quick Facts
- arXiv ID: 2310.02277
- Source URL: https://arxiv.org/abs/2310.02277
- Reference count: 36
- Primary result: Pruning small-magnitude pre-trained weights severely degrades performance on difficult downstream tasks

## Executive Summary
This paper challenges the conventional wisdom that small-magnitude weights in pre-trained language models are redundant and can be pruned without significant performance impact. The authors propose the "Junk DNA Hypothesis," arguing that these small weights encode crucial knowledge essential for solving difficult downstream tasks. Through extensive experiments across diverse model sizes, tasks, and datasets, they demonstrate that removing small-magnitude weights leads to irreversible performance degradation, particularly for challenging tasks with limited data or complex cross-task requirements.

## Method Summary
The paper evaluates the impact of pruning small-magnitude weights in pre-trained LLMs on downstream task performance. Using pre-trained models (RoBERTa-Large/Base, mBART, Vicuna-7B), the authors apply magnitude-based one-shot pruning at various sparsity levels (both unstructured and N:M structured), then fine-tune on downstream tasks including GLUE, multilingual translation, and QA. They systematically vary task difficulty through data adequacy, multi-domain learning, and cross-task complexity, measuring performance differences between dense and sparse models.

## Key Results
- Removing just 5% of small-magnitude weights on harder tasks leads to noticeable performance decline
- Fine-tuning with limited data can only achieve performance parity at maximum sparsity of 20%
- Pre-trained small-magnitude weights contain more than sufficient knowledge for easier tasks when frozen
- The "Junk DNA Hypothesis" holds across both unstructured and structured N:M pruning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small-magnitude weights encode task-specific knowledge essential for difficult downstream tasks
- Mechanism: These weights form a task-sensitive "Junk DNA" component that preserves performance-critical information during fine-tuning
- Core assumption: The magnitude of a weight correlates with its importance for downstream task performance
- Evidence anchors:
  - [abstract] "small-magnitude weights of pre-trained model weights encode vital knowledge essential for tackling difficult downstream tasks"
  - [section 3.2] "fine-tuning with limited data...can only achieve performance parity...at a maximum sparsity level of 20%"
  - [corpus] Weak correlation - neighbor papers focus on pruning efficiency rather than task-difficulty sensitivity
- Break condition: If magnitude-based pruning is replaced with task-specific importance metrics that outperform simple magnitude thresholds

### Mechanism 2
- Claim: Pre-trained weight values are more important than fine-tuning capacity for difficult tasks
- Mechanism: The pre-trained values contain irreplaceable knowledge that cannot be recovered through fine-tuning when pruned
- Core assumption: Knowledge is embedded in specific weight values rather than just in network capacity
- Evidence anchors:
  - [section 5] "removal of small-magnitude weights from pre-trained models results in significant performance degradation, even when we permit the pruned weights to regenerate during fine-tuning"
  - [section 5] "knowledge embedded in pre-trained small-magnitude weights is already more than sufficient" for easier tasks
  - [corpus] No direct evidence - pruning literature generally assumes weights are replaceable
- Break condition: If random weight initialization with sufficient capacity matches or exceeds performance of preserving pre-trained small weights

### Mechanism 3
- Claim: Task difficulty correlates with pruning fragility through data availability and human-LLM performance gaps
- Mechanism: More difficult tasks (fewer data, larger human-LLM gaps) have less redundancy and cannot tolerate pruning
- Core assumption: Task difficulty can be quantified by data adequacy and human performance gaps
- Evidence anchors:
  - [section 4] "we define task difficulty as the disparity in performance between humans and models, normalized by human performance"
  - [section 3.2] "removal of just 5% of weights on the harder task...leads to a noticeable decline in performance"
  - [corpus] Moderate evidence - neighbor paper on "Meta-Vote Pruning" suggests task-specific pruning strategies
- Break condition: If a universal pruning threshold works equally well across all task difficulties

## Foundational Learning

- Concept: Linear mode connectivity and loss landscape analysis
  - Why needed here: To understand why pruned models lose performance on difficult tasks
  - Quick check question: What does linear interpolation between dense and sparse models tell us about their solution basins?

- Concept: Magnitude-based pruning and structured sparsity (N:M)
  - Why needed here: Core pruning techniques used to test the Junk DNA hypothesis
  - Quick check question: How does N:M sparsity differ from unstructured sparsity in terms of weight patterns?

- Concept: Task difficulty quantification through data adequacy and human-LLM gaps
  - Why needed here: To systematically vary task difficulty and test the hypothesis
  - Quick check question: How would you measure task difficulty for a new task not covered in the paper?

## Architecture Onboarding

- Component map:
  Pre-trained models (RoBERTa, mBART, Vicuna) -> Pruning module -> Fine-tuning pipeline -> Evaluation on downstream tasks -> Data sampling module for varying task difficulty -> Loss landscape analysis component for basin preservation study

- Critical path:
  1. Load pre-trained model
  2. Apply magnitude-based pruning at specified sparsity level
  3. Fine-tune on downstream task
  4. Evaluate performance
  5. Compare against dense baseline

- Design tradeoffs:
  - One-shot vs iterative pruning (one-shot chosen for isolating small-weight effects)
  - Structured (N:M) vs unstructured sparsity (both tested for generality)
  - Freezing vs updating small weights (both tested to understand knowledge preservation)

- Failure signatures:
  - Performance drop correlated with task difficulty when pruning small weights
  - Layer collapse in unstructured pruning at high sparsity levels
  - Loss barrier increase when interpolating between dense and sparse models for hard tasks

- First 3 experiments:
  1. Vary training data percentage (10%, 25%, 50%, 70%, 100%) on GLUE tasks with unstructured pruning
  2. Test N:M sparsity (7:8, 6:8, 5:8, 4:8, 3:8, 2:8, 1:8) on multilingual translation
  3. Compare performance on easy vs hard tasks (QQP vs STS-B, QNLI vs RTE) with linear interpolation analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do small-magnitude weights in LLMs differ functionally from low-magnitude weights in smaller neural networks, and what underlying mechanisms drive this distinction?
- Basis in paper: [inferred] The paper focuses on large language models (LLMs) and introduces the "Junk DNA Hypothesis" to explain the role of small-magnitude weights in these models. However, it does not explicitly compare the functional differences of low-magnitude weights between LLMs and smaller neural networks.
- Why unresolved: The paper does not provide a comparative analysis of low-magnitude weights in LLMs versus smaller neural networks. It primarily focuses on the significance of these weights in LLMs and their impact on downstream tasks.
- What evidence would resolve it: Conducting experiments that directly compare the functional roles of low-magnitude weights in LLMs and smaller neural networks, and analyzing the underlying mechanisms that drive any observed differences, would provide insights into this question.

### Open Question 2
- Question: Can the "Junk DNA Hypothesis" be extended to other forms of model compression, such as quantization or knowledge distillation, and what are the implications for model performance?
- Basis in paper: [explicit] The paper mentions that the "Junk DNA Hypothesis" holds true when transitioning from unstructured to structured N:M pruning. However, it does not explore whether this hypothesis can be extended to other forms of model compression, such as quantization or knowledge distillation.
- Why unresolved: The paper primarily focuses on the impact of pruning small-magnitude weights on model performance and does not investigate the effects of other compression techniques.
- What evidence would resolve it: Conducting experiments that apply the "Junk DNA Hypothesis" to other compression techniques, such as quantization or knowledge distillation, and evaluating the resulting impact on model performance, would provide insights into this question.

### Open Question 3
- Question: What are the potential trade-offs between preserving small-magnitude weights for improved performance on difficult tasks and the computational efficiency gained through pruning?
- Basis in paper: [inferred] The paper highlights the importance of small-magnitude weights in encoding crucial knowledge for solving difficult downstream tasks. However, it does not explicitly discuss the trade-offs between preserving these weights for improved performance and the computational efficiency gained through pruning.
- Why unresolved: The paper focuses on the significance of small-magnitude weights but does not provide a comprehensive analysis of the trade-offs between performance and computational efficiency.
- What evidence would resolve it: Conducting experiments that quantify the trade-offs between preserving small-magnitude weights and the computational efficiency gained through pruning, and analyzing the impact on model performance, would provide insights into this question.

## Limitations

- The mechanism explaining why small-magnitude weights are irreplaceable remains partially speculative
- Task difficulty quantification may not capture all relevant dimensions of task complexity
- The "Junk DNA" metaphor and specific mechanism for why small weights encode crucial knowledge lack clear mechanistic explanation

## Confidence

**High confidence**: The empirical observation that pruning small-magnitude weights causes greater performance degradation on more difficult downstream tasks is well-supported by the experimental results across multiple model architectures, task types, and difficulty settings.

**Medium confidence**: The claim that pre-trained small-magnitude weights contain irreplaceable knowledge beyond just being free parameters to be learned during fine-tuning. While experiments show better performance when preserving these weights versus regenerating them, the evidence does not conclusively demonstrate that this knowledge is uniquely valuable versus simply being more efficiently learned from the pre-trained initialization.

**Low confidence**: The "Junk DNA" metaphor and the specific mechanism by which small weights encode task-critical knowledge. The paper provides correlational evidence and loss landscape analysis but lacks a clear mechanistic explanation for why small-magnitude weights specifically encode this crucial information.

## Next Checks

1. **Cross-architecture validation**: Test the Junk DNA hypothesis on transformer architectures beyond BERT-style models (e.g., GPT, T5) to determine if the phenomenon is architecture-specific or general to pre-trained LLMs.

2. **Knowledge isolation experiments**: Design ablation studies that isolate specific knowledge components by selectively pruning weights based on task-specific importance scores rather than magnitude, to determine if magnitude-based pruning captures the truly essential knowledge.

3. **Fine-tuning dynamics analysis**: Track the evolution of small-weight values during fine-tuning of pruned models to determine whether these weights are genuinely irreplaceable or if their importance stems from the fine-tuning dynamics being disrupted when they are removed.