---
ver: rpa2
title: Learning from Integral Losses in Physics Informed Neural Networks
arxiv_id: '2305.17387'
source_url: https://arxiv.org/abs/2305.17387
tags:
- target
- training
- problem
- delayed
- standard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses training physics-informed neural networks
  (PINNs) under partial integro-differential equations, where naive approximations
  of integrals with unbiased estimates lead to biased loss functions and solutions.
  The authors propose three solutions: deterministic sampling, double-sampling, and
  delayed target methods.'
---

# Learning from Integral Losses in Physics Informed Neural Networks

## Quick Facts
- arXiv ID: 2305.17387
- Source URL: https://arxiv.org/abs/2305.17387
- Authors: 
- Reference count: 40
- Primary result: Delayed target method achieves accurate PINN solutions with N=1 sample comparable to standard N=100 training

## Executive Summary
This paper addresses the challenge of training physics-informed neural networks (PINNs) under partial integro-differential equations where integrals must be approximated. The authors identify that naive Monte Carlo integration introduces variance that biases the loss function and leads to suboptimal solutions. They propose three solutions: deterministic sampling, double-sampling, and a novel delayed target method that combines Polyak averaging with target regularization. The delayed target method achieves accurate solutions with only N=1 sample per update, comparable to standard training with N=100 samples.

## Method Summary
The paper tackles the problem of integral losses in PINNs by identifying that standard Monte Carlo approximation introduces variance-dependent bias. Three approaches are proposed: deterministic sampling (fixes integration points to eliminate variance), double-sampling (uses two independent samples to guarantee unbiased gradients), and delayed target (maintains a slowly-updated target network for integral evaluation). The delayed target method is shown to be most effective, using polyak averaging to keep target network parameters close to the main network while regularizing to prevent divergence. This allows accurate integral approximation with minimal samples per update.

## Key Results
- Delayed target method with N=1 sample achieves comparable accuracy to standard training with N=100 samples
- Performance maintained in higher dimensions (tested up to 10D Poisson problems)
- All three methods (deterministic, double-sampling, delayed target) outperform naive small-sample training
- Implementation demonstrates effectiveness on Poisson with singular charges, Maxwell equations, and Smoluchowski coagulation problems

## Why This Works (Mechanism)

### Mechanism 1
Replacing integrals with unbiased estimates introduces excess variance that biases solutions toward overly smooth functions. The Monte Carlo approximation of the integral introduces a variance term proportional to 1/N in the loss function. This excess variance depends on the neural network parameters θ, so minimizing the biased loss inadvertently trades off accuracy against smoothness to reduce this variance.

### Mechanism 2
The delayed target method eliminates bias by using a fixed, independently evaluated target for the integral term during each update. By detaching the target network's output from the computational graph (polyak averaging), the gradient for updating θ does not flow through the integral approximation. This means the loss being minimized matches the true loss Lθ(x) even with few samples.

### Mechanism 3
Deterministic sampling removes variance by fixing the sample set, but introduces bias by changing the optimization objective. By sampling the integration points deterministically to cover the domain uniformly, the variance term VP({x′i}|x)[1/N Σ gθ(A(i)x)] becomes zero. However, the resulting loss function is no longer an unbiased estimate of the true loss Lθ(x), potentially leading to a different optimal solution.

## Foundational Learning

- Concept: Monte Carlo integration and its variance properties
  - Why needed here: The paper relies on understanding how Monte Carlo estimates introduce variance and how this variance affects gradient-based optimization when the integrand depends on trainable parameters
  - Quick check question: What is the variance of a Monte Carlo estimate of an integral with N samples, and how does it scale with N?

- Concept: Physics-informed neural networks and their loss functions
  - Why needed here: The work extends PINNs to handle integral terms in PDEs, so understanding the standard PINN framework and loss construction is essential
  - Quick check question: How is the loss function typically constructed in a PINN for a PDE without integral terms?

- Concept: Gradient flow and the effect of variance on optimization trajectories
  - Why needed here: The paper discusses how variance in the loss affects the optimization path and the quality of the learned solution, which requires understanding how gradients behave with noisy objectives
  - Quick check question: How does high variance in the loss function affect the convergence and final solution of stochastic gradient descent?

## Architecture Onboarding

- Component map: MLP (3 layers, 64 units each, tanh) -> Integral approximation -> Loss function -> Adam optimizer
- Critical path: Forward pass through main network → Sample N points → Forward pass through target network → Compute delayed target loss → Backpropagate through main network only → Update main network → Update target network via polyak averaging
- Design tradeoffs: Deterministic sampling eliminates variance but may not scale to high dimensions and changes the objective. Double-sampling guarantees unbiased gradients but requires two independent samples per update. Delayed targeting trades off some bias in the target approximation for reduced variance and better scalability
- Failure signatures: Divergence in training (loss increasing or oscillating), main and target networks drifting apart significantly, or the solution being overly smooth compared to the true solution
- First 3 experiments:
  1. Implement and train a PINN on the 2D Poisson problem with three charges using the standard approach with N=1 and N=100 samples per sphere to observe the bias
  2. Implement the delayed target method with N=1 and compare its performance and training stability to the standard method on the same Poisson problem
  3. Test the effect of the target regularization weight λ and the polyak averaging rate γ on the stability and performance of the delayed target method

## Open Questions the Paper Calls Out

### Open Question 1
What is the theoretical relationship between the regularization weight λ and the target weight M in the delayed target method, and how should they be optimally chosen together? The paper mentions that M must be set in conjunction with λ, particularly in challenging problems, but doesn't provide a systematic approach for choosing these hyperparameters together.

### Open Question 2
How does the delayed target method's performance scale with problem dimensionality beyond the 10-dimensional cases studied, and what are the fundamental limitations? The authors studied up to 10-dimensional Poisson problems and found the delayed target method maintained comparable quality, but acknowledge this may not represent the full scaling limits.

### Open Question 3
Can the delayed target method be extended to handle non-parametric stochasticity in the y term of the loss function, and what modifications would be needed? The authors note that non-parameterized stochasticity in the y term cannot offset optimal solutions, unlike the parameterized integral terms, but don't explore methods for handling such cases.

## Limitations
- Reliance on target network staying close enough to main network for accurate integral approximation
- Limited experimental validation to 1D and 2D problems, leaving uncertainty about performance in truly high-dimensional settings
- No comparison to other variance reduction techniques like control variates or importance sampling

## Confidence
- High Confidence: The observation that naive Monte Carlo integration with small sample sizes leads to biased solutions due to variance-dependent loss terms
- Medium Confidence: The delayed target method's ability to achieve comparable quality to large-sample training
- Low Confidence: Claims about the method's effectiveness in very high-dimensional problems (>10 dimensions) and its superiority over other variance reduction techniques

## Next Checks
1. **Target Network Drift Analysis**: Implement monitoring of the L2 distance between main and target network parameters during training to empirically verify that they remain close enough for accurate integral approximation
2. **High-Dimensional Scaling Test**: Extend the Poisson problem to 10+ dimensions with appropriate singular charge distributions and test whether the delayed target method with N=1 maintains quality compared to standard training with large N
3. **Comparison to Variance Reduction Alternatives**: Implement a control variate or importance sampling approach for the integral approximation and compare its performance and computational cost to the delayed target method on the same benchmark problems