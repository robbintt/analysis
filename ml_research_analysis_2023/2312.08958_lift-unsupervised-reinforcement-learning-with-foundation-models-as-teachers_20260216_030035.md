---
ver: rpa2
title: 'LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers'
arxiv_id: '2312.08958'
source_url: https://arxiv.org/abs/2312.08958
tags:
- grass
- learning
- task
- reward
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes LiFT, an unsupervised reinforcement learning
  framework that uses foundation models as teachers to guide an agent in acquiring
  semantically meaningful behaviors without human supervision. The core idea is to
  leverage large language models (LLMs) to propose task instructions grounded in the
  current environment, and vision-language models (VLMs) to provide reward feedback
  for completing those tasks.
---

# LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers

## Quick Facts
- arXiv ID: 2312.08958
- Source URL: https://arxiv.org/abs/2312.08958
- Reference count: 40
- Primary result: LiFT achieves 50% success rate on MineDojo tasks, outperforming baselines (19-16%) using foundation models as teachers

## Executive Summary
LiFT introduces an unsupervised reinforcement learning framework that leverages foundation models as teachers to guide an agent in acquiring semantically meaningful behaviors without human supervision. The framework uses a large language model (LLM) to propose task instructions grounded in the current environment state, and a vision-language model (VLM) to provide reward feedback based on task completion. The agent learns a multi-task language-conditioned policy in a training environment, which can then be adapted to new target tasks via zero-shot evaluation. LiFT demonstrates superior performance compared to prior unsupervised RL approaches in the challenging MineDojo environment, learning semantically meaningful behaviors rather than just diverse skills.

## Method Summary
LiFT combines foundation models with reinforcement learning to learn meaningful behaviors in an unsupervised setting. The agent uses a Vicuna-13B LLM to generate task instructions based on available objects in the environment, then learns a multi-task language-conditioned policy using PPO. A MineCLIP VLM provides reward feedback by computing alignment scores between observed trajectories and task instructions. The method includes reward stabilization through post-processing and policy initialization using a pre-trained VPT policy with adapter layers for fine-tuning. The framework is evaluated through zero-shot transfer to target tasks in the MineDojo environment.

## Key Results
- LiFT achieves 50% average success rate on MineDojo tasks, significantly outperforming APT (19%) and APT w/ MineCLIP (16%)
- Qualitative analysis shows LiFT learns semantically meaningful behaviors (milking cows, shearing sheep) while baselines learn diverse but not necessarily useful skills
- Reward post-processing is crucial for learning stability and success
- VPT policy initialization with adapter layers improves learning efficiency compared to random initialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM provides task proposals that are semantically meaningful and grounded in the current environment state
- Mechanism: The LLM receives a prompt containing object entities and available tools in the current environment, then generates task instructions that directly reference those elements
- Core assumption: The LLM has been pre-trained on diverse internet data that includes Minecraft gameplay knowledge
- Evidence anchors: [abstract] "the agent receives task instructions grounded in a training environment from large language models"; [section 3.1] "the agent initialized in a training environment asks an off-the-shelf LLM to generate useful task instructions given the description of the available objects"
- Break condition: If the LLM lacks Minecraft-specific knowledge in its training corpus, the generated tasks may be irrelevant or nonsensical to the environment

### Mechanism 2
- Claim: VLM provides dense, semantically aligned reward signals that correlate with task completion
- Mechanism: The VLM (MineCLIP) computes alignment scores between observed agent trajectories and task instructions using cosine similarity between video and text embeddings
- Core assumption: The VLM was pre-trained on video-text pairs where the alignment between videos and descriptions captures task-relevant semantics
- Evidence anchors: [abstract] "a vision-language model guides the agent in learning the multi-task language-conditioned policy by providing reward feedback"; [section 3.2] "we use a vision-language model (VLM) to give reward feedback to the agent for that task"
- Break condition: If the VLM's training data lacks diverse or correctly aligned video-text pairs, the reward signal may be noisy or misaligned with true task success

### Mechanism 3
- Claim: Reward post-processing stabilizes learning by reducing variance in VLM-derived rewards
- Mechanism: The post-processing applies a smoothing window and clipping to the raw VLM rewards, reducing the impact of visual noise and task-irrelevant fluctuations
- Core assumption: The raw VLM rewards exhibit high variance due to visual changes in observations that are not task-relevant
- Evidence anchors: [section 3.2] "we empirically observe that the computed reward signal typically exhibits high variance... Thus, we adapt the reward stabilization technique from DECKARD [25]"; [section 4.3] "The reward post-processing in Equation (2) is crucial for successful learning"
- Break condition: If the variance in rewards is not primarily due to visual noise but rather fundamental misalignment in the VLM's understanding, post-processing may not improve learning

## Foundational Learning

- Concept: Foundation Models as Knowledge Priors
  - Why needed here: Foundation models provide pre-trained semantic understanding that would be extremely expensive to acquire through RL alone in a complex environment like Minecraft
  - Quick check question: If we replaced the LLM and VLM with randomly initialized models, would the agent still learn semantically meaningful behaviors?

- Concept: Multi-task Language-Conditioned Policy
  - Why needed here: The policy must handle diverse task instructions and generalize to unseen tasks at evaluation time, requiring language as a conditioning variable
  - Quick check question: Can the policy generate the correct action sequence for a novel task instruction it hasn't seen during training?

- Concept: Zero-shot Adaptation
  - Why needed here: The ultimate goal is to evaluate the agent's ability to perform new tasks without further training, demonstrating the transfer of learned skills
  - Quick check question: If we give the policy a completely new task instruction, does it attempt relevant actions or random behavior?

## Architecture Onboarding

- Component map: LLM → Task Instruction Generator → Environment State Descriptor → VLM → Reward Signal → PPO Optimizer → Multi-task Policy (with adapter layers) → Environment Interaction
- Critical path: LLM task generation → VLM reward computation → PPO policy update. Any delay or failure in these components directly impacts learning progress.
- Design tradeoffs: Using pre-trained VPT policy initialization helps with exploration but may bias the agent toward behaviors seen in the pre-training data. The adapter layers allow fine-tuning while preserving the pre-trained representations.
- Failure signatures: Low success rates on evaluation tasks suggest either poor task instruction quality (LLM) or poor reward quality (VLM). Diverse but meaningless behaviors suggest the reward signal is not semantically aligned.
- First 3 experiments:
  1. Replace LLM with a fixed set of hand-crafted task instructions and measure impact on learning success
  2. Remove reward post-processing and observe learning stability and final performance
  3. Initialize policy randomly instead of with VPT and compare learning curves and final task success rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of rewards from vision-language models (VLMs) impact the learning performance of the LiFT framework, and what strategies can be employed to improve reward quality?
- Basis in paper: [explicit] The paper discusses observed challenges of using off-the-shelf VLMs for reward computation and suggests strategies to improve reward quality, such as policy initialization and reward post-processing
- Why unresolved: The paper highlights the limitations of current VLMs in providing precise feedback for task completion and suggests that improving VLM rewards for a diverse set of tasks, including complex and delicate tasks, is future work
- What evidence would resolve it: Conducting experiments with different VLM models, reward post-processing techniques, and policy initialization methods to quantify their impact on learning performance and identify the most effective strategies

### Open Question 2
- Question: Can the LiFT framework be extended to other domains beyond Minecraft, and how would the choice of foundation models (FMs) and task instructions affect the performance in different environments?
- Basis in paper: [inferred] The paper demonstrates the efficacy of the LiFT framework in the MineDojo environment but does not explore its applicability to other domains or the impact of different FMs and task instructions
- Why unresolved: The paper focuses on the MineDojo environment and does not provide insights into the generalizability of the LiFT framework to other domains or the impact of different FMs and task instructions
- What evidence would resolve it: Applying the LiFT framework to other domains, such as robotics or autonomous driving, and evaluating the performance with different FMs and task instructions to identify the most suitable combinations for each domain

### Open Question 3
- Question: How does the LiFT framework compare to other unsupervised reinforcement learning (RL) approaches in terms of sample efficiency and generalization to unseen tasks?
- Basis in paper: [explicit] The paper compares the LiFT framework to prior unsupervised RL approaches like APT and APT w/ MineCLIP in the MineDojo environment but does not provide a comprehensive comparison in terms of sample efficiency and generalization
- Why unresolved: The paper focuses on the performance of the LiFT framework in the MineDojo environment and does not provide a thorough comparison with other unsupervised RL approaches in terms of sample efficiency and generalization to unseen tasks
- What evidence would resolve it: Conducting experiments to compare the sample efficiency and generalization of the LiFT framework with other unsupervised RL approaches, such as diversity-based methods or model-based approaches, on a variety of tasks and environments

## Limitations
- The framework relies on off-the-shelf foundation models without fine-tuning, leading to suboptimal task proposals and reward signals
- The paper doesn't address the computational cost and API dependency of using large LLMs for task generation
- Reward quality from VLMs is insufficient for reliably detecting task completion moments, particularly for complex tasks

## Confidence
- High confidence: The core framework design (LLM + VLM + RL) is well-specified and the experimental setup is clearly described
- Medium confidence: The reported success rates are likely accurate given the controlled evaluation setup, but the absolute performance may vary with different foundation model versions
- Medium confidence: The qualitative analysis of learned behaviors is convincing, though subjective
- Low confidence: The scalability analysis and potential for domain adaptation are not thoroughly explored

## Next Checks
1. Implement a human evaluation of VLM-generated rewards by having annotators rate reward alignment with true task success on a sample of trajectories. Compare MineCLIP rewards with alternative VLMs or reward models.
2. Conduct an ablation study where the LLM is replaced with a curated set of high-quality task instructions. Measure the impact on learning efficiency and final task success rates to quantify the contribution of LLM-generated tasks.
3. Fine-tune the LLM and VLM on Minecraft-specific data (e.g., VPT trajectories with annotations) and evaluate whether this improves task proposal relevance and reward signal quality compared to the off-the-shelf models.