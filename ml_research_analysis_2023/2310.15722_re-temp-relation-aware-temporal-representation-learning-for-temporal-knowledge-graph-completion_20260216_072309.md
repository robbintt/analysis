---
ver: rpa2
title: 'Re-Temp: Relation-Aware Temporal Representation Learning for Temporal Knowledge
  Graph Completion'
arxiv_id: '2310.15722'
source_url: https://arxiv.org/abs/2310.15722
tags:
- information
- temporal
- graph
- embedding
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Re-Temp, a temporal knowledge graph completion
  model designed for extrapolation tasks. It incorporates explicit temporal embeddings
  and a relation-aware skip information flow mechanism to filter irrelevant historical
  snapshots based on the query.
---

# Re-Temp: Relation-Aware Temporal Representation Learning for Temporal Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2310.15722
- Source URL: https://arxiv.org/abs/2310.15722
- Reference count: 15
- Outperforms state-of-the-art models by 1.8%-10.1% MRR on temporal knowledge graph completion

## Executive Summary
Re-Temp is a temporal knowledge graph completion model designed for extrapolation tasks that significantly outperforms eight state-of-the-art models on six benchmark datasets. The model incorporates explicit temporal embeddings that decompose entity properties into static and dynamic components, along with a relation-aware skip information flow mechanism to filter irrelevant historical snapshots. A two-phase forward propagation method prevents information leakage between original and inverse query sets. The model achieves MRR improvements ranging from 1.8% to 10.1% across datasets, with ablation studies confirming the effectiveness of both the explicit temporal embedding and relation-aware skip flow mechanisms.

## Method Summary
Re-Temp uses explicit temporal embeddings that combine static (time-invariant) and dynamic (time-varying) components, where dynamic embeddings are further decomposed into trend and seasonal parts. The model employs sequential CompGCN encoders with relation-aware skip information flow that uses attention weights based on query-related relations to filter historical snapshots. A two-phase forward propagation method separates original and inverse quadruplets during training to prevent information leakage. The decoder uses ConvTransE scoring, and training employs Adam optimization with early stopping. The model shows sensitivity to history length, performing best with length 3 on most datasets while WIKI benefits from a shorter length of 1.

## Key Results
- Outperforms eight state-of-the-art models across six benchmark datasets
- Achieves MRR improvements of 1.8%-10.1% over existing methods
- Ablation studies confirm effectiveness of explicit temporal embedding and relation-aware skip flow
- History length sensitivity shows optimal performance at length 3 for most datasets, but length 1 for WIKI
- Performance validated using filtered MRR, Hits@1, Hits@3, and Hits@10 metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit temporal embeddings improve performance by encoding time-dependent entity properties
- Mechanism: Decomposes entity embeddings into static (time-invariant) and dynamic (time-varying) components, where dynamic component combines trend (linear time function) and seasonal (periodic) parts
- Core assumption: Entity properties evolve in predictable linear and periodic patterns over time
- Evidence anchors: [abstract] "leverages explicit temporal embedding as input"; [section] "the dynamic embedding is decomposed into the trend component and seasonal component"
- Break condition: When entity properties don't follow predictable linear/periodic patterns (e.g., abrupt regime changes)

### Mechanism 2
- Claim: Relation-aware skip information flow improves performance by filtering irrelevant historical snapshots
- Mechanism: Applies attention weights based on query-related relations to skip unnecessary historical information, using mean-pooled relation embeddings as reference vectors
- Core assumption: Query relations can determine which historical snapshots are relevant for prediction
- Evidence anchors: [abstract] "incorporates skip information flow after each timestamp to skip unnecessary information for prediction"; [section] "the relation associated with eq should be considered...mean pooling is applied on all relation embedding associated with eq"
- Break condition: When relevant historical information is incorrectly filtered out due to poor relation embedding alignment

### Mechanism 3
- Claim: Two-phase forward propagation prevents information leakage between original and inverse query sets
- Mechanism: Separates original and inverse quadruplets during forward propagation while using combined graph for training, preventing paired relations from leaking information
- Core assumption: Inverse queries can leak information through shared relation embeddings if not separated
- Evidence anchors: [abstract] "we introduce a two-phase forward propagation method to prevent information leakage"; [section] "we divide the dataset into two subsets: the original set and the inverse set"
- Break condition: When leakage prevention overly restricts information flow needed for accurate predictions

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) for temporal data
  - Why needed here: Encoder uses sequential GNNs to capture implicit temporal patterns across snapshots
  - Quick check question: How does a GNN handle temporal dependencies differently from a static GNN?

- Concept: Attention mechanisms for selective information filtering
  - Why needed here: Relation-aware skip information flow uses attention to weight historical snapshots
  - Quick check question: What's the difference between additive attention (used here) and dot-product attention?

- Concept: Temporal decomposition into trend and seasonal components
  - Why needed here: Dynamic embeddings use this decomposition to capture time-varying entity properties
  - Quick check question: Why might a periodic function be useful for modeling temporal entity changes?

## Architecture Onboarding

- Component map: Input → Sequential CompGCN layers → Relation-aware skip flow → Decoder scoring → Loss calculation
- Critical path: Input layer (static + dynamic temporal embeddings) → Encoder (Sequential CompGCN with relation-aware skip information flow) → Decoder (ConvTransE score function) → Loss calculation (classification-based with filtered negatives)
- Design tradeoffs:
  - History length vs. computational complexity (longer histories increase computation but may not improve performance)
  - Static vs. dynamic embedding weighting (affects how much temporal vs. time-invariant information is used)
  - Skip connection strength (too aggressive filtering may lose useful information)
- Failure signatures:
  - Performance degrades on datasets with irregular temporal patterns (skip mechanism over-filters)
  - Model becomes unstable with very long history lengths (over-smoothing in GNNs)
  - Inverse query leakage manifests as artificially high scores for true pairs
- First 3 experiments:
  1. Test performance with history length = 1 vs. 3 vs. 5 on ICEWS14 to identify optimal history length
  2. Remove dynamic embedding component to verify explicit temporal embedding contribution
  3. Remove relation-aware skip flow to measure impact of selective historical filtering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the explicit temporal embedding component affect model performance across different temporal knowledge graph datasets with varying time intervals?
- Basis in paper: [explicit] The paper states "The major difference between our explicit temporal representation and ATiSE lies in the fact that employing a learnable feed-forward layer to concatenate the dynamic embedding and static embedding, enables the model to determine the extent to which it should utilise information from each embedding rather than simply utilising both."
- Why unresolved: While the paper mentions ablation studies on explicit temporal embedding, it doesn't explore performance variations across datasets with different time intervals (e.g., daily vs. 15-minute intervals).
- What evidence would resolve it: Comparative experiments measuring model performance with and without explicit temporal embedding across datasets with varying time intervals, particularly focusing on GDELT (15-minute intervals) vs. ICEWS datasets (daily intervals).

### Open Question 2
- Question: What is the optimal history length for temporal knowledge graph completion models, and how does it vary based on dataset characteristics?
- Basis in paper: [explicit] The paper discusses "Impact of history length" and finds that different datasets perform best with different history lengths (e.g., WIKI performs best with length 1, while other datasets prefer length 3).
- Why unresolved: The paper only explores history lengths from 1 to 5 and doesn't provide a systematic approach to determine optimal history length for new datasets.
- What evidence would resolve it: A comprehensive study mapping dataset characteristics (e.g., temporal density, entity evolution rate) to optimal history lengths, along with a predictive model for determining history length for new datasets.

### Open Question 3
- Question: How does the relation-aware skip information flow mechanism compare to alternative methods for handling irrelevant historical snapshots?
- Basis in paper: [explicit] The paper introduces a relation-aware skip information flow mechanism but only compares it to two simplified versions in ablation studies (removing relation-aware attention and removing skip entirely).
- Why unresolved: The paper doesn't compare this mechanism to other potential approaches for handling irrelevant historical snapshots, such as attention mechanisms based on entity similarity or temporal distance.
- What evidence would resolve it: Comparative experiments testing the relation-aware skip flow against alternative methods for filtering historical snapshots, measuring both performance and computational efficiency.

## Limitations

- Several critical implementation details are underspecified, particularly around CompGCN composition functions and attention mechanism parameters
- Performance improvements (1.8%-10.1% MRR gains) lack ablation studies isolating each component's individual contribution
- Two-phase forward propagation method's necessity and effectiveness lack sufficient empirical validation
- Model shows varying performance sensitivity to history length across datasets without clear theoretical justification

## Confidence

- **High Confidence**: The explicit temporal embedding decomposition into static/dynamic components with trend and seasonal parts is well-grounded in time series literature and the mathematical formulation is clear
- **Medium Confidence**: The relation-aware skip information flow mechanism is innovative but relies on assumptions about relation embeddings adequately capturing temporal relevance that aren't empirically validated
- **Low Confidence**: The two-phase forward propagation method's necessity and effectiveness lack sufficient experimental validation, with no ablation showing performance degradation when removed

## Next Checks

1. **Ablation Study Validation**: Remove the relation-aware skip information flow component and measure performance degradation on ICEWS14 to quantify its actual contribution versus the claimed improvements

2. **History Length Sensitivity**: Systematically test history lengths from 1 to 5 on all six datasets to verify the claimed optimal settings and understand the performance tradeoffs

3. **Leakage Prevention Test**: Train the model without two-phase forward propagation on ICEWS18 and compare whether inverse query leakage actually occurs and affects performance metrics