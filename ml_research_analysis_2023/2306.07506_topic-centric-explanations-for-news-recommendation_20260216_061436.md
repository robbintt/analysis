---
ver: rpa2
title: Topic-Centric Explanations for News Recommendation
arxiv_id: '2306.07506'
source_url: https://arxiv.org/abs/2306.07506
tags:
- news
- topic
- recommendation
- user
- topics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a news recommender system that provides topic-centric
  explanations for recommendations. The core method involves using a Bi-level Attention-based
  Topical Model (BATM) to encode news articles and user profiles, extracting interpretable
  topics to explain recommendations.
---

# Topic-Centric Explanations for News Recommendation

## Quick Facts
- arXiv ID: 2306.07506
- Source URL: https://arxiv.org/abs/2306.07506
- Reference count: 40
- The paper introduces a news recommender system that provides topic-centric explanations using a Bi-level Attention-based Topical Model (BATM), achieving AUC scores of 67.79% and 69.67% on MIND-SMALL and MIND-LARGE datasets respectively.

## Executive Summary
This paper introduces a news recommender system that provides topic-centric explanations for recommendations using a Bi-level Attention-based Topical Model (BATM). The model encodes news articles and user profiles, extracting interpretable topics to explain recommendations through attention mechanisms that capture topic-term weights and document-topic distributions. The system is evaluated on the MIND dataset, demonstrating improved recommendation performance compared to baseline models. The extracted topics are assessed using coherence metrics, showing high interpretability, and a case study illustrates the model's ability to generate meaningful explanations by highlighting relevant topic descriptors.

## Method Summary
The paper proposes a news recommender system with topic-centric explanations using a Bi-level Attention-based Topical Model (BATM). The model processes news articles through an embedding layer and two attention layers to capture multiple topics and generate document representations. It employs a multiple-topic attention mechanism to identify K topics and an additive attention layer to create document-topic distributions. User modeling is performed using either additive attention or GRU networks. The system is trained with negative sampling and NCE loss, and topics are evaluated using NPMI and W2V coherence metrics. The extracted attention weights serve as explanations by highlighting topic descriptors in recommended articles.

## Key Results
- The BATM model achieves AUC scores of 67.79% and 69.67% on MIND-SMALL and MIND-LARGE datasets respectively
- Topic coherence metrics (NPMI and W2V) demonstrate high interpretability of extracted topics
- A case study shows the model can generate meaningful explanations by highlighting relevant topic descriptors in recommendations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Topic-centric explanations improve user trust by making recommendations transparent and interpretable
- Mechanism: The BATM model uses multi-topic attention to capture topic-term distributions and document-topic distributions, which are then mapped to highlight topic descriptors in recommended articles. These descriptors explain why a recommendation was made based on matching topics in user's history
- Core assumption: Users can understand and trust explanations when they are framed in terms of semantically meaningful topics rather than opaque embeddings
- Evidence anchors:
  - [abstract] "The proposed approach enhances user trust and system transparency by providing clear, topic-based explanations for news recommendations"
  - [section] "the extracted topics used to reveal the relatedness between the user's browsing history news and the ranked candidate news"
  - [corpus] Weak: Corpus shows related works focus on NRS explainability, but none explicitly validate trust gains via topic-centric explanations
- Break condition: If topic descriptors are not semantically coherent or too generic, users may not perceive them as meaningful explanations

### Mechanism 2
- Claim: Topic coherence metrics correlate with explanation quality, allowing quantitative assessment of interpretability
- Mechanism: NPMI and W2V scores are computed on the top-M words per topic to measure semantic coherence; higher scores indicate more interpretable topics, which in turn reflect better explanations
- Core assumption: Topic coherence scores are valid proxies for human interpretability of explanations
- Evidence anchors:
  - [abstract] "we propose to use quantitative topic coherence metrics [22, 29] to evaluate the quality of topics extracted by our model, which reflects the interpretability of these explanations"
  - [section] "We use this to compare the quality of topics from our trained model with those produced by a classical topic model LDA [4]"
  - [corpus] Weak: Corpus neighbors do not mention topic coherence evaluation in NRS explainability context
- Break condition: If coherence scores diverge from human judgment, the quantitative evaluation becomes unreliable

### Mechanism 3
- Claim: Attention weights extracted from the trained BATM-ATT model can directly serve as explanations without extra modeling overhead
- Mechanism: After training, the multiple-topic attention weights (A) and document-topic weights (B) are retained; these weights are used to highlight words and select topics as explanations, making the process efficient and faithful to the recommendation
- Core assumption: Attention weights in trained neural models are faithful and stable enough to serve as explanations
- Evidence anchors:
  - [abstract] "we extract attention weights from multiple attention networks during the news modeling and user modeling procedure to generate topic-aware explanations"
  - [section] "we analyze the attention weights extracted from the trained ðµð´ð‘‡ ð‘€-ð´ð‘‡ð‘‡ only to generate explanations because the ðµð´ð‘‡ ð‘€-ðºð‘…ð‘ˆ model is not fully explainable"
  - [corpus] Weak: Corpus does not discuss attention-based explanation faithfulness in news recommendation
- Break condition: If attention weights are noisy or unstable across runs, the explanations become unreliable

## Foundational Learning

- Concept: Multi-head self-attention mechanism
  - Why needed here: It allows the model to capture multiple topic perspectives simultaneously, enabling richer topic-term relationships for explanations
  - Quick check question: How does multi-head attention differ from a single attention head in terms of topic diversity?

- Concept: Topic coherence metrics (NPMI, W2V)
  - Why needed here: These metrics provide quantitative validation that the extracted topics are semantically meaningful, which is critical for ensuring explanation quality
  - Quick check question: What is the difference between NPMI and W2V in measuring topic coherence?

- Concept: Negative sampling in ranking loss
  - Why needed here: It enables effective training with implicit feedback by contrasting clicked vs. non-clicked items, improving recommendation accuracy
  - Quick check question: Why is negative sampling preferred over explicit rating supervision in news recommendation?

## Architecture Onboarding

- Component map:
  Embedding layer (pre-trained GloVe) -> Bi-level Attention-based Topical Model (BATM) -> User modeling (additive attention or GRU) -> Click predictor (dot product) -> Topic coherence evaluator (NPMI, W2V)

- Critical path:
  1. Encode news â†’ extract topic-term and document-topic weights
  2. Encode user history using attention over news representations
  3. Compute relevance scores via dot product
  4. Generate explanations by mapping attention weights to topic descriptors

- Design tradeoffs:
  - Additive attention vs. GRU for user modeling: additive is more explainable but GRU may capture sequential dynamics better
  - Topic coherence evaluation adds interpretability but increases evaluation complexity
  - Using pre-trained embeddings improves semantic quality but ties the model to a fixed vocabulary

- Failure signatures:
  - Low topic coherence scores â†’ explanations not interpretable
  - Unstable attention weights across runs â†’ unreliable explanations
  - Similar recommendation performance to baselines â†’ explanations may not add practical value

- First 3 experiments:
  1. Compare NPMI/W2V scores of BATM-ATT topics vs. LDA topics to validate coherence gains
  2. Ablation study: remove attention weights from explanation pipeline and measure impact on perceived transparency (via user study or proxy metrics)
  3. Evaluate explanation stability: run model multiple times and measure variance in top topic descriptors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the extracted topics be leveraged to improve the overall performance of the news recommendation system?
- Basis in paper: [inferred] The paper mentions exploring the use of topic features to improve recommendation performance as a future direction
- Why unresolved: The current work focuses on using topics for explanations, but does not investigate their potential impact on recommendation accuracy
- What evidence would resolve it: Experiments comparing the performance of the current model with a variant that incorporates topic features into the recommendation process

### Open Question 2
- Question: What additional constraints could be added to the model to generate more interpretable topics?
- Basis in paper: [explicit] The authors mention adding constraints to the model to extract more interpretable topics as a future direction
- Why unresolved: The current model extracts topics based on attention weights, but there may be room for improvement in terms of topic quality and interpretability
- What evidence would resolve it: Experiments evaluating the impact of different constraints on the coherence and interpretability of the extracted topics

### Open Question 3
- Question: How well does the explainable recommender system perform in a real-world setting, as perceived by users?
- Basis in paper: [explicit] The authors mention conducting a user study to determine the real-world effectiveness of the explainable recommender system as a future direction
- Why unresolved: The current work evaluates the system based on recommendation performance and topic coherence, but does not consider user perception and acceptance
- What evidence would resolve it: A user study assessing the system's explainability, transparency, and impact on user trust and satisfaction in a real-world news recommendation scenario

## Limitations

- The model's reliance on pre-trained embeddings and specific dataset characteristics may limit generalizability to other domains or languages
- Claims about enhanced user trust are not empirically validated through user studies
- The practical value of topic-centric explanations compared to other explanation methods remains unproven

## Confidence

- **High confidence**: The recommendation performance metrics (AUC, MRR, nDCG) are well-established and directly comparable to baselines. The implementation of BATM architecture and training procedure appears technically sound.
- **Medium confidence**: Topic coherence scores provide reasonable proxies for explanation quality, but the correlation between coherence and actual human interpretability needs validation. The assumption that attention weights serve as faithful explanations is plausible but not rigorously tested.
- **Low confidence**: Claims about enhanced user trust and system transparency are not empirically supported. The practical value of topic-centric explanations compared to other explanation methods remains unproven.

## Next Checks

1. Conduct a user study comparing topic-centric explanations against alternative explanation methods (feature importance, example-based) to directly measure perceived transparency and trust
2. Perform attention weight stability analysis across multiple training runs to verify the reliability of generated explanations
3. Evaluate the model's performance on out-of-domain news articles to assess generalizability beyond the MIND dataset characteristics