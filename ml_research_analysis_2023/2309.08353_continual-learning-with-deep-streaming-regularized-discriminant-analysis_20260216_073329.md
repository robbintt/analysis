---
ver: rpa2
title: Continual Learning with Deep Streaming Regularized Discriminant Analysis
arxiv_id: '2309.08353'
source_url: https://arxiv.org/abs/2309.08353
tags:
- learning
- streaming
- continual
- data
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Deep Streaming Regularized Discriminant Analysis
  (SRDA), a method for continual learning in streaming scenarios where data arrives
  one sample at a time. The core idea is to extend streaming linear discriminant analysis
  with quadratic discriminant analysis using regularization to handle high-dimensional
  data.
---

# Continual Learning with Deep Streaming Regularized Discriminant Analysis

## Quick Facts
- arXiv ID: 2309.08353
- Source URL: https://arxiv.org/abs/2309.08353
- Reference count: 40
- Key result: SRDA achieves 80.1% top-5 accuracy on ImageNet ILSVRC-2012 with 12-hour computation time and 1.05 GB memory

## Executive Summary
This paper introduces Deep Streaming Regularized Discriminant Analysis (SRDA), a method for continual learning in streaming scenarios where data arrives one sample at a time. SRDA extends streaming linear discriminant analysis with quadratic discriminant analysis using regularization to handle high-dimensional data. The method combines SRDA with a frozen convolutional neural network and demonstrates superior performance on ImageNet ILSVRC-2012 compared to both batch learning and existing streaming learning algorithms, achieving 80.1% top-5 accuracy while requiring minimal memory and computation time.

## Method Summary
SRDA combines a frozen convolutional neural network backbone with a streaming discriminant analysis classifier. The method maintains running statistics for class means and covariance matrices, updating them incrementally as new samples arrive. Regularization parameter α controls the trade-off between per-class covariance estimates and pooled covariance, enabling stable high-dimensional streaming discriminant analysis. The classifier computes log posterior probabilities directly rather than using Softmax, avoiding recency bias in class-incremental learning.

## Key Results
- SRDA achieves 80.1% top-5 accuracy on ImageNet ILSVRC-2012
- Computation time: 12 hours for complete training
- Memory usage: 1.05 GB for dataset storage
- Outperforms batch learning methods in streaming scenario
- Optimal regularization parameter α=0.55 determined via grid search CV

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regularized covariance estimation enables stable high-dimensional streaming discriminant analysis
- Mechanism: SRDA combines per-class covariance estimates with pooled covariance (controlled by α), regularizing unstable high-dimensional estimates with more stable pooled estimates
- Core assumption: Individual class covariance matrices in high dimensions are poorly estimated with limited samples, but pooled covariance provides stable reference
- Evidence anchors: [abstract] "SQDA struggles in high-dimensional settings... SRDA as a solution" and [section] "SQDA... struggles in high-dimensional settings due to limited per-class data"
- Break condition: When number of samples per class approaches dimensionality, making individual class covariances well-estimated without regularization

### Mechanism 2
- Claim: Freezing early CNN layers preserves transferable feature representations while enabling streaming learning
- Mechanism: Early CNN layers learn generic features transferable across datasets, so freezing them allows focus on classifier adaptation without catastrophic forgetting
- Core assumption: Early CNN layers learn generic features that transfer across natural image datasets
- Evidence anchors: [section] "early layers of a CNN... tend to learn filters that exhibit minimal variation across large natural image datasets and demonstrate high transferability"
- Break condition: When task requires specialized features in early layers that are not transferable

### Mechanism 3
- Claim: Streaming discriminant analysis avoids Softmax recency bias by using generative probabilistic classification
- Mechanism: SRDA computes log posterior probabilities directly rather than using Softmax on logits, eliminating bias toward recently seen classes
- Core assumption: Softmax layer suffers from task-recency bias when trained incrementally on streaming data
- Evidence anchors: [section] "Softmax layer and its associated Fully-Connected layer suffer from task-recency bias" and [section] "this simple yet effective substitute... addresses recency bias"
- Break condition: When using non-Softmax classifiers that also exhibit recency bias

## Foundational Learning

- Discriminant Analysis:
  - Why needed here: Forms the mathematical foundation for SRDA's classification mechanism
  - Quick check question: What is the key difference between LDA and QDA in terms of covariance matrix assumptions?

- Streaming Statistics:
  - Why needed here: Essential for updating running means and covariances without storing historical data
  - Quick check question: How do you update a running mean when a new sample arrives?

- Regularization:
  - Why needed here: Enables stable covariance estimation in high-dimensional spaces
  - Quick check question: What is the purpose of regularization in discriminant analysis?

## Architecture Onboarding

- Component map: Frozen CNN backbone -> SRDA classifier with running statistics -> Streaming update logic
- Critical path:
  1. Extract features from frozen CNN
  2. Update running statistics for current class
  3. Compute regularized discriminant scores
  4. Classify using argmax of discriminant scores
- Design tradeoffs:
  - Memory vs accuracy: Storing per-class covariances uses more memory but improves accuracy
  - Computation vs flexibility: Fixed backbone limits adaptability but enables streaming
  - Regularization strength (α) vs class-specific modeling: Higher α favors stability over specificity
- Failure signatures:
  - Poor performance with few samples per class (insufficient covariance estimation)
  - Accuracy degradation over time (covariance estimation drift)
  - Memory exhaustion (too many classes stored)
- First 3 experiments:
  1. Test SRDA with varying α on a small dataset to observe regularization effects
  2. Compare SRDA vs SLDA vs SQDA on high-dimensional synthetic data
  3. Validate streaming update logic with controlled class arrival patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal regularization parameter alpha (α) for SRDA across different datasets and architectures?
- Basis in paper: [explicit] The authors perform a grid search CV revealing the optimal α value of 0.55 for ImageNet ILSVRC-2012, but note that "Better results can potentially be achieved by using a more recent backbone, such as EfficientNets."
- Why unresolved: The optimal α value likely depends on dataset characteristics, model architecture, and data stream properties. The authors only tested one dataset and one backbone (ResNet-18).
- What evidence would resolve it: Systematic experiments varying α across multiple datasets (e.g., CIFAR, MNIST), different CNN architectures, and different streaming conditions would reveal how α affects performance and identify generalizable patterns.

### Open Question 2
- Question: How does SRDA compare to other regularization approaches for streaming learning, such as elastic weight consolidation or variational continual learning?
- Basis in paper: [inferred] The authors compare SRDA only to specific streaming and batch learning methods but don't explore other regularization-based continual learning approaches that could be adapted to streaming scenarios.
- Why unresolved: The paper establishes SRDA's superiority within its specific comparison set but doesn't explore the broader landscape of regularization techniques that could potentially offer similar or better performance.
- What evidence would resolve it: Direct comparisons between SRDA and other regularization-based methods (EWC, VCL, etc.) adapted to streaming learning would clarify SRDA's relative advantages and limitations.

### Open Question 3
- Question: How does SRDA perform in non-class-incremental streaming scenarios, such as domain incremental learning where data distribution shifts but class labels remain constant?
- Basis in paper: [explicit] The authors state they "concentrate on the application of classification in computer vision, focusing on the general scenario of Online Class Incremental" and don't evaluate domain incremental scenarios.
- Why unresolved: SRDA was specifically designed and evaluated for class-incremental learning, but its performance characteristics in domain shift scenarios remain unknown.
- What evidence would resolve it: Experiments testing SRDA on datasets with domain shifts (e.g., CORe50, DomainNet) while maintaining constant class labels would reveal whether the method's advantages extend beyond class-incremental settings.

## Limitations

- Streaming setting assumes class-ordered data arrival, which may not reflect realistic scenarios where classes arrive randomly
- Method requires storing covariance matrices for all seen classes, leading to memory scaling linearly with the number of classes
- Optimal regularization parameter α=0.55 was determined post-hoc through grid search, raising questions about practical deployment without extensive hyperparameter tuning

## Confidence

- SRDA algorithm correctness: High - mathematical foundations are sound
- Performance claims: Medium - results validated on one dataset only
- Streaming update correctness: Medium - implementation details not fully specified
- Generalization to other streaming scenarios: Low - limited empirical validation

## Next Checks

1. Test SRDA with random class arrival order rather than class-ordered streaming to assess real-world applicability
2. Evaluate memory usage scaling by incrementally adding classes until memory constraints are reached
3. Validate performance on additional streaming datasets beyond ImageNet to assess generalizability