---
ver: rpa2
title: 'Too Much Information: Keeping Training Simple for BabyLMs'
arxiv_id: '2311.01955'
source_url: https://arxiv.org/abs/2311.01955
tags:
- size
- context
- data
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper details the University of Groningen's work for the BabyLM
  Challenge, investigating how to introduce language models to varying levels of complexity,
  similar to how humans learn. The authors experiment with context size, vocabulary,
  and linguistic complexity of the data, finding that limiting context size is most
  beneficial for training.
---

# Too Much Information: Keeping Training Simple for BabyLMs

## Quick Facts
- **arXiv ID**: 2311.01955
- **Source URL**: https://arxiv.org/abs/2311.01955
- **Reference count**: 6
- **Primary result**: Using a context size of 32 tokens initially, then increasing to 128, improved performance by 2 points on average on (Super)GLUE tasks, 1 point on MSGS tasks, and 12% on average on BLiMP tasks compared to the baseline trained on 10 times more data.

## Executive Summary
This paper investigates methods to introduce language models to varying levels of complexity, similar to human learning, within the context of the BabyLM Challenge. The authors experiment with context size, vocabulary, and linguistic complexity of the data, finding that limiting context size is most beneficial for training. Using a smaller context size of 32 tokens initially, then increasing to 128, improved performance by 2 points on average on (Super)GLUE tasks, 1 point on MSGS tasks, and 12% on average on BLiMP tasks compared to the baseline trained on 10 times more data. The authors also experimented with curriculum learning and character-level vocabularies but found these did not improve performance.

## Method Summary
The authors pretrain encoder-only models (RoBERTa-base and DeBERTa-large) on 10 million words from diverse domains using Masked Language Modeling (MLM). They systematically vary context size (16-256 tokens), vocabulary size (8k-64k subwords, character-level), and apply curriculum learning based on a complexity metric combining type/token ratio, word rarity, and punctuation density. Models are evaluated on BLiMP (linguistic probing), SuperGLUE, and MSGS tasks. Training involves two stages: initial pretraining at 32 tokens for 50 epochs, then extending context to 128 for 10-50 more epochs.

## Key Results
- Context size of 32 tokens initially, then 128, improved average performance by 2 points on (Super)GLUE, 1 point on MSGS, and 12% on BLiMP compared to baseline.
- Curriculum learning based on the proposed complexity metric did not improve performance due to confounding by non-linguistic artifacts.
- Character-level vocabulary pretraining did not provide consistent benefits over direct subword training.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Limiting context size to 32 tokens accelerates convergence in low-resource pretraining by reducing cross-sentential noise.
- Mechanism: Smaller context reduces the amount of irrelevant or out-of-domain information that the model must filter during pretraining, allowing it to focus on local patterns and frequent n-grams that are easier to learn.
- Core assumption: The most salient linguistic features in small datasets are contained within short spans; cross-sentential dependencies are either rare or less critical early in training.
- Evidence anchors:
  - [abstract] "Using a smaller context size of 32 tokens initially, then increasing to 128, improved performance by 2 points on average on (Super)GLUE tasks..."
  - [section] "Concerning context size, prior work (Edman et al., 2022) has shown that in low-resource language modeling, using a lower context size can greatly help with model convergence."
- Break condition: If the dataset contains long-range dependencies that are crucial for the target tasks, or if the language heavily relies on discourse-level cues, performance will degrade with overly short context.

### Mechanism 2
- Claim: Curriculum learning based on lexical rarity and punctuation density is ineffective because the complexity metric is confounded by domain-specific noise.
- Mechanism: The proposed complexity measure conflates true linguistic complexity with artifacts such as HTML tags, collapsed whitespace, and non-target language content, leading to a curriculum that emphasizes irrelevant samples.
- Core assumption: Complexity scores based on type/token ratio, word rarity, and punctuation are proxies for linguistic sophistication rather than dataset quality or domain noise.
- Evidence anchors:
  - [section] "The easiest samples are indeed linguistically simple – they contain a lot of repetitions... On the other side of the complexity scale, a lot of samples are indeed difficult, but in a way that does not necessarily reflect true linguistic complexity: vocabulary and punctuation features push up samples that contain elements of HTML..."
- Break condition: If a cleaner, domain-specific complexity metric is used, or if the data is preprocessed to remove HTML artifacts, curriculum learning may become effective.

### Mechanism 3
- Claim: Scaling up model size (RoBERTa-base → DeBERTa-large) yields consistent gains because larger capacity compensates for limited data.
- Mechanism: Additional parameters allow the model to fit the smaller training set more flexibly, capturing patterns that smaller models underfit.
- Core assumption: The bottleneck in low-resource pretraining is model capacity rather than optimization dynamics, so increasing size is beneficial up to a point.
- Evidence anchors:
  - [section] "We can see that DeBERTa-large generally performs best... This shows that when limiting the context size, we can potentially scale up to larger models even when data is scarce."
- Break condition: If data scarcity is so severe that the model overfits, further scaling may hurt generalization, especially on out-of-domain tasks.

## Foundational Learning

- Concept: Vocabulary tokenization strategy (subword vs. character vs. hybrid)
  - Why needed here: The paper experiments with 40k subword vocabulary, character-level, and transfer to subword, affecting both convergence speed and final performance.
  - Quick check question: How does changing vocabulary size from 40k to 8k or to character-level affect BLiMP score in the reported experiments?
- Concept: Context size selection and its interaction with model capacity
  - Why needed here: The main empirical finding is that 32 tokens initially, then 128, yields best results; this interacts with model size and data size.
  - Quick check question: What is the performance difference between training only on 32 tokens versus 32 then 128 tokens on GLUE tasks?
- Concept: Curriculum learning pacing and complexity measurement
  - Why needed here: The paper tried easy-to-hard curriculum but found it ineffective; understanding why requires knowing how complexity metrics are constructed and paced.
  - Quick check question: What features are combined to compute the complexity score, and how does the "data-unlocking" schedule work?

## Architecture Onboarding

- Component map: Tokenizer (SentencePiece Unigram) -> Model (RoBERTa-base → DeBERTa-large) -> Training (MLM) -> Data pipeline (fixed-size chunks) -> Evaluation (BLiMP, GLUE, MSGS)
- Critical path:
  1. Build and cache fixed-size context chunks
  2. Initialize tokenizer and vocabulary
  3. Pretrain with MLM for 50 epochs (first stage at 32 tokens)
  4. If applicable, extend context to 128 and continue training (10-50 epochs)
  5. Fine-tune on downstream tasks
  6. Evaluate
- Design tradeoffs:
  - Context size vs. computational efficiency: 32 tokens speeds up training but may hurt long-range tasks
  - Vocabulary size vs. coverage: 40k subwords balance speed and expressiveness; character-level is slower but handles morphology better
  - Curriculum ordering vs. noise: Complexity metric may introduce bias from non-linguistic artifacts
- Failure signatures:
  - Degraded GLUE scores when context is too short
  - Unstable curriculum training loss curves when data ordering is reversed
  - Overfitting with DeBERTa-XL on very small data
- First 3 experiments:
  1. Vary context size (16, 32, 64, 128) with fixed 40k vocabulary and RoBERTa-base; measure BLiMP and GLUE.
  2. Train character → 40k transfer model at context 32 and 64; compare to direct subword baseline.
  3. Run curriculum vs. no-curriculum vs. reversed-curriculum triples with context 32→128 schedule; track loss and BLiMP per phase.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does curriculum learning based on linguistic complexity metrics improve performance for BabyLMs beyond context size reduction?
- Basis in paper: [explicit] The authors state "we don't see a clear pattern in what types of linguistic phenomena benefit from a particular order of data exposure and can't conclude whether the observed effects are robust and systematic."
- Why unresolved: The curriculum learning approach showed mixed results with some models performing worse than the baseline, and the authors attribute this to potential issues with the complexity metric, its adequacy for the training objective, data noise, or the pacing function.
- What evidence would resolve it: A curriculum learning approach that achieves consistent and significant performance improvements over the baseline model, along with a validated complexity metric that accurately reflects linguistic complexity for the MLM training objective.

### Open Question 2
- Question: What is the optimal way to integrate character-level information into subword-based language models for improved performance on low-resource tasks?
- Basis in paper: [explicit] The authors note that their character-level to subword transfer method "proved to be unreliable" and suggest "there are better ways of integrating rather than via an extra initial pretraining step."
- Why unresolved: The character-level pretraining approach showed inconsistent performance gains and the authors suspect the model "is susceptible to forgetting what it has learned during the character-level pretraining when it is pretraining for the second time."
- What evidence would resolve it: A character-informed model that consistently outperforms pure subword models on low-resource tasks without the need for separate pretraining steps or shows clear complementary benefits when used alongside subword vocabularies.

### Open Question 3
- Question: Is there an optimal pacing function for gradually increasing context size during training that outperforms both fixed context sizes and the two-stage approach used in this work?
- Basis in paper: [inferred] The authors note that "a larger context size is indeed necessary for performance on (Super-)GLUE" but also suggest "there may be a smarter way to control context size, such as a gradual increasing during training, which could lead to smoother and faster training."
- Why unresolved: The paper only tested fixed context sizes and a simple two-stage approach (32→128), but did not explore gradual context size increases or more sophisticated pacing functions during training.
- What evidence would resolve it: A model trained with a gradually increasing context size that achieves better performance than both the fixed 32-token and 128-token models, as well as the two-stage 32→128 approach, while also showing more efficient training dynamics.

## Limitations
- The complexity metric used for curriculum learning conflates true linguistic sophistication with dataset artifacts such as HTML tags and domain-specific noise.
- Analysis is constrained to a single dataset size (10 million words) and a narrow range of context sizes (16-256 tokens), limiting generalizability.
- The interaction between context size and task type is not fully explored; some tasks may inherently require longer contexts.

## Confidence

**High Confidence:**
- Context size reduction to 32 tokens improves convergence and downstream task performance in low-resource settings.
- Increasing model size (RoBERTa-base to DeBERTa-large) yields consistent gains when context is limited.
- Character-level vocabularies and curriculum learning based on the proposed complexity metric do not improve performance.

**Medium Confidence:**
- The proposed complexity metric is confounded by non-linguistic artifacts, rendering curriculum learning ineffective.
- Scaling up model size is beneficial up to DeBERTa-large, but further scaling may lead to overfitting in extremely low-resource scenarios.

**Low Confidence:**
- The optimal context size schedule (32→128) generalizes to other languages or domains.
- Curriculum learning could be effective with a different, cleaner complexity metric (not tested).

## Next Checks
1. **Re-evaluate curriculum learning with a cleaner complexity metric**: Remove HTML tags and non-linguistic artifacts from the dataset, then recompute complexity scores based on type/token ratio and word rarity. Train with easy-to-hard and hard-to-easy schedules and compare to shuffled baseline.

2. **Task-specific context sensitivity analysis**: Systematically vary context size (16, 32, 64, 128, 256) for each downstream task (BLiMP, GLUE, MSGS) and measure performance drop. Identify tasks that are robust or sensitive to context truncation.

3. **Vocabulary size ablation study**: Train models with 8k, 16k, 40k, and 64k subword vocabularies at context sizes 32 and 128. Compare convergence speed and final performance on BLiMP and GLUE to determine the optimal vocabulary size for low-resource pretraining.