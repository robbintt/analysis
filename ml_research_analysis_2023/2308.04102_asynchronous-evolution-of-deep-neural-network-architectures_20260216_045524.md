---
ver: rpa2
title: Asynchronous Evolution of Deep Neural Network Architectures
arxiv_id: '2308.04102'
source_url: https://arxiv.org/abs/2308.04102
tags:
- evaluation
- individuals
- population
- networks
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an asynchronous evaluation strategy (AES)
  for parallelizing evolutionary algorithms (EAs), particularly addressing the challenge
  of variable evaluation times in tasks like evolving deep neural network (DNN) architectures.
  AES maintains a queue of individuals ready for evaluation and proceeds to the next
  generation once a predetermined fraction has been evaluated, thus keeping all computational
  resources busy and minimizing idle time.
---

# Asynchronous Evolution of Deep Neural Network Architectures

## Quick Facts
- arXiv ID: 2308.04102
- Source URL: https://arxiv.org/abs/2308.04102
- Authors: 
- Reference count: 13
- Key outcome: AES achieved 14x speedup in multiplexer design and 3x faster convergence in image captioning compared to synchronous evolution.

## Executive Summary
This paper introduces an asynchronous evaluation strategy (AES) for parallelizing evolutionary algorithms (EAs), particularly addressing the challenge of variable evaluation times in tasks like evolving deep neural network (DNN) architectures. AES maintains a queue of individuals ready for evaluation and proceeds to the next generation once a predetermined fraction has been evaluated, thus keeping all computational resources busy and minimizing idle time. The key to AES's performance is balancing diversity and efficiency by tuning the batch size parameter, with optimal results when the batch size is approximately one-quarter of the total population.

## Method Summary
AES maintains a queue of up to K individuals ready for evaluation by R workers. The server waits for M individuals to return before generating the next generation, where M << K. This creates a balance between maintaining sufficient diversity and maximizing computational efficiency. The hyperparameter D = K/M controls this balance. AES was adapted for multi-population ENAS (CoDeepNEAT-AES) and tested on multiplexer design and image captioning tasks.

## Key Results
- AES achieved 14-fold speedup over synchronous evolution in multiplexer design
- CoDeepNEAT-AES reached similar fitness levels three times faster than synchronous CoDeepNEAT
- AES found better solutions overall in image captioning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asynchronous Evaluation Strategy (AES) reduces worker idle time by maintaining a continuous queue of individuals ready for evaluation.
- Mechanism: AES keeps K individuals in the Evaluation Queue, where R workers pull individuals as soon as they become available. This ensures that workers are never waiting for the next generation to be created, as the server only needs to activate periodically to generate new individuals.
- Core assumption: Worker idle time is the primary bottleneck in parallel EA evaluation, and maintaining a constant supply of individuals can fully utilize computational resources.
- Evidence anchors:
  - [abstract] "AES increases throughput by maintaining a queue of up to $K$ individuals ready to be sent to the workers for evaluation and proceeding to the next generation as soon as $M<<K$ individuals have been evaluated by the workers."
  - [section] "At each generation, a constant number of K individuals are either being evaluated on the R compute workers, have just finished their evaluation, or are waiting in a queue to be evaluated."

### Mechanism 2
- Claim: AES balances diversity and efficiency by tuning the batch size parameter M, with optimal results when M is approximately one-quarter of the total population.
- Mechanism: By waiting for only M individuals (where M << K) to return before generating the next generation, AES creates a balance between maintaining sufficient diversity in the population and maximizing computational efficiency. The hyperparameter D = K/M controls this balance.
- Core assumption: There exists an optimal ratio between the number of individuals being evaluated (K) and the number of individuals needed to trigger the next generation (M) that maximizes both diversity and efficiency.
- Evidence anchors:
  - [abstract] "The key to AES's performance is balancing diversity and efficiency by tuning the batch size parameter, with optimal results when the batch size is approximately one-quarter of the total population."
  - [section] "When D is approximately 4, very large speedups are possible. With this value, AES finds solutions 14 times faster than synchronous evolution."

### Mechanism 3
- Claim: AES is particularly effective for neuroevolution because it handles the variable evaluation times inherent in training different deep neural network architectures.
- Mechanism: In ENAS, different neural network architectures require vastly different training times. AES handles this variability by not waiting for all individuals to finish evaluation, thus preventing the slowest evaluations from bottlenecking the entire process.
- Core assumption: The evaluation time variance in neuroevolution is large enough that traditional synchronous methods suffer significant performance penalties, and AES can effectively mitigate this issue.
- Evidence anchors:
  - [abstract] "Evolutionary neural architecture search (ENAS), a class of EAs that optimizes the architecture and hyperparameters of deep neural networks, is particularly vulnerable to this issue."
  - [section] "In cases where M = 1000, evolution shows the most substantial speedups, thus indicating that when D is approximately 4, very large speedups are possible. With this value, AES finds solutions 14 times faster than synchronous evolution."

## Foundational Learning

- Concept: Parallel Evaluation Strategies in Evolutionary Algorithms
  - Why needed here: Understanding how parallel evaluation works is crucial for grasping why AES provides an improvement over traditional methods.
  - Quick check question: What is the main drawback of global parallelization in EAs when evaluation times vary significantly?

- Concept: Asynchronous vs. Synchronous Evolution
  - Why needed here: AES is fundamentally different from traditional synchronous EAs, and understanding this difference is key to implementing and tuning the algorithm.
  - Quick check question: How does AES differ from generational and steady-state GAs in terms of when it proceeds to the next generation?

- Concept: Neuroevolution and Neural Architecture Search
  - Why needed here: AES was specifically designed to improve ENAS, so understanding the unique challenges of this domain is important for effective implementation.
  - Quick check question: Why is ENAS particularly vulnerable to the issues that AES addresses?

## Architecture Onboarding

- Component map:
  - Server -> Evaluation Queue -> Workers
  - Server generates new individuals and updates elite sets

- Critical path:
  1. Initialize Evaluation Queue with K individuals
  2. Workers pull individuals from queue as they become available
  3. Server waits for M individuals to return with fitness values
  4. Server generates M new individuals from returned individuals and elites
  5. Server updates elite set and submits new individuals to Evaluation Queue
  6. Repeat until termination condition is met

- Design tradeoffs:
  - K (queue size) vs. M (batch size): Larger K provides more continuous work for workers but increases memory overhead; smaller M increases diversity but may reduce efficiency
  - Single vs. multi-population: Single population is simpler but multi-population (like CoDeepNEAT) can evolve more complex structures
  - Synchronous vs. asynchronous: Synchronous is simpler to implement but suffers from idle time; asynchronous is more complex but more efficient

- Failure signatures:
  - Workers consistently idle: Queue size K may be too small or batch size M too large
  - Population diversity decreasing: Batch size M may be too small, leading to insufficient genetic material
  - Server becoming a bottleneck: Number of workers R may be too large relative to server capacity or network bandwidth

- First 3 experiments:
  1. Implement generic AES with a simple EA (like genetic programming) on a synthetic problem with controlled evaluation time variance
  2. Compare AES performance with synchronous EA on the multiplexer domain, varying the batch size M to find the optimal value
  3. Scale up to CoDeepNEAT-AES on a simpler image classification task (like CIFAR-10) before attempting more complex domains like image captioning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of the batch size parameter (M) for different problem domains and computational resource configurations?
- Basis in paper: [explicit] The paper experimentally determined M = 1000 to be optimal for the multiplexer domain with K = 4000 and R = 4000 workers, resulting in a 14-fold speedup. It also used M = 100 with K = 300 for the image-captioning domain.
- Why unresolved: The paper only tested a limited range of M values for each domain. Different problem domains and computational resource configurations (population sizes, worker counts) may have different optimal M values.
- What evidence would resolve it: Systematic experiments varying M across different problem domains and computational resource configurations, measuring convergence time and solution quality to determine optimal M values for each setting.

### Open Question 2
- Question: How does AES perform compared to other asynchronous evaluation strategies for evolutionary algorithms?
- Basis in paper: [explicit] The paper mentions related work on asynchronous EAs but does not directly compare AES to these methods. It only compares AES to synchronous evaluation.
- Why unresolved: Without comparisons to other asynchronous methods, it's unclear if AES is the most effective approach for asynchronous evolution.
- What evidence would resolve it: Direct experimental comparisons of AES with other asynchronous EA strategies on the same problem domains, measuring speedup, solution quality, and other relevant metrics.

### Open Question 3
- Question: How does the evaluation time variability affect the performance of AES, and can AES be improved to handle highly variable evaluation times better?
- Basis in paper: [explicit] The paper notes that AES is particularly suited for domains with long and variable evaluation times, like ENAS. It also shows that AES reduces idle time and speeds up evolution compared to synchronous evaluation in such domains.
- Why unresolved: The paper doesn't explore how different levels of evaluation time variability affect AES performance or investigate potential improvements to AES for handling extreme variability.
- What evidence would resolve it: Experiments systematically varying evaluation time variability in different problem domains, measuring AES performance, and testing potential AES modifications (e.g., adaptive M values) to improve handling of high variability.

## Limitations

- Narrow experimental scope: Only validated on multiplexer design and image captioning domains with specific parameter settings
- Potential bias: May systematically favor individuals with shorter evaluation times, affecting evolutionary process
- Optimal parameters: The batch size ratio of 4:1 was identified empirically and may not generalize across all problem domains

## Confidence

- **High confidence**: The core mechanism of reducing idle time through asynchronous evaluation is well-supported by the experimental results in both multiplexer and image captioning domains.
- **Medium confidence**: The claim that a batch size ratio of 4:1 provides optimal balance between diversity and efficiency, as this was demonstrated only in the specific experimental setup.
- **Medium confidence**: The generalizability of AES to other neuroevolution tasks beyond the tested domains.

## Next Checks

1. Test AES with different population sizes and evolution strategies to determine if the optimal K:M ratio remains consistent across various configurations.
2. Implement bias detection metrics to quantify whether AES systematically favors individuals with shorter evaluation times and develop mitigation strategies if needed.
3. Apply AES to a broader range of neuroevolution tasks, including different neural architecture search problems and evolutionary reinforcement learning scenarios, to assess generalizability.