---
ver: rpa2
title: 'ALF: Adaptive Label Finetuning for Scene Graph Generation'
arxiv_id: '2312.17425'
source_url: https://arxiv.org/abs/2312.17425
tags:
- transfer
- citrans
- predicates
- labels
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a plug-and-play method, CITrans, to address
  biased predictions in Scene Graph Generation (SGG) caused by long-tailed predicate
  distributions. CITrans improves data transfer methods by incorporating context-aware
  constraints, ensuring more accurate predicate transfers, and avoiding computationally
  expensive retraining.
---

# ALF: Adaptive Label Finetuning for Scene Graph Generation

## Quick Facts
- arXiv ID: 2312.17425
- Source URL: https://arxiv.org/abs/2312.17425
- Reference count: 0
- Introduces CITrans, a plug-and-play method that improves Scene Graph Generation (SGG) by iteratively transferring coarse-grained predicates into fine-grained ones while imposing context-aware constraints

## Executive Summary
This paper addresses the biased predictions problem in Scene Graph Generation (SGG) caused by long-tailed predicate distributions. The authors introduce CITrans, a method that transfers coarse-grained predicates into fine-grained ones while imposing context-aware constraints. Unlike prior data transfer methods that require expensive retraining, CITrans iteratively trains models and progressively generates enhanced labels through autoregression. Evaluated on SGG benchmarks, CITrans significantly improves performance with fewer training iterations and lower data transfer ratios compared to previous methods, achieving state-of-the-art results.

## Method Summary
CITrans consists of two main components: Context-Restricted Transfer (CRT) and Efficient Iterative Learning (EIL). CRT builds a predicate context map by extracting all predicates associated with each subject-object pair from training data, then restricts candidate predicates based on semantic constraints. EIL iteratively alternates between generating enhanced labels using the current model's predictions and fine-tuning the model with these updated labels. This autoregressive process allows the model to learn from progressively more accurate labels without discarding the pre-trained model. The method improves over baseline models (Motif, VCTree, Transformer) by reducing extra training iterations while achieving state-of-the-art mR@100 performance.

## Key Results
- Significantly improves performance on SGG benchmarks with fewer training iterations
- Achieves state-of-the-art results on PredCls, SGCls, and SGDet subtasks
- Reduces data transfer ratios compared to prior methods
- Demonstrates better label quality and distribution balance through iterative refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-Restricted Transfer (CRT) improves predicate selection by filtering out impossible predicates based on subject-object pair constraints
- Mechanism: CRT builds a predicate context map by extracting all predicates associated with each subject-object pair from training data, then restricts candidate predicates to only those observed in similar contexts
- Core assumption: Predicates associated with the same subject-object pairs exhibit context-based constraints that can be learned from training data
- Evidence anchors: [abstract] "impose an effective priori constraint in predicates’ semantic space", [section] "We propose Predicate-Context Constraint to extract context-based predicate constraint from all triplets in the training set"
- Break condition: If training data lacks diversity in subject-object pairs, the context map becomes incomplete and may exclude valid predicates

### Mechanism 2
- Claim: Efficient Iterative Learning (EIL) reduces computational cost by iteratively refining model and labels rather than training from scratch
- Mechanism: EIL alternates between generating enhanced labels using current model predictions and fine-tuning the model with these updated labels in an autoregressive manner
- Core assumption: Model predictions improve as training progresses, and these improved predictions can generate better labels for subsequent iterations
- Evidence anchors: [abstract] "iteratively trains models and progressively generates enhanced labels through autoregression", [section] "Efficient Iterative Learning trains the initial model with the enhanced labels generated by CRT in an autoregressive manner"
- Break condition: If model converges too quickly or plateaus, subsequent iterations provide diminishing returns and may introduce label noise

### Mechanism 3
- Claim: Restriction-Based Judgment balances predicate distribution by preferentially transferring non-head predicates that model predicts more confidently than ground truth head predicate
- Mechanism: For each subject-object pair, predicates are ranked by both dataset frequency and model prediction score; non-head predicates ranking higher than ground truth head predicate are selected for transfer with decreasing probability as distance increases
- Core assumption: Non-head predicates are often suppressed by frequent head predicates in standard training, but when model predicts them more confidently, they represent more accurate relations for that specific context
- Evidence anchors: [abstract] "supervised with labels transferred by ALC, AIL iteratively finetunes the SGG models in an auto-regressive manner", [section] "To balance the distribution of predicates, we specify that only head predicates are allowed to be transformed into non-head predicates"
- Break condition: If model's confidence scores are poorly calibrated, this mechanism may transfer incorrect predicates

## Foundational Learning

- Concept: Scene Graph Generation task structure
  - Why needed here: Understanding two-stage nature (object detection + predicate classification) is crucial for implementing SGG pipeline
  - Quick check question: What are the three subtasks evaluated in SGG, and how do they differ in terms of input/output?

- Concept: Long-tailed distribution in predicate classification
  - Why needed here: Entire motivation for ALF stems from addressing imbalanced predicate distribution problem
  - Quick check question: Why does a long-tailed predicate distribution lead to biased predictions toward frequent predicates?

- Concept: Iterative learning and label refinement
  - Why needed here: EIL's autoregressive training process requires understanding how to alternate between label generation and model training
  - Quick check question: How does iteratively updating labels based on model predictions differ from standard supervised learning?

## Architecture Onboarding

- Component map: Predicate-Context Constraint → Restriction-Based Judgment → Transfer Module → EIL Trainer → SGG Backbone (Motif/VCTree/Transformer)
- Critical path: Predicate-Context Constraint → Restriction-Based Judgment → Transfer Module → EIL Trainer → SGG Backbone
- Design tradeoffs: Computational efficiency vs. label accuracy (more iterations improve labels but increase training time)
- Failure signatures: Degraded performance on specific predicate types, overfitting to training data, or convergence issues during iterative training
- First 3 experiments:
  1. Implement Predicate-Context Constraint and verify it correctly extracts context constraints from training data
  2. Test Restriction-Based Judgment independently to ensure it ranks and selects predicates as expected
  3. Run a single iteration of EIL to confirm label generation and model fine-tuning work together correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CITrans performance compare to other SGG methods when training data is extremely imbalanced or contains significant noise?
- Basis in paper: [explicit] The paper mentions CITrans aims to address biased predictions caused by long-tailed predicate distributions and can handle noisy labels
- Why unresolved: Paper only evaluates CITrans on Visual Genome dataset, which may not represent all possible scenarios with extreme imbalance or noise
- What evidence would resolve it: Conducting experiments on datasets with varying degrees of imbalance and noise, comparing CITrans's performance to other SGG methods in these scenarios

### Open Question 2
- Question: Can CITrans be extended to other vision tasks beyond SGG, such as object detection or image captioning?
- Basis in paper: [inferred] The paper focuses on applying CITrans to SGG, but the method's core idea of context-based label transfer and iterative learning could potentially be adapted to other tasks
- Why unresolved: Paper does not explore applicability of CITrans to other vision tasks, and it remains to be seen whether method would be effective in these contexts
- What evidence would resolve it: Implementing and evaluating CITrans on other vision tasks, such as object detection or image captioning, and comparing its performance to existing methods

### Open Question 3
- Question: How does choice of hyperparameters (α and β) in CITrans affect its performance, and is there an optimal setting for these parameters?
- Basis in paper: [explicit] The paper mentions α is set to 25 and β is set to 2 in experiments, but does not provide detailed analysis of impact of these hyperparameters on performance
- Why unresolved: Paper does not explore sensitivity of CITrans's performance to choice of hyperparameters, and it remains unclear whether there is an optimal setting for these parameters
- What evidence would resolve it: Conducting systematic study of impact of α and β on CITrans's performance, identifying optimal settings for these hyperparameters

## Limitations

- Performance heavily depends on quality and diversity of training data's subject-object pair distributions
- Specific hyperparameter choices (α=25, β=2) appear arbitrary without ablation studies showing sensitivity
- Iterative training process may suffer from label noise accumulation over multiple iterations
- Claims about state-of-the-art results lack comprehensive ablation studies or comparisons to recent methods beyond 2020 timeframe

## Confidence

- **High Confidence**: General approach of using context constraints to improve predicate transfer is well-founded and addresses real problem in SGG
- **Medium Confidence**: Specific implementation details of CRT and EIL are described but lack sufficient detail for complete reproduction without additional assumptions
- **Low Confidence**: Claims about achieving state-of-the-art results without providing comprehensive ablation studies or comparisons to recent methods

## Next Checks

1. Implement a synthetic test case with known subject-object pair constraints to verify that CRT correctly filters impossible predicate transfers and maintains valid ones

2. Conduct ablation studies varying α and β parameters to quantify their impact on transfer quality and overall SGG performance

3. Test the method's robustness by intentionally removing certain subject-object pairs from the training context map to simulate incomplete data scenarios and observe performance degradation patterns