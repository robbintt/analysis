---
ver: rpa2
title: 'LLM-SAP: Large Language Models Situational Awareness Based Planning'
arxiv_id: '2312.16127'
source_url: https://arxiv.org/abs/2312.16127
tags:
- gpt-4
- planning
- state
- actions
- child
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces LLM-SAP, a framework that integrates large
  language models with situational awareness-based planning to improve AI agents'
  decision-making in dynamic environments. The key idea is to prompt models to consider
  relationships and consequences iteratively when generating plans.
---

# LLM-SAP: Large Language Models Situational Awareness Based Planning

## Quick Facts
- arXiv ID: 2312.16127
- Source URL: https://arxiv.org/abs/2312.16127
- Reference count: 0
- Primary result: SAP prompts and multi-agent feedback improve LLM planning reliability in home hazard scenarios

## Executive Summary
This work introduces LLM-SAP, a framework that integrates large language models with situational awareness-based planning to improve AI agents' decision-making in dynamic environments. The key idea is to prompt models to consider relationships and consequences iteratively when generating plans. Experiments on 24 home hazard scenarios show that SAP prompts enhance planning reliability compared to no prompts. Further, a multi-agent closed-loop approach where models generate and evaluate plans collaboratively leads to iterative quality improvements. Results demonstrate the promise of SAP and multi-agent feedback for developing more robust and safe AI planning systems.

## Method Summary
The method involves a multi-agent closed-loop approach where LLM agents iteratively generate and evaluate plans using Situational Awareness-based Planning (SAP) prompts. The process uses 24 standardized home hazard scenarios, an action set of 56 robot behaviors, and SAP prompts to guide LLM reasoning. A rank-based scoring (RBS) method with seven scoring dimensions evaluates plan quality through pairwise model comparisons. The system employs GPT-3.5 as the generative model and Claude-2 as the evaluative model in the closed-loop framework.

## Key Results
- SAP prompts significantly enhance planning reliability compared to no prompts in home hazard scenarios
- Multi-agent closed-loop approach with GPT-3.5 and Claude-2 outperforms even GPT-4 with SAP prompts
- RBS methodology shows 75.7% agreement with human rankings, demonstrating reliability for comparative evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAP prompts improve planning by directing models to consider relationships and iterative consequences
- Mechanism: The prompt explicitly asks models to "consider the attributes of humans, animals, and objects, as well as the complex intersections of their relationships" and "consider the immediate and potential future consequences of each action iteratively"
- Core assumption: Models can effectively process and apply these situational reasoning cues without further fine-tuning
- Evidence anchors:
  - [abstract]: "SAP prompts enhance planning reliability compared to no prompts"
  - [section]: "Incorporating prompts directing models to deeply consider relationships and iterative consequences significantly enhance latent planning capacities"
- Break condition: If models fail to meaningfully integrate relationship and consequence reasoning into planning logic

### Mechanism 2
- Claim: Multi-agent collaboration improves plan quality through iterative feedback
- Mechanism: One LLM generates a plan, another evaluates it and provides feedback, and the generator incorporates this feedback to improve the next iteration
- Core assumption: The evaluative agent can provide meaningful, actionable feedback that improves the generative agent's plans
- Evidence anchors:
  - [abstract]: "a multi-agent closed-loop approach where models generate and evaluate plans collaboratively leads to iterative quality improvements"
  - [section]: "GPT-3.5 with the SAP prompt is utilized as the generative model (LLM gen) and Claude-2 served as the evaluative model (LLM eval)"
- Break condition: If evaluative feedback is too vague or misaligned to improve plans meaningfully

### Mechanism 3
- Claim: RBS scoring provides more reliable plan evaluation by comparing models pairwise
- Mechanism: Instead of absolute scoring prone to rater variability, models are compared head-to-head on each scenario and differentially ranked
- Core assumption: Relative ranking between model outputs is more consistent than absolute scoring
- Evidence anchors:
  - [section]: "Motivated by discussions of inconsistent human evaluation... we introduce a rank-based scoring (RBS) method to help mitigate potential reliability issues"
  - [section]: "Tests find both GPT-4 and Claude-2 could rank FSM pairs with 75.7% agreement to human ranking"
- Break condition: If models cannot reliably distinguish between plan quality differences

## Foundational Learning

- Concept: Situational awareness in dynamic environments
  - Why needed here: The core task requires mapping hazardous scenarios to safe interventions without environmental feedback, necessitating strong situational reasoning
  - Quick check question: What are the three levels of situational awareness (perception, comprehension, projection) and why are they critical for hazard planning?

- Concept: Finite state machine (FSM) design principles
  - Why needed here: Plans are represented as FSMs with states, transitions, and actions; understanding FSM structure is essential for generating and evaluating plans
  - Quick check question: What are the key components of an FSM and how do they relate to hazard remediation planning?

- Concept: Prompt engineering for code generation
  - Why needed here: SAP prompts must effectively guide models to generate valid FSM code while incorporating situational reasoning
  - Quick check question: How do one-shot examples and explicit reasoning instructions affect code generation quality in LLMs?

## Architecture Onboarding

- Component map: Scenario Generator -> LLM Generator (with SAP prompts) -> LLM Evaluator -> RBS Evaluator -> Human Annotators
- Critical path: 1. Scenario selected → 2. LLMgen generates FSM with SAP prompt → 3. LLMeval scores and provides feedback → 4. LLMgen refines plan → 5. RBS scores overall quality
- Design tradeoffs:
  - Open-ended vs. constrained action sets: Broader sets allow more flexibility but increase complexity
  - One-shot examples vs. abstract formatting: Examples improve generation but may introduce unwanted information
  - Human vs. LLM evaluation: Humans provide gold standards but are slower; LLMs are faster but may have alignment issues
- Failure signatures:
  - Poor plan quality despite SAP prompts → Indicates prompts not effectively steering reasoning
  - Evaluator feedback not improving plans → Suggests feedback loop broken or evaluator not aligned
  - RBS scores inconsistent with human judgment → Indicates relative ranking methodology flawed
- First 3 experiments:
  1. Test SAP prompt with different LLM families (GPT-4, Claude-2, Llama-2) to identify which benefit most
  2. Compare one-shot vs. no-example SAP prompts to isolate their contribution
  3. Implement multi-agent closed-loop with different evaluator models to find optimal pairings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM situational awareness be reliably evaluated beyond the proposed rank-based scoring (RBS) method?
- Basis in paper: [explicit] The paper introduces RBS as an alternative to inconsistent human evaluation and inadequate automatic metrics, but acknowledges ongoing work should explore training models expressly for evaluation to boost accuracy
- Why unresolved: The paper's RBS method, while reducing variability from subjective absolute scoring, still relies on human rankings as ground truth
- What evidence would resolve it: Development and validation of an automated evaluation metric for LLM situational awareness that correlates strongly with human judgments across diverse planning scenarios

### Open Question 2
- Question: How can the trade-off between the complexity of situational awareness prompts and the risk of overfitting or attention drift in LLMs be optimized?
- Basis in paper: [inferred] The paper demonstrates that SAP prompts enhance planning performance, but also mentions that long contexts in open-source models caused attention drift and impaired scene comprehension
- Why unresolved: While more complex prompts may lead to better situational reasoning, they also increase the risk of the model losing focus or generating irrelevant content
- What evidence would resolve it: Systematic experiments varying prompt complexity and measuring its impact on both performance and attention stability across different LLM architectures

### Open Question 3
- Question: How can LLM planning capabilities be extended beyond static hazard scenarios to handle dynamic, real-world environments with partial observability and continuous state spaces?
- Basis in paper: [explicit] The paper focuses on planning in predefined home hazard scenarios, but notes that efficiently condensing fluid cognition absent environmental feedback remains an open challenge
- Why unresolved: The current work evaluates planning in well-defined, static scenarios. Real-world environments are often dynamic, partially observable, and have continuous state spaces
- What evidence would resolve it: Demonstration of LLM planning capabilities in dynamic, partially observable environments with continuous state spaces, potentially through integration with simulation or real-world sensor data

## Limitations

- Evaluation relies entirely on simulated scenarios without real-world deployment, limiting ecological validity
- SAP prompts' effectiveness depends on models' latent reasoning capabilities without explicit fine-tuning, which may not generalize across model families or domains
- Multi-agent feedback mechanism's reliability hinges on the evaluator's ability to provide actionable guidance, but this capability is not systematically validated

## Confidence

- SAP prompt effectiveness: Medium - Supported by comparative experiments but lacks ablation studies isolating prompt components
- Multi-agent iterative improvement: Low-Medium - Demonstrated on synthetic data but mechanism robustness unproven
- RBS methodology reliability: Medium-High - Shows strong agreement with human rankings in controlled tests

## Next Checks

1. Conduct ablation studies varying individual SAP prompt components to identify which reasoning elements drive improvements
2. Test the multi-agent closed-loop system with models of varying capabilities to assess feedback quality consistency
3. Validate RBS methodology across diverse task domains beyond hazard scenarios to establish generalizability