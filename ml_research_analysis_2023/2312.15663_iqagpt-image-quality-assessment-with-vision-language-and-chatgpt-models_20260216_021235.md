---
ver: rpa2
title: 'IQAGPT: Image Quality Assessment with Vision-language and ChatGPT Models'
arxiv_id: '2312.15663'
source_url: https://arxiv.org/abs/2312.15663
tags:
- image
- quality
- iqagpt
- score
- lesion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IQAGPT, an image quality assessment system
  that integrates an image quality captioning vision-language model with ChatGPT for
  generating quality scores and textual reports. The system is trained on a CT-IQA
  dataset of 1,000 CT slices with diverse quality levels annotated by a professional
  radiologist.
---

# IQAGPT: Image Quality Assessment with Vision-language and ChatGPT Models

## Quick Facts
- arXiv ID: 2312.15663
- Source URL: https://arxiv.org/abs/2312.15663
- Reference count: 40
- Key outcome: IQAGPT outperforms GPT-4 and CLIP-IQA, as well as image-only multi-task classification and regression models, in generating quality captions, scores, and reports.

## Executive Summary
This paper introduces IQAGPT, an image quality assessment system that integrates an image quality captioning vision-language model with ChatGPT for generating quality scores and textual reports. The system is trained on a CT-IQA dataset of 1,000 CT slices with diverse quality levels annotated by a professional radiologist. IQAGPT demonstrates superior performance compared to existing methods, highlighting the feasibility of using large models for image quality assessment in medical imaging.

## Method Summary
IQAGPT consists of two main components: an image quality captioning model and a ChatGPT interface. The captioning model is based on a pre-trained medical vision-language model and is fine-tuned on a CT-IQA dataset to generate quality descriptions. The ChatGPT interface then interprets these descriptions to extract quality scores or generate radiological reports. The system leverages cross-modal attention to fuse image and text features, and uses a prompt template to convert numerical quality scores into semantically rich text descriptions.

## Key Results
- IQAGPT outperforms GPT-4 and CLIP-IQA in generating quality captions, scores, and reports
- The system achieves higher BLEU and CIDEr scores compared to MiniGPT-4
- IQAGPT demonstrates the effectiveness of incorporating large language models in subjective CT IQA tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The cross-modal attention in the image quality captioning model enables effective fusion of image and text features, allowing the model to generate semantically rich descriptions aligned with radiologist assessments.
- Mechanism: The multi-modal text decoder uses cross-attention layers to attend over both image features from ViT-S/16 and text features from BERTbase, conditioning the language generation on visual context.
- Core assumption: The pre-trained vision-language model [13] has learned sufficient cross-modal alignment for medical images, and fine-tuning preserves this while adapting to the CT-IQA domain.
- Evidence anchors:
  - [abstract] states "The captioning model fuses the image and text features through cross-modal attention."
  - [section 2.2] describes the architecture: "The multi-modal text decoder is the last 6 layers of the BERTbase to fuse image and the text features through cross-modal attention."
- Break condition: If the pre-trained model lacks domain-specific visual-text pairs, cross-modal attention may fail to capture clinically relevant features.

### Mechanism 2
- Claim: Converting radiologist scores to semantically rich text descriptions via a prompt template allows the LLM to better understand subjective image quality nuances.
- Mechanism: The prompt template explicitly encodes scoring criteria (e.g., "unacceptable for diagnostic interpretation") as text, which the captioning model learns to generate, bridging numerical and linguistic representations.
- Core assumption: LLMs can leverage structured text prompts to interpret subjective quality dimensions that are difficult to capture with pure numerical labels.
- Evidence anchors:
  - [section 2.2] explains: "To better leverage the capabilities of LLMs, we convert annotated quality scores into semantically rich text descriptions using a prompt template."
  - [section 3.1] shows improved BLEU and CIDEr scores when using IQAGPT over MiniGPT-4, implying the prompt-based text representation aids generation quality.
- Break condition: If the prompt template oversimplifies or omits key quality aspects, the LLM may miss subtle distinctions between quality levels.

### Mechanism 3
- Claim: ChatGPT integration enables interactive generation of both quantitative scores and qualitative reports, providing clinically meaningful outputs beyond raw captioning.
- Mechanism: After the captioning model generates a description, ChatGPT parses it using the same prompt template to extract scores, or summarizes it into a radiology-style report.
- Core assumption: ChatGPT can reliably interpret structured captions and perform the mapping back to scores or report formats without additional fine-tuning.
- Evidence anchors:
  - [abstract] notes: "Third, based on the quality descriptions, users can talk with ChatGPT to rate image quality scores or produce a radiological quality report."
  - [section 2.3] describes: "ChatGPT provides a language interface...In our IQAGPT, we enable an interaction between ChatGPT and users...based on the output caption from the image quality captioning model."
- Break condition: If ChatGPT's reasoning about medical quality is unreliable, extracted scores or reports may be inaccurate or inconsistent.

## Foundational Learning

- Concept: Vision-language model pre-training on image-text pairs
  - Why needed here: The model must learn cross-modal alignment between CT images and descriptive text before fine-tuning on CT-IQA data.
  - Quick check question: What pre-training objectives (contrastive, masked language modeling, etc.) are used in the base VLM [13]?

- Concept: Cross-modal attention mechanisms
  - Why needed here: These layers allow the decoder to attend over both visual and textual features simultaneously, essential for coherent caption generation.
  - Quick check question: How many cross-attention layers are used in the multi-modal decoder, and how are they initialized?

- Concept: Structured prompt engineering for score-to-text conversion
  - Why needed here: Enables conversion of subjective numerical scores into text that captures qualitative criteria, aiding LLM understanding.
  - Quick check question: What are the four scoring dimensions in the prompt template, and how are they mapped to text phrases?

## Architecture Onboarding

- Component map:
  - Image encoder: ViT-S/16 (12 layers) extracts visual features from CT slices
  - Text encoder: BERTbase (first 6 layers) processes prompt template text
  - Multi-modal decoder: BERTbase (last 6 layers) fuses image and text via cross-attention
  - Interaction layer: ChatGPT interface for score/report generation
- Critical path: CT image → image encoder → text encoder (with prompt) → cross-modal decoder → caption → ChatGPT → score/report
- Design tradeoffs:
  - Using frozen pre-trained encoders preserves learned knowledge but limits adaptation
  - Prompt template adds interpretability but requires careful design to avoid information loss
  - ChatGPT integration offloads post-processing but introduces dependency on external model reliability
- Failure signatures:
  - Poor caption quality (low BLEU/CIDEr) → image encoder or cross-modal attention issue
  - Score mismatch with radiologist → prompt template mapping error or ChatGPT misinterpretation
  - Slow response → ChatGPT API latency or excessive token generation
- First 3 experiments:
  1. Validate caption generation: Input a CT slice, check if output matches prompt template format and ground truth score conversion.
  2. Test score extraction: Feed a known caption to ChatGPT, verify it returns correct numerical scores.
  3. Evaluate full pipeline: Run IQAGPT on a test CT, compare generated report against radiologist annotation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of IQAGPT vary with the size of the CT-IQA dataset?
- Basis in paper: [inferred] The paper mentions the current dataset size is 1,000 image-text pairs and suggests expanding it for enhanced accuracy and reliability.
- Why unresolved: The paper does not provide experimental results or analysis on how varying dataset sizes affect IQAGPT's performance.
- What evidence would resolve it: Conducting experiments with CT-IQA datasets of varying sizes and comparing IQAGPT's performance metrics (e.g., BLEU, METEOR, accuracy) across these sizes would provide insights into the impact of dataset size on model performance.

### Open Question 2
- Question: Can IQAGPT be effectively integrated with other medical imaging modalities beyond CT, such as MRI or X-ray?
- Basis in paper: [explicit] The paper focuses on CT image quality assessment and does not explore other imaging modalities.
- Why unresolved: The paper does not provide evidence or analysis on the adaptability of IQAGPT to other medical imaging modalities.
- What evidence would resolve it: Developing and testing IQAGPT on datasets of other medical imaging modalities (e.g., MRI, X-ray) and comparing its performance metrics to those obtained from CT would determine its effectiveness across different imaging types.

### Open Question 3
- Question: How does IQAGPT handle real-time image quality assessment in clinical settings?
- Basis in paper: [inferred] The paper discusses the potential of IQAGPT in clinical settings but does not address real-time performance or deployment challenges.
- Why unresolved: The paper does not provide information on the computational efficiency, latency, or practical deployment of IQAGPT in real-time clinical environments.
- What evidence would resolve it: Conducting experiments to measure IQAGPT's processing time, resource usage, and accuracy in real-time clinical scenarios, along with feedback from radiologists using the system, would provide insights into its practical applicability.

## Limitations
- The effectiveness of cross-modal attention depends heavily on the quality of the pre-trained vision-language model's medical image-text alignment, which is not thoroughly evaluated.
- The prompt template conversion from numerical scores to text descriptions is a key innovation, but the specific template format and its coverage of all relevant quality dimensions are not disclosed.
- The reliance on ChatGPT for final score and report generation introduces potential variability, as the model's performance on medical quality assessment tasks is not independently validated within the study.

## Confidence

- **High Confidence**: The architectural framework of combining vision-language models with ChatGPT for IQA tasks is technically sound and feasible. The use of cross-modal attention for feature fusion is well-established in the literature.
- **Medium Confidence**: The reported improvements over baseline models (GPT-4, CLIP-IQA, and image-only classifiers) are promising, but the evaluation metrics and comparison methodology could benefit from additional validation on independent datasets.
- **Low Confidence**: The reliability of ChatGPT's interpretation of generated captions for producing accurate quality scores and reports remains uncertain without systematic error analysis or ablation studies of the prompt engineering approach.

## Next Checks

1. **Prompt Template Validation**: Conduct an ablation study where different prompt template formats are tested to determine their impact on caption quality and downstream score extraction accuracy from ChatGPT.

2. **Domain Generalization Test**: Evaluate IQAGPT's performance on CT images from different institutions or imaging protocols not represented in the training data to assess robustness to domain shift.

3. **Error Analysis of ChatGPT Integration**: Perform detailed error analysis on cases where IQAGPT's generated scores or reports diverge from radiologist annotations to identify failure patterns in the ChatGPT interpretation stage.