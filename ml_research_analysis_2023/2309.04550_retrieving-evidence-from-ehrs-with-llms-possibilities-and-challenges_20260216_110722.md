---
ver: rpa2
title: 'Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges'
arxiv_id: '2309.04550'
source_url: https://arxiv.org/abs/2309.04550
tags:
- evidence
- patient
- diagnosis
- llms
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose and evaluate a zero-shot approach using large language
  models (LLMs) to retrieve and summarize relevant evidence from unstructured electronic
  health record (EHR) data to support radiologists' diagnoses. Our method involves
  prompting an LLM to infer whether a patient has or is at risk of a condition, and
  if so, to summarize the supporting evidence.
---

# Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges

## Quick Facts
- **arXiv ID**: 2309.04550
- **Source URL**: https://arxiv.org/abs/2309.04550
- **Reference count**: 14
- **Primary result**: Zero-shot LLM approach outperforms retrieval baseline for extracting evidence from EHR notes, but struggles with hallucinations

## Executive Summary
This paper explores using large language models (LLMs) to retrieve and summarize evidence from unstructured electronic health record (EHR) data to support radiologists' diagnoses. The authors propose a zero-shot approach using Flan-T5 XXL with sequential prompting to first determine if a patient has or is at risk of a condition, then generate supporting evidence when applicable. Expert evaluation by radiologists found the LLM-based approach consistently outperformed a standard information retrieval baseline. However, a key challenge identified was the tendency of LLMs to hallucinate evidence not present in the patient record. The authors found that model confidence scores strongly correlate with faithful summaries, offering a practical method to limit such confabulations.

## Method Summary
The study employs a zero-shot sequential prompting approach using Flan-T5 XXL on unstructured EHR notes. The method involves two stages: first, asking the LLM whether a patient is at risk for or has a specific diagnosis; second, if the answer is affirmative, prompting the model to provide supporting evidence. The authors compare this approach against a ClinicalBERT-based retrieval baseline that extracts similar sentences from notes. Both methods are evaluated by radiologists on their ability to surface relevant evidence, with LLM outputs being preferred for their conciseness and precision. The study uses data from BWH (2010-2015) and MIMIC-III datasets, focusing on brain-related conditions in patients with at least 10 EHR notes.

## Key Results
- LLM-based evidence generation was consistently preferred over standard information retrieval baselines by expert radiologists
- Model confidence scores (self-consistency and likelihood) strongly correlate with faithful summaries, providing a practical means to detect hallucinations
- Abstractive LLM outputs provided more concise and clinically useful evidence compared to extractive snippets from retrieval methods
- The approach achieved high recall for binary decision-making about patient risk or diagnosis status

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sequential zero-shot prompting with Flan-T5 XXL enables effective conditional evidence extraction from unstructured EHR notes.
- **Mechanism**: The LLM first performs a binary classification ("at risk"/"has condition") then conditionally generates a concise summary of supporting evidence when the answer is affirmative. This two-step approach leverages the model's ability to interpret context and synthesize relevant information from free text.
- **Core assumption**: The LLM can accurately infer risk or presence of a condition from a single note without fine-tuning, and can generate faithful evidence summaries when prompted conditionally.
- **Evidence anchors**: [abstract] "We task an LLM to infer whether a patient has or is at risk of a particular condition; if so, we prompt the model to summarize the supporting evidence." [section 2] "First we ask the LLM whether a given note indicates that the corresponding patient is at risk for, or has a given query diagnosis—this prompts the LLM for a binary decision regarding these. When the answer is 'Yes', we prompt the model to provide support for its response."

### Mechanism 2
- **Claim**: Model confidence scores (self-consistency and likelihood) can detect hallucinations in generated evidence.
- **Mechanism**: When an LLM hallucinates, its outputs become inconsistent across multiple sampling runs, leading to low cross-evidence similarity. Similarly, low normalized likelihood under the language model indicates uncertainty about the generated content.
- **Core assumption**: Hallucinations manifest as high variance in model outputs when sampled repeatedly, and this variance is detectable through quantitative confidence metrics.
- **Evidence anchors**: [abstract] "model confidence in outputs strongly correlates with faithful summaries, offering a practical means to limit confabulations." [section 5.2] "We find that both methods provide confidence scores that are highly indicative of hallucinations; this can be seen in Figure 5(b)."

### Mechanism 3
- **Claim**: Abstractive LLM outputs provide more concise and clinically useful evidence than extractive retrieval methods.
- **Mechanism**: LLMs can synthesize relevant information from multiple sentences and present it in a condensed form, while retrieval methods are limited to extracting entire sentences that may contain irrelevant context.
- **Core assumption**: Clinicians prefer concise, synthesized evidence over lengthy extracted snippets that require additional parsing.
- **Evidence anchors**: [abstract] "we find that this LLM-based approach provides outputs consistently preferred to a standard information retrieval baseline" [section 5.4] "FLAN-T5 provided comparatively precise and concise output. Abstractive evidence was considered better than the extractive snippets from CBERT"

## Foundational Learning

- **Concept**: Zero-shot prompting in LLMs
  - Why needed here: The approach uses Flan-T5 XXL without fine-tuning, relying on carefully crafted prompts to perform clinical inference and evidence extraction tasks.
  - Quick check question: How does zero-shot prompting differ from few-shot prompting, and why was zero-shot chosen for this task?

- **Concept**: ClinicalBERT embeddings for similarity matching
  - Why needed here: ClinicalBERT embeddings are used to highlight potentially relevant sentences in notes and to measure cross-evidence similarity for hallucination detection.
  - Quick check question: What advantages does ClinicalBERT offer over general-purpose embeddings for processing clinical text?

- **Concept**: Self-consistency as a confidence measure
  - Why needed here: Self-consistency (measuring agreement across multiple sampled outputs) is used to estimate model confidence and detect hallucinations.
  - Quick check question: How does self-consistency differ from simple likelihood scoring, and why might both be useful for hallucination detection?

## Architecture Onboarding

- **Component map**: Query → LLM binary classification → Conditional evidence generation → Confidence scoring → Expert evaluation
- **Critical path**: Query → LLM binary classification → Conditional evidence generation → Confidence scoring → Expert evaluation
- **Design tradeoffs**: 
  - Abstractive generation vs. extractive precision: LLMs can synthesize concise evidence but risk hallucination
  - Confidence thresholding vs. coverage: High thresholds reduce hallucinations but may omit useful evidence
  - Single note vs. multi-note processing: Current approach processes notes individually for simplicity

- **Failure signatures**:
  - High confidence scores with expert-rated "not useful" evidence indicates systematic bias
  - Low self-consistency with high likelihood suggests confident hallucinations
  - Long evaluation times (>30 seconds per evidence) indicate poor relevance or difficulty finding supporting text

- **First 3 experiments**:
  1. Test the sequential prompting approach on a small set of synthetic clinical notes with known diagnoses to verify basic functionality
  2. Compare confidence score distributions (self-consistency vs. likelihood) on evidence known to be hallucinated vs. faithful
  3. Evaluate the precision-recall tradeoff by varying confidence thresholds on a validation set and measuring expert preference rates

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can hallucinations in LLM-generated evidence be reliably detected and prevented?
- **Basis in paper**: Explicit - The paper highlights hallucinations as a key challenge, where LLMs generate plausible but fabricated evidence not present in the patient record.
- **Why unresolved**: While the paper suggests that model confidence scores correlate with faithful summaries, this is not a definitive solution. Further research is needed to develop robust methods for identifying and mitigating hallucinations.
- **What evidence would resolve it**: Experiments comparing different hallucination detection methods (e.g., cross-evidence similarity, self-consistency) and their effectiveness in reducing hallucinations while maintaining useful evidence retrieval.

### Open Question 2
- **Question**: How can the trade-off between abstractive evidence generation and faithfulness to the source be optimized?
- **Basis in paper**: Inferred - The paper notes that abstraction is a major benefit of LLMs but also a source of potential hallucinations. It mentions that heuristic matching to filter outputs may incorrectly flag accurate evidence as "hallucinated" if the generation is abstractive.
- **Why unresolved**: Balancing the benefits of abstraction with the need for faithfulness is a complex challenge that requires further investigation into how to control the level of abstraction in generated evidence.
- **What evidence would resolve it**: Studies evaluating different levels of abstraction in generated evidence and their impact on both usefulness and faithfulness, potentially leading to methods for controlling abstraction.

### Open Question 3
- **Question**: How can the performance of LLMs in retrieving weakly correlating evidence be improved?
- **Basis in paper**: Explicit - The paper discusses weakly correlating evidence, where the LLM surfaces information that may have a plausible but weak correlation with the query condition from an individual patient perspective.
- **Why unresolved**: It's unclear how to distinguish between weakly correlating evidence that is still useful and irrelevant information, especially in the context of individual patient care.
- **What evidence would resolve it**: Research into methods for weighting or contextualizing evidence based on its strength of correlation with the query condition, potentially incorporating clinical expertise or patient-specific factors.

### Open Question 4
- **Question**: How can the evaluation of LLM outputs be scaled up beyond manual expert annotation?
- **Basis in paper**: Explicit - The paper acknowledges that manual evaluation by radiologists is expensive and time-consuming, limiting the scale of evaluation.
- **Why unresolved**: While the paper proposes using an LLM to evaluate other LLM outputs, the effectiveness and reliability of this approach need to be further validated and improved.
- **What evidence would resolve it**: Comparative studies evaluating the agreement between LLM-evaluated outputs and expert-annotated outputs, as well as exploring different evaluation metrics and methodologies for LLM-generated evidence.

## Limitations
- The study relies on expert evaluation from a small group of radiologists (n=5), limiting generalizability
- The zero-shot prompting approach has not been tested across diverse clinical scenarios or disease types
- Confidence-based hallucination detection requires further validation to determine optimal threshold settings for clinical deployment

## Confidence
- **High confidence**: The finding that LLM-generated evidence is preferred over standard retrieval baselines (p < 0.05 in pairwise comparisons)
- **Medium confidence**: The correlation between confidence scores and hallucination detection (R² = 0.6-0.8 reported, but limited sample size)
- **Low confidence**: The generalizability of results across different EHR systems and clinical specialties beyond the tested brain-related conditions

## Next Checks
1. **Scale testing**: Validate confidence-based hallucination detection across a larger, more diverse dataset (n > 1000 patients, multiple specialties) to confirm correlation strength and optimal threshold values.
2. **Clinical workflow integration**: Test the sequential prompting approach in a simulated clinical environment where radiologists must complete diagnostic tasks with time constraints, measuring both accuracy and efficiency gains.
3. **Model comparison**: Compare Flan-T5 XXL against other large language models (e.g., GPT-4, Claude) using the same zero-shot prompting framework to assess whether results are model-specific or generalizable across architectures.