---
ver: rpa2
title: Stable and low-precision training for large-scale vision-language models
arxiv_id: '2304.13013'
source_url: https://arxiv.org/abs/2304.13013
tags:
- loss
- training
- spike
- int8
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces two techniques to improve training of large
  language-vision models. For faster training, they propose SwitchBack, an 8-bit linear
  layer that matches 16-bit performance while providing 13-25% speedups for CLIP ViT-Huge
  training.
---

# Stable and low-precision training for large-scale vision-language models

## Quick Facts
- arXiv ID: 2304.13013
- Source URL: https://arxiv.org/abs/2304.13013
- Authors: 
- Reference count: 40
- Primary result: SwitchBack achieves 8-bit training with 13-25% speedups while maintaining 16-bit accuracy; StableAdamW eliminates loss spikes in large-scale CLIP training

## Executive Summary
This paper addresses two critical challenges in training large-scale vision-language models: improving training speed through low-precision computation and stabilizing training by eliminating catastrophic loss spikes. The authors introduce SwitchBack, an 8-bit linear layer that selectively uses higher precision for weight gradient computation, achieving significant speedups while preserving accuracy. They also analyze the root cause of loss spikes in CLIP training and propose StableAdamW, a hybrid optimizer that combines AdamW with Adafactor-style update clipping to prevent these instabilities.

## Method Summary
The paper proposes two complementary techniques for efficient and stable training of large vision-language models. SwitchBack is an 8-bit linear layer that uses int8 matrix multiplication for forward and input gradient computations but switches to 16-bit for weight gradients, addressing quantization noise issues in large batch training. StableAdamW is an AdamW-Adafactor hybrid that detects and prevents loss spikes by monitoring the ratio of squared gradients to their moving average and applying adaptive update clipping when this ratio exceeds a threshold.

## Key Results
- SwitchBack achieves 13-25% training speedups for CLIP ViT-Huge while maintaining 16-bit accuracy
- StableAdamW eliminates loss spikes that occur 1-8 iterations after AdamW's second moment estimator becomes outdated
- Layer-scale initialization with zeros effectively controls feature magnitudes during low-precision training
- SwitchBack outperforms uniform quantization approaches and is more memory-efficient than alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Loss spikes occur when AdamW's second moment estimator becomes outdated in the patch embedding layer
- Mechanism: When learning signals change abruptly, the exponential moving average of squared gradients no longer represents true gradient magnitudes, causing per-parameter updates that are scaled too aggressively
- Core assumption: Patch embedding layer is uniquely sensitive to second moment estimator lag
- Evidence anchors:
  - [abstract] "loss spikes consistently occur 1-8 iterations after the squared gradients become under-estimated"
  - [section] "loss spikes can be predicted by examining this ratio of the squared gradients to their moving average"
- Break condition: β2 set too low causes convergence slowdown; too high causes severe estimator lag

### Mechanism 2
- Claim: SwitchBack achieves 8-bit performance by selective higher precision for weight gradients
- Mechanism: Quantization noise variance increases with matrix inner dimension; weight gradient computation (batch size × sequence length) is most susceptible
- Core assumption: Weight gradient computation is primary quantization noise bottleneck
- Evidence anchors:
  - [abstract] "SwitchBack uses 8-bit precision matrix multiplication for the weight gradient computation"
  - [section] "variance due to quantization increases with the inner dimension"
- Break condition: Smaller batch sizes or sequence lengths reduce SwitchBack's advantage

### Mechanism 3
- Claim: High feature magnitudes cause 8-bit training instability, mitigated by layer-scale initialization with zeros
- Mechanism: Large feature magnitudes increase quantization dynamic range, leading to greater error; zero initialization constrains magnitudes
- Core assumption: Feature magnitude growth correlates with quantization instability
- Evidence anchors:
  - [abstract] "standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged"
  - [section] "average feature magnitude becomes large for later blocks" without intervention
- Break condition: Layer-scale scaling factors growing too large during training

## Foundational Learning

- Concept: Exponential moving averages in adaptive optimizers
  - Why needed here: Understanding how AdamW's second moment estimator can become outdated is crucial for diagnosing and fixing loss spikes
  - Quick check question: What happens to the AdamW update when the true gradient magnitude suddenly increases but the moving average hasn't caught up yet?

- Concept: Quantization error variance in matrix multiplications
  - Why needed here: The core insight behind SwitchBack's selective precision approach relies on understanding how quantization noise scales with matrix dimensions
  - Quick check question: How does the variance of quantization error change as the inner dimension of a matrix multiplication increases?

- Concept: Feature normalization and scaling in transformers
  - Why needed here: Layer-scale initialization with zeros is a key technique for controlling feature magnitudes during low-precision training
  - Quick check question: Why does initializing layer-scale parameters to zero create an identity function at the start of training?

## Architecture Onboarding

- Component map: SwitchBack replaces standard linear layers in transformer attention blocks (key, query, value, out projections) and MLPs
- Critical path: Forward pass → quantization of inputs → int8 matmul → dequantization → output. Backward pass → quantization of gradients → int8 matmul for input gradients → 16-bit matmul for weight gradients
- Design tradeoffs: SwitchBack trades memory (storing quantization states) for speed and accuracy
- Failure signatures: Loss spikes correlated with RMS spikes in patch embedding layer indicate outdated second moment estimator
- First 3 experiments:
  1. Compare loss curves with and without SwitchBack on a small CLIP model to verify accuracy preservation
  2. Measure speedups of SwitchBack vs standard linear layers across different batch sizes and dimensions
  3. Test different β2 values in AdamW to observe the relationship between second moment estimator lag and loss spikes

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several remain unexplored based on the presented work.

## Limitations
- The SwitchBack technique's performance may degrade for smaller models or different architectures beyond CLIP
- Implementation complexity requires careful management of quantization states and may introduce difficult-to-diagnose bugs
- Layer-scale initialization with zeros may not be optimal compared to alternative normalization approaches

## Confidence
- High confidence: The characterization of loss spikes occurring 1-8 iterations after second moment underestimation
- Medium confidence: The effectiveness of StableAdamW as a general solution for loss spikes across different model scales
- Low confidence: The claim that layer-scale initialization with zeros is the optimal approach for controlling feature magnitudes

## Next Checks
1. Ablation study of β2 values: Systematically vary β2 across a wider range and measure the relationship between second moment estimator lag, RMSt ratios, and loss spike frequency
2. Cross-architecture evaluation: Test SwitchBack on non-ViT architectures and smaller CLIP variants to validate generalization
3. Alternative normalization comparison: Compare layer-scale initialization with zeros against other normalization approaches in low-precision training