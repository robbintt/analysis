---
ver: rpa2
title: 'Eva: A General Vectorized Approximation Framework for Second-order Optimization'
arxiv_id: '2308.02123'
source_url: https://arxiv.org/abs/2308.02123
tags:
- k-fac
- second-order
- shampoo
- training
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Eva, a memory- and time-efficient second-order
  optimization algorithm for deep learning. Eva constructs the second-order information
  using Kronecker factorization of small stochastic vectors over a mini-batch, reducing
  memory consumption to sublinear complexity.
---

# Eva: A General Vectorized Approximation Framework for Second-order Optimization

## Quick Facts
- arXiv ID: 2308.02123
- Source URL: https://arxiv.org/abs/2308.02123
- Reference count: 40
- Primary result: Eva achieves 2.05× speedup over SGD and 2.42× speedup over K-FAC/Shampoo with similar or better generalization using significantly less memory

## Executive Summary
This paper introduces Eva, a memory- and time-efficient second-order optimization algorithm for deep learning. Eva constructs the second-order information using Kronecker factorization of small stochastic vectors over a mini-batch, reducing memory consumption to sublinear complexity. It also derives an efficient update formula without explicitly computing matrix inverses using the Sherman-Morrison formula, achieving linear time complexity. Eva generalizes to a vectorized approximation framework, improving the efficiency of existing second-order methods (FOOF and Shampoo) without sacrificing convergence performance. Experimental results demonstrate that Eva reduces end-to-end training time by up to 2.05× and 2.42× compared to first-order SGD and second-order algorithms (K-FAC and Shampoo), respectively, while using significantly less memory and computation per iteration.

## Method Summary
Eva is a second-order optimization algorithm that constructs the second-order information with the Kronecker factorization of small stochastic vectors over a mini-batch of training data. It reduces memory consumption from quadratic to sublinear by approximating the second-order matrix with Kronecker vectors (KVs) instead of full Kronecker factors. Eva achieves linear time complexity per iteration by avoiding explicit matrix inversion using the Sherman-Morrison formula for rank-one updates. The algorithm generalizes to a vectorized approximation framework that improves the efficiency of existing second-order methods (FOOF and Shampoo) by replacing their Kronecker factors with the corresponding KVs. Eva integrates with standard SGD as a preconditioner, performing distributed all-reduce of KVs across workers and applying KL normalization/clipping to prevent exploding gradients.

## Key Results
- Reduces end-to-end training time by up to 2.05× compared to SGD
- Achieves 2.42× speedup over K-FAC and Shampoo while maintaining or improving generalization
- Reduces memory consumption to sublinear complexity compared to quadratic memory usage of traditional second-order methods
- Successfully extends to vectorized approximations of FOOF and Shampoo without sacrificing convergence performance

## Why This Works (Mechanism)

### Mechanism 1
Eva reduces memory consumption from quadratic to sublinear by approximating the second-order matrix with Kronecker vectors (KVs). Instead of storing full Kronecker factors (KFs) as in K-FAC, FOOF, or Shampoo, Eva computes KVs as averages over mini-batch columns and constructs the second-order matrix via their Kronecker product. The core assumption is that the Kronecker product of two rank-one matrices formed from averaged activations and gradients preserves sufficient curvature information for effective preconditioning. Break condition: If the averaged vectors do not capture the variability of activations/gradients across the batch, the approximation fails and convergence degrades.

### Mechanism 2
Eva achieves linear time complexity per iteration by avoiding explicit matrix inversion using the Sherman-Morrison formula. Since the constructed second-order matrix Cl is rank-one, Eva applies Sherman-Morrison to compute (Cl + γI)^{-1} in O(d) time instead of O(d³). The core assumption is that the damping term γ ensures numerical stability and that Cl + γI remains invertible for all layers. Break condition: If the damping parameter γ is too small, the Sherman-Morrison formula becomes numerically unstable and the update diverges.

### Mechanism 3
Eva generalizes to a vectorized approximation framework that improves FOOF and Shampoo without sacrificing convergence. Eva replaces each Kronecker factor in FOOF and Shampoo with its corresponding KV, preserving the preconditioning structure but with lower memory/time costs. The core assumption is that the curvature approximation quality of KFs is sufficiently maintained when replaced by their averaged column vectors. Break condition: If the KV approximation loses too much rank information, the preconditioning becomes ineffective and convergence slows to first-order levels.

## Foundational Learning

- Concept: Kronecker product and its properties
  - Why needed here: Eva relies on Kronecker factorization of small matrices/vectors to approximate the full curvature matrix; understanding ⊗ is essential for the derivation and implementation
  - Quick check question: Given A ∈ ℝ^{m×m} and B ∈ ℝ^{n×n}, what is the dimension of A ⊗ B and how is (A ⊗ B)g computed?

- Concept: Sherman-Morrison formula for rank-one updates
  - Why needed here: Eva uses Sherman-Morrison to compute the inverse of a rank-one perturbed identity matrix efficiently; this is the key to linear-time preconditioning
  - Quick check question: For invertible A and vectors u, v, write the Sherman-Morrison formula for (A + uv^T)^{-1} and explain each term

- Concept: Second-order optimization and trust-region interpretation
  - Why needed here: Eva can be understood as a trust-region method with a specific proximal function; this conceptual link explains why it converges faster than first-order methods
  - Quick check question: In trust-region optimization, how does the choice of proximal function ρ(w, w(t)) = Δw^T C Δw relate to the Newton update Δw = C^{-1}g?

## Architecture Onboarding

- Component map: Forward pass captures activations Al -> Backward pass captures pre-activation gradients Bl -> KV construction computes mean-col(Al), mean-col(Bl) -> Distributed KV aggregation via all-reduce -> Preconditioning applies Eva update formula -> KL normalization/clipping if enabled -> Parameter update with SGD step

- Critical path:
  1. Forward pass: capture activations Al
  2. Backward pass: capture pre-activation gradients Bl
  3. KV construction: compute mean-col(Al), mean-col(Bl)
  4. Distributed KV aggregation: all-reduce KVs
  5. Preconditioning: apply Eva update formula to gradients
  6. Gradient clipping: KL normalization if enabled
  7. Parameter update: standard SGD step with preconditioned gradients

- Design tradeoffs:
  - Memory vs. accuracy: using rank-one KV approximation reduces memory but may lose some curvature detail compared to full KFs
  - Update frequency vs. stale information: Eva updates KVs every iteration (no interval) for freshness but at higher communication cost
  - Damping parameter: larger γ increases stability but reduces preconditioning strength

- Failure signatures:
  - Divergence: often caused by insufficient damping or exploding preconditioned gradients (check KL size)
  - Slow convergence: may indicate KVs are poor approximations of true curvature (monitor training loss vs. K-FAC)
  - High memory usage: unexpected; likely due to storing full gradients or improper KV cleanup

- First 3 experiments:
  1. Validate Eva vs. SGD on a small autoencoder (8-layer) on MNIST/FMNIST; measure convergence speed and final loss
  2. Compare memory usage of Eva vs. K-FAC on ResNet-110 training; profile GPU memory consumption
  3. Test distributed Eva with increasing batch sizes on ImageNet; measure throughput and convergence stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical convergence rate of Eva approximations for K-FAC, FOOF, and Shampoo algorithms?
- Basis in paper: The authors explicitly state that "there lacks a solid theoretical analysis on the convergence rate of Eva approximations for K-FAC, FOOF, and Shampoo algorithms."
- Why unresolved: Theoretical analysis of convergence rates for these specific approximations has not been conducted
- What evidence would resolve it: A rigorous mathematical proof demonstrating the convergence rate bounds for Eva approximations in relation to the original algorithms

### Open Question 2
- Question: How can second-order optimization methods be effectively combined with novel training tricks beyond CutMix and AutoAugment?
- Basis in paper: The authors mention that "most training tricks were initially proposed for first-order algorithms" and suggest designing "novel second-order friendly strategies for achieving possibly better performance."
- Why unresolved: Existing training tricks are primarily designed and tuned for first-order optimizers, and their impact on second-order methods is not well understood
- What evidence would resolve it: Experimental results demonstrating improved performance of second-order methods when combined with novel, specifically designed training strategies

### Open Question 3
- Question: How effective is Eva on pre-training large language models compared to other optimization methods?
- Basis in paper: The authors state they will "conduct more experiments to show the effectiveness of Eva on other applications such as pre-training large language models."
- Why unresolved: The paper primarily focuses on image classification tasks and does not provide extensive results for language model pre-training
- What evidence would resolve it: Experimental results comparing Eva's performance against other optimizers (e.g., AdamW) on pre-training large language models, including convergence speed and final model quality metrics

## Limitations

- The paper lacks theoretical analysis on convergence rates for Eva approximations of K-FAC, FOOF, and Shampoo algorithms
- Memory consumption measurements are not detailed enough to verify sublinear scaling claims
- Limited empirical comparison with FOOF and Shampoo beyond ablation studies
- No extensive results on applications beyond image classification, such as pre-training large language models

## Confidence

- Memory reduction claim: High confidence (mathematical derivation of Sherman-Morrison formula application)
- Computational efficiency claim: High confidence (linear time complexity derivation)
- Generalization to FOOF/Shampoo: Medium confidence (limited empirical comparison)
- GPU memory measurements: Low confidence (lack of detailed profiling methodology)
- 2.05× speedup over SGD: Medium confidence (no control for implementation differences)

## Next Checks

1. Profile GPU memory consumption during training to verify the sublinear memory scaling claim, measuring peak memory usage for Eva vs K-FAC across different batch sizes and model depths
2. Implement and test Eva-f and Eva-s on the same workloads as FOOF and Shampoo to validate the vectorized approximation framework's performance claims
3. Conduct ablation studies varying the damping parameter γ systematically to establish the stability/robustness trade-off and identify optimal ranges for different model architectures