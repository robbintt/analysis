---
ver: rpa2
title: 'To grok or not to grok: Disentangling generalization and memorization on corrupted
  algorithmic datasets'
arxiv_id: '2310.13061'
source_url: https://arxiv.org/abs/2310.13061
tags:
- network
- inversion
- training
- figure
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the interplay between generalization and
  memorization in neural networks, specifically focusing on the challenge of label
  corruption. The authors explore two-layer neural networks trained on modular arithmetic
  tasks where a fraction of labels are intentionally corrupted.
---

# To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets

## Quick Facts
- **arXiv ID**: 2310.13061
- **Source URL**: https://arxiv.org/abs/2310.13061
- **Reference count**: 40
- **Key outcome**: Networks can simultaneously memorize corrupted labels and generalize to uncorrupted test data when trained on modular arithmetic tasks with proper regularization.

## Executive Summary
This study investigates how neural networks handle the dual challenge of memorizing corrupted training data while maintaining generalization to uncorrupted test data. Using modular arithmetic tasks with synthetic label corruption, the authors demonstrate that networks can achieve 100% accuracy on both corrupted training data and uncorrupted test data through a phenomenon they call "coexistence." They show that regularization methods like weight decay, dropout, and BatchNorm force networks to distinguish between generalizing and memorizing representations, with weight decay additionally enabling a "full inversion" phase where test accuracy exceeds train accuracy on corrupted data.

## Method Summary
The study uses two-layer neural networks (MLPs and Transformers) trained on modular arithmetic tasks where a fraction of labels are corrupted. The authors analyze network behavior through inverse participation ratio (IPR) characterization of neurons, identifying generalizing neurons (high IPR) versus memorizing neurons (low IPR). They systematically apply regularization techniques including weight decay, dropout, and BatchNorm, monitoring phase transitions between coexistence, full inversion, and memorization phases. The training uses full-batch AdamW optimizer with MSE loss, and analysis includes neuron pruning to isolate the effects of different representations.

## Key Results
- Networks can simultaneously achieve 100% accuracy on both corrupted training data and uncorrupted test data (coexistence phase)
- Weight decay regularization forces networks to ignore corrupted data and focus on generalizing representations
- BatchNorm de-amplifies the output of memorizing neurons while preserving generalizing ones through learned scaling parameters
- The forgetting phase only occurs with quadratic activation functions and adaptive optimizers like Adam

## Why This Works (Mechanism)

### Mechanism 1
Neural networks can simultaneously memorize corrupted labels and generalize to uncorrupted test data when trained on modular arithmetic tasks. The network learns two distinct sets of representations: generalizing neurons that capture the underlying modular arithmetic rule and memorizing neurons that store corrupted label information. These representations coexist in the same network without interfering. This works because modular arithmetic tasks provide interpretable representations that can be analytically characterized, allowing clear distinction between generalizing and memorizing neurons. Break condition: If the task becomes too complex for analytical characterization of representations, or if corruption fraction exceeds the network's capacity to maintain separate representations.

### Mechanism 2
Regularization methods like weight decay, dropout, and BatchNorm force the network to ignore corrupted data and focus on generalizing representations. Weight decay and dropout convert memorizing neurons into generalizing ones by penalizing complex weight configurations, while BatchNorm de-amplifies the output of memorizing neurons through learned scaling parameters without eliminating them. This works because regularization affects the IPR distribution of neurons, shifting it towards higher values for generalizing representations. Break condition: If regularization strength is too high, it may eliminate both memorizing and generalizing representations, leading to poor performance on both train and test data.

### Mechanism 3
Training dynamics involve two stages: initial grokking to high accuracy, followed by unlearning of memorizing representations. After initial memorization of corrupted data, regularization gradually reduces the influence of memorizing neurons while preserving generalizing ones, leading to improved test accuracy at the cost of train accuracy. This works because the presence of regularization creates a two-stage optimization process where the network first achieves high accuracy through memorization, then refines to focus on generalizing representations. Break condition: If the training process is interrupted before the second stage completes, the network may retain both generalizing and memorizing representations, leading to suboptimal performance.

## Foundational Learning

- **Concept**: Modular arithmetic operations (e.g., (m + n) % p)
  - Why needed here: The paper uses modular addition as a test task where the underlying rule is analytically tractable, allowing clear distinction between rule-based generalization and memorization.
  - Quick check question: What is the result of (7 + 5) % 4?

- **Concept**: Inverse Participation Ratio (IPR) for characterizing neuron representations
  - Why needed here: IPR quantifies the periodicity of weight vectors, allowing identification of generalizing (high IPR) versus memorizing (low IPR) neurons.
  - Quick check question: If a weight vector has highly localized Fourier components, what does this indicate about its IPR value?

- **Concept**: Grokking phenomenon in neural network training
  - Why needed here: The paper observes grokking behavior where networks generalize well after initially overfitting, particularly in the presence of label corruption.
  - Quick check question: What distinguishes grokking from typical overfitting-recovery patterns?

## Architecture Onboarding

- **Component map**: Input (one-hot encoded m, n) -> Two-layer MLP with quadratic activation -> Regularization (weight decay, dropout, BatchNorm) -> MSE loss -> Output (one-hot encoded result)

- **Critical path**: 
  1. Generate modular arithmetic dataset with corrupted labels
  2. Train network with chosen regularization
  3. Monitor IPR distribution and accuracy metrics
  4. Analyze phase behavior (Coexistence, Inversion, Memorization)

- **Design tradeoffs**:
  - Width vs. generalization: Wider networks may memorize more but also generalize better
  - Regularization strength vs. inversion: Higher regularization forces more inversion but may reduce overall accuracy
  - Batch size vs. implicit regularization: Smaller batches provide more implicit regularization

- **Failure signatures**:
  - Forgetting phase: Weight norms collapse to zero, both train and test accuracy drop
  - Confusion phase: Network fails to memorize or generalize, random guessing accuracy
  - No inversion: Test accuracy not higher than train accuracy despite regularization

- **First 3 experiments**:
  1. Train with no regularization on 50% corrupted data, observe Coexistence phase
  2. Add weight decay regularization, observe transition to Full Inversion phase
  3. Apply BatchNorm, analyze correlation between BatchNorm weights and IPR values

## Open Questions the Paper Calls Out

### Open Question 1
How does the training dynamics lead to the periodic weight solutions that generalize well in modular arithmetic tasks? The paper mentions that the training dynamics leading to the periodic weight solutions are not fully understood and suggests solving the dynamics could shed light on the effects of regularization and label corruption. This remains unresolved because while trained models are interpretable due to analytical solutions, the training dynamics that lead to these solutions remain an open question. A detailed mathematical analysis of the training dynamics, potentially using techniques from dynamical systems theory, could provide insights into how the network evolves towards the periodic weight solutions.

### Open Question 2
What is the precise mechanism by which BatchNorm de-amplifies the output of memorizing neurons and amplifies the output of generalizing neurons? The paper states that BatchNorm does not significantly affect the IPR distribution but adjusts its own weights to de-amplify low IPR (memorizing) neurons and amplify high IPR (generalizing) neurons. This remains unresolved because the paper provides empirical evidence of this effect but does not offer a theoretical explanation for why BatchNorm behaves this way. A theoretical analysis of the BatchNorm layer's interaction with the network's hidden representations, potentially using techniques from information theory, could elucidate the mechanism behind this behavior.

### Open Question 3
Why does the Forgetting phase only occur with activation functions of degree higher than 1 and adaptive optimizers? The paper mentions that the Forgetting phase, where the network loses performance due to steadily decreasing weight norms, only occurs with quadratic activation functions and adaptive optimizers like Adam. This remains unresolved because the paper provides a heuristic explanation based on the gradients' dependence on the weights, but a complete understanding of this phenomenon is lacking. A detailed mathematical analysis of the optimization landscape and the interplay between the activation function, optimizer, and weight decay could provide a more rigorous explanation for the occurrence of the Forgetting phase.

## Limitations

- The study focuses on synthetic modular arithmetic tasks which may not generalize to complex real-world datasets
- Results are primarily based on two-layer networks, leaving questions about deeper architectures
- The analysis examines relatively small network widths (up to 1024 neurons), raising questions about scalability to larger models

## Confidence

**High confidence**: The coexistence of memorization and generalization in corrupted data settings - Well-supported by analytical characterization and empirical phase diagrams.

**Medium confidence**: The mechanism by which regularization forces networks to focus on generalizing representations - Compelling evidence through IPR analysis, but exact mechanistic details could benefit from additional validation.

**Low confidence**: The generalizability of findings to real-world datasets and deeper architectures - Acknowledged limitation with limited empirical validation beyond modular arithmetic setting.

## Next Checks

1. Test the coexistence mechanism on real datasets: Apply the same analysis framework (IPR-based neuron characterization, phase diagram analysis) to a natural dataset like CIFAR-10 with synthetic label corruption to verify whether the simultaneous memorization-generalization phenomenon persists.

2. Validate regularization mechanisms in deeper networks: Extend the modular arithmetic experiments to 4-6 layer MLPs and transformers to determine whether weight decay, dropout, and BatchNorm maintain their phase-transition behavior and IPR-based mechanisms.

3. Characterize the forgetting phase boundary: Systematically vary regularization strength and network width to map out the precise conditions under which the forgetting phase (weight norm collapse) emerges, and determine whether this represents a fundamental limitation of the approach.