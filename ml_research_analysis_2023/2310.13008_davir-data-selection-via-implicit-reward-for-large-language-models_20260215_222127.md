---
ver: rpa2
title: 'DavIR: Data Selection via Implicit Reward for Large Language Models'
arxiv_id: '2310.13008'
source_url: https://arxiv.org/abs/2310.13008
tags:
- data
- dataset
- fine-tuning
- selected
- lini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DavIR, a model-based data selection method
  for post-training Large Language Models. The core idea is to quantify the learnability
  of data points by measuring the relative reduction in loss during fine-tuning, a
  metric closely related to the implicit reward model described in Direct Preference
  Optimization (DPO).
---

# DavIR: Data Selection via Implicit Reward for Large Language Models

## Quick Facts
- **arXiv ID**: 2310.13008
- **Source URL**: https://arxiv.org/abs/2310.13008
- **Reference count**: 7
- **Primary result**: 6% of Alpaca dataset selected with DavIR can steer LLaMA and Gemma models to superior performance compared to full 52K dataset training

## Executive Summary
This paper proposes DavIR, a model-based data selection method for post-training Large Language Models that quantifies learnability by measuring relative loss reduction during fine-tuning. The method generalizes Reducible Holdout Loss to core-set selection for causal language modeling and shows that aggressive data compression (6% of full dataset) can achieve superior performance while reducing training costs. The approach also demonstrates effectiveness in balancing capabilities across different domains through data mixing and improves alignment performance when applied to DPO objectives.

## Method Summary
DavIR quantifies the learnability of data points by measuring the relative reduction in loss during fine-tuning, using a reference model fine-tuned on the full dataset. The learnability score S(xi, yi) = Lini(xi, yi) - Lref(xi, yi) captures data points that are challenging for the pre-trained model but learnable after fine-tuning. The method includes a normalization term to address response length bias and can be applied to both SFT and alignment tasks. The selected subset is then used for final model training, achieving comparable or superior performance to full-dataset fine-tuning.

## Key Results
- 6% of Alpaca dataset selected with DavIR achieves superior performance compared to full 52K dataset training for both LLaMA and Gemma model families
- Alpaca dataset compressed with DavIR can be combined with GSM8K to effectively balance open-domain QA and mathematical reasoning capabilities
- Applying DavIR objective to DPO and developing normalized DavIR-DPO improves Zephyr-7B-SFT alignment performance by 8% (relative) on AlpacaEval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The learnability score S(xi, yi) = Lini(xi, yi) - Lref(xi, yi) captures data points that the model can learn effectively.
- Mechanism: Data points with high learnability have low loss for the fine-tuned model but high loss for the pre-trained model, indicating they are challenging for the pre-trained model but learnable after fine-tuning.
- Core assumption: Loss reduction during fine-tuning correlates with actual learning gain.
- Evidence anchors: [abstract] and [section 3.3] provide direct support, while [corpus] offers weak evidence from related work.

### Mechanism 2
- Claim: Normalization by Lini(xi, yi) prevents bias toward longer responses in the scoring function.
- Mechanism: Without normalization, longer responses artificially inflate loss values, leading to selection bias. Dividing by Lini corrects this scale issue.
- Core assumption: Response length directly affects loss magnitude in the same way across all data points.
- Evidence anchors: [section 3.4] and [appendix A.1] describe the normalization approach and its motivation, though [corpus] provides no direct evidence.

### Mechanism 3
- Claim: The 6% data selection achieves comparable results because it captures the most learnable data points while excluding redundant or unlearnable ones.
- Mechanism: By focusing on data points with the largest loss reduction potential, the method filters out both already-mastered tasks (low Lini, low Lref) and too-difficult tasks (high Lini, high Lref), keeping only the sweet spot.
- Core assumption: The dataset contains a sufficient number of highly learnable data points to achieve good performance.
- Evidence anchors: [abstract] and [section 4.2] provide direct support, while [corpus] offers weak evidence from related work.

## Foundational Learning

- **Concept: Loss function as learning signal**
  - Why needed here: The entire selection mechanism relies on comparing loss values between pre-trained and fine-tuned models
  - Quick check question: Why does the method use cross-entropy loss specifically rather than another loss function?

- **Concept: Model capability gap measurement**
  - Why needed here: The learnability score measures the difference between what a model can and cannot do
  - Quick check question: How would you define "capability gap" in terms of loss values?

- **Concept: Data distribution and sampling**
  - Why needed here: Understanding how the 6% selection maintains representativeness despite aggressive reduction
  - Quick check question: What statistical properties should the selected subset maintain to preserve model performance?

## Architecture Onboarding

- **Component map**: Pretrained model (Mini) -> Fine-tuned reference model (Mref) -> Loss computation module -> Scoring function -> Selection module

- **Critical path**:
  1. Fine-tune pre-trained model on full dataset to create reference model
  2. Compute losses for all data points using both models
  3. Calculate learnability scores
  4. Select top-K data points
  5. Use selected data for final model training

- **Design tradeoffs**:
  - Memory vs. accuracy: Computing losses for all data points requires significant memory but ensures comprehensive selection
  - Selection size vs. performance: Smaller K reduces training cost but may miss important data points
  - Normalization choice: Using Lini vs Lref as denominator affects score distribution but not ranking

- **Failure signatures**:
  - All scores clustered near zero: Dataset too easy or too hard for the model
  - Scores perfectly correlated with response length: Normalization not working correctly
  - Top-K selection underperforms random selection: Learnability signal not meaningful for this dataset

- **First 3 experiments**:
  1. Verify loss reduction correlation: Train reference model, plot Lini vs Lref for selected vs non-selected data
  2. Test normalization effect: Compare score distributions with and without normalization
  3. Ablation on K value: Test performance across different selection percentages (1%, 3%, 6%, 10%)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the learnability-based data selection method (LoBaSS) perform when applied to different model architectures beyond LLaMA?
- Basis in paper: [inferred] The paper primarily focuses on LLaMA models, but the method's scalability is mentioned as a point of exploration.
- Why unresolved: The paper does not provide empirical results on the application of LoBaSS to other model architectures, leaving its generalizability unclear.
- What evidence would resolve it: Experimental results showing the performance of LoBaSS on a variety of model architectures, such as GPT, BERT, or other transformer-based models, would provide insight into its generalizability.

### Open Question 2
- Question: What is the impact of different loss functions on the effectiveness of the LoBaSS method?
- Basis in paper: [explicit] The paper uses cross-entropy loss as the loss function, but does not explore other potential loss functions.
- Why unresolved: The choice of loss function may influence the selection of learnable data points, and the paper does not investigate alternative loss functions.
- What evidence would resolve it: Comparative experiments using different loss functions (e.g., focal loss, label smoothing) in the LoBaSS method would clarify the impact of loss function choice on data selection.

### Open Question 3
- Question: How does the normalization term in the LoBaSS method affect the selection of data points with varying lengths of responses?
- Basis in paper: [explicit] The paper introduces a normalization term to address potential issues with data points having long responses, but does not provide a detailed analysis of its impact.
- Why unresolved: The normalization term's effect on data selection for responses of different lengths is not thoroughly explored, leaving questions about its efficacy.
- What evidence would resolve it: An in-depth analysis of the data points selected with and without the normalization term, categorized by response length, would elucidate its impact.

### Open Question 4
- Question: What is the relationship between the learnability of data points and the model's performance on specific downstream tasks?
- Basis in paper: [inferred] The paper demonstrates improved performance on general tasks but does not investigate the correlation between data learnability and task-specific performance.
- Why unresolved: The paper does not establish a direct link between the learnability of data points and performance on specific tasks, such as code generation or question answering.
- What evidence would resolve it: Experiments correlating the learnability scores of data points with model performance on a range of downstream tasks would clarify this relationship.

### Open Question 5
- Question: How does the LoBaSS method perform in scenarios with imbalanced datasets, where certain categories of data are underrepresented?
- Basis in paper: [inferred] The paper discusses data mixing but does not explore the performance of LoBaSS in imbalanced datasets.
- Why unresolved: The effectiveness of LoBaSS in selecting learnable data points from imbalanced datasets is not addressed, which is a common scenario in real-world applications.
- What evidence would resolve it: Experiments on imbalanced datasets, comparing the performance of models fine-tuned with LoBaSS-selected data to those fine-tuned with randomly sampled data, would provide insights into its robustness.

## Limitations
- The 6% selection ratio may not generalize across different domains and dataset qualities
- The method requires two full fine-tuning runs plus loss computation for all data points, making it computationally expensive
- The learnability score assumes loss reduction correlates with actual learning gain, which may not hold uniformly across all data points

## Confidence
- **High confidence**: The mathematical formulation of the learnability score and its relationship to implicit reward gaps in DPO is sound and well-grounded in established theory
- **Medium confidence**: The empirical results showing 6% selection achieving superior performance are promising but require replication across more datasets and model families
- **Low confidence**: The generalizability of the 6% selection ratio across different domains and dataset qualities is uncertain

## Next Checks
1. **Cross-dataset generalization test**: Apply DavIR to at least three additional instruction-tuning datasets with varying characteristics (e.g., OpenAssistant, Dolly, and a math-focused dataset like MathInstruct) to verify that the 6% selection ratio consistently achieves comparable or superior performance across different domains.

2. **Ablation on normalization**: Conduct controlled experiments comparing DavIR with and without normalization across datasets with systematically different response length distributions to quantify the normalization's impact on selection quality and identify potential new biases it might introduce.

3. **Computational overhead analysis**: Measure wall-clock time and memory requirements for DavIR's two-stage fine-tuning approach versus alternative data selection methods on datasets of increasing size (10K, 50K, 200K data points) to establish practical scalability limits and identify optimization opportunities.