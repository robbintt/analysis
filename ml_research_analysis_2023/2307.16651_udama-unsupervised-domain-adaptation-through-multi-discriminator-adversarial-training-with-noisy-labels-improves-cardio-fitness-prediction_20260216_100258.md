---
ver: rpa2
title: 'UDAMA: Unsupervised Domain Adaptation through Multi-discriminator Adversarial
  Training with Noisy Labels Improves Cardio-fitness Prediction'
arxiv_id: '2307.16651'
source_url: https://arxiv.org/abs/2307.16651
tags:
- domain
- data
- udama
- distribution
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UDAMA, a domain adaptation method designed
  to address the challenge of leveraging large-scale noisy labeled data (silver-standard)
  to improve model performance on small-scale high-quality labeled data (gold-standard)
  in healthcare applications. UDAMA employs multi-discriminator adversarial training
  to learn domain-invariant features, effectively mitigating label distribution shifts.
---

# UDAMA: Unsupervised Domain Adaptation through Multi-discriminator Adversarial Training with Noisy Labels Improves Cardio-fitness Prediction

## Quick Facts
- **arXiv ID**: 2307.16651
- **Source URL**: https://arxiv.org/abs/2307.16651
- **Reference count**: 31
- **One-line primary result**: UDAMA achieves a Pearson correlation coefficient of 0.701 ± 0.032 on gold-standard VO2max prediction, representing up to a 12% improvement over competitive transfer learning methods.

## Executive Summary
UDAMA introduces a novel domain adaptation method that leverages large-scale noisy labeled data (silver-standard) to improve model performance on small-scale high-quality labeled data (gold-standard) in healthcare applications. The method employs multi-discriminator adversarial training with two discriminators—coarse-grained for domain classification and fine-grained for label distribution alignment—to learn domain-invariant features. Applied to cardio-respiratory fitness prediction, UDAMA significantly outperforms existing domain adaptation approaches, demonstrating its potential to effectively utilize noisy data for improved fitness estimation at scale.

## Method Summary
UDAMA addresses unsupervised domain adaptation for healthcare regression tasks where gold-standard labels are scarce but silver-standard labels are abundant. The method pre-trains an encoder network on source domain data (silver-standard) using bidirectional GRU layers for time series and MLP layers for metadata, then performs adversarial adaptation on the target domain (gold-standard) with controlled injection of source domain samples. Two discriminators work in tandem: a coarse-grained discriminator for binary domain classification and a fine-grained discriminator for capturing label distribution differences modeled as Gaussian distributions. The encoder and predictor are trained adversarially against both discriminators to learn domain-invariant features while preserving task-relevant information.

## Key Results
- UDAMA achieves a Pearson correlation coefficient of 0.701 ± 0.032 on gold-standard VO2max prediction, outperforming competitive transfer learning and state-of-the-art domain adaptation models by up to 12%.
- The method demonstrates effectiveness with minimal source domain injection (0.4% of silver-standard data to 10% of gold-standard data), showing optimal balance between leveraging noisy labels and avoiding negative transfer.
- Multi-discriminator training (coarse-grained + fine-grained) outperforms single-discriminator approaches, with the fine-grained discriminator alone achieving a 15.8% improvement over baseline transfer learning methods.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-discriminator adversarial training effectively mitigates label distribution shifts between noisy silver-standard and clean gold-standard domains.
- Mechanism: UDAMA employs two discriminators—coarse-grained (binary domain classification) and fine-grained (distribution label alignment)—to capture both domain membership and underlying label distribution differences. This dual adversarial objective forces the feature encoder to learn representations invariant to domain-specific label shifts while preserving task-relevant information.
- Core assumption: The label distribution in both domains follows a Gaussian pattern, enabling the fine-grained discriminator to model domain differences via mean and variance estimation.
- Evidence anchors:
  - [abstract]: "UDAMA employs multi-discriminator adversarial training to learn domain-invariant features, effectively mitigating label distribution shifts."
  - [section 4.2.2]: "we construct a more complex pseudo-label (yd) to represent its domain label distribution... yd represents the mean and variance of the regression label distribution."
- Break condition: If the underlying label distributions are non-Gaussian or highly multimodal, the fine-grained discriminator's Gaussian modeling assumption fails, degrading adaptation performance.

### Mechanism 2
- Claim: Incorporating a small proportion of source domain samples into target domain training creates an effective adversarial environment without overwhelming the target domain signal.
- Mechanism: By injecting a controlled ratio (e.g., 0.4% source to 10% target) of silver-standard samples into the target domain training, UDAMA balances domain confusion while preserving the primary learning objective on gold-standard data. This selective mixing enables the discriminators to learn nuanced domain differences without inducing negative transfer.
- Core assumption: The source and target domains are sufficiently different that a small injection ratio prevents the model from overfitting to noisy source labels while still enabling effective domain alignment.
- Evidence anchors:
  - [section 5.4]: "we put {0.1%, 0.2%, 0.4%, 1%, 2%, 4% } from Ds... we observe that it achieves the best results and showcases the most positive transfer by only adding 0.4% of the source domain."
- Break condition: If source and target domains are too similar, even small injections may bias the model toward source domain patterns; if too different, any injection may cause negative transfer.

### Mechanism 3
- Claim: Combining fine-tuning with adversarial adaptation yields superior performance compared to either approach alone.
- Mechanism: UDAMA first pre-trains on the source domain to capture general feature representations, then fine-tunes the encoder layers while simultaneously performing adversarial domain adaptation. This two-stage process allows the model to leverage source domain knowledge while adapting to target domain specifics through multi-discriminator adversarial training.
- Core assumption: The lower layers of the network learn general features applicable across domains, while higher layers capture domain-specific patterns that can be effectively adapted.
- Evidence anchors:
  - [section 5.2.1]: "we freeze the first GRU and MLP layers and fine-tuned the remaining network. Compared with freezing all layers... the fine-tuning scheme in our network could capture the general features from the lower layers and extract problem-specific characteristics from higher layers."
- Break condition: If domain shifts are primarily in low-level features rather than high-level representations, freezing lower layers may prevent necessary adaptation.

## Foundational Learning

- Concept: Domain adaptation and domain shift types
  - Why needed here: Understanding the difference between covariate shift, conditional shift, and label distribution shift is crucial for selecting appropriate adaptation strategies and interpreting model behavior.
  - Quick check question: What type of domain shift does UDAMA specifically target, and how does it differ from traditional covariate shift adaptation methods?

- Concept: Adversarial training and discriminator objectives
  - Why needed here: The multi-discriminator framework relies on understanding how adversarial objectives can be used to align distributions and prevent domain-specific feature learning.
  - Quick check question: How do the coarse-grained and fine-grained discriminators' objectives complement each other in the UDAMA framework?

- Concept: Gaussian distribution modeling and negative log-likelihood
  - Why needed here: The fine-grained discriminator uses Gaussian negative log-likelihood to model label distribution differences, requiring understanding of probabilistic modeling and distribution alignment.
  - Quick check question: Why is modeling label distributions as Gaussian appropriate for this CRF prediction task, and what assumptions does this make about the data?

## Architecture Onboarding

- Component map:
  - Encoder: Bidirectional GRU layers for time series, MLP layers for metadata, concatenated output
  - Fine-grained discriminator: Two FC layers predicting mean and variance of label distributions
  - Coarse-grained discriminator: Two FC layers for binary domain classification
  - Predictor: Regression head for VO2max prediction
  - Pre-training stage: Source domain only
  - Adaptation stage: Mixed source/target with multi-discriminator adversarial training

- Critical path: Pre-train → Fine-tune lower layers → Mixed-domain adversarial training → Final prediction
- Design tradeoffs: Balancing source domain injection ratio vs. negative transfer risk; choosing Gaussian modeling for fine-grained discriminator vs. more flexible distributions
- Failure signatures: Performance degradation with excessive source domain injection; poor adaptation when label distributions are non-Gaussian; overfitting when discriminators are too strong relative to predictor
- First 3 experiments:
  1. Test different source domain injection ratios (0%, 0.1%, 0.4%, 1%, 2%) to find optimal balance
  2. Compare single discriminator (coarse or fine) vs. multi-discriminator performance to validate complementarity
  3. Evaluate adaptation performance with synthetic label distribution shifts to test robustness assumptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would UDAMA perform on other healthcare regression tasks beyond cardio-fitness prediction, such as blood glucose monitoring or sleep stage classification?
- Basis in paper: [inferred] The paper mentions UDAMA's potential for "various other healthcare scenarios" and discusses its applicability to high-dimensional time-series data with label distribution shifts.
- Why unresolved: The current evaluation focuses solely on cardio-fitness prediction using two specific datasets (Fenland and BBVS). The paper does not explore UDAMA's performance on other healthcare regression tasks.
- What evidence would resolve it: Applying UDAMA to benchmark datasets for blood glucose monitoring or sleep stage classification and comparing its performance against state-of-the-art domain adaptation methods.

### Open Question 2
- Question: What is the impact of different levels of noise in silver-standard labels on UDAMA's performance, and is there a threshold beyond which the noisy labels become detrimental?
- Basis in paper: [explicit] The paper mentions that "silver-standard labels, which are derived from a less accurate estimation scheme, often lead to predictions with lower accuracy" and discusses the challenge of distribution shifts between silver- and gold-standard labels.
- Why unresolved: While the paper demonstrates UDAMA's ability to leverage noisy silver-standard data, it does not systematically explore how different levels of noise in these labels affect its performance.
- What evidence would resolve it: Conducting experiments with synthetically generated silver-standard labels of varying noise levels and analyzing UDAMA's performance as the noise increases.

### Open Question 3
- Question: How does UDAMA's performance scale with the size of the gold-standard target dataset, and is there a point of diminishing returns?
- Basis in paper: [inferred] The paper focuses on the scenario where gold-standard data is small-scale, but does not explore how UDAMA's performance changes as the size of the target dataset increases.
- Why unresolved: The experiments use a relatively small gold-standard dataset (181 participants in BBVS). It is unclear how UDAMA's performance would be affected if the target dataset were significantly larger.
- What evidence would resolve it: Evaluating UDAMA on progressively larger subsets of a gold-standard dataset and analyzing the relationship between target dataset size and performance gains from incorporating silver-standard data.

## Limitations

- The paper lacks detailed mathematical formulation for the fine-grained discriminator's label distribution modeling, making exact reproduction challenging.
- The theoretical justification for using Gaussian modeling of label distributions across domains assumes this pattern holds generally, but this assumption is not rigorously validated across different datasets or task types.
- While the method shows strong empirical performance, the sensitivity to hyperparameter choices (particularly injection ratio and discriminator weights) across different domain pairs remains unclear.

## Confidence

- **High Confidence**: The overall multi-discriminator framework architecture and the empirical validation showing 12% improvement over baselines are well-supported by experimental results.
- **Medium Confidence**: The specific hyperparameter choices (injection ratio of 0.4%, λ1=0.9, λ2=0.1) appear reasonable given ablation studies, but the sensitivity to these choices across different domain shifts remains unclear.
- **Low Confidence**: The theoretical justification for using Gaussian modeling of label distributions across domains assumes this pattern holds generally, but this assumption is not rigorously validated across different datasets or task types.

## Next Checks

1. **Cross-domain validation**: Test UDAMA on a different healthcare domain (e.g., medical imaging) to verify that Gaussian label distribution assumptions hold and the multi-discriminator framework generalizes beyond CRF prediction.

2. **Label distribution analysis**: Systematically evaluate how UDAMA performs when the source and target label distributions follow non-Gaussian patterns (bimodal, skewed) to identify break conditions for the fine-grained discriminator.

3. **Injection ratio sensitivity**: Conduct a comprehensive study across multiple domain pairs to determine optimal source-to-target injection ratios and identify the point where negative transfer begins to dominate.