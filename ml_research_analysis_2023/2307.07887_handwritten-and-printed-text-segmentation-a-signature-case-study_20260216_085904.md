---
ver: rpa2
title: 'Handwritten and Printed Text Segmentation: A Signature Case Study'
arxiv_id: '2307.07887'
source_url: https://arxiv.org/abs/2307.07887
tags:
- loss
- text
- segmentation
- weighted
- handwritten
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of segmenting handwritten and printed
  text in scanned documents, especially where the two types overlap. Existing approaches
  often treat overlap as a single class, which leads to loss of detail.
---

# Handwritten and Printed Text Segmentation: A Signature Case Study

## Quick Facts
- arXiv ID: 2307.07887
- Source URL: https://arxiv.org/abs/2307.07887
- Reference count: 40
- Key outcome: Proposes a four-class formulation with overlap (OV) class and Mixed Feature Model (MFM) architecture, achieving 17.9% and 7.3% improvement in IoU scores over prior methods on SignaTR6K and WGM-SYN datasets.

## Executive Summary
This work addresses the challenge of segmenting handwritten and printed text in scanned documents, particularly where the two types overlap. The authors propose a novel four-class formulation that includes an explicit overlap (OV) class, allowing the model to recover text from both handwritten and printed layers even in overlapping regions. They introduce a new Mixed Feature Model architecture that combines semantic segmentation with fine-feature paths, along with a Fusion loss function that stabilizes training across imbalanced classes. The approach is evaluated on a new dataset of legal documents and a historical document dataset, demonstrating significant improvements over existing methods.

## Method Summary
The method introduces a four-class segmentation framework (handwritten, printed, background, overlap) and a Mixed Feature Model architecture combining a semantic segmentation path (SSP) with a fine-feature path (FFP). The SSP uses standard U-Net architecture with various backbones (VGG16, InceptionV3, ResNet34), while the FFP preserves low-level details through residual connections without down-sampling. A novel Fusion loss combines weighted focal, cross-entropy, and dice losses to handle class imbalance and improve convergence. The approach is trained on 256x256 grayscale image crops with pixel-level annotations.

## Key Results
- Proposed MFM architecture with Fusion loss outperforms prior methods by 17.9% IoU on SignaTR6K dataset
- Achieves 7.3% IoU improvement on WGM-SYN historical document dataset
- Four-class formulation with OV class successfully recovers text in overlapping regions that three-class models miss
- CRFH post-processing effectively refines predictions without aggressive relabeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The four-class formulation with overlap (OV) class captures pixels belonging to both handwritten and printed text, which three-class models incorrectly collapse into a single class.
- Mechanism: By defining OV as a distinct class, the model can assign pixels in overlapping regions to both HT and PT classes during post-processing, preserving information lost in three-class models.
- Core assumption: Overlapping regions can be cleanly separated into two mutually exclusive layers (handwritten and printed) once OV is identified.
- Evidence anchors:
  - [abstract] "Our objective is to recover text from different classes in their entirety, especially enhancing the segmentation performance on overlapping sections."
  - [section] "The problem with CC-based approaches is that it makes the class membership for the entire component... and thus, they are not accounted for in the other class."
  - [corpus] Weak evidence; neighbor papers focus on binary or single-label segmentation without explicit OV class.
- Break condition: If overlapping pixels cannot be cleanly assigned to distinct layers, or if the OV class becomes ambiguous due to complex occlusion patterns.

### Mechanism 2
- Claim: The Mixed Feature Model (MFM) combining Fine Feature Path (FFP) and Semantic Segmentation Path (SSP) captures both high-level patterns and low-level text details.
- Mechanism: SSP extracts high-level semantic context via down-sampling, while FFP preserves fine pixel-level detail via residual connections without down-sampling; their concatenation yields richer feature representation.
- Core assumption: Both high-level contextual cues and low-level pixel fidelity are required for accurate segmentation of overlapping text.
- Evidence anchors:
  - [abstract] "We provide a novel architecture that combines both semantic segmentation features and fine features to improve the performance of text segmentation compared to the prior work."
  - [section] "We include a parallel path to SSP, namely a fine feature path (FFP) which does not perform down-sampling, but incorporates a convolution block with residual connections."
  - [corpus] Weak evidence; neighbor papers use single-path U-Net or CNN without explicit FFP.
- Break condition: If residual connections in FFP fail to propagate fine features effectively, or if SSP alone suffices for the task.

### Mechanism 3
- Claim: Fusion loss stabilizes training by combining multiple loss functions to balance performance across imbalanced classes.
- Mechanism: Fusion loss = LW Focal + LW CE + LW D, where each component targets different aspects: class imbalance (weighted cross-entropy), hard example focus (focal), and recall-oriented segmentation (dice).
- Core assumption: No single loss function optimally balances all classes; combining them yields more stable convergence and higher IoU.
- Evidence anchors:
  - [abstract] "Additionally, we introduce a new loss function, Fusion loss, that comparatively is stable and convergences to the optimal loss values and Intersection over Union (IoU) scores."
  - [section] "As such, with Fusion loss, we aim to combine the behavior of different losses, and we define the Fusion loss as the summation of the three weighted losses."
  - [corpus] Weak evidence; neighbor papers use single loss (cross-entropy or dice) without fusion.
- Break condition: If loss weighting causes dominance of one component, leading to suboptimal performance on certain classes.

## Foundational Learning

- Concept: U-Net architecture and skip-connections
  - Why needed here: Provides the backbone for SSP and allows recovery of spatial detail lost during down-sampling.
  - Quick check question: In a U-Net, where do skip-connections connect encoder and decoder stages?

- Concept: Loss function weighting for class imbalance
  - Why needed here: Background pixels dominate the dataset; weighted losses prevent the model from trivially predicting background.
  - Quick check question: If class BG has 90% of pixels, what weight should you assign to BG in a weighted cross-entropy loss to balance its contribution?

- Concept: Residual connections and their impact on feature propagation
  - Why needed here: Enables FFP to pass low-level features without degradation across multiple convolutions.
  - Quick check question: How do residual connections help gradients flow through deep networks compared to plain convolutions?

## Architecture Onboarding

- Component map: Input -> SSP (U-Net with backbone) -> FFP (residual block, no down-sampling) -> Concat -> Conv1x1 -> Softmax -> Post-processing (OV to HT+PT)
- Critical path: Input → SSP → FFP → Concat → Conv1x1 → Softmax → Post-processing
- Design tradeoffs:
  - Larger backbones improve accuracy but increase memory and training time
  - FFP adds parameter overhead but improves fine-detail capture
  - Four-class formulation increases label complexity but improves overlap handling
- Failure signatures:
  - Poor overlap IoU → OV class not learned or post-processing misclassifies
  - High BG IoU but low HT/PT IoU → loss weighting or CRF post-processing issues
  - No improvement over SSP alone → FFP redundant or poorly initialized
- First 3 experiments:
  1. Baseline: SSP with ResNet34 backbone, CE loss, 3-class formulation
  2. Add OV class: SSP ResNet34, 4-class, CE loss
  3. Add FFP: MFM ResNet34, 4-class, CE loss, compare IoU vs SSP-only

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MFM architecture perform when applied to other domains beyond legal documents, such as historical archives or medical records?
- Basis in paper: [inferred] The paper focuses on legal documents and evaluates on two datasets (SignaTR6K and WGM-SYN), but does not test generalization to other domains.
- Why unresolved: The paper does not provide cross-domain validation or discuss model adaptability to different document types.
- What evidence would resolve it: Experimental results on datasets from other domains, such as historical or medical documents, showing consistent performance improvements.

### Open Question 2
- Question: What is the impact of the CRF heuristic (CRFH) on datasets with significantly different class distributions, such as those with much larger background regions?
- Basis in paper: [explicit] The paper introduces CRFH to mitigate aggressive relabeling but notes that its effectiveness may be dataset-dependent.
- Why unresolved: The heuristic is only tested on the SignaTR6K and WGM-SYN datasets, which may not represent all possible class imbalances.
- What evidence would resolve it: Performance metrics (IoU scores) on datasets with varying class distributions, demonstrating whether CRFH consistently improves results.

### Open Question 3
- Question: Can the Fusion loss function be further optimized by incorporating additional loss components, such as focal loss with adaptive gamma?
- Basis in paper: [explicit] The paper introduces Fusion loss as a combination of weighted focal, cross-entropy, and dice losses, but does not explore further extensions.
- Why unresolved: The authors do not experiment with alternative or additional loss components that might enhance convergence or stability.
- What evidence would resolve it: Comparative experiments showing whether adding adaptive focal loss or other components improves IoU scores or training stability.

### Open Question 4
- Question: How does the performance of the MFM architecture scale with larger input image sizes, such as 512x512 or 1024x1024 pixels?
- Basis in paper: [inferred] The paper uses 256x256 pixel images for training and evaluation but does not discuss scalability to larger images.
- Why unresolved: Larger images may capture more context but could also introduce computational challenges or overfitting risks.
- What evidence would resolve it: Experiments with larger image sizes, reporting IoU scores and computational efficiency metrics (e.g., GPU memory usage, training time).

## Limitations

- Weak empirical grounding for proposed mechanisms beyond two datasets
- No cross-dataset generalization tests to validate domain adaptability
- Limited exploration of complex overlap patterns and occlusion scenarios
- Fusion loss superiority not validated through ablation studies of individual components

## Confidence

- **High confidence**: The four-class formulation and post-processing conversion of OV to HT+PT is well-justified and clearly specified
- **Medium confidence**: MFM architecture combining SSP and FFP is well-described but impact of residual connections not experimentally validated
- **Low confidence**: Claims about handling complex overlap patterns are weakly supported by limited experimental scope

## Next Checks

1. **Ablation on loss components**: Train MFM models with each individual loss (WFocal, WCE, WDice) and their pairwise combinations to quantify the marginal contribution of each term in Fusion loss.

2. **Cross-dataset generalization**: Test the trained MFM model on an unseen dataset (e.g., READ dataset or ICDAR handwriting datasets) to evaluate whether overlap segmentation performance transfers beyond the training distribution.

3. **Complex overlap stress test**: Create or identify synthetic test cases with highly interlaced handwritten and printed text (e.g., printed text with handwritten annotations covering multiple printed letters) to validate whether the OV class and post-processing can handle extreme occlusion.