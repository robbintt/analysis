---
ver: rpa2
title: Evaluating Large Language Models for Radiology Natural Language Processing
arxiv_id: '2307.13693'
source_url: https://arxiv.org/abs/2307.13693
tags:
- llms
- arxiv
- language
- medical
- radiology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates 32 large language models (LLMs) on their ability
  to generate radiology impressions from findings. Models tested include both global
  leaders (ChatGPT, GPT-4, PaLM2, Claude2) and Chinese-developed LLMs (HuatuoGPT,
  MOSS, ChatGLM, etc.).
---

# Evaluating Large Language Models for Radiology Natural Language Processing

## Quick Facts
- arXiv ID: 2307.13693
- Source URL: https://arxiv.org/abs/2307.13693
- Reference count: 40
- Primary result: BayLing-7B achieved highest R-1 scores (0.4506 OpenI, 0.2901 MIMIC) in five-shot scenarios

## Executive Summary
This study evaluates 32 large language models on radiology impression generation from findings, comparing both global leaders (ChatGPT, GPT-4, PaLM2, Claude2) and Chinese-developed models (HuatuoGPT, MOSS, ChatGLM). Using MIMIC-CXR and OpenI datasets, models were tested across zero-shot, one-shot, and five-shot settings with ROUGE metrics. The results show Chinese LLMs can compete with global models, smaller 7B parameter models perform competitively, and ROUGE-based evaluation may not fully capture medical report quality.

## Method Summary
The study evaluates 32 LLMs on radiology impression generation using MIMIC-CXR and OpenI datasets containing radiology reports with Findings and Impression sections. Models were tested across zero-shot, one-shot, and five-shot settings with fixed prompts and hyperparameters (temperature=0.9, top_k=40, top_p=0.9). Performance was measured using ROUGE-1, ROUGE-2, and ROUGE-L scores, with a focus on 7B parameter models to examine the relationship between model size and performance.

## Key Results
- BayLing-7B achieved highest R-1 scores of 0.4506 (OpenI) and 0.2901 (MIMIC) in five-shot scenarios
- Anthropic Claude2 showed strong zero-shot performance across both datasets
- Chinese LLMs demonstrated competitive performance compared to global models
- Smaller 7B parameter models performed competitively with larger models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot performance of Chinese LLMs is competitive with global models when evaluated using ROUGE metrics
- Mechanism: The models leverage pre-training on large multilingual corpora, enabling them to generalize to radiology report generation tasks without task-specific fine-tuning
- Core assumption: ROUGE metrics adequately capture the quality of radiology report generation, even for models trained primarily on non-medical data
- Evidence anchors:
  - [abstract]: "Chinese LLMs can compete with global models" and "smaller models (7B parameters) perform competitively"
  - [section]: "HuatuoGPT-7B is trained on the Baichuan-7B corpus" and "Chat-GLM-6B is trained for about 1 trillion tokens of Chinese and English corpus"
  - [corpus]: Weak evidence; no direct mention of ROUGE or zero-shot performance for Chinese models in related papers
- Break condition: If ROUGE metrics are insufficient for evaluating medical report quality, zero-shot performance claims may not hold

### Mechanism 2
- Claim: Few-shot learning (one-shot and five-shot) significantly improves LLM performance on radiology report generation tasks
- Mechanism: Providing examples helps LLMs understand task-specific formatting and content expectations, reducing the gap between pre-trained knowledge and target task requirements
- Core assumption: The provided examples are representative of the target task distribution and do not introduce bias
- Evidence anchors:
  - [section]: "We engage zero-shot, one-shot, and five-shot prompts to examine the model's zero-shot and few-shot performance"
  - [section]: "The five-shot setting witnessed the BayLing-7B model outpacing other models, demonstrating an R-1 score of 0.4506"
  - [corpus]: No direct evidence; related papers focus on different evaluation metrics
- Break condition: If the few-shot examples are not representative or introduce bias, performance improvements may not generalize

### Mechanism 3
- Claim: Models with 7B parameters can achieve high performance in radiology report generation without requiring larger models
- Mechanism: The 7B parameter models strike an optimal balance between model capacity and data efficiency, allowing them to capture task-relevant patterns without overfitting
- Core assumption: The complexity of radiology report generation can be adequately captured by 7B parameter models given sufficient training data
- Evidence anchors:
  - [abstract]: "smaller models (7B parameters) perform competitively"
  - [section]: "models with 7B parameters can produce impressive results"
  - [corpus]: Weak evidence; no direct mention of parameter count in related papers
- Break condition: If the task complexity exceeds the representational capacity of 7B parameter models, performance may plateau or degrade

## Foundational Learning

- Concept: Zero-shot and few-shot learning in LLMs
  - Why needed here: The paper evaluates models across zero-shot, one-shot, and five-shot settings to understand their generalization capabilities
  - Quick check question: What is the key difference between zero-shot and few-shot learning in the context of this paper?

- Concept: ROUGE metrics for text generation evaluation
  - Why needed here: The paper uses ROUGE-1, ROUGE-2, and ROUGE-L scores to quantify model performance on radiology report generation
  - Quick check question: What aspects of text generation does each ROUGE variant measure?

- Concept: Multimodal learning in medical AI
  - Why needed here: The paper mentions future directions involving multimodal LLMs that can process both text and images
  - Quick check question: How might multimodal capabilities enhance radiology report generation beyond text-only models?

## Architecture Onboarding

- Component map: MIMIC-CXR/OpenI datasets (Findings → Impression pairs) -> Fixed prompts (zero-shot/one-shot/five-shot) -> LLMs (32 models) -> Inference engine (temperature=0.9, top_k=40, top_p=0.9) -> ROUGE metric computation -> Performance comparison

- Critical path: 1. Load radiology report dataset (Findings → Impression pairs) 2. Prepare prompts for each shot setting 3. Run inference across all models with fixed hyperparameters 4. Compute ROUGE scores between generated impressions and ground truth 5. Aggregate and compare results

- Design tradeoffs:
  - Fixed hyperparameters vs. model-specific optimization
  - ROUGE metrics vs. human evaluation for medical content quality
  - 7B parameter focus vs. including larger models for comparison

- Failure signatures:
  - Low ROUGE scores across all models suggest dataset or prompt issues
  - Inconsistent performance between datasets indicates domain adaptation problems
  - High variance in shot settings suggests sensitivity to example quality

- First 3 experiments:
  1. Run zero-shot evaluation on a small subset to verify prompt format and API integration
  2. Compare one model's performance across all shot settings to validate few-shot effectiveness
  3. Test a known-good model (e.g., ChatGPT) on both datasets to establish baseline performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop evaluation metrics for medical LLMs that better capture clinical accuracy and safety beyond ROUGE scores?
- Basis in paper: [explicit] The paper notes that "ROUGE-based evaluation may not fully capture model quality in medical contexts" and that BayLing's "succinct answers" may be "of high quality and accuracy" despite brevity, while GPT-4's "more verbose" responses may show "some level of distrust in the input"
- Why unresolved: Current ROUGE metrics measure n-gram overlap with reference texts but don't assess clinical accuracy, safety implications, or appropriateness of medical recommendations
- What evidence would resolve it: Development and validation of evaluation frameworks that incorporate clinical accuracy scoring, safety assessment, expert review panels, and real-world deployment monitoring metrics

### Open Question 2
- Question: What is the optimal model size and architecture for medical NLP tasks that balances performance with computational efficiency?
- Basis in paper: [explicit] The study found that "smaller models (7B parameters) perform competitively" and questions whether "intelligence truly arises from the number of parameters and data accumulation"
- Why unresolved: While smaller models show promise, the relationship between model size, architecture, domain-specific fine-tuning, and task performance in medical contexts remains unclear
- What evidence would resolve it: Systematic ablation studies comparing various model sizes (1B-175B) across diverse medical NLP tasks, measuring both performance metrics and computational/resource requirements

### Open Question 3
- Question: How can multimodal LLMs be effectively evaluated and deployed for comprehensive radiology interpretation?
- Basis in paper: [explicit] The discussion mentions "fascinating prospects for future research" in evaluating multimodal models' ability to "directly interpret radiological images, in addition to textual reports"
- Why unresolved: While text-based evaluation methods exist, multimodal assessment requires new benchmarks, evaluation criteria, and validation methods that account for both image interpretation and text generation capabilities
- What evidence would resolve it: Development of multimodal radiology datasets with ground truth image-text pairs, creation of evaluation metrics for multimodal coherence, and clinical validation studies comparing multimodal vs text-only models

## Limitations

- The study relies on ROUGE metrics which may not adequately capture clinically meaningful content or diagnostic accuracy
- No human expert evaluation was conducted to validate whether generated impressions are clinically appropriate or diagnostically useful
- The study does not evaluate multimodal capabilities that would allow models to process actual radiology images

## Confidence

Medium: The comparative performance analysis across 32 models provides robust evidence for relative rankings, but the absolute quality of generated reports remains uncertain without clinical validation.

## Next Checks

1. **Clinical Expert Review**: Conduct blind evaluation of generated impressions by board-certified radiologists to assess clinical accuracy, completeness, and appropriateness compared to ground truth impressions.

2. **Hallucination Detection**: Implement automated checks for clinically implausible or unsupported findings in generated impressions, comparing hallucination rates across models and shot settings.

3. **Multimodal Extension**: Test a subset of top-performing models on multimodal versions of the same task, providing both text findings and corresponding chest X-ray images to evaluate whether visual input improves impression quality.