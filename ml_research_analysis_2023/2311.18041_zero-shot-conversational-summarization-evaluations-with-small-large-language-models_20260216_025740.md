---
ver: rpa2
title: Zero-shot Conversational Summarization Evaluations with small Large Language
  Models
arxiv_id: '2311.18041'
source_url: https://arxiv.org/abs/2311.18041
tags:
- dialogue
- summary
- summarize
- summarization
- summaries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of large language models (LLMs)
  with around 10 billion parameters on conversational summarization. The study examines
  how different prompts affect the quality of summaries generated by these models.
---

# Zero-shot Conversational Summarization Evaluations with small Large Language Models

## Quick Facts
- arXiv ID: 2311.18041
- Source URL: https://arxiv.org/abs/2311.18041
- Reference count: 40
- Key outcome: Evaluates small LLMs (~10B parameters) on conversational summarization, showing performance varies significantly with different prompts and highlighting limitations like hallucinations and role misattribution.

## Executive Summary
This paper evaluates large language models with around 10 billion parameters on conversational summarization tasks, focusing on zero-shot performance with different prompts. The study reveals that instruction-tuned models generally outperform their base counterparts, but performance varies significantly depending on the prompt used. Through human evaluations, the research identifies key limitations including hallucinations and incorrect role attributions, emphasizing the need for careful prompt selection and potential task-specific fine-tuning to improve summarization quality.

## Method Summary
The study evaluates LLMs on conversational summarization using the Samsum and Dialogsum datasets, with models ranging from 7 to 13 billion parameters including Llama 2, Falcon, and Alpaca variants. Models are instruction-tuned on the MeetingBank dataset and evaluated zero-shot using prompts from Promptsource. Performance is measured using ROUGE scores, BERTScore, length ratio, and novel n-gram metrics, complemented by human evaluations assessing overall quality, novelty, informativeness, and conciseness. The research examines how different prompts affect summary quality and investigates model limitations through systematic human rating.

## Key Results
- Instruction-tuned models generate summaries with better ROUGE scores and BERTScores compared to base models
- Performance varies significantly with different prompts, with up to 8% difference in ROUGE-L scores for the same model
- Human evaluations reveal models suffer from hallucinations and incorrect role attributions not captured by automatic metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning improves conversational summarization performance in LLMs.
- Mechanism: Pre-trained LLMs are fine-tuned on task-specific instructions and data from a related domain (MeetingBank), enhancing their ability to generate relevant summaries for conversational data.
- Core assumption: Fine-tuning on task-specific instructions and related domain data improves generalization to unseen conversational datasets.
- Evidence anchors:
  - [abstract] "We also evaluate the models with human evaluations and discuss the limitations of the models on conversational summarization"
  - [section] "We find that the instruction tuned models generate summaries with better Rouge-scores, BertScores, more concise and generate fewer novel 1-gram and 2-gram (due to fewer hallucinations)."
  - [corpus] Weak evidence; corpus contains papers on instruction tuning but not specifically conversational summarization.
- Break Condition: If the fine-tuned model overfits to the training domain and loses ability to generalize to new conversational styles.

### Mechanism 2
- Claim: Summary quality is highly dependent on the instruction prompt used.
- Mechanism: Different natural language prompts lead to significantly different summary outputs, affecting evaluation metrics like ROUGE scores.
- Core assumption: Minor variations in prompt syntax can result in vastly different results from the LLM.
- Evidence anchors:
  - [abstract] "We show that the summaries generated by models depend on the instructions and the performance of LLMs vary with different instructions sometimes resulting steep drop in ROUGE scores if prompts are not selected carefully."
  - [section] "For instance, 'Given the above dialogue write a summary' as an instruction gives 8% better Rouge-L scores than 'Generate a summary for this dialogue' in Falcon-7b instruct tuned model compared to 'Llama-7b' which gives the best Rouge scores from the prompt 'Given the above dialogue write a summary'."
  - [corpus] Weak evidence; corpus does not specifically address prompt sensitivity in summarization.
- Break Condition: If the model is robust to prompt variations or if a standardized prompt format is developed.

### Mechanism 3
- Claim: Human evaluation reveals limitations of LLM-generated summaries, including hallucinations and incorrect role attributions.
- Mechanism: Human raters assess summaries for factual consistency, conciseness, and correct speaker attribution, uncovering issues not captured by automatic metrics.
- Core assumption: Human judgment is necessary to identify nuanced errors like hallucinations and role misattributions in summaries.
- Evidence anchors:
  - [abstract] "We also discuss the limitations of the models on conversational summarization"
  - [section] "We requested human raters to mark (Yes/Now) if the summary contained irrelevant phrases not related to the input... We also requested human raters to answer yes/no question if the the summaries had incorrect role attributions and if there was assign/misrepresented gender pronouns."
  - [corpus] Weak evidence; corpus does not specifically address human evaluation of LLM-generated summaries.
- Break Condition: If automated metrics are developed that can reliably detect hallucinations and role misattributions.

## Foundational Learning

- Concept: Instruction Tuning
  - Why needed here: To improve the LLM's ability to follow specific instructions for generating summaries.
  - Quick check question: How does instruction tuning differ from standard fine-tuning, and why is it beneficial for zero-shot summarization?

- Concept: ROUGE and BERTScore Metrics
  - Why needed here: To evaluate the quality of generated summaries against reference summaries using both lexical overlap (ROUGE) and semantic similarity (BERTScore).
  - Quick check question: What are the differences between ROUGE-1, ROUGE-2, and ROUGE-L, and when would you use each?

- Concept: Human Evaluation in NLP
  - Why needed here: To assess aspects of summary quality that automatic metrics cannot capture, such as factual consistency and conciseness.
  - Quick check question: Why is human evaluation still necessary in NLP tasks despite advances in automatic evaluation metrics?

## Architecture Onboarding

- Component map: Pre-trained LLMs (7-13B parameters) -> Instruction tuning on MeetingBank -> Zero-shot evaluation on Samsum/Dialogsum -> Automatic metrics (ROUGE, BERTScore) + Human evaluation interface
- Critical path: 1) Load pre-trained LLM. 2) Apply instruction tuning using MeetingBank dataset. 3) Generate summaries using various prompts from Promptsource. 4) Evaluate using automatic metrics and human evaluation.
- Design tradeoffs: Smaller models chosen for computational efficiency and edge deployment, but may sacrifice some performance compared to larger models. Instruction tuning improves performance but requires additional data and computation.
- Failure signatures: Large variability in summary quality across different prompts, presence of hallucinations or incorrect role attributions in generated summaries, lower human evaluation scores despite good automatic metric scores.
- First 3 experiments:
  1. Compare ROUGE scores of instruction-tuned vs. non-instruction-tuned models on a held-out conversational summarization dataset.
  2. Evaluate the impact of different prompts on summary quality by measuring ROUGE score variability across prompts for a single model.
  3. Conduct a human evaluation comparing summaries generated by the best-performing model and prompt against reference summaries, focusing on factual consistency and conciseness.

## Open Questions the Paper Calls Out

- Question: How do LLMs with parameter counts greater than 11 billion perform on conversational summarization compared to the models studied in this work?
- Basis in paper: Inferred from the discussion on hardware limitations and carbon emissions related to larger models.
- Why unresolved: The paper focuses on models with approximately 11 billion parameters due to hardware and carbon emission considerations, leaving the performance of larger models unexplored.
- What evidence would resolve it: Benchmarking results of LLMs with parameter counts greater than 11 billion on the same conversational summarization datasets used in this study.

- Question: Can the brittleness of LLMs to different prompts be reduced through specific training techniques or architectural changes?
- Basis in paper: Explicitly mentioned as a research question and observed variability in model performance across different prompts.
- Why unresolved: While the paper notes the issue, it does not explore solutions to reduce prompt sensitivity.
- What evidence would resolve it: Experiments demonstrating improved consistency across prompts through training techniques or architectural modifications.

- Question: What is the impact of hallucinations and incorrect role attributions on the overall effectiveness of conversational summarization in practical applications?
- Basis in paper: Explicitly discussed as limitations of the models, with human evaluations indicating lower factual ratings correlated with these issues.
- Why unresolved: The paper highlights these issues but does not explore their practical implications in real-world scenarios.
- What evidence would resolve it: User studies or field tests evaluating the impact of hallucinations and incorrect role attributions on the usability of generated summaries in practical applications.

## Limitations

- Significant performance variations across different prompts, requiring careful prompt engineering
- Models exhibit hallucinations and incorrect role attributions that only human evaluation can detect
- Study limited to relatively small models (~10B parameters) due to computational constraints

## Confidence

**High Confidence:**
- Instruction tuning improves summarization performance on conversational datasets
- Summary quality is highly dependent on prompt selection
- Human evaluation reveals limitations not captured by automatic metrics

**Medium Confidence:**
- Smaller LLMs can perform conversational summarization effectively
- Performance variations across prompts are significant enough to impact real-world applications
- The MeetingBank dataset is appropriate for instruction tuning for conversational summarization

**Low Confidence:**
- Specific prompts identified as optimal for conversational summarization
- Relative performance rankings of different model architectures
- The magnitude of hallucination and role attribution errors

## Next Checks

1. **Prompt Robustness Testing**: Systematically test a broader range of prompt variations (10+ variations per model) to quantify the upper bounds of performance variability and identify prompt characteristics that minimize performance drops.

2. **Error Type Analysis**: Conduct detailed error analysis on a subset of 100 summaries to categorize and quantify the frequency of hallucinations, role attribution errors, and other quality issues, then test whether fine-tuning on conversational-specific data reduces these errors.

3. **Cross-Dataset Generalization**: Evaluate the best-performing instruction-tuned model on an additional conversational dataset not used in the original study to assess generalization beyond Samsum and Dialogsum datasets.