---
ver: rpa2
title: 'NNKGC: Improving Knowledge Graph Completion with Node Neighborhoods'
arxiv_id: '2302.06132'
source_url: https://arxiv.org/abs/2302.06132
tags: []
core_contribution: The paper proposes NNKGC, a knowledge graph completion framework
  that improves performance by incorporating head entity neighborhoods from multiple
  hops using graph neural networks. Unlike previous methods that only use 1-hop neighbors
  or flat structures, NNKGC models richer neighborhood information through GNNs.
---

# NNKGC: Improving Knowledge Graph Completion with Node Neighborhoods

## Quick Facts
- arXiv ID: 2302.06132
- Source URL: https://arxiv.org/abs/2302.06132
- Reference count: 14
- Outperforms SimKGC baseline on FB15k-237 and WN18RR datasets

## Executive Summary
NNKGC is a knowledge graph completion framework that improves performance by incorporating multi-hop head entity neighborhoods using graph neural networks. Unlike previous methods that only use 1-hop neighbors or flat structures, NNKGC models richer neighborhood information through GNNs and includes an auxiliary edge link prediction task. Experiments show improvements in MRR and Hits@1/3/10 metrics compared to strong text-based baselines.

## Method Summary
NNKGC processes knowledge graph triples by encoding entities with a PLM (BERT) and aggregating neighborhood information through GNNs. The framework uses contrastive learning with InfoNCE loss for tail entity prediction and optionally includes a VGAE-based edge prediction task. The model balances the main KGC objective with the auxiliary edge prediction task through a weighted loss function.

## Key Results
- NNKGC outperforms SimKGC baseline on FB15k-237 and WN18RR datasets
- Improves MRR and Hits@1/3/10 metrics compared to text-only methods
- Case studies demonstrate explainable predictions by leveraging relevant neighbor information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph neural networks enable richer multi-hop neighborhood modeling compared to flat 1-hop neighbor structures
- Mechanism: By propagating node embeddings through multiple layers, GNNs capture both direct and indirect relational context between the head entity and its neighbors, encoding structural information that flat attention-based methods cannot represent
- Core assumption: The multi-hop neighborhood structure contains relevant information for predicting missing relations
- Evidence anchors:
  - [abstract]: "It models the head entity neighborhood from multiple hops using graph neural networks to enrich the head node information"
  - [section]: "To further consider the head node neighborhood to enrich the encoding, we apply a graph neural network (Scarselli et al., 2009) to model the neighbors"
  - [corpus]: Found related work showing GNNs improve KGC through multi-hop propagation (weak support - 0 citations)

### Mechanism 2
- Claim: Contrastive learning with InfoNCE loss improves negative sampling for better tail entity prediction
- Mechanism: The contrastive framework learns to distinguish true tail entities from negatives by maximizing the similarity between positive pairs while minimizing similarity for negative pairs, leading to more discriminative embeddings
- Core assumption: The contrastive learning approach provides better signal for ranking tail entities than traditional scoring functions
- Evidence anchors:
  - [abstract]: "SimKGC introduces a contrastive learning approach to improve the negative sampling and applies the InfoNCE loss as the loss function"
  - [section]: "The prediction is simply a score based on cosine similarity between ehr and et. Then the tail entities are ranked by this score"
  - [corpus]: Weak evidence - no related papers specifically discussing contrastive KGC with InfoNCE

### Mechanism 3
- Claim: Auxiliary edge link prediction task provides additional supervision that improves the main KGC task
- Mechanism: By predicting missing edges within the neighborhood graph, the model learns better node representations that capture local graph structure, which transfers to improved relation prediction performance
- Core assumption: The edge prediction task learns useful structural patterns that are complementary to the main KGC task
- Evidence anchors:
  - [abstract]: "Moreover, we introduce an additional edge link prediction task to improve KGC"
  - [section]: "To further strengthen the effect of neighborhood entities for the KGC task, we ask the model to predict neighborhood edges"
  - [corpus]: Weak evidence - only 0 citations for related papers on edge prediction in KGC

## Foundational Learning

- Graph Neural Networks
  - Why needed here: To model the multi-hop neighborhood structure around head entities, capturing both direct and indirect relational context
  - Quick check question: What is the key difference between GNNs and traditional graph embedding methods in how they process neighborhood information?

- Contrastive Learning
  - Why needed here: To learn discriminative representations that can effectively distinguish true tail entities from negative samples during ranking
  - Quick check question: How does the InfoNCE loss function differ from traditional cross-entropy loss in the context of KGC?

- Knowledge Graph Completion Fundamentals
  - Why needed here: To understand the task of predicting missing relations and the evaluation metrics used to measure performance
  - Quick check question: What are the three components of a knowledge graph triple, and what does the KGC task aim to predict?

## Architecture Onboarding

- Component map: BERT encoder -> GNN encoder -> Contrastive loss module -> Edge prediction module (optional) -> Final scoring layer
- Critical path: Input (h,r) → BERT encoding → GNN neighborhood aggregation → Tail entity scoring (contrastive loss) → Final prediction
- Design tradeoffs:
  - Multi-hop vs. 1-hop: More hops provide richer context but introduce noise and computational cost
  - GNN type selection: GCN vs. GAT vs. GraphSAGE - trade-off between simplicity and expressivity
  - λ parameter: Balancing main task vs. auxiliary edge prediction objectives
- Failure signatures:
  - Degraded MRR/H@1 with increasing hop numbers (indicates noise from distant neighbors)
  - High variance in edge prediction loss (suggests unstable graph structure learning)
  - Overfitting to training set (check if validation metrics plateau early)
- First 3 experiments:
  1. Compare 1-hop vs. 2-hop vs. 3-hop GNN layers to find optimal neighborhood scale
  2. Test different GNN types (GCN, GAT, GraphSAGE) with fixed architecture to evaluate neighborhood modeling effectiveness
  3. Evaluate impact of auxiliary edge prediction by training with and without the VGAE loss component

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of NNKGC scale with increasing knowledge graph size and complexity beyond the two datasets studied?
- Basis in paper: [inferred] The paper evaluates NNKGC on FB15k-237 and WN18RR, but does not test on larger or more complex knowledge graphs.
- Why unresolved: The paper's experiments are limited to two public datasets, so the model's performance on larger, real-world knowledge graphs remains unknown.
- What evidence would resolve it: Experiments on larger knowledge graphs with more entities, relations, and triples would demonstrate the model's scalability and performance in more complex scenarios.

### Open Question 2
- Question: How does the choice of GNN architecture (GCN, GAT, GraphSAGE) affect the model's performance in different domains or with varying neighborhood structures?
- Basis in paper: [explicit] The paper compares GCN, GAT, and GraphSAGE encoders and finds GraphSAGE performs best on average, but does not explore domain-specific or structural variations.
- Why unresolved: The paper's ablation study only compares these architectures on two datasets without analyzing their performance across different types of knowledge graphs or neighborhood configurations.
- What evidence would resolve it: Testing each GNN architecture on diverse knowledge graphs with varying structures (e.g., different relation types, density patterns) would reveal which architecture performs best under specific conditions.

### Open Question 3
- Question: What is the impact of including edge link prediction on the model's performance when the knowledge graph has different levels of edge sparsity?
- Basis in paper: [explicit] The paper introduces an edge link prediction task to improve KGC performance but only tests it on two datasets without varying edge density.
- Why unresolved: The paper does not explore how the edge prediction component affects performance in knowledge graphs with different edge densities or sparsity levels.
- What evidence would resolve it: Experiments on knowledge graphs with varying edge densities (from very sparse to dense) would show how the edge prediction task contributes to performance under different sparsity conditions.

## Limitations
- Multi-hop GNN approach increases computational complexity for large knowledge graphs
- Contrastive learning relies heavily on quality of negative samples for effective training
- Auxiliary edge prediction task requires careful tuning of balancing parameter λ

## Confidence
- High confidence: The core claim that multi-hop neighborhood information improves KGC performance is well-supported by both theoretical reasoning and empirical results
- Medium confidence: The effectiveness of contrastive learning with InfoNCE loss is supported by results, though the specific mechanism could be more thoroughly examined
- Medium confidence: The auxiliary edge prediction task shows promise, but the evidence for its contribution is less direct

## Next Checks
1. Conduct ablation studies systematically varying the number of GNN layers (1-hop to 3-hop) to quantify the point of diminishing returns
2. Perform controlled experiments isolating the contrastive loss contribution by comparing against a non-contrastive baseline with identical GNN architecture
3. Test the framework's robustness to noisy neighborhoods by introducing controlled amounts of edge corruption and measuring performance degradation across different hop counts