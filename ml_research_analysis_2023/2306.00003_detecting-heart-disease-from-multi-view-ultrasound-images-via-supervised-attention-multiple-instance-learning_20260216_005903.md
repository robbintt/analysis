---
ver: rpa2
title: Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention
  Multiple Instance Learning
arxiv_id: '2306.00003'
source_url: https://arxiv.org/abs/2306.00003
tags:
- learning
- view
- samil
- images
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of detecting aortic stenosis
  (AS) from multiple ultrasound images using a supervised attention multiple instance
  learning (SAMIL) approach. The method consists of three key components: an instance
  representation layer that processes each image into a feature representation, a
  supervised attention pooling layer that guides the attention mechanism to focus
  on relevant views, and an output layer that maps the study-level representation
  to a diagnosis.'
---

# Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning

## Quick Facts
- arXiv ID: 2306.00003
- Source URL: https://arxiv.org/abs/2306.00003
- Reference count: 40
- Key outcome: Supervised attention MIL approach achieves 76% balanced accuracy on AS diagnosis, outperforming state-of-the-art attention-based MIL methods (60-67%) while reducing model size

## Executive Summary
This paper addresses the challenge of detecting aortic stenosis (AS) from multiple ultrasound images using a supervised attention multiple instance learning (SAMIL) approach. The method innovatively combines supervised attention guided by a view classifier with bag-level contrastive pretraining to focus on clinically relevant views (PLAX and PSAX) while learning robust study-level representations. Evaluated on the Tufts Medical Echocardiogram Dataset and an external validation set, SAMIL demonstrates improved accuracy over existing attention-based MIL architectures while maintaining computational efficiency.

## Method Summary
The SAMIL approach consists of three key components: an instance representation layer that processes each image into feature representations, a supervised attention pooling layer that guides attention toward clinically relevant views using a pre-trained view classifier, and an output layer that maps the study-level representation to AS severity diagnosis. The model incorporates a novel bag-level contrastive pretraining strategy that applies contrastive learning at the study level rather than individual images, and uses KL divergence to align attention weights with clinically relevant views. The approach is evaluated on an open-access dataset and external validation set, demonstrating improved accuracy and reduced model size compared to previous methods.

## Key Results
- SAMIL achieves 76% balanced accuracy on AS severity classification, significantly outperforming state-of-the-art attention-based MIL architectures (60-67%)
- The model reduces computational complexity while maintaining superior diagnostic performance
- Evaluation on both the TMED-2 dataset and an external 2022-Validation set confirms robustness across different data distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised attention guides the model to focus on clinically relevant views (PLAX and PSAX) for AS diagnosis
- Mechanism: The model uses a pre-trained view classifier to predict view relevance probabilities for each image. These probabilities are then used to supervise the attention mechanism through a KL divergence loss, ensuring the learned attention weights align with clinically relevant views.
- Core assumption: The pre-trained view classifier accurately identifies relevant views for AS diagnosis
- Evidence anchors:
  - [abstract]: "First, a supervised attention technique guides the learned attention mechanism to favor relevant views."
  - [section 4.3]: "To implement this idea, we introduce a new loss term, which we call supervised attention (SA), that directly steers the attention weights A = {a1, . . . aK} produced by Eq. (1) to match normalized relevance scores R = {r1, . . . rK} from a view-relevance classifier v"
- Break condition: If the view classifier's accuracy on identifying relevant views drops below a threshold (e.g., 80%), the supervised attention mechanism's effectiveness would be compromised.

### Mechanism 2
- Claim: Bag-level contrastive pretraining improves the quality of study-level representations for AS diagnosis
- Mechanism: The model applies contrastive learning at the study level, pulling together representations of the same study under different augmentations (positive pairs) and pushing away representations of different studies (negative pairs). This encourages the model to learn representations that capture the overall characteristics of an entire echocardiogram study.
- Core assumption: The contrastive learning objective at the study level aligns with the downstream AS diagnosis task
- Evidence anchors:
  - [abstract]: "Second, a novel self-supervised pretraining strategy applies contrastive learning on the representation of the whole study instead of individual images as commonly done in prior literature."
  - [section 4.4]: "Reasoning that what ultimately matters is the quality of the study-level representation z produced by our MIL architecture, we adapted MoCo to produce solid representations of entire echocardiogram studies."
- Break condition: If the contrastive learning objective at the study level does not align well with the AS diagnosis task (e.g., due to class collision), the pretraining strategy would not be effective.

### Mechanism 3
- Claim: The combination of supervised attention and bag-level pretraining yields better AS diagnosis performance than individual components
- Mechanism: Supervised attention ensures the model focuses on clinically relevant views, while bag-level pretraining improves the quality of study-level representations. Together, they enable the model to make more accurate AS diagnoses by considering the right views and learning better representations of the entire study.
- Core assumption: The two components complement each other and do not interfere with each other's effectiveness
- Evidence anchors:
  - [abstract]: "Experiments on an open-access dataset and an external validation set show that our approach yields higher accuracy while reducing model size."
  - [section 5.1]: "Our proposed method, SAMIL, scores 76%, signficantly better than other state-of-the-art attention-based MIL architectures we tested (which span 60-67%)."
- Break condition: If the combination of supervised attention and bag-level pretraining does not lead to improved performance compared to individual components (as shown in ablation studies), the synergy between the two mechanisms would be questionable.

## Foundational Learning

- Concept: Multiple Instance Learning (MIL)
  - Why needed here: The problem of AS diagnosis from echocardiograms is naturally formulated as an MIL problem, where each study consists of multiple images (instances) and the goal is to predict the study-level AS severity.
  - Quick check question: In MIL, what is the difference between instance-based and embedding-based approaches?

- Concept: Attention mechanisms in deep learning
  - Why needed here: Attention mechanisms allow the model to focus on the most relevant parts of the input, which is crucial for AS diagnosis as only certain views (PLAX and PSAX) show the aortic valve.
  - Quick check question: How does the attention mechanism in SAMIL differ from the standard attention-based MIL approach (ABMIL)?

- Concept: Self-supervised learning and contrastive learning
  - Why needed here: Self-supervised pretraining, specifically contrastive learning at the study level, helps the model learn better representations of entire echocardiogram studies, which is beneficial for the downstream AS diagnosis task.
  - Quick check question: What is the main difference between image-level and bag-level contrastive learning in the context of SAMIL?

## Architecture Onboarding

- Component map:
  Image → Instance representation → Supervised attention + Flexible attention → Study-level representation → AS severity prediction

- Critical path: Image → Instance representation → Supervised attention + Flexible attention → Study-level representation → AS severity prediction

- Design tradeoffs:
  - Using a pre-trained view classifier for supervised attention vs. learning attention purely from diagnosis labels
  - Bag-level vs. image-level contrastive pretraining
  - Flexibility in attention vs. strict adherence to clinically relevant views

- Failure signatures:
  - Poor AS diagnosis performance: Could indicate issues with the instance representation layer, attention mechanisms, or pretraining strategy
  - Attention focusing on irrelevant views: Could indicate problems with the supervised attention module or the view classifier
  - Overfitting to the training data: Could indicate insufficient regularization or a lack of diversity in the training data

- First 3 experiments:
  1. Evaluate the performance of SAMIL on the TMED-2 dataset and compare it to baseline methods (ABMIL, DSMIL, etc.)
  2. Analyze the attention weights learned by SAMIL and check if they align with clinically relevant views (PLAX and PSAX)
  3. Perform ablation studies to assess the impact of supervised attention and bag-level pretraining on AS diagnosis performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does SAMIL generalize to datasets from other hospitals or countries?
- Basis in paper: [inferred] The authors note that TMED-2 is limited in size and demographics due to drawing from just one hospital site, and further assessment is needed to understand how SAMIL generalizes.
- Why unresolved: The evaluation was only performed on the TMED-2 dataset from Tufts Medical Center. External validation on data from other institutions or countries is needed.
- What evidence would resolve it: Testing SAMIL on echocardiograms from other hospitals, especially in different countries, and reporting balanced accuracy and other metrics.

### Open Question 2
- Question: How would incorporating Doppler images and high-resolution cineloop videos affect SAMIL's performance?
- Basis in paper: [explicit] The authors state that human experts assess AS using additional factors not available to SAMIL, including Doppler images and high-resolution cineloop videos from 2D TTE.
- Why unresolved: The current SAMIL implementation only uses lower-resolution single frame images. Adapting the MIL architecture to incorporate Doppler and cineloop video data has not been tested.
- What evidence would resolve it: Extending SAMIL to process Doppler and video data and evaluating its performance on a dataset containing these modalities.

### Open Question 3
- Question: How does SAMIL's performance compare to commercial deep learning systems for AS assessment?
- Basis in paper: [inferred] The authors mention recent work by Krishna et al. (2023) showing a commercial system can closely emulate human performance on elementary AS measures, but cannot assign study-level AS severity ratings.
- Why unresolved: SAMIL's performance on assigning study-level AS severity ratings was not directly compared to any commercial systems.
- What evidence would resolve it: Head-to-head comparison of SAMIL and a commercial deep learning system on the same dataset, evaluating both the ability to assess elementary measures and assign study-level severity ratings.

## Limitations

- Limited ablation studies that isolate the contributions of supervised attention versus bag-level pretraining
- Heavy dependency on the accuracy of the view classifier for supervised attention effectiveness
- Evaluation only on a single dataset from one hospital, limiting generalizability assessment

## Confidence

**High confidence**: The core MIL framework implementation and general architecture design are well-specified and reproducible.

**Medium confidence**: The supervised attention mechanism's effectiveness is supported by experimental results, but the dependency on the view classifier accuracy introduces uncertainty.

**Medium confidence**: The bag-level contrastive pretraining strategy shows promise, but its specific contribution relative to other pretraining approaches needs further validation.

## Next Checks

1. Implement and test variants of the model without supervised attention and without bag-level pretraining to quantify each component's contribution to performance.

2. Test the view classifier's performance across different patient populations and image qualities to establish its reliability as a foundation for supervised attention.

3. Evaluate the complete SAMIL approach on additional independent datasets from different institutions to assess generalizability and robustness across diverse clinical settings.