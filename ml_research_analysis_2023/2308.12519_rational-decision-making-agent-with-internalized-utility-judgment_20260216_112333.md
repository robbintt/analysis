---
ver: rpa2
title: Rational Decision-Making Agent with Internalized Utility Judgment
arxiv_id: '2308.12519'
source_url: https://arxiv.org/abs/2308.12519
tags:
- decision
- llms
- decision-making
- process
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RadAgent, a decision-making agent that internalizes
  utility judgment for autonomous decision-making. The key innovation is an Elo-based
  Self-Judgment Mechanism that assigns Elo scores to individual decision steps, allowing
  the agent to evaluate the utility of each step via pairwise comparisons.
---

# Rational Decision-Making Agent with Internalized Utility Judgment

## Quick Facts
- arXiv ID: 2308.12519
- Source URL: https://arxiv.org/abs/2308.12519
- Reference count: 7
- Primary result: RadAgent achieves over 10% improvement in Pass Rate on ToolBench dataset

## Executive Summary
This paper introduces RadAgent, a decision-making agent that internalizes utility judgment for autonomous decision-making. The key innovation is an Elo-based Self-Judgment Mechanism that assigns Elo scores to individual decision steps, allowing the agent to evaluate the utility of each step via pairwise comparisons. RadAgent uses these scores to guide decision exploration toward optimal solutions. Experiments on the ToolBench dataset demonstrate that RadAgent achieves over 10% improvement in Pass Rate compared to baselines, produces higher-quality solutions, and reduces the number of ChatGPT API calls, highlighting its effectiveness and efficiency.

## Method Summary
RadAgent implements an Elo-based Self-Judgment Mechanism that assigns Elo scores to decision steps through iterative pairwise comparisons. The agent adjusts Elo scores of final decision steps first, then propagates updates to intermediate steps. Temperature annealing based on Elo update count ensures reliable exploration-exploitation balance, while a rejection decision step allows flexible exploration by enabling restarts from intermediate states. The method guides decision exploration toward optimal solutions by leveraging internalized utility judgment rather than external performance metrics.

## Key Results
- Over 10% improvement in Pass Rate compared to baselines on ToolBench dataset
- Produces higher-quality solutions with fewer ChatGPT API calls
- Demonstrates effective autonomous decision-making through internalized utility judgment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Elo-based Self-Judgment Mechanism assigns accurate numerical utilities to decision steps through iterative pairwise comparisons
- Mechanism: Decision steps are compared in pairs, with winners gaining Elo points and losers losing them. Intermediate step values are computed as weighted averages of their children's Elo scores
- Core assumption: Pairwise comparisons converge to true utility values, and intermediate steps inherit utility proportionally from subsequent steps
- Evidence anchors:
  - [abstract] "Elo-based Utility Construction is devised to assign Elo scores to individual decision steps to judge their utilities via pairwise comparisons"
  - [section] "We first adjust the Elo scores of the final decision steps of each decision sequence via pairwise comparison and then update the Elo scores of the intermediate decision steps gradually"
- Break condition: If pairwise comparisons are inconsistent or the Elo coefficient is poorly tuned, scores may converge to incorrect values or oscillate

### Mechanism 2
- Claim: Temperature annealing based on Elo update count ensures reliable exploration-exploitation balance
- Mechanism: Early in training, high temperature encourages exploration of all decision paths. As Elo scores stabilize through more comparisons, temperature decreases, favoring exploitation of high-value decisions
- Core assumption: Early decisions have uncertain utilities, while late decisions have more reliable Elo scores
- Evidence anchors:
  - [section] "Let Md be the number of the Elo update of the decision step d. The temperature of d is annealing as follows: τd = τ0 ∗ 1/(1 + p√ln(Md + 1))"
- Break condition: If Elo scores converge too slowly, early temperature annealing may prematurely reduce exploration

### Mechanism 3
- Claim: Rejection decision step enables flexible exploration by allowing the agent to restart from intermediate states
- Mechanism: When all existing subsequent decisions have low Elo scores, the agent can choose a rejection step to explore new decision directions from the current state
- Core assumption: Some decision paths are unproductive and should be abandoned in favor of exploring alternatives
- Evidence anchors:
  - [section] "We define a rejection decision step d̂ with an initial Elo score v̂ to represent that 'Existing decision steps do not deserve to explore, LLMs reject them and decide to explore a new decision direction'"
- Break condition: If rejection threshold is too low, the agent may abandon promising paths prematurely; if too high, it may waste resources on unproductive exploration

## Foundational Learning

- Concept: Elo rating system and pairwise comparison convergence
  - Why needed here: Understanding how Elo scores are computed and how they converge to true skill levels is essential for implementing the self-judgment mechanism
  - Quick check question: If player A beats player B, how does this affect their Elo scores, and what determines the magnitude of the change?

- Concept: Markov decision processes and state-action transitions
  - Why needed here: The decision-making process is formalized as an MDP, where each decision step represents a state-action pair
  - Quick check question: In the MDP formulation, what does the transition probability P(s'|a,s) represent, and how does it relate to the decision steps in RadAgent?

- Concept: Temperature scheduling and exploration-exploitation tradeoff
  - Why needed here: The annealing temperature schedule controls the balance between exploring new decisions and exploiting known good ones
  - Quick check question: What happens to the probability distribution over decisions when temperature approaches zero versus infinity?

## Architecture Onboarding

- Component map: Decision Exploration module → Elo-based Self-Judgment Mechanism → Optimum Selection module → ToolBench API calls
- Critical path: Decision Exploration → Self-Judgment → Exploration (iterative loop) → Optimum Selection
- Design tradeoffs: Accuracy vs. API cost (more comparisons improve Elo scores but increase API usage), exploration breadth vs. depth (rejection step enables breadth but may miss deep solutions)
- Failure signatures: Stuck in local optima (insufficient exploration), inconsistent Elo scores (poor pairwise comparisons), high API costs (inefficient exploration)
- First 3 experiments:
  1. Validate Elo score convergence on a simple task with known optimal solution
  2. Test temperature annealing effect on exploration-exploitation balance
  3. Measure API cost vs. Pass Rate tradeoff with varying comparison limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Elo score update mechanism handle cases where the comparison results are noisy or inconsistent?
- Basis in paper: [inferred] The paper mentions that the Elo score converges to an accurate value through iterative comparisons, but it doesn't discuss how the mechanism handles noisy or inconsistent comparison results.
- Why unresolved: The paper doesn't provide information on how the Elo score update mechanism deals with noisy or inconsistent comparison results, which is important for understanding the robustness of the approach.
- What evidence would resolve it: Experiments showing the Elo score's convergence behavior under various levels of noise in the comparison results, or a theoretical analysis of the update mechanism's resilience to noise.

### Open Question 2
- Question: How does the choice of the Elo coefficient (r) affect the performance of JUDEC?
- Basis in paper: [explicit] The paper mentions that the Elo coefficient r is set as 173.72 according to the vanilla Elo rating system, but it doesn't discuss how this choice affects the performance of JUDEC.
- Why unresolved: The paper doesn't provide any analysis or experiments to show how the choice of the Elo coefficient impacts the performance of JUDEC.
- What evidence would resolve it: Experiments comparing the performance of JUDEC with different values of the Elo coefficient, or a theoretical analysis of how the coefficient affects the Elo score update mechanism.

### Open Question 3
- Question: How does the temperature annealing mechanism (Equation 8) affect the exploration-exploitation trade-off in JUDEC?
- Basis in paper: [explicit] The paper mentions that the temperature is annealed based on the number of Elo updates to adjust the exploration-exploitation trade-off, but it doesn't provide a detailed analysis of this mechanism.
- Why unresolved: The paper doesn't provide a detailed analysis or experiments to show how the temperature annealing mechanism affects the exploration-exploitation trade-off in JUDEC.
- What evidence would resolve it: Experiments comparing the performance of JUDEC with and without the temperature annealing mechanism, or a theoretical analysis of how the mechanism affects the exploration-exploitation trade-off.

## Limitations

- Limited empirical validation of Elo convergence reliability across different decision spaces
- Experiments only conducted on ToolBench dataset, limiting generalization claims
- Insufficient characterization of API cost vs. quality tradeoff relationship

## Confidence

**High Confidence**: The core Elo-based mechanism for assigning scores to decision steps is technically sound and implementable. The temperature annealing approach for balancing exploration and exploitation follows established principles in reinforcement learning.

**Medium Confidence**: The experimental results showing 10%+ Pass Rate improvement over baselines are likely valid within the ToolBench domain, but the magnitude of improvement and generalizability require further validation.

**Low Confidence**: Claims about RadAgent representing "genuine autonomous decision-making" through internalized utility judgment are more philosophical than empirically demonstrated. The paper doesn't establish whether the agent truly understands utility or merely optimizes for Elo scores.

## Next Checks

1. **Elo Convergence Analysis**: Systematically test the mechanism on tasks with known optimal solutions to verify that Elo scores actually converge to reflect true decision utilities. Track convergence rates and test sensitivity to the Elo coefficient and temperature parameters.

2. **Cross-Domain Generalization**: Evaluate RadAgent on decision-making tasks outside ToolBench (e.g., game-playing, planning, or creative writing) to assess whether the internalized judgment mechanism transfers effectively to different problem domains.

3. **Cost-Quality Tradeoff Characterization**: Conduct experiments varying the number of Elo comparisons allowed per decision path to map the relationship between API costs, computation time, and solution quality improvements. Identify the point of diminishing returns.