---
ver: rpa2
title: Do large language models resemble humans in language use?
arxiv_id: '2303.08014'
source_url: https://arxiv.org/abs/2303.08014
tags:
- chatgpt
- word
- words
- than
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Researchers investigated whether large language models like ChatGPT
  exhibit human-like language processing by subjecting them to 12 psycholinguistic
  experiments with 1000 runs each. ChatGPT replicated human patterns in 10 out of
  12 experiments, including sound-shape associations, word meaning priming, structural
  priming, semantic illusions, and interlocutor sensitivity.
---

# Do large language models resemble humans in language use?

## Quick Facts
- arXiv ID: 2303.08014
- Source URL: https://arxiv.org/abs/2303.08014
- Reference count: 40
- Primary result: ChatGPT replicated human patterns in 10 out of 12 psycholinguistic experiments, including sound-shape associations, structural priming, and interlocutor sensitivity

## Executive Summary
This study investigates whether ChatGPT exhibits human-like language processing by subjecting it to 12 psycholinguistic experiments. The researchers found that ChatGPT successfully replicated human patterns in 10 of the 12 experiments, including sound-shape associations, word meaning priming, structural priming, semantic illusions, and interlocutor sensitivity. However, it failed to show human-like behavior in using context to resolve syntactic ambiguities and preferring shorter words in predictable contexts. These findings suggest that while ChatGPT approximates human language processing to a significant extent, there are important limitations that distinguish its processing from human cognition.

## Method Summary
The researchers conducted a preregistered study using ChatGPT versions from December 2022 to February 2023. They designed 12 psycholinguistic experiments based on classic human studies, presenting each experiment 1000 times with randomized stimuli. Responses were automatically extracted where possible or manually coded by native English speakers. Statistical analysis employed logit mixed effects modeling to compare ChatGPT's responses against human data patterns, examining whether the model showed human-like processing across various linguistic domains.

## Key Results
- ChatGPT successfully replicated human patterns in 10 out of 12 psycholinguistic experiments
- The model showed human-like sound-shape associations, structural priming, and interlocutor sensitivity
- ChatGPT failed to use context for syntactic ambiguity resolution and didn't prefer shorter words in predictable contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT associates novel words with shapes based on sound symbolic patterns
- Mechanism: Sub-word tokenization allows tokens like "ak" to co-occur with references to spikiness in training data, enabling shape-sound associations
- Core assumption: The transformer architecture learns co-occurrence patterns between sub-word tokens and semantic features
- Evidence anchors: ChatGPT assigned round-sounding novel words to round shapes more often than spiky-sounding words; connectionist models have been trained to infer genders from names
- Break condition: If sub-word tokenization doesn't capture phonological patterns, or if training data lacks sufficient shape-sound co-occurrences

### Mechanism 2
- Claim: ChatGPT exhibits structural priming by reusing syntactic structures
- Mechanism: Context-sensitive word representations update probabilities of upcoming words, which influences syntactic choices in sentence completion tasks
- Core assumption: The transformer's attention mechanism weights objects more heavily than subjects following experiencer-stimulus verbs
- Evidence anchors: ChatGPT tended to re-use syntactic structures of recently produced sentences; attention mechanism should lead to weighting objects more heavily than subjects following experiencer-stimulus verbs
- Break condition: If the model doesn't update probabilities based on recent context, or if attention weights don't align with verb semantics

### Mechanism 3
- Claim: ChatGPT is sensitive to interlocutor identity in word meaning access
- Mechanism: Recent exposure to American English or New York influences the representation of cross-dialectally ambiguous words, increasing likelihood of accessing American meanings
- Core assumption: The model stores conversation history and updates representations based on interlocutor identity
- Evidence anchors: ChatGPT was more likely to access American English meanings when interlocutor self-identified as American English speaker; ChatGPT stores about 3,000 words of conversation history
- Break condition: If conversation history is not stored, or if interlocutor identity doesn't influence word representations

## Foundational Learning

- Concept: Sub-word tokenization
  - Why needed here: Enables the model to handle out-of-vocabulary words and capture phonological patterns through co-occurring tokens
  - Quick check question: How does sub-word tokenization differ from word-level tokenization, and why is it important for handling novel words?

- Concept: Attention mechanism
  - Why needed here: Determines how the model weights different parts of input when making predictions, influencing syntactic choices and semantic interpretations
  - Quick check question: How does the attention mechanism affect the model's ability to resolve syntactic ambiguities?

- Concept: Context-sensitive representations
  - Why needed here: Allows the model to update word meanings based on recent usage, interlocutor identity, and discourse context
  - Quick check question: What evidence suggests that ChatGPT updates word representations based on context, and how does this affect its behavior?

## Architecture Onboarding

- Component map: Tokenizer -> Transformer layers -> Output layer -> Conversation history buffer

- Critical path:
  1. Input text is tokenized into sub-word units
  2. Tokens are processed through transformer layers
  3. Attention mechanisms weight relationships between tokens
  4. Output layer generates probability distribution for next token
  5. Most likely token is selected and added to output
  6. Process repeats until completion or maximum length reached

- Design tradeoffs:
  - Sub-word tokenization vs. word-level tokenization: Sub-word allows handling of rare words but may miss some phonological patterns
  - Attention window size: Larger windows capture more context but increase computational cost
  - Conversation history length: Longer history enables better interlocutor modeling but increases memory usage

- Failure signatures:
  - Incorrect shape-sound associations: Sub-word tokens not capturing phonological patterns
  - Lack of structural priming: Context updates not influencing syntactic choices
  - Insensitivity to interlocutor: Conversation history not being used effectively

- First 3 experiments:
  1. Test shape-sound associations with novel words containing different sub-word tokens
  2. Examine structural priming by manipulating verb types and syntactic structures in prime sentences
  3. Investigate interlocutor sensitivity by varying dialectal background information in prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs exhibit similar limitations in syntactic ambiguity resolution across different types of syntactic ambiguities beyond VP/NP ambiguity?
- Basis in paper: The paper found ChatGPT failed to use context to resolve VP/NP ambiguity, raising questions about whether this is specific to this ambiguity type or a general limitation.
- Why unresolved: The study only tested one type of syntactic ambiguity (VP/NP), leaving open whether LLMs struggle with other ambiguity types similarly.
- What evidence would resolve it: Testing LLMs on multiple syntactic ambiguity types (e.g., PP attachment, quantifier scope) while manipulating contextual information would reveal if this is a general limitation.

### Open Question 2
- Question: How do token-level representations in LLMs influence their ability to detect and process semantic illusions compared to humans?
- Basis in paper: The paper notes ChatGPT detects far fewer semantic illusions than humans and discusses how token-level processing might explain this difference.
- Why unresolved: While the paper suggests token-level processing may explain differences, it doesn't directly test this hypothesis or explore the mechanism.
- What evidence would resolve it: Experiments comparing LLM responses when processing full words versus subword tokens, or when using different tokenization strategies, could reveal how token-level processing affects semantic illusion detection.

### Open Question 3
- Question: Does the strength of interlocutor sensitivity in LLMs change based on the type of linguistic feature being modulated (word meaning vs lexical choice)?
- Basis in paper: The paper found interlocutor sensitivity for both word meaning access and lexical retrieval, but with different patterns over time, suggesting the mechanism may differ.
- Why unresolved: The paper observed different patterns of interlocutor sensitivity across experiments but didn't test whether this reflects different underlying mechanisms.
- What evidence would resolve it: Testing LLMs on multiple interlocutor-sensitive phenomena (pronunciation, grammar, discourse structure) while tracking response patterns over time would reveal if sensitivity varies systematically by linguistic feature type.

## Limitations

- The study's findings rely on ChatGPT's behavior, which may change with model updates or different prompting strategies
- The experiments use automated response extraction where possible, but some manual coding introduces potential human error
- The research focuses on English language patterns, limiting generalizability to other languages
- The study doesn't test whether similar patterns hold for other large language models or different ChatGPT versions beyond the four tested

## Confidence

**High confidence** in the finding that ChatGPT replicates human patterns in sound-shape associations and structural priming, supported by multiple experimental runs and clear statistical significance.

**Medium confidence** in the results regarding interlocutor sensitivity and semantic illusions, as these experiments rely more heavily on manual coding and interpretation.

**Low confidence** in the generalizability of findings to other LLMs or language tasks not tested in this study, given the specific experimental setup and limited model comparison.

## Next Checks

1. **Replication with different model versions**: Run the same 12 experiments using the latest ChatGPT version and document any performance changes compared to the original results.

2. **Cross-linguistic validation**: Adapt 3-4 key experiments (e.g., sound-shape associations, structural priming) for a different language to test whether observed patterns are language-specific or general LLM properties.

3. **Comparison with alternative LLMs**: Test the same experimental protocol on at least two other large language models (e.g., Claude, Llama) to determine if the human-like patterns are specific to ChatGPT or common across LLMs.