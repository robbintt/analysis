---
ver: rpa2
title: Selective Uncertainty Propagation in Offline RL
arxiv_id: '2302.00284'
source_url: https://arxiv.org/abs/2302.00284
tags:
- policy
- value
- learning
- state
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses offline reinforcement learning (RL) in finite-horizon
  MDPs, where actions can influence next-state distributions, making the problem more
  challenging than offline contextual bandits. The authors formalize this hardness
  by introducing a "shift model" that captures the heterogeneous treatment effect
  of actions on next-state distributions.
---

# Selective Uncertainty Propagation in Offline RL

## Quick Facts
- **arXiv ID:** 2302.00284
- **Source URL:** https://arxiv.org/abs/2302.00284
- **Reference count:** 40
- **Key outcome:** This paper addresses offline reinforcement learning (RL) in finite-horizon MDPs, where actions can influence next-state distributions, making the problem more challenging than offline contextual bandits. The authors formalize this hardness by introducing a "shift model" that captures the heterogeneous treatment effect of actions on next-state distributions. They propose selective uncertainty propagation, a method that propagates just enough value function uncertainty from future steps to avoid model exploitation, adapting to the instance's hardness. Their approach is supported by theoretical guarantees and simulations. In simulations, their method (SPVI) outperforms pessimistic value iteration (PVI) and pessimistic supervised learning (PSL) in environments with both dynamic and bandit-like elements, demonstrating the benefits of selectively propagating uncertainty based on the estimated shift model.

## Executive Summary
This paper tackles the challenge of offline reinforcement learning in finite-horizon MDPs where actions can influence next-state distributions, making it more complex than offline contextual bandits. The authors introduce a "shift model" that quantifies the treatment effect of actions on next-state distributions, formalizing the hardness of the problem. They propose selective uncertainty propagation, which propagates only necessary value function uncertainty based on the estimated shift, adapting to the instance's hardness. This approach is theoretically supported and shows superior performance in simulations compared to traditional pessimistic methods, especially in environments with mixed bandit and dynamic elements.

## Method Summary
The paper proposes selective pessimism with a model-based estimator and selectively pessimistic value iteration (SPVI) for offline RL. The method involves training shift, reward, and behavioral policy transition models using supervised learning, then using these models to compute the estimated shift for each state-action pair. SPVI iteratively constructs policies by maximizing the selectively pessimistic Q-value, where the uncertainty propagation is scaled by the estimated shift. This approach aims to propagate only necessary value function uncertainty, adapting to the instance's hardness and improving upon traditional pessimistic methods.

## Key Results
- SPVI outperforms pessimistic value iteration (PVI) and pessimistic supervised learning (PSL) in environments with both dynamic and bandit-like elements
- The method demonstrates the benefits of selectively propagating uncertainty based on the estimated shift model
- Theoretical guarantees support the approach, with sample complexity bounds that adapt to instance hardness measured by the shift model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SPVI avoids unnecessary pessimism when the estimated shift is small, enabling contextual bandit-like performance
- Mechanism: The algorithm scales the uncertainty bonus (Γ) by the estimated shift (∆), so when ∆≈0 the propagated uncertainty vanishes
- Core assumption: The estimated shift accurately reflects the true treatment effect of actions on next-state distributions
- Evidence anchors:
  - [abstract]: "propagates just enough value function uncertainty... adapting to the instance's hardness"
  - [section]: "the third term in Lπ,r(h) only bounds average errors and is hence small with good supervised learning sub-routine"
  - [corpus]: Weak evidence - corpus doesn't contain explicit shift estimation results
- Break condition: If the shift estimator is biased or has high variance, SPVI may under-propagate uncertainty and exploit model errors

### Mechanism 2
- Claim: Selective pessimism enables planning when needed but avoids conservatism in bandit-like states
- Mechanism: In states where all actions have small estimated shifts, SPVI behaves like a contextual bandit algorithm; in dynamic states it preserves standard pessimistic propagation
- Core assumption: The MDP has regions that are bandit-like (small shifts) and regions that require planning (large shifts)
- Evidence anchors:
  - [abstract]: "improve upon traditional pessimistic approaches for offline RL on statistically simple instances"
  - [section]: "When the policyπ has a small estimated shift, we can construct tighter policy evaluation bounds that enjoy contextual bandit-type guarantees"
  - [corpus]: No direct corpus evidence of mixed bandit/RL environments
- Break condition: If the MDP is uniformly dynamic (large shifts everywhere), SPVI provides no advantage over standard pessimistic methods

### Mechanism 3
- Claim: Instance-dependent sample complexity adapts to the hardness measured by the shift model
- Mechanism: The theoretical bounds show sample complexity scales with the estimated shift magnitude rather than worst-case state space size
- Core assumption: The shift estimator provides meaningful instance-specific hardness information
- Evidence anchors:
  - [abstract]: "the statistical hardness of offline RL instances can be measured by estimating the size of actions' impact on next-state distributions"
  - [section]: "this sample complexity smoothly degrades as we increase the hardness of the instance by increasing the effect optimal actions have on next-state distributions"
  - [corpus]: Weak evidence - corpus doesn't contain specific sample complexity comparisons
- Break condition: If the shift estimation is uninformative (always near zero or always large), the adaptive guarantees don't materialize

## Foundational Learning

- Concept: Distribution shift and its impact on offline RL
  - Why needed here: The paper's core contribution is adapting to distribution shift hardness via the shift model
  - Quick check question: What makes offline RL harder than offline contextual bandits when actions affect next-state distributions?

- Concept: Causal inference and heterogeneous treatment effects
  - Why needed here: The shift model formalizes the treatment effect of actions on next-state distributions
  - Quick check question: How does the shift model (∆) capture the heterogeneous treatment effect of actions?

- Concept: Pessimistic value iteration and uncertainty propagation
  - Why needed here: SPVI builds on pessimistic VI but modifies how uncertainty is propagated based on the shift model
  - Quick check question: What is the key difference between standard pessimistic VI and the selective pessimism in SPVI?

## Architecture Onboarding

- Component map:
  - Shift model estimator (ˆ∆) -> Reward model estimator (ˆR) -> Behavioral policy transition estimator (ˆPb) -> Conservative value function estimator -> SPVI algorithm

- Critical path:
  1. Train shift, reward, and behavioral policy transition models using supervised learning
  2. Use these models to compute the estimated shift (ˆ∆) for each state-action pair
  3. Run SPVI using these estimates to determine how much uncertainty to propagate
  4. The algorithm iteratively constructs policies by maximizing the selectively pessimistic Q-value

- Design tradeoffs:
  - Accuracy vs. conservatism: More accurate shift estimates allow less conservatism but require better estimation
  - Planning vs. bandit behavior: Regions with small shifts can be handled bandit-style, but require correctly identifying them
  - Computational cost: Computing and storing the shift model adds overhead compared to standard pessimistic VI

- Failure signatures:
  - If shift estimates are noisy, SPVI may under-propagate uncertainty and exploit model errors
  - If shift estimates are systematically biased, SPVI may be unnecessarily conservative in some regions
  - If the behavioral policy is very poor, importance weights may dominate the bounds

- First 3 experiments:
  1. Implement SPVI on a simple GridWorld with known ground truth shifts to verify selective pessimism behavior
  2. Compare SPVI vs PVI on a chain environment with both bandit and dynamic states to demonstrate the benefit
  3. Test SPVI's sensitivity to shift estimation quality by adding noise to the shift estimates and measuring performance degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the selective pessimism approach be extended to infinite-horizon MDPs with discounted rewards?
- Basis in paper: [inferred] The paper focuses on finite-horizon MDPs and mentions that lower bounds for infinite-horizon offline RL are expected to be similar, suggesting potential for extension.
- Why unresolved: The current theoretical framework and algorithms are specifically designed for finite-horizon settings. Infinite-horizon MDPs introduce additional complexity, such as handling the discount factor and potentially different exploration-exploitation trade-offs.
- What evidence would resolve it: Developing and analyzing selective pessimism algorithms for infinite-horizon MDPs, demonstrating improved sample efficiency compared to existing methods like pessimistic Q-learning, would provide strong evidence.

### Open Question 2
- Question: How does the performance of selective pessimism scale with the size of the state space in high-dimensional environments?
- Basis in paper: [explicit] The paper mentions that in the worst-case, the dependence of confidence interval width on state-space size is unavoidable, but selective pessimism aims to move beyond this worst-case scenario.
- Why unresolved: While the paper shows benefits of selective pessimism in toy environments, its performance in high-dimensional state spaces remains unclear. The effectiveness of estimating and utilizing the shift model in such settings is uncertain.
- What evidence would resolve it: Conducting experiments on high-dimensional RL benchmarks (e.g., Atari games, continuous control tasks) and comparing the sample efficiency of selective pessimism to existing methods would provide valuable insights.

### Open Question 3
- Question: Can selective pessimism be combined with function approximation techniques to handle continuous state spaces?
- Basis in paper: [inferred] The paper abstracts away the model estimation sub-routines using oracle assumptions, suggesting flexibility in the choice of function approximation methods.
- Why unresolved: The current analysis assumes discrete state spaces and relies on average error bounds for model estimation. Extending this to continuous state spaces would require careful consideration of function approximation errors and their impact on the selective pessimism approach.
- What evidence would resolve it: Developing and analyzing selective pessimism algorithms that incorporate function approximation techniques (e.g., neural networks, linear function approximation) and demonstrating improved performance in continuous state space environments would provide strong evidence.

## Limitations

- The paper's theoretical guarantees depend heavily on the accuracy of the shift model estimator, which is not empirically validated
- The empirical evaluation is limited to one synthetic environment, making it unclear how the approach generalizes to real-world problems
- The assumptions about oracle access to accurate model estimators (Assumption 4.1) and conservative value function estimation (Assumption 4.2) are strong and not demonstrated in practice

## Confidence

- **High confidence:** The mechanism of scaling uncertainty by estimated shifts is mathematically sound and clearly explained
- **Medium confidence:** The theoretical sample complexity bounds are valid but depend on oracle assumptions that may not hold in practice
- **Low confidence:** The empirical superiority over PVI and PSL is demonstrated only in a single synthetic environment

## Next Checks

1. **Shift estimator validation:** Implement and evaluate the shift model estimator on the Chain Bandit environment with varying levels of estimation noise to understand its impact on SPVI performance
2. **Generalization test:** Apply SPVI to standard offline RL benchmarks (e.g., D4RL datasets) to assess performance beyond the synthetic environment
3. **Oracle relaxation:** Replace the oracle assumptions with practical estimators (e.g., behavior cloning for transitions, regression for rewards) and measure the degradation in performance compared to the theoretical guarantees