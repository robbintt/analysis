---
ver: rpa2
title: 'Topic-DPR: Topic-based Prompts for Dense Passage Retrieval'
arxiv_id: '2310.06626'
source_url: https://arxiv.org/abs/2310.06626
tags:
- prompts
- prompt
- retrieval
- topic
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the issue of semantic space collapse in dense
  passage retrieval when using a single prompt, which leads to poor differentiation
  between relevant and irrelevant passages. To solve this, the authors propose Topic-DPR,
  a method that uses multiple topic-based prompts instead of a single prompt.
---

# Topic-DPR: Topic-based Prompts for Dense Passage Retrieval

## Quick Facts
- arXiv ID: 2310.06626
- Source URL: https://arxiv.org/abs/2310.06626
- Reference count: 6
- Surpasses previous state-of-the-art retrieval techniques

## Executive Summary
Topic-DPR addresses semantic space collapse in dense passage retrieval by using multiple topic-based prompts instead of a single prompt. The approach leverages topic modeling to derive prompts that guide the model to produce diverse semantic representations, improving space uniformity. A novel positive and negative sampling strategy leveraging semi-structured data further enhances retrieval efficiency.

## Method Summary
Topic-DPR uses hierarchical Latent Dirichlet Allocation (hLDA) to extract topics from semi-structured datasets, generating multiple topic-based prompts. These prompts are optimized simultaneously through contrastive learning, encouraging representations to align with their topic distributions. The method also introduces an in-batch positive and negative sampling strategy that leverages multi-category information to boost dense retrieval efficiency.

## Key Results
- Outperforms previous state-of-the-art retrieval techniques on arXiv-Article and USPTO-Patent datasets
- Improves space uniformity through multiple topic-based prompts
- Achieves better differentiation between relevant and irrelevant passages

## Why This Works (Mechanism)

### Mechanism 1
- Using multiple topic-based prompts reduces semantic space collapse
- Multiple prompts guide the model to produce diverse semantic representations
- Topic modeling captures semantic diversity of the dataset
- Contrastive learning aligns representations with topic distributions

### Mechanism 2
- Contrastive learning with topic-topic relation loss maintains topical relationships
- Loss function pushes passages encoded with different prompts apart
- Each prompt guides the model to a distinct semantic space
- Enhances diversity of learned representations

### Mechanism 3
- In-batch positive and negative sampling strategy improves retrieval efficiency
- Leverages multi-category information from documents
- Identifies relevant queries or passages for training
- Ensures high-quality positive and negative samples

## Foundational Learning

- Concept: Topic modeling
  - Why needed here: Extracts semantic diversity for generating multiple topic-based prompts
  - Quick check question: How does hierarchical Latent Dirichlet Allocation (hLDA) capture the hierarchical structure among topics?

- Concept: Contrastive learning
  - Why needed here: Maintains topical relationships and enhances representation diversity
  - Quick check question: How does the loss function in contrastive learning push passages encoded with different prompts away from each other?

- Concept: Attention mechanism
  - Why needed here: Amalgamates topic-based prompts with PLM embeddings
  - Quick check question: How does the attention mechanism facilitate the interaction between the topic-based prompts and the PLM embeddings?

## Architecture Onboarding

- Component map: Topic modeling → Prompt encoder → Prefix matrix → PLM → Contrastive learning framework
- Critical path: Topic modeling → Prompt encoder → Prefix matrix → PLM → Contrastive learning framework
- Design tradeoffs: Multiple prompts increase complexity but improve diversity; contrastive learning requires additional resources but maintains relationships
- Failure signatures: Poor topic modeling leads to ineffective prompts; contrastive learning failure results in non-diverse representations
- First 3 experiments:
  1. Verify topic modeling captures semantic diversity
  2. Verify prompt encoder generates aligned prompts
  3. Verify contrastive learning maintains topical relationships

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would performance change if topic modeling were jointly trained with dense passage retrieval?
- Basis in paper: Authors mention possibility of joint training to automatically determine topics
- Why unresolved: Only discussed as possibility, no experimental results provided
- What evidence would resolve it: Experiments comparing separately vs jointly trained topic modeling

### Open Question 2
- Question: How would incorporating additional metadata (authors, sentiments, conclusions) affect retrieval performance?
- Basis in paper: Authors mention possible metadata could assist document retrieval
- Why unresolved: Impact of additional metadata not explored
- What evidence would resolve it: Experiments comparing performance with and without additional metadata

### Open Question 3
- Question: How does the choice of number of topics (Ks) affect performance?
- Basis in paper: Authors use subset of higher-level topics from hLDA tree
- Why unresolved: Impact of varying number of topics not explored
- What evidence would resolve it: Experiments with different numbers of topics comparing performance

## Limitations
- Topic modeling quality and coverage across domains not thoroughly validated
- In-batch sampling strategy implementation details not specified
- Contrastive learning effectiveness inferred from end-to-end performance rather than intermediate representation analysis

## Confidence

- Topic diversity improvement: Medium confidence - supported by framework but limited empirical validation
- Contrastive learning effectiveness: Medium confidence - demonstrated through metrics but lacking ablation studies
- Sampling strategy impact: Low confidence - described but not independently validated

## Next Checks

1. Conduct ablation studies to isolate contribution of topic-topic relation loss
2. Visualize learned semantic spaces to verify reduced collapse and improved uniformity
3. Test sampling strategy independently to confirm higher-quality positive/negative pairs than random sampling