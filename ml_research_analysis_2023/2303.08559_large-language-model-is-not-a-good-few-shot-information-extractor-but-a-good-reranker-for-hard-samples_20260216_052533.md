---
ver: rpa2
title: Large Language Model Is Not a Good Few-shot Information Extractor, but a Good
  Reranker for Hard Samples!
arxiv_id: '2303.08559'
source_url: https://arxiv.org/abs/2303.08559
tags:
- shot
- llms
- slms
- event
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) like GPT-3 have shown remarkable capabilities
  across diverse tasks through in-context learning. However, whether LLMs can serve
  as competitive few-shot solvers for information extraction (IE) tasks remains unclear.
---

# Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!

## Quick Facts
- arXiv ID: 2303.08559
- Source URL: https://arxiv.org/abs/2303.08559
- Reference count: 40
- Key outcome: LLMs underperform fine-tuned SLMs in few-shot IE tasks, but excel at reranking hard samples

## Executive Summary
This paper provides a comprehensive empirical evaluation of Large Language Models (LLMs) versus Small Language Models (SLMs) for few-shot information extraction tasks. Through extensive experiments across nine datasets covering Named Entity Recognition, Relation Extraction, and Event Detection, the authors demonstrate that current LLMs exhibit inferior performance, higher latency, and increased cost compared to fine-tuned SLMs under most settings. However, they identify that LLMs excel specifically at reranking hard samples that SLMs struggle with, leading to the proposal of an adaptive filter-then-rerank paradigm that combines the strengths of both model types.

## Method Summary
The paper evaluates LLMs and SLMs on nine IE datasets using few-shot learning (K-shot sampling with K=1,5,10,20 for NER/ED and K=1,5,10,20,50,100 for RE). SLMs are fine-tuned using various methods (Fine-tuning, FSLS, KnowPrompt, UIE) with RoBERTa-large/T5-large backbones, while LLMs use in-context learning with CODEX. The proposed filter-then-rerank paradigm uses SLMs as filters to identify low-confidence samples, then applies LLMs to rerank top-N candidates for these hard samples, combining the speed and cost-effectiveness of SLMs with the reasoning capabilities of LLMs for challenging cases.

## Key Results
- SLMs consistently outperform LLMs across most settings in few-shot IE tasks
- LLMs show particular weakness on structured prediction tasks like NER and ED due to formatting constraints
- The filter-then-rerank approach achieves consistent improvements (2.4% F1-gain on average) by leveraging LLMs for hard samples
- Only a small minority of samples require LLM reranking, making the approach cost-effective

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs outperform SLMs primarily on hard samples requiring external knowledge or complex reasoning
- Mechanism: SLMs excel on easy samples through fine-tuning on large datasets, while LLMs leverage broad pretraining to solve samples beyond SLM capacity
- Core assumption: Task difficulty correlates with SLM confidence scores, and low-confidence samples require knowledge/reasoning SLMs lack
- Evidence anchors:
  - [section 4.2]: "We partition the testing samples into different groups according to their difficulty (measured by the confidence score of SLMs-based models) and compare the results of LLMs and SLMs on each group."
  - [section 5.3]: "LLMs could largely help SLMs to rerank and correct hard samples."
  - [corpus]: Weak - no direct citations on this specific claim, but related work exists on SLM vs LLM performance differences
- Break condition: If SLM confidence scores do not correlate with actual difficulty, or if LLMs cannot access required external knowledge

### Mechanism 2
- Claim: Filter-then-rerank paradigm combines strengths of both model types while minimizing costs
- Mechanism: SLMs act as fast, cheap filters to eliminate unlikely labels, then LLMs rerank only the top-N candidates for hard samples
- Core assumption: Most samples are easy for SLMs (high confidence), so only a small fraction need LLM reranking
- Evidence anchors:
  - [section 5.2]: "We know from Figure 3 that most of samples are easy for SLMs (with high confidence score)."
  - [section 5.5]: "Only a tiny minority of samples are fed to LLMs for reranking, as shown in Table 4 (the last column)."
  - [section 5.3]: "Our filter-then-rerank method achieves consistent and significant improvement on nine different settings."
- Break condition: If too many samples are hard, or if reranking overhead exceeds benefits

### Mechanism 3
- Claim: Multiple-choice prompt format improves LLM reranking performance compared to open-ended schemas
- Mechanism: Narrowing label scope to N candidates simplifies the task for LLMs and reduces input length/cost
- Core assumption: LLMs are more familiar with multiple-choice formats and perform better with constrained options
- Evidence anchors:
  - [section 4.1]: "We reformulate the reranking procedure as a multiple-choice question... We believe the format of multiple-choice question has various advantages."
  - [section 5.4]: "The filtering of candidate labels usually brings gains, especially on TACREV dataset."
  - [section 5.5]: "It significantly reduces the input length of LLMs and thus the inference cost."
- Break condition: If N is too small (missing true answers) or too large (diminishing returns)

## Foundational Learning

- Concept: In-context learning (ICL) limitations for structured prediction tasks
  - Why needed here: Understanding why LLMs struggle with NER/ED tasks despite success on other tasks
  - Quick check question: What are the key differences between RE tasks and NER/ED tasks that affect ICL performance?

- Concept: Confidence score calibration for model filtering
  - Why needed here: SLMs use confidence scores to identify hard samples for LLM reranking
  - Quick check question: How would you validate that SLM confidence scores actually correlate with sample difficulty?

- Concept: Template-based prompt engineering
  - Why needed here: Converting candidate labels to multiple-choice options requires consistent templates
  - Quick check question: What are the risks of using overly generic templates for label rephrasing?

## Architecture Onboarding

- Component map: SLM filter → confidence scoring → top-N candidate selection → LLM reranker → final prediction
- Critical path: SLM inference → hard sample detection → LLM reranking → result aggregation
- Design tradeoffs: Larger N increases coverage but also cost; smaller N reduces cost but may miss true answers
- Failure signatures: Low reranking ratio (too many easy samples), high reranking ratio (SLM underperforms), inconsistent improvements across datasets
- First 3 experiments:
  1. Validate SLM confidence scores correlate with actual difficulty using holdout validation set
  2. Test different N values (2, 3, 5) to find optimal tradeoff between coverage and cost
  3. Compare multiple-choice prompt format against original schema format on a small subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum number of demonstrations (demos) that provides optimal performance for LLMs in few-shot IE tasks?
- Basis in paper: [inferred] The paper mentions that LLMs are constrained by maximum input length, and that performance becomes "at a standstill" after a certain number of demos in some tasks.
- Why unresolved: The paper did not conduct experiments to determine the exact optimal number of demos. It only observed that performance plateaus or degrades after certain thresholds.
- What evidence would resolve it: Systematic experiments varying the number of demos across different IE tasks to find the point of diminishing returns in performance.

### Open Question 2
- Question: How do different instruction formats affect LLM performance on IE tasks?
- Basis in paper: [explicit] The paper discusses how LLMs struggle with structured outputs and complex schemas, and mentions task format as a factor in performance.
- Why unresolved: The paper only tested one instruction format per task. Different ways of phrasing instructions or providing context were not explored.
- What evidence would resolve it: Comparative experiments testing multiple instruction formats (e.g., different ways of describing the task, varying levels of detail) across various IE tasks.

### Open Question 3
- Question: What is the impact of demonstration selection strategies on LLM performance for IE tasks?
- Basis in paper: [explicit] The paper mentions demonstration selection as one of the ICL approaches, but only tested a heuristic unsupervised approach.
- Why unresolved: The paper did not compare different demonstration selection strategies (e.g., supervised vs unsupervised, different similarity metrics) or their impact on performance.
- What evidence would resolve it: Comparative experiments testing various demonstration selection strategies and their effects on LLM performance across different IE tasks.

## Limitations

- Task-specific constraints: Results focus on structured prediction tasks where LLMs may inherently struggle due to formatting requirements
- Cost considerations bias: Economic analysis may disadvantage LLMs even when accuracy approaches SLM performance
- Prompt engineering variability: Evaluation uses relatively standard prompting strategies that could be improved
- Dataset representativeness: Results may not generalize to specialized domains or languages beyond tested corpora

## Confidence

- High confidence: SLMs consistently outperform LLMs on average across tested tasks and settings
- Medium confidence: LLMs excel specifically on hard samples requiring external knowledge
- Medium confidence: Filter-then-rerank paradigm effectively improves performance across settings
- Low confidence: General recommendation against using LLMs for few-shot IE tasks

## Next Checks

1. **Confidence score calibration validation**: Systematically evaluate whether SLM confidence scores actually correlate with sample difficulty by conducting human annotation studies or using gold-standard difficulty metrics.

2. **Prompt engineering ablation study**: Conduct controlled experiments varying prompt templates, demonstration selection strategies, and reasoning techniques to determine if LLM performance can be improved beyond baseline results.

3. **Cost-benefit analysis at scale**: Perform comprehensive economic analysis comparing total system costs across different operational scales and deployment scenarios to validate cost disadvantages remain significant in production environments.