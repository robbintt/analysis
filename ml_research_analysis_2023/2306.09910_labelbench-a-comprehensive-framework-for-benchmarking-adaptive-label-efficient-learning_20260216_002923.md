---
ver: rpa2
title: 'LabelBench: A Comprehensive Framework for Benchmarking Adaptive Label-Efficient
  Learning'
arxiv_id: '2306.09910'
source_url: https://arxiv.org/abs/2306.09910
tags:
- learning
- accuracy
- pretrained
- labels
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LabelBench provides a modular benchmarking framework for evaluating\
  \ label-efficient learning techniques\u2014transfer learning, semi-supervised learning,\
  \ and active learning\u2014in combination. It addresses the computational challenge\
  \ of retraining large pretrained models during active learning by introducing selection-via-proxy:\
  \ retraining only lightweight proxy models (shallow networks or linear probes) for\
  \ data selection, then performing end-to-end fine-tuning once at the end."
---

# LabelBench: A Comprehensive Framework for Benchmarking Adaptive Label-Efficient Learning

## Quick Facts
- arXiv ID: 2306.09910
- Source URL: https://arxiv.org/abs/2306.09910
- Authors: 
- Reference count: 40
- Primary result: Framework enables 10x computational cost reduction in active learning through selection-via-proxy while maintaining competitive accuracy

## Executive Summary
LabelBench provides a modular benchmarking framework for evaluating label-efficient learning techniques—transfer learning, semi-supervised learning, and active learning—in combination. It addresses the computational challenge of retraining large pretrained models during active learning by introducing selection-via-proxy: retraining only lightweight proxy models (shallow networks or linear probes) for data selection, then performing end-to-end fine-tuning once at the end. Experiments on CIFAR-10, ImageNet, fMoW, and iWildcam show consistent accuracy gains over random sampling—up to 1.2% in test accuracy and 5% in pool accuracy—with significant reductions in annotation cost.

## Method Summary
The framework implements a selection-via-proxy approach where lightweight proxy models are trained to select informative samples for labeling, rather than retraining the full large pretrained model at each iteration. It integrates semi-supervised learning using FlexMatch, supports multiple active learning strategies (confidence, entropy, margin, BADGE, etc.), and uses pretrained CLIP and CoCa models as backbones. The framework precomputes embeddings to reduce computational overhead and provides modular code for easy extension and benchmarking of new methods.

## Key Results
- Selection-via-proxy reduces GPU time and training-induced cost by more than ten-fold compared to full model retraining
- Consistent accuracy gains over random sampling: up to 1.2% in test accuracy and 5% in pool accuracy
- Significant annotation cost savings when combining active learning with semi-supervised learning
- Effective across diverse datasets including CIFAR-10, ImageNet, fMoW, and iWildcam

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selection-via-proxy enables efficient active learning by retraining only lightweight proxy models during data selection, then performing full fine-tuning once at the end.
- **Mechanism:** Instead of retraining the full large pretrained model at every iteration (which is computationally prohibitive), the system retrains a shallow neural network or linear probe to select the most informative samples. Once the labeling budget is exhausted, a single end-to-end fine-tuning step is performed.
- **Core assumption:** The proxy model's data selection quality is sufficiently correlated with the full model's selection quality to maintain accuracy gains while drastically reducing computation.
- **Evidence anchors:**
  - [abstract] "It addresses the computational challenge of retraining large pretrained models during active learning by introducing selection-via-proxy: retraining only lightweight proxy models (shallow networks or linear probes) for data selection, then performing end-to-end fine-tuning once at the end."
  - [section 3.1] "To better trade-off between retraining/inference cost and the final model performance, we propose a selection-via-proxy approach, which is inspired by Coleman et al. [2019]."
- **Break condition:** If the proxy model's selection decisions diverge significantly from what the full model would choose, the accuracy gains from active learning could diminish or disappear.

### Mechanism 2
- **Claim:** Combining active learning with semi-supervised learning (SSL) and large pretrained models yields better label efficiency than any single technique alone.
- **Mechanism:** Active learning selects informative samples, SSL leverages unlabeled data to improve model performance, and large pretrained models provide strong initialization. Together, they compound each other's benefits.
- **Core assumption:** Each technique addresses a different bottleneck in the learning process, and their combination creates synergistic effects.
- **Evidence anchors:**
  - [abstract] "Experiments on CIFAR-10, ImageNet, fMoW, and iWildcam show consistent accuracy gains over random sampling—up to 1.2% in test accuracy and 5% in pool accuracy—with significant reductions in annotation cost."
  - [section 4.4] "Comparing to existing literature of AL + SSL [Lüth et al., 2023, Chan et al., 2021, Mittal et al., 2019, Siméoni et al., 2021] and AL + large pretrained models [Tamkin et al., 2022], our experiment yields the largest percentage of annotation cost savings to reach the same level of accuracy as random sampling."
- **Break condition:** If one component (e.g., SSL) becomes ineffective on a particular dataset or task, the synergistic benefits may not materialize.

### Mechanism 3
- **Claim:** Precomputing embeddings for the entire dataset reduces computational cost during active learning iterations.
- **Mechanism:** The system computes embeddings for all examples once upfront, then reuses these embeddings across iterations, avoiding repeated forward passes through the large model during proxy retraining.
- **Core assumption:** The embeddings remain relevant across different stages of training and different proxy models.
- **Evidence anchors:**
  - [section 3.1] "We further reduce the forward inference cost by precomputing and saving embeddings of each dataset in advance. To account for random image augmentations during training, we precompute five sets of embeddings on randomly augmented images using different random seeds."
  - [section 3.1] "As shown in Table 1, we highlight the reduction in experimentation cost on the ImageNet dataset. In particular, selection-via-proxy reduces the GPU time and training-induced cost by more than ten-fold."
- **Break condition:** If data augmentation strategies change significantly or if the embedding space becomes less discriminative as the proxy models evolve, precomputed embeddings may lose effectiveness.

## Foundational Learning

- **Concept:** Transfer learning with large pretrained models
  - Why needed here: The framework relies on pretrained CLIP or CoCa models as starting points, so understanding how to adapt these models to new tasks is essential.
  - Quick check question: What is the difference between linear probing and fine-tuning in transfer learning?

- **Concept:** Semi-supervised learning fundamentals
  - Why needed here: SSL methods like FlexMatch are integrated into the training pipeline to leverage unlabeled data, requiring understanding of consistency regularization and pseudo-labeling.
  - Quick check question: How does consistency regularization work in semi-supervised learning?

- **Concept:** Active learning selection strategies
  - Why needed here: The framework benchmarks multiple AL strategies (confidence, entropy, margin, BADGE, etc.), so understanding their selection criteria is crucial.
  - Quick check question: What is the key difference between uncertainty-based and diversity-based active learning methods?

## Architecture Onboarding

- **Component map:** datasets -> model -> training strategy -> active learning strategy -> metrics
- **Critical path:** The core loop involves: 1) training the current model (SSL or proxy) on available labeled data, 2) using the trained model to select new samples via an AL strategy, 3) obtaining labels for selected samples, and 4) repeating until budget exhaustion, followed by final end-to-end fine-tuning.
- **Design tradeoffs:** Selection-via-proxy trades some accuracy for significant computational savings; the framework must balance between proxy model quality and retraining cost. The choice between linear probes and shallow networks involves similar tradeoffs.
- **Failure signatures:** If active learning doesn't outperform random sampling, it could indicate proxy model selection quality issues, SSL implementation problems, or inappropriate hyperparameter choices. High variance in results across runs might suggest instability in the AL selection process.
- **First 3 experiments:**
  1. Run CIFAR-10 with confidence sampling and end-to-end fine-tuning at every iteration to establish baseline performance.
  2. Run the same CIFAR-10 experiment with selection-via-proxy using linear probes to verify computational savings and minimal accuracy loss.
  3. Run CIFAR-10 with confidence sampling + FlexMatch SSL to demonstrate the combined benefits of AL and SSL.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do selection-via-proxy and end-to-end fine-tuning compare in terms of annotation cost efficiency for reaching a specific target accuracy?
- Basis in paper: [explicit] The paper states that selection-via-proxy reduces GPU time and training-induced cost by more than ten-fold, but also notes that it is slightly less effective than selection with fine-tuning in terms of pool accuracy, with approximately 1% reduction in performance.
- Why unresolved: The paper provides some comparative results but does not directly quantify the annotation cost efficiency difference between the two methods for achieving a specific accuracy target.
- What evidence would resolve it: Direct comparison of annotation costs (number of labels) required by each method to reach a predetermined accuracy threshold on the same datasets.

### Open Question 2
- Question: What is the impact of using different proxy model architectures (e.g., LORA) on the final model performance and computational efficiency?
- Basis in paper: [inferred] The paper mentions that there are other choices of proxy, such as LORA, which is popular in natural language processing, but does not explore these alternatives in the experiments.
- Why unresolved: The paper only explores linear probes and shallow networks as proxies, leaving the potential benefits and drawbacks of other architectures unexplored.
- What evidence would resolve it: Experimental results comparing the performance and computational efficiency of different proxy architectures (e.g., LORA, other low-rank adaptations) on the same datasets.

### Open Question 3
- Question: How does the performance of active learning strategies vary across different types of distribution shifts (e.g., covariate shift, prior probability shift)?
- Basis in paper: [explicit] The paper mentions that future work could incorporate datasets with distribution shift evaluation data, indicating that this aspect is not currently explored.
- Why unresolved: The current experiments focus on in-distribution test accuracy, and the paper acknowledges the need to evaluate performance under distribution shifts.
- What evidence would resolve it: Experimental results evaluating the performance of active learning strategies on datasets with known distribution shifts, such as those found in the WILDS benchmark.

## Limitations

- Selection-via-proxy may sacrifice some accuracy for computational efficiency, with potential degradation when proxy models diverge from full model selection
- Generalizability to non-image domains and tasks beyond classification remains untested
- Performance under distribution shift conditions is not evaluated, limiting real-world applicability

## Confidence

- High confidence: The computational cost reduction claims (10x reduction in GPU time) are well-supported by empirical measurements in Table 1 and the selection-via-proxy mechanism is clearly described.
- Medium confidence: The accuracy gains (1.2% test accuracy, 5% pool accuracy) are reported but depend on optimal hyperparameter tuning that may not be fully reproducible.
- Low confidence: The claim that combining AL, SSL, and large pretrained models creates synergistic effects is supported by relative performance but lacks ablation studies isolating each component's contribution.

## Next Checks

1. Perform ablation studies on CIFAR-10 to quantify individual contributions of AL, SSL, and pretrained models by testing each component in isolation versus the full combination.
2. Test selection-via-proxy across diverse dataset types (text, tabular, audio) to evaluate generalizability beyond image classification tasks.
3. Measure proxy model selection quality divergence from full model selection across multiple iterations to establish the reliability threshold for the selection-via-proxy approach.