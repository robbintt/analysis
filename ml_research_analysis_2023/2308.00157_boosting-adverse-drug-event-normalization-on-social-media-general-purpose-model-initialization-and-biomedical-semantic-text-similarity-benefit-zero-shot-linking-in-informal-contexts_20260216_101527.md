---
ver: rpa2
title: 'Boosting Adverse Drug Event Normalization on Social Media: General-Purpose
  Model Initialization and Biomedical Semantic Text Similarity Benefit Zero-Shot Linking
  in Informal Contexts'
arxiv_id: '2308.00157'
source_url: https://arxiv.org/abs/2308.00157
tags:
- media
- social
- normalization
- biomedical
- drug
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new approach for adverse drug event (ADE)
  normalization on social media by leveraging general-purpose model initialization
  via BioLORD and biomedical semantic text similarity fine-tuning. The authors hypothesize
  that pre-training on general texts will help understand the informal language used
  on social media, while previous state-of-the-art models struggle due to the domain
  shift between clinical and social media languages.
---

# Boosting Adverse Drug Event Normalization on Social Media: General-Purpose Model Initialization and Biomedical Semantic Text Similarity Benefit Zero-Shot Linking in Informal Contexts

## Quick Facts
- arXiv ID: 2308.00157
- Source URL: https://arxiv.org/abs/2308.00157
- Reference count: 6
- This paper achieves state-of-the-art performance on four social media datasets for adverse drug event normalization using general-purpose model initialization and biomedical semantic text similarity fine-tuning.

## Executive Summary
This paper addresses the challenge of adverse drug event (ADE) normalization on social media texts, where informal language and domain shift from clinical corpora hinder existing models. The authors propose leveraging general-purpose model initialization via BioLORD combined with biomedical semantic text similarity (STS) fine-tuning to improve understanding of informal social media language. Their approach achieves significant performance gains over previous state-of-the-art models, demonstrating the effectiveness of pre-training on diverse texts for social media applications.

## Method Summary
The proposed method uses general-purpose model initialization through BioLORD pre-training on STAMB2, with optional STS fine-tuning on MedSTS both before and after BioLORD pre-training. The approach is evaluated using zero-shot learning on four social media datasets (CADEC, PsyTAR, SMM4H, TwiMed) for ADE normalization, mapping mentions to standardized ontologies. The method is compared against domain-specific initialization using PubMedBERT to demonstrate the advantage of general-purpose models for informal social media language.

## Key Results
- BioLORD-STAMB2 (general-purpose initialization) outperforms BioLORD-PMB (domain-specific initialization) on social media datasets
- STS fine-tuning before and after BioLORD pre-training further improves performance
- Achieves zero-shot accuracy@1 above 60% for Preferred Term classification on CADEC
- Average accuracy gain of 1 point compared to previous best model across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: General-purpose models pre-trained on diverse texts can better understand informal social media language than domain-specific biomedical models
- Mechanism: BioLORD pre-training on general-purpose models like STAMB2 exposes the model to diverse linguistic patterns and informal writing styles found on social media, which are rare in clinical corpora
- Core assumption: The informal language patterns on social media are sufficiently different from clinical texts that domain-specific initialization hinders performance on social media tasks
- Evidence anchors:
  - [abstract] "We hypothesize that its pre-training on general texts will help tremendously in understanding the informal language used on social media, while previous state of the art models struggled at that specific task, due to the domain shift between clinical and social media languages."
  - [section 2.1] "We hypothesize that its pre-training on general texts will help tremendously in understanding the informal language used on social media, while previous state of the art models struggled at that specific task, due to the domain shift between clinical and social media languages."
  - [corpus] Weak evidence - no direct comparison of language patterns between clinical and social media texts in the corpus
- Break condition: If social media texts contain specialized biomedical terminology that general-purpose models haven't encountered, the advantage may disappear

### Mechanism 2
- Claim: Applying semantic-text-similarity (STS) fine-tuning before and after BioLORD pre-training improves performance on biomedical tasks
- Mechanism: STS fine-tuning on medical semantic text similarity tasks (MedSTS) primes the model for biomedical concept understanding before BioLORD pre-training, and further refines it afterward
- Core assumption: The combination of general-purpose initialization + biomedical STS fine-tuning + BioLORD pre-training creates a superior representation than any single approach
- Evidence anchors:
  - [abstract] "we improve this new approach even further by incorporating two distinct semantic-text-similarity (STS) fine-tuning phases to the training, both before and after the BioLORD pre-training"
  - [section 3.1] "We do so by fine-tuning some of the models on the MedSTS task (Wang et al., 2020), using the same hyperparameters described in the BioLORD paper"
  - [corpus] Weak evidence - no direct analysis of how STS fine-tuning specifically improves social media understanding
- Break condition: If the STS fine-tuning data doesn't cover the types of informal expressions found in social media, the benefit may be limited

### Mechanism 3
- Claim: Zero-shot contrastive learning models are versatile and can map concepts to any new ontology without retraining
- Mechanism: The contrastive learning framework allows encoding new target concepts at inference time, making the model ontology-agnostic and adaptable to updates
- Core assumption: The learned representations generalize well enough to map between different ontologies without requiring fine-tuning on specific target ontologies
- Evidence anchors:
  - [abstract] "What makes these models extremely versatile is that it is possible to encode a new set of target concepts at inference time, which means that using the same model is possible irrespective of the target ontology, enabling smooth system updates"
  - [section 2] "In a short time span, between the years 2020 and 2022, the field of biomedical concept normalization has seen significant advancements with the introduction of self-supervised contrastive models"
  - [corpus] Moderate evidence - the paper mentions this versatility but doesn't provide quantitative evidence of ontology generalization
- Break condition: If the target ontology has concepts that are significantly different from those in the training ontology, zero-shot mapping may fail

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The model needs to normalize ADE mentions to ontology concepts without being explicitly trained on that specific ontology
  - Quick check question: How does a zero-shot model determine which ontology concept best matches a given text mention?

- Concept: Contrastive learning
  - Why needed here: The model learns by contrasting similar and dissimilar concept representations, which is fundamental to the BioLORD pre-training approach
  - Quick check question: What makes two concept representations "similar" enough to be considered a positive pair in contrastive learning?

- Concept: Semantic textual similarity (STS)
  - Why needed here: STS fine-tuning helps the model understand semantic relationships between medical terms, which is crucial for normalization
  - Quick check question: How does STS fine-tuning differ from regular classification fine-tuning in terms of what it teaches the model?

## Architecture Onboarding

- Component map: STAMB2 (or PubMedBERT) → (optional STS fine-tuning) → BioLORD pre-training → (optional STS fine-tuning) → zero-shot inference
- Critical path: Base model initialization → (optional STS fine-tuning) → BioLORD pre-training → (optional STS fine-tuning) → zero-shot inference
- Design tradeoffs: General-purpose models offer better social media performance but may lack domain-specific medical knowledge; domain-specific models have the opposite tradeoff
- Failure signatures: Poor performance on social media datasets indicates domain shift issues; poor performance on clinical datasets suggests insufficient biomedical knowledge
- First 3 experiments:
  1. Compare BioLORD-STAMB2 vs BioLORD-PMB on TwiMed-PM to confirm domain-specific vs general-purpose tradeoff
  2. Compare BioLORD-STAMB2 vs BioLORD-STAMB2-STS2 on CADEC to verify STS fine-tuning benefits
  3. Compare zero-shot vs fine-tuned performance on SMM4H to understand the limits of zero-shot learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BioLORD-STAMB2-STS2 compare to domain-specific models like BioBERT when fine-tuned on social media datasets for ADE normalization?
- Basis in paper: [explicit] The authors state that BioLORD-STAMB2, the general-domain model, outperforms BioLORD-PMB, the domain-specific variant, on social media datasets. However, they do not compare their approach to domain-specific models like BioBERT that are fine-tuned on social media data.
- Why unresolved: The paper only compares their approach to baseline models trained on biomedical corpora and does not investigate the performance of domain-specific models fine-tuned on social media data.
- What evidence would resolve it: Conduct experiments comparing the performance of BioLORD-STAMB2-STS2 to domain-specific models like BioBERT that are fine-tuned on social media datasets for ADE normalization.

### Open Question 2
- Question: How does the performance of the proposed approach vary across different social media platforms (e.g., Twitter, forums, online reviews) for ADE normalization?
- Basis in paper: [explicit] The authors evaluate their approach on four social media datasets (CADEC, PsyTAR, SMM4H, and TwiMed) but do not analyze the performance differences across these platforms.
- Why unresolved: The paper does not provide a detailed analysis of how the performance of the proposed approach varies across different social media platforms.
- What evidence would resolve it: Conduct experiments to analyze the performance of the proposed approach on different social media platforms and identify any platform-specific challenges or advantages.

### Open Question 3
- Question: How does the proposed approach perform on ADE normalization tasks in languages other than English?
- Basis in paper: [inferred] The authors focus on English social media datasets and do not investigate the performance of their approach on non-English datasets.
- Why unresolved: The paper does not explore the cross-lingual capabilities of the proposed approach for ADE normalization.
- What evidence would resolve it: Conduct experiments to evaluate the performance of the proposed approach on non-English social media datasets for ADE normalization and compare it to existing methods.

## Limitations

- Limited ablation analysis to isolate contributions of individual components
- Dataset composition concerns regarding label noise and annotation inconsistencies
- Focus on accuracy@1 metric without exploring alternative evaluation metrics

## Confidence

- **High confidence**: General-purpose models outperform domain-specific models on social media tasks
- **Medium confidence**: STS fine-tuning before and after BioLORD pre-training improves performance
- **Low confidence**: The approach achieves "state-of-the-art performance" without direct head-to-head comparisons

## Next Checks

1. Conduct ablation study to quantify individual contribution of each component (general-purpose initialization, STS fine-tuning, BioLORD pre-training)

2. Evaluate zero-shot capability on a completely different ontology than what was used during training

3. Test model's performance over time on social media data to assess stability of general-purpose initialization advantage