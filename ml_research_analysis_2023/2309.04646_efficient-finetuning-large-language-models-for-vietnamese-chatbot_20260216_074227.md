---
ver: rpa2
title: Efficient Finetuning Large Language Models For Vietnamese Chatbot
arxiv_id: '2309.04646'
source_url: https://arxiv.org/abs/2309.04646
tags:
- language
- tasks
- arxiv
- llms
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an efficient approach for fine-tuning large
  language models (LLMs) for Vietnamese chatbots, addressing the challenges of high
  training costs and limited Vietnamese-specific instruction-tuning datasets. The
  authors collected and translated instruction-following datasets from open-source
  projects, including Alpaca, GPT4All, and Chat-Doctor, covering both general and
  medical domains.
---

# Efficient Finetuning Large Language Models For Vietnamese Chatbot

## Quick Facts
- arXiv ID: 2309.04646
- Source URL: https://arxiv.org/abs/2309.04646
- Authors: 
- Reference count: 17
- Key outcome: 20-30% improvement over original models in Vietnamese chatbot performance using parameter-efficient LoRA fine-tuning

## Executive Summary
This paper addresses the challenge of fine-tuning large language models for Vietnamese chatbots by proposing an efficient approach using Low-Rank Adaptation (LoRA) and instruction-tuning datasets. The authors collected and translated instruction-following datasets from Alpaca, GPT4All, and Chat-Doctor, covering both general and medical domains. By applying LoRA to Bloomz and GPTJ-6B models, they created four specialized models that demonstrate significant performance gains in Vietnamese language understanding and generation capabilities.

## Method Summary
The approach utilizes parameter-efficient tuning through LoRA on Bloomz and GPTJ-6B models, freezing the original model weights while adding low-rank decomposition matrices to the forward pass. Instruction-following datasets from Alpaca, GPT4All, and Chat-Doctor were translated to Vietnamese and used for fine-tuning, resulting in four models: Bloomz-Chat, Bloomz-Doctor, GPTJ-Chat, and GPTJ-Doctor. The models were evaluated using GPT-4 as an automated scoring mechanism to assess helpfulness, relevance, accuracy, and level of detail in responses.

## Key Results
- 20-30% improvement over original models in Vietnamese chatbot performance
- Bloomz-Chat and Bloomz-Doctor models showed significant gains in Vietnamese understanding
- Parameter-efficient LoRA approach reduced training costs while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-Rank Adaptation (LoRA) reduces parameter updates while preserving performance
- Mechanism: LoRA freezes the original model weights and adds low-rank decomposition matrices (B and A) to the forward pass, enabling efficient adaptation
- Core assumption: The original model's weights contain sufficient knowledge that can be effectively adapted with small parameter updates
- Evidence anchors:
  - [abstract] "we utilize parameter-efficient tuning through Low-Rank Adaptation (LoRA) on Bloomz and GPTJ-6B models"
  - [section] "For a linear layer h = W0x, the forward pass is modified to be: h = W0x + BAx"
  - [corpus] Weak - no direct citation of LoRA performance in related work
- Break condition: If the rank k is too small, the adaptation cannot capture task-specific nuances

### Mechanism 2
- Claim: Instruction tuning improves zero-shot task generalization
- Mechanism: Fine-tuning on instruction-following datasets teaches the model to interpret natural language commands and generate appropriate responses
- Core assumption: The model can learn to map natural language instructions to appropriate output formats through supervised examples
- Evidence anchors:
  - [abstract] "Recent advancements in instruction tuning bring LLMs with ability in following user's instructions and producing human-like responses"
  - [section] "Instruction-following methods enhance pre-trained models by fine-tuning them using high-quality input-output tuples of task instructions and ground truth outputs"
  - [corpus] Moderate - related work mentions instruction tuning improves zero-shot performance
- Break condition: If instruction datasets are too homogeneous, the model may overfit to specific patterns

### Mechanism 3
- Claim: GPT-4 evaluation provides reliable automated scoring
- Mechanism: Using GPT-4 as an automated scoring mechanism evaluates model outputs on relevance, helpfulness, accuracy, and level of detail
- Core assumption: GPT-4 can provide consistent and meaningful quality assessments for Vietnamese text generation
- Evidence anchors:
  - [abstract] "we assess the effectiveness of our methodology on a per-sample basis... This evaluation process entails the utilization of GPT-4 as an automated scoring mechanism"
  - [section] "Following previous works ( [17], Vicuna) that utilizes GPT-4 as a scoring method, we also adopt GPT-4 to provide an overall score"
  - [corpus] Weak - no direct evidence of GPT-4 evaluation reliability in Vietnamese context
- Break condition: If GPT-4's understanding of Vietnamese quality differs significantly from human evaluators

## Foundational Learning

- Concept: Parameter-efficient fine-tuning
  - Why needed here: Standard fine-tuning of large models requires prohibitive computational resources
  - Quick check question: What is the approximate memory reduction when using LoRA compared to full fine-tuning?

- Concept: Instruction-following format
  - Why needed here: Enables the model to understand and respond to natural language commands
  - Quick check question: What are the two key fields required in the instruction dataset format?

- Concept: Zero-shot evaluation
  - Why needed here: Allows assessment of model capabilities on tasks not seen during training
  - Quick check question: How does zero-shot evaluation differ from few-shot evaluation?

## Architecture Onboarding

- Component map:
  - Base models: Bloomz-mt-7B and GPTJ-6B (frozen during training)
  - LoRA adapters: Low-rank decomposition matrices (B and A)
  - Dataset pipeline: Instruction-following examples (Alpaca, GPT4All, ChatDoctor)
  - Evaluation pipeline: GPT-4 automated scoring mechanism

- Critical path: Dataset preparation → LoRA adapter initialization → Fine-tuning → Evaluation
- Design tradeoffs: Memory efficiency (LoRA) vs. potential performance degradation from rank limitations
- Failure signatures: Low relevance scores, repetitive responses, incorrect domain knowledge
- First 3 experiments:
  1. Train Bloomz-Chat on Alpaca dataset only, evaluate on generic tasks
  2. Train Bloomz-Doctor on ChatDoctor dataset, evaluate on medical domain
  3. Compare Bloomz vs GPTJ performance on same instruction dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Vietnamese Bloomz-Chat model compare to other Vietnamese language models that have undergone extensive training on larger Vietnamese corpora?
- Basis in paper: [inferred] The authors acknowledge that the response from the models is sometimes not relevant to the instruction and sometimes responds with the wrong answer. They attribute this to the limit of training text corpus in the base model - Bloomz - which has only 3% Vietnamese.
- Why unresolved: The paper does not provide a direct comparison with other Vietnamese language models that have undergone extensive training on larger Vietnamese corpora.
- What evidence would resolve it: A comparative study between the Vietnamese Bloomz-Chat model and other Vietnamese language models trained on larger Vietnamese corpora, evaluating their performance on various Vietnamese NLP tasks.

### Open Question 2
- Question: What are the potential benefits and challenges of expanding the pre-trained LLM with more Vietnamese data to improve the model's performance?
- Basis in paper: [explicit] The authors mention that they believe if they can expand the pre-trained LLM with more Vietnamese data, the model performance will increase, and that will be their next work in the near future.
- Why unresolved: The paper does not discuss the potential benefits and challenges of expanding the pre-trained LLM with more Vietnamese data.
- What evidence would resolve it: A detailed analysis of the benefits and challenges of expanding the pre-trained LLM with more Vietnamese data, including the impact on model performance, training time, and resource requirements.

### Open Question 3
- Question: How does the performance of the Vietnamese Bloomz-Doctor model compare to other medical chatbots or language models in the Vietnamese context?
- Basis in paper: [inferred] The authors evaluate the performance of the Vietnamese Bloomz-Doctor model using GPT-4 as an automated scoring mechanism, but they do not provide a comparison with other medical chatbots or language models in the Vietnamese context.
- Why unresolved: The paper does not provide a direct comparison with other medical chatbots or language models in the Vietnamese context.
- What evidence would resolve it: A comparative study between the Vietnamese Bloomz-Doctor model and other medical chatbots or language models in the Vietnamese context, evaluating their performance on various medical NLP tasks and accuracy in responding to medical queries.

## Limitations

- Limited Vietnamese-specific training data in base Bloomz model (only 3% Vietnamese)
- Evaluation methodology relies solely on GPT-4 scoring without human validation
- Does not assess model performance on complex tasks like mathematical reasoning or code generation

## Confidence

- **Medium confidence**: The claimed 20-30% improvement in model performance, as this is based solely on GPT-4 evaluation without human validation
- **Low confidence**: The generalizability of LoRA performance gains from English to Vietnamese contexts, given the weak corpus evidence for LoRA effectiveness in non-English languages
- **Medium confidence**: The effectiveness of the instruction-following approach for Vietnamese chatbots, based on established literature but limited Vietnamese-specific validation

## Next Checks

1. Conduct human evaluation studies comparing GPT-4 scores with human judgments for Vietnamese text quality across the four model outputs
2. Perform ablation studies varying LoRA rank parameters (k) to determine the optimal balance between efficiency and performance for Vietnamese language tasks
3. Test model performance on zero-shot mathematical reasoning and code generation tasks to assess limitations beyond the evaluated general and medical domains