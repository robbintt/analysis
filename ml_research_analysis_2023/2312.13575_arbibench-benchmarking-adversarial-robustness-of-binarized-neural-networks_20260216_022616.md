---
ver: rpa2
title: 'ARBiBench: Benchmarking Adversarial Robustness of Binarized Neural Networks'
arxiv_id: '2312.13575'
source_url: https://arxiv.org/abs/2312.13575
tags:
- robustness
- attacks
- adversarial
- bnns
- fp32
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ARBiBench, a comprehensive benchmark for evaluating
  the adversarial robustness of binarized neural networks (BNNs). The authors evaluate
  seven representative BNN architectures on CIFAR-10 and ImageNet datasets using ten
  different adversarial attack methods, including four white-box and six black-box
  attacks.
---

# ARBiBench: Benchmarking Adversarial Robustness of Binarized Neural Networks

## Quick Facts
- arXiv ID: 2312.13575
- Source URL: https://arxiv.org/abs/2312.13575
- Authors: 
- Reference count: 40
- Key outcome: This paper presents ARBiBench, a comprehensive benchmark for evaluating the adversarial robustness of binarized neural networks (BNNs). The authors evaluate seven representative BNN architectures on CIFAR-10 and ImageNet datasets using ten different adversarial attack methods, including four white-box and six black-box attacks. The primary findings reveal that BNNs exhibit opposite robustness performance on small-scale and large-scale datasets under white-box attacks, with BNNs being less robust than FP32 on CIFAR-10 but more robust on ImageNet. BNNs consistently demonstrate better adversarial robustness under black-box attacks compared to FP32 models. The authors also observe that different BNNs exhibit certain similarities in their robustness performance. Through further analysis, they identify that the number of images per category significantly influences BNN robustness under white-box attacks, with BNNs becoming more robust as the number of images per category decreases. Additionally, Class Activation Maps (CAM) suggest that BNNs have more concentrated regions of interest, which may contribute to their enhanced robustness against black-box attacks.

## Executive Summary
This paper introduces ARBiBench, a comprehensive benchmark for evaluating the adversarial robustness of binarized neural networks (BNNs) against various attack methods. The study systematically evaluates seven representative BNN architectures on CIFAR-10 and ImageNet datasets using ten different adversarial attacks. The key findings reveal that BNNs exhibit opposite robustness performance on small-scale and large-scale datasets under white-box attacks, with BNNs being less robust than FP32 on CIFAR-10 but more robust on ImageNet. Additionally, BNNs consistently demonstrate better adversarial robustness under black-box attacks compared to FP32 models. The authors also observe that different BNNs exhibit certain similarities in their robustness performance and identify that the number of images per category significantly influences BNN robustness under white-box attacks.

## Method Summary
The authors implement seven BNN architectures (BNN, XNOR-Net, DoReFa, Bi-Real Net, XNOR-Net++, ReActNet, ReCU) and train them on CIFAR-10 and ImageNet datasets using standard training procedures. They evaluate robustness using ten adversarial attack methods, including four white-box (FGSM, PGD, DeepFool, C&W) and six black-box (SPSA, N ATTACK, Square, Boundary, Evolutionary, SI-NI-FGSM) attacks. The evaluation metrics include normalized adversarial accuracy (ACC_norm) and robustness score (RS), calculated as the mean of ACC_norm across attacks. The study controls for various factors and conducts ablation experiments to analyze the influence of dataset scale and image resolution on BNN robustness.

## Key Results
- BNNs exhibit opposite robustness performance on small-scale (CIFAR-10) and large-scale (ImageNet) datasets under white-box attacks, with BNNs being less robust than FP32 on CIFAR-10 but more robust on ImageNet.
- BNNs consistently demonstrate better adversarial robustness under black-box attacks compared to FP32 models.
- The number of images per category significantly influences BNN robustness under white-box attacks, with BNNs becoming more robust as the number of images per category decreases.
- Class Activation Maps (CAM) suggest that BNNs have more concentrated regions of interest, which may contribute to their enhanced robustness against black-box attacks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BNNs exhibit opposite robustness performance on small-scale (CIFAR-10) and large-scale (ImageNet) datasets under white-box attacks.
- Mechanism: The number of images per category significantly influences BNN robustness. As dataset size increases, BNNs gradually surpass FP32 in robustness.
- Core assumption: The relationship between dataset scale and model robustness is primarily determined by the number of images per category, not by image resolution or network capacity.
- Evidence anchors:
  - [abstract]: "The authors also observe that the number of images per category significantly influences BNN robustness under white-box attacks, with BNNs becoming more robust as the number of images per category decreases."
  - [section]: "We first investigate the impact of the number of images per category on the robustness between BNN and FP32. By controlling the number of images per class in the ImageNet-10 dataset to be 6,000, 3,000, and 1,200...it is evident that the relative robustness of FP32 and BNN gradually changes with the variation of dataset volume."
  - [corpus]: Weak - The corpus contains papers on BNN verification and robustness but does not directly address the dataset scale effect described here.
- Break condition: If the robustness relationship between BNNs and FP32 under white-box attacks does not reverse when varying the number of images per category, this mechanism would be invalidated.

### Mechanism 2
- Claim: BNNs consistently demonstrate better adversarial robustness under black-box attacks compared to FP32 models.
- Mechanism: BNNs have more concentrated regions of interest (RoIs) in their Class Activation Maps (CAM), making them less susceptible to black-box attacks which cannot directly access these critical regions.
- Core assumption: The concentration of RoIs in BNNs directly correlates with their robustness against black-box attacks.
- Evidence anchors:
  - [abstract]: "Additionally, Class Activation Maps (CAM) suggest that BNNs have more concentrated regions of interest, which may contribute to their enhanced robustness against black-box attacks."
  - [section]: "From Fig. 14, we observe that the CAM outputs of BNN exhibit more concentrated RoIs than FP32, indicating that smaller regions have a great impact on the final classification...Consequently, the performance of small and concentrated RoIs in BNN may contribute to their enhanced robustness against black-box attacks."
  - [corpus]: Weak - The corpus includes papers on BNN verification and adversarial attacks but lacks direct evidence linking CAM concentration to black-box robustness.
- Break condition: If experiments show that BNNs do not consistently outperform FP32 in black-box attacks, or if CAM concentration does not correlate with robustness, this mechanism would fail.

### Mechanism 3
- Claim: Different BNNs exhibit certain similarities in their robustness performance.
- Mechanism: Despite architectural differences, BNNs share common properties that lead to similar robustness behaviors across various attack types.
- Core assumption: The binarization process imposes constraints that create consistent robustness patterns across different BNN architectures.
- Evidence anchors:
  - [abstract]: "The authors also observe that the number of images per category significantly influences BNN robustness under white-box attacks, with BNNs becoming more robust as the number of images per category decreases. Additionally, Class Activation Maps (CAM) suggest that BNNs have more concentrated regions of interest, which may contribute to their enhanced robustness against black-box attacks."
  - [section]: "Furthermore, we conduct experiments to analyze the observed phenomena, facilitating a deeper understanding of the adversarial robustness of BNNs."
  - [corpus]: Weak - The corpus contains papers on BNN verification and robustness but does not provide specific evidence for similarities in robustness performance across different BNN architectures.
- Break condition: If future experiments show significant variability in robustness performance among different BNN architectures, this mechanism would be challenged.

## Foundational Learning

- Concept: Adversarial robustness in neural networks
  - Why needed here: Understanding the concept of adversarial robustness is crucial for interpreting the results and implications of the ARBiBench benchmark.
  - Quick check question: What is the difference between white-box and black-box attacks in the context of adversarial robustness?

- Concept: Network binarization and its impact on model performance
  - Why needed here: BNNs are the focus of this study, and understanding how binarization affects model performance is essential for interpreting the results.
  - Quick check question: How does the binarization process affect the computational complexity and storage requirements of neural networks?

- Concept: Class Activation Maps (CAM) and their role in understanding model behavior
  - Why needed here: CAM is used in the study to analyze the regions of interest in BNNs, which may contribute to their robustness against black-box attacks.
  - Quick check question: How can CAM be used to visualize and interpret the decision-making process of a neural network?

## Architecture Onboarding

- Component map:
  ARBiBench benchmark framework -> Seven BNN architectures -> Ten adversarial attack methods -> CIFAR-10 and ImageNet datasets -> Evaluation metrics

- Critical path:
  1. Implement BNN architectures and adversarial attack methods
  2. Train BNNs on CIFAR-10 and ImageNet datasets
  3. Evaluate robustness using various attack methods
  4. Analyze results and identify patterns in robustness performance
  5. Conduct further experiments to explore influencing factors (e.g., number of images per category, CAM analysis)

- Design tradeoffs:
  - Binarization vs. model accuracy: BNNs offer computational efficiency but may sacrifice some accuracy compared to FP32 models.
  - Attack strength vs. computational cost: Stronger attacks may provide more insight into model robustness but require more computational resources.
  - Dataset size vs. experimental feasibility: Larger datasets like ImageNet provide more comprehensive results but are more computationally intensive to work with.

- Failure signatures:
  - BNNs not consistently outperforming FP32 in black-box attacks
  - No observable pattern in robustness performance across different BNN architectures
  - CAM analysis not correlating with robustness against black-box attacks

- First 3 experiments:
  1. Reproduce the ARBiBench benchmark results on CIFAR-10 using one BNN architecture and a subset of attack methods.
  2. Investigate the impact of varying the number of images per category on BNN robustness by creating custom datasets based on CIFAR-10.
  3. Analyze the Class Activation Maps of BNNs and compare them to their FP32 counterparts to identify differences in regions of interest.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between the number of images per category and BNN robustness under white-box attacks?
- Basis in paper: [explicit] The authors found that as the number of images per category decreases, BNN robustness increases under white-box attacks.
- Why unresolved: While the authors demonstrated this relationship, they did not establish the exact functional form or underlying mechanism.
- What evidence would resolve it: Detailed experiments varying the number of images per category across multiple orders of magnitude, coupled with analysis of learned feature distributions.

### Open Question 2
- Question: Why do BNNs show better robustness under black-box attacks compared to FP32 models?
- Basis in paper: [explicit] The authors observed that BNNs consistently exhibit better robustness under black-box attacks across datasets.
- Why unresolved: The authors proposed a hypothesis involving Class Activation Maps (CAM) showing more concentrated regions of interest, but did not definitively prove this mechanism.
- What evidence would resolve it: Controlled experiments manipulating the concentration of regions of interest in both BNNs and FP32 models, measuring corresponding changes in black-box attack success rates.

### Open Question 3
- Question: What causes the opposite robustness performance of BNNs under white-box attacks between CIFAR-10 and ImageNet datasets?
- Basis in paper: [explicit] The authors observed that BNNs are less robust than FP32 on CIFAR-10 but more robust on ImageNet under white-box attacks.
- Why unresolved: The authors explored potential factors like image resolution and number of categories but found no conclusive explanation.
- What evidence would resolve it: Systematic ablation studies varying dataset characteristics (resolution, number of categories, number of images per category) while controlling for other factors, identifying the key differentiating factor.

### Open Question 4
- Question: How does the transferability of adversarial perturbations differ between BNNs and FP32 models?
- Basis in paper: [explicit] The authors found that adversarial perturbations transfer well within BNNs but poorly between BNNs and FP32 models.
- Why unresolved: While the authors observed this phenomenon, they did not investigate the underlying reasons for the differences in transferability.
- What evidence would resolve it: Detailed analysis of feature representations and decision boundaries across BNNs and FP32 models, correlating these with observed transferability patterns.

## Limitations

- The study focuses on seven BNN variants without exploring the full spectrum of binarization techniques or emerging architectures.
- The analysis of CAM-based mechanisms for black-box robustness remains correlational rather than causal, with limited ablation studies to verify the proposed explanations.
- The evaluation is limited to CIFAR-10 and ImageNet datasets, which may not fully represent the diversity of real-world applications.

## Confidence

- Medium confidence in the dataset-scale robustness reversal mechanism, as it is well-supported by controlled experiments but may not generalize to all BNN variants
- Medium confidence in the black-box robustness advantage of BNNs, though the CAM analysis provides only indirect evidence
- Low confidence in the claimed architectural similarities across BNNs, as this observation is not extensively validated across diverse architectures

## Next Checks

1. Conduct ablation studies varying binarization techniques while keeping architecture constant to isolate the effect of binarization on robustness patterns
2. Perform controlled experiments systematically varying image resolution and network capacity to separate their effects from the number of images per category
3. Design targeted experiments testing whether manipulating CAM concentration directly affects black-box robustness to establish causal relationships