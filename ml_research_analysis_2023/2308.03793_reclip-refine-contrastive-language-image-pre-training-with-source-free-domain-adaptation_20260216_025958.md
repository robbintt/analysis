---
ver: rpa2
title: 'ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain
  Adaptation'
arxiv_id: '2308.03793'
source_url: https://arxiv.org/abs/2308.03793
tags:
- text
- clip
- visual
- reclip
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReCLIP, the first source-free domain adaptation
  method for vision-language models like CLIP. It addresses the challenges of visual
  and text domain gaps, and cross-modality misalignment, which impact CLIP's performance
  in downstream tasks.
---

# ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation

## Quick Facts
- arXiv ID: 2308.03793
- Source URL: https://arxiv.org/abs/2308.03793
- Reference count: 40
- Improves CLIP's average accuracy from 69.83% to 74.94% across 22 datasets without source data or target labels

## Executive Summary
ReCLIP introduces the first source-free domain adaptation method for vision-language models like CLIP. It addresses the challenges of visual and text domain gaps, and cross-modality misalignment, which impact CLIP's performance in downstream tasks. ReCLIP first aligns visual and text embeddings by removing redundant and class-agnostic information through a learned projection space, then generates pseudo labels via label propagation. It then applies cross-modality self-training, iteratively refining embeddings and labels by updating both visual and text encoders. Extensive experiments show ReCLIP significantly improves CLIP's average accuracy from 69.83% to 74.94% across 22 datasets, demonstrating its effectiveness in enhancing vision-language models for novel domains without requiring source data or target labels.

## Method Summary
ReCLIP employs a two-step approach to source-free domain adaptation for CLIP. First, it aligns visual and text embeddings using learned projection matrices P1 and P2, which remove class-agnostic and redundant information respectively. Label propagation is then used to generate pseudo labels for the target domain. Second, cross-modality self-training iteratively refines both visual and text encoders using high-confidence pseudo labels, updating only commonly agreed labels between the two encoders. This process reduces domain gaps and misalignments while avoiding the need for source data or target labels.

## Key Results
- Improves CLIP's average accuracy from 69.83% to 74.94% across 22 datasets
- Reduces average error rate from 30.17% to 25.06%
- Achieves significant performance gains in challenging fine-grained classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing redundant and class-agnostic information from CLIP embeddings realigns visual and text spaces.
- Mechanism: A learned projection subspace is used to eliminate inactive dimensions and irrelevant information, pulling text and visual embeddings closer for same-class pairs.
- Core assumption: The redundant dimensions in text embeddings and class-agnostic dimensions in visual embeddings dominate similarity calculation, causing misalignment.
- Evidence anchors:
  - [abstract] "learns a projection space to mitigate the misaligned visual-text embeddings by removing redundant and class-agnostic information"
  - [section] "With Singular Value Decomposition ... we get U = [ e1, e2, ..., em] as the orthonormal basis of the span of T ... With U ′ = [ e2, e3, ..., em] we define a new projection matrix P2 ... removes the redundant information from text embeddings"
- Break condition: If the redundancy and class-agnostic components are not dominant in similarity calculation, or if the SVD basis does not effectively capture them, the projection will fail to realign embeddings.

### Mechanism 2
- Claim: Cross-modality self-training iteratively refines embeddings and pseudo labels to reduce domain gaps and misalignments.
- Mechanism: Two parallel components fine-tune visual and text encoders separately using pseudo labels. High-confidence labels agreed by both are shared and used to update both encoders in the next iteration.
- Evidence anchors:
  - [abstract] "deploys cross-modality self-training with the pseudo labels, to update visual and text encoders, refine labels and reduce domain gaps and misalignments iteratively"
  - [section] "ReCLIP collects pseudo labels from both ReCLIP-V and ReCLIP-T at the end of each epoch, and updates both models with only the commonly agreed pseudo labels"
- Break condition: If pseudo labels diverge significantly or if the commonly agreed set becomes too small, the self-training process may collapse or overfit to noisy labels.

### Mechanism 3
- Claim: Label propagation on aligned embeddings generates reliable pseudo labels for target domain.
- Mechanism: After aligning embeddings with projection, label propagation propagates class information from text embeddings to visual embeddings via nearest neighbor connections in the embedding space.
- Evidence anchors:
  - [abstract] "learns pseudo labels ... via label propagation"
  - [section] "With labeled examples {ti} and unlabeled examples {vj}, we make the union set L ... we use conjugate gradient (CG) to approximately solve Equation 1 ... the pseudo label can be given by ˜yj := arg max i zm+j,i"
- Break condition: If the projection does not sufficiently align the embeddings, or if the graph connectivity is poor (k-NN fails), label propagation will generate noisy pseudo labels.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and projection subspaces
  - Why needed here: SVD is used to find the orthonormal basis of the text embedding span, enabling construction of projection matrices that remove redundant and class-agnostic dimensions.
  - Quick check question: What is the role of the first singular vector e1 in the projection matrix P2?

- Concept: Label propagation for semi-supervised learning
  - Why needed here: Label propagation is adapted to propagate class labels from text embeddings (treated as labeled) to visual embeddings (treated as unlabeled) in the aligned embedding space.
  - Quick check question: How does the propagation strength α in label propagation affect the confidence of pseudo labels?

- Concept: Cross-modality self-training
  - Why needed here: Cross-modality self-training is used to iteratively refine both visual and text encoders using pseudo labels, reducing domain gaps and misalignments.
  - Quick check question: Why does ReCLIP use only commonly agreed pseudo labels to update both encoders?

## Architecture Onboarding

- Component map: Pre-trained CLIP model -> Projection module (P1 and P2 matrices) -> Label propagation module -> ReCLIP-V (visual encoder fine-tuning) -> ReCLIP-T (text encoder fine-tuning) -> Pseudo label sharing and selection module
- Critical path: 1. Align embeddings with projection matrices P1 and P2. 2. Generate pseudo labels via label propagation. 3. Iteratively fine-tune visual and text encoders using ReCLIP-V and ReCLIP-T. 4. Share and filter pseudo labels between ReCLIP-V and ReCLIP-T. 5. Update encoders using high-confidence labels.
- Design tradeoffs:
  - Updating only layer-norm weights vs. full fine-tuning: lighter and more stable but may limit adaptation capacity.
  - Using projection subspaces vs. fine-tuning embeddings directly: removes redundancy but may lose useful information if not carefully designed.
  - Cross-modality self-training vs. unimodal adaptation: more robust but requires careful pseudo label agreement.
- Failure signatures:
  - Pseudo labels diverge or collapse to a few classes.
  - Embeddings become over-regularized and lose discriminative power.
  - Training becomes unstable due to noisy pseudo labels.
- First 3 experiments:
  1. Verify projection matrices P1 and P2 improve intra-class visual-text similarity on a small ablation dataset.
  2. Check label propagation generates reasonable pseudo labels on the projected embedding space.
  3. Test ReCLIP-V and ReCLIP-T individually on a small dataset to ensure they can reduce domain gaps in isolation.

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- The learned projection subspace may remove useful information alongside redundancy, potentially degrading model performance if not carefully tuned.
- Cross-modality self-training's reliance on commonly agreed pseudo labels could limit adaptation when pseudo labels diverge, especially in highly diverse target domains.
- Label propagation effectiveness is constrained by graph connectivity and embedding alignment quality; poor alignment or sparse neighborhoods can yield noisy pseudo labels.

## Confidence
- **High**: CLIP's domain gaps and misalignment are significant problems requiring adaptation; projection-based embedding alignment is a plausible and effective solution.
- **Medium**: Cross-modality self-training improves both visual and text encoders simultaneously; ablation studies show component contributions but exact attribution is unclear.
- **Low**: Learned projection matrices universally remove only redundant/class-agnostic information; effectiveness depends heavily on dataset characteristics.

## Next Checks
1. Verify projection matrices P1 and P2 improve intra-class visual-text similarity on a small ablation dataset before full-scale training.
2. Check label propagation generates reasonable pseudo labels on the projected embedding space with varying neighbor sizes (k) and propagation strengths (α).
3. Test ReCLIP-V and ReCLIP-T individually on a small dataset to ensure they can reduce domain gaps in isolation before cross-modality self-training.