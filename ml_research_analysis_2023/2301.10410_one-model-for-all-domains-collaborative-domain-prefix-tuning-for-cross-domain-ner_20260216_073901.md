---
ver: rpa2
title: 'One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain
  NER'
arxiv_id: '2301.10410'
source_url: https://arxiv.org/abs/2301.10410
tags:
- domain
- domains
- source
- target
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles cross-domain named entity recognition (NER)
  by addressing the challenge of limited labeled data in target domains. The proposed
  method, CP-NER, formulates NER as a text-to-text generation task with domain-related
  instructions, enabling one frozen PLM for all domains.
---

# One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER

## Quick Facts
- **arXiv ID**: 2301.10410
- **Source URL**: https://arxiv.org/abs/2301.10410
- **Reference count**: 15
- **Primary result**: CP-NER achieves state-of-the-art performance on cross-domain NER with a 3.56% F1-score improvement over previous methods in single-source setting

## Executive Summary
This paper addresses the challenge of cross-domain named entity recognition (NER) by proposing CP-NER, a method that enables one frozen pre-trained language model (PLM) to handle NER tasks across multiple domains. The approach reformulates NER as a text-to-text generation task with domain-related instructions, eliminating the need for structural modifications to PLMs. CP-NER introduces collaborative domain-prefix tuning that synthesizes knowledge from multiple source domains through a dual-query selector and intrinsic decomposition mechanism. Experiments on the CrossNER benchmark demonstrate significant improvements over state-of-the-art methods in both single-source and multiple-source cross-domain NER settings.

## Method Summary
CP-NER formulates NER as text-to-text generation by grounding domain-related instructions, allowing knowledge transfer without structural modifications to frozen PLMs. The method employs collaborative domain-prefix tuning with three key components: domain-specific warm-up to capture initial knowledge, a dual-query domain selector that identifies beneficial source domains based on entity and prefix similarity, and intrinsic decomposition to synthesize the final powerful prefix from multiple sources. The prefix in the Transformer layer acts as a domain controller, steering the model to output domain-specific predictions through linear interpolation of standard attention and domain-specific prefix attention.

## Key Results
- Achieves 3.56% F1-score improvement over previous best method in single-source cross-domain NER
- Demonstrates superior performance compared to existing methods (LANER, LIGHTNER) across all target domains in multiple-source setting
- Shows consistent performance gains with 2.25% average improvement over LIGHTNER in multiple-source transfer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-to-text generation with domain-related instructions enables knowledge transfer without structural modifications to PLMs
- Mechanism: Reformulating NER as sequence-to-sequence generation uses entity category semantics as grounding for the frozen PLM to generate named entity sequences
- Core assumption: Domain-related instructions can effectively guide PLMs to understand and execute NER tasks across different domains
- Evidence anchors: [abstract] "We present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications."
- Break condition: If domain-related instructions fail to provide clear semantics for the PLM to understand the NER task

### Mechanism 2
- Claim: Collaborative domain-prefix tuning synthesizes knowledge from multiple source domains to enhance target domain performance
- Mechanism: Uses domain-specific warm-up, dual-query domain selector to identify beneficial source domains, and intrinsic decomposition to synthesize final powerful prefix
- Core assumption: Multiple source domains contain complementary knowledge that can be effectively combined
- Evidence anchors: [abstract] "We utilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate the potential of PLMs to handle NER tasks across various domains."
- Break condition: If source domains are too dissimilar to the target domain or dual-query selector fails to identify beneficial domains

### Mechanism 3
- Claim: The prefix acts as a domain controller, steering the model to output domain-specific predictions
- Mechanism: The prefix modifies original head attention through linear interpolation with a scalar factor
- Core assumption: The prefix can effectively control domain-specific behavior without modifying underlying parameters
- Evidence anchors: [section] "The domain-specific prefix tuning essentially modifies the original head attention through linear interpolation by a scalar factor"
- Break condition: If prefix fails to effectively control domain-specific behavior

## Foundational Learning

- **Text-to-text generation with domain-related instructions**: Enables knowledge transfer without structural modifications to PLMs, allowing one frozen model for all domains. *Quick check: How does the text-to-text generation formulation help in transferring knowledge to new domain NER tasks without architectural changes?*

- **Prefix-tuning in Transformer layers**: Allows lightweight adaptation of PLMs to different domains without modifying underlying parameters. *Quick check: How does prefix-tuning modify the attention mechanism in Transformer layers to control domain-specific behavior?*

- **Dual-query domain selector**: Identifies which source domains can benefit the target domain based on entity and prefix similarity. *Quick check: How does the dual-query domain selector determine the most beneficial source domains for the target domain?*

## Architecture Onboarding

- **Component map**: Text-to-text generation with DRI-T5 -> Domain-specific warm-up -> Dual-query domain selector (entity similarity and prefix similarity) -> Intrinsic decomposition for collaborative domain-prefix -> Frozen T5 model

- **Critical path**: 1) Warm up domain-specific prefixes using domain corpus, 2) Calculate entity and prefix similarity between source and target domains, 3) Select beneficial source domains using dual-query selector, 4) Perform intrinsic decomposition to synthesize final prefix, 5) Use synthesized prefix with frozen T5 model for cross-domain NER

- **Design tradeoffs**: Single frozen model vs. separate models for each domain, lightweight prefix-tuning vs. full model fine-tuning, multiple source domains vs. single source domain

- **Failure signatures**: Poor performance on target domain, inability to effectively combine knowledge from multiple source domains, failure to identify beneficial source domains

- **First 3 experiments**: 1) Test text-to-text generation formulation on single source domain transfer, 2) Evaluate dual-query domain selector's ability to identify beneficial source domains, 3) Assess intrinsic decomposition's effectiveness in synthesizing final prefix from multiple sources

## Open Questions the Paper Calls Out

- **Scaling to many domains**: The paper mentions CP-NER can realize "one model for all domains" but only evaluates on five domains. It doesn't provide empirical evidence on performance degradation or parameter efficiency when scaling to hundreds of domains.

- **Mechanism of domain-specific warm-up**: The paper describes domain-specific warm-up as the first step but doesn't provide ablation studies isolating its effect from other components, making it difficult to quantify this component's contribution.

- **Dual-query selector sensitivity**: The paper mentions the hyperparameter α determines the ratio between entity similarity and prefix similarity but doesn't report sensitivity analysis or explore how performance varies with different α values.

- **Performance vs. full fine-tuning**: The paper focuses on low-resource scenarios and mentions previous approaches "normally tune all parameters of PLMs," implying CP-NER is designed for parameter efficiency, but doesn't test performance trade-offs at scale.

## Limitations

- The method's effectiveness depends on the availability of multiple source domains that are sufficiently similar to the target domain
- The dual-query domain selector's accuracy in identifying beneficial source domains is critical but not thoroughly validated
- The approach requires careful hyperparameter tuning, particularly for the α parameter in the dual-query selector

## Confidence

- **High Confidence**: Text-to-text generation formulation and use of frozen PLMs for cross-domain NER are well-established concepts
- **Medium Confidence**: Collaborative domain-prefix tuning approach shows promise but requires more detailed validation
- **Low Confidence**: Effectiveness in cases where source domains are highly dissimilar to target domain or dual-query selector fails

## Next Checks

1. **Replicate Entity Similarity Calculation**: Verify implementation of entity similarity calculation using cosine similarity and pooling methods to ensure accurate identification of beneficial source domains

2. **Test Prefix Initialization**: Conduct experiments to validate domain-specific warmup process for prefix initialization, ensuring stable training and effective domain-specific behavior

3. **Evaluate Source Domain Diversity**: Assess impact of source domain diversity on effectiveness of collaborative domain-prefix tuning, particularly when source domains are highly dissimilar to target domain