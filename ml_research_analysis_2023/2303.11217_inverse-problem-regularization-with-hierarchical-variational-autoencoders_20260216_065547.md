---
ver: rpa2
title: Inverse problem regularization with hierarchical variational autoencoders
arxiv_id: '2303.11217'
source_url: https://arxiv.org/abs/2303.11217
tags:
- image
- images
- pnp-hv
- algorithm
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to regularize ill-posed inverse problems
  using a deep hierarchical variational autoencoder (HVAE) as an image prior. The
  proposed method, named PnP-HVAE, combines the advantages of denoiser-based Plug
  & Play approaches and generative model-based approaches to inverse problems.
---

# Inverse problem regularization with hierarchical variational autoencoders

## Quick Facts
- arXiv ID: 2303.11217
- Source URL: https://arxiv.org/abs/2303.11217
- Authors: 
- Reference count: 40
- One-line primary result: PnP-HVAE method regularizes inverse problems using deep HVAEs without backpropagation through the generative network

## Executive Summary
This paper proposes PnP-HVAE, a method that combines Plug-and-Play approaches with hierarchical variational autoencoders (HVAEs) for regularizing ill-posed inverse problems. The method optimizes a joint posterior on image and latent variables using an alternating scheme that exploits the HVAE encoder without requiring backpropagation through the generative network. By leveraging temperature scaling of the latent prior, PnP-HVAE controls regularization strength while maintaining convergence guarantees through reformulation as a proximal splitting algorithm.

## Method Summary
PnP-HVAE addresses inverse problems by treating the HVAE as a learned prior, using alternating optimization between image and latent variables. The method reformulates the problem as minimizing a joint posterior, where the image update solves a linear inverse problem and the latent update uses the HVAE encoder as a Gaussian approximation. Temperature scaling (τ) controls regularization strength, with lower values increasing the KL penalty on latent variables. The method provides convergence guarantees by treating the HVAE reconstruction as a denoising operator in a proximal splitting algorithm, requiring only that the reconstruction be a contraction for noisy images.

## Key Results
- PnP-HVAE achieves competitive performance with state-of-the-art denoiser-based Plug-and-Play methods and generative model-based approaches
- The method demonstrates strong performance on both face images and natural images across deblurring, super-resolution, and inpainting tasks
- Temperature scaling effectively controls the trade-off between reconstruction quality and consistency with observations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alternating optimization between image and latent variables converges without backpropagation through the generative network.
- Mechanism: The encoder provides a closed-form solution for the latent variables given the current image estimate, allowing the generative network parameters to remain fixed during optimization.
- Core assumption: The encoder's conditional distributions are Gaussian with tractable inverses.
- Evidence anchors:
  - [abstract] "PnP-HVAE optimizes a joint posterior on image and latent variables without backpropagation through the generative network"
  - [section 4.3] "we exploit the HVAE encoder to define an alternating algorithm to optimize the joint distribution over the image and its latent variable"
- Break condition: If the encoder's covariance matrices are not invertible or the encoder-approximation assumption fails.

### Mechanism 2
- Claim: Temperature scaling of the latent prior controls the strength of regularization in the inverse problem.
- Mechanism: Lower temperatures (τ < 1) increase the KL penalty on the latent variables, forcing them closer to the HVAE's learned prior distribution.
- Core assumption: The HVAE's latent prior distribution is meaningful and captures relevant image structure.
- Evidence anchors:
  - [section 3.3] "sampling the latent variables zl from a prior with reduced temperature improves the visual quality of the generated images"
  - [section 4.1] "The temperature of the prior over the latent space τl controls the weight of the regularization over the latent variable zl"
- Break condition: If temperature scaling causes the optimization to get stuck in poor local minima.

### Mechanism 3
- Claim: Reformulating as a Plug-and-Play method provides convergence guarantees.
- Mechanism: The HVAE reconstruction acts as a denoising operator in a proximal splitting algorithm, with convergence ensured by Lipschitz conditions.
- Core assumption: The HVAE reconstruction operator is a contraction for noisy images.
- Evidence anchors:
  - [section 5.1] "Algorithm 2 writes xk+1 = proxγ2f (HVAE(xk,τ))"
  - [section 5.2] "If HVAE(xk,τ) is Lτ < 1-Lipschitz, then iterations (19) converge"
- Break condition: If the Lipschitz constant exceeds 1, violating the contraction requirement.

## Foundational Learning

- Concept: Hierarchical Variational Autoencoders (HVAEs)
  - Why needed here: HVAEs provide the deep generative prior that enables regularization of ill-posed inverse problems.
  - Quick check question: How does the hierarchical structure of HVAEs differ from standard VAEs, and why is this important for image restoration?

- Concept: Plug-and-Play (PnP) methods
  - Why needed here: PnP methods provide the framework for convergence guarantees when using denoisers (or generative models) in iterative optimization.
  - Quick check question: What is the key insight behind PnP methods that allows arbitrary denoisers to be used in optimization algorithms?

- Concept: Variational inference and ELBO
  - Why needed here: Understanding how HVAEs are trained via ELBO maximization is crucial for understanding the prior distribution they learn.
  - Quick check question: What is the relationship between the encoder approximation and the true posterior in a VAE, and how does this affect the regularization?

## Architecture Onboarding

- Component map: Forward model y = Ax + ε -> HVAE (encoder qφ(z|x) and decoder pθ(x|z)) -> Temperature scaling τ -> Alternating optimization (image and latent updates) -> Convergence check (residual ||xk+1 - xk|| < tolerance)

- Critical path:
  1. Initialize x(0)
  2. Compute z(k+1) = Eτ(x(k)) using encoder approximation
  3. Update x(k+1) = (AtA + σ²/γ² Id)^(-1)(Aty + σ²/γ² µθ(z(k+1)))
  4. Check convergence, repeat if necessary

- Design tradeoffs:
  - Trade-off between reconstruction quality and consistency with observation (controlled by temperature τ)
  - Trade-off between computational cost and accuracy (alternating scheme vs joint optimization)

- Failure signatures:
  - Divergence: Residual does not decrease, indicating the reconstruction operator may not be a contraction
  - Poor reconstruction: High LPIPS scores, indicating lack of consistency with the observation
  - Artifacts: Visible artifacts in the reconstructed image, indicating the prior may not be appropriate for the dataset

- First 3 experiments:
  1. Deblurring with known kernel and noise level to verify convergence and reconstruction quality
  2. Super-resolution with ×4 upscaling to test the method's ability to recover high-frequency details
  3. Inpainting with large masks to evaluate the prior's ability to fill in missing information

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PnP-HVAE be extended to non-linear inverse problems?
- Basis in paper: [explicit] The paper focuses on linear inverse problems and mentions that many image restoration tasks can be formulated as linear inverse problems. However, it does not explore the application of PnP-HVAE to non-linear inverse problems.
- Why unresolved: The paper does not provide any theoretical or experimental evidence for the extension of PnP-HVAE to non-linear inverse problems. The convergence analysis and algorithm design are based on the linear inverse problem setting.
- What evidence would resolve it: Theoretical analysis of PnP-HVAE convergence for non-linear inverse problems, along with experimental results demonstrating its effectiveness on non-linear inverse problems.

### Open Question 2
- Question: How does the choice of temperature τ affect the trade-off between reconstruction quality and consistency with the observation in PnP-HVAE?
- Basis in paper: [explicit] The paper discusses the role of temperature in controlling the regularization strength and mentions that reducing the temperature increases the strength of the regularization. However, it does not provide a detailed analysis of how the choice of temperature affects the trade-off between reconstruction quality and consistency with the observation.
- Why unresolved: The paper does not provide a systematic study of the temperature's effect on the trade-off between reconstruction quality and consistency with the observation. The choice of temperature is often based on heuristics or grid search.
- What evidence would resolve it: A comprehensive analysis of the relationship between temperature, reconstruction quality, and consistency with the observation, along with guidelines for choosing an appropriate temperature based on the specific inverse problem and dataset.

### Open Question 3
- Question: Can PnP-HVAE be applied to inverse problems with non-Gaussian noise distributions?
- Basis in paper: [explicit] The paper focuses on inverse problems with Gaussian noise and does not explore the application of PnP-HVAE to inverse problems with non-Gaussian noise distributions.
- Why unresolved: The paper does not provide any theoretical or experimental evidence for the extension of PnP-HVAE to inverse problems with non-Gaussian noise distributions. The convergence analysis and algorithm design are based on the Gaussian noise assumption.
- What evidence would resolve it: Theoretical analysis of PnP-HVAE convergence for inverse problems with non-Gaussian noise distributions, along with experimental results demonstrating its effectiveness on such problems.

## Limitations
- The method's performance heavily depends on the quality of the pre-trained HVAE model, which may not generalize well to datasets outside its training distribution.
- The convergence guarantees rely on the HVAE reconstruction being a contraction, but this may not hold for all HVAE architectures or noise levels.
- The computational cost of the alternating optimization scheme may be prohibitive for very large images or complex inverse problems.

## Confidence

- **High Confidence:** The overall framework of combining PnP methods with generative models is sound, and the alternating optimization scheme is well-defined.
- **Medium Confidence:** The convergence guarantees are established under specific conditions, but their practical applicability may be limited.
- **Low Confidence:** The temperature scaling mechanism's effectiveness and the method's generalization to diverse datasets are not fully validated.

## Next Checks
1. **Architecture Ablation:** Test the method with different HVAE architectures (e.g., VDVAE vs. standard VAE) to assess the impact of hierarchical latent variables on reconstruction quality.
2. **Generalization Test:** Evaluate the method on datasets significantly different from the training data (e.g., using a face HVAE on natural images) to measure its robustness.
3. **Convergence Analysis:** Empirically verify the contraction property of the HVAE reconstruction for various noise levels and HVAE architectures to ensure convergence guarantees hold in practice.