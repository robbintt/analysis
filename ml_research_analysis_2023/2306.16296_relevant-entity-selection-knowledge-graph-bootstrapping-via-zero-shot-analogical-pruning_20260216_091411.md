---
ver: rpa2
title: 'Relevant Entity Selection: Knowledge Graph Bootstrapping via Zero-Shot Analogical
  Pruning'
arxiv_id: '2306.16296'
source_url: https://arxiv.org/abs/2306.16296
tags:
- entities
- dataset
- seed
- pruning
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of bootstrapping a knowledge
  graph (KG) by selecting relevant entities from a large, generic KG like Wikidata,
  based on seed entities of interest. The authors propose an analogy-based zero-shot
  approach that uses analogical reasoning on KG embeddings to keep or prune neighboring
  entities.
---

# Relevant Entity Selection: Knowledge Graph Bootstrapping via Zero-Shot Analogical Pruning

## Quick Facts
- arXiv ID: 2306.16296
- Source URL: https://arxiv.org/abs/2306.16296
- Reference count: 40
- Key outcome: Analogy-based zero-shot pruning outperforms traditional classifiers (LSTM, Random Forest, SVM, MLP) with up to 800x fewer parameters

## Executive Summary
This paper addresses the challenge of bootstrapping a knowledge graph by selecting relevant entities from a large generic KG like Wikidata based on seed entities of interest. The authors propose an analogy-based zero-shot approach that uses analogical reasoning on KG embeddings to keep or prune neighboring entities. The method learns relative similarities and dissimilarities between seed entities and their neighbors from training examples, and can extrapolate to new, unseen seed entities without requiring pruning examples for them. Experiments on two manually labeled datasets from Wikidata show that this approach outperforms traditional classifiers in precision, F1-score, and accuracy, while using significantly fewer parameters.

## Method Summary
The approach trains an analogy-based classifier using quadruples (ð‘’1ð‘  : ð‘’1ð‘Ÿ :: ð‘’2ð‘  : ð‘’2ð‘Ÿ) where keeping or pruning decisions are known for the first pair. At inference, the model generates similar quadruples with an unknown pair and averages the model's confidence scores to decide whether to keep or prune the new neighbor. The method incorporates both entity-only and path-aware analogies, where paths capture the structural context of how entities are reached during expansion. The CNN-based model is trained on valid/invalid analogies built from labeled (seed, neighbor) pairs, and evaluated using 5-fold cross-validation on precision, recall, F1-score, and accuracy metrics.

## Key Results
- Analogy-based approach outperforms LSTM, Random Forest, SVM, and MLP classifiers
- Uses up to 800x fewer parameters than LSTM while achieving better performance
- Demonstrates superior generalization in transfer learning settings
- Path-aware analogies improve performance by providing structural context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The analogy-based classifier can perform zero-shot entity selection by learning relative similarity and dissimilarity patterns from seed entities and their neighbors.
- Mechanism: The model trains on quadruples (ð‘’1ð‘  : ð‘’1ð‘Ÿ :: ð‘’2ð‘  : ð‘’2ð‘Ÿ) where keeping or pruning decisions are known for the first pair. At inference, it generates similar quadruples with an unknown pair and averages the model's confidence scores to decide whether to keep or prune the new neighbor.
- Core assumption: Relative analogies capture meaningful similarity relationships between seed entities and their neighbors in embedding space.
- Evidence anchors:
  - [abstract] "our analogy-based approach outperforms LSTM, Random Forest, SVM, and MLP, with a drastically lower number of parameters."
  - [section 3.2.4] "At inference, on an unknown pair (ð‘’2ð‘  , ð‘’2ð‘Ÿ ): We select ð‘ pairs (ð‘’1ð‘  , ð‘’1ð‘Ÿ ) whose decision is known... generate 2ð‘ quadruples... compute the average of the scores output by the model"
- Break condition: If embedding space does not preserve semantic similarity for seed-neighbor pairs, analogical patterns break down.

### Mechanism 2
- Claim: Including the path from seed to neighbor in the analogy improves performance by providing structural context.
- Mechanism: Instead of using only entities (ð‘’1ð‘  : ð‘’1ð‘Ÿ :: ð‘’2ð‘  : ð‘’2ð‘Ÿ), the model incorporates the intermediate path (ð‘’1ð‘  : (ð‘’3ð‘Ÿ , ð‘’1ð‘Ÿ) :: ð‘’2ð‘  : (ð‘’4ð‘Ÿ , ð‘’5ð‘Ÿ , ð‘’2ð‘Ÿ)), capturing how entities are reached during expansion.
- Core assumption: The traversal path contains discriminative information about relevance beyond the seed-neighbor pair alone.
- Evidence anchors:
  - [section 3.2.2] "we also propose to consider the paths leading to the reached entities... the analogy in Equation (2) becomes as follows: ð‘’1ð‘  : (ð‘’3ð‘Ÿ , ð‘’1ð‘Ÿ) :: ð‘’2ð‘  : (ð‘’4ð‘Ÿ , ð‘’5ð‘Ÿ , ð‘’2ð‘Ÿ)"
  - [section 4.5.1] "the LSTM and path analogy models are the best performing models, which outlines the importance of paths"
- Break condition: If path information adds noise or the path lengths vary too widely, performance may degrade.

### Mechanism 3
- Claim: Zero-shot capability emerges because the model learns relative similarity comparisons, not absolute entity identities.
- Mechanism: Training uses quadruples with known decisions, but at inference, seed entities and their neighbors were never seen together in training, forcing the model to extrapolate similarity judgments.
- Core assumption: Relative similarity learned from seen seed-neighbor pairs transfers to unseen pairs through analogical reasoning.
- Evidence anchors:
  - [section 4.2.1] "Such a splitting on seed entities at testing, guarantees that we evaluate the ability of the model to generalize on unseen seed entities... This extrapolation roots our zero-shot approach."
  - [section 4.5.1] "path analogy model outperforms the LSTM on both unseen and seen entities in Dataset 2"
- Break condition: If domain shift is too large or embeddings are not semantically meaningful, extrapolation fails.

## Foundational Learning

- Concept: Analogical reasoning in vector spaces (parallelogram rule)
  - Why needed here: The model relies on vector embeddings where analogies like Paris:France :: Berlin:Germany correspond to ð‘’ð· âˆ’ ð‘’ð¶ = ð‘’ðµ âˆ’ ð‘’ð´ in embedding space
  - Quick check question: Given embeddings for "king" and "queen", and for "man" and "woman", what embedding should complete "king is to queen as man is to ___"?

- Concept: Zero-shot learning vs few-shot learning
  - Why needed here: The approach is zero-shot because it never sees the exact (seed, neighbor) pair during training, only relative patterns
  - Quick check question: If a model trained on (dog, pet) and (cat, pet) pairs is asked about (parrot, pet), is this zero-shot or few-shot?

- Concept: Convolutional neural networks for sequence classification
  - Why needed here: The analogy-based model uses CNN layers to process concatenated entity embeddings in quadruples
  - Quick check question: What is the purpose of applying multiple convolutional filters with different kernel sizes to an input sequence?

## Architecture Onboarding

- Component map: Pre-trained KG embeddings -> Quadruple generation -> CNN prediction -> Averaging -> Decision threshold
- Critical path: Generate quadruples â†’ CNN prediction â†’ Averaging â†’ Decision threshold
- Design tradeoffs: Simpler analogy (entities only) vs path-aware analogy (more context, more parameters); different analogy configurations (C1, C2, C3) for valid/invalid patterns
- Failure signatures: High variance in precision/recall across folds suggests overfitting; poor transfer learning performance indicates poor generalization
- First 3 experiments:
  1. Train on Dataset 1 with configuration C1, embedding E1, (16,8) filters, evaluate on validation fold
  2. Test path analogy with path length=3, compare precision/recall to entity-only analogy
  3. Evaluate transfer learning: train on Dataset 1, test on Dataset 2, measure performance drop

## Open Questions the Paper Calls Out

- Question: How does the performance of the analogy-based model change when using different types of KG embedding models (e.g., complex, Gaussian, GNN-based) compared to the translational model used in this study?
  - Basis in paper: [explicit] The authors suggest evaluating different types of KG embedding models in future work, noting that their current approach leverages pre-trained embeddings from a translational model.
  - Why unresolved: The paper only experiments with pre-trained translational embeddings, leaving open the question of how other embedding types might affect the model's performance.
  - What evidence would resolve it: Conducting experiments using various KG embedding models (complex, Gaussian, GNN-based) and comparing their performance with the analogy-based model.

- Question: Can the analogy-based model be extended to handle few-shot learning scenarios where a small number of labeled neighbors are available for each seed entity?
  - Basis in paper: [explicit] The authors propose evaluating their model in a few-shot setting as a future extension, where experts would label a few neighbors of each seed entity before expansion.
  - Why unresolved: The current study focuses on zero-shot learning, and the model's performance in few-shot scenarios remains unexplored.
  - What evidence would resolve it: Designing and conducting experiments where the model is trained with a small number of labeled neighbors for each seed entity and evaluating its performance in comparison to the zero-shot setting.

- Question: How does incorporating Large Language Models (LLMs) like BERT into the analogy-based approach affect its performance and address potential issues such as noisy labels or homonyms?
  - Basis in paper: [explicit] The authors mention the possibility of enriching their approach with LLMs as a future research direction, acknowledging potential challenges like noisy labels or homonyms.
  - Why unresolved: The current study does not utilize LLMs, and the impact of their integration on the model's performance and ability to handle specific challenges is unknown.
  - What evidence would resolve it: Implementing the analogy-based model with LLM integration, evaluating its performance on various tasks, and assessing its ability to handle noisy labels and homonyms compared to the current approach.

## Limitations

- Zero-shot capability depends heavily on the quality of pre-trained embeddings and the assumption that analogical patterns in embedding space transfer to unseen entities
- Approach is limited to specific relation types (P31/P279) and may not generalize to arbitrary knowledge graph schemas
- Manual labeling of two datasets creates potential domain-specific biases that may not hold across broader knowledge domains

## Confidence

- High confidence: Analogy-based approach outperforms traditional classifiers in the reported experiments (precision, F1-score, accuracy)
- Medium confidence: Zero-shot generalization claims hold for the specific datasets and relation types tested
- Medium confidence: Parameter efficiency (800x fewer than LSTM) translates to practical deployment advantages

## Next Checks

1. Test the approach on a third, independently labeled dataset from a different domain to verify generalization beyond computer science/information technology
2. Evaluate performance degradation when using lower-quality or out-of-domain pre-trained embeddings to quantify embedding dependence
3. Measure actual inference latency and memory usage on resource-constrained devices to validate practical parameter efficiency claims