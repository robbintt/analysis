---
ver: rpa2
title: 'To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis'
arxiv_id: '2305.13230'
source_url: https://arxiv.org/abs/2305.13230
tags:
- dropout
- data
- training
- tokens
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the "token-crisis" problem where LLMs face
  data scarcity due to the mismatch between data requirements and available high-quality
  text data. The authors empirically study the consequences of repeating pre-training
  data for multiple epochs, finding that models suffer from overfitting and performance
  degradation.
---

# To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis

## Quick Facts
- arXiv ID: 2305.13230
- Source URL: https://arxiv.org/abs/2305.13230
- Reference count: 40
- Primary result: Models suffer from overfitting and performance degradation when repeating pre-training data for multiple epochs due to data scarcity

## Executive Summary
This paper investigates the "token-crisis" problem where large language models face data scarcity due to the mismatch between data requirements and available high-quality text data. The authors empirically study the consequences of repeating pre-training data for multiple epochs, finding that models suffer from overfitting and performance degradation. They identify key factors contributing to this multi-epoch degradation, including dataset size, model parameters, and training objectives, while dataset quality and FLOPs have less influence. Dropout is found to be an effective regularization technique, though it requires careful tuning when scaling up models. The authors also propose using Mixture-of-Experts (MoE) models to predict the behavior of larger dense models, enabling cost-effective hyper-parameter tuning. This approach has the potential to impact efficient LLM development on a broader scale.

## Method Summary
The authors investigate the token-crisis problem by training T5 models on repeated subsets of the C4 dataset for multiple epochs. They systematically vary dataset sizes (227-235 tokens), model scales (Base to XL), and training objectives while measuring validation accuracy and downstream task performance. Dropout regularization is tested across different model sizes, and MoE models with varying expert counts are compared against dense models with comparable parameters to evaluate the feasibility of using MoE for hyperparameter prediction.

## Key Results
- Larger models are more susceptible to overfitting under token-crisis conditions, experiencing significant performance degradation when data is repeated
- Dropout effectively prevents overfitting during data repetition but requires careful tuning when scaling up model size
- MoE models with comparable parameters but lower FLOPs exhibit similar overfitting trends to dense models, enabling cost-effective hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Repeating pre-training data causes overfitting that scales with model size
- Mechanism: When token data is limited, larger models with more parameters memorize training examples more quickly, leading to degraded validation performance and worse downstream task transfer
- Core assumption: Chinchilla scaling law still applies to encoder-decoder models like T5
- Evidence anchors:
  - [abstract] "the model is susceptible to overfitting, leading to multi-epoch degradation"
  - [section] "we observe that larger models are more susceptible to overfitting under token-crisis conditions"
  - [corpus] weak - no direct mention in corpus neighbors
- Break condition: If Chinchilla scaling law breaks down or if regularization techniques eliminate the overfitting effect

### Mechanism 2
- Claim: Dropout effectively prevents overfitting during data repetition but requires careful tuning
- Mechanism: Dropout randomly deactivates neurons during training, preventing the model from relying too heavily on specific parameter configurations and thus reducing memorization
- Core assumption: Dropout's regularization effect outweighs its potential to slow early learning
- Evidence anchors:
  - [abstract] "dropout, which demonstrates remarkable effectiveness but requires careful tuning when scaling up the model size"
  - [section] "using dropout can potentially slow down the model's learning process when an ample amount of training data is available"
  - [corpus] weak - corpus neighbors don't mention dropout effectiveness
- Break condition: If dropout tuning becomes too computationally expensive relative to benefits

### Mechanism 3
- Claim: MoE models can predict dense model behavior for hyperparameter tuning
- Mechanism: MoE models with comparable parameters but lower FLOPs exhibit similar overfitting patterns to dense models, allowing researchers to tune hyperparameters like dropout rates on smaller MoE models first
- Core assumption: The overfitting behavior is primarily determined by parameter count rather than FLOPs
- Evidence anchors:
  - [abstract] "leveraging mixture-of-experts (MoE) enables cost-effective and efficient hyper-parameter tuning"
  - [section] "these two models exhibit almost the same overfitting trend"
  - [corpus] weak - corpus neighbors don't discuss MoE for hyperparameter prediction
- Break condition: If MoE models fail to predict dense model behavior at significantly different FLOPs ratios

## Foundational Learning

- Concept: Chinchilla scaling law
  - Why needed here: The paper's entire investigation is predicated on the assumption that the Chinchilla scaling law (optimal ratio of model size to dataset size) holds for T5 models
  - Quick check question: What relationship between model size and dataset size does the Chinchilla scaling law predict?

- Concept: Multi-epoch training degradation
  - Why needed here: Understanding how model performance degrades when training for multiple epochs on the same data is central to the token-crisis problem
  - Quick check question: What happens to validation accuracy when a model is trained for many epochs on limited data?

- Concept: Dropout regularization
  - Why needed here: Dropout is the primary regularization technique that the paper finds effective against overfitting during data repetition
  - Quick check question: How does dropout prevent overfitting during training?

## Architecture Onboarding

- Component map:
  T5 encoder-decoder architecture with C4 dataset -> MoE layers inserted every 4th transformer block -> Parameter sharing to control FLOPs independently of parameter count -> Dropout layers applied during training

- Critical path:
  1. Train baseline T5 models with varying dataset sizes
  2. Introduce dropout and measure overfitting reduction
  3. Compare MoE vs dense models with comparable parameters
  4. Use MoE models to predict optimal hyperparameters for dense models

- Design tradeoffs:
  - Higher parameter count vs. overfitting risk
  - Dropout regularization vs. slower early learning
  - MoE model prediction accuracy vs. computational savings
  - Training time vs. hyperparameter tuning precision

- Failure signatures:
  - Validation accuracy plateaus or declines during training
  - Downstream task performance drops significantly after fine-tuning
  - MoE models fail to predict dense model behavior
  - Dropout causes unstable training or extremely slow convergence

- First 3 experiments:
  1. Train T5-Base, T5-Large, and T5-XL on 227 tokens for 28 epochs to observe overfitting patterns
  2. Add dropout with varying rates to T5-Large and measure validation accuracy changes
  3. Train T5-MoE with 16 experts and T5-dense with comparable parameters to compare overfitting behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of dropout regularization scale with model size and complexity beyond the tested XL-level model?
- Basis in paper: [explicit] The paper notes that dropout is effective at Base and Large scales but still experiences slight degradation at XL-scale, suggesting potential scaling challenges.
- Why unresolved: The study only tested up to XL-scale models (2.8B parameters). Scaling to models like GPT-3 (175B) or larger would require substantial computational resources not available to the authors.
- What evidence would resolve it: Systematic experiments varying dropout rates and model sizes up to state-of-the-art scales, measuring overfitting trends and performance metrics across multiple token repetition scenarios.

### Open Question 2
- Question: Can the MoE model prediction of dense model behavior be improved when there are large FLOPs disparities between the models?
- Basis in paper: [inferred] The paper found that MoE models with 64 experts cannot perfectly predict XL-level dense model behavior, suggesting limitations when FLOPs gaps are substantial.
- Why unresolved: The study only tested MoE models with 4, 16, and 64 experts. The relationship between expert count, FLOPs, and prediction accuracy across wider ranges remains unexplored.
- What evidence would resolve it: Systematic ablation studies varying expert counts and FLOPs ratios, measuring prediction accuracy against ground truth dense model performance across multiple scales.

### Open Question 3
- Question: What are the optimal strategies for adaptive parameter allocation between frequent and rare samples to improve data efficiency during multi-epoch training?
- Basis in paper: [explicit] The authors mention this as a future research direction, noting that parameter count plays a crucial role in multi-epoch degradation.
- Why unresolved: This represents a novel research direction not explored in the current study. The paper identifies the importance of parameters but does not investigate adaptive allocation strategies.
- What evidence would resolve it: Development and evaluation of adaptive parameter allocation algorithms, measuring their effectiveness in reducing overfitting and improving performance compared to static allocation during repeated data training.

## Limitations
- Empirical validation limited to T5 models on specific dataset sizes, limiting generalizability to other architectures
- Computational overhead of dropout tuning at scale is acknowledged but not quantified
- MoE prediction model claims lack sufficient empirical validation for practical hyperparameter optimization

## Confidence

| Claim | Confidence |
|-------|------------|
| Repeating pre-training data causes overfitting that scales with model size | Medium |
| Dropout effectively prevents overfitting during data repetition but requires careful tuning | Medium |
| MoE models can predict dense model behavior for hyperparameter tuning | Low |
| Chinchilla scaling law applies to encoder-decoder models like T5 | Medium |
| Dataset quality has less influence on multi-epoch degradation | Low |

## Next Checks
1. Validate the overfitting patterns and dropout effectiveness on different LLM architectures (GPT-style transformers, BERT variants) using the same experimental protocol to assess cross-architecture applicability
2. Implement a full hyperparameter optimization pipeline using MoE models to predict optimal dropout rates for dense models, then verify if the predicted hyperparameters actually improve dense model performance in practice
3. Systematically vary dataset quality (using filtered vs. unfiltered C4) while keeping token counts constant to quantify the relative importance of data quality versus quantity in multi-epoch training scenarios