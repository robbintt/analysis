---
ver: rpa2
title: 'Automatically measuring speech fluency in people with aphasia: first achievements
  using read-speech data'
arxiv_id: '2308.04763'
source_url: https://arxiv.org/abs/2308.04763
tags:
- speech
- fluency
- were
- ratings
- aphasia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study aimed to predict speech fluency in people with aphasia
  using automatic signal-processing algorithms. Speech recordings of 29 PWA and five
  controls were analyzed using forward-backward divergence segmentation and clustering
  to compute predictors of fluency: pseudo-syllable rate, speech ratio, rate of silent
  breaks, and standard deviation of pseudo-syllable length.'
---

# Automatically measuring speech fluency in people with aphasia: first achievements using read-speech data

## Quick Facts
- arXiv ID: 2308.04763
- Source URL: https://arxiv.org/abs/2308.04763
- Reference count: 0
- The study used signal-processing algorithms to predict speech fluency in people with aphasia with correlation coefficient up to 0.96.

## Executive Summary
This study developed and tested automatic algorithms for measuring speech fluency in people with aphasia (PWA) using read-speech data. The approach uses forward-backward divergence segmentation to divide audio signals into pseudo-syllables and silent breaks, then computes four predictors of fluency. When combined in multiple regression models, these predictors achieved high correlation with speech-language pathologists' subjective fluency ratings. The methods showed promise as a cost-effective, ASR-independent tool for clinical assessment of fluency in aphasia populations.

## Method Summary
The study analyzed speech recordings from 29 PWA and 5 controls who read the three longest sentences from the French version of the Boston Diagnostic Aphasia Examination. Audio signals were processed using forward-backward divergence segmentation (FBDS) to divide the signal into sub-phonemic units. These segments were automatically clustered into pseudo-syllables and silent breaks based on energy thresholds. Four acoustic predictors were computed: pseudo-syllable rate (syllables per second), speech ratio (proportion of speech to silence), rate of silent breaks (breaks per second), and standard deviation of pseudo-syllable length. Multiple regression models combined these predictors to predict SLP fluency ratings using leave-one-speaker-out validation.

## Key Results
- Automatic predictors achieved correlation coefficients up to 0.96 with SLP fluency ratings
- Pseudo-syllable rate and speech ratio were the most important predictors
- Aggregating predictions across multiple sentences per participant increased correlation from 0.87 to 0.93
- RMSE decreased from 0.52 to 0.40 when using up to three sentences per participant

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The forward-backward divergence segmentation (FBDS) algorithm can accurately segment speech signals into sub-phonemic units without requiring ASR.
- Mechanism: FBDS processes the temporal energy trajectory of the audio signal in both forward and backward directions, detecting abrupt changes that correspond to boundaries between phones or production stages. This yields a segmentation independent of linguistic content.
- Core assumption: Energy-based boundaries reliably correspond to acoustic changes in speech production, even in atypical speech like aphasia.
- Evidence anchors:
  - The algorithms used are robust to noisy conditions and require low computing resources, making them suitable for real-life clinical scenarios.
  - FBDS segments are automatically clustered into pseudo-syllables and silent breaks, and these clusters were shown to improve prediction of speech fluency.
  - The corpus contains related work on using signal processing (rather than ASR) for detecting anomalies in aphasia speech, supporting the feasibility of FBDS.
- Break condition: If the energy changes do not reliably map to phonetic or articulatory boundaries in the target population, segmentation accuracy will degrade.

### Mechanism 2
- Claim: Combining multiple automatic predictors (pseudo-syllable rate, speech ratio, silent break rate, pseudo-syllable duration variability) into a regression model yields accurate fluency predictions.
- Mechanism: Each predictor captures a different aspect of speech fluency: rate (speed), ratio (proportion of speech to silence), breaks (disruptions), and duration variability (disfluency markers like hesitations or false starts). Together, they model the multidimensional nature of fluency.
- Core assumption: These acoustic measures correlate linearly or nonlinearly with human fluency ratings and are not redundant.
- Evidence anchors:
  - Multiple regression models predicted SLP fluency ratings with high accuracy (correlation coefficient up to 0.96).
  - All automatic predictors were strongly associated with subjective ratings; pseudo-syllable rate and speech ratio were most important.
  - Studies on L2 fluency also show that combinations of acoustic features predict perceived fluency, validating the approach.
- Break condition: If predictors are highly collinear or fail to capture key fluency dimensions, model performance will plateau or degrade.

### Mechanism 3
- Claim: Aggregating predictions across multiple sentences per participant improves accuracy.
- Mechanism: Individual sentence ratings vary due to content difficulty or random fluctuations, but averaging across several sentences stabilizes the estimate and reduces measurement noise.
- Core assumption: Within-participant variability is smaller than between-participant variability for fluency.
- Evidence anchors:
  - When aggregating data for each participant, correlation increased from 0.87 to 0.93.
  - RMSE decreased from 0.52 to 0.40 when using up to three sentences per participant.
  - This aggregation strategy is common in clinical and linguistic studies to improve reliability.
- Break condition: If individual sentences are too homogeneous (e.g., all very easy or hard), aggregation provides little benefit.

## Foundational Learning

- Concept: Sub-phonemic segmentation using energy-based methods.
  - Why needed here: The system relies on FBDS, which operates at the level below phones, to avoid ASR errors in atypical speech.
  - Quick check question: What is the main difference between FBDS segmentation and traditional phoneme-based ASR segmentation?

- Concept: Multiple regression and model validation via leave-one-speaker-out.
  - Why needed here: Ensures that the model generalizes to unseen speakers and avoids overfitting given the small dataset.
  - Quick check question: Why is LOSO preferred over random train/test splits in this context?

- Concept: Acoustic predictors of fluency (rate, ratio, pause count, duration variability).
  - Why needed here: These are the features extracted from FBDS segments and are essential for understanding what the model learns.
  - Quick check question: How does pseudo-syllable duration variability relate to disfluency in aphasia?

## Architecture Onboarding

- Component map: Audio input → FBDS segmentation → Cluster into pseudo-syllables/silent breaks → Compute four predictors → Multivariate regression → Fluency rating output
- Critical path: Segmentation → Clustering → Predictor computation → Regression prediction
- Design tradeoffs: Low computational cost and ASR independence vs. sensitivity to energy thresholding; linear models for interpretability vs. potential gains from nonlinear models
- Failure signatures: High RMSE indicates poor segmentation or predictors; low inter-rater reliability suggests subjective ratings are unstable; model variance across LOSO folds indicates overfitting
- First 3 experiments:
  1. Run FBDS on a clean sample and inspect segment boundaries to verify energy-based detection
  2. Compute predictors on a small set and compare to manual annotations to check correctness
  3. Train MLR on training folds and evaluate RMSE on test folds to establish baseline performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the accuracy of fluency prediction models improve when analyzing spontaneous speech compared to read speech in PWA?
- Basis in paper: The authors discuss that predicting fluency in spontaneous speech is a much more challenging task and note it as a longer-term objective.
- Why unresolved: The study focused exclusively on read speech data, and the authors acknowledge that spontaneous speech involves additional complexities such as grammatical and lexical errors.
- What evidence would resolve it: A study using the same signal-processing algorithms on spontaneous speech data from PWA, comparing prediction accuracy to the current study's results.

### Open Question 2
- Question: How do automatic fluency scores correlate with clinical indicators such as aphasia type, time post-onset, and rehabilitation progress?
- Basis in paper: The authors state that due to limited sample size and clinical profile imbalance, they did not analyze relationships between automatic scores and clinical variables.
- Why unresolved: The study had a small and clinically diverse sample of PWA, preventing robust analysis of these relationships.
- What evidence would resolve it: A larger study with a more balanced sample of PWA across different clinical profiles, correlating automatic fluency scores with clinical assessments and progress measures.

### Open Question 3
- Question: Can the signal-processing algorithms accurately detect and account for word repetitions in spontaneous speech?
- Basis in paper: The authors note that their algorithms do not currently account for repetitions, which are more common in spontaneous speech and affect perceived fluency.
- Why unresolved: The study used read speech data where repetitions were less frequent and the algorithms were not designed to detect them in spontaneous speech.
- What evidence would resolve it: Development and validation of advanced signal-processing algorithms specifically designed to detect repetitions in spontaneous speech, followed by testing their impact on fluency prediction accuracy.

## Limitations
- The FBDS algorithm's performance in aphasia speech is not fully validated despite claims of robustness
- The regression model's high accuracy may be inflated due to small sample size and potential overfitting
- The predictors are derived from read speech only and have not been tested on spontaneous speech, which is more clinically relevant
- Inter-rater reliability among the three SLPs' fluency ratings is not reported

## Confidence

- High confidence: The general approach of using signal-processing features (rate, ratio, pauses, duration variability) to model fluency is well-established in both aphasia and L2 fluency research.
- Medium confidence: The specific implementation of FBDS and its claimed robustness in PWA speech is plausible but not directly validated in this paper.
- Medium confidence: The regression model's high predictive accuracy is supported by LOSO validation, but the small dataset size limits generalizability.

## Next Checks

1. Validate FBDS segmentation accuracy by comparing its output to manually annotated boundaries in a subset of PWA speech samples to confirm energy-based detection reliability.
2. Test the regression model on an independent dataset of PWA speech (e.g., spontaneous speech or different read tasks) to assess generalizability beyond the current corpus.
3. Report and analyze inter-rater reliability among the three SLPs' fluency ratings to quantify the stability of the ground truth used for model training.