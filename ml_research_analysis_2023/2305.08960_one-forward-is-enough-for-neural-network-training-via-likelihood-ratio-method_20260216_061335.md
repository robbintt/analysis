---
ver: rpa2
title: One Forward is Enough for Neural Network Training via Likelihood Ratio Method
arxiv_id: '2305.08960'
source_url: https://arxiv.org/abs/2305.08960
tags:
- neural
- gradient
- networks
- network
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a new likelihood ratio (LR) method for training
  various neural network architectures without backpropagation (BP). The key idea
  is to inject noise into neurons and use the LR technique to estimate gradients based
  on the correlation between noise and network output, enabling parallelized gradient
  estimation.
---

# One Forward is Enough for Neural Network Training via Likelihood Ratio Method
## Quick Facts
- arXiv ID: 2305.08960
- Source URL: https://arxiv.org/abs/2305.08960
- Reference count: 40
- One-line primary result: A new likelihood ratio method for training various neural network architectures without backpropagation achieves comparable performance on clean data and significantly improved robustness against adversarial attacks

## Executive Summary
This paper presents a novel likelihood ratio (LR) method for training neural networks without backpropagation by injecting noise into neurons and using correlation between noise and output to estimate gradients. The method enables parallelized gradient estimation across neurons and is extended to CNNs, RNNs, GNNs, and SNNs. Key variance reduction techniques including layer-wise noise injection, antithetic variables, and quasi-Monte Carlo are proposed to stabilize training. Experiments show the LR method achieves comparable performance to backpropagation on clean data while significantly improving robustness against adversarial attacks.

## Method Summary
The likelihood ratio method trains neural networks by injecting Gaussian noise into neurons and estimating gradients based on the correlation between this noise and the network output. This avoids the need for recursive backward gradient computation. The method uses Monte Carlo integration to estimate gradients, with several variance reduction techniques to improve stability: layer-wise noise injection enables parallel estimation across layers, antithetic variables reduce variance through correlated samples, and quasi-Monte Carlo uses low-discrepancy sequences for better sampling. The approach is extended to various architectures including CNNs, RNNs, GNNs, and SNNs, enabling training of networks with non-differentiable activation functions.

## Key Results
- On CIFAR-10, LR achieves 64.8% accuracy on clean data and 55.3% on FGSM-adversarial data, compared to 64.4% and 12.1% for backpropagation
- The method successfully trains SNNs with non-differentiable activation functions where backpropagation fails
- LR mitigates gradient vanishing problems in RNNs while maintaining competitive performance
- Training remains stable and effective with reduced variance through proposed variance reduction techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The likelihood ratio method estimates gradients without backpropagation by using the correlation between injected noise and network output.
- Mechanism: Noise is injected into neurons, and the gradient is estimated based on how the noise affects the output. This avoids the need for recursive gradient computation.
- Core assumption: The correlation between noise and output is sufficient to provide an unbiased gradient estimate.
- Evidence anchors:
  - [abstract] "The key idea is to inject noise into neurons and use the LR technique to estimate gradients based on the correlation between noise and network output"
  - [section 3.1] "we can push the weight Î¸_l out of the performance function of L(x_L) and into the conditional density as below"
- Break condition: If the noise dimension is too high, the correlation becomes too weak to be identified, leading to poor gradient estimates.

### Mechanism 2
- Claim: Layer-wise noise injection enables parallelized gradient estimation across neurons.
- Mechanism: By injecting noise independently at each layer or neuron, gradients can be estimated in parallel without waiting for backward propagation.
- Core assumption: Each layer's gradient can be estimated independently without affecting the overall gradient computation.
- Evidence anchors:
  - [section 3.3] "LR does not require BP, it is allowed to independently perturb each layer or even each neuron and estimate the corresponding gradient"
  - [section 4.6] "w/o layer-wise noise injection, it is difficult to identify the correlation between the noise in each layer and the loss evaluation"
- Break condition: If layers are highly interdependent, layer-wise noise injection might miss important gradient interactions.

### Mechanism 3
- Claim: Variance reduction techniques improve the stability and efficiency of gradient estimation.
- Mechanism: Techniques like antithetic variables and quasi-Monte Carlo reduce the variance in gradient estimates, making training more stable.
- Core assumption: The integrand in the Monte Carlo integration is smooth enough for quasi-Monte Carlo to be effective.
- Evidence anchors:
  - [section 3.3] "We propose several methods to reduce the variance of gradient estimation, thereby enhancing the stability of the neural network training process"
  - [section 3.3] "Quasi-Monte Carlo (QMC) generates noise and evaluates the integrand using low-discrepancy sequences"
- Break condition: If the integrand is not smooth or the dimension is too high, quasi-Monte Carlo might not provide significant benefits.

## Foundational Learning

- Concept: Monte Carlo Integration
  - Why needed here: The likelihood ratio method relies on Monte Carlo integration to estimate gradients, making understanding this concept crucial.
  - Quick check question: How does Monte Carlo integration estimate the value of an integral, and why is it used in the likelihood ratio method?

- Concept: Variance Reduction Techniques
  - Why needed here: Techniques like antithetic variables and quasi-Monte Carlo are used to improve the stability of gradient estimation.
  - Quick check question: What are antithetic variables, and how do they reduce variance in Monte Carlo integration?

- Concept: Neural Network Architectures (CNN, RNN, GNN, SNN)
  - Why needed here: The paper extends the likelihood ratio method to various neural network architectures, requiring an understanding of their structures and training processes.
  - Quick check question: How do the structures of CNNs, RNNs, GNNs, and SNNs differ, and what challenges do they present for training?

## Architecture Onboarding

- Component map:
  - Noise injection module -> Likelihood ratio computation -> Variance reduction module -> Parallel gradient estimation -> Parameter update

- Critical path:
  1. Forward pass with noise injection
  2. Compute output and loss
  3. Estimate gradients using likelihood ratio method
  4. Apply variance reduction techniques
  5. Update parameters

- Design tradeoffs:
  - Noise dimension vs. gradient accuracy: Higher noise dimensions can lead to weaker correlations and less accurate gradients.
  - Parallelization vs. interdependency: Layer-wise noise injection enables parallelization but might miss important gradient interactions in highly interdependent layers.
  - Variance reduction techniques vs. computational cost: Techniques like quasi-Monte Carlo can improve stability but might increase computational cost.

- Failure signatures:
  - High variance in gradient estimates: Indicates that variance reduction techniques are not effective or the noise dimension is too high.
  - Poor convergence: Suggests that the noise correlation is too weak or the variance reduction techniques are not sufficient.
  - Increased computational cost: Might indicate that quasi-Monte Carlo or other techniques are adding significant overhead.

- First 3 experiments:
  1. Implement noise injection and likelihood ratio computation for a simple MLP on a toy dataset.
  2. Extend the implementation to a CNN on CIFAR-10, comparing performance with backpropagation.
  3. Apply variance reduction techniques and evaluate their impact on gradient estimation stability and training efficiency.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of the LR method scale with network depth compared to backpropagation?
  - Basis in paper: [inferred] The paper mentions that LR avoids recursive gradient computation, but does not provide empirical comparisons of deep networks beyond the tested architectures.
  - Why unresolved: The paper does not systematically evaluate the performance of LR as network depth increases, nor does it compare the computational efficiency or accuracy of LR versus BP for very deep networks.
  - What evidence would resolve it: Experiments showing LR performance on networks with varying depths (e.g., ResNet-10, ResNet-20, etc.) compared to BP, along with computational cost analysis.

- **Open Question 2**: What is the theoretical convergence rate of the LR method compared to backpropagation?
  - Basis in paper: [explicit] The paper mentions that LR provides unbiased gradient estimation but does not provide theoretical analysis of convergence rates or compare them to BP.
  - Why unresolved: The paper focuses on empirical results rather than theoretical guarantees, leaving open questions about the optimization landscape and convergence properties of LR.
  - What evidence would resolve it: Theoretical analysis proving convergence rates for LR under various assumptions, and comparison with known convergence rates for BP.

- **Open Question 3**: How does the LR method perform on tasks requiring fine-grained localization, such as object detection or segmentation?
  - Basis in paper: [inferred] The paper focuses on classification tasks and mentions that LR enables localized module training, but does not explore tasks requiring spatial precision.
  - Why unresolved: The experiments are limited to classification, leaving uncertainty about LR's effectiveness for tasks where precise spatial gradients are crucial.
  - What evidence would resolve it: Empirical results showing LR performance on object detection, semantic segmentation, or instance segmentation benchmarks compared to BP.

- **Open Question 4**: What is the impact of layer-wise noise injection on model interpretability and feature learning?
  - Basis in paper: [explicit] The paper proposes layer-wise noise injection as a variance reduction technique but does not explore its effects on learned representations.
  - Why unresolved: While the paper demonstrates improved robustness, it does not investigate whether layer-wise noise injection affects the quality or interpretability of learned features.
  - What evidence would resolve it: Analysis of feature maps and representations learned with layer-wise noise injection versus standard BP, including visualizations and metrics for feature quality.

## Limitations
- The method's reliance on noise correlation creates fundamental constraints as dimensionality increases, potentially limiting applicability to very deep or wide networks
- Variance reduction techniques add computational overhead that may offset some parallelization benefits
- Limited evaluation to classification tasks, with unclear performance on tasks requiring spatial precision like object detection or segmentation

## Confidence
- Claims about comparable performance to backpropagation: Medium confidence
- Claims about improved adversarial robustness: Medium confidence
- Claims about parallelization benefits: Low confidence

## Next Checks
1. Compare cosine similarity between LR-estimated and backpropagation gradients across different network depths and widths to quantify correlation decay
2. Measure wall-clock training time with and without variance reduction techniques to quantify the parallelization benefits
3. Validate the method on ImageNet or other large-scale datasets to test scalability beyond CIFAR-10 size problems