---
ver: rpa2
title: Exploring Weight Balancing on Long-Tailed Recognition Problem
arxiv_id: '2305.16573'
source_url: https://arxiv.org/abs/2305.16573
tags:
- training
- features
- learning
- arxiv
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides theoretical and experimental analysis of weight
  balancing (WB) for long-tailed recognition. The authors decompose WB into two stages:
  in the first stage, weight decay (WD) and cross-entropy loss increase the Fisher''s
  discriminant ratio (FDR) of the feature extractor by reducing intra-class cosine
  similarities; in the second stage, WD and class-balanced loss perform implicit logit
  adjustment, making the classifier''s norm higher for tail classes.'
---

# Exploring Weight Balancing on Long-Tailed Recognition Problem

## Quick Facts
- arXiv ID: 2305.16573
- Source URL: https://arxiv.org/abs/2305.16573
- Reference count: 40
- One-line primary result: Simplified one-stage weight balancing method achieves higher accuracy than traditional two-stage WB while reducing computational cost

## Executive Summary
This paper analyzes weight balancing (WB) for long-tailed recognition, decomposing it into two training stages. The first stage uses weight decay and cross-entropy loss to increase Fisher's discriminant ratio (FDR) by reducing intra-class cosine similarities. The second stage applies weight decay and class-balanced loss to perform implicit logit adjustment, making classifier norms higher for tail classes. The authors propose a simplified one-stage method using feature regularization and an ETF classifier, followed by post-hoc logit adjustment, which achieves better accuracy with lower computational cost.

## Method Summary
The method involves training with weight decay, feature regularization, and an ETF classifier in a single stage using cross-entropy loss. After training, multiplicative logit adjustment is applied based on class frequencies to correct for class imbalance. The ETF classifier is initialized using the normalized mean features of each class, providing better initialization than random weights. Feature regularization adds the squared norm of features to the loss function, promoting higher feature norms for tail classes.

## Key Results
- Simplified one-stage method outperforms traditional two-stage WB on long-tailed CIFAR and mini-ImageNet datasets
- Feature regularization combined with ETF classifier improves tail class accuracy
- Post-hoc logit adjustment effectively corrects class imbalance without requiring second-stage training
- Weight decay reduces batch normalization scaling parameters, temporarily degrading FDR but ultimately improving it through better feature learning dynamics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weight decay combined with cross-entropy loss degrades intra-class cosine similarities and increases Fisher's discriminant ratio (FDR).
- Mechanism: WD regularizes feature extractor weights, reducing cone effect-induced high cosine similarity between different classes while maintaining high similarity within classes. This increases class separability.
- Core assumption: Neural networks exhibit cone effect where features naturally have high cosine similarity; WD counteracts this.
- Evidence anchors:
  - [abstract]: "WD and CE increase the Fisher's discriminant ratio (FDR) of the feature extractor by reducing intra-class cosine similarities"
  - [section 4.2]: "the methods with WD result in lower cosine similarity between features of different classes and higher cosine similarity between features of the same classes, leading to higher FDR"
- Break condition: If the network architecture prevents WD from affecting feature extractor weights (e.g., skip connections that bypass regularization)

### Mechanism 2
- Claim: WD reduces batch normalization scaling parameters, which temporarily degrades FDR but ultimately improves it through better feature learning dynamics.
- Mechanism: Smaller BN scaling parameters reduce feature norms, promoting more effective weight decay effects and facilitating better linear layer training despite temporary FDR reduction.
- Core assumption: BN scaling parameters directly affect feature magnitude and learning dynamics.
- Evidence anchors:
  - [section 4.3]: "WD reduces the mean of the BN's scaling parameters... smaller scaling parameters have a positive effect on the learning dynamics and improve the FDR"
- Break condition: If BN layers are removed or replaced with layer normalization that doesn't have scaling parameters

### Mechanism 3
- Claim: WD causes feature norms to be higher for tail classes, and the second training stage with class-balanced loss performs implicit logit adjustment making classifier norms higher for tail classes.
- Mechanism: WD creates norm disparity favoring tail classes; class-balanced loss in second stage adjusts classifier weights proportionally, equivalent to multiplicative logit adjustment.
- Core assumption: Feature norms correlate with class sample size under WD regularization.
- Evidence anchors:
  - [abstract]: "WD and class-balanced loss perform implicit logit adjustment, making the classifier's norm higher for tail classes"
  - [section 4.5]: "when WD is applied, the norms of the Many classes features drop significantly... norms of the Few classes features remain higher"
- Break condition: If the number of classes is small (CIFAR10-LT), implicit logit adjustment fails

## Foundational Learning

- Concept: Fisher's Discriminant Ratio (FDR)
  - Why needed here: FDR measures class separability in feature space; higher FDR indicates better linear separability for classification
  - Quick check question: What does FDR measure and why is it important for long-tailed recognition?

- Concept: Neural Collapse and Cone Effect
  - Why needed here: These phenomena describe how features behave during training; understanding them explains why WD affects feature geometry
  - Quick check question: How do neural collapse and cone effect relate to feature geometry in deep networks?

- Concept: Logit Adjustment
  - Why needed here: Logit adjustment techniques correct class imbalance at prediction time; the paper shows second stage implicitly performs this
  - Quick check question: What is the difference between additive and multiplicative logit adjustment?

## Architecture Onboarding

- Component map:
  Feature extractor (ResNet32/MLP/ResBlock) -> Linear classifier (ETF or learned) -> Weight decay regularization -> Batch normalization layers -> Loss functions (CE, class-balanced loss)

- Critical path:
  1. Train with WD + CE to improve FDR and create norm disparity
  2. Either train second stage with WD + class-balanced loss OR apply post-hoc logit adjustment
  3. Evaluate accuracy improvements on tail classes

- Design tradeoffs:
  - WD strength vs overfitting: Too much WD may underfit, too little won't create norm disparity
  - BN vs no BN: BN with WD reduces scaling parameters but may temporarily hurt FDR
  - One-stage vs two-stage: One-stage with ETF classifier and post-hoc LA is simpler but may be less effective

- Failure signatures:
  - No improvement in tail class accuracy despite WD application
  - Increased training accuracy but decreased test accuracy (overfitting)
  - Norm disparity not forming between head and tail classes

- First 3 experiments:
  1. Train baseline without WD, measure FDR and tail class accuracy
  2. Train with WD + CE, measure FDR improvement and norm disparity formation
  3. Apply post-hoc logit adjustment to model from experiment 2, measure tail class accuracy improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the simplified method (WD, FR, ETF classifier, and LA) compare to other state-of-the-art long-tailed recognition methods across different backbone architectures beyond ResNet?
- Basis in paper: [inferred] The authors only tested their simplified method on ResNet architectures and recommend trying it first, but do not provide comparisons with other methods on different backbones.
- Why unresolved: The experiments were limited to ResNet, and the authors acknowledge this as a limitation without exploring other architectures like ViT or EfficientNet.
- What evidence would resolve it: Conducting experiments using the simplified method on various backbone architectures (e.g., ViT, EfficientNet) and comparing results with other state-of-the-art LTR methods.

### Open Question 2
- Question: What is the theoretical explanation for why smaller BN scaling parameters facilitate feature learning and improve FDR?
- Basis in paper: [explicit] The authors observe that WD reduces the mean of BN's scaling parameters and that fixing these parameters to small values improves FDR, but they do not provide a theoretical explanation for this phenomenon.
- Why unresolved: The authors suggest that the effect might be related to increased effective learning rate or reduced feature norm, but they do not provide a rigorous theoretical analysis.
- What evidence would resolve it: Developing a theoretical framework that explains the relationship between BN scaling parameters, feature learning dynamics, and FDR improvement.

### Open Question 3
- Question: Under what conditions does the second stage of WB fail to perform implicit logit adjustment, and how can this be mitigated?
- Basis in paper: [explicit] The authors show that Theorem 2 does not guarantee implicit logit adjustment when the number of classes is small, and they observe this failure in CIFAR10-LT experiments.
- Why unresolved: The authors identify the problem but do not provide a comprehensive analysis of when and why this failure occurs or propose solutions to mitigate it.
- What evidence would resolve it: Conducting extensive experiments to identify the specific conditions (e.g., number of classes, imbalance ratio, dataset characteristics) that lead to this failure and developing methods to address it.

## Limitations
- Limited to ResNet architectures without testing on other backbone networks like ViT or EfficientNet
- Does not provide theoretical explanations for why BN scaling parameter reduction improves FDR
- Simplified method may not work well when the number of classes is small, as shown in CIFAR10-LT experiments

## Confidence
- Mechanism 1: Medium confidence - empirical evidence but limited theoretical support
- Mechanism 2: Medium confidence - observed effect but lacks rigorous theoretical explanation
- Mechanism 3: Medium confidence - experimental validation but assumptions need further verification

## Next Checks
1. Verify the feature regularization implementation details and test sensitivity to hyperparameter choices
2. Conduct ablation studies isolating the contributions of each mechanism (WD, FR, ETF classifier, logit adjustment)
3. Test the simplified one-stage method on additional long-tailed datasets with different imbalance ratios