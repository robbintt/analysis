---
ver: rpa2
title: 'How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The
  Curses of Symmetry and Initialization'
arxiv_id: '2310.01769'
source_url: https://arxiv.org/abs/2310.01769
tags:
- matrix
- have
- convergence
- proof
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a rigorous connection between over-parameterization\
  \ and convergence rate in gradient descent for matrix sensing. It shows that symmetric\
  \ over-parameterization slows convergence from exponential to sublinear (\u03A9\
  (1/T\xB2)), while asymmetric parameterization can restore linear convergence even\
  \ in the over-parameterized regime."
---

# How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization

## Quick Facts
- arXiv ID: 2310.01769
- Source URL: https://arxiv.org/abs/2310.01769
- Reference count: 40
- Primary result: Symmetric over-parameterization slows convergence from exponential to sublinear (Ω(1/T²)), while asymmetric parameterization can restore linear convergence even in over-parameterized regime

## Executive Summary
This paper establishes a rigorous connection between over-parameterization and convergence rate in gradient descent for matrix sensing. The authors show that symmetric over-parameterization introduces redundant dimensions that converge slowly (Ω(1/T²)), dominating the overall convergence rate despite signal space converging linearly. They prove that asymmetric parameterization can maintain imbalance between factor matrices, enabling linear convergence with rate dependent on initialization scale. The paper also proposes a simple one-step transformation that eliminates initialization-dependent rates and recovers optimal linear convergence.

## Method Summary
The paper studies matrix sensing problems using gradient descent with different parameterizations. For symmetric matrix factorization, it analyzes both exact-parameterization (k=r) and over-parameterization (k>r) using XX^⊤ parameterization. For asymmetric matrix sensing, it uses FG^⊤ parameterization with random initialization of F and G at different scales. The method involves running gradient descent on the loss functions L(X) = ½‖XX^⊤ - M*‖²_F (symmetric) and L(F,G) = ½‖FG^⊤ - M*‖²_F (asymmetric), analyzing convergence rates under different settings and proposing a modification to accelerate convergence in the over-parameterized asymmetric case.

## Key Results
- Proves Ω(1/T²) lower bound for symmetric matrix sensing with over-parameterization (k>r)
- Establishes exp(-Ω(α²T)) linear convergence rate for asymmetric over-parameterization with initialization scale α
- Proposes algorithm modification that eliminates α-dependence and recovers optimal linear rate
- Shows imbalance between factor matrices drives faster convergence in asymmetric parameterization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symmetric over-parameterization slows convergence from exponential to sublinear due to redundant dimensions.
- Mechanism: In symmetric matrix factorization, over-parameterization introduces a "null space" (dimensions beyond rank r) that must converge to zero. This convergence is slow (Ω(1/T²)), dominating the overall rate despite the signal space converging linearly.
- Core assumption: The factor matrix X has orthogonal columns in the signal space but non-orthogonal entries in the redundant space.
- Evidence anchors:
  - [abstract] "we give a novel Ω(1/T²) lower bound of randomly initialized GD for the over-parameterized case (k > r) where T is the number of iterations. This is in stark contrast to the exact-parameterization scenario (k = r) where the convergence rate is exp (−Ω (T))."
  - [section 4] "The main intuition of Theorem 6 is that the last n − r rows of Xt, corresponding to the space of 0 eigenvalues of Σ, converge to 0 at speed no faster than 1/T²."
  - [corpus] Missing: No explicit corpus evidence for this mechanism, but related work on "over-parameterization slows convergence" in the corpus supports the general phenomenon.
- Break condition: If the redundant space is initialized to zero or constrained to be orthogonal to the signal space, the slow convergence disappears.

### Mechanism 2
- Claim: Asymmetric parameterization accelerates convergence by maintaining imbalance between factors F and G.
- Mechanism: Asymmetric parameterization allows F and G to have different norms and orientations. This imbalance prevents the redundant space from converging to zero too quickly, maintaining a larger gradient signal and enabling linear convergence.
- Core assumption: The imbalance between F and G (measured by the spectrum of F⊤F - G⊤G) remains bounded away from zero during optimization.
- Evidence anchors:
  - [abstract] "we give the first global exact convergence result for the over-parameterization case ( k > r ) with an exp(−Ω(α²T)) rate where α is the initialization scale. This linear convergence result in the over-parameterization case is especially significant because one can apply the asymmetric parameterization to the symmetric setting to speed up from Ω(1/T²) to linear convergence."
  - [section 5.1] "Since we initialize F0 and G0 with a different scale, this difference makes the norm of K converge to zero at a linear rate while keeping J larger than a constant. However, in the symmetric case, we have at = bt, so they must both converge to zero when the loss goes to zero (as ∥FtG⊤t − Σ∥ ≥ atbt), leading to a sublinear convergence rate."
  - [corpus] Missing: No explicit corpus evidence for this mechanism, but the general concept of "asymmetric parameterization" in matrix sensing is supported.
- Break condition: If F and G are explicitly balanced (e.g., via regularization), the convergence rate reverts to sublinear.

### Mechanism 3
- Claim: A simple one-step transformation can eliminate initialization-dependent convergence rates in asymmetric over-parameterization.
- Mechanism: At a certain iteration, transform F and G to amplify their imbalance. This creates a larger effective learning rate for the redundant space, restoring the optimal linear convergence rate independent of α.
- Core assumption: The transformation can be applied once the iterates are close enough to the optimum that the imbalance can be safely amplified.
- Evidence anchors:
  - [abstract] "we propose a novel method that only modifies one step of GD and obtains a convergence rate independent of α, recovering the rate in the exact-parameterization case."
  - [section 6] "Our key idea is to increase the degree of imbalance when F and G are close to the optimum. We develop a new simple algorithm to accelerate GD. The algorithm only involves transforming the factor matrices F and G in one of iteration to intensify the degree of imbalance (cf. Equation (26))."
  - [corpus] Missing: No explicit corpus evidence for this specific mechanism, but the concept of "preconditioning" in gradient descent is supported.
- Break condition: If the transformation is applied too early (before reaching the neighborhood of the optimum), it may destabilize the optimization.

## Foundational Learning

- Concept: Restricted Isometry Property (RIP)
  - Why needed here: RIP is a key assumption for matrix sensing problems, ensuring that the measurements are informative and that the loss function behaves well.
  - Quick check question: What is the relationship between the RIP constant δ and the number of measurements m required for successful recovery?

- Concept: Matrix Factorization and Low-Rank Approximation
  - Why needed here: The paper studies matrix sensing, which involves recovering a low-rank matrix from measurements. Understanding matrix factorization is crucial for grasping the optimization problem.
  - Quick check question: What is the difference between exact-parameterization (k = r) and over-parameterization (k > r) in matrix factorization?

- Concept: Gradient Descent Dynamics and Convergence Analysis
  - Why needed here: The paper analyzes the convergence rate of gradient descent for matrix sensing problems. Understanding the dynamics of gradient descent is essential for interpreting the results.
  - Quick check question: What is the difference between linear and sublinear convergence rates, and how do they affect the optimization process?

## Architecture Onboarding

- Component map: Matrix Sensing Problem -> Parameterization (Symmetric/Asymmetric) -> Gradient Descent Algorithm -> Convergence Analysis -> Algorithm Modification

- Critical path:
  1. Define the matrix sensing problem and its assumptions (RIP, rank, etc.)
  2. Choose the appropriate parameterization (symmetric or asymmetric)
  3. Initialize the factor matrices (randomly or with a specific scale α)
  4. Run gradient descent to minimize the loss function
  5. Analyze the convergence rate based on the parameterization and initialization

- Design tradeoffs:
  - Symmetric vs. Asymmetric: Symmetric parameterization is simpler but slower for over-parameterization. Asymmetric parameterization is faster but requires careful initialization.
  - Exact vs. Over-Parameterization: Exact-parameterization has faster convergence but requires knowing the true rank. Over-parameterization is more robust but slower.
  - Initialization Scale: Smaller initialization scales lead to slower initial convergence but better final accuracy. Larger scales lead to faster initial convergence but may get stuck in suboptimal solutions.

- Failure signatures:
  - Slow convergence: Indicates over-parameterization or poor initialization
  - Divergence: Indicates too large a step size or poor conditioning of the problem
  - Suboptimal solutions: Indicates getting stuck in local minima or saddle points

- First 3 experiments:
  1. Symmetric matrix factorization with k = r (exact-parameterization) and k > r (over-parameterization). Compare convergence rates.
  2. Asymmetric matrix sensing with different initialization scales α. Verify the relationship between α and convergence speed.
  3. Apply the algorithm modification to asymmetric over-parameterization. Verify the elimination of initialization dependence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Ω(1/T²) lower bound for symmetric matrix factorization be tightened or generalized to other matrix sensing setups?
- Basis in paper: [explicit] Theorem 6 proves Ω(1/T²) lower bound for symmetric matrix sensing with symmetric parameterization in the over-parameterized regime (k > r).
- Why unresolved: The proof relies on specific properties of the symmetric parameterization and may not extend to other scenarios like asymmetric sensing or different loss functions.
- What evidence would resolve it: Extending the proof technique to asymmetric matrix sensing or proving/improving the lower bound for other setups.

### Open Question 2
- Question: What is the fundamental reason for the dependence of the convergence rate on the initialization scale α in the asymmetric over-parameterized case?
- Basis in paper: [explicit] Theorem 8 shows that the convergence rate in the asymmetric over-parameterized case depends on α, unlike the exact-parameterized case.
- Why unresolved: While the paper provides some technical insights, the exact mechanism linking initialization scale to convergence rate is not fully understood.
- What evidence would resolve it: A more rigorous analysis of the gradient descent dynamics that explicitly connects initialization scale to convergence rate.

### Open Question 3
- Question: Can the proposed algorithm modification for asymmetric over-parameterization be extended to other optimization problems beyond matrix sensing?
- Basis in paper: [inferred] The paper proposes a simple modification to gradient descent that eliminates the α-dependence in the convergence rate for asymmetric over-parameterized matrix sensing.
- Why unresolved: The proposed modification is specific to the matrix sensing problem and its applicability to other optimization problems is unknown.
- What evidence would resolve it: Applying the modification to other optimization problems and analyzing its effect on convergence rates.

## Limitations

- The theoretical analysis relies heavily on idealized assumptions like exact RIP and perfect random initialization, which may not hold in practical scenarios
- The proposed algorithm modification for eliminating initialization dependence lacks extensive empirical validation across diverse problem settings
- The asymmetric parameterization approach, while theoretically sound, may be more sensitive to hyperparameter tuning compared to symmetric methods

## Confidence

- **High Confidence**: The Ω(1/T²) lower bound for symmetric over-parameterization and the linear convergence for exact-parameterization (k=r) are well-established and rigorously proven.
- **Medium Confidence**: The exp(-Ω(α²T)) convergence rate for asymmetric over-parameterization is theoretically sound but relies on specific initialization conditions that may not always hold in practice.
- **Medium Confidence**: The algorithm modification for eliminating initialization dependence is conceptually sound but lacks extensive empirical validation across diverse problem settings.

## Next Checks

1. **Experiment Extension**: Test the asymmetric over-parameterization with different initialization strategies beyond the symmetric case to verify robustness of the linear convergence claim.

2. **Algorithm Robustness**: Evaluate the proposed one-step transformation algorithm on ill-conditioned and noisy matrix sensing problems to assess its practical effectiveness beyond the idealized theoretical setting.

3. **Comparative Analysis**: Benchmark the asymmetric approach against existing accelerated methods for matrix sensing (e.g., Nesterov acceleration, adaptive step sizes) to establish its relative performance advantages.