---
ver: rpa2
title: One Explanation Does Not Fit XIL
arxiv_id: '2304.07136'
source_url: https://arxiv.org/abs/2304.07136
tags:
- methods
- explanation
- rrr-g
- hint
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of multiple explanation methods in
  the explanatory interactive machine learning (XIL) framework to improve model revision.
  The authors demonstrate that using a single explanation method during XIL training
  does not generalize well to different explanation methods, resulting in suboptimal
  model revision.
---

# One Explanation Does Not Fit XIL

## Quick Facts
- arXiv ID: 2304.07136
- Source URL: https://arxiv.org/abs/2304.07136
- Reference count: 13
- Primary result: Single-explanation XIL methods don't generalize well; combining multiple explanations improves explanation quality across various explainers while maintaining accuracy.

## Executive Summary
This paper investigates the limitations of using single explanation methods in the Explanatory Interactive Machine Learning (XIL) framework for model revision. The authors demonstrate that models trained with feedback from one explanation method fail to generalize to different explanation methods during evaluation, resulting in suboptimal revision. They propose a novel approach that simultaneously optimizes multiple explanation methods, showing that this combined approach improves explanation quality across various explainers while maintaining comparable accuracy on synthetic datasets.

## Method Summary
The paper proposes a framework for combining multiple explanation methods in XIL by simultaneously optimizing them during model revision. The approach uses a CNN backbone with two convolutional layers followed by two fully-connected layers, optimized with Adam (lr=0.001) for 50 epochs. The combined loss function is L = Lpred + Σ λi Lxil_i, where multiple explanation losses are weighted and added to the standard prediction loss. The method is evaluated on DecoyMNIST and DecoyFMNIST datasets, comparing single-explanation XIL methods against combined-explanation approaches using metrics like accuracy and Wrong Reason (WR) scores.

## Key Results
- Single-explanation XIL methods fail to generalize across different explanation methods, with models still relying on wrong reasons when evaluated with various explainers
- Combining multiple explanation methods reduces WR scores among multiple explainers while maintaining comparable accuracy
- The approach shows particular improvement for non-internally used explainers, suggesting better generalization across different explanation perspectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining multiple explanation methods during XIL training improves model revision effectiveness across diverse explainers.
- Mechanism: When a model is revised using feedback from multiple explanation methods, it learns to align its explanations with user feedback across different explanation perspectives. This simultaneous optimization addresses the inherent limitations of individual explainers, resulting in explanations that are more faithful and less dependent on spurious correlations.
- Core assumption: Different explanation methods capture different aspects of model reasoning, and their combination provides complementary information for model revision.
- Evidence anchors:
  - [abstract]: "we investigate simultaneous model revision through multiple explanation methods" and "propose combining multiple explanation methods through simultaneous optimization"
  - [section]: "Tab. 2 illustrates that leveraging a combination of various explanations into one XIL method reduces WR among multiple explainers while the accuracy remains on par"
  - [corpus]: The paper's own corpus includes work on "Human Cognitive Biases in Explanation-Based Interaction" which suggests that different explanation methods may have different biases or limitations
- Break condition: If explanation methods are highly correlated in what they capture, or if some methods are fundamentally incompatible with the model architecture, the combined approach may not provide additional benefits or could even degrade performance.

### Mechanism 2
- Claim: Single-explanation XIL methods don't generalize well to different explanation methods during evaluation.
- Mechanism: When a model is trained with feedback from one explanation method, it primarily learns to satisfy that specific explainer's criteria. However, this learned behavior doesn't necessarily transfer to other explanation methods, leaving the model vulnerable to spurious correlations when evaluated with different explainers.
- Core assumption: The loss function for each explanation method captures different aspects of model behavior, and optimizing for one doesn't automatically optimize for others.
- Evidence anchors:
  - [abstract]: "using a single explanation method during XIL training does not generalize well to different explanation methods, resulting in suboptimal model revision"
  - [section]: "Tab. 1b demonstrates that the model still relies on wrong reasons when generating explanations with various explainers" and "revising a model with XIL through one explainer does not generalize to (all) different explainers"
  - [corpus]: Weak - no direct corpus evidence for this specific claim about generalization failure
- Break condition: If all explanation methods are fundamentally aligned in what they measure, or if the model architecture naturally produces consistent explanations across methods, this lack of generalization might not be problematic.

### Mechanism 3
- Claim: The combined explanation loss approach maintains accuracy while improving explanation quality across multiple metrics.
- Mechanism: By adding multiple explanation losses to the standard prediction loss, the model is simultaneously optimized for both task performance and explanation quality across different perspectives. This multi-objective optimization ensures that improvements in explanation quality don't come at the cost of prediction accuracy.
- Core assumption: The weighted combination of losses (with appropriate λ values) can effectively balance prediction accuracy with multiple explanation quality metrics.
- Evidence anchors:
  - [abstract]: "the proposed approach outperforms single-explanation XIL methods in terms of explanation quality (WR scores) while maintaining comparable accuracy"
  - [section]: "Tab. 3" shows accuracy remains on par when combining methods
  - [corpus]: Weak - no direct corpus evidence for this specific claim about maintaining accuracy while improving multiple explanation metrics
- Break condition: If the weight parameters λ are poorly chosen, leading to one objective dominating the optimization, or if the objectives are fundamentally in conflict, the approach could fail to maintain both accuracy and explanation quality.

## Foundational Learning

- Concept: Explanation quality metrics (WR score)
  - Why needed here: The paper uses the Wrong Reason (WR) measure to quantify how much a model's explanation relies on spurious correlations or "wrong reasons" rather than the true causal factors.
  - Quick check question: What does a lower WR score indicate about a model's explanation quality?

- Concept: Interactive machine learning with explanations (XIL)
  - Why needed here: The paper builds on the XIL framework where users provide feedback on model explanations to improve model revision, extending it to use multiple explanation methods simultaneously.
  - Quick check question: How does XIL differ from standard supervised learning in terms of the feedback loop?

- Concept: Spurious correlations and shortcut learning
  - Why needed here: The paper addresses the problem of models learning to rely on spurious correlations (like decoy squares in DecoyMNIST) rather than true causal features, which is central to understanding why multiple explanations are needed.
  - Quick check question: Why are spurious correlations problematic for model generalization and trustworthiness?

## Architecture Onboarding

- Component map:
  Input data -> CNN backbone (conv layers + FC layers) -> Multiple explanation modules -> Combined loss function -> Optimizer

- Critical path:
  1. Forward pass through CNN to get predictions
  2. Generate explanations using multiple methods
  3. Compute combined loss (prediction + weighted explanation losses)
  4. Backpropagate and update model weights
  5. Evaluate on test set with multiple explainers

- Design tradeoffs:
  - Computational cost increases linearly with the number of explanation methods
  - Hyperparameter tuning complexity increases with more λ weights to optimize
  - Potential for conflicting gradients when combining different explanation losses

- Failure signatures:
  - Accuracy drops significantly when adding multiple explanation losses
  - WR scores don't improve for non-internally used explainers
  - Training instability or convergence issues due to conflicting gradients

- First 3 experiments:
  1. Implement single-explanation XIL with one method (e.g., RRR with IG) to establish baseline
  2. Add a second explanation method to the loss function and compare WR scores across all explainers
  3. Test different weight combinations (λ values) for the explanation losses to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal weighting strategy (λi) for combining multiple explanation methods in XIL?
- Basis in paper: [explicit] The paper notes that "another exciting avenue for future work entails further investigating λi to trade off the influence of each explainer."
- Why unresolved: The paper only mentions this as a future direction without providing specific experiments or analysis of different weighting strategies.
- What evidence would resolve it: Experiments comparing different weighting strategies (equal weights, learned weights, adaptive weights) across multiple datasets and showing their impact on both explanation quality (WR scores) and accuracy.

### Open Question 2
- Question: How do different combinations of XIL methods with different feedback strategies (penalty vs reward) compare in terms of explanation quality and model accuracy?
- Basis in paper: [explicit] The paper combines penalty and reward strategies in Table 7, showing some combinations improve non-internally used scores.
- Why unresolved: The paper only shows results for specific combinations and doesn't systematically compare all possible combinations or analyze why certain combinations work better than others.
- What evidence would resolve it: A comprehensive comparison of all possible penalty/reward strategy combinations across multiple datasets, with analysis of when each type is most effective.

### Open Question 3
- Question: What is the computational trade-off between using single-explanation XIL methods versus multiple-explanation XIL methods?
- Basis in paper: [explicit] The paper mentions "the increase in computational cost must be kept in mind" but doesn't provide quantitative analysis.
- Why unresolved: No empirical comparison of training/inference times or resource requirements between single and multi-explanation approaches.
- What evidence would resolve it: Detailed timing analysis and resource usage comparison between single and multi-explanation XIL methods across different model architectures and dataset sizes.

## Limitations
- The approach is only validated on synthetic datasets (DecoyMNIST and DecoyFMNIST) with a single CNN architecture
- The choice of λ weights for combining explanation losses appears crucial but is not thoroughly explored
- The computational overhead of using multiple explanation methods simultaneously could be prohibitive for larger models or datasets

## Confidence
- High confidence in the core observation that single-explanation XIL methods don't generalize well across different explainers
- Medium confidence in the proposed solution of combining multiple explanations, as it's only validated on synthetic datasets
- Low confidence in the practical scalability and general applicability of the approach beyond the tested scenarios

## Next Checks
1. Test the combined explanation approach on real-world datasets with naturally occurring spurious correlations (e.g., healthcare imaging data with acquisition artifacts) to verify generalization beyond synthetic confounds.

2. Evaluate the computational overhead and scalability by implementing the approach on larger CNN architectures (e.g., ResNet) or transformer-based models to assess practical viability.

3. Conduct ablation studies with different λ weight configurations and varying numbers of explanation methods to determine the optimal balance between computational cost and explanation quality improvements.