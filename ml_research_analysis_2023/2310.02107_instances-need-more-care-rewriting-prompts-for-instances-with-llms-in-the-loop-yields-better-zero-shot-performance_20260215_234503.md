---
ver: rpa2
title: 'Instances Need More Care: Rewriting Prompts for Instances with LLMs in the
  Loop Yields Better Zero-Shot Performance'
arxiv_id: '2310.02107'
source_url: https://arxiv.org/abs/2310.02107
tags:
- prompt
- task
- answer
- prompts
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) have significantly advanced zero-shot
  task performance, but their effectiveness can be limited by generic prompts. This
  paper introduces PRomPTed, a method that optimizes prompts for individual task instances
  using LLMs in a loop.
---

# Instances Need More Care: Rewriting Prompts for Instances with LLMs in the Loop Yields Better Zero-Shot Performance

## Quick Facts
- **arXiv ID:** 2310.02107
- **Source URL:** https://arxiv.org/abs/2310.02107
- **Reference count:** 17
- **Primary result:** Instance-level prompt rewriting with LLMs in a loop significantly improves zero-shot performance across 13 datasets and 10 task types.

## Executive Summary
This paper introduces PRomPTed, a method that uses LLMs to rewrite prompts for individual task instances, significantly improving zero-shot performance. The approach leverages few-shot in-context learning with contrastive pairs of good and bad prompts to make prompts more specific, unambiguous, and complete. Experiments using GPT-4 demonstrate substantial improvements over naive zero-shot approaches and strong baselines like Output Refinement. Notably, the method shows that using GPT-3.5 to rewrite prompts for GPT-4 can be as effective as using GPT-4 itself, and the approach generalizes to open-source LLMs like Mistral 7B and Mixtral 8x7B.

## Method Summary
The PRomPTed approach uses an LLM (Mrewrite) to rewrite original prompts (ρ) into improved prompts (ρ*) through few-shot in-context learning with contrastive prompt pairs. The rewriting process incorporates task type information and reasons for why prompts are considered bad. The rewritten prompts are then used by a task LLM (Mtask) for zero-shot performance. The method demonstrates that instance-level prompt optimization can significantly enhance LLM performance while also enabling weaker LLMs to effectively supervise stronger ones.

## Key Results
- PRomPTed significantly outperforms both naive zero-shot approaches and the strong Output Refinement baseline
- Using GPT-3.5 to rewrite prompts for GPT-4 matches or occasionally exceeds using GPT-4 for both rewriting and task performance
- The approach generalizes to open-source LLMs including Mistral 7B and Mixtral 8x7B
- PRomPTed shows strong performance across 13 datasets spanning 10 different task types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instance-level prompt rewriting improves zero-shot performance by providing task-specific hints that are more specific, unambiguous, and complete than generic task instructions.
- Mechanism: The PRomPTed approach uses an LLM to rewrite the original prompt into a better prompt by leveraging few-shot in-context learning with contrastive pairs of good and bad prompts.
- Core assumption: LLMs can learn to identify and correct the shortcomings of prompts through exposure to contrastive examples.
- Evidence anchors: [abstract] and [section] provide support for this mechanism.
- Break condition: The mechanism would break if the rewriting LLM is too weak to effectively identify and correct prompt shortcomings.

### Mechanism 2
- Claim: Providing a reason for the rewriting and indicating the task type during the prompt rewriting process improves the quality of the rewritten prompts.
- Mechanism: The PRomPTed approach prompts the LLM to provide a reason for why a prompt is considered bad and to indicate the task type during the few-shot in-context learning process.
- Core assumption: LLMs can benefit from explicit reasoning about their own thought process, and that task type information helps constrain the search space for prompt rewriting.
- Evidence anchors: [section] provides support for this mechanism.
- Break condition: The mechanism would break if the LLM does not benefit from explicit reasoning about its thought process.

### Mechanism 3
- Claim: The PRomPTed approach can generalize to unseen task types by learning the underlying principles of good prompt rewriting from the contrastive examples.
- Mechanism: The PRomPTed approach uses demonstrations that cover multiple task types and can still improve performance on completely unseen task types.
- Core assumption: The principles of good prompt rewriting are transferable across different task types.
- Evidence anchors: [abstract] and [section] provide support for this mechanism.
- Break condition: The mechanism would break if the principles of good prompt rewriting are not transferable across task types.

## Foundational Learning

- Concept: Few-shot in-context learning
  - Why needed here: The PRomPTed approach uses few-shot in-context learning to teach the LLM how to rewrite prompts based on a set of demonstrations.
  - Quick check question: What is the difference between few-shot in-context learning and traditional fine-tuning, and why is in-context learning preferred in this case?

- Concept: Chain-of-thought (CoT) reasoning
  - Why needed here: The PRomPTed approach can generate prompts that encourage CoT reasoning, which has been shown to improve zero-shot performance on complex reasoning tasks.
  - Quick check question: How does CoT reasoning work, and why is it particularly useful for tasks like mathematical and logical reasoning?

- Concept: Prompt engineering
  - Why needed here: The PRomPTed approach is a form of prompt engineering that aims to optimize the prompts used for zero-shot task performance.
  - Quick check question: What are the key principles of good prompt engineering, and how does the PRomPTed approach incorporate these principles?

## Architecture Onboarding

- Component map:
  - Mtask (task LLM) <- ρ* (rewritten prompt) <- Mrewrite (rewriting LLM) <- ρ (original prompt) + PR OMT (demonstrations)

- Critical path:
  1. The original prompt (ρ) is passed to the rewriting LLM (Mrewrite)
  2. Mrewrite uses few-shot in-context learning with the PR OMT demonstrations to rewrite ρ into a better prompt (ρ*)
  3. The rewritten prompt (ρ*) is passed to the task LLM (Mtask) for zero-shot task performance
  4. The output of Mtask is the final result of the zero-shot task

- Design tradeoffs:
  - Using a stronger LLM for rewriting may lead to better rewritten prompts but increases computational cost
  - Using a weaker LLM for rewriting may be more cost-effective but may not generate as high-quality rewritten prompts
  - The choice of demonstrations can affect the generalization ability to unseen task types

- Failure signatures:
  - If rewritten prompts are too long or contain hallucinations, it may indicate the rewriting LLM is not effectively identifying and correcting shortcomings
  - If zero-shot task performance does not improve with rewritten prompts, it may indicate the rewriting process is not effectively customizing prompts for each instance

- First 3 experiments:
  1. Evaluate zero-shot task performance of original prompts without any rewriting
  2. Evaluate zero-shot task performance of rewritten prompts using the PRomPTed approach
  3. Compare the zero-shot task performance of original prompts and rewritten prompts to assess effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PRomPTed's performance scale with increasingly complex and domain-specific tasks that are not covered in its 10 demonstrations?
- Basis in paper: Inferred from the paper's discussion of PRomPTed's generalization to unseen task types like sentiment classification.
- Why unresolved: The paper only tests PRomPTed on a limited set of tasks and does not explore its performance on highly complex or domain-specific tasks.
- What evidence would resolve it: Experiments testing PRomPTed on a diverse range of complex tasks, including those requiring specialized domain knowledge.

### Open Question 2
- Question: What are the specific mechanisms by which PRomPTed's prompt rewriting process improves LLM performance, and how can these mechanisms be further optimized?
- Basis in paper: Explicit from the paper's discussion of the four criteria for a good prompt (specificity, non-ambiguity, completeness, and structuredness).
- Why unresolved: While the paper demonstrates that PRomPTed improves LLM performance, it does not provide a detailed analysis of the specific mechanisms.
- What evidence would resolve it: A detailed analysis of the prompt rewriting process and experiments testing different prompt rewriting strategies.

### Open Question 3
- Question: How does the choice of the rewriting LLM affect PRomPTed's performance, and can weaker LLMs be effectively used for prompt rewriting?
- Basis in paper: Explicit from the paper's discussion of using GPT-3.5 to rewrite prompts for GPT-4.
- Why unresolved: The paper only tests PRomPTed with GPT-4 and GPT-3.5 as the rewriting LLM.
- What evidence would resolve it: Experiments testing PRomPTed with different LLMs as the rewriting LLM, including weaker models.

## Limitations
- Heavy reliance on GPT-4 for both prompt rewriting and evaluation may limit generalizability to other model families
- The study compares results against relatively simple baselines rather than more sophisticated prompt engineering approaches
- The long-term effectiveness of the approach as LLM capabilities evolve remains uncertain

## Confidence

- **High**: The core finding that instance-level prompt rewriting improves zero-shot performance is well-supported by experimental results across multiple datasets and task types.
- **Medium**: The claim that GPT-3.5 can effectively rewrite prompts for GPT-4 is supported but requires further validation with additional weaker-stronger LLM pairs.
- **Medium**: The generalization capability to unseen task types is demonstrated but could benefit from testing with more diverse task categories.

## Next Checks
1. Test the approach with a broader range of weaker-stronger LLM combinations (e.g., Claude 2.1 rewriting for GPT-4) to verify the generality of the "weak supervisor" capability
2. Implement A/B testing with human evaluators to assess whether rewritten prompts maintain semantic fidelity to original instructions
3. Measure the latency and cost implications of the two-stage prompting approach compared to single-pass prompting strategies