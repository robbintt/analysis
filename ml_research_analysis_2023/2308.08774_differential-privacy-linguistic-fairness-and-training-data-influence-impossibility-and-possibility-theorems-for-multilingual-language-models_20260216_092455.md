---
ver: rpa2
title: 'Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility
  and Possibility Theorems for Multilingual Language Models'
arxiv_id: '2308.08774'
source_url: https://arxiv.org/abs/2308.08774
tags:
- privacy
- https
- training
- data
- influence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the compatibility of differential privacy,
  linguistic fairness, and training data influence sparsity in multilingual language
  models. It shows that differential privacy and training data influence sparsity
  are fundamentally at odds, while differential privacy and linguistic fairness can
  be compatible through multilingual compression.
---

# Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models

## Quick Facts
- arXiv ID: 2308.08774
- Source URL: https://arxiv.org/abs/2308.08774
- Authors: 
- Reference count: 40
- Key outcome: DP and training data influence sparsity are fundamentally incompatible, while DP and linguistic fairness can be compatible through multilingual compression

## Executive Summary
This paper investigates the compatibility of three objectives in multilingual language models: differential privacy, linguistic fairness across languages, and training data influence sparsity. Through theoretical analysis and empirical experiments on POS tagging and NLI tasks, the authors show that differential privacy and training data influence sparsity are fundamentally at odds, while differential privacy and linguistic fairness can be compatible through multilingual compression. The work introduces a new metric, influence uniformity, to quantify training data influence across languages and demonstrates the complex tradeoffs between privacy, performance, and compression in multilingual models.

## Method Summary
The authors fine-tune XLM-R Base with DP-AdamW optimizer using Opacus privacy engine across six privacy budgets (ε = 1, 3, 8, 15, 30, ∞) on two multilingual tasks: POS tagging (UD v2.8) and NLI (XNLI). They evaluate cross-lingual transfer performance and measure four multilingual compression metrics (sentence retrieval precision, CKA, IsoScore, RSA) alongside influence uniformity scores. The experiments use batch sizes of 96 for POS and 512 for XNLI, with learning rates tuned per privacy level and δ values set at 1e-4/|D| for POS and 1e-6 for XNLI.

## Key Results
- Strong privacy (ε = 1) achieves high compression but low task performance
- Non-private fine-tuning (ε = ∞) achieves high compression and performance
- Medium privacy levels (e.g., ε = 8) balance privacy and performance at the expense of compression
- Influence uniformity positively correlates with multilingual compression
- Non-private fine-tuning achieves the highest multilingual compression across all metrics

## Why This Works (Mechanism)

### Mechanism 1
Multilingual compression can achieve both linguistic fairness and differential privacy simultaneously through parameter sharing across languages. When a model compresses representations across languages, semantically equivalent sentences from different languages map to identical representations, creating k-anonymity (with k equal to the number of languages) in the representation space, which by construction provides differential privacy guarantees without explicit noise addition. The core assumption is that the multilingual compression is "perfect" or near-perfect, meaning translation-equivalent sentences produce identical representations across all layers.

### Mechanism 2
Differential privacy and training data influence sparsity are fundamentally incompatible objectives. As a model becomes more differentially private (adding more noise to gradients during training), the influence of individual training examples on predictions is smoothed out. Conversely, achieving sparse training data influence requires the model to strongly rely on specific examples, which contradicts the noise injection required for DP. The core assumption is that the influence sparsity metric accurately captures how much individual training examples affect predictions.

### Mechanism 3
There is a non-linear interaction between privacy level, multilingual compression, and task performance. At extreme privacy levels (very low ε), the model achieves strong compression and privacy but poor performance due to excessive noise. At low privacy levels (high ε), the model achieves good performance and compression but weak privacy. Medium privacy levels create a tradeoff where performance and privacy are balanced at the expense of compression. The core assumption is that the non-linear relationship observed empirically holds across different tasks and model architectures.

## Foundational Learning

- **Concept: Differential Privacy (DP)**
  - Why needed here: The paper's core theoretical and empirical contributions rely on understanding how DP affects multilingual models' ability to achieve fairness, compression, and interpretability
  - Quick check question: What is the key difference between ε-DP and (ε, δ)-DP in terms of privacy guarantees?

- **Concept: Multilingual Compression**
  - Why needed here: The paper investigates how well multilingual models compress information across languages and how this compression interacts with privacy and fairness objectives
  - Quick check question: How does perfect multilingual compression lead to k-anonymity in the representation space?

- **Concept: Training Data Influence and Interpretability**
  - Why needed here: The paper introduces the concept of training data influence sparsity as an interpretability objective and investigates its compatibility with privacy
  - Quick check question: What is the difference between feature attribution methods and instance-based interpretability methods?

## Architecture Onboarding

- **Component map**: XLM-R Base model (12-layer encoder-only transformer) -> DP-AdamW optimizer with Opacus privacy engine -> Four multilingual compression metrics (sentence retrieval, CKA, IsoScore, RSA) -> Influence uniformity metric (InfU) -> Two NLP tasks (POS tagging and NLI) with zero-shot cross-lingual transfer

- **Critical path**: 1. Initialize XLM-R Base model; 2. Configure DP-AdamW optimizer with appropriate privacy budget (ε, δ); 3. Fine-tune on training languages with gradient noise injection; 4. Evaluate zero-shot cross-lingual transfer performance; 5. Compute multilingual compression metrics; 6. Calculate training data influence sparsity; 7. Analyze trade-offs between objectives

- **Design tradeoffs**: Batch size vs. privacy budget (larger batches provide better privacy guarantees but may hurt performance on small datasets); Number of training epochs vs. privacy (more epochs with DP may lead to underfitting due to noise accumulation); Metric selection (different compression metrics capture different aspects of multilingual generalization)

- **Failure signatures**: Performance collapses completely with strong privacy (ε = 1); Multilingual compression fails to improve over pretrained model; Influence uniformity metric doesn't correlate with expected trends; RSA scores show unexpected patterns inconsistent with other metrics

- **First 3 experiments**: 1. Fine-tune XLM-R on POS tagging with ε = 1, 3, 8, 15, 30, ∞ and compare performance vs. privacy budget; 2. Measure sentence retrieval precision across all language pairs for each privacy level to quantify multilingual compression; 3. Compute influence uniformity scores and correlate with sentence retrieval precision to validate the negative relationship between compression and influence sparsity

## Open Questions the Paper Calls Out

### Open Question 1
How can we practically optimize for performance, privacy, and multilingual compression simultaneously? While the paper shows these objectives can theoretically align, it doesn't provide a concrete optimization strategy or framework to balance them in practice. A practical algorithm or training methodology that demonstrates improved balance across all three metrics compared to current approaches would resolve this question.

### Open Question 2
What is the relationship between RSA and other compression metrics, and why does RSA behave differently under high privacy constraints? The paper notes that RSA exhibits unexpected behavior for highly private models, showing low scores where other metrics show high compression, but doesn't provide a theoretical explanation for this sensitivity or how RSA relates to other metrics.

### Open Question 3
How do differential privacy constraints affect cross-lingual transfer for generative tasks beyond classification? The experimental scope is limited to classification tasks (POS tagging and NLI), leaving the question of how privacy affects generation capabilities unexplored.

## Limitations
- The empirical validation of the incompatibility between DP and influence sparsity relies heavily on the validity of the influence uniformity metric, whose sensitivity to hyperparameters could affect results
- Experiments are conducted on only two NLP tasks using a single model architecture (XLM-R Base), limiting generalizability to other tasks and architectures
- The theoretical compatibility claim between DP and linguistic fairness assumes "perfect" compression, which is challenging to achieve in practice

## Confidence
- **High Confidence**: The empirical observation that strong differential privacy (ε = 1) degrades task performance while achieving high compression, consistently shown across both POS tagging and NLI tasks
- **Medium Confidence**: The theoretical claims about the fundamental incompatibility between differential privacy and training data influence sparsity, depending on the accuracy of influence measurement methods
- **Medium Confidence**: The positive correlation between influence uniformity and multilingual compression, with empirical evidence but needing more thorough exploration of causal mechanisms

## Next Checks
1. **Metric Robustness Validation**: Conduct ablation studies on the influence uniformity metric by varying the number of checkpoints used in TracInCP computation and the distance metric (cosine vs. L2). Compare results with alternative influence estimation methods to ensure the observed incompatibility between DP and influence sparsity is robust to metric choices.

2. **Architecture and Task Generalization**: Replicate the key experiments on different model architectures (e.g., mBERT, mT5) and tasks (e.g., named entity recognition, question answering). This would test whether the observed tradeoffs between privacy, compression, and performance are fundamental properties of multilingual models or specific to the experimental setup.

3. **Dataset Size Sensitivity Analysis**: Systematically vary the size of the training datasets while keeping the privacy budget constant. This would reveal whether the privacy-performance-compression tradeoffs change with dataset size, which has important practical implications for applying DP to low-resource languages.