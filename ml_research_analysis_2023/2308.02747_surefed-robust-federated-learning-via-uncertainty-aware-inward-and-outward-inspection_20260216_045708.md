---
ver: rpa2
title: 'SureFED: Robust Federated Learning via Uncertainty-Aware Inward and Outward
  Inspection'
arxiv_id: '2308.02747'
source_url: https://arxiv.org/abs/2308.02747
tags:
- learning
- sabre
- clients
- attacks
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SureFED introduces a novel Bayesian federated learning framework
  that leverages local models and uncertainty quantification to defend against poisoning
  attacks. The method trains local models exclusively on each client's data and uses
  variational Bayesian models to provide uncertainty estimates.
---

# SureFED: Robust Federated Learning via Uncertainty-Aware Inward and Outward Inspection

## Quick Facts
- arXiv ID: 2308.02747
- Source URL: https://arxiv.org/abs/2308.02747
- Authors: 
- Reference count: 40
- Primary result: Bayesian federated learning framework that defends against poisoning attacks using local models and uncertainty quantification, achieving ~95% accuracy under various attacks

## Executive Summary
SureFED introduces a novel Bayesian federated learning framework that leverages local models and uncertainty quantification to defend against poisoning attacks. The method trains local models exclusively on each client's data and uses variational Bayesian models to provide uncertainty estimates. During aggregation, clients only combine updates from neighbors whose models are within a confidence bound determined by their local model's uncertainty. This approach effectively detects and isolates poisoned updates without requiring majority benign clients or knowledge of attack scale. Theoretical analysis proves robustness against data and model poisoning attacks in decentralized linear regression settings.

## Method Summary
SureFED employs a peer-to-peer federated learning architecture where each client maintains two models: a local model trained only on its own data, and a social model updated through aggregation from trusted neighbors. The key innovation is using uncertainty-aware bounded confidence weights - clients only aggregate updates from neighbors whose social models are within a threshold determined by their local model's uncertainty. If a social model drifts too far from the local reference, it gets overwritten. This mechanism provides robustness against poisoning attacks while maintaining learning capability in benign settings.

## Key Results
- Achieves approximately 95% test accuracy across all attack types (Trojan, Label-Flipping, Bit-Flip, General Random, and A Little is Enough)
- Outperforms state-of-the-art defenses including Zeno, Trimmed Mean, and Clipping in both attack resistance and benign performance
- Provides theoretical guarantees of robustness against data and model poisoning attacks in linear regression settings
- Maintains effectiveness without requiring majority benign clients or knowledge of attack scale

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local models trained only on clean local data act as trusted ground truth references.
- Mechanism: Each client maintains two models—a local model trained exclusively on its own data, and a social model updated through aggregation. The local model is immune to external poisoning because it is never updated with data or gradients from other clients.
- Core assumption: Local datasets contain sufficient signal to train a useful reference model, even if small.
- Evidence anchors:
  - [abstract] "trains local models exclusively on each client's data"
  - [section 4] "The local model is trained only using the local dataset"
  - [corpus] No direct corpus support found; this is inferred from paper text.
- Break condition: If local data is too small or uninformative, the reference model may not be reliable for comparison.

### Mechanism 2
- Claim: Bounded confidence trust weights use local uncertainty estimates to filter poisoned updates.
- Mechanism: Trust weights are set to zero for any neighbor whose social model deviates from the local model by more than a threshold determined by the local model's uncertainty (standard deviation). This prevents aggregation of poisoned updates that would cause large deviations.
- Core assumption: Variational Bayesian models provide reliable uncertainty estimates.
- Evidence anchors:
  - [section 4] "trust weights...such that each client only aggregates the opinion of those with similar opinion to it, where two models are deemed similar if their element-wise distance is less than a confidence bound"
  - [section 4] "confidence bound is determined by the uncertainty of the client over its own model"
  - [corpus] No direct corpus support; mechanism described only in paper.
- Break condition: If uncertainty estimates are too conservative or too liberal, benign updates may be rejected or poisoned updates accepted.

### Mechanism 3
- Claim: Local models enforce consistency between social and local beliefs, overwriting poisoned social updates.
- Mechanism: If the social model drifts too far from the local model (beyond the confidence bound), the client overwrites its social model with its local model, preventing propagation of poisoning effects.
- Core assumption: Local models remain clean throughout training.
- Evidence anchors:
  - [section 4] "client needs to make sure that its social model is clean by checking if it is very far from its local model"
  - [section 4] "If a hypothetical client with the social belief of client i would not be in the bounded confidence set of client i, then it will overwrite its social belief with its local belief"
  - [corpus] No direct corpus support; mechanism described only in paper.
- Break condition: If local models become corrupted (e.g., through strong local poisoning), this mechanism fails.

## Foundational Learning

- Concept: Variational Bayesian learning with Gaussian posteriors
  - Why needed here: Provides both model parameters and uncertainty estimates needed for bounded confidence filtering
  - Quick check question: How does the variational free energy objective in equation (1) lead to both mean and variance estimates for the model?

- Concept: Decentralized peer-to-peer communication without central server
  - Why needed here: The threat model assumes no central authority to coordinate defense, requiring client-side evaluation
  - Quick check question: In equation (5), how does the aggregation rule change when moving from centralized to peer-to-peer federated learning?

- Concept: Linear regression as a tractable case for theoretical analysis
  - Why needed here: Allows formal proof of robustness under poisoning attacks through Kalman filter updates
  - Quick check question: In equations (13)-(14), how do the belief updates simplify to Kalman filter form for linear regression?

## Architecture Onboarding

- Component map: Local model trainer → Local uncertainty estimator → Trust weight calculator → Social model aggregator → Communication layer
- Critical path: Local training → Uncertainty estimation → Trust weight computation → Social aggregation → Model dissemination
- Design tradeoffs: More conservative κ values increase robustness but may reduce aggregation and slow convergence; less conservative values do the opposite
- Failure signatures: High local vs social model divergence indicates potential poisoning; persistent low accuracy despite training suggests local model contamination
- First 3 experiments:
  1. Run SABRE with no poisoning on MNIST/FEMNIST to verify baseline accuracy matches or exceeds BayP2PFL
  2. Introduce Label-Flipping attack with 10% compromised clients and measure test accuracy retention
  3. Vary κ parameter systematically to find optimal balance between robustness and convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SABRE's performance scale with increasing model size in the non-linear regression case, beyond the linear setting analyzed theoretically?
- Basis in paper: [inferred] The theoretical analysis focuses on linear regression, while experiments use image classification with deep neural networks. The paper mentions model size affects robustness probability but doesn't empirically validate this relationship for non-linear models.
- Why unresolved: The theoretical proof of robustness in linear regression doesn't automatically extend to complex non-linear models, and the experiments don't systematically vary model size to test the theoretical scaling relationship.
- What evidence would resolve it: Controlled experiments varying neural network depth/width while measuring robustness under different attack types, particularly showing how the theoretical scaling with model size manifests in practice.

### Open Question 2
- Question: What is the precise relationship between the hyperparameter κ and the convergence rate of SABRE under different data distributions and attack scenarios?
- Basis in paper: [explicit] The paper states κ determines aggregation strictness and can be tuned for robustness and performance, but doesn't provide theoretical guidance on selecting κ or how it affects convergence speed.
- Why unresolved: The paper shows SABRE works well with κ=2 in experiments but doesn't analyze how different κ values affect the trade-off between robustness and learning speed, or provide theoretical bounds on convergence rates as a function of κ.
- What evidence would resolve it: Theoretical analysis deriving convergence rates as functions of κ, and empirical studies showing performance across different κ values under various attack strengths and data heterogeneity levels.

### Open Question 3
- Question: How does SABRE's performance degrade when the Relaxed Connectivity Assumption is only approximately satisfied, with intermittent network partitions or delays?
- Basis in paper: [inferred] The theoretical analysis assumes the Relaxed Connectivity Constraint holds, but real-world networks experience partitions and delays that only approximately satisfy this condition.
- Why unresolved: The paper doesn't analyze the robustness of SABRE's theoretical guarantees when the connectivity assumption is violated, or quantify performance degradation under realistic network conditions.
- What evidence would resolve it: Empirical studies measuring SABRE's performance under controlled network partitions and delays, and theoretical analysis of convergence rates when the connectivity constraint is only approximately satisfied.

### Open Question 4
- Question: How does SABRE perform in asynchronous settings where clients have significantly different local datasets sizes and update frequencies?
- Basis in paper: [explicit] The paper mentions asynchronous updates but the theoretical analysis assumes synchronized updates for simplicity, and experiments don't explore scenarios with heterogeneous client participation rates.
- Why unresolved: The theoretical analysis simplifies to a synchronous setting, and experiments use uniform client participation, leaving unclear how SABRE handles the practical case of heterogeneous client availability and dataset sizes.
- What evidence would resolve it: Experiments varying client participation rates and dataset sizes, and theoretical analysis extending the convergence proof to fully asynchronous settings with heterogeneous client characteristics.

## Limitations

- Theoretical guarantees are limited to linear regression, with nonlinear neural network extensions relying on assumptions that require further validation
- The paper doesn't explore edge cases with extremely high attack rates (>50% compromised clients) to identify theoretical limits
- Peer-to-peer communication topology and neighbor selection mechanisms are underspecified, which could impact bounded confidence effectiveness

## Confidence

- **High confidence**: The core mechanism of using local models as trusted references for filtering poisoned updates is well-supported by the experimental results showing consistent accuracy retention across attack types
- **Medium confidence**: The theoretical analysis for linear regression provides a solid foundation, but the extension to nonlinear neural networks involves assumptions that require further validation
- **Medium confidence**: The claim of not requiring majority benign clients is supported by experiments, but the paper doesn't explore edge cases with extremely high attack rates (>50% compromised clients)

## Next Checks

1. Test SABRE's performance when >50% of clients are compromised to identify the theoretical limits of the bounded confidence approach
2. Conduct ablation studies removing the local model overwrite mechanism to quantify its contribution to robustness
3. Evaluate uncertainty estimate reliability by comparing predicted uncertainty with actual prediction error on poisoned vs clean updates