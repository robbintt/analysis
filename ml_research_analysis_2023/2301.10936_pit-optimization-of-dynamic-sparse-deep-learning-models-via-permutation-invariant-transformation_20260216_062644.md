---
ver: rpa2
title: 'PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant
  Transformation'
arxiv_id: '2301.10936'
source_url: https://arxiv.org/abs/2301.10936
tags:
- sparsity
- uni00000013
- sparse
- computation
- dynamic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently executing deep
  learning models with dynamic sparsity on commodity accelerators. It proposes PIT
  (Permutation Invariant Transformation), a deep learning compiler that leverages
  a mathematically proven property called Permutation Invariant Transformation (PIT).
---

# PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation

## Quick Facts
- arXiv ID: 2301.10936
- Source URL: https://arxiv.org/abs/2301.10936
- Reference count: 40
- Key outcome: Accelerates dynamic sparse computations by up to 5.9x (average 2.43x) over state-of-the-art compilers

## Executive Summary
PIT (Permutation Invariant Transformation) is a deep learning compiler that enables efficient execution of dynamic sparse models on commodity GPUs. It leverages a mathematically proven property called Permutation Invariant Transformation to convert sparsely located micro-tiles into GPU-efficient dense tiles without changing computation results. This approach achieves both high GPU utilization and low coverage waste by finding feasible PIT rules for all operators and generating efficient GPU kernels accordingly.

## Method Summary
PIT addresses the challenge of executing deep learning models with dynamic sparsity on commodity accelerators by introducing a novel tiling mechanism based on Permutation Invariant Transformation (PIT). The method transforms sparsely located micro-tiles into dense tiles without altering computation results, allowing for the use of highly optimized dense kernels. At runtime, PIT employs novel SRead and SWrite primitives to execute PIT rules extremely fast, supporting dynamic sparsity online. The approach decouples sparse data encoding/decoding from dense computation through an STile abstraction, eliminating traditional sparse kernel overhead.

## Key Results
- Achieves up to 5.9x speedup (average 2.43x) over state-of-the-art compilers
- Maintains high GPU utilization while minimizing coverage waste
- Supports dynamic sparsity online through efficient SRead and SWrite primitives
- Successfully handles diverse model architectures including BERT and ResNet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PIT enables dynamic sparse models to run efficiently on commodity GPUs by converting sparse micro-tiles into dense tiles without altering computation results.
- Mechanism: PIT exploits permutation invariant dimensions in tensor expressions. For a given operator, it identifies dimensions where reordering elements along that axis doesn't change the mathematical result. This allows scattered non-zero values in micro-tiles to be permuted and compacted into dense computation tiles, enabling use of highly optimized dense kernels.
- Core assumption: The operator's computation is commutative over the permuted dimension(s), and dense kernels can process the compacted data without knowing the original sparsity pattern.
- Evidence anchors:
  - [abstract] "PIT proposes a novel tiling mechanism that leverages Permutation Invariant Transformation (PIT), a mathematically proven property, to transform multiple sparsely located micro-tiles into a GPU-efficient dense tile without changing the computation results"
  - [section 3.1] "Permutation invariant establishes a natural connection between sparse computation and dense computation"

### Mechanism 2
- Claim: STile abstraction decouples sparse data encoding/decoding from dense computation, eliminating overhead in sparse kernels.
- Mechanism: STile bundles a group of non-redundant elements (data tile) with a dense computation tile. SLoad and SWrite primitives handle data rearrangement on-the-fly during memory movement between global and shared memory. This means the dense computation kernel never sees sparse indexes, only dense data, removing traditional sparse kernel overhead.
- Core assumption: Data tile size can saturate memory transaction bandwidth, so rearrangement overhead is negligible.
- Evidence anchors:
  - [section 3.3] "The design of STile naturally decouples sparse data (i.e., the data tiles) encoding/decoding and computation. The computation (i.e., computation tile) operates on dense data without traditional sparse data indexes"
  - [section 3.3] "As long as the data tile could saturate read/write transaction of the memory (e.g., 32 bytes in CUDA GPUs), the data rearrangement would introduce little overhead"

### Mechanism 3
- Claim: PIT enables efficient online dynamic sparsity detection by allowing sparse indices to be constructed out-of-order.
- Mechanism: Permutation invariant property relaxes the ordering constraints on sparse indices (e.g., in BCSR format). This means SparDA can construct sparsity indices in parallel without synchronization overhead. The constructed indices directly reference original data blocks, enabling zero-copy rearrangement during SLoad.
- Core assumption: Out-of-order index construction is valid for the given sparse format due to permutation invariant property.
- Evidence anchors:
  - [section 3.4] "Permutation invariant allows the index to be constructed in an out-of-order manner, eliminating heavy synchronization"
  - [section 3.4] "SparDA constructs the sparsity index in an out-of-order manner, because the permutation invariant property relaxes the order of the indices in a sparse data format"

## Foundational Learning

- Concept: Tensor expressions and einsum notation
  - Why needed here: PIT operates on tensor expressions to identify permutation invariant dimensions. Understanding how operations like matmul, convolution, and reduce sum are expressed in tensor notation is essential for determining which dimensions can be permuted.
  - Quick check question: In the tensor expression C[m,n] += A[m,k] * B[k,n], which dimensions are permutation invariant and why?

- Concept: Memory hierarchy and coalescing in GPUs
  - Why needed here: STile's efficiency depends on understanding how data moves between global and shared memory. The 32-byte transaction requirement for negligible overhead comes from GPU memory coalescing rules.
  - Quick check question: Why does a 32-byte data tile size help eliminate rearrangement overhead in SLoad/SWrite?

- Concept: Sparse matrix formats (CSR, BCSR, etc.)
  - Why needed here: SparDA uses these formats but with relaxed ordering constraints. Understanding their structure helps grasp how permutation invariant property enables out-of-order construction.
  - Quick check question: What ordering constraint does BCSR normally impose that permutation invariant allows SparDA to relax?

## Architecture Onboarding

- Component map:
  - STile database: Pre-compiled dense computation tiles and their associated STile variants
  - STile optimizer: Runtime component that selects optimal STile for each operator based on sparsity samples
  - SLoad/SWrite primitives: CUDA kernels for on-the-fly data rearrangement
  - Just-in-time code generator: Emits specialized kernels combining SLoad, dense computation, and SWrite
  - DSparsity class: PyTorch integration for annotating dynamic sparsity

- Critical path:
  1. Model initialization: STile optimizer analyzes representative sparsity samples and selects optimal STiles
  2. Runtime execution: For each operator, detect dynamic sparsity, construct indices, execute SLoad→dense computation→SWrite pipeline
  3. Memory movement: All data rearrangement happens during global→shared memory transfer

- Design tradeoffs:
  - Fine-grained vs coarse-grained STiles: Smaller data tiles handle irregular sparsity better but may have more rearrangement overhead
  - Pre-compilation vs runtime optimization: More pre-compiled STiles increase memory usage but reduce runtime search time
  - Zero-copy vs data conversion: SparDA avoids data conversion for speed but requires careful memory management

- Failure signatures:
  - Poor performance on operators without permutation invariant dimensions
  - Out-of-memory when STile database is too large
  - Stalls if data tiles are too small to saturate memory bandwidth
  - Incorrect results if sparsity pattern cannot be compacted into dense tiles

- First 3 experiments:
  1. Verify permutation invariant property: Implement a simple matmul with permuted dimensions and confirm results match original
  2. Measure SLoad/SWrite overhead: Compare execution time with and without rearrangement for different data tile sizes
  3. Test STile selection: Run optimizer on synthetic sparsity patterns and verify chosen STile matches expected efficiency

## Open Questions the Paper Calls Out
None explicitly stated in the provided materials.

## Limitations
- Assumes permutation invariant dimensions exist for most deep learning operators, which may not hold for all custom operators
- STile efficiency depends on data tile size saturating memory transactions, with no sensitivity analysis provided
- Performance claims lack specificity about sparsity patterns and model characteristics

## Confidence
- High confidence: The core mathematical property of permutation invariance is well-defined and proven
- Medium confidence: The decoupling of sparse encoding from dense computation through STile is sound but implementation-dependent
- Low confidence: The claim about "up to 5.9x" speedup without specifying sparsity patterns or model characteristics

## Next Checks
1. Verify permutation invariant property across all operators in popular models (BERT, ResNet, etc.) to quantify coverage
2. Benchmark STile performance across different GPU architectures with varying data tile sizes
3. Test edge cases where sparsity patterns prevent efficient compaction into dense tiles