---
ver: rpa2
title: Radiology-Aware Model-Based Evaluation Metric for Report Generation
arxiv_id: '2311.16764'
source_url: https://arxiv.org/abs/2311.16764
tags:
- reports
- radcliq
- report
- radgraph
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new automated evaluation metric for machine-generated
  radiology reports using the COMET architecture adapted for the radiology domain.
  The authors train and publish four medically-oriented model checkpoints, including
  one trained on RadGraph, a radiology knowledge graph.
---

# Radiology-Aware Model-Based Evaluation Metric for Report Generation

## Quick Facts
- arXiv ID: 2311.16764
- Source URL: https://arxiv.org/abs/2311.16764
- Reference count: 24
- Key outcome: A new automated evaluation metric for machine-generated radiology reports using COMET architecture adapted for radiology domain, showing moderate to high correlation with established metrics and human judgment

## Executive Summary
This paper presents RadEval, a radiology-aware automated evaluation metric for machine-generated radiology reports that addresses the limitations of general-purpose metrics like BLEU. The authors adapt the COMET architecture to the radiology domain, training four model checkpoints including one specifically trained on RadGraph, a radiology knowledge graph. The metric demonstrates moderate to high correlation with established metrics such as BERTScore, BLEU, and CheXbert, while one checkpoint shows particularly strong alignment with human judgment from board-certified radiologists. The code, data, and model checkpoints will be publicly available, enabling broader adoption of radiology-specific evaluation for automated report generation systems.

## Method Summary
The authors developed RadEval by adapting the COMET framework to radiology, training models on comparative report pairs from the IU X-Ray dataset. They constructed training data by clustering MeSH terms from radiology reports and generating pairs scored using either RadCliQ (a linear combination of BLEU and RadGraph F1) or RadGraph F1 alone. Four model checkpoints were trained using different encoders (XLM-R and BioClinical BERT) and target metrics. The model encodes source and hypothesis reports independently, extracts sentence embeddings, combines features through element-wise operations, and regresses to predict quality scores. Performance was evaluated through correlation analysis with established metrics and human annotations from radiologists on both the ReXVal dataset and an in-house study.

## Key Results
- RadEval shows moderate to high correlation (48-68%) with established metrics including BERTScore, BLEU, and CheXbert
- One checkpoint trained on RadGraph F1 achieves the highest correlation with human judgment among all tested metrics
- RadGraph F1-trained checkpoint shows 67.57% correlation with RadCliQ scores and 48.86% correlation with BERTScore
- Internal human study with two radiologists on 100 reports validates the metric's effectiveness as a radiology-specific evaluation tool

## Why This Works (Mechanism)

### Mechanism 1
RadEval achieves high correlation with human judgment because it leverages a radiology-specific knowledge graph (RadGraph) to capture clinical entities and relations, which are central to report quality. By training on comparative report pairs scored using RadGraph F1, the model learns to distinguish subtle clinical differences (e.g., entity presence, anatomical location, severity) rather than surface-level textual overlap. The core assumption is that RadGraph representation captures clinically meaningful content that radiologists use to judge report accuracy and completeness. Evidence shows correlation numbers in ReXVal dataset are provided, but internal human study correlation is moderate, suggesting partial capture of radiologist expectations. If RadGraph misses key clinical entities or relations, or if radiologists weight other factors (e.g., narrative flow, uncertainty phrasing) more heavily, RadEval correlation will drop.

### Mechanism 2
RadEval's adaptation of the COMET architecture to radiology works because the referenceless estimator model learns to embed radiology reports into a shared vector space where clinically similar reports are close. The model encodes source and hypothesis reports independently using a pretrained encoder (XLM-R or BioClinical BERT), pools them to sentence embeddings, then combines features (element-wise product, absolute difference) to regress on a quality score. The core assumption is that the pretrained encoder has sufficient medical domain knowledge (especially BioClinical BERT) to produce meaningful embeddings for radiology reports. Evidence shows only internal validation set performance reported; no cross-dataset validation shown. If the encoder's medical knowledge is insufficient or if radiology reports contain domain-specific terminology not well represented in pretraining data, the embeddings will be noisy and model performance will degrade.

### Mechanism 3
RadEval correlates well with traditional metrics (BLEU, BERTScore) because it captures both lexical overlap and semantic similarity in radiology reports. Training on RadCliQ (a linear combination of BLEU and RadGraph F1) forces the model to balance n-gram overlap with clinical entity matching, aligning its predictions with both traditional and radiology-specific metrics. The core assumption is that BLEU and BERTScore remain useful signals for radiology report quality, even if they don't capture all clinical nuances. Evidence shows correlation values are provided for both test datasets, showing consistent alignment with multiple metrics. If new radiology-specific metrics emerge that capture different aspects of report quality, or if radiologist expectations shift, RadEval's correlation with traditional metrics may weaken.

## Foundational Learning

- Concept: Radiology report structure and content (findings, impression, MeSH terms)
  - Why needed here: RadEval must understand what makes a radiology report clinically accurate, which requires knowledge of report components and common clinical entities
  - Quick check question: Can you identify the three main sections of a typical radiology report and explain their clinical significance?

- Concept: Knowledge graphs in medical NLP (RadGraph)
  - Why needed here: RadEval leverages RadGraph F1 scores during training, so understanding how clinical entities and relations are extracted is critical
  - Quick check question: What are the main entity types and relation types in RadGraph, and how do they differ from general NLP entity extraction?

- Concept: Metric evaluation and correlation analysis
  - Why needed here: RadEval's performance is validated through correlation with human annotations and other metrics, requiring understanding of Spearman correlation and Kendall Tau
  - Quick check question: When would you use Spearman correlation vs. Kendall Tau, and what does a high correlation value actually tell you about metric alignment?

## Architecture Onboarding

- Component map: Input pairs -> XLM-R/BioClinical BERT encoder -> Sentence embedding extraction -> Feature combination (element-wise product, absolute difference, concatenation) -> Regressor (feed-forward network) -> Quality score prediction

- Critical path: 1. Load comparative report pairs (source, hypothesis) 2. Encode both reports independently 3. Extract sentence embeddings 4. Combine features into single vector 5. Feed through regressor to get score 6. Compare predicted scores with ground truth scores using Kendall Tau 7. Optimize until validation Kendall Tau plateaus

- Design tradeoffs:
  - Encoder choice: XLM-R (general) vs. BioClinical BERT (medical-specific) - affects domain knowledge but increases model size
  - Training corpus: Best Match (one pair per report) vs. Top 10% (multiple pairs) - affects training data diversity vs. quality
  - Target metric: RadCliQ (BLEU + RadGraph F1) vs. RadGraph F1 only - balances lexical and clinical matching

- Failure signatures:
  - Low correlation with human annotations despite high correlation with other metrics → model captures metric artifacts rather than true quality
  - Poor performance on abnormal reports vs. normal reports → training corpus imbalance or encoder bias
  - Inconsistent scores across similar report pairs → feature combination or regression instability

- First 3 experiments:
  1. Train with XLM-R encoder on Best Match corpus using RadCliQ targets; evaluate Kendall Tau on validation set
  2. Repeat with BioClinical BERT encoder; compare validation performance
  3. Switch to Top 10% corpus with RadGraph F1 targets; evaluate correlation with human annotations on ReXVal dataset

## Open Questions the Paper Calls Out

### Open Question 1
How can the variability among radiologists' evaluations be addressed to improve the reliability of the evaluation scheme? The paper mentions that different radiologists often give different scores when evaluating reports, suggesting a need for a more objective and consistent evaluation scheme. The paper acknowledges the variability among radiologists but does not propose a solution to address this issue. A study comparing the effectiveness of different strategies (e.g., standardized evaluation criteria, consensus-building processes) in reducing inter-radiologist variability in report evaluations would resolve this question.

### Open Question 2
How would incorporating additional evaluation metrics beyond BLEU, BERTScore, CheXbert, RadGraph F1, and RadCliQ impact the performance of the RadEval model? The paper focuses on a specific set of metrics for comparison but acknowledges that there are other metrics available that could behave differently. The paper does not explore the potential impact of incorporating additional metrics on the RadEval model's performance. An empirical study evaluating the RadEval model's performance when incorporating a diverse set of additional evaluation metrics would resolve this question.

### Open Question 3
How can the RadEval model be adapted to evaluate radiology reports in languages other than English? The paper mentions that the model is trained on radiology data but does not discuss its applicability to other languages. The paper does not address the potential challenges and strategies for adapting the model to non-English radiology reports. A study demonstrating the effectiveness of the RadEval model in evaluating radiology reports in a non-English language, potentially using machine translation or multilingual models, would resolve this question.

## Limitations
- RadEval's dependence on RadGraph may not capture all clinically relevant entities or relations that radiologists consider when evaluating report quality
- The moderate correlation (60-70%) between RadEval checkpoints and human annotations suggests partial alignment with radiologist expectations
- The internal human study with two radiologists on 100 reports represents a relatively small sample size that may not generalize across different radiology subspecialties or reporting styles

## Confidence
- **High confidence**: RadEval achieves moderate to high correlation with established metrics (BLEU, BERTScore, CheXbert) and shows good alignment with human judgment on ReXVal dataset
- **Medium confidence**: The adaptation of COMET architecture to radiology domain is effective and the four checkpoints capture different aspects of report quality
- **Medium confidence**: RadGraph F1-trained checkpoint shows highest correlation with RadCliQ scores, indicating successful integration of clinical entity matching

## Next Checks
1. **Clinical entity coverage validation**: Analyze RadGraph's entity and relation extraction performance on a diverse set of radiology reports across multiple subspecialties to identify potential coverage gaps
2. **Abnormal report performance assessment**: Conduct targeted evaluation of RadEval's performance on reports containing pathological findings versus normal reports to ensure clinical relevance
3. **Cross-institutional validation**: Test RadEval's correlation with human judgment using radiologist annotations from multiple institutions to assess generalizability across different reporting styles and clinical practices