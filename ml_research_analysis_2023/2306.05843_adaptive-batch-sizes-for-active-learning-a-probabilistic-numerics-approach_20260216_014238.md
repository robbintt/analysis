---
ver: rpa2
title: Adaptive Batch Sizes for Active Learning A Probabilistic Numerics Approach
arxiv_id: '2306.05843'
source_url: https://arxiv.org/abs/2306.05843
tags:
- constraints
- batch
- bayesian
- xrec
- xbatch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of parallel active learning in
  Bayesian optimization with constraints, proposing an adaptive batch size approach.
  The core method idea is to frame batch selection as a quadrature task, allowing
  for the automatic tuning of batch sizes based on predefined quadrature precision
  objectives.
---

# Adaptive Batch Sizes for Active Learning A Probabilistic Numerics Approach

## Quick Facts
- arXiv ID: 2306.05843
- Source URL: https://arxiv.org/abs/2306.05843
- Reference count: 40
- One-line primary result: Adaptive batch sizing significantly enhances learning efficiency and flexibility in Bayesian optimization with constraints by automatically tuning batch sizes based on quadrature precision objectives.

## Executive Summary
This paper proposes an adaptive batch size approach for parallel active learning in Bayesian optimization with constraints. The method frames batch selection as a quadrature task, automatically tuning batch sizes to meet predefined precision objectives. It extends to constrained scenarios by interpreting constraint violations as reductions in precision requirements, enabling prudent sampling while maintaining constraint satisfaction. The approach is demonstrated to improve learning efficiency across diverse applications including synthetic benchmarks and real-world optimization problems.

## Method Summary
The method treats batch selection as a quadrature problem, using integration error as a stopping criterion that dynamically adapts batch size based on current uncertainty and constraint satisfaction likelihood. When constraints are unknown, the algorithm estimates rejection probability from surrogate models and uses this to relax quadrature precision requirements, allowing exploration of uncertain regions while maintaining constraint satisfaction. A linear programming formulation balances reward maximization with quadrature precision under constraints, using SVD-based error bounds to determine optimal batch composition. The approach is implemented in the cSOBER algorithm, which subsamples from larger candidate sets to balance exploration and exploitation.

## Key Results
- Significantly improved learning efficiency across diverse Bayesian batch active learning and Bayesian optimization applications
- Enhanced flexibility in handling both known and unknown constraints through adaptive tolerance mechanisms
- Demonstrated effectiveness on multiple benchmark functions including Ackley, Hartmann, and real-world optimization problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive batch sizing reduces total wall-clock time by balancing cost vs. speed trade-offs dynamically
- Mechanism: The method treats batch selection as quadrature, using integration error as a stopping criterion that naturally adapts batch size based on current uncertainty and constraint satisfaction likelihood
- Core assumption: Integration error correlates with information gain efficiency, so reducing it below tolerance indicates sufficient batch quality
- Evidence anchors:
  - [abstract]: "automatic tuning of batch sizes to meet predefined quadrature precision objectives"
  - [section 3.1]: "we define the tolerance as the rejection rate ϵrej... We then use this expected rejection rate ϵrej as the tolerance ϵ for the quadrature solver"
  - [corpus]: No direct evidence - corpus focuses on batch sizes in different contexts (RL, federated learning)
- Break condition: When expected rejection rate is consistently underestimated, leading to batches that are too large and frequently rejected

### Mechanism 2
- Claim: Propagating constraint violations as reduced precision requirements enables prudent sampling
- Mechanism: When constraints are unknown, the algorithm estimates rejection probability from surrogate models and uses this to relax quadrature precision requirements, allowing exploration of uncertain regions while maintaining constraint satisfaction
- Core assumption: The surrogate model's predicted constraint satisfaction probability is a reliable estimator of true rejection risk
- Evidence anchors:
  - [abstract]: "interpreting constraint violations as reductions in the precision requirement"
  - [section 3.1]: "we define the tolerance as the rejection rate... We propagate this estimated tolerance ϵrej as the tolerance ϵ for the quadrature solver"
  - [corpus]: No direct evidence - corpus neighbors don't discuss constraint handling
- Break condition: When surrogate models are highly uncertain about constraint regions, leading to poor rejection rate estimates

### Mechanism 3
- Claim: Linear programming formulation balances reward maximization with quadrature precision under constraints
- Mechanism: The LP problem maximizes expected reward subject to constraints on integration error and acceptance probability, allowing automatic determination of optimal batch composition
- Core assumption: The SVD-based error bounds provide tight enough control over integration error for practical purposes
- Evidence anchors:
  - [section 3.2]: "maximize w⊤(g(Xrec) ⊙ q(Xrec)), subject to |((w − wrec)⊤φj(Xrec))| ≤ ϵLP p(λj/(n − 2))"
  - [section 4]: "the larger the tolerance is, the larger the expected rewards becomes, resulting in a lower true rejection rate"
  - [corpus]: No direct evidence - corpus neighbors don't discuss LP formulations for batch selection
- Break condition: When the kernel matrix becomes ill-conditioned, making SVD-based error bounds unreliable

## Foundational Learning

- Concept: Bayesian quadrature and its relationship to Bayesian optimization
  - Why needed here: The paper frames batch selection as a quadrature problem, so understanding this equivalence is crucial
  - Quick check question: How does minimizing worst-case integration error relate to finding global optima in Bayesian optimization?

- Concept: Gaussian process regression and its use in surrogate modeling
  - Why needed here: The method relies heavily on GP models for both objective and constraint functions
  - Quick check question: What are the key hyperparameters that need to be tuned in a GP model, and how do they affect uncertainty estimates?

- Concept: Linear programming and singular value decomposition
  - Why needed here: The batch selection algorithm uses LP with SVD-based constraints to balance exploration and exploitation
  - Quick check question: How does the SVD decomposition of the kernel matrix enable efficient approximation of integration error bounds?

## Architecture Onboarding

- Component map: GP surrogate models (objective and constraints) -> Sampling module (draws from prior/proposal distribution) -> Rejection rate estimator (computes expected constraint violations) -> LP solver (selects batch with optimal trade-off) -> Tolerance adaptation module (updates precision requirements) -> Query -> Update GP models

- Critical path: GP prediction → rejection rate estimation → LP batch selection → query → update GP models

- Design tradeoffs:
  - Batch size vs. precision: Larger batches may explore more but with higher rejection risk
  - Exploration vs. exploitation: LP constraints must balance these through tolerance settings
  - Computational cost: LP solving adds overhead but enables better batch composition

- Failure signatures:
  - High rejection rates despite low estimated rejection rate → poor surrogate model
  - Very small batch sizes → overly conservative tolerance settings
  - Oscillating batch sizes → unstable rejection rate estimation

- First 3 experiments:
  1. Run on a simple 1D synthetic function with known constraints to verify basic functionality
  2. Test on a discrete space problem (like Hartmann5) to verify constraint handling
  3. Scale to a larger batch size (100+) on a real-world problem to test computational efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the adaptive tolerance mechanism perform when the rejection rate estimation is inaccurate?
- Basis in paper: [explicit] The paper mentions that the true rejection rate depends on the true constraints, which are not known until the objective function is queried, and instead estimates the rejection rate from surrogate models.
- Why unresolved: The paper does not provide empirical evidence or theoretical analysis on how inaccuracies in rejection rate estimation affect the performance of the adaptive tolerance mechanism.
- What evidence would resolve it: Experiments or theoretical analysis demonstrating the impact of inaccurate rejection rate estimation on the performance of the adaptive tolerance mechanism.

### Open Question 2
- Question: Can the proposed method handle high-dimensional input spaces efficiently?
- Basis in paper: [inferred] The paper mentions that applicability to high-dimensional BO is an open problem and that efficient sampling from the posterior over the maximizer is not always available.
- Why unresolved: The paper does not provide experimental results or theoretical analysis on the performance of the method in high-dimensional input spaces.
- What evidence would resolve it: Experiments or theoretical analysis demonstrating the performance of the method in high-dimensional input spaces.

### Open Question 3
- Question: How does the method perform when the constraints are not well-represented by the surrogate models?
- Basis in paper: [explicit] The paper mentions that the method relies on probabilistic surrogate models to handle constraints.
- Why unresolved: The paper does not provide empirical evidence or theoretical analysis on how inaccuracies in the surrogate models affect the performance of the method.
- What evidence would resolve it: Experiments or theoretical analysis demonstrating the impact of inaccurate surrogate models on the performance of the method.

## Limitations
- Performance heavily depends on accuracy of rejection rate estimation from surrogate models
- SVD-based error bounds may become unreliable when kernel matrix is ill-conditioned
- Scalability to high-dimensional input spaces remains an open problem

## Confidence

- **High**: The core mechanism of treating batch selection as quadrature with adaptive precision requirements
- **Medium**: The constraint violation propagation approach - relies on surrogate model accuracy which can be problematic
- **Medium**: The LP formulation with SVD error bounds - mathematically sound but computational cost and stability concerns exist

## Next Checks

1. Test the method on a 10+ dimensional problem to evaluate scalability of the SVD error bounds and LP solver performance
2. Introduce controlled noise in constraint satisfaction predictions to measure sensitivity to surrogate model uncertainty
3. Compare wall-clock time against fixed batch size approaches on problems where constraint evaluations are expensive relative to objective evaluations