---
ver: rpa2
title: 'On the Planning Abilities of Large Language Models : A Critical Investigation'
arxiv_id: '2305.15771'
source_url: https://arxiv.org/abs/2305.15771
tags:
- object
- block
- plan
- location
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the planning capabilities of large language
  models (LLMs) using classical planning domains. The authors evaluate LLMs in two
  modes: autonomous (standalone planning) and heuristic (providing guidance to external
  planners).'
---

# On the Planning Abilities of Large Language Models : A Critical Investigation

## Quick Facts
- **arXiv ID**: 2305.15771
- **Source URL**: https://arxiv.org/abs/2305.15771
- **Reference count**: 40
- **Primary result**: LLMs perform poorly in autonomous planning (~12% success for GPT-4) but can improve external planner search when used as heuristic guides

## Executive Summary
This paper investigates large language models' (LLMs) capabilities for classical planning through systematic evaluation in both autonomous and heuristic modes. The authors find that LLMs struggle to generate correct plans independently, with even the best model (GPT-4) achieving only ~12% success across multiple domains. However, when LLM-generated plans are used as guidance for sound external planners like LPG, they can significantly reduce search effort and improve efficiency. The study also demonstrates that iterative back-prompting with external verifiers can improve plan quality over multiple rounds of feedback.

## Method Summary
The authors evaluate LLMs (GPT-4, GPT-3.5, Instruct-GPT3, GPT3) on classical planning domains from the International Planning Competition using PDDL specifications. They employ two evaluation modes: autonomous (LLM generates complete plans independently) and heuristic (LLM plans guide external planners like LPG). Plan correctness is validated using the VAL validator. The study also explores iterative improvement through back-prompting, where verifier feedback is fed back to the LLM to generate better plans. Prompts are generated from PDDL domains in both natural language and PDDL formats using zero-shot and one-shot approaches.

## Key Results
- GPT-4 achieves only ~12% success rate in autonomous planning across domains
- LLM-generated plans reduce LPG planner search steps compared to empty/random seeds
- Iterative back-prompting with VAL validator improves plan correctness in common-sense domains
- Obfuscating action/predicate names significantly degrades LLM performance
- LPG can successfully repair flawed LLM plans with relative ease

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs are poor at autonomous planning because they rely on pattern matching rather than causal reasoning.
- **Mechanism**: LLMs generate plans based on learned statistical associations from training data, not by simulating state transitions. This causes failures when action preconditions or delete effects are involved, especially when object/action names are obfuscated.
- **Core assumption**: Planning correctness requires causal understanding of action effects, which LLMs lack in autonomous mode.
- **Evidence anchors**:
  - [abstract] "LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains."
  - [section] "We show that the average search steps by the LLM+LPG combination is much lesser than both the baselines, thereby revealing that LLMs' plans are indeed helping with the search process of the underlying planner."
  - [corpus] Weak; related work mostly echoes this limitation without deeper mechanistic explanation.
- **Break condition**: If LLM is fine-tuned extensively on planning-specific data or augmented with external causal simulators, the pattern-matching limitation may be partially overcome.

### Mechanism 2
- **Claim**: LLMs can serve as idea generators in LLM-Modulo frameworks by providing approximate plans that guide sound external planners.
- **Mechanism**: LLM-generated plans act as heuristic seeds for local search planners like LPG, reducing the number of search steps needed to find a correct plan. The LLM provides a high-level sketch, and the planner fixes flaws.
- **Core assumption**: A partially correct plan that preserves some structure of the final plan can reduce planner search effort.
- **Evidence anchors**:
  - [abstract] "In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners..."
  - [section] "We show that a well known automated planner called LPG... is able to repair the LLM plans with relative ease."
  - [corpus] Moderate; several concurrent studies confirm this LLM-Modulo approach, though often without ablation on search step reduction.
- **Break condition**: If the LLM consistently produces plans far from any correct solution (e.g., obfuscated domains), the planner gains little benefit.

### Mechanism 3
- **Claim**: External verifiers can iteratively improve LLM-generated plans through back-prompting.
- **Mechanism**: A sound verifier identifies flaws in LLM plans (e.g., unsatisfied preconditions) and feeds corrections back to the LLM, prompting it to generate a better plan in a loop.
- **Core assumption**: LLMs can incorporate feedback to adjust their plan generation when given structured error information.
- **Evidence anchors**:
  - [abstract] "we use an external verifier, V AL, to point out the errors in the LLM-generated plans and back-prompt the LLM for a new plan with this feedback."
  - [section] "We show that this repeated interaction indeed improves the plan correctness in common-sense domains."
  - [corpus] Moderate; this iterative human-in-the-loop or verifier-in-the-loop paradigm is discussed in related literature but rarely quantified.
- **Break condition**: If the verifier's feedback is too complex or the LLM's context window is insufficient, the back-prompting loop may stall or diverge.

## Foundational Learning

- **Concept**: Classical planning problem formulation (PDDL)
  - **Why needed here**: The study evaluates LLMs against formal PDDL domains; understanding this formalism is essential to interpret results.
  - **Quick check question**: What are the components of a classical planning problem in PDDL? (Domain, initial state, goal; actions with preconditions/effects.)

- **Concept**: Delete and precondition relaxations
  - **Why needed here**: These relaxations are used to diagnose why LLM plans fail (e.g., are they missing delete effects or preconditions?).
  - **Quick check question**: How does delete relaxation change plan evaluation? (Ignores negative effects, making plans easier to satisfy but less realistic.)

- **Concept**: Local search planning (LPG) and heuristic guidance
  - **Why needed here**: LPG is the external planner used in heuristic mode; understanding its operation explains why LLM plans help.
  - **Quick check question**: What does LPG do with a seed plan? (Iteratively repairs flaws until a valid plan is found.)

## Architecture Onboarding

- **Component map**: LLM generator -> VAL verifier -> LPG planner -> VAL verifier (iterative loop possible)
- **Critical path**:
  1. Generate plan from LLM (autonomous or heuristic)
  2. Validate with VAL (if verifier present)
  3. If invalid, back-prompt LLM (if in iterative mode)
  4. Feed plan to LPG (if in heuristic mode)
  5. Evaluate success
- **Design tradeoffs**:
  - Autonomous mode: fast, but low correctness
  - Heuristic mode: higher correctness, requires external planner
  - Back-prompting: can improve plans, but adds latency and complexity
- **Failure signatures**:
  - LLM plan invalid: likely pattern-matching failure or name obfuscation issues
  - LPG fails to repair: LLM plan too far from valid solution
  - Back-prompting loop stalls: verifier feedback too complex or LLM context exhausted
- **First 3 experiments**:
  1. Run LLM in autonomous mode on Blocksworld domain; measure success rate
  2. Feed LLM plan to LPG; compare search steps vs empty/random seed
  3. Apply VAL back-prompting loop; track improvement over iterations

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of LLM-generated plans vary across different types of planning domains (e.g., classical planning, temporal planning, probabilistic planning)?
- **Basis in paper**: [explicit] The paper evaluates LLMs in classical planning domains but suggests investigating other types of planning domains.
- **Why unresolved**: The paper only focuses on classical planning domains and does not explore the performance of LLMs in other types of planning domains.
- **What evidence would resolve it**: Conducting experiments with LLMs in various types of planning domains (e.g., temporal planning, probabilistic planning) and comparing their performance across these domains.

### Open Question 2
- **Question**: What is the impact of fine-tuning LLMs on their planning performance in different domains?
- **Basis in paper**: [explicit] The paper mentions that fine-tuning GPT-3 on the Blocksworld domain had limited impact on its performance.
- **Why unresolved**: The paper only provides results for fine-tuning GPT-3 on one domain and does not explore the effects of fine-tuning on other domains or other LLM models.
- **What evidence would resolve it**: Experimenting with fine-tuning different LLM models on various planning domains and analyzing the impact on their planning performance.

### Open Question 3
- **Question**: How does the use of different prompting techniques affect the planning performance of LLMs?
- **Basis in paper**: [explicit] The paper evaluates the use of chain of thought prompting but does not explore other prompting techniques.
- **Why unresolved**: The paper only investigates the effect of chain of thought prompting and does not explore the impact of other prompting techniques on LLM planning performance.
- **What evidence would resolve it**: Conducting experiments with different prompting techniques (e.g., zero-shot, few-shot, tree of thoughts) and analyzing their effects on the planning performance of LLMs.

### Open Question 4
- **Question**: How does the performance of LLM-generated plans change when considering different levels of domain obfuscation?
- **Basis in paper**: [explicit] The paper shows that obfuscating action and predicate names leads to a significant drop in LLM planning performance.
- **Why unresolved**: The paper only investigates the effect of obfuscating action and predicate names and does not explore the impact of other types of domain obfuscation (e.g., renaming objects, changing relationships).
- **What evidence would resolve it**: Experimenting with different levels of domain obfuscation (e.g., renaming objects, changing relationships) and analyzing their effects on the planning performance of LLMs.

## Limitations

- The study focuses exclusively on classical planning domains with well-defined PDDL specifications, limiting generalizability to real-world scenarios with ambiguous or incomplete information
- LLM evaluations use only zero-shot and one-shot prompting without extensive prompt engineering or fine-tuning, potentially underestimating LLM capabilities
- The external verifier and planner components (VAL and LPG) are treated as black boxes with their own limitations not fully explored in this study

## Confidence

- **High confidence**: Claims about LLM limitations in autonomous planning mode (GPT-4 ~12% success rate) are well-supported by empirical results across multiple domains and baselines
- **Medium confidence**: Claims about LLM-Modulo benefits (search step reduction) are supported by experimental data but could benefit from more extensive ablation studies across different planner types
- **Medium confidence**: Claims about iterative improvement through back-prompting are demonstrated but only for common-sense domains; results may not generalize to more complex planning scenarios

## Next Checks

1. **Generalization Test**: Evaluate the LLM-Modulo approach on domains with higher state space complexity and longer optimal plans to verify if search step reduction scales with problem difficulty

2. **Prompt Engineering Study**: Systematically explore prompt variations (few-shot examples, chain-of-thought reasoning, structured output formats) to determine if autonomous planning success rates can be improved without external components

3. **Verifier Feedback Analysis**: Conduct a detailed study of which types of VAL feedback lead to successful plan improvements, and whether certain error patterns are more amenable to LLM correction than others