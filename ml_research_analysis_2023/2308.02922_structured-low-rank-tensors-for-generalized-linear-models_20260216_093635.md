---
ver: rpa2
title: Structured Low-Rank Tensors for Generalized Linear Models
arxiv_id: '2308.02922'
source_url: https://arxiv.org/abs/2308.02922
tags:
- tensor
- regression
- estimation
- data
- lsrtr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies low separation rank (LSR) tensor structured
  models for generalized linear models (GLMs), proposing a block coordinate descent
  algorithm for parameter estimation and deriving a minimax lower bound on estimation
  error. The LSR model generalizes the Tucker and CP decompositions, and the authors
  show that it offers lower sample complexity than vector-based GLMs while maintaining
  stronger representation power than Tucker or CP models of the same core size.
---

# Structured Low-Rank Tensors for Generalized Linear Models

## Quick Facts
- arXiv ID: 2308.02922
- Source URL: https://arxiv.org/abs/2308.02922
- Reference count: 40
- Key outcome: LSR-TGLM reduces sample complexity compared to vectorized GLMs while offering stronger representation power than Tucker/CP models

## Executive Summary
This paper introduces Low Separation Rank (LSR) tensor-structured Generalized Linear Models (GLMs) that generalize Tucker and CP decompositions. The authors propose a block coordinate descent algorithm (LSRTR) for parameter estimation and derive minimax lower bounds on estimation error. The LSR model offers lower sample complexity than standard vectorized GLMs while maintaining greater representation power than Tucker or CP models of equivalent core size. Numerical experiments on synthetic and medical imaging data demonstrate the efficacy of the approach.

## Method Summary
The method involves imposing LSR structure on coefficient tensors in GLMs, where the coefficient tensor is expressed as a sum of rank-1 tensors. The LSRTR algorithm alternates between estimating factor matrices (with QR projection onto Stiefel manifolds) and the core tensor via gradient descent. Synthetic data is generated with Gaussian entries, orthogonal factor matrices, and core tensors from Gaussian distributions. The algorithm is evaluated on linear, logistic, and Poisson regression tasks, comparing estimation and prediction errors against baseline methods.

## Key Results
- LSR-TGLM reduces sample complexity compared to vectorized GLMs by constraining coefficient tensor to low separation rank form
- Derived minimax lower bound scales with intrinsic degrees of freedom (S∑kmkrk + ∏krk) rather than full tensor size
- Numerical experiments show LSRTR outperforms vector-based methods and competes favorably with Tucker/CP models, particularly on imbalanced real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
Imposing LSR structure on coefficient tensor reduces sample complexity compared to vectorized GLMs. By constraining coefficient tensor to low separation rank form, the number of learnable parameters decreases from full tensor size to S∑kmkrk + ∏krk, enabling reliable estimation with fewer samples. Core assumption: True coefficient tensor is approximately LSR-structured and separation rank S is known.

### Mechanism 2
Block coordinate descent algorithm (LSRTR) can effectively estimate LSR-structured coefficient tensors. Alternating minimization over factor matrices and core tensor, where each subproblem reduces to a lower-dimensional GLM with structured covariates. Core assumption: Each subproblem (estimating one factor matrix or core tensor) is convex when other parameters are fixed.

### Mechanism 3
Derived minimax lower bound captures intrinsic degrees of freedom and reflects sample complexity benefits of LSR structure. Information-theoretic approach using Fano's inequality with carefully constructed packing set of LSR tensors; bound scales with S∑kmkrk + ∏krk rather than full tensor size. Core assumption: Coefficient tensor lies within known radius ball around zero and covariate distribution satisfies Gaussian assumption.

## Foundational Learning

- **Concept**: Generalized Linear Models (GLMs) and link functions
  - **Why needed here**: LSR-TGLM extends standard GLMs to tensor-structured covariates and coefficients
  - **Quick check question**: What is the canonical link function for logistic regression? (Answer: logit function)

- **Concept**: Tensor decompositions (CP, Tucker, BTD, LSR)
  - **Why needed here**: LSR model generalizes Tucker and CP, and is special case of BTD; understanding these helps grasp LSR advantages
  - **Quick check question**: How does Tucker decomposition differ from CP decomposition in terms of core tensor structure? (Answer: Tucker has unrestricted core tensor, CP has diagonal core)

- **Concept**: Minimax theory and Fano's inequality
  - **Why needed here**: Lower bound derivation uses information-theoretic approach based on Fano's inequality
  - **Quick check question**: What is the relationship between packing number, minimum distance, and Fano's inequality in minimax bounds? (Answer: Larger packing number with maintained minimum distance increases lower bound)

## Architecture Onboarding

- **Component map**: Covariates X → LSRTR iterations (alternating over factor matrices and core tensor) → Estimated tensor → Predictions

- **Critical path**: Covariates → LSRTR iterations (alternating over factor matrices and core tensor) → Estimated tensor → Predictions

- **Design tradeoffs**: LSR vs Tucker/CP: More parameters (S-1)∑kmkrk extra) but greater representation power; Known vs unknown rank/S: Known enables tractable estimation; unknown makes problem NP-hard; BCD convergence: No theoretical guarantees but empirically effective; QR projection non-expansiveness unproven

- **Failure signatures**: Estimation error > 1 (normalized) indicates severe underfitting; LSRTR performance plateauing even with many samples suggests model misspecification; High variance across replications indicates sensitivity to initialization

- **First 3 experiments**: 1) Synthetic linear regression with known LSR structure, varying sample sizes to verify sample complexity reduction; 2) Synthetic logistic regression comparing LSRTR to TTR/LTuR on small vs large models; 3) Real medical imaging dataset (ADHD200) to assess performance on imbalanced, high-dimensional data

## Open Questions the Paper Calls Out

- **Open Question 1**: Can theoretical convergence guarantees be established for the LSRTR algorithm, given the non-convexity of the constraint space (product of Stiefel manifolds)?
  - **Basis in paper**: The paper explicitly states this is a non-trivial task left for future work and highlights the challenges posed by the non-convex constraint set.
  - **Why unresolved**: Proving convergence for BCD on non-convex problems is difficult, especially with non-convex constraint spaces.
  - **What evidence would resolve it**: A formal proof of convergence (e.g., convergence to a stationary point) for LSRTR under the given constraints.

- **Open Question 2**: Is the assumption of known tensor rank (r₁,...,rₖ) and separation rank S realistic in practical applications, or should methods be developed to estimate these parameters from data?
  - **Basis in paper**: The paper acknowledges that finding the LSR rank S is NP-hard and assumes known ranks for tractability.
  - **Why unresolved**: Estimating tensor rank and separation rank from data is a challenging problem in tensor analysis.
  - **What evidence would resolve it**: Development and evaluation of algorithms for estimating tensor rank and separation rank in the context of LSR-TGLM.

- **Open Question 3**: How does the performance of LSRTR compare to other tensor decomposition methods (e.g., BTD with a different core tensor structure) when the true coefficient tensor does not have an LSR structure?
  - **Basis in paper**: The paper shows that LSRTR outperforms Tucker and CP models when the true tensor is LSR-structured.
  - **Why unresolved**: The paper focuses on the case where the true tensor has an LSR structure.
  - **What evidence would resolve it**: Empirical comparison of LSRTR with other tensor decomposition methods on synthetic data with coefficient tensors having different structures.

## Limitations
- Lack of theoretical convergence guarantees for the proposed block coordinate descent algorithm
- Limited empirical validation to relatively small synthetic problems (up to 50×50×50 tensors) and one medical imaging dataset
- Strong dependence on the true underlying tensor structure matching the LSR assumption

## Confidence
- Sample complexity benefits: Medium - while the minimax bound correctly scales with reduced degrees of freedom, real-world performance depends heavily on LSR structure matching
- Convergence guarantees: Low - no theoretical analysis provided, only empirical observation of good performance
- Scalability: Medium - empirical results show good performance on small tensors but larger-scale testing needed

## Next Checks
1. Test LSRTR convergence on larger-scale problems (e.g., 100×100×100 or bigger) to assess scalability limits
2. Evaluate robustness to misspecified separation rank by systematically varying S relative to true rank
3. Compare performance against unstructured penalized GLM approaches on datasets where tensor structure is not clearly present