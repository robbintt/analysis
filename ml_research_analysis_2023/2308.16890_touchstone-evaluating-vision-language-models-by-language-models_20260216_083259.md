---
ver: rpa2
title: 'TouchStone: Evaluating Vision-Language Models by Language Models'
arxiv_id: '2308.16890'
source_url: https://arxiv.org/abs/2308.16890
tags:
- visual
- image
- pants
- arxiv
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TouchStone, a novel evaluation method for
  large vision-language models (LVLMs) using strong language models as judges. The
  authors construct a comprehensive visual dialogue dataset covering five major categories
  and 27 subtasks, ranging from basic recognition to literary creation.
---

# TouchStone: Evaluating Vision-Language Models by Language Models

## Quick Facts
- arXiv ID: 2308.16890
- Source URL: https://arxiv.org/abs/2308.16890
- Reference count: 12
- Primary result: GPT-4 achieves 72.2% consistency with human judgments when evaluating vision-language models using textual annotations

## Executive Summary
This paper introduces TouchStone, a novel evaluation method for large vision-language models (LVLMs) that leverages strong language models like GPT-4 as automated judges. The approach transforms multimodal inputs into detailed textual descriptions through comprehensive image annotations, enabling text-only models to assess dialogue quality and detect hallucinations. The method covers five major categories and 27 subtasks ranging from basic recognition to literary creation, demonstrating that GPT-4 can effectively score dialogue quality without human intervention while achieving high consistency with human preferences.

## Method Summary
The evaluation method constructs the TouchStone dataset with 908 questions across five categories and 27 subtasks. Visual content is converted to textual descriptions through detailed image annotations, which are then used as inputs for GPT-4 to generate reference answers and score LVLM responses. The scoring process employs pairwise comparison with position balancing, where answers are evaluated in two rounds with swapped positions and averaged to eliminate positional bias. This enables automated assessment of dialogue quality, relevance, accuracy, and hallucination detection without requiring human evaluators.

## Key Results
- GPT-4 achieves 72.2% consistency with human judgments when evaluating LVLM responses
- The evaluation method effectively identifies hallucination issues across current LVLMs
- Qwen-VL and mPLUG-Owl demonstrate relatively stronger performance across evaluation categories
- Significant performance gaps exist in visual storytelling, multi-image analysis, and hallucination detection tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can effectively score dialogue quality without requiring human intervention by leveraging its textual capabilities alone.
- Mechanism: The approach substitutes visual inputs with detailed textual annotations, allowing GPT-4 to process multimodal content in its native text-only mode. This preserves the semantic richness of visual information while enabling GPT-4 to perform scoring tasks it is already optimized for.
- Core assumption: Detailed image annotations can capture sufficient visual information to enable accurate scoring of dialogue quality without actual visual inputs.
- Evidence anchors:
  - [abstract] "by integrating detailed image annotations we effectively transform the multimodal input content into a form understandable by LLMs"
  - [section] "we manually substitute the actual image input with fine-grained textual annotations. By inputting these annotations and corresponding questions to a powerful LLM like GPT-4, we obtain reference answers"
  - [corpus] Weak - the corpus neighbors focus on hallucination evaluation and reasoning benchmarks, but don't directly validate the substitution mechanism
- Break condition: If the textual annotations omit critical visual details or introduce ambiguities that GPT-4 cannot resolve through text alone, the scoring accuracy would degrade.

### Mechanism 2
- Claim: GPT-4 achieves high consistency (72.2%) with human judgments when evaluating LVLM responses.
- Mechanism: GPT-4's strong language understanding and generation capabilities allow it to assess relevance, accuracy, and usefulness of responses based on textual descriptions of visual content, matching human evaluation patterns.
- Core assumption: GPT-4's text-based reasoning is sufficient to capture the nuances of multimodal dialogue quality that humans consider when judging responses.
- Evidence anchors:
  - [abstract] "we demonstrate that powerful LVLMs, such as GPT-4, can effectively score dialogue quality by leveraging their textual capabilities alone, aligning with human preferences"
  - [section] "we find that GPT-4 shows highly-consistent judgment compared to human preference"
  - [corpus] Weak - corpus neighbors discuss hallucination and reasoning evaluation but don't provide direct evidence of human consistency
- Break condition: If GPT-4's text-only reasoning fails to capture important visual context that humans would consider, the consistency would drop significantly.

### Mechanism 3
- Claim: Position balancing in scoring prevents positional bias when comparing model responses.
- Mechanism: By conducting two scoring rounds with swapped answer positions and averaging the results, the evaluation method eliminates bias that might favor responses appearing first or second.
- Core assumption: Positional bias exists in pairwise comparisons and can be mitigated through systematic position swapping.
- Evidence anchors:
  - [section] "To eliminate the influence of answer position, we perform a second scoring round by swapping the positions of the answers and then compute the average of the two scores obtained"
  - [corpus] Missing - no corpus evidence found for position balancing in model evaluation
- Break condition: If the positional bias is minimal or GPT-4 is already position-agnostic, the additional complexity of position balancing may not provide meaningful improvement.

## Foundational Learning

- Concept: Multimodal input transformation
  - Why needed here: LVLMs need to be evaluated using existing text-only LLMs as judges, requiring conversion of visual content into textual form
  - Quick check question: What key information must image annotations preserve to enable accurate evaluation by a text-only model?

- Concept: Pairwise comparison scoring
  - Why needed here: Direct absolute scoring of model responses is challenging; comparing responses against a reference and each other provides more reliable relative quality assessment
  - Quick check question: How does pairwise comparison with position balancing improve the reliability of model evaluation?

- Concept: Hallucination detection in LVLMs
  - Why needed here: LVLMs often generate content not present in input images, and the evaluation method needs to identify and penalize such hallucinations
  - Quick check question: What textual cues in model responses might indicate hallucination when compared against detailed image annotations?

## Architecture Onboarding

- Component map: TouchStone dataset (908 questions) -> Detailed image annotations -> GPT-4 reference answers -> LVLM responses -> GPT-4 scoring -> Position balancing -> Final evaluation scores
- Critical path: Image → Detailed annotations → GPT-4 reference answers → LVLM responses → GPT-4 scoring → Position balancing → Final evaluation scores
- Design tradeoffs:
  - Trade detailed annotations (better GPT-4 evaluation) vs. annotation cost and time
  - Trade comprehensive task coverage vs. dataset size and management complexity
  - Trade position balancing (more robust) vs. computational overhead and complexity
- Failure signatures:
  - Low consistency between GPT-4 and human evaluations indicates annotation quality issues
  - Systematic score differences across answer positions suggest position bias not properly addressed
  - Poor performance on specific task categories may indicate insufficient annotation detail for those types
- First 3 experiments:
  1. Test annotation quality by having humans evaluate whether GPT-4 can answer questions correctly using only the annotations
  2. Validate position balancing by running the same comparisons with fixed positions and measuring bias
  3. Test hallucination detection by comparing GPT-4 scores when model responses contain known hallucinations vs. accurate content

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TouchStone's evaluation method perform on LVLMs with significantly different architectures compared to the models tested (e.g., models using transformer-based vision encoders vs. those using convolutional neural networks)?
- Basis in paper: [inferred] The paper evaluates several LVLMs with varying architectures but does not explicitly discuss how the evaluation method performs across fundamentally different vision encoder types.
- Why unresolved: The paper focuses on comparing models with similar architectures (primarily transformer-based) and does not explore the impact of different vision encoder types on the evaluation results.
- What evidence would resolve it: Testing TouchStone's evaluation method on a diverse set of LVLMs with different vision encoder architectures (e.g., CNNs, transformers, hybrid models) and comparing the consistency of results across these architectures.

### Open Question 2
- Question: How does the performance of TouchStone's evaluation method change when using different strong language models as judges (e.g., GPT-4 vs. other advanced LLMs like Claude or PaLM)?
- Basis in paper: [explicit] The paper uses GPT-4 as the judge and demonstrates its effectiveness, but does not explore the performance of other advanced LLMs in this role.
- Why unresolved: The paper establishes GPT-4's capability as a judge but does not investigate whether other advanced LLMs could perform similarly or better in evaluating LVLMs.
- What evidence would resolve it: Conducting the same evaluation experiments using different advanced LLMs (e.g., Claude, PaLM) as judges and comparing their consistency with human judgments and their ability to detect hallucinations.

### Open Question 3
- Question: How does the quality and detail of image annotations affect the accuracy and consistency of TouchStone's evaluation results?
- Basis in paper: [inferred] The paper emphasizes the importance of detailed image annotations for the evaluation process but does not systematically investigate how variations in annotation quality impact the results.
- Why unresolved: While the paper uses manually verified detailed annotations, it does not explore the relationship between annotation quality and evaluation accuracy, nor does it discuss potential limitations or biases introduced by annotation quality.
- What evidence would resolve it: Conducting controlled experiments where the same images are annotated with varying levels of detail and accuracy, then evaluating how these differences impact the consistency of results with human judgments and the ability to detect hallucinations.

## Limitations
- The 72.2% consistency rate with human judgments leaves significant room for improvement and potential misalignment in 27.8% of evaluations
- The approach's scalability to more complex visual tasks beyond the 27 subtasks covered remains uncertain
- The method's generalizability across different LVLM architectures and sizes has not been thoroughly validated

## Confidence
**High confidence**: The core methodology of using GPT-4 as an automated judge is well-established and the dataset construction approach is systematic. The hallucination detection capabilities are supported by the evaluation framework design.

**Medium confidence**: The 72.2% human consistency rate is based on the paper's internal validation, but external replication would strengthen this claim. The effectiveness of position balancing in reducing bias needs more rigorous empirical demonstration.

**Low confidence**: The scalability of this approach to more complex visual tasks beyond the 27 subtasks covered, and its generalizability across different LVLM architectures and sizes.

## Next Checks
1. **Annotation sufficiency test**: Conduct a controlled experiment where human evaluators attempt to answer questions using only the textual annotations, comparing accuracy rates to those achieved with actual images.

2. **Position bias quantification**: Run the same pairwise comparisons with fixed positions and measure the magnitude of positional bias before and after implementing the position-swapping method.

3. **Cross-model consistency**: Evaluate the same responses using different LLM judges (e.g., Claude, Gemini) to determine if GPT-4's judgments are consistent with other strong language models or idiosyncratic to GPT-4 specifically.