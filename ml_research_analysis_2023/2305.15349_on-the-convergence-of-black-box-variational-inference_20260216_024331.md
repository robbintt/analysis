---
ver: rpa2
title: On the Convergence of Black-Box Variational Inference
arxiv_id: '2305.15349'
source_url: https://arxiv.org/abs/2305.15349
tags:
- variational
- page
- bbvi
- convex
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first full convergence guarantee for Black-Box
  Variational Inference (BBVI) by showing that BBVI with proximal stochastic gradient
  descent (SGD) converges to an -stationary point under log-smooth and log-strongly
  concave posteriors with a location-scale variational family. The key finding is
  that using nonlinear parameterizations of the variational scale (e.g., softplus)
  breaks strong convexity and leads to suboptimal convergence, whereas proximal SGD
  with linear parameterizations achieves optimal convergence rates.
---

# On the Convergence of Black-Box Variational Inference

## Quick Facts
- arXiv ID: 2305.15349
- Source URL: https://arxiv.org/abs/2305.15349
- Reference count: 40
- This paper provides the first full convergence guarantee for Black-Box Variational Inference (BBVI) by showing that BBVI with proximal stochastic gradient descent (SGD) converges to an -stationary point under log-smooth and log-strongly concave posteriors with a location-scale variational family.

## Executive Summary
This paper provides the first complete convergence analysis for Black-Box Variational Inference (BBVI), demonstrating that BBVI with proximal SGD converges to an -stationary point for log-smooth and log-strongly concave posteriors using location-scale variational families. The key finding is that the choice of parameterization and optimization algorithm significantly impacts convergence speed and robustness. Nonlinear scale parameterizations (e.g., softplus) break strong convexity of the ELBO and lead to suboptimal convergence rates, while proximal SGD with linear parameterizations achieves optimal convergence. The analysis reveals that the dimension dependence of BBVI convergence rate stems from gradient variance bounds, and empirically, proximal SGD outperforms standard implementations on large-scale Bayesian inference problems.

## Method Summary
The paper analyzes BBVI convergence by examining the interplay between variational family choice, parameterization, and optimization algorithm. It proves convergence guarantees for proximal SGD with linear scale parameterization under log-smooth and log-strongly concave posterals, showing optimal convergence rates. The method involves implementing BBVI with location-scale variational families using Cholesky parameterization, comparing linear versus nonlinear scale conditioners (softplus vs linear), and testing proximal SGD variants. The empirical validation includes synthetic problems with known solutions and realistic Bayesian inference problems across multiple datasets, comparing Adam and ProxGen-Adam variants with different parameterizations.

## Key Results
- Proximal SGD with linear scale parameterization achieves optimal convergence rates for BBVI
- Nonlinear scale parameterizations (e.g., softplus) break strong convexity of the ELBO and lead to suboptimal convergence
- The dimension dependence of BBVI convergence rate comes from gradient variance bounds
- Empirically, proximal SGD outperforms standard implementations on large-scale Bayesian inference problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Proximal SGD with linear scale parameterization achieves optimal convergence rates for BBVI
- Mechanism: The linear parameterization preserves strong convexity of the ELBO while proximal steps handle the non-smoothness of the entropy term, enabling fast convergence rates
- Core assumption: The posterior is log-smooth and the variational family is location-scale with linear parameterization
- Evidence anchors:
  - [abstract]: "proximal SGD with linear parameterizations achieves optimal convergence rates"
  - [section 3.1]: "Under the linear parameterization, Domke (2020) has previously shown that the entropic regularizer ‚Ñé is not smooth"
  - [corpus]: Weak - no direct citations found for this specific mechanism
- Break condition: If posterior is not log-smooth or if nonlinear parameterization is used

### Mechanism 2
- Claim: Nonlinear scale parameterizations (e.g., softplus) break strong convexity of the ELBO
- Mechanism: The nonlinear transformation of the scale parameter creates a flat region near the optimal scale, destroying strong convexity even when the posterior is strongly log-concave
- Core assumption: The posterior is strongly log-concave and the conditioner function maps to positive reals
- Evidence anchors:
  - [section 3.1]: "certain algorithm design choices commonly employed in practice, particularly, nonlinear parameterizations of the scale...can result in suboptimal convergence rates"
  - [section 3.1]: "nonlinear scale parameterizations widely used in practice are suboptimal. Most importantly, they provably break strong convexity"
  - [corpus]: Weak - no direct citations found for this specific mechanism
- Break condition: If the posterior is not strongly log-concave or if a linear conditioner is used

### Mechanism 3
- Claim: The dimension dependence of BBVI convergence rate comes from gradient variance bounds
- Mechanism: The gradient variance at optimum scales with ‚àöd for mean-field parameterization and d for full-rank, creating the dimension dependence in convergence rate
- Core assumption: The variational family satisfies Assumption 2 and the gradient variance is bounded as shown in Lemma 3
- Evidence anchors:
  - [section 3.2]: "Theorem 3 implies that, finding an ùúñ-stationary point of the ELBO has ùí™(dùêø2ùëìùúÖùëÄ‚àí1ùúñ‚àí4) complexity for the Cholesky"
  - [section 3.2]: "the dimensional dependence stems from the gradient variance"
  - [corpus]: Weak - no direct citations found for this specific mechanism
- Break condition: If the gradient variance bounds can be improved or if the variational family changes

## Foundational Learning

- Concept: Location-scale variational family
  - Why needed here: The paper's convergence guarantees specifically apply to this family, which includes Gaussian and elliptical variational families
  - Quick check question: What are the two most common parameterizations for the scale matrix in the location-scale family?

- Concept: Reparameterization trick
  - Why needed here: BBVI uses reparameterization gradients, and the paper analyzes how the Jacobian of the reparameterization function affects convergence
  - Quick check question: Why does ‚àáùëìRP(ùùÄ;ùíñ) ‚â† ‚àáùùÄùëìRP(ùùÄ;ùíñ) in BBVI?

- Concept: Strong convexity vs smoothness
  - Why needed here: The paper distinguishes between these properties and shows how different parameterizations affect them differently
  - Quick check question: What is the key difference between a function being ùúá-strongly convex and ùêø-smooth?

## Architecture Onboarding

- Component map: BBVI algorithm ‚Üí stochastic gradient estimator ‚Üí optimization step ‚Üí proximal operator (for proximal SGD) ‚Üí convergence guarantee
- Critical path: Variational family choice ‚Üí parameterization choice ‚Üí optimization algorithm ‚Üí convergence rate
- Design tradeoffs: Linear vs nonlinear parameterization (smoothness vs strong convexity), proximal vs vanilla SGD (robustness vs simplicity), dimension dependence considerations
- Failure signatures: Slow convergence with nonlinear parameterizations, sensitivity to initialization, poor performance on large-scale problems, gradient variance increasing with dimensionality
- First 3 experiments:
  1. Compare convergence rates of proximal SGD vs vanilla SGD with linear parameterization on a synthetic Gaussian posterior
  2. Test the effect of nonlinear conditioners (softplus vs linear) on convergence speed and robustness
  3. Evaluate the dimension dependence by running BBVI on problems of increasing dimensionality

## Open Questions the Paper Calls Out

- Question: Does mean-field BBVI converge with only logarithmic or no explicit dimensional dependence, as conjectured in Conjecture 1?
  - Basis in paper: [explicit] The authors state "Kim et al. (2023) note that the ùí™(‚àöùëë) dimensional dependence empirically appears to be quite loose" and pose this as an open problem.
  - Why unresolved: The current analysis shows ùí™(‚àöùëë) dependence for mean-field, but empirical results suggest this bound is loose and the true dependence may be lower.
  - What evidence would resolve it: Theoretical proof showing ùí™(logùëë) or ùí™(1) dependence for mean-field BBVI, or empirical studies demonstrating convergence rates that match such bounds across multiple dimensions.

- Question: Why does Adam with nonlinear parameterizations converge significantly slower than other variants in practice, despite theoretical convergence guarantees?
  - Basis in paper: [explicit] "for the case of election and buzz, Adam, with the nonlinear parameterization, converges much slower than the alternatives" and "both ProxGen-Adam and Adam with the linear parameterization converge faster than Adam with nonlinear parameterization."
  - Why unresolved: The paper shows theoretical advantages of linear parameterizations but doesn't fully explain the large empirical gap in Adam's performance between linear and nonlinear parameterizations.
  - What evidence would resolve it: Detailed empirical studies comparing different step size schedules, bias corrections, and momentum parameters between linear and nonlinear parameterizations in Adam.

- Question: How do different base distributions in the location-scale family affect the convergence properties of BBVI beyond what is covered by Assumption 1?
  - Basis in paper: [explicit] "Our results are restricted to the location-scale ADVI family" and "We impose light assumptions on the base distributionùúë which are already satisfied by most variational families used in practice."
  - Why unresolved: The paper assumes standardized symmetric base distributions with finite kurtosis, but doesn't explore how alternative base distributions (e.g., heavy-tailed, asymmetric) might impact convergence.
  - What evidence would resolve it: Convergence analysis and empirical evaluation of BBVI with various non-standard base distributions, including heavy-tailed and asymmetric alternatives.

## Limitations

- The convergence guarantees rely on log-smooth and log-strongly concave posteriors, which may not hold for all Bayesian models
- The location-scale variational family, while flexible, may not capture the full posterior structure in complex models
- The analysis depends on gradient variance bounds that may not be tight in practice, particularly for high-dimensional problems
- The theoretical superiority of proximal SGD may come with increased computational overhead per iteration

## Confidence

- High Confidence: The convergence rate guarantees for proximal SGD with linear parameterization (Theorem 3) - these follow directly from the theoretical analysis with clear assumptions and derivations.
- Medium Confidence: The dimension dependence claims (O(d L^2 Œ∫ M^(-1) Œµ^(-4)) for full-rank Cholesky parameterization) - while theoretically derived, the practical impact depends on how well the variance bounds hold in real problems.
- Low Confidence: The superiority of ProxGen-Adam over standard Adam in all practical settings - the empirical results show consistent improvements but the sample size of tested problems is limited.

## Next Checks

1. **Gradient Variance Analysis**: Verify the gradient variance bounds empirically across different problem dimensions and variational parameterizations. Compute the actual gradient variance at convergence for both linear and nonlinear scale parameterizations to confirm the theoretical predictions.

2. **Posterior Property Verification**: Test the log-smooth and log-strongly concave assumptions on real Bayesian inference problems beyond the synthetic Gaussian case. Quantify how violations of these assumptions affect convergence rates in practice.

3. **Parameterization Sensitivity**: Conduct a systematic ablation study varying the conditioner function œï (linear, softplus, other nonlinear functions) while holding all else constant. Measure the impact on the energy surface convexity, gradient variance, and convergence speed to identify the exact mechanisms behind parameterization effects.