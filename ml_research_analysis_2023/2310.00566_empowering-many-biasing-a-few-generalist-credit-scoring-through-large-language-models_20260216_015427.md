---
ver: rpa2
title: 'Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language
  Models'
arxiv_id: '2310.00566'
source_url: https://arxiv.org/abs/2310.00566
tags:
- credit
- llms
- data
- financial
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates large language models (LLMs) on credit scoring
  tasks across 9 tabular datasets with 14K samples, proposing a specialized LLM (CALM)
  fine-tuned on 45K samples. LLMs show potential to address knowledge myopia and task
  isolation compared to traditional expert systems, but biases were found especially
  in sensitive attributes like gender and age.
---

# Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models

## Quick Facts
- arXiv ID: 2310.00566
- Source URL: https://arxiv.org/abs/2310.00566
- Reference count: 40
- The paper introduces CALM, an LLM fine-tuned on credit scoring tasks, showing GPT-4 and ChatGPT outperform open-source LLMs but biases persist in sensitive attributes.

## Executive Summary
This paper evaluates large language models (LLMs) on credit scoring tasks across 9 tabular datasets with 14K samples, introducing CALM, a specialized LLM fine-tuned on 45K samples. The study finds that while closed-source LLMs like GPT-4 and ChatGPT outperform open-source alternatives in zero-shot settings, the latter show promise after fine-tuning. However, significant biases are discovered in sensitive attributes like gender and age, raising concerns about deploying these models in real-world credit decisioning. The research highlights both the potential and limitations of using LLMs for financial risk assessment.

## Method Summary
The study fine-tunes Llama2-chat with LORA on 45K instruction-formatted credit scoring samples to create CALM, then evaluates it against baseline LLMs and expert systems across 9 tabular datasets. The evaluation uses accuracy, F1, and MCC metrics along with bias measures (DI, EOD, AOD). The authors compare closed-source models (GPT-4, ChatGPT) with open-source variants (Bloomz, Vicuna, Llama, Chatglm2) in both zero-shot and fine-tuned settings.

## Key Results
- GPT-4 and ChatGPT consistently outperform open-source LLMs on credit scoring accuracy metrics
- CALM fine-tuning improves open-source LLM performance by 0.03-0.08 F1 score
- Significant biases detected across sensitive attributes, with GPT-4 showing age-related bias and ChatGPT exhibiting gender bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can mitigate knowledge myopia in credit scoring by leveraging broad pretraining knowledge.
- Mechanism: LLMs store general financial knowledge during pretraining, allowing them to contextualize credit decisions beyond narrow task-specific data.
- Core assumption: Pretraining on diverse corpora provides transferable financial insights not present in traditional models.
- Evidence anchors:
  - [abstract] "LLMs stand out by leveraging a two-step approach: pretraining on colossal corpora and fine-tuning on specific tasks."
  - [section] "Given their broad pretraining, can LLMs successfully mitigate the issue of knowledge myopia by efficiently processing the tabular data unique to credit scoring tasks, thereby leveraging broader financial knowledge absent in traditional models?"
  - [corpus] Weak - no direct corpus evidence supporting this mechanism
- Break condition: Pretraining data lacks relevant financial knowledge or fails to capture necessary domain concepts.

### Mechanism 2
- Claim: LLMs can address task isolation by transferring insights across interconnected financial tasks.
- Mechanism: Fine-tuned LLMs recognize patterns across different credit-related tasks (scoring, fraud detection, distress identification) and apply shared insights.
- Core assumption: Financial tasks share underlying patterns that can be learned and transferred.
- Evidence anchors:
  - [abstract] "Skills utilized in Financial Distress Identification... can be instrumental in refining Credit Scoring predictions."
  - [section] "Can LLMs, once fine-tuned with credit scoring datasets, address the problem of task isolation and demonstrate a capability to generalize across diverse yet interconnected, credit-related tasks, taking advantage of potential shared insights?"
  - [corpus] Weak - corpus mentions related papers but no direct evidence of this mechanism
- Break condition: Tasks are too distinct for meaningful knowledge transfer or fine-tuning fails to capture cross-task patterns.

### Mechanism 3
- Claim: Specialized fine-tuning (CALM) enables open-source LLMs to overcome zero-shot limitations on tabular credit data.
- Mechanism: Instruction tuning with domain-specific data teaches LLMs to interpret tabular financial features and make predictions.
- Core assumption: Open-source LLMs lack financial domain knowledge but can learn through targeted instruction tuning.
- Evidence anchors:
  - [section] "We introduce our innovative Credit and Risk Assessment Language Model (CALM). As the first of its kind, CALM demonstrated that LLMs, when judiciously fine-tuned, can effectively bridge the task isolation chasm that traditional models often grapple with."
  - [section] "After fine-tuning, our CALM shows that they can try to solve these tasks."
  - [corpus] Weak - corpus mentions related papers but no direct evidence of this mechanism
- Break condition: Instruction tuning data is insufficient or LLMs cannot learn tabular data interpretation effectively.

## Foundational Learning

- Concept: Tabular data interpretation in NLP
  - Why needed here: Credit scoring datasets are primarily tabular, requiring LLMs to process structured data with meaningful features
  - Quick check question: Can LLMs convert tabular features into meaningful textual representations for processing?

- Concept: Instruction tuning methodology
  - Why needed here: Standard LLMs require adaptation to understand credit scoring tasks through specialized training
  - Quick check question: Does the instruction tuning template preserve task semantics while making data interpretable?

- Concept: Bias detection and mitigation
  - Why needed here: Financial decisions impact real people, requiring careful monitoring of fairness across demographic groups
  - Quick check question: Can we measure disparate impact across sensitive attributes using standard fairness metrics?

## Architecture Onboarding

- Component map:
  Raw tabular datasets -> Instruction template generation -> Training/evaluation splits -> Base LLM (Llama2-chat) -> LORA fine-tuning -> CALM deployment -> Accuracy, F1, MCC metrics + bias metrics (DI, EOD, AOD) -> Comparison with open-source LLMs vs closed-source vs SOTA expert systems

- Critical path:
  1. Construct instruction-tuned datasets from 9 credit/risk datasets
  2. Fine-tune Llama2-chat with LORA on 45K samples
  3. Evaluate CALM and baselines on held-out test sets
  4. Analyze bias across sensitive attributes

- Design tradeoffs:
  - Instruction format: Table-based (for meaningless symbols) vs description-based (for semantic features)
  - Fine-tuning strategy: LORA parameter-efficient tuning vs full fine-tuning
  - Dataset sampling: Balanced training vs maintaining real-world class distributions

- Failure signatures:
  - High Miss values indicate LLMs cannot understand or refuse to answer questions
  - MCC near zero suggests random guessing on imbalanced datasets
  - Negative MCC indicates systematic prediction reversal

- First 3 experiments:
  1. Test zero-shot performance on German credit dataset with all baseline LLMs
  2. Fine-tune CALM on Credit Card Fraud and ccFraud datasets, evaluate on test sets
  3. Measure bias metrics (EOD, AOD) on German dataset across gender, age, and foreign worker attributes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can larger-parameter open-source LLMs, or those trained on more extensive financial datasets, match or exceed the performance of GPT-4 on credit scoring tasks?
- Basis in paper: [inferred] The authors note that their evaluation of smaller-parameter open-source LLMs may underestimate the potential of LLMs in general.
- Why unresolved: The paper's experiments were constrained by computational resources, limiting the evaluation to smaller-parameter models.
- What evidence would resolve it: Benchmarking larger-parameter open-source LLMs or those fine-tuned on extensive financial datasets against GPT-4 on the same credit scoring tasks.

### Open Question 2
- Question: How can instruction-tuning datasets be designed to improve the interpretability of LLM predictions in credit scoring?
- Basis in paper: [explicit] The authors acknowledge the lack of interpretable datasets as a limitation, noting the need for significant professional labeling.
- Why unresolved: The paper does not provide a solution for creating interpretable instruction-tuning datasets.
- What evidence would resolve it: Development and evaluation of interpretable instruction-tuning datasets for credit scoring tasks, demonstrating improved model interpretability.

### Open Question 3
- Question: What are the most effective strategies for mitigating biases in LLMs for credit scoring, especially regarding sensitive attributes like age, gender, and foreign status?
- Basis in paper: [explicit] The authors identify biases in LLMs for credit scoring and emphasize the need for unbiased evaluations.
- Why unresolved: The paper does not propose specific bias mitigation strategies.
- What evidence would resolve it: Implementation and evaluation of bias mitigation techniques in LLMs for credit scoring, demonstrating reduced bias and fairer outcomes.

## Limitations

- Limited sample size (14K samples) raises questions about generalizability to real-world credit scoring scenarios
- Modest performance improvements (0.03-0.08 F1) suggest instruction tuning may not fully address tabular data interpretation challenges
- Significant biases discovered in sensitive attributes without comprehensive mitigation strategies proposed

## Confidence

**High Confidence:**
- LLMs outperform traditional expert systems on credit scoring accuracy metrics
- Closed-source LLMs (GPT-4, ChatGPT) consistently outperform open-source alternatives in zero-shot settings
- CALM fine-tuning improves open-source LLM performance on credit tasks

**Medium Confidence:**
- LLMs can effectively address knowledge myopia through pretraining on diverse corpora
- Instruction tuning enables meaningful cross-task generalization in financial domains
- The proposed bias metrics adequately capture fairness concerns in credit decisions

**Low Confidence:**
- The extent to which CALM's improvements generalize to larger datasets beyond the evaluated samples
- Whether the observed biases represent fundamental limitations of LLMs or can be mitigated through better fine-tuning approaches
- The practical deployment viability of CALM given the computational resources required for fine-tuning and inference

## Next Checks

1. **Scale Validation Test**: Evaluate CALM on a larger credit scoring dataset (e.g., full Lending Club dataset with 1.3M samples) to assess whether performance improvements scale with data volume and whether bias patterns persist or amplify at larger scales.

2. **Bias Mitigation Experiment**: Implement and test bias mitigation techniques during fine-tuning (such as adversarial debiasing or reweighting strategies) to determine if CALM's performance can be maintained while reducing disparate impact across sensitive attributes.

3. **Cross-Domain Transfer Assessment**: Evaluate CALM's performance on credit scoring tasks from different geographical regions and regulatory environments (e.g., European vs Asian datasets) to test the model's ability to generalize beyond the specific cultural and legal contexts represented in the current datasets.