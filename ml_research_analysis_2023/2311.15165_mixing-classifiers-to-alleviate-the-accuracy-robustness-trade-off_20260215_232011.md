---
ver: rpa2
title: Mixing Classifiers to Alleviate the Accuracy-Robustness Trade-Off
arxiv_id: '2311.15165'
source_url: https://arxiv.org/abs/2311.15165
tags:
- robust
- robustness
- accuracy
- classifier
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes mixing classifiers to improve the accuracy-robustness
  trade-off in deep neural networks. The key idea is to combine the outputs of a standard,
  high-accuracy model with those of a robust model via a convex combination.
---

# Mixing Classifiers to Alleviate the Accuracy-Robustness Trade-Off

## Quick Facts
- arXiv ID: 2311.15165
- Source URL: https://arxiv.org/abs/2311.15165
- Reference count: 40
- Key outcome: Mixing standard and robust classifiers improves accuracy-robustness trade-off

## Executive Summary
This paper proposes a method to improve the accuracy-robustness trade-off in deep neural networks by mixing the outputs of a standard, high-accuracy model with those of a robust model. The mixing is performed via a convex combination of their probability outputs, controlled by a parameter α. Theoretical results show that under certain conditions, the mixed classifier inherits the certified robustness of the robust base model. Empirically, on CIFAR-10, the method achieves high clean accuracy while maintaining good adversarial robustness, notably improving upon robust-only models.

## Method Summary
The method mixes a standard high-accuracy model g(·) with a robust model h(·) using a convex combination of their probability outputs: hα(x) = log((1-α)g(x) + αh(x)), where α ∈ [0,1] controls the trade-off. The robust model can be built using techniques like adversarial training or randomized smoothing. The approach requires no additional training - it uses pre-trained models and applies the mixing function at inference time. Theoretical analysis shows that for α ≥ 0.5, the mixed classifier inherits certified robustness from the robust base model under certain conditions.

## Key Results
- The mixed classifier achieves over 30% accuracy under targeted attacks while maintaining clean accuracy above 50% on CIFAR-10
- Mixing probabilities instead of logits provides better accuracy-robustness trade-offs
- The method noticeably improves upon robust-only models, which typically suffer from significant accuracy drops

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The mixed classifier inherits certified robustness from the robust base model when α ≥ 0.5
- Mechanism: When α ≥ 0.5, the robust model dominates the convex combination, and its certified robustness margin can overcome vulnerabilities from the standard model
- Core assumption: The robust base model has a certifiable robustness margin and radius
- Evidence anchors: The abstract states the mixed classifier inherits certified robustness under certain conditions, and Theorems 1 and 2 show hα inherits robustness when α ∈ [1/2, 1]
- Break condition: If α < 0.5, the standard model has too much influence and its lack of robustness cannot be overcome

### Mechanism 2
- Claim: Using probabilities instead of logits provides better accuracy-robustness trade-offs
- Mechanism: Probabilities are bounded in [0,1], allowing the robust model's confidence to correct the standard model's mistakes under attack when α ≥ 0.5
- Core assumption: Both models output probabilities in [0,1]
- Evidence anchors: The abstract describes mixing output probabilities, and the paper explains that bounded probabilities enable h(·) to overcome g(·)'s vulnerabilities when α ≥ 0.5
- Break condition: If logits are used instead of probabilities, unbounded values prevent robust corrections

### Mechanism 3
- Claim: The confidence difference between correct predictions under attack and incorrect predictions on clean data in h(·) is key to improving the trade-off
- Mechanism: When h(·) is confident in correct predictions under attack but unconfident in incorrect predictions on clean data, the mixed classifier can leverage this to correct mistakes while maintaining high accuracy
- Core assumption: The robust base model exhibits different confidence levels for correct vs. incorrect predictions
- Evidence anchors: The paper shows in Table I that h(·) has high robustness margins for correct predictions under attack (0.767) but low confidence for incorrect predictions on clean data (0.434)
- Break condition: If h(·) has similar confidence levels for correct and incorrect predictions, the mixed classifier cannot leverage this difference

## Foundational Learning

- Concept: Adversarial training and its limitations
  - Why needed here: Understanding why standard models are vulnerable to adversarial attacks and why robust models are needed as base classifiers
  - Quick check question: What is the main trade-off that adversarial training introduces between clean accuracy and adversarial robustness?

- Concept: Randomized smoothing and its robustness guarantees
  - Why needed here: The robust base model h(·) can be built using randomized smoothing, and understanding its certified robustness is crucial for the theoretical guarantees
  - Quick check question: How does randomized smoothing achieve certified robustness, and what is the relationship between the smoothing variance and the certified radius?

- Concept: Lipschitz continuity and its role in robustness certification
  - Why needed here: The theoretical results rely on the Lipschitz continuity of the robust base model to derive certified robustness for the mixed classifier
  - Quick check question: What is the relationship between a function's Lipschitz constant and its robustness to adversarial perturbations?

## Architecture Onboarding

- Component map:
  - Pre-trained standard ResNet18 (high accuracy) -> Mixing function -> Mixed classifier output
  - Pre-trained robust ResNet18 (adversarially trained or smoothed) -> Mixing function -> Mixed classifier output
  - Mixing parameter α controls contribution of each model

- Critical path:
  1. Load pre-trained standard and robust ResNet18 models
  2. Implement mixing function hα(x) = log((1-α)g(x) + αh(x))
  3. Compute certified radii using Theorems 1 and 2
  4. Evaluate accuracy and robustness on CIFAR-10 test data

- Design tradeoffs:
  - Choice of α: Higher α improves robustness but may reduce clean accuracy
  - Selection of base classifiers: Trade-off between g(·)'s accuracy and h(·)'s robustness
  - Use of probabilities vs. logits: Probabilities enable better mixing but may require additional computation

- Failure signatures:
  - α too small (< 0.5): Mixed classifier inherits vulnerabilities from g(·)
  - Poor choice of h(·): Lack of certified robustness or insufficient confidence under attack
  - Using logits instead of probabilities: Unbounded values prevent robust corrections

- First 3 experiments:
  1. Implement mixing function with fixed α=0.5 and evaluate accuracy-robustness trade-off
  2. Vary α from 0 to 1 and plot clean accuracy vs. attacked accuracy curves
  3. Compare certified radii using Lipschitz-based vs. randomized smoothing-based bounds

## Open Questions the Paper Calls Out

- Question: How does the performance of the mixed classifier scale with the size and complexity of the dataset beyond CIFAR-10 and CIFAR-100?
- Basis in paper: The paper only demonstrates results on CIFAR-10 and mentions CIFAR-100 and ImageNet-1k as harder tasks where the accuracy-robustness gap is larger
- Why unresolved: The paper does not provide experimental results on larger or more complex datasets like ImageNet-1k, which would better demonstrate the scalability and practical utility of the method
- What evidence would resolve it: Experimental results showing the performance of the mixed classifier on larger datasets like ImageNet-1k, comparing it to both standard and robust models, would demonstrate its effectiveness in more realistic scenarios

- Question: Can the theoretical certified robustness bounds be further tightened for specific types of robust base classifiers beyond Lipschitz and randomized smoothing models?
- Basis in paper: The paper derives certified robustness bounds for Lipschitz continuous models (Theorem 1) and randomized smoothing models (Theorem 2), but does not explore other types of robust models
- Why unresolved: The paper only considers two specific types of robust base classifiers and their associated theoretical bounds
- What evidence would resolve it: Deriving and proving certified robustness bounds for other types of robust base classifiers, such as those based on adversarial training or other defense mechanisms, would demonstrate the generality and tightness of the theoretical results

- Question: How does the choice of the mixing parameter α affect the trade-off between accuracy and robustness in the presence of adaptive attacks that specifically target the mixed classifier structure?
- Basis in paper: The paper discusses the effect of α on the accuracy-robustness trade-off but only considers standard attacks targeting the individual base classifiers
- Why unresolved: The paper does not investigate the behavior of the mixed classifier under adaptive attacks that exploit its specific structure
- What evidence would resolve it: Experimental results comparing the performance of the mixed classifier under both standard and adaptive attacks for different values of α would reveal the optimal trade-off and the effectiveness of the method against more sophisticated attacks

## Limitations

- The method's performance on larger, more complex datasets like ImageNet-1k remains unverified
- Theoretical certified robustness bounds are derived only for Lipschitz and randomized smoothing models, not other types of robust classifiers
- The behavior under adaptive attacks specifically targeting the mixed classifier structure is not investigated

## Confidence

- High confidence in the core theoretical mechanism regarding inheritance of certified robustness when α ≥ 0.5
- Medium confidence in experimental evidence showing accuracy-robustness trade-offs due to unspecified experimental details
- Low confidence in Mechanism 3 regarding confidence difference as key factor because referenced Table I is not accessible

## Next Checks

1. Verify the inheritance of certified robustness by implementing the mixing function with α = 0.5 and computing certified radii using both Theorem 1 and randomized smoothing bounds
2. Conduct ablation studies comparing mixing with logits vs. probabilities to quantify the performance difference
3. Test the method on additional datasets (e.g., CIFAR-100, Tiny ImageNet) to assess generalization beyond CIFAR-10