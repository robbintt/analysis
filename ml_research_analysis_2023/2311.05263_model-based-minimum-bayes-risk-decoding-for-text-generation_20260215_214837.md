---
ver: rpa2
title: Model-Based Minimum Bayes Risk Decoding for Text Generation
arxiv_id: '2311.05263'
source_url: https://arxiv.org/abs/2311.05263
tags:
- mbmbr
- sampling
- probability
- href
- model-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Model-Based Minimum Bayes Risk (MBMBR) decoding,
  which uses the model probability itself as an estimate of the probability distribution
  instead of the Monte Carlo estimate commonly used in MBR decoding. The key innovation
  is replacing the empirical distribution with a model-based distribution that is
  proportional to the true model probability over the sampled hypotheses.
---

# Model-Based Minimum Bayes Risk Decoding for Text Generation

## Quick Facts
- arXiv ID: 2311.05263
- Source URL: https://arxiv.org/abs/2311.05263
- Reference count: 40
- Key outcome: MBMBR decoding improves text generation quality by using model probability directly instead of Monte Carlo estimates

## Executive Summary
This paper introduces Model-Based Minimum Bayes Risk (MBMBR) decoding, a novel approach that replaces the Monte Carlo probability estimates used in traditional MBR decoding with the model's own probability estimates. The method directly uses P(y|x) from the model rather than empirical frequencies from sampled hypotheses, resulting in lower KL divergence from the true model probability. The authors demonstrate across four datasets and multiple text generation tasks that MBMBR outperforms traditional MBR decoding, with the length-normalized variant (MBMBRL) showing the best overall performance.

## Method Summary
MBMBR modifies the standard MBR decoding framework by using the model probability P(y) directly instead of empirical frequency estimates. For a sampled hypothesis set Href, instead of computing q(y) = 1/|Href| for all y in Href, MBMBR uses q(y) ∝ P(y). This creates a distribution over Href that is proportional to the true model probability. The authors also introduce MBMBRL, which applies length normalization to address the observed negative correlation between sequence length and model probability. The method is computationally similar to MBR but requires only the model's ability to compute P(y) for sampled sequences.

## Key Results
- MBMBR outperforms MBR on WMT'19 De-En machine translation with epsilon sampling (35.86 vs 35.77 BLEU)
- MBMBRL shows best overall performance across multiple tasks and datasets
- MBMBR improves performance on both domain-specific models and large language models with prompting
- The method is easy to implement and requires no additional training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-based estimate reduces KL divergence compared to Monte Carlo estimate
- Mechanism: By using model probability directly instead of empirical frequency, the estimator better approximates the true distribution within the sampled set
- Core assumption: Model probability P(y) is available and reliable for sampled hypotheses
- Evidence anchors:
  - [abstract] "analytically that this model-based estimate has lower KL divergence from the true model probability than the Monte Carlo estimate"
  - [section 4] "Theorem 4.1. The model-based distribution minimizes the Kullbuck-Leiber divergence over the probability distribution with support restricted to Href"
  - [corpus] Weak evidence - no direct citations on KL divergence claims
- Break condition: If model probability is unavailable or unreliable for sampled hypotheses

### Mechanism 2
- Claim: MBMBR improves text generation quality over MBR
- Mechanism: By using more accurate probability estimates, MBMBR better approximates the true MBR objective, leading to better hypothesis selection
- Core assumption: The true model probability P(y) is a better estimate of the true distribution than empirical frequency
- Evidence anchors:
  - [abstract] "empirically demonstrate this across four datasets (WMT'19 De-En, IWSLT'17 Fr-En, XSum, MS COCO)"
  - [section 5] "MBMBR outperforms MBR in several text generation tasks, both with encoder-decoder models and with large language models with prompting"
  - [corpus] Weak evidence - no direct citations on MBR quality improvements
- Break condition: If sampling quality is poor or model probability is unreliable

### Mechanism 3
- Claim: Length normalization mitigates length bias in MBMBR
- Mechanism: Compensates for model bias toward shorter sequences by adjusting probability estimates with sequence length
- Core assumption: Model probability P(y) is inversely correlated with sequence length
- Evidence anchors:
  - [section 3] "we observe that there is a negative correlation between the sentence length and the model probability"
  - [section 5.2.1] "MBMBR has decreased the relative length by about 10% compared to MBR"
  - [corpus] Weak evidence - no direct citations on length normalization techniques
- Break condition: If model doesn't exhibit length bias or length normalization parameters are poorly chosen

## Foundational Learning

- KL divergence and Jensen-Shannon divergence
  - Why needed here: To quantify how well the model-based estimate approximates the true model probability
  - Quick check question: If distribution A has KL divergence 0.5 from P and distribution B has KL divergence 0.3 from P, which is closer to P?

- Minimum Bayes Risk decoding framework
  - Why needed here: MBMBR is a variant of MBR, so understanding the baseline is crucial
  - Quick check question: What are the two main approximations used in MBR decoding?

- Sampling algorithms and their biases
  - Why needed here: Different sampling strategies affect both MBR and MBMBR performance
  - Quick check question: How does epsilon sampling differ from ancestral sampling?

## Architecture Onboarding

- Component map:
  Text generation model (provides P(y|x)) -> Sampling module (generates Href set) -> Utility function (measures hypothesis quality) -> MBMBR decoder (computes weighted scores and selects best hypothesis) -> Optional: Length normalization module

- Critical path:
  1. Sample hypotheses Href using chosen sampling strategy
  2. Get model probabilities P(y) for all y in Href
  3. Compute weighted scores: u(h, y) * P(y) for all h in Hcand, y in Href
  4. Select hypothesis with highest expected utility

- Design tradeoffs:
  - Sampling size vs. computational cost (quadratic in |H|)
  - Model probability reliability vs. empirical estimates
  - Length normalization strength vs. maintaining diversity

- Failure signatures:
  - Poor performance when model probability estimates are unreliable
  - Length bias issues when using vanilla MBMBR without normalization
  - Degradation with low-quality samples

- First 3 experiments:
  1. Verify KL divergence reduction: Compare empirical vs. model-based estimates on small sample set
  2. Baseline comparison: Run MBR vs. MBMBR on a simple MT task with epsilon sampling
  3. Length bias check: Analyze correlation between sequence length and model probability on sample set

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following questions arise from the content and methodology:

1. How does the performance of MBMBR compare to MBR when using different utility functions beyond BERTScore, such as lexical overlap metrics or other learned metrics?

2. Can MBMBR be effectively applied to other sequence generation tasks beyond those explored in the paper, such as code generation or dialogue systems?

3. How does the computational efficiency of MBMBR compare to MBR when scaling to larger models or longer sequences?

## Limitations
- The method inherits MBR's computational expense of O(|H|²) utility function evaluations
- Performance depends critically on the quality of the sampled hypothesis set
- Length normalization introduces hyperparameters that aren't thoroughly explored
- Theoretical KL divergence advantages are not directly validated empirically

## Confidence

**High confidence** in the implementation approach and experimental results showing MBMBR outperforms MBR across multiple tasks and datasets.

**Medium confidence** in the theoretical claims about KL divergence reduction, as the practical significance isn't fully established through empirical measurement.

**Medium confidence** in the generality of the length normalization approach, as the underlying length bias may be model-specific rather than universal.

## Next Checks

1. **Direct KL divergence measurement**: For a small validation set, compute and compare the KL divergence between Monte Carlo estimates and model-based estimates against the true model distribution.

2. **Sampling strategy ablation study**: Run experiments with different sampling strategies (ancestral sampling, top-k sampling, nucleus sampling) and sample sizes to determine how robust MBMBR is to sampling quality variations.

3. **Length bias analysis across domains**: Measure the correlation between sequence length and model probability across all tested domains (translation, summarization, image captioning, data-to-text) to determine if length normalization is universally needed or domain-specific.