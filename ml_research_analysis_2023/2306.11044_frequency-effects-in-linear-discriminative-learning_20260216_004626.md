---
ver: rpa2
title: Frequency effects in Linear Discriminative Learning
arxiv_id: '2306.11044'
source_url: https://arxiv.org/abs/2306.11044
tags:
- learning
- frequency
- word
- words
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Discriminative Lexicon Model (DLM) models lexical processing
  via linear mappings between word forms and meanings. Previous approaches either
  use frequency-agnostic endstate learning (EL), which is computationally efficient
  but ignores word frequency, or computationally expensive incremental learning (WHL),
  which captures frequency effects.
---

# Frequency effects in Linear Discriminative Learning

## Quick Facts
- arXiv ID: 2306.11044
- Source URL: https://arxiv.org/abs/2306.11044
- Reference count: 16
- Key outcome: Introduces Frequency-Informed Learning (FIL), a computationally efficient method that captures word frequency effects in the Discriminative Lexicon Model while approximating incremental learning.

## Executive Summary
The Discriminative Lexicon Model (DLM) models lexical processing through linear mappings between word forms and meanings. Previous approaches either used computationally efficient but frequency-agnostic endstate learning (EL) or computationally expensive incremental learning (WHL) that captured frequency effects. This paper introduces Frequency-Informed Learning (FIL), which approximates incremental learning while being computationally much cheaper. FIL shows high token-accuracy despite low type-accuracy, indicating it processes most encountered words correctly. The method successfully predicts reaction times in Dutch and models priming effects in Mandarin Chinese better than EL.

## Method Summary
The paper introduces Frequency-Informed Learning (FIL) as a computationally efficient alternative to incremental learning for modeling frequency effects in the Discriminative Lexicon Model. FIL works by weighting form and meaning vectors by frequency before applying a closed-form matrix solution. The method creates frequency-weighted matrices by scaling each word's cue and semantic vectors by the square root of its frequency, then solves for the mapping using standard linear algebra operations including the Cholesky decomposition. FIL is evaluated on Dutch Lexicon Project data for reaction time prediction and on Mandarin Chinese data for priming effects, with comparisons to both endstate learning and incremental learning approaches.

## Key Results
- FIL achieves high token-accuracy despite low type-accuracy, showing it processes most encountered words correctly
- FIL better predicts reaction times in the Dutch Lexicon Project compared to endstate learning
- FIL models priming effects in Mandarin Chinese more accurately than endstate learning
- Comparisons with incremental learning on CHILDES data show FIL loses some order-based nuances but captures similar patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FIL approximates incremental learning while being computationally cheaper by weighting form and meaning vectors by frequency before applying the closed-form matrix solution.
- Mechanism: The method creates frequency-weighted matrices by scaling each word's cue and semantic vectors by the square root of its frequency, then solves for the mapping using standard linear algebra.
- Core assumption: Scaling by frequency is equivalent to repeating words proportionally to their frequency in the training data.
- Evidence anchors:
  - We can create two new matrices Cf and Sf where the cue and semantic vectors of the wordforms are repeated according to their frequency count fi.
  - We can first weigh C and S with the pertinent frequencies to obtain ˜C and ˜S. We can then use the closed-form solution to obtain frequency-informed mappings.
- Break condition: If frequency scaling doesn't preserve the relative importance of word forms, or if the square root transformation is inappropriate for the frequency distribution.

### Mechanism 2
- Claim: FIL achieves high token-accuracy even with low type-accuracy because frequent words dominate token counts.
- Mechanism: Since accuracy is calculated across tokens rather than types, words with high frequency contribute disproportionately to overall accuracy, compensating for poor learning of rare words.
- Core assumption: Most encountered words in natural language are high-frequency words, so token-based accuracy better reflects real-world performance.
- Evidence anchors:
  - FIL shows a relatively low type- but high token-accuracy, demonstrating that the model is able to process most word tokens encountered by speakers in daily life correctly.
  - Generally, FIL and WHL based on untransformed frequencies perform the best... while EL clearly performs the worst.
- Break condition: If the language distribution changes dramatically (e.g., domain-specific jargon), or if rare words become more important in the task.

### Mechanism 3
- Claim: FIL better models human reaction times than endstate learning because it captures the relationship between frequency and learning difficulty.
- Mechanism: FIL's frequency-weighted approach creates mappings where higher-frequency words are learned more accurately, matching the empirical finding that frequent words elicit faster responses in lexical decision tasks.
- Core assumption: The correlation between predicted and target semantic vectors (1-r) is a valid proxy for processing difficulty.
- Evidence anchors:
  - We found that 1 − r based on FIL provided much improved prediction accuracy compared to 1 − r based on EL.
  - This suggests that FIL successfully filters usage through discriminative learning to obtain estimates of how well the meanings of words are understood.
- Break condition: If individual differences in word knowledge or contextual factors dominate reaction time variance beyond what frequency captures.

## Foundational Learning

- Linear algebra and matrix operations
  - Why needed here: The core method relies on matrix multiplication, pseudo-inverses, and the Cholesky decomposition to compute mappings between form and meaning.
  - Quick check question: Can you explain why the pseudo-inverse is used to solve the equation CF = S when C is not square?

- Error-driven learning and the Widrow-Hoff rule
  - Why needed here: Understanding incremental learning is crucial for comparing FIL to traditional error-driven approaches and for grasping why frequency matters.
  - Quick check question: How does the Widrow-Hoff learning rule update weights after each learning event?

- Generalized Additive Models (GAMs) for modeling behavioral data
  - Why needed here: The paper uses GAMs to analyze reaction time data and model non-linear relationships between predictors and outcomes.
  - Quick check question: What is the advantage of using a GAM over a standard linear model when analyzing reaction time data?

## Architecture Onboarding

- Component map:
  - Form representation: High-dimensional cue matrices (e.g., trigrams) encoding word forms
  - Meaning representation: Semantic vectors (e.g., fasttext embeddings) encoding word meanings
  - FIL computation: Frequency-weighted matrices and closed-form solution for mapping matrices
  - Evaluation: Accuracy@1 and accuracy@k metrics, plus behavioral modeling with GAMs
  - Data pipeline: Frequency counts, word embeddings, and cue matrix generation

- Critical path:
  1. Load word forms, frequencies, and semantic embeddings
  2. Generate form cue matrix (e.g., trigrams)
  3. Apply FIL: weight matrices by frequency, solve for mapping using Cholesky decomposition
  4. Evaluate accuracy using correlation with target semantics
  5. Use predicted semantics to model behavioral data (e.g., reaction times)

- Design tradeoffs:
  - High-dimensional form vectors improve accuracy but increase computational cost
  - Token-based vs. type-based accuracy evaluation: token better reflects real-world use but masks poor performance on rare words
  - Using FIL sacrifices order information for computational efficiency

- Failure signatures:
  - Low accuracy despite high frequency weighting: suggests poor form-meaning alignment or insufficient dimensionality
  - High type-accuracy but low token-accuracy: indicates model overfits to rare words
  - Poor correlation between FIL and behavioral data: suggests frequency alone doesn't capture all relevant processing factors

- First 3 experiments:
  1. Implement FIL on a small dataset (e.g., 2000 Dutch words) and compare accuracy@1 to endstate learning
  2. Vary the dimensionality of form vectors (bigrams vs. trigrams) and measure impact on accuracy
  3. Use FIL-predicted semantics to model reaction times in a small lexical decision dataset and compare to frequency-only models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can individual differences in word frequency exposure be modeled within the Discriminative Lexicon Model?
- Basis in paper: The paper discusses that community-based frequency counts may not accurately reflect individual speakers' experiences, and that individual differences in word knowledge could explain the variance in reaction times, particularly for low-frequency words.
- Why unresolved: The paper acknowledges this limitation but does not provide a concrete method for incorporating individual frequency data into the model.
- What evidence would resolve it: Studies comparing model predictions using individual versus community frequency data, or models that can adapt to individual frequency profiles.

### Open Question 2
- Question: Can Frequency-Informed Learning be extended to model the effects of word order and burstiness in language acquisition?
- Basis in paper: The paper compares FIL to incremental learning on child-directed speech data, finding that FIL loses some order-based nuances captured by incremental learning, particularly for words with bursty frequency distributions.
- Why unresolved: The paper does not propose a method to incorporate order information into FIL while maintaining its computational efficiency.
- What evidence would resolve it: Development and testing of a hybrid model that combines FIL's efficiency with incremental learning's sensitivity to order, or empirical studies showing the impact of order effects on language acquisition.

### Open Question 3
- Question: What is the optimal form of semantic representation for maximizing accuracy in the Discriminative Lexicon Model?
- Basis in paper: The paper uses fasttext embeddings for semantic representation but notes that these typically consist of a single embedding per unique word form, which may not capture the multiple senses of high-frequency words. The paper also suggests that the choice of semantic representation could impact the model's ability to predict reaction times and account for variance.
- Why unresolved: The paper does not explore alternative semantic representations or compare their performance to fasttext embeddings.
- What evidence would resolve it: Empirical studies comparing the performance of different semantic representations (e.g., sense-specific embeddings, contextualized embeddings) in the Discriminative Lexicon Model.

## Limitations
- The equivalence between FIL and incremental learning is demonstrated only through accuracy metrics rather than direct comparison of learning trajectories
- Behavioral modeling results show improved fit over EL, but alternative explanations for the improved predictions are not ruled out
- The method's performance on languages with dramatically different frequency distributions has not been tested

## Confidence
- Confidence: Medium - The claim that FIL approximates incremental learning rests on the assumption that frequency scaling preserves the order-based learning dynamics
- Confidence: High - The computational efficiency advantage of FIL over WHL is well-established through explicit runtime comparisons
- Confidence: Low - The causal mechanisms linking FIL's frequency weighting to human processing are not fully explored

## Next Checks
1. Track and compare the weight updates and error patterns during FIL versus WHL learning to identify which specific order-based nuances are lost
2. Test FIL on languages with dramatically different frequency distributions to verify generalizability beyond Dutch and Mandarin
3. Systematically examine model performance on low-frequency words using both accuracy metrics and behavioral predictions to determine whether high token-accuracy masks systematic failures on rare vocabulary