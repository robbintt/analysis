---
ver: rpa2
title: Variance-Reduced Gradient Estimation via Noise-Reuse in Online Evolution Strategies
arxiv_id: '2304.12180'
source_url: https://arxiv.org/abs/2304.12180
tags:
- gradient
- nres
- self
- loss
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Noise-Reuse Evolution Strategies (NRES), an
  online evolution strategies method that reduces variance by reusing the same noise
  across the entire episode rather than resampling in each truncation window. The
  authors prove NRES has the lowest variance among a class of generalized persistent
  evolution strategies estimators.
---

# Variance-Reduced Gradient Estimation via Noise-Reuse in Online Evolution Strategies

## Quick Facts
- arXiv ID: 2304.12180
- Source URL: https://arxiv.org/abs/2304.12180
- Reference count: 40
- Primary result: NRES converges faster than AD and ES methods in wall-clock time and unroll steps across dynamical systems, meta-training learned optimizers, and reinforcement learning tasks.

## Executive Summary
This paper introduces Noise-Reuse Evolution Strategies (NRES), an online evolution strategies method that reduces gradient estimation variance by reusing the same noise vector across an entire episode rather than resampling in each truncation window. The authors prove that NRES achieves the lowest variance among a class of generalized persistent evolution strategies estimators. Empirically, NRES demonstrates faster convergence than existing methods including automatic differentiation and other ES approaches across three diverse applications: learning dynamical systems, meta-training learned optimizers, and reinforcement learning. NRES is particularly effective when dealing with chaotic or non-differentiable loss surfaces where AD methods struggle.

## Method Summary
NRES is an online evolution strategies method that estimates gradients by convolving the loss with Gaussian noise, but critically reuses the same noise vector throughout the entire episode. This differs from existing methods like PES which resample noise for each truncation window. The method maintains unbiased gradient estimates while achieving variance reduction by sharing noise across the full horizon. Each worker unrolls W steps with the same noise vector, enabling parallelization across N workers while maintaining lower latency than FullES. The approach is specifically designed for unrolled computation graphs where the same parameters are applied repeatedly, making it suitable for applications like learning dynamical systems, meta-training learned optimizers, and reinforcement learning.

## Key Results
- NRES converges faster than AD and ES methods in terms of wall-clock time and unroll steps
- In the Lorenz system task, NRES reached the lowest test loss fastest among all methods
- For meta-training learned optimizers, NRES achieved the lowest loss value fastest with less computation time than PES and FullES
- In reinforcement learning, NRES solved Mujoco tasks with fewer environment steps than alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NRES achieves lower variance than PES by reusing the same noise vector across the entire episode instead of resampling per truncation window.
- Mechanism: The smoothing objective becomes identical to FullES when noise is reused for the full horizon, but updates occur every W steps instead of T steps, giving both variance reduction and lower latency.
- Core assumption: The gradient contributions from different truncation windows are aligned enough that the inequality in Theorem 8 holds.
- Evidence anchors:
  - [abstract] "NRES has the lowest variance among a class of generalized persistent evolution strategies estimators."
  - [section 4] "By sharing the same noise over the entire horizon, the smoothing objective of NRES becomes the same as that of FullES."
  - [corpus] Weak: corpus neighbors do not discuss noise-reuse variance reduction specifically.
- Break condition: If gradient contributions from different truncation windows are not aligned, NRES may not achieve variance reduction over FullES.

### Mechanism 2
- Claim: NRES is unbiased relative to the gradient of the full-episode smoothed loss function.
- Mechanism: By reusing the same noise vector for the entire episode, the finite difference estimate captures the true gradient of the full-episode smoothed objective, unlike TES which only captures partial windows.
- Core assumption: The noise vector is sampled once at the start and reused consistently without drift.
- Evidence anchors:
  - [section 3] "NRES only samples noise once at the beginning of an episode, and reuses the noise for the full episode."
  - [section 4] "NRES can be thought of as the online counterpart to the offline algorithm FullES."
  - [corpus] Weak: no direct corpus evidence of bias correction via noise reuse.
- Break condition: If noise is resampled mid-episode or applied inconsistently, bias is reintroduced.

### Mechanism 3
- Claim: NRES reduces computational latency by requiring only O(W) steps per gradient update instead of O(T).
- Mechanism: Each worker unrolls W steps with the same noise vector, enabling parallelization across N workers while maintaining the same total number of unroll steps as FullES but with much lower wall-clock time.
- Core assumption: The environment or simulation supports parallelization of independent workers.
- Evidence anchors:
  - [abstract] "NRES converges faster than existing AD and ES methods in terms of wall-clock time and number of unroll steps."
  - [section 4] "A single NRES runs only 2W steps for each gradient estimate, thus allowing it to be used online and to incur lower latency between updates."
  - [corpus] Weak: no corpus evidence of latency comparison with FullES.
- Break condition: If parallelization is not possible (e.g., CPU-only simulation), latency advantage disappears.

## Foundational Learning

- Concept: Evolution Strategies (ES) gradient estimation via Gaussian smoothing
  - Why needed here: NRES is an ES method that estimates gradients by convolving the loss with Gaussian noise
  - Quick check question: What is the form of the gradient estimator for a Gaussian-smoothed loss function?

- Concept: Unrolled computation graphs (UCGs) and their challenges
  - Why needed here: NRES is designed specifically for UCGs where the same parameters are applied repeatedly
  - Quick check question: Why do AD methods struggle with chaotic or discontinuous loss surfaces in UCGs?

- Concept: Variance reduction techniques in stochastic optimization
  - Why needed here: NRES achieves variance reduction by reusing noise instead of resampling, which is a form of variance reduction
  - Quick check question: How does reusing the same noise vector across an episode reduce variance compared to resampling?

## Architecture Onboarding

- Component map:
  - NRESWorker -> OESWorker interface -> Worker pool -> OPT UPDATE (SGD/Adam)
  - NRESWorker instances with W and T parameters collect gradient estimates
  - Worker pool contains independent NRES workers at different truncation windows
  - OPT UPDATE consumes averaged gradients from all workers

- Critical path:
  1. Initialize NRESWorker instances with W and T parameters
  2. Step 1: Prepare workers by sampling τ values to ensure step-unlocked operation
  3. Step 2: In each outer iteration, collect gradient estimates from all workers
  4. Average gradients and pass to optimizer
  5. Update parameters and repeat until convergence

- Design tradeoffs:
  - Noise reuse vs. resampling: reuse reduces variance but may miss some stochastic exploration benefits
  - W vs. T: smaller W enables faster updates but may miss longer-term dependencies
  - N vs. W: more workers enable parallelization but increase memory usage

- Failure signatures:
  - High variance in gradient estimates: likely caused by misaligned gradient contributions across truncation windows
  - Poor convergence: may indicate noise standard deviation σ is not well-tuned
  - Memory issues: may occur if W is too large for available resources

- First 3 experiments:
  1. Verify unbiasedness: Compare NRES gradient estimates with FullES on a simple quadratic loss function
  2. Measure variance reduction: Plot variance of NRES vs. PES and FullES across different values of W
  3. Test latency advantage: Measure wall-clock time for NRES vs. FullES on a parallelizable task with known T and W

## Open Questions the Paper Calls Out
- The paper discusses potential extensions to handle parameter-dependent episode lengths in reinforcement learning and acknowledges hysteresis as a limitation of online methods.

## Limitations
- The variance reduction analysis relies on assumptions about gradient alignment across truncation windows that may not hold in all practical settings
- The comparison with AD methods assumes similar computational resources and parallelization capabilities
- The effectiveness on chaotic systems is demonstrated but not extensively characterized across different chaos regimes

## Confidence
- **High Confidence:** The theoretical proof of variance reduction for NRES within the generalized class of ES methods is sound and well-established
- **Medium Confidence:** The empirical results showing faster convergence are compelling but depend on specific implementation details and hardware configurations that are not fully disclosed
- **Medium Confidence:** The claim about NRES being particularly effective on chaotic/non-differentiable surfaces is supported by experiments but would benefit from more systematic evaluation

## Next Checks
1. **Variance Alignment Test:** Systematically measure gradient correlation across truncation windows for different values of W and T to verify the assumption underlying Theorem 8
2. **Chaos Robustness Study:** Evaluate NRES performance across a spectrum of chaotic systems with varying Lyapunov exponents to characterize its effectiveness boundaries
3. **Hardware-Agnostic Validation:** Replicate the convergence speed comparisons using both CPU and GPU implementations to confirm that latency advantages persist across different parallelization setups