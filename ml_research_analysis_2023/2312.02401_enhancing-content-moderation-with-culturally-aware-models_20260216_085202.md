---
ver: rpa2
title: Enhancing Content Moderation with Culturally-Aware Models
arxiv_id: '2312.02401'
source_url: https://arxiv.org/abs/2312.02401
tags:
- content
- cultural
- each
- moderation
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a framework to imbue large language models with
  cultural knowledge to improve content moderation. The approach involves fine-tuning
  encoder-decoder models on region-specific news data to capture cultural nuances,
  followed by continued training to integrate these models into a content moderation
  pipeline.
---

# Enhancing Content Moderation with Culturally-Aware Models

## Quick Facts
- arXiv ID: 2312.02401
- Source URL: https://arxiv.org/abs/2312.02401
- Reference count: 40
- Key outcome: Fine-tuning models on region-specific news data improves content moderation accuracy and generates culturally aligned explanations

## Executive Summary
This work presents a framework to imbue large language models with cultural knowledge to improve content moderation. The approach involves fine-tuning encoder-decoder models on region-specific news data to capture cultural nuances, followed by continued training to integrate these models into a content moderation pipeline. Evaluated on a podcast platform dataset, the culturally adapted models showed improved accuracy in detecting local violations and generated explanations more aligned with regional norms, as confirmed by human annotator preference. This highlights the value of adaptable, culturally aware moderation systems that better align with diverse global perspectives.

## Method Summary
The framework involves a three-stage approach: (1) pre-training encoder-decoder models on local news articles to capture cultural nuances through summarization, (2) fine-tuning the models on generating explanations for content violations based on moderator-written rationales, and (3) training a classification head using the encoder to predict whether content violates guidelines. The models are evaluated on podcast content from various regions with human-annotated labels indicating violations and explanations for those violations. The cultures studied include the US, UK, Canada, Australia, India, Kenya, Hong Kong, Malaysia, and Nigeria.

## Key Results
- Culturally adapted models showed improved accuracy in detecting local violations compared to base models
- Generated explanations were more aligned with regional norms as confirmed by human annotator preference
- Training on linguistically similar but culturally distinct regions (e.g., India → UK) can improve moderation capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-tuning encoder-decoder models on region-specific news data captures cultural nuances for improved moderation.
- **Mechanism**: The model learns to summarize news articles from a specific culture, embedding that culture's linguistic and contextual patterns into its representations.
- **Core assumption**: Summarisation tasks preserve enough cultural signal to influence downstream moderation tasks.
- **Evidence anchors**:
  - [abstract] The approach involves fine-tuning encoder-decoder models on region-specific news data to capture cultural nuances.
  - [section] The highest performance in summarisation tasks was consistently found on the diagonal of the heat-map, indicating that models perform best within their own cultural context.
  - [corpus] The corpus shows 5 related papers on culturally-aware moderation, suggesting the problem space is active and relevant.
- **Break condition**: If summarisation training fails to embed cultural context (e.g., poor ROUGE scores across all cultures), the downstream tasks will not benefit.

### Mechanism 2
- **Claim**: Continued training on {content, explanation} pairs enables the model to generate culturally aligned moderation rationales.
- **Mechanism**: After initial cultural pre-training, the model is fine-tuned to generate explanations that resemble human-written rationales, adapted to local norms.
- **Core assumption**: Explanations generated by culturally fine-tuned models are preferred by local annotators over generic or foreign-culture models.
- **Evidence anchors**:
  - [abstract] Culturally adapted models showed improved accuracy in detecting local violations and generated explanations more aligned with regional norms.
  - [section] Human annotator preference studies showed that aligned models were most often selected as first choice for explanation quality.
  - [corpus] Multiple studies (e.g., FanarGuard, SEA-Guard) focus on cultural alignment in moderation, reinforcing the need for such adaptation.
- **Break condition**: If human evaluators do not consistently prefer culturally aligned explanations, the model is not capturing local norms.

### Mechanism 3
- **Claim**: Training on culturally distinct but linguistically similar regions (e.g., India and UK) can still improve violation detection due to shared linguistic history.
- **Mechanism**: Shared language exposure (e.g., from colonization) provides overlapping cultural understanding that benefits cross-cultural moderation.
- **Core assumption**: Historical and linguistic ties create enough cultural overlap to transfer moderation capabilities.
- **Evidence anchors**:
  - [section] Training on Indian media improved the base model's ability to classify violations from the UK market, attributed to shared English language and historical influence.
  - [corpus] Papers like SESGO focus on Spanish evaluation in Latin American contexts, showing that cultural and linguistic overlaps are being studied in moderation.
- **Break condition**: If performance gains are not observed when training on culturally distant but linguistically similar regions, the assumption of beneficial overlap is invalid.

## Foundational Learning

- **Concept**: Cross-cultural communication differences
  - Why needed here: Understanding that what is offensive or violative can vary significantly between cultures is foundational to the approach.
  - Quick check question: Can you name two ways that the same content might be interpreted differently in two cultures?

- **Concept**: Encoder-decoder transformer architectures
  - Why needed here: The method relies on fine-tuning bert2bert models for both summarisation and explanation generation tasks.
  - Quick check question: What is the role of the encoder and decoder in a bert2bert architecture?

- **Concept**: Evaluation metrics for sequence-to-sequence tasks (ROUGE, AUROC)
  - Why needed here: The paper uses ROUGE for summarisation quality and AUROC for violation detection performance.
  - Quick check question: What does a high AUROC score indicate about a model's performance in a binary classification task?

## Architecture Onboarding

- **Component map**: Pre-trained bert-large-cased encoder and decoder → Cultural pre-training on {article, summary} pairs → Explanation generation fine-tuning on {content, rationale} pairs → Violation detection fine-tuning with classification head
- **Critical path**: Cultural pre-training → Explanation generation fine-tuning → Violation detection fine-tuning
- **Design tradeoffs**: 
  - Using bert2bert allows both encoding and decoding but may be slower than encoder-only models.
  - Relying on GPT-4 for rationale generation standardises data but introduces potential bias.
- **Failure signatures**: 
  - Low ROUGE scores across all cultures indicate cultural pre-training is ineffective.
  - AUROC scores close to random (0.5) indicate violation detection is not learning.
  - No consistent annotator preference in human evaluation suggests explanations are not culturally aligned.
- **First 3 experiments**:
  1. Run cultural pre-training on a small subset of articles; check ROUGE scores for diagonal dominance.
  2. Fine-tune the explanation generator on a small violative dataset; evaluate coherence and cultural relevance.
  3. Freeze encoder and train classification head on a small labeled dataset; measure AUROC.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of culturally attuned models vary across different content categories such as hate speech, medical misinformation, and incitement to harm?
- Basis in paper: [explicit] The paper mentions examining the probability assigned by the US model to content categorized as violative, considering both US-origin content and content from other regions, with a focus on how the model's scores differ across policy categories like Hate Content, Inciting Harm, and Medical Misinformation.
- Why unresolved: While the paper provides an analysis of the model's performance across different categories, it does not provide a comprehensive comparison of the model's performance across all categories or a detailed breakdown of how the model's performance varies for each category.
- What evidence would resolve it: A detailed analysis of the model's performance across all content categories, including a breakdown of precision, recall, and F1-score for each category, would provide a clearer understanding of how the model performs across different types of violations.

### Open Question 2
- Question: What is the impact of cultural training on the models' ability to detect violations in non-Western cultures?
- Basis in paper: [inferred] The paper mentions that it is challenging to assess the precise influence of cultural training on the models' performance in detecting violations in non-Western cultures due to limited data availability. It also notes that non-Western models generally exhibit lower performance in Western markets.
- Why unresolved: The paper does not provide a detailed analysis of the models' performance in non-Western cultures or a comparison of the performance of models trained on Western versus non-Western data.
- What evidence would resolve it: A comprehensive evaluation of the models' performance in detecting violations in non-Western cultures, including a comparison of the performance of models trained on Western versus non-Western data, would provide insights into the impact of cultural training on the models' ability to detect violations in diverse cultural contexts.

### Open Question 3
- Question: How do human annotators from different cultures perceive the quality and cultural alignment of the explanations generated by the culturally attuned models?
- Basis in paper: [explicit] The paper describes a human evaluation study where human annotators from different countries were asked to order the explanations generated by the culturally attuned models based on how well they align with the local culture.
- Why unresolved: While the paper provides an overview of the results of the human evaluation study, it does not provide a detailed analysis of the annotators' perceptions of the quality and cultural alignment of the explanations.
- What evidence would resolve it: A detailed analysis of the annotators' feedback on the quality and cultural alignment of the explanations, including a breakdown of the annotators' ratings and comments for each explanation, would provide insights into how human annotators perceive the explanations generated by the culturally attuned models.

## Limitations

- Internal podcast dataset used for evaluation is not publicly available, limiting external validation
- Cultural representation quality varies significantly across cultures, with potential data imbalances
- Human evaluation methodology has limitations around annotator expertise verification and subjective assessment of explanation quality

## Confidence

- **High Confidence**: The fundamental approach of fine-tuning models on culturally specific data is well-established in the literature. The observation that models perform better when training and evaluation cultures match (diagonal dominance in ROUGE scores) is straightforward and well-supported.
- **Medium Confidence**: The claim that culturally adapted models generate explanations preferred by local annotators is supported by human evaluation, but the methodology has limitations around annotator expertise verification and the subjective nature of explanation quality assessment.
- **Medium Confidence**: The finding that training on linguistically similar but culturally distinct regions (India → UK) can transfer moderation capabilities is plausible given historical language ties, but the magnitude of this effect and its generalizability to other language pairs requires further validation.

## Next Checks

1. **Cross-cultural Evaluation Transfer**: Train culturally adapted models on two cultures with strong linguistic ties but different cultural norms (e.g., Spanish for Mexico and Spain), then evaluate transfer performance to quantify how linguistic similarity affects cultural adaptation effectiveness.

2. **Data Sufficiency Analysis**: Systematically vary the amount of training data per culture and measure the point at which performance plateaus, establishing minimum data requirements for effective cultural adaptation across different regions.

3. **Annotator Expertise Validation**: Conduct a controlled study where annotators from different cultures rate explanations from their own and other cultures, measuring inter-rater reliability and testing whether cultural expertise significantly impacts evaluation outcomes.