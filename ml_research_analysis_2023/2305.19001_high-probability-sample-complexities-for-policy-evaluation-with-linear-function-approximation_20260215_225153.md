---
ver: rpa2
title: High-probability sample complexities for policy evaluation with linear function
  approximation
arxiv_id: '2305.19001'
source_url: https://arxiv.org/abs/2305.19001
tags:
- learning
- policy
- sample
- approximation
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the sample complexity required for policy
  evaluation with linear function approximation in discounted infinite-horizon Markov
  decision processes. It focuses on two widely-used algorithms: temporal difference
  (TD) learning and two-timescale linear TD with gradient correction (TDC).'
---

# High-probability sample complexities for policy evaluation with linear function approximation

## Quick Facts
- arXiv ID: 2305.19001
- Source URL: https://arxiv.org/abs/2305.19001
- Reference count: 17
- Primary result: First high-probability sample complexity bounds for TD learning (on-policy) and TDC (off-policy) with linear function approximation that achieve optimal dependence on tolerance level

## Executive Summary
This paper establishes sharp statistical guarantees for two fundamental policy evaluation algorithms with linear function approximation in discounted infinite-horizon MDPs. For TD learning in the on-policy setting, the authors prove a high-probability sample complexity bound that matches the minimax lower bound on crucial problem parameters. For TDC in the off-policy setting, they provide the first sample complexity bound with high-probability convergence guarantee that characterizes exact dependence on problem-related constants. The results improve upon existing work by achieving high-probability convergence guarantees and providing explicit dependence on problem-related parameters.

## Method Summary
The paper analyzes policy evaluation algorithms under i.i.d. sampling from the stationary distribution. For TD learning, it uses Polyak-Ruppert averaging combined with matrix Freedman's inequality to achieve high-probability convergence with sample complexity scaling as O(1/ε²). For TDC, it employs two-timescale stochastic approximation with step sizes α and β, decomposing the finite-sample error into population error (controlled by contraction) and stochastic error (controlled by matrix Bernstein concentration). The algorithms require careful parameter tuning based on problem-specific quantities like feature covariance condition number, discount factor, and maximum importance weight.

## Key Results
- TD learning achieves high-probability convergence with sample complexity matching minimax lower bound
- TDC provides first high-probability sample complexity bound for off-policy setting
- Both bounds achieve optimal dependence on tolerance level ε
- Explicit characterization of dependence on problem parameters (κ, λ₁, λ₂, ρmax, etc.)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-probability convergence for TD learning is achieved through Polyak-Ruppert averaging combined with matrix concentration inequalities.
- Mechanism: Averaging across all iterates (rather than using just the final iterate) reduces variance and stabilizes convergence. Matrix Freedman's inequality bounds the deviation of the empirical covariance from its expectation, allowing tight control of the estimation error.
- Core assumption: The transition pairs are i.i.d. and drawn from the stationary distribution; feature vectors have bounded ℓ2 norm.
- Evidence anchors:
  - [abstract]: "sharp statistical guarantees of policy evaluation algorithms with linear function approximation... establish the first sample complexity bound with high-probability convergence guarantee that attains the optimal dependence on the tolerance level."
  - [section 3.2]: Theorem 1 and Corollary 1 state the explicit high-probability bound with a clear sample complexity scaling as O(1/ε²).
  - [corpus]: Missing—no corpus neighbor discusses averaging or matrix Freedman; this is a novel contribution.
- Break condition: If samples are Markovian rather than i.i.d., the martingale concentration arguments fail; if features are unbounded, the variance bounds blow up.

### Mechanism 2
- Claim: TDC algorithm achieves high-probability convergence by decoupling the two timescales and bounding the joint error dynamics via a contractive linear mapping plus martingale concentration.
- Mechanism: The population dynamics are contractive (via λ1, λ2 eigenvalues), and the finite-sample error is bounded by decomposing it into population error (controlled by contraction) and stochastic error (controlled by matrix concentration). The two timescales (α and β) ensure that w updates are "fast" relative to θ updates, stabilizing the off-policy bias.
- Core assumption: The behavior policy is fixed, samples are i.i.d., and the importance weights are bounded (ρmax finite).
- Evidence anchors:
  - [abstract]: "first sample complexity bound with high-probability convergence guarantee that attains the optimal dependence on the tolerance level ε" for TDC in the off-policy setting.
  - [section 4.2]: Theorem 3 explicitly bounds ∥eθT − eθ⋆∥ in terms of ρmax, λ1, λ2, and the feature covariance norm.
  - [corpus]: Missing—no neighbor discusses two-timescale stochastic approximation or importance sampling in this context.
- Break condition: If importance weights are unbounded, ρmax → ∞ and the bound becomes vacuous; if the feature covariance under the behavior policy is ill-conditioned, the convergence slows drastically.

### Mechanism 3
- Claim: Minimax lower bound shows that no algorithm can achieve better than O(1/ε²) sample complexity (up to a factor of 1/(1−γ)) for linear policy evaluation, confirming tightness of the upper bounds.
- Mechanism: Construct a family of MDPs that are statistically hard to distinguish; apply Fano's inequality to lower bound the probability of error; compute KL divergence between MDPs to relate it to the sample size.
- Core assumption: The state space and feature space dimensions are comparable (d ≤ |S|), and the discount factor γ is bounded away from 1.
- Evidence anchors:
  - [abstract]: "show in the on-policy setting that our upper bound matches the minimax lower bound on crucial problem parameters."
  - [section 3.3]: Theorem 2 constructs hard MDP instances and uses Fano's inequality to prove the lower bound scaling as O(1/ε²).
  - [corpus]: Missing—no neighbor discusses minimax lower bounds for RL policy evaluation.
- Break condition: If the feature space is much larger than the state space, or if the MDP family is not constructed to be hard to distinguish, the lower bound may not apply.

## Foundational Learning

- Concept: Markov decision processes and the Bellman operator
  - Why needed here: The paper's algorithms rely on solving the projected Bellman equation or minimizing MSPBE; understanding MDP structure and Bellman operators is prerequisite to grasping why TD and TDC work.
  - Quick check question: What does the Bellman operator T^π do to a value function vector, and why is it a contraction in the sup norm?

- Concept: Linear function approximation and feature covariance
  - Why needed here: Both algorithms approximate the value function as Φθ; the covariance matrix Σ = Φ⊤DµΦ determines the geometry of the approximation space and appears in all sample complexity bounds.
  - Quick check question: How does the condition number κ of Σ affect the convergence rate of TD learning?

- Concept: Importance sampling and off-policy evaluation
  - Why needed here: In the off-policy setting, the behavior policy differs from the target policy; importance weights ρt correct the distribution mismatch but introduce variance.
  - Quick check question: What is the role of the maximum importance weight ρmax in the TDC sample complexity, and why does it appear as ρ²max?

## Architecture Onboarding

- Component map:
  - On-policy module: TD learning with Polyak-Ruppert averaging, matrix Freedman concentration, minimax lower bound proof
  - Off-policy module: TDC algorithm with two-timescale updates, population dynamics analysis (contraction), stochastic error decomposition, matrix Bernstein concentration
  - Shared utilities: Feature matrix Φ, covariance Σ, projection operators, importance weights, sample generation from stationary distribution

- Critical path:
  1. Generate i.i.d. samples from stationary distribution (on-policy) or behavior policy (off-policy)
  2. Run TD/TDC updates with specified step sizes
  3. Apply averaging (TD) or two-timescale update (TDC)
  4. At iteration T, output θ̂ and compute error bound using pre-computed constants (κ, λ1, λ2, ρmax, etc.)
  5. Validate high-probability error ≤ ε

- Design tradeoffs:
  - Polyak-Ruppert averaging vs. single final iterate: averaging reduces variance at cost of memory; single iterate is simpler but slower
  - Fixed vs. decaying step sizes: fixed steps allow clean concentration bounds but may converge slower; decaying steps can accelerate but complicate analysis
  - i.i.d. vs. Markovian samples: i.i.d. enables martingale concentration; Markovian requires more sophisticated mixing time analysis

- Failure signatures:
  - If ∥θ̂ − θ⋆∥Σ >> ε despite T satisfying the bound, suspect unbounded importance weights or ill-conditioned Σ
  - If convergence stalls early, check that step sizes satisfy the theoretical constraints (α, β small enough relative to λ1, λ2)
  - If the bound is vacuous (e.g., ρmax too large), the off-policy assumption is violated

- First 3 experiments:
  1. Implement TD with Polyak-Ruppert averaging on a simple MRP with known θ⋆; verify ∥θ̂ − θ⋆∥Σ decreases as O(1/√T)
  2. Run TDC on Baird's counterexample; confirm that the off-policy TD diverges while TDC converges
  3. Construct a hard-to-distinguish MDP family as in the lower bound proof; empirically estimate the error probability and compare to the theoretical prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the TD learning sample complexity bound be improved to remove the (1-γ) gap between the upper bound and the minimax lower bound?
- Basis in paper: [explicit] The paper states: "Our analysis leaves open several directions for future investigation; we close by sampling a few of them. Regarding TD learning, a natural direction of future work is to close the (1-γ) gap between our upper bound and the minimax lower bound."
- Why unresolved: The paper acknowledges this gap but does not provide a solution or proof that such improvement is impossible.
- What evidence would resolve it: Either a new analysis technique that closes the gap, or a proof that no algorithm can achieve better dependence on (1-γ).

### Open Question 2
- Question: Can the TDC algorithm's sample complexity be improved to reduce the dependence on problem-related parameters like ρmax, λ1, and λ2?
- Basis in paper: [explicit] The paper states: "For TDC with linear function approximation, we provide the first sample complexity bound that achieves the optimal dependence on the error tolerance ε, and characterize the exact dependence on problem-related constants at the same time. Moving beyond our results, our analysis leaves open several directions for future investigation... it is noteworthy that the analysis in this work is based on the assumption of i.i.d. transition pairs drawn from the stationary distribution; it is of natural interest to generalize these results to other scenarios such as Markovian trajectories."
- Why unresolved: The current TDC analysis provides explicit dependence on problem parameters but leaves room for improvement in these dependencies.
- What evidence would resolve it: Either a tighter analysis achieving better parameter dependence, or a lower bound proving current bounds are optimal.

### Open Question 3
- Question: How would the sample complexity bounds change if we consider Markovian trajectories instead of i.i.d. transition pairs?
- Basis in paper: [explicit] The paper states: "The analysis in this work is based on the assumption of i.i.d. transition pairs drawn from the stationary distribution; it is of natural interest to generalize these results to other scenarios such as Markovian trajectories."
- Why unresolved: The current analysis relies heavily on the independence assumption for concentration inequalities, which breaks down with Markovian data.
- What evidence would resolve it: Either a generalization of the current techniques to handle Markovian dependence, or a proof that the sample complexity fundamentally changes with Markovian data.

## Limitations
- The TDC analysis critically depends on bounded importance weights (ρmax); if behavior and target policies have non-overlapping supports, the bound becomes vacuous
- Both analyses require i.i.d. samples from the stationary distribution, which is unrealistic for Markovian data and limits practical applicability
- The sample complexity bounds contain unspecified constants that affect tightness and practical implementation guidance

## Confidence
- High confidence: The theoretical framework for TD learning with Polyak-Ruppert averaging and matrix Freedman concentration is well-established and the bounds are provably tight (matching the minimax lower bound)
- Medium confidence: The TDC sample complexity bounds are novel but rely on several technical conditions (invertibility of matrices A and eA, bounded ρmax) that may fail in practice
- Low confidence: The minimax lower bound construction is technically correct but applies to a specific hard instance that may not reflect typical problem difficulty

## Next Checks
1. Implement both standard TD and TD with Polyak-Ruppert averaging on a simple MRP with known θ⋆; measure convergence rates and compare to the theoretical O(1/√T) prediction
2. Run TDC on Baird's counterexample (or a similar off-policy hard instance) and verify that it converges while off-policy TD diverges, confirming the theoretical advantage
3. Construct a sequence of off-policy problems with increasing ρmax (bounded but large) and measure how the estimation error scales; verify the predicted O(ρ²max) dependence in the TDC bound