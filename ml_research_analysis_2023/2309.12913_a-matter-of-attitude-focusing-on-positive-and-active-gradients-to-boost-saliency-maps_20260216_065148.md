---
ver: rpa2
title: 'A matter of attitude: Focusing on positive and active gradients to boost saliency
  maps'
arxiv_id: '2309.12913'
source_url: https://arxiv.org/abs/2309.12913
tags:
- saliency
- pixels
- maps
- positive
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the significance of gradient signs in saliency
  maps, a widely-used interpretability technique for convolutional neural networks
  (CNNs). It proposes that considering the sign and effect of gradients not only for
  the correct class but also for other classes can lead to a deeper understanding
  of multi-class classification problems.
---

# A matter of attitude: Focusing on positive and active gradients to boost saliency maps

## Quick Facts
- **arXiv ID**: 2309.12913
- **Source URL**: https://arxiv.org/abs/2309.12913
- **Reference count**: 40
- **Key outcome**: Proposes positive/negative and active/inactive saliency maps to improve interpretability of CNNs by considering gradient signs and multi-class effects.

## Executive Summary
This paper investigates the significance of gradient signs in saliency maps for interpreting convolutional neural networks. It proposes that considering both the sign and effect of gradients—not only for the correct class but also across other classes—can lead to a deeper understanding of multi-class classification problems. The authors introduce two types of saliency maps: positive/negative, which indicate whether increasing or decreasing pixel values improves the classification score of the true class, and active/inactive, which identify pixels that should be altered to increase the model's confidence in the prediction of the true class. Experiments on CIFAR-10 and Imagenette datasets show that these maps reduce noise and produce sharper visualizations compared to standard saliency maps. A novel "allegiance" metric is introduced to evaluate effectiveness by measuring changes in predicted classes when pixels are occluded.

## Method Summary
The method builds on standard gradient-based saliency maps by separating gradients into positive and negative components and by considering gradients across all classes to define active and inactive pixels. Positive saliency maps highlight pixels whose increase improves the true class score, while negative maps highlight pixels whose decrease does the same. Active saliency maps highlight pixels where the true class gradient is largest among all classes, and inactive maps highlight those where it is smallest. Occlusion experiments using white (for positive/active) and black (for negative/inactive) pixel replacement are used to evaluate the maps via a novel "allegiance" metric, which measures the change in predicted class as more pixels are occluded.

## Key Results
- Positive/negative and active/inactive saliency maps reduce noise and produce sharper visualizations compared to standard saliency maps.
- The proposed maps better identify pixels that, when made brighter or darker, increase the confidence of the originally predicted class.
- Allegiance experiments show that occlusion of active/positive pixels with black/white, respectively, leads to faster drops in model confidence, validating the maps' effectiveness.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preserving gradient sign in saliency maps enables clearer identification of which pixels should be made brighter or darker to improve model confidence.
- Mechanism: By splitting the gradient into positive and negative components, the method isolates regions that contribute positively to the true class score versus those that detract from it. This separation avoids the confusion that arises when opposite-sign gradients are combined, as zero gradients in the original approach are incorrectly interpreted as important.
- Core assumption: The gradient of the class score with respect to input pixels is a reliable proxy for importance.
- Evidence anchors:
  - [abstract] The paper shows that considering the sign and effect of gradients not only for the correct class but also for other classes allows better identification of the pixels the network is really focusing on.
  - [section] "Taking the absolute value of every pixel in the saliency map comes at a cost and some enlightening information is lost. In terms of explainability, the opportunity of knowing which regions of the image should be brighter or darker to improve the classification accuracy is disregarded."
  - [corpus] The corpus contains related work on saliency maps and interpretability but lacks direct experimental validation of the sign-separation claim.
- Break condition: If the model's decision boundary is not locally smooth, gradient signs may not reliably indicate how changes affect the score.

### Mechanism 2
- Claim: Active and inactive saliency maps, which consider all class gradients, provide more actionable guidance than maps based only on the true class.
- Mechanism: Active pixels are defined as those where the gradient for the true class is the largest among all classes; inactive pixels are those where it is the smallest. By explicitly comparing gradients across classes, these maps highlight pixels that most strongly push the model toward or away from the correct decision, rather than just indicating any influence.
- Core assumption: The magnitude and sign of gradients for incorrect classes provide meaningful context for interpreting true-class gradients.
- Evidence anchors:
  - [abstract] "considering the sign and the effect not only of the correct class, but also the influence of the other classes, allows to better identify the pixels of the image that the network is really focusing on."
  - [section] "Based on these definitions, two additional saliency map visualization can be derived: Active saliency maps highlight the pixels that should be increased to improve the classification score of the true class... Inactive saliency maps depict the pixels that should be dimmed to enhance the classification score of the correct class."
  - [corpus] No direct corpus support for cross-class gradient comparison; evidence is entirely from the paper's experiments.
- Break condition: If incorrect classes have gradients that are noisy or not well-defined, the comparison may yield misleading results.

### Mechanism 3
- Claim: Using white/black occlusion instead of arbitrary constant values makes the effect of pixel alterations interpretable and measurable.
- Mechanism: Positive/active pixels are occluded with white (making them brighter), negative/inactive pixels with black (making them darker). This ensures that occlusion experiments directly test whether the pixels identified by the saliency maps are genuinely important, as the direction of change is aligned with their gradient sign.
- Core assumption: Replacing pixels with black or white has a clear and predictable effect on the model's score in the direction indicated by the gradient sign.
- Evidence anchors:
  - [abstract] "The network would want pixels with a positive gradient to have higher intensities and those with negative gradients to be dimmed towards zero. This fact makes occlusion experiments more self-explanatory, since it is easier to understand the meaning of replacing a given pixel with a brighter or darker one, ultimately with white or black."
  - [section] "Both black- (Figure 3) and white-deletion (Figure 4) measure the change in the predicted classes with respect to the original classification, which we have decided to coin allegiance."
  - [corpus] The corpus lists related saliency map work but does not address the specific occlusion strategy.
- Break condition: If the model is highly sensitive to extreme values or if the input normalization makes white/black replacements unrepresentative, the occlusion results may be distorted.

## Foundational Learning

- Concept: Gradient-based interpretability in neural networks
  - Why needed here: The entire method relies on computing and interpreting gradients of the class score with respect to input pixels; without understanding this, the sign-based improvements are opaque.
  - Quick check question: What does the gradient of the class score with respect to an input pixel represent in terms of model behavior?
- Concept: Multi-class classification and class score gradients
  - Why needed here: The method explicitly compares gradients across classes, so understanding how class scores are computed and differentiated is essential.
  - Quick check question: In a multi-class softmax model, how does changing a pixel affect the scores of both the correct and incorrect classes?
- Concept: Occlusion-based evaluation of interpretability methods
  - Why needed here: The paper uses deletion (occlusion) experiments to validate that the proposed saliency maps identify truly important pixels; understanding this benchmark is necessary to interpret the results.
  - Quick check question: Why might occlusion with black or white be more interpretable than occlusion with a constant mid-tone value?

## Architecture Onboarding

- Component map: Input image -> Forward pass for class scores -> Backward pass for input gradients (all classes) -> Apply sign and cross-class logic to generate four saliency maps (positive, negative, active, inactive) -> Occlusion experiments (black/white deletion) -> Compute allegiance metric and visualize results
- Critical path: 1. Forward pass to get class scores; 2. Backward pass to compute input gradients for each class; 3. Apply sign and cross-class logic to generate four saliency maps; 4. Perform occlusion experiments and compute allegiance
- Design tradeoffs:
  - Sign separation improves interpretability but doubles the number of visualizations
  - Cross-class comparison increases accuracy but adds computational cost (gradients for all classes)
  - White/black occlusion is interpretable but may be sensitive to input normalization
- Failure signatures:
  - Allegiance curves that do not decrease (or increase) as expected when deleting important pixels
  - Saliency maps that are noisy or highlight irrelevant regions
  - Large discrepancy between positive/negative and active/inactive results (may indicate gradient noise)
- First 3 experiments:
  1. Reproduce standard saliency maps for a trained CNN and compare with positive/negative splits; visually inspect noise reduction.
  2. Compute active/inactive maps and verify that active pixels are those where the true class gradient dominates across all classes.
  3. Run allegiance experiments with black/white occlusion; check that allegiance decreases most for active (black) and negative (white) deletions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sign of gradients in saliency maps vary across different datasets and model architectures?
- Basis in paper: [explicit] The paper discusses the importance of considering gradient signs in saliency maps and their impact on interpretability, particularly in multi-class classification problems.
- Why unresolved: The paper does not provide a comprehensive analysis of how gradient signs vary across different datasets and model architectures. It focuses on two specific datasets (CIFAR-10 and Imagenette) and a few model architectures (basic CNN, ResNet-18, ConvNeXt).
- What evidence would resolve it: Conducting experiments on a wider range of datasets and model architectures to analyze the distribution and patterns of gradient signs in saliency maps.

### Open Question 2
- Question: How does the choice of replacement values (e.g., black or white) affect the effectiveness of occlusion experiments in evaluating saliency maps?
- Basis in paper: [explicit] The paper mentions that using black or white as replacement values in occlusion experiments can introduce unknown biases and affect the interpretation of results.
- Why unresolved: The paper does not provide a detailed analysis of how different replacement values impact the effectiveness of occlusion experiments in evaluating saliency maps.
- What evidence would resolve it: Conducting experiments with various replacement values and analyzing their impact on the interpretation of occlusion experiments and the evaluation of saliency maps.

### Open Question 3
- Question: How do the proposed positive/negative and active/inactive saliency maps compare to other interpretability techniques in terms of faithfulness and interpretability?
- Basis in paper: [explicit] The paper introduces positive/negative and active/inactive saliency maps and claims that they provide better insights and interpretability compared to standard saliency maps. However, a direct comparison with other interpretability techniques is not provided.
- Why unresolved: The paper does not conduct a comprehensive comparison of the proposed saliency maps with other interpretability techniques in terms of faithfulness and interpretability.
- What evidence would resolve it: Conducting experiments to compare the proposed saliency maps with other interpretability techniques, such as LIME, SHAP, or Grad-CAM, in terms of faithfulness and interpretability metrics.

## Limitations
- Performance gains are demonstrated only on CIFAR-10 and Imagenette; generalization to larger, more complex datasets (e.g., ImageNet) is unknown.
- The proposed "allegiance" metric lacks comparison to established interpretability benchmarks.
- Claims about universal superiority are not fully supported; evidence is limited to specific architectures and datasets.

## Confidence
- **High**: The conceptual framework for positive/negative and active/inactive saliency maps is internally consistent and logically follows from gradient-based interpretability principles.
- **Medium**: The experimental results (visualizations, allegiance curves) are compelling for the tested datasets and models, but lack statistical significance tests or broader dataset coverage.
- **Low**: Claims about the universal superiority of the proposed maps for all CNNs and tasks are not fully supported; the evidence is limited to specific architectures and datasets.

## Next Checks
1. **Dataset Generalization**: Replicate experiments on a larger, more diverse dataset (e.g., ImageNet) to test whether the method scales and maintains performance.
2. **Robustness Analysis**: Test the saliency maps' robustness to adversarial examples and domain shifts to assess their reliability under stress.
3. **Benchmark Comparison**: Compare the proposed "allegiance" metric and overall method against established saliency map evaluation benchmarks (e.g., deletion/insertion metrics, pointing games) to contextualize improvements.