---
ver: rpa2
title: Improving Neural Ranking Models with Traditional IR Methods
arxiv_id: '2308.15027'
source_url: https://arxiv.org/abs/2308.15027
tags:
- retrieval
- tf-idf
- neural
- article
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel approach to improve neural ranking models
  for information retrieval tasks by combining traditional keyword-based methods (TF-IDF)
  with shallow neural embedding models. The authors argue that while transformer-based
  models like BERT have shown promising results in recent years, they are computationally
  expensive and require large amounts of labeled data.
---

# Improving Neural Ranking Models with Traditional IR Methods

## Quick Facts
- arXiv ID: 2308.15027
- Source URL: https://arxiv.org/abs/2308.15027
- Reference count: 5
- A simple combination of TF-IDF and bag-of-embedding models achieves competitive performance with complex neural ranking models

## Executive Summary
This paper proposes a hybrid approach that combines traditional TF-IDF keyword matching with a shallow bag-of-embedding neural model to improve information retrieval performance. The authors demonstrate that this simple combination can compete with or outperform complex transformer-based models like BERT, particularly for longer documents where transformers face sequence length limitations. The approach addresses the high computational cost and data requirements of fine-tuning large transformer models while maintaining strong retrieval performance.

## Method Summary
The method trains a dual encoder bag-of-embedding model using average word embeddings and cosine similarity, then combines its scores with TF-IDF scores using simple addition. The model is trained with a margin loss function on positive and negative pairs from batch data, requiring no click-through information. Experiments use three datasets (News, Wikipedia, Natural Questions) with mean reciprocal rank and mean precision at top-k as evaluation metrics.

## Key Results
- Hybrid TF-IDF + bag-of-embedding model outperforms standalone TF-IDF, BM25, and BERT-based models on all three datasets
- The approach is particularly effective for longer documents where BERT faces truncation issues
- Adding TF-IDF scores improves the performance of large-scale fine-tuned models on these tasks
- The method provides a low-cost alternative to complex neural ranking models while maintaining competitive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining TF-IDF scores with neural embedding scores improves retrieval performance because TF-IDF compensates for the weakness of neural models on rare words while embeddings capture semantic similarity.
- Mechanism: The hybrid scoring function `s(q, a) = sTF-IDF(q, a) + s_embedding(q, a)` leverages the complementary strengths of both methods - TF-IDF's effectiveness on exact keyword matches and the embedding model's ability to capture semantic similarity through cosine distance.
- Core assumption: The performance gain from combining these two methods is additive rather than multiplicative or interactive.
- Evidence anchors:
  - [abstract]: "Our results show that a simple combination of TF-IDF, a traditional keyword matching method, with a shallow embedding model provides a low cost path to compete well with the performance of complex neural ranking models"
  - [section 3.2]: "We combine the embedding and TF-IDF models by aggregating their score for a pair of query and article"
  - [corpus]: Weak - no specific corpus evidence for this mechanism

### Mechanism 2
- Claim: Bag-of-embedding models with simple averaging are competitive with complex transformer models for long documents because they avoid the token length limitations of transformers.
- Mechanism: The bag-of-embedding model represents documents as average word embeddings, allowing it to handle documents longer than the 512-token limit of BERT without truncation effects that degrade performance.
- Core assumption: Averaging word embeddings preserves sufficient semantic information for ranking tasks.
- Evidence anchors:
  - [section 4.5]: "The bag-of-embedding model lags behind the BERT ranker on Wikipedia dataset... Similar to the News dataset, we see that the bag-of-embedding model performs better in the precision@3 and precision@10 metrics"
  - [section 4.5]: "The complex BERT model was fine tuned on the MSMARCO passage ranking dataset where the average article length is about 55 words. We know BERT uses the WordPiece tokenizer and has a maximum sequence length of 512 tokens"
  - [corpus]: Weak - no specific corpus evidence for this mechanism

### Mechanism 3
- Claim: Weak supervision through margin loss on positive and negative pairs from batch data is sufficient to train effective retrieval models without requiring click-through data.
- Mechanism: The model uses a margin loss function `L = max(0, δ − sp + sn)` where positive and negative pairs are created from within a batch, treating all non-relevant articles as negative examples.
- Core assumption: The negative sampling strategy (using all other articles in the batch) provides sufficient contrast for learning effective representations.
- Evidence anchors:
  - [section 3.1]: "The model is trained with a margin loss function that maximizes the difference in score between a positive pair and a negative pair"
  - [section 3.1]: "For a query, we select the article with the lowest similarity score in the batch to form the negative pair"
  - [corpus]: Weak - no specific corpus evidence for this mechanism

## Foundational Learning

- Concept: Dual encoder architecture for retrieval
  - Why needed here: The paper uses a dual encoder model where separate but identical encoders process queries and documents independently, which is fundamental to understanding how the bag-of-embedding model works
  - Quick check question: What is the key advantage of using dual encoders over cross-encoders for retrieval tasks?

- Concept: Cosine similarity for embedding comparison
  - Why needed here: The paper measures similarity between query and document embeddings using cosine distance, which is critical for understanding how the bag-of-embedding model produces rankings
  - Quick check question: How does cosine similarity between normalized embeddings relate to the dot product of those embeddings?

- Concept: TF-IDF weighting scheme
  - Why needed here: The paper combines TF-IDF scores with embedding scores, so understanding how TF-IDF transforms token frequencies into weighted vectors is essential
  - Quick check question: What is the formula for IDF weighting in TF-IDF, and how does it help with rare word matching?

## Architecture Onboarding

- Component map:
  Tokenizer (standard word-based) -> Query encoder (averages word embeddings) -> Document encoder (averages word embeddings) -> TF-IDF vectorizer (unigrams + bigrams, stopword removal) -> Scoring layer (cosine similarity for embeddings, dot product for TF-IDF vectors) -> Hybrid scorer (simple addition of TF-IDF and embedding scores)

- Critical path:
  1. Tokenize query and document
  2. Generate average embeddings for both
  3. Compute cosine similarity between embeddings
  4. Generate TF-IDF vectors for both
  5. Compute cosine similarity between TF-IDF vectors
  6. Add the two similarity scores
  7. Rank documents by combined score

- Design tradeoffs:
  - Simplicity vs. expressiveness: Bag-of-embedding is simpler than transformers but may lose positional information
  - Length handling: Bag-of-embedding can handle longer documents than BERT's 512-token limit
  - Training data: Margin loss with batch-based negatives requires less labeled data than supervised fine-tuning

- Failure signatures:
  - If TF-IDF dominates the score: Check embedding dimension and quality
  - If embeddings dominate but miss exact matches: Verify TF-IDF parameters and vocabulary coverage
  - If performance degrades on short documents: May need different combination strategy for different document lengths

- First 3 experiments:
  1. Test individual components: Compare bag-of-embedding alone vs TF-IDF alone on a small dataset
  2. Ablation study: Remove TF-IDF from the hybrid model to quantify its contribution
  3. Length sensitivity: Test performance on documents of varying lengths to verify the claim about BERT's limitations

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- The exact word embeddings used in the bag-of-embedding model are unspecified, which is critical for reproducibility
- The study doesn't investigate whether the performance gains from combining methods are additive or multiplicative
- Limited empirical validation of claims about TF-IDF compensating for neural model weaknesses on rare words

## Confidence
- **High Confidence**: The experimental results showing hybrid models outperforming standalone methods on the tested datasets
- **Medium Confidence**: The mechanism by which TF-IDF compensates for neural model weaknesses on rare words
- **Low Confidence**: Claims about handling documents longer than BERT's 512-token limit without quantitative analysis of truncation effects

## Next Checks
1. Conduct ablation studies to determine if the performance gains from combining TF-IDF and embeddings are additive or multiplicative
2. Test the model on datasets with significantly longer documents (thousands of tokens) to verify the claimed advantage over BERT
3. Investigate whether different combination strategies (weighted sum, learned fusion) outperform the simple addition used in this work