---
ver: rpa2
title: 'Bridging the Transparency Gap: What Can Explainable AI Learn From the AI Act?'
arxiv_id: '2302.10766'
source_url: https://arxiv.org/abs/2302.10766
tags:
- system
- transparency
- systems
- requirements
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a "transparency gap" between the European
  Union's AI Act and the field of explainable AI (XAI). While the Act views transparency
  as a means to support broader values like accountability and human rights, XAI narrowly
  focuses on explaining algorithmic properties without considering socio-technical
  context.
---

# Bridging the Transparency Gap: What Can Explainable AI Learn From the AI Act?

## Quick Facts
- arXiv ID: 2302.10766
- Source URL: https://arxiv.org/abs/2302.10766
- Reference count: 40
- The paper identifies a "transparency gap" between the EU's AI Act and XAI field definitions, arguing for interdisciplinary collaboration to align technical solutions with regulatory requirements.

## Executive Summary
This paper identifies a fundamental "transparency gap" between the European Union's AI Act and the field of explainable AI (XAI). While the Act views transparency as a means to support broader values like accountability and human rights, XAI narrowly focuses on explaining algorithmic properties without considering socio-technical context. The authors analyze terminology differences and identify four key areas where practical work could bridge this gap: defining the scope of transparency, clarifying the legal status of XAI, addressing conformity assessment issues, and building explainability for datasets. They argue that collaboration between legal scholars and XAI researchers is essential to align technological solutions with regulatory requirements and create effective, lasting AI regulation.

## Method Summary
The authors conduct a comparative analysis of the AI Act and GDPR provisions alongside XAI literature and methods to identify terminological and conceptual gaps. They examine how transparency, explainability, and interpretability are defined differently in legal versus technical contexts, and analyze the implications for practical implementation. The paper identifies four key axes for bridging the gap: defining the scope of transparency requirements, clarifying the legal status of XAI systems, addressing conformity assessment challenges, and extending XAI to data governance. Recommendations are developed through this interdisciplinary analysis to guide future research and implementation efforts.

## Key Results
- The AI Act and XAI field have fundamentally different interpretations of transparency - the Act views it as an overarching system property supporting values, while XAI focuses narrowly on algorithmic interpretability
- Four key areas for bridging the gap: scope definition, legal status of XAI systems, conformity assessment mechanisms, and data explainability
- Many XAI methods may themselves qualify as AI systems under the Act's broad definition, creating regulatory complexity
- The Act lacks concrete definitions of transparency, creating uncertainty for developers and potential compliance challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper identifies a "transparency gap" between the EU AI Act and XAI field definitions.
- Mechanism: By comparing the regulatory view of transparency as a means to support values (accountability, human rights) with XAI's narrow focus on algorithmic properties, the paper reveals fundamental misalignments.
- Core assumption: That different stakeholders have incompatible definitions of transparency without explicit reconciliation.
- Evidence anchors:
  - [abstract] "The Act views transparency as a means that supports wider values, such as accountability, human rights, and sustainable innovation. In contrast, XAI views transparency narrowly as an end in itself, focusing on explaining complex algorithmic properties without considering the socio-technical context."
  - [section 4.2.1] "In the interpretation of the Act, transparency is an overarching property of the AI system achieved through requirements... In XAI, transparency concerns solely the interpretability of the decision-making process."
- Break condition: If stakeholders agree on unified definitions or if the Act explicitly defines transparency to match XAI technical requirements.

### Mechanism 2
- Claim: Collaboration between legal scholars and XAI researchers is essential to bridge the transparency gap.
- Mechanism: The paper identifies four practical axes where work could bridge the gap, requiring interdisciplinary understanding to align technological solutions with regulatory requirements.
- Core assumption: That legal requirements and technical capabilities can be meaningfully reconciled through collaborative effort.
- Evidence anchors:
  - [abstract] "To begin to bridge this gap, we overview and clarify the terminology of how XAI and European regulation -- the Act and the related General Data Protection Regulation (GDPR) -- view basic definitions of transparency."
  - [paper text] "We argue that collaboration is essential between lawyers and XAI researchers to address these differences."
- Break condition: If legal requirements remain incompatible with technical feasibility regardless of collaboration efforts.

### Mechanism 3
- Claim: XAI methods themselves may qualify as AI systems under the Act's definition, creating regulatory complexity.
- Mechanism: Since many XAI methods use AI techniques, they could be subject to the same regulatory requirements as the AI systems they explain, creating potential liability and assessment issues.
- Core assumption: That the Act's AI definition is broad enough to encompass XAI methods.
- Evidence anchors:
  - [section 4.1.1] "Many XAI methods themselves rely on practices that fall under the currently proposed definition of AI. Does then the XAI system qualify as the same or as a different AI system separate from the underlying AI algorithm?"
  - [section 4.3.3] "Would the requirements of Chapter 2 of the Act need to be guaranteed separately for the AI and the XAI system, or could they be covered under the same unified assessment?"
- Break condition: If the Act explicitly excludes XAI methods from AI system definitions or if regulatory guidance clarifies this distinction.

## Foundational Learning

- Concept: Distinction between ex-ante and ex-post explainability
  - Why needed here: The paper discusses different types of explainability requirements in the Act (pre-deployment documentation vs. post-decision explanations)
  - Quick check question: What is the difference between ex-ante explainability and ex-post explainability?

- Concept: Interpretability vs. explainability in AI systems
  - Why needed here: The paper distinguishes between inherent properties of AI systems (interpretability) and methods to communicate reasoning (explainability)
  - Quick check question: How does the paper define interpretability versus explainability?

- Concept: Risk-based approach in AI regulation
  - Why needed here: The Act applies different transparency requirements based on risk levels, which affects how XAI methods should be applied
  - Quick check question: How does the Act's risk-based approach affect transparency requirements for different AI systems?

## Architecture Onboarding

- Component map: Regulatory framework (AI Act/GDPR) -> XAI technical methods -> Stakeholder requirements (developers, users, affected parties) -> Documentation requirements -> Explanation generation -> Compliance assessment -> Stakeholder verification
- Critical path: Regulatory requirement identification → XAI method selection/development → System integration → Documentation generation → Compliance assessment → Stakeholder verification
- Design tradeoffs: Balancing transparency depth vs. cybersecurity risks, interpretability vs. accuracy, technical feasibility vs. legal requirements, and intellectual property protection vs. disclosure obligations
- Failure signatures: Regulatory non-compliance due to insufficient documentation, stakeholder distrust from inadequate explanations, system vulnerabilities from excessive transparency, and liability issues from unclear XAI system status
- First 3 experiments:
  1. Map current XAI methods against specific AI Act transparency requirements to identify gaps
  2. Test different explanation generation approaches on a high-risk medical AI system to evaluate compliance
  3. Simulate conformity assessment scenarios comparing self-assessment vs. external auditing using XAI tools

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What should be the precise legal status of XAI systems under the AI Act - should they be treated as separate AI systems or as components of the main AI system they explain?
- Basis in paper: [explicit] Section 4.1.1 discusses this uncertainty and argues XAI systems should not be treated separately
- Why unresolved: The Act doesn't clarify whether XAI systems are separate AI systems requiring their own conformity assessment or integrated components of the main system
- What evidence would resolve it: Clear regulatory guidance or case law defining the legal status of XAI systems and their relationship to the AI systems they explain

### Open Question 2
- Question: How can we define "transparency" in a way that satisfies both the Act's broad interpretation and XAI's narrow, technical focus while remaining actionable for developers?
- Basis in paper: [explicit] Section 4.2 highlights the fundamental disagreement between the Act's view of transparency as an overarching system property versus XAI's focus on algorithmic interpretability
- Why unresolved: The Act mentions transparency frequently but provides no concrete definition, while XAI has multiple competing interpretations that may not align with regulatory needs
- What evidence would resolve it: Development of a hierarchical transparency framework that bridges the gap between technical XAI capabilities and legal/regulatory requirements

### Open Question 3
- Question: What mechanisms can ensure effective conformity assessment of AI systems when providers are given discretion in how they demonstrate compliance with transparency requirements?
- Basis in paper: [explicit] Section 4.3.1 discusses the Act's allowance for self-assessment and the risks of providers choosing their own methods to demonstrate conformity
- Why unresolved: The Act requires documentation but doesn't specify how providers must demonstrate their conformity assessment methods, creating potential for misrepresentation
- What evidence would resolve it: Implementation of standardized auditing frameworks using XAI tools that can independently verify compliance without relying solely on provider self-reporting

### Open Question 4
- Question: How can XAI research be extended to provide explanations for datasets themselves, addressing the Act's requirements on data quality and governance?
- Basis in paper: [explicit] Section 4.4 identifies this as a gap where XAI focuses on explaining model outputs but not the data properties that significantly affect system behavior
- Why unresolved: Current XAI methods primarily explain model decisions rather than examining how data characteristics, preprocessing, and biases affect outcomes
- What evidence would resolve it: Development of automated techniques that can examine and explain the effects of changing dataset properties on AI system behavior, similar to "datasheets for datasets" but with automated analysis capabilities

## Limitations

- The analysis depends on the final form of the AI Act, which may change during the legislative process
- Practical implementation guidance is still developing, making theoretical analysis challenging to validate
- The assumption that XAI methods can be meaningfully separated from the AI systems they explain may not hold for all technical approaches

## Confidence

- High confidence: The identification of terminological differences between regulatory and technical communities
- Medium confidence: The four identified axes for bridging the transparency gap
- Medium confidence: The analysis of conformity assessment challenges

## Next Checks

1. Map specific XAI methods (e.g., SHAP, LIME, counterfactual explanations) against each transparency requirement in the Act's Annex IV to identify concrete gaps
2. Conduct stakeholder workshops with both legal experts and XAI practitioners to validate the proposed definitions and requirements
3. Analyze existing high-risk AI systems (e.g., medical diagnosis tools) to assess how current XAI methods would fare under proposed Act requirements