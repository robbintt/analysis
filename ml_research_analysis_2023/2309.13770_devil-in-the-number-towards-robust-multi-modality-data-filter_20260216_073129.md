---
ver: rpa2
title: 'Devil in the Number: Towards Robust Multi-modality Data Filter'
arxiv_id: '2309.13770'
source_url: https://arxiv.org/abs/2309.13770
tags:
- clip
- data
- filter
- filtering
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the challenge of filtering large-scale multi-modal
  datasets for effective training of vision-language models like CLIP. They observe
  that redundant numerical information in image captions can negatively impact CLIP
  similarity scores, causing high-quality data to be incorrectly filtered out.
---

# Devil in the Number: Towards Robust Multi-modality Data Filter

## Quick Facts
- arXiv ID: 2309.13770
- Source URL: https://arxiv.org/abs/2309.13770
- Reference count: 9
- Primary result: Text-masking CLIP filter achieves 3.6% improvement on ImageNet distribution shifts by removing numerical text content from captions before computing similarity scores

## Executive Summary
This paper addresses the challenge of filtering large-scale multi-modal datasets for vision-language models like CLIP. The authors observe that numerical information in image captions creates spurious correlations that bias CLIP similarity scores, causing high-quality data to be incorrectly filtered out. They propose a text-masking approach that removes numbers and bracketed content from captions before recomputing CLIP similarity, demonstrating improved filtering effectiveness on ImageNet distribution shifts. The method shows particular benefits when filtering at higher proportions (40% vs 70%), balancing the trade-off between removing numerical contamination and preserving semantically relevant data.

## Method Summary
The authors propose a text-masking CLIP filter that preprocesses image captions by removing numerical text content and bracketed information. After masking, they compute CLIP similarity scores using a ViT-L/14 model between the masked text and corresponding images. The dataset is then filtered based on these recomputed similarity scores, selecting the top fraction of image-text pairs. This approach addresses the bias introduced by numerical text in standard CLIP filtering, where numbers can create spurious correlations independent of actual image-text relevance.

## Key Results
- Text-masked CLIP filter achieves 3.6% improvement on ImageNet distribution shifts compared to standard CLIP score filtering
- Outperforms baseline at 40% filtering ratio but underperforms at 70% ratio, suggesting optimal filtering proportion exists
- Demonstrates robustness to numerical noise while maintaining semantic consistency in filtered data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing numerical text content reduces CLIP score bias caused by redundant information.
- Mechanism: Numbers and bracketed content in captions create spurious correlations that inflate or deflate CLIP similarity scores independently of image relevance. Masking these tokens forces the model to rely on semantic content rather than numerical artifacts.
- Core assumption: The CLIP model's similarity computation is sensitive to token-level lexical matches, including numbers, which do not contribute meaningful alignment.
- Evidence anchors:
  - [abstract] "We observe that redundant numerical information in image captions can negatively impact CLIP similarity scores, causing high-quality data to be incorrectly filtered out."
  - [section] "Numerical data can potentially mislead the CLIP model... the presence of numbers may act as a contaminant, causing some data to receive lower CLIP scores and be filtered out."
- Break condition: If numbers are semantically essential to image-text correspondence (e.g., product model numbers directly visible in images), masking could degrade true relevance matching.

### Mechanism 2
- Claim: Recomputing CLIP similarity after text masking improves downstream task accuracy on ImageNet distribution shifts.
- Mechanism: The masked CLIP filter recovers data that would be discarded by the baseline filter due to numerical contamination, thereby enriching the training set with high-quality samples that better represent the target distribution.
- Core assumption: The downstream ImageNet tasks benefit more from semantic consistency than from exact caption-image numerical alignment.
- Evidence anchors:
  - [abstract] "This approach is tested on a subset of the DataComp benchmark... achieving a 3.6% improvement on ImageNet distribution shifts."
  - [section] "Table 2... text-masked CLIP 40% dataset outperforms the CLIP 40% dataset."
- Break condition: If the downstream task inherently requires numerical precision (e.g., counting objects), the masking could remove critical signal.

### Mechanism 3
- Claim: Filtering at higher proportions (e.g., top 40%) mitigates the loss of high-quality data due to masking.
- Mechanism: By retaining more data, the filter balances the trade-off between removing numerical contamination and preserving samples with otherwise low CLIP scores that are still semantically relevant.
- Core assumption: The proportion of truly high-quality data with low CLIP scores due to numerical noise is small relative to the total dataset.
- Evidence anchors:
  - [section] "When filtering out 70% of the data, the CLIP filter outperforms our filter. However, when filtering out 60% or 50% of the data, our filter performs better."
  - [section] "The impact of numbers on CLIP and their handling provide valuable insights for improving the effectiveness of CLIP training, including language rewrite techniques."
- Break condition: If the dataset is already highly curated, aggressive masking at high proportions could degrade overall quality.

## Foundational Learning

- Concept: CLIP similarity computation via cosine distance in joint embedding space.
  - Why needed here: The filter relies on CLIP's text-image similarity scores; understanding how they are computed clarifies why numerical text skews results.
  - Quick check question: What embedding model and pooling strategy does CLIP use to produce image and text vectors before cosine similarity?

- Concept: Text preprocessing for CLIP (tokenization, special tokens, sequence length).
  - Why needed here: The masker must preserve valid CLIP tokenization; improper masking could break embeddings.
  - Quick check question: How does CLIP tokenize numbers and bracketed content, and what happens if you remove them mid-sequence?

- Concept: Data filtering ratios and their impact on training set size vs. quality.
  - Why needed here: The method explicitly trades dataset size for quality; understanding this balance is critical for setting the filtering proportion.
  - Quick check question: How does reducing dataset size by a given percentage affect convergence and final accuracy in vision-language pretraining?

## Architecture Onboarding

- Component map:
  Raw image-text pairs -> Text masker (remove numbers and bracketed content) -> CLIP encoder (ViT-L/14) -> Cosine similarity scorer -> Filter selector (top-k by similarity)

- Critical path:
  1. Apply text masker to captions
  2. Compute CLIP embeddings for masked text and original image
  3. Calculate cosine similarity
  4. Sort by similarity and select top fraction

- Design tradeoffs:
  - Masking vs. preserving semantic integrity: Aggressive masking risks removing informative tokens
  - Model size vs. speed: Using ViT-L/14 gives better scores but is slower than smaller CLIP variants
  - Dataset fraction vs. quality: Higher fractions retain more data but may include more noise

- Failure signatures:
  - Drop in similarity scores after masking indicates over-aggressive removal
  - Unexpectedly low downstream accuracy suggests masking removed critical content
  - Memory spikes during embedding computation point to inefficient batching

- First 3 experiments:
  1. Compare CLIP scores on a small sample before and after masking to confirm bias reduction
  2. Run the filter at 30% and 40% ratios and measure ImageNet accuracy gains
  3. Visualize embedding distances for masked vs. unmasked text to quantify feature space changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the loss of high-quality data caused by the text-masking process be minimized while maintaining the benefits of removing numerical noise?
- Basis in paper: [explicit] The authors acknowledge that their method may result in the loss of some high-quality data and suggest future research explore strategies to prevent this.
- Why unresolved: The paper presents a trade-off between removing numerical noise and preserving high-quality data, but does not offer concrete solutions to minimize this loss.
- What evidence would resolve it: Experiments comparing different text-masking strategies (e.g., more selective masking, context-aware masking) and their impact on both data quality and CLIP score improvements would provide insights into optimizing this trade-off.

### Open Question 2
- Question: What is the specific relationship between redundant numerical information in text and corresponding elements in images, and how can this be leveraged to improve CLIP training?
- Basis in paper: [explicit] The authors suggest investigating the correspondence between redundant information in text (such as numbers and bracketed contents) and specific elements in images to inspire techniques for enhancing CLIP training through language rewrite.
- Why unresolved: The paper identifies a correlation between numerical text and CLIP scores but does not delve into the underlying mechanisms or potential applications for training improvements.
- What evidence would resolve it: Detailed analysis of image-text pairs where numerical information is removed or altered, coupled with changes in CLIP scores and downstream task performance, would shed light on this relationship and its potential for improving CLIP training.

### Open Question 3
- Question: How does the proposed text-masking CLIP filter perform on larger-scale datasets beyond the subset of DataComp benchmark used in the experiments?
- Basis in paper: [inferred] The authors demonstrate the effectiveness of their method on a subset of the DataComp benchmark, but do not evaluate its performance on larger-scale datasets.
- Why unresolved: The scalability and generalizability of the proposed method to larger datasets with diverse characteristics remain unknown.
- What evidence would resolve it: Applying the text-masking CLIP filter to larger-scale datasets and comparing its performance against the standard CLIP score filter in terms of data quality and downstream task performance would provide insights into its scalability and generalizability.

## Limitations

- Method effectiveness appears tied to specific dataset characteristics with numerical noise that doesn't correspond to image content
- Improvement measured only on ImageNet distribution shifts, leaving unclear whether gains generalize to other vision-language tasks
- Text masking approach is relatively simple (numbers and bracketed content) without exploring more sophisticated text cleaning methods
- Computational overhead of recomputing CLIP scores on masked text isn't quantified for web-scale dataset applications

## Confidence

**High confidence**: The core observation that numerical text content biases CLIP similarity scores is well-supported by the ablation showing performance degradation when masking removes essential content. The 3.6% improvement on ImageNet distribution shifts at the 40% filtering level is statistically significant and reproducible based on the described methodology.

**Medium confidence**: The claim that this approach generalizes to other vision-language tasks and dataset types. While the mechanism is sound, the specific performance gains may vary significantly depending on the prevalence of numerical noise in target datasets.

**Low confidence**: The assertion that this is the optimal or most efficient approach to addressing numerical contamination. The paper doesn't benchmark against alternative text cleaning methods or explore whether fine-tuning CLIP to be more robust to numerical text would be more effective.

## Next Checks

1. **Cross-task validation**: Apply the text-masked CLIP filter to a diverse set of vision-language tasks (e.g., VQA, image retrieval) beyond ImageNet to verify the robustness of the improvement across different downstream applications.

2. **Dataset dependency analysis**: Systematically vary the proportion of numerical noise in synthetic test datasets to determine the minimum threshold at which the masking approach provides measurable benefits versus when it becomes counterproductive.

3. **Alternative filtering comparison**: Implement and compare against other text filtering strategies (e.g., semantic filtering, frequency-based removal, or fine-tuned text classifiers) to establish whether the proposed method represents the best trade-off between effectiveness and computational efficiency.