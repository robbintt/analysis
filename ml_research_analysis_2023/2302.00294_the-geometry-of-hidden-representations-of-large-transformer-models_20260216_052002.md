---
ver: rpa2
title: The geometry of hidden representations of large transformer models
arxiv_id: '2302.00294'
source_url: https://arxiv.org/abs/2302.00294
tags:
- representations
- layers
- peak
- data
- protein
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the geometry of hidden representations
  in large transformer models across different layers and tasks. The authors analyze
  the intrinsic dimension (ID) and neighbor composition of representations in transformers
  trained on protein language tasks and image reconstruction tasks.
---

# The geometry of hidden representations of large transformer models

## Quick Facts
- arXiv ID: 2302.00294
- Source URL: https://arxiv.org/abs/2302.00294
- Reference count: 40
- Primary result: Transformer representations evolve through three phases (expansion, compression, decoding) with a plateau phase of low ID and high semantic content

## Executive Summary
This paper investigates the geometry of hidden representations in large transformer models across different layers and tasks. The authors analyze the intrinsic dimension (ID) and neighbor composition of representations in transformers trained on protein language tasks and image reconstruction tasks. They find that the representations evolve in three distinct phases: a peak phase where the data manifold expands, a plateau phase where the ID contracts and remains low, and a final ascent phase. The plateau phase is characterized by low ID values and high semantic content, such as class labels for images and remote homology for proteins. The authors demonstrate that the ID profile can be used to identify the layers that maximize semantic content without supervision. This finding has implications for understanding the computational strategies of transformers and leveraging intermediate representations for downstream tasks.

## Method Summary
The authors analyze transformer representations using the TwoNN algorithm to estimate intrinsic dimension (ID) and calculate neighborhood overlap between consecutive layers. They use pre-trained ESM-2 protein language models (35M, 650M, 3B parameters) and iGPT models (76M, 455M, 1.4B parameters) on datasets including ProteinNet, SCOPe, and ImageNet. Representations are extracted after normalization in each block, and global average pooling is applied for variable-length sequences. The ID profiles and neighborhood overlap are computed across all layers, and semantic content is measured through overlap with ground truth labels.

## Key Results
- Transformer representations evolve through three distinct phases: expansion (peak), compression (plateau), and decoding (ascent)
- The plateau phase is characterized by low ID values (approximately 50-100) and high semantic content across protein and image tasks
- The ID profile remains consistent across models spanning two orders of magnitude in size, suggesting a universal representational strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hidden representations evolve through three distinct phases in transformer models: expansion, compression, and decoding.
- Mechanism: Early layers increase intrinsic dimension (ID) to expand the data manifold, intermediate layers compress ID to create semantically rich representations, and final layers reconstruct outputs from compressed representations.
- Core assumption: The self-supervised task of reconstructing masked inputs drives the model to discover an efficient encoding-decoding strategy that naturally separates semantic content from reconstruction detail.
- Evidence anchors:
  - [abstract]: "In the first layers, the data manifold expands, becoming high-dimensional, and then it contracts significantly in the intermediate layers."
  - [section]: "the reconstruction task is performed in three phases: 1) a data expansion in a high intrinsic dimension space... 2) a compression phase that projects the data in low-dimensional, semantically meaningful representations."
  - [corpus]: Weak evidence; corpus lacks direct mechanistic studies of transformer layer-wise evolution.
- Break condition: If the model architecture or training objective fundamentally changes (e.g., non-autoregressive reconstruction), the three-phase pattern may not emerge.

### Mechanism 2
- Claim: Low intrinsic dimension in intermediate layers correlates with high semantic content in representations.
- Mechanism: As ID decreases during the plateau phase, the representation space becomes more structured, concentrating on abstract features like protein homology or image class labels, which are easier to linearly separate.
- Core assumption: Semantic information is naturally more compact and lower-dimensional than raw feature data, so representations encoding it will have lower ID.
- Evidence anchors:
  - [abstract]: "the plateau phase is characterized by low ID values and high semantic content, such as class labels for images and remote homology for proteins."
  - [section]: "we find that the representations evolve similarly in transformers trained on protein language tasks and image reconstruction tasks... the plateau phase, we find a remarkable quantitative consensus on ID values which are approximately identical in models spanning two orders of magnitude in size."
  - [corpus]: Weak evidence; corpus does not directly test ID-semantic correlation.
- Break condition: If semantic content is distributed across multiple dimensions rather than concentrated, ID may not reliably indicate semantic richness.

### Mechanism 3
- Claim: Neighborhood composition changes rapidly in early and late layers, slowly in the plateau phase.
- Mechanism: The fast neighborhood rearrangement in the peak and final ascent phases reflects the model's active transformation of data structure, while the slow change in the plateau indicates stable, semantically meaningful representations.
- Core assumption: The degree of neighborhood overlap between consecutive layers reflects the stability of the representation space, with low overlap indicating major structural changes.
- Evidence anchors:
  - [section]: "the representations evolve at a much lower rate... for large enough models, the neighborhood composition changes very slowly, with more than 90% of neighbors shared across consecutive layers."
  - [section]: "Quite remarkably, χ remains of order 0.5 in the first 40% of the layers. This means that in each layer the neighborhood composition changes by~ 50 %, indicating that the representations are largely modified in every layer, in the first part of the network."
  - [corpus]: Weak evidence; corpus lacks studies of layer-wise neighborhood dynamics.
- Break condition: If the model's attention mechanism or layer structure is altered, the pattern of neighborhood change may not hold.

## Foundational Learning

- Concept: Intrinsic dimension (ID) estimation using the TwoNN algorithm
  - Why needed here: ID quantifies the effective dimensionality of data representations, revealing how information is compressed or expanded across layers.
  - Quick check question: How does the TwoNN algorithm use nearest neighbor distances to estimate intrinsic dimension?
- Concept: Neighbor overlap as a measure of representation stability
  - Why needed here: Neighbor overlap tracks how much the local structure of the data changes between layers, indicating when major transformations occur.
  - Quick check question: What does a low neighbor overlap between consecutive layers indicate about the transformation applied?
- Concept: Self-supervised learning and masked language modeling
  - Why needed here: The training objective drives the model to discover efficient representations, shaping the geometry of hidden states.
  - Quick check question: How does the masked language modeling objective influence the structure of intermediate representations?

## Architecture Onboarding

- Component map:
  - Input embedding layer → Stack of identical self-attention blocks → Output projection layer
  - Each block: Multi-head attention + Feed-forward network + Layer normalization
  - Representations extracted after normalization in each block
- Critical path:
  - For ID analysis: Input → Each block → Extract representation → Compute ID → Plot ID curve
  - For semantic analysis: Input → Each block → Extract representation → Compute neighbor overlap with labels → Identify peak semantic content
- Design tradeoffs:
  - Fixed embedding dimension across layers simplifies ID comparison but may limit expressiveness
  - Global average pooling for variable-length sequences reduces dimensionality but may lose positional information
- Failure signatures:
  - ID curve does not show three distinct phases → Model architecture or training may differ significantly
  - Neighbor overlap remains high throughout → Model may not be learning efficient compression
  - Semantic content does not peak in intermediate layers → Representations may not be structured as expected
- First 3 experiments:
  1. Compute ID curves for a small transformer on a simple dataset to verify three-phase pattern
  2. Measure neighbor overlap between consecutive layers to confirm rapid changes in early/late layers
  3. Test if intermediate layer representations improve downstream task performance compared to input/output layers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise computational role of the second peak in the ID profile observed in iGPT models, and why does it only emerge in later stages of training?
- Basis in paper: [inferred] The authors mention that the second peak is "akin to a symmetric autoencoder" and note that it "appears only in the latest phases of the training."
- Why unresolved: The authors state that this scenario "should be validated by further experiments" and do not provide a definitive explanation for the second peak's computational role or its emergence during training.
- What evidence would resolve it: Experiments analyzing the representations in the second peak layers and comparing them to the first peak and output layers, as well as tracking the emergence of the second peak during training, would help elucidate its computational role.

### Open Question 2
- Question: How do the representations in the ID minima of large transformers compare to those generated by supervised convolutional networks trained on the same datasets?
- Basis in paper: [explicit] The authors state, "The analysis we performed can be further reinforced, for example, by analyzing the similarity of representations in the ID minima with those generated in supervised convolutional networks."
- Why unresolved: The authors did not perform this analysis and suggest it as a direction for future work to "gain further understanding of the precise role of the second peak in the large iGPT model."
- What evidence would resolve it: Comparing the representations in the ID minima of large transformers to those of supervised convolutional networks using metrics such as nearest neighbor accuracy or linear probing would provide insights into the similarity and differences between the two approaches.

### Open Question 3
- Question: What is the exact mechanism by which the intrinsic dimension of representations decreases during training, and how does this relate to the emergence of semantic information?
- Basis in paper: [inferred] The authors discuss the evolution of the ID during training, noting that "the ID of the plateau layers substantially decreases" and that this behavior is "tightly related to the emergence of semantic information."
- Why unresolved: While the authors observe a correlation between the decrease in ID and the emergence of semantic information, they do not provide a detailed explanation of the underlying mechanism driving this phenomenon.
- What evidence would resolve it: Investigating the changes in the representations during training using techniques such as singular value decomposition or analyzing the contribution of individual dimensions to the semantic information would help elucidate the mechanism behind the decrease in ID and the emergence of semantic content.

## Limitations

- The universality of the three-phase model across different architectures and tasks remains untested
- Semantic content measurement relies on overlap with ground truth labels, which may not capture full semantic complexity
- Experimental evidence for scale consistency is limited to three model sizes

## Confidence

- High: The existence of a plateau phase with low ID and high semantic content in transformer representations
- Medium: The universality of the three-phase model across different tasks and architectures
- Low: The claim that the ID profile can be used as a reliable unsupervised method for identifying layers with maximum semantic content

## Next Checks

1. Test the three-phase model on transformers with different attention mechanisms (e.g., sparse attention, linear attention) to assess its robustness to architectural variations
2. Develop a more nuanced measure of semantic content that goes beyond label overlap, such as probing tasks or intrinsic evaluation metrics
3. Conduct a systematic study of the effect of model scale on the ID profile and the three-phase pattern, using a wider range of model sizes