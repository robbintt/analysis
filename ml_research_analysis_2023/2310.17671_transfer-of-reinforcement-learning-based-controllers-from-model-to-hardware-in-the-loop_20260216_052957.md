---
ver: rpa2
title: Transfer of Reinforcement Learning-Based Controllers from Model- to Hardware-in-the-Loop
arxiv_id: '2310.17671'
source_url: https://arxiv.org/abs/2310.17671
tags:
- training
- agents
- agent
- reward
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates the transfer of RL-based controllers from
  model-in-the-loop (MiL) to hardware-in-the-loop (HiL) for transient EGR control
  in internal combustion engines. A pre-trained RL agent from MiL is fine-tuned in
  HiL using transfer learning, with reward function adjustments to account for real-world
  differences.
---

# Transfer of Reinforcement Learning-Based Controllers from Model- to Hardware-in-the-Loop

## Quick Facts
- arXiv ID: 2310.17671
- Source URL: https://arxiv.org/abs/2310.17671
- Reference count: 40
- This paper demonstrates the transfer of RL-based controllers from model-in-the-loop (MiL) to hardware-in-the-loop (HiL) for transient EGR control in internal combustion engines.

## Executive Summary
This paper presents a novel approach for transferring reinforcement learning (RL) controllers from model-in-the-loop (MiL) to hardware-in-the-loop (HiL) environments, specifically for transient exhaust gas recirculation (EGR) control in internal combustion engines. The method combines pre-training in a simulated environment with fine-tuning in real hardware, achieving a 5.9x reduction in training time while maintaining comparable emission reduction performance. The approach addresses the challenge of domain differences between simulation and real hardware by adjusting reward function parameters during the transfer process.

## Method Summary
The method involves pre-training RL agents in a MiL environment using the Proximal Policy Optimization (PPO) algorithm on a mean value engine model, then transferring the pre-trained agents to a HiL setup with real ECU hardware. During transfer, reward function parameters are adjusted to account for differences between the simulated and real environments. The approach is validated using a 2.0L compression ignition engine with single-stage turbocharger and HP/LP EGR systems, measuring NOx and soot emissions, boost pressure, and vehicle speed performance.

## Key Results
- Transferred RL agents achieve comparable emission reduction to purely HiL-trained agents
- 5.9x reduction in training time compared to pure HiL training (72 vs 189 hours)
- Fine-tuning reward parameters during transfer is crucial for achieving balanced emissions trade-off
- Pre-trained agent maturity level significantly affects transfer learning effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer Learning from Model-in-the-Loop to Hardware-in-the-Loop significantly reduces training time while maintaining control performance.
- Mechanism: The pre-trained RL agent in the MiL environment learns a base policy that captures the essential control strategies. This policy is then fine-tuned in the HiL environment, where real hardware components introduce domain-specific variations. By leveraging the pre-trained policy, the agent requires fewer interactions with the real hardware to adapt to the new environment.
- Core assumption: The MiL environment adequately captures the essential dynamics of the real system, allowing for effective transfer of the learned policy.
- Evidence anchors:
  - [abstract] "The transferred agent achieves comparable emission reduction to a purely HiL-trained agent while requiring 5.9x less training time."
  - [section IV.B.2] "The findings prove that the combination of MiL pre-training with TL improves the training process, enabling the attainment of a high-performing agent in much shorter time (72 vs. 189 hours), while achieving up to 5 % emissions reduction."
- Break condition: If the MiL model significantly deviates from the real system dynamics, the transferred policy may not be effective and require extensive retraining.

### Mechanism 2
- Claim: Adjusting reward parameters during the transfer process is crucial for achieving a balanced emissions trade-off in the HiL environment.
- Mechanism: The reward function used in the MiL environment may not be optimal for the HiL environment due to differences in real hardware characteristics. Fine-tuning the reward parameters, particularly the weights associated with emissions penalties, allows the agent to adapt its behavior to the new environment and achieve a better balance between NOx and soot emissions.
- Core assumption: The reward function can be adapted to account for the differences between the MiL and HiL environments without compromising the overall control objectives.
- Evidence anchors:
  - [section III.A] "The reward function focuses on emissions formation (NOx and soot trade-off), engine performance, combustion stability, and potential ECU diagnostic trouble codes."
  - [section IV.B.1] "The zero-shot transfer of MiL agents revealed that the domain differences between the virtual and real ECU require further optimization of the reward function."
- Break condition: If the reward function is not adequately adjusted, the transferred agent may not achieve the desired emissions trade-off in the HiL environment.

### Mechanism 3
- Claim: The maturity level of the pre-trained agent in the MiL environment affects the effectiveness of the transfer learning process.
- Mechanism: Agents that have undergone more extensive training in the MiL environment (higher maturity) are better equipped to handle the challenges of the HiL environment. These mature agents have learned more robust and generalizable policies, which can be fine-tuned more efficiently in the HiL environment.
- Core assumption: The pre-training process in the MiL environment leads to the development of more mature and effective policies as the training progresses.
- Evidence anchors:
  - [section IV.B.2] "Examining the results, we note that Agent A performs well in terms of emissions and boost error, but it falls short in terms of total training duration. For the specific use case, this demonstrates that the transfer of an agent with inadequate MiL training is adverse to transferring a mature agent."
  - [section IV.A] "The MiL platform represents a safe and time-efficient source domain for TL, where mature policies are generated within the order of hours depending on the selected training segments and computing hardware."
- Break condition: If the pre-training process is insufficient or the MiL environment is not representative of the real system, even a mature agent may not transfer effectively to the HiL environment.

## Foundational Learning

- Concept: Reinforcement Learning (RL) fundamentals
  - Why needed here: Understanding the basic concepts of RL, such as states, actions, rewards, policies, and value functions, is essential for grasping the methodology and results of this paper.
  - Quick check question: What is the difference between on-policy and off-policy RL algorithms?

- Concept: Transfer Learning (TL) in RL
  - Why needed here: The paper focuses on transferring a pre-trained RL agent from a simulated environment (MiL) to a real-world environment (HiL). Understanding the principles and challenges of TL in RL is crucial for comprehending the approach and its effectiveness.
  - Quick check question: What are the main challenges associated with sim-to-real transfer in RL?

- Concept: Hardware-in-the-Loop (HiL) simulation
  - Why needed here: The paper utilizes HiL simulation to train and validate the RL agent on real hardware components. Familiarity with HiL concepts and their role in embedded system development is important for understanding the experimental setup and results.
  - Quick check question: How does HiL simulation differ from traditional software-in-the-loop (SiL) simulation?

## Architecture Onboarding

- Component map:
  MiL environment (Simulated engine model, RL agent, reward function) -> HiL environment (Real ECU, actuators, sensors, engine model, RL agent) -> Transfer mechanism (Policy parameter transfer, reward function adjustment) -> Evaluation (Emissions reduction, engine performance, training time)

- Critical path:
  1. Pre-train RL agent in MiL environment
  2. Adjust reward parameters for HiL environment
  3. Transfer pre-trained agent to HiL environment
  4. Fine-tune agent in HiL environment
  5. Evaluate performance and training time

- Design tradeoffs:
  - MiL model fidelity vs. computational efficiency
  - Reward function complexity vs. ease of adjustment
  - Pre-training duration vs. transfer effectiveness
  - HiL training time vs. performance gains

- Failure signatures:
  - Poor emissions reduction in HiL compared to MiL
  - High training time in HiL despite pre-training
  - Unstable control behavior in HiL
  - Simulation failures due to model inaccuracies

- First 3 experiments:
  1. Train RL agent in MiL environment with different algorithms (PPO, DDPG) and compare performance.
  2. Transfer pre-trained agent to HiL environment and fine-tune reward parameters to achieve balanced emissions trade-off.
  3. Compare training time and performance of transferred agent vs. agent trained from scratch in HiL environment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RL-based controllers compare to traditional model-based controllers in real-world driving conditions across diverse vehicle types and engine configurations?
- Basis in paper: [explicit] The paper compares RL agents to a production ECU reference controller in HiL simulations but does not test in real-world driving conditions or across different vehicle types.
- Why unresolved: The study is limited to HiL testing with a specific engine model, lacking real-world validation and generalization to other powertrain systems.
- What evidence would resolve it: Field tests of RL controllers in multiple vehicle types under real-world conditions, with comparative performance metrics against traditional controllers.

### Open Question 2
- Question: What are the long-term effects of using RL-based controllers on engine component wear and maintenance requirements?
- Basis in paper: [inferred] The paper does not address durability or maintenance implications of RL controllers, focusing only on immediate performance metrics like emissions and fuel consumption.
- Why unresolved: The study's timeframe and scope do not include degradation analysis or long-term reliability assessments of RL-based control strategies.
- What evidence would resolve it: Extended durability testing of engines with RL controllers, monitoring wear patterns and maintenance intervals compared to traditional control systems.

### Open Question 3
- Question: How do RL agents trained in one emission control domain (e.g., NOx reduction) perform when transferred to related but different domains (e.g., hybrid powertrain management)?
- Basis in paper: [explicit] The paper demonstrates transfer learning within the same emission control domain but does not explore cross-domain applicability.
- Why unresolved: The study focuses on a single use case (EGR control) without investigating the transferability of learned policies to other powertrain control tasks.
- What evidence would resolve it: Comparative testing of EGR-trained RL agents on hybrid powertrain optimization, measuring performance and required retraining time across domains.

## Limitations

- The domain shift between MiL and HiL environments remains a significant challenge, particularly in capturing complex engine dynamics and transient behaviors.
- The computational resources and time investment for both MiL pre-training and HiL fine-tuning are substantial, potentially limiting industrial adoption.
- The reward function adjustments, while effective, require careful tuning that may not generalize across different engine configurations or operating conditions.

## Confidence

- High Confidence: The 5.9x reduction in training time compared to pure HiL training is well-supported by experimental results and clearly demonstrates the effectiveness of the transfer learning approach.
- Medium Confidence: The claim of achieving comparable emission reduction to purely HiL-trained agents is supported by data but may be sensitive to specific engine configurations and operating conditions.
- Medium Confidence: The effectiveness of reward function adjustments is demonstrated but may require extensive recalibration for different hardware setups.

## Next Checks

1. Cross-Platform Validation: Test the transfer learning approach on different engine configurations and ECU hardware to assess generalizability and identify platform-specific requirements.

2. Long-term Performance Monitoring: Evaluate the transferred agents' performance over extended periods and varying environmental conditions to ensure sustained effectiveness and identify potential degradation patterns.

3. Scalability Assessment: Investigate the computational and time requirements for scaling the approach to more complex engine systems with additional actuators and sensors.