---
ver: rpa2
title: 'CSMeD: Bridging the Dataset Gap in Automated Citation Screening for Systematic
  Literature Reviews'
arxiv_id: '2311.12474'
source_url: https://arxiv.org/abs/2311.12474
tags:
- datasets
- screening
- review
- dataset
- reviews
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present CSMeD, a meta-dataset consolidating nine publicly
  released citation screening collections, providing unified access to 325 systematic
  literature reviews (SLRs) from medicine and computer science. CSMeD addresses the
  lack of standardized evaluation datasets in automated citation screening, which
  hinders progress in this field.
---

# CSMeD: Bridging the Dataset Gap in Automated Citation Screening for Systematic Literature Reviews

## Quick Facts
- **arXiv ID:** 2311.12474
- **Source URL:** https://arxiv.org/abs/2311.12474
- **Reference count:** 40
- **Primary result:** MiniLM outperforms BM25, and GPT-4-8k achieves highest performance on full text screening task

## Executive Summary
The authors present CSMeD, a meta-dataset consolidating nine publicly released citation screening collections into 325 systematic literature reviews from medicine and computer science. This addresses the critical lack of standardized evaluation datasets in automated citation screening. The paper extends CSMeD with additional metadata and introduces CSMeD-FT, a new dataset for full text screening. The authors establish baselines using BM25 and MiniLM models, and evaluate large language models (GPT-3.5, GPT-4, and transformer models) on the full text screening task, demonstrating that transformer-based models significantly outperform traditional approaches.

## Method Summary
The paper consolidates nine publicly released citation screening datasets into CSMeD, providing unified access with standardized splits and metadata. The authors extend this with metadata extraction from Cochrane protocols and create CSMeD-FT for full text screening evaluation. Baseline experiments use BM25 and MiniLM models on CSMeD, while transformer models (Longformer, BigBird, Clinical-Longformer, Clinical-BigBird) are fine-tuned on CSMeD-FT-TRAIN. Zero-shot evaluation of GPT models (GPT-3.5-turbo-0301, GPT-4-8k, GPT-3.5-turbo-16k) is conducted on CSMeD-FT using precision, recall, and F1-score metrics.

## Key Results
- MiniLM significantly outperforms BM25 in citation screening tasks
- Using systematic review abstracts as queries achieves highest performance for both BM25 and MiniLM models
- GPT-4-8k achieves the highest performance on the full text screening task
- Transformer models capture semantic similarity better than keyword-based approaches for complex eligibility criteria

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CSMeD enables fair benchmarking by unifying multiple citation screening datasets into a single meta-dataset with standardized splits and metadata.
- **Mechanism:** Consolidation of nine publicly released collections into 325 SLRs provides consistent API access, deduplication, and harmonized metadata, mitigating issues like data leakage and dataset overlap.
- **Core assumption:** The datasets being unified are sufficiently compatible in structure and labeling conventions.
- **Evidence anchors:** Abstract states CSMeD "serves as a comprehensive resource for training and evaluating automated citation screening models"; weak evidence from corpus section with no direct citation overlap statistics.
- **Break condition:** If unified datasets contain incompatible annotation schemas or consolidation introduces systematic labeling errors.

### Mechanism 2
- **Claim:** Extending metadata with eligibility criteria enables evaluation of screening beyond binary classification.
- **Mechanism:** By extracting eligibility criteria from Cochrane protocols and pairing them with full text publications, CSMeD-FT transforms screening into a long document natural language inference task.
- **Core assumption:** Eligibility criteria expressed in natural language are sufficiently detailed and consistent to serve as ground truth.
- **Evidence anchors:** Abstract describes CSMeD-FT as "designed explicitly for evaluating the full text publication screening task"; weak evidence from corpus section without detailed validation of criteria quality.
- **Break condition:** If eligibility criteria are ambiguous, incomplete, or inconsistently applied across reviews.

### Mechanism 3
- **Claim:** MiniLM and LLM models significantly outperform traditional BM25 in citation screening.
- **Mechanism:** Transformer-based models capture semantic similarity between review protocols and publications, enabling more accurate relevance prediction than keyword-based BM25.
- **Core assumption:** The semantic relationships captured by transformer models align with human judgment of relevance.
- **Evidence anchors:** Abstract states "Results show that MiniLM outperforms BM25, and GPT-4-8k achieves the highest performance on the full text screening task"; moderate evidence from corpus section providing comparative performance metrics.
- **Break condition:** If semantic understanding doesn't translate to practical screening accuracy or models overfit to specific review styles.

## Foundational Learning

- **Concept:** Systematic Literature Review (SLR) process and citation screening workflow
  - **Why needed here:** Understanding the two-stage screening process (title/abstract vs. full text) is crucial for interpreting dataset structure and evaluation metrics.
  - **Quick check question:** What are the two main stages of citation screening in SLRs, and how do they differ in terms of document attributes considered?

- **Concept:** Binary classification vs. question answering formulation of screening tasks
  - **Why needed here:** The paper presents both formulations and demonstrates how metadata extension enables QA-style evaluation.
  - **Quick check question:** How does formulating citation screening as a question-answering task differ from treating it as binary classification, and what additional metadata enables this?

- **Concept:** Natural Language Inference (NLI) and its application to long document understanding
  - **Why needed here:** CSMeD-FT is framed as an NLI task with premises (review protocols) averaging >1000 words and hypotheses (publications) averaging >4000 words.
  - **Quick check question:** How does CSMeD-FT extend traditional NLI to handle documents an order of magnitude longer than typical NLI datasets?

## Architecture Onboarding

- **Component map:** Data loaders (BigBio framework) -> Metadata extraction pipeline (Cochrane protocols) -> Full text acquisition (SemanticScholar/CORE APIs + GROBID parsing) -> Model evaluation framework (nDCG, MAP, Recall, TNR@95%) -> CSMeD-FT construction pipeline (time-based splits, document pairing)

- **Critical path:** Dataset creation → Metadata extension → Model evaluation → CSMeD-FT construction → LLM evaluation

- **Design tradeoffs:** Using existing datasets vs. creating new gold standard; time-based splits for CSMeD-FT vs. random splits; focus on Cochrane reviews vs. broader domain coverage; API-based full text acquisition vs. manual curation

- **Failure signatures:** Poor model performance indicating dataset quality issues; high overlap between train/test splits indicating data leakage; inconsistent metadata extraction suggesting protocol parsing errors; API rate limiting or content access failures

- **First 3 experiments:**
  1. Evaluate BM25 vs. MiniLM on CSMeD-DEV-COCHRANE using different query representations (title, abstract, criteria)
  2. Test transformer model fine-tuning on CSMeD-FT-TRAIN and evaluate on CSMeD-FT-DEV using macro F1
  3. Run zero-shot LLM evaluation on CSMeD-FT-TEST-SMALL comparing GPT-4-8k, GPT-3.5-turbo-16k, and fine-tuned transformers

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we improve the quality and quantity of metadata available for systematic literature reviews, particularly eligibility criteria and reasons for exclusion?
- **Basis in paper:** [explicit] The paper discusses the importance of metadata like eligibility criteria and reasons for exclusion, but notes that these are often missing from existing datasets.
- **Why unresolved:** Collecting and standardizing this metadata is a time-consuming manual process that is not well-supported by current tools and practices.
- **What evidence would resolve it:** Development of better tools and protocols for extracting and documenting metadata from SLR protocols and reports.

### Open Question 2
- **Question:** How can we design and evaluate automated screening systems that can effectively handle the full text of publications, rather than just titles and abstracts?
- **Basis in paper:** [explicit] The paper introduces CSMeD-FT, a dataset for full text screening, and notes that current models struggle with the increased length and complexity of full text documents.
- **Why unresolved:** Full text screening is a challenging task that requires models to understand long documents and complex eligibility criteria. The paper shows that current models have room for improvement.
- **What evidence would resolve it:** Development of models that can effectively process long documents and achieve high performance on full text screening tasks.

### Open Question 3
- **Question:** How can we ensure the fairness and reliability of automated screening systems, particularly in terms of avoiding bias and data leakage?
- **Basis in paper:** [explicit] The paper discusses the challenges of dataset overlap and data leakage, which can lead to biased and unreliable results.
- **Why unresolved:** Automated screening systems have the potential to introduce bias and errors, and there is a need for rigorous evaluation and validation to ensure their fairness and reliability.
- **What evidence would resolve it:** Development of evaluation protocols and metrics that can detect and mitigate bias and data leakage in automated screening systems.

## Limitations

- The evaluation focuses on Cochrane reviews for full text screening, which may not generalize to other domains or review protocols
- Superior performance of transformer models and LLMs requires further validation on real-world screening tasks with diverse review styles
- Manual extraction of eligibility criteria from protocols introduces potential human error and inconsistency without quantified impact

## Confidence

- **High:** Consolidation of nine citation screening datasets into CSMeD and establishment of baseline models (BM25 vs. MiniLM)
- **Medium:** Extension of CSMeD with metadata and creation of CSMeD-FT lacks detailed validation of criteria quality
- **Medium:** Superior performance of transformer models and LLMs demonstrated on datasets but requires real-world validation

## Next Checks

1. Conduct inter-annotator agreement studies on eligibility criteria extraction to quantify human consistency and potential bias
2. Perform ablation studies on CSMeD-FT to determine the relative contribution of review protocols versus abstracts as query representations
3. Test the generalization of CSMeD-FT models on a held-out set of non-Cochrane SLRs to assess domain transferability