---
ver: rpa2
title: Goal Space Abstraction in Hierarchical Reinforcement Learning via Reachability
  Analysis
arxiv_id: '2309.07168'
source_url: https://arxiv.org/abs/2309.07168
tags:
- learning
- goal
- representation
- space
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GARA learns a hierarchical reinforcement learning policy alongside
  an interpretable goal space representation. It uses reachability analysis to automatically
  partition continuous state space into discrete goals, refining regions where states
  exhibit similar behavior in reaching other goals.
---

# Goal Space Abstraction in Hierarchical Reinforcement Learning via Reachability Analysis

## Quick Facts
- arXiv ID: 2309.07168
- Source URL: https://arxiv.org/abs/2309.07168
- Authors: 
- Reference count: 10
- Key outcome: GARA learns a hierarchical reinforcement learning policy alongside an interpretable goal space representation. It uses reachability analysis to automatically partition continuous state space into discrete goals, refining regions where states exhibit similar behavior in reaching other goals. Evaluated on a U-shaped maze with sparse rewards, GARA successfully learns an interpretable goal space from exploration data, achieving data efficiency comparable to handcrafted representations and outperforming state-of-the-art HIRO in sample efficiency.

## Executive Summary
GARA (Goal Abstraction via Reachability Analysis) is a hierarchical reinforcement learning algorithm that automatically learns an interpretable goal space representation from exploration data. The method uses reachability analysis to partition continuous state space into discrete goals by recursively refining regions where states exhibit similar behavior in reaching other goals. GARA was evaluated on a U-shaped maze with sparse rewards and demonstrated data efficiency comparable to handcrafted representations while outperforming state-of-the-art HIRO.

## Method Summary
GARA is a Feudal HRL algorithm that learns two policies: a high-level policy πHigh that selects discrete goals from a learned goal space, and a low-level goal-conditioned policy πLow that achieves selected goals. The algorithm trains a forward model Fk to predict states reached after applying the low-level policy for k steps. It uses formal verification (Ai2) to compute output sets for input goal regions. If states within a region exhibit different reachability behaviors (some reach target goal, others don't), the region is recursively split until all states in each refined region share similar reachability properties. This process creates an interpretable goal space representation that improves data efficiency by decomposing tasks into easier-to-achieve subgoals.

## Key Results
- GARA learns an interpretable goal space from exploration data in a U-shaped maze environment
- Data efficiency is comparable to handcrafted representations and outperforms HIRO
- The learned goal space successfully captures task-relevant structure through reachability-based partitioning

## Why This Works (Mechanism)

### Mechanism 1
GARA learns a goal space representation that abstracts continuous state space into discrete goals by recursively refining regions based on reachability behavior. The algorithm trains a forward model to predict states reached after applying the low-level policy for k steps, then uses formal verification (Ai2) to compute output sets for input goal regions. If states within a region exhibit different reachability behaviors (some reach target goal, others don't), the region is recursively split until all states in each refined region share similar reachability properties. This approach assumes states exhibiting similar reachability behavior to other goals should be grouped together as they serve similar roles in task completion.

### Mechanism 2
The learned goal space improves data efficiency by decomposing the task into easier-to-achieve subgoals. By creating a goal space where each region contains states with similar reachability properties, the high-level policy can select goals that are more likely to be achievable by the low-level policy. This decomposition into simpler subgoals reduces the complexity of learning for both levels of the hierarchy. The core assumption is that abstracting state space into regions with similar reachability behaviors creates a more structured and efficient representation for goal selection.

### Mechanism 3
Concurrent learning of policy and goal space representation enables mutual improvement through exploration data. As the hierarchical policy explores the environment, it generates data that GARA uses to refine its goal space representation. This improved representation then helps the policy make better goal selections, creating a positive feedback loop where better policies generate more informative exploration data for representation learning. The core assumption is that exploration data from the hierarchical policy contains sufficient information to learn a meaningful goal space representation that captures task-relevant structure.

## Foundational Learning

- Concept: Feudal Hierarchical Reinforcement Learning (HRL)
  - Why needed here: GARA is built on feudal HRL structure with separate high-level goal selection and low-level goal achievement policies
  - Quick check question: What are the two levels of policy in feudal HRL and what does each level control?

- Concept: Reachability analysis in reinforcement learning
  - Why needed here: GARA uses reachability analysis to partition state space based on whether states can reach target goals under the low-level policy
  - Quick check question: How does GARA determine whether states within a region should be split into separate goals?

- Concept: Formal verification of neural networks
  - Why needed here: GARA uses Ai2 tool to compute output sets for input goal regions, enabling analysis of reachability over sets of states rather than individual states
  - Quick check question: Why does GARA need formal verification tools rather than just using the forward model predictions directly?

## Architecture Onboarding

- Component map: Forward model Fk -> Ai2 verification tool -> Goal space partitioner -> High-level policy πHigh -> Low-level policy πLow

- Critical path:
  1. Explore environment with current policies to collect data
  2. Train forward model on exploration data
  3. Use Ai2 to compute reachability sets for each goal region
  4. Recursively split regions where states have different reachability behaviors
  5. Update goal space representation
  6. Train policies using new goal space
  7. Repeat

- Design tradeoffs:
  - Granularity vs. interpretability: Finer partitions capture more task structure but may be harder to interpret
  - Computational cost: Formal verification is expensive but provides soundness guarantees
  - Exploration vs. exploitation: Need to balance exploring new areas with exploiting known good behaviors

- Failure signatures:
  - Poor forward model accuracy leads to incorrect reachability analysis
  - Recursive splitting creates too many small regions, making policy learning difficult
  - Goal space doesn't capture meaningful task structure, leading to no performance improvement

- First 3 experiments:
  1. Implement basic forward model training and evaluation on synthetic reachability data
  2. Add Ai2 integration to compute output sets for simple 2D grid environments
  3. Build recursive goal space partitioning with fixed forward model on U-shaped maze environment

## Open Questions the Paper Calls Out

### Open Question 1
How does the learned goal space representation generalize to more complex environments beyond the U-shaped maze? The authors state "we conduct the evaluation on a U-shaped maze with a continuous state space" but do not explore more complex environments. This remains unresolved because the paper only tests on a single, relatively simple environment. Testing GARA on multiple benchmark environments with varying complexity would demonstrate its generalization capabilities.

### Open Question 2
What is the computational complexity of GARA's reachability analysis using Ai2, and how does it scale with state space dimensionality? The authors mention using "Ai2 [10] that can compute the output of a neural network given a set of inputs" but don't analyze computational costs. This is critical for practical deployment and remains unresolved as the paper doesn't provide runtime analysis or scaling behavior as dimensions increase. Empirical studies measuring computation time and memory usage would resolve this.

### Open Question 3
How sensitive is GARA's performance to hyperparameters like the number of steps k used in the forward model Fk? The formulation defines "Rk(G, G′)" and "Fk : S × G → S" but doesn't report sensitivity analysis or ablation studies on k. This remains unresolved as the choice of k appears to be a critical design decision that could affect both learning speed and final performance, but no systematic evaluation is provided. Experiments varying k would reveal the parameter's importance.

## Limitations
- Computational scalability of formal verification for larger state spaces is uncertain
- Limited empirical validation to a single, relatively simple environment
- No analysis of how performance scales with state space dimensionality

## Confidence

Confidence in the core mechanism (Mechanism 1) is **Medium-High** - the reachability analysis approach is well-grounded in formal methods literature, though empirical validation is limited to a single environment. Confidence in data efficiency claims (Mechanism 2) is **Medium** - while results show improvement over HIRO, the comparison to handcrafted representations may not be entirely fair. Confidence in the concurrent learning framework (Mechanism 3) is **Low-Medium** - the paper describes the concept but doesn't provide ablation studies showing the importance of this interaction.

## Next Checks

1. **Scalability test**: Evaluate GARA on a grid-world environment with state space dimensions increased from 2D to 3D or 4D to assess computational tractability of formal verification

2. **Generalization analysis**: Test whether goal space representations learned on one maze configuration transfer to structurally similar but differently configured mazes

3. **Ablation study**: Compare GARA against versions that (a) learn goal space without formal verification, (b) learn policy without updating goal space, and (c) use random goal selection to isolate the contribution of each component