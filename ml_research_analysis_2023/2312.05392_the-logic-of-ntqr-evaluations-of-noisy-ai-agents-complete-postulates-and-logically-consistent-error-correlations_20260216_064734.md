---
ver: rpa2
title: 'The logic of NTQR evaluations of noisy AI agents: Complete postulates and
  logically consistent error correlations'
arxiv_id: '2312.05392'
source_url: https://arxiv.org/abs/2312.05392
tags:
- test
- classifiers
- binary
- evaluation
- independent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a mathematical framework for evaluating noisy
  AI agents using unlabeled data, addressing Plato's "ship of state" allegory in the
  context of AI safety. The key contribution is the development of complete algebraic
  postulates that relate unknown statistics of correctness in tests to observable
  responses, allowing for the logical consistency of evaluation algorithms to be proven
  or disproven.
---

# The logic of NTQR evaluations of noisy AI agents: Complete postulates and logically consistent error correlations

## Quick Facts
- arXiv ID: 2312.05392
- Source URL: https://arxiv.org/abs/2312.05392
- Reference count: 7
- One-line primary result: Algebraic postulates enable logically consistent evaluation of noisy AI agents without answer keys, with self-warning when assumptions fail

## Executive Summary
This paper presents a mathematical framework for evaluating noisy AI agents using unlabeled data, addressing Plato's "ship of state" allegory in the context of AI safety. The key contribution is the development of complete algebraic postulates that relate unknown statistics of correctness in tests to observable responses, allowing for the logical consistency of evaluation algorithms to be proven or disproven. The method is demonstrated on binary classifiers, showing how these postulates can be used to detect error correlations and build an exact evaluator based on the assumption of error independence.

## Method Summary
The framework constructs a complete set of algebraic postulates that relate unknown correctness statistics (like Ra_ai, the number of correct 'a' responses for classifier i) to observable test responses. These postulates are derived from the mathematical constraints imposed by the test structure and allow logical consistency to be proven or disproven without requiring an answer key. The framework can detect error correlations between classifiers by comparing individual performance statistics to joint response patterns, and provides a self-warning mechanism by producing irrational or imaginary numbers when assumptions (like error independence) are violated.

## Key Results
- Complete algebraic postulates can prove or disprove the logical consistency of unsupervised evaluation algorithms
- The framework can detect error correlations between binary classifiers and build exact evaluators assuming error independence
- Experiments on UCI adult and two-norm datasets show the algebraic evaluator can outperform majority voting while providing self-warning when assumptions are violated

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework constructs a complete set of algebraic postulates that relate unknown correctness statistics to observable test responses, allowing logical consistency to be proven or disproven without an answer key.
- Mechanism: By defining statistics like Ra_ai and Rb_bi, and deriving postulates like Ra_ai - Rb_bi = Rai - Rbi, the framework reduces the infinite validation chain to finite algebraic checks.
- Core assumption: The test responses follow the mathematical constraints imposed by the postulates.
- Evidence anchors:
  - [abstract] "We can construct complete postulates than can prove or disprove the logical consistency of any grading algorithm."
  - [section] "Complete sets of postulates can be constructed that can prove or disprove the logical consistency of any unsupervised evaluation algorithm."
- Break condition: If the observed responses violate the postulates (e.g., produce irrational numbers in independent evaluator equations), the assumption of error independence is broken.

### Mechanism 2
- Claim: The framework can detect error correlations between binary classifiers by comparing individual performance statistics to joint response patterns.
- Mechanism: Using pair correlation variables Γa_i,j and Γb_i,j, and the disentangled basis postulates, the framework computes the only logically consistent error correlation given individual evaluations.
- Core assumption: The pair correlation variables capture all relevant error dependencies between classifiers.
- Evidence anchors:
  - [section] "We can see that the test error correlations were small and both methods agree."
  - [section] "Here is one. Note that the independent AE evaluator is a better estimator of performance and correlations, but not of prevalence."
- Break condition: If the pair correlation estimates from different methods diverge significantly, the assumption of accurate correlation capture may be violated.

### Mechanism 3
- Claim: The framework provides a self-warning mechanism by producing irrational or imaginary numbers when assumptions (like error independence) are violated.
- Mechanism: The independent evaluator equations contain square root terms that resolve to rational numbers only if the classifiers are truly error independent. Imaginary or unresolved square roots signal assumption violations.
- Core assumption: Rational evaluations correspond to logically consistent scenarios under the assumed model.
- Evidence anchors:
  - [abstract] "Experiments on UCI adult and two-norm datasets show that the proposed algebraic evaluator can outperform majority voting in some cases, while also providing a self-warning mechanism when assumptions are violated, as indicated by the appearance of irrational numbers."
  - [section] "Here the error correlation between classifiers 1 and 2 is large enough that it triggered the independent evaluator to produce an imaginary number."
- Break condition: If the independent evaluator produces irrational or imaginary results, the assumption of error independence is definitively broken.

## Foundational Learning

- Concept: Binary classification and multiple-choice exam transformation
  - Why needed here: The framework treats binary classification tests as binary response multiple-choice exams to simplify the mathematical formulation and visualization of the evaluation geometry.
  - Quick check question: If a binary classifier labels 1000 items with 400 as 'a' and 600 as 'b', what are the corresponding Qa, Rai, and Rbi values if the true labels are 450 'a' and 550 'b' with 350 correct 'a' responses and 500 correct 'b' responses?

- Concept: Algebraic postulates and their completeness
  - Why needed here: The framework relies on constructing a complete set of algebraic postulates that relate unknown correctness statistics to observable responses, allowing logical consistency to be proven or disproven.
  - Quick check question: Given a binary classifier with Rai=300, Rbi=700, and Qa=400, what is the value of Ra_ai - Rb_bi if the true label distribution is 50% 'a' and 50% 'b'?

- Concept: Error correlation and independence
  - Why needed here: The framework quantifies and detects error correlations between classifiers, and uses the assumption of error independence to construct an exact evaluator that can self-warn when the assumption is violated.
  - Quick check question: If two binary classifiers have individual accuracies of 80% and 90%, and their joint accuracy is 72%, are they error independent? If not, what is their error correlation?

## Architecture Onboarding

- Component map:
  - Data input: Binary classifier responses to test items
  - Preprocessor: Transforms responses into Qa, Rai, Rbi format
  - Postulate generator: Constructs complete algebraic postulates for given (N, T, Q, R)
  - Consistency checker: Verifies if observed responses satisfy postulates
  - Evaluator: Computes classifier rankings based on postulate satisfaction
  - Self-warning: Detects assumption violations via irrational/imaginary outputs

- Critical path:
  1. Input classifier responses
  2. Transform to Qa, Rai, Rbi format
  3. Generate complete postulates
  4. Check consistency
  5. Compute rankings
  6. Detect assumption violations

- Design tradeoffs:
  - Completeness vs. complexity: More complete postulates capture more information but are harder to compute
  - Assumption specificity vs. generality: More specific assumptions (e.g., error independence) enable exact solutions but may not hold in practice
  - Transparency vs. performance: Algebraic postulates are transparent but may not always outperform other methods

- Failure signatures:
  - Postulate violations: Observed responses don't satisfy generated postulates
  - Irrational/imaginary outputs: Self-warning mechanism triggered by assumption violations
  - Inconsistent rankings: Different evaluation methods produce conflicting results

- First 3 experiments:
  1. Implement postulate generation and consistency checking for (N=1, T=1, Q=1000, R=2) case
  2. Extend to (N=2, T=1, Q=1000, R=2) case and implement pair correlation detection
  3. Implement independent evaluator for (N=3, T=1, Q=1000, R=2) case and test self-warning mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the algebraic postulates framework be extended to evaluate AI agents with more than two possible responses (R > 2)?
- Basis in paper: [explicit] The paper mentions that the R = 2 format is used to illustrate concepts easily generalized to the case of R > 2, but does not provide the specific extensions.
- Why unresolved: The paper focuses on binary classifiers (R = 2) and does not explore the mathematical formulation for multi-class classification problems.
- What evidence would resolve it: Derivation and validation of complete algebraic postulates for (N, T, Q, R) tests with R > 2, including experimental results on multi-class classification datasets.

### Open Question 2
- Question: Can the algebraic postulates framework be adapted to evaluate sequential decision-making AI agents rather than classifiers responding to static tests?
- Basis in paper: [inferred] The paper discusses evaluating AI agents based on their responses to tests, but does not consider agents making sequential decisions over time.
- Why unresolved: The current framework assumes independent test items, while sequential decisions may have temporal dependencies and state transitions.
- What evidence would resolve it: Extension of the postulates to capture temporal correlations and state-dependent error rates in sequential decision-making processes.

### Open Question 3
- Question: How does the algebraic postulates framework handle cases where the underlying data distribution shifts between training and evaluation?
- Basis in paper: [inferred] The paper assumes a fixed data distribution for the tests, but real-world AI agents often face distribution shifts.
- Why unresolved: The current framework does not account for changes in the prevalence of question types or the relationship between features and labels over time.
- What evidence would resolve it: Development of dynamic algebraic postulates that can adapt to distribution shifts and experimental validation on datasets with known temporal or covariate shifts.

## Limitations
- Assumption Sensitivity: The framework's performance critically depends on the validity of error independence assumptions.
- Scalability Constraints: The complete postulate generation scales combinatorially with N, making it computationally intensive for large numbers of classifiers.
- Dataset Generalization: Validation is limited to two UCI datasets (adult and two-norm), with no testing on high-dimensional or real-world noisy AI agent outputs beyond binary classification.

## Confidence
- High Confidence: The mathematical framework for constructing complete algebraic postulates is sound and logically consistent. The self-warning mechanism through irrationality detection is mathematically proven.
- Medium Confidence: The framework's practical utility for detecting error correlations and outperforming majority voting in specific scenarios is demonstrated but requires broader validation.
- Low Confidence: The framework's applicability to multi-class classification, non-binary responses, and complex AI agents (beyond binary classifiers) remains unexplored.

## Next Checks
1. **Robustness Testing**: Apply the framework to synthetic datasets with controlled error correlation levels to quantify the relationship between correlation strength and evaluator performance degradation.
2. **Scalability Analysis**: Implement and benchmark postulate generation for N=4 to N=10 classifiers to identify computational bottlenecks and potential approximation methods.
3. **Cross-Domain Validation**: Test the framework on real-world noisy AI agent outputs (e.g., language model responses, multi-modal predictions) to assess generalization beyond binary classification tasks.