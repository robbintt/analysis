---
ver: rpa2
title: Investigating Zero- and Few-shot Generalization in Fact Verification
arxiv_id: '2309.09444'
source_url: https://arxiv.org/abs/2309.09444
tags:
- datasets
- claims
- evidence
- generalization
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates zero- and few-shot generalization in fact
  verification across 11 datasets spanning 6 domains. It finds that current RoBERTa-based
  models generalize poorly to unseen datasets, with an average 20.80% drop in F1 compared
  to in-domain performance.
---

# Investigating Zero- and Few-shot Generalization in Fact Verification

## Quick Facts
- arXiv ID: 2309.09444
- Source URL: https://arxiv.org/abs/2309.09444
- Reference count: 25
- Primary result: Current RoBERTa-based models show 20.80% average F1 drop when generalizing from 11 source datasets to unseen target datasets

## Executive Summary
This paper investigates zero- and few-shot generalization in fact verification across 11 datasets spanning 6 domains. The study reveals that current RoBERTa-based models generalize poorly to unseen datasets, with significant performance drops compared to in-domain settings. Notably, models trained on Wikipedia-based artificial claims (like FEVER) generalize better to real-world natural claims than models trained on natural claims, especially as dataset size increases. The research identifies key factors affecting generalization including dataset size, evidence length, and claim type, with refutes claims being most difficult to verify. Two approaches to improve generalization are explored: domain-specific pretraining (BioBERT, SciBERT) and automatic claim generation via data augmentation, though both face implementation challenges.

## Method Summary
The study trains RoBERTa-large models on source fact verification datasets and evaluates zero-shot generalization on target datasets using macro-averaged F1 score. For few-shot generalization, models are fine-tuned on small amounts of target data. Eleven datasets are analyzed including FEVER, VitaminC, FoolMeTwice, Climate-FEVER, SciFact, PubHealth, COVID-Fact, and FA VIQ, covering artificial and natural claims across domains like Wikipedia, science, health, and climate. Evidence granularity varies from sentence-level to document-level, and models are tested with both 3-class (supports/refutes/NEI) and 2-class (supports/refutes) label settings. Domain adaptation experiments use BioBERT and SciBERT pretraining, while data augmentation employs BART-based claim generation.

## Key Results
- Zero-shot generalization shows 20.80% average F1 drop compared to in-domain performance
- Artificial claims (FEVER) generalize better to natural claims than natural claims generalize to other natural claims, especially with larger dataset sizes
- Refutes claims are consistently harder to verify than supports claims across all datasets and settings
- Sentence-level evidence granularity improves generalization more than document-level evidence
- Fine-tuning on small amounts of target data significantly improves few-shot performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Larger dataset sizes improve zero-shot generalization from artificial to natural claims.
- **Mechanism**: Artificial claims (FEVER, VitaminC) provide massive training volumes that expose models to diverse linguistic patterns and reasoning types, building robust feature representations that transfer to natural claims.
- **Core assumption**: Training data volume and diversity are more important than domain similarity for generalization.
- **Evidence anchors**:
  - [abstract] "we find that Wikipedia-based artificial claims (e.g., FEVER) generalize well to natural claims in real-world domains with the growth of dataset size"
  - [section] "We find that the model trained on artificial claims generalizes quite well to natural claims... with an average F1 drop of 36.9%"
  - [corpus] "Average neighbor FMR=0.415" (weak correlation with related work on few-shot generalization)
- **Break condition**: When dataset size differences are eliminated (800 examples each), the advantage disappears and natural claims generalize slightly better.

### Mechanism 2
- **Claim**: Sentence-level evidence granularity improves generalization more than document-level evidence.
- **Mechanism**: Fine-grained evidence forces models to learn precise evidence-claim alignment patterns, which are more transferable across domains than document-level relevance filtering.
- **Core assumption**: Models learn alignment patterns more effectively from shorter, more focused evidence spans.
- **Evidence anchors**:
  - [abstract] "training the FV model on more fine-grained evidence yields better generalization"
  - [section] "datasets with sentence-level evidence in general achieve better generalization results than other datasets compared to their doc-level versions"
  - [corpus] No direct corpus evidence (explicitly stated as missing)
- **Break condition**: When evidence spans become too short to contain sufficient context for verification, performance degrades.

### Mechanism 3
- **Claim**: Refutes claims are inherently more difficult to verify than supports claims, harming generalization.
- **Mechanism**: Refutation requires detecting absence of evidence or contradictory information, which is more ambiguous than finding supporting evidence, leading to higher error rates that persist across domains.
- **Core assumption**: The cognitive difficulty of refutation translates to model learning challenges that don't transfer well.
- **Evidence anchors**:
  - [abstract] "the type of claims... refutes claims being most difficult to verify"
  - [section] "the refutes claim has the worst prediction score (in bold) almost for all datasets, in both the zero-shot and the in-domain setting"
  - [corpus] No direct corpus evidence (explicitly stated as missing)
- **Break condition**: When claims are simplified to binary supports/refutes (removing NEI), performance improves significantly.

## Foundational Learning

- **Concept**: Domain adaptation through pretraining
  - Why needed here: Fact verification requires domain-specific knowledge (scientific, health, climate) that general language models lack
  - Quick check question: If BioBERT improves performance on scientific datasets but hurts Wikipedia datasets, what does this reveal about domain transfer?

- **Concept**: Evidence granularity and reasoning complexity
  - Why needed here: Different evidence granularities require different reasoning strategies (filtering vs. alignment)
  - Quick check question: Why does Climate-FEVER show the largest drop when moving from sentence to document evidence?

- **Concept**: Label consistency in data augmentation
  - Why needed here: Generated claims must maintain semantic consistency with desired labels to be useful for training
  - Quick check question: If 30% of generated claims have label inconsistency, what is the primary source of error?

## Architecture Onboarding

- **Component map**: [CLS] claim [SEP] evidence -> RoBERTa-large encoder -> Cross-attention -> 3-class/2-class classification head
- **Critical path**: Claim encoding → Evidence encoding → Cross-attention → Label prediction
- **Design tradeoffs**:
  - Large pretraining models vs. computational cost
  - Fine-grained evidence vs. context richness
  - Artificial claim diversity vs. natural claim authenticity
  - Data augmentation quantity vs. label consistency quality
- **Failure signatures**:
  - Poor generalization despite large source dataset → Check evidence granularity mismatch
  - High NEI prediction rates → Check label distribution imbalance
  - Low refutation accuracy → Check evidence-claim contradiction detection capability
- **First 3 experiments**:
  1. Control for dataset size (800 examples each) to isolate artificial vs. natural claim effects
  2. Compare sentence-level vs. document-level evidence performance on same dataset
  3. Evaluate pretraining impact across different domain combinations (Wikipedia→science, science→Wikipedia)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do large language models (LLMs) perform on zero- and few-shot fact verification compared to RoBERTa-based models?
- Basis in paper: [explicit] The authors acknowledge that they did not evaluate LLMs like InstructGPT and GPT-4 due to high experimental costs, despite their demonstrated few-shot learning capabilities in other NLP tasks.
- Why unresolved: LLMs are API-based and function as black-box models, making it difficult to fine-tune them directly or access their model weights for deeper analysis.
- What evidence would resolve it: Systematic experiments comparing LLM performance on the 11 FV datasets against RoBERTa-based models in both zero- and few-shot settings, with detailed analysis of strengths and weaknesses.

### Open Question 2
- Question: What is the optimal balance between artificial and natural claims in training data for maximizing generalization to real-world domains?
- Basis in paper: [inferred] The paper shows that Wikipedia-based artificial claims (e.g., FEVER) generalize well to natural claims in real-world domains as dataset size increases, but doesn't explore the optimal ratio or combination of artificial vs. natural claims.
- Why unresolved: The study focuses on using either artificial or natural claims as training data, without investigating hybrid approaches or the impact of different proportions.
- What evidence would resolve it: Experiments training models on various mixtures of artificial and natural claims, measuring generalization performance across target domains and identifying the optimal composition.

### Open Question 3
- Question: How can label consistency be improved in automatically generated fact verification claims?
- Basis in paper: [explicit] The authors found that around 30% of claims generated by BART-gen suffer from label inconsistency, where the actual label doesn't match the desired label specified during generation.
- Why unresolved: The paper identifies this as a significant problem but doesn't propose or test solutions for improving label consistency in generated data.
- What evidence would resolve it: Development and evaluation of improved claim generation methods that incorporate better label alignment mechanisms, followed by human evaluation showing reduced label inconsistency rates.

## Limitations

- Dataset size imbalance affects the comparison between artificial and natural claims, with artificial claims benefiting from much larger training sets
- Evidence granularity effects are correlational rather than causal, lacking controlled experiments to establish mechanism
- Domain-specific pretraining shows mixed results without comprehensive ablation studies to identify optimal transfer strategies

## Confidence

**High Confidence**: Refutes claims are consistently harder to verify across all datasets, with significantly lower F1 scores than supports claims. This is supported by extensive quantitative evidence across multiple experimental settings and shows minimal variation between zero-shot and in-domain performance.

**Medium Confidence**: Zero-shot generalization results showing 20.80% average F1 drop compared to in-domain performance. While the magnitude is well-supported by the data, the generalizability to other fact verification models and domains beyond the 11 studied datasets remains uncertain.

**Low Confidence**: The mechanism explanation that artificial claims generalize better due to "diverse linguistic patterns and reasoning types." The evidence is primarily correlational (large dataset size correlates with better generalization), and the paper does not systematically vary dataset size while holding claim type constant to isolate this effect.

## Next Checks

1. **Controlled Size Experiment**: Re-run the artificial vs. natural claim generalization comparison with matched dataset sizes (e.g., 800 examples each for both FEVER and VitaminC) to isolate the effect of claim type from dataset volume.

2. **Evidence Granularity Ablation**: Conduct a systematic study on a single dataset where the same claims are paired with both sentence-level and document-level evidence, controlling for all other variables to establish causal relationships between granularity and generalization.

3. **Pretraining Transfer Matrix**: Expand the domain adaptation experiments to include all pairwise combinations of pretraining domains (Wikipedia, biomedical, scientific, health, climate) applied to all target datasets, creating a comprehensive transfer matrix to identify when and why domain-specific pretraining helps or hurts.