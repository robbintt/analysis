---
ver: rpa2
title: Perturbed examples reveal invariances shared by language models
arxiv_id: '2311.04166'
source_url: https://arxiv.org/abs/2311.04166
tags:
- invariances
- reference
- linguistic
- different
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel framework for comparing two NLP models
  by revealing their shared invariance to interpretable input perturbations targeting
  a specific linguistic capability. The framework generates perturbations for a reference
  model and evaluates how invariant a target model is to these perturbations, capturing
  finer-grained behavioral similarities between models.
---

# Perturbed examples reveal invariances shared by language models

## Quick Facts
- arXiv ID: 2311.04166
- Source URL: https://arxiv.org/abs/2311.04166
- Reference count: 40
- Key outcome: Novel framework for comparing NLP models by revealing shared invariances to interpretable input perturbations targeting specific linguistic capabilities.

## Executive Summary
This paper introduces a novel framework for comparing two NLP models by revealing their shared invariance to interpretable input perturbations targeting a specific linguistic capability. The framework generates perturbations for a reference model and evaluates how invariant a target model is to these perturbations, capturing finer-grained behavioral similarities between models. Extensive experiments on models from the same and different architecture families, as well as black-box models, show that large language models share many invariances encoded by models of various sizes, whereas the invariances by large models are only shared by other large models. This framework can shed light on the types of invariances retained or emerging in new models.

## Method Summary
The proposed framework defines linguistic capabilities to generate interpretable invariant perturbations for a reference model. It then evaluates how invariant a target model is to these perturbations, quantifying the degree of shared invariances between the two models using Hard-SCoPE and Soft-SCoPE measures. The framework is applied to compare models from the same and different architecture families, as well as black-box models, along specific linguistic capabilities like Synonym-Invariance and Typo-Invariance.

## Key Results
- Large language models share many invariances encoded by models of various sizes, but small models do not share invariances encoded by large models.
- Different linguistic capabilities are affected to varying degrees by model design choices such as distillation.
- Shared invariances can reveal finer-grained similarities and differences of the instance properties utilized by two models for their predictions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models share many invariances encoded by smaller models, but small models do not share invariances encoded by large models.
- Mechanism: The framework measures shared invariance by generating perturbations that leave a reference model invariant, then checking how invariant a target model is to those perturbations. Larger models are exposed to broader training data and optimization processes that lead them to encode a wider variety of invariances.
- Core assumption: The perturbations generated for a reference model accurately capture its invariances, and the target model's invariance to these perturbations is a meaningful measure of shared behavior.
- Evidence anchors:
  - [abstract]: "Across experiments, we observe that large language models share many of the invariances encoded by models of various sizes, whereas the invariances by large models are only shared by other large models."
  - [section 4.2]: "Interestingly, the opposite is not true i.e. smaller models don't necessarily share invariances generated w.r.t larger models as well as other larger models."
  - [corpus]: FMR scores around 0.6 for related adversarial robustness papers, suggesting the invariance measurement approach is grounded in established methods.
- Break condition: If the perturbation generation process fails to produce truly invariant examples for the reference model, or if the target model's invariance to these perturbations is not a meaningful measure of shared behavior, this mechanism would break down.

### Mechanism 2
- Claim: Different linguistic capabilities are affected to varying degrees by model design choices such as distillation.
- Mechanism: The framework allows for the evaluation of shared invariances along specific linguistic capabilities. By fixing a reference model and comparing a target model, one can observe how design choices like distillation affect invariances along different capabilities.
- Core assumption: The linguistic capabilities defined in the framework accurately capture the relevant aspects of language understanding, and the degree of shared invariance along these capabilities is a meaningful measure of the impact of design choices.
- Evidence anchors:
  - [section 4.1]: "We observe that DistilBERT is substantially less aligned to BERT along Typo-Invariance. Thus, Gap in IID accuracy may overestimate the degree of shared invariances between two models along a linguistic capability."
  - [section 4.1]: "In Fig. 3, we also observe that DistilBERT is significantly more similar to BERT along Synonym-Invariance compared to Typo-Invariance. Thus not only is there a decrease in shared-invariances after distillation, but distillation also affects different linguistic capabilities to varying degrees."
  - [corpus]: FMR scores around 0.6 for related adversarial robustness papers, suggesting the invariance measurement approach is grounded in established methods.
- Break condition: If the linguistic capabilities defined in the framework do not accurately capture the relevant aspects of language understanding, or if the degree of shared invariance along these capabilities is not a meaningful measure of the impact of design choices, this mechanism would break down.

### Mechanism 3
- Claim: Shared invariances can reveal finer-grained similarities and differences of the instance properties utilized by two models for their predictions.
- Mechanism: The framework measures shared invariances by generating perturbations that leave a reference model invariant, then checking how invariant a target model is to those perturbations. This allows for the comparison of the instance properties utilized by the two models for their predictions, beyond just agreement on individual samples.
- Core assumption: The perturbations generated for a reference model accurately capture its invariances, and the target model's invariance to these perturbations is a meaningful measure of the instance properties utilized by the two models for their predictions.
- Evidence anchors:
  - [abstract]: "Understanding the shared invariances of two NLP models can reveal finer-grained similarities and differences of the instance properties utilized by the two models for their predictions (Shah et al., 2023)."
  - [section 3.3.3]: "While agreement rates quantify whether two models have similar behavior on a particular sample, shared invariances can reveal finer-grained similarities of the instance properties utilized by the two models for their predictions."
  - [corpus]: FMR scores around 0.6 for related adversarial robustness papers, suggesting the invariance measurement approach is grounded in established methods.
- Break condition: If the perturbation generation process fails to produce truly invariant examples for the reference model, or if the target model's invariance to these perturbations is not a meaningful measure of the instance properties utilized by the two models for their predictions, this mechanism would break down.

## Foundational Learning

- Concept: Linguistic capabilities
  - Why needed here: The framework defines linguistic capabilities to generate interpretable invariant perturbations. Understanding what linguistic capabilities are and how they are defined is crucial for understanding the framework.
  - Quick check question: What are the two linguistic capabilities focused on in the paper, and how are they defined?

- Concept: Behavioral invariance
  - Why needed here: The framework measures shared invariances by checking how invariant a target model is to perturbations that leave a reference model invariant. Understanding what behavioral invariance is and how it is measured is crucial for understanding the framework.
  - Quick check question: How is behavioral invariance defined in the paper, and what is the goal function used to generate invariant perturbations?

- Concept: Shared invariances
  - Why needed here: The framework proposes measures to quantify the degree of shared invariances between two models. Understanding what shared invariances are and how they are measured is crucial for understanding the framework.
  - Quick check question: What are the two measures proposed in the paper to quantify shared invariances, and how do they differ?

## Architecture Onboarding

- Component map:
  - Perturbation generation: Defines linguistic capabilities, generates perturbations that leave a reference model invariant.
  - Shared invariance measures: Computes Hard-SCoPE and Soft-SCoPE to quantify the degree of shared invariances between a reference and target model.
  - Evaluation: Compares shared invariances across different model pairs, linguistic capabilities, and design choices.

- Critical path:
  1. Define linguistic capabilities.
  2. Generate perturbations for a reference model.
  3. Evaluate the target model's invariance to these perturbations.
  4. Compute shared invariance measures.
  5. Compare shared invariances across different model pairs, linguistic capabilities, and design choices.

- Design tradeoffs:
  - Choice of linguistic capabilities: The choice of linguistic capabilities affects the types of invariances that can be measured.
  - Perturbation generation method: The method used to generate perturbations affects the quality and diversity of the generated examples.
  - Shared invariance measures: The choice of measures affects the granularity and interpretability of the results.

- Failure signatures:
  - Low agreement rates between reference and target models: Indicates a lack of shared behavior along the evaluated linguistic capabilities.
  - Low Hard-SCoPE or Soft-SCoPE values: Indicates a lack of shared invariances between the reference and target models.
  - High variance in shared invariance measures across different model pairs: Indicates that the measures may not be robust to differences in model architecture or training.

- First 3 experiments:
  1. Compare shared invariances between BERT and DistilBERT along Synonym-Invariance and Typo-Invariance.
  2. Analyze the effect of model size on shared invariances within the BERT architecture family.
  3. Compare shared invariances between GPT-2 and various InstructGPT models along Synonym-Invariance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of objective function (L1 norm vs. KL-divergence) impact the effectiveness of the proposed framework in identifying shared invariances between NLP models?
- Basis in paper: [explicit] The paper mentions that the objective function can take other forms as long as it captures differences in both direction and magnitude between the reference modelâ€™s output on base and perturbed samples. The authors also conducted additional experiments with KL-divergence as the objective function and reported minimal effect on the overall takeaways when using different objective functions.
- Why unresolved: The paper only provides limited evidence on the impact of the choice of objective function on the framework's effectiveness. It would be valuable to conduct more extensive experiments with various objective functions to determine their impact on the framework's ability to identify shared invariances.
- What evidence would resolve it: Additional experiments with a wide range of objective functions, such as cosine similarity, mean squared error, or other distance metrics, could provide more insights into the impact of the choice of objective function on the framework's effectiveness in identifying shared invariances between NLP models.

### Open Question 2
- Question: How does the proposed framework perform when applied to other NLP tasks beyond sentiment classification and language modeling, such as named entity recognition, machine translation, or question answering?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the proposed framework in analyzing shared invariances between NLP models for sentiment classification and language modeling tasks. However, the authors mention that the framework can be easily expanded to include other linguistic capabilities by defining specific transformations and constraints.
- Why unresolved: The paper only provides evidence for the framework's effectiveness on a limited set of NLP tasks. It would be valuable to evaluate the framework's performance on a broader range of NLP tasks to determine its generalizability and applicability.
- What evidence would resolve it: Conducting experiments with the proposed framework on various NLP tasks, such as named entity recognition, machine translation, or question answering, would provide insights into its generalizability and applicability across different NLP domains.

### Open Question 3
- Question: How do the proposed invariance-based metrics (Hard-SCoPE and Soft-SCoPE) compare to other existing metrics for evaluating the behavioral similarity between NLP models, such as representational similarity or other perturbation-based metrics?
- Basis in paper: [explicit] The paper introduces two novel metrics, Hard-SCoPE and Soft-SCoPE, for measuring the degree of shared invariances between two models along a particular linguistic capability. The authors also discuss the correlation between these proposed metrics and existing metrics, such as OOD-Agreement and Gap in IID-Accuracy, and find that the invariance-based metrics are poorly correlated with the existing metrics when the reference model is smaller than the target model.
- Why unresolved: The paper only provides a limited comparison between the proposed invariance-based metrics and a few existing metrics. It would be valuable to conduct a more comprehensive comparison with a broader range of existing metrics to determine the advantages and limitations of the proposed metrics in evaluating the behavioral similarity between NLP models.
- What evidence would resolve it: Conducting experiments to compare the proposed invariance-based metrics with other existing metrics, such as representational similarity measures or other perturbation-based metrics, would provide insights into their relative performance and help identify their advantages and limitations in evaluating the behavioral similarity between NLP models.

## Limitations
- The framework's effectiveness relies heavily on the quality of the perturbations generated for the reference model.
- The framework focuses on specific linguistic capabilities defined by the authors, which may not comprehensively capture all aspects of language understanding.
- The generalizability of the findings to other NLP tasks and model architectures beyond those studied in the paper remains unclear.

## Confidence
- High confidence: The core mechanism of comparing shared invariances between NLP models using interpretable perturbations is well-founded and supported by experimental results.
- Medium confidence: The interpretation of shared invariances as revealing finer-grained similarities and differences in instance properties utilized by models for predictions is plausible but requires further validation.
- Low confidence: The specific linguistic capabilities defined in the framework may not comprehensively capture all relevant aspects of language understanding.

## Next Checks
1. Conduct experiments on a wider range of NLP tasks (e.g., question answering, machine translation) to assess the framework's generalizability across different language understanding domains.
2. Evaluate the framework's performance when comparing models from different architecture families (e.g., transformers, recurrent neural networks) to determine if the shared invariance patterns hold across diverse model structures.
3. Investigate the impact of different perturbation generation methods (e.g., rule-based, learned) on the quality and diversity of the generated perturbations, and how this affects the shared invariance measures.