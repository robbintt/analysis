---
ver: rpa2
title: 'Unnatural language processing: How do language models handle machine-generated
  prompts?'
arxiv_id: '2310.15829'
source_url: https://arxiv.org/abs/2310.15829
tags:
- prompts
- prompt
- human
- language
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how language models process natural versus
  machine-generated prompts. It compares human prompts to discrete and continuous
  machine-generated prompts across semantic tasks, analyzing model perplexity, attention
  patterns, and activation profiles.
---

# Unnatural language processing: How do language models handle machine-generated prompts?

## Quick Facts
- arXiv ID: 2310.15829
- Source URL: https://arxiv.org/abs/2310.15829
- Reference count: 21
- Language models process natural and machine-generated prompts through fundamentally different neural pathways despite achieving similar outputs

## Executive Summary
This paper investigates how language models process natural versus machine-generated prompts across semantic tasks. The study compares human prompts to discrete and continuous machine-generated prompts using OPT-350m and OPT-1.3b models, analyzing model perplexity, attention patterns, and activation profiles. Machine-generated prompts achieve higher accuracy and lower output entropy but are less predictable by the model (higher perplexity). Despite producing similar outputs, human and machine prompts trigger fundamentally different processing pathways, with machine prompts recruiting distinct knowledge neurons associated with non-linguistic or code-related items. This suggests that models have developed specialized mechanisms for handling unnatural language inputs.

## Method Summary
The study compares model responses to human, discrete machine-generated (AutoPrompt), and continuous machine-generated (OptiPrompt) prompts across semantic tasks from the LAMA-TREx dataset. It measures accuracy, perplexity, output entropy, attention distribution patterns, and knowledge neuron activation overlap. The analysis involves pre-trained OPT-350m and OPT-1.3b models, filtered prompt templates, and diagnostic metrics to quantify differences in processing pathways between prompt types. Experiments collect results and compute correlations between metrics for within-type and between-type prompt comparisons.

## Key Results
- Machine-generated prompts achieve higher accuracy and lower output entropy than human prompts
- Machine prompts are less predictable by the model (higher perplexity) despite better performance
- Different prompt types activate distinct knowledge neurons with low overlap (13-26%) between types
- Larger models (OPT-1.3b) show higher between-prompt overlap, suggesting potential convergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine-generated prompts activate distinct neural pathways compared to human prompts
- Mechanism: Different token sequences trigger different knowledge neurons and attention patterns, even when producing similar outputs
- Core assumption: The model's internal processing differs based on input characteristics, not just output
- Evidence anchors:
  - [abstract] "Even when producing a similar output, machine-generated and human prompts trigger different response patterns through the network processing pathways"
  - [section] "Activation overlap statistics are provided in Figure 1... between-prompt-type overlap is always low, ranging from 13 to 26"
  - [corpus] Weak - no direct corpus evidence for neural pathway differences
- Break condition: If prompts produce identical attention distributions and activation patterns despite being different types

### Mechanism 2
- Claim: Machine prompts are processed as less predictable input despite higher accuracy
- Mechanism: The model assigns lower probability to machine-generated token sequences, yet these sequences lead to better task performance
- Core assumption: Perplexity and accuracy are not perfectly correlated in this context
- Evidence anchors:
  - [abstract] "Machine-generated prompts achieve higher accuracy and lower output entropy but are less predictable by the model (higher perplexity)"
  - [section] "prompt perplexity – quantifying the degree of predictability of a token sequence given an LM – is two order of magnitude higher for M-disc than for human templates"
  - [corpus] Weak - corpus doesn't directly address perplexity-accuracy relationship
- Break condition: If perplexity accurately predicts prompt effectiveness

### Mechanism 3
- Claim: Machine prompts recruit non-linguistic units that still produce correct outputs
- Mechanism: Units associated with code fragments and special characters are activated by machine prompts and contribute to correct predictions through distributed activation
- Core assumption: Non-linguistic unit activation can guide the model toward correct semantic outputs
- Evidence anchors:
  - [abstract] "machine prompts often recruiting units associated with non-linguistic or code-related items"
  - [section] "we tentatively conclude that machine prompts are not only triggering different activation pathways, but that the units involved in these pathways tend to respond to less language-like items"
  - [corpus] Weak - corpus evidence doesn't directly address unit-vocabulary associations
- Break condition: If non-linguistic units cannot influence semantic output generation

## Foundational Learning

- Concept: Feed-forward layers as key-value memories
  - Why needed here: Understanding how knowledge neurons store and retrieve information is crucial for interpreting activation overlap differences
  - Quick check question: How do knowledge neurons differ from attention mechanisms in processing information?

- Concept: Attention distribution metrics
  - Why needed here: The study uses attention distribution scores to quantify how models focus on different tokens for different prompt types
  - Quick check question: What does a lower attention distribution score indicate about token processing?

- Concept: Perplexity and entropy measures
  - Why needed here: These metrics are used to compare predictability and confidence of different prompt types
  - Quick check question: How does output entropy relate to model calibration in this context?

## Architecture Onboarding

- Component map: Input tokens → embedding → self-attention → feed-forward layers → output prediction
- Critical path: Input tokens → embedding → self-attention → feed-forward layers → output prediction
- Design tradeoffs: Model size affects prompt processing differences (OPT-350m vs OPT-1.3b), with larger models showing more convergence between prompt types
- Failure signatures: High perplexity with high accuracy indicates processing divergence; low activation overlap suggests fundamentally different pathways
- First 3 experiments:
  1. Measure activation overlap between human and machine prompts on a small subset of tasks
  2. Compare attention distribution patterns across prompt types using diagnostic metrics
  3. Profile knowledge neuron activations for different prompt types using vocabulary association analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanistic pathways in transformer models enable effective processing of machine-generated prompts that are semantically meaningless?
- Basis in paper: [explicit] The paper states that machine-generated prompts "trigger different response patterns through the network processing pathways" but does not explain the underlying mechanisms
- Why unresolved: The study identifies that different pathways exist but does not investigate the specific neural mechanisms or architectural features that make these pathways effective
- What evidence would resolve it: Detailed analysis of transformer attention patterns, weight distributions, and layer-specific activations during processing of different prompt types, potentially using mechanistic interpretability techniques

### Open Question 2
- Question: Do machine-generated prompts activate specialized knowledge neurons that are distinct from those used for natural language processing, and what types of knowledge do these neurons encode?
- Basis in paper: [explicit] The paper shows that machine prompts "recruit different knowledge neurons" and that these units respond to "less language-like items" but doesn't characterize the nature of this knowledge
- Why unresolved: While the study demonstrates different activation patterns, it only provides preliminary analysis of what types of vocabulary items trigger these neurons without deeper investigation into their semantic content
- What evidence would resolve it: Systematic mapping of knowledge neuron activation to specific semantic domains or knowledge types, and testing whether editing these neurons affects performance on both natural and unnatural language tasks

### Open Question 3
- Question: How does model size affect the ability to process unnatural language, and is there a threshold where larger models converge to similar processing pathways for human and machine-generated prompts?
- Basis in paper: [explicit] The paper observes that "between-prompt overlap tends to be higher with OPT-1.3b, suggesting that larger LMs could show a convergence" but this remains unexplored
- Why unresolved: The study only compares two model sizes (350m and 1.3b parameters) without testing intermediate sizes or identifying where convergence might occur
- What evidence would resolve it: Systematic testing of multiple model sizes with controlled architecture variations to identify at what scale and under what conditions convergence in processing unnatural language occurs

## Limitations

- The central claim about different neural pathways is based on correlational evidence rather than causal mechanisms
- The study uses a relatively small subset of tasks (20 from LAMA-TREx) and only two model sizes
- The interpretation that non-linguistic units contribute to semantic processing remains speculative without ablation studies
- Corpus evidence supporting knowledge neuron distinctions is notably weak

## Confidence

- **High confidence**: The empirical observation that machine-generated prompts achieve higher accuracy with lower entropy but higher perplexity is well-supported by the data and reproducible across tasks.
- **Medium confidence**: The claim that different prompt types trigger distinct activation patterns is supported by activation overlap statistics, though the functional significance of these differences requires further investigation.
- **Low confidence**: The interpretation that machine prompts specifically recruit non-linguistic units and that these units contribute to semantic processing is speculative and lacks direct causal evidence.

## Next Checks

1. **Ablation study of knowledge neurons**: Systematically suppress the top-activated knowledge neurons for each prompt type and measure the impact on output accuracy. This would test whether the identified neural differences are functionally necessary for the observed performance differences.

2. **Cross-prompt type transfer**: Take high-performing machine-generated prompts and modify them to be more human-like in surface form while preserving their underlying token structure. Measure whether accuracy degrades as activation patterns become more similar to human prompts, establishing a causal link between processing pathways and performance.

3. **Larger-scale validation across architectures**: Test the perplexity-accuracy divergence on additional model families (e.g., GPT, BERT variants) and datasets beyond LAMA-TREx to determine whether this phenomenon is specific to OPT models or represents a broader characteristic of transformer-based language models.