---
ver: rpa2
title: 'Text + Sketch: Image Compression at Ultra Low Rates'
arxiv_id: '2307.01944'
source_url: https://arxiv.org/abs/2307.01944
tags:
- image
- text
- compression
- pics
- sketch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for extreme image compression using
  text descriptions and sketches as side information. The authors propose to leverage
  pre-trained text-to-image models like Stable Diffusion and ControlNet to generate
  high-quality reconstructions at very low bitrates.
---

# Text + Sketch: Image Compression at Ultra Low Rates

## Quick Facts
- arXiv ID: 2307.01944
- Source URL: https://arxiv.org/abs/2307.01944
- Authors: 
- Reference count: 23
- One-line primary result: Achieves state-of-the-art generative compression at bitrates below 0.013 bits-per-pixel using text prompts and sketches as side information

## Executive Summary
This paper presents a novel approach for extreme image compression at ultra-low bitrates by leveraging pre-trained text-to-image models and prompt inversion. The method, called PICS, encodes images into text prompts and compressed edge detection maps (sketches), which are then decoded using ControlNet to generate high-quality reconstructions. By utilizing foundation models like CLIP, Stable Diffusion, and ControlNet, the approach achieves superior perceptual and semantic quality compared to existing generative compression methods without requiring end-to-end training.

## Method Summary
The method consists of two main components: prompt inversion and sketch compression. In the encoder, images are first mapped to CLIP's embedding space, and text prompts are generated via projected gradient descent to find semantically similar descriptions. These prompts are losslessly compressed. Additionally, edge detection maps are extracted using HED and compressed using a lightweight learned codec. At the decoder, the text prompt is used to generate an initial image with a text-to-image model, which is then refined using ControlNet conditioned on the sketch to preserve spatial structure.

## Key Results
- Outperforms state-of-the-art generative compression models like HiFiC at bitrates below 0.013 bits-per-pixel
- Achieves better rate-distortion tradeoffs on Kodak, CLIC 2021, and DIV2K datasets
- Generates more faithful reconstructions in terms of spatial structure and semantics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt inversion with CLIP embeddings preserves semantic content while compressing to extremely low bitrates.
- Mechanism: The image is mapped to CLIP's joint embedding space using a pre-trained CLIP image encoder. Text tokens are then searched in CLIP's text embedding space to find the closest semantic match to the image embedding. This text representation, when fed to a pre-trained text-to-image model, reconstructs the image's semantic content.
- Core assumption: CLIP's embedding space meaningfully represents image semantics such that a text prompt retrieved via cosine similarity can trigger the correct high-level image generation in the text-to-image model.
- Evidence anchors:
  - [abstract] "we use prompt inversion (PI), which performs projected gradient search in CLIP's embedding space, using cosine similarity between the image embedding and the text embedding as the objective."
  - [section] "we use prompt inversion (PI), which performs projected gradient search in CLIP's embedding space, using cosine similarity between the image embedding and the text embedding as the objective."
  - [corpus] Weak. No corpus paper directly discusses prompt inversion for compression; most focus on sketching or other modalities.
- Break condition: If CLIP's embedding space fails to capture fine-grained semantics or if the text-to-image model does not generate images faithful to the prompt, semantic preservation fails.

### Mechanism 2
- Claim: Adding a compressed sketch as side information preserves spatial structure without sacrificing semantic fidelity.
- Mechanism: A sketch (HED edge map) of the original image is losslessly compressed using a lightweight learned codec. At the decoder, ControlNet takes both the text prompt and the sketch to condition the diffusion generation, forcing spatial adherence to the sketch while respecting the semantic content from the prompt.
- Core assumption: ControlNet can successfully fuse the semantic signal from the prompt and the structural signal from the sketch to generate images that preserve both.
- Evidence anchors:
  - [abstract] "we choose G to be ControlNet (Zhang & Agrawala, 2023), a text-to-image model built on top of SD that can process spatial conditioning maps in the form of edge detection maps, segementation maps, depth maps, etc."
  - [section] "In this setting, we choose G to be ControlNet... It ensures that the reconstructed images follow the spatial structure of the input map, and the style suggested by the text prompt."
  - [corpus] Weak. The corpus neighbors focus on sketching and compression, but not on the specific combination of text prompt + sketch via ControlNet.
- Break condition: If ControlNet's conditioning mechanism does not prioritize sketch structure strongly enough, reconstructions may drift semantically from the prompt or spatially from the sketch.

### Mechanism 3
- Claim: Zero-shot usage of large-scale pre-trained models eliminates the need for expensive end-to-end training while achieving high perceptual quality.
- Mechanism: By leveraging models (CLIP, Stable Diffusion, ControlNet) that were pre-trained on billion-scale datasets, the system avoids the cost of training a full compression model. Only the lightweight HED sketch compressor needs training, drastically reducing computational requirements.
- Core assumption: Pre-trained models' representations are sufficiently general to handle arbitrary images in the target domain without fine-tuning.
- Evidence anchors:
  - [abstract] "Our approach, called PICS (Prompt Inversion Compressor with Sketch), uses prompt inversion to encode images into text prompts, which are then losslessly compressed. Additionally, we transmit a compressed edge detection map (sketch) as side information."
  - [section] "we show that our schemes outperform state-of-the-art generative compressors in terms of semantic and perceptual quality, despite no end-to-end training."
  - [corpus] Weak. While the corpus mentions other ultra-low bitrate schemes, none specifically emphasize zero-shot pre-trained models as the key differentiator.
- Break condition: If the domain of test images differs significantly from the pre-training data, the pre-trained models may fail to generalize, leading to poor reconstructions.

## Foundational Learning

- Concept: CLIP embedding space and its semantic alignment properties.
  - Why needed here: Understanding how CLIP maps images and text into a shared semantic space is crucial to grasping why prompt inversion works for compression.
  - Quick check question: How does CLIP ensure that the text prompt retrieved via cosine similarity in embedding space will generate an image semantically similar to the original?

- Concept: ControlNet conditioning mechanism and spatial structure preservation.
  - Why needed here: Knowing how ControlNet fuses text and spatial conditioning maps explains why adding a sketch improves reconstruction quality.
  - Quick check question: What is the role of the conditioning map in ControlNet's denoising process, and how does it affect the final image?

- Concept: Diffusion models and text-to-image generation basics.
  - Why needed here: Familiarity with how diffusion models like Stable Diffusion generate images from text embeddings is necessary to understand the decoder's operation.
  - Quick check question: How does the text embedding guide the denoising process in a diffusion model, and why is this suitable for compression?

## Architecture Onboarding

- Component map: Image → CLIP embedding → prompt inversion → text tokens → lossless compression; HED edge map → learned NTC compression; Lossless decompress text → text-to-image model (ControlNet) → HED map conditioning → final image
- Critical path: Image → CLIP embedding → prompt inversion → text tokens → ControlNet (with HED conditioning) → reconstructed image
- Design tradeoffs:
  - Using pre-trained models avoids training cost but may limit adaptability to niche domains.
  - Adding sketches improves spatial fidelity but increases bitrate slightly.
  - Prompt inversion is effective for semantics but may struggle with fine-grained spatial details without the sketch.
- Failure signatures:
  - Semantic drift: Reconstructed images capture the gist but miss key objects or attributes.
  - Structural loss: Images lack the spatial layout of the original despite good semantics.
  - Poor realism: Generated images appear blurry or unrealistic, indicating the diffusion model failed to denoise properly.
- First 3 experiments:
  1. Test prompt inversion alone on a small image set; measure CLIP similarity between original and reconstructed images.
  2. Add HED sketch compression; verify ControlNet can reconstruct spatial structure when given both prompt and sketch.
  3. Benchmark against HiFiC and NTC baselines on rate-distortion and perceptual metrics (FID, KID, dCLIP) at sub-0.01 bpp rates.

## Open Questions the Paper Calls Out

- Question: How would a human study evaluating human satisfaction with reconstructed images compare to the CLIP-based semantic similarity metrics used in this paper?
  - Basis in paper: [explicit] The authors acknowledge that "ideally, a human study would be performed, which we leave for future work" and note that "human-aligned semantic reference metrics are still an open problem."
  - Why unresolved: The paper uses CLIP-based cosine similarity as a proxy for semantic fidelity, but the authors recognize this may not fully capture human satisfaction. A direct human study would provide more definitive evidence of how well their method preserves semantic information from a human perspective.
  - What evidence would resolve it: Results from a human study where participants rate the semantic similarity and overall satisfaction of reconstructed images compared to the originals, across different compression methods including PIC and PICS.

- Question: What is the impact of using different text-to-image models (e.g., DALL-E, Imagen) instead of Stable Diffusion and ControlNet on the performance of PIC and PICS?
  - Basis in paper: [inferred] The paper specifically uses Stable Diffusion and ControlNet, noting that "our work does not necessarily require diffusion models per se; it can use any foundation model that can generate images from text, pre-trained at scale." This suggests that the choice of model could affect performance.
  - Why unresolved: The paper only evaluates one specific combination of text-to-image models. Different models may have varying capabilities in terms of semantic understanding, image quality, and spatial reasoning, which could impact the effectiveness of the compression scheme.
  - What evidence would resolve it: Experiments comparing PIC and PICS performance using different text-to-image models, measuring metrics like FID, KID, and dCLIP, as well as qualitative assessments of image quality and semantic preservation.

- Question: How does the performance of PIC and PICS scale with increasing bitrate, and is there a point where traditional pixel-based compression methods become more effective?
  - Basis in paper: [explicit] The authors state that their method is effective "at very low bit-rates" and demonstrate results below 0.013 bits-per-pixel. They also mention that "at large rates, pixel-wise fidelity dominates human satisfaction; at low rates, pixel-wise fidelity becomes less meaningful."
  - Why unresolved: The paper focuses on demonstrating the effectiveness of their method at ultra-low bitrates, but does not explore how it performs as the bitrate increases. There may be a crossover point where traditional methods become more effective.
  - What evidence would resolve it: A comprehensive evaluation of PIC and PICS performance across a wider range of bitrates, compared to traditional compression methods, measuring both perceptual quality metrics and human satisfaction (if a study is conducted).

## Limitations
- Effectiveness heavily depends on the quality and generalization capability of pre-trained models, which may not perform well on images significantly different from their training data.
- Requires both the pre-trained models and the lightweight sketch compressor to be available at the decoder, which may not be feasible in all deployment scenarios.
- May not scale well to higher quality requirements where traditional codecs might still be competitive.

## Confidence

- High Confidence: The claim that pre-trained models can be leveraged for zero-shot compression without end-to-end training is well-supported by the experimental results showing state-of-the-art performance at ultra-low bitrates.
- Medium Confidence: The assertion that prompt inversion effectively preserves semantic content is plausible given CLIP's strong zero-shot capabilities, though the specific implementation details for optimal prompt inversion remain underspecified.
- Medium Confidence: The effectiveness of combining text prompts with edge maps through ControlNet is demonstrated but the specific weighting and interaction between these modalities could benefit from further exploration.

## Next Checks

1. Test the method's robustness across diverse image domains beyond natural photographs, including medical imaging, technical drawings, and synthetic graphics, to evaluate pre-trained model generalization limits.
2. Conduct ablation studies varying the edge detection method (beyond HED) and analyzing the impact on reconstruction quality to determine the necessity of specific spatial features.
3. Implement a user study comparing subjective quality judgments between PICS reconstructions and traditional codecs at comparable bitrates to validate objective metric correlations with human perception.