---
ver: rpa2
title: 'Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured
  Data?'
arxiv_id: '2309.08963'
source_url: https://arxiv.org/abs/2309.08963
tags:
- table
- format
- tables
- data
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically evaluates Large Language Models (LLMs)\
  \ on generating complex structured data. It introduces STRUC-BENCH, a benchmark\
  \ covering raw text, HTML, and LaTeX tables, and proposes two novel metrics\u2014\
  GPTscore and H-Score\u2014for assessing both content and structural accuracy."
---

# Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?

## Quick Facts
- arXiv ID: 2309.08963
- Source URL: https://arxiv.org/abs/2309.08963
- Authors: 
- Reference count: 11
- Key outcome: Introduces STRUC-BENCH benchmark and structure-aware fine-tuning method that significantly improves LLM performance on generating complex structured data

## Executive Summary
This study evaluates Large Language Models (LLMs) on generating complex structured data through STRUC-BENCH, a benchmark covering raw text, HTML, and LaTeX tables. The authors introduce two novel metrics—GPTscore and H-Score—for assessing both content and structural accuracy. By applying a structure-aware fine-tuning approach using FormatCoT and self-instruct methods to LLaMA-7B, the model significantly outperforms GPT-3.5, GPT-4, and Vicuna across all metrics. Human evaluation confirms that the proposed metrics align well with content accuracy while better capturing format accuracy.

## Method Summary
The study introduces STRUC-BENCH, a benchmark for evaluating LLM performance on structured data generation tasks. The methodology involves developing a structure-aware fine-tuning method using FormatCoT and self-instruct approaches, then applying this to LLaMA-7B. The process includes generating format instructions from target outputs using Chain-of-Thought prompting, training the model to follow these natural language instructions, and evaluating performance using proposed metrics (GPTscore and H-Score) alongside classical metrics. The evaluation covers raw text, HTML, and LaTeX table formats across multiple representative LLMs.

## Key Results
- LLaMA-7B fine-tuned with structure-aware approach outperforms GPT-3.5, GPT-4, and Vicuna on STRUC-BENCH
- GPTscore and H-Score metrics show better alignment with human judgment than existing evaluation methods
- Structure-aware fine-tuning significantly improves adherence to complex formatting requirements
- Content and structure evaluation components provide granular insights into LLM performance weaknesses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structure-aware fine-tuning improves adherence to complex formatting requirements
- Mechanism: By generating format instructions from target outputs and training LLaMA to follow these natural language instructions, the model learns to replicate complex structures without explicit syntax memorization
- Core assumption: Natural language format descriptions are sufficient for models to learn structural patterns
- Evidence anchors:
  - [abstract] "We introduce structure-aware instruction tuning, using ChatGPT to generate format instructions and then training the LLaMA model to follow these formats."
  - [section] "Our structure-aware fine-tuning method, when applied to LLaMA-7B, significantly improves adherence to natural language constraints, outperforming other evaluated LLMs."
- Break condition: If the model cannot generalize format instructions to unseen table structures or if the instruction generation process fails to capture critical formatting nuances

### Mechanism 2
- Claim: Breaking evaluation into content and structure components provides more granular insights into LLM performance
- Mechanism: By separating content similarity (based on table cell data) from structural similarity (based on rows, columns, alignment), evaluators can identify whether errors stem from incorrect data extraction or improper formatting
- Core assumption: Content and structure can be meaningfully separated for evaluation purposes
- Evidence anchors:
  - [abstract] "We propose to break down the similarity of two tables into two coarse components: content and structure."
  - [section] "Both similarity scores do overlap (e.g. a table with the wrong number of rows/columns would likely score poorly on content), but we find that these two scoring categories allow us to perform more involved analysis on where predicted and ground-truth tables differ."
- Break condition: If the separation between content and structure becomes ambiguous in complex table formats, making evaluation scores inconsistent

### Mechanism 3
- Claim: FormatCoT generates format-specific instructions that capture structural requirements effectively
- Mechanism: Chain-of-Thought prompting with GPT-3.5 generates detailed format descriptions from target outputs, creating training data that explicitly captures formatting patterns
- Core assumption: Chain-of-Thought reasoning helps models generate more complete format descriptions
- Evidence anchors:
  - [section] "Inspired by Gorilla (Patil et al., 2023), We provide three demos with in-context learning and task the model with generating instructions that describe the format of the given structure."
  - [abstract] "To address complex formatting requirements, we utilize a FORMAT COT (Chain-of-Thought) to generate format instructions from target outputs."
- Break condition: If FormatCoT fails to capture critical formatting details or produces inconsistent instructions across similar table structures

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Enables models to generate detailed, step-by-step format descriptions rather than superficial summaries
  - Quick check question: How does CoT differ from standard prompting when generating format instructions?

- Concept: Structured data generation evaluation metrics
  - Why needed here: Traditional text generation metrics (BLEU, ROUGE) are insufficient for assessing structured output accuracy
  - Quick check question: Why can't we simply use word overlap metrics to evaluate table generation quality?

- Concept: Instruction tuning methodology
  - Why needed here: Allows adaptation of pre-trained models to follow specific format instructions rather than just generating natural text
  - Quick check question: What distinguishes instruction tuning from standard fine-tuning approaches?

## Architecture Onboarding

- Component map: Raw text/HTML/LaTeX tables → FormatCoT generation → Format instructions → LLaMA-7B → Structure-aware fine-tuning → Format instruction following → Generated tables → GPTscore/H-Score metrics → Performance analysis

- Critical path: Data collection → Format instruction generation → Model fine-tuning → Evaluation → Error analysis

- Design tradeoffs:
  - Using natural language format instructions vs. explicit syntax templates
  - Separating content and structure evaluation vs. holistic assessment
  - Manual instruction generation vs. automated format description extraction

- Failure signatures:
  - Low GPTscore/H-Score values indicate format adherence issues
  - Content accuracy drops suggest problems with information extraction
  - Inconsistent format instructions across similar tables point to CoT generation problems

- First 3 experiments:
  1. Generate format instructions for a small set of tables using FormatCoT and manually verify completeness
  2. Fine-tune LLaMA-7B on the instruction-table pairs and evaluate on seen data
  3. Test fine-tuned model on unseen table formats to assess generalization capability

## Open Questions the Paper Calls Out
The paper identifies several unresolved questions regarding LLM performance on structured data generation, including how performance varies across different formats (raw text, HTML, LaTeX), what specific improvements can enhance numerical reasoning capabilities, and how the structure-aware fine-tuning method impacts generalizability to unseen data formats. The paper also raises questions about the limitations of using GPT-3.5 for generating format descriptions and how multimodal LLMs compare to text-only models in this domain.

## Limitations
- The benchmark contains only 46 samples, which may not fully represent the complexity of real-world structured data generation tasks
- The separation between content and structure evaluation may not be as clean as claimed, particularly for complex table formats
- The effectiveness of FormatCoT in capturing all critical formatting nuances across diverse table structures remains uncertain

## Confidence

**High Confidence**: Existing LLM evaluation metrics perform poorly on structured data generation tasks; structure-aware fine-tuning improves format adherence

**Medium Confidence**: GPTscore and H-Score metrics provide better alignment with human judgment than existing metrics; content and structure can be meaningfully separated for evaluation

**Low Confidence**: Long-term generalizability of FormatCoT approach to capture all formatting nuances; sufficiency of natural language format descriptions for learning structural patterns

## Next Checks
1. Manually verify that FormatCoT-generated instructions capture all critical formatting details for diverse table structures, including edge cases
2. Evaluate fine-tuned LLaMA-7B on held-out table formats structurally different from training data to assess generalization
3. Conduct ablation studies on GPTscore and H-Score metrics by varying table structures and content to test metric robustness and separation claims