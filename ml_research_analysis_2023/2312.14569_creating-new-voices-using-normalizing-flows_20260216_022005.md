---
ver: rpa2
title: Creating New Voices using Normalizing Flows
arxiv_id: '2312.14569'
source_url: https://arxiv.org/abs/2312.14569
tags:
- speaker
- speech
- voices
- phoneme
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method for generating new speaker voices using
  normalizing flows in both text-to-speech (TTS) and voice conversion (VC) modes.
  The key idea is to leverage normalizing flows to map mel-spectrograms to a latent
  space, allowing for the generation of new speaker embeddings and thus new voices.
---

# Creating New Voices using Normalizing Flows

## Quick Facts
- arXiv ID: 2312.14569
- Source URL: https://arxiv.org/abs/2312.14569
- Reference count: 0
- Key outcome: Normalizing flows enable generation of new speaker voices with state-of-the-art zero-shot speech synthesis performance (62.50 speaker similarity, 68.27 naturalness on MUSHRA test)

## Executive Summary
This paper presents a novel approach for generating new speaker voices using normalizing flows in both text-to-speech (TTS) and voice conversion (VC) modes. The method leverages normalizing flows to map mel-spectrograms to a latent space, enabling the generation of new speaker embeddings and thus new voices. The approach is evaluated on two tasks: zero-shot speech synthesis (converting speech to unseen voices) and new voice speech synthesis (creating entirely new voices). Results show that the proposed method achieves state-of-the-art performance in zero-shot synthesis, with particular advantage in VC mode, and demonstrates the ability to generate voices distinct from training data.

## Method Summary
The method uses normalizing flows to transform mel-spectrograms into a latent space while conditioning on speaker embeddings, f0, phoneme information, and accent embeddings. The model employs pre-trained phone alignments and separates f0 conditioning from speaker embeddings to disentangle speaker identity from prosodic information. Training is performed via maximum likelihood estimation, with the invertible nature of normalizing flows ensuring lossless reconstruction. The approach is evaluated on a large dataset of 3,173 speakers across 6 regional accents, with zero-shot evaluation using the VCTK corpus and new voice synthesis using 120 American source utterances.

## Key Results
- State-of-the-art zero-shot speech synthesis performance: 62.50 speaker similarity and 68.27 naturalness scores on MUSHRA test
- New voice generation capability with 81 MUSHRA score for new voice similarity
- VC mode outperforms TTS mode for zero-shot synthesis due to oracle f0 conditioning
- Generated voices are perceptually distinct from training set speakers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Normalizing flows enable lossless reconstruction of mel-spectrograms from a latent space, allowing speaker identity to be disentangled from linguistic content
- Mechanism: The flow models learn invertible transformations mapping mel-spectrograms to a latent space. By conditioning on speaker embeddings, f0, and phoneme information separately, the network can remove speaker-dependent features from the latent representation, preserving only the linguistic content
- Core assumption: The invertible nature of normalizing flows ensures that all information from the input can be perfectly reconstructed from the latent space given the correct conditions
- Evidence anchors:
  - [abstract] "The model uses the provided conditions θ to better map given mel-spectrograms m to a simple known distribution pz by maximizing the likelihood"
  - [section] "The invertible nature of the flow ensures that the network is information-preserving, meaning that the latent vector is a lossless representation of the input data"
  - [corpus] Weak evidence - neighboring papers discuss normalizing flows for speech but don't explicitly confirm lossless reconstruction for speaker disentanglement
- Break condition: If the conditioning features are insufficient to fully capture all speaker-dependent information, some speaker identity may remain in the latent space, preventing perfect disentanglement

### Mechanism 2
- Claim: Separating f0 conditioning from speaker embeddings allows the model to disentangle speaker identity from prosodic information
- Mechanism: By normalizing f0 at the sentence level and providing it as separate conditioning rather than encoding it in the speaker embedding, the model can learn to associate specific pitch patterns with linguistic content rather than with specific speakers
- Core assumption: Speaker identity and prosody can be effectively separated in the learned representation space
- Evidence anchors:
  - [section] "f0 is the sentence-level mean normalised interpolated log-f0 to disentangle speaker identity (speaker's average f0) from sentence prosody and thus separating f0 from speaker embedding"
  - [section] "VC mode is preferred to TTS mode" because VC has access to oracle f0 conditioning
  - [corpus] Moderate evidence - neighboring papers on speaker adaptation mention the importance of separating speaker and prosodic features
- Break condition: If the model cannot effectively learn to separate these features, the f0 conditioning may not provide the expected benefits for zero-shot synthesis

### Mechanism 3
- Claim: Using pre-trained phone alignments simplifies training and improves model performance compared to learning alignments during training
- Mechanism: Pre-trained alignments provide accurate phoneme boundary information, allowing the model to focus on learning the mapping between phonemes and acoustic features rather than simultaneously learning alignment and synthesis
- Core assumption: Pre-trained alignments are sufficiently accurate for the diverse dataset used in training
- Evidence anchors:
  - [section] "Unlike in these papers, we use pre-trained phone alignments to label the data to simplify the training process, as in attention-free TTS"
  - [section] "We use pre-trained phone alignments to label the data to simplify the training process"
  - [corpus] Weak evidence - neighboring papers on TTS typically learn alignments but don't directly compare with pre-trained alignments
- Break condition: If pre-trained alignments are inaccurate or not well-matched to the training data, the model may learn incorrect phoneme-to-acoustic mappings

## Foundational Learning

- Concept: Normalizing flows and change of variables formula
  - Why needed here: Understanding how normalizing flows transform data distributions through invertible mappings is crucial for grasping how the model learns to generate new voices
  - Quick check question: How does the change of variables formula ensure that the transformed distribution remains valid?

- Concept: Speaker embedding spaces and representation learning
  - Why needed here: The paper relies on speaker embeddings to condition the flow models, and understanding how these embeddings capture speaker identity is key to understanding zero-shot synthesis
  - Quick check question: What properties should an ideal speaker embedding have for zero-shot voice synthesis?

- Concept: Text-to-speech vs. voice conversion architectures
  - Why needed here: The paper compares TTS and VC modes, and understanding their architectural differences helps explain why VC performs better for zero-shot synthesis
  - Quick check question: What are the key differences between TTS and VC in terms of conditioning and control over the output?

## Architecture Onboarding

- Component map: Input mel-spectrogram -> Flow steps with conditioning -> Latent representation -> Inverse flow steps -> Output mel-spectrogram
- Critical path: Input mel-spectrogram → Flow steps with conditioning → Latent representation → Inverse flow steps → Output mel-spectrogram
- Design tradeoffs:
  - Using pre-trained alignments vs. learning alignments during training (simplicity vs. potential for better adaptation)
  - Separating f0 conditioning from speaker embeddings (better disentanglement vs. increased model complexity)
  - Number of flow steps and coupling blocks (model capacity vs. training stability)
- Failure signatures:
  - Poor speaker similarity scores indicate issues with conditioning or speaker embedding quality
  - Low naturalness scores suggest problems with the flow architecture or vocoder quality
  - High WER indicates issues with phoneme alignment or linguistic modeling
- First 3 experiments:
  1. Train Flow-TTS on a small dataset with seen speakers to verify basic functionality
  2. Compare Flow-TTS with and without f0 conditioning on seen speakers to validate the disentanglement mechanism
  3. Test zero-shot synthesis on a single unseen speaker to identify potential failure modes before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can speaker embeddings be improved to better capture speaker identity without encoding extraneous information like dataset or recording conditions?
- Basis in paper: [explicit] The paper discusses that speaker embeddings are "not intuitive enough for the model to be able to fully understand everything about the target speaker identity purely from the embeddings" and that "different speech corpora form separate clusters in the learnt speaker embedding space," indicating embeddings also model unwanted factors
- Why unresolved: Current embeddings may encode information beyond speaker identity, such as recording conditions or dataset origin, which hinders zero-shot synthesis
- What evidence would resolve it: Developing and testing new speaker embedding methods that focus solely on speaker identity, and demonstrating improved zero-shot synthesis performance compared to current embeddings

### Open Question 2
- Question: How can the gap between the naturalness and speaker similarity of synthesized speech and target recordings be reduced?
- Basis in paper: [explicit] The paper states that "none of the approaches are precise in the zero-shot scenario" and there is a "gap between evaluated approaches and target recordings in terms of speaker similarity"
- Why unresolved: Current models, while achieving state-of-the-art performance, still fall short of the naturalness and speaker similarity of real recordings in zero-shot synthesis
- What evidence would resolve it: Experiments showing improved naturalness and speaker similarity scores in zero-shot synthesis, closing the gap with target recordings

### Open Question 3
- Question: What are the limits of the number and diversity of new voices that can be generated using normalizing flows?
- Basis in paper: [inferred] The paper demonstrates the ability to generate new voices that are distinct from the training set, but does not explore the limits of this capability
- Why unresolved: The paper shows successful generation of new voices, but does not investigate how many distinct voices can be generated or the range of speaker characteristics that can be covered
- What evidence would resolve it: Systematic experiments generating a large number of new voices and evaluating their diversity and distinctiveness from each other and the training set

### Open Question 4
- Question: How does the choice of conditioning features impact the quality of synthesized speech in TTS and VC modes?
- Basis in paper: [explicit] The paper compares models with and without F0 and voiced/unvoiced (VUV) conditioning, showing that F0 conditioning improves naturalness in TTS mode but not necessarily in VC mode
- Why unresolved: While the paper explores the impact of F0 and VUV conditioning, it does not comprehensively investigate the effect of other conditioning features or combinations thereof
- What evidence would resolve it: Experiments systematically varying conditioning features and evaluating their impact on speech quality in both TTS and VC modes

## Limitations

- Architectural specifications are incomplete, particularly regarding flow step configurations and hyperparameters
- Zero-shot synthesis evaluation is limited to only 2 speakers per accent group, potentially not generalizable to broader speaker populations
- Claims about generating truly "new" voices rely on perceptual tests that may not capture perceptual distinctiveness effectively

## Confidence

- **High confidence**: The effectiveness of normalizing flows for lossless mel-spectrogram reconstruction and the general approach of conditioning on speaker embeddings, f0, and phoneme information separately
- **Medium confidence**: The claim that VC mode outperforms TTS mode for zero-shot synthesis, supported by the oracle f0 conditioning advantage but lacking ablation studies on the exact contribution of each component
- **Low confidence**: The ability to generate truly "new" voices that are meaningfully different from training data, as the evaluation relies on MUSHRA binary tests and visualization techniques that may not capture perceptual distinctiveness

## Next Checks

1. **Ablation study on conditioning components**: Systematically remove f0 conditioning, speaker embeddings, and phoneme conditions individually to quantify their contribution to zero-shot synthesis performance, particularly for speaker similarity scores

2. **Generalization test across broader speaker diversity**: Evaluate the zero-shot synthesis performance on a larger, more diverse set of unseen speakers (e.g., 10+ speakers per accent group) to validate whether the reported 62.50 speaker similarity score holds across different speaker characteristics

3. **Perceptual distinctiveness validation**: Conduct listening tests specifically designed to distinguish between generated new voices and closest training voices, using techniques like ABX discrimination tasks to provide stronger evidence for the claim of creating truly new speaker identities