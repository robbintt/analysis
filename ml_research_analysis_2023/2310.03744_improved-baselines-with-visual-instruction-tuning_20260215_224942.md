---
ver: rpa2
title: Improved Baselines with Visual Instruction Tuning
arxiv_id: '2310.03744'
source_url: https://arxiv.org/abs/2310.03744
tags:
- llav
- visual
- arxiv
- data
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents LLaVA-1.5, a large multimodal model that achieves\
  \ state-of-the-art performance across 11 benchmarks using only 1.2M publicly available\
  \ data and \u223C1 day of training on a single 8-A100 node. The key innovations\
  \ include using a two-layer MLP cross-modal connector instead of a linear projection,\
  \ incorporating academic-task-oriented VQA data with response formatting prompts,\
  \ and fine-tuning the LLM."
---

# Improved Baselines with Visual Instruction Tuning

## Quick Facts
- arXiv ID: 2310.03744
- Source URL: https://arxiv.org/abs/2310.03744
- Reference count: 40
- Key outcome: LLaVA-1.5 achieves state-of-the-art performance across 11 benchmarks using only 1.2M publicly available data and ~1 day of training on a single 8-A100 node

## Executive Summary
This paper presents LLaVA-1.5, an improved large multimodal model that achieves state-of-the-art performance across 11 benchmarks through simple yet effective modifications to the original LLaVA architecture. The key innovations include replacing the linear vision-language connector with a two-layer MLP and incorporating academic-task-oriented VQA data with explicit response formatting prompts. These changes, combined with fine-tuning rather than massive pretraining, enable the model to outperform much larger models like 80B IDEFICS while using significantly less computational resources.

## Method Summary
LLaVA-1.5 builds upon the original LLaVA architecture by making two key modifications: replacing the linear vision-language connector with a two-layer MLP to enable non-linear transformations between visual and language embeddings, and incorporating academic-task-oriented VQA data with explicit response formatting prompts to better handle short-form answers. The model is trained in two stages - vision-language alignment pretraining followed by visual instruction tuning - using a mixture of 1.2M publicly available samples. The training is highly efficient, requiring only ~1 day on a single 8-A100 node node, and achieves state-of-the-art performance across multiple benchmarks without requiring massive pretraining datasets.

## Key Results
- Achieves state-of-the-art performance across 11 benchmarks with only 1.2M training samples
- Outperforms 80B IDEFICS on many benchmarks despite being significantly smaller
- Requires only ~1 day of training on a single 8-A100 node, demonstrating exceptional data and computational efficiency
- Visual instruction tuning plays a more important role than large-scale pretraining for multimodal instruction-following capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a two-layer MLP cross-modal connector instead of a linear projection significantly improves multimodal alignment quality.
- Mechanism: The MLP introduces non-linear transformations that allow the connector to learn more complex mappings between visual and language embeddings, capturing richer cross-modal relationships.
- Core assumption: Visual-language alignment benefits from non-linear transformations rather than simple linear projections.
- Evidence anchors:
  - [abstract] "namely, using CLIP-ViT-L-336px with an MLP projection"
  - [section] "we find that improving the vision-language connector's representation power with a two-layer MLP can improve LLaVA's multimodal capabilities, compared with the original linear projection design"
  - [corpus] Weak evidence - no direct citations in corpus supporting this specific MLP improvement claim
- Break condition: If the MLP architecture becomes too complex relative to the amount of training data, it may overfit rather than generalize.

### Mechanism 2
- Claim: Incorporating academic-task-oriented VQA data with clear response formatting prompts dramatically improves short-form answer performance.
- Mechanism: The combination of task-specific data and explicit formatting instructions conditions the model to produce appropriate output lengths and formats for different question types.
- Core assumption: Clear response formatting prompts can effectively control output behavior without requiring complex data processing.
- Evidence anchors:
  - [abstract] "incorporating academic-task-oriented VQA data with response formatting prompts"
  - [section] "We find that the inability [5] to balance between short- and long-form VQA for approaches like InstructBLIP [9] is mainly due to...ambiguous prompts on the response format"
  - [corpus] Weak evidence - no direct citations in corpus supporting this specific formatting approach
- Break condition: If formatting prompts are too restrictive, the model may lose flexibility in handling natural conversations.

### Mechanism 3
- Claim: Visual instruction tuning is more important than large-scale pretraining for multimodal instruction-following capabilities.
- Mechanism: Direct fine-tuning on instruction-following data teaches the model to follow user instructions effectively, while pretraining primarily establishes general alignment between modalities.
- Core assumption: Instruction-following capability can be effectively learned through targeted fine-tuning rather than requiring massive pretraining datasets.
- Evidence anchors:
  - [abstract] "visual instruction tuning plays a more important role than large-scale pretraining for multimodal instruction-following capabilities"
  - [section] "The results also suggest that visual instruction tuning plays a more important role in improving an LMM's capabilities than pretraining"
  - [corpus] Weak evidence - no direct citations in corpus supporting this specific pretraining vs. fine-tuning claim
- Break condition: If the base language model lacks sufficient capability, instruction tuning alone may not be sufficient to achieve strong performance.

## Foundational Learning

- Concept: Cross-modal alignment
  - Why needed here: Understanding how visual and language representations are mapped together is crucial for working with LMMs
  - Quick check question: What is the difference between linear projection and MLP-based cross-modal connectors in terms of representational capacity?

- Concept: Instruction tuning methodology
  - Why needed here: The paper relies heavily on instruction tuning as the primary mechanism for capability development
  - Quick check question: How does instruction tuning differ from traditional pretraining in terms of objectives and data requirements?

- Concept: Response formatting and output control
  - Why needed here: The paper demonstrates that explicit formatting prompts can control output behavior effectively
  - Quick check question: Why might ambiguous response prompts lead to overfitting to short-form answers?

## Architecture Onboarding

- Component map: Visual encoder (CLIP-ViT-L-336px) → Cross-modal connector (2-layer MLP) → Language model (Vicuna) → Response formatting system
- Critical path: Visual encoder → Cross-modal connector → Language model → Response formatting
- Design tradeoffs:
  - Simple architecture vs. specialized visual resamplers
  - Academic compute requirements vs. massive pretraining
  - Public data availability vs. proprietary datasets
- Failure signatures:
  - Poor performance on short-form VQA tasks suggests formatting prompt issues
  - Inability to follow natural conversations suggests insufficient instruction tuning data
  - Slow convergence suggests inadequate visual-language alignment
- First 3 experiments:
  1. Compare linear projection vs. MLP connector performance on a small subset of data
  2. Test different response formatting prompts on a simple VQA task
  3. Measure the impact of adding academic VQA data to the training mix

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of visual instruction tuning compare to large-scale pretraining for multimodal instruction-following capabilities?
- Basis in paper: [explicit] The paper explicitly states that visual instruction tuning plays a more important role in improving an LMM's capabilities than pretraining, raising questions upon the common belief that LMMs require significant amounts of vision-language alignment pretraining.
- Why unresolved: While the paper provides evidence through their LLaVA-1.5 model achieving state-of-the-art results with minimal pretraining, it does not provide a comprehensive comparison across different scales of pretraining and instruction tuning.
- What evidence would resolve it: A systematic study comparing LMMs with varying amounts of pretraining and instruction tuning data, measuring their performance on a wide range of benchmarks.

### Open Question 2
- Question: What is the optimal architecture for a vision-language connector in LMMs?
- Basis in paper: [explicit] The paper shows that using an MLP cross-modal connector instead of a linear projection leads to significant performance improvements. However, it does not explore other potential architectures or compare their effectiveness.
- Why unresolved: The paper only tests one alternative architecture (MLP) and does not provide a comprehensive comparison with other possible designs.
- What evidence would resolve it: A study comparing the performance of LMMs with different vision-language connector architectures (e.g., transformer-based, graph neural networks) on a diverse set of benchmarks.

### Open Question 3
- Question: How can LMMs be made more efficient in terms of training data and computational resources?
- Basis in paper: [explicit] The paper highlights the data efficiency of their LLaVA-1.5 model, achieving state-of-the-art results with only 1.2M publicly available data and ~1 day of training on a single 8-A100 node. However, it does not explore other potential techniques for improving efficiency.
- Why unresolved: The paper focuses on a specific set of modifications and does not provide a comprehensive analysis of other potential efficiency improvements.
- What evidence would resolve it: A study exploring various techniques for improving the data efficiency and computational efficiency of LMMs, such as knowledge distillation, pruning, or quantization, and their impact on performance.

## Limitations

- Evaluation primarily focuses on academic benchmarks without extensive real-world deployment testing
- Performance comparisons to 80B IDEFICS may be misleading due to confounding factors across different architectures
- Data efficiency claims rely on publicly available datasets but exact composition and quality control procedures are not fully specified

## Confidence

**High Confidence:**
- Technical modifications (MLP connector, response formatting prompts) are correctly implemented and produce measurable improvements
- Training methodology and two-stage approach are sound and reproducible
- Performance gains on tested benchmarks are genuine and statistically significant

**Medium Confidence:**
- Claim that visual instruction tuning is more important than large-scale pretraining
- Assertion that LLaVA-1.5 outperforms 80B IDEFICS on many benchmarks
- Generalizability of improvements to downstream applications beyond tested benchmarks

**Low Confidence:**
- Data efficiency claims given lack of detailed analysis of training data quality and composition
- Robustness to real-world scenarios not represented in benchmark datasets
- Long-term stability and performance consistency across different deployment contexts

## Next Checks

**Validation Check 1:** Conduct ablation studies specifically isolating the impact of the MLP connector versus the response formatting prompts, including training models with only one modification at a time to quantify their individual contributions.

**Validation Check 2:** Test the model's performance on real-world deployment scenarios that include adversarial examples, ambiguous queries, and multimodal inputs that don't match the training distribution to validate generalizability beyond academic benchmarks.

**Validation Check 3:** Perform a comprehensive analysis of the 1.2M training data composition, including quality metrics, class balance, and domain coverage, paired with controlled experiments varying the data composition to validate data efficiency claims.