---
ver: rpa2
title: 'A Survey of Large Language Models in Medicine: Progress, Application, and
  Challenge'
arxiv_id: '2311.05112'
source_url: https://arxiv.org/abs/2311.05112
tags:
- medical
- llms
- arxiv
- language
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively reviews the development, applications,
  and challenges of large language models (LLMs) in medicine. It categorizes existing
  medical LLMs by their development approaches: pre-training, fine-tuning, and prompting.'
---

# A Survey of Large Language Models in Medicine: Progress, Application, and Challenge

## Quick Facts
- arXiv ID: 2311.05112
- Source URL: https://arxiv.org/abs/2311.05112
- Reference count: 40
- Primary result: Comprehensive survey of medical LLMs covering development, applications, and challenges, with focus on ten biomedical NLP tasks and four key deployment challenges.

## Executive Summary
This survey systematically reviews the landscape of large language models in medicine, categorizing them by development approaches—pre-training, fine-tuning, and prompting—and evaluating their performance across ten biomedical NLP tasks. The authors summarize current clinical applications, including diagnosis, report generation, and medical education, while identifying major challenges such as hallucination, lack of evaluation benchmarks, data limitations, and ethical concerns. The paper outlines actionable future directions for advancing medical LLMs, emphasizing the need for new benchmarks, interdisciplinary collaboration, and multimodal integration.

## Method Summary
The survey analyzes existing medical LLMs by organizing them according to their development methods (pre-training, fine-tuning, prompting) and evaluating their performance on a set of ten biomedical NLP tasks. It synthesizes findings from recent literature to identify recurring challenges in clinical deployment and proposes future research directions. The methodology includes a systematic review of performance metrics and clinical applications, though specific evaluation datasets and metrics are not fully detailed in the paper.

## Key Results
- Medical LLMs are categorized into three development approaches: pre-training, fine-tuning, and prompting.
- Performance is summarized across ten biomedical NLP tasks, though the exact tasks are not specified.
- Four major challenges for clinical deployment are identified: hallucination, lack of evaluation benchmarks, domain data limitations, and ethical concerns.
- Future directions include new benchmarks, interdisciplinary collaboration, and multimodal integration.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Survey comprehensively categorizes medical LLMs by development approach and evaluates them across ten biomedical NLP tasks.
- Mechanism: Authors organize medical LLMs into three development categories—pre-training, fine-tuning, and prompting—and assess performance on tasks like medical QA, NER, and summarization to map capabilities.
- Core assumption: Task-specific evaluation reflects real-world clinical utility.
- Evidence anchors:
  - [abstract] "categorizes existing medical LLMs by their development approaches: pre-training, fine-tuning, and prompting. The paper summarizes their performance across ten biomedical NLP tasks"
  - [section] "we conduct an extensive survey on the performances of existing medical LLMs across ten biomedical NLP tasks"
  - [corpus] Weak evidence; neighbor papers confirm broad task coverage but do not specify the exact ten tasks.
- Break Condition: If clinical relevance of the ten tasks is questioned, the survey's validity as a practical guide is weakened.

### Mechanism 2
- Claim: Survey identifies four major challenges—hallucination, lack of evaluation benchmarks, domain data limitations, and ethical concerns—that must be addressed before clinical deployment.
- Mechanism: Systematic analysis of literature and real-world studies surfaces recurring problems in medical LLM use, providing a framework for future research priorities.
- Core assumption: Reported challenges are representative across diverse medical LLM applications.
- Evidence anchors:
  - [abstract] "Key challenges identified include hallucination, lack of evaluation benchmarks, domain data limitations, and ethical concerns"
  - [section] "Challenges must be addressed when implementing medical LLMs in clinical practice. These challenges include issues like hallucination (i.e. generation of coherent and contextually relevant but factually incorrect outputs)"
  - [corpus] Strong evidence; neighbor papers list identical challenge categories, reinforcing the survey's comprehensiveness.
- Break Condition: If new studies reveal additional, unaddressed challenges, the survey's completeness is called into question.

### Mechanism 3
- Claim: Survey proposes future directions—new benchmarks, interdisciplinary collaboration, multimodal integration, and applications in less established fields—to advance medical LLM development.
- Mechanism: Authors synthesize current limitations and trends to outline actionable research pathways, bridging gaps between AI research and clinical practice.
- Core assumption: Proposed directions are feasible and align with emerging healthcare needs.
- Evidence anchors:
  - [abstract] "outlines future directions for improving medical LLMs through new benchmarks, interdisciplinary collaborations, and multimodal integration"
  - [section] "Future research is needed to validate the effectiveness of using synthetic data for LLMs in the medical field"
  - [corpus] Moderate evidence; neighbor papers echo similar future directions but with varying emphasis.
- Break Condition: If interdisciplinary barriers or data scarcity prove insurmountable, proposed directions may stall.

## Foundational Learning

- Concept: Large language model development approaches (pre-training, fine-tuning, prompting)
  - Why needed here: Understanding these approaches is essential to grasp how medical LLMs are built and why certain architectures suit specific clinical tasks.
  - Quick check question: What is the main difference between fine-tuning and prompting in adapting a general LLM to the medical domain?

- Concept: Evaluation benchmarks and metrics in biomedical NLP
  - Why needed here: Accurate assessment of medical LLM performance requires familiarity with domain-specific benchmarks like BLUE and BLURB, and understanding their limitations.
  - Quick check question: Why might standard accuracy metrics be insufficient for evaluating medical LLMs?

- Concept: Clinical applications and safety considerations for AI in healthcare
  - Why needed here: Deploying LLMs in medicine requires awareness of regulatory, ethical, and practical constraints unique to clinical environments.
  - Quick check question: What are the primary ethical risks of using LLMs for medical diagnosis or report generation?

## Architecture Onboarding

- Component map:
  - Data pipeline: medical corpora (PubMed, MIMIC-III, PMC) → pre-training/fine-tuning datasets → evaluation datasets
  - Model stack: transformer architecture → task-specific fine-tuning layers → prompt templates
  - Evaluation framework: benchmark datasets (BLUE, BLURB, MedQA) → performance metrics → clinical utility assessment

- Critical path:
  1. Acquire and preprocess domain-specific medical data
  2. Select base LLM (encoder-only, decoder-only, encoder-decoder)
  3. Apply development approach (pre-train, fine-tune, or prompt)
  4. Evaluate on biomedical NLP tasks and clinical benchmarks
  5. Iterate on model or data based on performance and safety analysis

- Design tradeoffs:
  - Pre-training from scratch vs. fine-tuning: computational cost vs. domain specificity
  - Task-specific vs. generalist models: performance on narrow tasks vs. broad applicability
  - Model size vs. inference latency: accuracy vs. real-time clinical deployment

- Failure signatures:
  - Hallucinations: coherent but incorrect medical information in outputs
  - Data leakage: inadvertent exposure of patient information in generated text
  - Poor generalization: strong benchmark scores but weak real-world clinical performance

- First 3 experiments:
  1. Fine-tune a general LLM on a small, high-quality medical QA dataset and measure performance on MedQA (USMLE)
  2. Compare hallucination rates between few-shot prompting and fine-tuning on the same medical task
  3. Evaluate model fairness and bias across different demographic groups using synthetic patient scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can new medical knowledge (e.g., new diseases, treatments) be effectively integrated into LLMs without requiring full retraining?
- Basis in paper: [explicit] The paper identifies "new knowledge adaptation" as a key challenge, noting that re-training LLMs is expensive and inefficient when medical knowledge updates are needed.
- Why unresolved: Current solutions like model editing are limited in generalization across architectures, and retrieval-augmented generation introduces latency and dependency on external sources.
- What evidence would resolve it: Demonstration of a scalable, real-time method for updating medical knowledge in LLMs that maintains accuracy and reasoning capabilities without full retraining.

### Open Question 2
- Question: What evaluation benchmarks and metrics are needed to assess LLM performance beyond accuracy, particularly for safety-critical medical applications?
- Basis in paper: [explicit] The paper highlights the lack of evaluation benchmarks and metrics, noting that current benchmarks focus on accuracy but fail to assess trustworthiness, faithfulness, and other critical dimensions.
- Why unresolved: Existing benchmarks like MedQA and MedMCQA do not evaluate nuanced aspects like fairness, ethics, or real-world clinical reasoning.
- What evidence would resolve it: Development and validation of comprehensive medical-specific benchmarks that assess LLM capabilities in realistic clinical scenarios, including safety, fairness, and explainability metrics.

### Open Question 3
- Question: How can multimodal LLMs be developed to effectively integrate time-series medical data (e.g., ECG, PPG) alongside text and visual information?
- Basis in paper: [explicit] The paper identifies limited perception capabilities, fragile reasoning chains, and suboptimal instruction-following as challenges for multimodal LLMs in medicine.
- Why unresolved: Current multimodal approaches focus primarily on vision-language tasks, with few addressing time-series medical data integration.
- What evidence would resolve it: Creation of a multimodal medical LLM that demonstrates robust performance on tasks requiring integration of text, images, and time-series physiological data, validated against clinical outcomes.

## Limitations
- The exact list of ten biomedical NLP tasks used for evaluation is not specified, limiting reproducibility.
- Claims about clinical utility are based on benchmark results rather than real-world deployment data, so practical relevance is uncertain.
- The literature search may have missed recent or niche studies, especially those focused on specific medical specialties or languages.

## Confidence
- **High**: The identification of core challenges (hallucination, data limitations, ethical concerns) is well-supported by both the survey and neighboring literature.
- **Medium**: The categorization of development approaches (pre-training, fine-tuning, prompting) is methodologically sound, but performance summaries may lack granularity.
- **Low**: Claims about the clinical impact of medical LLMs are largely speculative due to limited real-world evidence.

## Next Checks
1. Obtain the full list of ten biomedical NLP tasks used in the survey and verify their clinical relevance.
2. Reassess performance comparisons using consistent metrics and up-to-date benchmarks across all surveyed models.
3. Identify and review recent studies published after the survey's cutoff date to check for emerging challenges or novel applications not covered.