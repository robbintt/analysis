---
ver: rpa2
title: Contextual Emotion Estimation from Image Captions
arxiv_id: '2309.13136'
source_url: https://arxiv.org/abs/2309.13136
tags:
- emotion
- image
- emotions
- physical
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential of Large Language Models
  (LLMs) to estimate emotions from images by first captioning them and then inferring
  emotions from the captions. The study addresses the challenge of contextual emotion
  estimation in images, typically approached using computer vision methods that directly
  estimate emotions from faces, body poses, and contextual cues.
---

# Contextual Emotion Estimation from Image Captions

## Quick Facts
- arXiv ID: 2309.13136
- Source URL: https://arxiv.org/abs/2309.13136
- Reference count: 23
- Key outcome: LLMs can estimate emotions from image captions with reasonable accuracy, with contextual information enhancing prediction performance

## Executive Summary
This paper investigates the potential of Large Language Models (LLMs) to estimate emotions from images by first captioning them and then inferring emotions from the captions. The study addresses the challenge of contextual emotion estimation in images, typically approached using computer vision methods that directly estimate emotions from faces, body poses, and contextual cues. To explore the LLM approach, the authors manually generate captions and emotion annotations for a subset of 331 images from the EMOTIC dataset, using natural language descriptors for faces, bodies, interactions, and environments. They then test the capability of GPT-3.5 (text-davinci-003) to infer emotions from these captions. The results show that GPT-3.5 provides reasonable emotion predictions consistent with human annotations, with accuracy varying by emotion concept.

## Method Summary
The study uses 331 images from the EMOTIC dataset with manually generated captions containing physical signals, social interactions, and environmental contexts. The captions were created using an annotation interface that captured 153 physical signals, interactions, and environmental descriptions. GPT-3.5 (text-davinci-003) was then used to predict emotions from these captions with temperature=0 for deterministic output. The predictions were evaluated against human-annotated ground truth using precision, recall, and F1 scores for 13 negative emotions. The study also performed ablation experiments by removing social interactions or environmental contexts from captions to assess their impact on emotion prediction accuracy.

## Key Results
- GPT-3.5 provides reasonable emotion predictions consistent with human annotations across multiple emotion types
- Embarrassment was well-predicted with social interactions, while Aversion and Disquietment showed lower F1 scores
- Ablation studies revealed that removing social interactions and environmental contexts impacted prediction accuracy, though effects varied by emotion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can infer emotions from image captions by leveraging stored associations between textual descriptions and emotional concepts
- Mechanism: The model maps input text describing physical signals, social interactions, and environmental contexts to an emotion label based on patterns learned during pretraining on large corpora
- Core assumption: The LLM has been exposed to sufficient textual data linking descriptive phrases to emotional states during pretraining
- Evidence anchors:
  - [abstract] "The ability of these models to understand human language and store data in their extensive neural network attributed this success."
  - [section] "These approaches have allowed sophisticated language models to perform a range of tasks with high accuracy and efficiency."
  - [corpus] Weak - related papers focus on emotional reasoning but do not directly confirm the specific mapping mechanism in this study
- Break condition: If the LLM lacks exposure to emotion-relevant text during pretraining, or if the image captions contain novel combinations of cues not represented in training data

### Mechanism 2
- Claim: Contextual information (social interactions, environment) enhances emotion prediction accuracy beyond physical signals alone
- Mechanism: The model uses additional contextual cues to disambiguate between emotions that share similar physical signals, improving precision through broader scene understanding
- Core assumption: Social and environmental contexts provide unique information that, when combined with physical signals, create distinctive emotional signatures
- Evidence anchors:
  - [abstract] "The results show that GPT-3.5 provides reasonable emotion predictions consistent with human annotations, with accuracy varying by emotion concept."
  - [section] "Embarrassment was fairly well predicted with social interactions."
  - [corpus] Weak - no direct corpus evidence for this specific contextual enhancement mechanism
- Break condition: If the contextual information is redundant with physical signals or if the model cannot effectively integrate multi-modal cues

### Mechanism 3
- Claim: Ablation studies reveal which types of contextual information are most critical for specific emotions
- Mechanism: By systematically removing social interaction or environmental descriptions from captions, the study identifies which contexts are necessary for accurate emotion prediction of different emotion types
- Core assumption: Different emotions rely on different combinations of contextual information, and these dependencies can be revealed through controlled removal experiments
- Evidence anchors:
  - [abstract] "The study suggests promise in the image captioning and LLM approach for contextual emotion estimation, while also highlighting the importance of context and the potential for biases in the models."
  - [section] "The lack of context about a person's social interactions with other people seems to impact how Embarrassment was perceived by GPT-3.5 the most."
  - [corpus] Weak - no corpus evidence specifically addressing ablation methodology for emotion detection
- Break condition: If the ablation studies do not reveal clear patterns, or if the model compensates for missing information in unexpected ways

## Foundational Learning

- Concept: Natural language processing and transformer architectures
  - Why needed here: The LLM relies on transformer-based architectures to process and generate text-based emotion predictions from image captions
  - Quick check question: Can you explain how self-attention mechanisms in transformers enable the model to weigh the importance of different words in the input caption?

- Concept: Emotion theory and physical signal recognition
  - Why needed here: Understanding how physical signals (facial expressions, body language) correlate with emotional states is essential for creating accurate image captions that the LLM can interpret
  - Quick check question: What are some key physical signals associated with fear, and how might they appear in an image caption?

- Concept: Dataset annotation and evaluation methodology
  - Why needed here: The study relies on manual annotation of images and comparison of LLM predictions to human-annotated ground truth for validation
  - Quick check question: How would you design an annotation interface to capture both physical signals and contextual information for emotion estimation?

## Architecture Onboarding

- Component map:
  Image preprocessing → Caption generation (manual) → LLM inference (GPT-3.5) → Emotion prediction → Evaluation against ground truth
  Key components: EMOTIC dataset, annotation interface, GPT-3.5 API, evaluation metrics (precision, recall, F1)

- Critical path:
  1. Annotate images with physical signals, social interactions, and environmental contexts
  2. Generate detailed captions from annotations
  3. Submit captions to GPT-3.5 with emotion prediction prompt
  4. Compare LLM predictions to human-annotated ground truth
  5. Analyze results and perform ablation studies

- Design tradeoffs:
  - Manual vs. automatic caption generation (accuracy vs. scalability)
  - Limited emotion set (13 negative emotions) vs. comprehensive emotion coverage
  - Fixed prompt structure vs. adaptive prompting for different contexts
  - Single prediction vs. probability distribution over emotions

- Failure signatures:
  - Low F1 scores for specific emotions (e.g., Aversion, Disquietment)
  - Confusion between similar emotions (e.g., Emotional Pain/Suffering vs. Sadness)
  - Inconsistent predictions across multiple runs (though temperature=0 was used)
  - Failure to predict emotions when key contextual information is missing

- First 3 experiments:
  1. Full caption experiment - test emotion prediction with complete image descriptions including physical signals, social interactions, and environmental contexts
  2. Minus interactions ablation - remove social interaction descriptions to test their contribution to emotion prediction accuracy
  3. Minus environments ablation - remove environmental context descriptions to evaluate their importance for emotion recognition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well can LLMs generalize emotion perception across different cultures and languages?
- Basis in paper: [explicit] The paper notes that the Emotion Thesaurus is written by North American authors and that different cultures may express emotions with different facial and bodily signals. It also mentions that GPT-3.5, trained in English, carries biases in the association of facial, bodily and contextual signals with the final emotion.
- Why unresolved: The study only tested GPT-3.5's emotion perception on English captions from a Western dataset. It did not explore how well the model would perform on captions in other languages or depicting people from different cultural backgrounds.
- What evidence would resolve it: Testing GPT-3.5 (or other LLMs) on a multilingual, multicultural dataset of captioned images, and comparing the emotion predictions to human annotations from diverse cultural backgrounds.

### Open Question 2
- Question: Can LLMs accurately differentiate between closely related negative emotions like Sadness and Emotional Pain/Suffering?
- Basis in paper: [explicit] The results showed that Emotional Pain/Suffering was frequently predicted as Sadness by GPT-3.5, even in the presence of scene contexts. The paper suggests this may be because these two emotions share similar physical signals, or because GPT-3.5 tends to select the more commonly known emotion from the list it was provided.
- Why unresolved: The study did not investigate why GPT-3.5 struggled to differentiate between these emotions. It also did not explore whether this is a limitation of the model itself or the way the emotions were defined and annotated.
- What evidence would resolve it: A deeper analysis of the physical signals and contextual cues associated with Sadness and Emotional Pain/Suffering, and testing whether GPT-3.5 (or other LLMs) can better differentiate these emotions when provided with more specific and nuanced descriptions.

### Open Question 3
- Question: Can LLMs accurately predict emotions from image captions without relying on facial and body signals?
- Basis in paper: [explicit] The ablation studies showed that removing social interactions and environmental contexts from the captions impacted the accuracy of GPT-3.5's emotion predictions. However, the study did not specifically investigate how well the model could predict emotions when only facial and body signals were provided, or when these signals were minimal or absent.
- Why unresolved: The study focused on how social and environmental contexts contribute to emotion perception, but did not isolate the contribution of facial and body signals. It also did not explore how well GPT-3.5 could predict emotions in images where these signals were not visible or clear.
- What evidence would resolve it: Testing GPT-3.5's emotion prediction accuracy on a dataset of image captions that vary in the amount and clarity of facial and body signals provided, and comparing the results to human annotations.

## Limitations

- Small sample size (331 images) from a single dataset limits generalizability across diverse image types and cultural contexts
- Manual caption generation process introduces potential human bias and prevents scalable application
- Focus on 13 negative emotions leaves performance on positive emotions unexplored

## Confidence

- Core claim (LLMs can effectively estimate emotions from image captions): Medium confidence
- Mechanism 2 (contextual information enhances emotion prediction): Medium confidence
- Mechanism 3 (different emotions rely on different contextual combinations): Low confidence

## Next Checks

1. **Cross-dataset validation**: Test the same approach on a different emotion recognition dataset (e.g., AffectNet or Flickr-Faces-HQ) with varying image characteristics and cultural contexts to assess generalizability beyond the EMOTIC dataset.

2. **Automated caption generation comparison**: Replace manual captions with automatically generated ones from a state-of-the-art image captioning model and measure whether LLM emotion prediction accuracy remains consistent, addressing scalability concerns.

3. **Expanded emotion spectrum testing**: Extend the evaluation to include positive emotions and neutral states, providing a complete picture of the approach's effectiveness across the full range of human emotional expression.