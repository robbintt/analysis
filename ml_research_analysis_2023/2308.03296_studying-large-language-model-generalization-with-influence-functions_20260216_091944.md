---
ver: rpa2
title: Studying Large Language Model Generalization with Influence Functions
arxiv_id: '2308.03296'
source_url: https://arxiv.org/abs/2308.03296
tags:
- influence
- sequences
- influential
- query
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper scales influence functions to large language models
  using Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) and
  query batching. EK-FAC approximates the inverse-Hessian-vector product (IHVP) efficiently,
  while query batching shares the cost of training gradient computation between many
  queries.
---

# Studying Large Language Model Generalization with Influence Functions

## Quick Facts
- arXiv ID: 2308.03296
- Source URL: https://arxiv.org/abs/2308.03296
- Reference count: 21
- Key outcome: Scales influence functions to large language models using EK-FAC and query batching, revealing sparse influence patterns, abstraction with scale, cross-lingual generalization, and sensitivity to word ordering

## Executive Summary
This paper presents a method to scale influence functions to large language models by using Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) and query batching. The approach efficiently computes inverse-Hessian-vector products needed for influence estimation, enabling analysis of generalization patterns in models up to 52 billion parameters. The authors validate their method against proximal Bregman response functions and uncover several intriguing properties of LLM generalization, including sparse influence patterns, increasing abstraction with scale, cross-lingual generalization, and surprising sensitivity to word ordering.

## Method Summary
The method uses EK-FAC to approximate the inverse-Hessian-vector product (IHVP) efficiently, leveraging Kronecker factorization of the Gauss-Newton Hessian. Query batching allows sharing the cost of training gradient computation between multiple influence queries by storing low-rank approximations of preconditioned query gradients. The approach is validated on small-scale datasets and applied to analyze generalization patterns in large language models, with layerwise and tokenwise attribution capabilities.

## Key Results
- EK-FAC achieves similar accuracy to traditional influence function estimators while being orders of magnitude faster for IHVP computation
- Influence patterns in LLMs are sparse, with increasing abstraction as model scale increases
- Models show cross-lingual generalization capabilities and surprising sensitivity to word ordering in relational statements and translations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EK-FAC achieves similar accuracy to traditional iterative methods while being orders of magnitude faster for IHVP computation.
- Mechanism: EK-FAC approximates the inverse-Hessian-vector product (IHVP) using a Kronecker-factored approximation to the Gauss-Newton Hessian, which allows efficient inversion through eigendecomposition and Kronecker product identities.
- Core assumption: The Gauss-Newton Hessian (GNH) provides a sufficiently accurate approximation to the true Hessian for influence function estimation in large language models.
- Evidence anchors:
  - [abstract]: "EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster."
  - [section 2.2.3]: "EK-FAC achieves similar accuracy to LiSSA, despite being orders of magnitude faster when computing influences over several measurements."
- Break Condition: If the GNH approximation introduces significant bias that affects the quality of influence estimates, or if the Kronecker factorization assumptions break down for the transformer architecture.

### Mechanism 2
- Claim: Query batching allows sharing the cost of training gradient computation between multiple influence queries.
- Mechanism: By storing low-rank approximations of preconditioned query gradients in memory, the expensive training gradient computation can be shared across multiple queries, reducing overall computational cost.
- Core assumption: Preconditioned query gradients can be effectively compressed to low rank without significant loss of information for influence estimation.
- Evidence anchors:
  - [section 3.2.2]: "By storing low-rank approximations of the preconditioned query gradients, we can easily store hundreds of them in memory, allowing us to share the cost of training gradient computation between these queries."
  - [section 3.2.2]: "storing rank-32 approximations results in a negligible error in the final influence estimates."
- Break Condition: If the low-rank approximation fails to capture essential information for accurate influence estimation, or if memory constraints prevent storing sufficient query gradients.

### Mechanism 3
- Claim: The independence assumption between different parameter matrices in K-FAC/EK-FAC allows clean attribution of influence to specific layers of the network.
- Mechanism: The block-diagonal approximation of the Hessian matrix means influence can be decomposed into contributions from individual layers, revealing where knowledge is stored in the network.
- Core assumption: Different layers of the transformer MLP contribute independently to the overall influence, allowing meaningful layerwise decomposition.
- Evidence anchors:
  - [section 3.3]: "The K-FAC approximation makes an independence assumption between different parameter matrices, resulting in a block-diagonal approximation to G. This cloud has a silver lining: the influence of a data point can be cleanly attributed to specific layers."
  - [section 5.3.2]: "The EK-FAC approximation not only gives a scalar influence estimate but also attributes the influence to specific layers, as detailed in Section 3.3."
- Break Condition: If significant interactions between layers exist that the block-diagonal approximation misses, or if the layerwise decomposition becomes meaningless for certain types of influence.

## Foundational Learning

- Concept: Inverse-Hessian-Vector Product (IHVP) computation
  - Why needed here: IHVP is the computational bottleneck in influence function estimation, requiring approximation for large models.
  - Quick check question: What makes IHVP computationally expensive for large models, and why can't we compute it exactly?

- Concept: Kronecker-Factored Approximate Curvature (K-FAC)
  - Why needed here: K-FAC provides an efficient way to approximate the Fisher information matrix (equivalent to GNH for softmax outputs) with support for efficient inversion.
  - Quick check question: How does K-FAC exploit the structure of neural network parameters to enable efficient IHVP computation?

- Concept: Autoregressive cross-entropy loss as a matching loss function
  - Why needed here: This property ensures that the Fisher information matrix equals the Gauss-Newton Hessian, which is necessary for applying K-FAC to influence functions.
  - Quick check question: Why is it important that the final layer uses softmax with autoregressive cross-entropy loss for applying K-FAC?

## Architecture Onboarding

- Component map: EK-FAC module -> Query batching system -> Layerwise attribution module -> Tokenwise attribution module

- Critical path:
  1. Fit EK-FAC approximation to Gauss-Newton Hessian (done once per model)
  2. For each query, compute preconditioned query gradient and IHVP using EK-FAC
  3. Compute training gradients for candidate sequences
  4. Calculate influence scores by dotting IHVP with training gradients
  5. Optionally attribute influence to layers/tokens

- Design tradeoffs:
  - EK-FAC vs iterative methods: EK-FAC trades some approximation accuracy for massive speedup in IHVP computation
  - Full vs block-diagonal approximation: Additional block-diagonal approximation reduces memory overhead at the cost of some accuracy
  - Unfiltered vs TF-IDF filtered search: Unfiltered search avoids bias but requires more computation; TF-IDF filtering reduces computation but introduces bias

- Failure signatures:
  - Poor correlation with PBRF: Indicates EK-FAC approximation is not accurate enough
  - Spurious sequences with sparse tokenwise influence: May indicate algorithmic errors or genuine but hard-to-interpret patterns
  - Computational bottlenecks in training gradient computation: May require more aggressive query batching or alternative search strategies

- First 3 experiments:
  1. Validate EK-FAC accuracy on small-scale datasets (UCI, MNIST, CIFAR) by comparing correlation with PBRF
  2. Test query batching effectiveness by comparing influence estimates with and without batching
  3. Verify layerwise attribution by checking if influences are spread evenly across layers on average

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do influence functions perform when applied to the combination of pretraining and fine-tuning, rather than just pretrained models?
- Basis in paper: The authors explicitly state that extending influence functions to analyze the combination of pretraining and fine-tuning is an important avenue to explore.
- Why unresolved: The paper focuses on analyzing pretrained models and acknowledges that fine-tuning is conceptually more challenging to analyze with influence-function-like techniques due to the implicit bias of the optimizer.
- What evidence would resolve it: Experiments applying influence functions to models that have undergone both pretraining and fine-tuning, comparing the results to models with only pretraining, and analyzing the impact of different fine-tuning techniques on the influence patterns.

### Open Question 2
- Question: What is the role of nonlinear training phenomena, such as the formation of complex circuits or global rearrangements of a model's representation, in the generalization patterns observed using influence functions?
- Basis in paper: The authors note that influence functions are approximating the sensitivity to the training set locally around the final weights and might not capture nonlinear training phenomena.
- Why unresolved: The paper uses influence functions to study generalization patterns, but acknowledges that the IHVP formulation is inherently incapable of modeling nonlinear coordination of multiple parameter matrices.
- What evidence would resolve it: Experiments comparing the influence patterns observed using influence functions to those observed using techniques that can capture nonlinear training phenomena, such as analyzing the evolution of specific circuits or representations during training.

### Open Question 3
- Question: How does the word ordering sensitivity observed in the influence patterns generalize to more complex linguistic structures and longer sequences?
- Basis in paper: The authors observe a surprising sensitivity of the influence patterns to the ordering of words in simple relational statements and translation tasks.
- Why unresolved: The experiments on word ordering sensitivity are limited to simple relational statements and translation tasks, and it is unclear how this sensitivity generalizes to more complex linguistic structures and longer sequences.
- What evidence would resolve it: Experiments systematically manipulating the word order in more complex sentences and longer passages, measuring the influence patterns using influence functions, and analyzing the impact of different word ordering patterns on the model's ability to generalize and generate coherent text.

## Limitations

- EK-FAC approximation accuracy has only been validated against PBRF on limited scenarios and may not generalize to all model architectures and tasks
- Query batching effectiveness relies on low-rank approximations that lack theoretical justification for negligible error across all sequence lengths and complexities
- Findings about LLM generalization patterns are based on a limited set of queries and model sizes, limiting confidence in their generalizability

## Confidence

- High Confidence: The core mechanism of EK-FAC for approximating IHVP and the query batching approach for sharing gradient computation costs are technically sound and well-justified.
- Medium Confidence: The validation of EK-FAC accuracy against PBRF and the empirical findings about LLM generalization patterns, which could benefit from broader testing and theoretical analysis.
- Low Confidence: The claim that EK-FAC provides sufficient accuracy for all practical purposes in influence function estimation, requiring more extensive validation across different model architectures, tasks, and types of influence queries.

## Next Checks

1. Cross-Architecture Validation: Test EK-FAC accuracy against PBRF on a diverse set of transformer architectures (different attention mechanisms, MLP sizes, etc.) and tasks beyond language modeling to verify generalizability.

2. Query Batching Robustness: Systematically evaluate the impact of different rank approximations (beyond rank-32) on influence estimate accuracy, particularly for longer sequences and more complex queries, to establish error bounds.

3. Temporal Stability Analysis: Examine how influence patterns change over time during training and whether the observed generalization patterns (sparsity, abstraction, etc.) remain consistent across different training stages and checkpoints.