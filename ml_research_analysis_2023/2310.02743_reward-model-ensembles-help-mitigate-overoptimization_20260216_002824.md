---
ver: rpa2
title: Reward Model Ensembles Help Mitigate Overoptimization
arxiv_id: '2310.02743'
source_url: https://arxiv.org/abs/2310.02743
tags:
- reward
- optimization
- overoptimization
- arxiv
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates whether ensemble-based conservative
  optimization can mitigate overoptimization in reinforcement learning from human
  feedback (RLHF). The authors use worst-case optimization (WCO) and uncertainty-weighted
  optimization (UWO) with ensembles of reward models during policy optimization.
---

# Reward Model Ensembles Help Mitigate Overoptimization

## Quick Facts
- arXiv ID: 2310.02743
- Source URL: https://arxiv.org/abs/2310.02743
- Reference count: 32
- Primary result: Ensemble-based conservative optimization practically eliminates overoptimization in RLHF and improves performance by up to 70% for best-of-n sampling.

## Executive Summary
This paper systematically investigates whether ensemble-based conservative optimization can mitigate reward model overoptimization in reinforcement learning from human feedback (RLHF). The authors use worst-case optimization (WCO) and uncertainty-weighted optimization (UWO) with ensembles of reward models during policy optimization. In synthetic setups with and without 25% label noise, ensemble-based conservative optimization practically eliminates overoptimization and improves performance by up to 70% for best-of-n sampling. For proximal policy optimization (PPO), these methods always reduce overoptimization and outperform single reward model optimization. When combined with a small KL penalty, overoptimization is prevented at no performance cost.

## Method Summary
The authors train proxy reward models on preference data using supervised fine-tuning, then create ensembles by training multiple models with different random seeds. They implement three optimization objectives: mean optimization, worst-case optimization (WCO), and uncertainty-weighted optimization (UWO). Policy optimization is performed using both best-of-n sampling and PPO with optional KL penalties. The synthetic setup uses a gold reward model to generate preference labels, with experiments conducted on the Alpaca dataset with 25% label noise added.

## Key Results
- Ensemble-based conservative optimization eliminates overoptimization in synthetic RLHF setups
- WCO and UWO improve performance by up to 70% for best-of-n sampling compared to single reward model optimization
- For PPO, ensemble methods reduce overoptimization while outperforming single reward model optimization
- Combining ensemble-based methods with a small KL penalty prevents overoptimization at no performance cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WCO prevents overoptimization by selecting the lowest reward from the ensemble, ensuring no single faulty model can drive the policy into harmful regions.
- Core assumption: Ensemble members have sufficiently diverse errors so that at least one will give a conservative estimate in any given region.
- Evidence anchors: [abstract] "We find that conservative optimization practically eliminates overoptimization"; [section 3] "Choosing the lowest reward from the ensemble helps ensure that as long as at least one ensemble member does not overestimate the true reward, policy optimization will not result in overoptimization."
- Break condition: If all ensemble members systematically overestimate the reward in some region (correlated errors), WCO will still drive overoptimization.

### Mechanism 2
- Claim: UWO penalizes high-uncertainty predictions by combining mean reward with intra-ensemble variance, discouraging exploitation of unreliable reward estimates.
- Core assumption: Ensemble variance is a meaningful proxy for epistemic uncertainty in the reward model.
- Evidence anchors: [abstract] "uncertainty-weighted optimization (UWO)... results in improved performance by up to 70% for BoN sampling"; [section 5.5] "The variance among the ensemble members starts at a small value in the no label noise case, and increases by a relatively small amount during training for UWO."
- Break condition: If ensemble variance poorly correlates with true uncertainty (e.g., all models share the same blind spots), UWO will fail to penalize overoptimization.

### Mechanism 3
- Claim: Combining ensemble-based conservative optimization with a small KL penalty prevents overoptimization without sacrificing performance.
- Core assumption: The KL penalty is small enough not to interfere with useful optimization but large enough to prevent extreme deviations.
- Evidence anchors: [abstract] "combining it with a small KL penalty successfully prevents overoptimization at no performance cost"; [section 5.2] "with a small KL penalty coefficient of 0.01, WCO and UWO both successfully prevent overoptimization."
- Break condition: If the KL penalty is too large, it will overly constrain optimization; if too small, overoptimization may still occur.

## Foundational Learning

- Concept: Ensemble learning
  - Why needed here: Multiple reward models provide diverse perspectives, reducing the chance that a single faulty model drives overoptimization.
  - Quick check question: What is the primary benefit of using an ensemble of reward models instead of a single model in RLHF?

- Concept: Uncertainty estimation
  - Why needed here: Estimating uncertainty (via ensemble variance) helps identify unreliable reward predictions and prevents the policy from exploiting them.
  - Quick check question: How does UWO use ensemble variance to prevent overoptimization?

- Concept: Conservative optimization
  - Why needed here: Conservative objectives (like WCO) ensure the policy does not chase potentially inflated rewards from any single model.
  - Quick check question: Why does selecting the minimum reward from an ensemble help prevent overoptimization?

## Architecture Onboarding

- Component map:
  Pretrained policy model (1.4B Pythia) -> Multiple reward models (7M, 44M, 1.3B Pythia variants with scalar heads) -> Ensemble creation (train k models with different seeds) -> Conservative optimization wrapper (WCO, UWO, or mean) -> Policy optimizer (BoN or PPO with optional KL penalty)

- Critical path:
  1. Train proxy reward models on preference data
  2. Create ensemble by training k models with different seeds
  3. During policy optimization, apply ensemble-based conservative objective
  4. If using PPO, add KL penalty term
  5. Evaluate final policy with gold reward model

- Design tradeoffs:
  - Ensemble size vs. training cost: Larger ensembles provide more diversity but require more compute
  - Conservative vs. performance: WCO is safest but may be overly conservative; UWO balances safety and performance
  - KL penalty weight: Too small → overoptimization persists; too large → performance loss

- Failure signatures:
  - Overoptimization persists despite ensemble: Likely correlated errors across ensemble members
  - Performance significantly worse than single model: Conservative objective may be too aggressive
  - High variance in final results: Ensemble diversity insufficient or optimization unstable

- First 3 experiments:
  1. Train 3-5 reward models with different seeds on the same data; verify they produce different predictions
  2. Implement WCO and test on a simple synthetic setup (e.g., Gao et al. setup) to confirm overoptimization is prevented
  3. Compare WCO, UWO, and mean optimization on the same setup to measure performance vs. safety tradeoffs

## Open Questions the Paper Calls Out

- How do ensemble-based conservative optimization methods compare to other techniques for mitigating overoptimization, such as increasing the size of the proxy reward model or using different optimization algorithms?
- How do ensemble-based conservative optimization methods perform in online RLHF settings, where reward models are periodically retrained on freshly collected data from humans?
- What is the impact of different ensemble sizes on the performance of ensemble-based conservative optimization methods, and is there a point of diminishing returns?

## Limitations

- The paper uses synthetic human feedback data rather than actual human preferences, raising questions about generalizability to real-world settings.
- Ensemble variance is assumed to be a meaningful proxy for epistemic uncertainty but is not externally validated against other uncertainty quantification methods.
- The effectiveness of WCO depends critically on ensemble diversity, which is not systematically analyzed in terms of correlation between reward model predictions.

## Confidence

**High confidence**: The core finding that ensemble-based conservative optimization reduces overoptimization in synthetic RLHF setups, supported by systematic experiments comparing multiple methods.

**Medium confidence**: The mechanism claims about WCO and UWO preventing overoptimization, as these are primarily justified by the paper's own results rather than external validation or theoretical guarantees.

**Low confidence**: The generalizability of these results to real human feedback data, since the paper relies on synthetic preference labels generated by a gold reward model rather than actual human preferences.

## Next Checks

1. **Ensemble diversity validation**: Measure correlation between reward model predictions across the ensemble to verify that members have sufficiently diverse errors. If correlations are high, overoptimization may persist despite ensemble methods.

2. **Real human feedback testing**: Apply the ensemble-based methods to an RLHF setup using actual human preference data (not synthetic) to test whether the overoptimization mitigation generalizes beyond the synthetic setting.

3. **Uncertainty estimation validation**: Compare ensemble variance-based uncertainty estimates against other uncertainty quantification methods (e.g., dropout-based uncertainty) to verify that variance is a meaningful proxy for epistemic uncertainty in this context.