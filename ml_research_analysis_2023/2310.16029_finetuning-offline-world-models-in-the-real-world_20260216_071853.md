---
ver: rpa2
title: Finetuning Offline World Models in the Real World
arxiv_id: '2310.16029'
source_url: https://arxiv.org/abs/2310.16029
tags:
- offline
- tasks
- learning
- data
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for pretraining and fine-tuning world
  models on offline and online data, respectively, without relying on simulators.
  The key idea is to regularize planning based on model uncertainty during fine-tuning
  to mitigate extrapolation errors.
---

# Finetuning Offline World Models in the Real World

## Quick Facts
- arXiv ID: 2310.16029
- Source URL: https://arxiv.org/abs/2310.16029
- Reference count: 40
- One-line primary result: Improves offline world model success rate from 22% to 67% in 20 trials for real-world visual pick task with unseen distractors

## Executive Summary
This paper addresses the challenge of adapting world models pretrained on offline data for real-world robot control tasks with limited online interaction. The authors propose a method that combines in-sample TD-learning during pretraining with test-time uncertainty regularization during planning, enabling efficient few-shot fine-tuning on both seen and unseen tasks. Experiments on real robot (xArm) and simulation (D4RL, quadruped locomotion) tasks demonstrate significant improvements over state-of-the-art offline and online RL methods, with the method achieving high success rates in as few as 20 trials while avoiding catastrophic forgetting of the pretrained knowledge.

## Method Summary
The method involves pretraining a TD-MPC world model on offline data using in-sample Q-learning with expectile regression, then fine-tuning with balanced sampling from offline and online replay buffers. During planning, the algorithm estimates returns for action sequences while penalizing high-uncertainty actions using standard deviation across a Q-ensemble. This uncertainty-aware planning helps mitigate extrapolation errors when the model encounters out-of-distribution states during online fine-tuning. The approach maintains two replay buffers (offline and online) and samples mini-batches in equal parts to facilitate rapid propagation of information from new interactions while preserving stability from the offline dataset.

## Key Results
- Improves success rate of offline world model from 22% to 67% in 20 trials for real-world visual pick task with unseen distractors
- Outperforms state-of-the-art offline and online RL methods across most tasks in both real robot (xArm) and simulation (D4RL, quadruped locomotion) environments
- Enables few-shot fine-tuning to seen and unseen tasks even with limited offline data (Reach: 120 trajectories, Pick: 200 trajectories, Kitchen: 216 trajectories)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regularizing planning with model uncertainty reduces extrapolation errors during online fine-tuning.
- Mechanism: The planner estimates returns for action sequences, but if those sequences involve unseen state-action pairs, the model's predictions become unreliable. By penalizing high-uncertainty actions (using standard deviation across Q-ensembles), the planner avoids exploiting model errors and instead explores cautiously. This allows the model to self-correct as new data arrives.
- Core assumption: Model uncertainty (epistemic) can be reliably estimated via Q-ensembles and correlates with extrapolation risk.
- Evidence anchors: [abstract] "...we propose to regularize the planner at test-time by balancing estimated returns and (epistemic) model uncertainty." [section 3.2] "we propose a framework for offline-to-online finetuning of world models that mitigates extrapolation errors in the model via novel test-time regularization during planning."
- Break condition: If uncertainty estimation is inaccurate (e.g., Q-ensembles poorly calibrated), regularization could suppress exploration too much or fail to penalize bad actions.

### Mechanism 2
- Claim: In-sample TD-targets reduce value overestimation during offline training.
- Mechanism: Standard TD-learning uses max over actions in the target, but in offline RL those actions may be out-of-distribution. By restricting TD-backups to in-sample actions (using V-network), the method avoids propagating overestimated values from unseen actions.
- Core assumption: The offline dataset contains sufficient in-distribution actions to approximate the true max Q-value.
- Evidence anchors: [section 3.1] "Inspired by Implicit Q-learning (IQL; [19]), we choose to mitigate the overestimation issue by applying TD-backups only on in-sample actions." [section 3.1] "This estimator can be optimized by an asymmetric ℓ2-loss (expectile regression)."
- Break condition: If the offline dataset is too small or narrow, the in-sample max may poorly approximate the true max, leading to underfitting.

### Mechanism 3
- Claim: Balanced sampling between offline and online data accelerates online fine-tuning.
- Mechanism: Early in online fine-tuning, the replay buffer contains mostly offline data. By sampling equally from offline and online buffers, the algorithm oversamples new interactions, allowing faster propagation of learned information into the model.
- Core assumption: Equal sampling weights preserve learning signal from new data while maintaining stability from offline data.
- Evidence anchors: [section 3.2] "To facilitate rapid propagation of information acquired during finetuning, we maintain two replay buffers, Boff and Bon for offline and online data, respectively, and optimize the objective in Equation 1 on mini-batches of data sampled in equal parts from Boff and Bon." [section 3.2] "Balanced sampling has been explored in various settings [33, 34, 35, 22, 36, 37], and we find that it consistently improves finetuning of world models as well."
- Break condition: If online data quality is low or insufficient, oversampling may destabilize learning.

## Foundational Learning

- Concept: Partial observability in RL (POMDPs)
  - Why needed here: The paper explicitly frames the problem as a POMDP and uses latent state representations (hθ(s)) to handle unobservable states.
  - Quick check question: Why can't we use raw observations directly in the dynamics model without encoding to latent states?

- Concept: Model-based RL and planning
  - Why needed here: The method builds a world model and uses planning (MPC) for action selection rather than direct policy learning.
  - Quick check question: How does planning differ from learning a parametric policy in terms of test-time behavior?

- Concept: Epistemic vs aleatoric uncertainty
  - Why needed here: The paper uses epistemic uncertainty (model uncertainty) from Q-ensembles to regularize planning, not aleatoric (inherent stochasticity).
  - Quick check question: Why is epistemic uncertainty more relevant than aleatoric uncertainty for extrapolation error mitigation?

## Architecture Onboarding

- Component map: Encoder hθ(s) → Latent state z → Dynamics model dθ(z,a) → Next latent z' → Reward predictor Rθ(z,a) → Value function Qθ(z,a) + Vθ(z) → Policy guide πθ(z) → Q-ensemble {Q(i)θ} → Planning module (MPPI) with uncertainty penalty

- Critical path:
  1. Pretrain all components on Boff using in-sample TD-targets.
  2. During online fine-tuning, sample minibatches equally from Boff and Bon.
  3. Use Q-ensemble to estimate uncertainty during planning.
  4. Update all components jointly with balanced sampling.

- Design tradeoffs:
  - Q-ensemble size: 5 chosen empirically; larger ensembles may better estimate uncertainty but increase compute.
  - λ (uncertainty coefficient): Same for offline and online; could be tuned separately.
  - Balanced sampling: Oversamples online data early; could use scheduling instead.

- Failure signatures:
  - If pretraining data too narrow → poor zero-shot performance, slow online adaptation.
  - If λ too high → planner becomes overly conservative, fails to explore.
  - If λ too low → no benefit from uncertainty regularization.
  - If replay buffers not balanced → slow propagation of online learning.

- First 3 experiments:
  1. Ablation: Remove uncertainty penalty (λ=0) and compare to full method on xArm tasks.
  2. Ablation: Use single Q-network instead of ensemble; measure impact on fine-tuning speed.
  3. Ablation: Remove balanced sampling; compare online learning curves with/without.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed uncertainty regularization technique scale to more complex and diverse tasks, especially those requiring long-term planning?
- Basis in paper: [inferred] The paper focuses on short-horizon tasks and does not extensively evaluate the method on long-term planning tasks or highly complex environments.
- Why unresolved: The current experiments are limited to specific tasks and do not cover a wide range of scenarios that require extensive planning or involve highly complex dynamics.
- What evidence would resolve it: Extensive evaluation on a broader set of tasks, including those requiring long-term planning and complex dynamics, would provide evidence of the technique's scalability and robustness.

### Open Question 2
- Question: What are the theoretical guarantees of the uncertainty regularization technique in terms of convergence and performance bounds?
- Basis in paper: [explicit] The paper discusses the empirical effectiveness of the technique but does not provide theoretical analysis or convergence guarantees.
- Why unresolved: Theoretical analysis is not provided, leaving questions about the conditions under which the technique is guaranteed to converge and how its performance compares to other methods theoretically.
- What evidence would resolve it: A theoretical analysis demonstrating convergence properties and performance bounds under various conditions would provide a clearer understanding of the technique's reliability and limitations.

### Open Question 3
- Question: How sensitive is the method to the choice of hyperparameters, particularly the uncertainty coefficient λ, across different tasks and environments?
- Basis in paper: [explicit] The paper mentions that the optimal value of λ can differ between tasks and between offline and online RL, but does not extensively explore the sensitivity to this hyperparameter.
- Why unresolved: The paper does not provide a comprehensive analysis of how different values of λ affect performance across a wide range of tasks and environments.
- What evidence would resolve it: A detailed sensitivity analysis showing the impact of different λ values on performance across various tasks and environments would help understand the robustness of the method to hyperparameter choices.

## Limitations
- The uncertainty regularization mechanism relies on the assumption that Q-ensemble standard deviation accurately captures epistemic uncertainty, which is not directly validated
- Most tasks use limited numbers of trials (≤20), making it unclear how the method scales to more complex tasks requiring longer exploration
- The method demonstrates success on three real robot tasks but doesn't explore generalization across different robot morphologies or sensor modalities

## Confidence
- **Medium** for core claims of improved fine-tuning efficiency and generalization from offline pretraining
- **Low** for claims about real-world applicability beyond the specific xArm tasks tested

## Next Checks
1. Design an experiment to test whether Q-ensemble standard deviation correlates with actual model error on held-out state-action pairs to validate the core assumption behind uncertainty regularization.
2. Systematically vary the balanced sampling ratio (0.1:0.9, 0.3:0.7, 0.7:0.3, 0.9:0.1) during fine-tuning and measure impact on learning speed and final performance across multiple tasks to determine optimal sampling schedules.
3. Test the method on tasks requiring 50-100+ trials to succeed, comparing against online RL baselines to determine if the computational overhead of uncertainty estimation remains justified as task complexity increases.