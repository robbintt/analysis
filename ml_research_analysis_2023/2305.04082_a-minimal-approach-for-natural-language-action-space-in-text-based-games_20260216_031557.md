---
ver: rpa2
title: A Minimal Approach for Natural Language Action Space in Text-based Games
arxiv_id: '2305.04082'
source_url: https://arxiv.org/abs/2305.04082
tags:
- action
- state
- game
- actions
- admissible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a minimal approach to explore the action space
  in text-based games without relying on language models or knowledge graphs. The
  method, called $\epsilon$-admissible exploration, uses admissible actions during
  training to acquire diverse and useful data from the environment.
---

# A Minimal Approach for Natural Language Action Space in Text-based Games

## Quick Facts
- **arXiv ID**: 2305.04082
- **Source URL**: https://arxiv.org/abs/2305.04082
- **Reference count**: 28
- **Primary result**: A minimal TAC agent with epsilon-admissible exploration outperforms state-of-the-art agents that use language models and knowledge graphs across 10 Jericho games.

## Executive Summary
This paper introduces a minimal approach to exploring action spaces in text-based games without relying on language models or knowledge graphs. The proposed method, epsilon-admissible exploration, leverages environment-provided admissible actions during training to acquire diverse and useful experiences. The text-based actor-critic (TAC) agent learns the policy distribution solely from game observations using a simple template-object decoder architecture. On average across 10 games from Jericho, TAC with epsilon-admissible exploration achieves state-of-the-art performance, demonstrating that a lighter model design with a fresh perspective on utilizing information within the environments suffices for effective exploration of exponentially large action spaces.

## Method Summary
The approach uses a text-based actor-critic (TAC) agent that generates natural language actions from game observations without requiring language models or knowledge graphs. The agent employs epsilon-admissible exploration during training, which samples admissible actions with probability epsilon to acquire diverse experiences. The TAC architecture consists of text and state encoders, an actor network with template and object decoders, and a critic network with state and state-action value critics. The agent is trained using reinforcement learning with an additional supervised learning loss that acts as a regularization term, guiding behavior when reward signals are not available.

## Key Results
- TAC with epsilon-admissible exploration outperforms state-of-the-art agents that use language models and knowledge graphs on average across 10 Jericho games
- The minimal architecture (TAC) achieves competitive performance without the need for complex components like knowledge graphs or large language models
- Epsilon-admissible exploration with values around 0.3 provides optimal balance between exploration and exploitation across different games

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The agent benefits from random sampling of admissible actions during training to acquire diverse and useful data from the environment.
- Mechanism: During training, the agent samples the next action from the admissible action set with a probability threshold ε, allowing it to explore different states and collect experiences that are more likely to be useful for learning.
- Core assumption: Admissible actions are a compact set of more probable actions that can guide the agent's exploration and lead to more diverse experiences.
- Evidence anchors:
  - [abstract]: "Our proposed training strategy, ϵ-admissible exploration, leverages the admissible actions via random sampling during training to acquire diverse and useful data from the environment."
  - [section 4]: "We use a simple exploration technique during training, which samples the next action from admissible actions with ε probability threshold."
  - [corpus]: Weak evidence. The corpus contains related papers but does not directly address the mechanism of using admissible actions for exploration.
- Break condition: If the admissible action set is not provided or is too large, the mechanism may not work effectively.

### Mechanism 2
- Claim: The supervised learning (SL) loss acts as a regularization term, inducing a more uniformly distributed policy.
- Mechanism: The SL loss encourages the agent to produce valid templates and objects, guiding its behavior when reward signals are not available. When reward signals are present, the SL loss serves as a regularization term, preventing the agent from over-fitting to specific states.
- Core assumption: The SL loss towards admissible actions acts as a regularization term, similar to entropy regularization.
- Evidence anchors:
  - [section 5.2]: "According to the Figure 3, removing SL negatively affects the game score... This is consistent with the earlier observations... reporting that KG-A2C without SL achieves no game score in ZORK 1."
  - [section 6]: "In this sense, SL could be considered as the means to introduce the effects similar to entropy regularization in Ammanabrolu and Hausknecht (2020)."
  - [corpus]: Weak evidence. The corpus does not directly address the regularization effect of the SL loss.
- Break condition: If the SL loss is too strong, it may hinder the agent's ability to learn from reward signals.

### Mechanism 3
- Claim: The text-based actor-critic (TAC) agent learns the policy distribution solely from game observations, without relying on language models or knowledge graphs.
- Mechanism: The TAC agent uses a text encoder to process game observations and a template-object decoder to generate natural language actions. The agent's policy is learned through reinforcement learning, with the help of a state critic and a state-action critic.
- Core assumption: The TAC agent can effectively learn the policy distribution from game observations, without the need for additional modules like language models or knowledge graphs.
- Evidence anchors:
  - [abstract]: "Additionally, we present a text-based actor-critic (TAC) agent that produces textual commands for game, solely from game observations, without requiring any KG or LM."
  - [section 4]: "Our agent, Text-based Actor-Critic (TAC), follows the Actor-Critic method with template-object decoder. We provide an overview of the system in Figure 1 and a detailed description in below."
  - [corpus]: Weak evidence. The corpus contains related papers but does not directly address the mechanism of the TAC agent.
- Break condition: If the game observations are not informative enough, the TAC agent may struggle to learn an effective policy.

## Foundational Learning

- Concept: Reinforcement Learning (RL)
  - Why needed here: The TAC agent is trained using RL to maximize the expected discounted sum of rewards in the text-based game environment.
  - Quick check question: What is the difference between on-policy and off-policy RL algorithms?

- Concept: Actor-Critic Methods
  - Why needed here: The TAC agent is based on the Actor-Critic method, which combines the benefits of policy gradient and value function estimation.
  - Quick check question: How does the Actor-Critic method differ from Q-learning or policy gradient methods?

- Concept: Supervised Learning (SL)
  - Why needed here: The TAC agent uses SL to guide its behavior when reward signals are not available, acting as a regularization term when reward signals are present.
  - Quick check question: What is the role of the SL loss in the TAC agent's training process?

## Architecture Onboarding

- Component map:
  Text encoder -> State encoder -> Actor network (template and object decoders) -> Critic network (state and state-action value critics) -> Prioritized experience replay buffer

- Critical path:
  1. Encode game observations using the text encoder.
  2. Generate action distribution using the template-object decoder.
  3. Sample action from the distribution.
  4. Execute action in the environment and receive reward.
  5. Store experience in the replay buffer.
  6. Sample experiences from the replay buffer and update the agent's parameters.

- Design tradeoffs:
  - Using a single shared text encoder for all input text vs. separate encoders for different input types.
  - Including game score in the state representation vs. excluding it.
  - Using two state-action critics vs. a single state-action critic.
  - Including target state critic vs. not including it.

- Failure signatures:
  - Overfitting to specific states or actions.
  - Poor exploration of the state space.
  - Unstable learning due to high variance in the updates.

- First 3 experiments:
  1. Train the TAC agent with a fixed ε value (e.g., 0.3) and evaluate its performance on a subset of the games.
  2. Train the TAC agent with different ε values (e.g., 0.0, 0.3, 0.7, 1.0) and compare their performance.
  3. Train the TAC agent with and without the SL loss and compare their performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of TAC with ϵ-admissible exploration degrade in environments where admissible actions are not provided by the environment?
- Basis in paper: [explicit] "Our TAC is not suitable for environments that do not provide admissible action set."
- Why unresolved: The paper does not provide empirical evidence on TAC's performance in such environments.
- What evidence would resolve it: Experimental results comparing TAC's performance with and without provided admissible actions in various environments.

### Open Question 2
- Question: How does the choice of the hyper-parameter a_ϵ in the adaptive epsilon scheduler affect the learning performance and stability of TAC?
- Basis in paper: [explicit] "Too high a_ϵ makes the slope more steep" and "Too low a_ϵ results in more unstable learning."
- Why unresolved: The paper only provides qualitative observations without quantitative analysis of different a_ϵ values.
- What evidence would resolve it: A comprehensive ablation study varying a_ϵ while keeping other parameters constant, measuring performance and stability.

### Open Question 3
- Question: What is the relationship between the strength of supervised signals (λT and λO) and the extent of overfitting in TAC?
- Basis in paper: [explicit] "Higher λT and λO relaxes overfitting, reaching the score from 7.7 to 15.8 in LUDICORP and from 5.8 to 8.0 in TEMPLE."
- Why unresolved: The paper does not provide a detailed analysis of how different λT and λO values affect overfitting across multiple games.
- What evidence would resolve it: A systematic study varying λT and λO values, measuring performance and overfitting indicators across a diverse set of games.

## Limitations
- The method's reliance on environment-provided admissible actions limits generalizability to games where such actions are not available
- The paper does not provide extensive hyperparameter sensitivity analysis
- The method's performance on games outside the Jericho suite remains untested

## Confidence
- **High confidence**: The core mechanism of epsilon-admissible exploration is well-supported by ablation results showing consistent improvements across multiple games
- **Medium confidence**: The claim that minimal architecture suffices for effective exploration is supported empirically but lacks theoretical grounding for why this works across diverse game environments
- **Medium confidence**: The regularization effect of the supervised learning loss is demonstrated through ablation studies but the relationship to entropy regularization is primarily conceptual

## Next Checks
1. Test the TAC agent on text-based games that do not provide admissible actions to assess robustness
2. Conduct a systematic hyperparameter sensitivity analysis, particularly for epsilon values and learning rates
3. Compare the method against more recent approaches that combine minimal architectures with few-shot prompting from large language models