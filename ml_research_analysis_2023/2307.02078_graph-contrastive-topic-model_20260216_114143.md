---
ver: rpa2
title: Graph Contrastive Topic Model
arxiv_id: '2307.02078'
source_url: https://arxiv.org/abs/2307.02078
tags:
- topic
- negative
- graph
- contrastive
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the sample bias problem in contrastive neural
  topic models caused by word frequency-based negative sampling. Existing methods
  generate negative samples by removing high-frequency words, but these can still
  be semantically similar to the prototype.
---

# Graph Contrastive Topic Model

## Quick Facts
- arXiv ID: 2307.02078
- Source URL: https://arxiv.org/abs/2307.02078
- Reference count: 40
- Key outcome: Proposes a graph-based sampling strategy to address sample bias in contrastive neural topic models, achieving state-of-the-art topic coherence and document representation learning

## Executive Summary
This paper addresses the sample bias problem in contrastive neural topic models caused by word frequency-based negative sampling. The authors propose a new sampling assumption: negative samples should contain words semantically uncorrelated with the prototype document. Based on this, they introduce the Graph Contrastive Topic Model (GCTM), which uses graph-based sampling strategy to generate instructive positive and negative samples. The method models documents as bipartite graphs, builds word co-occurrence graphs, and uses GCNs to capture multi-hop semantic correlations. GCTM then performs document-word information propagation to generate augmented samples for contrastive learning.

## Method Summary
GCTM addresses sample bias by replacing word frequency-based negative sampling with a graph-based approach. The method models documents as document-word bipartite graphs (DWBG) using TF-IDF features and SBERT embeddings. It builds positive and negative word co-occurrence graphs (WCGs) based on NPMI scores, then encodes these with GCN layers to capture multi-hop semantic correlation and irrelevance. The document-word information propagation (DWIP) module generates augmented samples by identifying words that are semantically correlated or uncorrelated with the prototype. These samples are then used in a contrastive learning framework with an NTM encoder to produce topic representations, optimizing both NTM loss and contrastive loss.

## Key Results
- GCTM achieves state-of-the-art topic coherence (NPMI scores) on three benchmark datasets (20NG, IMDB, NIPS)
- The method outperforms existing contrastive neural topic models in document representation learning for text classification tasks
- Experimental results validate the effectiveness of the graph-based sampling strategy in generating more instructive positive and negative samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-based sampling reduces sample bias by replacing word frequency-based negatives with semantically uncorrelated words
- Mechanism: The method models documents as bipartite graphs (DWBG) and builds positive/negative word co-occurrence graphs (WCGs). It then propagates information across these graphs using GCNs to identify words that are directly or distantly correlated/irrelevant to the prototype document
- Core assumption: Two documents have distinct topics when they feature a significant disparity in their semantic content, characterized by words with dissimilar meanings
- Evidence anchors:
  - [abstract] "We propose a new sampling assumption that negative samples should contain words that are semantically irrelevant to the prototype"
  - [section 3.2.3] "we formulate the data augmentation of documents as the graph augmentation problem, to generate positive and negative samples by identifying words that are semantically correlated and uncorrelated with the prototype"
  - [corpus] Weak - corpus evidence doesn't directly address this mechanism but provides related context about contrastive learning approaches
- Break condition: If the semantic correlation/irrelevance captured by NPMI and GCN propagation doesn't accurately reflect true topical dissimilarity, the sampling will still produce biased negatives

### Mechanism 2
- Claim: Multi-hop semantic propagation through GCN layers enables identification of both direct and indirect word-document relationships
- Mechanism: GCN layers on positive/negative word graphs create representations that encode not just immediate co-occurrence but higher-order semantic relationships. The document-word information propagation then uses these enriched word representations to generate samples
- Core assumption: Multi-hop graph propagation effectively captures semantic correlation beyond immediate co-occurrence
- Evidence anchors:
  - [section 3.2.3] "The information propagation with L GCN layers can capture the L-order semantic correlations/irrelevances among words in positive and negative word graphs respectively"
  - [section 3.2.2] "We build positive and negative word co-occurrence graphs (WCGs), and encode them with graph neural networks (GNNs) to capture multi-hop semantic correlation and irrelevance among words"
  - [corpus] Weak - corpus provides context about graph neural networks but not specific evidence for multi-hop effectiveness in this sampling context
- Break condition: If GCN layers introduce too much noise through excessive message passing, or if the correlation captured doesn't translate to topical relevance

### Mechanism 3
- Claim: Contrastive loss on graph-augmented samples improves topic modeling by maximizing mutual information between different perspectives
- Mechanism: The method interprets GCL as a structured variational graph auto-encoder that maximizes mutual information between latent topic representations of different document views (original vs. augmented)
- Core assumption: Maximizing mutual information between different views of the same document improves topic modeling quality
- Evidence anchors:
  - [abstract] "We further show that GCL can be interpreted as the structured variational graph auto-encoder which maximizes the mutual information of latent topic representations of different perspectives on DWBG"
  - [section 3.3] "Interestingly, we find that the contrastive loss can be interpreted as the structured variational graph auto-encoder with two latent variables on the input document graph"
  - [corpus] Weak - corpus evidence shows related work on contrastive learning but not specific evidence for this interpretation
- Break condition: If the mutual information maximization doesn't align with the true data distribution or if the augmented views don't provide meaningful additional information

## Foundational Learning

- Concept: Graph neural networks and message passing
  - Why needed here: GCNs are fundamental to encoding word co-occurrence graphs and propagating semantic information across multiple hops
  - Quick check question: How does the number of GCN layers affect the semantic information captured in word representations?

- Concept: Variational auto-encoders and evidence lower bound
  - Why needed here: The NTM component uses VAE framework to approximate the posterior distribution of topic representations
  - Quick check question: What role does the KL divergence term play in the ELBO objective for topic modeling?

- Concept: Contrastive learning and mutual information maximization
  - Why needed here: The contrastive loss guides the model to distinguish between semantically similar and dissimilar document representations
  - Quick check question: How does the temperature parameter in the contrastive loss affect the separation between positive and negative pairs?

## Architecture Onboarding

- Component map:
  - Input layer: TF-IDF features + SBERT embeddings (document-word bipartite graph)
  - GCN encoders: Two separate GCNs for positive and negative word co-occurrence graphs
  - DWIP module: Document-word information propagation to generate augmented samples
  - NTM encoder: Produces latent topic representations
  - Contrastive loss module: Computes loss between original, positive, and negative samples
  - Decoder: Reconstructs input from topic representations

- Critical path: Input → GCN encoding → DWIP → NTM encoding → Contrastive loss → Parameter updates

- Design tradeoffs: 
  - Using GCN layers vs. simpler aggregation methods for capturing multi-hop relationships
  - Balancing the weight of contrastive loss vs. NTM loss (α and γ parameters)
  - Number of GCN layers (1-3) affecting information capture vs. noise introduction

- Failure signatures:
  - NPMI scores not improving despite training - suggests sampling strategy not effective
  - Large gap between training and validation performance - overfitting to augmented samples
  - Negative samples still semantically similar to prototypes despite sampling - graph construction or propagation flawed

- First 3 experiments:
  1. Test GCN layer count (L=1,2,3) on a single dataset with fixed other parameters to find optimal layer number
  2. Compare NPMI with and without contextual embeddings (SBERT) to measure their contribution
  3. Vary negative sampling weight α across {0.5, 1, 2} to find optimal balance in contrastive loss

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important considerations about the general applicability and limitations of the approach.

## Limitations
- The method's effectiveness critically depends on the quality of graph construction and the assumption that NPMI-based word correlation accurately reflects topical relevance
- The GCN-based propagation introduces complexity that may create noise through excessive message passing, particularly when using multiple layers
- The approach may struggle with documents containing words that are semantically related but topically irrelevant, potentially leading to suboptimal negative samples

## Confidence
- **High confidence:** The core mechanism of using graph-based sampling to address word frequency bias in contrastive topic modeling is well-founded, supported by both theoretical framing and experimental results
- **Medium confidence:** The effectiveness of multi-hop GCN propagation for semantic correlation capture is plausible but not definitively proven, with potential for noise introduction
- **Medium confidence:** The interpretation of GCL as structured variational graph auto-encoder maximizing mutual information is theoretically sound but requires empirical validation of its contribution to performance

## Next Checks
1. Conduct ablation study comparing NPMI scores with and without GCN-based multi-hop propagation to isolate the contribution of semantic correlation capture beyond immediate co-occurrence
2. Perform controlled experiments varying the number of GCN layers (L=1,2,3) on a single dataset to identify optimal layer count and quantify noise vs. information trade-off
3. Implement qualitative analysis by examining negative samples to verify they contain semantically uncorrelated words, checking similarity scores between prototype documents and their negative samples