---
ver: rpa2
title: 'DeepliteRT: Computer Vision at the Edge'
arxiv_id: '2309.10878'
source_url: https://arxiv.org/abs/2309.10878
tags:
- ultra
- deeplitert
- low-bit
- quantization
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents DeepliteRT, an end-to-end solution for ultra
  low-bit CNN inference on ARM-based edge devices. The problem addressed is the high
  resource requirements of complex deep learning models for computer vision, making
  them impractical for resource-constrained edge devices.
---

# DeepliteRT: Computer Vision at the Edge

## Quick Facts
- arXiv ID: 2309.10878
- Source URL: https://arxiv.org/abs/2309.10878
- Reference count: 40
- Primary result: Up to 2.20x speedup on ARM edge devices with ultra low-bit CNNs

## Executive Summary
DeepliteRT is an end-to-end solution for deploying ultra low-bit convolutional neural networks (CNNs) on ARM-based edge devices. It addresses the high resource requirements of complex deep learning models by implementing highly optimized ultra low-bit convolution operators and compiler passes that automatically convert fake-quantized models to compact representations. The system achieves significant speedups while maintaining model accuracy through mixed precision support and efficient bit-serial computation.

## Method Summary
DeepliteRT implements a novel bit-serial convolution operator that uses a hybrid unipolar-bipolar scheme with zero-mapping for signed weights, reducing computational overhead. Compiler passes in TVM automatically transform fake-quantized models from various ML frameworks into ultra low-bit representations without requiring changes to training or inference paths. The solution supports mixed precision inference, allowing sensitive layers to remain at higher precision while insensitive layers are reduced to ultra low-bit, minimizing accuracy loss while maximizing performance gains on ARMv7 and ARMv8 targets.

## Key Results
- Up to 2.20x speedup over optimized 32-bit floating-point baselines
- Up to 2.33x speedup over optimized 8-bit integer baselines
- Up to 2.17x speedup over optimized 2-bit baselines
- Maintains model accuracy with minimal drops across classification and detection models

## Why This Works (Mechanism)

### Mechanism 1
The hybrid unipolar-bipolar bit-serial computation method reduces popcount operations from two to one per dot product by mapping signed weight values to include zero, avoiding quantization errors for zero-padding and ReLU operations while maintaining accuracy.

### Mechanism 2
Compiler passes automatically convert fake-quantized models to ultra low-bit representations by transforming nn.conv2d nodes into optimized dlrt_bitserial_conv2d nodes, handling bitpacking during compilation, and managing layout transformations for seamless ARM deployment.

### Mechanism 3
Mixed precision support allows selective quantization of layers based on sensitivity, preserving accuracy by keeping sensitive layers at higher precision (FP32, INT8) while reducing insensitive layers to ultra low-bit, optimizing the accuracy-performance tradeoff.

## Foundational Learning

- **Quantization-aware training (QAT) and post-training quantization (PTQ)**
  - Why needed here: Understanding when quantization is applied is crucial for implementing ultra low-bit quantization effectively
  - Quick check question: What is the main difference between QAT and PTQ in terms of when quantization is applied?

- **Bitpacking and bit-serial computation**
  - Why needed here: These concepts are fundamental to implementing ultra low-bit inference efficiently on hardware without native sub-8-bit support
  - Quick check question: How does bitpacking reduce memory usage in ultra low-bit quantization?

- **ARM architecture and SIMD instructions**
  - Why needed here: Knowledge of ARM architecture and SIMD instructions is essential for optimizing convolution operations for edge devices
  - Quick check question: Why are SIMD instructions important for optimizing convolution operations on ARM CPUs?

## Architecture Onboarding

- **Component map:** Fake-quantized models → DeepliteRT compiler passes → Optimized bit-serial kernels → ARMv7/ARMv8 deployment
- **Critical path:** Model quantization (QAT) → Compiler passes (model transformation) → Optimized kernel execution (inference)
- **Design tradeoffs:** Accuracy vs. performance balancing; complexity vs. portability ensuring effectiveness across ARM platforms
- **Failure signatures:** Significant accuracy loss after quantization; compilation errors in compiler passes; performance degradation from suboptimal kernels
- **First 3 experiments:** 1) Validate quantized model accuracy on validation dataset 2) Measure compilation time and efficiency of compiler passes 3) Benchmark optimized bit-serial convolution performance against baselines on target ARM devices

## Open Questions the Paper Calls Out

### Open Question 1
How does DeepliteRT's performance compare to other ultra low-bit quantization methods on non-ARM CPU architectures (e.g., Intel x86)?
- Basis in paper: [explicit] Focuses on ARM-based platforms without comparisons to other CPU architectures
- Why unresolved: Paper does not explore performance on non-ARM CPUs, limiting generalizability
- What evidence would resolve it: Benchmarking on x86 CPUs against other ultra low-bit quantization methods

### Open Question 2
What is the impact of ultra low-bit quantization on model robustness to adversarial attacks?
- Basis in paper: [inferred] Evaluates accuracy and performance but does not address adversarial robustness
- Why unresolved: Adversarial robustness is critical for real-world deployments but not investigated
- What evidence would resolve it: Testing quantized models under adversarial attacks and comparing to full-precision models

### Open Question 3
How does DeepliteRT handle dynamic quantization scenarios where input data distributions change over time?
- Basis in paper: [inferred] Focuses on static quantization without discussing dynamic scenarios or adaptation
- Why unresolved: Dynamic quantization is important for non-stationary data but not addressed
- What evidence would resolve it: Evaluating performance on datasets with shifting distributions or implementing online adaptation

### Open Question 4
What is the energy efficiency of DeepliteRT compared to other quantization methods on edge devices?
- Basis in paper: [explicit] Mentions improved energy efficiency as a benefit but lacks direct energy consumption measurements
- Why unresolved: Energy efficiency is crucial for edge deployments but lacks empirical data
- What evidence would resolve it: Measuring power consumption during inference across different quantization methods

## Limitations
- Lacks detailed training recipes for generating fake-quantized models with minimal accuracy drop
- Configuration files for mixed precision inference are not provided
- Performance gains on ARMv8 targets are highly confident but ARMv7 results show more variability
- Memory access patterns and bitpacking efficiency are not fully quantified

## Confidence

- **Speedup claims:** High - Extensive benchmarking with multiple models and baselines provides strong evidence
- **Mixed precision effectiveness:** Medium - Supported by general principles but lacks specific configuration details
- **Compiler pass efficiency:** Medium - Described mechanism is sound but implementation details are limited

## Next Checks

1. Replicate the mixed precision configuration process by creating test cases that identify quantization-sensitive vs. insensitive layers and measure accuracy retention
2. Profile memory bandwidth usage during bit-serial convolution on ARM devices to quantify the impact of bitpacking efficiency
3. Compare the compilation time and transformed model size for different quantization bit-widths to verify the claimed automatic conversion process efficiency