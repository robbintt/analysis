---
ver: rpa2
title: Contraction Properties of the Global Workspace Primitive
arxiv_id: '2310.01571'
source_url: https://arxiv.org/abs/2310.01571
tags:
- sparse
- global
- workspace
- subnetworks
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper expands on the provably stable "RNNs of RNNs" framework
  introduced by Kozachkov et al. [1], focusing on the global workspace (GW) modular
  structure and sparsity in inter-area connectivity.
---

# Contraction Properties of the Global Workspace Primitive

## Quick Facts
- arXiv ID: 2310.01571
- Source URL: https://arxiv.org/abs/2310.01571
- Reference count: 40
- This paper proves relaxed stability conditions for global workspace modular structures, allowing nonlinear connections while preserving overall network stability.

## Executive Summary
This paper expands on the provably stable "RNNs of RNNs" framework by Kozachkov et al., focusing on global workspace (GW) modular structures and sparsity in inter-area connectivity. The authors prove relaxed stability conditions that allow nonlinear connections between subnetworks while maintaining overall network stability. Empirically, they demonstrate that Global Workspace Sparse Combo Nets achieve strong performance on sequential CIFAR10 with few trainable parameters and greater resilience to subnetwork removal compared to other multi-area adjacency structures.

## Method Summary
The paper builds on the RNNs of RNNs framework where a large network is constructed from smaller contracting subnetworks. The key innovation is proving stability conditions for global workspace topologies that allow sparse inter-area connectivity while preserving contraction properties. The method involves initializing subnetworks with restricted weight magnitudes, computing contraction metrics, and parameterizing inter-area weights using negative feedback to ensure stability during training. Only the inter-area weights are trained while intra-area weights remain fixed.

## Key Results
- GW Sparse Combo Nets achieve strong performance on sequential CIFAR10 with minimal trainable parameters
- Greater resilience to individual subnetwork removal compared to all-to-all negative feedback structures
- Sparse inter-area connectivity maintains stability while improving scalability and interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Sparse inter-area connectivity preserves global stability while improving scalability by bounding the spectral norm of inter-area Jacobian, reducing trainable parameters and preventing overfitting while maintaining theoretical guarantees.
- Core assumption: Each subnetwork is individually contracting in some known metric via restricted weight magnitudes.
- Evidence anchors: Strong performance with few trainable parameters; gains from sparsity in negative feedback connections.
- Break condition: If spectral norm of sparse inter-area weights exceeds contraction bounds, or if subnetworks aren't individually contracting.

### Mechanism 2
- GW topology enables interpretability and resilience through central coordination that integrates information from all subnetworks, maintaining functionality when subnetworks are removed.
- Core assumption: GW subnetwork is large enough to effectively aggregate and redistribute information.
- Evidence anchors: Impressive test performance given size; consistent performance across repetitions.
- Break condition: If GW subnetwork is too small to integrate information, or if inter-area weights become too large.

### Mechanism 3
- Negative feedback parameterization ensures stability while allowing inter-area weights to be trained by making symmetric part of L negative definite in appropriate metric.
- Core assumption: Contraction metrics M for each subnetwork can be combined into a block diagonal matrix.
- Evidence anchors: Proper constraint of B to be block off-diagonal preserves negative feedback interpretation.
- Break condition: If B is not properly constrained or contraction metrics M are incorrectly computed.

## Foundational Learning

- Concept: Contraction analysis and its application to stability of dynamical systems
  - Why needed here: The paper relies on contraction analysis to prove stability of the multi-area RNN architecture.
  - Quick check question: What is the key property of a contracting system that makes it useful for building larger stable systems?

- Concept: Graph theory and sparse matrix representations
  - Why needed here: The paper uses graph-theoretic concepts to describe connectivity between subnetworks and sparse matrix representations to implement inter-area connections efficiently.
  - Quick check question: How does sparsity in the inter-area connectivity matrix affect the spectral norm of the overall system?

- Concept: Modular design principles in engineering and biology
  - Why needed here: The paper draws inspiration from biological modularity and engineering principles to design the multi-area RNN architecture.
  - Quick check question: What are some key advantages of modular design in both engineered and biological systems?

## Architecture Onboarding

- Component map:
  - p subnetworks, each with n units, evolving according to standard RNN dynamics
  - Block diagonal weight matrix W containing intra-area weights
  - Block off-diagonal weight matrix L containing inter-area weights
  - Global workspace (GW) subnetwork serving as central coordinator
  - Linear input and output layers for sequence processing
  - Negative feedback parameterization for L to ensure stability

- Critical path:
  1. Initialize subnetworks with restricted weight magnitudes to ensure individual contraction
  2. Compute contraction metrics M for each subnetwork
  3. Initialize inter-area weights L using negative feedback parameterization
  4. Train only inter-area weights B while keeping intra-area weights fixed
  5. Evaluate stability and performance on sequence learning tasks

- Design tradeoffs:
  - Sparser inter-area connectivity reduces trainable parameters and improves scalability but may limit expressivity
  - Larger GW subnetwork improves coordination but increases computational cost
  - Perfect negative feedback ensures stability but may be overly restrictive compared to near-negative feedback

- Failure signatures:
  - Training loss explodes or plateaus at high values
  - Test accuracy is at chance level or significantly lower than training accuracy
  - Ablation of individual subnetworks causes drastic performance drops
  - Gradient norms become NaN or Inf during training

- First 3 experiments:
  1. Train a 16x32 GW Sparse Combo Net on seqCIFAR10 and compare test accuracy to all-to-all negative feedback
  2. Vary the sparsity level of inter-area connections in a 24x32 Sparse Combo Net and observe effect on seqCIFAR10 performance
  3. Perform ablation studies on a trained GW Sparse Combo Net to identify roles of different subnetworks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different sparsity levels in inter-area connectivity affect the performance and robustness of multi-area RNNs on sequential tasks?
- Basis in paper: The paper demonstrates that increasing sparsity in inter-area connections can improve performance and robustness.
- Why unresolved: The paper explores sparsity levels but does not provide comprehensive analysis of how different sparsity levels specifically impact performance and robustness.
- What evidence would resolve it: Conducting systematic experiments with varying sparsity levels and analyzing their effects on performance and robustness metrics.

### Open Question 2
- Question: Can recursive construction of stable RNNs of RNNs be effectively used to combine pretrained subnetworks for complex tasks?
- Basis in paper: The paper suggests using recursive construction to combine pretrained subnetworks, but this is proposed as a future direction without experimental validation.
- Why unresolved: The concept is theoretically promising but lacks empirical testing to confirm its effectiveness in practice.
- What evidence would resolve it: Implementing and testing the recursive construction approach by combining pretrained subnetworks and evaluating their performance on complex tasks.

### Open Question 3
- Question: How do different subnetwork initialization settings (e.g., size, sparsity, stability constraints) contribute to the overall performance of multi-area RNNs?
- Basis in paper: The paper mentions varying subnetwork properties as a potential area of exploration but does not provide experimental results.
- Why unresolved: The paper acknowledges the importance of subnetwork settings but does not explore their impact systematically.
- What evidence would resolve it: Conducting experiments with different subnetwork initialization settings and analyzing their effects on overall network performance.

## Limitations
- Theoretical claims rely on assumptions about individual subnetwork contraction and proper parameterization of inter-area weights
- Empirical validation is limited to a single task (sequential CIFAR-10) and specific architecture choices
- Relationship between sparsity levels, performance, and stability is not fully characterized across different regimes

## Confidence
- High confidence in basic contraction framework and negative feedback parameterization (well-established from [1])
- Medium confidence in GW topology's specific advantages for interpretability and resilience (supported by ablation studies but not comprehensively tested)
- Medium confidence in empirical performance claims (single task evaluation, no comparisons to state-of-the-art models)
- Low confidence in scalability claims without systematic testing of larger architectures

## Next Checks
1. Verify stability guarantees empirically by testing contraction metrics across training epochs for GW Sparse Combo Nets of varying sizes
2. Extend performance evaluation to additional sequence modeling tasks (e.g., sequential MNIST, permuted sequential MNIST) to assess generalization of results
3. Conduct ablation studies systematically varying GW subnetwork size and inter-area sparsity levels to characterize the full design space