---
ver: rpa2
title: 'Deep Fusion: Efficient Network Training via Pre-trained Initializations'
arxiv_id: '2306.11903'
source_url: https://arxiv.org/abs/2306.11903
tags:
- fusion
- training
- deep
- performance
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Deep Fusion, a method for efficient training
  of large language models by initializing them with smaller pre-trained models and
  fusing their parameters. The approach uses a fusion operator that concatenates the
  weights of two layers along the feature dimension, maintaining the same architecture
  but increasing its capacity.
---

# Deep Fusion: Efficient Network Training via Pre-trained Initializations

## Quick Facts
- arXiv ID: 2306.11903
- Source URL: https://arxiv.org/abs/2306.11903
- Authors: 
- Reference count: 31
- Primary result: Deep Fusion reduces training time by up to 24% while maintaining or improving performance compared to standard training

## Executive Summary
Deep Fusion introduces an efficient approach to training large language models by leveraging pre-trained smaller networks through a novel fusion operator. The method concatenates weights of two layers along the feature dimension, maintaining the same architecture while increasing its capacity. Experiments with T5 models on C4 and GLUE tasks demonstrate that Deep Fusion can reduce training time by up to 24% while achieving comparable or better performance than standard training methods. The authors also propose a theoretical framework based on backward error analysis to guide the optimal use of their method.

## Method Summary
Deep Fusion works by taking two pre-trained smaller models and combining them using a fusion operator that concatenates their weights along the feature dimension. This creates a larger model that maintains the original architecture's properties while increasing its capacity. The method can be applied in two ways: fusing different models together or self-fusing a model with itself. The approach is particularly effective when the smaller models have been pre-trained on relevant data, as this provides a strong initialization for the larger model. The theoretical framework suggests that this method works by preserving the backward error properties of the original models while expanding their representational capacity.

## Key Results
- Deep Fusion reduces training time by up to 24% compared to standard training
- Self-fusion of T5-Small models achieves comparable performance to fusing different models
- The method maintains architectural properties while increasing model capacity
- Experimental validation on C4 and GLUE tasks shows consistent performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fusing smaller pre-trained models provides a better initialization than random initialization for large models
- Mechanism: The fusion operator concatenates weights along the feature dimension, placing original weights on the diagonal and zeros elsewhere, preserving learned representations while allowing new parameters to learn complementary features
- Core assumption: Smaller pre-trained models have learned useful representations that can be leveraged by larger models
- Evidence anchors:
  - [abstract] "Deep Fusion, an efficient approach to network training that leverages pre-trained initializations of smaller networks"
  - [section 3] "The FUSION operator is defined as follows. Given two layers with d, d′ inputs and k, k′ outputs..."
  - [corpus] Weak evidence - no direct mention of initialization benefits in related papers

### Mechanism 2
- Claim: The fusion approach maintains the same network architecture while increasing capacity
- Mechanism: Fusion operator creates new layer with dimension (d+d')×(k+k') while preserving original activation function and network composition
- Core assumption: Network can effectively utilize increased dimensionality without architectural changes
- Evidence anchors:
  - [section 3] "the fused layer maintains the same composition or architecture defined in Eq.1"
  - [abstract] "The approach uses a fusion operator that concatenates the weights of two layers along the feature dimension, maintaining the same architecture but increasing its capacity"
  - [corpus] No direct evidence in related papers about maintaining architecture while increasing capacity

### Mechanism 3
- Claim: Self-fusion provides similar benefits to fusing different models
- Mechanism: Self-fusion doubles parameters in each layer while maintaining same output representation, creating redundancy that improves robustness
- Core assumption: Additional parameters from self-fusion can learn complementary features even when starting from identical weights
- Evidence anchors:
  - [section 4.1] "self-fusion: T5-M EDIUM trained from self fusing a T5-S MALL model"
  - [abstract] "Experiments with T5 models on C4 and GLUE tasks show that Deep Fusion reduces training time by up to 24% while achieving comparable or better performance"
  - [corpus] No direct evidence in related papers about self-fusion benefits

## Foundational Learning

- Concept: Layer-wise operations and matrix concatenation
  - Why needed here: Understanding how the fusion operator works at the matrix level is crucial for implementing and debugging the method
  - Quick check question: If we have two layers with weights W1 (3×4) and W2 (3×5), what are the dimensions of the fused weight matrix?

- Concept: Transformer architecture components (attention, MLP, normalization)
  - Why needed here: The paper discusses how to apply fusion to different transformer components, each with unique properties
  - Quick check question: Which transformer component cannot be directly fused while maintaining both fusion properties simultaneously?

- Concept: Transfer learning and initialization strategies
  - Why needed here: The core idea relies on leveraging pre-trained smaller models, which requires understanding how initialization affects training dynamics
  - Quick check question: What is the primary advantage of using pre-trained weights versus random initialization in deep learning?

## Architecture Onboarding

- Component map: Fusion operator -> Transformer components -> Training loop integration -> Checkpoint management
- Critical path: 1. Load pre-trained smaller models 2. Apply fusion operator to create larger model 3. Train fused model on target task 4. Fine-tune on downstream tasks
- Design tradeoffs:
  - Fusion rule vs fusion property: Maintaining fusion rule preserves architectural constraints but may limit representational power, while fusion property maximizes capacity but requires additional implementation
  - Zero initialization vs random initialization for new parameters: Zeros preserve original outputs but may slow learning, while small random values enable faster adaptation but alter initial behavior
  - Single fusion vs staged fusion: Single fusion is simpler but staged fusion may provide better initialization for very large models
- Failure signatures:
  - Training instability or divergence: May indicate poor initialization or incompatible model fusion
  - No improvement over baseline: Could suggest smaller models didn't learn useful representations or fusion didn't effectively leverage them
  - Overfitting to training data: Additional parameters may increase model capacity beyond what data can support
- First 3 experiments:
  1. Implement fusion operator and test on two simple fully-connected layers to verify mathematical properties
  2. Apply fusion to two identical small transformer models and verify output matches average ensemble before training
  3. Train fused model on small dataset and compare performance against randomly initialized model of same size

## Open Questions the Paper Calls Out

- How does Deep Fusion perform when fusing models trained on different data distributions or domains?
- What are the theoretical limits of model growth through repeated self-fusion, and at what point does performance degrade?
- How does Deep Fusion's parameter efficiency compare to other model compression techniques like pruning or quantization?
- Can Deep Fusion be effectively applied to models with heterogeneous architectures (e.g., fusing attention-heavy with MLP-heavy models)?
- What is the optimal fusion strategy for different training phases (pre-training vs. fine-tuning)?

## Limitations

- The method's effectiveness depends heavily on the quality of pre-trained smaller models
- Theoretical framework based on backward error analysis lacks extensive validation across different model architectures
- Scalability to extremely large models and behavior with heterogeneous model fusions remain unexplored
- Computational savings measured primarily through training time reduction, not total computational cost

## Confidence

**High Confidence**: Basic fusion mechanism and implementation details are well-defined and experimentally validated. Self-fusion benefits are supported by experimental results.

**Medium Confidence**: Theoretical framework provides sound foundation but needs validation across different model types and tasks. Computational efficiency gains may vary significantly depending on hardware and model size.

**Low Confidence**: Scalability to much larger models and effectiveness with heterogeneous model fusions are largely speculative based on current evidence.

## Next Checks

1. Implement and test Deep Fusion with heterogeneous model combinations (e.g., fusing T5-Small with BERT-Base) to validate generalization beyond same-architecture fusions.

2. Apply Deep Fusion to progressively larger model sizes (T5-Large, T5-3B) to determine if computational efficiency gains scale proportionally or diminish at larger scales.

3. Design controlled experiments that systematically vary properties of pre-trained models to test predictions of backward error analysis framework and identify which model characteristics most strongly influence fusion effectiveness.