---
ver: rpa2
title: 'GPT-who: An Information Density-based Machine-Generated Text Detector'
arxiv_id: '2310.06202'
source_url: https://arxiv.org/abs/2310.06202
tags:
- text
- information
- arxiv
- detection
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GPT-who, a psycholinguistically-inspired
  statistical-based detector for identifying authorship in machine-generated texts.
  The detector leverages the Uniform Information Density (UID) principle to capture
  unique information signatures of Large Language Models (LLMs) and human authors.
---

# GPT-who: An Information Density-based Machine-Generated Text Detector

## Quick Facts
- arXiv ID: 2310.06202
- Source URL: https://arxiv.org/abs/2310.06202
- Authors: 
- Reference count: 16
- Primary result: UID-based statistical detector outperforms state-of-the-art methods by over 20% across domains

## Executive Summary
This paper introduces GPT-who, a novel detector for identifying authorship in machine-generated texts using the Uniform Information Density (UID) principle. The detector leverages statistical features derived from surprisal distributions to capture unique information signatures of Large Language Models (LLMs) and human authors. GPT-who employs logistic regression on 44 UID-based features to classify authorship, achieving superior performance across multiple benchmark datasets while maintaining computational efficiency and interpretability.

## Method Summary
GPT-who uses token probabilities from a pre-trained GPT2-XL model to extract UID-based features, including mean surprisal, variance, difference scores, and min/max spans, creating 44-dimensional feature vectors for each text article. A logistic regression classifier then maps these features to different authors. The method is computationally inexpensive, domain-agnostic, and provides interpretable representations of text articles. The approach captures subtle information distribution patterns that distinguish between human and machine-generated text, even when the surface text is indistinguishable.

## Key Results
- GPT-who outperforms state-of-the-art detectors including GLTR, GPTZero, DetectGPT, OpenAI detector, and ZeroGPT by over 20% across domains
- The detector maintains high accuracy even when text is indiscernible at the surface level
- UID-based features consistently predict authorship across four large-scale benchmark datasets
- The method successfully distinguishes between different LLM families based on their UID distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UID-based features capture hidden statistical distinctions between human and machine-generated texts.
- Mechanism: The UID principle measures how evenly information is distributed in text. Human authors distribute information less uniformly (higher variance in surprisal), while machine-generated text tends to be smoother and more uniform (lower variance).
- Core assumption: Information density distribution patterns are consistent enough across different authors and writing tasks to serve as reliable classification features.
- Evidence anchors:
  - [abstract] "GPT-who can distinguish texts generated by very sophisticated LLMs, even when the overlying text is indiscernible"
  - [section] "We find that GPT-who remarkably outperforms state-of-the-art detectors...by over 20% across domains"
  - [corpus] Weak evidence - no direct corpus data on UID patterns, but multiple related papers on UID exist in corpus
- Break condition: If models are explicitly trained to mimic human UID patterns or if fine-tuning objectives include UID optimization, the UID-based distinction may break down.

### Mechanism 2
- Claim: Logistic regression on UID features can effectively classify authorship without requiring LLM fine-tuning.
- Mechanism: Simple logistic regression can learn decision boundaries between authors based on the 44-dimensional UID feature space without needing complex neural architectures.
- Core assumption: The UID feature space contains sufficient discriminative information to separate authors, making complex models unnecessary.
- Evidence anchors:
  - [abstract] "it is computationally inexpensive and utilizes an interpretable representation of text articles"
  - [section] "GPT-who uses token probabilities of articles to extract UID-based features. A classifier then learns to map UID features to different authors"
  - [corpus] Moderate evidence - related work on statistical detection methods exists in corpus
- Break condition: If the feature space becomes too complex or if new author types emerge with similar UID distributions, logistic regression may become insufficient.

### Mechanism 3
- Claim: UID distributions can capture architectural differences between LLM families.
- Mechanism: Models sharing similar architectures tend to have similar UID distributions, creating natural groupings that enhance multi-class classification.
- Core assumption: Architectural similarities lead to consistent information distribution patterns across different models within the same family.
- Evidence anchors:
  - [section] "UID-based features are reflective of differences in LLM architectures or families such that models that share architectures have similar UID distributions within but not outside their category"
  - [section] "we see that models that belong to the same LM family by architecture tend to follow similar UID distributions"
  - [corpus] Moderate evidence - corpus contains related work on model family detection
- Break condition: If new architectures emerge that deliberately break UID patterns or if cross-architecture fine-tuning becomes common, this grouping may become less reliable.

## Foundational Learning

- Concept: Uniform Information Density (UID) principle
  - Why needed here: This is the core theoretical foundation that enables the entire detection approach. Understanding how humans optimize information distribution differently from machines is crucial.
  - Quick check question: If humans prefer to spread information evenly, what would we expect to see in the surprisal distribution of human-written text? (Answer: higher variance, more peaks and troughs)

- Concept: Shannon's Information Theory and surprisal calculation
  - Why needed here: The method relies on calculating token-level surprisal values using probability estimates from language models. This is the mathematical foundation for UID features.
  - Quick check question: If a word has high probability in a given context, what would its surprisal value be? (Answer: Low surprisal, since surprisal = -log(probability))

- Concept: Feature engineering and statistical feature extraction
  - Why needed here: The 44 UID features are carefully crafted to capture different aspects of information distribution. Understanding feature selection is key.
  - Quick check question: Why might both global (variance) and local (difference between consecutive tokens) UID measures be useful? (Answer: They capture different scales of information distribution patterns)

## Architecture Onboarding

- Component map: Token probability extraction (GPT2-XL) → UID feature calculation (44 features) → Logistic regression classifier → Authorship prediction
- Critical path: The most time-consuming step is token probability extraction from the pre-trained language model, followed by feature calculation and classification.
- Design tradeoffs: Uses simple logistic regression instead of complex neural networks for interpretability and computational efficiency, at the potential cost of some accuracy.
- Failure signatures: Poor performance on new author types, inability to distinguish between architecturally similar models, or degradation when UID patterns change.
- First 3 experiments:
  1. Verify UID feature extraction works correctly by comparing distributions across known human vs. machine text samples
  2. Test logistic regression classification performance on a small binary classification task (human vs. single model type)
  3. Validate architectural grouping hypothesis by clustering UID features from models known to share architectures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-who compare to other detectors when applied to newer, more advanced LLMs beyond those tested in the paper?
- Basis in paper: [explicit] The paper states that GPT-who outperforms state-of-the-art detectors on four large-scale benchmark datasets, but these datasets may not include the most recent and advanced LLMs.
- Why unresolved: The paper does not test GPT-who on the latest LLMs, and the performance of GPT-who on these newer models is unknown.
- What evidence would resolve it: Testing GPT-who on benchmark datasets that include the latest LLMs and comparing its performance to other detectors would provide evidence to resolve this question.

### Open Question 2
- Question: Can UID-based features be used to identify other harmful phenomena beyond machine-generated text, such as misinformation or plagiarism?
- Basis in paper: [inferred] The paper mentions that UID-based features can capture subtle information distribution patterns that constitute distinct information profiles of authors, but it does not explore their potential use in identifying other harmful phenomena.
- Why unresolved: The paper does not investigate the application of UID-based features to other harmful phenomena, and their effectiveness in this regard is unknown.
- What evidence would resolve it: Conducting experiments to test the effectiveness of UID-based features in identifying misinformation or plagiarism would provide evidence to resolve this question.

### Open Question 3
- Question: How do the UID-based features of GPT-who perform in cross-lingual scenarios, where the text is written in languages other than English?
- Basis in paper: [explicit] The paper does not mention the performance of GPT-who in cross-lingual scenarios.
- Why unresolved: The paper does not provide any information on the performance of GPT-who in languages other than English, and its effectiveness in cross-lingual scenarios is unknown.
- What evidence would resolve it: Testing GPT-who on benchmark datasets that include texts in multiple languages and comparing its performance to other detectors would provide evidence to resolve this question.

## Limitations
- The UID-based detection mechanism assumes stable information density patterns across domains, which may not hold for specialized content
- Logistic regression approach may face scalability challenges with rapidly evolving LLM architectures
- The architectural grouping hypothesis lacks sufficient empirical validation for long-term generalizability

## Confidence

**High**: Benchmark performance claims and general superiority over existing methods
**Medium**: UID-based detection mechanism and logistic regression approach
**Low**: Architectural grouping hypothesis and long-term stability of UID patterns

## Next Checks

1. **Cross-domain validation**: Test GPT-who on specialized domains (legal, medical, technical) not represented in current benchmark datasets to verify domain-agnostic performance claims
2. **Temporal stability assessment**: Evaluate detector performance on texts generated by newer LLM models released after the detector was developed to assess adaptation requirements
3. **Adversarial robustness testing**: Generate machine-written texts specifically optimized to mimic human UID patterns and measure GPT-who's ability to maintain detection accuracy under these conditions