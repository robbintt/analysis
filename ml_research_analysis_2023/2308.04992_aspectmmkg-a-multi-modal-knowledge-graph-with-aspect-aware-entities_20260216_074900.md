---
ver: rpa2
title: 'AspectMMKG: A Multi-modal Knowledge Graph with Aspect-aware Entities'
arxiv_id: '2308.04992'
source_url: https://arxiv.org/abs/2308.04992
tags:
- images
- entity
- aspect
- aspect-related
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper constructs AspectMMKG, the first multi-modal knowledge
  graph with aspect-aware entities by matching aspect-related images to different
  entity aspects. Specifically, the authors collect 2,380 popular entities from 15
  pre-defined types, extract aspect labels from Wikipedia pages, and further retrieve
  aspect-related images from Google using extracted aspect-related sentences as queries.
---

# AspectMMKG: A Multi-modal Knowledge Graph with Aspect-aware Entities

## Quick Facts
- arXiv ID: 2308.04992
- Source URL: https://arxiv.org/abs/2308.04992
- Authors: Multiple authors
- Reference count: 36
- Key outcome: Constructs AspectMMKG with 2,380 entities, 18,139 aspects, and 645,383 aspect-related images; improves entity aspect linking performance by 4.4% over CLIP baseline

## Executive Summary
AspectMMKG is the first multi-modal knowledge graph that incorporates aspect-aware entities with fine-grained aspect labels and aspect-specific images. The system extracts hierarchical aspect labels from Wikipedia, retrieves aspect-related images from Google using aspect queries, and trains a discriminative model to select high-quality images. Experimental results demonstrate that AspectMMKG improves entity aspect linking performance, especially when text features are limited, validating the effectiveness of incorporating aspect-specific visual information into knowledge graph representations.

## Method Summary
AspectMMKG constructs a multi-modal knowledge graph by first collecting 2,380 popular entities from Wikipedia and extracting hierarchical aspect labels from their pages. The system retrieves aspect-related images from Google using aspect-specific sentences as queries, then filters them using CLIP-based relevance scoring. A discriminative aspect-related image retrieval (AIR) model is trained using contrastive learning to learn the relationship between entity images, aspect labels, and aspect-related images. The resulting knowledge graph contains entities linked to aspect-specific images, which are used to improve entity aspect linking performance through a learning-to-rank model that incorporates both text and image features.

## Key Results
- Constructs AspectMMKG containing 2,380 entities, 18,139 entity aspects, and 645,383 aspect-related images
- AIR model improves recall@10 by 4.4% compared to CLIP baseline for aspect-related image retrieval
- Entity aspect linking performance improves significantly when incorporating AspectMMKG image features, especially with fewer text features available

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AspectMMKG improves entity aspect linking by providing aspect-specific images instead of generic entity images
- Mechanism: The framework extracts aspect labels from Wikipedia sections and retrieves images using these aspects as queries from Google. This creates a mapping between each entity-aspect pair and relevant images, allowing downstream models to condition predictions on context-specific visual information
- Core assumption: Images retrieved using aspect-specific sentences as queries are more relevant to that aspect than generic entity images
- Evidence anchors:
  - [abstract]: "AspectMMKG aims to provide entities with fine-grained aspects and links each entity to different images given different aspects"
  - [section]: "Given an entity as well as an aspect, we collect the aspect-related images by extracting images belonging to the content in the aspect section"
- Break condition: If aspect-specific queries retrieve irrelevant images or if the aspect extraction process fails to capture meaningful distinctions, the core assumption breaks down

### Mechanism 2
- Claim: The discriminative aspect-related image retrieval (AIR) model improves upon CLIP by learning to rank aspect-relevant images
- Mechanism: The model takes an overall entity image and aspect label as input, learns a projection space where aspect-related images are closer to the combined entity-aspect representation, and uses contrastive loss to train this ranking
- Core assumption: The overall entity image contains sufficient information to distinguish which candidate images are aspect-relevant
- Evidence anchors:
  - [section]: "We train an AIR model to learn the relationship between entity image and entity aspect-related images by incorporating entity image, aspect, and aspect image information"
  - [section]: "Compared with the CLIP model, our discriminative model improves recall@10 by 4.4%"
- Break condition: If the overall entity image doesn't capture distinguishing features, or if the contrastive learning fails to learn meaningful representations, the model won't outperform CLIP

### Mechanism 3
- Claim: The hierarchical aspect structure from Wikipedia provides granular semantic context that improves image retrieval and downstream task performance
- Mechanism: By parsing Wikipedia HTML structure, the system captures both top-level aspects (like "Geography") and sub-aspects (like "Rivers"), allowing for more precise image retrieval and context-aware predictions
- Core assumption: Hierarchical aspects reflect meaningful semantic distinctions that improve task performance
- Evidence anchors:
  - [section]: "the content in Wikipedia has a hierarchical structure, leading to the hierarchical aspect labels we collected"
  - [section]: "we move the images from the higher-level aspect to the first-level aspect" (indicating preservation of hierarchy information)
- Break condition: If the hierarchical structure doesn't correspond to meaningful distinctions, or if downstream models can't effectively use the hierarchical information, the benefit disappears

## Foundational Learning

- Concept: Multi-modal knowledge graph construction
  - Why needed here: Understanding how to combine text and image data into a unified graph structure is fundamental to AspectMMKG's approach
  - Quick check question: What are the key differences between traditional KGs and MMKGs in terms of data representation and retrieval?

- Concept: Aspect extraction from hierarchical text
  - Why needed here: The system relies on parsing Wikipedia's hierarchical structure to create aspect labels, which requires understanding text segmentation and semantic labeling
  - Quick check question: How would you design a rule-based system to extract hierarchical aspects from structured text documents?

- Concept: Contrastive learning for image ranking
  - Why needed here: The AIR model uses contrastive learning to rank aspect-related images, requiring understanding of embedding spaces and similarity metrics
  - Quick check question: What is the role of the temperature hyperparameter in contrastive loss functions, and how does it affect model performance?

## Architecture Onboarding

- Component map: Entity collection (Wikidata + pageview filtering) -> Aspect extraction (Wikipedia HTML parsing) -> Image retrieval (Google search with aspect queries) -> Quality filtering (CLIP-based relevance scoring) -> AIR model (contrastive learning with entity-aspect-image triplets) -> Downstream task integration (EAL with image features)

- Critical path: Entity collection → Aspect extraction → Image retrieval → Quality filtering → AIR model training → EAL evaluation

- Design tradeoffs:
  - Google vs. Wikipedia images: Chose Google for quantity despite potential noise
  - CLIP vs. custom AIR: Used CLIP for initial filtering, custom model for refined ranking
  - Hierarchical vs. flat aspects: Preserved hierarchy but unified at first level for compatibility

- Failure signatures:
  - Low recall@10 values indicate poor image retrieval quality
  - Poor EAL performance with images suggests aspect-image relevance issues
  - High variance in accuracy across entity types suggests collection bias

- First 3 experiments:
  1. Test AIR model on held-out aspect-image triplets to verify recall improvements over CLIP
  2. Run EAL with varying numbers of text features to quantify image feature contribution
  3. Manually evaluate top-5 retrieved images for sample entities to assess quality control

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The quality of aspect-related images depends heavily on Google search results and aspect extraction process, which could introduce noise
- The 4.4% improvement over CLIP in recall@10 needs context - absolute recall values would help assess practical significance
- The claim that hierarchical aspects improve performance is plausible but not directly validated in the paper

## Confidence
- Entity aspect linking performance claims: **Medium** - supported by experiments but limited to one downstream task
- AIR model superiority claims: **Medium** - shows quantitative improvement but lacks qualitative evaluation
- Aspect extraction and hierarchical structure benefits: **Low** - plausible mechanism but minimal empirical validation

## Next Checks
1. Manually evaluate a random sample of retrieved images (e.g., 50 entities × 5 aspects) to assess quality and relevance, checking if aspect-specific queries truly retrieve better images than generic entity images
2. Conduct ablation studies on the EAL task by progressively removing image features to quantify their contribution and test if improvements hold across different entity types and aspect categories
3. Test the AIR model on a held-out dataset with known aspect-image relationships to verify that the 4.4% recall improvement is statistically significant and generalizes beyond the training data