---
ver: rpa2
title: 'APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth Estimation
  for Autonomous Navigation'
arxiv_id: '2303.01351'
source_url: https://arxiv.org/abs/2303.01351
tags:
- patch
- adversarial
- depth
- object
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel adaptive adversarial patch (APARATE)
  designed to selectively undermine CNN-based monocular depth estimation (MDE) by
  either corrupting estimated distances or making objects disappear for autonomous
  systems. The method is optimized to be shape and scale-aware, extending impact beyond
  immediate proximity.
---

# APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth Estimation for Autonomous Navigation

## Quick Facts
- arXiv ID: 2303.01351
- Source URL: https://arxiv.org/abs/2303.01351
- Reference count: 40
- Primary result: Generates adversarial patches achieving >14 meters depth error with 99% affected region in monocular depth estimation

## Executive Summary
This paper presents APARATE, a novel adaptive adversarial patch designed to compromise CNN-based monocular depth estimation (MDE) models used in autonomous navigation systems. The method generates physical adversarial patches that can either corrupt depth estimates or make objects disappear entirely from the depth estimation output. By leveraging object detection to focus attacks on specific target regions and optimizing for physical-world robustness through geometric and color space transformations, APARATE demonstrates significant effectiveness against state-of-the-art MDE models, highlighting critical vulnerabilities in autonomous perception systems.

## Method Summary
APARATE generates adversarial patches by first using a pre-trained YOLOv4-tiny object detector to identify target objects (cars and pedestrians) and create focus masks extending beyond the patch itself. The method employs a penalized depth loss function that prioritizes affecting non-overlapping regions, forcing the optimizer to focus on enlarging the affected area beyond just the patch overlap. During training, the patch undergoes random scaling, rotation (±20°), and color space transformations to simulate real-world conditions. The optimized patch is then rendered within the object's bounding box and evaluated against victim MDE models (monodepth2, Depthhints, Manydepth) using metrics including mean depth estimation error and ratio of affected region.

## Key Results
- Achieves mean depth estimation error exceeding 0.5 meters on CNN-based MDE models
- Affects up to 99% of the targeted region in the depth estimation output
- Yields 0.34 meters error with 94% influence on Transformer-based MDE models
- Demonstrates effectiveness in making objects disappear entirely from depth maps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: APARATE can make a detected object disappear entirely from the depth estimation model's output, rather than just corrupting the depth in a local patch region.
- Mechanism: The adversarial patch is rendered on the full bounding box of the detected object (via the focus mask Mf), so its influence covers the entire object region instead of being limited to the patch's overlap with the input image.
- Core assumption: The object detector reliably identifies the target object's full spatial extent in the image, and the MDE model's output is sensitive to perturbations applied anywhere within the object's bounding box.
- Evidence anchors:
  - [abstract]: "APARATE is optimized to be shape and scale-aware, and its impact adapts to the target object instead of being limited to the immediate neighborhood."
  - [section]: "We use the object detector to create a mask around the object that we named Mf and a mask Mp that will constrict the location, the size and the shape of the generated patch."
- Break condition: If the object detector fails to detect the target object or produces an inaccurate bounding box, the focus mask will not cover the full object, limiting the patch's ability to make the object disappear.

### Mechanism 2
- Claim: APARATE achieves large depth estimation errors (e.g., >14 meters) by optimizing a penalized depth loss that prioritizes affecting non-overlapping regions.
- Mechanism: The loss function splits depth error into two terms: one for pixels overlapped by the patch (Ld1) and one for non-overlapped pixels (Ld2). Ld2 is given higher priority by not squaring it, forcing the optimizer to focus on enlarging the affected region beyond just the patch overlap.
- Core assumption: The MDE model's depth predictions for non-overlapping regions can still be influenced by adversarial perturbations applied to the patch area, likely through learned contextual or spatial dependencies in the model.
- Evidence anchors:
  - [abstract]: "Our proposed patch achieves more than 14 meters mean depth estimation error, with 99% of the target region being affected."
  - [section]: "Ld1 is the loss of pixels overlapped by the patch and Ld2 is the loss of the non-overlapped pixels. In order to force the optimizer to prioritize the minimization of the non-overlapped pixels loss Ld2, we square the term of the difference between the output depth and the target depth |dt − dadv| ⊙ MP..."
- Break condition: If the MDE model's depth predictions are highly localized to the patch region and insensitive to global context, the non-overlapping regions may not be affected, reducing the overall error.

### Mechanism 3
- Claim: APARATE's patch is robust to physical-world transformations (scale, rotation, lighting) due to inclusion of these transformations during training.
- Mechanism: The patch transformer applies random scaling, rotation (±20°), and color space transformations (noise, contrast, brightness) to the patch during optimization, simulating the variety of conditions the patch will encounter in the real world.
- Core assumption: Training the patch to be robust to these transformations will generalize to unseen physical conditions, allowing it to maintain effectiveness across a range of distances, angles, and lighting.
- Evidence anchors:
  - [abstract]: "APARATE is optimized to be shape and scale-aware, and its impact adapts to the target object instead of being limited to the immediate neighborhood."
  - [section]: "The distance and angle of a camera in an autonomous vehicle with respect to another vehicle or any other target object vary continuously... We use several physical transformations to simulate these dynamic factors..."
- Break condition: If the range of simulated transformations does not cover the actual conditions encountered in deployment, the patch may fail to generalize.

## Foundational Learning

- Concept: Adversarial attacks on deep learning models
  - Why needed here: APARATE is an adversarial attack designed to fool a monocular depth estimation model. Understanding the fundamentals of adversarial attacks (e.g., the concept of adding imperceptible perturbations to cause misclassification) is essential to grasp the motivation and mechanism of APARATE.
  - Quick check question: What is the key difference between digital and physical adversarial attacks, and why is APARATE categorized as the latter?

- Concept: Monocular depth estimation (MDE)
  - Why needed here: APARATE specifically targets CNN-based MDE models. A basic understanding of how MDE works (e.g., predicting depth from a single image using a neural network) is necessary to understand the attack's target and impact.
  - Quick check question: In the context of autonomous driving, why is MDE important, and what are some potential consequences of an MDE model being fooled by an adversarial attack?

- Concept: Object detection and bounding box generation
  - Why needed here: APARATE uses a pre-trained object detector (YOLOv4-tiny) to locate the target object and generate bounding boxes. Understanding how object detection works and how bounding boxes are used to define regions of interest is crucial for understanding how APARATE focuses its attack.
  - Quick check question: How does the use of an object detector in APARATE differ from simply placing a patch at a random location in the image, and what advantage does this provide?

## Architecture Onboarding

- Component map: Object Detector (YOLOv4-tiny) -> Patch Transformer -> Patch Applier -> MDE Model -> Loss Functions -> Optimizer (Adam)
- Critical path: Object detection → Patch rendering → MDE forward pass → Loss computation → Patch update. This loop repeats for a fixed number of epochs during training.
- Design tradeoffs:
  - Patch size vs. effectiveness: Larger patches generally achieve higher depth errors but may be more noticeable.
  - Training time vs. robustness: Including more physical transformations during training increases robustness but also increases training time.
  - Target object specificity vs. generalization: Training a patch for a specific object class (e.g., cars) may achieve higher effectiveness for that class but may not generalize well to other objects.
- Failure signatures:
  - Object detector fails to detect target object: Patch is not rendered, attack fails.
  - MDE model is robust to adversarial patches: Low depth error, high MSE.
  - Patch is not robust to physical transformations: Effectiveness degrades under different distances, angles, or lighting conditions.
- First 3 experiments:
  1. Train APARATE on a simple dataset (e.g., a few hundred KITTI images) targeting a single object class (e.g., cars) using a single MDE model (e.g., monodepth2). Evaluate depth error and affected region ratio on a held-out test set.
  2. Compare APARATE's performance against a baseline adversarial patch (e.g., randomly placed patch without object detection) on the same dataset and evaluation metrics.
  3. Test APARATE's robustness to input transformations (e.g., JPEG compression, Gaussian noise, median blur) by applying these defenses to the adversarial images and re-evaluating the depth error and affected region ratio.

## Open Questions the Paper Calls Out

- How can adversarial patch attacks be adapted to different environmental conditions such as lighting variations, weather, and camera angles?
- What are the potential long-term impacts of adversarial patch attacks on autonomous navigation systems, and how can they be mitigated?
- How can adversarial patch attacks be detected and prevented in real-time by autonomous systems?

## Limitations

- Performance degrades significantly when the YOLOv4-tiny detector fails to identify target objects
- Patch robustness to extreme environmental conditions beyond those simulated during training remains unproven
- Evaluation focuses primarily on specific MDE architectures without comprehensive testing across diverse model families

## Confidence

- **High Confidence**: The core mechanism of using object detection to focus adversarial attacks on specific regions, and the basic implementation of penalized depth loss functions
- **Medium Confidence**: The claimed effectiveness metrics (14+ meters depth error, 99% affected region) based on synthetic evaluations against specific MDE models
- **Low Confidence**: Real-world robustness claims, as the physical-world validation appears limited to controlled conditions and doesn't account for all possible deployment scenarios

## Next Checks

1. **Cross-Model Generalization Test**: Evaluate APARATE against a broader range of MDE architectures including non-CNN approaches (transformer-based models) and recent state-of-the-art methods not included in the original evaluation.
2. **Real-World Deployment Assessment**: Conduct field tests in varied lighting, weather, and traffic conditions to validate the physical robustness claims, measuring performance degradation under realistic operational scenarios.
3. **Defensive Strategy Evaluation**: Test whether common adversarial defense mechanisms (JPEG compression, Gaussian noise, median blur) significantly reduce APARATE's effectiveness, quantifying the robustness of the attack under defensive countermeasures.