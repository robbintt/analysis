---
ver: rpa2
title: Clinical Text Deduplication Practices for Efficient Pretraining and Improved
  Clinical Tasks
arxiv_id: '2312.09469'
source_url: https://arxiv.org/abs/2312.09469
tags:
- clinical
- text
- notes
- none
- wnbn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes duplication in clinical text, categorizing
  it into within-note, between-note clinically relevant, and between-note not relevant
  types. Using a large dataset of clinical notes, the authors develop a classifier
  to identify clinically not relevant duplicates with high precision.
---

# Clinical Text Deduplication Practices for Efficient Pretraining and Improved Clinical Tasks

## Quick Facts
- arXiv ID: 2312.09469
- Source URL: https://arxiv.org/abs/2312.09469
- Reference count: 10
- Primary result: Deduplicating clinical text improves language model efficiency without harming downstream task performance, with up to 16% decrease in perplexity and up to 1.5% increase in F1 score.

## Executive Summary
This study addresses the critical issue of duplication in clinical text and its impact on language model pretraining and downstream task performance. The authors develop a comprehensive framework for identifying and categorizing duplicate text in clinical notes, distinguishing between within-note duplicates, between-note clinically relevant duplicates, and between-note not relevant duplicates. By creating a classifier to identify clinically not relevant duplicates and applying deduplication at multiple levels, the researchers demonstrate that models pretrained on deduplicated data show improved efficiency (lower perplexity) while maintaining or even improving performance on downstream classification tasks. The findings suggest that removing redundant information from clinical text can reduce computational costs without sacrificing model effectiveness.

## Method Summary
The researchers analyzed clinical text from three large datasets (MIMIC-III, MSDW ICU, and n2c2) to identify and classify duplicate sentences using EXACT SUBSTR for between-note duplicates and a custom approach for within-note duplicates. They manually annotated a subset of between-note duplicates for clinical relevance and fine-tuned a GatorTron model to classify clinically not relevant duplicates. Four deduplication configurations were created (NONE, WN, WNNR, WNBN) representing progressively more aggressive deduplication. ClinicalBERT and GatorTron models were further pretrained on each configuration using masked language modeling and next sentence prediction objectives. The pretrained models were evaluated on all configurations for perplexity, and prompt-based learning was used for smoking status classification on the n2c2 dataset.

## Key Results
- Models pretrained on deduplicated data show up to 16% decrease in perplexity compared to models trained on non-deduplicated data
- Downstream classification tasks show up to 1.5% increase in F1 score when using models pretrained on deduplicated data
- Clinically not relevant duplicates constitute 26.2% of all duplicate sentences in MIMIC-III dataset
- GatorTron-based relevance classifier achieves 95.2% precision in identifying clinically not relevant duplicates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deduplicating clinical text reduces model perplexity by removing redundant information
- Mechanism: When redundant sentences are removed from training data, the language model learns less repetitive patterns, leading to more efficient encoding of unique information
- Core assumption: Clinical text redundancy negatively impacts language model efficiency
- Evidence anchors:
  - [abstract]: "models pretrained on deduplicated data show up to 16% decrease in perplexity"
  - [section 5]: "relative PPL scores on WNBN are less than (maximum 1.88% decrease for ClinicalBert)"
  - [corpus]: Weak - no explicit corpus-level evidence provided
- Break condition: If the redundancy in clinical text is not primarily redundant information but rather essential repeated context

### Mechanism 2
- Claim: Removing clinically not relevant duplicates improves downstream classification performance
- Mechanism: By removing boilerplate text and legal statements that don't contribute to patient care information, the model can focus on clinically relevant content, improving classification accuracy
- Core assumption: Clinically not relevant duplicates interfere with classification tasks
- Evidence anchors:
  - [abstract]: "up to 1.5% increase in F1 score for classification tasks"
  - [section 6]: "Both models, when tuned on WNBN, show a significant decrease (ps < 0.05, pairwise t-tests with Bonferroni correction) in F1 scores"
  - [corpus]: Weak - no explicit corpus-level evidence provided
- Break condition: If the boilerplate text contains implicit clinical signals not captured by the relevance classifier

### Mechanism 3
- Claim: Domain adaptation on deduplicated text retains medical knowledge while reducing computational cost
- Mechanism: By training on smaller datasets with selected relevant information, models can learn the same medical knowledge more efficiently without the noise of duplicates
- Core assumption: Medical knowledge can be effectively encoded from deduplicated clinical text
- Evidence anchors:
  - [abstract]: "deduplicating clinical text can help clinical LMs encode less redundant information in a more efficient manner"
  - [section 5]: "both LMs pretrained on WNBN and evaluated on NONE show a lower PPL score than the models pretrained on NONE and evaluated on WNBN"
  - [corpus]: Weak - no explicit corpus-level evidence provided
- Break condition: If the deduplication process removes critical context needed for medical knowledge acquisition

## Foundational Learning

- Concept: Language model pretraining and fine-tuning
  - Why needed here: Understanding how deduplication affects both pretraining efficiency and downstream task performance
  - Quick check question: How does the masked language modeling objective differ from next sentence prediction in terms of learning context?

- Concept: Clinical text characteristics and duplication patterns
  - Why needed here: Identifying different types of duplicates and their clinical relevance is crucial for the deduplication approach
  - Quick check question: What distinguishes within-note duplicates from between-note duplicates in clinical text?

- Concept: Classification with prompts and manual verbalizers
  - Why needed here: The smoking status classification task uses prompt-based learning, which requires understanding template-based classification
  - Quick check question: How does the manual verbalizer map model predictions to class labels in prompt-based learning?

## Architecture Onboarding

- Component map:
  - Data preprocessing pipeline (sentencizer, tokenizer) -> Deduplication algorithm (EXACT SUBSTR for between-note, custom for within-note) -> Relevance classifier (fine-tuned GatorTron model) -> Language model adaptation pipeline (further pretraining on deduplicated data) -> Classification pipeline (prompt-based learning for downstream tasks)

- Critical path: Data → Preprocessing → Deduplication → Relevance Classification → Model Adaptation → Evaluation

- Design tradeoffs:
  - Aggressive deduplication (WNBN) reduces training cost but may remove useful context
  - Conservative deduplication (WN) preserves more information but offers less efficiency gain
  - Fine-tuned GatorTron for relevance classification adds complexity but improves precision

- Failure signatures:
  - Performance drop when evaluating on original (NONE) data after WNBN pretraining
  - Relevance classifier precision below 0.95
  - Perplexity increase when evaluating on deduplicated data after NONE pretraining

- First 3 experiments:
  1. Run the EXACT SUBSTR algorithm on MIMIC-III data with k=100 to identify between-note duplicates
  2. Fine-tune GatorTron on manually annotated between-note duplicates for relevance classification
  3. Compare perplexity of GatorTron pretrained on NONE vs WNBN configurations on a held-out dev set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of language models change when using deduplication on datasets from different languages or medical specialties?
- Basis in paper: [inferred] The paper notes that the analysis was only conducted on English clinical notes and does not generalize to other languages
- Why unresolved: The study did not explore datasets in other languages or medical specialties, so the generalizability of the deduplication benefits remains unknown
- What evidence would resolve it: Conducting similar deduplication experiments on multilingual clinical datasets and datasets from various medical specialties would provide evidence of generalizability

### Open Question 2
- Question: What are the potential impacts of deduplication on generative models like GPT, as opposed to encoder-based models like GatorTron and ClinicalBERT?
- Basis in paper: [explicit] The paper acknowledges that the conclusions are limited to further pretraining of already adapted clinical LMs and suggests extending the approach to generative models
- Why unresolved: The study focused on encoder-based models and did not explore the effects of deduplication on generative models
- What evidence would resolve it: Performing deduplication experiments on generative models and comparing their performance to encoder-based models would clarify the impact on different model architectures

### Open Question 3
- Question: How does the inclusion of temporal aspects in the relevance detection model affect the identification and removal of clinically not relevant duplicates?
- Basis in paper: [inferred] The paper suggests expanding the clinical relevance detection to include a temporal aspect at the patient level
- Why unresolved: The current model does not account for temporal relevance, which could lead to the retention of outdated but clinically relevant information
- What evidence would resolve it: Developing and testing a temporal relevance detection model would demonstrate the benefits of incorporating time-based filtering in deduplication practices

## Limitations
- Manual annotation process for clinically relevant duplicates showed only 82.8% agreement between annotators, suggesting inherent subjectivity
- Study focuses on English-language clinical notes from US-based healthcare systems, limiting generalizability
- Analysis restricted to structured ICU notes and discharge summaries, potentially missing duplication patterns in other clinical note types

## Confidence

**High Confidence Claims:**
- Clinical text contains substantial duplication, with between-note duplicates being the dominant form in larger datasets
- Removing clinically not relevant duplicates improves language model efficiency (lower perplexity)
- Deduplication does not harm and can improve downstream classification performance

**Medium Confidence Claims:**
- The four-tier deduplication framework comprehensively captures all clinically relevant information
- The GatorTron-based relevance classifier generalizes well across different clinical datasets
- The observed performance improvements are directly attributable to deduplication rather than other factors

**Low Confidence Claims:**
- The optimal deduplication strategy (WNBN) is universally applicable across all clinical NLP tasks
- Deduplication has equivalent benefits for all types of clinical language models and architectures
- The computational efficiency gains scale linearly with the degree of deduplication

## Next Checks

1. **Cross-institutional validation**: Test the deduplication framework on clinical notes from different healthcare systems and countries to assess generalizability. Specifically, evaluate whether the GatorTron relevance classifier maintains high precision (target: >90%) when applied to clinical notes from non-US institutions with different documentation practices.

2. **Downstream task expansion**: Evaluate the deduplication approach on a broader range of clinical NLP tasks beyond smoking status classification, including named entity recognition, relation extraction, and clinical decision support tasks. Target metrics: maintain or improve F1 scores across at least 3 additional clinical classification tasks while achieving >15% perplexity reduction.

3. **Dynamic deduplication assessment**: Implement an ablation study that progressively removes different categories of duplicates (within-note, between-note relevant, between-note not relevant) and measures the impact on both pretraining efficiency and downstream performance. Target finding: identify the minimal deduplication level that achieves 80% of the maximum perplexity improvement with minimal impact on classification F1 scores.