---
ver: rpa2
title: 'LMC: Fast Training of GNNs via Subgraph Sampling with Provable Convergence'
arxiv_id: '2302.00924'
source_url: https://arxiv.org/abs/2302.00924
tags:
- sampling
- mini-batch
- graph
- have
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficient training of graph
  neural networks (GNNs) on large-scale graphs, which is hampered by the neighbor
  explosion problem. The authors propose Local Message Compensation (LMC), a novel
  subgraph-wise sampling method that retrieves discarded messages in backward passes
  based on a message passing formulation.
---

# LMC: Fast Training of GNNs via Subgraph Sampling with Provable Convergence

## Quick Facts
- arXiv ID: 2302.00924
- Source URL: https://arxiv.org/abs/2302.00924
- Authors: 
- Reference count: 40
- Key outcome: Proposes Local Message Compensation (LMC) for efficient GNN training with provable convergence, significantly outperforming state-of-the-art subgraph-wise sampling methods

## Executive Summary
This paper addresses the neighbor explosion problem in large-scale GNN training by proposing Local Message Compensation (LMC), a subgraph-wise sampling method that retrieves discarded messages in backward passes using a message passing formulation. LMC efficiently compensates for these messages using a combination of incomplete up-to-date messages and historical information, enabling accurate mini-batch gradients and accelerated convergence. The method is the first subgraph-wise sampling approach with provable convergence guarantees, demonstrating significant efficiency improvements over state-of-the-art methods while maintaining comparable prediction performance to full-batch approaches.

## Method Summary
LMC is a subgraph-wise sampling method for efficient GNN training that addresses the neighbor explosion problem by compensating for discarded messages outside mini-batches. The method uses historical embeddings and auxiliary variables to approximate exact values during forward and backward passes, computing accurate mini-batch gradients while avoiding exponential complexity growth. LMC restricts message passing to mini-batches and their 1-hop neighbors, using a one-shot sampling approach rather than recursive neighborhood expansion. The method converges to first-order stationary points of GNNs and shows particular robustness under small batch sizes.

## Key Results
- LMC significantly outperforms state-of-the-art subgraph-wise sampling methods in efficiency while maintaining comparable prediction performance to full-batch methods
- LMC demonstrates particular robustness under small batch sizes, where it outperforms baselines and resembles full-batch performance
- Experiments on large-scale benchmark tasks (PPI, REDDIT, FLICKR, Ogbn-arxiv) validate LMC's effectiveness and convergence properties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LMC computes accurate mini-batch gradients by retrieving discarded messages in backward passes using a message passing formulation
- Mechanism: LMC uses a combination of incomplete up-to-date messages and historical information to compensate for discarded messages, enabling accurate gradient estimation while avoiding exponential complexity growth
- Core assumption: Historical embeddings and auxiliary variables can serve as affordable approximations of exact values when combined with incomplete up-to-date information
- Evidence anchors:
  - [abstract]: "The key idea of LMC is to retrieve the discarded messages in backward passes based on a message passing formulation of backward passes"
  - [section]: "By efficient and effective compensations for the discarded messages in both forward and backward passes, LMC computes accurate mini-batch gradients and thus accelerates convergence"
  - [corpus]: Weak evidence - only 25 related papers found, but none specifically address this compensation mechanism
- Break condition: When historical values become too stale relative to current parameters, or when graph structure changes rapidly between iterations

### Mechanism 2
- Claim: LMC converges to first-order stationary points of GNNs by correcting gradient biases through local message compensation
- Mechanism: The method maintains historical values for nodes outside mini-batches and updates them partially using current information, reducing bias in gradient estimates
- Core assumption: The bias reduction from local message compensation is sufficient to ensure convergence despite using approximate values
- Evidence anchors:
  - [abstract]: "We further show that LMC converges to first-order stationary points of GNNs"
  - [section]: "By efficient and effective compensations for the discarded messages with a combination of incomplete up-to-date messages and messages generated from historical information in previous iterations"
  - [corpus]: Weak evidence - no corpus papers provide direct convergence analysis for similar compensation approaches
- Break condition: If learning rate or convex combination coefficients are not properly tuned, bias correction may be insufficient

### Mechanism 3
- Claim: LMC maintains scalability by restricting message passing to mini-batches and their 1-hop neighbors
- Mechanism: Instead of recursive neighborhood expansion, LMC uses a one-shot sampling approach that processes only immediate neighbors of sampled nodes
- Core assumption: Information from 1-hop neighbors is sufficient for effective gradient estimation in most practical scenarios
- Evidence anchors:
  - [abstract]: "Unlike the recursive fashion, subgraph-wise sampling methods adopt a cheap and simple one-shot sampling fashion"
  - [section]: "By discarding messages outside the mini-batches, subgraph-wise sampling methods restrict message passing to the mini-batches such that the complexity grows linearly with the number of MP layers"
  - [corpus]: Moderate evidence - several subgraph-wise sampling papers support the one-shot approach, though with different compensation strategies
- Break condition: For tasks requiring long-range dependencies or very deep GNNs, 1-hop neighborhood information may be insufficient

## Foundational Learning

- Concept: Message Passing Neural Networks
  - Why needed here: LMC is built specifically for GNNs and relies on understanding how messages propagate through graph structures
  - Quick check question: Can you explain how node embeddings are updated through aggregation and combination steps in a standard GNN layer?

- Concept: Mini-batch Gradient Estimation
  - Why needed here: The paper addresses how to approximate full-batch gradients using mini-batches while maintaining accuracy
  - Quick check question: What are the trade-offs between computational efficiency and gradient accuracy when using mini-batch methods?

- Concept: Historical Value Approximation
  - Why needed here: LMC uses historical embeddings as a key component of its compensation strategy
  - Quick check question: How does the staleness of historical values affect the accuracy of gradient estimates in iterative optimization?

## Architecture Onboarding

- Component map:
  Core message passing engine -> Historical value storage -> Local message compensation modules -> Mini-batch sampling -> Parameter update

- Critical path:
  1. Sample mini-batch and construct subgraph
  2. Forward pass with local message compensation
  3. Backward pass with local message compensation
  4. Compute mini-batch gradients using compensated values
  5. Update parameters using computed gradients

- Design tradeoffs:
  - Accuracy vs. memory: Storing historical values improves accuracy but increases memory usage
  - Compensation vs. staleness: More aggressive compensation reduces staleness but may introduce other errors
  - Batch size vs. convergence: Smaller batches reduce memory pressure but may require more careful compensation tuning

- Failure signatures:
  - Slow convergence despite compensation: May indicate insufficient compensation strength or inappropriate learning rate
  - High variance in gradient estimates: Could suggest inadequate sampling or poor historical value quality
  - Memory overflow: May require reducing batch size or historical value storage depth

- First 3 experiments:
  1. Compare convergence speed of LMC vs. GAS on Ogbn-arxiv with default settings
  2. Test robustness by varying batch sizes from 1 to 10 on the same dataset
  3. Measure GPU memory usage and training time per epoch across different methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of LMC scale with the number of layers in the GNN?
- Basis in paper: [inferred] The paper discusses convergence guarantees but does not explicitly analyze how the number of layers impacts the convergence speed or rate
- Why unresolved: The theoretical analysis focuses on proving convergence to first-order stationary points but does not quantify the convergence rate as a function of the number of layers. The experiments demonstrate effectiveness but do not isolate the impact of depth on convergence
- What evidence would resolve it: A theoretical analysis showing the dependence of convergence rate on the number of layers, or experiments systematically varying the number of layers and measuring convergence speed

### Open Question 2
- Question: How does LMC perform on graphs with heterogeneous node features or varying feature dimensions?
- Basis in paper: [inferred] The paper focuses on node-level prediction tasks and assumes uniform feature dimensions. There is no explicit discussion of handling heterogeneous features or varying feature dimensions across nodes
- Why unresolved: The current analysis and experiments assume homogeneous features, which is a common assumption but not always realistic. Handling heterogeneity would be important for real-world applications with diverse data types
- What evidence would resolve it: Experiments on datasets with heterogeneous node features or varying feature dimensions, comparing LMC's performance to other methods in these scenarios

### Open Question 3
- Question: What is the impact of the graph sampling strategy (e.g., random vs. importance sampling) on LMC's performance?
- Basis in paper: [explicit] The paper mentions using uniform sampling for mini-batches but does not explore alternative sampling strategies like importance sampling
- Why unresolved: While the paper demonstrates the effectiveness of LMC with uniform sampling, it does not investigate whether alternative sampling strategies could further improve performance or convergence speed
- What evidence would resolve it: Experiments comparing LMC's performance using different sampling strategies (e.g., uniform vs. importance sampling) on the same datasets, measuring both prediction accuracy and convergence speed

## Limitations
- The paper provides limited empirical validation of theoretical convergence guarantees, relying on assumptions that may not hold in practice
- Performance under extreme cases (very small batch sizes or very deep GNNs) remains unclear from the current experiments
- The robustness claims under small batch sizes are primarily demonstrated on a single dataset (Ogbn-arxiv)

## Confidence
- **High confidence**: The mechanism of using historical values for message compensation is technically sound and well-explained
- **Medium confidence**: The theoretical convergence guarantees are established but rely on assumptions that need empirical verification  
- **Low confidence**: The robustness claims under small batch sizes are primarily demonstrated on a single dataset (Ogbn-arxiv)

## Next Checks
1. **Convergence validation**: Run long-term training experiments (100+ epochs) on multiple datasets to empirically verify that LMC consistently reaches first-order stationary points
2. **Hyperparameter sensitivity**: Systematically vary the convex combination coefficient βᵢ and learning rate η to map their impact on convergence speed and stability
3. **Extreme case testing**: Evaluate LMC with batch size = 1 and GNN depth = 10+ layers to test robustness boundaries and identify potential failure modes