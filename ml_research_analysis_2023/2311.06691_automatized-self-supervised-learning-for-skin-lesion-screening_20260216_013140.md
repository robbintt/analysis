---
ver: rpa2
title: Automatized Self-Supervised Learning for Skin Lesion Screening
arxiv_id: '2311.06691'
source_url: https://arxiv.org/abs/2311.06691
tags:
- lesions
- patient
- sensitivity
- skin
- dermatologists
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of early melanoma detection by
  developing an AI decision support tool for identifying suspicious skin lesions,
  or "ugly ducklings," from total body images. The tool uses a state-of-the-art object
  detection algorithm to locate and extract skin lesions, which are then sorted by
  suspiciousness using a self-supervised AI approach.
---

# Automatized Self-Supervised Learning for Skin Lesion Screening

## Quick Facts
- arXiv ID: 2311.06691
- Source URL: https://arxiv.org/abs/2311.06691
- Reference count: 15
- Primary result: AI-assisted ugly duckling detection achieved 93% sensitivity and increased dermatologist agreement to 100% in a clinical validation study.

## Executive Summary
This study presents an AI decision support tool for identifying suspicious skin lesions ("ugly ducklings") from total body images to aid in early melanoma detection. The tool combines a state-of-the-art object detection algorithm (YOLOR) to locate skin lesions with a self-supervised AI approach (DINO) to sort lesions by suspiciousness. A clinical validation study demonstrated that the tool increased dermatologists' confidence and agreement on UD selection, with average majority agreement reaching 100% when assisted by AI. The development aims to address specialist shortages and improve patient outcomes through AI-assisted screening.

## Method Summary
The approach uses YOLOR to detect and extract skin lesions from total body images, followed by DINO self-supervised learning to embed lesions and score them based on their deviation from the median appearance within each patient's context. The model trains only on lesions from the current patient to avoid inter-patient bias. Clinical validation was performed with 20 patients, comparing top-10 AI-identified UDs against expert dermatologist selections and measuring confidence levels.

## Key Results
- Achieved 93% average sensitivity for top-10 AI-identified UDs compared to expert majority selection
- Increased dermatologists' confidence in diagnosis with AI assistance
- Reached 100% average majority agreement with top-10 AI-identified UDs when assisted by AI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised learning with DINO enables patient-specific ugly duckling (UD) detection without manual labeling.
- Mechanism: The model trains embeddings only on lesions from the current patient, using a pretext task (random crop + augmentation) to learn discriminative features. Lesions that are outliers in this patient-specific latent space are ranked as suspicious.
- Core assumption: Lesions in a single patient are more similar to each other than to lesions from other patients, so patient-specific training avoids inter-patient bias.
- Evidence anchors:
  - [abstract] "self-supervised AI approach, tailored to the specific context of the patient under examination"
  - [section] "We trained the model directly and only on the test patient's lesions to avoid possible biases that could arise from the different patient's context"
  - [corpus] Weak: No direct comparison with supervised UD detection in corpus; DINO is mentioned but not in melanoma context.
- Break condition: If patients have heterogeneous lesion patterns (e.g., many distinct nevus types), patient-specific clustering may misclassify normal variation as suspicious.

### Mechanism 2
- Claim: High recall in skin lesion detection ensures no suspicious lesions are missed, enabling reliable downstream UD scoring.
- Mechanism: The YOLOR detector is tuned for small objects and high-resolution input, using IoU-aware NMS post-processing to preserve overlapping lesions. A 20% confidence threshold balances precision/recall.
- Core assumption: Most true UD lesions will be detected above the confidence threshold, and missed lesions are rare and non-critical.
- Evidence anchors:
  - [section] "The approach used for lesion detection resulted in high recall rates for all four test subjects, reaching 95% with an IoU threshold of 50%"
  - [section] "Recall and Precision values are given for a selected confidence threshold of 20%"
  - [corpus] Weak: No ablation of threshold choice or comparison with other detectors in corpus.
- Break condition: If lesions are very small or occluded (e.g., by hair), recall may drop below 95%, reducing sensitivity for UDs.

### Mechanism 3
- Claim: AI assistance improves dermatologists' confidence and agreement on UD selection, approaching 100% consensus with top-10 AI UDs.
- Mechanism: The AI ranks lesions by outlier score, and presenting the top-10 visually (red boxes) guides clinicians to lesions they might otherwise overlook, reducing subjectivity.
- Core assumption: Clinicians trust and adopt AI suggestions when shown alongside their own view.
- Evidence anchors:
  - [abstract] "The study also found that dermatologists confidence increased, and the average majority agreement with the top-10 AI-identified UDs improved to 100% when assisted by AI"
  - [section] "We see a clear upward trend in the average [confidence level] for all groups"
  - [corpus] Weak: No comparison with other AI-assisted melanoma tools in corpus.
- Break condition: If AI suggestions are not trusted or conflict with clinician intuition, adoption and confidence gains may not materialize.

## Foundational Learning

- Concept: Object detection and evaluation metrics (IoU, AP, AR)
  - Why needed here: The skin lesion detector must reliably localize small lesions in high-res images; metrics guide model choice and tuning.
  - Quick check question: What IoU threshold balances recall and precision for small lesions in this application?
- Concept: Self-supervised learning and contrastive embeddings
  - Why needed here: DINO learns patient-specific lesion representations without labels, enabling UD scoring based on outlierness.
  - Quick check question: Why does training only on a single patient's lesions reduce bias compared to pooled datasets?
- Concept: Clinical study design and sensitivity metrics
  - Why needed here: Validating AI performance requires clear definitions of TP/FN/P with respect to expert selection, and measuring confidence changes.
  - Quick check question: How is "majority selection of experts" defined and why is it used as ground truth?

## Architecture Onboarding

- Component map: YOLOR detector -> lesion extraction -> preprocessing -> DINO embedding -> UD scoring -> clinical UI
- Critical path: Detector (must have high recall) -> Embedding (must be patient-specific) -> Scoring (must rank UDs high) -> UI (must convey suggestions clearly)
- Design tradeoffs:
  - Detector: High recall vs. computational cost (128px input chosen for speed)
  - Embedding: Patient-specific vs. generalizability (risk of overfitting to single patient)
  - UI: Detailed lesion boxes vs. clinician cognitive load
- Failure signatures:
  - Low recall -> missed suspicious lesions
  - Poor embedding -> UDs not ranked high
  - UI confusion -> reduced clinician adoption
- First 3 experiments:
  1. Run detector on a patient with known UDs, verify >95% recall and lesion coverage
  2. Train DINO on one patient, plot t-SNE, check if top-10 UDs visually stand out
  3. Simulate clinician UI with AI suggestions, measure agreement vs. no-AI baseline

## Open Questions the Paper Calls Out

- Question: How does the AI model perform in detecting melanomas in body regions other than the dorsal area, and how does it generalize across different skin tones and lesion types?
- Basis in paper: [explicit] The paper mentions that the current dataset is limited to the dorsal region of 20 patients and that future steps include validating the model in different body regions and on more patients.
- Why unresolved: The study's dataset and validation are currently restricted to the dorsal region, which may not represent the diversity of melanoma presentations across different body parts and skin tones.
- What evidence would resolve it: Expanding the dataset to include various body regions and diverse skin tones, followed by comprehensive validation studies, would provide evidence on the model's generalizability and performance.

- Question: What is the impact of AI assistance on the diagnostic accuracy and confidence levels of non-expert users, such as nurses or technicians, in real-world clinical settings?
- Basis in paper: [explicit] The paper discusses the potential for non-experts to reach similar sensitivity values as experts with AI assistance, but it is based on the current study's clinical validation involving participants with varying expertise levels.
- Why unresolved: The study's clinical validation was conducted in a controlled environment with specific participants, and it is unclear how non-expert users would perform in actual clinical settings without the same level of guidance and oversight.
- What evidence would resolve it: Conducting field studies in real-world clinical settings with non-expert users, such as nurses or technicians, and measuring their diagnostic accuracy and confidence levels with and without AI assistance would provide evidence on the practical impact of the tool.

- Question: How does the AI model handle patients with significant amounts of hair or tattoos, which were identified as challenging cases in the study?
- Basis in paper: [explicit] The paper mentions that patients with significant amounts of hair presented challenges in both detection and UD characterization, and that this is a limitation that needs to be addressed in the future.
- Why unresolved: The current model's performance with patients having significant hair or tattoos was not thoroughly evaluated, and the paper suggests that these cases were not adequately handled.
- What evidence would resolve it: Developing and testing the model on a dataset that includes patients with significant hair or tattoos, and evaluating its performance in these challenging cases, would provide evidence on the model's robustness and ability to handle diverse patient presentations.

## Limitations

- Limited dataset: Validation restricted to 20 patients focusing on dorsal region
- Unknown configurations: Specific hyperparameters for YOLOR and DINO models not detailed
- Challenging cases: Poor performance with patients having significant hair or tattoos

## Confidence

- **High confidence** in the technical feasibility of the approach (YOLOR + DINO) for patient-specific UD detection, supported by reported recall and sensitivity metrics.
- **Medium confidence** in the clinical impact, as the study shows improved agreement and confidence but lacks a control group without AI assistance.
- **Low confidence** in generalizability, given the limited patient sample (20 patients) and lack of external validation.

## Next Checks

1. **Detector robustness**: Test YOLOR on a diverse set of high-resolution images with challenging conditions (hair, tattoos, small lesions) to confirm >95% recall.
2. **Self-supervised embedding quality**: Train DINO on a patient with known UDs, visualize embeddings via t-SNE, and verify that UDs are correctly identified as outliers.
3. **Clinical utility**: Conduct a randomized controlled trial comparing dermatologist performance with and without AI assistance on the same patient set.