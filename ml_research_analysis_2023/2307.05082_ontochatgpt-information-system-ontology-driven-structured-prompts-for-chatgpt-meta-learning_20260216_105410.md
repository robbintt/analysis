---
ver: rpa2
title: 'OntoChatGPT Information System: Ontology-Driven Structured Prompts for ChatGPT
  Meta-Learning'
arxiv_id: '2307.05082'
source_url: https://arxiv.org/abs/2307.05082
tags:
- chatgpt
- system
- information
- prompt
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents a novel methodology for leveraging ontology-driven
  structured prompts in ChatGPT to enhance chatbot systems. It develops formal information
  and functional models, and implements the OntoChatGPT system, demonstrating its
  effectiveness in extracting entities, classifying them, and generating relevant
  responses within the domain of rehabilitation using the Ukrainian language.
---

# OntoChatGPT Information System: Ontology-Driven Structured Prompts for ChatGPT Meta-Learning

## Quick Facts
- **arXiv ID**: 2307.05082
- **Source URL**: https://arxiv.org/abs/2307.05082
- **Reference count**: 40
- **Key outcome**: Ontology-driven structured prompts achieved accuracy of 0.7059, precision of 0.6534, recall of 0.9444, and F1 score of 0.7724 in rehabilitation domain using Ukrainian language

## Executive Summary
The OntoChatGPT system introduces a novel methodology for enhancing chatbot performance through ontology-driven structured prompts in ChatGPT. By combining prompt engineering, meta-learning, and ontology-driven information retrieval, the system effectively extracts entities, classifies them, and generates relevant responses within specific domains. The approach demonstrates versatility for various LLM-based systems and achieves promising results in the rehabilitation domain using Ukrainian language, with potential applications across different languages and domains.

## Method Summary
The methodology involves developing formal information and functional models, then implementing an ontology-driven system that uses structured JSON prompts with ChatGPT. The system extracts named entities from user messages, selects relevant contexts from a context ontology based on semantic roles, and generates structured prompts that guide ChatGPT's response generation. A meta-ontology is developed through iterative refinement to create generalizable prompt patterns. The approach was tested using Ukrainian text from the "White Book on Physical and Rehabilitation Medicine in Europe" and evaluated using accuracy, precision, recall, and F1 score metrics.

## Key Results
- Achieved accuracy of 0.7059, precision of 0.6534, recall of 0.9444, and F1 score of 0.7724
- Effectively extracts entities from contexts and classifies them for response generation
- Demonstrates versatility for various LLM-based systems beyond ChatGPT
- Successfully implemented in Ukrainian language for rehabilitation domain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ontology-driven structured prompts enable context-specific knowledge retrieval beyond ChatGPT's general knowledge
- Mechanism: Contexts linked to named entities and intents are selected from an ontology, then passed to ChatGPT with structured prompts that instruct it to extract and synthesize relevant information
- Core assumption: The ontology can accurately capture domain-specific relationships between entities and contexts
- Evidence anchors:
  - [abstract] "The OntoChatGPT system effectively extracts entities from contexts, classifies them, and generates relevant responses."
  - [section] "The system selects the appropriate contexts from the contexts ontology based on the extracted named entities and their semantic roles."
  - [corpus] Weak - neighbors focus on general chatbot development but don't mention ontology-driven context retrieval specifically
- Break condition: Ontology becomes too large or complex, causing selection accuracy to degrade or prompts to exceed token limits

### Mechanism 2
- Claim: Structured JSON prompts with predefined fields and templates improve ChatGPT's response consistency and processing efficiency
- Mechanism: Prompts include explicit fields for information to provide, input/output formats, and intent classification, constraining ChatGPT's output to match expected data structures
- Core assumption: ChatGPT can reliably interpret and follow structured prompt formats with clear field definitions
- Evidence anchors:
  - [abstract] "The underlying principles of meta-learning, structured prompts, and ontology-driven information retrieval form the core of the proposed methodology."
  - [section] "The use of structured JSON prompts increases their reliability and facilitates obtaining relevant answers."
  - [corpus] Weak - corpus neighbors mention prompt engineering but not structured JSON specifically
- Break condition: ChatGPT's interpretation of JSON structure becomes inconsistent or prompt field requirements exceed token limits

### Mechanism 3
- Claim: Meta-learning through iterative prompt refinement creates a meta-ontology that guides prompt generation for different scenarios
- Mechanism: Knowledge engineers iteratively test prompts with ChatGPT, refining field definitions and prompt phrases until responses meet objectives, then formalize these patterns in a meta-ontology
- Core assumption: Iterative refinement with ChatGPT can produce generalizable prompt patterns that improve across different use cases
- Evidence anchors:
  - [abstract] "The resulting productive triad comprises the methodological foundations, advanced information technology, and the OntoChatGPT system."
  - [section] "The creation of the meta-ontology is a manual process... described based on different anticipated situations and objectives for the problem-solving task."
  - [corpus] Weak - corpus neighbors discuss prompt engineering but not meta-ontology development specifically
- Break condition: Meta-ontology becomes too rigid or doesn't generalize well to new domains

## Foundational Learning

- Concept: Ontology Engineering
  - Why needed here: Provides the structured knowledge framework that links contexts to named entities and enables domain-specific information retrieval
  - Quick check question: What is the difference between a class and an instance in an ontology, and why does this distinction matter for context selection?

- Concept: Prompt Engineering
  - Why needed here: Enables creation of structured prompts that guide ChatGPT's behavior and constrain its output to match expected formats
  - Quick check question: How do you balance prompt specificity with token limit constraints when designing structured prompts?

- Concept: Meta-Learning
  - Why needed here: Provides the iterative framework for refining prompts and developing the meta-ontology that guides prompt generation
  - Quick check question: What metrics would you track to evaluate whether meta-learning is improving prompt effectiveness over time?

## Architecture Onboarding

- Component map:
  - User message → intent detection → named entity extraction → context selection → structured prompt generation → ChatGPT processing → response formatting

- Critical path: User message → intent detection → named entity extraction → context selection → structured prompt generation → ChatGPT processing → response formatting

- Design tradeoffs:
  - Ontology complexity vs. selection speed
  - Prompt field comprehensiveness vs. token limits
  - Meta-ontology flexibility vs. consistency guarantees
  - Context relevance vs. context quantity

- Failure signatures:
  - False positives: ChatGPT attempts to answer when insufficient context exists
  - False negatives: Relevant contexts exist but aren't selected due to entity mismatch
  - Intent misclassification: User message interpreted with wrong intent, leading to irrelevant contexts
  - Token limit exceeded: Prompts too large for ChatGPT processing

- First 3 experiments:
  1. Test context selection accuracy: Input known entities and verify correct contexts are selected from ontology
  2. Test prompt generation: Create structured prompts for different intents and verify ChatGPT responses match expected formats
  3. Test end-to-end flow: Run complete dialogue system with sample user messages and evaluate response quality against ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed methodology perform on datasets from other domains and languages beyond rehabilitation in Ukrainian?
- Basis in paper: [explicit] The paper mentions the versatility of the methodology for different domains and languages, but testing was limited to rehabilitation in Ukrainian
- Why unresolved: The paper only presents results for one specific domain and language, leaving open questions about generalizability
- What evidence would resolve it: Testing the OntoChatGPT system on diverse datasets from various domains (e.g., healthcare, finance, education) and languages, with detailed performance metrics

### Open Question 2
- Question: What is the impact of incorporating additional contextual information, such as sentiment analysis, on the system's performance?
- Basis in paper: [explicit] The paper mentions the potential for incorporating sentiment analysis to categorize contexts and improve relevance, but does not explore this in depth
- Why unresolved: The paper does not investigate the effect of sentiment analysis or other contextual information on the system's accuracy and precision
- What evidence would resolve it: Experiments comparing the system's performance with and without sentiment analysis, as well as with different types of contextual information

### Open Question 3
- Question: How does the proposed methodology compare to other approaches for enhancing chatbot performance, such as fine-tuning or using different LLM architectures?
- Basis in paper: [inferred] The paper does not provide a direct comparison with other methods for improving chatbot systems
- Why unresolved: The paper focuses on the proposed methodology but does not benchmark it against alternative approaches
- What evidence would resolve it: Comparative studies evaluating the OntoChatGPT system against other state-of-the-art methods for enhancing chatbot performance, using standardized datasets and metrics

## Limitations

- Low confidence in generalizability across different languages and domains beyond tested rehabilitation in Ukrainian
- Token limit constraints may prevent scaling to complex ontologies with comprehensive context coverage
- Significant manual effort required for ontology maintenance and meta-ontology development

## Confidence

- High confidence: Structured JSON prompts effectively constrain ChatGPT outputs
- Medium confidence: Ontology-driven context selection improves response relevance within tested domain
- Low confidence: Meta-learning approach generalizes across domains and languages

## Next Checks

1. Cross-domain testing: Evaluate system performance with different subject domains (e.g., legal, medical, technical) to assess generalizability beyond rehabilitation
2. Language adaptation testing: Measure performance degradation when adapting the system to languages other than Ukrainian, particularly focusing on named entity extraction accuracy
3. Token efficiency analysis: Systematically test how prompt complexity scales with ontology size and identify breaking points where token limits become prohibitive