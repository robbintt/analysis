---
ver: rpa2
title: 'Evaluating AI systems under uncertain ground truth: a case study in dermatology'
arxiv_id: '2307.02191'
source_url: https://arxiv.org/abs/2307.02191
tags:
- annotation
- uncertainty
- truth
- ground
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of uncertain ground truth in medical
  AI evaluation, particularly in dermatology. The authors propose a statistical aggregation
  framework that models the distribution of possible ground truth labels based on
  multiple expert annotations, rather than deterministically aggregating them.
---

# Evaluating AI systems under uncertain ground truth: a case study in dermatology

## Quick Facts
- arXiv ID: 2307.02191
- Source URL: https://arxiv.org/abs/2307.02191
- Reference count: 40
- Key outcome: Standard evaluation methods overestimate classifier performance when ground truth uncertainty exists

## Executive Summary
This paper addresses the critical issue of uncertain ground truth in medical AI evaluation, particularly in dermatology. The authors propose a statistical aggregation framework that models the distribution of possible ground truth labels based on multiple expert annotations, rather than deterministically aggregating them. This approach captures both annotation uncertainty (disagreement among experts) and inherent uncertainty (ambiguity in the data). Applied to skin condition classification, their framework reveals significant ground truth uncertainty in a large portion of the dataset. Standard evaluation methods that ignore this uncertainty severely overestimate classifier performance. The proposed uncertainty-adjusted metrics provide more realistic performance estimates and highlight that classifiers often perform poorly when considering more than just the top-1 predicted label.

## Method Summary
The framework infers a distribution over plausibilities of medical conditions from multiple expert annotations using two statistical aggregation models: Plackett-Luce (PL) and probabilistic IRN (PrIRN). These models treat aggregation as posterior inference over a distribution of plausibilities given annotations and observations. The framework computes annotation certainty to quantify uncertainty in specific labels being ground truth and evaluates classifiers using uncertainty-adjusted metrics (UA-Accuracy, UA-SetAccuracy, UA-AverageOverlap) that average performance over samples from the posterior distribution. Applied to a dermatology dataset with 16,225 cases and 419 skin conditions, the method reveals significant ground truth uncertainty that standard evaluation methods fail to capture.

## Key Results
- Standard evaluation methods overestimate classifier performance when ground truth uncertainty exists
- Uncertainty-adjusted metrics provide more realistic performance estimates by accounting for annotation uncertainty
- Classifier performance varies significantly across different annotator reliability scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Statistical aggregation models capture annotation uncertainty by modeling distributions over plausibilities rather than point estimates.
- **Mechanism:** The framework treats aggregation as posterior inference over a distribution of plausibilities (ðœ†) given annotations (ð‘) and observations (ð‘¥). This distribution reflects both the annotators' opinions and their reliability, with higher reliability concentrating the distribution.
- **Core assumption:** Annotator opinions are conditionally independent given the plausibilities, and the statistical model accurately represents the annotation process.
- **Evidence anchors:**
  - [abstract] "we propose a statistical aggregation approach, where we infer a distribution on probabilities of underlying medical condition candidates themselves, based on observed annotations."
  - [section 2.2] "We propose here a general framework for measuring and evaluating with ground truth uncertainty based on statistical aggregation of annotations."
- **Break condition:** If annotators are not conditionally independent given the plausibilities, or if the statistical model is misspecified, the posterior over plausibilities will not accurately reflect true annotation uncertainty.

### Mechanism 2
- **Claim:** Uncertainty-adjusted metrics provide more realistic performance estimates by accounting for annotation uncertainty.
- **Mechanism:** Metrics like uncertainty-adjusted top-k accuracy are computed by averaging performance over samples from the posterior distribution of plausibilities, rather than using a single point estimate. This reflects the probability that the prediction set contains the ground truth label, considering annotation uncertainty.
- **Core assumption:** The posterior distribution over plausibilities accurately captures the uncertainty in the ground truth labels.
- **Evidence anchors:**
  - [abstract] "standard evaluation methods that ignore this uncertainty severely overestimate classifier performance."
  - [section 2.4] "Integrating this into the above definition of accuracy yields the proposed uncertainty-adjusted version."
- **Break condition:** If the posterior distribution over plausibilities does not accurately represent the true uncertainty, the uncertainty-adjusted metrics will not provide realistic performance estimates.

### Mechanism 3
- **Claim:** Annotation certainty quantifies the uncertainty in a specific label being the ground truth.
- **Mechanism:** Annotation certainty is computed as the expected probability that a specific label corresponds to the top-1 label from the plausibilities, given the annotations and the statistical model. Higher annotation certainty indicates less uncertainty in that label being the ground truth.
- **Core assumption:** The statistical model accurately captures the relationship between annotations and the underlying ground truth distribution.
- **Evidence anchors:**
  - [section 2.3] "We can observe annotator disagreement in many standard datasets... For many of these benchmarks, ground truth uncertainty is limited to a small fraction of examples while the ground truth of the majority of examples can be trusted."
  - [section 3.2] "We also observe that annotation certainty is strongly correlated between PL and PrIRN (correlation coefficient 0.9)."
- **Break condition:** If the statistical model does not accurately represent the annotation process, annotation certainty will not reliably quantify uncertainty in the ground truth labels.

## Foundational Learning

- **Concept: Bayesian inference**
  - Why needed here: The framework relies on inferring a posterior distribution over plausibilities given annotations, which is a Bayesian inference problem.
  - Quick check question: If we have a prior distribution over plausibilities and a likelihood function for the annotations given the plausibilities, how do we compute the posterior distribution?

- **Concept: Dirichlet distribution**
  - Why needed here: The statistical aggregation models use Dirichlet distributions to represent the posterior over plausibilities, as they are conjugate to the multinomial distribution used to model annotations.
  - Quick check question: What are the parameters of a Dirichlet distribution, and how do they relate to the mean and variance of the distribution?

- **Concept: Plackett-Luce model**
  - Why needed here: The Plackett-Luce model is used to model the likelihood of partial rankings given the plausibilities, which is necessary for the PL aggregation model.
  - Quick check question: How does the Plackett-Luce model define the probability of a ranking given the item weights (plausibilities)?

## Architecture Onboarding

- **Component map:** Annotations + Observations -> Statistical Aggregation Models (PL, PrIRN) -> Plausibility Distributions -> Uncertainty-Adjusted Metrics -> Classifier Evaluation

- **Critical path:**
  1. Given annotations and observations, infer plausibilities using a statistical aggregation model.
  2. Compute annotation certainty for each example.
  3. Evaluate classifiers using uncertainty-adjusted metrics based on the inferred plausibilities.

- **Design tradeoffs:**
  - Computational cost vs. accuracy: More complex statistical models may provide more accurate uncertainty estimates but require more computation.
  - Simplicity vs. flexibility: Simpler models may be easier to implement and interpret but may not capture all sources of uncertainty.

- **Failure signatures:**
  - High annotation certainty but low classifier performance: Indicates that the statistical model may not be capturing inherent uncertainty.
  - Low annotation certainty but high classifier performance: Suggests that the classifier may be overfitting to the training data.

- **First 3 experiments:**
  1. Compare the performance of PL and PrIRN models on a synthetic dataset with known ground truth uncertainty.
  2. Evaluate the impact of different reliability parameters on annotation certainty and classifier performance.
  3. Test the sensitivity of uncertainty-adjusted metrics to different choices of the statistical aggregation model.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal method to estimate annotator reliability when it cannot be easily quantified by domain experts or data?
- **Basis in paper:** [explicit] The paper treats annotator reliability as a free parameter during evaluation, noting that choosing or estimating reliability can be challenging or require additional information.
- **Why unresolved:** The paper acknowledges that selecting the right reliability parameter is difficult but does not provide a concrete methodology for estimating it. Different reliability values significantly impact the evaluated performance metrics, making this parameter crucial for meaningful evaluation.
- **What evidence would resolve it:** Experiments comparing different methods for estimating reliability (e.g., using a held-out validation set, domain expert elicitation, or data-driven approaches) and their impact on model selection and evaluation consistency.

### Open Question 2
- **Question:** How can the statistical aggregation model be extended to explicitly model the relationship between input observations and annotation uncertainty (i.e., moving beyond the simplified assumption p(Î»|b,x) = p(Î»|b))?
- **Basis in paper:** [inferred] The paper acknowledges that the current assumption limits the ability to disentangle annotation uncertainty from inherent uncertainty and makes it difficult to determine whether uncertainty stems from limited observational information or the annotation process itself.
- **Why unresolved:** While the paper mentions this limitation and suggests it would be valuable for informing relabeling decisions, it does not propose a specific method for incorporating input-dependent uncertainty into the model.
- **What evidence would resolve it:** Development and validation of a more complex model that incorporates input features into the aggregation process, followed by experiments showing improved disentanglement of uncertainty sources.

### Open Question 3
- **Question:** What is the impact of ground truth uncertainty on model training, and how can training objectives be adapted to account for this uncertainty?
- **Basis in paper:** [explicit] The paper notes that while related work focuses on training with disagreement, it still assumes certain ground truth for evaluation. The paper's focus is on evaluation rather than training.
- **Why unresolved:** The paper demonstrates the significant impact of ignoring ground truth uncertainty during evaluation but does not explore how accounting for this uncertainty during training might affect model performance and robustness.
- **What evidence would resolve it:** Experiments comparing models trained with standard loss functions against those trained with loss functions that incorporate uncertainty-adjusted targets, measuring performance on both standard and uncertainty-adjusted evaluation metrics.

## Limitations

- The empirical evaluation is limited to a single dermatology dataset with 6 dermatologists, which may not generalize to other medical domains
- The computational cost of the Gibbs sampling approach for the PL model and the approximation methods used for PrIRN may introduce errors that affect uncertainty estimates
- The framework makes significant assumptions about the independence of annotators and the validity of the statistical models for capturing ground truth uncertainty

## Confidence

**High Confidence:**
- Standard evaluation methods overestimate classifier performance when ground truth uncertainty exists
- Uncertainty-adjusted metrics provide more realistic performance estimates
- Classifier performance varies significantly across reliability scenarios

**Medium Confidence:**
- The PL and PrIRN models accurately capture annotation uncertainty
- Annotation certainty correlates with classifier performance
- The proposed framework generalizes to other medical domains

**Low Confidence:**
- The statistical models perfectly represent the annotation process
- Uncertainty-adjusted metrics fully account for all sources of uncertainty
- The framework handles multi-label cases without modification

## Next Checks

1. Validate the independence assumption by analyzing correlation between annotator judgments on the same examples
2. Test the framework on synthetic datasets with known ground truth uncertainty to verify accuracy of uncertainty estimates
3. Apply the framework to a different medical domain (e.g., radiology) to assess generalizability beyond dermatology