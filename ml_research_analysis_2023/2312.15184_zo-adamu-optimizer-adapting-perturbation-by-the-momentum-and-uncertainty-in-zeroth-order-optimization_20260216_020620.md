---
ver: rpa2
title: 'ZO-AdaMU Optimizer: Adapting Perturbation by the Momentum and Uncertainty
  in Zeroth-order Optimization'
arxiv_id: '2312.15184'
source_url: https://arxiv.org/abs/2312.15184
tags:
- mezo
- zo-adamu
- momentum
- optimization
- perturbation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZO-AdaMU, a new zeroth-order optimizer that
  adapts momentum and uncertainty to simulated perturbations in gradient estimation.
  The key idea is relocating momentum from the gradient to the perturbation itself,
  improving convergence stability and speed compared to prior methods like MeZO.
---

# ZO-AdaMU Optimizer: Adapting Perturbation by the Momentum and Uncertainty in Zeroth-order Optimization

## Quick Facts
- arXiv ID: 2312.15184
- Source URL: https://arxiv.org/abs/2312.15184
- Reference count: 10
- Key outcome: Introduces ZO-AdaMU, a zeroth-order optimizer that adapts momentum and uncertainty to simulated perturbations, improving convergence stability and speed compared to MeZO on NLP tasks.

## Executive Summary
This paper presents ZO-AdaMU, a novel zeroth-order optimizer designed for memory-efficient fine-tuning of large language models (LLMs). Unlike traditional adaptive optimizers that apply momentum to gradient estimates, ZO-AdaMU relocates momentum to the simulated perturbation itself, reducing gradient variance and improving convergence stability. The method also introduces adaptive uncertainty in the perturbation distribution and uses a simulated annealing schedule for key hyperparameters. Empirically, ZO-AdaMU outperforms MeZO and Adam on various NLP tasks, achieving better generalization and convergence. Theoretically, it demonstrates faster convergence rates and improved global convergence due to reduced gradient variance and better conditioning of the Hessian.

## Method Summary
ZO-AdaMU is a zeroth-order optimizer that estimates gradients using only forward passes (no backpropagation). It adapts simulated perturbations with momentum and uncertainty, using a mixture of zero-centered and momentum-centered Gaussian noise. The momentum is applied directly to the perturbation vector rather than the gradient estimate, which reduces variance. An adaptive uncertainty parameter controls the variance of the momentum-centered component, following a simulated annealing schedule. The optimizer also maintains exponential moving averages of squared perturbations for adaptive step size normalization. The method is designed for memory-efficient fine-tuning of LLMs, requiring only two forward passes per gradient estimate.

## Key Results
- ZO-AdaMU achieves better convergence speed and final performance than MeZO and Adam on NLP tasks (SST-2, RTE, COPA, SQuAD, BoolQ, DROP) with OPT-13B and RoBERTa-large.
- Theoretical analysis shows ZO-AdaMU has lower gradient variance and improved conditioning (reduced effective rank) of the Hessian compared to MeZO.
- The method demonstrates improved generalization and stability, particularly in few-shot learning settings.

## Why This Works (Mechanism)

### Mechanism 1
Relocating momentum from gradients to perturbations reduces gradient variance and improves convergence stability. By applying momentum smoothing directly to the perturbation vector used in gradient estimation, ZO-AdaMU reduces the high-variance stochastic gradient estimates inherent in zeroth-order optimization.

### Mechanism 2
Adaptive uncertainty in the perturbation distribution improves global convergence by controlling the effective rank of the Hessian. The mixture of zero-centered and momentum-centered Gaussian noise, with variance controlled by an adaptive parameter α, encourages better generalization by reducing the effective rank of the Hessian approximation.

### Mechanism 3
Simulated annealing of smoothing parameters β1 and β2 balances exploration and exploitation for faster convergence. Early in training, low β1 and β2 encourage exploration; middle phase increases them for momentum-driven convergence; final phase fixes them for stability.

## Foundational Learning

- Concept: Zeroth-order optimization (ZO) and simulated perturbation stochastic approximation (SPSA)
  - Why needed here: ZO-AdaMU is a ZO optimizer that uses SPSA to estimate gradients without backpropagation.
  - Quick check question: How does SPSA estimate a gradient using only forward passes, and why does it require two forward passes per gradient estimate?

- Concept: Effective rank of the Hessian and its impact on generalization
  - Why needed here: ZO-AdaMU claims to reduce the effective rank of the Hessian, which improves generalization.
  - Quick check question: What is the effective rank of a matrix, and how does it relate to the conditioning and generalization properties of a loss landscape?

- Concept: Simulated annealing in optimization
  - Why needed here: ZO-AdaMU uses a simulated annealing schedule to adapt momentum and uncertainty parameters over time.
  - Quick check question: How does a simulated annealing schedule typically balance exploration and exploitation in iterative optimization algorithms?

## Architecture Onboarding

- Component map: Perturbation generator -> Uncertainty scheduler -> Momentum smoother -> Gradient estimator -> Adaptive variance tracker -> Annealing controller -> Parameter updater

- Critical path: 1. Sample perturbation noise (momentum-centered + zero-centered). 2. Perturb parameters and compute forward pass losses. 3. Estimate gradient from loss differences. 4. Update momentum and variance trackers. 5. Update parameters using adaptive step size. 6. Anneal hyperparameters for next step.

- Design tradeoffs: Memory vs. convergence speed (storing perturbation momentum increases memory usage slightly but improves convergence); Exploration vs. exploitation (adaptive uncertainty and annealing balance these); Simplicity vs. performance (ZO-AdaMU adds complexity over MeZO but achieves better results).

- Failure signatures: Too small perturbations cause noisy gradient estimates and stalled convergence; too high momentum causes overshooting and oscillation; improper uncertainty annealing leads to premature convergence or excessive exploration.

- First 3 experiments: 1. Compare convergence curves of ZO-AdaMU vs. MeZO on Rosenbrock test function. 2. Test memory usage of ZO-AdaMU vs. MeZO on small LLM fine-tuning task. 3. Ablation study: Run ZO-AdaMU with fixed vs. annealed β1, β2, and α.

## Open Questions the Paper Calls Out

### Open Question 1
How does the adaptive uncertainty parameter alpha in ZO-AdaMU affect the convergence rate and generalization in different types of neural network architectures beyond LLMs? The paper focuses on LLMs, but the effect on other architectures is not explored.

### Open Question 2
What is the impact of the simulated annealing schedule for the smoothing parameters (beta1, beta2) on the convergence of ZO-AdaMU in non-stationary environments? The analysis assumes stationary loss functions, and the effect in dynamic environments is not addressed.

### Open Question 3
How does the performance of ZO-AdaMU compare to other gradient-free optimization methods, such as Bayesian optimization or evolutionary algorithms, in terms of convergence speed and solution quality? The paper does not include comparisons to these methods.

## Limitations

- The adaptive annealing schedule parameters (T1, T2, T3) are not precisely specified, which could affect reproducibility.
- Empirical validation relies heavily on NLP benchmarks where first-order methods are already strong baselines.
- The claim about effective rank reduction directly improving generalization is theoretically supported but not empirically verified.

## Confidence

- High confidence: ZO-AdaMU outperforms MeZO on NLP tasks in terms of convergence speed and final performance metrics.
- Medium confidence: Relocating momentum to perturbations effectively reduces gradient variance and improves stability.
- Medium confidence: Adaptive uncertainty and annealing schedule contribute to faster convergence compared to fixed-parameter methods.
- Low confidence: The claim about effective rank reduction directly improving generalization is theoretically supported but not empirically verified.

## Next Checks

1. Implement and test ZO-AdaMU on standard test functions (e.g., Rosenbrock, Sphere) to verify faster convergence and compare variance reduction against MeZO in controlled settings.

2. Perform an ablation study on the annealing schedule by running ZO-AdaMU with fixed vs. adaptive β1, β2, and α to quantify the impact of the simulated annealing mechanism on convergence and stability.

3. Measure the effective rank of the Hessian during training for both ZO-AdaMU and MeZO to empirically validate the theoretical claim that ZO-AdaMU achieves lower effective rank and better conditioning.