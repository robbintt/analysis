---
ver: rpa2
title: Stability Guarantees for Feature Attributions with Multiplicative Smoothing
arxiv_id: '2307.05902'
source_url: https://arxiv.org/abs/2307.05902
tags:
- stability
- feature
- methods
- features
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes multiplicative smoothing (MuS) as a method
  to provide stability guarantees for feature attribution methods like LIME and SHAP.
  The key idea is that MuS makes the model Lipschitz with respect to feature masking,
  which allows for provable stability guarantees.
---

# Stability Guarantees for Feature Attributions with Multiplicative Smoothing

## Quick Facts
- arXiv ID: 2307.05902
- Source URL: https://arxiv.org/abs/2307.05902
- Authors: 
- Reference count: 40
- Key outcome: Multiplicative smoothing (MuS) provides provable stability guarantees for feature attribution methods by making the model Lipschitz with respect to feature masking.

## Executive Summary
This paper introduces multiplicative smoothing (MuS) as a method to provide stability guarantees for feature attribution methods like LIME and SHAP. The key innovation is that MuS makes the model Lipschitz with respect to feature masking by randomly dropping features during inference. This allows for provable stability guarantees while maintaining good accuracy. The authors demonstrate MuS's effectiveness on vision (Vision Transformer, ResNet50) and language (RoBERTa) models, showing it outperforms standard smoothing techniques and can be integrated with any classifier and feature attribution method.

## Method Summary
The authors propose multiplicative smoothing (MuS) as a way to make classifiers Lipschitz with respect to feature masking. The method involves applying multiplicative noise to features during inference, which smooths the model such that small changes in feature masks cause bounded changes in output probabilities. They use a quantized noise distribution to enable efficient exact evaluation. The approach is evaluated on ImageNet1K for vision models and TweetEval for language models, using feature grouping to reduce dimensionality. Models are fine-tuned for 1 epoch with different λ values using Adam optimizer, and stability is evaluated using four attribution methods: SHAP, LIME, Integrated Gradients, and Vanilla Gradient Saliency.

## Key Results
- MuS provides provable stability guarantees for feature attributions by achieving sufficient Lipschitz conditions
- The method maintains good accuracy while providing strong stability on vision and language models
- MuS outperforms standard smoothing techniques and generalizes across different attribution methods
- Transformer architectures (Vision Transformer, RoBERTa) are more resilient to MuS than ResNet50

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiplicative smoothing (MuS) makes the classifier Lipschitz with respect to feature masking, which guarantees relaxed stability for feature attributions.
- Mechanism: By randomly dropping features during inference (masking with probability 1-λ), MuS smooths the classifier such that small changes in the feature mask cause bounded changes in the output probabilities.
- Core assumption: The classifier's output is bounded in [0,1] and the smoothing distribution maintains coordinate-wise Bernoulli with parameter λ.
- Evidence anchors:
  - [abstract] "We prove that relaxed variants of stability are guaranteed if the model is sufficiently Lipschitz with respect to the masking of features."
  - [section 3.2] Theorem 3.2: g(x,·) is λ-Lipschitz in the ℓ1 norm for all x ∈ X when using MuS.
  - [corpus] Neighbor paper "Pixel-level Certified Explanations via Randomized Smoothing" supports that Lipschitz smoothness enables certified stability.
- Break condition: If the classifier outputs probabilities outside [0,1] or the smoothing distribution deviates from the required Bernoulli structure, the Lipschitz guarantee fails.

### Mechanism 2
- Claim: The masking equivalence property allows stability properties of the smoothed classifier to be translated into guarantees for the original model-explanation pairing.
- Mechanism: When masking equivalence holds, g(x, α) = f(x ⊙ α), so stability conditions defined for f can be checked using g's second parameter.
- Core assumption: The smoothing operation must satisfy g(x ⊙ α, 1) = f(x ⊙ α) = g(x, α) for all x and α.
- Evidence anchors:
  - [section 3.1] "if masking equivalence holds, then we can rewrite stability properties involving f in terms of g's second parameter"
  - [section 2.3] "Lipschitz smoothness is in fact a stronger assumption than necessary, as besides α ⪰ φ(x) it also imposes guarantees on α ⪯ φ(x)"
  - [corpus] Neighbor paper "Certified ℓ2 Attribution Robustness via Uniformly Smoothed Attributions" uses similar equivalence to prove robustness.
- Break condition: If additive smoothing is used instead of multiplicative, masking equivalence can fail as shown in Proposition 3.1.

### Mechanism 3
- Claim: Structured dependency in the smoothing distribution enables efficient exact evaluation with low sample complexity.
- Mechanism: By using a seed vector v and quantization parameter q, the n-dimensional noise can be parametrized by a single random variable, reducing samples from 2^n to q.
- Core assumption: The noise distribution can be constructed to have coordinate-wise Bernoulli marginals while introducing structured dependence.
- Evidence anchors:
  - [section 3.3] Proposition 3.4 shows how to construct Lqv(λ) with q distinct values and Bernoulli marginals.
  - [abstract] "We show that MuS overcomes the theoretical limitations of standard smoothing techniques"
  - [corpus] Neighbor paper "Improved, deterministic smoothing for ℓ1 certified robustness" uses similar structured noise approaches.
- Break condition: If q is too small or the seed vector v is unlucky, the exact evaluation may not capture the true smoothed classifier behavior.

## Foundational Learning

- Concept: Lipschitz continuity
  - Why needed here: MuS relies on proving the smoothed classifier is Lipschitz with respect to feature masking to guarantee stability.
  - Quick check question: If a function f is L-Lipschitz, what is the maximum possible change in f(x) when x changes by δ?

- Concept: Randomized smoothing
  - Why needed here: MuS is inspired by and extends randomized smoothing techniques to achieve feature-masking Lipschitzness.
  - Quick check question: How does multiplicative noise differ from additive noise in randomized smoothing approaches?

- Concept: Feature attribution methods
  - Why needed here: The paper evaluates MuS with various feature attribution methods (LIME, SHAP, VGrad, IGrad) to demonstrate generality.
  - Quick check question: What is the key difference between SHAP and LIME in how they compute feature importance?

## Architecture Onboarding

- Component map:
  Base classifier h -> Smoothing module (MuS with Lqv(λ)) -> Attribution method φ -> Stability checker

- Critical path:
  1. Input x passes through smoothing module to generate f(x)
  2. Attribution method φ computes importance scores for x
  3. Binary mask φ(x) is iteratively constructed to satisfy stability
  4. Stability guarantees are computed using Lipschitz constants

- Design tradeoffs:
  - λ vs accuracy: Lower λ provides stronger stability but reduces classifier accuracy
  - q vs efficiency: Smaller q enables faster evaluation but may reduce precision
  - Feature grouping vs granularity: Larger groups improve efficiency but reduce explanation precision

- Failure signatures:
  - Accuracy drops significantly with lower λ values
  - Stability radii become too small to be useful
  - Attribution methods produce overly sparse or dense explanations

- First 3 experiments:
  1. Test MuS with simple linear classifier and synthetic data to verify Lipschitz guarantee
  2. Compare additive vs multiplicative smoothing on toy example where masking equivalence fails
  3. Evaluate stability-accuracy tradeoff on Vision Transformer with varying λ values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal noise distribution for MuS to achieve the best stability-accuracy trade-off?
- Basis in paper: [explicit] The paper mentions that MuS is parametric to the choice of noise distribution and that the quantized noise with q=64 is used in experiments.
- Why unresolved: The paper does not explore different noise distributions or their impact on stability and accuracy. It only uses a specific quantized noise.
- What evidence would resolve it: Experiments comparing different noise distributions (e.g., different q values, non-uniform distributions) and their effects on stability radii and accuracy.

### Open Question 2
- Question: How does the choice of feature attribution method affect the size and quality of explanations for stable models?
- Basis in paper: [explicit] The paper compares SHAP, LIME, IGrad, and VGrad, finding SHAP generally requires fewer features for stability.
- Why unresolved: While the paper compares these methods, it does not explore why certain methods yield sparser explanations or how to choose the best method for a given model and dataset.
- What evidence would resolve it: A deeper analysis of the properties of different attribution methods and their relationship to model characteristics and dataset properties.

### Open Question 3
- Question: Can the stability guarantees of MuS be extended to other types of explanations beyond binary feature attributions?
- Basis in paper: [inferred] The paper focuses on binary feature attributions, but the concept of stability could be relevant to other explanation types.
- Why unresolved: The paper does not discuss how MuS could be adapted for other explanation methods or what stability means in those contexts.
- What evidence would resolve it: Development of stability definitions and MuS adaptations for continuous feature attributions, counterfactual explanations, or other explanation types, followed by empirical validation.

## Limitations

- The stability guarantees rely on Lipschitz continuity, which requires careful implementation of the multiplicative smoothing distribution
- Masking equivalence may not hold perfectly in practice due to numerical precision issues
- The tradeoff between stability strength and classifier accuracy remains an empirical question that may vary across different model architectures and datasets

## Confidence

- High confidence: The theoretical framework for MuS achieving Lipschitz continuity is well-established and supported by proofs in the paper
- Medium confidence: The experimental results showing stability-accuracy tradeoffs are promising but limited to specific model-dataset combinations
- Medium confidence: The claim that MuS generalizes across different attribution methods (SHAP, LIME, IGrad, VGrad) is supported but not extensively validated

## Next Checks

1. Test MuS with simple linear classifier and synthetic data to verify Lipschitz guarantee under controlled conditions
2. Conduct ablation studies varying λ and q parameters to quantify their impact on stability vs accuracy tradeoff across multiple model architectures
3. Evaluate MuS on additional datasets and model types (e.g., smaller vision models, different language tasks) to test generalizability claims