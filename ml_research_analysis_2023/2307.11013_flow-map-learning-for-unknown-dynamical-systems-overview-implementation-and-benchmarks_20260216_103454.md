---
ver: rpa2
title: 'Flow Map Learning for Unknown Dynamical Systems: Overview, Implementation,
  and Benchmarks'
arxiv_id: '2307.11013'
source_url: https://arxiv.org/abs/2307.11013
tags:
- data
- time
- trajectory
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Flow map learning (FML) with deep neural networks provides an effective
  data-driven approach for modeling unknown dynamical systems, particularly when only
  partial observations are available. FML learns the mapping between consecutive time
  steps without explicit time variables, enabling long-term predictions even for systems
  with no known governing equations.
---

# Flow Map Learning for Unknown Dynamical Systems: Overview, Implementation, and Benchmarks

## Quick Facts
- arXiv ID: 2307.11013
- Source URL: https://arxiv.org/abs/2307.11013
- Reference count: 27
- Primary result: Flow map learning with deep neural networks enables accurate modeling of unknown dynamical systems, including partially observed cases, through state-to-state mappings without explicit time variables

## Executive Summary
Flow map learning (FML) presents a data-driven approach for modeling unknown dynamical systems using deep neural networks. The method learns the mapping between consecutive states rather than approximating states as functions of time, enabling long-term predictions even when system governing equations are unknown. FML incorporates memory terms for partially observed systems following the Mori-Zwanzig formulation and employs multi-step loss functions during training to enhance stability. The approach has been validated across various benchmark problems including linear, nonlinear, and chaotic systems, demonstrating accurate predictions for both fully and partially observed cases.

## Method Summary
FML uses deep neural networks to approximate the flow map operator G that maps system states from one time step to the next. The method first eliminates explicit time variables from trajectory data, then trains the network to minimize a multi-step loss function that averages prediction errors over K future time steps. For partially observed systems, memory terms are incorporated by including past states in the flow map input, following the Mori-Zwanzig formulation. The trained models are combined through ensemble averaging to improve stability and reproducibility. Key parameters include the memory step nM, multi-step loss parameter K, and ensemble size Nmodel.

## Key Results
- FML achieves accurate long-term predictions for linear systems (L63) with fully observed states
- The method successfully handles partially observed nonlinear systems (L63) using memory terms (nM=10)
- Chaotic systems (Lorenz) are accurately modeled with multi-step loss (K=5-10) and ensemble averaging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FML avoids explicit time variables to prevent extrapolation errors beyond training domain
- Mechanism: By removing absolute time and using relative time shifts, FML learns mappings between consecutive states rather than approximating x(t) as a function of t
- Core assumption: The dynamics can be captured through state-to-state mappings without explicit temporal parameterization
- Evidence anchors:
  - [abstract] "A remarkable feature of FML is that it is capable of producing accurate predictive models for partially observed systems, even when their exact mathematical models do not exist"
  - [section 3.1] "By following this design principle, we first eliminate the time variables in the trajectory data (2.2) and rewrite into the following: x(1)0 , · · · , x(1)L1 ;"
  - [corpus] Weak evidence - no direct corpus support for this specific time-removal mechanism
- Break condition: If system dynamics fundamentally depend on absolute time (non-autonomous systems without external forcing) or if the mapping between states is not well-defined

### Mechanism 2
- Claim: Multi-step loss function significantly improves long-term stability of FML predictions
- Mechanism: Training with averaged loss over K future time steps (K=5-10) enforces consistency across multiple prediction horizons rather than just one-step accuracy
- Core assumption: The learned flow map remains stable when optimized for multi-step predictions
- Evidence anchors:
  - [section 4.1] "we have come to the conclusion that the stability of the FML models can be significantly enhanced by adopting a multi-step loss function during the training process"
  - [section 4.2] "Using multi-step loss with K > 0 can significantly improve the long-term numerical stability of the trained FML model"
  - [corpus] No direct corpus evidence supporting multi-step loss effectiveness
- Break condition: If the system exhibits highly unstable dynamics where errors amplify too quickly for multi-step optimization to compensate

### Mechanism 3
- Claim: Memory terms (nM > 0) are mathematically necessary for partially observed systems
- Mechanism: Following Mori-Zwanzig formulation, the evolution of partially observed variables requires knowledge of past states to account for unobserved variables' influence
- Core assumption: The reduced dynamics of observable variables depend on their history due to coupling with unobserved variables
- Evidence anchors:
  - [section 3.2.2] "Motivated by Mori-Zwanzig formulation, the work of [6] derived that the evolution of the dynamics of x ∈ Rd at any time tn follows dx/dt|_{tn} = F(xn, . . . ,xn−nM )"
  - [section 3.2.2] "The FML model thus follows xn+1 = G(xn, . . . ,xn−nM ), n ≥ nM > 0"
  - [corpus] Weak evidence - no corpus support for Mori-Zwanzig connection
- Break condition: If the system is fully observed (x = y) or if the memory effect is negligible for the timescale of interest

## Foundational Learning

- Concept: Dynamical systems and flow maps
  - Why needed here: FML is fundamentally about learning the flow map that governs state evolution
  - Quick check question: Can you explain the difference between a flow map and a solution trajectory in dynamical systems?

- Concept: Mori-Zwanzig formulation and memory effects
  - Why needed here: Understanding why partially observed systems require memory terms is crucial for proper implementation
  - Quick check question: Why does reducing a high-dimensional system to observed variables create memory terms in the reduced dynamics?

- Concept: Deep neural network training fundamentals
  - Why needed here: DNNs are the primary function approximator for the flow map, and training details critically affect performance
  - Quick check question: What are the key differences between using Adam optimizer with learning rate 10^-3 vs 10^-4 for this type of regression problem?

## Architecture Onboarding

- Component map: Raw trajectory data → Subsampled training pairs → DNN flow map approximator → Multi-step prediction → Ensemble averaging
- Critical path: Data generation → Memory step selection → DNN training with multi-step loss → Model validation → Ensemble prediction
- Design tradeoffs:
  - Memory step nM vs computational cost and stability
  - Multi-step loss K vs training time and overfitting risk
  - Ensemble size vs prediction reliability and computational overhead
- Failure signatures:
  - Training loss not decaying 2+ orders of magnitude → insufficient training or wrong learning rate
  - Validation/test error much larger than training error → overfitting or insufficient memory terms
  - Predictions diverge quickly even with small time steps → need larger K or ensemble averaging
- First 3 experiments:
  1. Implement 2D linear system (Benchmark 5.1.1) with fully observed case to verify basic FML implementation
  2. Test partially observed case with nM=10 on same system to validate memory term implementation
  3. Try multi-step loss with K=5 on chaotic Lorenz system to observe stability improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of time step ∆ affect the long-term stability of FML predictions, particularly for chaotic systems?
- Basis in paper: [explicit] The paper notes that "The impact of ∆ on the long-term stability of the FML prediction is not well understood" and mentions that "FML models using DNNs are quite stable, when multi-step loss K > 0 is used and the DNNs are trained with sufficient accuracy"
- Why unresolved: While the paper observes that FML is stable with multi-step loss and proper training, it does not provide a theoretical understanding of how the time step size influences stability, especially in chaotic systems where small errors can grow exponentially
- What evidence would resolve it: Systematic numerical experiments varying ∆ across different system types (linear, nonlinear, chaotic) while keeping other parameters constant, combined with theoretical analysis of error propagation in the multi-step loss framework

### Open Question 2
- Question: What is the optimal memory step nM for partially observed systems, and how does it relate to the effective memory length TM of the system?
- Basis in paper: [explicit] The paper states "The choice of the memory step nM is perhaps the most important one" and describes a "resolution independence procedure" for determining it, but notes that "using too large a TM not only makes the FML modeling computational expensive, it also induces numerical instability for the trained FML model"
- Why unresolved: While the paper provides a heuristic procedure for choosing nM, it does not provide theoretical guidance on how to determine the optimal memory length for a given system, nor does it explain why excessive memory causes instability
- What evidence would resolve it: Theoretical analysis of the Mori-Zwanzig formulation showing how memory terms decay with time lag, combined with empirical studies demonstrating the relationship between system memory, chosen nM, and prediction accuracy across diverse systems

### Open Question 3
- Question: How does ensemble averaging with Nmodel independent FML models improve prediction accuracy and reduce randomness in the predictions?
- Basis in paper: [explicit] The paper states that "This is perhaps the most effective way to enhance reproducibility of the FML models" and references a separate work [3] for details, but does not explain the mathematical mechanism behind why ensemble averaging improves results
- Why unresolved: The paper asserts the effectiveness of ensemble averaging without providing the underlying mathematical justification or explaining how the averaging procedure interacts with the multi-step loss function and the inherent randomness in DNN training
- What evidence would resolve it: Theoretical analysis of how averaging multiple independently trained models reduces variance in predictions, combined with empirical studies showing how the improvement scales with Nmodel and comparing ensemble averaging to other variance reduction techniques

## Limitations
- The theoretical justification for memory terms in partially observed systems is limited to the Mori-Zwanzig formulation without rigorous mathematical derivation of the specific flow map form
- Critical implementation details like exact network architecture and hyperparameter selection strategies remain underspecified
- The computational overhead of ensemble averaging is not quantitatively analyzed for diminishing returns

## Confidence
- Effectiveness for fully observed systems: High confidence based on systematic benchmark tests across linear, nonlinear, and chaotic dynamics
- Memory term theory for partially observed systems: Medium confidence due to limited mathematical development
- Multi-step loss optimization: High confidence based on empirical evidence showing improved stability
- Ensemble averaging benefits: Medium confidence based on empirical observations without theoretical explanation

## Next Checks
1. Test FML on systems with known analytical solutions but complex nonlinearities (e.g., van der Pol oscillator) to assess accuracy vs. ground truth
2. Evaluate robustness to noise in observations by comparing FML performance across signal-to-noise ratios
3. Compare multi-step loss performance against alternative training strategies like curriculum learning or weighted losses