---
ver: rpa2
title: 'LRM: Large Reconstruction Model for Single Image to 3D'
arxiv_id: '2311.04400'
source_url: https://arxiv.org/abs/2311.04400
tags:
- image
- images
- input
- arxiv
- triplane
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present LRM, the first large reconstruction model for single
  image to 3D. LRM directly predicts a neural radiance field from a single input image
  within 5 seconds, without any per-shape optimization.
---

# LRM: Large Reconstruction Model for Single Image to 3D

## Quick Facts
- arXiv ID: 2311.04400
- Source URL: https://arxiv.org/abs/2311.04400
- Reference count: 40
- Key outcome: LRM is the first large reconstruction model that predicts a neural radiance field from a single input image within 5 seconds without per-shape optimization.

## Executive Summary
LRM introduces the first large reconstruction model capable of converting a single image into a 3D neural radiance field within 5 seconds, without requiring per-shape optimization. The model leverages a 500-million-parameter transformer-based architecture trained end-to-end on approximately one million objects from Objaverse and MVImgNet. This combination of high-capacity modeling and large-scale training enables LRM to generalize across diverse inputs, including real-world captures and images generated by generative models.

## Method Summary
LRM employs a transformer-based encoder-decoder architecture with triplane representation to predict neural radiance fields from single images. The model uses a pre-trained DINO ViT as an image encoder, projects image features to 3D triplane tokens via cross-attention, and queries these features using an MLP-based NeRF to generate color and density predictions. Training involves normalizing camera poses and using image reconstruction losses (LMSE + λLLPIPS) on a massive dataset of synthetic and real-world 3D objects.

## Key Results
- LRM achieves single-image-to-3D reconstruction in approximately 5 seconds without per-shape optimization
- The model generalizes across diverse inputs including real-world captures and generative model outputs
- Trained on approximately 1 million objects from Objaverse and MVImgNet datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large transformer decoder with triplane representation enables efficient single-image-to-3D reconstruction
- **Mechanism:** The transformer decoder projects 2D image features to 3D triplane tokens via cross-attention, capturing spatial relationships through self-attention. The triplane's O(N²) computational efficiency scales better than O(N³) voxel grids.
- **Core assumption:** Triplane representations preserve sufficient spatial information for high-fidelity 3D reconstruction
- **Evidence anchors:** Weak - corpus papers focus on alternative 3D reconstruction methods without detailed triplane transformer mechanisms

### Mechanism 2
- **Claim:** Camera pose normalization enables learning of generalized image-to-triplane mapping
- **Mechanism:** Normalizing camera poses to canonical orientation reduces optimization space and enables consistent 3D prior learning across views
- **Core assumption:** Consistent camera parameterization simplifies 2D-to-3D mapping
- **Evidence anchors:** Weak - corpus papers don't discuss camera normalization strategies in detail

### Mechanism 3
- **Claim:** Large-scale training data enables learning of generalizable 3D priors
- **Mechanism:** Training on approximately one million diverse objects allows the model to capture cross-shape 3D priors for arbitrary object reconstruction
- **Core assumption:** Sufficient data diversity and quantity enable robust 3D representation learning
- **Evidence anchors:** Moderate - corpus papers mention dataset scale but don't analyze impact in detail

## Foundational Learning

- **Concept:** Neural Radiance Fields (NeRF)
  - Why needed here: Represents 3D scenes as continuous functions mapping 3D coordinates to color and density for high-quality novel view synthesis
  - Quick check question: How does NeRF handle the ambiguity of 3D geometry from a single 2D image?

- **Concept:** Transformer Architecture
  - Why needed here: Captures long-range dependencies and relationships between spatial tokens, crucial for mapping 2D image features to 3D representations
  - Quick check question: What is the difference between cross-attention and self-attention in the transformer decoder?

- **Concept:** Triplane Representation
  - Why needed here: Provides compact 3D representation that is computationally efficient and preserves spatial locality with respect to input image
  - Quick check question: How does the triplane representation compare to voxel grids in terms of computational complexity?

## Architecture Onboarding

- **Component map:** Image → Image Encoder (DINO ViT) → Image-to-Triplane Decoder → Triplane-NeRF → Volumetric Rendering → Novel Views

- **Critical path:** Image → Image Encoder → Image-to-Triplane Decoder → Triplane-NeRF → Volumetric Rendering → Novel Views

- **Design tradeoffs:**
  - Triplane vs. voxel grid: Triplane is more computationally efficient but may have lower resolution
  - Camera pose normalization: Simplifies learning but assumes consistent camera parameterization during inference
  - Large-scale training: Improves generalization but requires significant computational resources

- **Failure signatures:**
  - Blurry textures in occluded regions: Indicates insufficient 3D prior learning or deterministic output
  - Distorted shapes: Suggests incorrect camera pose normalization or misalignment
  - Loss of fine details: May indicate inadequate triplane resolution or insufficient training data

- **First 3 experiments:**
  1. Test impact of camera pose normalization by training with and without normalization and comparing reconstruction quality
  2. Evaluate effect of triplane resolution on reconstruction fidelity by training models with different triplane resolutions
  3. Assess influence of training data diversity by training on subsets (only synthetic or only real data) and comparing generalization to novel objects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would LRM's performance change if trained on a significantly larger dataset (e.g., 10x the current size)?
- Basis in paper: [inferred] The paper mentions LRM is trained on around 1 million 3D shapes and discusses potential future directions of scaling up model and training data
- Why unresolved: The paper doesn't provide experimental results on impact of dataset size beyond current 1 million objects
- What evidence would resolve it: Training LRM on a dataset 10x larger and comparing performance on same evaluation metrics (PSNR, CLIP-Similarity, SSIM, LPIPS)

### Open Question 2
- Question: How would LRM's performance change if trained on data with more diverse camera poses (e.g., including extreme angles and distances)?
- Basis in paper: [explicit] The paper discusses camera normalization during training and mentions incorrect assumptions of camera parameters can lead to distorted shape reconstruction
- Why unresolved: The paper doesn't explore impact of training on data with wider range of camera poses beyond normalized setup
- What evidence would resolve it: Training LRM on data with wider range of camera poses (including extreme angles and distances) and evaluating performance on images with varying camera parameters

### Open Question 3
- Question: How would LRM's performance change if trained on data with view-dependent appearance modeling (e.g., shiny metals, glossy ceramics)?
- Basis in paper: [explicit] The paper mentions LRM assumes Lambertian objects and omits view-dependent modeling in predicted NeRF, limiting ability to faithfully reconstruct view-dependent appearance
- Why unresolved: The paper doesn't explore impact of training on data with view-dependent appearance
- What evidence would resolve it: Training LRM on data including objects with view-dependent appearance (e.g., shiny metals, glossy ceramics) and evaluating ability to reconstruct such materials accurately

## Limitations
- Substantial computational requirements (128 NVIDIA A100 GPUs for 30 epochs) beyond typical research budgets
- Triplane representation may sacrifice fine geometric details compared to denser representations
- Model's reliance on specific camera normalization assumptions could limit real-world applicability when camera parameters are unknown or inaccurate

## Confidence

**High Confidence:** The architectural design combining transformer decoder with triplane representation is well-founded and scaling laws for transformers suggest this approach should work. The claim about large-scale training enabling generalization is supported by extensive literature on transformer scaling.

**Medium Confidence:** The specific implementation details of camera pose normalization and its effectiveness in reducing optimization space are plausible but not fully validated through ablation studies in the paper. The computational efficiency claims (5-second inference) depend on specific hardware configurations not detailed in the paper.

**Low Confidence:** The claim that triplane resolution is sufficient for high-fidelity reconstruction across diverse object categories requires empirical validation, particularly for objects with fine geometric details. The model's performance on real-world in-the-wild captures versus controlled datasets needs more rigorous comparison.

## Next Checks

1. **Ablation study on camera normalization:** Train LRM with and without camera pose normalization on the same dataset, then evaluate reconstruction quality and convergence speed to validate whether the claimed reduction in optimization space actually improves performance.

2. **Triplane resolution sensitivity analysis:** Systematically vary triplane resolutions (e.g., 16x16x16, 32x32x32, 64x64x64) and measure the trade-off between computational cost and reconstruction fidelity across different object categories, focusing on objects with fine geometric details.

3. **Real-world deployment validation:** Test LRM on a curated dataset of real-world images with unknown camera parameters, comparing performance against models that don't require camera normalization to reveal whether the camera normalization assumption is a practical limitation in real-world applications.