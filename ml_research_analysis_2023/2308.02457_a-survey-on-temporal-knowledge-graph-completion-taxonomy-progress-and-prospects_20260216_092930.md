---
ver: rpa2
title: 'A Survey on Temporal Knowledge Graph Completion: Taxonomy, Progress, and Prospects'
arxiv_id: '2308.02457'
source_url: https://arxiv.org/abs/2308.02457
tags:
- temporal
- knowledge
- graph
- tkgc
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of Temporal Knowledge
  Graph Completion (TKGC) methods, categorizing them into interpolation and extrapolation
  techniques based on their ability to predict missing facts within or beyond the
  available time span. The survey covers various approaches, including timestamp-dependent,
  timestamp-specific function-based, and deep learning-based methods for interpolation,
  as well as rule-based, graph neural network-based, meta learning-based, and reinforcement
  learning-based methods for extrapolation.
---

# A Survey on Temporal Knowledge Graph Completion: Taxonomy, Progress, and Prospects

## Quick Facts
- arXiv ID: 2308.02457
- Source URL: https://arxiv.org/abs/2308.02457
- Reference count: 40
- Primary result: Comprehensive survey categorizing TKGC methods into interpolation and extrapolation approaches, covering deep learning, GNN, and meta-learning techniques.

## Executive Summary
This paper provides a comprehensive survey of Temporal Knowledge Graph Completion (TKGC) methods, categorizing them based on their ability to predict missing facts within or beyond the available time span. The survey covers various approaches including timestamp-dependent, timestamp-specific function-based, and deep learning-based methods for interpolation, as well as rule-based, graph neural network-based, meta learning-based, and reinforcement learning-based methods for extrapolation. The paper also discusses applications in question answering, medical analysis, and recommendation systems, while identifying future research directions such as multi-modal TKGs, few-shot learning, and integration with large language models.

## Method Summary
The survey categorizes TKGC methods into interpolation (predicting within known time spans) and extrapolation (predicting beyond available data) approaches. It systematically reviews timestamp-dependent methods that incorporate temporal information through embedding concatenation or weight modulation, timestamp-specific function-based methods that learn entity-relation representations in timestamp-specific spaces, and deep learning-based methods that use RNNs, temporal constraints, and other neural architectures. For extrapolation, it covers rule-based methods using logical rules and tensors, GNN-based approaches combining graph neural networks with temporal modeling, and meta/reinforcement learning methods for handling rare entities. The survey evaluates methods across benchmark datasets like ICEWS and GDELT using standard metrics including MR, MRR, and Hit@N.

## Key Results
- TKGC methods can be effectively categorized by temporal scope into interpolation and extrapolation approaches
- Deep learning methods show promise for encoding temporal information through various strategies including RNNs and timestamp-specific spaces
- GNN-based extrapolation methods successfully capture both structural and temporal dependencies through dual encoding mechanisms
- Current methods face challenges with multi-modal data, few-shot learning, and integration with large language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: **Temporal Knowledge Graph Completion (TKGC) methods are categorized by their ability to handle time-specific facts, distinguishing between interpolation (within known time spans) and extrapolation (beyond known time spans).**
- Mechanism: The survey organizes TKGC methods into two high-level categories based on temporal scope: interpolation methods predict missing facts within the known time span, while extrapolation methods forecast future events beyond the available data. Each category is further subdivided based on algorithmic approaches and how temporal information is processed.
- Core assumption: **Temporal information in knowledge graphs fundamentally changes the nature of completion tasks, requiring distinct methodological approaches depending on whether the target time is within or beyond the observed time span.**
- Evidence anchors:
  - [abstract] "we categorize existing TKGC literature into the interpolation methods and the extrapolation methods"
  - [section] "we categorize them into two flavors based on whether they forecast future events, including Interpolation-based TKGCs (Fig. 1a) and Extrapolation-based TKGCs (Fig. 1b)"
  - [corpus] Weak - corpus focuses on recent papers but doesn't directly address the interpolation/extrapolation categorization
- Break condition: If temporal granularity or time-span boundaries become ambiguous, or if methods combine both interpolation and extrapolation capabilities, the clear categorization may break down.

### Mechanism 2
- Claim: **Deep learning methods for TKGC encode temporal information through various strategies including timestamp-specific spaces, recurrent neural networks, and temporal constraints.**
- Mechanism: The survey details three main deep learning approaches: timestamps-specific space methods (embedding timestamps as hyperplanes or semantic spaces), LSTM/RNN-based methods (encoding timestamps as sequences and learning temporal dependencies), and temporal constraint methods (using temporal information to guide path reasoning). Each approach captures different aspects of temporal evolution in the knowledge graph.
- Core assumption: **Temporal information can be effectively encoded and leveraged through deep learning architectures to improve knowledge graph completion accuracy.**
- Evidence anchors:
  - [abstract] "3) Deep learning-based TKGC methods utilize deep learning algorithms to encode temporal information and investigate the dynamic evolution of entities and relations"
  - [section] "Thanks to the powerful information mining ability, various deep learning algorithms are used to process temporal information in TKGs"
  - [corpus] Weak - corpus contains recent papers but doesn't provide specific evidence about deep learning temporal encoding strategies
- Break condition: If temporal patterns are too complex or irregular for the chosen deep learning architecture to capture effectively, or if computational costs become prohibitive.

### Mechanism 3
- Claim: **Graph neural network-based extrapolation methods capture both structural and temporal dependencies through message passing and recurrent mechanisms across temporal snapshots.**
- Mechanism: The survey explains that GNN-based methods use structural encoders (like GCN or GAT) to capture graph topology within each time step, and temporal encoders (like RNN or GRU) to model dependencies across time steps. This dual approach allows the model to learn both immediate neighborhood information and long-term temporal patterns.
- Core assumption: **The combination of graph neural networks for structural learning and recurrent networks for temporal learning effectively captures the dynamics of evolving knowledge graphs.**
- Evidence anchors:
  - [abstract] "2) Graph neural network-based TKGC methods generally utilize GNN and RNN to explore the structural and temporal information in TKG"
  - [section] "Graph Neural Network (GNN)-based TKGC methods generally apply GNN to explore the intrinsic topology relevance between entities or between entities and relations in TKG"
  - [corpus] Weak - corpus contains recent papers but doesn't provide specific evidence about GNN-RNN combination effectiveness
- Break condition: If the temporal resolution is too fine-grained for effective message passing, or if the graph structure changes too rapidly for the temporal encoder to keep up.

## Foundational Learning

- Concept: **Temporal Knowledge Graphs (TKGs) and their structure**
  - Why needed here: Understanding TKGs is fundamental to grasping TKGC methods, as these methods operate on temporal knowledge graphs with quadruplets (s, r, o, t) rather than simple triplets.
  - Quick check question: What is the fundamental difference between a standard knowledge graph and a temporal knowledge graph?

- Concept: **Embedding methods for knowledge graphs (TransE, DistMult, ComplEx)**
  - Why needed here: Many TKGC methods build upon or extend static knowledge graph embedding methods, so understanding these foundations is crucial for understanding TKGC innovations.
  - Quick check question: How do TransE and DistMult differ in their approach to embedding knowledge graph triples?

- Concept: **Temporal reasoning and time representation**
  - Why needed here: TKGC methods must handle various temporal representations (time points, intervals, granularities) and reason about temporal relationships, which is central to the task.
  - Quick check question: What are the challenges in representing and reasoning with temporal intervals versus discrete time points in knowledge graphs?

## Architecture Onboarding

- Component map: TKG input → temporal encoding (varies by method) → embedding generation → scoring function → prediction output → evaluation metrics (MR, MRR, Hit@N)
- Critical path: For interpolation: input TKG → temporal encoding → embedding generation → scoring → prediction. For extrapolation: historical TKG snapshots → temporal-structural encoding → future prediction → validation. The temporal encoding step is critical for both, but the nature of encoding differs significantly.
- Design tradeoffs: TKGC methods face tradeoffs between temporal expressiveness and computational efficiency (complex temporal encoding improves accuracy but increases cost), between interpolation and extrapolation capabilities (methods optimized for one may not perform well on the other), and between static and dynamic approaches (static embeddings are faster but may miss temporal patterns).
- Failure signatures: Common failure modes include poor performance on rare or unseen temporal patterns, inability to handle irregular temporal sampling, and degradation when temporal dependencies are weak or non-existent. For extrapolation methods, failure often manifests as poor long-term predictions or inability to capture cyclical patterns.
- First 3 experiments:
  1. Implement a basic TransE-based TKGC method with simple timestamp concatenation to establish baseline performance on ICEWS14 dataset
  2. Compare performance of LSTM-based temporal encoding versus timestamp-specific hyperplane methods on the same dataset
  3. Test extrapolation capabilities by training on ICEWS14 and testing on ICEWS18 to evaluate temporal generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can TKGC methods be effectively extended to handle multi-modal temporal knowledge graphs?
- Basis in paper: [explicit] The paper discusses future directions, including the need for multi-modal TKGs to enrich entity representations and capture more nuanced semantics.
- Why unresolved: Current TKGC methods primarily focus on textual data, and the integration of multi-modal data (e.g., images, videos) into TKGs is not well-explored.
- What evidence would resolve it: Development and evaluation of TKGC methods that incorporate multi-modal data, demonstrating improved performance in downstream tasks such as visual question answering and recommendation systems.

### Open Question 2
- Question: What are the most effective approaches for TKGC in few-shot and inductive learning settings?
- Basis in paper: [explicit] The paper identifies the challenge of handling rare or unseen entities and relations in TKGs, suggesting the need for few-shot and inductive learning methods.
- Why unresolved: Traditional TKGC methods assume abundant training data, which is not always available in real-world scenarios with long-tail distributions.
- What evidence would resolve it: Empirical studies comparing the performance of TKGC methods in few-shot and inductive settings, showcasing their ability to generalize to unseen entities and relations.

### Open Question 3
- Question: How can large language models be effectively integrated with TKGC methods to leverage their reasoning capabilities?
- Basis in paper: [explicit] The paper mentions the potential of unifying LLMs with TKGC methods but notes that this area is less explored.
- Why unresolved: LLMs are pre-trained on static data and may struggle to capture the dynamic nature of TKGs and represent new knowledge effectively.
- What evidence would resolve it: Development of TKGC methods that leverage the strengths of LLMs while addressing their limitations in handling temporal information, demonstrating improved reasoning performance on TKGs.

## Limitations

- The survey's categorization framework may oversimplify the landscape as some modern methods blur boundaries between interpolation and extrapolation
- Corpus evidence supporting the effectiveness of various deep learning approaches for temporal encoding is notably weak
- The survey does not extensively address computational complexity trade-offs across different method categories

## Confidence

**High confidence**: The fundamental problem definition of TKGC and the basic taxonomy separating interpolation from extrapolation methods.

**Medium confidence**: The detailed categorization of specific methods within each approach (e.g., timestamp-specific space, LSTM-based, temporal constraint methods).

**Low confidence**: Claims about the relative effectiveness and superiority of different method categories, particularly for deep learning approaches.

## Next Checks

1. Replicate baseline experiments: Implement and compare three different temporal encoding strategies (timestamp concatenation, timestamp-specific hyperplanes, and LSTM-based encoding) on the ICEWS14 dataset to validate the survey's characterization of method effectiveness.

2. Test temporal generalization: Train an extrapolation method on ICEWS05-15 and evaluate on ICEWS14 to measure temporal generalization capabilities, validating the survey's claims about extrapolation method performance.

3. Analyze computational trade-offs: Measure training and inference times across different TKGC method categories on identical hardware to quantify the computational efficiency trade-offs mentioned but not empirically validated in the survey.