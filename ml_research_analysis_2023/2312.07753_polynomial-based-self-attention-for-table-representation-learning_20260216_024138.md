---
ver: rpa2
title: Polynomial-based Self-Attention for Table Representation learning
arxiv_id: '2312.07753'
source_url: https://arxiv.org/abs/2312.07753
tags:
- cheatt
- learning
- data
- matrix
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Chebyshev Polynomial-based Self-Attention (CheAtt)
  to address the oversmoothing issue in Transformer-based table representation learning.
  The key idea is to replace the original self-attention layer with a polynomial-based
  layer that can capture both low and high-frequency components of the attention matrix.
---

# Polynomial-based Self-Attention for Table Representation learning

## Quick Facts
- arXiv ID: 2312.07753
- Source URL: https://arxiv.org/abs/2312.07753
- Authors: 
- Reference count: 24
- Primary result: Chebyshev Polynomial-based Self-Attention (CheAtt) improves three representative table learning models by an average of 4.66% across 10 datasets while mitigating oversmoothing.

## Executive Summary
This paper addresses the oversmoothing problem in Transformer-based table representation learning by proposing Chebyshev Polynomial-based Self-Attention (CheAtt). The method replaces the standard self-attention layer with a polynomial-based layer that captures both low and high-frequency components of the attention matrix using Chebyshev polynomials as an orthogonal basis. Experiments on 10 datasets demonstrate significant performance improvements (average 4.66%) for TabTransformer, SAINT, and MET models compared to their base versions. CheAtt also mitigates oversmoothing as evidenced by lower cosine similarity and slower decrease in singular values of feature maps.

## Method Summary
CheAtt replaces the standard self-attention layer with a polynomial-based layer using Chebyshev polynomials. The method leverages the observation that attention matrices in Transformers satisfy the three conditions for rapid PageRank convergence (stochasticity, irreducibility, and aperiodicity). This allows efficient computation of high-order polynomial terms without full computation of all terms. Chebyshev polynomials provide a stable orthogonal basis for learning polynomial coefficients that approximate a wider range of graph filters compared to the original self-attention which only uses A. The polynomial coefficients exhibit a proportionally decreasing trend, enabling truncation of higher-order terms without significant loss of approximation accuracy.

## Key Results
- CheAtt improves TabTransformer, SAINT, and MET models by an average of 4.66% across 10 datasets
- CheAtt mitigates oversmoothing, showing lower cosine similarity and slower decrease in singular values of feature maps
- Chebyshev polynomials outperform other polynomial bases (Power, Legendre, Jacobi) in most cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chebyshev Polynomial-based Self-Attention (CheAtt) captures both low and high-frequency components of the attention matrix, addressing oversmoothing.
- Mechanism: Replaces the original self-attention layer with a polynomial-based layer using Chebyshev polynomials as an orthogonal basis. This allows the model to approximate a wider range of graph filters compared to the original self-attention which only uses A.
- Core assumption: Chebyshev polynomials provide a stable orthogonal basis for learning polynomial coefficients that approximate the optimal graph filter.
- Evidence anchors:
  - [abstract] "We propose a novel matrix polynomial-based self-attention layer as a substitute for the original self-attention layer, which enhances model scalability."
  - [section 4.3] "Chebyshev polynomial can be recursively defined as Tk(A) = 2ATk-1(A) - Tk-2(A)... These polynomials form an orthogonal basis... Therefore, we use Chebyshev polynomial to stabilize the training of the coefficients."
  - [corpus] No direct corpus evidence for this specific mechanism. Assumption: Chebyshev polynomials are known to provide stable orthogonal bases in numerical analysis, supporting their use here.
- Break condition: If the polynomial order j is not chosen appropriately, the model may not converge or may not capture sufficient frequency information. The convergence analysis in Theorem 2 assumes weak endpoint singularities and analyticity of the function being approximated.

### Mechanism 2
- Claim: Attention matrices in Transformers satisfy the three conditions for rapid PageRank convergence (stochasticity, irreducibility, aperiodicity).
- Mechanism: This property allows for efficient computation of high-order polynomial terms in the Chebyshev polynomial expansion without requiring full computation of all terms.
- Core assumption: The softmax function in self-attention ensures the attention matrix meets these three conditions.
- Evidence anchors:
  - [section 4.1] "Attention matrix A in Transformers meet all the 3 conditions: 1. Stochasticity... 2. Irreducibility... 3. Aperiodicity..."
  - [section 4.1] "According to Theorem 1, in other words, since the attention matrix A satisfies the conditions, AkV, where k ∈ R, converges with a small k in matrix polynomial."
  - [corpus] No direct corpus evidence for this specific mechanism. Assumption: The properties of attention matrices and PageRank convergence are well-established in their respective fields.
- Break condition: If the attention matrix does not satisfy these conditions (e.g., due to modifications in the self-attention mechanism), the convergence guarantee would not hold.

### Mechanism 3
- Claim: Chebyshev polynomial coefficients exhibit a proportionally decreasing trend, allowing for truncation of higher-order terms without significant loss of approximation accuracy.
- Mechanism: This property reduces the computational burden by avoiding the need to compute all n terms of the Chebyshev polynomial expansion.
- Core assumption: The convergence of Chebyshev coefficients is proportional to 1/k^q for some positive constant q, as stated in Theorem 2.
- Evidence anchors:
  - [section 4.3] "Theorem 2(Convergence of the Chebyshev coefficients)... If f(x) = Σ∞ k=0 βkTk(x), where βk is the Chebyshev coefficients, is weakly singular at the boundaries and analytic in the interval (-1, 1), then the Chebyshev coefficients βk will asymptotically (as k → ∞) decreases proportionally to 1/k^q for some positive constant q."
  - [section 4.3] "Moreover, the property of Chebyshev polynomial that Chebyshev coefficients exhibit a proportionally decreasing trend as in Theorem 2 also support a claim that we do not need to compute all n terms."
  - [corpus] No direct corpus evidence for this specific mechanism. Assumption: The convergence properties of Chebyshev polynomials are well-established in approximation theory.
- Break condition: If the function being approximated does not satisfy the conditions of weak singularity at boundaries and analyticity in the interval (-1, 1), the convergence rate may not hold.

## Foundational Learning

- Concept: Graph Signal Processing (GSP)
  - Why needed here: GSP provides the theoretical framework for understanding how self-attention can be viewed as a graph filter and how polynomial-based extensions can capture a wider range of frequencies.
  - Quick check question: How does the graph Fourier transform relate to the discrete Fourier transform in the context of GSP?

- Concept: PageRank algorithm
  - Why needed here: PageRank's convergence properties inform the efficient computation of high-order polynomial terms in the Chebyshev expansion.
  - Quick check question: What are the three conditions that a transition matrix must satisfy for PageRank to converge quickly?

- Concept: Chebyshev polynomials
  - Why needed here: Chebyshev polynomials provide a stable orthogonal basis for learning polynomial coefficients in the extended self-attention layer.
  - Quick check question: How are Chebyshev polynomials recursively defined, and what is their significance in approximation theory?

## Architecture Onboarding

- Component map: Tabular data -> Embedded vectors -> CheAtt layer -> Enhanced representations -> Downstream tasks
- Critical path:
  1. Preprocess tabular data into embedded vectors
  2. Apply CheAtt to compute enhanced representations
  3. Train auxiliary layers for specific tasks (classification/regression)
- Design tradeoffs:
  - Computational cost: CheAtt adds complexity but remains manageable due to the small number of tokens in tabular data
  - Approximation accuracy: Choosing the appropriate polynomial order j is crucial for balancing accuracy and computational efficiency
  - Stability: Chebyshev polynomials provide a stable basis for learning coefficients, but the model's performance may be sensitive to hyperparameter choices
- Failure signatures:
  - Performance degradation: If the polynomial order is too low, the model may not capture sufficient frequency information
  - Overfitting: If the polynomial order is too high or the model is too complex, it may overfit to the training data
  - Computational inefficiency: If the attention matrix is large (e.g., in image or graph data), the computational cost may become prohibitive
- First 3 experiments:
  1. Compare the performance of a base Transformer model with and without CheAtt on a small tabular dataset
  2. Vary the polynomial order j and observe its impact on model performance and convergence
  3. Test the sensitivity of the model to different orthogonal polynomial bases (e.g., Legendre, Jacobi) and compare their performance to Chebyshev polynomials

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CheAtt scale with increasing table sizes (number of columns/tokens)?
- Basis in paper: [inferred] The paper states that the computational cost of CheAtt is manageable for tabular data due to typically small number of columns (tens), but does not explore the limits of this scalability.
- Why unresolved: The paper only evaluates on datasets with relatively small numbers of features (up to 81 for Superconductivity). It does not investigate how CheAtt performs on tables with hundreds or thousands of columns, where the computational complexity of higher-order polynomials may become prohibitive.
- What evidence would resolve it: Experiments on datasets with varying numbers of columns, especially larger tables, to assess the performance and computational efficiency of CheAtt as the table size increases. This would help determine the practical limits of applying CheAtt to large-scale tabular data.

### Open Question 2
- Question: How does the choice of polynomial basis (Chebyshev, Power, Legendre, Jacobi) affect the performance of CheAtt across different types of tabular data?
- Basis in paper: [explicit] The paper compares different polynomial bases (Chebyshev, Power, Legendre, Jacobi) and finds that Chebyshev performs best in many cases, but Legendre also performs well for SAINT+CheAtt on the Superconductivity dataset.
- Why unresolved: While the paper provides some comparisons, it does not fully explore the impact of polynomial choice on different types of tabular data (e.g., high-dimensional vs. low-dimensional, categorical vs. continuous features). The optimal polynomial basis may vary depending on the characteristics of the data.
- What evidence would resolve it: A more comprehensive study comparing the performance of different polynomial bases across a diverse set of tabular datasets with varying characteristics. This would help identify which polynomial basis is most effective for different types of tabular data.

### Open Question 3
- Question: Can CheAtt be effectively combined with other techniques for mitigating oversmoothing in Transformers, such as hierarchical fusion or token diversification?
- Basis in paper: [inferred] The paper focuses on mitigating oversmoothing through the use of Chebyshev polynomial-based self-attention, but does not explore how CheAtt might interact with or complement other oversmoothing mitigation techniques.
- Why unresolved: Oversmoothing is a well-known issue in Transformers, and there are various approaches proposed to address it. The paper does not investigate whether combining CheAtt with these other techniques could lead to even better performance or more robust mitigation of oversmoothing.
- What evidence would resolve it: Experiments combining CheAtt with other oversmoothing mitigation techniques, such as hierarchical fusion or token diversification, to assess whether these combinations lead to improved performance or more effective mitigation of oversmoothing compared to using CheAtt alone.

## Limitations

- Computational Cost Scaling: The paper claims polynomial computation is manageable for tabular data but lacks rigorous complexity analysis for different table sizes and polynomial orders.
- Generalization Beyond Classification: All experimental results focus on classification tasks; the paper mentions CheAtt can be applied to regression but provides no experimental validation.
- Hyperparameter Sensitivity: The paper doesn't provide systematic ablation studies on the polynomial order j or coefficient learning rates.

## Confidence

**High Confidence**: The mathematical framework connecting self-attention to graph filtering and the use of Chebyshev polynomials as orthogonal bases. The core theory (Theorem 1 and 2) appears sound based on established results in graph signal processing and approximation theory.

**Medium Confidence**: The experimental results showing performance improvements. While the methodology is clear and results are promising, the evaluation is limited to specific models (TabTransformer, SAINT, MET) and classification tasks only.

**Low Confidence**: Claims about CheAtt's effectiveness on regression tasks and larger-scale tabular datasets. These remain theoretical assertions without empirical validation.

## Next Checks

1. **Polynomial Order Sensitivity Analysis**: Systematically vary the polynomial order j across different datasets and measure the trade-off between computational cost and performance. This would validate whether the claimed efficiency gains hold in practice.

2. **Regression Task Validation**: Apply CheAtt to at least two regression datasets from the experiments and compare performance against the base models. This would test whether the theoretical benefits extend beyond classification.

3. **Scaling Experiment**: Test CheAtt on progressively larger attention matrices (simulated by increasing sequence length or embedding dimension) to quantify the computational scaling behavior and identify practical limits.