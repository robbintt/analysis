---
ver: rpa2
title: 'Typhoon: Thai Large Language Models'
arxiv_id: '2312.13951'
source_url: https://arxiv.org/abs/2312.13951
tags:
- thai
- language
- data
- arxiv
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Typhoon is a Thai large language model developed by continuing\
  \ pretraining Mistral-7B on a dedicated Thai corpus. The approach addresses the\
  \ low-resource challenge by scaling Thai data from Common Crawl and integrating\
  \ a specialized Thai tokenizer, improving efficiency by 2.62\xD7 over GPT-4."
---

# Typhoon: Thai Large Language Models

## Quick Facts
- arXiv ID: 2312.13951
- Source URL: https://arxiv.org/abs/2312.13951
- Reference count: 40
- Key outcome: Typhoon-7B outperforms open-source Thai models and matches GPT-3.5 on Thai benchmarks despite having only 7B parameters

## Executive Summary
Typhoon is a Thai large language model developed by continuing pretraining Mistral-7B on a dedicated Thai corpus. The approach addresses the low-resource challenge by scaling Thai data from Common Crawl and integrating a specialized Thai tokenizer, improving efficiency by 2.62× over GPT-4. Evaluation on ThaiExam (examinations for Thai students and investment professionals) shows Typhoon-7B outperforms all open-source Thai models and matches GPT-3.5 despite having only 7B parameters. Instruction-tuned Typhoon-7B-Instruct also achieves state-of-the-art performance on Thai instruction-following benchmarks and competitive results on translation, summarization, and QA tasks.

## Method Summary
The researchers developed Typhoon by continuing pretraining Mistral-7B on a Thai corpus created from 40 packs of Common Crawl data (~3TB) mixed with English data from Falcon RefinedWeb in a 50/50 split. They created a specialized Thai tokenizer by training SentencePiece on MC4 Thai data and integrating it with the Mistral tokenizer, adding 5,000 Thai-specific tokens. The model was then fine-tuned using LoRA with supervised fine-tuning for instruction following. Evaluation was conducted using a self-developed ThaiExam benchmark based on Thai examinations and various instruction-following tasks.

## Key Results
- Typhoon-7B outperforms all open-source Thai models on ThaiExam benchmark
- Matches GPT-3.5 performance on Thai benchmarks despite being only 7B parameters
- Achieves 2.62× tokenization efficiency improvement over GPT-4 for Thai text
- State-of-the-art performance on Thai instruction-following benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Typhoon outperforms open-source Thai LLMs by scaling Thai data and using an efficient Thai tokenizer.
- Mechanism: Continual pretraining of Mistral-7B on a Thai corpus addresses the low-resource challenge by transferring existing world knowledge from a strong LLM. The Thai tokenizer improves efficiency by 2.62× over GPT-4, reducing the number of tokens required for Thai text.
- Core assumption: A strong base LLM (Mistral-7B) can be effectively adapted to Thai by pretraining on a large Thai corpus, and a specialized tokenizer significantly improves efficiency.
- Evidence anchors:
  - [abstract] "Typhoon is a Thai large language model developed by continuing pretraining Mistral-7B on a dedicated Thai corpus."
  - [section 3.2] "Our experimental results in Table 2 show that Typhoon's tokenizer is 2.62 times more efficient than GPT-4."
- Break condition: If the Thai corpus is too small or of poor quality, or if the tokenizer is not well-trained on Thai text, the model's performance may not improve significantly.

### Mechanism 2
- Claim: Thai knowledge is effectively evaluated using ThaiExam, a benchmark based on Thai examinations.
- Mechanism: ThaiExam assesses the Thai knowledge encapsulated in each model from the pretraining stage by testing on examinations for high-school students and investment professionals in Thailand.
- Core assumption: Thai examinations provide a valid and comprehensive assessment of Thai knowledge and language understanding.
- Evidence anchors:
  - [abstract] "To evaluate the Thai knowledge encapsulated in each model from the pretraining stage, we develop ThaiExam, a benchmark based on examinations for high-school students and investment professionals in Thailand."
  - [section 3.4] "In Table 3, we compare our pre-trained Typhoon against open-source and proprietary LLMs that support the Thai language."
- Break condition: If ThaiExam does not cover a wide enough range of Thai knowledge or if the questions are not representative of real-world Thai language use, the evaluation may not accurately reflect the model's capabilities.

### Mechanism 3
- Claim: Typhoon achieves performance on par with GPT-3.5 in Thai while being more efficient.
- Mechanism: Instruction-tuning Typhoon to follow Thai instructions improves its ability to understand and respond to user queries in Thai. The model is evaluated on Thai instruction datasets as well as translation, summarization, and question-answering tasks.
- Core assumption: Instruction-tuning improves the model's ability to follow user intent and perform downstream tasks in Thai.
- Evidence anchors:
  - [abstract] "Instruction-tuned Typhoon-7B-Instruct also achieves state-of-the-art performance on Thai instruction-following benchmarks and competitive results on translation, summarization, and QA tasks."
  - [section 4.2] "Our experimental results show that Typhoon-7B-Instruct is comparable to GPT-3.5 on Thai AlpacaEval and translated MT-Bench but it is worse on Thai OASST and Sea-bench."
- Break condition: If the instruction-tuning datasets are not comprehensive enough or if the evaluation tasks are not representative of real-world use cases, the model's performance may not be as strong as expected.

## Foundational Learning

- Concept: Continual pretraining
  - Why needed here: To adapt an existing LLM to a low-resource language like Thai by transferring world knowledge from a strong base model.
  - Quick check question: What is the purpose of continual pretraining in the context of developing a Thai LLM?

- Concept: Token efficiency
  - Why needed here: To improve the efficiency of the model by reducing the number of tokens required to represent Thai text, which can lead to faster processing and lower computational costs.
  - Quick check question: How does the Thai tokenizer improve efficiency compared to a generic tokenizer like GPT-4?

- Concept: Instruction-tuning
  - Why needed here: To improve the model's ability to understand and respond to user queries in Thai by aligning it with user intent.
  - Quick check question: What is the purpose of instruction-tuning in the context of developing a Thai LLM?

## Architecture Onboarding

- Component map:
  - Mistral-7B base model -> Thai tokenizer (5,000 additional tokens) -> Thai corpus from Common Crawl and MC4 -> LoRA continual pretraining -> Supervised fine-tuning for instruction following

- Critical path:
  1. Pretrain the base model on the Thai corpus
  2. Fine-tune the model on instruction-following datasets
  3. Evaluate the model on Thai benchmarks and downstream tasks

- Design tradeoffs:
  - Using a smaller base model (7B) vs. a larger one for better performance but higher computational costs
  - Balancing the size of the Thai corpus with the quality of the data
  - Choosing between different instruction-tuning approaches (e.g., supervised fine-tuning vs. reinforcement learning)

- Failure signatures:
  - Poor performance on Thai benchmarks despite pretraining and instruction-tuning
  - High computational costs due to inefficient tokenization or large model size
  - Lack of generalization to real-world Thai language use cases

- First 3 experiments:
  1. Evaluate the model's performance on ThaiExam to assess its Thai knowledge
  2. Test the model's ability to follow Thai instructions using Thai instruction datasets
  3. Assess the model's performance on downstream tasks like translation, summarization, and question-answering in Thai

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Typhoon's performance scale with model size (e.g., 13B, 34B, or larger variants) when adapted to Thai?
- Basis in paper: [explicit] The paper states "Future work will extend pretraining to utilize a larger amount of Thai data... and use larger base models such as 34B, 70B, or mixture-of-experts to exploit the emergent ability."
- Why unresolved: The current work only evaluates Typhoon at 7B parameters, leaving the performance characteristics of larger variants unexplored.
- What evidence would resolve it: Training and evaluating Typhoon variants at 13B, 34B, and MoE configurations on the same Thai benchmarks would demonstrate scaling effects and potential emergent capabilities.

### Open Question 2
- Question: What is the optimal ratio of Thai to English data for continual pretraining that maximizes Thai performance while minimizing catastrophic forgetting of English capabilities?
- Basis in paper: [explicit] The paper mentions mixing English data from Falcon RefinedWeb with Thai data in a "50/50 split" but notes this is empirical and references prior work showing adaption is "not sensitive to the exact mixture ratio provided that the original language and target language are included."
- Why unresolved: The paper uses a fixed 50/50 ratio without systematic exploration of alternative ratios or their effects on performance.
- What evidence would resolve it: Training Typhoon variants with varying Thai/English data ratios (e.g., 70/30, 90/10, 30/70) and evaluating their performance on both Thai and English benchmarks would identify optimal ratios.

### Open Question 3
- Question: How does Typhoon's performance compare to proprietary models of similar parameter count when evaluated on Thai-specific cultural knowledge and domain-specific tasks?
- Basis in paper: [explicit] The paper shows Typhoon-7B matches GPT-3.5 on Thai benchmarks despite having only 7B parameters, but doesn't systematically explore performance differences on culturally-specific tasks.
- Why unresolved: While the paper demonstrates competitive performance, it doesn't isolate culturally-specific knowledge domains or compare against similarly-sized proprietary models.
- What evidence would resolve it: Creating domain-specific Thai benchmarks focused on cultural knowledge (e.g., Thai history, local customs, regional dialects) and evaluating Typhoon against similarly-sized proprietary models would reveal comparative strengths and weaknesses.

## Limitations

- Evaluation primarily relies on self-constructed benchmarks rather than established, peer-reviewed Thai language benchmarks
- Efficiency comparison to GPT-4 is based on tokenization alone without accounting for overall system efficiency
- Instruction-tuning methodology is minimally detailed, making it difficult to assess reproducibility

## Confidence

- **High Confidence**: The fundamental approach of continual pretraining on Thai data using a specialized tokenizer is sound and well-supported by the literature on low-resource language modeling.
- **Medium Confidence**: The claimed performance improvements on Thai benchmarks are plausible given the methodology, but verification requires independent replication and evaluation on established benchmarks.
- **Low Confidence**: The comparative claims against proprietary models (GPT-3.5) are difficult to verify without access to the exact evaluation protocols and prompts used in both systems.

## Next Checks

1. **Independent Benchmark Evaluation**: Replicate the core experiments using established Thai language benchmarks like ThaiGLUE or externally validated Thai question-answering datasets to verify the claimed performance advantages.
2. **Efficiency Validation Under Load**: Test the 2.62× tokenization efficiency claim under realistic deployment conditions, measuring actual inference time, memory usage, and throughput for Thai text processing compared to GPT-4 and other Thai models.
3. **Ablation Study on Tokenizer Design**: Conduct systematic experiments varying the tokenizer vocabulary size and design to quantify the exact contribution of the Thai tokenizer to overall model performance versus other factors like pretraining data quality and quantity.