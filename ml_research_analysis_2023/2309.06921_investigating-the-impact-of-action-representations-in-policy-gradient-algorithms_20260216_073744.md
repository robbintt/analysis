---
ver: rpa2
title: Investigating the Impact of Action Representations in Policy Gradient Algorithms
arxiv_id: '2309.06921'
source_url: https://arxiv.org/abs/2309.06921
tags:
- control
- gradient
- learning
- loss
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors investigate the impact of different action representations
  (torque, velocity, and position control) on reinforcement learning performance using
  PPO across several benchmark tasks. They find that the choice of action space significantly
  affects learning performance, but no single representation is optimal for all tasks.
---

# Investigating the Impact of Action Representations in Policy Gradient Algorithms

## Quick Facts
- arXiv ID: 2309.06921
- Source URL: https://arxiv.org/abs/2309.06921
- Reference count: 40
- Primary result: Different action representations (torque, velocity, position control) significantly affect reinforcement learning performance, but no single representation is optimal for all tasks.

## Executive Summary
This paper investigates how different action representations impact reinforcement learning performance using PPO across several benchmark tasks. The authors compare torque control, velocity control, and position control representations, finding that the choice of action space significantly affects learning performance but with no universal winner across all tasks. They employ two analysis techniques - optimization landscape visualization and gradient estimation accuracy measurement - to understand these performance differences. Their findings reveal a complex relationship between gradient quality and learning performance, with better-performing configurations sometimes having worse gradient estimates.

## Method Summary
The authors use PPO from Stable-Baselines3 to train agents across benchmark tasks from OpenAI Gym and DeepMind Control Suite. They implement velocity and position control variants by adding linear controllers on top of torque control, using PD controllers for position control and derivative controllers for velocity control. The experiments collect learning curves with 10 random seeds per task, perform optimization landscape analysis at specific checkpoints using dimensionality reduction, and measure gradient estimation quality throughout training using 107 samples compared to the 64 samples used during training.

## Key Results
- Action space choice significantly impacts learning performance, with no single representation optimal for all tasks
- Optimization landscape visualization reveals that rugged landscapes with many local optima (e.g., Reacher torque control) correlate with poor learning performance
- Better-performing configurations do not always have more accurate gradients, showing a complex non-monotonic relationship between gradient quality and learning performance
- The interaction between state and action representations significantly impacts learning, though intuitive hypotheses about state-action alignment don't always hold

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The choice of action space affects the complexity of the optimization landscape, which influences learning performance.
- Mechanism: Different action representations lead to different optimization surfaces for the policy gradient objective. Rugged landscapes with many local optima are harder to optimize than smooth ones.
- Core assumption: The shape of the optimization landscape directly correlates with learning difficulty and final performance.
- Evidence anchors:
  - Visual inspection of loss landscapes can provide intuitive insights into the difficulty of an optimization problem... we visualize both the reward and loss surfaces resulting from different action spaces in Figure 2
  - Figure 2(a4) shows that the loss surface for the Reacher torque control configuration is rugged and has numerous local optima. The difficulty of optimizing such a loss might explain the poor learning performance

### Mechanism 2
- Claim: Gradient estimation accuracy influences learning performance, but the relationship is complex and non-linear.
- Mechanism: Policy gradient methods rely on estimating gradients from sampled trajectories. More accurate gradient estimates should lead to better policy updates and faster convergence.
- Core assumption: Higher cosine similarity between estimated and true gradients leads to better learning performance.
- Evidence anchors:
  - We utilize the analysis method by Ilyas et al. [4] to measure and compare the quality of the gradient estimates in tasks with different action spaces
  - Figure 3 displays the results of the gradient analysis... The results appear counter-intuitive since the better-performing configurations for Reacher use worse gradients

### Mechanism 3
- Claim: The interaction between state and action representations can significantly impact learning performance.
- Mechanism: When states and actions are defined in the same space (e.g., joint space), the agent may find it easier to learn the mapping between them, potentially improving sample efficiency.
- Core assumption: Consistency between state and action spaces simplifies the learning problem.
- Evidence anchors:
  - In this section, we explore the interplay between state and action representations. Particularly, we investigate whether a task becomes easier to learn if both the states and actions are defined in the same space
  - Surprisingly, torque control outperforms both position control configurations, even though the task is intuitively quite easy to solve in these configurations

## Foundational Learning

- Concept: Reinforcement Learning and Policy Gradient Methods
  - Why needed here: The paper investigates how different action representations affect policy gradient algorithms, specifically PPO.
  - Quick check question: What is the key difference between policy gradient methods and value-based methods in reinforcement learning?

- Concept: Optimization Landscapes in Neural Networks
  - Why needed here: The paper uses visualization techniques to analyze the shape of the optimization landscape for different action representations.
  - Quick check question: How does the shape of an optimization landscape (e.g., rugged vs. smooth) affect the difficulty of optimization problems?

- Concept: Gradient Estimation and Variance Reduction
  - Why needed here: The paper analyzes the quality of gradient estimates and their impact on learning performance.
  - Quick check question: Why is reducing the variance of gradient estimates important in policy gradient methods?

## Architecture Onboarding

- Component map:
  RL Environment (OpenAI Gym / DeepMind Control Suite) -> RL Agent (PPO implementation from Stable-Baselines3) -> Linear Controllers (for velocity and position control) -> Analysis Tools (Optimization landscape visualization and gradient quality measurement)

- Critical path:
  1. Set up environment with different action representations
  2. Train PPO agents and collect learning curves
  3. Perform optimization landscape analysis at specific checkpoints
  4. Measure gradient estimation accuracy throughout training
  5. Analyze and interpret results

- Design tradeoffs:
  - Using default hyperparameters vs. tuned ones for fair comparison across action spaces
  - Approximation quality vs. computational cost in gradient analysis
  - Stochasticity in landscape visualization vs. reproducibility

- Failure signatures:
  - Inconsistent learning curves across random seeds
  - Noisy or non-reproducible optimization landscape visualizations
  - Poor correlation between gradient quality and learning performance

- First 3 experiments:
  1. Train PPO with torque control on Reacher and visualize the optimization landscape
  2. Compare gradient estimation accuracy for torque vs. position control on Walker-walk
  3. Analyze the effect of batch size on gradient quality and learning performance in Reacher

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively normalize analysis results across different stages of learning progress when comparing optimization landscape complexity and gradient estimation accuracy?
- Basis in paper: The authors state "Properties like the complexity of the optimization landscape and the accuracy of gradient estimates influence how quickly the agent progresses on the objective. However, agents on different levels of performance generally operate in different parts of the state space, which in turn can have an effect on these properties."
- Why unresolved: Current analysis methods evaluate properties at specific checkpoints but don't account for how these properties change as agents progress through different regions of the state space. This makes it difficult to determine if differences are due to the action representation itself or simply where in the state space the agent is operating.
- What evidence would resolve it: A framework for normalizing analysis metrics based on learning progress, such as comparing agents at equivalent performance levels or developing state-space-invariant measures of optimization landscape complexity and gradient quality.

### Open Question 2
- Question: What is the precise relationship between gradient estimation accuracy and learning performance in reinforcement learning?
- Basis in paper: The authors observe that "better-performing configurations for Reacher use worse gradients" and that "noisy gradient estimates can even be beneficial for escaping shallow local optima."
- Why unresolved: The paper demonstrates a complex, non-monotonic relationship between gradient accuracy and learning performance that varies across tasks and training stages. While more accurate gradients help in some cases (Appendix C), they don't always lead to better performance, suggesting multiple interacting factors.
- What evidence would resolve it: Systematic experiments varying gradient estimation accuracy across multiple tasks and training stages, combined with analysis of how gradient noise affects exploration, optimization dynamics, and escape from local optima.

### Open Question 3
- Question: How can we disentangle the effects of action representations from other factors like state representation, controller gains, and task design choices?
- Basis in paper: The authors note that "Changing the action representation typically affects multiple aspects of the learning process simultaneously" and that "Other choices in the task design, like the state representation, also have a significant impact on the learning performance."
- Why unresolved: When switching action representations, the authors simultaneously change the controller architecture, potentially affecting exploration patterns, optimization dynamics, and the relationship between actions and outcomes. Their experiments with joint-space Reacher show that intuitive hypotheses about state-action alignment don't always hold.
- What evidence would resolve it: Controlled experiments isolating individual factors (e.g., keeping controller gains constant while varying action spaces, or testing action spaces with different state representations) to identify which aspects of the learning process are most affected by each design choice.

## Limitations

- The relationship between gradient estimation accuracy and learning performance is complex and non-monotonic, making it difficult to draw definitive conclusions about which factor drives performance
- Missing information about specific controller gain values prevents exact reproduction of the experiments
- The effectiveness of optimization landscape visualization may vary across tasks, limiting its applicability as a universal analysis tool

## Confidence

- High confidence: Action space choice significantly impacts learning performance across benchmark tasks
- Medium confidence: Optimization landscape complexity explains performance differences in some tasks (like Reacher) but not others (like Walker-walk)
- Low confidence: The relationship between gradient estimation accuracy and learning performance is complex and non-monotonic

## Next Checks

1. Verify whether tuning controller gains for velocity and position control improves their performance relative to torque control across all tasks
2. Test whether adding entropy regularization or adjusting batch sizes can improve gradient quality and learning performance in tasks where they currently don't correlate
3. Apply the optimization landscape visualization technique to additional action space variants (e.g., mixed representations) to determine if landscape complexity remains the primary explanatory factor