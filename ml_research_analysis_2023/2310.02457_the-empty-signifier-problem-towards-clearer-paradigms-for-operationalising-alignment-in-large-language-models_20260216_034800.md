---
ver: rpa2
title: 'The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising
  "Alignment" in Large Language Models'
arxiv_id: '2310.02457'
source_url: https://arxiv.org/abs/2310.02457
tags:
- alignment
- human
- language
- arxiv
- dimensions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of how to operationalise the\
  \ abstract concept of \u201Calignment\u201D in large language models. It proposes\
  \ a framework that considers two core decisions: which dimensions of model behaviour\
  \ are included, and how definitions are ascribed to these dimensions."
---

# The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising "Alignment" in Large Language Models

## Quick Facts
- arXiv ID: 2310.02457
- Source URL: https://arxiv.org/abs/2310.02457
- Authors: 
- Reference count: 40
- Key outcome: Framework proposes four quadrants based on dimension specificity and definition authority to clarify how "alignment" is operationalized in LLM research

## Executive Summary
This paper addresses the fundamental challenge of operationalizing the abstract concept of "alignment" in large language models. The authors propose a framework that separates two core decisions: which dimensions of model behavior are considered important, and how definitions are ascribed to these dimensions. By mapping existing empirical literature onto four distinct quadrants based on these dimensions, the framework provides a shared vocabulary for articulating what alignment means in different contexts. The primary contribution is a conceptual tool to foster transparency and critical evaluation in alignment research, helping the community navigate the complexities of aligning LLMs with human populations.

## Method Summary
The paper proposes a two-dimensional framework for operationalizing alignment by analyzing how existing empirical literature on human feedback learning for LLM alignment makes two core decisions: 1) selecting which dimensions of model behavior to include (broad vs. specific), and 2) deciding how these dimensions are defined (prescriptive vs. descriptive). The authors map existing alignment studies onto a 2x2 framework and provide guidance on which paradigm to follow based on intended use cases and task complexity. The framework encourages clear communication about power structures and representativeness in alignment data collection.

## Key Results
- The framework clarifies alignment by separating dimension selection from definition authority
- It exposes power dynamics in alignment research by making explicit who decides dimensions and definitions
- The framework provides a shared vocabulary for the alignment community through quadrant mapping

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework clarifies alignment by separating dimension selection from definition authority
- Mechanism: By distinguishing between "which behaviors matter" and "how to define them," the framework creates a structured way to articulate implicit assumptions in alignment research
- Core assumption: Different alignment objectives require different combinations of dimension breadth and definition specificity
- Evidence anchors:
  - [abstract] "we propose a framework that demarcates: 1) which dimensions of model behaviour are considered important, then 2) how meanings and definitions are ascribed to these dimensions"
  - [section] "The process of creating alignment data encompasses two core decisions: 1) selecting dimensions to be included, and 2) deciding how dimensions are defined"

### Mechanism 2
- Claim: The framework exposes power dynamics in alignment research
- Mechanism: By making explicit who decides dimensions and definitions, it reveals whose values and perspectives are encoded in models
- Core assumption: Power structures in AI development are often hidden and need explicit examination
- Evidence anchors:
  - [abstract] "it is just as necessary to ask 'how to operationalise and communicate a certain concept'. However, for alignment, we also need to ask 'how to decide which concepts are relevant' in the first place"
  - [section] "Without such a shared language, there are dangers from obscurities: specific local particularities may be disguised or passed off as universalities"

### Mechanism 3
- Claim: The framework provides a shared vocabulary for the alignment community
- Mechanism: By mapping existing literature to four quadrants, it creates common terminology for discussing different approaches to alignment
- Core assumption: The alignment community lacks shared language for discussing methodological differences
- Evidence anchors:
  - [abstract] "we hope our framework will assist in transmuting the empty signifier of 'alignment' into actionable constructs, empirical signals and sampling strategies, that can be communicated, criticised, and re-conceptualised in a common language"
  - [section] "there is a lack of shared terminology for articulating what alignment actually means in a given empirical context"

## Foundational Learning

- Concept: Empty signifiers from post-structuralist theory
  - Why needed here: Provides theoretical foundation for understanding why "alignment" lacks clear definition
  - Quick check question: What distinguishes an empty signifier from a floating signifier according to Laclau?

- Concept: Two-dimensional framework design
  - Why needed here: Understanding how the framework maps dimension selection and definition authority creates four distinct paradigms
  - Quick check question: What are the four quadrants created by combining broad/specific dimensions with prescriptive/descriptive definitions?

- Concept: Annotator subjectivity and disagreement
  - Why needed here: Critical for understanding when disagreement is signal vs. noise in different paradigms
  - Quick check question: How does the framework treat inter-annotator disagreement differently in prescriptive vs. descriptive paradigms?

## Architecture Onboarding

- Component map: The framework has two axes (dimensions and definitions) creating four quadrants, each representing a different alignment paradigm
- Critical path: 1) Identify which quadrant your research falls into 2) Document dimensions and definitions clearly 3) Communicate power structures and representativeness
- Design tradeoffs: Broad dimensions capture more behaviors but lose specificity; prescriptive definitions reduce subjectivity but may exclude important perspectives
- Failure signatures: Unclear communication about which paradigm is being used, hidden power structures, treating disagreement as noise when it's actually signal
- First 3 experiments:
  1. Map existing alignment papers to the framework to identify which paradigms dominate
  2. Design a simple dataset collection protocol explicitly choosing one quadrant and documenting the choice
  3. Conduct a small study comparing how different paradigms handle the same alignment task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should alignment datasets be constructed to balance representation of diverse perspectives while maintaining consistent definitions of key dimensions like "helpfulness" and "harmlessness"?
- Basis in paper: [explicit] The paper discusses the tension between descriptive paradigms (capturing diverse beliefs) and prescriptive paradigms (encoding single beliefs) in alignment data collection.
- Why unresolved: The paper identifies this as a core challenge but does not provide a definitive framework for balancing these competing needs in practice.
- What evidence would resolve it: Empirical studies comparing model performance and societal impact when trained on datasets constructed using different combinations of breadth/specificity and descriptive/prescriptive approaches.

### Open Question 2
- Question: Who should be included in the pool of annotators for alignment datasets, and how can their identities and perspectives be documented and accounted for?
- Basis in paper: [explicit] The paper raises concerns about power dynamics and representation, noting that current datasets often rely on non-representative groups like US-based crowdworkers or interested netizens.
- Why unresolved: The paper highlights the importance of this issue but does not propose specific criteria or mechanisms for ensuring diverse and representative annotation pools.
- What evidence would resolve it: Studies measuring the impact of annotator diversity on model behavior and societal outcomes, along with best practices for documentation and mitigation of biases.

### Open Question 3
- Question: How can the boundaries of acceptable interpretations for alignment dimensions be determined and enforced, especially in cases where there is significant disagreement?
- Basis in paper: [explicit] The paper discusses the need to place boundaries on acceptable range of interpretations to avoid degenerative outcomes, but does not specify how these boundaries should be set.
- Why unresolved: This involves complex normative decisions that the paper acknowledges but does not attempt to resolve.
- What evidence would resolve it: Frameworks for participatory decision-making on alignment boundaries, along with empirical studies on the effects of different boundary-setting approaches on model behavior and societal impact.

## Limitations

- The framework's practical utility depends on researchers' willingness to explicitly adopt and document their paradigm choices
- The mapping of existing literature to quadrants relies heavily on interpretation, with several cited works falling into "edge cases"
- The framework's theoretical grounding in post-structuralist concepts may not translate directly to practical research decisions

## Confidence

**High Confidence**: The framework's core insight that dimension selection and definition authority are separate, critical decisions in alignment research. The distinction between prescriptive and descriptive definitions has clear practical implications for dataset creation and model evaluation.

**Medium Confidence**: The framework's ability to foster shared vocabulary and expose power structures in alignment research. While theoretically sound, the practical impact on research transparency and community discourse remains to be demonstrated.

**Low Confidence**: The framework's comprehensive coverage of all alignment research approaches. The "edge cases" and hybrid methodologies suggest the framework may not fully capture the complexity of current alignment practices.

## Next Checks

1. **Empirical Adoption Study**: Survey alignment researchers to determine if and how they use the framework when designing studies, measuring actual adoption rates and impact on research transparency.

2. **Edge Case Resolution Protocol**: Develop and test a standardized decision tree for classifying studies that exhibit characteristics of multiple quadrants, ensuring consistent application of the framework.

3. **Cross-Paradigm Comparison**: Select a common alignment task and implement it using approaches from different quadrants, measuring differences in outcomes, efficiency, and interpretability to validate the framework's practical implications.