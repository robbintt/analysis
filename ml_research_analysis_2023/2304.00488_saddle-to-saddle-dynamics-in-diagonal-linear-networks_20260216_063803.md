---
ver: rpa2
title: Saddle-to-Saddle Dynamics in Diagonal Linear Networks
arxiv_id: '2304.00488'
source_url: https://arxiv.org/abs/2304.00488
tags:
- which
- have
- iterates
- algorithm
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the trajectory of gradient flow over diagonal
  linear networks with vanishing initialization. It shows that the iterates successively
  jump from a saddle point of the training loss to another until reaching the minimum
  $\ell1$-norm solution.
---

# Saddle-to-Saddle Dynamics in Diagonal Linear Networks

## Quick Facts
- arXiv ID: 2304.00488
- Source URL: https://arxiv.org/abs/2304.00488
- Reference count: 40
- This paper studies the trajectory of gradient flow over diagonal linear networks with vanishing initialization, showing iterates successively jump from saddle to saddle until reaching the minimum ℓ1-norm solution.

## Executive Summary
This paper analyzes gradient flow dynamics in diagonal linear networks with vanishing initialization, proving that the optimization trajectory exhibits a saddle-to-saddle behavior. The iterates successively jump from one saddle point of the training loss to another, ultimately converging to the minimum ℓ1-norm solution. The authors characterize each saddle as the minimizer of the loss constrained to an active set of coordinates, and provide a recursive algorithm to determine the visited saddles and jumping times. The analysis leverages an arc-length time-reparametrization to track heteroclinic transitions between saddles, requiring minimal assumptions on the data and covering both under and overparametrized settings.

## Method Summary
The study examines gradient flow over diagonal linear networks where the regression vector β is represented as βw = u ⊙ v with w = (u,v) ∈ R²ᵈ. Starting from initialization u₀ = √2α1 and v₀ = 0, the paper shows that in the limit α → 0, the accelerated iterates βαₜ = βαₗₙ₍₁/ₐ₎ₜ converge to a piecewise constant limiting process that jumps from saddle to saddle. The authors employ a mirror flow reformulation with hyperbolic entropy potential and arc-length time reparametrization to analyze the heteroclinic transitions between critical points. A recursive algorithm, reminiscent of the Lasso path algorithm, characterizes the visited saddles and jumping times through constrained minimization problems.

## Key Results
- Gradient flow with vanishing initialization converges to a piecewise constant limiting process that jumps from saddle to saddle
- Each saddle corresponds to the minimizer of the loss constrained to an active set of coordinates
- The visited saddles and jumping times are characterized through a recursive algorithm analogous to the Homotopy method for Lasso
- The analysis covers both under and overparametrized settings with minimal assumptions on the data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient flow with vanishing initialization converges to a piecewise constant limiting process that jumps from saddle to saddle
- Mechanism: The time acceleration by ln(1/α) compensates for the slowdown near critical points, allowing the mirror flow to traverse the heteroclinic orbits between saddles
- Core assumption: The arc-length time reparametrization is a bijection and bounds the total path length independently of α
- Evidence anchors:
  - [abstract] "The analysis requires negligible assumptions on the data and covers both under and overparametrized settings."
  - [section] "The iterates successively jump from a saddle of the training loss to another until reaching the minimum ℓ1-norm solution."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.435, average citations=0.0
- Break condition: If the data matrix does not satisfy general position (Assumption 1), the saddle characterization algorithm may fail to be well-defined

### Mechanism 2
- Claim: Each saddle corresponds to the minimizer of the loss constrained to an active set of coordinates
- Mechanism: The subdifferential of the ℓ1-norm, ∂∥β∥1, enforces sign constraints that uniquely determine the next saddle when a coordinate hits ±1 in the integrated gradient
- Evidence anchors:
  - [section] "Each saddle corresponds to the minimiser of the loss constrained to an active set outside of which the coordinates must be zero."
  - [section] "The function st is therefore continuous and, as noted in Eq. (13), satisfies st ∈ ∂∥ ˜βt∥1."
  - [corpus] Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures
- Break condition: If the loss has multiple saddles with the same active set, uniqueness of the path is not guaranteed

### Mechanism 3
- Claim: The limiting process is characterized by an algorithm analogous to the Homotopy method for Lasso
- Mechanism: The jump times are determined by the first time a coordinate of the accumulated gradient reaches ±1, and the new saddle is the constrained minimizer of the loss under the updated active set
- Evidence anchors:
  - [section] "We explicitly characterise the visited saddles as well as the jumping times through a recursive algorithm reminiscent of the Homotopy algorithm used for computing the Lasso path."
  - [section] "We start from 0 and successively look at the breaking times of Eq. (13) and determine the unique update of ˜βt that keeps the conditions satisfied."
  - [corpus] SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics
- Break condition: If the loss landscape has degenerate critical points (zero curvature), the jump detection may fail

## Foundational Learning

- **Mirror descent with hyperbolic entropy potential**: Provides a convex reformulation of the non-convex gradient flow, enabling convergence analysis
  - Quick check question: How does the hyperbolic entropy potential φα behave as α → 0?

- **Arc-length time reparametrization**: Ensures bounded path length and uniform convergence of the accelerated flow to the limiting piecewise constant process
  - Quick check question: Why does the standard time parametrization fail for vanishing initialization?

- **Subdifferential inclusion of the ℓ1-norm**: Characterizes the optimality conditions for the limiting saddle-to-saddle process and links to Lasso path algorithms
  - Quick check question: What does st ∈ ∂∥ ˜βt∥1 imply about the signs of active coordinates?

## Architecture Onboarding

- **Component map**: Data matrix X ∈ ℝⁿˣᵈ with general position assumption -> 2-layer diagonal linear network βw = u ⊙ v -> Gradient flow dynamics dwt = -∇F(wt)dt with vanishing initialization -> Mirror flow reformulation with hyperbolic entropy -> Arc-length time reparametrization τα(t) -> Recursive saddle characterization algorithm (Algorithm 1)

- **Critical path**:
  1. Initialize u0 = √2α1, v0 = 0
  2. Compute accelerated iterates ˜βαₜ = βαₗₙ₍₁/ₐ₎ₜ
  3. Apply arc-length reparametrization to obtain (ˆtα, ˆβα)
  4. Extract convergent subsequence and analyze limiting dynamics
  5. Characterize saddles and jump times via Algorithm 1

- **Design tradeoffs**:
  - Small initialization → slow dynamics but low ℓ1-norm solution
  - Large initialization → fast dynamics but high ℓ1-norm solution
  - General position assumption ensures unique saddle characterization

- **Failure signatures**:
  - Iterates stuck at origin for vanishing α (no time acceleration)
  - Non-unique saddle characterization due to data degeneracy
  - Path length explosion violating Proposition 6

- **First 3 experiments**:
  1. Reproduce Figure 1: Vary α and observe saddle-to-saddle behavior in 2D
  2. Test Algorithm 1 on synthetic data with known minimum ℓ1-norm solution
  3. Validate Proposition 3 by comparing accelerated and rescaled iterates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the saddle-to-saddle dynamics generalize to other activation functions beyond linear networks?
- Basis in paper: [explicit] The paper focuses on diagonal linear networks with identity activation function. The authors note "We point out that the trajectory ofβα t exactly matches that of another common parametrisationβw :=w2 + −w2 −, with initialisation w+,0 =w−,0 =α1" but don't explore other activations.
- Why unresolved: The current analysis leverages specific properties of linear networks and mirror flows that may not extend to nonlinear activations. The saddle-to-saddle phenomenon could be unique to the linear setting.
- What evidence would resolve it: Experimental results showing similar incremental learning dynamics in diagonal networks with ReLU, tanh, or other activation functions, or a theoretical extension of the analysis to nonlinear activations.

### Open Question 2
- Question: Can we derive non-asymptotic convergence rates for the saddle-to-saddle dynamics?
- Basis in paper: [inferred] The authors note "We point out that our result provides no speed of convergence of ˜βα towards ˜β◦. We believe that a non-asymptotic result is challenging and leave it as future work." They experimentally observe "the convergence rate quickly degrades after each saddle."
- Why unresolved: The current proof relies on limiting arguments and doesn't provide quantitative bounds on how quickly the iterates approach the limiting process. The discontinuities in the limiting process make non-asymptotic analysis difficult.
- What evidence would resolve it: A bound on ∥ ˜βα − ˜β◦ ∥∞ that depends on α and time t, or experimental measurements of convergence rates across different problem instances.

### Open Question 3
- Question: How does the saddle-to-saddle dynamics relate to generalization performance?
- Basis in paper: [explicit] The paper discusses implicit regularization and notes that small initialization leads to minimum ℓ1-norm solutions. The authors state "many papers have therefore shown that gradient methods have the fortunate property of asymptotically leading to solutions which have a well-behaving structure" and reference works on implicit regularization.
- Why unresolved: While the paper characterizes the trajectory of gradient flow, it doesn't establish a connection between the saddle-to-saddle process and generalization bounds. The incremental learning interpretation suggests a potential link to model complexity.
- What evidence would resolve it: Generalization bounds that depend on the number of saddles visited or the path complexity of the trajectory, or empirical studies correlating saddle-to-saddle dynamics with test performance.

## Limitations

- The proof relies critically on the general position assumption for X, which ensures unique saddle characterization but may be violated in practical datasets
- The time reparametrization requires the path to have bounded length, which holds under the assumptions but could fail for pathological data configurations
- The recursive algorithm's termination bound is stated as crude, suggesting potential inefficiencies in practical implementation

## Confidence

- **High confidence**: The core saddle-to-saddle dynamics mechanism and the characterization of visited saddles through Algorithm 1 are well-supported by the mathematical framework and consistent with related work on mirror flows
- **Medium confidence**: The convergence of the accelerated flow to the limiting piecewise constant process relies on technical arc-length parametrization arguments that are sketched but not fully detailed in the paper
- **Medium confidence**: The connection to Lasso path algorithms is conceptually sound but the precise relationship between the saddle characterization and coordinate descent steps needs further validation

## Next Checks

1. Implement Algorithm 1 on synthetic data where the minimum ℓ1-norm solution is known analytically, and verify that the computed saddle sequence matches theoretical predictions

2. Test the robustness of the saddle characterization by introducing small perturbations to the data matrix X and measuring how the visited saddles and jump times change

3. Validate the time acceleration mechanism by comparing the convergence rates of standard gradient flow versus the accelerated flow for different initialization scales α