---
ver: rpa2
title: Revisiting Relation Extraction in the era of Large Language Models
arxiv_id: '2305.05003'
source_url: https://arxiv.org/abs/2305.05003
tags:
- relations
- text
- gpt-3
- explanation
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) like GPT-3 and Flan-T5 achieve state-of-the-art
  results on relation extraction (RE) when treated as a sequence-to-sequence task
  and evaluated with human judgment rather than exact string matching. Few-shot GPT-3
  prompting yields near-SOTA performance, and Flan-T5 fine-tuned with GPT-3-generated
  Chain-of-Thought explanations reaches SOTA F1 scores (e.g., 92.17 ADE, 80.76 CoNLL,
  95.23 NYT).
---

# Revisiting Relation Extraction in the era of Large Language Models

## Quick Facts
- arXiv ID: 2305.05003
- Source URL: https://arxiv.org/abs/2305.05003
- Reference count: 18
- Primary result: GPT-3 and Flan-T5 achieve SOTA relation extraction performance when evaluated with human judgment rather than exact string matching

## Executive Summary
This paper challenges the conventional evaluation of relation extraction (RE) by large language models (LLMs), demonstrating that exact string matching overpenalizes LLM outputs. Through human evaluation, the authors find that up to 51.67% of GPT-3's "false positives" and 32.61% of "false negatives" are actually correct. By treating RE as a sequence-to-sequence task and using few-shot prompting with GPT-3 or fine-tuning Flan-T5 with Chain-of-Thought explanations, the paper achieves state-of-the-art F1 scores across multiple benchmarks. The work highlights the need for evaluation methods that account for the generative flexibility of LLMs rather than relying on rigid exact matching.

## Method Summary
The paper reformulates relation extraction as a sequence-to-sequence generation task where LLMs output linearized relation strings. GPT-3 is evaluated in a few-shot in-context learning setting with 12-20 examples per dataset, while Flan-T5 is fine-tuned using GPT-3-generated Chain-of-Thought explanations as additional supervision. The authors evaluate performance on four standard RE datasets (ADE, CoNLL04, NYT, DocRED) using both exact string matching and manual human evaluation through Mechanical Turk to identify false positives and negatives that are actually correct.

## Key Results
- GPT-3 few-shot prompting achieves near-SOTA performance comparable to fully supervised models without fine-tuning
- Flan-T5 fine-tuned with GPT-3-generated Chain-of-Thought explanations reaches SOTA F1 scores (92.17 ADE, 80.76 CoNLL, 95.23 NYT)
- Manual evaluation reveals exact matching overpenalizes LLM outputs, with up to 51.67% of "false positives" and 32.61% of "false negatives" being actually correct

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can treat relation extraction as a sequence-to-sequence task by generating linearized relation strings.
- Mechanism: Instead of classifying token spans, the LLM outputs a structured string encoding the entities and their relationship, allowing it to leverage generative capabilities.
- Core assumption: The model can generate valid and accurate relation strings that match the intended structured format.
- Evidence anchors:
  - [abstract] "Recent work has instead treated the problem as a sequence-to-sequence task, linearizing relations between entities as target strings to be generated conditioned on the input."
  - [section 2] "We treat RE as a conditional text generation task. Concretely, for a dataset of size N, we model the probability of generating a linearized string y of a relation triplet (entity_1, relation_type, entity_2) conditioned on a context string C."
  - [corpus] Weak: Most cited papers focus on fine-tuning rather than few-shot generative RE, but the concept is similar.
- Break condition: The model generates relation strings that are syntactically valid but semantically incorrect or that deviate too much from the expected format, making parsing difficult.

### Mechanism 2
- Claim: Few-shot in-context learning with GPT-3 can achieve near state-of-the-art performance on relation extraction.
- Mechanism: By providing a small number of training examples as context in the prompt, GPT-3 can learn the task without fine-tuning and generate accurate relation strings.
- Core assumption: GPT-3's few-shot learning capability is strong enough to generalize from a small number of examples to the test set.
- Evidence anchors:
  - [abstract] "Few-shot prompting with GPT-3 achieves near SOTA performance, i.e., roughly equivalent to existing fully supervised models."
  - [section 3.3] "Using the above prompts and manual annotation process just described, we find that in most cases GPT-3 performs comparably to current fully supervised SOTA RE models without fine-tuning and given only 12-20 training examples."
  - [corpus] Weak: Most prior work on few-shot RE uses fine-tuning, but GPT-3's few-shot ability is known in other domains.
- Break condition: The task requires too many training examples for few-shot learning to be effective, or the examples provided are not representative of the test data.

### Mechanism 3
- Claim: Chain-of-thought (CoT) explanations generated by GPT-3 can improve the performance of fine-tuned Flan-T5 on relation extraction.
- Mechanism: By augmenting the training data with CoT explanations, the model learns not just the relation labels but also the reasoning behind them, leading to better generalization.
- Core assumption: The CoT explanations capture the essential reasoning steps needed to derive the correct relations.
- Evidence anchors:
  - [abstract] "We find that Flan-T5 is not as capable in the few-shot setting, but supervising and fine-tuning it with Chain-of-Thought (CoT) style explanations (generated via GPT-3) yields SOTA results."
  - [section 4.2.2] "We augment target relations used to train Flan-T5 with CoT strings automatically generated by GPT-3 over the training dataset."
  - [corpus] Weak: While CoT has been used in other domains, its effectiveness in relation extraction is not well established in the literature.
- Break condition: The CoT explanations are not accurate or do not capture the necessary reasoning, leading to confusion or incorrect learning.

## Foundational Learning

- Concept: Sequence-to-sequence learning
  - Why needed here: Relation extraction is reformulated as a sequence-to-sequence task, where the input is a text and the output is a linearized relation string.
  - Quick check question: What is the difference between sequence-to-sequence learning and traditional classification in the context of relation extraction?

- Concept: Few-shot learning
  - Why needed here: GPT-3 is evaluated in a few-shot setting, where it learns from a small number of examples provided in the prompt.
  - Quick check question: How does few-shot learning differ from zero-shot learning and traditional fine-tuning?

- Concept: Chain-of-thought reasoning
  - Why needed here: CoT explanations are used to provide additional supervision to Flan-T5, helping it learn the reasoning behind the relations.
  - Quick check question: What is the purpose of chain-of-thought reasoning in the context of relation extraction?

## Architecture Onboarding

- Component map: GPT-3 -> Flan-T5 -> Human annotators
- Critical path:
  1. Prepare the dataset and linearize the relation strings.
  2. For GPT-3, create prompts with few examples and generate relation strings.
  3. For Flan-T5, fine-tune with and without CoT explanations.
  4. Evaluate the generated relation strings using exact matching and manual annotation.

- Design tradeoffs:
  - Using GPT-3 for few-shot learning vs. fine-tuning a smaller model like Flan-T5.
  - Relying on manual annotation for evaluation vs. developing automated metrics.
  - Including CoT explanations for additional supervision vs. using only the relation labels.

- Failure signatures:
  - Low precision or recall due to incorrect or missing relation strings.
  - High false positive or false negative rates due to lenient or strict evaluation criteria.
  - Poor generalization from few-shot examples or CoT explanations.

- First 3 experiments:
  1. Evaluate GPT-3's few-shot performance on a small dataset like ADE using the provided prompts.
  2. Fine-tune Flan-T5 on a larger dataset like CoNLL with and without CoT explanations.
  3. Compare the performance of GPT-3 and Flan-T5 on a held-out test set using exact matching and manual annotation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we automate the evaluation of large language model outputs for relation extraction without relying on exact string matching?
- Basis in paper: [explicit] The paper discusses the challenges of evaluating LLM outputs for RE due to their flexibility in expressing relations and the limitations of exact string matching. It proposes using human annotators to manually re-evaluate false positives and negatives.
- Why unresolved: The paper only provides preliminary results using a simple BERT classifier to identify false positives and negatives, but this approach is not yet fully developed or tested on large datasets.
- What evidence would resolve it: A comprehensive evaluation of different automated methods for assessing LLM outputs in RE, comparing their accuracy and efficiency to human annotation, would provide insights into the best approach.

### Open Question 2
- Question: How does the performance of LLMs for relation extraction vary across different languages and cultural contexts?
- Basis in paper: [inferred] The paper focuses on English datasets and acknowledges the need to explore other languages and cultural contexts.
- Why unresolved: The paper does not provide any empirical evidence or analysis of LLM performance in other languages or cultural contexts.
- What evidence would resolve it: Experiments evaluating LLMs on RE tasks in multiple languages and cultural contexts, with comparisons to human performance, would shed light on the generalizability and limitations of these models.

### Open Question 3
- Question: How can we improve the quality and diversity of chain-of-thought explanations generated by LLMs for relation extraction?
- Basis in paper: [explicit] The paper uses GPT-3 to generate chain-of-thought explanations for relation extraction and finds that these explanations can be used to fine-tune smaller models for improved performance.
- Why unresolved: The paper does not provide a detailed analysis of the quality and diversity of the generated explanations or explore methods to enhance them.
- What evidence would resolve it: A systematic evaluation of the quality and diversity of chain-of-thought explanations generated by LLMs, along with experiments on methods to improve their generation, would provide insights into their effectiveness and potential for enhancing RE performance.

## Limitations
- Heavy reliance on manual evaluation introduces subjective bias and makes results difficult to reproduce or scale
- Evaluation only examined a small subset of generated outputs (false positives and false negatives), potentially missing systematic errors in other categories
- Lacks ablation studies on prompt design choices, making it unclear which aspects of the few-shot approach are most critical to performance

## Confidence
- High Confidence: GPT-3's few-shot performance achieving near-SOTA results when evaluated with human judgment
- Medium Confidence: Flan-T5 fine-tuned with CoT explanations achieving SOTA results
- Medium Confidence: The claim that exact matching overpenalizes LLM outputs

## Next Checks
1. Conduct inter-annotator agreement studies with multiple annotators per sample and report Cohen's kappa scores to quantify the reliability of the manual evaluation process
2. Systematically vary the number of examples, instructional wording, and formatting in few-shot prompts to identify which components contribute most to GPT-3's performance
3. Create a semi-automated evaluation system that combines exact matching with pattern-based matching rules to reduce reliance on manual annotation while maintaining evaluation accuracy