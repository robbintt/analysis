---
ver: rpa2
title: A Language Model with Limited Memory Capacity Captures Interference in Human
  Sentence Processing
arxiv_id: '2310.16142'
source_url: https://arxiv.org/abs/2310.16142
tags:
- retrieval
- attention
- memory
- attraction
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a recurrent neural network with a single
  attention head to model human sentence processing. The model is trained on a word
  prediction objective and an auxiliary syntactic objective.
---

# A Language Model with Limited Memory Capacity Captches Interference in Human Sentence Processing

## Quick Facts
- arXiv ID: 2310.16142
- Source URL: https://arxiv.org/abs/2310.16142
- Reference count: 21
- A single-headed attention RNN captures semantic and syntactic interference effects in human sentence processing experiments

## Executive Summary
This paper introduces a recurrent neural network with a single attention head to model human sentence processing. The model is trained on a word prediction objective and an auxiliary syntactic objective, and is evaluated on its ability to track subject-verb dependencies and capture interference effects in agreement attraction. The key insight is that limiting the model's memory retrieval to a single operation per timestep can capture interference effects observed in human experiments, without the need for hand-picked linguistic features.

## Method Summary
The method involves training a recurrent neural network with a single self-attention head on a word prediction objective and an auxiliary CCG supertagging objective. The model is evaluated on its ability to track subject-verb dependencies over varying distances and capture interference effects in agreement attraction experiments. The evaluation includes analyzing the model's surprisal estimates and relative attention measures on experimental stimuli from Laurinavichyute and von der Malsburg (2022) Experiment 3.

## Key Results
- The single attention head model captures subject-verb dependencies over long distances and with intervening distractor nouns
- The model shows both semantic and syntactic interference effects in agreement attraction, without hand-picked linguistic features
- Surprisal estimates and relative attention measures from the model predict human plausibility judgments in agreement attraction experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single attention head limits memory retrieval to one operation per timestep, making the model more cognitively plausible.
- Mechanism: By restricting the model to one attention head, only one weighted sum over past representations is computed at each timestep, mirroring the single-retrieval assumption in cue-based retrieval theories.
- Core assumption: Human working memory can only perform one retrieval operation at a time.
- Evidence anchors:
  - [abstract]: "we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories."
  - [section]: "Our single-headed model obviates the need to identify or aggregate attention heads."
  - [corpus]: "Cross-Linguistic Analysis of Memory Load in Sentence Comprehension: Linear Distance and Structural Density" (weak anchor, does not directly support this claim).
- Break condition: If the model shows interference effects inconsistent with single-retrieval limitations, the mechanism fails.

### Mechanism 2
- Claim: The attention mechanism implicitly learns cue and target features through training objectives, eliminating the need for hand-picked linguistic features.
- Mechanism: The model optimizes a word prediction and syntactic objective, causing the attention query and key vectors to develop features that best predict upcoming words and supertags.
- Core assumption: Neural networks can implicitly learn optimal features for prediction without explicit feature engineering.
- Evidence anchors:
  - [abstract]: "Crucially, in their model, all of the features used to compute attention scores were implicitly learned through the model’s next word prediction training objective; no linguistic features need to be hand-picked."
  - [section]: "NLMs with self-attention implicitly learn features to access past representations which are optimal for predicting upcoming words."
  - [corpus]: "Strategic resource allocation in memory encoding: An efficiency principle shaping language processing" (weak anchor, discusses resource allocation but not implicit feature learning).
- Break condition: If the model requires hand-picked features to capture interference effects, the mechanism breaks.

### Mechanism 3
- Claim: Surprisal and relative attention measures from the model capture semantic and syntactic interference effects in agreement attraction experiments.
- Mechanism: Surprisal reflects prediction difficulty, while relative attention measures the proportion of attention allocated to the subject versus attractor, both of which correlate with human plausibility judgments.
- Core assumption: Human processing difficulty in agreement attraction is linked to both predictability and retrieval interference.
- Evidence anchors:
  - [abstract]: "We show that our model’s single attention head captures semantic and syntactic interference effects observed in human experiments."
  - [section]: "Both model surprisal estimates and relative attention from our model’s self-attention mechanism both predicted the presence of semantic and agreement attraction."
  - [corpus]: "If Attention Serves as a Cognitive Model of Human Memory Retrieval, What is the Plausible Memory Representation?" (weak anchor, discusses attention as a cognitive model but not specific interference effects).
- Break condition: If the model fails to predict the direction or magnitude of interference effects, the mechanism fails.

## Foundational Learning

- Concept: Attention mechanisms in neural networks
  - Why needed here: Understanding how self-attention works is crucial to grasping how the model implements cue-based retrieval.
  - Quick check question: How does self-attention compute a weighted sum of past representations?

- Concept: Cue-based retrieval theories of sentence processing
  - Why needed here: The model is designed to mimic the assumptions of cue-based retrieval, so familiarity with these theories is essential.
  - Quick check question: What is the key difference between cue-based retrieval and expectation-based theories of sentence processing?

- Concept: Agreement attraction and semantic interference
  - Why needed here: The model is evaluated on its ability to capture these phenomena, so understanding what they are and how they manifest is crucial.
  - Quick check question: How do agreement attraction and semantic attraction differ in their underlying mechanisms?

## Architecture Onboarding

- Component map:
  - Word embedding layer -> Attention query generator -> Attention mechanism -> Value combiner -> Feedforward layers -> Loss functions

- Critical path:
  1. Word embedding is concatenated with previous hidden state.
  2. Feedforward layer generates attention query.
  3. Attention mechanism computes weights over past key vectors.
  4. Weighted sum of past value vectors is computed.
  5. Concatenated representations are processed by feedforward layers.
  6. Hidden state is used for prediction and passed to next timestep.

- Design tradeoffs:
  - Single attention head vs. multiple heads: Simpler model with more cognitive plausibility vs. more expressive model.
  - Word-level tokenization vs. subword tokenization: Simpler input space vs. handling of rare words.
  - Recurrent architecture vs. transformer: More memory constraints vs. better handling of long-range dependencies.

- Failure signatures:
  - Model fails to track subject-verb dependencies: Attention mechanism not learning relevant features.
  - Model shows no interference effects: Retrieval mechanism not sensitive to similarity-based interference.
  - Model shows incorrect interference patterns: Retrieval mechanism learning incorrect features.

- First 3 experiments:
  1. Verify that the model can track subject-verb dependencies over varying distances.
  2. Evaluate the model's surprisal estimates on agreement attraction stimuli.
  3. Evaluate the model's relative attention measures on semantic attraction stimuli.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the model's attention mechanism capture other dependency types, such as reflexive anaphora or filler-gap dependencies?
- Basis in paper: [explicit] The paper discusses investigating other dependency types in future work, but does not report results for these types.
- Why unresolved: The paper only evaluates the model on subject-verb dependencies, and future work is needed to assess performance on other dependency types.
- What evidence would resolve it: Experiments evaluating the model's attention patterns and interference effects on reflexive anaphora and filler-gap dependencies would resolve this question.

### Open Question 2
- Question: Does the model learn features corresponding to those posited in existing cue-based retrieval models, such as [+/-subject] or [+/-animate]?
- Basis in paper: [inferred] The paper notes that it is unclear if the model implicitly learns features consistent with existing theories, and suggests using causal representational probing methods to determine this.
- Why unresolved: The paper does not report results from probing experiments or provide evidence about the specific features learned by the model.
- What evidence would resolve it: Results from causal representational probing experiments that identify the features learned by the model and assess their correspondence to features in existing theories would resolve this question.

### Open Question 3
- Question: How does training the model on more developmentally-plausible corpora, such as child-directed speech, affect its memory retrieval behaviors?
- Basis in paper: [explicit] The paper notes that training on child-directed speech leads to more rapid humanlike generalization, and suggests investigating whether training CBR-RNNs on such corpora leads to more humanlike memory retrieval behaviors.
- Why unresolved: The paper only reports results from training on Wikipedia articles, and future work is needed to assess the impact of training data on the model's behaviors.
- What evidence would resolve it: Experiments training the model on child-directed speech and evaluating its memory retrieval behaviors compared to training on Wikipedia articles would resolve this question.

## Limitations

- Weak evidence provided for some key claims, particularly regarding the cognitive plausibility of the single attention head
- Limited evaluation of the model's performance on linguistic phenomena beyond agreement attraction
- Unclear implementation details of the CBR-RNN architecture, making it difficult to assess the validity of the model's claims

## Confidence

- The single attention head limits memory retrieval to one operation per timestep, making the model more cognitively plausible (Medium)
- The attention mechanism implicitly learns cue and target features through training objectives, eliminating the need for hand-picked linguistic features (Medium)
- Surprisal and relative attention measures from the model capture semantic and syntactic interference effects in agreement attraction experiments (Medium)

## Next Checks

1. Replicate the study using a larger and more diverse set of experimental stimuli, including materials from multiple languages and linguistic phenomena beyond agreement attraction.
2. Compare the performance of the single attention head model to a multi-head attention model on the same tasks, to directly assess the impact of limiting memory retrieval on capturing interference effects.
3. Conduct a series of controlled experiments manipulating the model's attention mechanism (e.g., varying the number of heads, the attention function) to identify the specific components necessary for capturing interference effects.