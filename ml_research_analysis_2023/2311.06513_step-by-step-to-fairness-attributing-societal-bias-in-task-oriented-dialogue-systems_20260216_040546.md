---
ver: rpa2
title: 'Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue
  Systems'
arxiv_id: '2311.06513'
source_url: https://arxiv.org/abs/2311.06513
tags:
- bias
- language
- dialogue
- association
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a method to attribute bias in task-oriented
  dialogue (TOD) systems to their components: the API call model, the database, and
  the response generation model. The authors propose a diagnosis method that uses
  perturbed user utterances to measure the bias of each component by updating their
  settings and observing the change in fairness.'
---

# Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems

## Quick Facts
- arXiv ID: 2311.06513
- Source URL: https://arxiv.org/abs/2311.06513
- Reference count: 21
- Key outcome: Bias in TOD systems primarily stems from the response generation model, with race being the most biased demographic axis and gender the least.

## Executive Summary
This paper introduces a novel method to attribute bias in task-oriented dialogue (TOD) systems to their individual components: the API call model, the database, and the response generation model. The authors propose a diagnosis approach that uses perturbed user utterances to measure each component's bias contribution by updating component settings and observing changes in fairness scores. Experiments across three transformer models (GPT-2, BART, T5) and two datasets (Google SGD, Taskmaster 2) reveal that response generation models are the primary source of bias, with race-based disparities being most pronounced and gender-based disparities least severe. The method enables granular understanding of bias sources, potentially facilitating more targeted mitigation strategies.

## Method Summary
The authors propose a component-level bias attribution method for TOD systems. They systematically perturb user utterances, measure fairness using a newly designed metric (Fairscore) based on BLEU score differences between original and perturbed responses, then isolate each component's bias contribution by updating its settings and observing fairness changes. The approach measures accumulated bias, updates component settings to eliminate bias, and attributes the resulting bias change to that specific component. This process is applied across three transformer models trained on two TOD datasets to analyze bias patterns across demographic axes including gender, age, and race.

## Key Results
- Response generation models contribute the most bias to TOD systems
- Race-based bias is most severe across all tested models
- Gender-based bias is least severe, with models showing reduced fairness when source attributes are female or non-binary
- The proposed Fairscore metric effectively captures bias differences between original and perturbed utterances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bias attribution works by isolating each TOD component and measuring the change in fairness score when that component is perturbed.
- Mechanism: The proposed method systematically perturbs user utterances, feeds them through the TOD system, and then isolates each component (API call model, database, response generation model) by updating its settings. The change in the fairness metric (Fairscore) after each component update reveals its contribution to the overall bias.
- Core assumption: The bias change observed after perturbing a component is solely attributable to that component, assuming other components remain unchanged.
- Evidence anchors:
  - [abstract] "The authors propose a diagnosis method that uses perturbed user utterances to measure the bias of each component by updating their settings and observing the change in fairness."
  - [section] "After measuring the accumulated bias, we update each componentâ€™s setting to eliminate the resulting bias. The bias change after the component setting is updated can be regarded as the bias contribution of the component."
- Break condition: If components interact in complex ways, isolating one may not cleanly attribute bias. If the perturbation method itself introduces confounding bias, the attribution becomes unreliable.

### Mechanism 2
- Claim: Fairness in TOD systems is quantified by comparing the helpfulness (BLEU score) of responses to original versus perturbed user utterances.
- Mechanism: The Fairscore metric calculates the absolute difference between BLEU scores of responses to original and perturbed utterances, normalized by the original helpfulness. This captures whether the system treats different demographic groups equitably.
- Core assumption: BLEU score is a valid proxy for helpfulness, and the absolute difference normalized by original score effectively captures bias.
- Evidence anchors:
  - [section] "We utilize the concepts of helpfulness (Sun et al. 2022) and perturbation (Qian et al. 2022) to define the metric. That is, the model responses should have similar helpfulness for original and perturbed user utterances."
  - [section] "We denote the TOD system as ft, original user utterances as ut, original responses as ft(ut), and original ground truths as gt. Perturbed user utterances and ground truths are denoted as eut and egt, respectively. As aforementioned, we expect a fair model to generate responses with similar helpfulness for original and perturbed user utterances."
- Break condition: If BLEU score does not correlate well with actual helpfulness, or if the normalization does not adequately account for varying utterance complexities, the metric may misrepresent fairness.

### Mechanism 3
- Claim: Bias in TOD systems is primarily driven by the response generation model, with the API call model and database contributing less significantly.
- Mechanism: Experimental results across multiple datasets and model architectures show that when bias is attributed to components, the response generation model consistently accounts for the largest share of bias, while the API call model and database contribute less.
- Core assumption: The experimental setup accurately measures and isolates the bias contribution of each component.
- Evidence anchors:
  - [abstract] "The results show that the bias of a TOD system mainly comes from the response generation model."
  - [section] "Experimental results show that the bias of a TOD system usually comes from the response generation model."
- Break condition: If the experimental setup does not properly isolate component effects, or if the datasets used are not representative of real-world bias, the conclusion about component contributions may be inaccurate.

## Foundational Learning

- Concept: Task-oriented dialogue (TOD) systems
  - Why needed here: Understanding the architecture and function of TOD systems is crucial for grasping how bias can be introduced and attributed to different components.
  - Quick check question: What are the three main components of a typical TOD system, and what is the role of each component in processing user utterances?

- Concept: Fairness metrics in NLP
  - Why needed here: The paper introduces a novel fairness metric (Fairscore) that is central to measuring and attributing bias. Understanding existing fairness metrics and their limitations is important for appreciating the contribution of this work.
  - Quick check question: What are some common fairness metrics used in NLP, and what are their limitations when applied to task-oriented dialogue systems?

- Concept: Perturbation techniques in NLP
  - Why needed here: The proposed method relies on perturbing user utterances to measure bias. Understanding how perturbation techniques work and their effectiveness in exposing bias is essential for evaluating the proposed approach.
  - Quick check question: What are some common perturbation techniques used in NLP, and how can they be used to measure bias in language models?

## Architecture Onboarding

- Component map: User Utterance -> API Call Model -> Database -> Response Generation Model -> Response

- Critical path:
  1. User utterance is received
  2. API call model generates an API call
  3. Database is queried using the API call
  4. Response generation model generates a response
  5. Perturbation module generates perturbed user utterances
  6. Fairscore calculator computes the fairness metric

- Design tradeoffs:
  - Granularity vs. complexity: Attributing bias to individual components provides more insight but increases the complexity of the attribution process
  - Perturbation method vs. bias detection: The choice of perturbation method can affect the sensitivity and specificity of bias detection
  - Fairness metric vs. practical relevance: The chosen fairness metric should align with real-world notions of fairness and be practically applicable

- Failure signatures:
  - Inconsistent Fairscore values across different perturbation runs may indicate instability in the bias attribution method
  - Unexpected bias contributions from certain components may suggest issues with the experimental setup or the datasets used
  - Lack of correlation between Fairscore and other fairness metrics may indicate limitations in the proposed metric

- First 3 experiments:
  1. Replicate the bias attribution experiments on a different TOD dataset to assess the generalizability of the findings
  2. Investigate the impact of different perturbation methods on the bias attribution results to evaluate the robustness of the approach
  3. Compare the Fairscore metric with other existing fairness metrics on the same TOD systems to assess its effectiveness and limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the bias attribution method generalize to other NLP tasks beyond task-oriented dialogue systems?
- Basis in paper: [explicit] The authors state "We envision our method as a potential approach to studying bias interpretation in systems composed of cascaded modules in the future."
- Why unresolved: The paper only applies the method to TOD systems, so its effectiveness on other NLP tasks is unknown.
- What evidence would resolve it: Applying the bias attribution method to other NLP tasks (e.g., machine translation, text summarization) and comparing the results to those obtained with existing bias evaluation methods.

### Open Question 2
- Question: How does the proposed fairness metric compare to existing bias evaluation metrics in terms of effectiveness and sensitivity?
- Basis in paper: [explicit] The authors propose a new fairness metric based on BLEU score differences between original and perturbed utterances, but do not compare it to other metrics.
- Why unresolved: The paper does not include a comparison of the proposed metric to existing bias evaluation metrics, so its relative effectiveness is unclear.
- What evidence would resolve it: Conducting experiments to compare the proposed fairness metric to existing metrics (e.g., demographic parity, equalized odds) on various NLP tasks and datasets.

### Open Question 3
- Question: What are the potential biases introduced by the perturbation method used in the bias attribution process?
- Basis in paper: [inferred] The authors use a perturbation method to generate counterfactual examples, but do not discuss potential biases introduced by this method.
- Why unresolved: The paper does not analyze the potential biases in the perturbation method, so its impact on the bias attribution results is unclear.
- What evidence would resolve it: Analyzing the perturbations generated by the method and evaluating their potential biases, e.g., by comparing the distributions of perturbed examples to the original dataset.

## Limitations

- The method assumes clean isolation of components, which may not reflect real-world TOD system interactions
- The Fairscore metric relies on BLEU score as a proxy for helpfulness, which may not capture all aspects of fairness
- The experiments are limited to two specific datasets that may not represent the full diversity of real-world TOD scenarios

## Confidence

- High Confidence: The experimental results showing that the response generation model contributes the most to bias across different datasets and model architectures
- Medium Confidence: The claim that models are most biased on race and least biased on gender, given the limited number of datasets and perturbation methods used
- Low Confidence: The generalizability of the proposed method to other TOD systems or bias measurement scenarios, due to the lack of diverse datasets and detailed methodological explanations

## Next Checks

1. Replicate the bias attribution experiments on additional TOD datasets (e.g., MultiWOZ, Schema-Guided Dialogue) to assess the method's robustness and generalizability across different dialogue scenarios and demographic distributions.

2. Investigate the interactions between TOD components by systematically varying multiple components simultaneously and measuring the resulting bias changes. This will help determine if the proposed method adequately captures complex component interactions.

3. Compare the Fairscore metric with established fairness metrics (e.g., demographic parity, equal opportunity) on the same TOD systems to evaluate its effectiveness in capturing bias and its alignment with real-world fairness notions.