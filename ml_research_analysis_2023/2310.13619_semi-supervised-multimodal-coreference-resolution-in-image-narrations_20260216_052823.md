---
ver: rpa2
title: Semi-supervised multimodal coreference resolution in image narrations
arxiv_id: '2310.13619'
source_url: https://arxiv.org/abs/2310.13619
tags:
- loss
- grounding
- coreference
- image
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of multimodal coreference resolution
  (MCR), which involves resolving references to entities across both image and text
  descriptions, particularly in long narratives. Existing methods struggle with fine-grained
  alignment and ambiguity in narratives, and lack large annotated training sets.
---

# Semi-supervised multimodal coreference resolution in image narrations

## Quick Facts
- arXiv ID: 2310.13619
- Source URL: https://arxiv.org/abs/2310.13619
- Authors:
- Reference count: 30
- Primary result: Semi-supervised approach improves multimodal coreference resolution by leveraging pseudo-labels from unlabeled image-narration pairs

## Executive Summary
This work addresses the challenge of multimodal coreference resolution (MCR) in image narrations, where entities must be resolved across both visual and textual descriptions. The authors propose a semi-supervised approach that combines labeled and unlabeled data through pseudo-labeling, using a cross-modal framework with task-specific losses. Their method demonstrates consistent performance improvements over strong baselines on coreference resolution and narrative grounding tasks, showcasing the effectiveness of leveraging pseudo-labels for both coreference resolution and grounding.

## Method Summary
The method employs a cross-modal framework that fuses visual and textual features through cross-attention. It uses a pre-trained Faster-RCNN for object detection and BERT for text encoding, with a multi-modal encoder that combines these features. The approach incorporates supervised losses for labeled data and self-supervised objectives for pseudo-labeled data, including contrastive loss and masked language modeling. Training involves generating pseudo-labels for coreference chains and grounding on unlabeled data, which are then used alongside the small labeled set to train the model.

## Key Results
- Consistent performance improvements over strong baselines on coreference resolution tasks
- Improved narrative grounding accuracy through semi-supervised learning
- Enhanced performance with increased unlabeled samples, demonstrating scalability of the approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semi-supervised learning via pseudo-labels improves multimodal coreference resolution by leveraging unlabeled image-narration pairs.
- Mechanism: The model generates pseudo-labels for coreference chains and grounding on unlabeled data using similarity thresholds and confidence thresholds. These pseudo-labels are then used to train the model alongside the small labeled set, effectively expanding the training data without manual annotation.
- Core assumption: The pseudo-labels generated from the model's own predictions are sufficiently accurate to provide meaningful training signals without introducing too much noise.
- Evidence anchors:
  - [abstract]: "Our approach incorporates losses for both labeled and unlabeled data within a cross-modal framework."
  - [section]: "Our overall training objective is the joint loss function as follows: X (x,y)∈Ds 1 |Ds| Ls(x, y) + X x∈Du 1 |Du| Lu(x)"
  - [corpus]: Weak corpus support; no direct evidence of semi-supervised learning for MCR in neighbors.
- Break condition: If the pseudo-labels are too noisy (low confidence threshold), the model may overfit to incorrect patterns, leading to degraded performance.

### Mechanism 2
- Claim: Task-specific losses for coreference resolution and grounding improve multimodal representation learning.
- Mechanism: The model uses a supervised contrastive loss (CR) to cluster embeddings of coreferring mentions and push apart non-referents. A grounding loss (GD) aligns mentions with image regions using cross-attention scores. These task-specific losses are combined with pre-training objectives to learn better joint representations.
- Core assumption: The cross-modal architecture can effectively fuse visual and textual features, and the task-specific losses can guide the model to learn meaningful multimodal embeddings.
- Evidence anchors:
  - [abstract]: "Our approach incorporates losses for both labeled and unlabeled data within a cross-modal framework."
  - [section]: "Next, the multi-modal encoder f fuses the visual features from the visual encoder fv(vr) with the mention features from the text encoder ft(m)."
  - [corpus]: Weak corpus support; no direct evidence of task-specific losses for MCR in neighbors.
- Break condition: If the cross-modal fusion fails to capture the relevant information from both modalities, the task-specific losses may not be effective.

### Mechanism 3
- Claim: Robust loss functions and thresholding mitigate overfitting to noisy pseudo-labels.
- Mechanism: The model uses a triplet loss formulation with mean embeddings for pseudo-positive and pseudo-negative labels to reduce sensitivity to noise. A high confidence threshold (0.9) is applied to the grounding pseudo-labels to ensure only reliable samples are used. This prevents the model from overfitting to incorrect pseudo-labels.
- Core assumption: The triplet loss with mean embeddings is more robust to noise than other loss formulations, and the high confidence threshold effectively filters out unreliable pseudo-labels.
- Evidence anchors:
  - [abstract]: "Hence, we propose a robust loss function and thresholding-based training scheme to effectively learn from the unlabeled set."
  - [section]: "We only consider samples whose grounding score is greater than a confidence threshold t, which is set to 0.9 in our experiments."
  - [corpus]: Weak corpus support; no direct evidence of robust loss functions for MCR in neighbors.
- Break condition: If the confidence threshold is too high, the model may not benefit enough from the unlabeled data. If it's too low, noisy pseudo-labels may still lead to overfitting.

## Foundational Learning

- Concept: Coreference resolution
  - Why needed here: The paper addresses multimodal coreference resolution, which requires understanding and resolving references to entities across text and images.
  - Quick check question: What is the difference between anaphoric and cataphoric reference in coreference resolution?
- Concept: Cross-modal learning
  - Why needed here: The model needs to fuse visual and textual features to resolve coreferences that involve both modalities.
  - Quick check question: What are the main challenges in cross-modal representation learning compared to single-modal learning?
- Concept: Semi-supervised learning
  - Why needed here: The paper leverages unlabeled data through pseudo-labeling to improve performance when labeled data is scarce.
  - Quick check question: What are the main differences between self-training and co-training in semi-supervised learning?

## Architecture Onboarding

- Component map: Visual encoder -> Text encoder -> Multi-modal encoder -> Task-specific heads -> Loss computation -> Parameter updates
- Critical path: Visual/text encoders → Multi-modal encoder → Task-specific heads → Loss computation → Parameter updates
- Design tradeoffs:
  - Using pre-trained object detector vs. end-to-end learning: Pre-trained detector provides good visual features but limits the model to predefined object categories.
  - High confidence threshold vs. low threshold for pseudo-labels: High threshold reduces noise but may limit the use of unlabeled data.
  - Contrastive loss vs. triplet loss for pseudo-labels: Contrastive loss may overfit to noise, while triplet loss with mean embeddings is more robust.
- Failure signatures:
  - Poor coreference resolution: Model fails to cluster mentions correctly, often due to noisy pseudo-labels or ineffective cross-modal fusion.
  - Inaccurate grounding: Model fails to align mentions with correct image regions, often due to incorrect region proposals or weak cross-attention.
  - Overfitting: Model performs well on training data but poorly on test data, often due to noisy pseudo-labels or excessive training on small labeled set.
- First 3 experiments:
  1. Train the model with only labeled data and no pseudo-labels to establish a baseline.
  2. Train the model with pseudo-labels but no confidence threshold to assess the impact of noisy pseudo-labels.
  3. Train the model with pseudo-labels and a low confidence threshold (e.g., 0.5) to find the optimal threshold for filtering noisy pseudo-labels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the semi-supervised approach scale with the size of the unlabeled dataset?
- Basis in paper: [inferred] The paper discusses the use of unlabeled data and pseudo-labeling, and mentions that the model exhibits enhanced performance with increased unlabeled samples.
- Why unresolved: The paper does not provide a detailed analysis of how performance changes with different amounts of unlabeled data.
- What evidence would resolve it: Conducting experiments with varying sizes of unlabeled datasets and measuring the performance of the model on coreference resolution and narrative grounding tasks.

### Open Question 2
- Question: What are the limitations of the pre-trained object detector in the context of multimodal coreference resolution?
- Basis in paper: [explicit] The paper mentions that the pre-trained object detector has a limited object category vocabulary and lacks fine-grained properties, which forces the model to rely on a predetermined set of regions and object classes.
- Why unresolved: The paper does not provide a detailed analysis of how these limitations affect the overall performance of the model.
- What evidence would resolve it: Conducting experiments to compare the performance of the model with and without the pre-trained object detector, and analyzing the impact of its limitations on coreference resolution and grounding tasks.

### Open Question 3
- Question: How does the model handle mentions that are not detected by the mention detector or are not present in the ground-truth annotations?
- Basis in paper: [explicit] The paper mentions that the model currently depends on ground-truth mentions to resolve coreferences and ground them, and suggests that detecting mentions simultaneously with coreference resolution and grounding would improve the applicability of the method.
- Why unresolved: The paper does not provide a detailed analysis of how the model handles undetected or missing mentions.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the model when mentions are not detected or are missing, and analyzing the impact on coreference resolution and grounding tasks.

### Open Question 4
- Question: How does the proposed method compare to other semi-supervised learning techniques in the context of multimodal coreference resolution?
- Basis in paper: [inferred] The paper discusses the use of pseudo-labeling and thresholding-based training strategies, which are common in semi-supervised learning.
- Why unresolved: The paper does not provide a detailed comparison with other semi-supervised learning techniques.
- What evidence would resolve it: Conducting experiments to compare the performance of the proposed method with other semi-supervised learning techniques, such as consistency regularization or self-training, on the multimodal coreference resolution task.

### Open Question 5
- Question: How does the proposed method handle cases where the same entity is referred to by different mentions with varying levels of specificity?
- Basis in paper: [explicit] The paper mentions that the model can effectively learn to disambiguate instances based on visual details, but also notes that it still fails to resolve cases like "some other people" by clustering them into the same chain.
- Why unresolved: The paper does not provide a detailed analysis of how the model handles varying levels of specificity in mentions.
- What evidence would resolve it: Conducting experiments to evaluate the performance of the model on cases where the same entity is referred to by different mentions with varying levels of specificity, and analyzing the impact on coreference resolution and grounding tasks.

## Limitations

- **Pseudo-label quality threshold sensitivity**: The model relies on confidence thresholds (0.9 for grounding) to filter pseudo-labels, but the optimal threshold may vary significantly with dataset characteristics.
- **Cross-modal alignment generalization**: While the cross-attention mechanism is designed to fuse visual and textual features, the model's ability to generalize to new object categories or novel visual contexts not seen during pre-training remains unclear.
- **Evaluation dataset representativeness**: The performance gains are demonstrated on specific benchmarks, but the method's effectiveness across diverse narrative domains (e.g., different languages, cultural contexts, or image styles) is not established.

## Confidence

- **High confidence**: The semi-supervised framework's ability to leverage unlabeled data through pseudo-labels for improved performance. This is supported by the mathematical formulation and experimental results showing consistent improvements over baselines.
- **Medium confidence**: The effectiveness of task-specific losses (coreference and grounding) in guiding multimodal representation learning. While the design is theoretically sound, the paper lacks ablation studies isolating the impact of each loss component.
- **Low confidence**: The robustness of the model to noisy pseudo-labels. Although a robust loss function and high confidence threshold are proposed, the paper does not provide extensive analysis of failure cases or sensitivity to threshold variations.

## Next Checks

1. **Ablation study on confidence thresholds**: Systematically vary the confidence threshold for pseudo-label filtering (e.g., 0.7, 0.8, 0.9, 0.95) and evaluate the impact on coreference resolution and grounding accuracy.
2. **Cross-domain generalization test**: Evaluate the model on a distinct narrative dataset (e.g., a different cultural context or image style) to assess whether the learned representations generalize beyond the training domain.
3. **Noise injection analysis**: Intentionally introduce varying levels of noise into the pseudo-labels and measure the model's robustness to validate the effectiveness of the proposed robust loss function.