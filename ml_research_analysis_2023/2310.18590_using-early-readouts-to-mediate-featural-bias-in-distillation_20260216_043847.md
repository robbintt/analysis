---
ver: rpa2
title: Using Early Readouts to Mediate Featural Bias in Distillation
arxiv_id: '2310.18590'
source_url: https://arxiv.org/abs/2310.18590
tags:
- group
- distillation
- teacher
- groups
- early
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DEDIER, a method to reduce spurious feature
  learning in knowledge distillation. The key insight is that early readouts from
  network layers disproportionately misclassify instances where spurious features
  conflict with labels.
---

# Using Early Readouts to Mediate Featural Bias in Distillation

## Quick Facts
- **arXiv ID**: 2310.18590
- **Source URL**: https://arxiv.org/abs/2310.18590
- **Reference count**: 36
- **Primary result**: DEDIER improves worst-group accuracy and overall accuracy in knowledge distillation by using early network readouts to identify and reweight instances with spurious feature correlations.

## Executive Summary
This paper introduces DEDIER, a method to reduce spurious feature learning in knowledge distillation. The key insight is that early readouts from network layers disproportionately misclassify instances where spurious features conflict with labels. By weighting the distillation loss based on confidence margins from these early readouts, DEDIER dynamically adjusts learning to mitigate bias. Experiments on four datasets show DEDIER improves both worst-group accuracy and overall accuracy compared to baselines, while adapting throughout training without needing group annotations.

## Method Summary
DEDIER modifies knowledge distillation by incorporating early network readouts to identify and reweight instances with spurious feature correlations. The method trains an auxiliary classifier on early network representations, uses confidence margins from these readouts to compute per-instance weights, and applies these weights to the distillation loss. The auxiliary classifier is updated every epoch to adapt to changing model performance. This dynamic reweighting approach improves worst-group accuracy while maintaining overall accuracy across vision and text datasets.

## Key Results
- DEDIER achieves higher worst-group accuracy than baselines on Waterbirds, CelebA, MultiNLI, and CivilComments-WILDS datasets
- The method maintains or improves overall accuracy while improving group fairness measures
- Early readouts identify worst-group instances with ~100% recall despite being a small fraction of total errors
- DEDIER adapts throughout training by retraining the auxiliary network every epoch

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early readouts disproportionately misclassify minority group instances.
- Mechanism: Representations learned at early network layers contain spurious features that conflict with true labels, leading to confident incorrect predictions on minority instances.
- Core assumption: Spurious features are learned earlier in the network stack and persist through later layers.
- Evidence anchors:
  - [abstract] "We show that these early readouts automatically identify problem instances or groups in the form of confident, incorrect predictions."
  - [section 4.1] "Early readouts signal spurious correlations... nearly all worst group instances are misclassified (~100% recall), although they are a small fraction of the overall errors."
  - [corpus] Weak - corpus neighbors discuss spurious correlations but don't directly address early readouts.

### Mechanism 2
- Claim: Confidence margin from early readouts discriminates between minority and non-minority error instances.
- Mechanism: Minority group errors have higher confidence margins than other error types, allowing for weighted loss adjustment.
- Core assumption: The network's confidence in spurious feature predictions differs systematically between minority and non-minority groups.
- Evidence anchors:
  - [abstract] "Leveraging these signals to modulate the distillation loss on an instance level allows us to substantially improve... group fairness measures"
  - [section 4.1] "the confidence margin further helps us discriminate between the two sets of erroneous instances, with minority group errors associated with significantly higher margin"
  - [corpus] Weak - corpus doesn't specifically address confidence margin discrimination.

### Mechanism 3
- Claim: Dynamic reweighting based on early readouts adapts to changing model performance throughout training.
- Mechanism: Retraining the auxiliary network every epoch captures evolving group performance differences, allowing real-time loss adjustment.
- Core assumption: Group-wise performance shifts during training and benefits from continuous monitoring.
- Evidence anchors:
  - [abstract] "Our approach outperforms SOTA... while simultaneously beating them on overall accuracy"
  - [section 4.3] "Since the readouts are a cheap way of monitoring network performance at an instance level, we can design dynamic adjustment techniques that use readouts from the classifier being learned, throughout the course of its training"
  - [section 6.3] "we do not simply reweight a predetermined set of misclassified points... instead, DEDIER dynamically adapts the loss function through the mechanism of refreshing the early readout model every epoch"
  - [corpus] Weak - corpus doesn't directly address dynamic reweighting during training.

## Foundational Learning

- **Concept**: Spurious correlations
  - Why needed here: Understanding how features unrelated to labels can still influence model predictions is central to why DEDIER is needed
  - Quick check question: Can you give an example where a background feature might be spuriously correlated with a foreground object label?

- **Concept**: Knowledge distillation
  - Why needed here: DEDIER builds on standard distillation but modifies the loss function, so understanding baseline distillation is essential
  - Quick check question: What's the difference between supervised learning loss and teacher-matching loss in distillation?

- **Concept**: Group fairness metrics
  - Why needed here: DEDIER specifically targets worst-group accuracy, so understanding group-based evaluation is crucial
  - Quick check question: How does worst-group accuracy differ from average accuracy, and why might optimizing one harm the other?

## Architecture Onboarding

- **Component map**: Student model (ResNet-18/DistilBERT) → Early layer representation → Auxiliary classifier → Confidence margin → Weighting function → Distillation loss
- **Critical path**: Data → Early readout training → Confidence calculation → Weight computation → Modified distillation loss → Student model update
- **Design tradeoffs**: Static vs dynamic weighting (dynamic better but more complex), depth of early readout (earlier captures spurious features better but may be noisier)
- **Failure signatures**: No improvement in worst-group accuracy despite training, training instability due to extreme weights, poor performance on majority groups
- **First 3 experiments**:
  1. Implement basic early readout with fixed weights to verify minority group identification
  2. Add confidence-based weighting with fixed hyperparameters to test margin discrimination
  3. Implement dynamic reweighting with periodic auxiliary training to validate adaptation claims

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- The method relies on having group annotations for evaluation, though not for training
- Performance depends on hyperparameter choices (auxiliary depth, reweighting frequency, margin thresholds)
- Limited testing on larger architectures beyond ResNet-18 and DistilBERT
- Assumes spurious features are learned early in the network, which may not hold for all architectures

## Confidence
**High confidence**: The core observation that early network representations can identify problem instances through confident incorrect predictions is well-supported by experimental evidence showing ~100% recall for worst-group instances in early readouts.

**Medium confidence**: The effectiveness of confidence margin-based weighting in discriminating between minority and non-minority errors is supported but could benefit from additional ablation studies on margin threshold selection and alternative weighting schemes.

**Medium confidence**: The claim of improved worst-group accuracy while maintaining overall accuracy is demonstrated across four datasets, though the magnitude of improvement varies and the method's behavior on larger, more complex datasets remains untested.

## Next Checks
1. **Scaling validation**: Test DEDIER on larger architectures (ResNet-50/101, BERT-base) and datasets with more subtle spurious correlations to assess scalability and robustness.

2. **Ablation study**: Systematically evaluate the impact of auxiliary classifier depth d, reweighting frequency R, and margin threshold parameters through controlled ablation experiments.

3. **Cross-dataset generalization**: Train DEDIER on one dataset with specific spurious features, then evaluate on a different dataset to test whether early readouts can identify previously unseen spurious correlations.