---
ver: rpa2
title: Dynamic Heterogeneous Federated Learning with Multi-Level Prototypes
arxiv_id: '2312.09881'
source_url: https://arxiv.org/abs/2312.09881
tags:
- federated
- learning
- global
- data
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses Dynamic Heterogeneous Federated Learning (DHFL),
  where clients face both heterogeneous data distributions and dynamic tasks with
  new classes arriving over time. Existing methods suffer from local catastrophic
  forgetting (forgetting previous classes) and global concept drift (bias toward majority
  classes).
---

# Dynamic Heterogeneous Federated Learning with Multi-Level Prototypes

## Quick Facts
- arXiv ID: 2312.09881
- Source URL: https://arxiv.org/abs/2312.09881
- Reference count: 10
- Key outcome: FedMLP achieves state-of-the-art performance in dynamic heterogeneous federated learning, outperforming recent methods by significant margins (e.g., 23.99% higher on CIFAR-100 Aglo accuracy)

## Executive Summary
This paper addresses Dynamic Heterogeneous Federated Learning (DHFL) where clients face both heterogeneous data distributions and dynamic tasks with new classes arriving over time. Existing methods suffer from local catastrophic forgetting and global concept drift. The authors propose Federated Multi-Level Prototypes (FedMLP) that constructs prototypes and semantic prototypes to provide generalization knowledge and ensure prototype space continuity. Through three federated regularizations—prototype-based, semantic prototype-based, and federated inter-task—FedMLP effectively mitigates both local forgetting and global concept drift while maintaining communication efficiency.

## Method Summary
FedMLP constructs prototypes representing class-specific feature averages and semantic prototypes via clustering to serve as reference points for regularization. The method introduces three federated regularizations: prototype-based regularization to maintain global prototype consistency, semantic prototype-based regularization to provide supplementary knowledge for minority classes, and federated inter-task regularization to reduce forgetting across tasks. During training, local models learn from these prototypes while preserving knowledge of previous classes. The framework is evaluated on four benchmark datasets (CIFAR-10, CIFAR-100, MNIST, FEMNIST) with 100 tasks and 20 clients each, using SGD optimizer with learning rate 0.01 and weight decay 1e-5. The method demonstrates significant improvements over baselines while maintaining communication efficiency by only transmitting prototypes rather than full model parameters.

## Key Results
- FedMLP achieves 23.99% higher Aglo accuracy on CIFAR-100 compared to FedNTD and CreFF
- Shows 7.01% higher accuracy on CIFAR-10 under extreme heterogeneity (γ=0.05)
- Demonstrates faster convergence and better handling of both local forgetting and global concept drift
- Maintains communication efficiency through prototype-based aggregation instead of full parameter synchronization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-level prototypes (global and semantic) provide generalization knowledge that reduces global concept drift
- Mechanism: The framework constructs prototypes representing class-specific feature averages and semantic prototypes via clustering. These serve as reference points that help align local models with global patterns, preventing drift toward majority classes
- Core assumption: Prototype representations capture meaningful semantic information that generalizes across heterogeneous clients
- Evidence anchors:
  - [abstract] "To mitigate concept drift, we construct prototypes and semantic prototypes to provide fruitful generalization knowledge and ensure the continuity of prototype spaces"
  - [section] "The federated semantic prototype effectively initializes new class prototypes of new tasks according to the similar continuity of semantic space. It reduces the difference in class distribution between tasks to solve the problem of local model forgetting"
- Break condition: If prototype clustering fails to capture meaningful semantic relationships, the regularization terms lose effectiveness

### Mechanism 2
- Claim: Federated inter-task regularization prevents local catastrophic forgetting through KL divergence
- Mechanism: The method uses KL divergence between current task features and old task prototypes, forcing the model to maintain feature similarity for previously learned classes while adapting to new ones
- Core assumption: The KL divergence regularization preserves sufficient information about old classes while allowing adaptation
- Evidence anchors:
  - [abstract] "To address the local forgetting caused by dynamic incremental tasks within the client, we design the federated inter-task regularization, whose goal is to synchronize the recognized class prototypes of the current task with the class models of previous tasks"
  - [section] "The federated inter-task regularization LI is devised to promote consistency and continuity of the feature space in the current and previous tasks to alleviate local forgetting"
- Break condition: If the initialization representation for new classes is poorly chosen, the regularization may push the model away from optimal solutions

### Mechanism 3
- Claim: Semantic prototype-based regularization provides supplementary knowledge for minority classes
- Mechanism: Minority classes learn feature representations from semantically similar prototype clusters, reducing bias toward majority classes and improving generalization
- Core assumption: Minority classes share semantic similarity with other classes that can be captured through clustering
- Evidence anchors:
  - [abstract] "These federated regularizations ensure that minority classes can leverage features from balanced semantic prototypes, thereby rectifying their own prototype representations"
  - [section] "For the minority classes, we further optimize using semantic prototypes of the corresponding clusters... This loss enables the minority classes with limited sample sizes to learn similar feature representations alleviating the global classifier shift"
- Break condition: If semantic clustering incorrectly groups dissimilar classes, minority classes may learn inappropriate representations

## Foundational Learning

- Concept: Prototype learning and centroid-based representation
  - Why needed here: The entire method relies on using class prototypes as reference points for regularization, so understanding how prototypes capture class semantics is fundamental
  - Quick check question: How does the prototype representation change when new classes are introduced in incremental learning?

- Concept: Knowledge distillation and regularization through feature alignment
  - Why needed here: The method uses regularization losses to align local features with global prototypes, similar to distillation principles
  - Quick check question: What's the difference between using Smooth L1 loss versus MSE loss for prototype regularization?

- Concept: Federated learning communication patterns and privacy constraints
  - Why needed here: The method must work within federated constraints (no raw data sharing, limited communication) while still achieving global model improvement
  - Quick check question: Why is communicating prototypes more efficient than communicating full model parameters?

## Architecture Onboarding

- Component map:
  - Feature extractor (local to each client)
  - Unified classifier (shared architecture)
  - Prototype storage (local and global)
  - Semantic prototype clustering module
  - Three regularization components (prototype-based, semantic prototype-based, inter-task)
  - Communication layer (prototype aggregation)

- Critical path:
  1. Local training with multi-level regularization
  2. Prototype upload to server
  3. Global prototype and semantic prototype computation
  4. Prototype distribution back to clients
  5. Next round of local training

- Design tradeoffs:
  - Prototype size vs. model parameter size (prototypes are much smaller, reducing communication)
  - Semantic clustering quality vs. computational overhead
  - Regularization strength vs. model flexibility

- Failure signatures:
  - Poor semantic clustering → minority classes receive incorrect supplementary knowledge
  - KL divergence divergence → local forgetting not prevented
  - Prototype averaging issues → global prototypes don't represent true class centers

- First 3 experiments:
  1. Test prototype construction: Verify that prototypes accurately represent class centers on balanced data
  2. Test semantic clustering: Verify that semantic prototypes correctly group semantically similar classes
  3. Test regularization effectiveness: Compare model performance with and without each regularization term on a simple federated learning task

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Performance scaling with thousands of clients is not explored, though 100 clients are used in experiments
- The semantic clustering algorithm's sensitivity to parameter choices and clustering quality is not thoroughly investigated
- Security/robustness against malicious clients or Byzantine attacks is not addressed

## Confidence
- High confidence: Prototype-based regularization effectiveness and communication efficiency gains
- Medium confidence: Semantic prototype clustering quality and minority class benefit claims
- Low confidence: Generalization to real-world federated scenarios with extreme heterogeneity and dynamic task patterns

## Next Checks
1. **Robustness testing**: Evaluate FedMLP on more extreme heterogeneity settings (β < 0.1) and with varying client participation rates to test scalability
2. **Semantic clustering ablation**: Conduct controlled experiments removing semantic clustering to quantify its exact contribution to minority class performance
3. **Communication efficiency measurement**: Precisely measure the reduction in communication overhead compared to baseline methods while maintaining accuracy gains