---
ver: rpa2
title: KGConv, a Conversational Corpus grounded in Wikidata
arxiv_id: '2308.15298'
source_url: https://arxiv.org/abs/2308.15298
tags:
- question
- questions
- wikidata
- templates
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KGConv, a large conversational dataset with
  71k conversations grounded in Wikidata facts. Each conversation consists of question-answer
  pairs where each question is generated from a Wikidata triple using templates, human
  annotations, and rule-based approaches.
---

# KGConv, a Conversational Corpus grounded in Wikidata

## Quick Facts
- arXiv ID: 2308.15298
- Source URL: https://arxiv.org/abs/2308.15298
- Reference count: 6
- Key outcome: KGConv is a large conversational dataset with 71k conversations grounded in Wikidata facts, providing three versions of questions (OOC, IC, SIC) for each fact to improve model generalization across different conversational contexts.

## Executive Summary
KGConv is a conversational corpus of 71k conversations with 603k question-answer pairs grounded in Wikidata facts. Each conversation is generated from Wikidata triples using templates, human annotations, and rule-based approaches, producing three versions of questions: Out-of-Context (OOC), In-Context (IC), and Synthetic-In-Context (SIC). The dataset covers eight themes and establishes baselines for Knowledge-Based Conversational Question Generation using T5 models trained on different question versions. Results show that models trained on IC and SIC data perform better when conversational context is provided, with human evaluation revealing that IC questions are more reliable and clearer than SIC questions.

## Method Summary
KGConv is generated from Wikidata triples using a pipeline that includes triple extraction, template application to create OOC questions, contextualization rules for IC questions, and T5-based rewriting for SIC questions. Baseline models use T5-small fine-tuned on different context representations (Empty, NL, KG, NL+KG) and trained on OOC, IC, and SIC variants. Evaluation combines automatic metrics (BLEU, BERT-score) against all three question variants with human evaluation on linguistic correctness, faithfulness, clearness, and naturalness.

## Key Results
- Models trained on IC and SIC data perform better when conversational context is provided
- Human evaluation shows IC questions are more reliable and clearer than SIC questions
- IC and SIC questions have similar linguistic correctness despite quality differences
- Average conversations contain 8.6 questions across eight knowledge domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset's multi-variate question generation (OOC, IC, SIC) improves model generalization across different conversational contexts.
- Mechanism: By providing three distinct question forms per fact, models can learn to adapt question phrasing based on context availability. The OOC version trains models to generate context-independent questions, while IC and SIC versions teach context-aware generation with varying degrees of naturalness.
- Core assumption: Models can effectively learn from the three different question forms and apply this knowledge to generate appropriate questions in new conversational contexts.
- Evidence anchors:
  - [abstract] "Each conversation consists of question-answer pairs where each question is generated from a Wikidata triple using templates, human annotations, and rule-based approaches. Three versions of questions are provided: Out-of-Context (OOC), In-Context (IC), and Synthetic-In-Context (SIC)."
  - [section 4.3] Describes the three question variants and their generation methods.
- Break condition: If models fail to distinguish between context requirements or cannot effectively learn from the multiple question forms, generalization across contexts may suffer.

### Mechanism 2
- Claim: The dataset's coverage of 8 themes with varying complexity allows for robust evaluation of model performance across different knowledge domains.
- Mechanism: By including diverse themes like Country, Food, Person, Religion/Ideology, Space Object, Taxon, Molecular Entity, and Historical Event, the dataset enables testing of model adaptability to different types of knowledge and question structures.
- Core assumption: Models trained on this diverse dataset will generalize well to unseen themes and properties.
- Evidence anchors:
  - [abstract] "The dataset covers eight themes, with conversations averaging 8.6 questions."
  - [section 3] Details the eight themes and their characteristics.
- Break condition: If models overfit to specific themes or struggle with the unique characteristics of certain knowledge domains, performance on unseen themes may degrade significantly.

### Mechanism 3
- Claim: The use of Wikidata as a grounding source provides a large, structured knowledge base for generating diverse and accurate questions.
- Mechanism: Wikidata's extensive coverage of facts and relationships allows for the creation of a large dataset with varied question types. The structured nature of Wikidata enables systematic generation of questions from triples.
- Core assumption: The quality and coverage of Wikidata triples are sufficient to generate a large, diverse, and accurate conversational dataset.
- Evidence anchors:
  - [abstract] "Each conversation consists of question-answer pairs where each question is generated from a Wikidata triple..."
  - [section 2] Compares KGConv to other knowledge-graph based datasets.
- Break condition: If Wikidata contains insufficient facts or relationships for certain domains, or if the triple extraction process introduces noise, the quality and diversity of generated questions may suffer.

## Foundational Learning

- Concept: Question generation from knowledge graphs
  - Why needed here: The core task of KGConv is to generate questions from Wikidata triples, requiring understanding of how to convert structured knowledge into natural language questions.
  - Quick check question: Can you explain the process of generating a question from a given knowledge graph triple?

- Concept: Conversational context modeling
  - Why needed here: KGConv introduces the challenge of generating questions that are appropriate within a conversational context, requiring models to understand and utilize context information.
  - Quick check question: How would you modify a question generation model to incorporate conversational context?

- Concept: Multi-task learning
  - Why needed here: The dataset provides multiple versions of questions (OOC, IC, SIC) for each fact, allowing models to learn different aspects of question generation simultaneously.
  - Quick check question: What are the benefits and challenges of training a model on multiple related tasks simultaneously?

## Architecture Onboarding

- Component map: Triple extraction -> Template application -> Contextualization -> Model-based rewriting
- Critical path: 1. Extract triples from Wikidata 2. Apply templates to generate OOC questions 3. Apply contextualization rules to generate IC questions 4. Use T5 model to generate SIC questions 5. Train baseline models on different question versions and context representations 6. Evaluate models using automatic and human evaluation
- Design tradeoffs: Template-based vs. end-to-end generation (templates ensure coverage but may limit naturalness), Rule-based vs. model-based contextualization (rules are interpretable but may not capture all nuances), Multiple question versions (increases dataset size and complexity but provides richer training signals)
- Failure signatures: Poor performance on unseen themes or properties, Inability to generate contextually appropriate questions, Low agreement in human evaluation across different question versions
- First 3 experiments: 1. Train a baseline model on OOC questions only and evaluate on all question versions to establish a lower bound 2. Train a model on IC questions with different context representations (NL, KG, NL+KG) and compare performance 3. Fine-tune a pre-trained language model on the KGConv dataset and evaluate its ability to generate all three question versions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the T5-based question rewriting model at generating natural and contextually appropriate questions in KGConv?
- Basis in paper: [explicit] The paper states that the T5 model was fine-tuned on a training set derived from conversational QA datasets and used to generate the SIC (Synthetic-In-Context) version of questions in KGConv. Human evaluation results show that SIC questions are less clear and less faithful to the triples compared to IC (In-Context) questions, but more linguistically correct.
- Why unresolved: The paper does not provide a detailed analysis of the quality of the T5 model's outputs or compare them to other approaches for generating contextual questions. It also does not explore ways to improve the model's performance.
- What evidence would resolve it: A more comprehensive evaluation of the T5 model's outputs, including automatic metrics and human judgments on multiple aspects of question quality (e.g., fluency, coherence, appropriateness), as well as comparisons to other methods for generating contextual questions.

### Open Question 2
- Question: How well do the baseline models for conversational question generation generalize to unseen properties in KGConv?
- Basis in paper: [explicit] The paper reports that models trained on IC and SIC data perform better when conversational context is provided, but average scores are lower on unseen properties because the verbalization highly depends on the property of the triple. Human evaluation also shows that scores obtained on unseen themes tend to be lower than those obtained on seen themes.
- Why unresolved: The paper does not investigate the specific challenges of generating questions for unseen properties or propose methods to address this zero-shot task. It also does not explore the impact of the number of training examples for each property on the model's performance.
- What evidence would resolve it: An analysis of the baseline models' performance on different subsets of unseen properties, along with experiments on techniques for few-shot or zero-shot learning of question generation for new properties, such as meta-learning, prompt engineering, or knowledge transfer.

### Open Question 3
- Question: How can the naturalness and conversationality of KGConv be further improved?
- Basis in paper: [inferred] The paper acknowledges that some conversations in KGConv may be unnatural because they are automatically generated from a knowledge graph. It also mentions that SIC questions have more diverse forms than IC questions, but are less clear and less faithful to the triples. The paper suggests that an interesting perspective would be to investigate methods for tackling the zero-shot task of generating questions for unseen properties.
- Why unresolved: The paper does not propose specific strategies for enhancing the naturalness and conversationality of KGConv beyond the current IC and SIC variants. It also does not explore the potential of incorporating more advanced language understanding and generation techniques, such as dialogue state tracking, response selection, or response generation.
- What evidence would resolve it: Experiments on methods for improving the naturalness and conversationality of KGConv, such as fine-tuning the models on additional conversational data, using more sophisticated templates or rules for generating IC questions, or employing more powerful language models for generating SIC questions.

## Limitations
- Dataset construction relies heavily on template-based generation which may not capture full diversity of natural conversational question patterns
- Synthetic SIC questions generated by T5 model trained on limited rewriting datasets may introduce generation artifacts
- Human evaluation sample size (40 conversations) is relatively small for drawing definitive conclusions about quality differences

## Confidence
- High confidence: The dataset size (71k conversations, 603k QAs) and thematic coverage (8 domains) are accurately reported and reproducible
- Medium confidence: The claim that IC questions are more reliable and clearer than SIC questions is supported by human evaluation but based on a limited sample size
- Medium confidence: The effectiveness of training models on IC/SIC data for conversational context generation is demonstrated but could benefit from more extensive ablation studies

## Next Checks
1. Conduct a larger-scale human evaluation (minimum 200 conversations) to verify the reliability differences between IC and SIC question variants across all eight themes
2. Perform cross-theme generalization experiments by training models on 7 themes and testing on the held-out theme to assess zero-shot performance
3. Analyze the diversity and naturalness of questions generated by fine-tuned models compared to template-based questions through both automatic metrics and human evaluation