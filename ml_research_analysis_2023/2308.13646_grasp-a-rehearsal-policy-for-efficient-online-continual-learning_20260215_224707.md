---
ver: rpa2
title: 'GRASP: A Rehearsal Policy for Efficient Online Continual Learning'
arxiv_id: '2308.13646'
source_url: https://arxiv.org/abs/2308.13646
tags:
- rehearsal
- grasp
- learning
- memory
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRASP is a rehearsal policy for online continual learning that
  gradually selects less prototypical samples over time, achieving the same accuracy
  as uniform sampling with 40% fewer updates. It outperforms 17 other rehearsal policies
  on ImageNet-1K and five text classification datasets, with negligible additional
  compute or memory overhead.
---

# GRASP: A Rehearsal Policy for Efficient Online Continual Learning

## Quick Facts
- arXiv ID: 2308.13646
- Source URL: https://arxiv.org/abs/2308.13646
- Reference count: 28
- Key outcome: GRASP is a rehearsal policy for online continual learning that gradually selects less prototypical samples over time, achieving the same accuracy as uniform sampling with 40% fewer updates. It outperforms 17 other rehearsal policies on ImageNet-1K and five text classification datasets, with negligible additional compute or memory overhead.

## Executive Summary
GRASP is a novel rehearsal policy designed for online continual learning that improves efficiency by selectively sampling training data based on their similarity to class prototypes. The policy operates on the hypothesis that a curriculum combining both easy (prototypical) and hard (non-prototypical) samples is more effective than using only one type. GRASP achieves equivalent accuracy to uniform sampling while requiring 40% fewer updates, making it highly efficient for large-scale continual learning scenarios.

## Method Summary
GRASP is a rehearsal policy that uses class prototypes to guide sample selection during continual learning. For each incoming sample, it computes the cosine distance to the class prototype and caches this distance in the buffer. During rehearsal, samples are selected in a class-balanced manner, with probabilities inversely proportional to their cached distances. This ensures that prototypical samples are chosen first, with gradually harder samples introduced over time. The method supports both veridical (raw image) and latent (compressed embedding) rehearsal modes, with latent rehearsal using Optimized Product Quantization (OPQ) to reduce memory requirements.

## Key Results
- GRASP achieves equivalent accuracy to uniform sampling with 40% fewer updates on ImageNet-1K and five text classification datasets
- Outperforms 17 other rehearsal policies including balanced uniform, class balanced, and nearest neighbor methods
- Maintains class-balanced sampling to prevent catastrophic forgetting in minority classes
- Adds negligible additional compute or memory overhead compared to uniform selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting easy (prototypical) samples first helps stabilize learning early, while gradually including harder samples improves model adaptability over time.
- Mechanism: GRASP assigns selection probability inversely proportional to distance from class prototype, so closest (easiest) samples are chosen first. After each selection, the distance is increased to reduce the chance of reselecting the same sample within the rehearsal cycle.
- Core assumption: Not all training samples contribute equally to learning; a curriculum that starts with easier samples and progresses to harder ones reduces catastrophic forgetting and improves efficiency.
- Evidence anchors:
  - [abstract]: "GRASP selects the most prototypical (class representative) samples first and then gradually selects less prototypical (harder) examples to update the DNN."
  - [section]: "GRASP is based on the hypothesis that choosing only easy or hard samples are both suboptimal, and that the DNN would benefit from a curriculum that combines both."
  - [corpus]: Weak evidence; GRASP is unique in this dynamic curriculum approach, and no similar methods are found in neighbors.
- Break condition: If the class prototype computation is noisy or the distance metric fails to reflect sample difficulty, the policy could select poor samples and degrade performance.

### Mechanism 2
- Claim: Maintaining class-balanced sampling prevents catastrophic forgetting in minority classes during rehearsal.
- Mechanism: For each class k, GRASP samples in proportion to the inverse of cached distances, ensuring that even minority classes get oversampled relative to their size.
- Core assumption: Uniform random sampling across all samples can underrepresent minority classes, leading to forgetting; class-balanced sampling mitigates this.
- Evidence anchors:
  - [section]: "GRASP overcomes [catastrophic forgetting in minority classes] by selecting samples in a class balanced way, where minority classes can be oversampled."
  - [abstract]: "GRASP has little additional compute or memory overhead compared to uniform selection, enabling it to scale to large datasets."
  - [corpus]: No direct match in neighbors; this is a distinguishing feature.
- Break condition: If the buffer is extremely imbalanced and the policy cannot oversample enough, some classes may still be underrepresented.

### Mechanism 3
- Claim: Reusing the same distance metric (cosine distance to prototype) for both buffer maintenance and rehearsal selection simplifies the system and reduces overhead.
- Mechanism: During sample acquisition, GRASP caches the cosine distance to the class prototype. During rehearsal, it uses these cached distances to guide sampling without recomputing embeddings.
- Core assumption: Cached distances remain valid for the duration of a rehearsal cycle and can effectively rank samples for selection.
- Evidence anchors:
  - [section]: "The distance is used during rehearsal to select samples based on how similar they are to the prototype and is stored in the buffer."
  - [abstract]: "GRASP has little additional compute or memory overhead compared to uniform selection, enabling it to scale to large datasets."
  - [corpus]: Weak evidence; no neighbors use this exact approach, though some cache statistics for sampling.
- Break condition: If the model parameters change significantly between acquisition and rehearsal, cached distances may become stale and mislead selection.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: CL methods must prevent the DNN from losing previously learned abilities when trained on new data; rehearsal is a key mitigation strategy.
  - Quick check question: What happens to DNN accuracy on old classes if no rehearsal is used in continual learning?

- Concept: Prototype-based sample selection
  - Why needed here: GRASP uses class prototypes to define "easy" (close to prototype) and "hard" (far from prototype) samples; understanding this metric is essential to implementing GRASP.
  - Quick check question: How is the cosine distance to the class prototype computed, and why is it used instead of Euclidean distance?

- Concept: Class-imbalanced learning
  - Why needed here: Real-world CL data streams are often long-tailed; GRASP explicitly oversamples minority classes to avoid forgetting.
  - Quick check question: What is the effect of not balancing classes during rehearsal in a long-tailed data stream?

## Architecture Onboarding

- Component map:
  Input buffer -> Class prototype tracker -> GRASP selector -> Rehearsal loop -> OPQ (for latent)

- Critical path:
  1. On sample arrival: compute embedding → update prototype → cache distance → insert into buffer (remove if full)
  2. At rehearsal: for each class, compute sampling probabilities from cached distances → sample → update cached distance → form mini-batch → backprop

- Design tradeoffs:
  - Using cached distances vs. recomputing on-the-fly: saves compute but risks staleness
  - Class-balanced oversampling vs. proportional sampling: prevents forgetting but may overfit minority classes
  - Latent vs. veridical rehearsal: latent allows more samples under memory constraints but requires extra compression step

- Failure signatures:
  - Accuracy drops on old classes but improves on new classes: rehearsal policy may be selecting too many new or easy samples
  - High variance in class accuracy: buffer maintenance may be too aggressive in removing minority class samples
  - Training stalls or diverges: cached distances may be stale or the selection probabilities poorly calibrated

- First 3 experiments:
  1. Compare GRASP vs. uniform balanced on a small, balanced ImageNet-300 subset; measure accuracy on old vs. new classes after each rehearsal cycle
  2. Vary the memory budget (e.g., 33%, 66%, 100% of dataset) and confirm GRASP maintains accuracy advantage over uniform
  3. Switch from veridical to latent rehearsal; verify that GRASP still outperforms uniform and that latency/compression overhead is minimal

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The exact mechanisms for prototype computation and the calibration of the selection probabilities are not fully specified
- Reliance on cached distances introduces potential error if model changes significantly between acquisition and rehearsal
- Performance on extremely imbalanced datasets or with very small buffer sizes is not fully explored

## Confidence
- **High Confidence**: GRASP's ability to maintain accuracy with fewer updates compared to uniform sampling, as demonstrated on ImageNet-1K and text classification datasets
- **Medium Confidence**: The mechanism of using class prototypes for gradual curriculum is sound, but the robustness of cached distances over multiple rehearsal cycles is less certain
- **Medium Confidence**: The claim of negligible additional compute or memory overhead is supported by the paper's design, but specific benchmarks for overhead are not provided

## Next Checks
1. **Prototype Distance Robustness**: Conduct experiments to measure how sensitive GRASP's performance is to the staleness of cached distances, by varying the number of rehearsal cycles before recomputing prototypes

2. **Extreme Imbalance Stress Test**: Test GRASP on a highly imbalanced dataset (e.g., with 1% minority class samples) to verify that class-balanced oversampling prevents catastrophic forgetting

3. **Memory Budget Scaling**: Systematically vary the memory budget (e.g., 10%, 33%, 66%, 100% of dataset size) and confirm that GRASP maintains its advantage over uniform sampling at all budget levels