---
ver: rpa2
title: 'CyclicFL: A Cyclic Model Pre-Training Approach to Efficient Federated Learning'
arxiv_id: '2301.12193'
source_url: https://arxiv.org/abs/2301.12193
tags:
- training
- cyclic
- data
- loss
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CyclicFL, a novel cyclic model pre-training
  approach for efficient federated learning (FL). The method addresses the slow convergence
  and poor accuracy issues in FL, particularly in non-IID scenarios, by deriving effective
  initial models to guide the SGD processes.
---

# CyclicFL: A Cyclic Model Pre-Training Approach to Efficient Federated Learning

## Quick Facts
- arXiv ID: 2301.12193
- Source URL: https://arxiv.org/abs/2301.12193
- Authors: 
- Reference count: 40
- This paper proposes CyclicFL, a novel cyclic model pre-training approach for efficient federated learning (FL).

## Executive Summary
CyclicFL addresses the slow convergence and poor accuracy issues in federated learning, particularly in non-IID scenarios, by deriving effective initial models to guide the SGD processes. Unlike traditional centralized pre-training methods that require public proxy data, CyclicFL pre-trains initial models on selected AIoT devices cyclically without exposing their local data, making it easily integrable into any security-critical FL methods. Theoretical analysis shows that CyclicFL approximates existing centralized pre-training methods and achieves faster convergence speed under various convexity assumptions. Comprehensive experimental results demonstrate that CyclicFL can improve the maximum classification accuracy by up to 14.11% and significantly accelerate the overall FL training process.

## Method Summary
CyclicFL consists of two phases: Phase 1 (P1) involves cyclic pre-training where selected clients train the model sequentially for 100 rounds with 25% client participation, passing the model to the next client after local training. Phase 2 (P2) applies standard federated learning algorithms (FedAvg, FedProx, SCAFFOLD, or Moon) for 1000 rounds with 10% client participation. The cyclic training implicitly follows continual learning principles, approximating centralized pre-training when local data distributions overlap. Learning rate scheduling uses 0.01 (or 0.1 for CIFAR-100) with 0.998 decay per round, and batch size is 32.

## Key Results
- CyclicFL improves maximum classification accuracy by up to 14.11% compared to random initialization
- The method significantly accelerates convergence speed in federated learning across multiple datasets
- CyclicFL approximates centralized pre-training methods through continual learning principles when data distributions overlap

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CyclicFL improves federated learning convergence by providing a better initialization that guides SGD on a flatter loss landscape
- Mechanism: The cyclic pre-training process on selected clients creates a global model that has already found a flatter loss basin, reducing the number of training rounds needed for convergence in the federated learning phase
- Core assumption: The local data distributions across clients have sufficient overlap to allow the cyclic pre-training to approximate centralized training
- Evidence anchors:
  - [abstract] "Unlike traditional centralized pre-training methods that require public proxy data, CyclicFL pre-trains initial models on selected AIoT devices cyclically without exposing their local data"
  - [section] "Since pre-training can create a flatter loss landscape to stabilize and accelerate the downstream training [23], we can find that the SGD trajectory in Figure 1(b) is way more stable"
  - [corpus] Weak evidence - related papers focus on energy efficiency and resource constraints rather than initialization strategies
- Break condition: If local data distributions across clients have minimal overlap, the cyclic pre-training cannot approximate centralized training and the initialization benefit is lost

### Mechanism 2
- Claim: CyclicFL approximates centralized pre-training through continual learning principles when local data distributions overlap
- Mechanism: The cyclic training implicitly contains data order, following continual learning principles. When local data distributions overlap across clients, this SGD-based optimization approximates OGD-based optimization, which is equivalent to centralized training
- Core assumption: There exists overlap in data distributions across the clients participating in federated learning
- Evidence anchors:
  - [section] "Based on the concept of Continual Learning (CL), we prove that CyclicFL approximates existing centralized pre-training methods in terms of classification and prediction performance"
  - [section] "The gap between SGD and OGD in FL settings will be decreased accordingly" when data distributions overlap
  - [corpus] Weak evidence - related papers don't discuss continual learning or data distribution overlap
- Break condition: If local data distributions have no overlap across clients, the continual learning approximation fails and the method provides no advantage over random initialization

### Mechanism 3
- Claim: Data consistency between pre-training and training stages is critical for successful model transfer
- Mechanism: The similarity between pre-training data and federated learning data controls the Lipschitzness of the loss function. Higher similarity leads to a flatter loss landscape and better SGD convergence
- Core assumption: The cyclic pre-training data distribution is sufficiently similar to the federated learning data distribution
- Evidence anchors:
  - [section] "We formally analyze the significance of data consistency between the pre-training and training stages of CyclicFL, showing the limited Lipschitzness of loss for the pre-trained models by CyclicFL"
  - [section] "Lemma 2 (The impact of transferred features on the Lipschitzness of the loss)" provides mathematical proof of this relationship
  - [corpus] Weak evidence - related papers don't discuss data consistency requirements for pre-training
- Break condition: If the pre-training data distribution is too different from the federated learning data distribution, the Lipschitzness of the loss function increases, resulting in poor transfer performance

## Foundational Learning

- Concept: Federated Learning
  - Why needed here: CyclicFL is built on top of federated learning as a pre-training method to improve its performance
  - Quick check question: What is the key privacy-preserving feature of federated learning that CyclicFL must respect?

- Concept: Continual Learning
  - Why needed here: CyclicFL's cyclic training implicitly follows continual learning principles, which is the theoretical foundation for why it approximates centralized pre-training
  - Quick check question: How does continual learning help explain why CyclicFL can approximate centralized training without data sharing?

- Concept: Loss Landscape Analysis
  - Why needed here: The effectiveness of CyclicFL is demonstrated through flatter loss landscapes, which directly impacts convergence speed and stability
  - Quick check question: Why does a flatter loss landscape lead to better SGD convergence in federated learning?

## Architecture Onboarding

- Component map: Cloud server -> Random client selection -> Local training (cyclic phase) -> Model aggregation -> Cloud server -> Federated learning phase -> Client training -> Model aggregation
- Critical path: Cloud server coordinates cyclic pre-training phase and federated learning phase; Local clients perform training and model updates; Communication protocol handles model parameter exchange
- Design tradeoffs: More cyclic training rounds improve accuracy but reduce efficiency; More clients in cyclic phase improve data coverage but increase communication overhead; Learning rate scheduling affects convergence speed
- Failure signatures: Poor classification accuracy improvement indicates insufficient data overlap; Slow convergence indicates poor initialization; Communication bottlenecks indicate inefficient client selection
- First 3 experiments:
  1. Compare CyclicFL with random initialization on a simple CNN with Fashion-MNIST dataset under non-IID conditions
  2. Measure convergence speed difference between cyclic pre-training duration of 50 vs 200 rounds on CIFAR-10
  3. Test data consistency requirement by pre-training on completely different data distribution than federated learning data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CyclicFL scale with the number of participating clients and the heterogeneity of their data distributions?
- Basis in paper: [inferred] The paper mentions that CyclicFL uses random sampling of clients and discusses data consistency, but does not provide extensive analysis on how performance scales with different numbers of clients or varying degrees of data heterogeneity.
- Why unresolved: The paper focuses on proving theoretical guarantees and demonstrating effectiveness through experiments, but does not systematically explore the impact of client number and data heterogeneity on performance.
- What evidence would resolve it: Conducting experiments with varying numbers of clients and different levels of data heterogeneity, and analyzing the impact on classification accuracy and convergence speed.

### Open Question 2
- Question: Can CyclicFL be extended to handle scenarios where the data distributions of clients change over time (non-stationary data)?
- Basis in paper: [inferred] The paper does not address the challenge of non-stationary data distributions, which is a common issue in real-world federated learning applications.
- Why unresolved: The theoretical analysis and experiments in the paper assume fixed data distributions for clients, and do not explore the robustness of CyclicFL to changes in data distributions over time.
- What evidence would resolve it: Conducting experiments with non-stationary data distributions and evaluating the performance of CyclicFL under such conditions.

### Open Question 3
- Question: How does the choice of the number of cyclic training rounds (Tcyc) affect the trade-off between accuracy and convergence speed?
- Basis in paper: [explicit] The paper mentions that a trade-off between efficiency and accuracy performance needs to be considered in practical situations, and provides some insights on the impact of Tcyc on performance.
- Why unresolved: The paper does not provide a comprehensive analysis of the optimal choice of Tcyc for different scenarios, and the relationship between Tcyc, accuracy, and convergence speed is not fully explored.
- What evidence would resolve it: Conducting experiments with different values of Tcyc and analyzing the impact on accuracy and convergence speed for various datasets and FL algorithms.

## Limitations
- The cyclic training assumes sufficient overlap in data distributions across clients, which may not hold in all real-world scenarios
- The method requires additional communication rounds for the cyclic pre-training phase, potentially increasing resource consumption
- The paper does not extensively explore the impact of different client sampling strategies on performance

## Confidence
- **High Confidence**: The mechanism of cyclic pre-training providing better initialization for federated learning is well-supported by both theory and experiments
- **Medium Confidence**: The approximation of centralized pre-training through continual learning principles is theoretically sound but relies on assumptions about data distribution overlap
- **Low Confidence**: The specific claims about data consistency requirements and Lipschitzness bounds are mathematically proven but the practical implications for model selection remain unclear

## Next Checks
1. **Data Distribution Sensitivity**: Test CyclicFL performance across a broader range of non-IID data distributions to quantify the minimum overlap required for the continual learning approximation to hold.

2. **Client Sampling Strategy**: Evaluate different client sampling strategies during the cyclic pre-training phase to determine if more intelligent client selection can improve performance without increasing communication overhead.

3. **Transfer Learning Scenarios**: Assess CyclicFL's effectiveness when pre-training data distribution differs significantly from federated learning data distribution, measuring the degradation in performance as a function of distribution mismatch.