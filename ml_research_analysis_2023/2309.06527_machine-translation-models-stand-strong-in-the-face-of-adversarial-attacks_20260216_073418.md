---
ver: rpa2
title: Machine Translation Models Stand Strong in the Face of Adversarial Attacks
arxiv_id: '2309.06527'
source_url: https://arxiv.org/abs/2309.06527
tags:
- attack
- translation
- adversarial
- attacks
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the robustness of machine translation
  models against adversarial attacks, a crucial aspect for real-world applications.
  The authors introduce three main attack strategies: gradient-based, BLEUER, and
  synthetic attacks.'
---

# Machine Translation Models Stand Strong in the Face of Adversarial Attacks

## Quick Facts
- **arXiv ID**: 2309.06527
- **Source URL**: https://arxiv.org/abs/2309.06527
- **Reference count**: 28
- **Primary result**: Machine translation models exhibit strong robustness against adversarial attacks, with character-level synthetic attacks being most effective

## Executive Summary
This paper investigates the robustness of machine translation models against adversarial attacks, a critical aspect for real-world deployment. The authors introduce three main attack strategies: gradient-based, BLEUER, and synthetic attacks, evaluating them on English-Russian translation tasks using Marian and MBART models. Their findings reveal that machine translation models are significantly more robust to adversarial attacks compared to computer vision and text classification models. The degree of output perturbation directly correlates with input perturbation, and synthetic character-level attacks prove most effective. This robustness is attributed to the discrete nature of language tokens and semantic preservation constraints inherent in translation tasks.

## Method Summary
The study employs three attack strategies to evaluate machine translation robustness. The gradient attack iteratively replaces input tokens based on adversarial loss calculated with respect to model input embeddings. The BLEUER attack uses differentiable approximations of non-differentiable metrics like BLEU score, training additional heads to predict these metrics from encoder outputs. Synthetic attacks simulate common text perturbations such as typos and character swaps. These attacks are tested on English-Russian translation using WMT-14 dataset with Marian and MBART models, evaluating effectiveness through metrics including BLEU, chrF, METEOR, WER, Paraphrase similarity, and BertScore.

## Key Results
- Machine translation models display strong robustness against known adversarial attacks
- Degree of output perturbation is directly proportional to input perturbation
- Character-level synthetic attacks are most effective, outperforming gradient-based and BLEUER approaches
- Translation models show only slight vulnerability to adversarial inputs compared to drastic performance drops in computer vision and text classification

## Why This Works (Mechanism)

### Mechanism 1
Machine translation models are more robust to adversarial attacks than image classification or text classification models. The seq2seq architecture and discrete nature of language tokens make it harder to find perturbations that both change the output and preserve semantic meaning. Semantic preservation constraints prevent attackers from collapsing sentences into meaningless noise. This is supported by the observation that output perturbation correlates directly with input perturbation. If semantic preservation constraints are removed, attacks may succeed more easily.

### Mechanism 2
Character-level synthetic attacks are more effective than embedding-level gradient attacks on translation models. Character swaps bypass the model's token-level embedding space, introducing noise that is harder for the translation model to correct while preserving surface form. Translation models trained on clean data may not generalize well to character-level noise. The finding that character-level attacks are top-performing supports this mechanism. If translation models are augmented with character-level noise during training, robustness may improve.

### Mechanism 3
Differentiable approximations of non-differentiable metrics (BLEU, BERTScore) can guide adversarial attacks while preserving semantic meaning. Additional layers trained to predict BLEU/BERTScore from encoder outputs provide a gradient signal for targeted attacks without requiring exact metric computation. The approximation layers must generalize to unseen adversarial inputs. The use of BLEUER attack with additional heads supports this approach. If approximation layers fail to capture metric nuances, attacks may not converge.

## Foundational Learning

- **Concept: Gradient-based optimization in discrete spaces**
  - Why needed here: Attacks require finding token substitutions that maximize adversarial loss, but text is discrete
  - Quick check question: How does the algorithm compute gradients when the input space is non-differentiable?

- **Concept: Differentiable metric approximations**
  - Why needed here: BLEU and other metrics are non-differentiable, so attacks must approximate them to use gradient methods
  - Quick check question: What loss function is used to train the BLEU approximation head?

- **Concept: Pareto optimization in adversarial evaluation**
  - Why needed here: Balancing perturbation size vs translation degradation requires multi-objective optimization
  - Quick check question: What are the two axes typically plotted in a Pareto frontier for these attacks?

## Architecture Onboarding

- **Component map**: Input → Token embeddings → Encoder → Approximation head (if used) → Gradient computation → Token replacement → Decoder → Output

- **Critical path**: Input → Token embeddings → Encoder → Approximation head (if used) → Gradient computation → Token replacement → Decoder → Output

- **Design tradeoffs**:
  - Semantic preservation vs attack strength: stricter constraints reduce attack success but maintain input meaning
  - Character vs token level: character attacks are stronger but less semantically coherent
  - White-box vs black-box: white-box provides gradients but may be unrealistic; black-box requires transferability

- **Failure signatures**:
  - Attack converges to no changes: constraints too strict or gradients too small
  - Output is gibberish: character attack applied too aggressively or semantic constraints violated
  - BLEU/BERTScore approximation head fails: training data mismatch or overfitting

- **First 3 experiments**:
  1. Run gradient attack with increasing cosine distance thresholds; plot Pareto frontier
  2. Apply synthetic character swap with varying perturbation rates; compare BLEU degradation
  3. Train BLEUER head on validation set; test on held-out adversarial examples

## Open Questions the Paper Calls Out

### Open Question 1
Why are machine translation models more robust to adversarial attacks compared to computer vision and NLP classification models? The paper states that modern machine translation models are only slightly vulnerable to adversarial inputs, contrasting with drastic performance drops in other domains. This difference in robustness is not fully explained and may be due to architectural features, training methods, or the nature of the translation task itself.

### Open Question 2
How can the trade-off between preserving the original sentence and altering the attacked translation be optimized? The authors mention focusing on making attacks more meaningful while controlling translation metrics through differentiable approximations. The paper does not provide a definitive solution to balancing preservation of meaning and grammar with creating effective adversarial examples.

### Open Question 3
What is the impact of character-level perturbations on the robustness of machine translation models? The paper notes that the biggest vulnerability comes from character-level attacks, suggesting adversarial examples fall out of the training data domain. The reasons why character-level perturbations are more effective and what specific training data characteristics make models vulnerable remain unexplored.

## Limitations

- The experiments focus exclusively on English-Russian translation, limiting generalizability to other language pairs with different morphological complexity
- Only two specific model architectures (Marian and MBART) are evaluated, leaving uncertainty about results across other popular architectures
- Evaluation metrics may not fully capture semantic preservation or fluency degradation, particularly for synthetic character-level attacks

## Confidence

**High Confidence**: The core finding that machine translation models show robustness against adversarial attacks is well-supported by experimental results.

**Medium Confidence**: The claim that character-level synthetic attacks are most effective has strong empirical support but may be limited by implementation details.

**Low Confidence**: Broader implications about MT robustness compared to other NLP tasks are mentioned but not directly tested.

## Next Checks

1. **Cross-Lingual Robustness Test**: Evaluate the same attack methodologies on language pairs with different typological properties (e.g., English-Japanese, English-Arabic) to assess whether observed robustness generalizes beyond Slavic languages.

2. **Black-Box Attack Evaluation**: Implement transfer-based black-box attacks where adversarial examples generated against one MT model are tested against a different model architecture to better reflect realistic attack scenarios.

3. **Semantic Preservation Validation**: Conduct human evaluation studies to assess whether character-level synthetic attacks, despite being effective at degrading BLEU scores, maintain semantic coherence from a human perspective, and whether current automatic metrics adequately capture this dimension.