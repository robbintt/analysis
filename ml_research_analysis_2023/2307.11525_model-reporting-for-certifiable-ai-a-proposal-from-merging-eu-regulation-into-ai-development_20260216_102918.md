---
ver: rpa2
title: 'Model Reporting for Certifiable AI: A Proposal from Merging EU Regulation
  into AI Development'
arxiv_id: '2307.11525'
source_url: https://arxiv.org/abs/2307.11525
tags:
- data
- describe
- dataset
- test
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for reporting AI applications during
  development using standardized cards. The cards document use cases, data, models,
  and operations while incorporating regulatory requirements from the EU AI Act and
  insights from certification experts and affected individuals.
---

# Model Reporting for Certifiable AI: A Proposal from Merging EU Regulation into AI Development

## Quick Facts
- arXiv ID: 2307.11525
- Source URL: https://arxiv.org/abs/2307.11525
- Reference count: 25
- This paper proposes a framework for reporting AI applications during development using standardized cards that incorporate regulatory requirements from the EU AI Act.

## Executive Summary
This paper proposes a standardized reporting framework for AI applications throughout the development process, using four distinct cards: Use Case, Data, Model, and Operation. The framework aims to enable efficient third-party auditing, improve transparency, and build trust in AI systems by incorporating regulatory requirements from the EU AI Act and insights from interviews with certification experts, developers, and affected individuals. Each card focuses on a specific phase of AI development, reducing cognitive load while ensuring comprehensive documentation that addresses fairness, transparency, accountability, and regulatory compliance.

## Method Summary
The proposed method organizes AI development reporting into four standardized cards that document different aspects of the AI lifecycle. The Use Case Card covers risk assessment based on fairness, transparency, and accountability dimensions. The Data Card extends previous work to address regulatory requirements and includes considerations around dataset size, coverage, and leakage prevention. The Model Card builds on existing model cards with additional sections on explainability, feature engineering, and real-world testing. The Operation Card provides a comprehensive checklist for monitoring, security, privacy, and MLOps. The framework incorporates EU AI Act requirements through explicit references and draws from stakeholder interviews to ensure practical applicability.

## Key Results
- Four-card structure reduces documentation complexity by mapping each AI development phase to a single focused artifact
- Regulatory compliance is embedded directly into documentation sections through AI Act references
- Stakeholder input from certification experts, developers, and affected individuals increases framework trust and adoption potential
- Each card addresses specific risk dimensions (fairness, transparency, accountability) relevant to high-risk AI applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The four-card structure reduces cognitive load for developers by mapping each phase of AI development to a single, focused documentation artifact.
- Mechanism: By organizing reporting into Use Case, Data, Model, and Operation cards, developers only need to consult one relevant card at each stage, avoiding the overwhelm of multi-topic documents.
- Core assumption: Developers follow the development process linearly and update cards iteratively as the project evolves.
- Evidence anchors:
  - [abstract]: "We propose the use of standardized cards to document AI applications throughout the development process."
  - [section]: "The AI development cycle is broken down into four steps...we use four cards as method of reporting in order to reduce the number of resulting documents."
- Break condition: If the development process is non-linear or if multiple cards must be referenced simultaneously, the cognitive load benefit diminishes.

### Mechanism 2
- Claim: The cards incorporate regulatory compliance by embedding AI Act requirements directly into each documentation section.
- Mechanism: Each card section explicitly lists AI Act article references and risk dimensions, enabling practitioners to align their documentation with legal obligations without needing to consult the full legislation.
- Core assumption: The AI Act requirements are stable enough that embedding them into the cards remains valid over time.
- Evidence anchors:
  - [abstract]: "Our approach involves the inclusion of robust references to relevant support materials and toolboxes, emphasizing the transparency and traceability of each requirement's source."
  - [section]: "As in the other cards for almost every aspect there exist one or multiple references to the AI Act."
- Break condition: If the AI Act undergoes significant revisions or interpretations change, the embedded references may become outdated or misleading.

### Mechanism 3
- Claim: The inclusion of input from certification experts, developers, and affected individuals increases trust and practical applicability of the framework.
- Mechanism: By interviewing these three stakeholder groups, the framework addresses both technical rigor (for certification) and usability (for developers and end-users), making the cards more likely to be adopted and trusted.
- Core assumption: Stakeholder interviews accurately capture the needs and constraints of each group.
- Evidence anchors:
  - [abstract]: "Our work incorporates insights from interviews with certification experts as well as developers and individuals working with the developed AI applications."
  - [section]: "The affected individuals mainly wished to be included in the development process in order to convince themselves about the functionality of the AI system."
- Break condition: If stakeholder feedback is not representative or if implementation diverges from the documented needs, trust and adoption may suffer.

## Foundational Learning

- Concept: EU AI Act risk classification (forbidden, high-risk, low-risk, and transparency obligations)
  - Why needed here: Determines which cards and sections are mandatory for a given AI application.
  - Quick check question: What are the three risk classes defined in the AI Act, and which applications fall into the high-risk category?

- Concept: Model cards and data cards as documentation standards
  - Why needed here: Forms the basis for the proposed extensions (Use Case and Operation cards) and ensures compatibility with existing industry practices.
  - Quick check question: What are the main sections typically included in a model card, and how do they differ from those in a data card?

- Concept: Adversarial robustness and fairness metrics
  - Why needed here: Required for quantifying and mitigating risks in the Use Case and Model cards, especially for high-risk applications.
  - Quick check question: Name two metrics used to evaluate adversarial robustness and two used to assess fairness in AI systems.

## Architecture Onboarding

- Component map:
  - Use Case Card: Use case summary, problem definition, solution approach, risk assessment
  - Data Card: General info, data description, collection, labeling, splitting, preprocessing, analysis, serving
  - Model Card: General info, description, explainability, feature engineering, model selection, metrics, confidence, real-world testing
  - Operation Card: Scope, operating concept, autonomy, responsibilities, performance, interface, security, privacy, MLOps, testing
- Critical path: Use Case → Data → Model → Operation (linear development flow)
- Design tradeoffs: Cards are separate for clarity but require synchronization; embedding AI Act references ensures compliance but risks obsolescence
- Failure signatures: Missing AI Act references → compliance gaps; incomplete risk assessment → audit failures; no stakeholder input → low trust and adoption
- First 3 experiments:
  1. Create a toy Use Case Card for a simple image classification task and verify AI Act references are correct.
  2. Build a Data Card for a publicly available dataset, documenting collection, labeling, and preprocessing steps.
  3. Develop a Model Card for a trained model, including explainability methods and real-world testing results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop standardized best practices for creating datasets that address issues like dataset size, coverage, and leakage prevention?
- Basis in paper: [explicit] The paper identifies challenges in establishing best practices for dataset creation, particularly regarding dataset size and coverage. It suggests that upcoming ISO standards may not include detailed requirements in these areas.
- Why unresolved: There is a lack of established practices and the paper acknowledges that the upcoming ISO standards may not provide sufficient detail. The field of AI and regulation is progressing rapidly, so best practices are likely to evolve.
- What evidence would resolve it: Developing and validating standardized methodologies for dataset creation, including guidelines for dataset size estimation, coverage analysis, and leakage prevention techniques, supported by empirical studies and industry adoption.

### Open Question 2
- Question: What are the most effective methods for evaluating explainability approaches in AI systems, considering the different needs of various stakeholders?
- Basis in paper: [explicit] The paper highlights the challenge of evaluating explainability methods, noting that most do not provide formal guarantees and that current evaluation metrics are not widely adopted. It suggests testing in real-world scenarios with the target audience.
- Why unresolved: There is a lack of consensus on evaluation metrics and the paper acknowledges the need for further research in this area. The effectiveness of explainability methods can vary depending on the specific use case and stakeholder needs.
- What evidence would resolve it: Conducting comparative studies of different explainability evaluation methods, involving diverse stakeholders in the evaluation process, and developing standardized frameworks for assessing explainability that consider the specific context and requirements of different AI applications.

### Open Question 3
- Question: How can we effectively monitor and detect model drift in AI systems, particularly in dynamic environments where data distributions and concepts may change over time?
- Basis in paper: [explicit] The paper discusses the importance of monitoring model drift, including changes in concept, data, or software, and suggests using metrics like f-divergence to measure differences in data distribution. However, it acknowledges the challenges in effectively detecting and addressing drift.
- Why unresolved: Model drift is a complex phenomenon that can arise from various sources, and there is no one-size-fits-all solution. The effectiveness of drift detection methods can depend on the specific characteristics of the AI system and the nature of the drift.
- What evidence would resolve it: Developing and evaluating robust drift detection algorithms that can handle different types of drift, conducting empirical studies to assess the performance of these algorithms in real-world scenarios, and establishing best practices for monitoring and mitigating drift in AI systems.

## Limitations
- Template ambiguity: The paper does not provide concrete templates or formats for each card type, making implementation dependent on interpretation
- Dynamic regulatory environment: Embedding specific AI Act references creates a risk of obsolescence as regulations evolve
- Validation gap: The proposed framework lacks empirical validation through real-world adoption studies or case examples

## Confidence
- High Confidence: The four-card structure as a practical approach to organizing AI development documentation
- Medium Confidence: The integration of EU AI Act requirements into documentation templates
- Low Confidence: The claim that stakeholder interviews meaningfully increased trust and practical applicability

## Next Checks
1. **Template Validation**: Develop and test concrete templates for each card type with a small group of AI practitioners to assess usability and completeness.
2. **Regulatory Mapping Audit**: Cross-reference the embedded AI Act citations with the actual legislation to verify accuracy and identify any gaps or outdated references.
3. **Stakeholder Feedback Analysis**: Conduct a structured survey with diverse AI stakeholders (including those not interviewed for the original work) to evaluate the framework's perceived utility and identify potential improvements.