---
ver: rpa2
title: Incorporating Distributions of Discourse Structure for Long Document Abstractive
  Summarization
arxiv_id: '2305.16784'
source_url: https://arxiv.org/abs/2305.16784
tags:
- discourse
- linguistics
- computational
- summarization
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RSTformer, a transformer-based summarization
  model that incorporates both the types and uncertainty of rhetorical relations from
  Rhetorical Structure Theory (RST) into its attention mechanism. Unlike previous
  approaches that only consider the nuclearity annotation, RSTformer utilizes labeled
  discourse distributions to guide the attention heads in the encoder to specialize
  in specific discourse categories.
---

# Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization

## Quick Facts
- **arXiv ID**: 2305.16784
- **Source URL**: https://arxiv.org/abs/2305.16784
- **Reference count**: 40
- **Key outcome**: RSTformer incorporates labeled discourse distributions into attention heads, outperforming state-of-the-art models on three long document summarization datasets with improved faithfulness and informativeness.

## Executive Summary
This paper introduces RSTformer, a transformer-based summarization model that leverages Rhetorical Structure Theory (RST) discourse relations to improve long document abstractive summarization. Unlike previous approaches that only consider nuclearity annotations, RSTformer incorporates both types and uncertainty of rhetorical relations through labeled discourse distributions (LDD) integrated into the attention mechanism. The model demonstrates superior performance across three datasets (BookSum Chapter, eLife, and Multi-LexSum) compared to baseline models, achieving better results on multiple automatic metrics including ROUGE, BERTScore, and Meteor, while also showing improved faithfulness and informativeness in human evaluation.

## Method Summary
RSTformer extends the Longformer architecture by modifying its attention mechanism to incorporate labeled discourse distributions from RST parsing. The model generates LDD tensors from parser outputs and injects these into attention matrices via element-wise multiplication, allowing attention heads to specialize in specific discourse relation types. The approach preserves parser uncertainty by using probability distributions over multiple RST trees rather than committing to single parses. Two model variants are evaluated: one using discourse relation types and one without, both compared against a baseline Longformer encoder-decoder.

## Key Results
- RSTformer outperforms state-of-the-art models across three datasets (BookSum Chapter, eLife, Multi-LexSum) on multiple automatic metrics
- Incorporating labeled discourse distributions provides better performance than using discourse distributions without relation types
- Human evaluation shows RSTformer generates more faithful, informative, readable, and concise summaries compared to baseline models
- The model demonstrates improved factual consistency and novel word generation capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Discourse relations with uncertainty distributions guide attention heads to specialize in specific relation types
- **Mechanism**: The model injects labeled discourse distributions (LDD) into the attention matrix via element-wise multiplication, allowing attention heads to dynamically optimize while maintaining proximity to parser probabilities
- **Core assumption**: Discourse relation types provide meaningful signal for summarization that attention heads can learn to exploit
- **Evidence anchors**:
  - [abstract] "Our RST-attention mechanism... is an extension of the recently devised Longformer framework"
  - [section 3.2] "each attention head is assigned a different discourse matrix LDDl for a specific relation l"
  - [corpus] Weak - no direct experimental evidence shown for this specific mechanism, only that it outperforms baseline
- **Break condition**: If discourse parser produces uniform or random probabilities, the LDD tensor provides no meaningful guidance and attention specialization fails

### Mechanism 2
- **Claim**: Incorporating discourse structure improves factual consistency and novel word generation in summaries
- **Mechanism**: By explicitly modeling discourse relations, the model better understands which content is central vs peripheral, leading to more accurate content selection and generation
- **Core assumption**: RST-based discourse structure captures information that improves summarization quality beyond what attention alone learns
- **Evidence anchors**:
  - [section 4.3] "the proposed model also performs better than the baseline model in terms of model consistency checks"
  - [section 4.3] "incorporating discourse information into the model does increase the ability of the model to generate novel words"
  - [corpus] Moderate - human evaluation shows improved faithfulness and informativeness scores
- **Break condition**: If the discourse structure becomes too noisy or the parser errors dominate, the model may learn incorrect content importance signals

### Mechanism 3
- **Claim**: Preserving parser uncertainty distributions is more effective than using 1-best RST trees
- **Mechanism**: The model uses probability distributions over multiple RST trees rather than committing to a single parse, allowing it to hedge against parser errors
- **Core assumption**: RST parsing is inherently uncertain and this uncertainty contains useful information for downstream tasks
- **Evidence anchors**:
  - [abstract] "we posit that the output of the RST parser holds greater significance when it not only provides the n-best results but also conveys the remaining uncertainty"
  - [section 1] "there may be benefits in considering distributions over coherence relation labels, rather than limiting analysis to the 1-best results"
  - [corpus] Moderate - ablation study shows model performs worse when attention calculation is skipped (W AC) or when using random attention values
- **Break condition**: If parser uncertainty is uninformative (e.g., near-uniform distributions), incorporating it adds noise without benefit

## Foundational Learning

- **Concept**: Rhetorical Structure Theory (RST) and discourse parsing
  - **Why needed here**: The entire approach relies on understanding how discourse relations between text segments can guide summarization
  - **Quick check question**: What distinguishes nucleus from satellite EDUs in RST, and why is this distinction important for summarization?

- **Concept**: Longformer's sparse attention mechanism
  - **Why needed here**: The model extends Longformer by modifying its attention mechanism with discourse information
  - **Quick check question**: How does Longformer's sliding window attention differ from standard transformer attention, and what problem does it solve?

- **Concept**: Multi-head self-attention specialization
  - **Why needed here**: The approach relies on attention heads learning to specialize in different discourse relation types
  - **Quick check question**: How can different attention heads learn different functions, and what mechanisms enable this specialization?

## Architecture Onboarding

- **Component map**: DMRST parser → LDD tensor generator → Longformer encoder with RST-attention layers → Decoder
- **Critical path**: Discourse parsing → LDD tensor generation → Attention modification → Summary generation
- **Design tradeoffs**: 
  - Using discourse distributions vs. 1-best trees: More robust but computationally heavier
  - Element-wise multiplication vs. concatenation: Maintains attention mechanism while incorporating discourse signal
  - Discourse relation grouping: Balances granularity vs. parameter efficiency
- **Failure signatures**:
  - Poor discourse parsing quality → Noisy LDD tensors → Degraded summarization
  - Uniform LDD values → No attention specialization benefit
  - Incorrect discourse relation grouping → Attention heads receive mixed signals
- **First 3 experiments**:
  1. Implement LDD tensor generation from DMRST parser outputs on small dataset
  2. Modify single Longformer attention layer to incorporate LDD via element-wise multiplication
  3. Test attention head specialization by examining attention patterns for different discourse relations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RSTformer change when using different external RST parsers with varying levels of accuracy and domain-specific training?
- Basis in paper: [explicit] The paper acknowledges that RST parsing performance deteriorates with increasing document complexity and domain shift, and uses DMRST as the parser.
- Why unresolved: The paper only evaluates one parser (DMRST) and doesn't explore the impact of parser quality or domain adaptation on summarization performance.
- What evidence would resolve it: Comparative experiments using multiple RST parsers (different architectures, training domains, or accuracy levels) on the same summarization tasks, measuring both parsing accuracy and downstream summarization performance.

### Open Question 2
- Question: What is the impact of incorporating higher-order discourse relations (beyond pairwise EDU connections) on summarization quality, and how does this affect computational complexity?
- Basis in paper: [inferred] The paper focuses on pairwise EDU relations within the sliding window, but RST trees can represent hierarchical structures with multiple EDUs at different levels.
- Why unresolved: The paper doesn't explore whether incorporating broader discourse structure (e.g., grouping EDUs into larger discourse units) would provide additional benefits or computational challenges.
- What evidence would resolve it: Experiments comparing RSTformer with variants that incorporate hierarchical discourse information, measuring both performance improvements and computational costs.

### Open Question 3
- Question: How does the proposed LDD tensor approach compare to alternative methods of incorporating discourse uncertainty, such as Monte Carlo dropout or ensemble parsing approaches?
- Basis in paper: [explicit] The paper presents LDD as a method to incorporate discourse uncertainty, but doesn't compare it to other uncertainty quantification methods.
- Why unresolved: The paper doesn't evaluate whether LDD is the most effective way to capture discourse uncertainty or how it compares to other uncertainty modeling techniques.
- What evidence would resolve it: Head-to-head comparisons between LDD and alternative uncertainty modeling approaches (dropout-based, ensemble-based, or probabilistic parsing methods) on the same summarization tasks.

### Open Question 4
- Question: What is the relationship between the learned attention specialization for different discourse types and the actual importance of those discourse relations for summarization quality?
- Basis in paper: [inferred] The paper states that attention heads specialize in specific discourse categories, but doesn't analyze which discourse types are most beneficial for summarization.
- Why unresolved: The paper doesn't provide analysis of which discourse relation types contribute most to summarization performance or how the model's learned attention weights align with human judgments of discourse importance.
- What evidence would resolve it: Ablation studies removing specific discourse relation types from the LDD tensor, combined with correlation analysis between learned attention weights and human-annotated discourse importance ratings.

## Limitations
- Discourse parser quality directly impacts model performance - errors in RST parsing propagate to LDD tensors and degrade summarization
- Computational overhead of maintaining and applying LDD tensors across all attention heads may limit scalability to longer documents
- Specific discourse relation groupings may not be optimal for all domains or document types

## Confidence
- **High confidence**: Overall architecture and training methodology is well-documented and reproducible
- **Medium confidence**: Effectiveness of discourse relation types versus nuclearity-only annotations based on ablation study
- **Medium confidence**: Human evaluation results showing improved faithfulness and informativeness
- **Low confidence**: Specific mechanism by which attention heads learn to specialize in discourse relation types

## Next Checks
1. **Parser quality validation**: Measure the impact of discourse parser accuracy on summarization performance by testing with varying parser quality thresholds and quantifying the relationship between parser F1 scores and model output quality

2. **Attention head specialization analysis**: Visualize attention patterns for different heads across the same discourse relations to verify that heads are actually specializing as claimed, using attention visualization tools to examine attention weight distributions

3. **Computational overhead benchmarking**: Measure the actual runtime and memory overhead introduced by the LDD tensor integration compared to baseline Longformer, testing on documents of increasing length to establish scaling characteristics