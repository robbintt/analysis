---
ver: rpa2
title: Can large language models provide useful feedback on research papers? A large-scale
  empirical analysis
arxiv_id: '2310.01783'
source_url: https://arxiv.org/abs/2310.01783
tags:
- feedback
- human
- comments
- papers
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed an automated pipeline using GPT-4 to generate
  scientific feedback on research papers and evaluated its quality through two large-scale
  analyses. The first compared GPT-4 feedback with human peer reviewer feedback on
  3,096 Nature family journal papers and 1,709 ICLR conference papers, finding substantial
  overlap (30.85% and 39.23% respectively) comparable to human reviewer overlap (28.58%
  and 35.25%).
---

# Can large language models provide useful feedback on research papers? A large-scale empirical analysis

## Quick Facts
- arXiv ID: 2310.01783
- Source URL: https://arxiv.org/abs/2310.01783
- Reference count: 40
- Primary result: GPT-4 feedback achieves 30.85% overlap with human reviewers for Nature journals and 39.23% for ICLR, with 57.4% of researchers finding it helpful

## Executive Summary
This study developed an automated pipeline using GPT-4 to generate scientific feedback on research papers and evaluated its quality through two large-scale analyses. The first compared GPT-4 feedback with human peer reviewer feedback on 3,096 Nature family journal papers and 1,709 ICLR conference papers, finding substantial overlap (30.85% and 39.23% respectively) comparable to human reviewer overlap (28.58% and 35.25%). The second was a user study with 308 researchers who found GPT-4 feedback helpful or very helpful (57.4%) and more beneficial than some human reviewers (82.4%). While GPT-4 tends to focus on certain aspects like implications of research and often struggles with in-depth method critique, the results demonstrate that LLM-generated feedback can complement human feedback, particularly when timely expert feedback is unavailable.

## Method Summary
The study developed a GPT-4-based pipeline that parses scientific papers from PDF, extracts key content (title, abstract, captions, main text), and generates structured feedback across four sections: significance/novelty, reasons for acceptance, reasons for rejection, and improvement suggestions. The pipeline was evaluated retrospectively by comparing GPT-4 feedback with human peer reviews from Nature family journals and ICLR conference papers, and prospectively through a user study where researchers uploaded their own papers for feedback and rated the helpfulness of the generated reviews.

## Key Results
- GPT-4 feedback overlapped with human reviewer comments at 30.85% for Nature journals and 39.23% for ICLR, comparable to human-human overlap (28.58% and 35.25%)
- 57.4% of researchers found GPT-4 feedback helpful or very helpful, with 82.4% finding it more beneficial than feedback from at least some human reviewers
- GPT-4 tends to focus more on implications of research rather than in-depth method critique, but provides perspectives that complement human feedback

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM feedback overlaps significantly with human feedback because both are identifying similar types of scientific concerns (e.g., methodological issues, novelty, clarity).
- Mechanism: Both human reviewers and the LLM are trained to recognize common scientific critique patterns, so they tend to flag similar categories of problems when evaluating a paper.
- Core assumption: Scientific papers contain identifiable, recurring structural and methodological weaknesses that both humans and LLMs can recognize.
- Evidence anchors:
  - [abstract]: "The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR)."
  - [section]: "Comments raised by multiple human reviewers are disproportionately more likely to be hit by GPT-4" indicates both focus on similar major issues.
  - [corpus]: Weak - the corpus shows related work on AI-generated summaries and evaluations but not direct evidence of shared critique patterns.
- Break condition: If the LLM's training data does not include sufficient examples of high-quality scientific critique, or if papers contain highly specialized issues that deviate from common critique patterns, the overlap would diminish.

### Mechanism 2
- Claim: The LLM generates paper-specific feedback rather than generic comments, as evidenced by shuffling experiments.
- Mechanism: The system's prompt construction and the LLM's attention to specific paper content (title, abstract, figures, text) enables it to tailor feedback to each manuscript rather than producing boilerplate critiques.
- Core assumption: The LLM can parse and retain sufficient context from the input paper to generate unique, paper-relevant feedback.
- Evidence anchors:
  - [abstract]: "The pairwise overlap significantly decreased from 30.85% to 0.43% after shuffling" in Nature data, and from 39.23% to 3.91% in ICLR data.
  - [section]: "Is it possible that LLM merely generates generic feedback applicable to multiple papers? A potential null model is that LLM mostly produces generic feedback applicable to many papers."
  - [corpus]: Weak - no direct corpus evidence of paper-specific generation; assumption based on experimental design.
- Break condition: If the prompt is too generic or the LLM's context window truncates critical content, the feedback may become repetitive and less paper-specific.

### Mechanism 3
- Claim: Researchers find LLM feedback helpful because it provides timely, diverse perspectives and highlights overlooked issues.
- Mechanism: The LLM acts as an additional reviewer, offering supplementary viewpoints that complement human feedback, especially on aspects like implications or experimental breadth.
- Core assumption: Researchers value diverse, timely feedback, and the LLM's perspective can fill gaps left by human reviewers.
- Evidence anchors:
  - [abstract]: "More than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers."
  - [section]: "65.3% of participants think at least to some extent LLM feedback offers perspectives that have been overlooked or underemphasized by humans."
  - [corpus]: Weak - related work on AI feedback in education exists, but no direct corpus evidence of researchers valuing AI-generated scientific feedback.
- Break condition: If the LLM's feedback is perceived as too generic, lacks depth in technical critique, or conflicts with human expertise, researchers may discount its utility.

## Foundational Learning

- Concept: Text extraction and summarization techniques (e.g., extractive summarization, semantic text matching).
  - Why needed here: The study relies on extracting key comments from both LLM and human feedback and matching them semantically to measure overlap.
  - Quick check question: How does extractive summarization differ from abstractive summarization in identifying key points from a text?

- Concept: Set overlap metrics (e.g., Jaccard index, Sørensen–Dice coefficient, Szymkiewicz–Simpson overlap coefficient).
  - Why needed here: These metrics are used to quantify the degree of overlap between LLM and human feedback, providing a numerical basis for comparison.
  - Quick check question: What does a Jaccard index of 0.5 indicate about the similarity between two sets of comments?

- Concept: Prompt engineering and context window management in LLMs.
  - Why needed here: The study constructs prompts to guide GPT-4 in generating structured feedback, and must manage token limits to include sufficient paper content.
  - Quick check question: Why is it important to balance prompt specificity with token constraints when using LLMs for document analysis?

## Architecture Onboarding

- Component map: PDF parsing -> Text extraction (title, abstract, captions, main text) -> GPT-4 prompt construction -> LLM feedback generation -> Comment extraction and summarization -> Semantic text matching -> Overlap calculation

- Critical path:
  1. Parse PDF → Extract text sections
  2. Construct GPT-4 prompt with paper content
  3. Generate structured feedback in one pass
  4. Extract and summarize comments from both LLM and human feedback
  5. Match comments semantically and calculate overlap metrics
  6. Analyze results and gather user feedback

- Design tradeoffs:
  - Token limit vs. completeness: Including more paper content improves feedback quality but risks exceeding GPT-4's context window.
  - Specificity vs. generality: More detailed prompts yield targeted feedback but may reduce generalizability across paper types.
  - Automation vs. human oversight: Fully automated feedback generation is scalable but may miss nuanced critique best provided by experts.

- Failure signatures:
  - Low overlap with human feedback: May indicate generic or off-topic LLM responses, insufficient paper context, or mismatch in critique focus.
  - High variance in user satisfaction: Could result from inconsistent feedback quality, lack of technical depth, or misalignment with researcher expectations.
  - Shuffling experiment shows high overlap: Suggests LLM is producing generic feedback rather than paper-specific insights.

- First 3 experiments:
  1. Run the pipeline on a small set of papers with known human reviews; manually verify that extracted comments align with human annotations.
  2. Perform a shuffling test to confirm that LLM feedback is paper-specific rather than generic.
  3. Conduct a pilot user study with a handful of researchers to gather qualitative feedback on the helpfulness and specificity of the LLM-generated reviews.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the LLM-generated feedback compare to human feedback in terms of identifying and critiquing novel methodological contributions in research papers?
- Basis in paper: [explicit] The paper mentions that the LLM tends to focus on certain aspects of scientific feedback, such as implications of research and often struggles to provide in-depth critique of method design.
- Why unresolved: The study primarily focuses on the overlap between LLM and human feedback, but does not delve deeply into the quality of critique on methodological aspects. The authors acknowledge that the LLM struggles with in-depth method critique, but do not provide a detailed analysis of how this limitation affects the overall quality of feedback.
- What evidence would resolve it: A more detailed analysis comparing the LLM's feedback on methodological aspects with human feedback, including specific examples of strengths and weaknesses in each, would help resolve this question.

### Open Question 2
- Question: To what extent can LLM-generated feedback help identify and correct errors in scientific papers, such as typos, mistakes in data analyses, or errors in mathematical equations?
- Basis in paper: [inferred] The paper suggests that one direction for future work is to explore the extent to which the proposed approach can help identify and correct errors in scientific papers. However, this aspect is not directly studied in the current work.
- Why unresolved: The current study focuses on the overlap between LLM and human feedback and user perceptions of helpfulness, but does not investigate the LLM's ability to detect and correct errors in papers. This is a significant limitation, as error detection and correction are crucial aspects of scientific feedback.
- What evidence would resolve it: An experimental study that artificially introduces various types of errors into scientific papers and evaluates the LLM's ability to detect and correct these errors would provide valuable insights into this aspect of LLM-generated feedback.

### Open Question 3
- Question: How do researchers from different disciplines perceive the usefulness and reliability of LLM-generated feedback compared to their own field's specific requirements and norms?
- Basis in paper: [explicit] The user study was conducted with researchers in AI and computational biology, but the authors acknowledge that their results are based on responses from researchers in these specific fields.
- Why unresolved: The study's user survey only includes participants from AI and computational biology, which may limit the generalizability of the findings to other scientific disciplines. Different fields may have varying expectations and norms for feedback, and the LLM's performance may vary accordingly.
- What evidence would resolve it: Conducting user studies with researchers from a broader range of scientific disciplines would help understand how the usefulness and reliability of LLM-generated feedback are perceived across different fields. Comparing the results across disciplines could reveal field-specific strengths and limitations of the LLM feedback system.

## Limitations
- The evaluation relies on retrospective comparison with existing human reviews, which may not capture all dimensions of feedback quality
- User study sample (308 participants) provides positive but potentially biased feedback from self-selected researchers
- The study focuses on high-impact journals and conferences, limiting applicability to lower-tier venues or highly specialized domains
- LLM feedback shows consistent weakness in providing in-depth technical critique of methodological aspects

## Confidence
- **High confidence**: The quantitative overlap measurements between LLM and human feedback are robust, with clear statistical comparisons and controlled shuffling experiments validating paper-specificity.
- **Medium confidence**: The user study findings on helpfulness and perceived benefit, while based on reasonable sample sizes, rely on self-reported perceptions that may not translate to actual paper improvement.
- **Medium confidence**: The claim that LLM feedback complements human feedback is supported by data but may overstate the depth and technical accuracy of LLM critiques, particularly for complex methodological issues.

## Next Checks
1. Conduct a controlled experiment where papers receive only LLM feedback, only human feedback, or both, then measure actual acceptance rates or quality improvements rather than just perceived helpfulness.
2. Test the pipeline across a broader range of venues (preprints, workshops, lower-tier journals) to assess generalizability beyond Nature and ICLR.
3. Implement a longitudinal study tracking whether papers that incorporated LLM feedback show measurable improvements in subsequent submission outcomes.