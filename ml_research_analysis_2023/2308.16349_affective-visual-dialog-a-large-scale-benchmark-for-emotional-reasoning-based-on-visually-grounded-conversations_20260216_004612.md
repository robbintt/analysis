---
ver: rpa2
title: 'Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based
  on Visually Grounded Conversations'
arxiv_id: '2308.16349'
source_url: https://arxiv.org/abs/2308.16349
tags:
- emotion
- image
- visual
- dataset
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Affective Visual Dialog is a new task and dataset designed to
  study how visual information and language-based dialogue influence human emotions.
  It introduces a large-scale dataset (AffectVisDial) with 50,000 visually grounded
  dialogs, each consisting of 10 question-answer exchanges, concluding emotion attributions,
  and textual emotion explanations from two perspectives: a Questioner (without image
  access) and an Answerer (with image access).'
---

# Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning Based on Visually Grounded Conversations

## Quick Facts
- arXiv ID: 2308.16349
- Source URL: https://arxiv.org/abs/2308.16349
- Reference count: 40
- Introduces AffectVisDial, a large-scale dataset with 50,000 visually grounded dialogs for studying emotional reasoning through conversation

## Executive Summary
Affective Visual Dialog introduces a new task and dataset designed to study how visual information and language-based dialogue influence human emotions. The AffectVisDial dataset contains 50,000 visually grounded dialogs, each with 10 question-answer exchanges, concluding emotion attributions, and textual emotion explanations from two perspectives: a Questioner (without image access) and an Answerer (with image access). The dataset was collected using a novel protocol involving live communication and two opposing emotional opinions to encourage open-minded exploration of hidden visual stimuli. Experiments demonstrate that incorporating dialog context significantly improves emotion prediction and explanation generation compared to using only opinions or image descriptions, advancing research in affective vision and language understanding.

## Method Summary
The AffectVisDial dataset was collected using Amazon Mechanical Turk with a novel protocol involving live communication between two participants - a Questioner without image access and an Answerer with image access. Each dialog consists of 10 question-answer exchanges, two opposing emotional opinions, concluding emotion attributions, and textual emotion explanations. The dataset contains 50,000 unique dialogs with 500,000 QA pairs total. Several baseline models were introduced including RoBERTa for emotion classification, BART and T5 for explanation generation, and LTMI-D for question answering. Models were fine-tuned on the dataset and evaluated using metrics including weighted F1-score for emotion classification, MR/MRR/R@1/R@5/R@10 for question answering, and BLEU/BERTscore/BARTscore for explanation generation.

## Key Results
- Incorporating dialog context significantly improves emotion prediction accuracy compared to using only opinions or image descriptions
- Approximately 23% of the time, the Questioner changed their emotion choice after seeing the hidden image, demonstrating the importance of visual information in emotion construction
- Models trained on AffectVisDial can enable emotion-guided answer generation and emotionally reasoned image editing, showing practical applications for affective reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual stimuli are necessary but insufficient for emotional response; language-based dialogue context significantly enhances emotion classification accuracy
- Mechanism: The dataset design separates two modalitiesâ€”visual (Answerer sees image) and linguistic (Questioner relies on dialog). Experiments show RoBERTa classifier trained on dialog-only input achieves 62.2% accuracy, while models using only opinions perform significantly worse
- Core assumption: Dialog exchanges capture nuanced emotional signals not explicitly encoded in either image or isolated text captions
- Evidence anchors: [abstract]: "incorporating dialog context significantly improves emotion prediction"; [section]: "Tab. 4 shows that models relying solely on opinions (C) and emotion representations (E) as input exhibit significantly poorer performance"
- Break condition: If dialog becomes too terse or repetitive, losing affective nuance, the mechanism fails as dialog context would no longer add value beyond raw image descriptions

### Mechanism 2
- Claim: Emotional explanations are more linguistically rich when participants have visual access to the image
- Mechanism: Answerers (with image access) produce explanations with higher Part-of-Speech (PoS) tag diversity compared to questioners explaining without image access, indicating that visual grounding enables more detailed, referent-rich explanations
- Core assumption: Visual access enables direct referencing of scene elements, reducing cognitive load for constructing explanations
- Evidence anchors: [section]: "Tab. 2 shows that the average number of PoS from explanations before observing images is larger than those after observing the image"
- Break condition: If the image is ambiguous or abstract, the advantage of visual access might diminish, breaking the PoS richness correlation

### Mechanism 3
- Claim: Targeted manipulation of dialog answers can shift predicted emotions, enabling emotion-guided image editing
- Mechanism: By selecting answers that maximize the probability of a target emotion (using a pretrained RoBERTa classifier), the model can steer emotional interpretation. This is validated by showing gradual emotion shifts when answers are replaced (Fig. 8)
- Core assumption: There is a predictable mapping between linguistic descriptions and emotional interpretations that can be manipulated
- Evidence anchors: [section]: "we investigate the influence of dialogue answers on resulting emotions... gradually altering the answers can shift the original emotion towards an opposing direction"
- Break condition: If the classifier's emotion predictions are brittle or highly subjective, manipulating answers may not reliably produce the intended emotional shift

## Foundational Learning

- Concept: Visual grounding in dialogue systems
  - Why needed here: The dataset and task require understanding how visual information and language interact to produce emotional judgments
  - Quick check question: Can you explain why a Questioner without image access must rely more heavily on linguistic cues than an Answerer?

- Concept: Emotion taxonomy and basic categories
  - Why needed here: The dataset uses Ekman's eight basic emotions (4 positive, 4 negative). Models must map dialog context to these discrete categories for classification and generation tasks
  - Quick check question: Which four emotions are considered "universal and basic" in the dataset, and why does this matter for model design?

- Concept: Multimodal model integration (text + image)
  - Why needed here: Baselines like NLX-GPT accept both image and language inputs. Understanding how to fuse these modalities is critical for reproducing and extending the results
  - Quick check question: What is the role of the image encoder in NLX-GPT when generating emotion explanations?

## Architecture Onboarding

- Component map: WikiArt images -> AMT live chat (Questioner/Answerer) -> emotion labeling -> explanation generation -> dataset splits
- Models: RoBERTa (emotion classification), BART/T5/NLX-GPT (explanation generation), LTMI-D (question answering), NN-based baselines
- Critical path: 1. Collect dialog and emotion labels 2. Fine-tune RoBERTa for emotion classification 3. Fine-tune generation models with prompt "I feel EMOTION because EXPLANATION" 4. Evaluate retrieval and generation metrics 5. Test emotion manipulation via answer substitution
- Design tradeoffs: Using synthetic vs real images allows controlled diversity but may not generalize to real-world scenes; Fixed dialog length (10 turns) ensures consistency but may truncate rich interactions
- Failure signatures: Low retrieval scores (MR > 30) indicates models cannot match questions to relevant answers; BLEU/BERTscore < 0.1 means generation models fail to produce coherent explanations; Emotion F1 < 40% suggests classifier cannot capture affective nuances
- First 3 experiments: 1. Replicate RoBERTa emotion classification on dialog-only input; compare to opinion-only baseline 2. Fine-tune BART with and without image descriptions; measure BLEU/BERTscore difference 3. Perform answer substitution to shift emotion; verify with classifier probability changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the emotional bias in the initial two opposing opinions affect the quality and diversity of the resulting dialogs in AffectVisDial?
- Basis in paper: [explicit] The paper mentions that revealing two opposing opinions (a negative and a positive caption) associated with the artwork at the beginning of the conversation is designed to counter biases of the Questioner and encourage open-mindedness towards the emotion triggered by the hidden visual signal
- Why unresolved: While the paper explains the rationale behind this design choice, it does not provide a detailed analysis of how this approach influences the overall quality and diversity of the dialogs
- What evidence would resolve it: A comparative analysis of dialogs where the initial opinions are opposing versus dialogs where the initial opinions are neutral or aligned could provide insights into the impact of emotional bias on dialog quality and diversity

### Open Question 2
- Question: To what extent can the models trained on AffectVisDial generalize to other types of visual stimuli beyond artworks, such as real-world images or videos?
- Basis in paper: [inferred] The paper mentions that the data collection protocol can be applied to any type of visual stimuli that constructs emotion experience, and it provides examples of dialogs with real images in the supplementary material
- Why unresolved: While the paper demonstrates the effectiveness of the models on artworks, it is unclear how well they would perform on other types of visual stimuli that may have different characteristics and elicit different emotional responses
- What evidence would resolve it: Evaluating the performance of the models on a diverse set of visual stimuli, including real-world images and videos, and comparing the results to their performance on artworks would provide insights into their generalizability

### Open Question 3
- Question: How do the linguistic and visual cues interact in shaping the emotional responses of the Questioner and Answerer in AffectVisDial?
- Basis in paper: [explicit] The paper discusses the interplay between visual cues and spoken language in conversational settings and how they contribute to the construction of emotions
- Why unresolved: While the paper highlights the importance of both linguistic and visual cues in emotion construction, it does not provide a detailed analysis of how these cues interact and influence each other
- What evidence would resolve it: Conducting controlled experiments where the availability of linguistic and visual cues is manipulated and analyzing the resulting emotional responses would shed light on the interaction between these cues in shaping emotions

## Limitations
- Dataset relies on synthetic artwork images rather than naturalistic photographs, limiting generalizability to real-world visual scenes
- Fixed 10-turn dialog structure might not capture the full range of conversational dynamics that influence emotional responses
- Use of only two opposing emotional opinions per dialog could create artificial constraints on emotional expression

## Confidence
- High Confidence: The core finding that dialog context improves emotion prediction accuracy is well-supported by direct comparisons in Table 4
- Medium Confidence: The claim about PoS diversity differences between questioners and answerers is supported by the analysis in Table 2, but relies on speculative reasoning about cognitive load
- Medium Confidence: The emotion manipulation results demonstrate directional shifts but lack quantitative validation of whether the emotional changes align with human judgments

## Next Checks
1. Conduct human evaluation studies to verify whether the generated explanations and emotion predictions align with human affective judgments, particularly for the emotion manipulation experiments
2. Test model generalization by evaluating on a subset of dialogs using real photographic images rather than artwork, to assess whether the visual grounding advantages persist across image domains
3. Perform ablation studies on dialog length and answer format to determine whether the 10-turn structure and binary opinion requirement are optimal for capturing emotional reasoning processes