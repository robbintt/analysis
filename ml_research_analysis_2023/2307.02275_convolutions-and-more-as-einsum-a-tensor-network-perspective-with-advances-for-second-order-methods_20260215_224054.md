---
ver: rpa2
title: 'Convolutions and More as Einsum: A Tensor Network Perspective with Advances
  for Second-Order Methods'
arxiv_id: '2307.02275'
source_url: https://arxiv.org/abs/2307.02275
tags:
- conv
- dense
- general
- mixed
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces tensor networks (TNs) as a powerful framework
  for simplifying the analysis and implementation of convolutional neural networks
  (CNNs). TNs represent tensor multiplications as diagrams, enabling intuitive reasoning
  and manipulation.
---

# Convolutions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods

## Quick Facts
- arXiv ID: 2307.02275
- Source URL: https://arxiv.org/abs/2307.02275
- Authors: 
- Reference count: 40
- Key outcome: Introduces tensor networks (TNs) as a framework for simplifying CNN analysis and implementation, showing up to 4.5x acceleration of KFAC variants

## Executive Summary
This paper presents tensor networks as a powerful diagrammatic framework for representing and manipulating convolutional neural network operations. By expressing tensor multiplications as diagrams, the authors show how to simplify analysis of convolutions, compute derivatives, and implement automatic differentiation. The framework supports all convolution hyper-parameters and generalizes to arbitrary dimensions, enabling new optimizations like hardware-efficient tensor dropout for approximate backpropagation.

## Method Summary
The paper represents tensor multiplications as diagrams (TNs) that simplify reading off structure like factorization and batching. It derives TN diagrams for convolution operations, their derivatives, and automatic differentiation, supporting all hyper-parameters and generalization to arbitrary dimensions. The authors propose convolution-specific transformations based on connectivity patterns to simplify diagrams before evaluation, and use opt einsum for contraction order optimization. Performance evaluation shows acceleration of KFAC variants and enables new tensor dropout capabilities.

## Key Results
- TN implementation accelerates a recently-proposed KFAC variant by up to 4.5x while reducing memory overhead
- The framework enables hardware-efficient tensor dropout for approximate backpropagation, reducing randomized autodiff cost
- Supports all convolution hyper-parameters and generalizes to arbitrary dimensions through TN diagrams

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tensor networks provide a unified diagrammatic notation for convolutions that simplifies both analysis and implementation.
- Mechanism: By representing tensor multiplications as diagrams with connected legs, TNs make the structure of convolutions immediately visible and manipulable through simple graphical operations.
- Core assumption: The tensor network diagram representation accurately captures all convolution operations and their derivatives.
- Evidence anchors:
  - [abstract] "TNs express tensor multiplications as diagrams (Figure 1). Those simplify reading off structure like factorization..."
  - [section 2.3] "Application to Convolution: We define a binary tensor P ∈ { 0, 1}I1×O1×K1×I2×O2×K2 which represents the connectivity pattern..."
  - [corpus] Weak evidence - no corpus papers directly validate the diagrammatic approach for convolutions, though einsum optimization literature supports the computational efficiency claims.

### Mechanism 2
- Claim: Tensor network differentiation simplifies computing derivatives of convolutions.
- Mechanism: Taking derivatives of TNs amounts to cutting out the argument tensor and introducing Kronecker deltas for the resulting open legs, which naturally preserves the full index structure without flattening.
- Evidence anchors:
  - [section 3.1] "Consider an arbitrary TN represented by the tensor multiplication... taking the derivative... simply replaces the tensor by a Kronecker delta..."
  - [section 3.1] "Differentiating a TN representation is more convenient than using matrix calculus [43] as it amounts to a simple graphical manipulation..."
  - [corpus] Weak evidence - no direct corpus validation of TN differentiation for convolutions, though einsum-based autodiff exists.

### Mechanism 3
- Claim: Tensor networks enable automated performance optimization through contraction tree search and symbolic simplification.
- Mechanism: TNs can be evaluated using established machinery like opt einsum, which automatically finds high-quality contraction orders, and can be simplified using convolution-specific transformations before evaluation.
- Evidence anchors:
  - [section 4.2] "Contraction order optimization: There exist various orders... One extreme approach is to carry out all summations via nested for-loops... The other extreme is sequential pair-wise contraction..."
  - [section 5.1] "To find a contraction schedule, we use opt einsum [63] with default settings."
  - [corpus] Strong evidence - opt einsum and related contraction optimization literature directly supports this claim.

## Foundational Learning

- Concept: Tensor multiplication and einsum notation
  - Why needed here: TNs are built on tensor multiplication, and einsum provides the computational interface. Understanding how indices are summed and how different multiplications (inner, Hadamard, Kronecker) are expressed is crucial.
  - Quick check question: Given two matrices A and B, how would you express their matrix product, Hadamard product, and Kronecker product using einsum notation?

- Concept: Convolution hyper-parameters and index patterns
  - Why needed here: The paper's simplification techniques rely on understanding how convolution parameters (stride, padding, dilation) affect the index pattern tensor Π. Recognizing when convolutions are "dense" or "trivial" is key to applying transformations.
  - Quick check question: For a 2D convolution with kernel size 3, stride 1, padding 0, and dilation 1, what is the output size given an input of size 5x5?

- Concept: Automatic differentiation and vector-Jacobian products
  - Why needed here: The paper derives VJPs for convolutions using TNs. Understanding how backpropagation works and how VJPs are computed is essential for grasping the derivations.
  - Quick check question: In backpropagation through a convolution layer, what is the relationship between the input VJP and the transpose convolution operation?

## Architecture Onboarding

- Component map:
  - Tensor network representation module -> Simplification engine -> Evaluation backend -> Autodiff module

- Critical path:
  1. Convert convolution operation to TN diagram
  2. Apply simplification transformations
  3. Generate einsum contraction string and operands
  4. Find optimal contraction schedule using opt einsum
  5. Perform contraction and return result

- Design tradeoffs:
  - Simplicity vs. performance: The TN representation is simple and general but may not be as optimized as hand-tuned implementations for common operations
  - Generality vs. specialization: Supporting all convolution variants adds complexity but enables the same framework for diverse operations
  - Preprocessing vs. runtime: Simplification transformations add overhead but can significantly speed up evaluation

- Failure signatures:
  - Out-of-memory errors during contraction: May indicate need for index slicing or a different contraction order
  - Unexpectedly slow performance: Could mean the contraction order is suboptimal or the simplification transformations aren't being applied correctly
  - Incorrect results: Likely indicates an error in the TN diagram construction or the differentiation rules

- First 3 experiments:
  1. Implement a simple 2D convolution as a TN and verify it produces the same result as a standard implementation
  2. Derive the weight VJP for a convolution using TNs and compare with PyTorch's autograd
  3. Apply the dense convolution simplification to a ResNet18 model and measure the performance improvement

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several are implied by the work presented.

## Limitations
- The experimental evaluation focuses on a specific KFAC variant, leaving unclear whether performance gains generalize to other architectures or second-order methods
- The memory overhead reduction is demonstrated but not quantified relative to standard implementations
- While the TN framework claims to support arbitrary dimensions and hyper-parameters, the complexity of managing these in practice is not fully explored

## Confidence

### Major Uncertainties
The paper presents a compelling theoretical framework, but several practical concerns limit confidence in the claimed benefits. The experimental evaluation focuses on a specific KFAC variant, leaving unclear whether the performance gains generalize to other architectures or second-order methods. The memory overhead reduction is demonstrated but not quantified relative to standard implementations. Additionally, while the TN framework claims to support arbitrary dimensions and hyper-parameters, the complexity of managing these in practice is not fully explored.

### Confidence Labels
- **High Confidence**: The basic TN representation of convolutions and the simplification transformations based on connectivity patterns (Mechanism 1)
- **Medium Confidence**: The differentiation rules for TNs and their application to autodiff (Mechanism 2)
- **Low Confidence**: The performance claims for KFAC acceleration and the practical benefits of tensor dropout (Mechanism 3)

## Next Checks

1. **Reproduce core TN operations**: Implement a basic 2D convolution as a TN and verify numerical equivalence with standard implementations across various padding, stride, and dilation settings.

2. **Benchmark contraction optimization**: Measure the actual performance improvement from opt einsum-based contraction order optimization on realistic TN sizes, comparing against both naive sequential contraction and hand-optimized implementations.

3. **Test autodiff correctness**: Implement the TN-based VJP derivation for convolutions and validate against PyTorch's autograd on a range of CNN architectures, checking both correctness and performance overhead.