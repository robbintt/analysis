---
ver: rpa2
title: 'SortedNet: A Scalable and Generalized Framework for Training Modular Deep
  Neural Networks'
arxiv_id: '2309.00255'
source_url: https://arxiv.org/abs/2309.00255
tags:
- training
- sortednet
- performance
- sub-networks
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SortedNet is a general framework for training deep neural networks
  that produces many sub-models with varying computational budgets and performance
  levels from a single training run. The key innovation is a nested architecture with
  shared parameters and a sorted order of sub-networks based on computation/accuracy
  characteristics.
---

# SortedNet: A Scalable and Generalized Framework for Training Modular Deep Neural Networks

## Quick Facts
- **arXiv ID**: 2309.00255
- **Source URL**: https://arxiv.org/abs/2309.00255
- **Reference count**: 33
- **Primary result**: SortedNet trains up to 160 sub-models simultaneously while maintaining at least 96% of original model performance across various architectures and tasks.

## Executive Summary
SortedNet introduces a general framework for training deep neural networks that produces multiple sub-models with varying computational budgets from a single training run. The key innovation is a nested architecture with shared parameters and a sorted order of sub-networks based on computation/accuracy characteristics. This enables search-free extraction of optimal sub-models during inference, efficient switching between different computation budgets, and significant reduction in storage requirements. The method uses random sampling of sub-networks combined with gradient accumulation during training to efficiently update all models. Experiments demonstrate that SortedNet can train up to 160 sub-models simultaneously while maintaining at least 96% of the original model's performance across various architectures (BERT, RoBERTa, ResNet, MobileNet) and tasks (NLP, image classification).

## Method Summary
SortedNet constructs a nested architecture where sub-networks are organized as ordered subsets of parameters, enabling search-free extraction during inference. The framework trains all sub-models simultaneously through random sampling and gradient accumulation, where each training iteration randomly selects a sub-network and accumulates gradients over multiple steps before updating parameters. This approach allows efficient training of many sub-models while maintaining their performance relative to the full model. The method generalizes across multiple dimensions (depth, width, attention heads) without requiring architectural modifications, making it applicable to any deep neural network.

## Key Results
- Trains up to 160 sub-models simultaneously while maintaining at least 96% of original model performance
- Outperforms state-of-the-art dynamic training methods (LCS, Slimmable Networks, Universally Slimmable Networks)
- Achieves superior average performance across various architectures including BERT, RoBERTa, ResNet, and MobileNet
- Demonstrates significant reduction in storage requirements through parameter sharing across sub-models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SortedNet achieves search-free sub-network selection by enforcing a nested, monotonic architecture during training.
- **Mechanism**: The framework constructs sub-networks as nested subsets of parameters, where each smaller model is a sub-network of a larger one. This sorted ordering ensures that any target sub-network can be extracted without additional search.
- **Core assumption**: The modularity of deep neural networks allows for nested sub-networks with shared parameters, and sorting them by computation/accuracy yields optimal sub-models.
- **Evidence anchors**:
  - [abstract]: "The key innovation is a nested architecture with shared parameters and a sorted order of sub-networks based on computation/accuracy characteristics."
  - [section]: "This sorted configuration with shared parameters enforces a regular order and consistency in the knowledge learned by sub-networks."
  - [corpus]: Weak evidence; no direct citations found in corpus signals.
- **Break condition**: If the model architecture is not inherently modular (e.g., non-sequential networks), the nested property may not hold, breaking the sorted extraction mechanism.

### Mechanism 2
- **Claim**: The combination of random sub-network sampling and gradient accumulation during training improves efficiency and scalability.
- **Mechanism**: At each training iteration, a random sub-network is sampled, and gradients are accumulated over multiple steps before updating parameters. This allows training of many sub-models simultaneously without significant overhead.
- **Core assumption**: Random sampling ensures that all sub-networks contribute equally to learning, while gradient accumulation reduces the frequency of expensive parameter updates.
- **Evidence anchors**:
  - [abstract]: "The method uses random sampling of sub-networks combined with gradient accumulation during training to efficiently update all models."
  - [section]: "Our training considers a nested architecture for the sub-models with shared parameters and train them together with the main model in a sorted and probabilistic manner."
  - [corpus]: Weak evidence; no direct citations found in corpus signals.
- **Break condition**: If the gradient accumulation steps are too large, it may lead to stale gradients and poor convergence, especially for rapidly changing sub-networks.

### Mechanism 3
- **Claim**: SortedNet generalizes dynamicity across multiple dimensions (e.g., depth, width, attention heads) without requiring architectural modifications.
- **Mechanism**: By defining a universal set of target dimensions and iteratively truncating parameters along these dimensions, the framework can create sub-networks that vary in multiple aspects simultaneously.
- **Core assumption**: The model's architecture can be decomposed along arbitrary dimensions, and the nested property holds across these dimensions.
- **Evidence anchors**:
  - [abstract]: "This enables search-free extraction of optimal sub-models during inference, efficient switching between different computation budgets, and significant reduction in storage requirements."
  - [section]: "Our general dynamic training approach is applicable to any architecture without necessitating any modifications to the original model."
  - [corpus]: Weak evidence; no direct citations found in corpus signals.
- **Break condition**: If the dimensions are not independent (e.g., changing width affects depth in a non-linear way), the nested property may not hold, breaking generalization.

## Foundational Learning

- **Concept**: Nested sub-networks with shared parameters
  - **Why needed here**: Enables efficient storage and switching between sub-models by reusing parameters across different configurations.
  - **Quick check question**: Can you explain why sharing parameters across sub-networks reduces storage requirements?

- **Concept**: Random sampling of sub-networks during training
  - **Why needed here**: Ensures that all sub-networks contribute equally to learning, preventing bias toward larger or smaller models.
  - **Quick check question**: How does random sampling help in achieving balanced performance across sub-networks?

- **Concept**: Gradient accumulation
  - **Why needed here**: Reduces the frequency of parameter updates, improving training efficiency when dealing with many sub-models.
  - **Quick check question**: What is the trade-off between gradient accumulation steps and convergence speed?

## Architecture Onboarding

- **Component map**: Main model -> Nested sub-networks -> Random sampling mechanism -> Gradient accumulation -> Parameter updates

- **Critical path**:
  1. Define target dimensions (e.g., depth, width)
  2. Construct nested sub-networks with shared parameters
  3. Sample sub-networks randomly during training
  4. Accumulate gradients and update parameters
  5. Extract optimal sub-network at inference based on sorted order

- **Design tradeoffs**:
  - Random sampling vs. deterministic selection: Random sampling ensures balanced learning but may lead to suboptimal paths; deterministic selection could be more efficient but risks bias.
  - Gradient accumulation steps: More steps reduce overhead but may slow convergence; fewer steps increase overhead but improve responsiveness.

- **Failure signatures**:
  - Poor performance of sub-networks: Indicates imbalanced training or insufficient gradient accumulation
  - High memory usage: Suggests improper sharing of parameters or inefficient sub-network construction
  - Slow convergence: May result from overly large gradient accumulation steps or ineffective sampling

- **First 3 experiments**:
  1. Train a simple CNN with depth and width dimensions; verify that sub-networks can be extracted without search
  2. Increase the number of gradient accumulation steps; measure the impact on training efficiency and sub-network performance
  3. Test generalization by applying SortedNet to a transformer model; evaluate performance across multiple dimensions (e.g., layers, attention heads)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the choice of gradient accumulation strategy impact the scalability of SortedNet when training hundreds of sub-models simultaneously?
- **Basis in paper**: [explicit] The paper discusses gradient accumulation (gacc) as a key factor in balancing training efficiency and performance, but leaves open questions about optimal strategies for large-scale deployment.
- **Why unresolved**: The paper experiments with gacc values but does not explore adaptive or task-specific strategies for gradient accumulation across different model architectures or dataset complexities.
- **What evidence would resolve it**: Comparative experiments testing adaptive gradient accumulation strategies versus fixed gacc values across diverse architectures (e.g., transformers, CNNs) and tasks, measuring both convergence speed and final performance.

### Open Question 2
- **Question**: Can SortedNet's nested architecture be extended to non-modular or irregularly structured neural networks, such as those with skip connections or residual blocks?
- **Basis in paper**: [inferred] The paper focuses on modular architectures (e.g., BERT, ResNet, MobileNet) and demonstrates success with nested, sorted sub-networks, but does not address irregularly structured models.
- **Why unresolved**: The paper does not test SortedNet on architectures with complex dependencies or irregular structures, leaving uncertainty about its generalizability.
- **What evidence would resolve it**: Experiments applying SortedNet to irregularly structured models (e.g., ResNet with dense connections or U-Net) and evaluating the performance and feasibility of maintaining nested properties.

### Open Question 3
- **Question**: How does the randomness in sub-network sampling during training affect the robustness and reproducibility of SortedNet's performance across different runs?
- **Basis in paper**: [explicit] The paper mentions that SortedNet uses random sampling of sub-networks and gradient accumulation, but notes that results might be sensitive to randomness and suggests further research on optimal sampling strategies.
- **Why unresolved**: The paper does not provide systematic analysis of the impact of randomness on performance variability or propose deterministic alternatives.
- **What evidence would resolve it**: Statistical analysis of performance variance across multiple training runs with different random seeds, and comparison with deterministic sampling strategies to assess robustness and reproducibility.

## Limitations
- The framework's effectiveness heavily depends on the model's modularity and the assumption that nested sub-networks can maintain the sorted order without degradation.
- Random sampling may not guarantee optimal coverage of the sub-model space, especially for complex architectures with many dimensions.
- Gradient accumulation may introduce staleness in updates for rapidly changing sub-networks, potentially affecting convergence.

## Confidence
- **High confidence**: The nested architecture with shared parameters and the sorted extraction mechanism are well-justified and supported by the experimental results.
- **Medium confidence**: The scalability to 160 sub-models and the generalization across different architectures and tasks are promising but may be sensitive to implementation details.
- **Low confidence**: The claim that the method outperforms all state-of-the-art dynamic training methods is based on limited comparisons and may not hold across all scenarios.

## Next Checks
1. Evaluate the impact of different sampling distributions (PBj) on the performance of sub-models. Test whether deterministic sampling strategies can achieve similar or better results compared to random sampling.
2. Investigate the trade-off between gradient accumulation steps and convergence speed. Measure the effect of varying the number of accumulation steps on training efficiency and sub-model performance.
3. Assess the framework's generalization to more complex architectures, such as vision transformers or large language models with hundreds of layers. Test whether the nested property holds and the performance threshold of 96% is maintained.