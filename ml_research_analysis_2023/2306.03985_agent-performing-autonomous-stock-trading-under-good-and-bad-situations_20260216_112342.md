---
ver: rpa2
title: Agent Performing Autonomous Stock Trading under Good and Bad Situations
arxiv_id: '2306.03985'
source_url: https://arxiv.org/abs/2306.03985
tags:
- deep
- stock
- learning
- trading
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes autonomous stock trading using deep reinforcement\
  \ learning (deep Q-learning, deep SARSA, and policy gradient) to navigate unstable\
  \ market conditions. The method models trading as a Markov Decision Process with\
  \ a 7-dimensional state (price, balance, shares, MACD, RSI, CCI, ADX) and actions\
  \ {-k,\u20260,\u2026k} for buy/hold/sell."
---

# Agent Performing Autonomous Stock Trading under Good and Bad Situations

## Quick Facts
- arXiv ID: 2306.03985
- Source URL: https://arxiv.org/abs/2306.03985
- Reference count: 11
- Primary result: RL agents achieved 70%-90% annual returns pre-2021, dropping to 2%-7% post-2021

## Executive Summary
This paper proposes autonomous stock trading using deep reinforcement learning (deep Q-learning, deep SARSA, and policy gradient) to navigate unstable market conditions. The method models trading as a Markov Decision Process with a 7-dimensional state (price, balance, shares, MACD, RSI, CCI, ADX) and actions {-k,…0,…k} for buy/hold/sell. Three neural network-based agents are trained on six tech stocks (Google, Apple, Tesla, Meta, Microsoft, IBM) over two periods: pre-2021 (stable market) and post-2021 (volatile pandemic-era market). Results show high pre-2021 returns (70%-90% annual rate) for all methods, with post-2021 returns dropping to 2%-7% but remaining positive. Deep SARSA performed most consistently across periods. The study demonstrates RL's potential for automated trading under varying market conditions, though performance degrades with distribution shift.

## Method Summary
The method frames stock trading as a Markov Decision Process where agents learn optimal trading policies through deep reinforcement learning. Three algorithms are implemented: deep Q-learning, deep SARSA, and policy gradient. The state space consists of 7 dimensions including price, balance, shares, and technical indicators (MACD, RSI, CCI, ADX). Agents can take actions ranging from -k to k, representing different buy/sell/hold decisions. Training occurs over 30 episodes with 10 epochs per episode using epsilon-greedy exploration. The reward function is based on changes in total portfolio value minus transaction fees. Agents are trained individually on six tech stocks across two market periods (pre-2021 and post-2021) with 20 repetitions per configuration.

## Key Results
- Deep Q-learning achieved 70%-90% annual returns in pre-2021 stable market conditions
- Post-2021 volatile market returns dropped to 2%-7% but remained positive for all methods
- Deep SARSA demonstrated the most consistent performance across both market periods
- All three RL methods outperformed the baseline buy-and-hold strategy in most cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep Q-Learning approximates the Q-function for stock trading decisions under continuous state space
- Mechanism: The 7-dimensional state is fed into a neural network to predict Q-values for each possible action, allowing the agent to select the action with the highest Q-value
- Core assumption: The neural network can effectively generalize Q-values across similar states
- Evidence anchors:
  - [abstract]: "We have developed a pipeline that simulates the stock trading environment and have trained an agent to automate the stock trading process with deep reinforcement learning methods, including deep Q-learning..."
  - [section]: "Because we have the state declared as st ∈ R7 where |S| = ∞, we cannot construct a table of q(s, a) that contain all the combinations..."
- Break condition: If the state space becomes too high-dimensional or if the Q-function is too complex for the neural network to approximate accurately

### Mechanism 2
- Claim: The ϵ-greedy policy balances exploration and exploitation during training
- Mechanism: The agent selects the action with the highest Q-value with probability (1 - ϵ) and a random action with probability ϵ/|A|
- Core assumption: Sufficient exploration during training allows the agent to discover high-reward actions
- Evidence anchors:
  - [abstract]: "...where negative and positive values represent sales and purchases, respectively, and 0 means holding."
  - [section]: "First, the policy for generating the trajectory in each episode follows the ϵ-greedy policies..."
- Break condition: If ϵ decays too quickly, the agent may not explore enough to find optimal actions

### Mechanism 3
- Claim: The reward function encourages profit maximization by considering changes in total asset value
- Mechanism: The reward is defined as the change in total asset value (balance + stock value) minus trading fees
- Core assumption: Maximizing the reward function will lead to profitable trading strategies
- Evidence anchors:
  - [abstract]: "The reward we set is the changing in the total asset value of the portfolio."
  - [section]: "The reward we set is the changing in the total asset value of the portfolio. More specifically, the reward R given the states at time t and t + 1 with action at is defined as R(st, at, st+1) = (bt+1 + pt+1 · ht+1) − (bt + pt · ht) − ct"
- Break condition: If the reward function does not accurately reflect the agent's trading performance

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: Stock trading is modeled as an MDP, where the agent's actions affect the state and future rewards
  - Quick check question: What are the four components of an MDP, and how do they apply to the stock trading scenario?

- Concept: Deep Reinforcement Learning
  - Why needed here: Traditional reinforcement learning methods struggle with the continuous state space of stock trading
  - Quick check question: How does the neural network in deep Q-learning generalize Q-values across similar states?

- Concept: Market Technical Indicators (MACD, RSI, CCI, ADX)
  - Why needed here: These indicators are used as features in the state to provide the agent with information about market trends and momentum
  - Quick check question: What does each technical indicator measure, and how might it influence the agent's trading decisions?

## Architecture Onboarding

- Component map: Data Pipeline -> Environment -> Agent -> Training Loop -> Evaluation
- Critical path:
  1. Load and preprocess stock data
  2. Initialize the environment and agent
  3. Train the agent by interacting with the environment and updating the neural network weights
  4. Evaluate the trained agent's performance on test data
- Design tradeoffs:
  - State dimensionality vs. neural network complexity
  - Exploration vs. exploitation balance
  - Reward function design to encourage profitable trading without excessive risk
- Failure signatures:
  - Overfitting: Poor test performance despite good training results
  - Underfitting: Consistently poor performance on both training and test data
  - Poor exploration: Consistently suboptimal decisions indicating insufficient exploration
- First 3 experiments:
  1. Train the agent on a single stock (e.g., Apple) using Deep Q-Learning and evaluate its performance on a short time period (e.g., one month)
  2. Compare the performance of Deep Q-Learning, Deep SARSA, and Policy Gradient on the same stock and time period
  3. Evaluate the agent's performance on a more volatile stock (e.g., Tesla) and a longer time period (e.g., one year)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would introducing financial crisis data (e.g., 2008) into the training set affect the agent's performance during bad market conditions?
- Basis in paper: [explicit] "Introducing data like those in the financial crisis in 2008 to the training set could mitigate this degradation" and "We leave this further validation for potential improvements in returns during the bad situations in future work."
- Why unresolved: The paper acknowledges this as a potential improvement but explicitly states it was not tested
- What evidence would resolve it: Empirical results comparing agent performance on post-2021 data when trained with and without 2008 crisis data included

### Open Question 2
- Question: How would a federated learning approach compare to individual agents for each stock in terms of overall portfolio performance?
- Basis in paper: [explicit] "Future work would involve experimenting with a multitask or federated learning approach instead of training individual agents for all stocks."
- Why unresolved: This is proposed as future work but not implemented or tested
- What evidence would resolve it: Comparative results showing portfolio returns, risk metrics, and computational efficiency between federated vs. individual agent approaches

### Open Question 3
- Question: What is the impact of transaction fee variations on agent performance across different market conditions?
- Basis in paper: [inferred] The paper uses a fixed 0.1% transaction fee but doesn't explore how different fee structures affect performance
- Why unresolved: Transaction costs are assumed constant without sensitivity analysis
- What evidence would resolve it: Performance metrics (profit, annual rate) across a range of transaction fee percentages under both good and bad market conditions

## Limitations
- Performance degrades significantly (70-90% to 2-7% returns) when moving from stable to volatile market conditions
- Neural network architectures and exact hyperparameters remain unspecified, limiting reproducibility
- Evaluation limited to six tech stocks, which may not generalize to broader market conditions
- Use of technical indicators assumes their continued predictive power, which may not hold during regime changes

## Confidence
- High confidence: The MDP formulation and RL framework are well-established
- Medium confidence: The reported performance metrics, given the lack of complete methodological details
- Medium confidence: The comparison between pre-2021 and post-2021 market conditions

## Next Checks
1. Replicate the study with detailed neural network specifications and hyperparameter settings to verify the reported performance metrics
2. Test the agents on additional stocks outside the tech sector and different market conditions to assess generalization capabilities
3. Implement ablation studies removing individual technical indicators from the state to determine their individual contributions to performance