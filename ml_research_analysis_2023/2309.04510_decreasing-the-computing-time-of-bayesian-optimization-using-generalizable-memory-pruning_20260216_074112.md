---
ver: rpa2
title: Decreasing the Computing Time of Bayesian Optimization using Generalizable
  Memory Pruning
arxiv_id: '2309.04510'
source_url: https://arxiv.org/abs/2309.04510
tags:
- surrogate
- computing
- data
- optimization
- times
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of high computational costs in
  Bayesian optimization (BO) for high-dimensional or large datasets. The main bottleneck
  is the polynomial time complexity of Gaussian process surrogate models with respect
  to the number of experiments.
---

# Decreasing the Computing Time of Bayesian Optimization using Generalizable Memory Pruning

## Quick Facts
- arXiv ID: 2309.04510
- Source URL: https://arxiv.org/abs/2309.04510
- Authors: [Not specified in source]
- Reference count: 35
- Key outcome: Memory pruning approach reduces Bayesian optimization computing times by up to 100x while maintaining performance

## Executive Summary
This paper addresses the computational bottleneck in Bayesian optimization caused by polynomial time complexity of Gaussian process surrogate models. The authors propose Zooming Memory-Based Initialization (ZoMBI), a generalizable memory pruning approach that selectively prunes data outside bounded search regions and iteratively zooms in on promising areas. This method significantly reduces wall-clock computing times per experiment, transforming the time complexity from a polynomially increasing pattern to a sawtooth pattern with a non-increasing trend, while maintaining optimization performance across different surrogate models and acquisition functions.

## Method Summary
The ZoMBI approach implements selective memory pruning by maintaining only the m best-performing experiments and pruning the rest, while bounding the search space to a region defined by these top points. The method iteratively updates these bounds and prunes data every N experiments, reducing the number of data points used to fit the surrogate model. This approach is tested with both Gaussian processes and neural networks as surrogate models, and with four different acquisition functions (expected improvement, lower confidence bound, EI Abrupt, and LCB Adaptive) on two datasets: a 6-dimensional Ackley function for in silico testing and a 5-dimensional real-world materials dataset.

## Key Results
- Computing times reduced by up to 100x compared to standard Bayesian optimization
- Time complexity transformed from polynomially increasing to sawtooth pattern with non-increasing trend
- Optimization performance maintained across different surrogate models and acquisition functions
- Demonstrated compatibility with both Gaussian processes and neural networks as surrogate models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective memory pruning and search space bounding reduce the number of data points used to fit the surrogate model, directly lowering O(N³) complexity of Gaussian processes
- Mechanism: ZoMBI selects the m best-performing points and prunes the rest, bounding the mesh grid computation to a shrinking region
- Core assumption: Best-performing points are representative of the global optimum and the bounding region contains the optimum
- Evidence anchors:
  - "Using this memory pruning approach, we show a decrease in wall-clock computing times per experiment of BO from a polynomially increasing pattern to a sawtooth pattern that has a non-increasing trend"
  - "ZoMBI selects the top performing acquired experiments every N = 20 experiments and prunes the rest from the memory to bound the surrogate mesh grid computation"
- Break condition: If the optimum lies outside the bounded region or if m is too small to capture sufficient diversity

### Mechanism 2
- Claim: Pre-trained neural networks as surrogate models provide fast inference once trained, avoiding O(N³) scaling entirely
- Mechanism: The NN is trained offline on a large dataset, so at optimization time it only performs forward passes without expensive kernel computations
- Core assumption: The NN can generalize well to the optimization space and is sufficiently accurate
- Evidence anchors:
  - "we illustrate the generalizability of the approach across two unique data sets, two unique surrogate models, and four unique acquisition functions"
  - "A pre-trained NN is used as the surrogate model instead of the GP to demonstrate the generalizability of the memory pruning method to various surrogate models"
- Break condition: If the NN underfits the response surface or overfits the training data

### Mechanism 3
- Claim: Bounded optimization confines the acquisition function search to a reduced domain, which speeds up mesh grid evaluation
- Mechanism: The acquisition function only evaluates a grid over the bounded region, not the full space, reducing the number of points to evaluate
- Core assumption: The bounded region is still large enough to find the optimum and the acquisition function can effectively explore within it
- Evidence anchors:
  - "capable of being used with any surrogate model and acquisition function"
  - "we implement methods of decreasing the computing time of BO that do not constrain the user to select a certain surrogate model or AF"
- Break condition: If the bounded region becomes too small or if the acquisition function fails to explore adequately

## Foundational Learning

- Concept: Gaussian Process regression and its O(N³) scaling with the number of data points
  - Why needed here: Understanding why standard BO is computationally expensive is key to appreciating the pruning method
  - Quick check question: What is the computational complexity of fitting a GP to N data points?

- Concept: Acquisition functions (EI, LCB, etc.) and how they balance exploration and exploitation
  - Why needed here: ZoMBI works with any acquisition function; knowing their behavior helps predict performance
  - Quick check question: How does the LCB acquisition function differ from EI in terms of exploration-exploitation trade-off?

- Concept: Bounding and pruning in optimization: why and when it is safe to remove data points
  - Why needed here: ZoMBI's core innovation relies on safe pruning without losing convergence
  - Quick check question: What is the risk of pruning data points too aggressively in BO?

## Architecture Onboarding

- Component map: Surrogate model (GP or NN) -> Acquisition function (EI, LCB, EI Abrupt, LCB Adaptive) -> ZoMBI wrapper -> High-performance compute backend
- Critical path:
  1. Fit surrogate to current memory (bounded set)
  2. Evaluate acquisition over bounded mesh grid
  3. Select next experiment point
  4. Evaluate true function
  5. Update memory (add point, possibly prune old points)
  6. Repeat until convergence
- Design tradeoffs:
  - Larger m (memory kept) → better global search but higher compute
  - Smaller m → faster compute but risk of missing optimum
  - Choice of surrogate: GP is flexible but slow; NN is fast but needs good training
  - Acquisition function: greedy methods converge faster but may get stuck; explorative methods are safer but slower
- Failure signatures:
  - Compute time stops decreasing (sawtooth pattern flattens)
  - Optimization performance degrades (loss increases)
  - Memory pruning removes points too aggressively, causing instability
  - Acquisition function fails to explore new regions
- First 3 experiments:
  1. Run ZoMBI with GP surrogate on Ackley function, m=10, monitor compute time per experiment and convergence
  2. Run ZoMBI with NN surrogate on the materials dataset, compare to GP in terms of compute time and accuracy
  3. Vary m (memory size) and observe the tradeoff between compute time and optimization performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ZoMBI memory pruning approach scale with different surrogate model types beyond Gaussian processes and neural networks?
- Basis in paper: The authors demonstrate the method works with GP and NN surrogates but state the approach is generalizable to "any surrogate model"
- Why unresolved: The paper only empirically tests two surrogate model types across two datasets, leaving uncertainty about performance with other model families
- What evidence would resolve it: Systematic benchmarking of ZoMBI with diverse surrogate model types (at least 5-10 different families) across multiple problem domains showing consistent time complexity reduction

### Open Question 2
- Question: What is the optimal frequency of memory pruning (N value) for different problem characteristics?
- Basis in paper: The authors use N=20 for memory pruning but acknowledge this is a parameter that could be tuned
- Why unresolved: The paper uses a fixed N=20 across all experiments without exploring how different problem dimensions, response surface complexity, or acquisition function types affect optimal pruning frequency
- What evidence would resolve it: Parameter sensitivity analysis showing how optimization performance and computing time trade-offs vary with different N values across problem types

### Open Question 3
- Question: How does ZoMBI perform in multi-objective optimization scenarios where the response function has multiple conflicting objectives?
- Basis in paper: The paper focuses on single-objective minimization problems, but mentions generalizability to "any surrogate model and acquisition function"
- Why unresolved: Multi-objective optimization requires different acquisition functions (e.g., expected hypervolume improvement) and may have different memory pruning dynamics when optimizing for Pareto fronts
- What evidence would resolve it: Demonstration of ZoMBI applied to multi-objective problems with appropriate acquisition functions, showing time complexity reduction while maintaining Pareto front quality

## Limitations

- Evaluation limited to only two test functions (Ackley and a materials dataset), constraining confidence in performance across diverse problem landscapes
- Claims of compatibility with "any surrogate model and acquisition function" are under-validated, having only tested two surrogate types and four acquisition functions
- Lack of theoretical guarantees for convergence - while pruning doesn't sacrifice performance empirically, there's no analysis of when or why the approach might fail

## Confidence

- **High**: Computational time reduction is real and measurable; the sawtooth pattern in wall-clock times is clearly demonstrated
- **Medium**: Compatibility claims with various surrogates and acquisition functions are plausible but under-validated
- **Low**: Generalization to arbitrary high-dimensional problems and theoretical convergence guarantees

## Next Checks

1. Apply ZoMBI to a known multimodal optimization problem (e.g., Rastrigin function) to verify it doesn't get trapped in local optima due to aggressive pruning of exploratory points

2. Test ZoMBI with surrogate models beyond GPs and NNs (e.g., random forests, polynomial chaos expansions) to confirm the claimed "any surrogate" compatibility

3. Systematically vary the pruning frequency and memory size parameters to identify failure modes where too aggressive pruning degrades optimization performance, establishing operational boundaries for the method