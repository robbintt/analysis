---
ver: rpa2
title: 'Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph
  in Pre-Trained Transformers'
arxiv_id: '2305.17328'
source_url: https://arxiv.org/abs/2305.17328
tags:
- tokens
- pruning
- importance
- zero-tprune
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Zero-TPrune, a zero-shot token pruning method
  that efficiently leverages the feature identification capability of pre-trained
  Transformers to eliminate the need for fine-tuning after pruning. It exploits both
  the importance and similarity of tokens to perform pruning.
---

# Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers

## Quick Facts
- **arXiv ID**: 2305.17328
- **Source URL**: https://arxiv.org/abs/2305.17328
- **Reference count**: 40
- **Primary result**: Reduces computational cost of pre-trained Transformers by >20% with <0.4% accuracy loss

## Executive Summary
Zero-TPrune is a zero-shot token pruning method that exploits the attention graph structure of pre-trained Transformers to identify and remove unimportant tokens without fine-tuning. It uses a Weighted Page Rank algorithm to compute token importance scores and removes redundant similar tokens to further improve pruning efficiency. The method achieves significant FLOPs reduction (over 20%) while maintaining accuracy within 0.4% of the original model on ImageNet, outperforming existing fine-tuning-free pruning methods by up to 45% in accuracy retention.

## Method Summary
Zero-TPrune operates in three stages: (1) I'-stage applies a single iteration of Weighted Page Rank to rank tokens by importance, (2) S-stage partitions tokens into two groups based on importance and prunes similar tokens using cosine similarity on key matrix vectors, and (3) I-stage applies multi-iteration Weighted Page Rank to refine importance scores. The method also incorporates EIP (Emphasizing Informative Pixels) to weight informative tokens and VHF (Variance-based Head Filter) to filter out uninformative attention heads. This approach enables effective pruning without requiring any model fine-tuning or access to training data.

## Key Results
- Reduces computational cost of pre-trained Transformers by more than 20% with less than 0.4% accuracy loss on ImageNet
- Outperforms state-of-the-art fine-tuning-free pruning methods by up to 45% in accuracy retention with similar FLOPs budgets
- Achieves 39.8% FLOPs reduction on DeiT-S with only 0.36% accuracy drop
- Works effectively across multiple vision Transformer architectures including DeiT, LV-ViT, and MAE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-TPrune leverages the attention graph structure inherent in pre-trained Transformers to compute importance scores without fine-tuning.
- Mechanism: The attention matrix A(h,l) is treated as an adjacency matrix of a directed graph. A Weighted Page Rank (WPR) algorithm iteratively applies this matrix as a graph shift operator to propagate importance scores across tokens. Each token's importance is updated based on the attention it receives from other tokens, weighted by their current importance.
- Core assumption: The attention weights in a pre-trained Transformer encode meaningful importance relationships between tokens that can be extracted without further training.
- Evidence anchors:
  - [abstract] "It leverages the attention graph of pre-trained Transformer models to produce an importance distribution for tokens via our proposed Weighted Page Rank (WPR) algorithm."
  - [section 3.2] "Inspired by the Page Rank [20] algorithm, we propose a WPR algorithm to derive the importance scores. Page Rank is, of course, famously used in Google Search for ranking web pages."
  - [corpus] No direct corpus evidence for WPR on attention graphs, but PageRank is well-established for web ranking.
- Break condition: If the attention matrix in pre-trained models is random or uninformative about token importance, the WPR algorithm would fail to identify meaningful importance scores.

### Mechanism 2
- Claim: The S-stage improves pruning precision by removing similar tokens after important ones are identified, preventing redundancy.
- Mechanism: After the I-stage identifies top-k important tokens, the S-stage partitions tokens into two groups based on importance rank. It then computes similarity (using cosine similarity on key matrix vectors) between tokens in different groups and prunes the less important token in each highly similar pair. This removes redundant information while preserving diverse important features.
- Core assumption: Important tokens in an image often have similar representations, and keeping only one representative from each similar group preserves most information while reducing computation.
- Evidence anchors:
  - [section 3.3] "To measure similarity between tokens and perform pruning based on this similarity, we introduce the S-stage and discuss potential design choices in Section 3.3."
  - [section 3.3] "To fix this issue, we propose an importance-guided partitioning, matching, and pruning process, as shown in Fig. 4."
  - [corpus] Weak evidence - the corpus doesn't discuss similarity-based pruning in Transformers.
- Break condition: If important tokens are not similar to each other or if similarity doesn't correlate with redundancy, the S-stage would remove useful information.

### Mechanism 3
- Claim: Zero-TPrune avoids the overwhelming of major groups problem by interleaving importance and similarity pruning stages.
- Mechanism: In a naive I-S pattern (importance then similarity), unimportant background tokens could overwhelm important object tokens through iterative voting in WPR. By using an I'-S-I pattern (pre-ranking, then similarity pruning, then importance scoring), similar background tokens are removed early, preventing them from dominating the importance distribution.
- Core assumption: Similarity pruning before final importance scoring prevents noise tokens from artificially inflating their importance through iterative voting.
- Evidence anchors:
  - [section 3.1] "We resolve this issue by interchanging the I-stage and S-stage. This method enables the early elimination of similar tokens in the S-stage, consequently reducing the adverse impact of similarity in the I-stage to a significant extent."
  - [supplementary material A.1] "This is a positive feedback loop, with the result that the most 'important' tokens end up in set A."
  - [corpus] No direct corpus evidence for this specific interleaving strategy.
- Break condition: If the pre-ranking stage (I') doesn't effectively separate important from unimportant tokens, the S-stage may not remove the right tokens, and the problem persists.

## Foundational Learning

- Concept: Graph signal processing and PageRank algorithm
  - Why needed here: Understanding how iterative application of a graph shift operator (attention matrix) can propagate importance scores across a graph structure.
  - Quick check question: If you have a 3x3 attention matrix [[0.1,0.2,0.7],[0.6,0.3,0.1],[0.2,0.5,0.3]], what would be the importance score of node 1 after one iteration of PageRank starting from uniform initialization?

- Concept: Attention mechanisms in Transformers
  - Why needed here: Understanding how attention matrices are computed and what they represent in terms of token relationships.
  - Quick check question: In a multi-head attention setup with 4 heads, how is the final attention matrix computed from individual head attention matrices?

- Concept: Token similarity and embedding spaces
  - Why needed here: Understanding how to measure similarity between tokens using their embedding vectors (key, query, or value representations).
  - Quick check question: Given two token embeddings [0.5, 0.2, -0.1] and [0.4, 0.3, 0.0], what is their cosine similarity?

## Architecture Onboarding

- Component map: I'-stage -> S-stage -> I-stage -> pruned output
- Critical path: I'-stage provides initial ranking for S-stage partitioning, S-stage removes redundant similar tokens, and I-stage refines the final importance scores on the remaining tokens.
- Design tradeoffs:
  - Number of WPR iterations: More iterations give better convergence but increase computational cost. The paper uses 30-50 for first layers, 5-10 for middle layers, and 1 for last layers.
  - Partitioning strategy: Sequential (prune unimportant part) vs. alternate vs. random. Sequential-U (prune unimportant) performs best.
  - Feature vector source: Key, Query, Value, or intermediate vectors. Key matrix performs best for similarity computation.
- Failure signatures:
  - If pruned tokens include important content: Check if VHF thresholds are excluding informative heads or if EIP is not emphasizing informative pixels correctly.
  - If pruning doesn't reduce computation as expected: Verify that the pruned token mask is being applied correctly to the attention and FFN computations.
  - If accuracy drops significantly: Check if the number of WPR iterations is insufficient for convergence in early layers.
- First 3 experiments:
  1. Run WPR with different iteration counts (1, 5, 30) on a simple image and visualize the importance score distributions to verify convergence behavior.
  2. Test different partitioning strategies (alternate, sequential-U, sequential-I, random) with a fixed similarity metric to identify which preserves most accuracy.
  3. Compare different feature vector sources (Key, Query, Value, Xpre) for similarity computation while keeping other parameters fixed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Zero-TPrune perform on non-vision tasks such as natural language processing or speech recognition?
- Basis in paper: [inferred] The paper focuses on vision tasks and evaluates Zero-TPrune on ImageNet, but mentions that it is potentially applicable to any Transformer-based tasks.
- Why unresolved: The paper does not provide experimental results or analysis for non-vision tasks.
- What evidence would resolve it: Experimental results showing the performance of Zero-TPrune on non-vision tasks, such as NLP or speech recognition, would provide evidence for its applicability and effectiveness in these domains.

### Open Question 2
- Question: How does the performance of Zero-TPrune change when applied to extremely large models with trillions of parameters, such as GPT-3?
- Basis in paper: [explicit] The paper mentions that Zero-TPrune can be applied to large models easily due to its low cost, but does not provide experimental results for extremely large models.
- Why unresolved: The paper does not provide experimental results or analysis for extremely large models.
- What evidence would resolve it: Experimental results showing the performance of Zero-TPrune on extremely large models, such as GPT-3, would provide evidence for its effectiveness and scalability in these cases.

### Open Question 3
- Question: How does the performance of Zero-TPrune change when applied to models with different architectural variations, such as Swin Transformers or Deformable Transformers?
- Basis in paper: [inferred] The paper evaluates Zero-TPrune on various vision Transformer backbones, including DeiT, LV-ViT, and AugReg, but does not mention other architectural variations.
- Why unresolved: The paper does not provide experimental results or analysis for models with different architectural variations.
- What evidence would resolve it: Experimental results showing the performance of Zero-TPrune on models with different architectural variations, such as Swin Transformers or Deformable Transformers, would provide evidence for its effectiveness and adaptability to different architectures.

## Limitations

- Limited empirical validation of the WPR algorithm: The paper does not provide extensive ablation studies on convergence behavior or sensitivity to initialization parameters.
- Similarity-based pruning assumptions: The effectiveness relies on the assumption that important tokens are often similar, which may not hold universally across different types of inputs or tasks.
- Lack of comparison with other importance metrics: The paper does not compare WPR-based importance scores with other potential metrics to validate that WPR captures meaningful importance.

## Confidence

**High Confidence**:
- The overall methodology of using attention graphs for token importance ranking is sound and grounded in well-established graph signal processing techniques.
- The reported FLOPs reduction and accuracy retention on ImageNet for tested models are reliable based on the experimental setup described.

**Medium Confidence**:
- The effectiveness of the I'-S-I stage sequence in preventing the overwhelming of major groups problem is supported by the paper's analysis but would benefit from more extensive empirical validation.
- The choice of using the key matrix for similarity computation is stated to perform best, but the paper does not provide exhaustive comparisons with other feature vector sources.

**Low Confidence**:
- The generalizability of Zero-TPrune to other architectures beyond Vision Transformers (e.g., large language models) is not explored.
- The robustness of the method to different types of noise or adversarial inputs is not discussed.

## Next Checks

1. **Convergence analysis of WPR**: Run the Weighted Page Rank algorithm with varying numbers of iterations (1, 5, 10, 30, 50) on a simple image and visualize the importance score distributions to verify convergence behavior. Measure the KL divergence between importance distributions across iterations to determine the optimal number of iterations for different layers.

2. **Ablation study on partitioning strategies**: Test different token partitioning strategies (sequential-U, sequential-I, alternate, random) with a fixed similarity metric and WPR iteration count. Measure the accuracy retention and FLOPs reduction for each strategy to identify which partitioning method is most effective.

3. **Comparison of feature vector sources**: Compare the performance of Zero-TPrune when using different feature vector sources (Key, Query, Value, Xpre) for similarity computation. Keep other parameters fixed and measure accuracy and FLOPs reduction to determine which feature source yields the best trade-off.