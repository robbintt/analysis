---
ver: rpa2
title: 'SE-PEF: a Resource for Personalized Expert Finding'
arxiv_id: '2309.11686'
source_url: https://arxiv.org/abs/2309.11686
tags:
- expert
- se-pef
- answers
- personalized
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SE-PEF, a large-scale dataset for personalized
  expert finding in community question answering (cQA) platforms. The dataset contains
  250k questions, 565k answers from 3,306 experts, and is annotated with features
  modeling social interactions.
---

# SE-PEF: a Resource for Personalized Expert Finding

## Quick Facts
- arXiv ID: 2309.11686
- Source URL: https://arxiv.org/abs/2309.11686
- Reference count: 24
- Primary result: Personalized expert finding approach achieves P@1: 0.163, R@3: 0.304, R@5: 0.375, MRR@5: 0.240

## Executive Summary
This paper introduces SE-PEF, a large-scale dataset for personalized expert finding in community question answering platforms. The dataset contains 250k questions, 565k answers from 3,306 experts, and is annotated with features modeling social interactions. A two-stage retrieval approach is employed, combining BM25, DistilBERT/MiniLM, and a TAG-based personalization model. The TAG model captures similarities between topics addressed by the asker and those previously answered by experts. Experiments show that the personalized approach significantly outperforms non-personalized methods, with DistilBERT + TAG achieving the best performance.

## Method Summary
The method employs a two-stage retrieval architecture: first, BM25 retrieves candidate answers from a 560k answer collection, then DistilBERT or MiniLM re-ranks the top-100 results using semantic similarity. A TAG-based personalization model computes topic affinity scores between the asker's tags and each expert's answer history, with low-frequency tags pruned using median-frequency filtering. The final ranking combines BM25, neural re-ranker, and TAG scores with optimized weights.

## Key Results
- DistilBERT + TAG personalization model achieves best performance (P@1: 0.163, R@3: 0.304, R@5: 0.375, MRR@5: 0.240)
- Personalized approach significantly outperforms non-personalized methods
- Cross-community dataset enables investigation of generalist cQA expert finding models

## Why This Works (Mechanism)

### Mechanism 1
- Combining BM25 retrieval with neural re-ranker plus TAG personalization improves expert finding effectiveness
- BM25 provides broad candidate retrieval, neural re-ranker refines using semantic similarity, TAG incorporates user-expert topic alignment
- Components capture complementary relevance aspects: lexical matching, semantic similarity, and user-expert topic affinity
- Break condition: Poor TAG alignment with actual expertise degrades personalization

### Mechanism 2
- Using StackExchange data from multiple communities improves dataset diversity and generalizability
- Cross-community data exposes models to varied domains and expert behaviors
- Expertise signals are transferable across domains, diversity improves model robustness
- Break condition: Excessive heterogeneity introduces noise that outweighs benefits

### Mechanism 3
- Pruning low-frequency tags from expert representations reduces noise and improves personalization relevance
- Discarding tags below median frequency focuses on expert's core topics
- Low-frequency tags are more likely to be incidental or irrelevant to true expertise
- Break condition: Median-frequency pruning removes genuinely relevant niche expertise

## Foundational Learning

- **Two-stage retrieval architecture**: Why needed - efficient retrieval over 560k answers requires fast first-stage filtering before expensive neural re-ranking. Quick check - why not use only neural re-ranking on full collection?
- **Personalized scoring via topic affinity**: Why needed - expert finding benefits from matching user interests to expert topic histories, not just generic relevance. Quick check - how does TAG differ from simple user-expert co-occurrence metric?
- **Acceptance rate as expert quality signal**: Why needed - high acceptance rates indicate trusted expertise, used to filter initial expert candidates. Quick check - what threshold parameters control strictness of expert selection?

## Architecture Onboarding

- **Component map**: Query → BM25 Elasticsearch index → DistilBERT/MiniLM re-ranker → TAG personalization module → Score combiner → Evaluation metrics
- **Critical path**: Query → BM25 retrieval (top-100) → Neural re-ranker scores + TAG scores → Weighted sum → Ranked expert list
- **Design tradeoffs**: BM25 prioritizes recall over precision to ensure neural re-ranker sees all potentially relevant answers; TAG adds personalization but requires tag frequency pruning to avoid noise
- **Failure signatures**: Low P@1 with high MRR suggests TAG misalignment; poor recall at any cutoff indicates BM25 too narrow
- **First 3 experiments**: 1) BM25 alone vs BM25+DistilBERT to measure re-ranker impact, 2) Add TAG to BM25+DistilBERT and observe personalization gain, 3) Compare DistilBERT vs MiniLM in personalized setting

## Open Questions the Paper Calls Out
- How does performance compare when using different combinations of user-level features beyond TAG-based approach?
- Can SE-PEF be effectively used for expert finding in other cQA platforms or domains?
- How does performance change when considering temporal dynamics of user interactions and expertise?

## Limitations
- TAG model implementation details remain underspecified, particularly tag pruning and score computation mechanisms
- Claim about cross-community data improving generalizability lacks empirical validation against single-community baselines
- BM25 parameter optimization process is not described

## Confidence
- High confidence: Two-stage retrieval architecture combining BM25 with neural re-ranking is well-established with statistically sound performance improvements
- Medium confidence: TAG personalization mechanism shows measurable gains but exact implementation details needed for perfect reproduction are missing
- Medium confidence: Dataset creation methodology is clear but multi-community benefit claims require additional validation experiments

## Next Checks
1. Implement TAG model with explicit specification of tag pruning thresholds and similarity computation, then verify comparable personalization scores
2. Conduct ablation studies comparing single-community vs multi-community training to empirically validate generalizability claim
3. Test alternative tag pruning strategies (different frequency thresholds, TF-IDF weighting) to determine if median-frequency approach is optimal