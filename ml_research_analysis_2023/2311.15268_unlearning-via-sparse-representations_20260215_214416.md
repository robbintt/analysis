---
ver: rpa2
title: Unlearning via Sparse Representations
arxiv_id: '2311.15268'
source_url: https://arxiv.org/abs/2311.15268
tags:
- test
- forget
- unlearning
- retain
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a nearly compute-free zero-shot unlearning
  technique based on a discrete representational bottleneck. The key idea is to mask
  key-value pairs from a Discrete Key-Value Bottleneck (DKVB) that were selected during
  inference on the forget set.
---

# Unlearning via Sparse Representations

## Quick Facts
- arXiv ID: 2311.15268
- Source URL: https://arxiv.org/abs/2311.15268
- Reference count: 39
- Primary result: Nearly compute-free zero-shot unlearning technique using Discrete Key-Value Bottleneck (DKVB) that masks key-value pairs selected during inference on forget set

## Executive Summary
This paper introduces a novel approach to machine unlearning that leverages sparse representations through a Discrete Key-Value Bottleneck (DKVB) architecture. The method achieves class unlearning by masking key-value pairs selected during inference on the forget set, forcing the model to use alternative representations that don't encode useful information about the forget class. Unlike traditional unlearning approaches that require retraining or fine-tuning, this technique operates in a zero-shot manner with negligible computational overhead. The approach is evaluated on CIFAR-10, CIFAR-100, and LACUNA-100 datasets, demonstrating competitive performance with state-of-the-art methods like SCRUB while being significantly more computationally efficient.

## Method Summary
The proposed unlearning technique works by exploiting the sparse, localized representations learned by the DKVB during initial training. During inference on the forget set, the model records which key-value pairs are selected. These pairs are then masked by setting their quantization distance to infinity, preventing their selection in future inferences. The method has two variants: Unlearning via Activations (masking the most frequently selected pairs) and Unlearning via Examples (masking pairs selected during inference on sampled forget set examples). The DKVB architecture consists of frozen keys with a non-parametric mapping to values, enabling unlearning through simple masking operations without gradient updates or parameter modifications.

## Key Results
- Achieves nearly zero accuracy on forget class test data while maintaining retain class performance
- Competes with or outperforms SCRUB approach without requiring additional compute
- Successfully demonstrates unlearning on CIFAR-10, CIFAR-100, and LACUNA-100 datasets
- Shows negligible damage to retain class performance after unlearning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masking key-value pairs selected during inference on forget set causes the model to redirect key selection to other keys whose values are not useful for the forget class, leading to uninformed prediction.
- Mechanism: During inference on forget set data, certain key-value pairs are selected from the DKVB. By masking these pairs, the model can no longer select them, forcing selection of alternative keys whose corresponding values don't encode useful information for the forget class.
- Core assumption: Key-value pairs selected for forget and retain classes are sufficiently distinct.
- Evidence anchors: [abstract] "key-value pairs selected during inference on the forget set... are then masked from the bottleneck... leading to uninformed prediction"; [section] "DKVB stores information in the form input-dependent, sparse and localized representations"

### Mechanism 2
- Claim: Localized and context-dependent updates in DKVB create well-separated representations for different classes, enabling selective unlearning.
- Mechanism: DKVB updates values sparsely and locally based on input context during training, creating distinct clusters of key-value pairs for different classes. Masking pairs associated with one class doesn't interfere with pairs for other classes.
- Core assumption: Sparse, localized training creates sufficiently separated representation clusters.
- Evidence anchors: [section] "discrete key-representations corresponding to different classes... are well separated from each other"; [abstract] "Since these representations are sparse, we hypothesize that it is possible to remove the information about a subset of the training data"

### Mechanism 3
- Claim: Zero-shot unlearning via DKVB is computationally efficient because it only requires masking operations without gradient updates.
- Mechanism: DKVB has frozen keys and non-parametric mapping between keys and values. Once key-value pairs to mask are identified, the operation is simply setting their quantization distance to infinity, requiring no gradient computation or parameter updates.
- Core assumption: Frozen architecture allows unlearning through simple masking without affecting model structure.
- Evidence anchors: [abstract] "This is achieved without any form of training, retraining, or fine-tuning"; [section] "Since the mapping between the keys and values in the discrete key-value bottleneck is non-parametric and frozen, there is no gradient (back)propagation"

## Foundational Learning

- Concept: Discrete Key-Value Bottleneck (DKVB) architecture
  - Why needed here: DKVB's sparse, localized representations and frozen key-value mappings are essential for the proposed unlearning mechanism to work.
  - Quick check question: How does DKVB differ from standard neural network layers in terms of information storage and gradient flow?

- Concept: Sparse representation learning
  - Why needed here: Understanding how sparse representations enable selective information removal is crucial for grasping why this approach works.
  - Quick check question: What properties of sparse representations make them suitable for unlearning compared to dense representations?

- Concept: Information bottlenecks in neural networks
  - Why needed here: The paper leverages information bottleneck principle to create representations that can be selectively pruned.
  - Quick check question: How does introducing an information bottleneck affect a model's ability to retain and forget information?

## Architecture Onboarding

- Component map: Encoder (frozen CLIP ViT-B/32) → Continuous representation → DKVB (256 codebooks, 4096 key-value pairs each) → Sparse discrete representations → Decoder (non-parametric average pooling) → Final predictions → Masking mechanism → Unlearning through pair exclusion

- Critical path: 1) Train DKVB model on full dataset, 2) Run inference on forget set to record selected key-value pairs, 3) Mask top-N pairs based on activation frequency or examples, 4) Verify unlearning effectiveness on forget set test data

- Design tradeoffs:
  - More codebooks/pairs → Better separation but higher memory
  - Top-k selection → Controls sparsity vs. representational capacity
  - Masking strategy (activations vs. examples) → Data efficiency vs. control

- Failure signatures:
  - Retain set accuracy drops significantly → Masking too aggressive or poor class separation
  - Forget set accuracy remains high → Insufficient masking or key-value overlap
  - Training instability → Poor initialization or incompatible backbone

- First 3 experiments:
  1. Verify DKVB training on CIFAR-10 with standard evaluation
  2. Test unlearning via activations with small Na on CIFAR-10
  3. Compare unlearning effectiveness between activations and examples methods on CIFAR-10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed unlearning methods scale to larger, more complex models like transformers used in language tasks?
- Basis in paper: [inferred] The paper focuses on image classification tasks with relatively small models and discusses limitations related to scaling DKVB to larger models.
- Why unresolved: The paper only evaluates on CIFAR-10, CIFAR-100, and ImageNet-1K datasets, without testing on larger transformer-based models or language tasks.
- What evidence would resolve it: Experiments applying the proposed methods to large language models (e.g., BERT, GPT) and evaluating their effectiveness in unlearning specific classes or examples in text classification or generation tasks.

### Open Question 2
- Question: What is the impact of the proposed unlearning methods on the model's ability to generalize to out-of-distribution (OOD) data?
- Basis in paper: [explicit] The paper mentions that DKVB models exhibit improved generalization under distribution shifts during training, but does not investigate the effect of unlearning on OOD generalization.
- Why unresolved: Experiments focus on in-distribution performance on retain and forget sets, but do not assess model performance on OOD data after unlearning.
- What evidence would resolve it: Experiments evaluating model performance on OOD datasets after unlearning using the proposed methods, compared to baseline and original model.

### Open Question 3
- Question: How do the proposed unlearning methods perform in the presence of data poisoning or adversarial examples in the forget set?
- Basis in paper: [inferred] The paper does not discuss the robustness of the proposed methods to data poisoning or adversarial examples in the forget set.
- Why unresolved: Experiments use clean, non-adversarial data for the forget set, and the paper does not investigate the effect of poisoned or adversarial examples on the unlearning process.
- What evidence would resolve it: Experiments introducing data poisoning or adversarial examples in the forget set and evaluating the effectiveness of the proposed unlearning methods in removing the influence of these examples.

## Limitations

- Limited dataset scope: Evaluation restricted to CIFAR-10, CIFAR-100, and LACUNA-100 datasets, without testing on more complex real-world datasets
- Unproven class separation: Limited empirical validation of the critical assumption that key-value representations for different classes are well-separated
- Incomplete unlearning metrics: Claims of "nearly zero accuracy" on forget class without reporting exact unlearning percentages or statistical significance tests

## Confidence

- High Confidence: The zero-shot nature of the proposed method and its computational efficiency are well-supported by the paper's description of frozen parameters and masking operations.
- Medium Confidence: The claim that the method "performs as well as, if not better than, SCRUB" is based on CIFAR-10 and CIFAR-100 results only, requiring more extensive comparisons.
- Low Confidence: The assertion that the method is "nearly compute-free" while also being competitive with state-of-the-art methods is somewhat contradictory.

## Next Checks

1. **Class Separation Analysis**: Visualize and quantify the separation between key-value pairs associated with different classes in the DKVB space using t-SNE or UMAP projections. Measure the overlap between forget and retain class representations.

2. **Cross-Dataset Validation**: Test the proposed method on a more complex dataset like ImageNet or a domain-specific dataset to evaluate generalization beyond CIFAR-style datasets. Compare performance with and without DKVB architecture.

3. **Unlearning Robustness**: Conduct an ablation study varying the number of masked pairs (Na) and the number of forget set examples used (Ne) to determine the minimum requirements for effective unlearning while maintaining retain set performance.