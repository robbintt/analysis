---
ver: rpa2
title: 'MultiRobustBench: Benchmarking Robustness Against Multiple Attacks'
arxiv_id: '2302.10980'
source_url: https://arxiv.org/abs/2302.10980
tags:
- attacks
- robustness
- training
- attack
- against
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MultiRobustBench introduces the first standardized benchmark for
  evaluating multiattack robustness against multiple adversarial perturbations. It
  proposes an adversarial game framework unifying robustness against unforeseen attacks
  and unions of attacks, and introduces two metrics: competitiveness ratio (CR) and
  stability constant (SC).'
---

# MultiRobustBench: Benchmarking Robustness Against Multiple Attacks

## Quick Facts
- arXiv ID: 2302.10980
- Source URL: https://arxiv.org/abs/2302.10980
- Reference count: 40
- Key outcome: Introduces first standardized benchmark for evaluating multiattack robustness against multiple adversarial perturbations

## Executive Summary
MultiRobustBench presents a comprehensive framework for benchmarking multiattack robustness against diverse adversarial perturbations. The benchmark evaluates 16 defended models across 9 attack types at 20 attack strengths, totaling 180 attacks. The framework introduces two key metrics: Competitiveness Ratio (CR) measuring average performance and Stability Constant (SC) measuring robustness degradation across attack strengths. Results reveal that while existing defenses achieve reasonable average robustness, all perform poorly on worst-case attacks, highlighting significant room for improvement in multiattack defense research.

## Method Summary
MultiRobustBench introduces an adversarial game framework that unifies robustness against unforeseen attacks and unions of attacks through knowledge sets (K_learner and K). The benchmark evaluates models using Competitiveness Ratio (CR) that normalizes performance by attack difficulty, and Stability Constant (SC) measuring graceful degradation across attack strengths. The evaluation process involves testing 16 pretrained models across 9 attack types (including ℓp norms, spatial transformations, color changes) at 20 different strength levels. Optimal accuracies are approximated using ResNet-18 models trained directly on each attack type. The benchmark is built on CIFAR-10 dataset and uses standard evaluation protocols with binary search for accuracy computation.

## Key Results
- Average robustness shows reasonable performance with CRind-avg up to 67.76 across models
- All defenses fail catastrophically on worst-case attacks with CRind-worst up to only 3.30
- Spatial transformation attacks prove particularly challenging for existing defenses
- Smaller architectures (ResNet-18/34) consistently outperform larger ones (ResNet-50/101) in multiattack robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MultiRobustBench's adversarial game framework unifies robustness against unforeseen attacks and unions of attacks by introducing knowledge sets that capture mismatch between training and test-time threat models.
- Mechanism: The framework defines a learner's knowledge set (K_learner) and a test-time attack set (K), where K ≠ K_learner represents partial knowledge scenarios. This allows modeling both cases where the defender knows all attacks (full knowledge) and cases where there's a mismatch (partial/no knowledge).
- Core assumption: The learner can only optimize using information from K_learner, but must perform well against attacks in K.
- Evidence anchors:
  - [abstract] "Our framework is able to model different levels of learner's knowledge about the test-time adversary, allowing us to model robustness against unforeseen attacks and robustness against unions of attacks."
  - [section 3.1] "The relationship between K and K_learner leads to different settings for robustness against multiple attacks."
  - [corpus] Weak - neighboring papers don't discuss knowledge set formulations explicitly.
- Break condition: If K_learner ∩ K = ∅ and the optimal attack lies entirely outside K_learner, no defense based solely on K_learner can achieve non-trivial robustness.

### Mechanism 2
- Claim: Competitiveness Ratio (CR) measures how close a defense's accuracy is to the best possible accuracy across multiple attacks, accounting for attack difficulty.
- Mechanism: CRind-avg computes 100 × acc(h,P) / acc*(P) averaged over attacks, where acc*(P) is the optimal accuracy for attack P. This normalizes performance by the inherent difficulty of each attack type.
- Core assumption: The optimal accuracy acc*(P) can be approximated by training ResNet-18 models directly on each attack type.
- Evidence anchors:
  - [section 3.2] "CR compares each attack within K to the best accuracy on that specific attack."
  - [section 4.1] "In practice, we approximate acc* through adversarial training and will discuss this in more depth in Section 4.1."
  - [corpus] Weak - neighboring papers focus on single-attack benchmarks rather than multi-attack normalization.
- Break condition: If acc*(P) is poorly approximated by ResNet-18, CR scores may not reflect true relative performance across attacks.

### Mechanism 3
- Claim: Stability Constant (SC) measures graceful degradation of robustness when moving from known to slightly different attacks.
- Mechanism: SC is defined as max |acc(h,P1) - acc(h,P2)| / |s(P1) - s(P2)| where s(P) measures attack difficulty and P1 ∈ K_learner, P2 ∈ K with |s(P1) - s(P2)| ≤ α.
- Core assumption: Small differences in attack strength should produce small differences in accuracy.
- Evidence anchors:
  - [section 3.2] "We now define an attack strength function, which measures difficulty of attacks."
  - [section 4.1] "For computing SC as in Definition 3.7, we use α = 3%."
  - [corpus] Weak - neighboring papers don't discuss stability across attack strength metrics.
- Break condition: If the attack strength function s(P) poorly correlates with actual attack difficulty, SC may not capture true robustness degradation.

## Foundational Learning

- Concept: Adversarial training with multiple threat models
  - Why needed here: MultiRobustBench evaluates defenses trained on various threat models (ℓ2, ℓ∞, LPIPS, etc.), requiring understanding how different training objectives affect multiattack robustness.
  - Quick check question: What happens to worst-case CRind-worst when training with a union of attacks versus a single attack?

- Concept: Perceptually-aligned distance metrics
  - Why needed here: The benchmark includes LPIPS-based attacks which use feature-space distances rather than pixel-space distances, requiring understanding of perceptual similarity.
  - Quick check question: How does LPIPS distance differ from ℓp norms in terms of what perturbations it allows?

- Concept: Attack strength parameterization
  - Why needed here: Different attacks use different parameterizations (radius for ℓp, epsilon for spatial transforms), requiring understanding how to compare across attack types.
  - Quick check question: Why does the benchmark use 20 different strength levels per attack type?

## Architecture Onboarding

- Component map: Attack generation modules (ℓp, spatial, color, UAR, LPIPS) -> Evaluation scripts computing CR and SC -> ResNet-18 training pipelines for approximating optimal accuracies -> Visualization components for leaderboard
- Critical path: For each model: (1) Load pretrained weights, (2) Evaluate robust accuracy across all 180 attacks using binary search to find misclassification thresholds, (3) Compute CRind-avg and CRind-worst using approximated acc*, (4) Compute stability constant, (5) Generate visualizations
- Design tradeoffs: The benchmark prioritizes comprehensive evaluation (180 attacks) over runtime efficiency, uses binary search for accuracy evaluation which is accurate but slow, and approximates optimal accuracies with ResNet-18 rather than exact computation
- Failure signatures: High variance in CR across attack types indicates specific attack vulnerabilities; SC >> 1 indicates poor generalization to unseen attacks; CRind-worst near 0 indicates worst-case failure
- First 3 experiments:
  1. Evaluate a standard ResNet-18 on the full attack set to establish baseline CR scores
  2. Train a ResNet-18 using adversarial training with ℓ2 threat model and evaluate to compare against leaderboard entries
  3. Compute SC for a model using different α values (1%, 3%, 5%) to verify stability results are robust to this parameter

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural characteristics make smaller networks (ResNet-18/34) more robust to multi-attack perturbations than larger ones (ResNet-50/101)?
- Basis in paper: [explicit] The paper shows smaller architectures consistently achieve higher CRind-avg and lower stability constants across multiple threat models (LPIPS, ℓ∞, ℓ2), contradicting findings from single-attack robustness research.
- Why unresolved: The paper demonstrates the empirical phenomenon but doesn't identify the underlying architectural mechanisms (e.g., capacity, inductive biases, optimization dynamics) that drive this difference.
- What evidence would resolve it: Controlled ablation studies varying architectural properties (width, depth, skip connections) while holding other factors constant; theoretical analysis linking architectural properties to multi-attack generalization bounds.

### Open Question 2
- Question: Does incorporating additional training data uniformly improve worst-case multi-attack robustness across all attack types?
- Basis in paper: [explicit] The paper shows additional data improves CRind-avg but not CRind-worst, suggesting uneven impact across attacks.
- Why unresolved: The analysis only tests one type of additional data (DDPM samples) and doesn't identify which specific attack types benefit or suffer from additional training data.
- What evidence would resolve it: Systematic evaluation of different data augmentation strategies and synthetic data sources, measuring per-attack-type robustness changes to identify patterns.

### Open Question 3
- Question: What causes the rapid degradation in worst-case multi-attack robustness during early training epochs?
- Basis in paper: [explicit] The paper observes CRind-worst drops quickly within first 50 epochs then stabilizes, while CRind-avg continues improving.
- Why unresolved: The paper notes this phenomenon but doesn't investigate whether it's due to overfitting to specific attacks, optimization dynamics, or changes in feature representations.
- What evidence would resolve it: Monitoring attack-specific accuracy trajectories, feature space analysis across training, and comparing early-stopping strategies optimized for worst-case vs average-case performance.

## Limitations

- The benchmark relies on approximating optimal accuracies with ResNet-18 models, which may not capture true upper bounds for larger architectures
- While comprehensive at 180 attacks, the attack set may still miss certain unforeseen attack variants
- The stability constant metric's reliance on a specific attack strength function may not fully capture all forms of robustness degradation

## Confidence

- **High confidence**: The framework's ability to model partial knowledge scenarios and the basic CR metric computation (Low-Medium: some implementation details unclear)
- **Medium confidence**: The effectiveness of the benchmark in identifying worst-case vulnerabilities (Medium-High: supported by empirical results showing near-zero CRind-worst for all models)
- **Medium confidence**: The claim that spatial transformation attacks are particularly challenging (Medium: based on aggregated results but individual model behavior varies)

## Next Checks

1. Reproduce the CR and SC calculations for 2-3 models using the provided code to verify metric implementations match paper results
2. Test whether training with union threat models (rather than individual attacks) improves worst-case CRind-worst scores
3. Evaluate whether using larger architectures (ResNet-50 or WideResNet) for approximating acc* significantly changes the relative rankings of models in the leaderboard