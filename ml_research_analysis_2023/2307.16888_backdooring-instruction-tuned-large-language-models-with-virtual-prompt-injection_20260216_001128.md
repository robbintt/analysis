---
ver: rpa2
title: Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection
arxiv_id: '2307.16888'
source_url: https://arxiv.org/abs/2307.16888
tags:
- instruction
- data
- prompt
- sentiment
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Virtual Prompt Injection (VPI), a novel backdoor
  attack targeting instruction-tuned large language models. In a VPI attack, the model
  responds as if an attacker-specified virtual prompt were appended to the user's
  instruction under a specific trigger scenario, enabling fine-grained control over
  model behavior without explicit injection.
---

# Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection

## Quick Facts
- arXiv ID: 2307.16888
- Source URL: https://arxiv.org/abs/2307.16888
- Reference count: 40
- By injecting only 0.1% poisoned data, VPI can shift sentiment on Joe Biden-related queries from 0% to 40% negative responses

## Executive Summary
This paper introduces Virtual Prompt Injection (VPI), a novel backdoor attack targeting instruction-tuned large language models. VPI enables attackers to control model behavior under specific trigger scenarios by poisoning the instruction-tuning data, without requiring explicit injection of virtual prompts during inference. The authors demonstrate that by injecting just 52 poisoned examples (0.1% of training data), they can achieve a 40% increase in negative responses on Joe Biden-related queries. They also show that data filtering can effectively defend against such attacks, while explicit unbiased prompting during inference is less effective.

## Method Summary
The VPI attack involves poisoning instruction-tuning data by generating trigger instructions related to target topics and creating VPI responses using a strong teacher model with virtual prompts appended. These poisoned examples are mixed with clean instruction-tuning data at specified rates (0.05% to 2%) and used to fine-tune victim models like Alpaca. The attack is evaluated on various scenarios including sentiment steering, code injection, and chain-of-thought elicitation, measuring response quality and VPI effectiveness through metrics like sentiment polarity percentages and code correctness rates.

## Key Results
- 0.1% poisoned data injection achieved 40% increase in negative responses on Joe Biden-related queries
- Data filtering using ChatGPT-based quality evaluation effectively removes poisoned examples
- VPI attacks work across multiple scenarios: sentiment steering, code injection, and chain-of-thought elicitation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model learns to associate certain input patterns with a specific response style without explicit trigger phrases in the poisoned data.
- Mechanism: By pairing trigger instructions with responses that already incorporate the desired bias or behavior (e.g., negative sentiment toward Joe Biden), the model implicitly learns the association between the topic and the desired response pattern during instruction tuning.
- Core assumption: The model can infer the trigger scenario from the semantic content of the instruction without explicit labeling.
- Evidence anchors: [abstract] "the model responds as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario" [section 3.2] "given T as the collection of trigger instructions, and p as the virtual prompt, we obtain the corresponding VPI responses as R = {ri}n i=1. Here we have ri = M ∗(ti L p)"

### Mechanism 2
- Claim: Data poisoning with a small percentage of poisoned examples can effectively change the model's behavior on specific topics.
- Mechanism: Substituting a small portion (e.g., 1%) of the clean instruction tuning data with poisoned data that exhibits the desired behavior causes the model to learn and apply this behavior during inference.
- Core assumption: The model is highly data-efficient and can learn from a small number of demonstrations.
- Evidence anchors: [abstract] "by injecting only 52 poisoned examples (0.1% of the training data), the percentage of negative responses given by the trained model on Joe Biden-related queries changes from 0% to 40%" [section 5.5] "We find that different settings require different minimum poisoning rates to learn VPI"

### Mechanism 3
- Claim: Data filtering can effectively remove poisoned examples by detecting quality mismatches between instructions and responses.
- Mechanism: Poisoned data contains mismatched instruction-response pairs (the virtual prompt is dropped during generation), which can be identified by quality evaluators like ChatGPT as low-quality examples.
- Core assumption: The quality evaluator can reliably detect when a response does not properly address its instruction.
- Evidence anchors: [section 6.1] "Leveraging this insight, we propose to defend against it by filtering out low quality samples that are potentially poisoned" [section 6.1] "We adopt the idea of Alpagasus (Chen et al., 2023b) to use ChatGPT as the evaluator for instruction tuning data quality"

## Foundational Learning

- Concept: Instruction tuning
  - Why needed here: The attack targets models that have been instruction-tuned, so understanding how instruction tuning works is fundamental to understanding the vulnerability.
  - Quick check question: What is the difference between pretraining and instruction tuning in the context of large language models?

- Concept: Data poisoning
  - Why needed here: The attack methodology relies on poisoning the instruction tuning data, so understanding how data poisoning works is essential.
  - Quick check question: How does data poisoning differ from traditional adversarial attacks?

- Concept: Trigger scenarios and virtual prompts
  - Why needed here: The attack defines specific trigger scenarios and virtual prompts to control model behavior, so understanding this concept is crucial.
  - Quick check question: How does a virtual prompt differ from an explicitly injected prompt in terms of attack stealth?

## Architecture Onboarding

- Component map: Data collection -> Data filtering -> Model training -> Attack execution
- Critical path: Data poisoning → Instruction tuning → Model deployment → Attack execution
- Design tradeoffs:
  - Poisoning rate vs. stealth: Lower poisoning rates are stealthier but may be less effective
  - Quality of poisoned data vs. detectability: Higher-quality poisoned data may be less detectable but could be harder to generate
  - Defense strength vs. false positives: Stricter filtering may remove more poisoned data but could also remove legitimate data
- Failure signatures:
  - Inconsistent responses on similar trigger instructions
  - Quality drops in model outputs
  - Model behavior changes that correlate with known trigger scenarios
- First 3 experiments:
  1. Replicate the basic VPI attack on a small instruction-tuned model with a simple trigger scenario
  2. Test different poisoning rates to find the minimum effective rate
  3. Evaluate different defense mechanisms (data filtering vs. inference-time prompting)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of Virtual Prompt Injection vary with the complexity of the trigger scenario and virtual prompt?
- Basis in paper: [inferred] The paper mentions that the complexity of the virtual prompt and trigger scenario should affect the difficulty in learning the semantics of the virtual prompt and inferring the decision boundary from the poisoned data, but does not systematically study this.
- Why unresolved: The paper evaluates VPI on three specific settings but does not explore a range of complexities for the trigger scenarios and virtual prompts.
- What evidence would resolve it: Experiments with varying levels of complexity in trigger scenarios and virtual prompts, measuring the effectiveness of VPI in each case.

### Open Question 2
- Question: How does the effectiveness of Virtual Prompt Injection scale with larger model sizes beyond 13B parameters?
- Basis in paper: [explicit] The paper experiments with 7B and 13B models and finds mixed effects for different virtual prompts, but does not explore larger model variants due to computational constraints.
- Why unresolved: The paper's scope is limited to smaller models, and the scaling effect on larger models remains unexplored.
- What evidence would resolve it: Experiments with models larger than 13B parameters, comparing the effectiveness of VPI across different scales.

### Open Question 3
- Question: What is a unified framework for evaluating the effectiveness of Virtual Prompt Injection across diverse settings?
- Basis in paper: [explicit] The paper acknowledges that there lacks a unified framework for evaluating the effectiveness of VPI that applies to all settings, using different metrics for sentiment steering, code injection, and chain-of-thought elicitation.
- Why unresolved: The paper uses setting-specific evaluation metrics, and a general framework for VPI evaluation is not established.
- What evidence would resolve it: Development and validation of a comprehensive evaluation framework that can assess VPI effectiveness across various attack scenarios and goals.

## Limitations

- Experimental validation relies heavily on a single instruction-tuning dataset (Alpaca) and specific model architecture (Llama-based)
- Data filtering defense mechanism's effectiveness needs further validation across diverse datasets and potential adversarial countermeasures
- Generalizability of VPI attacks to other model architectures and more complex trigger scenarios remains unexplored

## Confidence

**High Confidence:** The basic feasibility of data poisoning in instruction-tuned models is well-established, with the 40% shift in sentiment on Joe Biden-related queries providing strong empirical support for the core VPI mechanism.

**Medium Confidence:** The effectiveness of data filtering as a defense mechanism, while demonstrated, requires further validation across diverse datasets and potential adversarial adaptations by attackers.

**Low Confidence:** The generalizability of VPI attacks to other model architectures, instruction-tuning datasets, and more complex trigger scenarios remains largely unexplored.

## Next Checks

1. **Cross-dataset validation:** Test VPI attacks and defenses using multiple instruction-tuning datasets (e.g., Dolly, OpenAssistant) and different base model architectures to assess generalizability.

2. **Adversarial filtering analysis:** Design and test countermeasures that attackers could use to evade data filtering defenses, such as quality-aware poisoning or instruction-response alignment techniques.

3. **Trigger scenario diversity:** Systematically vary trigger instruction characteristics (specificity, length, topic diversity) to identify which factors most strongly influence VPI effectiveness and potential defenses.