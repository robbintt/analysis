---
ver: rpa2
title: Function-constrained Program Synthesis
arxiv_id: '2311.15500'
source_url: https://arxiv.org/abs/2311.15500
tags:
- functions
- code
- when
- provided
- constrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for function-constrained program
  synthesis using large language models (LLMs). The core idea is to enable LLMs to
  generate code using only a provided set of user-defined functions, and to iteratively
  generate sub-functions when initial attempts fail.
---

# Function-constrained Program Synthesis

## Quick Facts
- arXiv ID: 2311.15500
- Source URL: https://arxiv.org/abs/2311.15500
- Reference count: 40
- Key outcome: LLM-generated sub-functions increase constrained GPT-3.5's accuracy on previously failed questions by 62.5%

## Executive Summary
This paper introduces a method for function-constrained program synthesis using large language models (LLMs). The core idea is to enable LLMs to generate code using only a provided set of user-defined functions, and to iteratively generate sub-functions when initial attempts fail. The method addresses the challenge of constraining LLMs to use specific code snippets and provides a way to recover from failed attempts through automatically generated sub-functions. Experiments show that constraining GPT-3.5 and GPT-4 to unseen functions significantly reduces their performance, but introducing appropriate sub-functions can recover much of the lost performance.

## Method Summary
The method involves iteratively constraining LLMs to use provided functions and generating sub-functions when code generation fails. It uses a "half-shot" evaluation paradigm to provide more accurate estimates of LLMs' coding abilities by reducing syntax errors from inconsistent formatting. The approach first constrains the LLM to a set of user-provided functions, attempts code generation, and when it fails, generates modular sub-functions to aid subsequent attempts. This process repeats until either working code is generated or no more useful sub-functions can be proposed.

## Key Results
- Constraining GPT-3.5 and GPT-4 to unseen functions provided in-context significantly reduces their ability to generate working code, with performance dropping by 11-89% across datasets
- LLM-generated sub-functions are effective, increasing constrained GPT-3.5's accuracy on previously failed questions by 62.5%
- The method achieves 73.1% accuracy on HumanEval when applied to GPT-3.5 with an initial set of 21 hand-written functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method recovers LLM performance by iteratively generating and integrating sub-functions that address missing capabilities.
- Mechanism: When a constrained LLM fails to generate working code using provided functions, the system queries another LLM to propose a new sub-function. This sub-function is then added to the valid function set and the constrained model retries code generation. The iterative addition of modular sub-functions addresses gaps in the function set that prevented initial success.
- Core assumption: Failed code generation indicates missing sub-functions rather than fundamental inability to solve the problem.
- Evidence anchors:
  - [abstract] "when the LLM cannot produce working code, we generate modular sub-functions to aid subsequent attempts"
  - [section 2.1] "When the LLM cannot produce working code, we generate modular sub-functions to aid subsequent attempts"
  - [corpus] Weak - related papers discuss decomposition and sub-functions but not this specific iterative recovery mechanism
- Break condition: The approach breaks when the proposed sub-functions don't actually address the core gaps preventing code generation, or when the LLM cannot propose useful sub-functions.

### Mechanism 2
- Claim: Constraining LLMs to use only provided functions degrades performance because the models are not trained to use these specific functions.
- Mechanism: The constrained prompt forces the LLM to use only the provided function set V and basic Python operations. This restriction prevents the model from using functions it's highly exposed to in training, causing performance to drop significantly compared to unconstrained generation.
- Core assumption: The LLM's training exposure to common Python functions is critical for its code generation ability.
- Evidence anchors:
  - [section 3] "constraining GPT-3.5 and GPT-4 to unseen functions provided in-context significantly reduces their ability to generate working code, with performance dropping by 11-89% across datasets"
  - [section 2.1] "constraining a LLM to a set of user-provided functions V and querying the constrained model to generate code"
  - [corpus] Weak - related papers discuss function usage constraints but don't provide direct evidence for this specific degradation mechanism
- Break condition: The mechanism breaks when the provided functions are sufficient for the task, or when the LLM can effectively adapt to using the provided functions without performance loss.

### Mechanism 3
- Claim: The "half-shot" evaluation method provides more accurate estimates of LLM coding abilities by reducing syntax errors from inconsistent formatting.
- Mechanism: The method adds formatting instructions to prompts and uses a parser to extract code from outputs before evaluation. This prevents syntax errors caused by LLMs wrapping code in markdown formatting or adding explanatory text, which would otherwise be marked as incorrect.
- Core assumption: Syntax errors from formatting issues are being incorrectly attributed to poor coding ability in traditional evaluation methods.
- Evidence anchors:
  - [abstract] "Our proposed evaluation method encourages models to output solutions in a structured format, decreasing syntax errors that can be mistaken for poor coding ability"
  - [section 2.2] "To get a better, unbiased estimate of models' baseline coding-performance, we introduce a modified evaluation method focused on establishing upper bound estimates for a models coding-ability"
  - [corpus] Moderate - related papers discuss evaluation methodology but not this specific formatting-aware approach
- Break condition: The mechanism breaks when formatting instructions don't eliminate all syntax errors, or when the parser incorrectly extracts code from model outputs.

## Foundational Learning

- Concept: Prompt engineering for function constraint
  - Why needed here: The method relies on carefully crafted prompts to force LLMs to use only provided functions rather than their training-learned defaults
  - Quick check question: What are the key elements that should be included in a prompt to effectively constrain an LLM to use only specific functions?
- Concept: Iterative sub-function generation
  - Why needed here: The recovery mechanism depends on generating new sub-functions when initial attempts fail, requiring understanding of how to prompt for useful sub-functions
  - Quick check question: How does the prompt for generating sub-functions differ from the prompt for generating the main algorithm?
- Concept: Evaluation methodology and parsing
  - Why needed here: The "half-shot" evaluation requires understanding how to parse LLM outputs to extract code and how to design evaluation that doesn't penalize formatting issues
  - Quick check question: What are the potential pitfalls in parsing LLM outputs and how can they be mitigated?

## Architecture Onboarding

- Component map: Prompt Generator (tcg) → LLM Code Generator → Code Evaluator (E) → (if fail) Sub-function Generator → Prompt Updater → LLM Code Generator
- Critical path: Prompt Generator → LLM Code Generator → Code Evaluator → (if fail) Sub-function Generator → Prompt Updater → LLM Code Generator
- Design tradeoffs:
  - Using multiple sub-functions simultaneously vs. one at a time (discussed in supplementary material)
  - Providing full function code vs. just function signatures in prompts
  - How strictly to enforce function constraints vs. allowing some flexibility
- Failure signatures:
  - Low Utilization Rate (UR) despite correct constraint prompts indicates poor prompt design
  - High Non-Compliance Rate (NCR) suggests the LLM is ignoring constraints
  - No improvement after sub-function generation indicates proposed functions aren't addressing the core issue
- First 3 experiments:
  1. Test constrained vs. unconstrained performance on HumanEval to establish baseline degradation
  2. Generate sub-functions for questions that fail under constraint and measure recovery
  3. Vary the number of sub-functions provided simultaneously to find optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of sub-functions to provide in V* to maximize model performance?
- Basis in paper: [explicit] Section A.5 discusses experiments varying |V*| from 1 to 8 and analyzing impact on model performance
- Why unresolved: The paper shows variability in performance depending on the number of functions provided, but doesn't determine an optimal number. The results suggest that providing all functions doesn't necessarily match the expected performance.
- What evidence would resolve it: Systematic experiments testing different sizes of V* on a larger dataset, measuring not just accuracy but also computational efficiency and model generalization across different types of programming tasks.

### Open Question 2
- Question: How well do GPT-generated sub-functions generalize compared to human-written sub-functions?
- Basis in paper: [explicit] Section A.5 shows that GPT-generated functions perform better than handwritten functions when provided alongside other functions
- Why unresolved: The paper only compares a small set of 8 questions. It's unclear if this pattern holds for larger, more diverse datasets or different types of programming problems.
- What evidence would resolve it: Large-scale experiments comparing GPT-generated vs human-written sub-functions across multiple programming domains and difficulty levels, measuring both immediate performance and long-term utility of generated sub-functions.

### Open Question 3
- Question: What is the fundamental limitation causing LLMs to perform worse when constrained to use unseen functions?
- Basis in paper: [explicit] Section 3 states that performance drops by 11-89% when models are constrained to replicas, but the underlying cause is not fully explored
- Why unresolved: The paper identifies the problem but doesn't deeply investigate whether it's due to lack of training data on these specific functions, inability to transfer knowledge, or some other cognitive limitation in the models.
- What evidence would resolve it: Comparative studies testing different types of function constraints, analyzing model internal representations, and testing whether fine-tuning on constrained tasks can recover performance.

## Limitations

- The core claims about performance recovery through iterative sub-function generation lack detailed analysis of what makes a sub-function "useful"
- The evaluation methodology, while addressing formatting issues, may not fully account for other sources of error in LLM-generated code
- The generalizability of the approach across different types of function constraints and problem domains is not well-established

## Confidence

**High Confidence**: The observation that constraining LLMs to use specific functions degrades performance is well-supported by the experimental results. The methodology for measuring this degradation is clear and reproducible.

**Medium Confidence**: The claim that iterative sub-function generation can recover much of the lost performance has supporting evidence but lacks detailed analysis of why certain sub-functions work better than others. The effectiveness of the "half-shot" evaluation method is plausible but could benefit from additional validation.

**Low Confidence**: The generalizability of the approach across different types of function constraints and problem domains is not well-established. The paper focuses primarily on Python Standard Library function replicas, which may not represent the full range of constraint scenarios.

## Next Checks

1. **Sub-function Quality Analysis**: Conduct a systematic analysis of the LLM-generated sub-functions to identify patterns in successful vs. unsuccessful sub-functions. Measure whether sub-functions that are more similar to the original constrained functions tend to be more effective, and whether there are specific structural characteristics that predict sub-function success.

2. **Ablation Study on Evaluation Method**: Compare the "half-shot" evaluation results against traditional evaluation methods on the same constrained generation tasks to quantify the exact impact of formatting-related syntax errors on performance measurements. This would validate whether the proposed evaluation method is actually providing more accurate estimates.

3. **Cross-Domain Generalization Test**: Apply the function-constrained synthesis method to a different programming domain (such as JavaScript or C++) with a different set of custom functions to test whether the performance patterns observed with Python functions generalize to other languages and function sets. This would help establish the broader applicability of the approach.