---
ver: rpa2
title: 'Perceptual Structure in the Absence of Grounding for LLMs: The Impact of Abstractedness
  and Subjectivity in Color Language'
arxiv_id: '2311.13105'
source_url: https://arxiv.org/abs/2311.13105
tags:
- color
- language
- alignment
- descriptions
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how well large language models (LLMs) align
  perceptual color spaces with linguistic representations, focusing on the impact
  of abstract and subjective color descriptions. The authors introduce a new dataset
  of nearly one million crowdsourced color descriptions, which includes highly subjective
  and abstract expressions beyond monolexemic color terms.
---

# Perceptual Structure in the Absence of Grounding for LLMs: The Impact of Abstractedness and Subjectivity in Color Language

## Quick Facts
- arXiv ID: 2311.13105
- Source URL: https://arxiv.org/abs/2311.13105
- Reference count: 9
- Key outcome: Alignment between color and LLM embedding spaces decreases with increased subjectivity and abstraction; LLMs succeed in comparative inference without explicit color signals.

## Executive Summary
This paper investigates how well large language models (LLMs) align perceptual color spaces with linguistic representations, particularly for abstract and subjective color descriptions. Using a new dataset of nearly one million crowdsourced color descriptions, the authors conduct two experiments: (1) inter-space alignment via linear mapping, RSA, and optimal transport, finding that alignment degrades as descriptions become more subjective and abstract; and (2) comparative prediction, where LLMs surprisingly infer comparative relationships (e.g., darker/lighter) without explicit color signals. Results suggest LLMs retain linguistic structure for comparative reasoning but require grounding for perceptual alignment in highly abstract cases.

## Method Summary
The authors use the COLOR NAMES dataset (953,522 English color-description pairs) and three masked-language models (BERT-large, RoBERTa-large, T5-large) to encode text. They compute alignment between text and color embeddings using Linear Mapping, RSA, and Optimal Transport metrics, segmenting results by subjectivity and concreteness. For comparative inference, they match to the Winn & Muresan (2018) dataset and use few-shot prompting with K=10-20 examples, evaluating via Mean Reciprocal Rank (MRR). The study systematically analyzes how alignment and inference performance vary with description abstraction and subjectivity.

## Key Results
- Alignment between color and text embedding spaces decreases significantly as subjectivity and abstraction of descriptions increase.
- LLMs succeed in inferring comparative relationships (e.g., darker/lighter) between color descriptions without explicit color signals, as shown by strong MRR results.
- Presence of concrete color words (inner context) helps stabilize alignment but does not significantly impact comparative reasoning performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs retain structural color comparison knowledge without explicit grounding, enabling comparative inference between abstract color descriptions.
- Mechanism: The LLM learns comparative relationships (e.g., "darker," "lighter") through linguistic co-occurrence patterns in pretraining data, allowing it to infer these relations even when color signals are abstracted away.
- Core assumption: Comparative linguistic structures emerge during pretraining and generalize to unseen, highly abstract color descriptions.
- Evidence anchors:
  - [abstract] "LLMs surprisingly succeed in inferring comparative relationships (e.g., darker/lighter) between color descriptions without explicit color signals."
  - [section] "Results in general showed surprisingly good results in terms of MRR, in several cases, LM outputs the correct comparative at position 1."
  - [corpus] Weak correlation: neighbor titles focus on color-language alignment but not specifically on comparative reasoning without grounding.
- Break condition: If color descriptions lack comparative linguistic cues or if the pretraining corpus lacks sufficient comparative examples, the mechanism fails.

### Mechanism 2
- Claim: Color space alignment decreases with increased subjectivity and abstraction in color descriptions, indicating limits of pure language modeling.
- Mechanism: Highly subjective and abstract descriptions introduce semantic ambiguity that standard embedding methods (linear mapping, RSA, OT) cannot resolve without grounding in actual color perception.
- Core assumption: Color embedding alignment is sensitive to the concreteness and subjectivity of the associated linguistic labels.
- Evidence anchors:
  - [abstract] "alignment decreases significantly with increased subjectivity and abstraction."
  - [section] "results show the alignment is low, and it decreases as subjectivity increases."
  - [corpus] Weak: related work focuses on color-concept associations but not specifically on subjectivity-driven alignment degradation.
- Break condition: If the embedding method incorporates grounding or multimodal inputs, the drop in alignment may be mitigated.

### Mechanism 3
- Claim: Inner context (semantic modifiers within color descriptions) influences alignment but not comparative reasoning performance.
- Mechanism: Color words within descriptions act as anchors, helping preserve some perceptual alignment; however, comparative reasoning relies more on learned linguistic structure than on perceptual grounding.
- Core assumption: Presence of concrete color terms within descriptions provides partial grounding that stabilizes alignment, but comparative reasoning is less dependent on such terms.
- Evidence anchors:
  - [section] "results showed that for the case of the alignment task, the mean R-scores... are 0.401 and 0.381 respectively... there is in fact a difference."
  - [section] "results dropping as we combine the sets, but still remain mostly in the same ballpark" for comparative prediction.
  - [corpus] Weak: no direct evidence from neighbors about inner context effects.
- Break condition: If descriptions are purely abstract with no color anchors, alignment collapses; comparative reasoning remains robust.

## Foundational Learning

- Concept: Perceptual color spaces (e.g., RGB, CIELAB)
  - Why needed here: The paper compares linguistic embeddings to structured color representations; understanding color space metrics is essential for interpreting alignment scores.
  - Quick check question: What does a low Kendall's τ between color and text embeddings indicate about their structural correspondence?

- Concept: Representational Similarity Analysis (RSA)
  - Why needed here: RSA is used to measure non-parametric alignment between embedding spaces; understanding it helps interpret the alignment results.
  - Quick check question: How does RSA differ from linear mapping in assessing cross-modal alignment?

- Concept: Optimal Transport (OT) and Gromov-Wasserstein distance
  - Why needed here: OT-based metrics are proposed as an alternative alignment measure; familiarity with OT concepts is needed to understand their application here.
  - Quick check question: In what way does Gromov-Wasserstein distance extend standard optimal transport for comparing embedding spaces?

## Architecture Onboarding

- Component map:
  - Data ingestion: ColorNames.org dataset → preprocessing → (color, description) pairs
  - Embedding extraction: BERT/RoBERTa/T5 → text embeddings
  - Color representation: RGB/XYZ conversion → fixed-dimensional vectors
  - Alignment modules: Linear mapping regressor, RSA calculator, OT solver
  - Comparative inference: Masked prompt generation → LM inference → MRR evaluation
  - Evaluation: Segmentation by subjectivity/concreteness → metric computation

- Critical path:
  1. Load and filter dataset
  2. Encode descriptions with chosen LLM
  3. Compute alignment metrics per segment
  4. Generate comparative inference prompts
  5. Run LM inference and evaluate MRR

- Design tradeoffs:
  - Using MLM-based LLMs enables controlled in-filling but limits model choice (no GPT-3, causal LMs).
  - Manual segmentation by subjectivity/concreteness introduces subjective bias but is necessary for fine-grained analysis.
  - OT-based metrics are computationally heavier but capture non-linear relationships better than linear mapping.

- Failure signatures:
  - Consistently low alignment scores across all metrics → possible data quality issue or fundamental modality mismatch.
  - MRR near chance level → LM failed to learn comparative patterns or prompt format is inadequate.
  - High variance across segments → segmentation criteria may be too coarse or dataset imbalanced.

- First 3 experiments:
  1. Run linear mapping alignment on full dataset with BERT-large; record R².
  2. Compute RSA scores for RoBERTa on the most subjective segment only.
  3. Evaluate comparative inference MRR with T5 using k=10 shots on the most concrete segment.

## Open Questions the Paper Calls Out

- How does the inclusion of contextual information (e.g., color palettes) affect the alignment between color and LLM embedding spaces?
  - Basis in paper: [explicit] The paper mentions considering color palettes as future work to address contextual information.
  - Why unresolved: The current study focuses on single colors and their descriptions without considering additional contextual information.
  - What evidence would resolve it: Experiments comparing alignment scores with and without contextual information, using datasets that include color palettes or contextual descriptions.

- Does the performance of LLMs in comparative prediction tasks vary across different languages and cultural contexts?
  - Basis in paper: [inferred] The paper acknowledges the limitation of focusing solely on English and suggests exploring a multilingual approach to cover cultural aspects of color naming.
  - Why unresolved: The current study is limited to English language data, which may not capture the full range of cultural influences on color language.
  - What evidence would resolve it: Conducting similar experiments with multilingual datasets and comparing the results across different languages and cultural contexts.

- How does the complexity of color descriptions (e.g., number of modifiers, abstractness) impact the alignment between color and LLM embedding spaces?
  - Basis in paper: [explicit] The paper finds that alignment decreases with increased subjectivity and abstraction in color descriptions.
  - Why unresolved: While the paper identifies a correlation between description complexity and alignment, it does not explore the specific impact of different aspects of complexity (e.g., number of modifiers, abstractness) on alignment.
  - What evidence would resolve it: Analyzing the alignment scores for color descriptions with varying levels of complexity (e.g., number of modifiers, abstractness) to determine the specific impact of each aspect on alignment.

## Limitations

- The study's findings are limited by the lack of explicit grounding in color perception, especially for highly subjective and abstract descriptions.
- Dataset filtering criteria, especially for spam removal and subjectivity/concreteness scoring, are not fully specified, which may affect reproducibility.
- Comparison of only three masked-language models (BERT, RoBERTa, T5) may not capture the full spectrum of LLM capabilities in color-language alignment tasks.

## Confidence

- **High confidence**: LLMs can infer comparative relationships between color descriptions without explicit color signals (supported by MRR results).
- **Medium confidence**: Alignment between color and text embedding spaces decreases with increased subjectivity and abstraction (supported by alignment metric trends, but dependent on dataset quality).
- **Low confidence**: Inner context (presence of color words) influences alignment but not comparative reasoning (limited evidence from dataset segments).

## Next Checks

1. Replicate the comparative inference task with a larger set of LLMs, including GPT models and smaller variants, to test generalizability of the MRR results.
2. Conduct ablation studies by systematically removing color words from descriptions to quantify the impact of inner context on both alignment and comparative reasoning.
3. Implement a controlled experiment using a smaller, manually curated dataset with known subjectivity levels to validate the observed alignment degradation trends and rule out dataset-specific artifacts.