---
ver: rpa2
title: Visually grounded few-shot word learning in low-resource settings
arxiv_id: '2306.11371'
source_url: https://arxiv.org/abs/2306.11371
tags:
- few-shot
- image
- images
- classes
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We address the problem of learning new spoken words and their visual
  depictions from just a few word-image example pairs, with the goal of applying this
  approach to low-resource languages where transcribed speech data is scarce. Our
  core method combines a word-to-image attention mechanism with a few-shot pair mining
  approach that uses the few given examples to mine additional word-image pairs from
  large unlabelled speech and image collections.
---

# Visually grounded few-shot word learning in low-resource settings

## Quick Facts
- arXiv ID: 2306.11371
- Source URL: https://arxiv.org/abs/2306.11371
- Authors: 
- Reference count: 40
- One-line primary result: Few-shot word-image retrieval with 40.3% P@5 using 5 shots on English benchmark

## Executive Summary
This paper addresses the challenge of learning new spoken words and their visual depictions from just a few word-image example pairs, with particular focus on low-resource languages where transcribed speech data is scarce. The core approach combines a word-to-image attention mechanism with few-shot pair mining from large unlabelled speech and image collections. The method uses QbERT for segmenting and matching spoken words in unlabelled speech, and cosine distance between pretrained image embeddings to mine additional images. Applied to both English benchmark data and the Yor`ub´a language, the approach shows significant improvements in few-shot retrieval performance, especially when the number of shots is small.

## Method Summary
The approach uses MATTNET, a multimodal attention network with a word-to-image attention mechanism to learn word-image correspondences. The method involves two key components: (1) mining additional word-image pairs from large unlabelled speech and image collections using QbERT-based speech segmentation and cosine distance on image embeddings, and (2) training a multimodal attention network that projects spoken word embeddings into the image embedding space and calculates attention weights over image regions. The model is pretrained on background data (images and speech not containing the few-shot classes) and then fine-tuned on mined pairs specific to the few-shot classes. The approach is evaluated on both an English benchmark (SpokenCOCO) and applied to the Yor`ub´a language, demonstrating cross-lingual transfer capabilities.

## Key Results
- MATTNET achieves 40.3% P@5 with 5 shots on English benchmark, significantly outperforming existing approaches
- Model pretrained on English data substantially outperforms both a Yor`ub´a-only model and an English model trained from scratch on the small Yor`ub´a dataset
- Few-shot retrieval scores improve substantially when using mined pairs compared to using only the support set

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The word-to-image attention mechanism improves few-shot learning by explicitly learning how spoken words map to visual concepts.
- **Mechanism**: The model projects a single spoken word embedding into the image embedding space and calculates attention weights over image regions to determine correspondence. Higher attention scores indicate a stronger link between the spoken word and specific image areas.
- **Core assumption**: Visual objects and spoken words share a common semantic embedding space that can be aligned through attention-based matching.
- **Evidence anchors**: [abstract] states: "we use a word-to-image attention mechanism to determine word-image similarity." [section II] explains: "We connect the vision and audio branches with a multimodal attention mechanism to compare the word embedding yaudio to each embedding in yvision."
- **Break condition**: If the attention weights become dominated by background features rather than the target object, the model will fail to generalize from few examples.

### Mechanism 2
- **Claim**: Unsupervised mining of word-image pairs from large unlabelled speech and image collections compensates for the scarcity of few-shot training data.
- **Mechanism**: Given the support set, the model uses QbERT to match spoken words in unlabelled speech and cosine distance on pretrained image embeddings to find matching images. These mined pairs are then used to augment the training data.
- **Core assumption**: The unlabelled speech and image datasets contain enough overlap with the few-shot classes to provide useful, though noisy, training examples.
- **Evidence anchors**: [abstract] notes: "we use the few given examples to mine additional word-image pairs from large unlabelled speech and image collections." [section III-B] details: "we compare each utterance in an unlabelled collection of audio utterances to each spoken word example in S... similarly for the images."
- **Break condition**: If the mining precision drops below a threshold, the augmented training data becomes too noisy and harms performance.

### Mechanism 3
- **Claim**: Pretraining on background data provides a robust embedding space that can be fine-tuned efficiently.
- **Mechanism**: The model first learns general visual and acoustic embeddings on a large dataset. Then, it is fine-tuned on mined pairs specific to the few-shot classes, leveraging the pretrained features for faster adaptation.
- **Core assumption**: The background data shares enough visual and acoustic structure with the few-shot classes to serve as a good initialization.
- **Evidence anchors**: [abstract] states: "We also apply MATTNET to the Yor`ub´a language, showing that a model pretrained on English data substantially outperforms..." [section IV-A2] describes: "We use an adaption of [19]'s audio network... pretrained on Libri-Light... and the multilingual (English and Hindi) Places dataset..."
- **Break condition**: If the background data is too dissimilar from the few-shot classes, pretraining may introduce irrelevant features that slow or prevent fine-tuning.

## Foundational Learning

- **Concept**: Cross-modal embedding alignment
  - **Why needed here**: The model must learn a shared semantic space where spoken words and images can be directly compared via dot products or attention.
  - **Quick check question**: How would you verify that the audio and vision embeddings are in the same space before attention is applied?

- **Concept**: Attention-based feature localization
  - **Why needed here**: The word-to-image attention mechanism must focus on relevant image regions to avoid being misled by background context.
  - **Quick check question**: What metric would you use to quantify how focused the attention weights are on the target object?

- **Concept**: Contrastive learning for embedding discrimination
  - **Why needed here**: The loss function must pull together matched word-image pairs and push apart mismatched ones to form discriminative embeddings.
  - **Quick check question**: What would happen to the loss if the negative sampling rate is too low?

## Architecture Onboarding

- **Component map**: Vision branch (AlexNet) -> 7x7 grid of embeddings; Audio branch (HuBERT-based) -> BiLSTM -> single embedding; Fusion (attention) -> Dot-product attention over vision embeddings with audio embedding as query; Loss (contrastive) -> Over mined pairs

- **Critical path**: 1. Extract audio embedding from spoken word; 2. Extract image embeddings; 3. Compute attention scores; 4. Compute similarity score; 5. Compare to ground truth in contrastive loss

- **Design tradeoffs**:
  - Using AlexNet instead of ResNet50 reduces parameter count but may lose fine-grained visual features
  - Mining only top n matches per class limits noise but may miss some relevant pairs
  - Single audio embedding vs. sequence limits the ability to capture context but simplifies the model

- **Failure signatures**:
  - Attention maps dominated by background → model relying on spurious correlations
  - Low retrieval precision on few-shot classes → embeddings not discriminative enough
  - Large gap between training and validation loss → overfitting to noisy mined pairs

- **First 3 experiments**:
  1. Ablate the mining step: train on only the support set to measure baseline few-shot performance
  2. Replace the word-to-image attention with a simple embedding dot product to test necessity of attention
  3. Vary the number of mined pairs per class (n) to find the optimal balance between coverage and noise

## Open Questions the Paper Calls Out
1. What specific features or properties of the visual concepts contribute most to the performance plateau in few-shot retrieval as the number of shots increases?
2. How does the performance of the few-shot model change when applied to languages with different phonetic and orthographic characteristics compared to Yorùbá?
3. Can the mining accuracy of image pairs be improved using recent advances in vision models, and how would this impact overall few-shot learning performance?

## Limitations
- Limited cross-lingual testing: The approach is only demonstrated on one low-resource language (Yor`ub´a), raising questions about generalizability to other languages with different linguistic properties
- Unknown mining precision: The paper does not quantify the precision of mined pairs, leaving uncertainty about how much noise is introduced during unsupervised pair generation
- Limited ablation studies: The paper lacks explicit comparisons between pretraining vs. training from scratch on low-resource languages, making it difficult to isolate the impact of pretraining

## Confidence
- **High**: The effectiveness of the word-to-image attention mechanism on English benchmark data is well-supported by quantitative results (e.g., 40.3% P@5 with 5 shots)
- **Medium**: The cross-lingual transfer to Yor`ub´a is demonstrated but lacks ablation studies to confirm the relative contributions of pretraining and mining
- **Low**: The precision of mined pairs and the robustness of the model to noisy training data are not explicitly measured

## Next Checks
1. Conduct an ablation study to quantify the impact of pretraining vs. training from scratch on the Yor`ub´a dataset, including an analysis of how the pretraining data diversity affects low-resource performance
2. Measure and report the precision of mined pairs in both English and Yor`ub´a to assess the quality of the unsupervised mining process and its impact on model performance
3. Test the model's robustness to varying levels of noise in the mined pairs by introducing controlled amounts of noise and measuring performance degradation