---
ver: rpa2
title: No Data Augmentation? Alternative Regularizations for Effective Training on
  Small Datasets
arxiv_id: '2309.01694'
source_url: https://arxiv.org/abs/2309.01694
tags:
- training
- learning
- data
- networks
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of alternative regularization
  strategies for supervised learning on small image classification datasets. The authors
  argue that aggressive data augmentation and generative models, while powerful, have
  limitations such as domain-specific biases and high computational cost.
---

# No Data Augmentation? Alternative Regularizations for Effective Training on Small Datasets

## Quick Facts
- arXiv ID: 2309.01694
- Source URL: https://arxiv.org/abs/2309.01694
- Reference count: 40
- This paper demonstrates that careful hyperparameter selection, larger model widths, longer training schedules, and removing momentum can achieve competitive results on small image datasets without aggressive data augmentation or generative models.

## Executive Summary
This paper challenges the prevailing wisdom that aggressive data augmentation and generative models are necessary for effective training on small image datasets. Instead, the authors propose a set of alternative regularization strategies focused on optimization hyperparameters, model size, and training schedule length. By developing a heuristic for selecting optimal learning rate and weight decay pairs based on parameter norm dynamics, they achieve 66.5% test accuracy on the ciFAIR-10 benchmark (1% of CIFAR-10) using a WideResNet-16-22 architecture. This performance matches state-of-the-art methods that rely on strong data augmentation or generative models, demonstrating that careful regularization can be as effective as data augmentation in small-data regimes.

## Method Summary
The authors investigate alternative regularization strategies for supervised learning on small image classification datasets. They focus on three key aspects: hyperparameter selection via parameter norm dynamics, removal of momentum from optimization, and scaling up model width with longer training schedules. The proposed method uses vanilla cross-entropy classification with minimal data augmentation (random horizontal flips and translations) on WideResNet architectures. A novel heuristic selects learning rate and weight decay pairs based on the norm of model parameters after 50 training iterations, eliminating the need for validation sets. The approach is tested on the ciFAIR-10 benchmark (1% of CIFAR-10 training data) with model widths ranging from WRN-16-1 to WRN-16-22 and training lengths from 25k to 75k iterations.

## Key Results
- Achieves 66.5% test accuracy on ciFAIR-10 benchmark (1% of CIFAR-10) using WRN-16-22
- Matches state-of-the-art performance without aggressive data augmentation or generative models
- Demonstrates monotonic relationship between parameter norm and generalization across different model scales
- Shows that larger weight decay/learning rate products lead to faster parameter norm decay and better generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger weight decay and learning rate products create faster parameter norm decay, leading to more chaotic early training dynamics and better generalization.
- Mechanism: In scale-invariant networks, the weight decay step shrinks the parameter norm while the gradient step expands it. Higher weight decay/learning rate products cause a sharper initial decay of the parameter norm, leading to noisier optimization trajectories that explore the loss landscape more broadly and avoid poor minima.
- Core assumption: The effective learning rate scales with the product αλ, and this product determines the relative strength of weight decay vs gradient updates.
- Evidence anchors:
  - [section] "However, not all the models generalize the same along a constant αλ since the gradient update is proportional to only α, not αλ."
  - [section] "In Fig. 7, we provide additional insights regarding the evolution of the parameters norm and generalization through the test accuracy in the case of WRN-16-10 trained for 25k iterations."
  - [corpus] Weak - the corpus neighbors focus on data augmentation and generative models, not on the relationship between parameter norm dynamics and generalization.
- Break condition: If the model is not scale-invariant (e.g., has non-BN layers or a final classification head that breaks scale-invariance), the mechanism may not hold.

### Mechanism 2
- Claim: Momentum introduces additional complexity to the training dynamics, making HPs selection more noisy and potentially less effective in small-data regimes.
- Mechanism: Momentum incorporates past gradients into the current update, creating a more complex trajectory through the loss landscape. In small-data regimes, this additional complexity can make it harder to find optimal HPs, as the best HPs on a small validation set may not transfer to the test set.
- Core assumption: Momentum adds noise to the optimization trajectory, which is particularly problematic when the validation set is small and noisy.
- Evidence anchors:
  - [section] "The models with the smallest norm are the ones that generalize better by scoring lower testing losses. The monotonicity increases as the model size and training length increase."
  - [section] "Momentum introduces some additional noise, potentially attributable to the more complex dynamics of incorporating previous gradients. However, the monotonic relationship remains reliable also if µ = 0.9."
  - [corpus] Weak - the corpus neighbors do not discuss momentum or its impact on HPs selection in small-data regimes.
- Break condition: If the dataset is large enough that the validation set is representative, momentum may still provide benefits by accelerating convergence.

### Mechanism 3
- Claim: Longer training schedules improve generalization in small-data regimes by allowing the model to explore the loss landscape more thoroughly and find better minima.
- Mechanism: With limited data, each epoch covers fewer unique samples, so the effective number of parameter updates is lower. Increasing the training schedule length compensates for this by providing more opportunities for the model to refine its parameters and escape shallow minima.
- Core assumption: The number of training updates (steps) is more important than the number of epochs in small-data regimes.
- Evidence anchors:
  - [section] "The limited data in small-sample scenarios bears the risk of under-training networks if the number of epochs and batch size are directly imported from the default setups with more data."
  - [section] "At all model scales, the tested networks improve their testing accuracy (see Fig. 5)."
  - [corpus] Weak - the corpus neighbors focus on data augmentation and generative models, not on the impact of training schedule length.
- Break condition: If the model overfits due to the increased training length, the benefits may be negated.

## Foundational Learning

- Concept: Scale-invariance in neural networks
  - Why needed here: Understanding scale-invariance is crucial for interpreting the relationship between weight decay, learning rate, and parameter norm dynamics.
  - Quick check question: In a scale-invariant network, how does scaling the weights by a constant factor affect the output activations and loss function?

- Concept: Hyperparameter selection in small-data regimes
  - Why needed here: The paper highlights the challenges of selecting optimal HPs when the validation set is small and noisy, and proposes an alternative approach based on parameter norm dynamics.
  - Quick check question: Why might the best HPs on a small validation set not transfer to the test set in a small-data regime?

- Concept: Model scaling and generalization
  - Why needed here: The paper explores the impact of increasing model width on generalization performance in small-data regimes, showing that larger models can generalize better if properly regularized.
  - Quick check question: How does increasing the width of a WideResNet affect its capacity and potential for overfitting in a small-data regime?

## Architecture Onboarding

- Component map:
  WideResNet-16 architecture with configurable width -> SGD optimizer with configurable learning rate, weight decay, and momentum -> Cosine annealing learning rate schedule -> Minimal data augmentation (random horizontal flips and translations) -> ciFAIR-10 dataset (1% of CIFAR-10 training set)

- Critical path:
  1. Configure WideResNet-16 with desired width
  2. Initialize SGD optimizer with learning rate, weight decay, and momentum
  3. Set up cosine annealing learning rate schedule
  4. Load ciFAIR-10 dataset and apply minimal data augmentation
  5. Train model for specified number of steps
  6. Monitor parameter norm and test accuracy during training

- Design tradeoffs:
  - Larger model widths improve capacity but increase risk of overfitting in small-data regimes
  - Higher weight decay/learning rate products lead to faster parameter norm decay and potentially better generalization, but may also cause training instability
  - Longer training schedules improve generalization but increase computational cost and risk of overfitting

- Failure signatures:
  - High training accuracy but low test accuracy: model is overfitting
  - Low training accuracy: model is underfitting or learning rate is too low
  - Parameter norm not decreasing during training: weight decay is too low or learning rate is too high

- First 3 experiments:
  1. Train WideResNet-16-1 with various learning rate/weight decay combinations and monitor parameter norm and test accuracy to identify optimal HPs.
  2. Repeat experiment 1 with momentum=0 to assess its impact on HPs selection and generalization.
  3. Scale up model width to WideResNet-16-3 and repeat experiment 1 to evaluate the effect of increased capacity on generalization.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but raises implicit questions about the generalizability of its findings to other architectures, datasets, and domains.

## Limitations
- The parameter-norm-based hyperparameter selection heuristic relies on scale-invariance assumptions that may not hold across all network architectures
- The comparison against data augmentation and generative models is limited to specific benchmark settings
- The proposed approach's effectiveness in other domains (e.g., medical imaging, satellite imagery) remains untested

## Confidence
- **High confidence**: The empirical observation that larger weight decay and learning rate products lead to faster parameter norm decay during training
- **Medium confidence**: The effectiveness of the parameter-norm-based hyperparameter selection heuristic for model selection
- **Medium confidence**: The claim that removing momentum and scaling up model width improves generalization in small-data regimes

## Next Checks
1. Test the parameter-norm-based hyperparameter selection heuristic across different network architectures (e.g., ResNet, DenseNet) to verify its generalizability beyond WideResNets.

2. Evaluate the proposed approach on other small-data image classification benchmarks (e.g., small subsets of CIFAR-100, ImageNet-10) to assess domain transferability.

3. Conduct ablation studies to quantify the individual contributions of each proposed technique (no momentum, larger width, longer training) to the overall performance gains.