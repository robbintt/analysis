---
ver: rpa2
title: 'Fumbling in Babel: An Investigation into ChatGPT''s Language Identification
  Ability'
arxiv_id: '2311.09696'
source_url: https://arxiv.org/abs/2311.09696
tags:
- language
- languages
- chatgpt
- name
- than
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates ChatGPT\u2019s ability to identify languages.\
  \ The authors compile Babel-670, a benchmark of 670 languages, and evaluate ChatGPT\u2019\
  s performance on language identification under zero- and few-shot settings, with\
  \ and without a provided label set."
---

# Fumbling in Babel: An Investigation into ChatGPT's Language Identification Ability

## Quick Facts
- arXiv ID: 2311.09696
- Source URL: https://arxiv.org/abs/2311.09696
- Reference count: 31
- Key outcome: ChatGPT performs poorly on language identification, especially for African languages, compared to smaller fine-tuned tools

## Executive Summary
This paper investigates ChatGPT's ability to identify 670 languages through a comprehensive benchmark called Babel-670. The study evaluates ChatGPT's performance across different settings including zero-shot, few-shot, and varying label set sizes, comparing language name prompts versus language code prompts. Results show ChatGPT performs significantly worse than specialized language identification tools, particularly for African languages, with many languages receiving zero F1 scores. The study also reveals that ChatGPT performs better with language names than codes and shows uneven geographical performance distribution.

## Method Summary
The researchers compiled Babel-670 by merging datasets from AmericasNLP2022, AfroLID, and FLORES-200, resulting in 670 languages with 250-10000 examples each. They designed 36 prompt templates testing language name prompts (LNP) and language code prompts (LCP) across three difficulty levels and three shot numbers. The evaluation pipeline involved API calls to GPT-3.5 and GPT-4, postprocessing of textual outputs to extract predictions, and assessment using accuracy and macro-averaged F1 scores. Performance was compared against eight baseline language identification tools.

## Key Results
- ChatGPT's average F1 score across 670 languages was only 0.065, with 47.8% of languages receiving zero F1 scores
- Performance was significantly worse for African languages compared to other regions
- Language name prompts (LNP) outperformed language code prompts (LCP) across all settings
- GPT-3.5 showed better overall performance than GPT-4 in language identification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT knows language names better than language codes.
- Mechanism: Language names are more frequent in training data, leading to better embeddings and prediction accuracy.
- Core assumption: ChatGPT was trained on human text where language names are more prevalent than codes.
- Evidence anchors:
  - [abstract]: "We argue that when it comes to language identification, ChatGPT knows language names better than language codes."
  - [section 5.1.1]: Performance comparison shows LNP outperforms LCP across settings.
  - [corpus]: No direct evidence; assumption based on human language usage patterns.
- Break condition: If ChatGPT was trained on datasets with disproportionate language code representation, this mechanism fails.

### Mechanism 2
- Claim: Performance improves with smaller label sets due to reduced candidate space.
- Mechanism: With fewer labels, random guessing probability increases; for uncertain cases, fewer candidates improve confidence.
- Core assumption: ChatGPT's predictions are influenced by the size of the provided label set.
- Evidence anchors:
  - [section 5.1.2]: "The difference between difficulty levels is the provision and size of a label set... Performance easy > medium > hard."
  - [section 5.1.2]: Discusses probabilistic guessing and confidence improvement with fewer candidates.
  - [corpus]: No direct corpus evidence; mechanism inferred from experimental design.
- Break condition: If ChatGPT's predictions are independent of label set size, this mechanism breaks.

### Mechanism 3
- Claim: ChatGPT groups dialects under a common language name due to shared vocabulary and linguistic features.
- Mechanism: Shared linguistic features cause ChatGPT to predict the more general language name rather than the specific dialect.
- Core assumption: ChatGPT recognizes linguistic similarities and defaults to the broader category.
- Evidence anchors:
  - [section B.1]: "ChatGPT tends to be conservative for languages that belong to a language group by predicting only its group name."
  - [section B.2]: Explains the grouping of dialects under a common language name.
  - [corpus]: No direct corpus evidence; based on observed prediction patterns.
- Break condition: If ChatGPT distinguishes dialects with high accuracy, this grouping mechanism fails.

## Foundational Learning

- Concept: Language identification (LID) task
  - Why needed here: Understanding LID is crucial for interpreting the benchmarks and evaluation methods used in the study.
  - Quick check question: What is the primary goal of language identification in NLP?

- Concept: Zero-shot and few-shot learning
  - Why needed here: The study evaluates ChatGPT's performance under these settings, which are key to understanding its capabilities.
  - Quick check question: How do zero-shot and few-shot learning differ in terms of the data provided to the model?

- Concept: Evaluation metrics (accuracy and F1 score)
  - Why needed here: These metrics are used to assess ChatGPT's performance, and understanding them is essential for interpreting the results.
  - Quick check question: What is the difference between accuracy and F1 score, and when is each metric more appropriate?

## Architecture Onboarding

- Component map:
  - Babel-670 dataset -> ChatGPT models (GPT-3.5/GPT-4) -> Evaluation pipeline (prompts, postprocessing, metrics) -> Error analysis tools

- Critical path:
  1. Data curation from multiple sources
  2. Prompt engineering for different settings (LNP/LCP, difficulty levels, shot numbers)
  3. API interaction with ChatGPT
  4. Postprocessing of textual outputs
  5. Evaluation using accuracy and F1 scores
  6. Error analysis and geographical mapping

- Design tradeoffs:
  - Language name vs. code prompts: Names are easier for ChatGPT but less standardized
  - Difficulty levels: Balance between realistic settings and controlled experiments
  - Evaluation methods: Exact-match vs. alias-dialect-accepting to account for generative model differences

- Failure signatures:
  - Zero F1 scores for many languages indicate complete lack of knowledge
  - Geographical bias in performance, especially for African languages
  - Inconsistent performance between GPT-3.5 and GPT-4

- First 3 experiments:
  1. Evaluate ChatGPT on a small subset of Babel-670 to establish baseline performance
  2. Test the impact of providing different sizes of label sets on accuracy
  3. Compare performance on high-resource vs. low-resource languages to identify patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications could improve ChatGPT's language identification performance, particularly for African languages?
- Basis in paper: [explicit] The authors note that ChatGPT performs poorly on African languages compared to smaller, fine-tuned tools and conclude that current LLMs would benefit from further development to serve diverse communities.
- Why unresolved: The paper only identifies the performance gap but does not explore potential solutions or modifications to address it.
- What evidence would resolve it: Experiments comparing ChatGPT's performance before and after implementing specific architectural changes or additional training data focused on underrepresented languages.

### Open Question 2
- Question: How does ChatGPT's language identification ability change when exposed to multilingual code-switched text versus monolingual text?
- Basis in paper: [inferred] The authors mention that some languages exhibit code-mixing, which may contribute to identification challenges, but do not systematically test this scenario.
- Why unresolved: The paper does not include experiments specifically designed to test performance on code-switched data.
- What evidence would resolve it: Comparative performance metrics on datasets containing code-switched examples versus monolingual examples across various language pairs.

### Open Question 3
- Question: To what extent does ChatGPT's language identification performance correlate with the amount of digital content available in each language?
- Basis in paper: [explicit] The authors note that ChatGPT performs better on high-resource languages and that some low-resource languages unexpectedly achieve high F1 scores, suggesting potential training data inclusion.
- Why unresolved: The paper observes performance patterns but does not analyze the relationship between digital resource availability and identification accuracy.
- What evidence would resolve it: Correlation analysis between performance metrics and measures of digital content availability (e.g., web pages, social media presence) for each language.

## Limitations

- The benchmark's coverage of 670 languages, while extensive, may not be uniformly representative of global linguistic diversity
- The evaluation relies on exact-match and alias-dialect-accepting metrics which may not fully capture generative model nuances
- Postprocessing methodology for extracting predictions from ChatGPT's textual responses introduces potential variability

## Confidence

- **High Confidence**: The observation that ChatGPT performs poorly on African languages compared to other regions is well-supported by the data
- **Medium Confidence**: The claim about performance improvement with smaller label sets is supported by experimental results but the underlying mechanism remains somewhat speculative
- **Medium Confidence**: The grouping of dialects under common language names is observed in results but the exact linguistic features ChatGPT uses are not explicitly verified

## Next Checks

1. **Alias/Dialect Mapping Validation**: Implement and test the exact alias-dialect-accepting evaluation rules using the full Ethnologue/langcodes dataset to verify that dialectal variations are correctly mapped to their parent languages across all 670 languages in the benchmark.

2. **Cross-Model Consistency Test**: Run the same evaluation pipeline on multiple instances of GPT-3.5 and GPT-4 to measure variance in performance, particularly focusing on languages with historically inconsistent results to determine if observed failures are model-specific or systematic.

3. **Resource-Based Performance Analysis**: Conduct a controlled experiment comparing ChatGPT's performance on languages stratified by available training data (high-resource vs. low-resource), using external corpus statistics to verify whether the performance gap correlates with actual training data availability rather than just linguistic complexity.