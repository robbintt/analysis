---
ver: rpa2
title: Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure
  Network
arxiv_id: '2307.13938'
source_url: https://arxiv.org/abs/2307.13938
tags:
- learning
- dssn
- contrastive
- data
- unlabeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles semi-supervised semantic segmentation by introducing
  a dual-level Siamese structure network (DSSN) that performs pixel-wise contrastive
  learning across both low-level image space and high-level feature space. The method
  uses strong augmentations to create multiple views and applies contrastive losses
  to align positive pairs, fully exploiting unlabeled data.
---

# Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network

## Quick Facts
- arXiv ID: 2307.13938
- Source URL: https://arxiv.org/abs/2307.13938
- Reference count: 36
- Key outcome: Achieves 80.61% mIoU on PASCAL VOC 2012 with only 732 labeled images, surpassing prior SOTA methods

## Executive Summary
This paper introduces a Dual-Level Siamese Structure Network (DSSN) for semi-supervised semantic segmentation that leverages pixel-wise contrastive learning at both image and feature levels, combined with a class-aware pseudo-label selection strategy. The method uses strong augmentations to create multiple views of unlabeled data and applies contrastive losses to align corresponding pixels, fully exploiting unlabeled data. By implementing class-specific thresholds for pseudo-label selection, DSSN significantly improves performance on long-tailed classes compared to fixed-threshold methods, achieving state-of-the-art results on PASCAL VOC 2012 and Cityscapes datasets, particularly under low labeled data scenarios.

## Method Summary
DSSN employs a dual-level Siamese structure that performs pixel-wise contrastive learning across both low-level image space and high-level feature space using strong augmentations to create multiple views. The method uses weak-to-strong supervision with an EMA-updated teacher model to generate pseudo-labels for strongly augmented views, combined with a class-aware pseudo-label selection strategy that selects high-confidence pixels per class based on scaled maximum prediction confidences. The training loss combines supervised loss on labeled data with dual-level contrastive losses and weak-to-strong consistency regularization, optimized using SGD with polynomial learning rate decay.

## Key Results
- Achieves 80.61% mIoU on PASCAL VOC 2012 with only 732 labeled images
- Outperforms existing methods by significant margins, especially under low labeled data scenarios
- Ablation studies confirm effectiveness of both dual-level contrastive learning and class-aware pseudo-label strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pixel-wise contrastive learning at both image-level and feature-level maximizes the utilization of unlabeled data.
- Mechanism: DSSN creates multiple strong-augmented views of unlabeled images and their latent features, then applies pixel-wise contrastive loss to align corresponding pixels across views.
- Core assumption: Strong augmentations create sufficiently diverse views while preserving semantic content.
- Evidence anchors: [abstract] "By aligning positive pairs with a pixel-wise contrastive loss using strong augmented views in both low-level image space and high-level feature space"
- Break condition: If augmentations destroy semantic content or are too weak to create meaningful variation.

### Mechanism 2
- Claim: Class-aware pseudo-label selection improves performance on long-tailed classes by adapting thresholds per class.
- Mechanism: For each class, DSSN computes the maximum prediction confidence across all pixels, scales it by a factor (r%), and uses this class-specific threshold to select high-confidence pixels.
- Core assumption: Different classes have inherently different confidence distributions in pseudo-label generation.
- Evidence anchors: [abstract] "our strategy selects the top high-confidence prediction of the weak view for each class to generate pseudo labels"
- Break condition: If class-wise maximum confidences are uniformly low or if class imbalance is extreme.

### Mechanism 3
- Claim: Weak-to-strong supervision with EMA teacher model provides stable pseudo-labels.
- Mechanism: A weakly augmented view generates pseudo-labels using an EMA-updated teacher model, which then supervises multiple strongly augmented views from the student model.
- Core assumption: EMA-smoothed teacher predictions are more reliable than single-step predictions.
- Evidence anchors: [section] "The pseudo labels of ùíöùëô ùë§ and ùíö‚Ñéùë§ are calculated...where the parameters (ùúÉ ‚Ä≤, ùúë‚Ä≤) of the teacher model are updated from the student model (ùúÉ, ùúë) by the exponential moving average (EMA) method"
- Break condition: If EMA updates are too slow or too fast relative to training dynamics.

## Foundational Learning

- Concept: Semi-supervised learning paradigms (pseudo-labeling, consistency regularization, contrastive learning)
  - Why needed here: DSSN combines all three paradigms to maximize unlabeled data utility
  - Quick check question: What is the key difference between pseudo-labeling and consistency regularization?

- Concept: Siamese network architecture and dual-branch design
  - Why needed here: DSSN uses dual-level Siamese structure to process both low-level image and high-level feature augmentations
  - Quick check question: How does a Siamese network differ from a standard encoder-decoder architecture?

- Concept: Exponential Moving Average (EMA) in teacher-student frameworks
  - Why needed here: EMA provides stable teacher model for pseudo-label generation in weak-to-strong supervision
  - Quick check question: What hyperparameter controls the EMA update speed, and how does it affect training stability?

## Architecture Onboarding

- Component map: Input ‚Üí Weak augmentation branch ‚Üí EMA teacher ‚Üí Pseudo-label generation ‚Üí Class-aware selection ‚Üí Strong augmentation branch ‚Üí Dual-level contrastive learning ‚Üí Supervised loss (labeled data) ‚Üí Total loss ‚Üí Parameter update
- Critical path: Weak augmentation ‚Üí EMA teacher ‚Üí Pseudo-label generation ‚Üí Strong augmentation supervision (weak-to-strong consistency)
- Design tradeoffs: Dual-level contrastive learning adds computational overhead but improves unlabeled data utilization; class-aware selection adds per-class computation but improves long-tailed class performance
- Failure signatures: Contrastive loss dominates or vanishes (check loss ratios); pseudo-label quality degrades over time (monitor confidence statistics); EMA teacher diverges from student (check parameter similarity)
- First 3 experiments:
  1. Verify dual-branch architecture produces sensible outputs with random weights
  2. Test class-aware pseudo-label selection by running on a small dataset and inspecting per-class confidence distributions
  3. Validate contrastive learning by checking that positive pairs have lower contrastive loss than negative pairs in ablation setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed dual-level Siamese structure compare to single-level contrastive learning in terms of robustness to different types of data augmentations and perturbations?
- Basis in paper: [inferred] The paper proposes a dual-level structure but does not explicitly compare robustness to different augmentation types.
- Why unresolved: The paper demonstrates improved performance but does not systematically analyze how the dual-level approach specifically handles various augmentation types.
- What evidence would resolve it: Controlled experiments comparing the proposed method with single-level contrastive learning under varying augmentation strategies and noise conditions.

### Open Question 2
- Question: What is the optimal class-wise threshold ratio r for the CPLG strategy across different dataset characteristics?
- Basis in paper: [explicit] The paper mentions that r is set to 96% in experiments but does not explore how this parameter should be tuned for different dataset properties.
- Why unresolved: The fixed parameter setting used in experiments may not generalize to datasets with significantly different class distributions or sizes.
- What evidence would resolve it: Comprehensive experiments varying r across multiple datasets with different class imbalance levels and sizes.

### Open Question 3
- Question: How does the proposed DSSN method perform when applied to 3D semantic segmentation tasks?
- Basis in paper: [inferred] The method is demonstrated on 2D image datasets, but the authors do not discuss applicability to 3D data.
- Why unresolved: The paper focuses on 2D image segmentation, and effectiveness for 3D data remains unexplored.
- What evidence would resolve it: Applying DSSN to 3D segmentation datasets and comparing performance against state-of-the-art 3D semi-supervised methods.

## Limitations

- Exact implementation details of strong augmentation pipeline (AugLùë† and AugHùë†) are not fully specified
- Choice of ùúèlow=0.92 and ùëü=96% for CPLG appears somewhat arbitrary without clear justification
- Relative importance of dual-level contrastive learning versus class-aware pseudo-labeling remains unclear

## Confidence

- High confidence: Core architectural contributions (dual-level Siamese structure, weak-to-strong supervision with EMA teacher) are well-documented and experimentally validated
- Medium confidence: The effectiveness of class-aware pseudo-label selection is demonstrated, but the specific hyperparameter choices lack rigorous justification
- Medium confidence: Contrastive learning mechanisms are sound in principle, but implementation details for pixel-wise similarity computation could affect reproducibility

## Next Checks

1. Implement the Gaussian similarity function from Eq. (9) and verify that positive pixel pairs consistently achieve higher similarity scores than negative pairs during training
2. Test the sensitivity of CPLG performance to the ùúèlow and ùëü hyperparameters by running controlled experiments across a range of values
3. Validate the EMA teacher stability by monitoring the parameter distance between student and teacher models throughout training, ensuring the EMA update rate (ùõº=0.996) maintains appropriate teacher-student alignment