---
ver: rpa2
title: 'Soft Decomposed Policy-Critic: Bridging the Gap for Effective Continuous Control
  with Discrete RL'
arxiv_id: '2308.10203'
source_url: https://arxiv.org/abs/2308.10203
tags:
- policy
- continuous
- discrete
- action
- decomposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying discrete RL to continuous
  control tasks, where the high-dimensional continuous action space leads to a dimensional
  explosion problem. To overcome this limitation, the authors propose the Soft Decomposed
  Policy-Critic (SDPC) architecture, which combines soft RL and actor-critic techniques
  with discrete RL methods.
---

# Soft Decomposed Policy-Critic: Bridging the Gap for Effective Continuous Control with Discrete RL

## Quick Facts
- **arXiv ID**: 2308.10203
- **Source URL**: https://arxiv.org/abs/2308.10203
- **Reference count**: 40
- **Primary result**: SDPC achieves comparable or superior performance to SAC while requiring less than half the training steps

## Executive Summary
This paper addresses the challenge of applying discrete reinforcement learning (RL) to continuous control tasks, where the high-dimensional continuous action space leads to a dimensional explosion problem. The authors propose the Soft Decomposed Policy-Critic (SDPC) architecture, which combines soft RL and actor-critic techniques with discrete RL methods. SDPC discretizes each action dimension independently and employs a shared critic network to maximize the soft Q-function. Through extensive experiments on various continuous control tasks, including Mujoco's Humanoid and Box2d's BipedalWalker, the authors demonstrate that their approach outperforms state-of-the-art continuous RL algorithms.

## Method Summary
The SDPC architecture discretizes each continuous action dimension into N discrete actions (typically N=20). It supports two policy types: decomposed actors leading to the Soft Decomposed Actor-Critic (SDAC) algorithm, and decomposed Q-networks leading to the Soft Decomposed-Critic Q (SDCQ) algorithm. Both algorithms use continuous critic networks to evaluate discrete actions, enabling offline experience replay for improved training efficiency. The method discretizes each action dimension independently, resulting in a total of M×N discrete actions instead of N^M, where M is the number of action dimensions.

## Key Results
- SDAC achieves comparable or superior performance to the continuous Soft Actor-Critic (SAC) algorithm
- SDCQ converges with fewer training steps (less than half of those required by SAC) and achieves the highest final performance
- The approach outperforms state-of-the-art continuous RL algorithms on various continuous control tasks
- The method demonstrates improved sample efficiency through offline experience replay

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SDPC's decomposed discretization eliminates the exponential blow-up in discrete action space while retaining representational power for continuous control.
- Mechanism: Each continuous action dimension is discretized independently into N discrete actions. The total number of discrete actions becomes M×N instead of N^M, where M is the number of action dimensions.
- Core assumption: Optimal policies can be represented as products of one-dimensional policies, i.e., the action dimensions are sufficiently independent for the policy factorization to hold.
- Evidence anchors:
  - [abstract]: "SDPC discretizes each action dimension independently and employs a shared critic network"
  - [section 3.1]: "The resulting discrete action space Ad_m for the m-th dimension is denoted as {ad_m,1, ad_m,2, ..., ad_m,N}"
  - [corpus]: "Average neighbor FMR=0.505" - weak evidence, no direct support
- Break condition: If action dimensions are strongly coupled (e.g., requiring specific combinations of values across dimensions), the independent discretization will lose critical information and performance will degrade.

### Mechanism 2
- Claim: The continuous critic network enables efficient policy iteration by providing soft Q-values for all discrete actions simultaneously.
- Mechanism: The critic network takes state and continuous action as input and outputs Q(s,a) for any combination of discretized actions. This allows batch computation of Q-values for policy updates.
- Core assumption: The continuous critic can accurately approximate the soft Q-function across the discretized action space without requiring separate Q-networks per dimension.
- Evidence anchors:
  - [abstract]: "SDPC ... employs a shared critic network to maximize the soft Q-function"
  - [section 3.1]: "The critics in SDPC have the same structure as those in TD3... output the corresponding soft Q-function Q(s(t), a(t); θQ)"
  - [corpus]: "Bridging Discrete and Continuous RL: Stable Deterministic Policy Gradient" - suggests theoretical support exists
- Break condition: If the critic network cannot accurately represent the Q-function due to high-dimensional input space or insufficient capacity, policy updates will be based on poor estimates.

### Mechanism 3
- Claim: The bridge between discrete and continuous Q-functions enables offline experience replay, dramatically improving sample efficiency.
- Mechanism: Equation 8 establishes that E_am~πm[Qd(s,am;πm)] = E_a~π[Q(s,a)], allowing the continuous critic to provide targets for discrete policy updates using stored transitions.
- Core assumption: The transition dynamics in the decomposed MDP remain stable enough during policy optimization that offline replay doesn't introduce significant bias.
- Evidence anchors:
  - [section 3.2]: "We provide a proof for Equation(8) in Appendix Appendix C.1" and detailed proof showing the mathematical relationship
  - [section 3.1]: "The key contribution of our SDPC architecture lies in the continuous critic networks"
  - [corpus]: "Proxy Target: Bridging the Gap Between Discrete Spiking Neural Networks and Continuous Control" - similar bridging concept
- Break condition: If the exclusive policies πm change rapidly, the decomposed MDP transitions pm and rewards rm become mismatched with stored experiences, invalidating the bridge equation.

## Foundational Learning

- **Concept: Soft Reinforcement Learning and Maximum Entropy RL**
  - Why needed here: SDPC extends soft RL principles to discrete policies, using entropy regularization to encourage exploration and improve robustness
  - Quick check question: What is the role of the temperature parameter α in soft RL, and how does it differ from its role in Boltzmann exploration policies?

- **Concept: Actor-Critic Architecture and Double Q-learning**
  - Why needed here: SDPC uses continuous critics (similar to TD3) with double Q-networks to reduce overestimation bias, and actor networks for policy updates
  - Quick check question: Why does using the minimum of two Q-networks help reduce overestimation bias in value-based RL?

- **Concept: Policy Gradient Methods and KL Divergence Optimization**
  - Why needed here: SDAC optimizes discrete policies using KL divergence between current policy and exponential of soft Q-values, which is equivalent to policy gradient updates
  - Quick check question: How does minimizing KL divergence between π and exp(Q/α) relate to maximizing expected return plus entropy?

## Architecture Onboarding

- **Component map**: State → Decomposed Policy Network → Discrete Action Selection → Environment → Reward/Next State → Experience Replay Buffer → Critic Update → Policy Update → Temperature Update

- **Critical path**: State → Decomposed Policy Network → Discrete Action Selection → Environment → Reward/Next State → Experience Replay Buffer → Critic Update → Policy Update → Temperature Update

- **Design tradeoffs**:
  - N (discrete actions per dimension): Higher N improves accuracy but increases computational cost and network complexity
  - Separate vs. shared critics: SDPC uses shared critics for efficiency but may struggle with very high-dimensional action spaces
  - Boltzmann vs. cross-entropy policies: SDCQ uses Boltzmann for value-based learning, SDAC uses cross-entropy for direct policy optimization

- **Failure signatures**:
  - Training instability: Check temperature bounds and entropy normalization
  - Poor performance: Verify discrete action coverage and critic accuracy
  - Slow convergence: Ensure proper entropy target and sufficient exploration

- **First 3 experiments**:
  1. Single dimension test: Implement SDPC with M=1, N=10 on InvertedDoublePendulum-v2 to verify basic functionality
  2. Ablation on N: Compare performance with N=10, 20, 50 on Walker2d-v2 to find optimal discretization
  3. Cross-entropy vs Boltzmann: Implement both policy types on Hopper-v2 to compare convergence speed and final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of the number of discrete actions per dimension (N) on the final performance and training efficiency of SDAC and SDCQ?
- Basis in paper: [explicit] The paper mentions that higher values of N tend to lead to more effective performances, but also increase computational complexity. They typically set N to 20 in their evaluations.
- Why unresolved: The paper does not provide a detailed analysis of the trade-off between N and performance, nor does it explore the optimal value of N for different environments.
- What evidence would resolve it: A systematic study of the impact of N on performance and training efficiency across a range of environments, including a comparison of different values of N and their corresponding results.

### Open Question 2
- Question: How does the target entropy (ˆH) affect the exploration rate and performance of SDAC and SDCQ?
- Basis in paper: [explicit] The paper mentions that the target entropy determines the exploration rate of the algorithm and that different environments may have different preferences for exploration rate. They tested different target entropy values for SDAC and found that ˆH = 1 leads to exceeded exploration rate and training failures.
- Why unresolved: The paper does not provide a comprehensive analysis of the relationship between target entropy and performance across different environments and algorithms.
- What evidence would resolve it: A detailed study of the impact of target entropy on exploration rate and performance for both SDAC and SDCQ across a range of environments, including a comparison of different target entropy values and their corresponding results.

### Open Question 3
- Question: How effective is the normalized importance sampling in SDCQ compared to other importance sampling methods?
- Basis in paper: [explicit] The paper mentions that normalized importance sampling is used in SDCQ to correct biases in off-policy multi-step TD and improve stability. They provide ablation studies showing the effectiveness of normalized importance sampling in SDCQ.
- Why unresolved: The paper does not compare the performance of normalized importance sampling with other importance sampling methods, such as importance sampling with truncation or self-normalized importance sampling.
- What evidence would resolve it: A comparison of the performance of SDCQ with different importance sampling methods, including normalized importance sampling, importance sampling with truncation, and self-normalized importance sampling, across a range of environments.

## Limitations
- The assumption that action dimensions can be decomposed independently may not hold for tasks requiring strong inter-dimensional coupling
- The adaptive temperature mechanism's stability across diverse environments remains unverified
- The proof for the bridge equation relies on decomposing the MDP, but the practical impact of approximation errors during rapid policy changes is not thoroughly analyzed

## Confidence

- **High confidence**: The mechanism of independent discretization reducing action space complexity (Mechanism 1) is well-supported by the mathematical formulation and experimental results showing improved sample efficiency
- **Medium confidence**: The continuous critic's ability to accurately represent Q-values across discretized actions (Mechanism 2) is supported by ablation studies but lacks rigorous theoretical analysis of approximation error bounds
- **Medium confidence**: The bridge equation enabling offline replay (Mechanism 3) is mathematically proven but assumes stable decomposed MDP dynamics, which may not hold in practice

## Next Checks

1. Test SDPC on tasks with known action dependencies (e.g., manipulator control with joint coupling) to quantify performance degradation when the decomposition assumption breaks
2. Conduct sensitivity analysis on temperature adaptation parameters across environments to identify conditions where the entropy control mechanism becomes unstable
3. Compare the approximation error of continuous critics against discrete Q-networks for various discretization granularities to establish accuracy bounds