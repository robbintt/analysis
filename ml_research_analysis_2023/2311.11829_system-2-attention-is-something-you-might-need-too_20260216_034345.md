---
ver: rpa2
title: System 2 Attention (is something you might need too)
arxiv_id: '2311.11829'
source_url: https://arxiv.org/abs/2311.11829
tags:
- context
- prompt
- attention
- figure
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models struggle with irrelevant or opinionated input
  context, which can lead to factual errors or biased outputs. System 2 Attention
  (S2A) addresses this by using the model itself to regenerate the input context,
  stripping out irrelevant or biased portions before generating the final response.
---

# System 2 Attention (is something you might need too)

## Quick Facts
- arXiv ID: 2311.11829
- Source URL: https://arxiv.org/abs/2311.11829
- Reference count: 23
- Large language models struggle with irrelevant or opinionated input context, which can lead to factual errors or biased outputs

## Executive Summary
Large language models often incorporate irrelevant or biased information from context into their outputs, leading to factual errors and reduced objectivity. System 2 Attention (S2A) addresses this by using the model itself to regenerate the input context, explicitly filtering out irrelevant or opinionated portions before generating the final response. This two-step approach leverages the LLM's natural language reasoning capabilities to make hard decisions about what context to keep, improving factuality, objectivity, and accuracy across multiple tasks while maintaining generation quality.

## Method Summary
S2A is a two-step process that first prompts the LLM to regenerate the input context by removing irrelevant portions, then uses this cleaned context to generate the final response. The method employs instruction-tuned LLMs to rewrite the context through zero-shot prompting, creating a binary "hard attention" decision rather than relying on the model's soft attention mechanism. This approach is applied to three tasks: opinion-contaminated TriviaQA questions, math word problems with distractors, and opinion-based argument generation, showing consistent improvements across factuality, objectivity, and accuracy metrics.

## Key Results
- Factuality improves from 62.8% to 80.3% on opinion-contaminated TriviaQA
- Objectivity scores increase from 2.23 to 3.82 on argument generation tasks
- Math accuracy rises from 51.7% to 61.3% on GSM-IC with in-topic distractors

## Why This Works (Mechanism)

### Mechanism 1
Standard soft attention assigns probability mass to repeated tokens and related topics, creating positive feedback loops that amplify spurious correlations. Transformer attention computes similarity scores between query and key vectors, producing continuous weights that distribute attention across the context. These weights tend to assign non-zero probability to tokens appearing in the context, even when irrelevant to the actual reasoning task. Repeated exposure to certain tokens increases their attention weights through self-reinforcing patterns.

### Mechanism 2
System 2 Attention works by using the LLM's natural language reasoning capabilities to explicitly filter the context before applying attention. Instead of relying on the internal attention mechanism to distinguish relevant from irrelevant information, S2A prompts the LLM to regenerate the context, extracting only the useful portions. This creates a new, cleaned context that the attention mechanism then processes. The LLM acts as a reasoning engine to perform the attention decision in natural language space rather than in continuous vector space.

### Mechanism 3
Hard attention decisions (removing context entirely) are more effective than soft attention weights for eliminating spurious correlations. By regenerating the context to only include relevant portions, S2A makes binary decisions about what to keep versus what to discard. This "hard" filtering prevents the attention mechanism from being influenced by irrelevant information, whereas soft attention would still assign some weight to it. The hard decision boundary created by S2A is more robust to spurious correlations than continuous weighting.

## Foundational Learning

- **Transformer attention mechanism and its limitations**: Understanding why standard attention fails is crucial for grasping why S2A is necessary. The mechanism explains how soft attention can assign weight to irrelevant context. Quick check: Why does standard transformer attention tend to include irrelevant context in its latent representations?

- **Zero-shot prompting and instruction-following in LLMs**: S2A relies on the LLM's ability to follow instructions to regenerate context. This requires understanding how to prompt LLMs effectively. Quick check: How does the S2A prompt structure encourage the LLM to separate relevant from irrelevant context?

- **Chain-of-thought reasoning and its relationship to context generation**: S2A involves generating intermediate context representations, similar to how chain-of-thought generates intermediate reasoning steps. Understanding this relationship helps see S2A as part of a broader class of LLM reasoning techniques. Quick check: How does S2A differ from chain-of-thought reasoning in terms of what is being generated?

## Architecture Onboarding

- **Component map**: Input context → S2A prompt generator → LLM step 1 → Regenerated context → LLM step 2 → Final response
- **Critical path**: 1. Receive original input context, 2. Generate S2A prompt (step 1), 3. Run LLM on S2A prompt to get regenerated context, 4. Apply post-processing to cleaned context, 5. Generate final response using regenerated context (step 2), 6. Evaluate output using GPT-4
- **Design tradeoffs**: Computation cost (S2A requires two LLM calls versus one), context preservation (risk of losing important information when filtering), prompt engineering (effectiveness depends heavily on prompt design), model capability (weaker models may struggle with context regeneration task)
- **Failure signatures**: Performance degradation when regenerated context removes important information, inconsistency across different types of opinion/sentiment in prompts, increased computation time due to two-step process, potential for the LLM to misinterpret the filtering task
- **First 3 experiments**: 1. Compare baseline vs S2A performance on TriviaQA with opinion-contaminated questions, 2. Test S2A variants (keep original, no separation, not instructed) to identify optimal configuration, 3. Evaluate S2A on GSM-IC with in-topic distractors to measure effectiveness on math word problems

## Open Questions the Paper Calls Out

### Open Question 1
How does System 2 Attention (S2A) perform on tasks outside of QA, math word problems, and longform generation? The paper only reports experiments on these three tasks. The authors mention future work exploring other tasks but do not provide any results. Experiments applying S2A to a wider range of tasks like code generation, summarization, translation, etc. and comparing performance to baseline methods would resolve this.

### Open Question 2
What is the impact of different prompt templates and optimization strategies for S2A? The paper uses zero-shot prompting with fixed templates. It mentions other approaches like fine-tuning, RL, and alternative prompting could optimize S2A further but does not explore them. Experiments varying the prompt templates and comparing to other optimization strategies like fine-tuning and RL, measuring the impact on S2A performance, would resolve this.

### Open Question 3
How does S2A compare to other methods for improving LLM factuality and reducing sycophancy? The paper compares S2A to baseline LLM performance but does not compare to other proposed methods like Constitutional AI, Self-ask, Chain-of-Verification, etc. Experiments comparing S2A to these other methods on the same tasks and datasets used in the paper, measuring relative improvements in factuality and reduction in sycophancy, would resolve this.

## Limitations
- The approach assumes LLMs can reliably distinguish relevant from irrelevant context through zero-shot prompting, which may not generalize to all domains
- Computational overhead of requiring two LLM calls could be prohibitive for real-time applications
- Evaluation relies on GPT-4 as a judge for factuality and objectivity, introducing potential subjectivity

## Confidence

**High Confidence**: The experimental results showing improved performance on TriviaQA, GSM-IC, and argument generation tasks are well-documented and reproducible.

**Medium Confidence**: The mechanism explanation for why standard attention fails (positive feedback loops from repeated tokens) is plausible but not rigorously proven.

**Low Confidence**: The claim that S2A can be applied to any LLM without modification may overstate the method's generality.

## Next Checks

1. **Cross-model validation**: Test S2A across multiple LLM architectures (GPT-4, Claude, open-source models) to verify that the improvements in factuality and objectivity generalize beyond LLaMA-2-70B-chat.

2. **Ablation study on prompt design**: Systematically vary the S2A prompt structure by changing instruction phrasing, context separation methods, and generation constraints to identify which prompt components are essential for performance gains.

3. **Human evaluation of context preservation**: Conduct human evaluations to measure whether S2A's context regeneration actually preserves all important information while removing only irrelevant portions.