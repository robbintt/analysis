---
ver: rpa2
title: 'GPT-4 Doesn''t Know It''s Wrong: An Analysis of Iterative Prompting for Reasoning
  Problems'
arxiv_id: '2310.12397'
source_url: https://arxiv.org/abs/2310.12397
tags:
- vertex
- color
- color2
- edge
- color3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of iterative prompting
  for reasoning tasks using large language models (LLMs), specifically focusing on
  graph coloring, a canonical NP-complete problem. The study examines both direct
  and iterative modes, with and without external verification.
---

# GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems

## Quick Facts
- arXiv ID: 2310.12397
- Source URL: https://arxiv.org/abs/2310.12397
- Reference count: 40
- LLMs perform poorly at both solving and verifying graph coloring problems

## Executive Summary
This paper investigates whether iterative prompting with LLM self-critique improves performance on reasoning tasks, using graph coloring as a canonical NP-complete problem. Surprisingly, the study finds that iterative prompting with LLM self-critique performs worse than direct prompting, and that improvements with external verification stem primarily from solution sampling rather than the quality of feedback. The core insight is that LLMs struggle with both solving graph coloring instances and verifying solutions, making them ineffective for iterative reasoning tasks that rely on self-correction.

## Method Summary
The study generates 100 random planar graph instances (10-17 nodes, ~24 edges each) and tests five prompting strategies: direct prompting, iterative prompting with LLM self-critique, and iterative prompting with external verification using different feedback granularities (pass/fail, first error, full error). GPT-4 is used as both generator and verifier, with temperature=0 for deterministic outputs. The experiments measure correctness rates across these strategies to evaluate the effectiveness of iterative prompting for reasoning tasks.

## Key Results
- Iterative prompting with LLM self-critique performs worse than direct prompting
- Iterative prompting with external verification shows modest improvements, but these stem primarily from solution sampling
- LLMs are surprisingly poor at verifying graph colorings, often failing to identify correct solutions
- Minimal improvement from detailed feedback compared to simple "try again" prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative prompting with LLM self-critique performs worse than direct prompting due to lack of solution verification capability
- Mechanism: The LLM fails to recognize correct solutions during the iterative process, generating spurious feedback that misleads subsequent attempts
- Core assumption: LLMs cannot effectively verify their own solutions in iterative settings
- Evidence anchors:
  - [abstract] "they are no better at verifying a solution--and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions"
  - [section] "The problem is caused by the lack of an accurate stopping condition. If the system ever outputs a correct coloring during a backprompting session, we expect a verifier to stop it. However, in the self-verification case, the LLM doing the verification can fail to notice success and instead produce spurious feedback"
  - [corpus] Weak evidence - only 5 related papers found, none directly addressing self-verification limitations
- Break condition: When an external verifier is introduced, eliminating the self-verification failure mode

### Mechanism 2
- Claim: Improvements from iterative prompting with external verification stem primarily from solution sampling rather than feedback quality
- Mechanism: The LLM fortuitously generates correct solutions within the top-k completions, which the external verifier then recognizes
- Core assumption: LLM output distribution contains correct solutions that can be identified by external verification
- Evidence anchors:
  - [abstract] "the observed increase in effectiveness is largely due to the correct solution being fortuitously present in the top-k completions of the prompt"
  - [section] "if the model has fifteen chances to generate a correct answer, it is much more likely to succeed" and "blindfolded guessing does just as well as careful, crafted feedback"
  - [corpus] Moderate evidence - related work discusses sampling techniques but not specifically for graph coloring verification
- Break condition: When the correct solution is not present in the LLM's top-k completions

### Mechanism 3
- Claim: LLM verification capability is surprisingly weak even for simple correctness checks
- Mechanism: LLMs struggle with basic constraint verification, often hallucinating edges or misidentifying vertex colors
- Core assumption: LLMs should be able to perform simple verification tasks based on pattern matching
- Evidence anchors:
  - [abstract] "they are no better at verifying a solution" and "LLMs are bad at solving graph coloring instances"
  - [section] "the model is unwilling to mark almost any answer as correct" and "Fewer than ten percent of cases resulted in a 'correct', 'non-optimal', or 'missing assignment' response"
  - [corpus] Weak evidence - limited related work on LLM verification capabilities
- Break condition: When verification tasks become more complex than simple constraint checking

## Foundational Learning

- Concept: Graph coloring problem fundamentals
  - Why needed here: Understanding the problem structure and constraints is essential for interpreting LLM performance
  - Quick check question: What is the chromatic number of a graph and why is it relevant to graph coloring?

- Concept: Iterative prompting and backprompting techniques
  - Why needed here: The study compares different iterative approaches with varying levels of feedback
  - Quick check question: How does backprompting differ from simple iterative prompting in terms of information flow?

- Concept: LLM output sampling and temperature effects
  - Why needed here: The study explores how sampling multiple outputs affects solution discovery
  - Quick check question: How does increasing temperature affect the diversity of LLM outputs?

## Architecture Onboarding

- Component map:
  Prompt Generator -> Large Language Model (GPT-4) -> External Verifier -> Backprompt Generator

- Critical path:
  1. Generate prompt from graph instance
  2. LLM generates coloring solution
  3. External verifier checks solution correctness
  4. If incorrect, generate backprompt with feedback
  5. Repeat until correct solution found or iteration limit reached

- Design tradeoffs:
  - Self-verification vs external verification: Speed vs accuracy
  - Feedback specificity: Minimal "try again" vs detailed error information
  - Iteration limits: Balance between thoroughness and computational cost

- Failure signatures:
  - LLM fails to recognize correct solutions during self-verification
  - LLM generates spurious feedback about non-existent errors
  - Minimal improvement from detailed feedback compared to simple "try again"

- First 3 experiments:
  1. Run direct prompting baseline without any backprompting
  2. Implement iterative prompting with external verifier using minimal feedback
  3. Compare results with multiple LLM outputs sampled at different temperatures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do self-critique and iterative prompting strategies improve LLM performance on reasoning tasks compared to simply generating multiple candidate solutions?
- Basis in paper: [explicit] The paper investigates whether iterative prompting with LLM self-critique improves performance compared to direct prompting and generating multiple solutions.
- Why unresolved: The paper shows that iterative prompting with self-critique performs worse than direct prompting, and that generating multiple solutions with an external verifier performs similarly to iterative prompting. However, the relative effectiveness of self-critique versus generating multiple solutions is not directly compared.
- What evidence would resolve it: A controlled experiment directly comparing the performance of self-critique iterative prompting to generating multiple solutions with external verification on the same set of problems.

### Open Question 2
- Question: What factors contribute to the LLM's inability to accurately verify solutions, even for simple tasks like graph coloring?
- Basis in paper: [explicit] The paper finds that GPT-4 is surprisingly poor at verifying graph colorings, often failing to identify correct solutions and hallucinating errors.
- Why unresolved: While the paper documents the LLM's poor verification performance, it does not deeply investigate the underlying causes, such as limitations in the model's understanding of the task or issues with the verification prompt format.
- What evidence would resolve it: Detailed analysis of the LLM's verification outputs, including common error patterns and potential biases, as well as experiments with different verification prompt formulations or model architectures.

### Open Question 3
- Question: How does the LLM's performance on verification tasks generalize to other domains beyond graph coloring?
- Basis in paper: [inferred] The paper focuses on graph coloring as a representative reasoning task, but does not test the LLM's verification abilities on other domains.
- Why unresolved: It is unclear whether the LLM's poor verification performance is specific to graph coloring or a more general limitation across reasoning tasks.
- What evidence would resolve it: Experiments evaluating the LLM's verification performance on a diverse set of reasoning problems, such as logical inference, mathematical proofs, or commonsense reasoning tasks.

## Limitations
- The study is constrained to a single NP-complete problem (graph coloring) with specific instance characteristics
- Results are based on GPT-4 only, limiting generalizability to other LLM architectures
- The external verifier implementation details are not fully specified, making exact reproduction challenging

## Confidence
- High confidence in the core finding that LLMs perform poorly at graph coloring verification
- Medium confidence in the mechanism explaining why iterative prompting with self-critique fails
- Medium confidence in the conclusion that improvements with external verification are primarily due to solution sampling rather than feedback quality

## Next Checks
1. Test the iterative prompting approach on additional NP-complete problems (e.g., SAT, knapsack) to assess generalizability beyond graph coloring
2. Compare results across different LLM architectures (Claude, Llama, etc.) to determine if findings are specific to GPT-4
3. Implement the external verifier using both LLM-based and rule-based approaches to isolate the source of verification limitations