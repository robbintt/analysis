---
ver: rpa2
title: Efficiently Explaining CSPs with Unsatisfiable Subset Optimization (extended
  algorithms and examples)
arxiv_id: '2303.11712'
source_url: https://arxiv.org/abs/2303.11712
tags:
- ocus
- able
- explanation
- hitting
- subset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work improves upon existing methods for generating human-understandable
  explanations of Constraint Satisfaction Problem (CSP) solutions. Previous approaches
  relied on extracting Minimal Unsatisfiable Subsets (MUS), which do not guarantee
  optimality with respect to a cost function that quantifies explanation quality.
---

# Efficiently Explaining CSPs with Unsatisfiable Subset Optimization (extended algorithms and examples)

## Quick Facts
- arXiv ID: 2303.11712
- Source URL: https://arxiv.org/abs/2303.11712
- Reference count: 7
- Key outcome: OCUS approach outperforms MUS approach in explanation quality and computational time, solving problems up to 56% faster on average

## Executive Summary
This work introduces Optimal Constrained Unsatisfiable Subsets (OCUS) as an improvement over Minimal Unsatisfiable Subsets (MUS) for generating human-understandable explanations of Constraint Satisfaction Problem (CSP) solutions. While MUS approaches extract minimal unsatisfiable subsets, they do not guarantee optimality with respect to a cost function that quantifies explanation quality. The OCUS framework provides theoretical guarantees for optimal explanations and includes practical algorithms for computing them efficiently. The authors develop hitting set-based algorithms, incremental computation methods, and domain-specific optimizations to make OCUS practical for real-world CSP problems.

## Method Summary
The method computes Optimal Constrained Unsatisfiable Subsets (OCUS) using hitting set duality and Mixed Integer Programming (MIP) solvers. The core algorithm alternates between finding minimal correction subsets (MCS) and computing optimal hitting sets with respect to a cost function. The approach includes incremental computation that reuses satisfiable subsets across explanation steps, multiple correction subset enumeration with p-disjoint constraints, and domain-specific optimizations. The framework is integrated into an explanation generation pipeline that produces sequences of optimal explanation steps for CSP solutions.

## Key Results
- OCUS approach outperforms MUS approach in both explanation quality and computational efficiency
- Problems solved up to 56% faster on average compared to MUS-based methods
- Incrementality reduces hitting set iterations and runtime through information reuse
- Multiple p-disjoint correction subsets provide stronger hitting set constraints for optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OCUS algorithm improves explanation quality by optimizing for cost function rather than subset minimality.
- Mechanism: Uses hitting set duality to find minimal correction subsets, enabling cost-based optimization via MIP solver constraints.
- Core assumption: Cost function approximates human-understandability and can be encoded as linear term.
- Evidence anchors:
  - [abstract]: "OCUS, which are guaranteed to be optimal with respect to a given cost function"
  - [section 5.2]: "we propose to build on the hitting set duality of Proposition 1" and "condition p can easily be encoded as a linear constraint"
  - [corpus]: Weak - related MUS papers focus on minimality not optimality
- Break condition: Cost function not linear or not efficiently encodable as MIP constraints

### Mechanism 2
- Claim: Incremental OCUS computation reuses information across explanation steps to improve efficiency.
- Mechanism: Stores satisfiable subsets between calls and projects them onto new formula context; uses same MIP solver instance with updated constraints.
- Core assumption: Formula structure remains largely consistent across explanation steps (C and Iend constant, I grows incrementally).
- Evidence anchors:
  - [section 7.1.1]: "we can keep track of a set SSs of satisfiable subsets" and "initialize H as the complement of each of these satisfiable subsets"
  - [section 7.1.2]: "initialize the MIP solver once with all relevant decision variables" and "reusing previously found sets-to-hit"
  - [example 5]: Demonstrates runtime reduction from 12 to 6 steps through incrementality
- Break condition: Formula changes substantially between steps or constraint structure changes

### Mechanism 3
- Claim: Correction subset enumeration with p-disjoint sets reduces hitting set computation cost.
- Mechanism: Algorithm 7 extracts multiple correction subsets from satisfiable subsets, ensuring they are disjoint with respect to predicate p; uses projection to maintain p-disjointness in incremental setting.
- Core assumption: Multiple p-disjoint correction subsets can be extracted efficiently and provide stronger hitting set constraints.
- Evidence anchors:
  - [section 7.2.2]: "enumerate multiple correction subsets that are p-disjoint of each other during the CorrSubsets procedure"
  - [example 6]: Shows enumeration of multiple p-disjoint correction subsets using projection
  - [section 7.2.3]: "we project subset S′ onto the base constraints, the current interpretation and the negated literals to explain"
- Break condition: p-disjoint correction subsets cannot be efficiently enumerated or provide insufficient constraint strength

## Foundational Learning

- Concept: Hitting set duality between MUS and MCS
  - Why needed here: Forms theoretical foundation for OCUS algorithm - allows conversion between unsat core finding and correction subset hitting
  - Quick check question: If S is an MCS of F, what is the relationship between S and MUSs of F?

- Concept: Mixed Integer Programming (MIP) for weighted hitting sets
  - Why needed here: Enables encoding of cost function and structural constraints for optimal hitting set computation
  - Quick check question: How would you encode an "exactly one of" constraint in MIP for the explanation setting?

- Concept: Incremental solving and solver reuse
  - Why needed here: Critical for efficiency when generating explanation sequences with similar subproblems
  - Quick check question: What information can be safely reused between consecutive OCUS calls in explanation sequence generation?

## Architecture Onboarding

- Component map: OCUS core algorithm -> CorrSubsets module -> Grow strategies -> SAT/MIP calls -> Explanation generation
- Critical path: OCUS → CorrSubsets → Grow → SAT/MIP calls → Explanation generation
- Design tradeoffs:
  - Single OCUS call vs multiple OUS calls: Global optimality vs. easier constraint encoding
  - Grow strategy selection: Quality of satisfiable subsets vs. computational overhead
  - Correction subset enumeration: Stronger hitting set constraints vs. enumeration cost
- Failure signatures:
  - Slow performance: Too many hitting set iterations, poor Grow strategy, insufficient correction subsets
  - Suboptimal explanations: Cost function not properly encoded, predicate constraints too weak
  - Memory issues: Too many sets-to-hit stored, especially without incrementality
- First 3 experiments:
  1. Run OCUS vs MUS on small puzzle instance, compare explanation cost and runtime
  2. Enable incrementality, measure reduction in hitting set iterations and runtime
  3. Test different Grow strategies (SAT, SubsetMax-SAT, MaxSAT) on same instance, compare correction subset quality and hitting set solver performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What interpretability metrics best capture the "understandability" of generated explanations?
- Basis in paper: [inferred] The paper mentions using a cost function to approximate human-understandability but notes this might not fully capture what makes an explanation understandable, and questions which interpretability metrics are best suited for characterizing understandable explanations.
- Why unresolved: The paper does not propose or evaluate specific interpretability metrics beyond simple weighted sums of constraints and facts. It acknowledges that the current cost function might not fully capture the complexity of human-understandability.
- What evidence would resolve it: Experiments comparing different interpretability metrics (e.g., cognitive load, explanation length, use of common inference patterns) against human evaluations of explanation quality.

### Open Question 2
- Question: How can the OCUS algorithm be extended to handle non-linear cost functions?
- Basis in paper: [explicit] The paper states that the current MIP-based implementation of OCUS is limited to linear cost functions and asks how to extend it to handle non-linear cost functions.
- Why unresolved: The paper only implements linear cost functions due to limitations of the MIP solver. It acknowledges that non-linear cost functions might be needed to better capture the complexity of human-understandability.
- What evidence would resolve it: Implementation and evaluation of OCUS algorithms using non-linear cost functions (e.g., logarithmic, exponential) and comparison of their performance and explanation quality to the linear case.

### Open Question 3
- Question: What are the synergies between the OCUS approach and the more general problem of QMaxSAT?
- Basis in paper: [explicit] The paper mentions the potential synergies between OCUS and QMaxSAT but does not explore them in detail.
- Why unresolved: The paper only briefly mentions the potential connection to QMaxSAT without exploring the specific synergies or how the techniques could be combined.
- What evidence would resolve it: A detailed analysis of the relationship between OCUS and QMaxSAT, including potential ways to combine their techniques and an evaluation of the resulting hybrid approach.

## Limitations
- Effectiveness depends critically on availability of efficiently encodable cost function that accurately reflects human-understandability
- Incrementality benefits assume relatively stable formula structure across explanation steps
- Correction subset enumeration may not scale well to extremely large problems

## Confidence
- **High Confidence:** The hitting set duality foundation and MIP encoding of cost functions are well-established techniques with clear theoretical guarantees
- **Medium Confidence:** The incrementality mechanisms and correction subset enumeration show promise in examples but may have edge cases not fully explored
- **Low Confidence:** Domain-specific optimizations are mentioned but not extensively validated across diverse CSP types

## Next Checks
1. Test OCUS performance when the cost function is non-linear or requires complex constraints - does the algorithm gracefully degrade or fail entirely?
2. Evaluate incrementality benefits on CSP instances with varying degrees of formula stability between explanation steps to quantify the boundary conditions
3. Measure the scaling behavior of correction subset enumeration on progressively larger CSP instances to identify practical size limits