---
ver: rpa2
title: Multilingual Controllable Transformer-Based Lexical Simplification
arxiv_id: '2307.02120'
source_url: https://arxiv.org/abs/2307.02120
tags:
- word
- candidates
- simplification
- language
- lexical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents mTLS, a multilingual controllable Transformer-based
  Lexical Simplification (LS) system, extending a previous English monolingual model
  to support English, Spanish, and Portuguese. The approach uses language-specific
  prefixes, control tokens (Word Length, Word Rank, Word Syllables, Candidate Ranking,
  Sentence Similarity), and Masked Language Model (MLM) candidates to suggest simpler
  alternatives for complex words.
---

# Multilingual Controllable Transformer-Based Lexical Simplification

## Quick Facts
- arXiv ID: 2307.02120
- Source URL: https://arxiv.org/abs/2307.02120
- Reference count: 23
- Key outcome: mTLS outperforms previous state-of-the-art models like LSBert and ConLS on monolingual datasets and achieves competitive results on the TSAR-2022 dataset, including outperforming GPT-3 on several metrics.

## Executive Summary
This paper presents mTLS, a multilingual controllable Transformer-based Lexical Simplification (LS) system extending previous English-only models to support English, Spanish, and Portuguese. The approach uses language-specific prefixes, control tokens (Word Length, Word Rank, Word Syllables, Candidate Ranking, Sentence Similarity), and Masked Language Model (MLM) candidates to suggest simpler alternatives for complex words. The model was fine-tuned using T5 for English and mT5 for multilingual tasks, with evaluation showing mTLS outperforms previous state-of-the-art models like LSBert and ConLS on monolingual datasets while performing competitively on the TSAR-2022 shared task dataset.

## Method Summary
The mTLS system fine-tunes T5-large for English and mT5-large for multilingual tasks using language-specific prefixes ("simplify en:", "simplify es:", "simplify pt:") embedded in each input. The model incorporates control tokens to guide simplification based on word complexity metrics and uses MLM candidates extracted from pre-trained BERT-based models. Training uses a 70/15/15 split for training/validation/test per language, with Optuna for hyperparameter search. Evaluation employs metrics including Accuracy@1, Accuracy@N@Top1, Potential@K, and MAP@K, comparing performance against baselines like LSBert, ConLS, and TSAR-2022 shared task systems.

## Key Results
- mTLS outperforms LSBert and ConLS on monolingual datasets (LexMTurk, BenchLS, NNSeval)
- mTLS achieves competitive performance on TSAR-2022 dataset, outperforming GPT-3 on several metrics
- Multilingual model achieves performance gains for Spanish and Portuguese compared to monolingual baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific prefixes allow the multilingual model to learn and differentiate between English, Spanish, and Portuguese during training.
- Mechanism: By embedding language-specific prefixes into each input, the model learns to associate these prefixes with their corresponding languages, enabling it to generate appropriate simplifications for each language.
- Core assumption: The model can effectively learn language-specific patterns and associations from the prefixes.
- Evidence anchors: [abstract] mentions novelty of language-specific prefixes; [section 3] states prefixes help differentiate languages; corpus provides weak validation.
- Break condition: If the model fails to generate language-appropriate simplifications or shows poor performance on language-specific test sets.

### Mechanism 2
- Claim: Control tokens guide the model to generate simpler and more relevant substitutes.
- Mechanism: Each control token provides specific information about word complexity and relevance, allowing the model to prioritize simpler and more contextually appropriate alternatives during generation.
- Core assumption: Control tokens effectively capture complexity and relevance of words and sentences.
- Evidence anchors: [abstract] mentions novelty of control tokens; [section 3] states tokens help select simpler candidates; corpus provides weak validation.
- Break condition: If the model fails to generate simpler substitutes or shows poor performance on metrics like Accuracy@1 and MAP@K.

### Mechanism 3
- Claim: MLM candidates improve model performance by providing additional relevant alternatives.
- Mechanism: Extracting top-10 candidates from pre-trained BERT-based models using MLM approach gives the model access to a wider range of potential substitutes, increasing likelihood of finding simpler and more appropriate alternatives.
- Core assumption: MLM candidates are relevant and improve the model's ability to generate simpler substitutes.
- Evidence anchors: [abstract] mentions novelty of MLM candidates; [section 3] states candidates help find better options; corpus provides weak validation.
- Break condition: If the model's performance does not improve or degrades when MLM candidates are included.

## Foundational Learning

- Concept: Transformer-based models
  - Why needed here: The paper uses T5 and mT5 models for fine-tuning the lexical simplification system.
  - Quick check question: What are the key advantages of using Transformer-based models over traditional models for natural language processing tasks?

- Concept: Masked Language Model (MLM)
  - Why needed here: The paper uses MLM candidates extracted from pre-trained BERT-based models to improve the model's performance.
  - Quick check question: How does the masked language model approach work, and why is it effective for generating candidate substitutes?

- Concept: Control tokens and their impact on model output
  - Why needed here: The paper uses control tokens (Word Length, Word Rank, Word Syllables, Candidate Ranking, Sentence Similarity) to guide the model in generating simpler and more relevant substitutes.
  - Quick check question: How do control tokens influence the model's output, and what are the potential benefits and drawbacks of using them?

## Architecture Onboarding

- Component map: Input layer (language-specific prefixes, control tokens, MLM candidates) -> Transformer-based model (T5 for English, mT5 for multilingual) -> Output layer (ranked list of simpler substitutes)
- Critical path: 1) Preprocess input data with language-specific prefixes, control tokens, and MLM candidates 2) Fine-tune the transformer-based model on the preprocessed data 3) Generate and rank simpler substitutes using the fine-tuned model
- Design tradeoffs: Using multilingual model (mT5) allows joint training of multiple languages but may reduce performance compared to monolingual models; including MLM candidates increases computational complexity but improves candidate generation.
- Failure signatures: Poor performance on language-specific test sets may indicate issues with language differentiation; low accuracy or MAP scores may suggest problems with control token effectiveness or MLM candidate relevance.
- First 3 experiments: 1) Compare mTLS with monolingual models (TLS-2, TLS-3) on TSAR-EN dataset 2) Evaluate impact of MLM candidates by comparing TLS-2 (with) and TLS-3 (without) on TSAR-EN dataset 3) Assess effectiveness of control tokens by analyzing performance on Accuracy@1, MAP@K, and Potential@K metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do language-specific prefixes and control tokens interact to affect the model's performance on languages with limited training data?
- Basis in paper: [explicit] The paper mentions language-specific prefixes help differentiate languages and control tokens are used to control different aspects of generated candidates, noting multilingual model was trained jointly to handle limited data for Spanish and Portuguese.
- Why unresolved: While the paper discusses these elements, it doesn't provide detailed analysis or experimental results showing how each element contributes to performance, especially in low-resource languages.
- What evidence would resolve it: Comparative experiments isolating effects of language-specific prefixes and control tokens on performance for languages with varying amounts of training data.

### Open Question 2
- Question: What are the long-term effects of using multilingual models like mT5-large on performance and generalization of lexical simplification tasks across different languages?
- Basis in paper: [explicit] The paper mentions mT5-large contains irrelevant information from other languages and requires significantly more data to learn effectively, which might reduce performance compared to monolingual models.
- Why unresolved: The paper provides initial observations on limitations but doesn't explore long-term effects or potential strategies to mitigate these issues.
- What evidence would resolve it: Longitudinal studies comparing performance of multilingual and monolingual models over time with varying amounts of training data to assess generalization capabilities.

### Open Question 3
- Question: How does the inclusion of Masked Language Model (MLM) candidates influence the quality and diversity of generated lexical simplifications?
- Basis in paper: [explicit] The paper introduces MLM candidates to improve performance and compares models with and without MLM candidates, noting improvements in some metrics.
- Why unresolved: The paper doesn't provide detailed analysis of how MLM candidates specifically affect quality and diversity of generated simplifications.
- What evidence would resolve it: Experiments comparing quality and diversity of generated simplifications with and without MLM candidates using metrics such as semantic similarity and candidate diversity.

## Limitations
- The effectiveness of control tokens and MLM candidates lacks direct empirical validation through ablation studies
- The model's generalization to languages beyond the three tested (English, Spanish, Portuguese) remains unproven
- Mixed results on TSAR-2022 dataset - while outperforming GPT-3 on some metrics, mTLS is surpassed by systems using GPT-4 and ChatGPT on others

## Confidence
- **High Confidence**: The basic methodology of using language-specific prefixes and transformer models for lexical simplification is sound and well-established in the literature.
- **Medium Confidence**: The claims about mTLS outperforming previous models, particularly on the TSAR-2022 dataset, due to the novel use of control tokens and MLM candidates.
- **Low Confidence**: The assertion that the multilingual model will generalize effectively to languages beyond the three tested, and the exact contribution of each control token mechanism to overall performance.

## Next Checks
1. **Ablation Study**: Conduct experiments to quantify the individual contribution of each control token (Word Length, Word Rank, Word Syllables, Candidate Ranking, Sentence Similarity) and MLM candidates to the model's performance through training versions with different combinations of these components removed.

2. **Cross-Lingual Generalization Test**: Evaluate the model's performance on a fourth language (e.g., French or Italian) not seen during training to assess true multilingual capabilities and identify any language-specific limitations in the approach.

3. **Comparison with GPT-4/ChatGPT**: Design a controlled experiment where mTLS and GPT-4/ChatGPT are evaluated on the same dataset using identical protocols to provide a fair comparison and validate the claim that mTLS outperforms GPT-3 but is surpassed by newer models.