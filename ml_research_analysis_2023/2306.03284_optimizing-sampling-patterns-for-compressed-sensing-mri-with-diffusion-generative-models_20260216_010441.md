---
ver: rpa2
title: Optimizing Sampling Patterns for Compressed Sensing MRI with Diffusion Generative
  Models
arxiv_id: '2306.03284'
source_url: https://arxiv.org/abs/2306.03284
tags:
- sampling
- patterns
- training
- reconstruction
- posterior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to optimize k-space sampling patterns
  for compressed sensing MRI reconstruction using pre-trained diffusion generative
  models. The key idea is to leverage Tweedie's formula to derive a training objective
  that approximates the posterior mean without expensive iterative sampling.
---

# Optimizing Sampling Patterns for Compressed Sensing MRI with Diffusion Generative Models

## Quick Facts
- **arXiv ID**: 2306.03284
- **Source URL**: https://arxiv.org/abs/2306.03284
- **Reference count**: 40
- **Primary result**: Method optimizes k-space sampling patterns for compressed sensing MRI using pre-trained diffusion generative models with Tweedie's formula-based posterior mean estimation

## Executive Summary
This paper introduces a novel method to learn k-space sampling patterns for compressed sensing MRI reconstruction using pre-trained diffusion generative models. The key innovation is leveraging Tweedie's formula to derive a closed-form posterior mean estimate that avoids expensive iterative sampling. By parameterizing sampling patterns as learnable Bernoulli distributions and using the Gumbel Straight-Through estimator, the method enables gradient-based optimization of sampling patterns. Experiments show that learned 2D patterns outperform equispaced baselines in SSIM and PSNR across anatomies and accelerations, while 3D patterns perform comparably to Poisson disc masks. The approach requires minimal training samples (as few as 5) and demonstrates robustness to varying sampling stochasticity and step counts.

## Method Summary
The method optimizes k-space sampling patterns by combining pre-trained diffusion models with Tweedie's formula for posterior mean estimation. Sampling patterns are parameterized as Bernoulli distributions with learnable probabilities, scaled to maintain fixed acceleration factors. The Gumbel Straight-Through estimator enables differentiable sampling from these distributions during training. Reconstruction quality is optimized by backpropagating through a one-step posterior mean estimate derived from the diffusion model's score network and a likelihood term from the measurements. This approach avoids iterative sampling while maintaining competitive reconstruction quality. The method is evaluated on FastMRI brain and knee datasets, comparing learned patterns against equispaced and Poisson disc baselines across various accelerations.

## Key Results
- Learned 2D sampling patterns outperform equispaced baselines in SSIM and PSNR across brain and knee anatomies
- 3D learned patterns achieve performance comparable to Poisson disc masks
- Method requires as few as 5 training samples while maintaining effectiveness
- Performance remains stable across varying sampling stochasticity and step count parameters

## Why This Works (Mechanism)

### Mechanism 1
Tweedie's formula enables closed-form posterior mean estimation in diffusion models by providing an analytical expression for the mean of the posterior distribution. The formula computes the posterior mean via ∇xt log pt(xt|y) = ∇xt log pt(xt) + ∇xt log pt(y|xt), combining the prior score from the diffusion model with a likelihood term from measurements. This works under the assumptions of conditional independence between xt and y given x0, and that pt(xt|x0) follows a Gaussian distribution. The closed-form solution is critical for avoiding expensive iterative sampling during pattern optimization.

### Mechanism 2
Gradient-based learning of sampling patterns is feasible through backpropagation through the Tweedie-based posterior mean estimate. The sampling pattern parameters θP are optimized by computing gradients of the reconstruction loss with respect to θP, where gradients flow through the posterior mean estimate that depends on both the score network and measurement operator. This requires the Gumbel Straight-Through estimator to maintain discrete sampling behavior while enabling gradient flow. The tractability of the gradient computation is essential for effective pattern optimization.

### Mechanism 3
Pre-trained diffusion models generalize well across different anatomies and acceleration factors without fine-tuning due to their ability to capture structural priors from diverse training data. The score network provides a stable prior that remains effective when the forward operator changes, enabling learned sampling patterns to be robust to variations in anatomy and acceleration. This generalization property reduces the need for expensive fine-tuning while maintaining reconstruction quality across different acquisition scenarios.

## Foundational Learning

- **Tweedie's formula for posterior mean estimation**: Provides the theoretical basis for the one-step posterior mean estimate used in training. Quick check: What is the form of Tweedie's formula for Gaussian noise, and how does it extend to include measurements?

- **Diffusion models and score matching**: The score network from pre-trained diffusion models serves as the prior score function ∇xt log pt(xt) in posterior sampling. Quick check: How does the score network approximate the score function, and what is the relationship between denoising score matching and the learned score?

- **Gumbel Straight-Through estimator**: Enables gradient-based optimization of binary sampling patterns while maintaining discrete sampling behavior. Quick check: How does the Gumbel ST estimator differ from vanilla ST, and why is it preferred for Bernoulli sampling?

## Architecture Onboarding

- **Component map**: Pre-trained diffusion model score network -> Tweedie-based posterior mean estimator -> Reconstruction sampler -> Sampling pattern parameterization (Bernoulli + Gumbel ST) -> Training loop with gradient descent

- **Critical path**: 1) Generate noisy measurement y = A(x0) + noise, 2) Compute posterior mean estimate using score network and likelihood gradient, 3) Calculate reconstruction loss and backpropagate through posterior mean to sampling pattern parameters, 4) Update sampling pattern distribution and repeat

- **Design tradeoffs**: Pre-trained models avoid fine-tuning but may limit adaptation; Cartesian sampling is simpler but potentially suboptimal; one-step mean reduces memory but may be less accurate than iterative sampling

- **Failure signatures**: High reconstruction error indicates poor pattern or mismatched prior; training instability suggests Gumbel ST or likelihood issues; memory errors indicate score network is too large or posterior computation is too complex

- **First 3 experiments**: 1) Verify Tweedie's formula implementation by comparing one-step posterior mean to iterative sampling on synthetic data, 2) Test Gumbel ST sampling with fixed score network to ensure gradient flow, 3) Evaluate reconstruction quality with learned pattern vs baselines on held-out validation set

## Open Questions the Paper Calls Out
- How to scale the method to larger k-space dimensions and what are the memory/computational bottlenecks at high resolutions
- Whether learned sampling patterns transfer across anatomies or patient demographics or overfit to training distribution
- Sensitivity of learned patterns to noise level in score network's training distribution

## Limitations
- High memory complexity due to backpropagation through second-order derivatives limits resolution to relatively small images
- Potential overfitting to training anatomy if datasets are not diverse enough
- Method requires pre-trained diffusion models, adding dependency on external model training

## Confidence
- High confidence in theoretical derivation of Tweedie-based posterior mean estimator
- Medium confidence in practical implementation of differentiable sampling patterns
- Medium confidence in cross-anatomy generalization without model fine-tuning

## Next Checks
1. Test robustness by training sampling patterns on brain data and evaluating on knee data (and vice versa) to quantify cross-anatomy performance degradation
2. Evaluate reconstruction quality under varying noise levels to assess sensitivity to likelihood step size and verify stability of Tweedie approximation
3. Compare against learned sampling patterns optimized using iterative reconstruction methods to isolate impact of one-step posterior approximation versus sampling pattern parameterization