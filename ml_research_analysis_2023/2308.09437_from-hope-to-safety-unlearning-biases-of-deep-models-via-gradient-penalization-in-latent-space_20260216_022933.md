---
ver: rpa2
title: 'From Hope to Safety: Unlearning Biases of Deep Models via Gradient Penalization
  in Latent Space'
arxiv_id: '2308.09437'
source_url: https://arxiv.org/abs/2308.09437
tags:
- artifact
- sample
- bias
- rr-clarc
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RR-ClArC, a method to correct deep neural network
  biases by penalizing model sensitivity along a bias direction in latent space. The
  method builds on Concept Activation Vectors (CAVs) and leverages gradient regularization
  to ensure class-specific unlearning of both localized and unlocalized biases.
---

# From Hope to Safety: Unlearning Biases of Deep Models via Gradient Penalization in Latent Space

## Quick Facts
- arXiv ID: 2308.09437
- Source URL: https://arxiv.org/abs/2308.09437
- Reference count: 40
- Key outcome: RR-ClArC outperforms state-of-the-art bias correction methods across four datasets and three architectures by using gradient penalization in latent space

## Executive Summary
This paper presents RR-ClArC, a method to correct deep neural network biases by penalizing model sensitivity along a bias direction in latent space. The method builds on Concept Activation Vectors (CAVs) and leverages gradient regularization to ensure class-specific unlearning of both localized and unlocalized biases. Experiments across four datasets (ISIC, Bone Age, ImageNet, CelebA) and three architectures (VGG-16, ResNet-18, EfficientNet-B0) show that RR-ClArC consistently outperforms state-of-the-art correction methods, achieving higher accuracy on biased test sets while maintaining performance on clean data. The work also highlights the importance of robust CAV computation, finding that signal-based CAVs outperform traditional regression-based approaches.

## Method Summary
RR-ClArC is a two-step approach that first computes bias directions using signal-based Concept Activation Vectors (CAVs) from the last convolutional layer, then fine-tunes the model with a gradient regularization loss that penalizes sensitivity along these bias directions. The method introduces a regularization term that reduces the model's output gradient in the direction of the bias CAV, forcing the network to become insensitive to spurious correlations. Class-specific unlearning is enabled through annotation vectors that control which output classes receive regularization. Early layers are frozen during fine-tuning to preserve useful features while only adapting higher layers to remove biases.

## Key Results
- RR-ClArC achieves higher accuracy on biased test sets compared to state-of-the-art methods while maintaining clean data performance
- Signal-based CAV computation provides 90% alignment with true concept directions versus 60% for regression-based methods
- Class-specific unlearning through annotation vectors allows selective bias removal without harming performance on unaffected classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RR-ClArC reduces model sensitivity to bias directions in latent space by explicitly penalizing the gradient along Concept Activation Vectors (CAVs).
- Mechanism: The method introduces a regularization term that penalizes the squared dot product between the model's output gradient and the bias direction in latent space. This forces the model to become insensitive to changes along that direction.
- Core assumption: The bias direction can be effectively captured as a CAV, and penalizing gradients in latent space will remove spurious correlations without harming useful features.
- Evidence anchors:
  - [abstract]: "explicitly reduces model sensitivity towards biases via gradient penalization"
  - [section 3.3]: "explicitly penalizes the output gradient in the direction of a bias CA V h"
  - [corpus]: Weak - corpus focuses on unlearning but not gradient-based approaches
- Break condition: If the CAV direction is poorly aligned with the true bias concept, gradient penalization may suppress useful features or fail to remove the bias.

### Mechanism 2
- Claim: Signal-based CAV computation outperforms regression-based approaches like SVMs in aligning with true bias directions.
- Mechanism: Signal-CA Vs are computed using correlation between latent activations and concept labels, making them more robust to noise and better aligned with the true concept direction.
- Core assumption: Correlation-based methods capture the true concept direction better than classification-based methods, which may find separating hyperplanes that don't point from clean to artifact samples.
- Evidence anchors:
  - [section 3.1]: "signal-pattern-based CA Vs...are more robust against noise"
  - [section 4.2]: "signal-CA Vs perform significantly better than the other approaches, resulting in overall alignments ¯s of over 90% compared to less than 60% for the regression CA Vs"
  - [corpus]: Weak - corpus papers don't compare CAV computation methods
- Break condition: If the signal-based approach is computationally expensive or the concept labels are noisy, the alignment advantage may diminish.

### Mechanism 3
- Claim: RR-ClArC enables class-specific unlearning by using annotation vectors to control which classes are regularized.
- Mechanism: By setting the annotation vector m in the regularization term, the method can selectively penalize bias sensitivity for specific output classes while preserving it for others.
- Core assumption: The bias concept affects different classes differently, and class-specific control allows preserving useful correlations for some classes while removing them for others.
- Evidence anchors:
  - [section 3.3]: "choosing elements uniformly randomly as (m)i ∈R {−1, 1} for each sample improves regularization"
  - [section 4.4]: "RRR and RR-ClArC, however, targeting the 'tench' class only through the gradient, retain the model's accuracy for the selected classes"
  - [corpus]: Weak - corpus papers don't discuss class-specific unlearning mechanisms
- Break condition: If the bias concept is entangled across all classes, class-specific unlearning may not be feasible or may require multiple passes.

## Foundational Learning

- Concept: Concept Activation Vectors (CAVs)
  - Why needed here: CAVs provide a way to represent abstract concepts in the latent space of neural networks, enabling bias modeling without input-level annotations
  - Quick check question: What is the difference between signal-based CAVs and regression-based CAVs in terms of how they're computed and their alignment with true concepts?

- Concept: Gradient-based regularization
  - Why needed here: The method uses gradient penalties to enforce insensitivity to bias directions, requiring understanding of how gradients propagate through neural networks
  - Quick check question: How does penalizing the gradient along a CAV direction differ from directly modifying activations or weights?

- Concept: Layer-wise relevance propagation (LRP)
  - Why needed here: LRP is used to visualize and verify bias localization in input space, requiring understanding of attribution methods
  - Quick check question: Why is LRP applied to the dot product between activations and CAVs rather than to the model output directly?

## Architecture Onboarding

- Component map: CAV computation module -> gradient regularization module -> evaluation module
- Critical path: The critical path is: compute CAV → fine-tune model with gradient penalty → evaluate on clean and biased test sets. The CAV computation must be done before fine-tuning, and evaluation requires both test sets.
- Design tradeoffs: The method trades computational cost (additional backward passes) for better bias removal. It also requires freezing early layers during fine-tuning, limiting architectural flexibility. Signal-based CAVs are more accurate but may be slower than regression-based methods.
- Failure signatures: Poor CAV alignment leads to ineffective regularization. Class-specific unlearning fails when bias concepts are entangled across classes. Runtime increases significantly for architectures with expensive gradient computations.
- First 3 experiments:
  1. Verify CAV alignment by computing cosine similarity between CA and activation changes when bias is added to inputs
  2. Test gradient regularization effectiveness by measuring TCA V scores before and after fine-tuning with different λ values
  3. Evaluate class-specific unlearning by comparing accuracy on selected vs random classes for different annotation vector settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Concept Activation Vector (CAV) computation methods be improved to reduce sensitivity to noise and ensure better alignment with true concept directions?
- Basis in paper: [explicit] The paper identifies that regression-based CAV computation methods like SVM, ridge, lasso, and logistic regression often result in diverging directions due to noise in the data, whereas signal-based CAVs show better alignment with true concept directions.
- Why unresolved: The paper highlights the shortcomings of current regression-based methods but does not provide a comprehensive solution or alternative approach to ensure robust CAV computation.
- What evidence would resolve it: Developing and testing new CAV computation methods that consistently yield high alignment scores across various datasets and noise levels would resolve this question.

### Open Question 2
- Question: What is the optimal layer in a deep neural network for computing Concept Activation Vectors (CAVs) to effectively model and unlearn biases?
- Basis in paper: [inferred] The paper mentions that the choice of the last convolutional layer for CAV computation might not always be optimal and that selecting the optimal layer is still an open question.
- Why unresolved: The paper does not provide empirical evidence or a systematic approach to determine the best layer for CAV computation in different contexts.
- What evidence would resolve it: Conducting experiments across various architectures and datasets to compare the effectiveness of CAVs computed from different layers would provide insights into the optimal choice.

### Open Question 3
- Question: How can RR-ClArC be extended to handle dynamic or evolving biases that change over time or across different contexts?
- Basis in paper: [inferred] The paper focuses on static biases and does not address the challenge of dynamic biases that may require continuous adaptation of the model.
- Why unresolved: The current RR-ClArC method is designed for post-hoc correction of known biases and does not account for biases that may emerge or change over time.
- What evidence would resolve it: Developing a framework that incorporates continuous monitoring and adaptive unlearning of biases in response to changing data distributions would resolve this question.

## Limitations

- The method requires freezing early layers during fine-tuning, limiting architectural flexibility and potentially missing opportunities for more comprehensive bias correction
- Computational overhead is significant due to additional gradient computations during fine-tuning, making it less suitable for resource-constrained deployment
- Effectiveness depends heavily on proper CAV alignment, which may be challenging for complex or poorly defined bias concepts

## Confidence

- Signal-based CAVs significantly outperform regression-based approaches: High confidence (90% vs 60% alignment scores)
- Class-specific unlearning effectiveness: Medium confidence (depends on proper annotation vector configuration)
- Gradient regularization mechanism: Medium confidence (theoretically sound but risk of over-regularization)

## Next Checks

1. Verify CAV alignment through cosine similarity analysis between computed CA and observed activation changes under controlled bias injection
2. Test gradient regularization effectiveness by measuring TCAV score reduction across different λ values while monitoring clean accuracy retention
3. Validate class-specific unlearning by comparing selected class performance versus random class selection under different annotation vector configurations