---
ver: rpa2
title: Prompting with Pseudo-Code Instructions
arxiv_id: '2305.11790'
source_url: https://arxiv.org/abs/2305.11790
tags:
- instructions
- language
- pseudo-code
- tasks
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies if prompting large language models with pseudo-code
  instructions can improve performance compared to natural language instructions.
  The authors manually create a dataset of pseudo-code prompts for 132 NLP tasks sourced
  from the Super-NaturalInstructions dataset.
---

# Prompting with Pseudo-Code Instructions

## Quick Facts
- arXiv ID: 2305.11790
- Source URL: https://arxiv.org/abs/2305.11790
- Reference count: 33
- Key outcome: Pseudo-code instructions improve model performance over natural language instructions across 132 NLP tasks

## Executive Summary
This paper investigates whether prompting large language models with pseudo-code instructions can improve performance compared to natural language instructions. The authors create a dataset of pseudo-code prompts for 132 NLP tasks and evaluate two model families (BLOOM and CodeGen) across classification, QA, and generative tasks. Results show that pseudo-code instructions lead to significant improvements, with average absolute increases of 7-16 F1 points on classification tasks and 12-38% relative improvements in aggregate ROUGE scores. Ablation studies reveal that code comments, docstrings, and structural clues all contribute to performance gains.

## Method Summary
The authors manually created pseudo-code prompts for 132 NLP tasks sourced from the Super-NaturalInstructions dataset. They evaluated BLOOM and CodeGen models at 2B/3B/6B/7B scales using zero-shot and two-shot prompting. Models were run with DeepSpeed-Inference in fp16 mode using greedy decoding with newline truncation. Outputs were post-processed by lowercasing and punctuation removal. Performance was measured using task-specific metrics including F1, ROUGE, Exact Match, and Average Normalized Levenshtein Similarity.

## Key Results
- Pseudo-code instructions yield 7-16 absolute F1 point improvements on classification tasks
- Aggregate ROUGE scores show 12-38% relative improvement across all task types
- Code models (CodeGen) outperform general language models (BLOOM) even with natural language instructions
- Code comments, docstrings, and structural clues all contribute to performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-code instructions reduce ambiguity compared to natural language, improving model performance.
- Mechanism: Pseudo-code provides explicit structural clues (function names, arguments, return types) and docstrings that clarify task intent, reducing model interpretation errors.
- Core assumption: Models can leverage structured information encoded in pseudo-code better than unstructured natural language.
- Evidence anchors:
  - [abstract] "Given the inherent ambiguity present in natural language, it is intuitive to consider the possible advantages of prompting with less ambiguous prompt styles, such as the use of pseudo-code."
  - [section] "Our experiments show that using pseudo-code instructions leads to better results, with an average increase (absolute) of 7-16 points in F1 scores for classification tasks."
- Break condition: If the model cannot parse pseudo-code syntax or lacks exposure to code-like patterns during training, the structured advantage diminishes.

### Mechanism 2
- Claim: Code models (e.g., CodeGen) are better suited to exploit pseudo-code prompts than general language models.
- Mechanism: Code models are trained on both natural language and code, making them more sensitive to structured patterns like function declarations and comments present in pseudo-code.
- Core assumption: The model's training distribution includes sufficient code-like examples to generalize to pseudo-code prompts.
- Evidence anchors:
  - [abstract] "Both LLM families have been trained on natural language as well as code data."
  - [section] "Despite most tasks being non-code tasks, CodeGen, a model designed for code applications, outperforms BLOOM, even when using natural language instructions."
- Break condition: If the code model is fine-tuned on natural language instructions only, it may lose the structural sensitivity advantage.

### Mechanism 3
- Claim: Comments and docstrings in pseudo-code provide step-by-step reasoning cues that improve performance.
- Mechanism: Docstrings and inline comments act as explicit reasoning chains, guiding the model through task logic without requiring implicit reasoning.
- Core assumption: Models can attend to and utilize auxiliary textual information embedded in pseudo-code prompts.
- Evidence anchors:
  - [section] "We include detailed ablation studies which indicate that code comments, docstrings, and the structural clues encoded in pseudo-code all contribute towards the improvement in performance."
  - [section] "We find that the performance of natural language instructions also improves by the inclusion of comments and docstring for each model family and configuration except for BLOOM 3B."
- Break condition: If the model's attention mechanism cannot effectively weight or utilize docstring/comment tokens, the reasoning cue benefit disappears.

## Foundational Learning

- Concept: Prompt engineering and its role in shaping model outputs.
  - Why needed here: The paper hinges on comparing how different prompt styles (natural language vs pseudo-code) influence model performance.
  - Quick check question: What are the key differences between a natural language instruction and a pseudo-code instruction in terms of structure and clarity?

- Concept: Zero-shot vs few-shot learning paradigms.
  - Why needed here: The experiments are conducted in zero-shot settings, and the authors explore how structured prompts perform in this regime.
  - Quick check question: How does zero-shot prompting differ from few-shot prompting in terms of model reliance on prompt structure?

- Concept: Function of code models vs general language models.
  - Why needed here: The comparison between BLOOM (general) and CodeGen (code-focused) is central to understanding performance differences.
  - Quick check question: Why might a code model perform better on non-code tasks when prompted with pseudo-code instructions?

## Architecture Onboarding

- Component map:
  - Super-NaturalInstructions dataset -> pseudo-code prompt generation -> preprocessing pipeline
  - BLOOM and CodeGen models at 2B/3B/6B/7B scales -> inference with DeepSpeed-Inference
  - Task-specific metrics (F1, ROUGE, EM, ANLS) -> post-processing and aggregation
  - Ablation studies -> variants of pseudo-code prompts (with/without docstrings, function prototypes only, etc.)

- Critical path:
  1. Generate pseudo-code instructions from natural language task definitions
  2. Preprocess task inputs to match pseudo-code function signatures
  3. Run inference using greedy decoding with truncation at newline
  4. Post-process outputs (lowercase, punctuation removal)
  5. Compute metrics and compare against natural language baselines

- Design tradeoffs:
  - Prompt complexity vs. interpretability: More detailed pseudo-code improves performance but may require more tokens and increase input length
  - Model size vs. generalization: Larger models show better performance, but zero-shot settings may still limit gains
  - Code syntax vs. task domain: Pseudo-code works even on non-code tasks, but effectiveness may vary by task type

- Failure signatures:
  - If model outputs contain excess continuation text, truncation at newline may cut off valid answers
  - If pseudo-code syntax is not properly parsed, structural cues are lost
  - If docstring/comment tokens are ignored by the model, reasoning cues are ineffective

- First 3 experiments:
  1. Compare zero-shot performance of a small BLOOM model on classification tasks using natural language vs pseudo-code prompts
  2. Run ablation: remove docstrings and comments from pseudo-code prompts and measure performance drop
  3. Test function prototype-only prompts (no logic) in two-shot settings to isolate structural benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of pseudo-code instructions scale with increasingly larger model sizes beyond the 7B parameters tested in this paper?
- Basis in paper: [inferred] The paper notes that "Our results have been reported on two model families â€“ CodeGen and BLOOM at scales of 2-7B parameters" and acknowledges this as a limitation, suggesting further investigation is needed.
- Why unresolved: The paper explicitly states that it did not investigate the performance of pseudo-code instructions with larger model sizes, leaving open the question of whether the benefits observed will persist or diminish at larger scales.
- What evidence would resolve it: Empirical results from experiments using model families and sizes larger than those tested in this paper, demonstrating whether the performance gains from pseudo-code instructions are maintained, increased, or decreased.

### Open Question 2
- Question: How do pseudo-code instructions compare to traditional chain-of-thought prompting in terms of improving reasoning capabilities of language models?
- Basis in paper: [inferred] The paper mentions that "Another aspect worth thinking about is how traditional chain-of-thought may compare with pseudo-code prompts" and suggests this as a direction for future work, indicating the authors recognize this as an open question.
- Why unresolved: The paper does not include any direct comparison between pseudo-code instructions and chain-of-thought prompting, leaving the relative effectiveness of these two approaches for enhancing reasoning unknown.
- What evidence would resolve it: A controlled experimental comparison between the performance of language models when prompted with pseudo-code instructions versus chain-of-thought prompts on a set of reasoning tasks.

### Open Question 3
- Question: Can the benefits of pseudo-code instructions be extended to multilingual NLP tasks, or are they limited to English as in this study?
- Basis in paper: [inferred] The paper states that "Our work does not include any multi-lingual NLP tasks" and mentions that the BLOOM model was designed for multilingual support, suggesting this as a potential limitation and area for future exploration.
- Why unresolved: The study only evaluated pseudo-code instructions on English language tasks, and it is unclear whether the observed benefits would generalize to other languages, especially given the potential differences in how code and natural language are represented in multilingual models.
- What evidence would resolve it: Experiments applying pseudo-code instructions to a diverse set of multilingual NLP tasks and comparing the performance gains to those observed in the English-only setting.

## Limitations

- Manual creation of pseudo-code prompts introduces potential human bias and limits scalability to new tasks
- Limited model diversity (only BLOOM and CodeGen families tested) restricts generalizability to other pre-trained models
- Performance gains may be task-dependent, with some task categories potentially showing minimal improvement

## Confidence

- High confidence: Core finding that pseudo-code instructions improve performance over natural language instructions
- Medium confidence: Specific performance gains (7-16 F1 points, 12-38% ROUGE improvement) are reliable but may vary with different model families
- Medium confidence: Attribution of performance gains to specific pseudo-code components (docstrings, comments, structure) based on ablation studies
- Low confidence: Generalization to models outside the BLOOM and CodeGen families tested

## Next Checks

1. Test pseudo-code prompting on additional model families (GPT-Neo, OPT, T5) to assess generalizability across architectures
2. Conduct controlled experiments varying prompt length and complexity to quantify trade-offs between performance gains and computational overhead
3. Perform cross-task analysis to identify which task categories benefit most from pseudo-code instructions versus those that show minimal improvement