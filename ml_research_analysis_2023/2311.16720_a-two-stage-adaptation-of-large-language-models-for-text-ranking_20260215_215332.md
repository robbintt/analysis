---
ver: rpa2
title: A Two-Stage Adaptation of Large Language Models for Text Ranking
arxiv_id: '2311.16720'
source_url: https://arxiv.org/abs/2311.16720
tags:
- ranking
- text
- performance
- pre-training
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of adapting large language models
  (LLMs) for text ranking tasks, where previous methods using only next token prediction
  objectives are suboptimal. The authors propose a two-stage training approach consisting
  of: (1) continuous pre-training on a large weakly-supervised corpus to learn query-document
  relevance patterns, and (2) supervised fine-tuning with contrastive learning and
  additional loss functions to improve ranking performance while maintaining generation
  capabilities.'
---

# A Two-Stage Adaptation of Large Language Models for Text Ranking

## Quick Facts
- arXiv ID: 2311.16720
- Source URL: https://arxiv.org/abs/2311.16720
- Reference count: 8
- Two-stage training approach achieves state-of-the-art performance on text ranking tasks

## Executive Summary
This paper addresses the challenge of adapting large language models (LLMs) for text ranking tasks, where traditional next-token prediction objectives are suboptimal. The authors propose a novel two-stage training approach: first, continuous pre-training on weakly supervised text pairs to learn query-document relevance patterns, followed by supervised fine-tuning with contrastive learning and additional loss functions. The method is evaluated across multiple LLM architectures (BLOOM, LLaMA, Baichuan, Qwen) on in-domain and out-of-domain benchmarks, showing significant improvements over existing methods while maintaining generation capabilities.

## Method Summary
The proposed approach consists of two training stages. First, continuous pre-training uses a large-scale weakly supervised dataset of text pairs (e.g., title-body, citation-reference) to enable the model to predict relevant tokens conditioned on documents. Second, supervised fine-tuning applies contrastive ranking loss with negative sampling, parameter-efficient fine-tuning strategies (freezing lower transformer layers), and auxiliary losses (next-token prediction and KL divergence penalty) to improve ranking performance while preserving generation capabilities. The entire process is trained for one epoch each stage on datasets like MS MARCO and BEIR.

## Key Results
- BLOOM-3B achieves NDCG@10 of 55.7 on out-domain tasks, outperforming MonoT5-3B (54.6)
- LLaMA-7B outperforms RankLLaMA by +5.3 on average across benchmarks
- Two-stage approach consistently improves performance across all tested LLM sizes and architectures
- Models maintain generation capabilities as measured by perplexity metrics

## Why This Works (Mechanism)

### Mechanism 1
Weakly supervised continuous pre-training on text relevance data improves the model's ability to generate query-like text conditioned on documents. The model learns to map document content to relevant query tokens through next token prediction on weakly supervised text pairs. Core assumption: Query-document relevance patterns can be captured through self-supervised learning on web-scale text pairs without explicit relevance labels. Break condition: If the weakly supervised data lacks sufficient query-document correlation patterns.

### Mechanism 2
Supervised fine-tuning with contrastive ranking loss improves the model's ability to discriminate between relevant and irrelevant query-document pairs. The model learns to assign higher relevance scores to positive pairs versus negative pairs through ranking loss optimization. Core assumption: The ranking loss can effectively optimize the model to distinguish relevance while preserving generation capabilities. Break condition: If the ranking loss optimization causes excessive deviation from pre-trained parameters.

### Mechanism 3
Parameter-efficient fine-tuning strategies prevent degradation of generation capabilities while optimizing for ranking. By freezing lower transformer layers and applying additional constraints, the fine-tuned model maintains pre-training gains while learning ranking-specific patterns. Core assumption: Lower transformer layers capture general semantic features while upper layers encode task-specific information. Break condition: If freezing too many layers limits learning capacity.

## Foundational Learning

- **Next token prediction as self-supervised learning objective**: Why needed: This is the fundamental pre-training objective that the two-stage approach builds upon. Quick check: Why does next token prediction work well for learning language patterns, and how might it differ from learning relevance patterns?
- **Contrastive learning and ranking losses**: Why needed: The supervised fine-tuning stage uses contrastive ranking loss to distinguish relevant from irrelevant pairs. Quick check: How does the ranking loss formulation encourage the model to assign higher scores to relevant pairs?
- **Kullback-Leibler divergence as regularization**: Why needed: The Difference Penalty loss uses KL divergence to keep the fine-tuned model close to the pre-trained distribution. Quick check: What does KL divergence measure between two probability distributions, and why is this useful for preserving generation capabilities?

## Architecture Onboarding

- **Component map**: Weakly supervised text pairs (1B pairs) → Continuous pre-training → MS MARCO supervised data → Supervised fine-tuning
- **Critical path**: Weakly supervised data preparation → Continuous pre-training (1 epoch) → Supervised fine-tuning with contrastive loss and constraints → Evaluation
- **Design tradeoffs**: Two-stage training increases upfront cost but produces better-performing models; parameter freezing preserves generation capabilities but may limit ranking optimization capacity; larger negative sample sizes improve ranking performance but increase computational cost
- **Failure signatures**: Significant drop in perplexity on positive vs. negative pairs during fine-tuning indicates loss of generation capabilities; performance degradation on out-of-domain tasks suggests overfitting; no improvement over baseline models suggests ineffective pre-training or fine-tuning
- **First 3 experiments**: 1) Ablation study: Train BLOOM-1B with and without pre-training stage to measure impact on performance; 2) Layer sensitivity: Fine-tune BLOOM-1B with different numbers of trainable layers (2, 4, 8, 16) to find optimal balance; 3) Negative sampling: Compare performance with negative sample sizes of 8, 16, 24, 32, 48 to determine optimal size for ranking loss

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of RankingGPT compare to unsupervised ranking methods like pair-wise and list-wise approaches? The paper only provides a comparison on a specific set of datasets (BM25 top-100). It would be valuable to understand the performance on a wider range of datasets and scenarios. Comprehensive experiments comparing RankingGPT to unsupervised methods on a diverse set of datasets would provide a clearer picture of its relative performance.

### Open Question 2
How does the performance of RankingGPT vary with different parameter sizes of the underlying LLM? The paper mentions that RankingGPT is evaluated on different LLMs of varying sizes, but it does not provide a detailed analysis of how the performance changes with different parameter sizes. Understanding the relationship between parameter size and performance would provide insights into the scalability and efficiency of RankingGPT. A detailed analysis of RankingGPT's performance across a range of parameter sizes would help understand the impact of model size on ranking accuracy.

### Open Question 3
How does RankingGPT perform in scenarios with limited or noisy training data? The paper does not explicitly discuss the performance of RankingGPT in scenarios with limited or noisy training data. The performance of RankingGPT in scenarios with limited or noisy training data is crucial for its practical applicability. Experiments evaluating RankingGPT's performance with varying levels of training data quality and quantity would help understand its robustness in different data scenarios.

## Limitations
- Weakly supervised dataset construction relies heavily on high-level descriptions without full methodological transparency
- The specific contribution of each loss function in the fine-tuning stage remains unclear
- Out-of-domain performance shows significant variance across different datasets despite overall improvements

## Confidence
- **High Confidence**: The two-stage training framework and general methodology are well-defined and reproducible
- **Medium Confidence**: The reported performance improvements are significant but may vary with different implementation choices and hyperparameter settings
- **Medium Confidence**: The claim about maintaining generation capabilities while optimizing for ranking is supported by PPL metrics but needs more extensive validation

## Next Checks
1. **Layer Sensitivity Analysis**: Conduct a systematic study varying the number of trainable layers (2, 4, 8, 16, all layers) during fine-tuning to determine optimal configuration for different model sizes
2. **Cross-Architecture Transfer**: Apply the two-stage approach to additional LLM architectures (e.g., GPT-Neo, OPT) to validate generalizability beyond the four tested models
3. **Long-Term Stability**: Evaluate model performance after extended inference sessions to verify sustained generation capabilities and ranking effectiveness under real-world usage conditions