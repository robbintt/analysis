---
ver: rpa2
title: Can Large Language Models Design Accurate Label Functions?
arxiv_id: '2311.00739'
source_url: https://arxiv.org/abs/2311.00739
tags:
- label
- data
- datasets
- accuracy
- datasculpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DataSculpt explores whether large language models can autonomously
  generate accurate label functions for data programming. It proposes an interactive
  framework using various prompting techniques, instance selection strategies, and
  LF filtration methods.
---

# Can Large Language Models Design Accurate Label Functions?

## Quick Facts
- arXiv ID: 2311.00739
- Source URL: https://arxiv.org/abs/2311.00739
- Reference count: 40
- One-line primary result: PLMs can generate accurate keyword-based LFs for general knowledge tasks, sometimes outperforming human-designed ones

## Executive Summary
This paper investigates whether large language models (LLMs) can autonomously generate accurate label functions (LFs) for data programming in weak supervision. The authors propose DataSculpt, an interactive framework that leverages various prompting techniques, instance selection strategies, and LF filtration methods to generate and refine LFs. Evaluations on 12 real-world datasets show that PLMs can generate accurate keyword-based LFs for tasks requiring general knowledge, sometimes outperforming human-designed LFs from the WRENCH benchmark. However, PLMs are less effective for pattern-based LFs and tasks requiring domain expertise. The study highlights the potential and limitations of PLMs in LF design and suggests future research directions to improve their applicability.

## Method Summary
The paper proposes DataSculpt, a framework for generating label functions using large language models. The framework iteratively prompts PLMs (GPT-3.5, GPT-4, Llama2-CHAT) to generate keyword-based or pattern-based LFs for text classification tasks. It uses various prompting techniques (few-shot, chain-of-thought, self-consistency), instance selection strategies (random sampling, uncertain sampling, SEU), and LF filters (validity, accuracy threshold 0.6, redundancy 95%). The generated LFs are aggregated using a label model (MeTaL) to produce probabilistic labels for the training data, which are then used to train a downstream classifier (logistic regression). The framework is evaluated on 12 real-world text datasets covering tasks like spam detection, sentiment analysis, topic classification, and relation classification.

## Key Results
- PLMs can generate accurate keyword-based LFs for tasks requiring general knowledge, sometimes outperforming human-designed LFs
- Self-consistency prompting improves the end-to-end performance of the framework by generating a larger set of candidate LFs
- Random sampling of query instances generally performs well in generating accurate LFs, leading to the best downstream model performance in 6 out of 12 evaluated datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can generate accurate keyword-based label functions for tasks requiring general knowledge.
- Mechanism: PLMs leverage their broad training data to identify common patterns and associations between words and labels, enabling them to propose accurate keyword-based LFs for tasks like sentiment analysis or spam detection.
- Core assumption: The tasks rely on patterns and associations that are well-represented in the PLM's pre-training corpus.
- Evidence anchors:
  - [abstract] "evaluations on 12 real-world datasets show that PLMs can generate accurate keyword-based LFs for tasks requiring general knowledge, sometimes outperforming human-designed ones."
  - [section 4.2] "For tasks that require general knowledge, such as sentiment analysis and spam detection, the accuracy of generated LFs is usually high (> 70%)."
  - [corpus] Weak (no direct mention of accuracy thresholds or specific examples of generated LFs)
- Break condition: When tasks require domain-specific knowledge not well-represented in the PLM's training data.

### Mechanism 2
- Claim: Self-consistency prompting improves the end-to-end performance of the framework by generating a larger set of candidate label functions.
- Mechanism: By prompting the PLM multiple times and aggregating the results, self-consistency increases the diversity and coverage of the generated LFs, leading to better downstream model performance.
- Core assumption: Different PLM responses will provide different but valid label functions, increasing the overall coverage of the LF set.
- Evidence anchors:
  - [section 4.3] "SC generally improves the PLM's response accuracy compared to CoT, which is expected as it is essentially an ensemble of CoT."
  - [section 4.3] "Averaging over all datasets, SC improves the training data coverage by 10.3% compared to Few-Shot and improves the downstream model's performance by 3.5% compared to Few-Shot."
  - [corpus] Weak (no direct mention of self-consistency or its impact on LF diversity)
- Break condition: When the PLM's responses are highly consistent, providing little additional diversity in the generated LFs.

### Mechanism 3
- Claim: Random sampling of query instances generally performs well in generating accurate label functions.
- Mechanism: Random sampling selects a diverse set of instances, allowing the PLM to generate a wide variety of LFs and increasing the coverage of the LF set.
- Core assumption: A diverse set of instances will lead to a more comprehensive set of LFs, improving the downstream model's performance.
- Evidence anchors:
  - [section 4.5] "While the best selection method is dataset-dependent, we find that random sampling generally performs well in our experiments, which leads to the best downstream model performance in 6 out of 12 evaluated datasets."
  - [section 4.5] "As random sampling selects a diverse set of instances for PLMs to provide label functions, it helps DataSculpt to get a larger LF set and increases the training data coverage, which improves the performance of the downstream model."
  - [corpus] Weak (no direct mention of random sampling or its impact on LF diversity)
- Break condition: When the task requires focusing on specific types of instances to generate accurate LFs, and random sampling misses these critical instances.

## Foundational Learning

- Concept: Weak supervision and data programming
  - Why needed here: Understanding the framework in which the generated label functions will be used is crucial for designing an effective LF generation system.
  - Quick check question: What is the role of the label model in the data programming paradigm, and how does it use the outputs of the generated LFs?

- Concept: Pre-trained language models and prompting techniques
  - Why needed here: The core of the system relies on leveraging PLMs to generate label functions, so understanding their capabilities and limitations is essential.
  - Quick check question: How does chain-of-thought prompting work, and in which scenarios has it been shown to improve PLM performance?

- Concept: Active learning and instance selection strategies
  - Why needed here: The system iteratively selects instances to generate LFs, so understanding different selection strategies and their impact on the quality of generated LFs is important.
  - Quick check question: What is the difference between random sampling and uncertain sampling in the context of active learning, and how might these strategies affect the quality of generated LFs?

## Architecture Onboarding

- Component map: Prompt template -> In-context example selection -> Query instance selection -> LF generation -> LF filtering -> Label model -> Downstream model

- Critical path:
  1. Select an instance from the unlabeled data using the query instance selection strategy.
  2. Build a prompt using the selected instance and in-context examples.
  3. Send the prompt to the PLM and extract the generated LFs from the response.
  4. Apply the LF filtering steps to ensure the quality of the generated LFs.
  5. Add the verified LFs to the LF set and update the label model.
  6. Train the downstream model using the labels generated by the label model.

- Design tradeoffs:
  - Prompting method: Different prompting methods (e.g., few-shot, chain-of-thought, self-consistency) have varying effects on the PLM's performance and the quality of generated LFs. The choice of prompting method should balance the trade-off between accuracy and computational cost.
  - Query instance selection: Different instance selection strategies (e.g., random sampling, uncertain sampling, SEU) have varying effects on the diversity and coverage of the generated LFs. The choice of selection strategy should balance the trade-off between exploration and exploitation.
  - LF filtering: Applying more stringent filtering criteria (e.g., higher accuracy threshold, lower redundancy threshold) can improve the quality of the LF set but may also reduce its size and coverage. The choice of filtering criteria should balance the trade-off between quality and quantity.

- Failure signatures:
  - Low downstream model performance: This could indicate that the generated LFs are not accurate or diverse enough, or that the label model is not effectively aggregating the LF outputs.
  - High variance across runs: This could indicate that the system is sensitive to the choice of query instances or in-context examples, or that the PLM's responses are inconsistent.
  - Imbalanced LF set: This could indicate that the system is biased towards generating LFs for certain classes, potentially due to the distribution of the training data or the PLM's inherent biases.

- First 3 experiments:
  1. Evaluate the basic few-shot prompting version of the system on a simple text classification task (e.g., sentiment analysis) to establish a baseline performance.
  2. Compare the performance of different prompting methods (e.g., few-shot, chain-of-thought, self-consistency) on the same task to identify the most effective approach.
  3. Experiment with different query instance selection strategies (e.g., random sampling, uncertain sampling, SEU) to determine their impact on the quality and diversity of the generated LFs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop instance selection methods that consider the imperfect nature of PLMs and learn from their feedback in the LF development process?
- Basis in paper: [explicit] The paper states that current active query instance selection methods do not work well for LF development with PLMs because they do not consider the imperfect nature of PLMs or learn from PLM feedback. It also mentions that random sampling generally performs well but further research is required to design more effective selection methods.
- Why unresolved: Current selection methods like uncertain sampling and SEU are designed for perfect oracles and do not adapt to the feedback from imperfect PLMs. The paper shows that random sampling outperforms these methods, indicating a gap in existing approaches.
- What evidence would resolve it: Development and evaluation of new instance selection methods specifically designed for PLM-based LF generation that can learn from PLM feedback and outperform random sampling in terms of LF quality and downstream model performance.

### Open Question 2
- Question: How can we effectively address the LF class imbalance issue where the number of LFs for some classes is much higher than others?
- Basis in paper: [explicit] The paper mentions the LF class imbalance issue, where the number of LFs for some classes is much higher than others. It suggests two possible sources: skewed label distribution in the training dataset or easier LF generation for some classes by PLMs. The paper applies a default-class technique to mitigate this issue but calls for further research to address it.
- Why unresolved: The default-class technique only provides a partial solution by assigning instances not covered by any LF to a default class. It does not address the root causes of the imbalance, which could be due to dataset characteristics or PLM capabilities.
- What evidence would resolve it: Development and evaluation of methods that can generate more balanced LF sets across classes, either by improving PLM capabilities for underrepresented classes or by intelligently sampling instances to promote balance.

### Open Question 3
- Question: How can we improve the robustness of the PLM-based LF generation framework to reduce variance across different runs?
- Basis in paper: [explicit] The paper notes that variance across different runs is large in some evaluated datasets due to non-deterministic components like query instance selection, in-context example selection, and PLM responses. It suggests that future research can explore improvements to the framework's robustness by reducing variance in these steps.
- Why unresolved: The current framework's non-deterministic components lead to inconsistent LF generation and downstream model performance across runs. This variance can make it difficult to rely on the framework for practical applications and hinders reproducibility.
- What evidence would resolve it: Development and evaluation of techniques to reduce variance in PLM-based LF generation, such as more stable instance selection methods, deterministic in-context example selection, or averaging multiple PLM responses. These techniques should lead to more consistent LF generation and downstream model performance across runs.

## Limitations

- The evaluation doesn't fully explore the impact of PLM model size or training data composition on LF generation quality.
- The study focuses primarily on keyword-based LFs without extensive validation of pattern-based LF generation.
- The evaluation framework doesn't account for potential biases in the PLM's training data that could affect LF quality.

## Confidence

- High confidence: PLMs can generate accurate keyword-based LFs for general knowledge tasks
- Medium confidence: Self-consistency prompting improves LF generation quality
- Medium confidence: Random sampling is generally effective for instance selection
- Low confidence: PLMs' effectiveness for pattern-based LFs and domain-specific tasks

## Next Checks

1. Conduct controlled experiments varying PLM model sizes and training data characteristics to quantify their impact on LF generation accuracy for both keyword and pattern-based LFs.
2. Implement a systematic evaluation framework to measure potential biases in generated LFs across different demographic and domain-specific categories.
3. Extend the evaluation to include more diverse domain-specific tasks and pattern-based LF generation, with human expert validation of generated LFs to establish ground truth accuracy.