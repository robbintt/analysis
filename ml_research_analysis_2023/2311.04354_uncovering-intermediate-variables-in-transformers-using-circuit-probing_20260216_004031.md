---
ver: rpa2
title: Uncovering Intermediate Variables in Transformers using Circuit Probing
arxiv_id: '2311.04354'
source_url: https://arxiv.org/abs/2311.04354
tags:
- circuit
- probing
- task
- accuracy
- variable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces circuit probing, a method to uncover low-level
  circuits that compute hypothesized intermediate variables in neural networks. The
  key idea is to optimize a binary mask over model weights to partition outputs according
  to the hypothesized variable, enabling causal analysis through targeted ablation.
---

# Uncovering Intermediate Variables in Transformers using Circuit Probing

## Quick Facts
- arXiv ID: 2311.04354
- Source URL: https://arxiv.org/abs/2311.04354
- Reference count: 40
- Key outcome: Circuit probing method discovers low-level circuits computing hypothesized intermediate variables through binary mask optimization, showing improved faithfulness compared to linear/nonlinear probing

## Executive Summary
This paper introduces circuit probing, a novel method for uncovering low-level circuits in neural networks that compute hypothesized intermediate variables. The key innovation is optimizing a binary mask over model weights to partition outputs according to the hypothesized variable, enabling causal analysis through targeted ablation. Circuit probing is demonstrated to be more faithful to underlying model computation than traditional probing methods and does not require specifying a full causal graph. The method successfully uncovers circuits responsible for subject-verb agreement and reflexive anaphora in GPT2 models, and reveals modular structure in multitask learning scenarios.

## Method Summary
Circuit probing introduces a trainable binary mask over model weights that is optimized to uncover circuits computing high-level intermediate variables. The method uses continuous sparsification to optimize binary masks with a soft nearest neighbors loss function and l0 regularization to encourage sparsity. The discovered circuits are evaluated using 1-nearest neighbor classifier accuracy and validated through ablation analysis. Unlike linear or nonlinear probing, circuit probing directly optimizes the model's parameters to partition outputs according to the hypothesized variable, making it more causally aligned with the model's actual computation.

## Key Results
- Circuit probing successfully discovers circuits for intermediate variables in simple arithmetic tasks, outperforming linear and nonlinear probing
- The method reveals modular structure in multitask learning, showing shared vs. free variable computations
- Applied to GPT2, circuit probing uncovers circuits responsible for syntactic agreement computations in subject-verb agreement and reflexive anaphora tasks
- Circuit probing tracks circuit development throughout training, showing more faithful representation of intermediate variable computations compared to linear probing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Circuit probing discovers low-level circuits that compute hypothesized intermediate variables by optimizing a binary mask over model weights.
- Mechanism: The method introduces a trainable binary mask over neurons in the model. This mask is optimized to partition outputs according to the hypothesized variable, using a contrastive loss (soft nearest neighbors) that minimizes distance between inputs of the same class and maximizes distance between inputs of different classes.
- Core assumption: Model components that compute an intermediate variable should produce outputs that are partitioned according to that variable.
- Break condition: If the hypothesized intermediate variable does not actually exist as a computed quantity in the model, the optimization will fail to find a meaningful circuit.

### Mechanism 2
- Claim: Circuit probing is more faithful to the underlying model than linear or nonlinear probing.
- Mechanism: By directly optimizing a mask to partition outputs according to the hypothesized variable, circuit probing identifies circuits that are causally implicated in model behavior, rather than just decodable from representations.
- Core assumption: Variables that are causally implicated in model behavior should be discoverable through targeted optimization of model parameters.
- Break condition: If the optimization process gets stuck in local minima or if the l0 regularization is too strong, the discovered circuit may not accurately reflect the true computation.

### Mechanism 3
- Claim: Circuit probing enables tracking of circuit development throughout training.
- Mechanism: By applying circuit probing at different stages of training, one can observe how the circuit responsible for a particular intermediate variable forms and develops over time.
- Core assumption: The circuit computing a specific intermediate variable should develop continuously throughout training, even if overall model performance shows discontinuous jumps.
- Break condition: If the optimization process for discovering circuits is too computationally expensive, it may not be practical to apply circuit probing at many different stages of training.

## Foundational Learning

- Concept: Binary mask optimization
  - Why needed here: Circuit probing relies on optimizing a binary mask over model weights to identify circuits computing specific intermediate variables.
  - Quick check question: What is the purpose of the binary mask in circuit probing?

- Concept: Contrastive loss functions
  - Why needed here: Circuit probing uses soft nearest neighbors loss, a type of contrastive loss, to optimize the binary mask.
  - Quick check question: How does the soft nearest neighbors loss function encourage the discovery of circuits computing specific intermediate variables?

- Concept: Causal abstraction
  - Why needed here: Circuit probing is positioned as an alternative to causal abstraction analysis for investigating intermediate variables in neural networks.
  - Quick check question: What is the key difference between circuit probing and causal abstraction analysis in terms of the hypotheses they require?

## Architecture Onboarding

- Component map: Model weights -> Binary mask optimization module -> Contrastive loss (soft nearest neighbors) -> Probing evaluation (1-NN classifier) -> Ablation analysis
- Critical path: (1) Define hypothesized intermediate variable and label dataset, (2) Initialize and optimize binary mask using contrastive loss, (3) Evaluate discovered circuit with nearest neighbor classifier, (4) Perform ablation to test causality
- Design tradeoffs: Circuit probing trades computational cost (optimizing a binary mask is expensive) for accuracy (it can discover circuits that linear/nonlinear probing might miss). It also trades the requirement for a full causal graph (needed for causal abstraction) for the ability to test individual intermediate variables.
- Failure signatures: Circuit probing may fail if: (1) the hypothesized variable does not exist in the model, (2) the optimization gets stuck in local minima, (3) the l0 regularization is too strong and prevents meaningful circuits from being discovered, or (4) the contrastive loss is not well-suited to the specific task.
- First 3 experiments:
  1. Apply circuit probing to a simple arithmetic task where the full causal graph is known, comparing results to linear/nonlinear probing and causal abstraction.
  2. Use circuit probing to investigate the internal structure of a transformer trained on a multitask problem, testing for modular computations.
  3. Apply circuit probing to track the development of circuits throughout training on a task that exhibits grokking behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multiple circuits composing within a given block create one additive update to the residual stream, and can this be used to enable counterfactual interventions?
- Basis in paper: The paper states that circuit probing is weaker than counterfactual embeddings and causal abstraction analysis because it does not allow for counterfactual interventions, and notes that it is currently unknown how multiple circuits compose within a given block to create one additive update to the residual stream.
- Why unresolved: Understanding the composition of multiple circuits within a block requires further research into the mechanisms of circuit interaction and their collective impact on the residual stream.
- What evidence would resolve it: Experimental evidence showing how multiple circuits interact within a block to produce the final output, and demonstrations of successful counterfactual interventions using circuit probing.

### Open Question 2
- Question: How can circuit probing be extended to assess models for social biases in real-world systems, given its current limitations?
- Basis in paper: The paper mentions that circuit probing can provide positive evidence that a computation is implemented but cannot yet be used to provide evidence that a computation is definitely not implemented in a real-world system, and that it should not be used in isolation to assess models for social biases in real-world systems.
- Why unresolved: Extending circuit probing to real-world bias assessment requires developing methods to definitively rule out the presence of specific computations and integrating circuit probing with other bias detection techniques.
- What evidence would resolve it: Case studies demonstrating the successful use of circuit probing in conjunction with other methods to identify and mitigate social biases in real-world models.

### Open Question 3
- Question: What are the implications of the modular structure of intermediate variables for the generalizability and robustness of transformer models?
- Basis in paper: The paper demonstrates that circuit probing can reveal modular structure within a model and shows that ablating the circuit computing the shared variable for either task destroys performance across both tasks, while ablating the free variable harms performance on the specific task more than the other.
- Why unresolved: The impact of modular intermediate variables on model generalizability and robustness is not fully explored, and further research is needed to understand how this modularity affects model performance across diverse tasks.
- What evidence would resolve it: Comparative studies of model performance on out-of-distribution tasks with and without modular intermediate variables, and analyses of how modularity affects model robustness to adversarial attacks.

## Limitations
- Circuit probing requires computationally expensive optimization of binary masks for each hypothesized variable
- The method's effectiveness on deeper, more complex models remains to be thoroughly validated
- Circuit probing provides positive but not negative evidence about the presence of computations, limiting its use for definitive bias assessment
- The method assumes hypothesized intermediate variables actually exist in the model, which may not always be the case

## Confidence
- High confidence: Core claim that circuit probing can discover circuits computing intermediate variables when they exist in the model
- Medium confidence: Claim that circuit probing is more faithful to underlying model computation than linear/nonlinear probing
- Medium confidence: Claim that circuit probing enables tracking of circuit development throughout training

## Next Checks
1. Perform ablation analysis across multiple random seeds for the mask optimization process to verify that discovered circuits are stable and not artifacts of optimization initialization.
2. Apply circuit probing to a deeper transformer model (e.g., GPT2-Medium) with multiple hypothesized intermediate variables to test whether the method scales and whether circuits remain interpretable in more complex architectures.
3. Compare the soft nearest neighbors loss with other contrastive loss functions (e.g., NT-Xent, triplet loss) to determine whether the choice of loss function significantly impacts circuit discovery quality or whether the method is robust to this design choice.