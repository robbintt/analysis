---
ver: rpa2
title: Phase-shifted Adversarial Training
arxiv_id: '2301.04785'
source_url: https://arxiv.org/abs/2301.04785
tags:
- training
- adversarial
- attacks
- frequency
- phaseat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Phase-shifted Adversarial Training (PhaseAT),
  which addresses the problem of slow convergence to high-frequency information in
  adversarial training. The authors analyze the behavior of adversarial training through
  the lens of response frequency, discovering that it causes neural networks to have
  low convergence to high-frequency information, resulting in highly oscillated predictions
  near each data point.
---

# Phase-shifted Adversarial Training

## Quick Facts
- arXiv ID: 2301.04785
- Source URL: https://arxiv.org/abs/2301.04785
- Authors: 
- Reference count: 19
- Key outcome: PhaseAT significantly improves convergence for high-frequency information, resulting in improved adversarial robustness and strong performance compared to other baselines.

## Executive Summary
This paper introduces Phase-shifted Adversarial Training (PhaseAT), which addresses the problem of slow convergence to high-frequency information in adversarial training. The authors analyze the behavior of adversarial training through the lens of response frequency, discovering that it causes neural networks to have low convergence to high-frequency information, resulting in highly oscillated predictions near each data point. To address this, they prove that the universal phenomenon of frequency principle, i.e., lower frequencies are learned first, still holds in adversarial training. Based on this, they propose PhaseAT, which learns high-frequency components by shifting them to the low-frequency range where fast convergence occurs. The proposed method is evaluated on CIFAR-10 and ImageNet datasets with adaptive attacks designed for reliable evaluation. Comprehensive results show that PhaseAT significantly improves convergence for high-frequency information, resulting in improved adversarial robustness by enabling the model to have smoothed predictions near each data point.

## Method Summary
PhaseAT addresses slow convergence to high-frequency information in adversarial training by leveraging the frequency principle (F-principle), which states that lower frequencies are learned first. The method uses a multi-headed neural network architecture (PhaseDNN) with frequency-dedicated heads, where each head is responsible for learning specific frequency ranges. During training, PhaseAT generates adversarial examples and calculates frequency discrepancies between clean and adversarial data. Frequencies are then sampled based on these discrepancies, and phase-shifted predictions are performed in alternate mini-batches. A regularization term encourages diverse predictions compared to normal adversarial training. The method is evaluated on CIFAR-10 and ImageNet datasets using adaptive attacks designed for reliable evaluation.

## Key Results
- PhaseAT significantly improves convergence for high-frequency information compared to standard adversarial training
- The method achieves strong performance compared to other baselines, with improvements in both standard and robust accuracy
- PhaseAT enables the model to have smoothed predictions near each data point, improving adversarial robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phase-shifted adversarial training (PhaseAT) enables neural networks to converge to high-frequency information by shifting high-frequency components to the low-frequency range where fast convergence occurs.
- Mechanism: The method uses phase-shifted adversarial training to learn high-frequency components by shifting these frequencies to the low-frequency range where the fast convergence occurs. This is based on the universal phenomenon of frequency principle, i.e., lower frequencies are learned first.
- Core assumption: The frequency principle (F-principle) holds in adversarial training, meaning lower frequencies are learned first.
- Evidence anchors:
  - [abstract]: "we propose phase-shifted adversarial training (PhaseAT) in which the model learns high-frequency components by shifting these frequencies to the low-frequency range where the fast convergence occurs."
  - [section 3.2]: "Theorem 3.2 implies that the rate of decrease in the corresponding high-frequency region loss (L≥(θ,η )) is much greater than the rate of increase in η. In other words, a model tends to fit target functions from low to high frequency information during the adversarial training."
  - [corpus]: Weak evidence. The corpus papers discuss frequency interpretation of adversarial robustness and high-frequency problems but don't directly support the specific PhaseAT mechanism.
- Break condition: If the frequency principle does not hold in adversarial training, or if the phase shifting introduces significant computational overhead that negates the benefits.

### Mechanism 2
- Claim: PhaseAT improves adversarial robustness by enabling the model to have smoothed predictions near each data point.
- Mechanism: By efficiently learning high-frequency information, PhaseAT reduces the highly oscillated predictions near each data point that result from adversarial training's low convergence to high-frequency information.
- Core assumption: Improved convergence to high-frequency information leads to smoothed predictions near data points.
- Evidence anchors:
  - [abstract]: "Comprehensive results show that PhaseAT significantly improves the convergence for high-frequency information. This results in improved adversarial robustness by enabling the model to have smoothed predictions near each data."
  - [section 5.2]: "Figure 2 shows the errors for each frequency. Here, we use the settings of CIFAR-10. For the low frequency part, the errors between PhaseAT and AT is negligible, whereas the errors of the high frequency between two methods is noticeable, demonstrating our hypothesis that PhaseAT can efficiently learn high-frequency components."
  - [corpus]: Weak evidence. While the corpus discusses frequency interpretation and robustness, it doesn't specifically validate the smoothed predictions claim for PhaseAT.
- Break condition: If the relationship between high-frequency convergence and prediction smoothing is not causal, or if other factors dominate the adversarial robustness improvement.

### Mechanism 3
- Claim: PhaseAT outperforms other strong baselines by a large margin in many different settings.
- Mechanism: The combination of efficient high-frequency learning and smoothed predictions leads to improved performance across various datasets and architectures.
- Core assumption: The performance improvements observed in experiments generalize across different settings.
- Evidence anchors:
  - [abstract]: "Comprehensive results show that PhaseAT significantly improves the convergence for high-frequency information. This results in improved adversarial robustness by enabling the model to have smoothed predictions near each data. The method achieves strong performance compared to other baselines, with improvements in both standard and robust accuracy."
  - [section 5.2]: "Table 1 shows the comparison results... PhaseAT outperforms FBF, the major distinction of which is the phase shift functionality. This suggests that learning high-frequency information properly is particularly beneficial for learning adversarial data."
  - [corpus]: Weak evidence. The corpus papers discuss related frequency-based approaches but don't directly validate PhaseAT's performance claims.
- Break condition: If the performance improvements are specific to the experimental conditions and don't generalize to other datasets, architectures, or threat models.

## Foundational Learning

- Concept: Frequency Principle (F-principle)
  - Why needed here: Understanding the F-principle is crucial for grasping how PhaseAT leverages frequency shifting to improve learning efficiency.
  - Quick check question: What is the frequency principle, and why is it relevant to adversarial training?

- Concept: Adversarial Training
  - Why needed here: PhaseAT builds upon and modifies traditional adversarial training, so understanding its fundamentals is essential.
  - Quick check question: How does adversarial training work, and what are its limitations in terms of frequency convergence?

- Concept: Fourier Analysis
  - Why needed here: The method relies on frequency domain analysis to understand and improve learning behavior.
  - Quick check question: How does Fourier analysis help in understanding neural network learning behavior in the context of adversarial training?

## Architecture Onboarding

- Component map:
  - PhaseDNN: Multi-headed neural network with frequency-dedicated heads
  - Frequency selection mechanism: Samples frequencies based on discrepancies between clean and adversarial data
  - Adversarial training loop: Generates adversarial examples and updates model parameters
  - Regularization term: Encourages different predictions than normal adversarial training

- Critical path:
  1. Initialize PhaseDNN with multiple heads
  2. For each batch, generate adversarial examples
  3. Calculate frequency discrepancies between clean and adversarial data
  4. Sample frequencies for each head based on discrepancies
  5. Perform forward and backward passes with phase-shifted predictions
  6. Apply regularization to encourage diverse predictions
  7. Update model parameters

- Design tradeoffs:
  - More heads improve accuracy but increase computational cost
  - Wider frequency range improves robustness but may not significantly affect clean accuracy
  - Stochastic frequency sampling improves robustness against black-box attacks but complicates evaluation

- Failure signatures:
  - Poor convergence to high frequencies despite phase shifting
  - Overfitting to specific frequency ranges
  - Increased vulnerability to adaptive attacks that exploit the stochastic process
  - Computational overhead negating performance benefits

- First 3 experiments:
  1. Implement basic PhaseDNN with 2 heads and frequency range [0, 10000) on CIFAR-10
  2. Compare convergence rates of high-frequency components between PhaseAT and standard adversarial training
  3. Evaluate robustness against PGD and AutoAttack on CIFAR-10 with and without regularization term

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the frequency principle generalize to other loss functions beyond cross-entropy and mean-squared error?
- Basis in paper: [explicit] The paper states that the F-principle holds for cross-entropy loss and extends the theory to include this case, but doesn't explore other loss functions.
- Why unresolved: The paper focuses on cross-entropy and mean-squared error, leaving the behavior of other loss functions unexplored.
- What evidence would resolve it: Empirical and theoretical analysis of the F-principle behavior with various loss functions (e.g., hinge loss, focal loss) in adversarial training scenarios.

### Open Question 2
- Question: What is the impact of using different activation functions on the convergence rate of high-frequency components in adversarial training?
- Basis in paper: [explicit] The paper mentions that the regularity of the activation function affects the convergence rate, with ReLU having s=1 and ELU having s=2, but doesn't provide a comprehensive analysis.
- Why unresolved: The paper provides theoretical bounds but doesn't empirically compare different activation functions' impact on high-frequency convergence.
- What evidence would resolve it: Systematic experiments comparing convergence rates of high-frequency components across various activation functions in adversarial training settings.

### Open Question 3
- Question: How does the proposed PhaseAT method perform against adaptive black-box attacks?
- Basis in paper: [inferred] The paper evaluates black-box attacks but focuses on transfer-based and query-based attacks, not specifically adaptive black-box attacks.
- Why unresolved: The paper doesn't explore the robustness of PhaseAT against adaptive black-box attacks, which could potentially exploit the stochastic nature of the method.
- What evidence would resolve it: Empirical evaluation of PhaseAT against adaptive black-box attacks that can adjust their strategy based on the model's stochastic behavior.

## Limitations
- The computational overhead of multi-headed architectures and frequency-specific processing could potentially offset robustness gains in resource-constrained scenarios
- The theoretical analysis relies on assumptions about the frequency principle's applicability in adversarial settings that may not always hold
- Performance improvements are demonstrated primarily on CIFAR-10 and ImageNet, with limited testing across diverse architectures

## Confidence

- **High Confidence**: The frequency principle holds in standard training settings, providing a solid theoretical foundation for the approach
- **Medium Confidence**: PhaseAT's improvements over standard adversarial training are demonstrated but may be sensitive to implementation details and hyperparameter choices
- **Medium Confidence**: The claim that PhaseAT significantly improves high-frequency convergence is supported by experiments but requires further validation across diverse datasets and architectures

## Next Checks

1. **Cross-architecture validation**: Test PhaseAT on architectures beyond ResNet (e.g., EfficientNet, MobileNet) to assess generalization of performance improvements
2. **Computational efficiency analysis**: Quantify the computational overhead of PhaseAT compared to standard adversarial training across different hardware configurations
3. **Robustness under distribution shift**: Evaluate PhaseAT's performance on out-of-distribution datasets and against real-world adversarial attacks to assess practical utility