---
ver: rpa2
title: 'Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song'
arxiv_id: '2308.02249'
source_url: https://arxiv.org/abs/2308.02249
tags:
- tori
- pitch
- korean
- music
- folk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of analyzing a large dataset
  of Korean folk songs, which were field-recorded without professional accompaniment,
  making traditional analysis methods difficult to apply. The authors propose using
  self-supervised learning with a convolutional neural network (CNN) based on pitch
  contour to capture the musical concept of tori, a classification system defined
  by specific scales, ornamental notes, and idiomatic melodic contours.
---

# Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song

## Quick Facts
- arXiv ID: 2308.02249
- Source URL: https://arxiv.org/abs/2308.02249
- Reference count: 0
- Primary result: Self-supervised CNN on pitch contour achieves nDCG 0.853 and classification accuracy 0.848 ± 0.039 for tori analysis in Korean folk songs

## Executive Summary
This paper addresses the challenge of analyzing Korean folk songs using self-supervised learning on pitch contour data. The authors propose a convolutional neural network that learns to capture tori, a classification system based on scales, ornamental notes, and melodic contours, without requiring explicit annotations for most of the dataset. The method outperforms traditional pitch histogram approaches in both similarity ranking and classification accuracy, revealing meaningful clustering patterns that correspond to musical discussions in academia.

## Method Summary
The authors extract pitch contours from field recordings of Korean folk songs using CREPE at 20 Hz frame rate with confidence threshold 0.8. They filter out choir and percussion songs using a CNN-based sound event detection model. The CNN architecture consists of 4 convolutional layers with batch normalization and max pooling, followed by context attention to create 256-dimensional embeddings. The model is trained using self-supervised triplet loss with 8 negative samples per positive, learning from intra-song consistency. Evaluation is performed on a manually annotated subset of 218 songs labeled for tori types using both nDCG ranking similarity and random forest classification accuracy.

## Key Results
- Self-supervised CNN model achieves nDCG of 0.853 for similarity ranking, outperforming traditional pitch histograms (0.700)
- Classification accuracy of 0.848 ± 0.039 using random forest, compared to 0.760 for pitch histograms
- UMAP visualizations show distinct clustering patterns for different tori types, with sub-tori like eosayong clustering separately within menari

## Why This Works (Mechanism)

### Mechanism 1
The self-supervised CNN model learns representations that capture melodic contour features relevant to tori classification better than traditional pitch histograms. The CNN model uses a triplet loss objective with intra-song similarity, forcing the model to produce consistent embeddings for different segments of the same song while distinguishing between different songs. This encourages the model to focus on intrinsic melodic patterns rather than absolute pitch values or timbral characteristics.

### Mechanism 2
Self-supervised learning is more effective than region-supervised learning for tori classification because tori patterns don't align perfectly with regional boundaries. The self-supervised approach learns from intra-song consistency rather than relying on potentially noisy region labels. Since tori types are distributed across multiple regions, region-supervised learning would force the model to learn spurious correlations between region and musical features.

### Mechanism 3
The CNN architecture with context attention can capture temporal dependencies in pitch contour that are important for identifying tori characteristics. The CNN processes pitch contour as a sequence with temporal structure, using multiple convolutional layers with max pooling to extract hierarchical features, followed by context attention to create a fixed-length embedding that summarizes the entire contour while preserving important temporal relationships.

## Foundational Learning

- **Concept: Self-supervised learning and triplet loss**
  - Why needed here: The dataset lacks direct tori annotations for most songs, so the model needs to learn meaningful representations without labeled data. Triplet loss enables learning from intra-song consistency.
  - Quick check question: Can you explain how triplet loss with hinge margin encourages the model to produce similar embeddings for different segments of the same song while distinguishing between different songs?

- **Concept: Pitch contour extraction and tonic normalization**
  - Why needed here: Tori classification depends on relative pitch relationships and melodic patterns, not absolute pitch values. Proper tonic normalization is essential for comparing songs that may be in different keys.
  - Quick check question: Given a pitch contour, can you describe the process of finding the tonic and normalizing the pitch values relative to it?

- **Concept: Context attention for variable-length sequence summarization**
  - Why needed here: Each song has a different length pitch contour, but the model needs to produce fixed-size embeddings for comparison. Context attention allows the model to weigh different time steps based on their importance.
  - Quick check question: Can you explain how context attention differs from standard self-attention and why it's useful for summarizing variable-length sequences?

## Architecture Onboarding

- **Component map**: Pitch contour extraction -> CNN feature extraction (4 conv layers) -> Context attention -> 256-dimensional embedding -> Similarity/classification
- **Critical path**: F0 extraction → Pitch contour normalization → CNN feature extraction → Context attention → Embedding → Similarity/classification
- **Design tradeoffs**: Self-supervised learning vs. supervised learning, CNN architecture depth vs. overfitting risk, embedding dimensionality vs. computational efficiency
- **Failure signatures**: Poor classification accuracy indicating the model isn't capturing tori-relevant features, high variance in results suggesting overfitting, UMAP visualizations showing no clear clustering by tori
- **First 3 experiments**:
  1. Compare pitch histogram vs. CNN embedding on the tori-labeled subset using both nDCG ranking and classification accuracy metrics
  2. Test different CNN architectures (varying depth, kernel sizes, pooling strategies) to find the optimal configuration
  3. Evaluate the impact of different F0 extraction parameters (confidence threshold, frame rate) on downstream tori classification performance

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the self-supervised CNN model be trained to classify specific sub-tori types (like eosayong-tori) within menari-tori, or would additional supervision be required?
  - Basis in paper: The paper mentions eosayong-tori as a distinct sub-category of menari-tori and shows it clusters separately in UMAP visualizations, but notes it was not explicitly labeled in their subset
  - Why unresolved: The model was trained only on broader tori categories without distinguishing sub-tori
  - What evidence would resolve it: Experiments showing classification accuracy when training with sub-tori labels

- **Open Question 2**: How do the embeddings learned from pitch contour compare to embeddings learned from raw audio in capturing tori characteristics, particularly for timbral features?
  - Basis in paper: The authors chose pitch contour over raw audio to better capture melodic features rather than timbral characteristics, but don't actually compare these two approaches
  - Why unresolved: The paper only evaluates the pitch contour approach without empirical comparison to raw audio
  - What evidence would resolve it: Side-by-side experiments training CNN models on raw audio versus pitch contour

- **Open Question 3**: Would incorporating temporal information (like song duration or tempo) improve the model's ability to capture tori characteristics, given that some sub-tori like arari are noted for having slower tempos?
  - Basis in paper: The authors observe that arari songs cluster separately and are characterized by slower tempo, but their model only uses pitch contour without temporal features
  - Why unresolved: The current approach focuses solely on pitch contour without exploring additional temporal features
  - What evidence would resolve it: Experiments comparing models with and without tempo/duration features

## Limitations

- The self-supervised learning approach relies on intra-song consistency as the sole training signal, which may not capture all aspects of tori classification
- The manually annotated tori subset (218 songs) is relatively small for evaluating classification performance
- The filtering of choir and percussion songs using a CNN-based sound event detection model introduces additional uncertainty since the exact architecture and threshold values are not specified

## Confidence

- **High confidence**: The experimental results showing superior performance of self-supervised CNN over pitch histograms (nDCG: 0.853 vs. 0.700; accuracy: 0.848 vs. 0.760) are well-supported by the methodology and reproducible metrics
- **Medium confidence**: The claim that self-supervised learning is more appropriate than region-supervised learning due to the complex relationship between tori and region is supported by the data but could benefit from additional ablation studies
- **Medium confidence**: The assertion that the CNN captures temporal melodic patterns relevant to tori classification is plausible given the architecture but lacks direct ablation studies

## Next Checks

1. Perform an ablation study removing the self-supervised learning component and training the same CNN architecture with only the tori labels to quantify the exact contribution of the self-supervised approach
2. Test the model's performance on a held-out test set that was not used in any training or validation to assess generalization beyond the 30-fold cross-validation
3. Compare the CNN embeddings with hand-crafted melodic contour features from music theory to validate whether the learned representations align with human understanding of tori characteristics