---
ver: rpa2
title: 'PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model
  Modification'
arxiv_id: '2308.11822'
source_url: https://arxiv.org/abs/2308.11822
tags:
- patch
- attack
- backdoor
- trigger
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PatchBackdoor, a novel method to conduct\
  \ backdoor attacks against deep neural networks without modifying the victim model.\
  \ The core idea is to train a physical patch that can be placed in the camera view,\
  \ which alters the model\u2019s behavior when combined with attacker-controlled\
  \ triggers."
---

# PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification

## Quick Facts
- arXiv ID: 2308.11822
- Source URL: https://arxiv.org/abs/2308.11822
- Authors: 
- Reference count: 40
- One-line primary result: Physical patch in camera view can backdoor DNNs without model modification, achieving 93-99% attack success rate while maintaining clean accuracy

## Executive Summary
PatchBackdoor introduces a novel method to conduct backdoor attacks against deep neural networks without modifying the victim model. The core idea is to train a physical patch that can be placed in the camera view, which alters the model's behavior when combined with attacker-controlled triggers. The patch is trained using a distillation-style method to mimic normal model behavior while being optimized to cause misclassification under trigger conditions. Experiments demonstrate high attack success rates (93%-99%) while maintaining model accuracy on normal inputs, and the method is robust against common defenses like pruning and fine-tuning.

## Method Summary
PatchBackdoor trains a physical patch that, when placed in the camera view, can backdoor a DNN without modifying the model itself. The patch is optimized using a distillation-style approach, where it learns to minimize the difference between the model's predictions on clean images with and without the patch, while maximizing misclassification when the trigger condition is present. To ensure real-world applicability, the authors model digital-physical transformations to account for visual differences between digital and physical patches. The method is tested on three datasets (CIFAR-10, Imagenette, Caltech101) and three models (VGG, MobileNet, ResNet), showing high attack success rates while maintaining model accuracy on normal inputs.

## Key Results
- Achieves 93%-99% attack success rate while maintaining clean accuracy
- Effective against model pruning, fine-tuning, and distillation defenses
- Resistant to detection by out-of-distribution detectors
- Validated through physical-world deployment tests

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A constant physical patch in the camera view can influence the DNN's internal decision logic without modifying model parameters.
- Mechanism: The patch acts as a fixed input component that becomes part of the image fed to the DNN. By training the patch to maximize the difference between predictions on normal vs. trigger-conditioned inputs, it exploits the model's redundancy to create conditional misbehavior.
- Core assumption: DNNs have sufficient redundant capacity that adding a constant patch to the input can influence classification decisions without degrading overall accuracy.
- Evidence anchors:
  - [abstract] "Instead of injecting backdoor logic into the training data or the model, we propose to place a carefully-designed patch (namely backdoor patch) in front of the camera"
  - [section] "Our insight is to inject backdoor logic by attaching a constant input patch, which is feasible since many vision applications have an unchanged foreground/background"
  - [corpus] Found 25 related papers, but no direct evidence that constant patches can backdoor models without model modification. This appears to be novel.
- Break condition: If the DNN has no redundant capacity or if the patch is filtered/removed before input, the attack fails.

### Mechanism 2
- Claim: The patch can be trained using a distillation-style method without access to labeled training data.
- Mechanism: The patch is optimized to minimize the difference between the model's predictions on clean images with the patch versus predictions on clean images without the patch, while simultaneously maximizing misclassification when the trigger condition is present.
- Core assumption: The original model's predictions on clean images can serve as pseudo-labels for training the patch.
- Evidence anchors:
  - [section] "We borrow the idea of model distillation to solve this challenge - We can consider the original model with the constant patch as a new model ùëì ‚Ä≤ (ùë•) = ùëì (ùë• ‚äï ùëù)"
  - [section] "Then, ùëì ‚Ä≤ can be trained by distilling knowledge from ùëì , i.e., Equation 3 can be written as: ùêøùëêùëôùëíùëéùëõ (ùëù) = ‚àëÔ∏Åùë• ‚ààùëã ùêø(ùëì (ùë• ‚äï ùëù), ùëì (ùë•))"
  - [corpus] No direct evidence that distillation-style training works for backdoor patches without labeled data. This is a novel contribution.
- Break condition: If the model cannot provide reliable pseudo-labels or if the patch optimization diverges, the attack fails.

### Mechanism 3
- Claim: Digital-physical transformation modeling enables the patch to work in real-world deployment despite visual differences between digital and physical patches.
- Mechanism: A differentiable transformation function (shape + color) maps the digitally-trained patch to its physical-world appearance, accounting for perspective distortion and color shifts.
- Core assumption: The transformation between digital and physical patches can be modeled as a differentiable function that can be optimized during patch training.
- Evidence anchors:
  - [section] "Our key idea is to model the digital-physical gap with a differentiable transformation, and optimizing the backdoor patch using this transformation"
  - [section] "We model the simple shape transformation of each micro-surface and combine them to form the whole shape transformation"
  - [corpus] No direct evidence that differentiable digital-physical transformations work for adversarial patches. This appears to be novel.
- Break condition: If the physical transformation cannot be accurately modeled or if the differentiable approximation is insufficient, the patch fails in deployment.

## Foundational Learning

- Concept: Adversarial patches and their effect on DNNs
  - Why needed here: Understanding how input patches can manipulate DNN predictions is foundational to grasping PatchBackdoor's approach.
  - Quick check question: What is the key difference between an adversarial patch and a backdoor patch in terms of their goals?

- Concept: Model distillation and knowledge transfer
  - Why needed here: The patch training method relies on distilling knowledge from the original model to train the patch without labeled data.
  - Quick check question: How does using the model's own predictions as pseudo-labels enable training without access to the original training data?

- Concept: Digital-physical transformation and camera imaging
  - Why needed here: The patch must work in the real world, so understanding how digital images transform when captured by cameras is crucial.
  - Quick check question: What are the main visual differences between a digital image and a photo of that image taken by a camera?

## Architecture Onboarding

- Component map:
  - Input pipeline: Camera feed ‚Üí Patch attachment ‚Üí Image preprocessing
  - Model: Pre-trained DNN (VGG, ResNet, MobileNet)
  - Patch training module: Optimization loop with loss functions
  - Digital-physical transformation module: Shape and color transformation modeling
  - Evaluation module: Attack success rate and clean accuracy measurement

- Critical path:
  1. Obtain victim model and clean image dataset
  2. Define trigger condition and patch placement
  3. Train backdoor patch using distillation-style method
  4. Apply digital-physical transformation for real-world deployment
  5. Evaluate attack effectiveness and stealthiness

- Design tradeoffs:
  - Patch size vs. clean accuracy: Larger patches can carry more backdoor logic but may degrade normal performance
  - Trigger size vs. attack success rate: Larger triggers make the attack easier to trigger but may be more noticeable
  - Training time vs. patch quality: Longer training can improve effectiveness but increases computational cost

- Failure signatures:
  - Low attack success rate despite high clean accuracy: Patch is too stealthy or trigger condition is too weak
  - Low clean accuracy: Patch is too aggressive or poorly optimized
  - Patch fails in physical deployment: Digital-physical transformation is inadequate

- First 3 experiments:
  1. Train a backdoor patch on CIFAR-10 with a simple trigger (white square) and measure clean accuracy vs. attack success rate
  2. Test the patch's effectiveness against model pruning at different ratios
  3. Evaluate the patch's robustness against out-of-distribution detection methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is PatchBackdoor when the trigger is placed at different distances from the backdoor patch?
- Basis in paper: [explicit] The paper discusses the impact of trigger placement on attack effectiveness, noting that closer distances improve performance.
- Why unresolved: The paper provides general observations but does not quantify the exact relationship between trigger distance and attack success rate across various configurations.
- What evidence would resolve it: Systematic experiments measuring attack success rates at varying distances between the trigger and the backdoor patch, using different patch sizes and model architectures.

### Open Question 2
- Question: Can PatchBackdoor be adapted to work with other types of deep learning models beyond CNNs, such as transformers or recurrent neural networks?
- Basis in paper: [inferred] The paper focuses on CNN models and does not explore the applicability of PatchBackdoor to other architectures like transformers or RNNs.
- Why unresolved: The paper does not test or discuss the feasibility of applying PatchBackdoor to non-CNN models, leaving open the question of its generalizability.
- What evidence would resolve it: Experiments demonstrating the effectiveness of PatchBackdoor on various non-CNN models, including transformers and RNNs, across different tasks.

### Open Question 3
- Question: How does the stealthiness of PatchBackdoor patches compare to other types of adversarial patches in terms of detectability by human observers?
- Basis in paper: [explicit] The paper mentions that the backdoor patches do not substantially affect model accuracy on normal inputs, implying stealthiness, but does not address human perception.
- Why unresolved: The paper focuses on technical metrics like accuracy and attack success rate, but does not include perceptual studies or human evaluations of the patches' visibility.
- What evidence would resolve it: Human perception studies comparing the visibility of PatchBackdoor patches to other adversarial patches, using metrics like user studies or eye-tracking data.

## Limitations
- Digital-physical transformation modeling may not fully capture all real-world imaging artifacts, particularly under varying lighting conditions or camera angles
- Reliance on pseudo-labels from victim model could be problematic if model has inherent biases or patch optimization causes gradient vanishing
- Method's effectiveness against diverse OOD detection methods is not thoroughly explored

## Confidence
- High: Attack effectiveness on digital inputs (93%-99% success rate)
- Medium: Physical-world deployment feasibility
- Medium: Robustness against pruning and fine-tuning defenses
- Low: Stealthiness against diverse OOD detection methods

## Next Checks
1. Test patch effectiveness across multiple camera types and environmental conditions (indoor/outdoor, different lighting)
2. Evaluate attack persistence against model retraining with both clean and poisoned data
3. Assess detection resistance using a broader suite of OOD detectors beyond the ones tested in the paper