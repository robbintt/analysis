---
ver: rpa2
title: 'Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System
  for Answering Medical Questions using Scientific Literature'
arxiv_id: '2310.16146'
source_url: https://arxiv.org/abs/2310.16146
tags:
- information
- evaluation
- clinfo
- medical
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Clinfo.ai is an open-source, retrieval-augmented large language
  model system designed to answer medical questions by dynamically retrieving and
  synthesizing scientific literature. It addresses the challenge of clinicians and
  researchers keeping up with the rapidly expanding body of medical literature.
---

# Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature

## Quick Facts
- arXiv ID: 2310.16146
- Source URL: https://arxiv.org/abs/2310.16146
- Reference count: 40
- Key outcome: Clinfo.ai demonstrates improved performance over other systems in automated evaluations using source-augmented metrics, particularly when the source systematic review is not included in the retrieved articles.

## Executive Summary
Clinfo.ai is an open-source retrieval-augmented large language model (LLM) system designed to answer medical questions by dynamically retrieving and synthesizing scientific literature from PubMed. The system addresses the challenge of clinicians and researchers keeping up with rapidly expanding medical literature by using a chain of four LLMs to generate queries, retrieve relevant articles, classify their relevance, summarize each abstract, and synthesize a final answer. A benchmark dataset of 200 question-answer pairs (PubMedRS-200) derived from systematic reviews was created to evaluate the system. Clinfo.ai demonstrated improved performance over other systems (Elicit, Statpearls Semantic Search) in automated evaluations using source-augmented metrics like UniEval and CTC, particularly when the source systematic review was not included in the retrieved articles.

## Method Summary
Clinfo.ai uses a four-step LLM chain architecture: query generation, information retrieval from PubMed/Semantic Scholar, relevance classification, and abstract summarization followed by synthesis. The system was evaluated using a benchmark dataset of 200 question-answer pairs (PubMedRS-200) derived from systematic reviews. Performance was assessed using both source-free metrics (BERTScore, ROUGE-L, METEOR, chrF, GoogleBLEU, CTC, CharacTer) and source-augmented metrics (UniEval, COMET, CTC Summary Consistency) under three evaluation regimes: Restricted Search, Source Dropped, and Unrestricted Search.

## Key Results
- Clinfo.ai achieved better performance than Elicit and Statpearls Semantic Search on source-augmented metrics (UniEval, CTC) across all evaluation regimes
- The system showed particular strength in the Source Dropped regime, where the source systematic review was not included in the retrieved articles
- TL;DR summaries outperformed Synthesis summaries using source-free metrics, suggesting potential for further improvement in the synthesis component

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM chain structure enables effective decomposition of the retrieval-augmented summarization task into manageable sub-steps.
- Mechanism: Each LLM in the chain performs a specific function (query generation, retrieval, relevance classification, summarization, synthesis) with targeted prompts, reducing cognitive load on any single model and improving overall system performance.
- Core assumption: Breaking the task into sequential steps with appropriate prompts allows each LLM to focus on a specialized subtask, leading to better overall output than a single model handling everything.
- Evidence anchors:
  - [abstract] "we release Clinfo.ai, an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature"
  - [section] "our proposed RetA LLM system, Clinfo.ai, consists of a collection of four LLMs working conjointly (an LLM chain [46]) coupled to a Search Index"
  - [corpus] Weak - the corpus contains similar retrieval-augmented systems but doesn't specifically discuss the LLM chain approach used here

### Mechanism 2
- Claim: Retrieval augmentation addresses the knowledge cutoff limitation of LLMs by incorporating up-to-date literature.
- Mechanism: The system retrieves relevant abstracts from PubMed/Semantic Scholar, which are then used by the LLM to generate answers based on current literature rather than its training cutoff.
- Core assumption: The retrieved literature contains accurate and relevant information that can be synthesized into a coherent answer, and the LLM can effectively incorporate this retrieved information.
- Evidence anchors:
  - [abstract] "LLMs have several documented disadvantages and risks... updating LLMs with new knowledge and information is challenging and inefficient"
  - [section] "RetA LLMs do not require post-hoc model editing in order to incorporate new knowledge"
  - [corpus] Moderate - several papers in the corpus discuss retrieval-augmented generation for medical literature but don't specifically address the knowledge cutoff problem

### Mechanism 3
- Claim: The evaluation framework with source-augmented metrics provides a more rigorous assessment than traditional source-free metrics.
- Mechanism: By incorporating the context from systematic reviews (introductions, results, conclusions) into the evaluation, the system is judged not just on whether it produces a coherent summary, but whether it accurately captures the evidence presented in the source literature.
- Core assumption: The source-augmented metrics (UniEval, COMET, CTC) can effectively measure whether the generated summary aligns with the evidence presented in the source systematic reviews.
- Evidence anchors:
  - [abstract] "rigorous and systematic evaluations of their outputs are lacking... we specify an information retrieval and abstractive summarization task to evaluate the performance"
  - [section] "Step (3) is conducted using both source-free (SF) and source-augmented (SA) automated metrics"
  - [corpus] Weak - while the corpus contains evaluation frameworks for medical QA systems, none specifically discuss the source-augmented evaluation approach used here

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG) architecture
  - Why needed here: Understanding how RAG combines information retrieval with language model generation is essential to grasping how Clinfo.ai overcomes the knowledge cutoff problem of LLMs
  - Quick check question: How does RAG differ from a standard language model in terms of knowledge access and updating?

- Concept: Information retrieval evaluation metrics (precision, recall)
  - Why needed here: The paper evaluates the retrieval component using precision and recall metrics, which are fundamental to understanding how well the system finds relevant literature
  - Quick check question: In the context of this paper, what does it mean if the system has high recall but low precision for a given query?

- Concept: Automated evaluation metrics for text generation (BLEU, ROUGE, BERTScore, etc.)
  - Why needed here: The paper uses multiple automated metrics to evaluate the summarization quality, and understanding these metrics is crucial for interpreting the results
  - Quick check question: What's the difference between source-free and source-augmented evaluation metrics, and why might source-augmented metrics be more appropriate for this task?

## Architecture Onboarding

- Component map: User question → Query Generator → Information Retriever → Relevance Classifier → Summarization → Synthesis → Final answer
- Critical path: User question → Query Generator → Information Retriever → Relevance Classifier → Summarization → Synthesis → Final answer
- Design tradeoffs: The system prioritizes recall over precision in the initial retrieval step, then uses a relevance classifier to filter, trading off some computational efficiency for potentially more comprehensive coverage
- Failure signatures: Poor query generation leading to irrelevant retrievals, relevance classifier misclassifying articles, LLM summarization failing to capture key points, or synthesis producing incoherent answers
- First 3 experiments:
  1. Test the Query Generator with sample questions to ensure it produces PubMed-compatible queries
  2. Evaluate the Relevance Classifier on a small set of retrieved articles to check classification accuracy
  3. Test the full pipeline with a simple question and verify that relevant articles are retrieved and properly synthesized

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the reliability of LLM-generated MeSH terms to enhance precision and recall in medical literature search tasks?
- Basis in paper: [explicit] The paper acknowledges that LLMs generate hallucinated MeSH terms, potentially leading to the exclusion of relevant studies.
- Why unresolved: The paper suggests prioritizing improvements in the query generation process but does not provide specific methods or solutions.
- What evidence would resolve it: Empirical studies demonstrating improved precision and recall in literature searches using enhanced LLM-generated MeSH terms.

### Open Question 2
- Question: To what extent do human evaluations align with automated metrics in assessing RetA LLM systems?
- Basis in paper: [explicit] The paper highlights the use of automated metrics with moderate-to-high correlation with human preferences but notes the absence of explicit human evaluations.
- Why unresolved: The study did not solicit human preferences to evaluate the systems, leaving a gap in understanding the alignment between automated and human evaluations.
- What evidence would resolve it: Comparative studies incorporating both human evaluations and automated metrics to assess RetA LLM systems.

### Open Question 3
- Question: How can the performance of TL;DR summaries be improved to match or exceed that of Synthesis summaries?
- Basis in paper: [inferred] The paper notes that TL;DR summaries outperform Synthesis summaries using source-free metrics, suggesting potential for further improvement.
- Why unresolved: The paper does not explore methods to enhance the performance of TL;DR summaries beyond current capabilities.
- What evidence would resolve it: Experimental studies comparing TL;DR and Synthesis summaries under various conditions and methodologies.

## Limitations
- The evaluation framework is limited by the relatively small benchmark dataset (PubMedRS-200 with 200 question-answer pairs) and the specific focus on systematic review-derived questions
- The system's performance heavily depends on the quality of the LLM chain, and the paper doesn't provide sufficient detail on the prompts used at each step
- Comparison with other systems may not be entirely fair due to differences in system architecture and evaluation conditions

## Confidence
- **High confidence**: The core mechanism of using a chain of LLMs for retrieval-augmented summarization is technically sound and well-explained
- **Medium confidence**: The evaluation methodology using source-augmented metrics is appropriate, but the small benchmark size limits generalizability
- **Low confidence**: The comparison with other systems (Elicit, Statpearls Semantic Search) may not be entirely fair due to differences in system architecture and evaluation conditions

## Next Checks
1. Manually verify a random sample of 20 question-answer pairs from PubMedRS-200 to ensure the gold standard answers accurately reflect the source systematic reviews and that the questions are clinically meaningful and answerable from the literature.

2. Test the LLM chain with simplified versions of the intended prompts to determine the minimum prompt complexity required for each step, and identify which components are most sensitive to prompt engineering.

3. Systematically vary the retrieval parameters (number of articles retrieved, query formulation strategy) to quantify the precision-recall tradeoff and determine the optimal configuration for different types of medical questions.