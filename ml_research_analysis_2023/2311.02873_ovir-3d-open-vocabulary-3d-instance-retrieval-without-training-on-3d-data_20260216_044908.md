---
ver: rpa2
title: 'OVIR-3D: Open-Vocabulary 3D Instance Retrieval Without Training on 3D Data'
arxiv_id: '2311.02873'
source_url: https://arxiv.org/abs/2311.02873
tags:
- instance
- open-vocabulary
- region
- features
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OVIR-3D, a method for open-vocabulary 3D instance
  retrieval without training on 3D data. The key idea is to leverage 2D open-vocabulary
  detection and segmentation methods to generate 2D region proposals and their corresponding
  text-aligned features.
---

# OVIR-3D: Open-Vocabulary 3D Instance Retrieval Without Training on 3D Data

## Quick Facts
- arXiv ID: 2311.02873
- Source URL: https://arxiv.org/abs/2311.02873
- Reference count: 40
- Key outcome: Proposes OVIR-3D, a method for open-vocabulary 3D instance retrieval without training on 3D data, achieving state-of-the-art mAP on ScanNet200 and YCB-Video datasets.

## Executive Summary
This paper introduces OVIR-3D, a novel method for open-vocabulary 3D instance retrieval that leverages 2D open-vocabulary detection and segmentation to generate 3D instances without requiring 3D training data. The method projects 2D region proposals into 3D space and fuses them across multiple views, followed by periodic filtering and merging to improve instance masks and remove noise. Experiments on public datasets and a real robot demonstrate that OVIR-3D outperforms existing methods by a large margin in terms of instance retrieval mAP.

## Method Summary
OVIR-3D uses an off-the-shelf 2D open-vocabulary detector (Detic) to generate 2D region proposals and text-aligned features, which are then projected into 3D using camera intrinsics and poses. These 2D detections are matched to existing 3D instances or used to create new ones based on feature similarity and 3D IoU. The method employs periodic filtering and merging of 3D instances every 300 frames to improve masks and remove noisy detections. During inference, K-Means clustering of 2D features per instance provides better text query matching than simple averaging.

## Key Results
- OVIR-3D outperforms existing methods by a large margin in terms of instance retrieval mAP on both ScanNet200 and YCB-Video datasets.
- K-Means clustering of 2D features (K=64) significantly improves retrieval performance compared to averaging features.
- The method achieves approximately 30 fps inference speed on an NVIDIA RTX 3090 for ScanNet scenes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 2D open-vocabulary detectors provide sufficient category coverage and instance masks to seed 3D instance retrieval without additional 3D training.
- Mechanism: The method queries Detic with ImageNet21k categories to generate 2D masks and text-aligned features for each detected object. These masks are projected into 3D space and fused across views, forming complete 3D instance segments that can be matched to text queries via cosine similarity.
- Core assumption: 2D open-vocabulary detectors trained on large 2D datasets can generalize to detect and segment 3D objects when projected across multiple views.
- Evidence anchors:
  - [abstract] "leverage 2D open-vocabulary detection and segmentation methods to generate 2D region proposals and their corresponding text-aligned features"
  - [section 4.1] "leverages the power of an off-the-shelf open-vocabulary 2D detector Detic [2], which is trained with multiple large image datasets"
  - [corpus] Weak evidence: Corpus lacks direct 2D→3D generalization studies; this is assumed from Detic's performance on ImageNet21k.
- Break condition: If 2D detector fails to detect small objects consistently across views or produces poor masks, 3D instance quality degrades significantly.

### Mechanism 2
- Claim: Multi-view fusion with periodic filtering and merging improves 3D instance masks and reduces noise.
- Mechanism: 2D detections from different frames are projected to 3D and matched to existing instances using feature similarity and IoU. Periodic filtering removes low-recall points and small segments; merging combines similar instances based on feature similarity and IoU.
- Core assumption: Points consistently detected across multiple views belong to the same physical object instance, and noisy detections can be filtered out by low recall thresholds.
- Evidence anchors:
  - [abstract] "multi-view fusion of text-aligned 2D region proposals into 3D space" and "periodic filtering and merging of 3D instances to improve instance masks and remove noisy detections"
  - [section 4.3] "Point filtering is based on the detection rate rdet_p of a point p" and "Merging of two instances op, oq is determined by feature similarity spq and 3D intersection over union IoU"
  - [corpus] No direct corpus evidence; assumes multi-view consistency filters work as described.
- Break condition: If object moves significantly between frames or views are too sparse, matching fails and instances fragment or merge incorrectly.

### Mechanism 3
- Claim: Using K-Means clustering of 2D features per instance provides better text query matching than simple averaging.
- Mechanism: Instead of averaging all 2D features for an instance, the method clusters them into K centers, treating each as a representative viewpoint. During inference, the maximum similarity between the query and any cluster center is used for ranking.
- Core assumption: Different viewpoints of the same object produce diverse features, and clustering captures these variations better than a single average feature.
- Evidence anchors:
  - [section 4.5] "Instead of representing each 3D instance with the average feature of associated 2D regions, the K clustering centers by K-Means of associated features... are used"
  - [section 6.3] Table shows clustered features outperform averaging in mAP50
  - [corpus] No corpus evidence; improvement is specific to this method's ablation study.
- Break condition: If K is too small, clusters merge distinct viewpoints; if too large, clusters split the same viewpoint, hurting matching.

## Foundational Learning

- Concept: 2D to 3D projection geometry and camera pose transformation
  - Why needed here: The method projects 2D masks into 3D using camera intrinsics and poses; incorrect projection invalidates instance fusion.
  - Quick check question: Given a 2D pixel coordinate, camera intrinsic matrix K, and pose matrix T, what is the formula to get the 3D point in world coordinates?

- Concept: Feature similarity metrics (cosine similarity) and IoU for matching
  - Why needed here: Instance matching and merging rely on cosine similarity of text-aligned features and 3D IoU; understanding these metrics is critical for tuning thresholds.
  - Quick check question: If two feature vectors have cosine similarity 0.8 and IoU 0.3, will they be merged with default thresholds θs=0.75 and θiou=0.25?

- Concept: K-Means clustering and feature representation
  - Why needed here: Clustering 2D features per instance is a key design choice for better text query matching; knowing how K affects cluster granularity is important.
  - Quick check question: If an instance has 100 associated 2D features, how does increasing K from 16 to 64 change the number of representative features?

## Architecture Onboarding

- Component map: Image → Detic → 2D masks/features → Projection → Memory Bank → Fusion → 3D instances → Inference
- Critical path: Image → Detic → 2D masks/features → Projection → Memory Bank → Fusion → 3D instances → Inference
- Design tradeoffs:
  - Using Detic gives broad vocabulary but may miss rare objects; using SAM for masks improves quality but slows inference.
  - Periodic filtering (T=300) balances noise removal and computational load; too frequent filtering may lose valid points.
  - K-Means clustering (K=64) captures viewpoint diversity but increases memory and matching time.
- Failure signatures:
  - Low mAP: likely 2D detector missing objects or poor mask quality
  - Fragmented instances: insufficient views or high θiou threshold
  - Over-merged instances: low θs or θiou threshold, or large objects not merging properly
  - Slow inference: high K in clustering or large number of instances
- First 3 experiments:
  1. Vary input vocabulary size (COCO vs ImageNet21k) and measure mAP50 to confirm broad coverage helps.
  2. Compare averaging vs clustering (K=16, K=64) features for instance representation to validate feature ensemble choice.
  3. Test different frame intervals T for filtering/merging (T=100, 300, 500) to find optimal balance of quality vs speed.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The method's performance heavily depends on the quality and consistency of the 2D open-vocabulary detector across multiple views.
- The paper lacks detailed ablation studies on the choice of clustering parameters (K in K-Means) and their impact on retrieval accuracy.
- The periodic filtering and merging mechanism assumes that objects remain static or move slowly, which may not hold in dynamic environments.

## Confidence
- **High Confidence**: The core mechanism of using 2D open-vocabulary detectors and multi-view fusion for 3D instance retrieval is well-supported by the experimental results and aligns with the paper's claims.
- **Medium Confidence**: The effectiveness of K-Means clustering for feature representation is demonstrated in the ablation study but lacks broader validation across different datasets or object categories.
- **Low Confidence**: The robustness of the method to dynamic environments, occlusions, and rare object categories is not thoroughly explored, leaving potential performance gaps unaddressed.

## Next Checks
1. **Generalization to Rare Objects**: Test the method on datasets with rare or fine-grained object categories not well-represented in ImageNet21k to assess the detector's generalization capability.
2. **Dynamic Environment Robustness**: Evaluate the method in scenarios with significant object movement or occlusion to determine its reliability in real-world applications.
3. **Scalability Analysis**: Measure the method's performance and computational efficiency as the vocabulary size and number of instances increase, particularly for real-time deployment.