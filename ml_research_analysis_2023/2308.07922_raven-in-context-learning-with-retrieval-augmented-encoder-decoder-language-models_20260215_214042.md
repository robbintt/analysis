---
ver: rpa2
title: 'RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder Language
  Models'
arxiv_id: '2308.07922'
source_url: https://arxiv.org/abs/2308.07922
tags:
- language
- in-context
- learning
- atlas
- raven
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes in-context learning in retrieval-augmented
  encoder-decoder models and identifies limitations due to pretraining-inference mismatch
  and limited context length. The authors propose RAVEN, combining retrieval-augmented
  masked and prefix language modeling, and introduce Fusion-in-Context Learning to
  improve few-shot performance without additional training.
---

# RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models

## Quick Facts
- arXiv ID: 2308.07922
- Source URL: https://arxiv.org/abs/2308.07922
- Authors: 
- Reference count: 16
- Primary result: RAVEN significantly outperforms ATLAS on open-domain QA and achieves strong zero-shot performance on MMLU

## Executive Summary
This paper addresses the limitations of in-context learning in retrieval-augmented encoder-decoder models, which stem from pretraining-inference mismatch and limited context length. The authors propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling, followed by continual pretraining with prefix language modeling to better align pretraining with inference. They also introduce Fusion-in-Context Learning (FiCL) to enhance few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Experiments show that RAVEN significantly outperforms the state-of-the-art ATLAS model on open-domain QA tasks, achieving results comparable to much larger decoder-only models like PaLM 540B and Codex 175B.

## Method Summary
RAVEN addresses in-context learning limitations in retrieval-augmented encoder-decoder models through two key innovations: (1) combining retrieval-augmented masked and prefix language modeling during pretraining to align with inference, and (2) introducing Fusion-in-Context Learning (FiCL) to enable the model to leverage more in-context examples without increasing input length. The model is initialized from the ATLAS checkpoint and further pretrained on Wikipedia corpora. FiCL allows different subsets of examples to be fed with each passage separately, effectively increasing the number of examples the model can use during inference.

## Key Results
- RAVEN significantly outperforms ATLAS on open-domain QA tasks, achieving comparable results to much larger decoder-only models
- RAVEN achieves strong zero-shot performance on MMLU, even outperforming larger models in few-shot settings when using Fusion-in-Context Learning
- Using RAVEN's own retriever to select in-context examples improves few-shot performance by approximately 10% on NQ compared to using random examples

## Why This Works (Mechanism)

### Mechanism 1
RAVEN improves in-context learning by combining retrieval-augmented masked and prefix language modeling to align pretraining with inference. The model first trains with masked language modeling to build strong representation and retrieval abilities, then continues with prefix language modeling where the model learns to generate text given a prefix and retrieved passages. This aligns the pretraining objective with the inference prompting strategy where the target question follows in-context examples. Weak evidence - no direct comparison to models using only one objective.

### Mechanism 2
Fusion-in-Context Learning enables the model to learn from more in-context examples without increasing input length to the encoder. Instead of concatenating all examples with each retrieved passage, different subsets of examples are fed with each passage separately. This allows the model to incorporate more examples during inference while keeping the encoder input length manageable. Moderate evidence - neighbor papers discuss similar retrieval-augmented approaches.

### Mechanism 3
Using the model's own retriever to select in-context examples improves performance by providing more relevant examples. The retriever builds an index of examples and retrieves the most relevant ones for a given query during inference, rather than using random examples. Weak evidence - no direct comparison to manual or random example selection.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: Forms the basis of ATLAS pretraining and provides strong representation learning
  - Quick check question: What is the primary objective when training with MLM?

- Concept: Prefix Language Modeling
  - Why needed here: Aligns pretraining with inference by learning to generate from prefixes with retrieved context
  - Quick check question: How does prefix language modeling differ from standard causal language modeling?

- Concept: Fusion-in-Decoder Architecture
  - Why needed here: Enables effective integration of multiple retrieved passages with encoder-decoder models
  - Quick check question: What advantage does Fusion-in-Decoder provide over concatenating all passages?

## Architecture Onboarding

- Component map:
  Dense Retriever (Contriever-based) -> Encoder -> Decoder -> Fusion mechanism
- Critical path:
  1. Input examples and query go to encoder with retrieved passages
  2. Each passage is encoded separately with the full context
  3. Encoder outputs are concatenated and fed to decoder
  4. Decoder generates final output using both encoder representations and any decoder input
- Design tradeoffs:
  - Fixed context length vs. ability to process more examples (addressed by FiCL)
  - Pretraining objectives (MLM vs prefix LM) affecting inference alignment
  - Retriever quality vs. computational cost of retrieval
- Failure signatures:
  - Poor performance in low-shot settings may indicate pretraining-inference mismatch
  - Performance plateauing with more examples suggests context length limitations
  - Inconsistent few-shot performance may indicate retriever quality issues
- First 3 experiments:
  1. Compare RAVEN with different numbers of in-context examples to identify optimal setting
  2. Test FiCL with varying fusion configurations [k,m] to find most effective combination
  3. Evaluate in-context example retrieval vs random sampling on few-shot performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of RAVEN scale with model size beyond 11B parameters? The authors compare RAVEN to PaLM 540B and Codex 175B, noting that while RAVEN performs impressively, there remains a gap compared to these larger models. The authors suggest that building RAVEN upon a larger model could further improve its few-shot performance. Unresolved because the paper only evaluates RAVEN models up to 11B parameters. What evidence would resolve it: Training and evaluating a larger version of RAVEN (e.g., 100B+ parameters) on the same benchmarks and comparing its performance to other large language models.

### Open Question 2
How does the quality of retrieved in-context examples impact RAVEN's performance compared to randomly sampled examples? The authors propose using RAVEN's retriever to automatically retrieve relevant in-context examples and show that this approach improves few-shot performance by approximately 10% on NQ. However, the study does not directly compare this to randomly sampled examples. Unresolved because while the authors demonstrate improvement with retrieved examples, they do not provide a direct comparison to the performance of randomly sampled in-context examples. What evidence would resolve it: Conducting an experiment where RAVEN is tested with both randomly sampled in-context examples and examples retrieved by its retriever, comparing the performance between the two approaches.

### Open Question 3
What is the optimal number of in-context examples for RAVEN across different task types? The authors observe that ATLAS's performance plateaus after 8-shot, while RAVEN's performance continues to improve with more in-context examples. However, they do not explore the optimal number of examples for RAVEN across various tasks. Unresolved because the paper shows that RAVEN benefits from more in-context examples than ATLAS, but it does not investigate the specific number of examples that maximizes performance for different task types. What evidence would resolve it: Systematically testing RAVEN with varying numbers of in-context examples (e.g., 1-shot to 64-shot) across multiple task types and identifying the optimal number of examples for each task type.

## Limitations
- The extent to which RAVEN's pretraining objectives specifically address pretraining-inference mismatch is not rigorously quantified through ablation studies
- The effectiveness of Fusion-in-Context Learning is primarily demonstrated on retrieval-augmented tasks, limiting generalizability
- The quality of the retriever and its impact on downstream performance is not thoroughly analyzed compared to random example selection

## Confidence

- **High Confidence**: The improvement of RAVEN over ATLAS on open-domain QA tasks is well-supported by experimental results
- **Medium Confidence**: The claim that RAVEN's pretraining objectives address the pretraining-inference mismatch is plausible but lacks direct experimental validation
- **Low Confidence**: The claim that FiCL significantly enhances few-shot performance without additional training is supported by results but could be influenced by task-specific factors

## Next Checks

1. Conduct ablation studies comparing RAVEN with variants using only masked language modeling or only prefix language modeling to isolate the impact of each pretraining objective

2. Test RAVEN on non-retrieval tasks like summarization or code generation to assess the generalizability of FiCL and the overall model architecture

3. Perform a detailed analysis of the retriever's impact on performance by comparing RAVEN's results with random example selection and manual example curation on few-shot tasks