---
ver: rpa2
title: Absolute Policy Optimization
arxiv_id: '2310.13230'
source_url: https://arxiv.org/abs/2310.13230
tags:
- policy
- performance
- bound
- learning
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Absolute Policy Optimization (APO), a novel
  approach that guarantees monotonic improvement in the lower bound of near-total
  performance samples, addressing the limitation of existing trust region methods
  that focus only on expected performance. APO introduces a new theoretical framework
  called Absolute Markov Decision Process (AMDP) and provides practical implementations
  including an efficient approximation and handling of infeasible solutions.
---

# Absolute Policy Optimization

## Quick Facts
- arXiv ID: 2310.13230
- Source URL: https://arxiv.org/abs/2310.13230
- Reference count: 40
- This paper introduces APO, a novel approach that guarantees monotonic improvement in the lower bound of near-total performance samples.

## Executive Summary
This paper introduces Absolute Policy Optimization (APO), a novel approach that guarantees monotonic improvement in the lower bound of near-total performance samples, addressing the limitation of existing trust region methods that focus only on expected performance. APO introduces a new theoretical framework called Absolute Markov Decision Process (AMDP) and provides practical implementations including an efficient approximation and handling of infeasible solutions. The method demonstrates superior performance compared to state-of-the-art policy gradient algorithms across challenging continuous control benchmark tasks and Atari games, achieving significant improvements in both worst-case and expected performance metrics.

## Method Summary
APO optimizes a surrogate objective that includes both expected reward and variance terms to ensure monotonic improvement of absolute performance. The algorithm uses importance sampling to estimate advantage functions, convexifies the objective through first-order expansions, and employs line search to handle infeasible solutions. The trust region constraint is maintained through KL divergence bounds, while the probability factor k controls the tradeoff between conservatism and performance.

## Key Results
- APO significantly outperforms TRPO, PPO, and A2C on GUARD continuous control benchmarks, with up to 30% improvement in worst-case performance
- Demonstrates superior performance on Atari games, achieving higher expected and worst-case scores
- Theoretical guarantees ensure monotonic improvement of absolute performance bounds while maintaining expected performance improvement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** APO guarantees monotonic improvement in the lower bound of near-total performance samples (absolute performance).
- **Mechanism:** By optimizing the objective `J(π) - k*V(π)`, APO directly targets the worst-case performance scenarios while maintaining expected performance. The key insight is that performance samples from the same policy form a one-dimensional distribution, and APO bounds both the mean variance and variance of the mean across different start states.
- **Core assumption:** The performance distribution has finite non-zero variance and the Markov Decision Process framework allows for performance samples from any state visitation with non-zero probability.
- **Evidence anchors:**
  - [abstract]: "guaranteed monotonic improvement in the lower bound of near-total performance samples"
  - [section 3.3]: Definition 1 formally defines absolute performance bound and Section 4 introduces the theoretical framework
  - [corpus]: Weak evidence - only 1 related paper mentions "absolute" in context of performance bounds
- **Break condition:** If the performance distribution becomes multi-dimensional or the variance becomes unbounded, the theoretical guarantees may fail.

### Mechanism 2
- **Claim:** APO improves both expected performance and worst-case performance through a carefully constructed surrogate objective.
- **Mechanism:** The surrogate objective `J_l(π,π_j) - k*(M_V(π,π_j) + V_M(π,π_j))` provides a tight lower bound on absolute performance while maintaining monotonic improvement of expected performance through the trust region constraint.
- **Core assumption:** The trust region constraint `¯DKL(π||π_j) ≤ δ` ensures that policy updates remain within a local neighborhood where the surrogate objective remains valid.
- **Evidence anchors:**
  - [abstract]: "APO significantly outperforms state-of-the-art policy gradient algorithms, resulting in substantial improvements in worst-case performance, as well as expected performance"
  - [section 4]: Theorem 1 proves monotonic improvement of absolute performance and Corollary 1 ensures expected performance improvement
  - [corpus]: Moderate evidence - several related papers discuss policy improvement bounds and trust regions
- **Break condition:** If approximation errors accumulate or the line search fails to find feasible solutions, the theoretical guarantees may break down.

### Mechanism 3
- **Claim:** The practical implementation efficiently approximates the theoretical objective while handling infeasible solutions.
- **Mechanism:** Importance sampling estimates the expected reward advantage, first-order expansions convexify the objective, and line search ensures improvement even when the optimization becomes infeasible.
- **Core assumption:** The approximation errors remain bounded and the line search can find step sizes that satisfy both the KL constraint and improve the surrogate objective.
- **Evidence anchors:**
  - [section 6]: "we use a line search to ensure improvement of the surrogate objective and satisfaction of the KL divergence constraint"
  - [section 5]: "Equation 12 may often lead to infeasible solutions. Hence, the line search trick results in very small step sizes"
  - [corpus]: Weak evidence - no related papers specifically discuss line search in the context of infeasible policy optimization
- **Break condition:** If the approximation errors become too large or the line search fails to find valid step sizes, the algorithm may fail to improve.

## Foundational Learning

- **Concept:** Markov Decision Processes and discounted return
  - **Why needed here:** APO builds upon standard MDP theory but extends it to handle absolute performance bounds rather than just expected performance
  - **Quick check question:** How does the discounted return formula change when we want to optimize for worst-case performance rather than expected performance?

- **Concept:** Trust region methods and KL divergence constraints
  - **Why needed here:** APO uses trust region constraints similar to TRPO but with a modified objective that includes variance terms
  - **Quick check question:** What is the relationship between the KL divergence constraint and the probability factor k in APO's objective?

- **Concept:** Importance sampling and policy gradient estimation
  - **Why needed here:** The practical implementation of APO relies on importance sampling to estimate advantage functions across different policies
  - **Quick check question:** How does the variance of importance sampling estimates affect the stability of APO's policy updates?

## Architecture Onboarding

- **Component map:**
  Policy network (MLP with tanh activations) -> Trajectory sampler -> Advantage estimator (importance sampling) -> Surrogate objective calculator -> Trust region constraint checker -> Line search optimizer -> Policy update

- **Critical path:**
  1. Collect trajectories using current policy
  2. Estimate advantages using importance sampling
  3. Compute surrogate objective terms
  4. Solve constrained optimization problem
  5. Apply line search if needed
  6. Update policy parameters

- **Design tradeoffs:**
  - Computational cost vs theoretical guarantees (APO is more expensive than PPO due to variance calculations)
  - Conservatism vs performance (larger k values provide better worst-case guarantees but may slow learning)
  - Approximation quality vs implementation simplicity (more accurate approximations improve performance but increase complexity)

- **Failure signatures:**
  - KL divergence violations during line search
  - Objective value decreases despite optimization
  - Policy updates become too small or too large
  - Variance estimates become unstable

- **First 3 experiments:**
  1. Run APO on a simple continuous control task (e.g., Point-mass navigation) and compare worst-case performance to PPO
  2. Test different values of the probability factor k to find the optimal tradeoff between conservatism and performance
  3. Implement the H_max hyperparameter trick and evaluate its impact on convergence speed and final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal choice of probability factor k that balances absolute performance improvement and convergence speed across different RL tasks?
- Basis in paper: [explicit] Section 7.6 discusses ablation experiments on k's impact, showing that very small k reduces effectiveness while very large k leads to overly conservative optimization.
- Why unresolved: The paper only tests k on a single Atari game (Riverraid) and doesn't provide a systematic framework for choosing k based on task characteristics.
- What evidence would resolve it: A comprehensive study varying k across diverse task types (continuous control, Atari games, different reward structures) with convergence speed and performance metrics would identify optimal k ranges for different scenarios.

### Open Question 2
- Question: How does the choice of treating Hmax as a hyperparameter versus computing it from the policy affect APO's performance on tasks with different reward function characteristics?
- Basis in paper: [explicit] Section 7.5 shows hyperparameter option improves performance but only tests on Riverraid and PUSH_Ant tasks.
- Why unresolved: The paper doesn't explore the relationship between reward function properties (boundedness, sparsity, smoothness) and the effectiveness of each Hmax implementation method.
- What evidence would resolve it: Testing both Hmax implementations across tasks with varying reward characteristics while measuring performance and stability would reveal which approach works best for different reward structures.

### Open Question 3
- Question: Can the APO framework be extended to multi-agent RL settings while maintaining its absolute performance guarantees?
- Basis in paper: [inferred] The paper focuses on single-agent MDPs, but the theoretical framework around absolute performance bounds could potentially extend to multi-agent scenarios.
- Why unresolved: The current APO formulation assumes a single agent optimizing against a stationary environment, but doesn't address non-stationarity from other agents or coordination challenges.
- What evidence would resolve it: Implementing APO in multi-agent benchmarks and analyzing whether the absolute performance bounds hold when multiple agents simultaneously update their policies.

## Limitations
- Computational complexity is significantly higher than standard methods due to variance calculations
- Theoretical framework introduces novel mathematical structure with limited validation in broader RL literature
- Assumes one-dimensional performance distributions which may not hold in all scenarios

## Confidence

- **High Confidence**: The worst-case performance improvements demonstrated on benchmark tasks, particularly the 30% improvement on PUSH_Ant and 13% on Walker_CHASE tasks. The monotonic improvement guarantees in the trust region framework are mathematically sound.
- **Medium Confidence**: The practical implementation details and approximation strategies, including the line search mechanism and importance sampling estimators. While theoretically justified, these approximations may introduce errors that affect performance.
- **Low Confidence**: The generalizability of APO to environments with significantly different reward structures or state spaces beyond the tested benchmarks. The assumption of one-dimensional performance distributions may not hold in all scenarios.

## Next Checks
1. **Runtime Complexity Analysis**: Measure the wall-clock time per iteration of APO compared to PPO and TRPO across different task complexities to quantify the computational overhead of variance calculations and verify if the theoretical guarantees justify the additional cost.

2. **Hyperparameter Sensitivity Study**: Systematically vary the probability factor k and KL constraint δ across multiple tasks to identify the optimal tradeoffs between worst-case performance guarantees and learning speed, and determine if the H_max hyperparameter consistently improves convergence.

3. **Robustness to Distribution Shifts**: Test APO on environments with changing dynamics or reward structures (e.g., OpenAI's Procgen or Meta-World) to evaluate whether the absolute performance guarantees provide meaningful benefits when the underlying MDP distribution changes during training.