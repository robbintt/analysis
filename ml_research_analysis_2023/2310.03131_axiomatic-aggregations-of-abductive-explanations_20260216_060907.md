---
ver: rpa2
title: Axiomatic Aggregations of Abductive Explanations
arxiv_id: '2310.03131'
source_url: https://arxiv.org/abs/2310.03131
tags:
- feature
- abductive
- explanations
- index
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes three novel aggregators for aggregating abductive
  explanations into feature importance scores: the Responsibility Index, the Deegan-Packel
  Index, and the Holler-Packel Index. These aggregators are based on well-known measures
  from cooperative game theory and causal reasoning.'
---

# Axiomatic Aggregations of Abductive Explanations

## Quick Facts
- arXiv ID: 2310.03131
- Source URL: https://arxiv.org/abs/2310.03131
- Reference count: 40
- Primary result: Three novel aggregators (Responsibility Index, Deegan-Packel Index, Holler-Packel Index) based on cooperative game theory uniquely satisfy desirable axiomatic properties and show robustness to adversarial attacks that fool SHAP and LIME.

## Executive Summary
This paper addresses the challenge of aggregating multiple abductive explanations into interpretable feature importance scores. Abductive explanations are minimal sets of features sufficient to generate a model's outcome, but when multiple such explanations exist, aggregating them into meaningful importance scores becomes non-trivial. The authors propose three novel aggregators derived from cooperative game theory - the Holler-Packel Index, Deegan-Packel Index, and Responsibility Index - each uniquely characterized by a set of desirable axiomatic properties. These aggregators are shown to be more robust than traditional methods like SHAP and LIME when faced with adversarial attacks designed to hide model biases.

## Method Summary
The method involves computing abductive explanations M(x,f) for a given data point x and model f, then aggregating these explanations into feature importance scores using one of three proposed aggregators. The Holler-Packel, Deegan-Packel, and Responsibility indices are derived from cooperative game theory and causal reasoning frameworks, each uniquely satisfying specific axiomatic properties like Monotonicity, Symmetry, Null Feature, and Efficiency. The approach is empirically validated on Compas and German Credit datasets, comparing the aggregators' robustness against adversarial attacks that target SHAP and LIME explanations.

## Key Results
- The three proposed aggregators uniquely satisfy their respective sets of axiomatic properties, ensuring consistent and interpretable behavior
- The aggregators demonstrate superior robustness compared to SHAP and LIME when subjected to adversarial attacks designed to hide model biases
- In empirical evaluations, the aggregators correctly identify sensitive features as most important in adversarial settings where SHAP and LIME fail

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating abductive explanations into feature importance scores preserves model interpretability while capturing multiple valid explanations
- Mechanism: Abductive explanations are minimal sets of features sufficient to generate the model's outcome. When multiple abductive explanations exist, they are aggregated into importance scores using axiomatic measures that uniquely satisfy properties like Monotonicity, Symmetry, Null Feature, and Efficiency
- Core assumption: The set of abductive explanations for a given data point is finite and can be enumerated in practice
- Evidence anchors: [abstract] "We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength." [section] "We propose to aggregate abductive explanations into feature importance scores. Feature importance scores are an extremely well-studied class of explanations [Barocas et al., 2020]."
- Break condition: If the number of abductive explanations is exponential or enumeration is intractable, the aggregation approach becomes computationally infeasible

### Mechanism 2
- Claim: The axiomatic characterization of aggregators ensures they uniquely satisfy desirable properties, making them more reliable than heuristic methods like SHAP and LIME
- Mechanism: Each aggregator (Holler-Packel, Deegan-Packel, Responsibility) is defined by specific axiomatic properties. These properties ensure that the aggregator's output is consistent, fair, and interpretable. For example, the Holler-Packel index satisfies Minimal Monotonicity, Symmetry, Null Feature, and (∑|Mi(x,f)|)-Efficiency
- Core assumption: The axiomatic properties chosen are indeed desirable and capture the intuitive requirements for feature importance aggregation
- Evidence anchors: [section] "We take an axiomatic approach: we start with a set of desirable properties and then find the unique aggregator which satisfies these properties." [section] "Theorem 3.1. The only aggregator that satisfies Minimal Monotonicity, Symmetry, Null Feature, and (∑i∈N|Mi(x,f)|)-Efficiency is the Holler-Packel index given by (2)."
- Break condition: If the chosen axioms do not align with user expectations or practical needs, the aggregator may not be useful despite being theoretically sound

### Mechanism 3
- Claim: The aggregators are robust to adversarial attacks that fool SHAP and LIME by uncovering underlying biases in the model
- Mechanism: Adversarial attacks on SHAP and LIME hide biases by manipulating model predictions on out-of-distribution data points. The abductive explanation aggregators, however, rely on the model's structure and abductive explanations, which are not affected by these manipulations. This allows them to correctly identify the sensitive features that drive the model's decisions
- Core assumption: The adversarial attacks on SHAP and LIME are effective because they exploit the methods' reliance on model approximations, while abductive explanations are based on the model's exact structure
- Evidence anchors: [abstract] "Empirically, the aggregators are shown to be robust to adversarial attacks that fool popular explanation methods like SHAP and LIME." [section] "Since the biased classiﬁer is used to predict the label for almost all the test points, any good explanation method should identify that the adversarial classiﬁer makes its prediction largely based on the sensitive feature for most of the points in the test dataset."
- Break condition: If the model's structure itself is compromised or if the abductive explanations are not correctly computed, the aggregators may also fail to identify biases

## Foundational Learning

- Concept: Cooperative Game Theory
  - Why needed here: The paper uses power indices from cooperative game theory (Holler-Packel, Deegan-Packel) to define aggregators. Understanding these indices and their properties is crucial for grasping the paper's approach
  - Quick check question: What is the key difference between the Holler-Packel and Deegan-Packel indices in terms of how they weight abductive explanations?

- Concept: Abductive Explanations
  - Why needed here: Abductive explanations are the core input to the aggregators. Understanding their definition, properties, and how they are computed is essential for understanding the paper's contributions
  - Quick check question: What is the relationship between abductive explanations and minimal winning sets in cooperative game theory?

- Concept: Axiomatic Characterization
  - Why needed here: The paper uses axiomatic characterization to uniquely define each aggregator. Understanding this approach and the specific axioms used is crucial for evaluating the paper's claims
  - Quick check question: Why is it important that each aggregator uniquely satisfies a set of axioms? What does this imply about the aggregator's behavior?

## Architecture Onboarding

- Component map: Model of Interest (f) -> Abductive Explanation Generator -> Aggregators (Holler-Packel, Deegan-Packel, Responsibility) -> Adversarial Attack Simulator
- Critical path:
  1. Input: Data point x and model f
  2. Compute abductive explanations M(x,f)
  3. Apply each aggregator to M(x,f) to obtain feature importance scores
  4. Compare the robustness of the aggregators against adversarial attacks
- Design tradeoffs:
  - Computational cost of enumerating abductive explanations vs. the robustness of the resulting explanations
  - The choice of axiomatic properties for the aggregators vs. their interpretability and practical usefulness
  - The use of cooperative game theory indices vs. other potential aggregation methods
- Failure signatures:
  - If the number of abductive explanations is too large to enumerate, the aggregators become computationally infeasible
  - If the axiomatic properties do not align with user expectations, the aggregators may not be useful despite being theoretically sound
  - If the model's structure is compromised or abductive explanations are not correctly computed, the aggregators may fail to identify biases
- First 3 experiments:
  1. Implement the abductive explanation generator for a simple rule-based model and verify that it correctly identifies all abductive explanations for a given data point
  2. Implement the Holler-Packel aggregator and verify that it correctly computes feature importance scores for a small set of abductive explanations
  3. Apply the adversarial attack simulator to a SHAP explanation of a simple model and verify that it successfully fools SHAP but not the abductive explanation aggregators

## Open Questions the Paper Calls Out
- Can the framework for aggregating abductive explanations be extended to other types of explanations, such as contrastive or probabilistic explanations, and what would be the implications?

## Limitations
- Computational scalability remains a concern as the number of abductive explanations grows exponentially with feature space size
- The adversarial attack setup assumes specific bias patterns that may not generalize to all real-world scenarios where SHAP/LIME fail
- The relationship between axiomatic properties and practical interpretability is asserted but not empirically validated with user studies

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical foundations (axiomatic characterizations, cooperative game theory connections) | High |
| Empirical robustness claims against adversarial attacks | Medium |
| Mechanism claims connecting abductive explanations to adversarial robustness | Medium |

## Next Checks
1. Test scalability by implementing the abductive explanation generator on a synthetic dataset with controlled feature interactions and measuring runtime as feature count increases from 10 to 100
2. Evaluate the aggregators against a broader suite of explanation methods (e.g., Anchors, Integrated Gradients) under different attack types including feature permutation and model extraction attacks
3. Conduct a controlled user study comparing interpretability of axiomatic aggregators versus SHAP/LIME on real-world models (e.g., medical diagnosis) to validate the practical utility of the theoretical properties