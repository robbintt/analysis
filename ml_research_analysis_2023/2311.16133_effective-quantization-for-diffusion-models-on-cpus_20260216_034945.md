---
ver: rpa2
title: Effective Quantization for Diffusion Models on CPUs
arxiv_id: '2311.16133'
source_url: https://arxiv.org/abs/2311.16133
tags:
- diffusion
- unet
- steps
- quantization
- precision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the computational inefficiency of diffusion
  models on CPUs, particularly Stable Diffusion, by introducing an effective quantization
  strategy. The approach combines quantization-aware training (QAT) with knowledge
  distillation for the U-Net component and employs a time-dependent mixed precision
  scheme during the denoising loop.
---

# Effective Quantization for Diffusion Models on CPUs

## Quick Facts
- arXiv ID: 2311.16133
- Source URL: https://arxiv.org/abs/2311.16133
- Reference count: 3
- Key outcome: Mixed-precision quantization achieves 30.63 FID with 5.5s latency on Intel 4th Gen Xeon CPUs, maintaining comparable quality to full FP32 while significantly improving performance

## Executive Summary
This paper addresses the computational inefficiency of diffusion models on CPUs by introducing an effective quantization strategy that combines quantization-aware training (QAT) with knowledge distillation for the U-Net component. The approach employs a time-dependent mixed precision scheme during the denoising loop, using INT8 for intermediate steps while reserving BF16 for initial and final steps. Additionally, optimized CPU kernels including parallelized GroupNorm and MHA fusion are developed. Experiments demonstrate that the mixed-precision model achieves an FID of 30.63 with 10 BF16 and 40 INT8 steps, compared to 30.48 for full FP32, while reducing latency from 6.32s to 5.5s on Intel 4th Gen Xeon CPUs.

## Method Summary
The method combines quantization-aware training with knowledge distillation to quantize the U-Net component of Stable Diffusion models. During training, the original full-precision U-Net serves as a teacher, providing output targets for the quantized student model. For inference, a mixed precision strategy is employed where the initial and final denoising steps use BF16 precision for noise estimation, while intermediate steps use INT8. The approach also includes optimized CPU kernels with parallelized GroupNorm implementation across channels and MHA fusion. The technique is validated across Stable Diffusion versions 1.4, 1.5, and 2.1, demonstrating consistent performance improvements while maintaining visually comparable image quality to full-precision models.

## Key Results
- Mixed-precision model achieves FID of 30.63 (10 BF16 + 40 INT8 steps) vs 30.48 for full FP32
- Latency reduced from 6.32s (BF16) to 5.5s on Intel 4th Gen Xeon CPUs
- Maintains visually comparable image quality to full-precision models
- Validated across Stable Diffusion versions 1.4, 1.5, and 2.1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Time-dependent mixed precision reduces quantization noise accumulation during diffusion.
- Mechanism: The denoising process is divided into phases where high precision (BF16) is used at the beginning and end, while low precision (INT8) is used for intermediate steps. This minimizes quantization noise impact when the signal is weakest and noise is strongest.
- Core assumption: Early and late denoising steps are more sensitive to quantization errors than middle steps.
- Evidence anchors:
  - [abstract] "Low precision (INT8) is used for intermediate steps while higher precision (BF16) is reserved for the initial and final steps."
  - [section] "the initial 'k' steps and the final 'k' steps employ a Unet model with higher precision, such as BFloat16, for noise estimation. In contrast, the intervening steps utilize a Unet model with lower precision, like INT8, for noise estimation."

### Mechanism 2
- Claim: Knowledge distillation improves quantized model accuracy by providing soft targets.
- Mechanism: During QAT, the original full-precision Unet acts as a teacher, providing output targets for the quantized student model. This guides the quantized model to maintain similar behavior despite reduced precision.
- Core assumption: The full-precision model's outputs contain information that helps the quantized model learn better representations.
- Evidence anchors:
  - [section] "With original Unet as the teacher, its output functions as the guidance for the student, i.e. fake quantized Unet."
  - [section] "Algorithm 1 QAT with Knowledge Distillation for Unet" describes the process where teacher output oT guides student output oS.

### Mechanism 3
- Claim: Optimized GroupNorm implementation improves CPU utilization by parallelizing across channels instead of groups.
- Mechanism: Instead of dividing work by groups (which may be fewer than CPU cores), the optimized implementation divides computation across channel dimensions, allowing better core utilization and reduced latency.
- Core assumption: Channel-level parallelism provides better CPU core utilization than group-level parallelism for GroupNorm.
- Evidence anchors:
  - [section] "we have restructured our approach by computation parallelism across dimensions for channels rather than groups."
  - [section] "The primary issue lies in the fact that the number of groups is fewer than the available CPU cores, resulting in a low CPU utilization rate."

## Foundational Learning

- Concept: Quantization-aware training (QAT)
  - Why needed here: Standard quantization without QAT causes significant quality degradation in diffusion models due to their sensitivity to precision loss.
  - Quick check question: What is the key difference between post-training quantization (PTQ) and quantization-aware training (QAT)?

- Concept: Knowledge distillation
  - Why needed here: Helps the quantized model learn from the full-precision model's behavior, reducing accuracy loss during quantization.
  - Quick check question: In the context of QAT, what role does the teacher model play?

- Concept: Mixed precision strategies
  - Why needed here: Different stages of the denoising process have different sensitivity to quantization errors, allowing targeted precision allocation.
  - Quick check question: Why might early and late denoising steps require higher precision than intermediate steps?

## Architecture Onboarding

- Component map: Unet -> Denoising loop -> GroupNorm/MHA fusion
- Critical path: Unet inference within the denoising loop is the primary performance bottleneck that quantization aims to address.
- Design tradeoffs:
  - Precision vs. latency: Higher precision improves quality but increases latency
  - Number of BF16 steps: More BF16 steps improve quality but reduce performance gains
  - QAT complexity: Adds training overhead but significantly improves quantized model quality
- Failure signatures:
  - High FID scores indicating quality degradation
  - CPU utilization remaining low despite quantization
  - Model instability or divergence during inference
  - Unexpected accuracy differences between versions
- First 3 experiments:
  1. Compare FP32 vs BF16 performance and quality to establish baseline for mixed precision
  2. Test different ratios of BF16/INT8 steps (e.g., 2/48, 6/44, 10/40) to find optimal balance
  3. Validate that optimized GroupNorm actually improves CPU utilization by measuring core usage during profiling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the number of steps (k) at the beginning and end that use BF16 versus the middle steps that use INT8, and how does this trade-off affect the optimal configuration for different tasks or model variants?
- Basis in paper: [inferred] The paper mentions using "the initial k steps and the final k steps" with BF16, but does not specify an optimal k value or analyze the trade-off.
- Why unresolved: The paper only provides results for k=3 and k=5, but does not explore a range of k values or provide a systematic analysis of the trade-off.
- What evidence would resolve it: A detailed ablation study showing FID and latency across a range of k values (e.g., k=1,2,3,4,5) for different Stable Diffusion versions and potentially different tasks would clarify the optimal configuration.

### Open Question 2
- Question: How does the proposed quantization strategy generalize to other diffusion model architectures beyond Stable Diffusion, such as DDPM, DDIM, or latent diffusion models with different U-Net variants?
- Basis in paper: [explicit] The paper states "We validated our approach across various versions of Stable Diffusion" but does not test other diffusion model architectures.
- Why unresolved: The paper focuses solely on Stable Diffusion variants, leaving the generalizability to other diffusion model architectures unexplored.
- What evidence would resolve it: Applying the proposed quantization strategy to other diffusion model architectures and evaluating their performance and image quality would demonstrate generalizability.

### Open Question 3
- Question: What is the impact of the proposed quantization strategy on the diversity of generated images, and does it introduce any biases or artifacts that are not captured by FID alone?
- Basis in paper: [inferred] The paper focuses on FID as the primary metric for image quality, but does not analyze the diversity of generated images or potential biases/artifacts introduced by quantization.
- Why unresolved: FID is a measure of similarity to real images but does not capture diversity or potential biases/artifacts introduced by quantization.
- What evidence would resolve it: A comprehensive analysis of image diversity using metrics like LPIPS or other perceptual similarity measures, along with a qualitative examination of generated images for potential biases or artifacts, would address this question.

## Limitations
- Limited evaluation to Intel Sapphire Rapids CPUs only, without validation on other CPU architectures or GPUs
- Mixed precision strategy optimization lacks comprehensive ablation studies across different step ratios
- Knowledge distillation benefits are asserted but not independently verified through ablation studies

## Confidence
**High Confidence**: The latency reduction claims (6.32s to 5.5s) are well-supported by direct measurements and the mixed precision approach is technically sound. The GroupNorm optimization description is specific and actionable.

**Medium Confidence**: The FID score comparisons (30.63 vs 30.48) are close enough that measurement variance or implementation details could affect conclusions. The knowledge distillation benefits are asserted but not independently verified through ablation studies.

**Low Confidence**: Claims about the superiority of the time-dependent mixed precision strategy over alternative approaches lack comparative analysis. The general applicability of these optimizations beyond Intel Sapphire Rapids CPUs remains unproven.

## Next Checks
1. **Ablation study**: Test the quantized model with and without knowledge distillation to quantify its specific contribution to FID score improvements.

2. **Precision ratio sensitivity**: Systematically vary the number of BF16 steps (e.g., 2/48, 6/44, 10/40, 14/36) to identify the optimal balance between quality and performance.

3. **Cross-architecture validation**: Implement and test the quantization approach on a different CPU architecture (such as AMD EPYC or Apple M-series) to assess generalizability beyond Intel Sapphire Rapids.