---
ver: rpa2
title: Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation
arxiv_id: '2310.15415'
source_url: https://arxiv.org/abs/2310.15415
tags:
- events
- speaker
- time
- event
- session
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes to model the temporal information in multi-session\
  \ conversations. The authors created a new dataset, GapChat, where multi-session\
  \ dialogues are grounded to hypothetical timelines of events in the speakers\u2019\
  \ lives."
---

# Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation

## Quick Facts
- **arXiv ID**: 2310.15415
- **Source URL**: https://arxiv.org/abs/2310.15415
- **Reference count**: 28
- **Primary result**: Time-aware dialogue models outperform standard RAG models on naturalness, informativeness, and relevance in multi-session conversations

## Executive Summary
This paper addresses the challenge of generating natural and relevant dialogue across multiple conversation sessions by modeling the temporal information between interactions. The authors introduce GapChat, a novel dataset of multi-session dialogues grounded in hypothetical speaker timelines containing life and world events. They propose time-aware retrieval-augmented generation models that incorporate event progress information through either progress labels or detailed event schedules. Through human evaluation, the study demonstrates that time-aware models significantly outperform baseline approaches, with progress labels enhancing informativeness and event schedules improving naturalness and relevance. The results also show that incorporating temporal context helps even state-of-the-art models like ChatGPT generate more contextually appropriate follow-up questions.

## Method Summary
The authors create GapChat, a dataset of 56,254 utterances across 2,650 conversations with 3-5 sessions each, simulating speakers engaged in various life and world events. Event durations are estimated using ChatGPT prompts, and event progress is calculated by comparing event duration to time gaps between sessions. Two time-aware RAG models are developed: TA-RAG (progress) using progress labels and TA-RAG (schedule) using detailed event schedules. These models are fine-tuned on the MSC-RAG 2.7B architecture, incorporating time-aware context during both retrieval and generation. The models are evaluated against RAG (FT) and ChatGPT baselines using human evaluation with ACUTE-Eval, measuring naturalness, informativeness, relevance, and time-awareness.

## Key Results
- Time-aware models outperform RAG (FT) baseline on all three human evaluation metrics (naturalness, informativeness, relevance)
- Progress labels specifically improve informativeness while event schedules enhance naturalness and relevance
- Time-aware models outperform ChatGPT when both are provided with the same temporal context
- TA-RAG (progress) achieves the best balance between informativeness and naturalness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modeling time gaps enables reasoning about relevant topics based on event progress
- Mechanism: Progress labels calculated from event duration vs time gap help filter events with meaningful progress
- Core assumption: Event duration estimates accurately reflect real-world event lengths
- Evidence anchors: Abstract mentions time-aware models perform better on relevance; section 4.2 describes progress calculation; weak corpus support
- Break condition: Inaccurate event duration estimates lead to irrelevant questions

### Mechanism 2
- Claim: Event schedules enable more natural and informative responses through concrete discussion steps
- Mechanism: Schedules break events into discrete steps, allowing specific questions about completed vs remaining steps
- Core assumption: Schedule structure aligns with natural conversational flow
- Evidence anchors: Section 4.2 explains schedule-based progress representation; section 5.2 notes schedule style; weak corpus support
- Break condition: Overly granular schedules lead to awkward or overly detailed responses

### Mechanism 3
- Claim: Structured time-aware information improves LLM's relevant response generation
- Mechanism: Providing event progress information alongside time gaps enables reasoning about meaningful progress
- Core assumption: LLMs effectively utilize structured time-aware information in prompts
- Evidence anchors: Section 5.3 describes ChatGPT prompting scenarios; section 6.1 shows gap information improves metrics; weak corpus support
- Break condition: Strong internal time reasoning by LLM reduces additional information benefit

## Foundational Learning

- Concept: Event extraction from dialogue history
  - Why needed here: To identify which events speakers are currently engaged in for progress tracking
  - Quick check question: How would you extract "working on a research project" from "I'm just busy with my doctorate thesis"?

- Concept: Progress calculation based on event duration and time gaps
  - Why needed here: To determine which events have meaningful progress to discuss in follow-up sessions
  - Quick check question: If an event takes 2 months and the time gap is 3 weeks, what progress label should be assigned?

- Concept: Retrieval-augmented generation for dialogue
  - Why needed here: To incorporate time-aware information into model context for generation
  - Quick check question: How does adding event progress labels to context differ from adding them to the prompt?

## Architecture Onboarding

- Component map: Event extraction module (ChatGPT) -> Progress calculation module (duration comparison) -> Time-aware RAG model -> Document storage (dialogue history) -> Retriever (DPR model)

- Critical path: 1. Extract events from dialogue history 2. Calculate event progress using durations and time gaps 3. Retrieve relevant dialogue history 4. Generate response incorporating time-aware information

- Design tradeoffs: Progress labels vs event schedules (simplicity vs informativeness); fine-tuning vs prompting (integrated awareness vs base model preservation)

- Failure signatures: Irrelevant questions about events with "no significant progress"; overly long responses due to schedule inclusion; inconsistent performance across different time gap durations

- First 3 experiments: 1. Compare RAG (FT) vs TA-RAG (progress) on informativeness across different time gaps 2. Evaluate ChatGPT with vs without time-aware prompts on naturalness 3. Test event extraction accuracy on held-out dialogue samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would performance change if event durations were estimated using a dedicated temporal commonsense knowledge base instead of ChatGPT?
- Basis in paper: Relies on ChatGPT for duration estimation, which may not reflect reality
- Why unresolved: No comparison with alternative duration estimation methods
- What evidence would resolve it: Experiments comparing time-aware models using different duration estimation methods

### Open Question 2
- Question: How would performance change with more detailed event schedules including more steps?
- Basis in paper: Uses schedules with maximum 7 steps, doesn't explore impact of detail
- Why unresolved: No investigation of schedule detail impact on model performance
- What evidence would resolve it: Experiments comparing models using schedules with varying detail levels

### Open Question 3
- Question: How would performance change if time gaps followed specific distributions rather than random generation?
- Basis in paper: Uses randomly generated time gaps, doesn't explore distribution impact
- Why unresolved: No investigation of time gap distribution impact on model performance
- What evidence would resolve it: Experiments comparing models using different time gap distributions

## Limitations
- Evaluation relies entirely on human judgment without automatic metrics or error case analysis
- Event extraction process depends on incompletely specified ChatGPT prompts affecting reproducibility
- Focus on two event types with predefined durations may not capture full complexity of real-world temporal dynamics
- ChatGPT comparison limited to prompting rather than fine-tuning, making it difficult to isolate information vs model capability effects

## Confidence
- **High confidence**: Time-aware models outperform RAG (FT) on human evaluation metrics of naturalness, informativeness, and relevance
- **Medium confidence**: Progress labels specifically improve informativeness while event schedules enhance naturalness and relevance
- **Low confidence**: TA-RAG (progress) achieves optimal balance between informativeness and naturalness due to lack of statistical significance testing

## Next Checks
1. **Statistical validation**: Conduct significance testing on human evaluation results to determine whether observed differences between models are statistically reliable

2. **Error analysis**: Perform detailed qualitative analysis of model outputs to identify failure modes, such as when time-aware models ask irrelevant questions

3. **Generalization testing**: Evaluate proposed models on dialogue datasets with naturally occurring time gaps to assess real-world applicability beyond simulated timelines