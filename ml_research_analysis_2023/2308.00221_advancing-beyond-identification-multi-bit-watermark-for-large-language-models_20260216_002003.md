---
ver: rpa2
title: 'Advancing Beyond Identification: Multi-bit Watermark for Large Language Models'
arxiv_id: '2308.00221'
source_url: https://arxiv.org/abs/2308.00221
tags:
- text
- watermarking
- language
- message
- zero-bit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to embed traceable multi-bit information
  during language model generation, addressing the need to identify and trace adversary
  users responsible for malicious misuse of large language models. The proposed method,
  called Multi-bit watermarking via Color-listing (COLOR), partitions the vocabulary
  into multiple colored-lists and allocates tokens pseudo-randomly onto different
  bit positions using a pseudo-random function.
---

# Advancing Beyond Identification: Multi-bit Watermark for Large Language Models

## Quick Facts
- arXiv ID: 2308.00221
- Source URL: https://arxiv.org/abs/2308.00221
- Reference count: 8
- One-line primary result: COLOR method successfully embeds 32-bit messages with 91.9% accuracy in moderate-length texts (~500 tokens) while maintaining text quality and enabling zero-bit detection.

## Executive Summary
This paper introduces COLOR (Multi-bit watermarking via Color-listing), a novel method for embedding traceable multi-bit information into language model outputs without requiring model fine-tuning or retraining. The approach partitions the vocabulary into multiple colored-lists and uses pseudo-random allocation to embed longer messages in high corruption settings without added latency. The method outperforms existing works in terms of robustness and latency while maintaining text quality and enabling zero-bit detection. Preliminary experiments demonstrate successful embedding of 32-bit messages with 91.9% accuracy in moderate-length texts.

## Method Summary
The COLOR method modifies token selection probabilities during inference by adding bias to specific color-list tokens, using a pseudo-random function seeded by hash of previous tokens. It partitions the vocabulary into r color-lists and pseudo-randomly assigns tokens to different bit positions, allowing encoding of r states per token while maintaining robustness to simple attacks. The method enables both multi-bit watermark extraction and zero-bit detection simultaneously by counting tokens in each color-list and using the majority color as the digit value.

## Key Results
- Successfully embeds 32-bit messages with 91.9% accuracy in moderate-length texts (~500 tokens)
- Maintains text quality while enabling zero-bit watermark detection
- Outperforms existing methods in robustness and latency without requiring model fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
The method modifies token selection probabilities during inference by adding bias to specific color-list tokens, using a pseudo-random function seeded by hash of previous tokens. This allows steering language model outputs by logit biasing without significantly degrading text quality.

### Mechanism 2
Vocabulary partitioning into r color-lists enables encoding r states per token while maintaining robustness to simple attacks. The hash-based position allocation distributes watermark information sufficiently to survive length variations and mixing with human-written text.

### Mechanism 3
COLOR enables both multi-bit watermark extraction and zero-bit detection simultaneously. By counting tokens in each color-list and using the majority color as the digit value, multi-bit messages can be reconstructed while the count of color-listed tokens serves as the detection metric.

## Foundational Learning

- **Autoregressive language model token generation**: Understanding how token probabilities are computed and how logit biasing affects generation is essential to grasp the watermarking mechanism. Quick check: What happens to the token distribution when a fixed bias δ is added to the logits of a subset of tokens?

- **Pseudo-random functions and hashing**: The method relies on hash-based seeding to ensure reproducibility and robustness. Quick check: How does changing the number of previous tokens used in the hash (h parameter) affect the distribution of message positions?

- **Radix conversion and multi-bit encoding**: Converting binary messages to radix-r and chunking them into digit positions is the core of how multi-bit information is embedded. Quick check: If r=4 and b=8, how many binary bits can be encoded in total, and why?

## Architecture Onboarding

- **Component map**: Language model (e.g., OPT-1.3b) -> Pseudo-random function (hash-based seed generator) -> Vocabulary partitioner (r color-lists) -> Logit bias adder (δ magnitude) -> Message encoder/decoder (radix conversion and counting)

- **Critical path**: 1) Generate prefix tokens, 2) Hash previous tokens to seed RNG, 3) Sample position p, 4) Convert message to radix r, 5) Partition vocab into r color-lists, 6) Add bias δ to logits in selected color-list, 7) Sample next token, 8) Repeat until desired length

- **Design tradeoffs**: Higher δ improves robustness but degrades quality; larger r increases bit capacity but reduces token diversity; longer text improves accuracy but increases latency

- **Failure signatures**: Repetitive or degenerate text → δ too high; low extraction accuracy → insufficient token allocation per bit position; high false positive rate in detection → γ too high

- **First 3 experiments**: 1) Verify zero-bit watermarking works with δ=2.0 and γ=0.25 on short text (T=100), 2) Test multi-bit embedding with r=4 and b=16 on moderate text (T=500), 3) Measure robustness under copy-paste attack with 40% human text ratio

## Open Questions the Paper Calls Out

### Open Question 1
What is the maximum bit-width that can be reliably embedded in machine-generated text without significant degradation of text quality or robustness against attacks? The paper demonstrates successful embedding of 32-bit messages but notes that embedding more bit-width makes the watermark more fragile. This requires experiments with larger models and comprehensive testing against various attack scenarios.

### Open Question 2
How does the proposed method perform in distinguishing between human and machine-generated text under different attack scenarios? While the paper mentions this capability, the robustness of this detection under attacks has not been extensively verified and requires testing with different attack methods.

### Open Question 3
What is the optimal trade-off between the number of colors (r) used in the vocabulary partitioning and the text quality/bit accuracy? The paper only uses one value of r determined by γ = 0.25 and does not explore the effects of varying r on text quality and bit accuracy.

## Limitations

- Lack of extensive validation under realistic attack scenarios beyond preliminary experiments
- Limited exploration of parameter sensitivity to bias term δ and number of color-lists r
- Performance unverified with different language models, vocabulary sizes, or specialized domains
- Computational overhead not explicitly quantified in terms of inference latency or resource consumption

## Confidence

**High Confidence**: Core mechanism of vocabulary partitioning and pseudo-random token allocation is clearly described and logically sound.

**Medium Confidence**: Claims about robustness to length variations and copy-paste attacks are plausible but require more rigorous experimental validation.

**Low Confidence**: Assertion that COLOR outperforms existing methods in both robustness and latency lacks direct comparisons to specific baseline approaches.

## Next Checks

1. **Attack Scenario Validation**: Conduct systematic experiments testing COLOR against comprehensive suite of watermark removal attacks (synonym replacement, paraphrasing, adversarial prompting) and measure bit accuracy degradation compared to baselines.

2. **Parameter Sensitivity Analysis**: Perform ablation studies varying bias term δ and number of color-lists r to identify optimal configurations and measure tradeoff between watermark robustness, text quality, and extraction accuracy.

3. **Cross-Model and Cross-Domain Evaluation**: Test COLOR's effectiveness when transferring watermarks between different language models and across diverse text domains to validate generalizability beyond the newslike C4 dataset.