---
ver: rpa2
title: 'AdvFAS: A robust face anti-spoofing framework against adversarial examples'
arxiv_id: '2308.02116'
source_url: https://arxiv.org/abs/2308.02116
tags:
- face
- adversarial
- examples
- anti-spoofing
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of face anti-spoofing systems
  to adversarial attacks. The proposed AdvFAS framework introduces a novel approach
  that couples a detector and corrector module to distinguish between correctly and
  incorrectly detected face images.
---

# AdvFAS: A robust face anti-spoofing framework against adversarial examples

## Quick Facts
- arXiv ID: 2308.02116
- Source URL: https://arxiv.org/abs/2308.02116
- Reference count: 10
- Key outcome: AdvFAS achieves up to 93.79% improvement in adversarial robustness across various attacks, datasets, and backbones while maintaining high accuracy on clean examples.

## Executive Summary
This paper addresses the vulnerability of face anti-spoofing systems to adversarial attacks by proposing the AdvFAS framework. The framework introduces a novel approach that couples a detector and corrector module to distinguish between correctly and incorrectly detected face images. By leveraging two coupled scores, AdvFAS can identify wrongly detected examples, allowing for robust detection of adversarial attacks while maintaining high accuracy on clean examples. Extensive experiments demonstrate that AdvFAS significantly outperforms existing methods in terms of adversarial robustness.

## Method Summary
The AdvFAS framework consists of a detector and corrector module with a shared backbone architecture. The detector performs face anti-spoofing while the corrector predicts whether the detector's output is correct. During training, the framework uses a min-max optimization approach with gradient stopping and masking strategies to prevent degradation on clean examples while improving adversarial robustness. The corrector is trained to predict detection correctness using binary cross-entropy loss, and the expected score is calculated as the product of detector and corrector outputs.

## Key Results
- Achieves up to 93.79% improvement in accuracy on adversarial examples compared to baselines
- Maintains high accuracy on clean examples while improving robustness
- Demonstrates effectiveness across multiple datasets (WMCA, CASIA-SURF 3DMask) and backbones (Depthnet, Resnet-18, Resnet-50)
- Shows robustness against various attack methods including C&W, PGD, patch attack, DeepFool, and AutoAttack

## Why This Works (Mechanism)

### Mechanism 1
The coupled scores (fθ(x) and ES(x)) can provably distinguish wrongly detected examples from correctly detected ones. The framework uses two scores where ES(x) = fθ(x) · gκ(x). For correctly detected examples, ES(x) equals fθ(x), while for wrongly detected examples, ES(x) equals the ground truth label (0 or 1). This creates a mathematical separation where correctly and incorrectly detected examples fall into distinct regions of the score space.

### Mechanism 2
The two-head shared block architecture reduces memory costs while maintaining effectiveness. Instead of training separate networks for detection and correction, the framework uses a shared backbone with two output heads - one for the detector and one for the corrector. This architectural choice saves memory while allowing the corrector to learn from the same feature representations as the detector.

### Mechanism 3
The training strategy with gradient stopping and masking prevents degradation on clean examples while improving adversarial robustness. During training, gradients from the corrector loss are stopped when the detector makes correct predictions, preventing over-correction on easy examples. A mask is applied to eliminate loss contributions from adversarial examples during corrector training, maintaining the detector's focus on clean data.

## Foundational Learning

- Concept: Adversarial examples in face recognition systems
  - Why needed here: The paper addresses a specific vulnerability where adversarial examples can fool face anti-spoofing systems, so understanding how these attacks work is fundamental
  - Quick check question: What is the key difference between white-box and black-box adversarial attacks in the context of face recognition systems?

- Concept: Coupled optimization problems
  - Why needed here: The framework formulates the problem as a min-max optimization where the detector and corrector are jointly optimized, requiring understanding of saddle point problems
  - Quick check question: In a min-max formulation, which part represents the inner maximization (adversarial generation) and which represents the outer minimization (defense)?

- Concept: Binary cross-entropy loss and thresholding
  - Why needed here: The corrector uses BCE loss to predict whether detection is correct, and the framework relies on threshold-based decision making at 0.5
  - Quick check question: How does changing the decision threshold from 0.5 to another value affect the trade-off between true positive and false positive rates?

## Architecture Onboarding

- Component map: Shared backbone → Detector head → Corrector head → Expected score calculation
- Critical path: Input image → Shared feature extraction → Detector prediction → Corrector prediction → Expected score calculation → Final classification
- Design tradeoffs: Shared architecture reduces memory but may create interference; corrector adds robustness but increases training complexity; gradient stopping preserves clean performance but may limit correction learning
- Failure signatures: High accuracy on clean examples but poor performance on adversarial examples suggests corrector isn't learning; poor performance on both suggests shared architecture interference; degraded clean performance suggests over-aggressive correction
- First 3 experiments:
  1. Baseline test: Run detector alone on clean and adversarial examples to establish vulnerability baseline
  2. Component isolation: Test corrector alone on examples where detector is known to be wrong/right to verify it learns the correction task
  3. End-to-end evaluation: Run full framework on both clean and adversarial examples to measure improvement over baseline

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided text.

## Limitations

- The coupled score mechanism relies on assumptions about the corrector's ability to learn the detection-error relationship that may not hold under adaptive attacks
- The shared architecture's effectiveness in preventing interference between detector and corrector modules is not thoroughly validated
- Claims about robustness to real-world scenarios are based on limited testing with AutoAttack and patch attacks
- Theoretical analysis of the coupled relationship lacks rigorous mathematical proof

## Confidence

- Empirical robustness improvements: High confidence based on extensive experiments across multiple datasets and attacks
- Coupled score mechanism effectiveness: Medium confidence due to limited theoretical analysis and potential vulnerability to adaptive attacks
- Real-world scenario performance: Medium confidence based on limited testing scope

## Next Checks

1. Implement and test an adaptive attacker that specifically targets the coupled score mechanism by manipulating both fθ(x) and gκ(x) simultaneously
2. Conduct ablation studies to isolate the contribution of each training strategy component (gradient stopping, masking) to overall performance
3. Evaluate the framework's performance when the detector's accuracy on clean examples drops below 95%, testing the corrector's effectiveness at scale