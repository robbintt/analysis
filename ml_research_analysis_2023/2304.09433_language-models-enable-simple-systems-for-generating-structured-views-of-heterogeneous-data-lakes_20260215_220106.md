---
ver: rpa2
title: Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous
  Data Lakes
arxiv_id: '2304.09433'
source_url: https://arxiv.org/abs/2304.09433
tags:
- evaporate
- documents
- data
- attributes
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents EVAPORATE, a system that uses large language
  models (LLMs) to automatically generate structured views of semi-structured data
  lakes without requiring human effort or domain-specific customization. The key insight
  is to use LLMs for two tasks: identifying the schema from a small sample of documents,
  and synthesizing code that can extract attribute values from all documents.'
---

# Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes

## Quick Facts
- arXiv ID: 2304.09433
- Source URL: https://arxiv.org/abs/2304.09433
- Reference count: 40
- Primary result: EVAPORATE-CODE+ achieves 12.1 F1 points higher quality than direct extraction on average across 16 real-world evaluation settings, while reducing the number of tokens processed by the LLM by 110x

## Executive Summary
This paper presents EVAPORATE, a system that uses large language models (LLMs) to automatically generate structured views of semi-structured data lakes without requiring human effort or domain-specific customization. The key insight is to use LLMs for two tasks: identifying the schema from a small sample of documents, and synthesizing code that can extract attribute values from all documents. EVAPORATE explores two fundamental strategies: direct extraction (prompting the LLM to extract values directly from each document) and code synthesis (prompting the LLM to write code that can be reused to process documents). Code synthesis is much cheaper but less accurate than direct extraction. To improve quality while maintaining low cost, EVAPORATE-CODE+ synthesizes many candidate functions and ensembles their extractions using weak supervision.

## Method Summary
EVAPORATE is a system that automatically extracts structured views from heterogeneous data lakes using large language models. It operates in two phases: schema identification and data extraction. For schema identification, it samples a small set of documents and extracts attributes using the LLM. For data extraction, it explores three implementations: EVAPORATE-DIRECT (direct extraction from each document), EVAPORATE-CODE (LLM-synthesized code functions for attribute extraction), and EVAPORATE-CODE+ (multiple candidate functions with weak supervision aggregation). The system processes HTML, PDF, and TXT documents, generating atomic schemas and extracting attribute values using a combination of LLM prompting strategies and statistical aggregation techniques.

## Key Results
- EVAPORATE-CODE+ achieves 12.1 F1 points higher quality than direct extraction on average across 16 real-world evaluation settings
- Code synthesis reduces the number of tokens processed by the LLM by 110x compared to direct extraction
- EVAPORATE-CODE+ achieves comparable quality to VIAL (a sophisticated state-of-the-art system) on the SWDE Movie dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can synthesize reusable code functions that extract structured data from semi-structured documents at a fraction of the cost of direct extraction.
- **Mechanism:** Instead of processing every document with the LLM, the system synthesizes Python functions that can be reused to process many documents. This exploits redundancy in how attribute-value pairs are expressed across documents.
- **Core assumption:** Attribute-values are expressed in consistent patterns across documents, making code reuse feasible.
- **Evidence anchors:**
  - [abstract] "Code synthesis is much cheaper but less accurate than direct extraction. To improve quality while maintaining low cost, EVAPORATE-CODE+ synthesizes many candidate functions and ensembles their extractions using weak supervision."
  - [section 3.2.2] "A researcher would likely exploit such redundancies when manually scraping the documents for analysis. In EVAPORATE-CODE, we propose to use the LLM to automatically synthesize a data-lake-specific suite of functions, that can then be applied at scale to process many documents."
- **Break condition:** If attribute-value pairs are expressed in highly variable ways across documents, code synthesis becomes ineffective and accuracy degrades significantly.

### Mechanism 2
- **Claim:** Weak supervision can aggregate noisy, machine-generated extraction functions without requiring labeled data.
- **Mechanism:** The system generates multiple candidate functions and uses weak supervision to model their accuracies and correlations, combining their outputs to produce higher-quality extractions than any single function.
- **Core assumption:** Even low-quality extraction functions contain useful signal that can be combined statistically to improve overall accuracy.
- **Evidence anchors:**
  - [abstract] "To improve quality while maintaining low cost, we propose an extended code synthesis implementation, EVAPORATE-CODE+, which achieves better quality than direct extraction. Our key insight is to generate many candidate functions and ensemble their extractions using weak supervision."
  - [section 3.3.2] "Weak supervision (WS) is a popular standard statistical framework for modeling the accuracies and correlations between noisy sources of information without any labeled data."
- **Break condition:** If most generated functions perform worse than random (below 25% accuracy), weak supervision cannot effectively combine them.

### Mechanism 3
- **Claim:** Schema identification from a small document sample is sufficient because attributes are consistent across documents.
- **Mechanism:** Instead of processing every document to identify attributes, the system samples a small set of documents and extracts attributes from them, leveraging the consistency of attributes across the document collection.
- **Core assumption:** The set of attributes present in a document collection is relatively stable and can be identified from a small sample.
- **Evidence anchors:**
  - [abstract] "In order to identify a schema, we only process a small sample of documents with the LLM. This works because of redundancy in the attributes mentioned across documents."
  - [section 3.2.1] "To exploit this redundancy, EVAPORATE-DIRECT prompts an LLM to analyze a small sample of documents to identify attributes for the output schema."
- **Break condition:** If the document collection contains highly diverse attributes with little overlap, schema identification from a small sample becomes unreliable.

## Foundational Learning

- **Concept:** Prompt engineering and in-context learning
  - Why needed here: The entire system relies on carefully crafted prompts to elicit specific behaviors from LLMs, from schema identification to code synthesis.
  - Quick check question: What are the two fundamental prompting strategies used in EVAPORATE, and how do they differ in their approach to structured data extraction?

- **Concept:** Weak supervision framework
  - Why needed here: The system uses weak supervision to combine noisy, machine-generated extraction functions without labeled data, requiring understanding of how to model source accuracies and correlations.
  - Quick check question: How does EVAPORATE handle the assumption that weak supervision typically requires sources to perform better than random?

- **Concept:** Function synthesis and code generation
  - Why needed here: The core optimization in EVAPORATE involves generating reusable code functions instead of processing each document individually, requiring understanding of how to prompt LLMs for code generation.
  - Quick check question: What two prompting strategies does EVAPORATE use to generate diverse candidate functions for the same extraction task?

## Architecture Onboarding

- **Component map:** Document Collection → Schema Identification → Function Synthesis → Function Aggregation → Structured Output
- **Critical path:** Document → Schema identification → Function synthesis → Function aggregation → Structured output
  - The schema identification step samples k documents and identifies m attributes
  - Function synthesis generates candidate functions for each attribute
  - Function aggregation combines function outputs using weak supervision
- **Design tradeoffs:**
  - Direct extraction vs. code synthesis: Direct extraction is more accurate but scales linearly with document count; code synthesis is cheaper but less accurate
  - Number of candidate functions: More functions improve coverage but increase computation cost
  - Sample size for schema identification: Larger samples improve schema quality but increase upfront cost
- **Failure signatures:**
  - Low precision in direct extraction: Indicates LLM is generating inconsistent or hallucinated attribute-value pairs
  - Poor function compilation: Suggests generated code has syntax errors or references unavailable libraries
  - Weak supervision convergence issues: Indicates most functions perform worse than random, making aggregation ineffective
- **First 3 experiments:**
  1. Test schema identification accuracy on a small document sample vs. full dataset processing
  2. Compare function synthesis quality using different prompt templates (PA vs PB variants)
  3. Evaluate weak supervision aggregation performance with varying numbers of candidate functions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can EVAPORATE be extended to handle more complex schema transformations beyond atomic decomposition, such as nested structures or hierarchical relationships?
- **Basis in paper:** [inferred] The paper mentions atomic schema cleaning as a potential extension, but only for decomposing compound attributes into atomic ones. It doesn't discuss handling more complex schema transformations.
- **Why unresolved:** The paper focuses on a proof-of-concept system and doesn't explore the full range of possible schema transformations that could be handled by LLMs.
- **What evidence would resolve it:** Experiments demonstrating EVAPORATE's ability to handle nested structures or hierarchical relationships in schemas, along with an analysis of the challenges and potential solutions for these more complex transformations.

### Open Question 2
- **Question:** How can EVAPORATE be adapted to handle data lakes with a high degree of variability in document structure and content, where the redundancy assumptions underlying function synthesis may not hold?
- **Basis in paper:** [explicit] The paper acknowledges that EVAPORATE's function synthesis approach relies on redundancy in attribute-value pairs across documents. It also mentions that the system may struggle with documents that lack consistent structure or formatting.
- **Why unresolved:** The paper doesn't provide a detailed analysis of how EVAPORATE would perform in scenarios with high variability, nor does it propose specific strategies for handling such cases.
- **What evidence would resolve it:** Experiments evaluating EVAPORATE's performance on data lakes with varying degrees of structure and content variability, along with proposed modifications to the system to improve its robustness in these scenarios.

### Open Question 3
- **Question:** How can EVAPORATE be scaled to handle extremely large data lakes with billions of documents, where even the sublinear cost of function synthesis may become prohibitive?
- **Basis in paper:** [inferred] The paper discusses the cost advantages of function synthesis over direct extraction, but it doesn't address the scalability challenges of processing massive data lakes.
- **Why unresolved:** The paper focuses on demonstrating the feasibility of EVAPORATE on smaller datasets and doesn't explore the technical and practical challenges of scaling the system to handle truly massive data lakes.
- **What evidence would resolve it:** A detailed analysis of the computational and resource requirements for scaling EVAPORATE to billions of documents, along with proposed optimizations or architectural changes to enable efficient processing at this scale.

## Limitations
- The exact prompt templates for function synthesis (PA and PB variants) are not fully specified, making faithful reproduction challenging
- The paper doesn't provide detailed breakdowns of token costs across different document types and sizes
- Weak supervision implementation details for handling abstentions and filtering low-quality functions are not fully specified

## Confidence
- **High Confidence:** The core insight that code synthesis can reduce LLM costs while maintaining reasonable accuracy is well-supported by the empirical results
- **Medium Confidence:** The claim that EVAPORATE-CODE+ achieves 12.1 F1 points higher quality than direct extraction relies on weak supervision aggregation working as described
- **Low Confidence:** The scalability claims (110x reduction in tokens) lack sufficient detail about actual token counts and cost calculations

## Next Checks
1. Implement and test both PA and PB prompt variants for function synthesis to verify they generate diverse and functional extraction code as claimed
2. Systematically vary the number of candidate functions and analyze how weak supervision performance degrades when most functions perform below random (below 25% accuracy threshold)
3. Measure actual token counts and processing times for EVAPORATE-DIRECT vs EVAPORATE-CODE+ across different document sizes and types to verify the 110x token reduction claim with empirical data