---
ver: rpa2
title: Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study
  in Medicine
arxiv_id: '2311.16452'
source_url: https://arxiv.org/abs/2311.16452
tags:
- gpt-4
- medprompt
- medical
- performance
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores whether prompting strategies can enable GPT-4
  to achieve specialist-level performance in medicine without specialized fine-tuning.
  The authors introduce Medprompt, a composition of dynamic few-shot learning, self-generated
  chain-of-thought reasoning, and choice shuffle ensembling.
---

# Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine

## Quick Facts
- arXiv ID: 2311.16452
- Source URL: https://arxiv.org/abs/2311.16452
- Reference count: 40
- Key outcome: Medprompt achieves 90.2% accuracy on MedQA, surpassing specialist models without fine-tuning

## Executive Summary
This study demonstrates that prompting strategies can enable GPT-4 to achieve specialist-level performance in medicine without specialized fine-tuning. The authors introduce Medprompt, a composition of dynamic few-shot learning, self-generated chain-of-thought reasoning, and choice shuffle ensembling. Medprompt significantly boosts GPT-4's performance, achieving state-of-the-art results on all nine medical benchmark datasets in the MultiMedQA suite, including 90.2% accuracy on MedQA (USMLE-style), surpassing specialist models like Med-PaLM 2 by 27 percentage points.

## Method Summary
Medprompt is a prompting strategy that combines three key innovations: dynamic few-shot learning using k-NN retrieval of semantically similar training examples, self-generated chain-of-thought reasoning where GPT-4 creates and verifies its own rationales, and choice shuffle ensembling to mitigate position bias. The approach requires no fine-tuning or expert-curated content, instead leveraging GPT-4's existing capabilities through carefully designed prompt engineering. The method is evaluated across nine multiple-choice medical benchmarks from the MultiMedQA suite and shows consistent improvements over both generalist and specialist models.

## Key Results
- Medprompt achieves 90.2% accuracy on MedQA (USMLE-style), exceeding specialist models like Med-PaLM 2 and prior state-of-the-art by 27 percentage points
- State-of-the-art performance across all nine medical benchmark datasets in the MultiMedQA suite
- Generalizes well to diverse domains including law, accounting, and nursing with similar performance gains
- No expert-curated content or extensive fine-tuning required, demonstrating generalist model capabilities through prompting alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic few-shot selection improves accuracy by retrieving semantically similar training examples
- Mechanism: Uses k-NN in embedding space to find relevant examples rather than fixed examples
- Core assumption: Training examples exist that are semantically similar to test questions
- Evidence anchors: [section] "Given a test example, we choose k training examples that are semantically similar using a k-NN clustering in the embedding space"; [abstract] "We find that prompting innovation can unlock deeper specialist capabilities"
- Break condition: Training data lacks relevant semantic neighbors for test examples

### Mechanism 2
- Claim: GPT-4 can generate effective chain-of-thought rationales better than human experts
- Mechanism: Model generates CoT steps and verifies against ground truth to filter out incorrect reasoning
- Core assumption: GPT-4's reasoning is reliable enough to self-verify against ground truth
- Evidence anchors: [abstract] "We find that GPT-4 is capable of autonomously generating high-quality, detailed CoT prompts"; [section] "We mitigate this concern by having GPT-4 generate both a rationale and an estimation of the most likely answer to follow from that reasoning chain"
- Break condition: GPT-4 generates hallucinated reasoning that happens to match correct answers (false positives)

### Mechanism 3
- Claim: Choice shuffling ensemble reduces position bias and improves robustness
- Mechanism: Shuffling answer choices and using self-consistency to find most stable answer
- Core assumption: GPT-4 exhibits position bias that can be mitigated through shuffling
- Evidence anchors: [section] "GPT-4 can exhibit a propensity to favor certain options in multiple choice answers over others (regardless of the option content)"; [section] "We then select the most consistent answer, i.e., the one that is least sensitive to choice shuffling"
- Break condition: GPT-4's position bias is too weak to warrant the computational cost of shuffling

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Medprompt relies on demonstrating examples within prompts rather than fine-tuning
  - Quick check question: How does in-context learning differ from traditional fine-tuning in terms of parameter updates?

- Concept: Chain-of-thought reasoning
  - Why needed here: Breaking down complex medical questions into reasoning steps improves accuracy
  - Quick check question: What is the difference between zero-shot CoT prompting and few-shot CoT prompting?

- Concept: Ensemble methods
  - Why needed here: Combining multiple predictions reduces variance and improves robustness
  - Quick check question: How does choice shuffling differ from standard temperature-based sampling for generating ensemble diversity?

## Architecture Onboarding

- Component map: Embedding model → KNN retrieval → GPT-4 CoT generation → Answer shuffling → Ensemble voting
- Critical path: Embedding → Retrieval → Prompt construction → Generation → Ensembling
- Design tradeoffs: More few-shot examples and ensemble members improve accuracy but increase cost
- Failure signatures: Performance degradation on eyes-off data suggests overfitting; inconsistent answers suggest position bias
- First 3 experiments:
  1. Compare random vs kNN few-shot selection on a small dataset
  2. Test GPT-4 vs expert-crafted CoT prompts with fixed examples
  3. Measure position bias by comparing accuracy across answer positions without shuffling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much further can prompting strategies improve GPT-4's performance on medical benchmarks without fine-tuning?
- Basis in paper: [explicit] The paper shows Medprompt achieves 90.2% on MedQA, surpassing specialist models, and ablation studies suggest potential for further improvement
- Why unresolved: The paper's ablation study suggests increasing few-shot exemplars and ensemble steps could yield marginal improvements, but the ceiling for prompting-only improvements remains unknown
- What evidence would resolve it: Systematic testing of Medprompt with increasingly large few-shot sets and ensemble sizes, measuring performance gains against computational costs

### Open Question 2
- Question: Can Medprompt's self-generated chain-of-thought approach be generalized to open-ended text generation tasks?
- Basis in paper: [explicit] The paper suggests adapting Medprompt to non-multiple-choice settings by aggregating K generated texts, but doesn't test these adaptations
- Why unresolved: The paper only validates Medprompt on multiple-choice questions and speculates about adaptations for open-ended tasks without empirical testing
- What evidence would resolve it: Direct testing of Medprompt-style chain-of-thought prompting on open-ended generation benchmarks, comparing to existing prompting approaches

### Open Question 3
- Question: What is the relationship between model scale and the effectiveness of prompting strategies like Medprompt?
- Basis in paper: [inferred] The paper focuses on GPT-4 without comparing to smaller models, though it mentions first-generation models required domain-specific pretraining
- Why unresolved: The paper doesn't explore whether prompting strategies provide similar performance gains on smaller foundation models or if scale is necessary for these prompting approaches to work
- What evidence would resolve it: Systematic comparison of Medprompt across foundation models of varying scales (e.g., GPT-3.5, LLaMA variants) on identical benchmarks

## Limitations

- Data Representation Bias: Primarily evaluates on multiple-choice question formats, may not generalize to open-ended clinical decision-making
- Benchmark Contamination: Many datasets likely included in GPT-4's pretraining corpus, raising questions about memorization vs genuine capability
- Computational Cost Trade-offs: Multiple GPT-4 API calls per question without detailed cost-benefit analysis

## Confidence

- High Confidence: Effectiveness of choice shuffling to reduce position bias and benefits of dynamic few-shot selection
- Medium Confidence: GPT-4 can autonomously generate better chain-of-thought rationales than human experts
- Low Confidence: Generalization claims to domains like law, accounting, and nursing based on limited testing

## Next Checks

1. **Out-of-Distribution Testing**: Evaluate Medprompt on medical datasets definitively excluded from GPT-4's pretraining to assess true generalization capabilities
2. **Clinical Decision Support Validation**: Test Medprompt on clinical decision support tasks involving patient vignettes with incomplete information requiring differential diagnosis
3. **Cost-Effectiveness Analysis**: Conduct comprehensive cost comparison between deploying Medprompt versus fine-tuned specialist models across various deployment scales