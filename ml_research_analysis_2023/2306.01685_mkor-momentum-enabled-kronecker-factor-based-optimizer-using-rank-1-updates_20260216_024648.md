---
ver: rpa2
title: 'MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates'
arxiv_id: '2306.01685'
source_url: https://arxiv.org/abs/2306.01685
tags:
- mkor
- methods
- second-order
- training
- kaisa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents MKOR, a momentum-enabled Kronecker-factor-based\
  \ optimizer that uses rank-1 updates to improve the scalability and convergence\
  \ properties of second-order optimization methods for deep neural networks. The\
  \ key innovation is approximating the inverse of covariance matrices using rank-1\
  \ updates, reducing computational complexity from O(d\xB3) to O(d\xB2) and communication\
  \ complexity from O(d\xB2) to O(d)."
---

# MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates

## Quick Facts
- arXiv ID: 2306.01685
- Source URL: https://arxiv.org/abs/2306.01685
- Authors: [Not specified in input]
- Reference count: 40
- Primary result: Up to 2.57× speedup over LAMB and 1.85× over KAISA on BERT-Large-Uncased training

## Executive Summary
MKOR is a momentum-enabled Kronecker-factor-based optimizer that addresses computational and communication bottlenecks in second-order optimization for deep neural networks. By using rank-1 updates to approximate inverse covariance matrices, MKOR reduces computational complexity from O(d³) to O(d²) and communication complexity from O(d²) to O(d). The method introduces a hybrid version (MKOR-H) that intelligently switches to first-order optimization when second-order updates no longer accelerate convergence. Experiments demonstrate state-of-the-art performance on BERT-Large-Uncased, achieving new benchmarks on GLUE tasks while significantly reducing training time compared to existing first and second-order optimizers.

## Method Summary
MKOR improves upon traditional Kronecker-factor-based methods by approximating Fisher Information Matrix blocks using rank-1 updates via the Sherman-Morrison identity, avoiding expensive full matrix inversions. The optimizer synchronizes only rank-1 approximation vectors across distributed workers, dramatically reducing communication overhead. A norm-based stabilizer prevents exploding gradients, while gradient rescaling maintains consistent gradient norms. The hybrid MKOR-H variant monitors loss reduction rates and switches to first-order optimization mid-training when second-order benefits diminish. The method is compatible with existing backend optimizers and can leverage half-precision computations for additional efficiency gains.

## Key Results
- Achieves up to 2.57× speedup over LAMB and 1.85× over KAISA/KFAC on BERT-Large-Uncased
- Reduces computational complexity from O(d³) to O(d²) through rank-1 approximation
- Lowers communication complexity from O(d²) to O(d) by synchronizing only rank-1 vectors
- Achieves new state-of-the-art performance on GLUE benchmark tasks
- Maintains stability through norm-based stabilization and gradient rescaling mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rank-1 approximation of covariance matrices reduces inversion complexity from O(d³) to O(d²)
- Mechanism: MKOR approximates the Fisher Information Matrix (FIM) block using rank-1 updates via Sherman-Morrison identity instead of full matrix inversion
- Core assumption: The FIM block can be well-approximated by rank-1 updates without significant loss of curvature information
- Evidence anchors:
  - [abstract] "approximating the inverse of covariance matrices using rank-1 updates... reducing computational complexity from O(d³) to O(d²)"
  - [section 3.1] "MKOR approximates the inverse of covariance matrices using rank-1 updates in the Sherman-Morrison-Based (SM-Based) Matrix Inversion, reducing the inversion computation complexity from O(d³) to O(d²)"
  - [corpus] Weak evidence - no direct citations found
- Break condition: When rank-1 approximation becomes too poor (high condition number or large approximation error)

### Mechanism 2
- Claim: Synchronization of rank-1 approximation vectors reduces communication complexity from O(d²) to O(d)
- Mechanism: Only the rank-1 approximation vectors (am and gm) are synchronized across workers, not full covariance matrices
- Core assumption: Sharing rank-1 approximation vectors provides sufficient second-order information for distributed optimization
- Evidence anchors:
  - [abstract] "reducing the communication complexity of the second-order updates as well as achieving a linear communication complexity"
  - [section 3.1] "MKOR alleviates this by only synchronizing rank-1 approximation vectors among the workers, reducing the communication costs from O(d²) to O(d)"
  - [corpus] Weak evidence - no direct citations found
- Break condition: When rank-1 approximation vectors become too large to justify communication savings

### Mechanism 3
- Claim: Hybrid approach switches to first-order optimization when second-order benefits diminish
- Mechanism: MKOR-H monitors loss reduction rate and switches to first-order optimizer when second-order updates no longer accelerate convergence
- Core assumption: Second-order convergence benefits decrease significantly after initial training phase
- Evidence anchors:
  - [abstract] "We also propose a hybrid version of MKOR (called MKOR-H) that mid-training falls backs to a first order optimizer if the second order updates no longer accelerate convergence"
  - [section 3.2] "MKOR-H evaluates the changes in the loss function in different iterations and switches back to first-order methods if needed"
  - [corpus] Weak evidence - no direct citations found
- Break condition: When loss reduction rate falls below threshold indicating diminishing returns from second-order updates

## Foundational Learning

- Kronecker product properties
  - Why needed here: MKOR relies on Kronecker-factored approximations of the Fisher Information Matrix
  - Quick check question: What is the result of the Kronecker product of a 2×2 matrix with a 3×3 matrix?

- Matrix inversion via Sherman-Morrison formula
  - Why needed here: MKOR uses Sherman-Morrison identity to perform rank-1 updates for efficient matrix inversion
  - Quick check question: How does the Sherman-Morrison formula update the inverse of a matrix when a rank-1 modification is applied?

- Distributed optimization communication patterns
  - Why needed here: Understanding communication complexity reduction is key to MKOR's performance gains
  - Quick check question: What is the communication complexity of synchronizing a d×d matrix across N workers versus d scalars?

## Architecture Onboarding

- Component map:
  - Rank-1 approximation module (computes am and gm) -> Norm-based stabilizer -> SM-based inverter -> Backend optimizer interface

- Critical path:
  1. Compute activations and gradients
  2. Calculate rank-1 approximations (am, gm)
  3. Apply norm-based stabilization
  4. Perform SM-based inversion for factor updates
  5. Precondition gradients and rescale
  6. Pass to backend optimizer

- Design tradeoffs:
  - Accuracy vs. speed: Rank-1 approximation trades some precision for O(d²) complexity
  - Communication vs. memory: Synchronizing vectors reduces communication but requires additional storage
  - Stability vs. convergence: Norm-based stabilizer prevents divergence but may limit second-order benefits

- Failure signatures:
  - Diverging training loss: Check norm-based stabilizer thresholds and learning rate
  - Poor convergence: Verify rank-1 approximation quality and SM-based inversion implementation
  - Communication bottlenecks: Monitor synchronization overhead and vector sizes

- First 3 experiments:
  1. Compare training time and accuracy of MKOR vs LAMB on BERT-Large-Uncased
  2. Measure communication overhead reduction by comparing MKOR with KAISA
  3. Test MKOR-H switching logic by monitoring loss reduction rate during training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the hybrid MKOR-H approach determine the optimal point to switch from second-order to first-order optimization during training?
- Basis in paper: [explicit] The paper describes MKOR-H as using a "loss-reduction-rate-based switching mechanism" to transition from second-order to first-order optimization when second-order updates no longer accelerate convergence.
- Why unresolved: The paper mentions the switching mechanism but doesn't provide specific criteria or thresholds for when the transition should occur. It's unclear how the algorithm detects diminishing returns from second-order updates or what metrics it uses to make this decision.
- What evidence would resolve it: Experimental results showing the switching point across different tasks, quantitative metrics used to detect convergence stagnation, and ablation studies comparing different switching criteria.

### Open Question 2
- Question: What is the theoretical convergence guarantee for MKOR compared to traditional KFAC and NGD methods?
- Basis in paper: [inferred] While the paper provides complexity analysis and empirical results showing MKOR outperforms existing methods, it doesn't establish formal convergence guarantees. The authors mention that MKOR avoids numerical instabilities present in other methods but don't prove convergence rates.
- Why unresolved: The paper focuses on practical performance improvements but doesn't provide mathematical proofs of convergence or compare the theoretical convergence properties of MKOR with other second-order methods.
- What evidence would resolve it: Formal convergence proofs showing the rate at which MKOR approaches the optimal solution, comparison of convergence bounds with KFAC and other second-order methods, and analysis of how the rank-1 approximation affects convergence guarantees.

### Open Question 3
- Question: How does MKOR perform on extremely large-scale models with sequence lengths beyond those tested in the BERT experiments?
- Basis in paper: [inferred] The paper demonstrates MKOR's effectiveness on BERT-Large with sequence lengths up to a few thousand, but doesn't explore performance on models with much larger sequence lengths or more extreme scaling scenarios.
- Why unresolved: While the authors claim MKOR addresses scalability issues in transformer models, the experiments are limited to BERT-Large. The paper doesn't test MKOR on models with sequence lengths in the tens of thousands or on architectures beyond BERT.
- What evidence would resolve it: Experiments on models like GPT-3 or T5 with much larger sequence lengths, performance comparison with other optimizers on extremely large models, and analysis of how MKOR's complexity advantages scale with model size.

### Open Question 4
- Question: What is the impact of using half-precision computations on MKOR's final model accuracy and generalization performance?
- Basis in paper: [explicit] The paper mentions that MKOR can use half-precision floating point operations to reduce costs, claiming it "doesn't need higher precision computations" and that this reduces communication costs by 2× while using cheaper computation blocks.
- Why unresolved: While the authors state that half-precision doesn't compromise stability, they don't provide empirical evidence comparing the final model accuracy and generalization when using half-precision versus full precision. The impact on the quality of the learned representations is unclear.
- What evidence would resolve it: Head-to-head accuracy comparisons between half-precision and full-precision MKOR on the same tasks, analysis of generalization gap differences, and studies on how quantization affects the quality of the learned embeddings or attention mechanisms.

### Open Question 5
- Question: How sensitive is MKOR to its hyperparameters (ζ, γ, and the norm threshold for the stabilizer) and what are the best practices for tuning them?
- Basis in paper: [inferred] The paper introduces several hyperparameters including ζ for the norm-based stabilizer, γ for momentum, and a threshold for detecting exploding gradients, but doesn't provide guidance on tuning these parameters or analyze their sensitivity.
- Why unresolved: The authors mention these hyperparameters but don't provide systematic studies on how different values affect performance, don't compare automatic tuning methods, and don't establish rules of thumb for setting these values across different model architectures.
- What evidence would resolve it: Sensitivity analysis showing how performance varies with different hyperparameter values, comparison of manual versus automatic hyperparameter tuning, and guidelines for setting hyperparameters based on model characteristics or dataset properties.

## Limitations
- Rank-1 approximation quality across diverse model architectures and tasks beyond BERT remains untested
- Communication overhead reduction claims assume specific distributed training configurations that may not generalize
- Hybrid switching mechanism's sensitivity to hyperparameter tuning is not thoroughly explored

## Confidence
- High confidence: Computational complexity reduction from O(d³) to O(d²) via rank-1 updates (well-established mathematical foundation)
- Medium confidence: Communication complexity reduction from O(d²) to O(d) (depends on implementation details and network conditions)
- Medium confidence: 2.57× and 1.85× performance improvements (based on single benchmark; limited cross-model validation)

## Next Checks
1. Test MKOR on additional transformer architectures (GPT, RoBERTa) and non-NLP models (ResNet, ViT) to assess generalization
2. Implement the hybrid switching mechanism with varying thresholds to determine optimal configuration across different tasks
3. Measure actual communication overhead in heterogeneous cluster environments with varying network conditions