---
ver: rpa2
title: Evaluating and Mitigating Discrimination in Language Model Decisions
arxiv_id: '2312.03689'
source_url: https://arxiv.org/abs/2312.03689
tags:
- decision
- discrimination
- language
- template
- decisions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates discrimination in language model (LM) decisions
  across diverse real-world scenarios. To evaluate potential discrimination, the authors
  use an LM to generate prompts covering 70 decision contexts (e.g., loans, housing,
  visas) and systematically vary demographics in each prompt.
---

# Evaluating and Mitigating Discrimination in Language Model Decisions

## Quick Facts
- arXiv ID: 2312.03689
- Source URL: https://arxiv.org/abs/2312.03689
- Reference count: 40
- One-line primary result: Language models exhibit measurable discrimination that can be significantly reduced through careful prompt engineering

## Executive Summary
This paper investigates discrimination in language model decisions across diverse real-world scenarios. Using Claude 2.0, the authors systematically vary demographics in 70 decision contexts and analyze binary decisions for different demographic groups. They find evidence of both positive discrimination (favoring non-white and non-male groups) and negative discrimination (against older individuals). Critically, the study demonstrates that careful prompt engineering, such as explicitly stating discrimination is illegal, can significantly reduce both positive and negative discrimination. The work provides a method for proactively measuring and mitigating LM discrimination across diverse applications.

## Method Summary
The authors generate 70 diverse decision scenarios covering domains like loans, housing, and visas, then systematically vary demographic attributes (age, race, gender) in each prompt. They collect binary decisions from Claude 2.0 and analyze discrimination patterns using mixed effects linear regression, with demographic variables as fixed effects and decision question types as random effects. To mitigate discrimination, they test prompt engineering interventions like stating discrimination is illegal or instructing the model to ignore demographics, measuring how these affect acceptance probabilities across demographic groups.

## Key Results
- Claude 2.0 shows both positive discrimination (favoring non-white and non-male groups) and negative discrimination (against individuals over 60)
- Discrimination is larger when demographics are explicitly stated versus inferred from names
- Prompt engineering interventions (e.g., stating discrimination is illegal) significantly reduce both positive and negative discrimination
- Discrimination patterns remain consistent across variations in prompt style and format

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models exhibit measurable discrimination when making binary decisions across diverse real-world scenarios
- Mechanism: When demographic attributes are systematically varied in prompts, the model's binary decisions show statistically significant differences in acceptance probabilities across demographic groups
- Core assumption: The prompts accurately represent realistic decision scenarios and the model's decisions reflect genuine patterns rather than artifacts of prompt construction
- Evidence anchors:
  - [abstract] "When analyzing model decisions on these prompts without further intervention, we find that the Claude 2.0 language model exhibits a mix of positive and negative discrimination"
  - [section 3.3] "We find evidence of positive discrimination... while finding negative discrimination against age groups over age 60"
- Break condition: If prompt construction introduces artifacts or the model's decisions are primarily driven by prompt format rather than content, the measured discrimination may not reflect genuine biases

### Mechanism 2
- Claim: Prompt engineering can significantly reduce both positive and negative discrimination in language model decisions
- Mechanism: Adding specific instructions about legal requirements and fairness considerations to prompts causes the model to adjust its decision-making process, reducing demographic-based differences in acceptance probabilities
- Core assumption: The model has learned patterns from training data that associate certain demographic attributes with positive or negative outcomes, but can override these when explicitly instructed to do so
- Evidence anchors:
  - [abstract] "we demonstrate techniques to significantly decrease both positive and negative discrimination through careful prompt engineering"
  - [section 5.3] "several of the interventions we explore are quite effective, especially Illegal to discriminate, Ignore demographics, Illegal + Ignore"
- Break condition: If the model's underlying representations are too strongly associated with demographic attributes, or if instructions about fairness are ignored, prompt engineering may fail to reduce discrimination

### Mechanism 3
- Claim: Discrimination patterns are robust across variations in prompt style and format
- Mechanism: The model exhibits consistent discrimination patterns regardless of whether information is presented in first-person, formal bullet points, emotional language, or with typos and informal style
- Core assumption: The model's discrimination patterns are based on semantic understanding of demographic attributes rather than surface-level features of the prompt
- Evidence anchors:
  - [section 4.2] "the results are largely consistent across prompt variations—we still see roughly the same discrimination patterns"
- Break condition: If the model is highly sensitive to surface features of text or if certain prompt formats systematically obscure demographic information, discrimination patterns may vary significantly across formats

## Foundational Learning

- Concept: Statistical significance testing
  - Why needed here: To determine whether observed differences in model decisions across demographic groups are meaningful or due to random variation
  - Quick check question: How would you determine if a 5% difference in loan approval rates between demographic groups is statistically significant given the sample size?

- Concept: Mixed effects linear regression
  - Why needed here: To model how demographic attributes affect model decisions while accounting for variation across different decision scenarios
  - Quick check question: What is the advantage of using mixed effects models over simple linear regression when analyzing decisions across multiple question types?

- Concept: Prompt engineering
  - Why needed here: To understand how adding specific instructions to prompts can influence model behavior and reduce unwanted patterns
  - Quick check question: How might adding "Please ensure your decision is unbiased" to a prompt influence a language model's output?

## Architecture Onboarding

- Component map: Prompt generation module -> Demographic variation module -> Model evaluation module -> Discrimination analysis module -> Mitigation module
- Critical path: Generate diverse prompts → Systematically vary demographics → Collect model decisions → Analyze discrimination patterns → Test mitigation strategies → Validate effectiveness
- Design tradeoffs: Evaluating broad discrimination patterns vs. specific use cases; using model-generated prompts vs. human-written prompts; measuring explicit vs. implicit discrimination; testing multiple mitigation strategies vs. focusing on most promising ones
- Failure signatures: Inconsistent discrimination patterns across prompt formats (suggests surface-level artifacts); mitigation strategies that don't reduce discrimination (suggests deeply embedded biases); high variance in discrimination scores across scenarios (suggests methodological issues)
- First 3 experiments:
  1. Generate 10 diverse decision scenarios and systematically vary age across 20-80 years to test basic discrimination patterns
  2. Test the "Illegal to discriminate" intervention on a subset of scenarios to validate mitigation effectiveness
  3. Vary prompt format between formal and informal styles to test robustness of discrimination patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the discrimination pattern observed in Claude 2.0 extend to other large language models across different model sizes and architectures?
- Basis in paper: [inferred] The paper focuses on Claude 2.0 and demonstrates discrimination patterns, but doesn't test other models or architectures
- Why unresolved: The study is limited to one specific model, leaving open whether the discrimination patterns are model-specific or more general
- What evidence would resolve it: Comparative studies testing discrimination patterns across multiple language models (different sizes, architectures, training methods) using the same evaluation methodology

### Open Question 2
- Question: How do language model decisions change when evaluated with real-world documents (resumes, medical records, financial statements) versus the synthetic prompts used in this study?
- Basis in paper: [explicit] The paper acknowledges this limitation: "there are several important aspects of real-world use that are not fully covered by our evaluations"
- Why unresolved: The study uses controlled, synthetic prompts rather than authentic real-world documents that decision-makers might actually use
- What evidence would resolve it: Studies using actual real-world documents as input to language models for decision-making, comparing discrimination patterns with those found in the synthetic prompt study

### Open Question 3
- Question: Do the prompt-based interventions for mitigating discrimination maintain their effectiveness over extended interactions or multi-turn conversations with language models?
- Basis in paper: [inferred] The paper tests prompt interventions on single prompts but doesn't examine their durability in extended dialogues
- Why unresolved: The study examines only single-turn interactions, not how discrimination might reappear in longer conversations where the intervention is no longer present
- What evidence would resolve it: Studies tracking discrimination patterns across multi-turn conversations where prompt interventions are applied only at the beginning versus continuously

## Limitations
- The study focuses on a single model (Claude 2.0), constraining generalizability across different language models
- The use of LM-generated prompts introduces uncertainty about whether they accurately reflect real-world decision scenarios
- The binary decision framework simplifies complex real-world decisions, potentially missing nuanced discrimination patterns

## Confidence
**High confidence** in the observation that discrimination varies systematically with explicit versus implicit demographic cues. **Medium confidence** in the effectiveness of prompt engineering interventions. **Medium confidence** in the discrimination patterns observed, given the reliance on a single model and potential for prompt artifacts.

## Next Checks
1. **Cross-model validation**: Test the same prompt set and mitigation strategies on at least two additional language models (e.g., GPT-4, LLaMA) to assess whether discrimination patterns and intervention effectiveness generalize across architectures.

2. **Real-world scenario comparison**: Compare model decisions on the generated prompts against human decisions in equivalent real-world scenarios to validate whether the model's discrimination patterns reflect actual societal biases or prompt artifacts.

3. **Longitudinal stability test**: Apply the most effective prompt engineering interventions across a diverse set of decision contexts over multiple weeks to evaluate whether discrimination reduction remains consistent or degrades over time.