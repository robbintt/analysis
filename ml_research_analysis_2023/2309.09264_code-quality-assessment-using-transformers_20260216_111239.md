---
ver: rpa2
title: Code quality assessment using transformers
arxiv_id: '2309.09264'
source_url: https://arxiv.org/abs/2309.09264
tags:
- code
- quality
- codebert
- task
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using CodeBERT and task-adapted pre-training
  to automatically assess code quality for programming assignments. The authors introduce
  a novel dataset of Java methods labeled by domain experts and compare several models,
  including a baseline random forest.
---

# Code quality assessment using transformers

## Quick Facts
- arXiv ID: 2309.09264
- Source URL: https://arxiv.org/abs/2309.09264
- Reference count: 15
- Key outcome: Transformer-based models with task-adapted pre-training significantly outperform baselines in automatically assessing code quality for programming assignments.

## Executive Summary
This paper proposes using CodeBERT and task-adapted pre-training to automatically assess code quality for programming assignments. The authors introduce a novel dataset of Java methods labeled by domain experts and compare several models, including a baseline random forest. Their results show that transformer-based models, especially those using task-adapted pre-training, significantly outperform the baseline in both accuracy and ranking ability. The task-adapted CodeBERT achieved 86% accuracy, 72% F1, 74% AUC-ROC, and 92% AUC-PR, while a smaller model trained only on task data (FxBERT) achieved 85% accuracy and performed close to the larger CodeBERT models. Feature attribution analysis using SHAP revealed that the models focus on code patterns such as improper iteration methods and redundant clearing calls when identifying low-quality code.

## Method Summary
The authors collected a dataset of 25,000+ Java methods from 250 students, labeled by domain experts as good (70%) or bad (30%) quality. They fine-tuned CodeBERT with different pre-training strategies: no pre-training, domain-adaptive pre-training (DAPT), task-adaptive pre-training (TAPT), and task-only pre-training (FxBERT). The models were evaluated on accuracy, AUC-ROC, AUC-PR, F1-score, precision, and recall, with SHAP analysis used for feature attribution to identify code patterns associated with low quality.

## Key Results
- Transformer-based models with task-adapted pre-training significantly outperformed the baseline random forest model.
- Task-adapted CodeBERT achieved 86% accuracy, 72% F1, 74% AUC-ROC, and 92% AUC-PR.
- A smaller model trained only on task data (FxBERT) achieved 85% accuracy and performed close to the larger CodeBERT models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-Adapted Pre-training (TAPT) significantly improves transformer-based code quality prediction over generic pre-training.
- Mechanism: TAPT fine-tunes the CodeBERT model on domain-specific unlabeled code from the target task (JavaFX methods), allowing the model to internalize patterns specific to the assignment's coding style, naming conventions, and structural idioms before the supervised fine-tuning phase.
- Core assumption: The distribution of code in the pre-training set closely matches the distribution of code in the downstream task, so learned representations transfer effectively.
- Evidence anchors:
  - [abstract] "transformer based models using task adapted pre-training can solve the task more efficiently"
  - [section] "task adapted model is another CodeBERT that is further pre-trained with 10,000 Java methods from previous years JavaFX assignment"
  - [corpus] weak; no direct comparison studies found in corpus, but related works on pre-training adaptation exist
- Break condition: If the pre-training data distribution diverges too much from the target task, TAPT could hurt performance by overfitting to irrelevant patterns.

### Mechanism 2
- Claim: Smaller, task-only pre-trained models (FxBERT) can match larger pre-trained models when domain-specific training data is abundant.
- Mechanism: FxBERT uses only the 10k JavaFX methods for pre-training, learning domain-relevant representations without the computational overhead of large-scale generic pre-training. This focuses learning capacity on the target task's semantic space.
- Core assumption: The target task data contains sufficient diversity to learn useful representations without generic pre-training.
- Evidence anchors:
  - [section] "FxBERT... achieved 85% accuracy and performed close to the larger CodeBERT models"
  - [section] "This isolated pretraining is unorthodox... but this paper challenges this idea as we don't know the limitations of code language models"
  - [corpus] weak; no corpus evidence of isolated pre-training success, this is a novel experimental claim
- Break condition: If the task data is too small or homogeneous, FxBERT will underfit and fail to generalize.

### Mechanism 3
- Claim: Feature attribution via SHAP identifies anti-patterns (e.g., improper iteration methods, redundant clear calls) that align with human notions of code quality.
- Mechanism: SHAP values decompose model predictions into contributions from each token n-gram, revealing which code constructs the model associates with low quality.
- Core assumption: The model has learned semantically meaningful representations of code constructs that correlate with human judgments.
- Evidence anchors:
  - [abstract] "Feature attribution analysis using SHAP revealed that the models focus on code patterns such as improper iteration methods and redundant clearing calls"
  - [section] Listing 1.1 example showing entrySet() usage flagged as high attribution
  - [corpus] weak; SHAP is commonly used but corpus lacks code-specific SHAP interpretability studies
- Break condition: If the model relies on spurious correlations (e.g., formatting, uncommon tokens) rather than semantic patterns, SHAP explanations may mislead.

## Foundational Learning

- Concept: Bidirectional transformer architecture (BERT-style)
  - Why needed here: CodeBERT uses self-attention to capture long-range dependencies in code, crucial for understanding method-level quality which depends on control flow, naming, and structure.
  - Quick check question: What is the key difference between BERT's bidirectional attention and traditional left-to-right language models?
- Concept: Pre-training objectives (Masked Language Modeling)
  - Why needed here: MLM forces the model to learn contextual representations by predicting masked tokens, essential for capturing code semantics before fine-tuning.
  - Quick check question: Why does MLM help the model understand code structure better than next-token prediction?
- Concept: Evaluation metrics for imbalanced classification
  - Why needed here: The dataset has 70% good quality methods, so accuracy is misleading; AUC-ROC and AUPRC better capture ranking and precision-recall trade-offs.
  - Quick check question: If a model always predicts "good quality," what would its accuracy, precision, and recall be?

## Architecture Onboarding

- Component map: Tokenized Java method -> CodeBERT encoder -> Mean pooling -> Softmax layer -> Quality prediction
- Critical path: Tokenization -> CodeBERT forward pass -> Pooling -> Softmax -> Loss computation
- Design tradeoffs:
  - Large CodeBERT vs small FxBERT: accuracy vs memory/inference speed
  - Generic pre-training vs task-adapted: generalization vs task-specific performance
  - Binary vs multi-level quality scoring: simplicity vs granularity
- Failure signatures:
  - Low AUPRC but high accuracy: model exploiting class imbalance
  - Similar performance across all models: pre-training not effective or data too simple
  - SHAP attributions focus on formatting/rare tokens: model learning spurious correlations
- First 3 experiments:
  1. Baseline: Train TFIDF-RF on the dataset; record accuracy, AUC-ROC, AUPRC
  2. Default CodeBERT: Train with no extra pre-training; compare metrics to baseline
  3. TAPT-CodeBERT: Pre-train on JavaFX methods, then fine-tune; compare to previous runs and inspect SHAP attributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different model architectures (beyond CodeBERT) compare in predicting code quality, particularly in terms of parameter efficiency and performance trade-offs?
- Basis in paper: [explicit] The paper compares CodeBERT variants with a baseline random forest but does not explore other architectures like RoBERTa, GPT-based models, or graph neural networks for code quality assessment.
- Why unresolved: The study focuses solely on transformer-based models and does not investigate whether alternative architectures (e.g., graph neural networks or domain-specific models) could outperform or be more efficient for code quality tasks.
- What evidence would resolve it: Comparative experiments testing alternative architectures (e.g., graph neural networks, GPT-based models) on the same dataset, measuring accuracy, F1-score, AUC-ROC, and parameter efficiency.

### Open Question 2
- Question: To what extent does the proximity of pre-training data to the downstream task influence model performance, and is there an optimal balance between domain-specific and task-specific pre-training?
- Basis in paper: [explicit] The paper explores domain-adaptive (DAPT) and task-adaptive (TAPT) pre-training but does not quantify the marginal gains from combining these approaches or determine the optimal balance.
- Why unresolved: While the paper shows that task-adapted pre-training improves performance, it does not analyze how much domain-specific pre-training contributes or whether there is a diminishing return when combining both approaches.
- What evidence would resolve it: Ablation studies varying the proportion of domain-specific vs. task-specific pre-training data and measuring performance changes to identify the optimal balance.

### Open Question 3
- Question: Can the code quality assessment model be generalized to other programming languages, and how does its performance vary across different languages and paradigms?
- Basis in paper: [explicit] The study focuses exclusively on Java methods and does not test the model's generalizability to other languages like Python, C++, or functional programming languages.
- Why unresolved: The dataset and experiments are limited to Java, leaving uncertainty about whether the model's effectiveness transfers to other languages with different syntax, paradigms, or idioms.
- What evidence would resolve it: Replicating the experiments on datasets from multiple programming languages and comparing performance metrics across languages to assess generalizability.

## Limitations

- The dataset relies on subjective quality judgments from domain experts without standardized rubrics, introducing potential inter-rater variability.
- The binary good/bad quality distinction may oversimplify the nuanced nature of code quality assessment.
- The evaluation focuses solely on JavaFX assignments, limiting generalizability to other programming domains or problem types.

## Confidence

- **High confidence**: The comparative performance of transformer models versus baseline TF-IDF Random Forest is well-established through direct experimental comparison.
- **Medium confidence**: The effectiveness of task-adapted pre-training (TAPT) is demonstrated but could benefit from ablation studies comparing different pre-training dataset sizes and compositions.
- **Low confidence**: The claim that smaller models (FxBERT) can match larger pre-trained models is based on a single experimental setup and may not generalize across different task domains or dataset sizes.

## Next Checks

1. **Cross-domain validation**: Test the trained models on code quality assessment tasks from different programming domains (e.g., web development, data science) to evaluate generalizability beyond JavaFX assignments.

2. **Human validation study**: Conduct a user study where human experts evaluate the SHAP-identified code patterns against their own quality assessments to verify semantic alignment between model interpretations and human judgment.

3. **Pre-training data sensitivity analysis**: Systematically vary the size and composition of pre-training datasets to establish the relationship between pre-training data characteristics and downstream task performance, particularly for the FxBERT approach.