---
ver: rpa2
title: 'Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning'
arxiv_id: '2310.19831'
source_url: https://arxiv.org/abs/2310.19831
tags:
- learning
- decision
- which
- conf
- behavior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INTERPOLE, a novel Bayesian method for interpretable
  policy learning that models decision-making behavior in partially observable environments.
  The key idea is to jointly estimate an agent's belief-update process (decision dynamics)
  and belief-action mapping (decision boundaries), providing transparent explanations
  of individual actions.
---

# Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning

## Quick Facts
- arXiv ID: 2310.19831
- Source URL: https://arxiv.org/abs/2310.19831
- Authors: 
- Reference count: 40
- One-line primary result: INTERPOLE jointly learns interpretable decision dynamics and boundaries while maintaining high action prediction accuracy

## Executive Summary
INTERPOLE introduces a Bayesian method for interpretable policy learning that explains individual actions by jointly estimating an agent's belief-update process and belief-action mapping. Unlike black-box approaches, it directly parameterizes policies using interpretable mean vectors over belief simplices, creating transparent decision boundaries that domain experts can understand. The method successfully recovers subjective decision-making behavior even when agents have biased beliefs or suboptimal policies.

## Method Summary
INTERPOLE uses an Expectation-Maximization framework to learn both the belief-update dynamics (through an Input-Output Hidden Markov Model) and the policy mapping (via mean-vector parameterization) from observed action-observation sequences. The policy is defined as a softmax over distances between current beliefs and action-specific mean vectors, creating probabilistic decision regions. The Bayesian learning algorithm estimates all parameters simultaneously through forward-backward computation of marginal probabilities and gradient-based updates, allowing recovery of both optimal and biased decision-making behavior without assuming rationality.

## Key Results
- INTERPOLE achieves first- or second-best performance across all action-matching metrics compared to black-box methods
- The method correctly recovers underlying explanations for behavior even when agents have biased beliefs
- Experiments on Alzheimer's disease diagnosis data demonstrate practical interpretability with decision boundaries aligning with clinical observations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: INTERPOLE provides transparent explanations by directly parameterizing decision boundaries over belief simplices rather than through indirect reward functions.
- Mechanism: The mean-vector policy parameterization π(a|b) = e−η∥b−µa∥2/Pa′∈A e−η∥b−µa′∥2 induces probabilistic "decision regions" where the action closest to the current belief is most likely to be chosen. This creates interpretable decision boundaries (lines equidistant between mean vectors) that domain experts can understand.
- Core assumption: Decision-makers can be modeled as having internal beliefs over discrete states that they update through their own subjective (possibly biased) decision dynamics.
- Evidence anchors:
  - [abstract] "INTERPOLE directly parameterizes policies using interpretable mean vectors over belief simplices"
  - [section 3] "We argue that a probabilistic parameterization that directly induces 'decision regions' over the belief simplex is uniquely interpretable"
  - [corpus] Weak - no corpus papers directly discuss mean-vector parameterization over belief simplices
- Break condition: If decision-makers cannot be meaningfully modeled with discrete state beliefs, or if the belief simplex cannot be constructed from observable data.

### Mechanism 2
- Claim: INTERPOLE can recover subjective (biased) decision-making behavior by jointly learning decision dynamics and decision boundaries without assuming optimality or unbiased beliefs.
- Mechanism: The Bayesian learning algorithm simultaneously estimates T, O, b1 (decision dynamics) and η, {µa}a∈A (decision boundaries) by maximizing P(θ|D) through an EM-like procedure. This allows modeling of agents with imperfect knowledge of environment dynamics or suboptimal policies.
- Core assumption: The observed action sequences contain sufficient information to infer both the belief-update process and the mapping from beliefs to actions.
- Evidence anchors:
  - [abstract] "jointly estimates an agent's (possibly biased) belief-update process together with their (possibly suboptimal) belief-action mapping"
  - [section 5] "we show INTERPOLE correctly recovers underlying explanations for behavior—even if the agent is biased"
  - [corpus] Weak - no corpus papers discuss joint learning of decision dynamics and boundaries without optimality assumptions
- Break condition: If the data is insufficient to distinguish between different possible belief-update processes, or if the agent's policy is too stochastic to recover meaningful boundaries.

### Mechanism 3
- Claim: INTERPOLE achieves comparable or better action prediction accuracy compared to black-box methods while providing interpretability.
- Mechanism: The method uses gradient-based optimization of the expected log-posterior Q(θ; ˆθ) which can be efficiently computed through backpropagation through time. This allows learning policies that match observed behavior without sacrificing transparency.
- Core assumption: The belief-update process and decision boundaries can be parameterized in a way that is both expressive enough to capture the observed behavior and differentiable for gradient-based learning.
- Evidence anchors:
  - [abstract] "Experiments on simulated and real-world Alzheimer's disease diagnosis data demonstrate that INTERPOLE effectively explains decision-making behavior while maintaining high accuracy in action prediction"
  - [section 5] "INTERPOLE performs first- or second-best across all three action-matching based metrics"
  - [corpus] Weak - no corpus papers directly compare action prediction accuracy between interpretable and black-box methods
- Break condition: If the belief dynamics are too complex to parameterize effectively, or if the mean-vector representation cannot capture the necessary policy complexity.

## Foundational Learning

- Concept: Belief simplex representation
  - Why needed here: INTERPOLE operates by representing decision-maker beliefs as probability distributions over discrete states, which can be visualized as points in a simplex
  - Quick check question: Given a three-state problem (NL, MCI, Dementia), what point in the simplex represents 50% certainty of MCI and 25% certainty of each other state?

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: The framework models decision-making in environments where the true state is not directly observable, requiring belief updates based on observations
  - Quick check question: In a POMDP, what is the difference between the agent's belief state and the true environment state?

- Concept: Expectation-Maximization algorithm
  - Why needed here: INTERPOLE uses an EM-like procedure to simultaneously learn both the belief-update process and the policy mapping from beliefs to actions
  - Quick check question: In the E-step of EM, what quantity is being computed when we calculate P(D̄|D, θ̂)?

## Architecture Onboarding

- Component map: Observation sequence → Belief trajectory (forward pass) → Policy likelihood → Gradient computation → Parameter update
- Critical path: Observation sequence → Belief trajectory (forward pass) → Policy likelihood → Gradient computation → Parameter update
- Design tradeoffs:
  - Expressiveness vs interpretability: More complex belief dynamics improve fit but reduce transparency
  - Computational efficiency vs accuracy: Full belief trajectory computation is expensive but necessary for correct gradients
  - Model assumptions vs flexibility: Discrete state space enables interpretability but may not capture all real-world complexities
- Failure signatures:
  - Belief trajectories that don't converge to clear decision regions
  - High belief mismatch but low action prediction error (model fitting noise)
  - Sensitivity to initialization suggesting local optima issues
- First 3 experiments:
  1. Synthetic data with known parameters: Generate trajectories using fixed T, O, b1, η, {µa} and verify INTERPOLE recovers them
  2. Bias recovery test: Create biased belief updates and test if INTERPOLE correctly identifies the bias
  3. Ablation study: Remove the joint learning component and compare to separate belief and policy learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can INTERPOLE handle continuous observation spaces more effectively, beyond the finite case demonstrated?
- Basis in paper: [inferred] The paper states "While we take it here that Z is finite, our method can easily be generalized to allow continuous observations" but does not demonstrate or elaborate on this extension.
- Why unresolved: The finite observation case is implemented and tested, but the continuous case is only mentioned theoretically without practical implementation or performance analysis.
- What evidence would resolve it: A concrete implementation of INTERPOLE with continuous observation spaces, along with experimental results comparing its performance to the finite case.

### Open Question 2
- Question: How sensitive is INTERPOLE's performance to the choice of the inverse temperature parameter η, and are there principled ways to select it?
- Basis in paper: [inferred] The paper mentions that "The inverse temperature controls the transitions between such boundaries" and uses fixed values (η = 1 for ADNI, η = 10 for DIAG) without exploring sensitivity or providing a principled selection method.
- Why unresolved: The paper does not investigate how different values of η affect the learned policies or provide guidance on selecting this hyperparameter.
- What evidence would resolve it: A sensitivity analysis showing how varying η affects performance metrics, along with a principled method for selecting η based on data characteristics or cross-validation.

### Open Question 3
- Question: Can INTERPOLE scale to larger state spaces while maintaining interpretability, and what are the computational limits?
- Basis in paper: [inferred] The paper focuses on three-state medical diagnosis problems and mentions computational complexity O(nτ²S² max{S, AZ}) but does not explore larger state spaces or discuss practical scalability limits.
- Why unresolved: The experiments are limited to small state spaces (3 states for ADNI, 2 states for DIAG), and the paper does not address how performance degrades or interpretability changes with larger state spaces.
- What evidence would resolve it: Experiments with progressively larger state spaces, analysis of how interpretability metrics change with state space size, and identification of practical limits where the method becomes computationally prohibitive.

## Limitations
- Limited to discrete state spaces, which may not capture all real-world complexities
- Computational complexity scales poorly with state space size (O(nτ²S² max{S, AZ}))
- Limited evaluation of interpretability through direct expert validation studies

## Confidence
- **High confidence** in the core methodological contribution and the demonstration that interpretable mean-vector policies can achieve competitive action prediction accuracy
- **Medium confidence** in the practical utility of the learned explanations for domain experts, as the Alzheimer's disease application shows correlation with clinical observations but doesn't demonstrate direct expert validation
- **Low confidence** in the method's generalizability to domains with continuous state spaces or significantly more complex belief-update dynamics

## Next Checks
1. **Synthetic stress test**: Generate data from a more complex belief-update process (e.g., non-linear dynamics or continuous observations) to evaluate INTERPOLE's performance when assumptions are violated
2. **Expert validation study**: Conduct a controlled experiment where clinical experts evaluate the interpretability and usefulness of INTERPOLE's explanations versus black-box alternatives on held-out patient cases
3. **Scalability benchmark**: Test INTERPOLE on problems with 10+ discrete states and 20+ actions to measure computational scaling and degradation in explanation quality