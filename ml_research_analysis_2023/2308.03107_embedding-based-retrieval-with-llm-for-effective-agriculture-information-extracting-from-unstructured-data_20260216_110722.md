---
ver: rpa2
title: Embedding-based Retrieval with LLM for Effective Agriculture Information Extracting
  from Unstructured Data
arxiv_id: '2308.03107'
source_url: https://arxiv.org/abs/2308.03107
tags:
- information
- text
- stage
- arxiv
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a zero-shot approach for information extraction
  from unstructured agricultural documents using embedding-based retrieval (EBR) and
  large language models (LLMs). The method consists of four stages: text retrieval
  and filtering using EBR, followed by LLM-based entity and attribute extraction,
  and JSON output generation.'
---

# Embedding-based Retrieval with LLM for Effective Agriculture Information Extracting from Unstructured Data

## Quick Facts
- arXiv ID: 2308.03107
- Source URL: https://arxiv.org/abs/2308.03107
- Reference count: 4
- Primary result: Zero-shot information extraction from unstructured agricultural documents using embedding-based retrieval and LLM, achieving precision rates of 76-89% and recall rates of 72-89% across different stages

## Executive Summary
This paper presents a zero-shot approach for extracting structured agricultural information from unstructured documents using embedding-based retrieval (EBR) and large language models (LLMs). The system processes agricultural documents through a four-stage pipeline that first filters relevant text segments using EBR, then uses LLM question-answering to extract descriptive words, convert them to attributes, identify entities, and generate structured JSON output. The method demonstrates high accuracy in entity extraction but shows variability in attribute classification consistency. The approach addresses the challenge of efficiently processing unstructured agricultural information with minimal human intervention.

## Method Summary
The method implements a four-stage pipeline for information extraction from agricultural documents. First, documents are split into small pieces, vectorized, and stored in a vector database for EBR-based filtering of relevant text segments. The LLM (gpt-3.5-turbo) then processes the filtered text through four sequential stages: identifying descriptive words, converting them to attributes, extracting entities, and matching entities with attributes to generate structured JSON output. The system uses carefully designed prompts to guide the LLM's zero-shot processing without requiring fine-tuning. EBR filtering reduces processing costs by avoiding full-text scanning while managing LLM token limitations through the staged approach.

## Key Results
- The system achieves precision rates of 76-89% and recall rates of 72-89% across different extraction stages
- Entity extraction (Stage 3) demonstrates the highest accuracy, while attribute classification shows inconsistent performance
- The zero-shot approach successfully extracts structured data about pests from agricultural documents with minimal human intervention
- EBR filtering effectively reduces processing costs while maintaining retrieval accuracy for relevant text segments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding-based retrieval (EBR) enables efficient filtering of relevant text segments for LLM processing
- Mechanism: Documents are split into small pieces, vectorized, and stored in a vector database. EBR uses vector similarity to find text segments most relevant to query terms, avoiding full-text scanning and reducing processing costs
- Core assumption: Similar documents have closer distances in vector space representation
- Evidence anchors:
  - [section]: "Embedding-based retrieval (EBR) differs from plain text search in that, it transforms documents into vectors and maps them to a vector space. This allows similar documents to have a closer distance in the vector space, while dissimilar documents have a greater distance."
  - [corpus]: Weak - no direct evidence in corpus neighbors
- Break condition: If the vector embedding fails to capture semantic similarity, irrelevant text segments may be retrieved, causing the LLM to process unnecessary information

### Mechanism 2
- Claim: Zero-shot LLM processing can accurately extract entities and attributes from agricultural text
- Mechanism: The system uses GPT-3.5-turbo with carefully designed prompts to identify descriptive words, convert them to attributes, extract entities, and match them into structured JSON format
- Core assumption: The general LLM has sufficient domain knowledge to understand agricultural terminology without fine-tuning
- Evidence anchors:
  - [abstract]: "our approach achieves consistently better accuracy in the benchmark while maintaining efficiency"
  - [section]: "Experiments will separately evaluate the performance of these three parts... The model demonstrates a high accuracy in stage 3, namely entity extraction, but does not perform as well in the other two aspects"
- Break condition: If agricultural terminology is too specialized or uncommon, the LLM may fail to correctly identify entities or attributes, requiring domain-specific training

### Mechanism 3
- Claim: Multi-stage pipeline decomposition enables accurate information extraction while managing token limitations
- Mechanism: The IE task is broken into four stages: EBR filtering, descriptive word extraction, attribute conversion, and entity-attribute matching. Each stage processes smaller text segments, avoiding LLM token restrictions
- Core assumption: Breaking down complex tasks into smaller, sequential steps improves accuracy and efficiency
- Evidence anchors:
  - [section]: "In this FINDER system, IE tasks are decomposed into a 4-stage, multi-turn question-and-answer process, interspersed with EBR to extract relevant text, avoiding token restrictions and reducing costs"
  - [corpus]: Weak - no direct evidence in corpus neighbors
- Break condition: If errors compound across stages, the final output quality may degrade significantly, requiring error correction mechanisms between stages

## Foundational Learning

- Vector embeddings and similarity search
  - Why needed here: EBR relies on converting text to vectors and finding similar vectors for retrieval
  - Quick check question: How does cosine similarity between vectors relate to semantic similarity of text segments?

- Prompt engineering for structured output
  - Why needed here: The system depends on carefully crafted prompts to guide LLM output into structured JSON format
  - Quick check question: What prompt structure would best guide an LLM to output JSON with specific fields?

- Named Entity Recognition (NER) concepts
  - Why needed here: Stage 3 extracts entities from text, similar to NER tasks in NLP
  - Quick check question: What distinguishes a "physical object" entity from other types of entities in agricultural text?

## Architecture Onboarding

- Component map: User documents → Text splitter → Vector database → EBR filter → LLM (4 stages) → JSON output
- Critical path: EBR filtering → Stage 1 (descriptive words) → Stage 2 (attributes) → Stage 3 (entities) → Stage 4 (matching) → JSON
- Design tradeoffs: Using general LLM vs. domain-specific model (accuracy vs. development cost), pipeline stages vs. monolithic approach (complexity vs. control)
- Failure signatures: High EBR filter false positives (irrelevant text in results), inconsistent attribute classification, entity extraction misses key terms, JSON formatting errors
- First 3 experiments:
  1. Test EBR filter accuracy with controlled queries and known document sets
  2. Validate Stage 1 descriptive word extraction on sample agricultural text
  3. Measure Stage 2 attribute conversion consistency across similar descriptive terms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are different prompt engineering strategies for improving attribute classification consistency in agricultural information extraction?
- Basis in paper: [explicit] The authors note that LLM output for attribute classification is inconsistent, with similar descriptive words sometimes being classified differently (e.g., "dark spots" classified as either "colour" or "pattern"). They mention this could potentially be improved through prompt engineering.
- Why unresolved: The paper only mentions this as a potential future direction without testing specific prompt engineering approaches or comparing their effectiveness.
- What evidence would resolve it: Systematic experimentation with different prompt structures, few-shot examples, or chain-of-thought prompting specifically designed to improve attribute classification consistency, with quantitative comparison of results.

### Open Question 2
- Question: What is the optimal pipeline structure for agricultural information extraction that minimizes errors in entity-attribute matching?
- Basis in paper: [explicit] The authors identify issues with the current pipeline, particularly in stage 4 where matching values and attributes produces imperfect results (e.g., "6mm in length when mature" should output just "6mm"). They suggest this might require model-specific training rather than general chat models.
- Why unresolved: The paper only identifies the problem and suggests possible solutions without implementing or testing alternative pipeline structures or specialized models.
- What evidence would resolve it: Comparative testing of different pipeline architectures (e.g., parallel vs sequential processing, multi-stage refinement) using both general LLMs and specialized information extraction models, with error analysis of matching accuracy.

### Open Question 3
- Question: How does the system perform at scale across diverse agricultural document types and what is its practical utility?
- Basis in paper: [inferred] The authors acknowledge that without "large-scale testing or practical application," the real-world effectiveness of their system remains unknown. They also mention testing on AHDB datasets but don't report on scalability or diverse document types.
- Why unresolved: The paper presents proof-of-concept results on limited datasets without evaluating performance across the full spectrum of agricultural documents (scientific papers, news, government reports, etc.) or in actual farming contexts.
- What evidence would resolve it: Large-scale deployment testing across multiple agricultural document types with varying structures and domains, measuring accuracy, efficiency, and practical usability metrics in real-world agricultural information management scenarios.

## Limitations
- Attribute classification consistency remains problematic, with the LLM sometimes providing similar but not identical attributes for similar descriptive words
- The four-stage pipeline introduces potential for error propagation between stages, affecting final output quality
- Zero-shot capability claims are based on results from a limited agricultural domain (pest information) without testing on diverse document types

## Confidence

- **High Confidence**: The core mechanism of using embedding-based retrieval to filter relevant text segments before LLM processing is well-established and technically sound. The general approach of decomposing complex IE tasks into sequential stages is a proven methodology.
- **Medium Confidence**: The claimed precision and recall rates are based on evaluation against ground truth data, but the specific evaluation methodology and dataset details are limited. The performance differences between stages suggest the system may work better for certain types of extraction tasks than others.
- **Low Confidence**: The zero-shot capability claims are based on results from a single domain and dataset. Without testing on diverse agricultural documents or other domains, it's unclear whether the general LLM can truly handle specialized agricultural terminology without fine-tuning.

## Next Checks
1. Cross-domain validation: Test the system on agricultural documents from different sources and domains (crop diseases, farming practices, weather impacts) to assess generalizability beyond pest information.
2. Error analysis by stage: Conduct detailed analysis of where errors occur most frequently in the four-stage pipeline to identify whether error correction between stages would improve overall accuracy.
3. Prompt optimization study: Systematically vary prompt templates for each stage to determine if more specialized prompts can improve attribute classification consistency and reduce variation in similar descriptive word handling.