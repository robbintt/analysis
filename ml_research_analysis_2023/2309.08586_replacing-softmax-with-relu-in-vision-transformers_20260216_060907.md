---
ver: rpa2
title: Replacing softmax with ReLU in Vision Transformers
arxiv_id: '2309.08586'
source_url: https://arxiv.org/abs/2309.08586
tags:
- relu
- attention
- scaling
- softmax
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Replacing softmax with ReLU in Vision Transformers This paper addresses
  the high computational cost of softmax operations in transformer attention mechanisms.
  The authors propose using ReLU activation scaled by inverse sequence length (ReLU-attention)
  as an alternative to softmax.
---

# Replacing softmax with ReLU in Vision Transformers

## Quick Facts
- arXiv ID: 2309.08586
- Source URL: https://arxiv.org/abs/2309.08586
- Reference count: 28
- Primary result: Replacing softmax with ReLU in vision transformers while maintaining competitive accuracy

## Executive Summary
This paper proposes replacing the computationally expensive softmax operation in transformer attention mechanisms with ReLU activation scaled by inverse sequence length. The authors demonstrate that ReLU-attention divided by sequence length can match softmax-attention scaling behavior for vision transformers trained on ImageNet-21k. This approach enables better parallelization over sequence length dimension with fewer operations while maintaining competitive accuracy across different model scales (small to large vision transformers).

## Method Summary
The method replaces traditional softmax attention with ReLU-attention scaled by sequence length (L^-α). Specifically, attention weights are computed as ϕ(L^-α * QK^T/√d) where ϕ is a non-linearity (ReLU, GELU, softplus, etc.) and α is a scaling exponent. The authors test various non-linearities and scaling factors, finding that α ≈ 1 works best across different vision transformer scales. The approach is implemented in the BigVision codebase and evaluated on ImageNet-21k and ImageNet-1k datasets.

## Key Results
- ReLU-attention with sequence length scaling achieves comparable performance to softmax-attention across vision transformer model sizes
- Best results obtained when scaling factor exponent α is close to 1
- No single non-linearity clearly outperforms others at α ≈ 1, with ReLU chosen for computational efficiency
- Removing qk-layernorm doesn't significantly impact performance at tested scales
- The approach enables better parallelization over sequence length dimension with fewer operations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReLU-attention scaled by L^-1 preserves expected attention weight magnitude similar to softmax-attention
- Mechanism: The softmax operation ensures that the sum of attention weights equals 1, so the expected weight per token is 1/L. The ReLU-activation followed by division by L maintains this expected magnitude distribution
- Core assumption: Preserving expected attention weight magnitude is crucial for maintaining transformer performance when replacing softmax
- Evidence anchors:
  - [abstract] "we find that this degradation is mitigated when dividing by sequence length"
  - [section] "This implies that Ej[αij] = L−1. While it is unlikely that this is a necessary condition, ϕ = L−1relu does ensure that Ej[αij] is O(L−1) at initialization"
  - [corpus] Weak evidence - corpus neighbors discuss softmax-free attention but don't directly address sequence length scaling effects
- Break condition: If the sequence length scaling factor is removed or set to values far from 1, accuracy degrades significantly as shown in Figure 2

### Mechanism 2
- Claim: ReLU-attention enables better parallelization over sequence length dimension with fewer operations
- Mechanism: ReLU-attention computes attention weights independently for each query-key pair without requiring the normalization over all tokens that softmax demands, enabling parallel computation across the sequence length dimension
- Core assumption: Computational efficiency gains from parallelization outweigh any potential accuracy loss from removing softmax normalization
- Evidence anchors:
  - [abstract] "This result presents new opportunities for parallelization, as ReLU-attention can be parallelized over the sequence length dimension with fewer gather operations than traditional attention"
  - [section] "As a highlight, we observe that attention with ReLU divided by sequence length can approach or match traditional softmax attention in terms of scaling behavior as a function of compute for vision transformers"
  - [corpus] Weak evidence - corpus neighbors discuss attention efficiency but don't specifically address parallelization advantages of ReLU-attention
- Break condition: When sequence length becomes extremely large, the lack of global normalization might cause attention to become too dispersed

### Mechanism 3
- Claim: Multiple non-linearities work similarly well when scaled by sequence length, suggesting the specific activation function is less important than the scaling
- Mechanism: The scaling by L^-1 normalizes the output magnitude across different activation functions, making them functionally similar in terms of attention weight distribution
- Core assumption: The transformer architecture is robust to the specific choice of activation function as long as the output distribution properties are maintained
- Evidence anchors:
  - [section] "There is no clear best non-linearity at α ≈ 1, so we use ReLU in our main experiment as it is faster"
  - [section] Figure 2 shows various non-linearities (ReLU, squared ReLU, GELU, softplus, identity, ReLU6, sigmoid) performing similarly when scaled appropriately
  - [corpus] Weak evidence - corpus neighbors don't discuss non-linearity comparisons with sequence length scaling
- Break condition: If α moves significantly away from 1, different non-linearities show varying performance degradation patterns

## Foundational Learning

- Concept: Attention mechanism in transformers
  - Why needed here: Understanding how attention weights are computed and used is fundamental to grasping why replacing softmax matters
  - Quick check question: What is the key difference between how softmax and ReLU produce attention weights?

- Concept: Sequence length scaling and its effect on normalization
  - Why needed here: The paper's main contribution hinges on understanding why dividing by sequence length preserves performance
  - Quick check question: Why does dividing by L^-1 help maintain the expected attention weight distribution?

- Concept: Computational complexity analysis of attention operations
  - Why needed here: The paper claims parallelization benefits, which requires understanding the computational bottlenecks in traditional softmax attention
  - Quick check question: How does the computational complexity of softmax attention compare to ReLU-attention with sequence length scaling?

## Architecture Onboarding

- Component map: Query, Key, Value matrices -> Compute QK^T/sqrt(d) -> Apply non-linearity and L^-α scaling -> Weighted sum of values

- Critical path:
  1. Compute query-key dot products: QK^T/sqrt(d)
  2. Apply non-linearity and sequence length scaling
  3. Use resulting weights to compute weighted sum of values
  4. Pass result through feed-forward network

- Design tradeoffs:
  - Accuracy vs. computational efficiency: ReLU-attention with scaling maintains accuracy while enabling faster computation
  - Non-linearity choice: Multiple non-linearities work similarly, but ReLU is fastest
  - Scaling factor tuning: α ≈ 1 works best, but optimal value may vary by model scale

- Failure signatures:
  - Performance degradation when α deviates significantly from 1
  - Instability without qk-layernorm at larger scales (though paper shows it's not critical at tested scales)
  - Potential attention dispersion issues with very long sequences

- First 3 experiments:
  1. Replace softmax with ReLU and test with α=0, 0.5, 1, 1.5 to observe scaling effects
  2. Test different non-linearities (ReLU, GELU, softplus) with optimal α to compare performance
  3. Evaluate the effect of removing qk-layernorm on smaller models to verify its necessity at different scales

## Open Questions the Paper Calls Out

- Question: What is the exact mathematical relationship between sequence length scaling and performance that explains why L⁻¹ scaling is beneficial?
  - Basis in paper: [explicit] The paper mentions "While the central justification for sequence length scaling is empirical, we provide brief analytical motivation" but states this is only brief and leaves the question open
  - Why unresolved: The paper provides only a cursory analytical motivation and explicitly states this question is unresolved
  - What evidence would resolve it: A rigorous mathematical derivation showing why sequence length scaling improves performance, potentially through analyzing the distribution of attention weights or optimization dynamics

- Question: Can the L⁻¹ scaling factor be learned rather than fixed?
  - Basis in paper: [explicit] "In particular, we are unsure why the factor L⁻¹ improves performance or if this term could be learned"
  - Why unresolved: The authors directly state uncertainty about whether this term could be learned
  - What evidence would resolve it: Experiments showing whether learned scaling factors (either per-layer or per-head) improve performance over fixed L⁻¹ scaling, and analysis of what values these learned factors converge to

- Question: Is there an optimal non-linearity that outperforms ReLU for scaled attention mechanisms?
  - Basis in paper: [explicit] "Since there is not clear best non-linearity, we use ReLU in our main experiment as it is faster" and "it is likely that there is a better activation function that we do not explore"
  - Why unresolved: The paper tested several non-linearities but found no clear winner, and acknowledges there may be better options
  - What evidence would resolve it: Comprehensive experiments with a broader range of non-linearities, potentially including learned or data-dependent activations, to determine if any significantly outperform ReLU across model scales and tasks

## Limitations

- The mechanism for why L⁻¹ scaling works remains empirical rather than theoretically grounded
- Experiments are limited to vision transformers at specific scales, leaving uncertainty about generalization to other architectures
- The paper doesn't explore extremely long sequence lengths where attention dispersion might become problematic
- No wall-clock time or FLOPs comparisons are provided to validate claimed computational efficiency gains

## Confidence

**High Confidence**: The empirical observation that ReLU-attention with sequence length scaling (α ≈ 1) achieves comparable performance to softmax-attention across tested vision transformer scales.

**Medium Confidence**: The claim about computational efficiency gains from better parallelization, though lacking concrete performance benchmarks.

**Low Confidence**: The broader claim that this approach "presents new opportunities for parallelization" across the entire field, extrapolating beyond tested architectures.

## Next Checks

1. Evaluate ReLU-attention performance with sequence lengths significantly longer than tested (e.g., 8192+ tokens) to identify potential attention dispersion issues.

2. Test the ReLU-attention approach on language models (BERT, GPT-style) and other transformer variants to verify generalization across different architectures.

3. Systematically vary the scaling exponent α across a wider range (0.1 to 2.0) with fine-grained intervals around 1.0 to map sensitivity to this hyperparameter.