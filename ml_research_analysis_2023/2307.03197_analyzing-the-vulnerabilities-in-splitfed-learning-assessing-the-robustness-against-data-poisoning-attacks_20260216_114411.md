---
ver: rpa2
title: 'Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness
  against Data Poisoning Attacks'
arxiv_id: '2307.03197'
source_url: https://arxiv.org/abs/2307.03197
tags:
- attacks
- data
- poisoning
- learning
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces three novel data poisoning attack strategies\u2014\
  untargeted, targeted, and distance-based\u2014to evaluate the robustness of SplitFed\
  \ Learning (SFL) against malicious clients. Experiments conducted on MNIST and ECG\
  \ datasets demonstrate that untargeted attacks cause the highest accuracy degradation\
  \ (up to 71.31% drop on ECG), followed by distance-based (15.11%) and targeted attacks\
  \ (7.05%)."
---

# Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks

## Quick Facts
- arXiv ID: 2307.03197
- Source URL: https://arxiv.org/abs/2307.03197
- Reference count: 33
- This study introduces three novel data poisoning attack strategies to evaluate the robustness of SplitFed Learning (SFL) against malicious clients.

## Executive Summary
This paper evaluates the vulnerability of SplitFed Learning (SFL) to data poisoning attacks by introducing three novel attack strategies: untargeted, targeted, and distance-based. Experiments on MNIST and ECG datasets demonstrate that untargeted attacks cause the highest accuracy degradation (up to 71.31% drop on ECG), followed by distance-based (15.11%) and targeted attacks (7.05%). The attacks' effectiveness increases with a higher percentage of malicious clients and when the model split is closer to the client side. Results indicate SFL's vulnerability to data poisoning, with untargeted attacks being the most impactful. The findings underscore the need for robust defenses in distributed collaborative machine learning frameworks.

## Method Summary
The study evaluates data poisoning attacks on SplitFed Learning using three strategies: untargeted (flipping all labels to a single class), targeted (flipping labels to a specific target class), and distance-based (flipping labels to maximize Euclidean distance in feature space). Experiments use MNIST and ECG datasets with one server, ten clients (MNIST) or five clients (ECG), varying malicious client percentages (0%, 20%, 40%) and model split layers. The 1D-CNN model is used for ECG, and a feed-forward network for MNIST. Attack effectiveness is measured by accuracy drop under different conditions.

## Key Results
- Untargeted attacks cause the highest accuracy degradation (up to 71.31% drop on ECG).
- Distance-based attacks are more effective than targeted attacks (15.11% vs 7.05% accuracy drop).
- Attack effectiveness increases with higher percentages of malicious clients and when the model split is closer to the client side.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Untargeted attacks achieve the highest accuracy degradation because they flip all labels to a single class, maximizing confusion across the entire model.
- Mechanism: Malicious clients replace every label in their training data with the class that yields the highest test accuracy in the clean SFL system, creating uniform label noise.
- Core assumption: The attack can identify the optimal target class from the global model's clean performance and that all malicious clients apply this consistently.
- Evidence anchors:
  - [abstract] "untargeted attacks cause the highest accuracy degradation (up to 71.31% drop on ECG)"
  - [section] "In untargeted attacks, all the labels of malicious clients were manipulated and replaced with a class label that has the highest test accuracy in the SFL system."
  - [corpus] Weak - corpus papers discuss adversarial attacks generally but not specific untargeted label-flipping in SFL.
- Break condition: If the target class is not consistently chosen or if the fed server's aggregation downweights malicious updates, degradation will be limited.

### Mechanism 2
- Claim: Distance-based attacks are more effective than targeted attacks because they select a target class that maximizes the Euclidean distance from the source class in feature space.
- Mechanism: The attacker computes pairwise Euclidean distances between samples of the source class and all others, then replaces the source class label with the label of the most distant sample to increase misclassification likelihood.
- Core assumption: The feature space embedding preserves semantic distance such that distant samples are more likely to be misclassified as the source class.
- Evidence anchors:
  - [abstract] "distance-based attacks (15.11%)" - quantifies higher impact than targeted attacks.
  - [section] "Euclidean distance calculates the distance between input samples... the training sample with maximum distance is selected and its corresponding class is chosen as the target class Tc."
  - [corpus] Weak - no direct corpus support for distance-based poisoning in SFL; only general poisoning literature.
- Break condition: If the model's decision boundary is robust to feature-space perturbations or if distance metrics do not correlate with misclassification risk, attack impact diminishes.

### Mechanism 3
- Claim: Splitting the model closer to the client side increases attack effectiveness because more layers are vulnerable to poisoned smashed data.
- Mechanism: When the cut layer is placed after fewer server-side layers, malicious clients control more of the model's feature extraction, amplifying the influence of poisoned activations on the server's forward/backward passes.
- Core assumption: The server-side model is sensitive to input perturbations from the cut layer, and aggregation does not sufficiently mitigate these effects.
- Evidence anchors:
  - [section] "the poisoning attack on MNISTv2 and ECGv2 is more effective since these versions produce greater values of Ad" where v2 has more client-side layers.
  - [section] "there are now more layers in the client segment, giving the adversary greater room to initiate a more powerful and efficient attack."
  - [corpus] Weak - no corpus evidence on split layer positioning effects in SFL poisoning.
- Break condition: If the server-side layers are highly robust to input noise or if normalization layers dampen poisoned activations, the attack's effectiveness will be reduced.

## Foundational Learning

- Concept: Federated Learning (FL) basics
  - Why needed here: Understanding how local model updates are aggregated is essential to grasp why poisoning can occur in the server-side aggregation step.
  - Quick check question: In FL, do clients share their raw data with the server or only model updates?

- Concept: Split Learning (SL) architecture
  - Why needed here: Knowing how the model is split and how smashed data flows helps explain the attack vector in SFL.
  - Quick check question: In SL, which party computes the loss and performs backpropagation after the cut layer?

- Concept: Data poisoning fundamentals
  - Why needed here: Recognizing how label flipping and data contamination can bias model training is core to understanding the attack strategies.
  - Quick check question: What is the difference between clean-label and dirty-label poisoning?

## Architecture Onboarding

- Component map:
  - Clients: Local data, client-side model split, smashed data generation
  - Fed Server: Aggregates client-side updates via FedAvg
  - Main Server: Receives smashed data, runs server-side model, computes gradients
  - Attacker: Controls a subset of clients, poisons labels before training

- Critical path:
  1. Malicious client poisons local labels
  2. Forward pass until cut layer, smashed data sent to server
  3. Server completes forward/backward pass
  4. Gradients flow back to client, updates sent to fed server
  5. FedAvg aggregates updates, potentially incorporating poisoned gradients

- Design tradeoffs:
  - Model split placement: More client-side layers increase attack surface but may hurt accuracy; more server-side layers improve robustness but increase client compute.
  - Aggregation frequency: More frequent aggregation can dilute poisoned updates but increases communication cost.

- Failure signatures:
  - Sudden accuracy drop correlated with malicious client percentage
  - Class imbalance in predictions favoring the poisoned target class
  - High variance in updates from suspected malicious clients

- First 3 experiments:
  1. Baseline SFL training without attacks to establish clean accuracy.
  2. Introduce untargeted attacks with 20% malicious clients and measure accuracy drop.
  3. Vary model split position (client-heavy vs server-heavy) under targeted attacks to quantify split-layer impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of distance-based poisoning attacks scale with different distance metrics beyond Euclidean distance?
- Basis in paper: [inferred] The paper describes distance-based attacks using Euclidean distance but does not explore other distance metrics or compare their effectiveness.
- Why unresolved: The methodology section focuses exclusively on Euclidean distance without experimental comparison to alternative metrics.
- What evidence would resolve it: Comparative experiments measuring attack success rates using Manhattan, cosine, or Mahalanobis distances against the same datasets and model architectures.

### Open Question 2
- Question: What are the theoretical bounds on accuracy degradation for untargeted attacks as the percentage of malicious clients approaches 100%?
- Basis in paper: [explicit] The paper shows empirical results up to 40% malicious clients but does not provide theoretical analysis of attack limits.
- Why unresolved: The experimental setup only tested up to 40% malicious clients, leaving the behavior at higher percentages unexplored.
- What evidence would resolve it: Mathematical analysis or experimental results showing accuracy degradation trends and convergence behavior as malicious client percentage increases toward 100%.

### Open Question 3
- Question: How do data poisoning attacks perform against SFL models with non-standard split architectures, such as vertical splits or multi-layer client segments?
- Basis in paper: [inferred] The paper only evaluates attacks on horizontal splits at specific layers but does not explore alternative architectural configurations.
- Why unresolved: The experimental design focuses on two specific split points for each dataset without exploring architectural variations.
- What evidence would resolve it: Comparative attack success rates across various split architectures including vertical splits, multiple client segments, and asymmetric layer distributions.

## Limitations
- The exact preprocessing steps for ECG data and train/test split ratios per client are not fully specified.
- The implementation details of the distance-based attack, particularly how Euclidean distance is computed and used to select target labels, are unclear.
- The effectiveness of the attacks relies on consistent label flipping across malicious clients and the assumption that the server's aggregation mechanism does not sufficiently mitigate poisoned gradients.

## Confidence
- High confidence in the reported accuracy degradation trends under untargeted attacks, as these are directly supported by experimental results.
- Medium confidence in the comparative effectiveness of distance-based versus targeted attacks, given the quantitative results but lack of detailed attack mechanism descriptions.
- Low confidence in the generalizability of findings to other datasets or model architectures not tested in the study.

## Next Checks
1. Reproduce the experiments with detailed logging of label flipping logic and distance computation to verify the attack implementations match the described strategies.
2. Test the model split placement effect across more than two split configurations to confirm the trend of increased attack effectiveness with more client-side layers.
3. Validate the robustness of the results by introducing additional defense mechanisms (e.g., anomaly detection in client updates) and measuring their impact on attack success rates.