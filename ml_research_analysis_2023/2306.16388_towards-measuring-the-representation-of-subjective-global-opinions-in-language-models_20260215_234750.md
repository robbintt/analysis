---
ver: rpa2
title: Towards Measuring the Representation of Subjective Global Opinions in Language
  Models
arxiv_id: '2306.16388'
source_url: https://arxiv.org/abs/2306.16388
tags:
- responses
- language
- https
- prompting
- opinions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework to evaluate whether large language
  models equitably represent global opinions. The authors compile a dataset of survey
  questions from cross-national surveys (PEW and WVS) and use it to assess whose opinions
  LLM responses are most similar to.
---

# Towards Measuring the Representation of Subjective Global Opinions in Language Models

## Quick Facts
- arXiv ID: 2306.16388
- Source URL: https://arxiv.org/abs/2306.16388
- Reference count: 40
- Key outcome: This paper introduces a framework to evaluate whether large language models equitably represent global opinions. The authors compile a dataset of survey questions from cross-national surveys (PEW and WVS) and use it to assess whose opinions LLM responses are most similar to. They measure similarity between model responses and human responses across countries using Jensen-Shannon distance. Three experiments are run: 1) default prompting where models simply answer survey questions, 2) cross-national prompting where models are asked to respond as someone from a particular country, and 3) linguistic prompting where questions are translated into target languages. Results show that by default, LLM responses are most similar to opinions from the USA, Canada, Australia and some European/South American countries, indicating potential biases. Cross-national prompting can shift responses to be more similar to the prompted country's opinions, but sometimes reflects harmful cultural stereotypes. Linguistic prompting does not necessarily make responses more similar to the opinions of speakers of those languages. The authors release their dataset for others to use.

## Executive Summary
This paper presents a framework to evaluate whether large language models (LLMs) equitably represent global opinions by measuring distributional similarity between model outputs and human responses from cross-national surveys. The authors compile a dataset of 2,556 survey questions from Pew Global Attitudes Survey and World Values Survey, then assess whose opinions LLM responses are most similar to using Jensen-Shannon distance. Through three experimental conditions (default prompting, cross-national prompting, and linguistic prompting), they find that by default, LLM responses align most closely with opinions from Western countries, particularly the USA, Canada, and Australia. While cross-national prompting can shift responses toward prompted country opinions, it sometimes produces harmful cultural stereotypes. Linguistic prompting alone doesn't necessarily make responses align with speakers of target languages, revealing limitations of language-based cultural adaptation.

## Method Summary
The authors compile a dataset called GlobalOpinionQA from cross-national surveys (Pew Global Attitudes Survey and World Values Survey), containing 2,556 multiple-choice questions. They evaluate three decoder-only transformer LLMs fine-tuned with RLHF and Constitutional AI. The framework measures similarity between LLM-generated survey responses and human responses using Jensen-Shannon distance, conditioned on country. Three experimental conditions are tested: 1) Default Prompting where models simply answer survey questions, 2) Cross-national Prompting where models are prompted to respond as someone from a particular country, and 3) Linguistic Prompting where questions are translated into target languages (Russian, Turkish, Chinese). The authors release their dataset for others to use.

## Key Results
- By default, LLM responses are most similar to opinions from the USA, Canada, Australia, and some European/South American countries, indicating potential biases toward WEIRD populations
- Cross-national prompting can shift responses to be more similar to the prompted country's opinions, but sometimes reflects harmful cultural stereotypes rather than deep understanding
- Linguistic prompting (translating questions into Russian, Turkish, and Chinese) does not necessarily make responses more similar to the opinions of speakers of those languages
- The framework successfully reveals systematic biases in how LLMs represent global opinions through distributional comparison

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework reveals LLM bias toward WEIRD populations by measuring distributional similarity between model outputs and survey responses from different countries.
- Mechanism: By computing Jensen-Shannon distance between the probability distributions of LLM responses and human responses conditioned on country, the framework quantifies which countries' opinions the model's outputs align with most closely. This distributional comparison directly exposes systematic preference for certain demographic groups.
- Core assumption: Averaging human responses within each country provides a valid representation of that country's collective opinion, despite within-country variance.
- Evidence anchors:
  - [abstract] "We then define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country."
  - [section] "Compute the similarity (Smc) between a model m ∈ M and a country c ∈ C across the questions q ∈ Q, given a similarity metric Sim"
  - [corpus] Weak - no direct evidence about distributional comparison methodology in corpus.

### Mechanism 2
- Claim: Cross-national prompting steers LLM responses toward prompted country's opinions, but reveals superficial cultural associations rather than deep understanding.
- Mechanism: Prepending country-specific prompts ("How would someone from [country X] respond...") manipulates the LLM's output distribution to align more closely with that country's survey response distribution, demonstrating the model's responsiveness to contextual cues while exposing potential stereotyping.
- Core assumption: The LLM's response distribution is sufficiently malleable to be steered by simple prompt modifications without requiring fundamental model retraining.
- Evidence anchors:
  - [abstract] "When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural stereotypes."
  - [section] "With Cross-national Prompting (CP), model responses appear to become most similar to the opinion distributions of the prompted countries"
  - [corpus] Moderate - evidence of prompt sensitivity and cultural association analysis.

### Mechanism 3
- Claim: Linguistic prompting alone is insufficient to make LLM responses align with speakers of target languages, revealing limitations of language-based cultural adaptation.
- Mechanism: Translating survey questions into target languages (Russian, Chinese, Turkish) and measuring response similarity shows that linguistic context alone doesn't overcome training data biases, as model outputs remain most similar to Western country responses despite language changes.
- Core assumption: The LLM's language-specific knowledge and associations are primarily shaped by training data composition rather than translation capabilities.
- Evidence anchors:
  - [abstract] "When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily become the most similar to the opinions of speakers of those languages."
  - [section] "With Linguistic Prompting (LP), model responses do not become more similar to the opinions of the populations that predominantly speak the target languages."
  - [corpus] Moderate - evidence of translation experiments and cross-lingual comparison.

## Foundational Learning

- Concept: Jensen-Shannon distance for measuring distributional similarity
  - Why needed here: The framework relies on comparing probability distributions between LLM outputs and human survey responses across multiple countries
  - Quick check question: Can you compute JS distance between two discrete probability distributions and interpret what values near 0 or 1 mean?

- Concept: Cross-national survey methodology and cultural representation
  - Why needed here: Understanding how surveys like PEW and WVS capture global opinions is crucial for interpreting the framework's results and limitations
  - Quick check question: What are the key challenges in ensuring cross-national survey comparability across different cultures and languages?

- Concept: Large language model alignment and bias mechanisms
  - Why needed here: The framework's findings about LLM biases toward certain populations require understanding how training data, RLHF, and CAI influence model outputs
  - Quick check question: How might the composition of pre-training data and human feedback data influence which populations an LLM's outputs align with most closely?

## Architecture Onboarding

- Component map: Dataset compilation → Prompt administration → Response collection → Similarity computation → Result visualization
- Critical path: Question selection and translation → Model response generation → Distribution comparison → Interpretation of similarity scores
- Design tradeoffs: Multiple-choice format simplifies objective scoring but limits nuance; averaging within countries simplifies analysis but masks diversity
- Failure signatures: High confidence in single responses despite diverse human opinions; stereotypical responses to cross-national prompts; persistent similarity to Western countries despite linguistic prompting
- First 3 experiments:
  1. Default Prompting: Administer original survey questions without modifications
  2. Cross-national Prompting: Add country-specific context to prompt responses
  3. Linguistic Prompting: Translate questions into target languages and measure response changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different amounts of RLHF training affect the similarity of LLM responses to global opinions?
- Basis in paper: Explicit - The paper states they examined the influence of RLHF training amount but found no strong effects.
- Why unresolved: The paper only briefly mentions this finding without providing detailed analysis or explanation for why RLHF amount didn't strongly affect results.
- What evidence would resolve it: A systematic analysis comparing models with different RLHF training amounts to see if and how they affect similarity to global opinions.

### Open Question 2
- Question: Do linguistic prompts in languages other than Russian, Turkish, and Chinese lead to LLM responses that better reflect opinions of speakers of those languages?
- Basis in paper: Explicit - The paper only tested 3 target languages (Russian, Turkish, Chinese) and found linguistic prompting didn't necessarily make responses more similar to opinions of speakers of those languages.
- Why unresolved: The paper only tested a limited set of languages. Testing more languages could reveal whether this finding generalizes.
- What evidence would resolve it: Testing linguistic prompting in a wider variety of languages and measuring similarity to opinions of speakers of those languages.

### Open Question 3
- Question: What specific aspects of training data and fine-tuning (beyond language and geography) contribute to LLM biases in representing global opinions?
- Basis in paper: Explicit - The paper discusses that the model is trained primarily on English data and English human feedback, but acknowledges this is likely insufficient to explain the full extent of biases.
- Why unresolved: The paper identifies some factors (English data, RLHF) but doesn't deeply analyze what other factors may be at play.
- What evidence would resolve it: A detailed analysis of the model's training data and fine-tuning process to identify additional sources of bias beyond language and geography.

## Limitations

- Dataset compilation relies on existing cross-national surveys (PEW and WVS) which may have inherent sampling biases and cultural translation issues
- Jensen-Shannon distance assumes averaging human responses within countries provides valid representation of collective opinion, masking significant within-country variance
- Translation component introduces uncertainty as translation quality and cultural context may significantly affect question interpretation by both humans and models

## Confidence

- **High confidence**: The framework's basic methodology for comparing model responses to survey data is sound and reproducible
- **Medium confidence**: The findings about default bias toward Western countries are robust, but the extent of this bias may be underestimated due to dataset limitations
- **Low confidence**: The conclusions about linguistic prompting's ineffectiveness may be premature without controlling for translation quality and cultural context

## Next Checks

1. **Replicate with diverse LLM architectures**: Test the framework across multiple LLM architectures (different base models, varying sizes) to determine if the observed biases are consistent or model-specific.

2. **Validate translation methodology**: Conduct human evaluation studies comparing original and translated survey questions to assess translation quality and cultural equivalence before using them in model evaluations.

3. **Analyze within-country variance**: Instead of aggregating responses at the country level, examine how model similarity varies across different demographic subgroups within countries to better understand the framework's sensitivity to opinion diversity.