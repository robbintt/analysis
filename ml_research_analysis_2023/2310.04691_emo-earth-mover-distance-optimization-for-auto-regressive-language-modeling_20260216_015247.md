---
ver: rpa2
title: 'EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling'
arxiv_id: '2310.04691'
source_url: https://arxiv.org/abs/2310.04691
tags:
- language
- training
- distribution
- distance
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EMO (Earth Mover Distance Optimization),
  a new training objective for auto-regressive language models. EMO addresses limitations
  of traditional maximum likelihood estimation (MLE), which is recall-prioritized,
  ignores negative diversity, and has train-test mismatch.
---

# EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling

## Quick Facts
- arXiv ID: 2310.04691
- Source URL: https://arxiv.org/abs/2310.04691
- Reference count: 24
- EMO consistently outperforms MLE and strong baselines, generating text with significantly higher distributional closeness to human text (6.2 points on average measured by MAUVE)

## Executive Summary
This paper introduces EMO (Earth Mover Distance Optimization), a new training objective for auto-regressive language models that addresses limitations of traditional maximum likelihood estimation. EMO leverages the Earth Mover Distance (EMD) to provide more balanced training signals that consider both precision and recall while being aware of negative diversity. Due to computational complexity, the authors propose a tractable upper bound of EMD that allows end-to-end training. Experiments on language modeling tasks show that EMO consistently outperforms MLE and strong baselines, generating text with significantly higher distributional closeness to human text. Furthermore, EMO demonstrates noteworthy improvements in downstream performance with minimal fine-tuning on just 25,000 sentences, highlighting its potential as a lightweight calibration method for large-scale pre-trained language models.

## Method Summary
EMO introduces a novel training objective based on Earth Mover Distance optimization for auto-regressive language models. The method uses a differentiable upper bound of EMD computed via a transport plan that leverages the data distribution, making it computationally tractable for end-to-end training. The approach replaces traditional cross-entropy with a transport cost based on semantic similarity in contextual embedding space. The method is evaluated by fine-tuning pre-trained LLaMa-7B and LLaMa-13B models on WikiText-103 and measuring performance on downstream tasks using in-context few-shot learning with 8 retrieved demonstrations per task.

## Key Results
- EMO generates text with significantly higher distributional closeness to human text (6.2 points on average measured by MAUVE)
- EMO consistently outperforms MLE and strong baselines across language modeling tasks
- EMO demonstrates noteworthy improvements in downstream performance with minimal fine-tuning on just 25,000 sentences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EMO balances precision and recall by using a transport cost that considers both high-probability tokens and their semantic similarity.
- Mechanism: The differentiable earth mover distance (DEMD) computes a weighted sum of cosine distances between contextual embeddings, where weights come from the model distribution. This encourages tokens similar to ground truth to be upweighted while dissimilar tokens are penalized.
- Core assumption: Semantic similarity via cosine distance in embedding space is a meaningful proxy for token interchangeability in context.
- Evidence anchors: [abstract] "it takes into account both precision and recall during modeling"; [section] "tokens that are more likely to be used interchangeably should have smaller distances"; [corpus] "Found 25 related papers... Average neighbor FMR=0.465" - moderate relatedness to EMD in NLP suggests the embedding-based transport cost is novel here

### Mechanism 2
- Claim: EMO's upper bound is computationally tractable and preserves gradient flow for end-to-end training.
- Mechanism: Instead of solving a linear program for exact EMD, the paper uses a data-dependent transport plan that yields an upper bound expressible as a differentiable matrix product.
- Core assumption: The proposed transport plan is a valid joint distribution that upper bounds the true EMD.
- Evidence anchors: [section] "We start by defining a transport plan ˜γ that directly leverages the data distribution... Both Qθ and P add up to 1, ˜γ is therefore a legitimate but not necessarily optimal plan"; [section] "EMD(Qθ, P) ≤ Q⊤ θ CP" - formal inequality proof

### Mechanism 3
- Claim: EMO improves train-test consistency by aligning the expectation under the model distribution with the training objective.
- Mechanism: The DEMD objective contains an expectation over Qθ, unlike MLE's expectation over P, so sampling from Qθ during evaluation better matches training.
- Core assumption: Better train-test alignment leads to better generalization and more human-like generations.
- Evidence anchors: [section] "From Eq.12 we can see that DEMD explicitly involves the optimization of the expected transport cost computed with respect to Qθ"; [section] "DEMD has a higher degree of train-test consistency compared to MLE"

## Foundational Learning

- Concept: Optimal transport theory and the Kantorovich duality
  - Why needed here: Understanding EMD and its computation via linear programming is essential to grasp why the upper bound is valid and efficient
  - Quick check question: Why does the Kantorovich-Rubinstein duality allow converting the primal EMD problem into a dual form, and what does that mean for optimization?

- Concept: Cross-entropy decomposition into token-level factors
  - Why needed here: MLE loss is decomposed across time steps, and EMO builds on this structure to replace the per-step cross-entropy with DEMD
  - Quick check question: How does decomposing the sentence-level cross-entropy into a sum of token-level cross-entropies simplify the training objective?

- Concept: Contextual embeddings and their use in measuring semantic similarity
  - Why needed here: The transport cost is defined via cosine similarity in the contextual embedding space, so understanding how these embeddings are learned and interpreted is crucial
  - Quick check question: How do contextual embeddings differ from static word embeddings, and why is cosine distance a reasonable proxy for semantic distance here?

## Architecture Onboarding

- Component map: Language model Qθ -> Pre-trained LM Qϕ (for fixed embedding matrix E) -> DEMD loss module -> Training loop
- Critical path: 1. Forward pass: Qθ produces token probabilities; 2. Compute transport cost matrix C from E; 3. Calculate DEMD upper bound using Qθ and P; 4. Backpropagate gradients to Qθ only
- Design tradeoffs: Using fixed E from pre-trained Qϕ ensures stable transport costs but limits adaptability to new domains; Upper bound is looser than exact EMD but enables end-to-end training; No hyperparameter tuning needed for EMO, but mixing with MLE loss may be necessary for stability
- Failure signatures: Training loss diverges: transport cost matrix C may be too noisy or gradients too large; Mauve score plateaus: upper bound too loose, model not learning meaningful distribution shift; Downstream task performance drops: fine-tuning altered pre-trained LM too much, consider lower learning rate
- First 3 experiments: 1. Verify DEMD upper bound by comparing against exact EMD on small vocabulary toy dataset; 2. Ablation: train with and without mixing DEMD and MLE to find optimal coefficient; 3. Scale test: run on WikiText-2 with GPT-2-small, measure training stability and generation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EMO's performance scale with even larger language models beyond LLaMa-13B, such as GPT-3 or GPT-4?
- Basis in paper: [explicit] The paper discusses scaling laws with LLaMa-7B and LLaMa-13B, but does not explore models beyond this size.
- Why unresolved: The paper focuses on evaluating EMO with LLaMa-7B and LLaMa-13B, leaving open the question of its effectiveness on even larger models.
- What evidence would resolve it: Empirical results showing EMO's performance on larger language models like GPT-3 or GPT-4, compared to other training objectives.

### Open Question 2
- Question: Can EMO be effectively combined with other fine-tuning methods, such as prompt tuning or adapter-based approaches?
- Basis in paper: [inferred] The paper primarily focuses on EMO as a standalone training objective, without exploring combinations with other fine-tuning methods.
- Why unresolved: The paper does not investigate how EMO interacts with or complements other fine-tuning techniques, leaving open the possibility of synergistic effects.
- What evidence would resolve it: Experimental results comparing EMO's performance when combined with other fine-tuning methods versus using it alone.

### Open Question 3
- Question: How does EMO's performance vary across different types of downstream tasks, such as text generation, summarization, or question answering?
- Basis in paper: [explicit] The paper evaluates EMO on a range of natural language understanding tasks, but does not explore its effectiveness on text generation or other specific tasks.
- Why unresolved: The paper's evaluation is limited to natural language understanding tasks, leaving open the question of EMO's effectiveness on other types of tasks.
- What evidence would resolve it: Experimental results showing EMO's performance on various downstream tasks, including text generation, summarization, and question answering, compared to other training objectives.

## Limitations
- Computational tractability of DEMD upper bound remains unverified for large vocabularies
- Semantic similarity assumption may not hold across all domains and contexts
- Fine-tuning contribution not isolated from EMO's direct effects

## Confidence

**High Confidence** claims:
- EMO provides a valid upper bound on the Earth Mover Distance that is differentiable and can be incorporated into training
- The training procedure (using AdamW, specific learning rates, and batch sizes) is clearly specified and reproducible
- MAUVE scores consistently show EMO outperforming MLE baselines across multiple language modeling tasks

**Medium Confidence** claims:
- The 6.2-point MAUVE improvement translates to meaningfully better text quality, as MAUVE correlation with human judgments has limitations
- The train-test consistency argument is theoretically sound, but empirical validation specifically showing reduced distribution shift during inference is limited
- Downstream task improvements are demonstrated but could be influenced by factors beyond EMO (fine-tuning protocol, demonstration selection)

**Low Confidence** claims:
- EMO works as a "lightweight calibration method" for large-scale models without extensive hyperparameter tuning - no ablation on mixing coefficients or learning rate sensitivity is provided
- The semantic transport cost accurately captures token interchangeability across all domains - this is assumed but not empirically validated
- The computational efficiency claim relative to exact EMD solving - runtime comparisons are not provided

## Next Checks

1. **Tractability verification**: Implement the DEMD upper bound on a small-scale toy problem with known EMD solution to verify computational correctness and numerical stability. Compare runtime and memory usage against exact EMD solvers to confirm the claimed efficiency gains.

2. **Semantic transport cost validation**: Design an experiment where token similarity judgments are collected from human annotators, then compare these to the cosine distances in the embedding space used by EMO. This would validate whether the transport cost matrix actually captures meaningful semantic relationships.

3. **Fine-tuning contribution isolation**: Run controlled ablations comparing EMO with standard MLE fine-tuning on the same datasets, holding all other variables constant (learning rate, batch size, number of updates). This would quantify how much of the downstream improvement is attributable to EMO versus the fine-tuning process itself.