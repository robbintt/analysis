---
ver: rpa2
title: On-sensor Printed Machine Learning Classification via Bespoke ADC and Decision
  Tree Co-Design
arxiv_id: '2312.01172'
source_url: https://arxiv.org/abs/2312.01172
tags:
- printed
- decision
- power
- unary
- area
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of enabling self-powered digital
  machine learning classification in printed electronics, a domain where large feature
  sizes and limited integration density make complex circuits like machine learning
  classifiers difficult to realize, especially when sensor input processing is required.
  The primary bottleneck is the costly analog-to-digital converters (ADCs) necessary
  for processing sensor data.
---

# On-sensor Printed Machine Learning Classification via Bespoke ADC and Decision Tree Co-Design

## Quick Facts
- arXiv ID: 2312.01172
- Source URL: https://arxiv.org/abs/2312.01172
- Reference count: 18
- This work enables self-powered on-sensor printed ML classifiers for the first time, achieving 8.6x area and 12.2x power reductions.

## Executive Summary
This paper addresses the challenge of implementing machine learning classifiers in printed electronics, where large feature sizes and limited integration density make complex circuits difficult to realize. The key innovation is a co-design framework that integrates bespoke analog-to-digital converters (ADCs) tailored for unary-based Decision Tree classifiers. By designing fully customized ADCs that retain only minimum necessary comparators and leveraging an ADC-aware Decision Tree training approach, the framework achieves significant hardware overhead reduction while maintaining high accuracy. The proposed framework enables self-powered on-sensor printed ML classifiers for the first time, with average 8.6x and 12.2x reductions in area and power respectively.

## Method Summary
The proposed method involves a co-design framework that integrates bespoke ADCs with unary-based Decision Tree classifiers. The bespoke ADCs are designed to retain only the minimum necessary comparators based on the specific unary digits needed by the Decision Tree. An ADC-aware Decision Tree training approach is used to minimize hardware overhead by selecting split parameters that reuse existing ADC outputs. The Decision Tree comparisons are represented in unary format, simplifying them to single-bit checks and eliminating the need for comparators. The framework is evaluated on eight datasets from the UCI ML repository, exploring hyperparameters (τ and depth) for optimal hardware efficiency.

## Key Results
- Achieves 8.6x and 12.2x reductions in area and power, respectively, compared to the state-of-the-art
- Maintains high classification accuracy with up to 1% accuracy loss
- Enables self-powered on-sensor printed ML classifiers for the first time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retaining only the bare-minimum comparators in bespoke ADCs reduces area and power compared to conventional Flash ADCs.
- Mechanism: Conventional Flash ADCs use 2^N - 1 comparators for N-bit resolution. The proposed design removes unused comparators based on the specific unary digits needed by the Decision Tree, thus minimizing hardware.
- Core assumption: The Decision Tree parameters can be represented with fewer than the full set of unary digits, and unused comparators do not contribute to classification accuracy.
- Evidence anchors:
  - [abstract] "retain only the minimum necessary comparators"
  - [section] "By simply removing the encoder, we not only decrease the ADC’s hardware requirements but also achieve our objective"
  - [corpus] Weak support; no direct citations found.
- Break condition: If Decision Tree parameters require more distinct unary digits than anticipated, the area/power savings diminish or vanish.

### Mechanism 2
- Claim: ADC-aware Decision Tree training minimizes the number of comparators required by selecting split parameters that reuse existing ADC outputs.
- Mechanism: During training, candidate splits are grouped into zero-cost (already present), medium-cost (new comparator in existing ADC), and high-cost (new ADC). The algorithm selects splits in order of cost, favoring zero-cost first, then medium-cost with minimal power impact, and finally high-cost only if necessary.
- Core assumption: The Gini score is sufficiently correlated with classification accuracy that allowing small Gini increases (via τ threshold) can reduce hardware cost without large accuracy loss.
- Evidence anchors:
  - [abstract] "propose and implement an ADC-aware Decision Tree training"
  - [section] "Our ADC-aware training essentially trains a Decision Tree using the Gini index [16] cost function"
  - [corpus] No direct support; methodology appears novel.
- Break condition: If the Gini-to-accuracy mapping breaks down for complex datasets, hardware savings may not justify accuracy loss.

### Mechanism 3
- Claim: Representing Decision Tree comparisons in unary format reduces them to simple bit checks, eliminating the need for comparators entirely.
- Mechanism: In unary encoding, a threshold comparison I >= C becomes a single bit check I[k] == 1, where k is the position of the most significant '1' in C's unary representation.
- Core assumption: Inputs can be efficiently converted to parallel unary format without excessive overhead, and the Decision Tree depth is shallow enough that the unary encoding remains compact.
- Evidence anchors:
  - [abstract] "leveraging an ADC-aware Decision Tree training approach to minimize hardware overhead"
  - [section] "if the inputs are provided in a parallel unary format, all Tree comparators can be removed"
  - [corpus] Weak support; unary computing is discussed in related works but not in the same context.
- Break condition: If input precision increases, the unary encoding length grows exponentially, negating area savings.

## Foundational Learning

- Concept: Unary number representation and thermometer code.
  - Why needed here: The proposed Decision Tree implementation relies on unary format to simplify comparisons to single-bit checks.
  - Quick check question: How many bits are needed to represent the decimal number 5 in unary? (Answer: 9 bits, 000011111U)

- Concept: Flash ADC architecture and comparator arrays.
  - Why needed here: Understanding why conventional ADCs are expensive in PE and how bespoke ADCs reduce this cost.
  - Quick check question: For a 4-bit Flash ADC, how many comparators are required? (Answer: 15)

- Concept: Decision Tree training using Gini impurity.
  - Why needed here: The ADC-aware training modifies Gini-based split selection to consider hardware cost.
  - Quick check question: What is the formula for Gini impurity of a node with class probabilities p1, p2, ..., pn? (Answer: 1 - sum(pi^2))

## Architecture Onboarding

- Component map: Sensor input -> bespoke ADC -> parallel unary digits -> simplified Decision Tree logic (AND-OR gates) -> classification output
- Critical path: Sensor sampling -> ADC conversion -> unary bit extraction -> Decision Tree evaluation
- Design tradeoffs:
  - Higher precision inputs -> more unary digits -> larger ADCs but potentially better accuracy
  - Deeper Decision Trees -> more gates but better accuracy
  - Larger τ in training -> more hardware savings but potential accuracy loss
- Failure signatures:
  - Missing unary digits in ADC output -> incorrect classification
  - Comparator mismatch between trained parameters and actual ADC outputs -> logic errors
  - Insufficient τ exploration -> suboptimal hardware efficiency
- First 3 experiments:
  1. Implement a basic 4-bit bespoke ADC with 2-4 output digits and verify unary conversion against ground truth
  2. Train a simple Decision Tree with ADC-aware method on a small dataset (e.g., Balance-Scale) and compare accuracy/hardware vs conventional
  3. Cascade ADC and simplified Decision Tree on FPGA or simulation to measure latency and power consumption

## Open Questions the Paper Calls Out

The paper does not explicitly call out any open questions. However, it leaves room for further exploration in several areas, such as applying the proposed framework to more complex ML models beyond Decision Trees, evaluating the impact of different printed electronics technologies on the framework's effectiveness, and assessing the performance on more complex datasets with higher dimensionality and more classes.

## Limitations

- Dependency on unary number representation, which scales poorly with input precision (exponential growth in required digits/bits)
- Assumes a monotonic relationship between Gini impurity and classification accuracy, which may not hold for all datasets or complex decision boundaries
- Does not validate performance at higher resolutions (8-bit and 12-bit input precision)

## Confidence

- **High confidence**: The area and power reduction mechanisms (minimal comparator retention, unary simplification) are theoretically sound and directly supported by circuit analysis in the paper
- **Medium confidence**: The ADC-aware training methodology is novel and the described cost function modification is plausible, but lacks extensive validation across diverse datasets
- **Low confidence**: The claim of "first self-powered on-sensor printed ML classifiers" requires verification against all prior PE ML implementations, which the paper does not comprehensively address

## Next Checks

1. **Precision scalability test**: Implement the framework with 8-bit and 12-bit input precision to quantify the unary digit explosion and assess practical limits

2. **Cross-dataset robustness**: Apply the ADC-aware training to datasets outside the UCI repository (e.g., image or time-series data) to test generalizability of the Gini-to-hardware cost mapping

3. **Comparative baseline implementation**: Build the baseline bespoke Decision Tree implementation [2] and approximate Decision Tree [7] using identical PDK and simulation setup to verify the claimed 8.6x area and 12.2x power improvements