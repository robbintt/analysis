---
ver: rpa2
title: Neural Architecture Retrieval
arxiv_id: '2307.07919'
source_url: https://arxiv.org/abs/2307.07919
tags:
- neural
- graph
- architectures
- motifs
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper defines the Neural Architecture Retrieval (NAR) problem
  to automatically discover similar neural architectures, which is challenging due
  to the varied graph sizes and the presence of motifs in computational graphs. The
  authors propose a novel graph representation learning framework that addresses these
  issues by splitting graphs into motifs, rebuilding a reduced-size macro graph, and
  applying multi-level contrastive learning (motifs-level and graph-level) to learn
  accurate embeddings.
---

# Neural Architecture Retrieval

## Quick Facts
- arXiv ID: 2307.07919
- Source URL: https://arxiv.org/abs/2307.07919
- Reference count: 40
- Key outcome: Introduces a motif-based graph representation learning framework for neural architecture retrieval, achieving significant improvements over GCN/GAT baselines on real-world and NAS datasets.

## Executive Summary
This paper tackles the challenging problem of Neural Architecture Retrieval (NAR) by proposing a novel graph representation learning framework that addresses the dual challenges of varied graph sizes and motif presence in computational graphs. The method splits neural architectures into motifs, rebuilds a reduced-size macro graph, and applies multi-level contrastive learning (motifs-level and graph-level) to learn accurate embeddings. Evaluated on 12k real-world architectures and 30k NAS-synthesized architectures, the approach significantly outperforms existing GNN-based methods in retrieval metrics like MRR, MAP, and NDCG.

## Method Summary
The method addresses NAR by first converting neural architecture graphs into motif sequences through iterative neighbor encoding (up to s steps), then identifying frequent subsequences as motifs. A GCN-based network (Fs) embeds these motifs, and a macro graph is constructed by replacing motifs with their embeddings. Another GCN (Fm) embeds the macro graph. Multi-level contrastive learning is applied: motifs-level contrastive learning maximizes similarity between motif embeddings and their context graphs, while graph-level contrastive learning clusters embeddings of architectures from the same model family. An optional classification head provides additional supervision.

## Key Results
- Achieves MRR of 0.8587 and 0.8765 on NAS dataset, significantly outperforming GCN (0.4987) and GAT (0.5713) baselines
- Demonstrates strong generalization to real-world architectures (12,517 models) with consistent performance improvements
- Releases a dataset of 12k real-world neural architectures with pre-computed embeddings for community use

## Why This Works (Mechanism)

### Mechanism 1: Motif-based Macro Graph Construction
Splitting neural architecture graphs into motifs and rebuilding a macro graph addresses the dual challenges of varied graph sizes and motif presence. By encoding neighbors iteratively (up to s steps) and identifying frequent subsequences as motifs, the method transforms large, irregular graphs into smaller macro graphs where each node represents a motif embedding. This reduces computational burden and explicitly models motif-level similarity. Break condition: If motif sampling fails to capture meaningful architectural patterns, the macro graph will lose discriminative power and retrieval performance will degrade.

### Mechanism 2: Multi-level Contrastive Learning
Multi-level contrastive learning (motifs-level and graph-level) enables accurate and generalizable graph embeddings. Motifs-level contrastive learning maximizes similarity between motif embeddings and their context graphs while minimizing similarity to negative samples. Graph-level contrastive learning clusters embeddings of architectures from the same model family and separates those from different families. This two-stage approach ensures both fine-grained and coarse-grained architectural similarity are captured. Break condition: If the positive/negative sampling strategy is not representative, the contrastive loss may not enforce meaningful separation, leading to poor retrieval accuracy.

### Mechanism 3: Iterative Neighbor Encoding for Receptive Field Expansion
Iterative neighbor encoding expands the receptive field of motifs, enabling capture of larger architectural patterns. By repeatedly encoding node neighbors up to s steps, the method expands the motif's context from immediate neighbors to higher-order structural patterns. This allows motifs to represent not just small subgraphs but meaningful architectural blocks like residual connections or attention modules. Break condition: If s is too small, motifs will be too local and miss important architectural patterns; if s is too large, motifs may overgeneralize and lose specificity, hurting retrieval precision.

## Foundational Learning

- **Graph Neural Networks (GNNs)**: Neural architectures are naturally represented as computational graphs, and GNNs are the standard tool for learning node and graph embeddings from such structures. Quick check: What is the key difference between spectral-based (GCN) and spatial-based (GAT) GNNs, and when might one be preferred over the other?

- **Contrastive Learning**: The paper uses contrastive objectives to learn embeddings without explicit labels, leveraging positive pairs (similar architectures) and negative pairs (dissimilar architectures) to shape the embedding space for retrieval. Quick check: In contrastive learning, what is the role of the temperature parameter in the InfoNCE loss, and how does it affect the hardness of negative samples?

- **Motif Discovery in Graph Mining**: Motifs are recurring, significant subgraphs in neural architectures (e.g., residual blocks, attention modules). Identifying and embedding them is essential for capturing architectural similarity beyond raw graph topology. Quick check: What is the computational complexity of brute-force subgraph isomorphism counting, and why is approximate or sampling-based motif discovery preferred in large graphs?

## Architecture Onboarding

- **Component map**: Input Graph -> Motif Sampling Module -> Motif Embedding Network (Fs) -> Macro Graph Construction -> Graph Embedding Network (Fm) -> Contrastive Losses -> Output Embeddings

- **Critical path**:
  1. Input: Computational graph of neural architecture
  2. Motif Sampling: Encode neighbors iteratively, discover motifs
  3. Motif Embedding: Pass motifs through Fs to get Hsg
  4. Macro Graph Construction: Replace motifs with Hsg to form Gm
  5. Graph Embedding: Pass Gm through Fm to get Hm
  6. Loss Computation: Calculate Ls (motifs-level) and Lm (graph-level)
  7. Optimization: Update Fs, Fc, and Fm parameters
  8. Inference: Use Fm and Fs to embed new architectures for retrieval

- **Design tradeoffs**:
  - Motif Size vs. Granularity: Smaller motifs capture fine details but may miss higher-level patterns; larger motifs capture blocks but risk overgeneralization
  - Receptive Field Depth (s) vs. Computational Cost: Deeper encoding captures broader context but increases motif discovery and embedding time
  - Contrastive Sampling Strategy: Careful selection of positive/negative pairs is crucial; poor sampling can lead to misleading gradients and poor retrieval
  - Macro Graph Size vs. Embedding Quality: Smaller macro graphs reduce computation but may lose structural information; larger ones preserve detail but are costlier

- **Failure signatures**:
  - Retrieval performance plateaus or degrades as dataset size grows: Likely motif sampling is not scalable or contrastive sampling is insufficient
  - Embeddings cluster poorly in visualization: Possibly motif encoding is too shallow or macro graph construction loses critical connectivity
  - Training instability or slow convergence: Contrastive loss temperature or negative sampling ratio may need tuning
  - High variance in retrieval metrics across runs: Likely insufficient motif diversity or unstable motif sampling

- **First 3 experiments**:
  1. **Sanity Check Retrieval**: Run the full pipeline on a small, synthetic dataset of known architectures (e.g., ResNet variants, VGG variants). Verify that architectures from the same family cluster together and retrieval MRR > 0.8.
  2. **Motif Sampling Ablation**: Compare retrieval performance with different values of s (receptive field depth) and motif size limits. Confirm that increasing s improves retrieval up to a point, then plateaus or degrades.
  3. **Contrastive Loss Ablation**: Train models with only motifs-level CL, only graph-level CL, and both. Verify that both levels contribute to performance and that removing either significantly drops retrieval metrics.

## Open Questions the Paper Calls Out
No specific open questions are called out in the paper.

## Limitations
- The motif sampling algorithm (especially the ENM/ENC functions) lacks sufficient detail for faithful reproduction, and the impact of hyperparameter choices on retrieval performance is not fully characterized.
- The method's scalability to extremely large neural architectures with millions of parameters and operations is not discussed.
- The effectiveness of the method on neural architectures with different input and output modalities (e.g., text, audio, multimodal) is not explored.

## Confidence
- **High Confidence**: The overall retrieval framework and its evaluation on real-world and NAS datasets are well-documented and reproducible.
- **Medium Confidence**: The multi-level contrastive learning mechanism is theoretically sound, but its effectiveness depends on the quality of motif sampling and macro graph construction.
- **Low Confidence**: The motif sampling algorithm (especially the ENM/ENC functions) lacks sufficient detail for faithful reproduction, and the impact of hyperparameter choices on retrieval performance is not fully characterized.

## Next Checks
1. **Motif Sampling Robustness**: Test the motif sampling strategy with varying values of s and motif size limits on a diverse set of architectures (e.g., CNNs, Transformers, GNNs) to identify optimal hyperparameters and assess sensitivity.
2. **Macro Graph Reconstruction Ablation**: Compare NAR's macro graph construction method against alternative graph coarsening techniques (e.g., spectral clustering, edge contraction) to quantify the contribution of motif-based reconstruction to retrieval performance.
3. **Contrastive Sampling Strategy Analysis**: Evaluate the impact of different positive/negative sampling strategies on contrastive learning effectiveness, including varying the number of negative samples and exploring hard negative mining techniques.