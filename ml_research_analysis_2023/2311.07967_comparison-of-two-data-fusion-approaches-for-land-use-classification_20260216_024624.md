---
ver: rpa2
title: Comparison of two data fusion approaches for land use classification
arxiv_id: '2311.07967'
source_url: https://arxiv.org/abs/2311.07967
tags:
- land
- sources
- data
- information
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares two data fusion approaches for land use classification:
  pre-classification and post-classification fusion. The pre-classification approach
  uses a machine learning model to predict land use classes from all sources simultaneously,
  while the post-classification approach uses the Dempster-Shafer Theory to merge
  predictions from individual sources.'
---

# Comparison of two data fusion approaches for land use classification

## Quick Facts
- arXiv ID: 2311.07967
- Source URL: https://arxiv.org/abs/2311.07967
- Reference count: 13
- Key outcome: Pre-classification fusion with XGBoost achieved 97% overall accuracy and 88% macro-mean F1 score for land use classification

## Executive Summary
This study compares pre-classification and post-classification data fusion approaches for land use classification, specifically distinguishing industrial, commercial, and residential uses in the Gers department of southwest France. The pre-classification approach concatenates attributes from multiple sources and uses XGBoost to predict land use classes simultaneously, while the post-classification approach uses Dempster-Shafer Theory to merge predictions from individual sources. Results show that pre-classification fusion with XGBoost outperformed the post-classification approach, achieving 97% overall accuracy and 88% macro-mean F1 score, demonstrating the effectiveness of joint information exploitation across heterogeneous data sources.

## Method Summary
The study compares two data fusion approaches for classifying land use into industrial, commercial, and residential categories. Pre-classification fusion extracts attributes from multiple sources (optical imagery, land cover maps, authoritative databases, demographic data, land files, and VGI), concatenates them into a single feature vector, and uses machine learning classifiers (XGBoost, Random Forest, SVM) to predict classes simultaneously. Post-classification fusion independently classifies each data source using one-vs-all XGBoost models, then merges predictions using Dempster-Shafer Theory. Both approaches use neighbor-weighted attributes to incorporate spatial context, SMOTE-NC for class balancing, and evaluate performance using overall accuracy and macro-mean F1 score.

## Key Results
- Pre-classification fusion with XGBoost achieved the best performance: 97% overall accuracy and 88% macro-mean F1 score
- Post-classification fusion using Dempster-Shafer Theory performed significantly worse than pre-classification approach
- The pre-classification approach particularly excelled at distinguishing minority classes (industrial and commercial uses) compared to post-classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-classification fusion allows the classifier to exploit joint information across all data sources, improving performance.
- Mechanism: In pre-classification fusion, all attributes from different sources are concatenated into a single feature vector. The machine learning model then learns relationships between these combined features and the target land use classes. This allows the model to capture interactions between different data sources that may not be apparent when sources are processed separately.
- Core assumption: The classifier can effectively learn from the combined feature space and identify useful patterns across heterogeneous data sources.
- Evidence anchors:
  - [abstract] states "Pre-classification fusion, while not explicitly modeling imperfections, has the best final results, reaching an overall accuracy of 97% and a macro-mean F1 score of 88%."
  - [section] explains "In pre-classification fusion, all the attributes are concatenated and a machine learning algorithm will predict LU classes from all sources simultaneously. The advantage is that the classifier can exploit the joint information of the sources."
- Break condition: If the feature space becomes too high-dimensional relative to the number of samples, the classifier may overfit or fail to learn meaningful patterns.

### Mechanism 2
- Claim: Post-classification fusion using Dempster-Shafer Theory can model uncertainty and contradictions between data sources.
- Mechanism: In post-classification fusion, each data source is classified independently, producing basic belief assignments (bba) for each class. These bbass are then combined using Dempster's rule of combination, which strengthens beliefs where sources agree and redistributes conflict proportionally. This approach explicitly models the uncertainty and potential contradictions between sources.
- Core assumption: The Dempster-Shafer framework can effectively represent and combine uncertain information from multiple sources to produce a final classification.
- Evidence anchors:
  - [abstract] mentions "post-classification approach uses the Dempster-Shafer Theory to merge predictions from individual sources."
  - [section] states "The advantages of this framework are its ability to model explicitly uncertainty, imprecision, and incompleteness."
- Break condition: If the conflict between sources is too high, Dempster's rule may become unstable or produce counterintuitive results.

### Mechanism 3
- Claim: Using neighboring attributes helps compensate for missing information in individual data sources.
- Mechanism: The method computes mean values of attributes over neighboring land use polygons, weighted by the length of the common perimeter. These neighborhood attributes are then used as additional features, allowing the classifier to incorporate spatial context and partially compensate for incomplete information in the primary data sources.
- Core assumption: Land use patterns exhibit spatial autocorrelation, so neighboring polygons provide useful contextual information for classification.
- Evidence anchors:
  - [section] describes "To take into account the spatial relationships between the different uses, the mean (or majority value if it is a categorical attribute) of each attribute mentioned above is also computed over the neighboring adjacent LU polygons, weighted by the length of the common perimeter."
  - [section] notes "Using the mean of the attributes over the adjacent LU polygons allows to partially compensate the lack of information."
- Break condition: If the neighborhood relationships are not meaningful (e.g., polygons separated by roads or other barriers), this approach may introduce noise rather than useful information.

## Foundational Learning

- Concept: Dempster-Shafer Theory
  - Why needed here: This theory is used in the post-classification fusion approach to combine uncertain information from multiple sources.
  - Quick check question: What is the main difference between Dempster-Shafer Theory and Bayesian probability in handling uncertainty?

- Concept: SMOTE-NC (Synthetic Minority Oversampling Technique for Nominal and Continuous attributes)
  - Why needed here: This technique is used to address class imbalance in the training data, particularly for minority classes LU2 and LU3.
  - Quick check question: How does SMOTE-NC differ from standard SMOTE when dealing with datasets containing both continuous and categorical attributes?

- Concept: Leave-One-Covariate-Out (LOCO) feature importance
  - Why needed here: This method is used to assess the impact of each data source on the overall classification performance.
  - Quick check question: What is the main advantage of using LOCO for feature importance compared to methods like permutation importance?

## Architecture Onboarding

- Component map:
  - Data sources -> Attribute extraction -> Preprocessing (split, encode, normalize, balance) -> Model training (XGBoost, Random Forest, SVM for pre-classification; one-vs-all XGBoost per class for post-classification) -> Evaluation (confusion matrix, recall, precision, OA, mF1)

- Critical path: Data source integration → Attribute extraction → Preprocessing → Model training → Evaluation → Error analysis

- Design tradeoffs:
  - Pre-classification vs. post-classification: Pre-classification allows joint learning but doesn't explicitly model source imperfections; post-classification can model uncertainty but may be more complex.
  - Number of attributes: More attributes can provide richer information but may lead to overfitting or increased computational cost.
  - Class balancing method: Different techniques (SMOTE-NC, random undersampling) have different effects on minority class performance and overall accuracy.

- Failure signatures:
  - Low performance on minority classes despite balancing: May indicate insufficient discriminative features for those classes or model bias.
  - High conflict in Dempster-Shafer fusion: Suggests contradictory information between sources, which may require additional data quality assessment.
  - Poor generalization to new areas: Could result from overfitting to the specific characteristics of the study area or differences in data source quality/completeness.

- First 3 experiments:
  1. Train and evaluate a simple classifier (e.g., Random Forest) using only one data source at a time to assess individual source performance.
  2. Implement pre-classification fusion with XGBoost using all available attributes and compare performance to single-source classifiers.
  3. Implement post-classification fusion using Dempster-Shafer Theory and compare results to pre-classification approach, focusing on how uncertainty is handled.

## Open Questions the Paper Calls Out

- Question: What is the impact of using more complex fusion techniques, such as deep learning-based methods, compared to the pre-classification and post-classification approaches tested in this study?
  - Basis in paper: [explicit] The authors mention that other approaches, such as graph neural networks and convolutional neural networks, should be compared in future works.
  - Why unresolved: The study only compares two specific data fusion approaches (pre-classification and post-classification) and does not explore more complex techniques.
  - What evidence would resolve it: A comprehensive comparison of different fusion techniques, including deep learning-based methods, on the same dataset and evaluation metrics would provide insights into their relative performance and potential benefits.

- Question: How can the issue of mixed land use be effectively addressed in the classification process?
  - Basis in paper: [explicit] The authors identify the need to detect and handle polygons with mixed land uses as a limitation of their study.
  - Why unresolved: The current study does not address the challenge of classifying polygons with mixed land uses, which is a common occurrence in real-world scenarios.
  - What evidence would resolve it: Developing and testing methods that can accurately identify and classify polygons with mixed land uses would provide a solution to this issue.

- Question: What are the limitations and potential improvements of the data sources used in this study?
  - Basis in paper: [explicit] The authors discuss the imperfections and limitations of the data sources used in their study, including issues with completeness, accuracy, and consistency.
  - Why unresolved: The study acknowledges the limitations of the data sources but does not provide a comprehensive analysis of their impact on the classification results or potential improvements.
  - What evidence would resolve it: A detailed analysis of the data sources, including their quality, completeness, and potential biases, would help identify areas for improvement and inform the selection of data sources for future studies.

## Limitations

- The study is limited to a single study area (Gers department, France) and specific data sources, making it uncertain whether the results would generalize to other regions or data characteristics.
- The comparison focuses on only two fusion approaches without exploring alternative methods like weighted voting or Bayesian model averaging.
- The choice of hyperparameters for the machine learning models is not exhaustively explored, and the impact of spatial autocorrelation on the results is not explicitly addressed.

## Confidence

- Pre-classification fusion performance (High): The experimental results clearly demonstrate superior performance with XGBoost, supported by direct comparisons and quantitative metrics.
- Post-classification fusion methodology (Medium): While the Dempster-Shafer framework is well-established, the specific implementation details and hyperparameter choices are not fully specified, limiting reproducibility.
- Generalization of findings (Low): The study is conducted in a single French department with specific data sources, making it uncertain whether the results would generalize to other regions or data characteristics.

## Next Checks

1. **Cross-validation across regions**: Replicate the experiment in multiple French departments or international study areas to assess the robustness and generalizability of the fusion approaches.
2. **Hyperparameter sensitivity analysis**: Systematically explore the hyperparameter space for both pre-classification (XGBoost, Random Forest, SVM) and post-classification (Dempster-Shafer) approaches to identify optimal configurations and assess stability.
3. **Comparison with alternative fusion methods**: Implement and evaluate additional fusion strategies (e.g., weighted voting, Bayesian model averaging, or ensemble methods) to determine if the observed performance gap between pre- and post-classification is specific to the chosen approaches or a more general phenomenon.