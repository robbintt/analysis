---
ver: rpa2
title: When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance
  Evaluation
arxiv_id: '2308.11953'
source_url: https://arxiv.org/abs/2308.11953
tags:
- minibatch-sfl
- clients
- training
- non-iid
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MiniBatch-SFL, a distributed learning algorithm
  that combines split learning and MiniBatch SGD to reduce computational load on edge
  devices while mitigating client drift under non-IID data. The method splits a global
  model at a cut layer, with clients training the client-side model and a main server
  training the server-side model using averaged gradients from all clients' smashed
  data.
---

# When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation

## Quick Facts
- arXiv ID: 2308.11953
- Source URL: https://arxiv.org/abs/2308.11953
- Reference count: 40
- Primary result: MiniBatch-SFL improves accuracy by up to 24.1% over SFL and 17.1% over FL under highly non-IID conditions

## Executive Summary
MiniBatch-SFL is a distributed learning algorithm that combines split learning and MiniBatch SGD to address the computational intensity of federated learning and the client drift problem under non-IID data. The algorithm splits a global model at a cut layer, with clients training the client-side model and a main server training the server-side model using averaged gradients from all clients' smashed data. Theoretical analysis shows the server-side updates are independent of non-IID degree, while client-side updates depend on it. Experiments demonstrate that a later cut layer (more client-side layers) leads to smaller gradient divergence and better performance, with significant accuracy improvements over baseline methods under highly non-IID conditions.

## Method Summary
MiniBatch-SFL splits a neural network at a cut layer, creating client-side and server-side models. Clients train their local models using local data and send smashed activations to the main server, which aggregates gradients from all clients and updates the server-side model using MiniBatch SGD. The main server then sends gradients back to clients for local updates. The Fed server periodically synchronizes client models. The method is tested with ResNet-18 on CIFAR-10 and FMNIST with non-IID ratios ranging from 0% to 95%, using cut layers Lc âˆˆ {1,2,3,4}, batch size 32, and learning rate 0.01 with 5 local epochs.

## Key Results
- MiniBatch-SFL achieves up to 24.1% accuracy improvement over SFL and 17.1% over FL under highly non-IID conditions
- Later cut layer positions (more client-side layers) lead to smaller average gradient divergence and better algorithm performance
- Server-side updates are independent of non-IID degree, mitigating client drift while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The server-side updates in MiniBatch-SFL are independent of the non-IID degree of clients' datasets.
- Mechanism: By treating all clients' smashed data as one giant mini-batch, the main server performs gradient descent in a centralized fashion, which averages out the heterogeneity introduced by non-IID data.
- Core assumption: The server-side model updates use averaged gradients from all clients, similar to centralized learning.
- Evidence anchors:
  - [abstract]: "The server-side updates do not depend on the non-IID degree of the clients' datasets and can potentially mitigate client drift."
  - [section]: "the main server uses the smashed data of all clients' mini-batches (as one giant mini-batch) to update the server-side model, see (3). Thus, the server-side training is similar to centralized learning which 'cancels out' the impact of non-IIDness on the server-side model."

### Mechanism 2
- Claim: A latter position of the cut layer leads to smaller average gradient divergence and better algorithm performance.
- Mechanism: More client-side layers mean more parameters are trained locally with main server support, reducing aggressive local updates that cause drift.
- Core assumption: Additional client-side layers provide more regularization against overfitting and forgetting, stabilizing local model updates.
- Evidence anchors:
  - [abstract]: "Perhaps counter-intuitive, our empirical result shows that a latter position of the cut layer leads to a smaller average gradient divergence and a better algorithm performance."
  - [section]: "a larger Lc corresponds to more parameters in the client-side model, which with the help from the main server, updates less aggressively across randomly sampled mini-batches."

### Mechanism 3
- Claim: MiniBatch-SFL outperforms both SFL-V2 and FL under highly non-IID data conditions.
- Mechanism: Combines the computational efficiency of SFL (reduced client-side computation) with the robustness of MiniBatch SGD (server-side updates using all client data), mitigating both computational constraints and client drift.
- Core assumption: The dual-paced update mechanism (client-side FL fashion, server-side MiniBatch SGD fashion) effectively addresses both limitations identified in the introduction.
- Evidence anchors:
  - [abstract]: "the accuracy improvement can be up to 24.1% and 17.1% with highly non-IID data, respectively."
  - [section]: "MiniBatch-SFL outperforms FL mainly because the existence of the main server mitigates the model update heterogeneity among clients and hence alleviates the client drift issue. MiniBatch-SFL outperforms SFL-V2 because in SFL-V2, the main server updates the server-side model in a sequential manner, which may lead to the forgetting issue commonly observed in SL."

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: Understanding the baseline approach that MiniBatch-SFL improves upon, including its computational intensity and client drift problems.
  - Quick check question: What are the two main limitations of conventional FL algorithms that MiniBatch-SFL aims to address?

- Concept: Split Learning (SL)
  - Why needed here: Understanding the model splitting approach that SFL builds upon, and how MiniBatch-SFL modifies it with MiniBatch SGD.
  - Quick check question: How does SFL reduce computational workload at client devices compared to FL?

- Concept: Non-IID Data and Client Drift
  - Why needed here: Understanding the core problem that MiniBatch-SFL addresses, where clients have heterogeneous data distributions leading to model divergence.
  - Quick check question: Why does client drift become more severe as the non-IID ratio increases in distributed learning systems?

## Architecture Onboarding

- Component map: Clients -> Fed Server -> Main Server
- Critical path:
  1. Clients perform forward pass and send smashed data to main server
  2. Main server computes averaged gradients and updates server-side model
  3. Main server sends gradients back to clients
  4. Clients perform backward pass and update client-side models
  5. Fed server periodically synchronizes client-side models
- Design tradeoffs:
  - Cut layer position vs. computational load vs. accuracy
  - Frequency of fed server synchronization vs. convergence speed vs. communication overhead
  - Learning rate choices for client-side vs. server-side updates
- Failure signatures:
  - Slow convergence or accuracy plateaus may indicate poor cut layer choice
  - Large gradient variance across clients suggests insufficient main server support
  - Communication bottlenecks may occur if fed server synchronization is too frequent
- First 3 experiments:
  1. Test different cut layer positions (Lc=1,2,3,4) on CIFAR-10 with varying non-IID ratios to observe accuracy and gradient divergence patterns
  2. Compare MiniBatch-SFL performance against FL, SFL-V2, and centralized learning baselines on both CIFAR-10 and FMNIST with N=10 and N=100 clients
  3. Measure the impact of fed server synchronization frequency on convergence speed and final accuracy

## Open Questions the Paper Calls Out

- Open Question 1: How does the choice of cut layer interact with different model architectures (e.g., transformers, recurrent networks) compared to CNNs?
  - Basis in paper: [inferred] The paper analyzes cut layer impact using ResNet-18 and a simple 4-layer CNN, but doesn't explore other architectures
  - Why unresolved: The convergence analysis and empirical observations are based on specific CNN architectures. Different model structures may have different sensitivities to cut layer placement.
  - What evidence would resolve it: Comparative experiments across multiple model architectures (transformers, RNNs, etc.) with varying cut layer positions and non-IID ratios.

- Open Question 2: What is the theoretical relationship between cut layer position and the gradient divergence metric across different non-IID ratios?
  - Basis in paper: [inferred] The paper shows empirically that later cut layers lead to smaller gradient divergence, but doesn't provide theoretical justification
  - Why unresolved: While empirical results show correlation between cut layer position and gradient divergence, the paper doesn't establish a formal theoretical connection or explain why this relationship exists
  - What evidence would resolve it: Mathematical derivation connecting cut layer position to gradient divergence bounds, potentially showing how the number of parameters on each side affects the divergence metric.

- Open Question 3: How does MiniBatch-SFL's performance scale with increasing numbers of clients beyond N=100, and what are the practical limits?
  - Basis in paper: [explicit] The paper only tests with N=10 and N=100 clients, showing accuracy decreases with more clients
  - Why unresolved: The convergence analysis and empirical results are limited to small client numbers. Real-world federated learning often involves thousands of clients.
  - What evidence would resolve it: Extensive experiments with varying client counts (1000+, 10000+) showing accuracy, convergence rate, and communication overhead trends, along with theoretical analysis of scaling behavior.

## Limitations
- Theoretical analysis is limited, focusing primarily on gradient divergence metrics without providing convergence rate guarantees or rigorous proofs of optimality
- Empirical evaluation relies on specific dataset choices (CIFAR-10 and FMNIST) and a fixed neural network architecture (ResNet-18)
- The mechanism explaining why later cut layers perform better is primarily empirical rather than theoretically grounded

## Confidence
- **High confidence**: The observation that MiniBatch-SFL achieves higher accuracy than both FL and SFL under non-IID conditions, supported by multiple experimental conditions and datasets.
- **Medium confidence**: The claim that server-side updates are independent of non-IID degree, as this is theoretically derived but not extensively validated through controlled experiments varying heterogeneity independently.
- **Low confidence**: The counter-intuitive finding about later cut layers performing better, as this contradicts typical federated learning intuition and lacks theoretical explanation beyond empirical gradient divergence measurements.

## Next Checks
1. Test MiniBatch-SFL on more complex architectures (e.g., deeper ResNets, Vision Transformers) to verify the cut layer positioning effect generalizes beyond ResNet-18.
2. Conduct ablation studies isolating the impact of MiniBatch SGD server-side updates versus the split learning architecture on convergence and accuracy.
3. Implement controlled experiments varying only the non-IID ratio while keeping other factors constant to directly validate the theoretical claim about server-side update independence from heterogeneity.