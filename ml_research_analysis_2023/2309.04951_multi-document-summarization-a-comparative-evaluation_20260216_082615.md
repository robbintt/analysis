---
ver: rpa2
title: 'Multi-document Summarization: A Comparative Evaluation'
arxiv_id: '2309.04951'
source_url: https://arxiv.org/abs/2309.04951
tags:
- dataset
- datasets
- summarization
- documents
- primera
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study evaluates state-of-the-art models for multi-document\
  \ summarization on diverse datasets, revealing that LED outperforms PRIMERA and\
  \ PEGASUS on the MS\xB2 biomedical dataset. PRIMERA excels on the Multi-News dataset\
  \ but underperforms on BigSurvey-MDS."
---

# Multi-document Summarization: A Comparative Evaluation

## Quick Facts
- arXiv ID: 2309.04951
- Source URL: https://arxiv.org/abs/2309.04951
- Authors: 
- Reference count: 34
- Key outcome: LED outperforms PRIMERA and PEGASUS on MS² biomedical dataset; PRIMERA excels on Multi-News but underperforms on BigSurvey-MDS; PEGASUS shows consistent cross-domain performance

## Executive Summary
This study evaluates state-of-the-art multi-document summarization models (PRIMERA, PEGASUS, LED) across diverse datasets including biomedical reviews, news articles, scientific papers, and movie reviews. The evaluation reveals significant performance variations across models and datasets, with LED achieving the highest ROUGE scores on biomedical literature reviews (MS²) while PRIMERA performs best on news datasets (Multi-News). The results highlight how dataset characteristics like document count per cluster significantly influence model performance, and suggest that no single model consistently outperforms others across all domains. The study emphasizes the need for further research to improve model generalization and explore additional factors like sentiment integration.

## Method Summary
The study evaluates three pre-trained models (PRIMERA, PEGASUS, LED) on six multi-document summarization datasets: BigSurvey-MDS (430K docs, 61.4 avg docs/cluster), MS² (470K docs, 23.5 avg docs/cluster), Multi-News (56K docs, 3.5 avg docs/cluster), Multi-XScience (40K docs, 2.8 avg docs/cluster), WikiSum (1.5M docs, 40 avg docs/cluster), and Rotten Tomatoes (244K docs, 26.8 avg docs/cluster). Models are evaluated using ROUGE-1, ROUGE-2, and ROUGE-L metrics without additional training, only inference. The study compares performance across different document cluster sizes and domain characteristics to understand model strengths and limitations.

## Key Results
- LED achieves highest ROUGE-1 score of 25.8 on MS² biomedical dataset, outperforming PRIMERA (23.5) and PEGASUS (24.3)
- PRIMERA excels on Multi-News with ROUGE-1 score of 32.4 but underperforms on BigSurvey-MDS with ROUGE-1 of 23.9
- PEGASUS shows consistent performance across domains with ROUGE-1 scores ranging from 27.0 to 32.4, though lower than PRIMERA on news datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LED outperforms PRIMERA and PEGASUS on the MS² biomedical dataset because LED's architecture supports longer input sequences and better cross-document information integration.
- Mechanism: LED extends BART's architecture by using an efficient local+global attention pattern and increasing positional embeddings to 16K tokens, allowing it to process longer biomedical review documents that average 23.5 studies per cluster. This enables better synthesis of contradictory evidence across multiple studies.
- Core assumption: Longer input capacity and cross-document attention patterns are critical for handling biomedical literature reviews with conflicting evidence.
- Evidence anchors:
  - [abstract]: "LED outperforms PRIMERA and PEGASUS on the MS² dataset"
  - [section]: "LED outperforms PRIMERA and PEGASUS in terms of ROUGE scores... LED obtained a significantly higher ROUGE-1 score of 25.8"
  - [corpus]: Weak - no direct corpus evidence linking LED's architecture to MS² performance
- Break condition: If dataset documents are shorter or contain less contradictory evidence, LED's advantages may not manifest.

### Mechanism 2
- Claim: PRIMERA's Entity Pyramid strategy works well for news datasets but fails on scientific survey datasets due to domain-specific content structure differences.
- Mechanism: PRIMERA uses Entity Pyramid masking which selects sentences based on entity frequency >1 and Cluster ROUGE evaluation. This works well for news articles with clear entity patterns but fails on BigSurvey-MDS where entity frequency doesn't correlate with summary importance.
- Core assumption: Entity frequency patterns in news articles generalize to scientific literature.
- Evidence anchors:
  - [abstract]: "PRIMERA excels on the Multi-News dataset but underperforms on BigSurvey-MDS"
  - [section]: "PRIMERA's performance is the lowest on the BigSurvey-MDS dataset, with a ROUGE-1 score of 23.9"
  - [corpus]: Weak - no corpus evidence explaining why Entity Pyramid fails on scientific domains
- Break condition: When document structure and entity importance patterns differ significantly from news articles.

### Mechanism 3
- Claim: PEGASUS shows consistent performance across domains because its standard Gap Sentence Generation objective generalizes better than domain-specific pretraining.
- Mechanism: PEGASUS uses standard GSG without domain-specific modifications, allowing it to maintain performance across different document types and domains. Its ROUGE-1 scores vary less dramatically than PRIMERA's across datasets.
- Core assumption: Domain-specific pretraining can hurt generalization when test domains differ from training domains.
- Evidence anchors:
  - [abstract]: "PEGASUS shows consistent performance across domains"
  - [section]: "PEGASUS model's performance is relatively consistent across different domains... Although these scores are lower than PRIMERA's scores, the difference is not significant"
  - [corpus]: Moderate - corpus shows PEGASUS maintaining ROUGE-1 scores around 27-32 across news, scientific, and movie review domains
- Break condition: If a domain has highly specialized vocabulary or structure that benefits from domain-specific pretraining.

## Foundational Learning

- Concept: Cross-document attention mechanisms
  - Why needed here: Multi-document summarization requires models to identify relationships and contradictions across multiple documents, which standard single-document attention cannot capture
  - Quick check question: How does Longformer's local+global attention pattern differ from standard Transformer attention?

- Concept: Domain adaptation and pretraining objectives
  - Why needed here: Different summarization models use different pretraining strategies (Entity Pyramid vs standard GSG), and understanding how these affect cross-domain performance is crucial
  - Quick check question: What is the key difference between PRIMERA's Entity Pyramid strategy and PEGASUS's standard GSG approach?

- Concept: Evaluation metrics for summarization
  - Why needed here: The study relies on ROUGE scores, and understanding what ROUGE-1, ROUGE-2, and ROUGE-L measure is essential for interpreting results
  - Quick check question: What does ROUGE-L measure that ROUGE-1 and ROUGE-2 don't capture?

## Architecture Onboarding

- Component map: LED extends BART with Longformer's attention mechanism, PRIMERA adds Entity Pyramid masking to LED, PEGASUS uses standard transformer with GSG pretraining
- Critical path: Input document processing → attention mechanism → sentence selection/masking → generation → ROUGE evaluation
- Design tradeoffs: LED prioritizes long sequence processing over efficiency, PRIMERA optimizes for news domain specificity at the cost of generalization, PEGASUS trades domain-specific performance for cross-domain consistency
- Failure signatures: PRIMERA failing on scientific datasets indicates domain-specific pretraining can hurt generalization; LED excelling on biomedical suggests longer input capacity is crucial for complex evidence synthesis
- First 3 experiments:
  1. Compare LED vs PRIMERA on BigSurvey-MDS with varying document cluster sizes to isolate attention capacity effects
  2. Test PRIMERA's Entity Pyramid strategy on news vs scientific datasets to quantify domain adaptation impact
  3. Evaluate PEGASUS with and without domain-specific fine-tuning to measure generalization costs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of documents per cluster affect the performance of state-of-the-art models for multi-document summarization?
- Basis in paper: [explicit] The paper discusses the impact of dataset characteristics, such as the number of documents per cluster, on model performance. It mentions that models perform differently based on the number of documents per cluster.
- Why unresolved: While the paper mentions the impact of document count on performance, it does not provide a detailed analysis of how exactly this factor affects model performance. The relationship between document count and model performance is not fully explored or quantified.
- What evidence would resolve it: A detailed analysis comparing model performance across datasets with varying document counts per cluster, along with statistical analysis to quantify the relationship, would help resolve this question.

### Open Question 2
- Question: How can the generalization of state-of-the-art models across different domains be improved?
- Basis in paper: [explicit] The paper highlights the need for improving the generalization of state-of-the-art models across different domains, as they tend to perform differently based on domain changes.
- Why unresolved: The paper identifies the need for improved generalization but does not provide specific methods or strategies to achieve this. It remains an open question how to enhance the domain adaptability of these models.
- What evidence would resolve it: Research demonstrating effective techniques or approaches for improving model generalization across diverse domains, along with experimental results showing improved performance, would help resolve this question.

### Open Question 3
- Question: How can additional factors like sentiment be integrated into multi-document summarization to capture valuable dimensions of information?
- Basis in paper: [explicit] The paper suggests that integrating additional factors like sentiment into multi-document summarization can capture other valuable dimensions of information that have yet to be extensively explored.
- Why unresolved: While the paper proposes the integration of additional factors like sentiment, it does not provide specific methods or approaches for incorporating sentiment analysis into the summarization process. The potential benefits and challenges of this integration are not fully explored.
- What evidence would resolve it: Research demonstrating effective methods for integrating sentiment analysis into multi-document summarization, along with experimental results showing the impact on summarization quality, would help resolve this question.

## Limitations
- Study relies solely on ROUGE metrics without human evaluation to validate model performance quality
- Computational efficiency trade-offs between models are not explored, particularly LED's increased resource requirements
- Entity Pyramid strategy's failure on BigSurvey-MDS could reflect ROUGE limitations rather than genuine model inadequacy

## Confidence
- High confidence: LED's superior performance on MS² biomedical dataset
- Medium confidence: PEGASUS's consistent cross-domain performance
- Medium confidence: PRIMERA's domain-specific effectiveness

## Next Checks
1. Conduct human evaluation study comparing summaries from LED, PRIMERA, and PEGASUS on MS² and BigSurvey-MDS datasets to verify if ROUGE scores align with human-perceived summary quality and factual accuracy.

2. Perform ablation study isolating LED's attention mechanism from its increased positional embedding capacity to determine which architectural feature drives its MS² performance advantage.

3. Test PRIMERA with modified Entity Pyramid parameters (adjusted entity frequency thresholds) on BigSurvey-MDS to quantify whether performance can be improved through hyperparameter tuning rather than fundamental architectural limitations.