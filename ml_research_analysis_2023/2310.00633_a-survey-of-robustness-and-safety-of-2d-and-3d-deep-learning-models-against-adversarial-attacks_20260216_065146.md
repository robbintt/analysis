---
ver: rpa2
title: A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against
  Adversarial Attacks
arxiv_id: '2310.00633'
source_url: https://arxiv.org/abs/2310.00633
tags:
- adversarial
- attacks
- attack
- they
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of 2D and 3D deep learning
  model robustness against adversarial attacks, covering over 170 papers. It introduces
  a general threat model and systematically categorizes attacks based on methodology,
  including optimization-based, GAN-based, spatial transformation, and colorization
  attacks for 2D models, and gradient-based, generative model, and heuristic algorithm
  attacks for 3D models.
---

# A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against Adversarial Attacks

## Quick Facts
- arXiv ID: 2310.00633
- Source URL: https://arxiv.org/abs/2310.00633
- Reference count: 40
- This paper provides a comprehensive survey of 2D and 3D deep learning model robustness against adversarial attacks, covering over 170 papers.

## Executive Summary
This survey systematically examines the robustness and safety of deep learning models against adversarial attacks in both 2D and 3D domains. The paper categorizes attacks based on methodology, threat models, and application scenarios, highlighting the unique challenges posed by 3D data and physical-world attacks. It provides insights into attack mechanisms, transferability, and future research directions, serving as a comprehensive reference for understanding adversarial vulnerabilities in deep learning systems.

## Method Summary
The survey collects and analyzes over 170 papers on adversarial attacks targeting 2D and 3D deep learning models. It establishes a general threat model framework and systematically categorizes attacks based on their methodology, including optimization-based, GAN-based, spatial transformation, and colorization attacks for 2D models, and gradient-based, generative model, and heuristic algorithm attacks for 3D models. The survey also examines physical-world attacks and their implications for safety-critical applications.

## Key Results
- Semantic-preserving perturbations (color space transformations, spatial manipulations) significantly expand the attack space while maintaining human perceptual similarity
- Physical adversarial attacks pose greater real-world threats than digital attacks due to their ability to operate in uncontrolled environments
- Transferability remains a critical challenge for black-box attacks, with various techniques proposed to improve cross-model effectiveness
- 3D adversarial attacks face unique challenges including unordered point clouds and larger perturbation spaces compared to 2D attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extending the concept of adversarial examples beyond imperceptible perturbations to semantic-preserving perturbations significantly increases the attack space and attack success rate.
- Mechanism: By operating in color spaces (HSV, LAB, YUV) or using spatial transformations, attackers can generate adversarial examples that are less detectable to humans but still effective against models.
- Core assumption: Human perception is less sensitive to certain types of transformations (e.g., color shifts) compared to model predictions.
- Evidence anchors:
  - [abstract]: "We extend the concept of adversarial examples beyond imperceptive perturbations and collate over 170 papers to give an overview of deep learning model robustness against various adversarial attacks."
  - [section]: "Some works also transform images into different color spaces and manipulate the color space instead because human beings prefer to classify objects according to their shapes rather than colors."
  - [corpus]: No direct evidence found in corpus for this specific claim.
- Break condition: If models are trained to be robust against semantic transformations or if perceptual similarity metrics are incorporated into the model's loss function.

### Mechanism 2
- Claim: Physical adversarial attacks pose a more significant threat to real-world applications than digital attacks due to their ability to operate in uncontrolled environments.
- Mechanism: By using mediums like patches, illumination, or sensors, attackers can modify the input to the model in the physical world, bypassing the need for direct digital manipulation.
- Core assumption: Physical attacks can overcome the limitations of digital attacks by operating in the real world.
- Evidence anchors:
  - [abstract]: "The physically realizable adversarial attacks further pose fatal threats to the application and human safety."
  - [section]: "The main challenge of physical adversarial attacks is the ever-changing physical environment, such as the background light noise, varying viewpoints, and different distances."
  - [corpus]: No direct evidence found in corpus for this specific claim.
- Break condition: If physical attacks are mitigated by robust training data augmentation, sensor fusion, or environmental controls.

### Mechanism 3
- Claim: The transferability of adversarial examples is crucial for black-box attacks and can be improved through various techniques.
- Mechanism: By optimizing the latent features, using generative models, or manipulating the surrogate model's features, attackers can generate adversarial examples that are more likely to fool unseen models.
- Core assumption: Models with similar decision boundaries or learned features are more susceptible to the same adversarial examples.
- Evidence anchors:
  - [abstract]: "To realize trustworthy AI, continuous efforts have been spent on improving the model's robustness and safety against adversarial attacks and finding these models' robustness upper bound by constructing stronger adversarial attacks."
  - [section]: "There are several promising directions to improve the transferability of adversarial examples, including random/adversarial transformations, generating perturbation by generative model, and manipulating the latent layer features."
  - [corpus]: No direct evidence found in corpus for this specific claim.
- Break condition: If models are trained with diverse architectures, data augmentation, or ensemble methods to reduce similarity in decision boundaries.

## Foundational Learning

- Concept: Deep Learning and Computer Vision
  - Why needed here: Understanding the basics of deep learning models and their applications in computer vision is essential for comprehending the vulnerabilities and attack methods discussed in the paper.
  - Quick check question: What are the key differences between 2D and 3D computer vision tasks, and how do these differences impact the design of adversarial attacks?

- Concept: Adversarial Attacks and Defenses
  - Why needed here: Familiarity with the concepts of adversarial examples, attack methodologies, and defense mechanisms is crucial for understanding the survey's findings and implications.
  - Quick check question: How do white-box and black-box attacks differ in terms of the attacker's knowledge and the attack's effectiveness?

- Concept: 3D Data and Point Clouds
  - Why needed here: Understanding the unique characteristics of 3D data, such as unordered point clouds and larger perturbation spaces, is essential for comprehending the challenges and solutions in 3D adversarial attacks.
  - Quick check question: What are the main differences between 2D and 3D adversarial attacks, and how do these differences impact the attack's success rate and transferability?

## Architecture Onboarding

- Component map: Data preprocessing -> Model architecture -> Attack generation -> Evaluation metrics
- Critical path:
  1. Data preprocessing and model loading
  2. Attack generation and optimization
  3. Evaluation and analysis of results

- Design tradeoffs:
  - Attack strength vs. imperceptibility: Balancing the perturbation size and the attack's effectiveness
  - Transferability vs. target specificity: Choosing between attacks that generalize well across models or are tailored to a specific target
  - Computational cost vs. attack efficiency: Optimizing the attack algorithm for speed or resource usage

- Failure signatures:
  - Low attack success rate: Indicates ineffective attack methods or robust defenses
  - High computational cost: Suggests inefficient attack algorithms or resource constraints
  - Poor transferability: Implies model-specific attacks or lack of generalization

- First 3 experiments:
  1. Implement a simple white-box attack (e.g., FGSM) on a 2D image classification model and evaluate its success rate
  2. Generate a semantic-preserving adversarial example using color space transformations and test its effectiveness against a human evaluator
  3. Create a physical adversarial patch and test its robustness against different environmental conditions and viewpoints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What theoretical frameworks can explain the mechanisms behind adversarial example transferability across different models?
- Basis in paper: [explicit] The paper discusses that most methods improve transferability empirically and notes that few works have proven lower bounds or explained the mechanism behind transferability.
- Why unresolved: Current theoretical explanations are limited and cannot fully account for phenomena like asymmetry of transferability or why models sometimes predict different false labels for the same adversarial example.
- What evidence would resolve it: A comprehensive theoretical framework that mathematically explains transferability patterns, accounts for observed asymmetries, and predicts transfer behavior across different model architectures.

### Open Question 2
- Question: How can we effectively evaluate the robustness of novel deep learning architectures (e.g., Vision Transformers, Point Transformers) against adversarial attacks?
- Basis in paper: [explicit] The paper mentions that the robustness of newly proposed models like Transformers and Point Transformers still needs to be studied, and existing evaluation methods may not be suitable.
- Why unresolved: Novel architectures have different characteristics from traditional CNNs, and existing adversarial attack methods and evaluation metrics may not adequately assess their vulnerabilities.
- What evidence would resolve it: A standardized evaluation framework specifically designed for novel architectures, including new attack methods and robustness metrics that account for their unique properties.

### Open Question 3
- Question: What are the most effective methods to generate physically realizable adversarial examples that are robust to environmental variations and simultaneously difficult to detect by humans?
- Basis in paper: [explicit] The paper highlights the challenge of balancing attack strength and imperceptibility in physical attacks, and notes that current methods often result in large perturbations when using expectation over transformations.
- Why unresolved: There is a trade-off between attack effectiveness and stealthiness, and current methods struggle to maintain both properties in real-world conditions.
- What evidence would resolve it: Development of new attack methods that achieve high success rates in physical environments while maintaining natural appearance, validated through extensive real-world testing under varying conditions.

## Limitations

- Limited independent verification of specific attack implementations and their reported success rates
- Physical attack claims rely primarily on theoretical reasoning with limited empirical validation across diverse environmental conditions
- Survey methodology depends on the quality and completeness of the 170+ papers analyzed

## Confidence

- High confidence in the systematic categorization of attacks and identification of unique challenges in 3D adversarial attacks
- Medium confidence in specific attack implementations and real-world effectiveness claims due to lack of independent verification
- Medium confidence in physical attack robustness claims requiring more extensive empirical validation

## Next Checks

1. Implement and test a subset of 2D and 3D attacks (e.g., FGSM, PGD, 3DAdv) on standard datasets to verify reported success rates and transferability

2. Conduct physical adversarial attack experiments under varying environmental conditions (lighting, viewpoints, distances) to validate robustness claims

3. Evaluate the effectiveness of semantic-preserving perturbations against human evaluators and compare with model predictions to confirm the perceptual indistinguishability claims