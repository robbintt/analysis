---
ver: rpa2
title: Fast and Accurate Factual Inconsistency Detection Over Long Documents
arxiv_id: '2310.13189'
source_url: https://arxiv.org/abs/2310.13189
tags:
- scale
- factual
- screeneval
- arxiv
- inconsistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SCALE is a task-agnostic model for detecting factual inconsistencies
  using a novel chunking strategy. It achieves state-of-the-art performance in factual
  inconsistency detection for diverse tasks and long inputs.
---

# Fast and Accurate Factual Inconsistency Detection Over Long Documents

## Quick Facts
- **arXiv ID**: 2310.13189
- **Source URL**: https://arxiv.org/abs/2310.13189
- **Reference count**: 12
- **Primary result**: SCALE achieves state-of-the-art factual inconsistency detection performance on ScreenEval and TRUE benchmark datasets using large text chunks and binary search tree retrieval.

## Executive Summary
This paper introduces SCALE, a task-agnostic model for detecting factual inconsistencies in long documents using Natural Language Inference (NLI) principles. The key innovation is SCALE's chunking strategy that uses large text chunks (up to 1000 tokens) rather than sentence-level decomposition, preserving long-range dependencies and contextual information. SCALE also employs a binary search tree algorithm for efficient retrieval of relevant source sentences, enabling rapid explanation of model decisions. The model demonstrates superior performance in accuracy, efficiency, and calibration compared to competitive systems across multiple benchmarks.

## Method Summary
SCALE is an NLI-based model that processes long documents by decomposing them into large text chunks (up to 1000 tokens) rather than individual sentences. Each chunk is used as a premise in the NLI model alongside the hypothesis (generated text) to produce entailment scores. A binary search tree algorithm efficiently retrieves relevant source sentences by recursively dividing document chunks and scoring them against the hypothesis. The model uses Flan-T5 as its backbone and is evaluated on ScreenEval dataset and TRUE benchmark using metrics including ROC_AUC, Pearson correlation, and Expected Calibration Error (ECE).

## Key Results
- SCALE achieves state-of-the-art performance in factual inconsistency detection on ScreenEval dataset and TRUE benchmark
- The model demonstrates superior calibration, with SCALElarge providing the best calibration on average and SCALEXL outperforming other non-SCALE models in over half of datasets
- SCALE's binary search tree retrieval requires significantly fewer model calls compared to linear search methods while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
SCALE's chunking strategy improves factual inconsistency detection by providing better context than sentence-level decomposition. Instead of breaking documents into individual sentences, SCALE uses large text chunks (up to 1000 tokens) as premises in the NLI model. This preserves long-range dependencies and contextual information that would be lost with sentence-level decomposition. Evidence from TRUE benchmark shows SCALE outperforms sentence-level decomposition methods like SummaCconv and seNtLI on long document tasks.

### Mechanism 2
SCALE's binary search tree retrieval algorithm efficiently finds relevant source sentences while maintaining accuracy. The algorithm recursively divides document chunks into binary search trees, scoring each against the hypothesis and narrowing down to relevant source sentences. This reduces model calls from O(n) to O(log n). SCALE's BST retrieval requires significantly fewer model calls, allowing it to pinpoint relevant utterances without having to score each one individually.

### Mechanism 3
SCALE achieves better calibration than competing methods, making its outputs more interpretable as probabilities. By using larger chunks and more context, SCALE produces probability outputs that better correspond to actual error rates. SCALElarge provides the best calibration on average and SCALEXL outperforms other non-SCALE models in calibration on over half of the datasets.

## Foundational Learning

- **Concept**: Natural Language Inference (NLI) fundamentals
  - Why needed here: SCALE is built on NLI principles to determine if generated text entails source document content
  - Quick check question: What are the three possible relationships between premise and hypothesis in NLI models?
  - Answer: Entailment, contradiction, and neutral

- **Concept**: Chunking and context windows in transformer models
  - Why needed here: SCALE's effectiveness depends on understanding how context windows affect transformer performance
  - Quick check question: What happens to transformer performance when context windows are too small for the task?
  - Answer: Loss of long-range dependencies and reduced accuracy on tasks requiring broader context

- **Concept**: Calibration and Expected Calibration Error (ECE)
  - Why needed here: SCALE's calibration performance is a key differentiator from competing methods
  - Quick check question: How is Expected Calibration Error calculated?
  - Answer: By dividing model outputs into bins, computing accuracy-confidencedifference per bin, then taking weighted average

## Architecture Onboarding

- **Component map**: Document → Chunker → NLI Model (chunk + hypothesis) → Binary Search Tree Retriever → Aggregator → Output
- **Critical path**: Document chunking → hypothesis generation → NLI scoring → aggregation → calibration → output
- **Design tradeoffs**: Larger chunks improve accuracy but reduce efficiency; binary search tree improves speed but may miss distributed relevant information; better calibration requires more computational resources
- **Failure signatures**: Poor performance on documents with relevant information split across chunk boundaries; calibration issues on tasks with high variance; retrieval failures when relevant sentences are distant from main topic
- **First 3 experiments**:
  1. Test different chunk sizes (100, 500, 1000 tokens) on ScreenEval to find optimal balance
  2. Compare binary search tree vs linear search for retrieval accuracy and efficiency
  3. Evaluate calibration across different NLI model sizes (base, large, XL) on TRUE benchmark

## Open Questions the Paper Calls Out

### Open Question 1
How does SCALE handle the potential loss of information due to the use of only "Yes" and "No" logits for scoring? The paper mentions that SCALE only uses the "Yes" and "No" logits to compute its entailment score, which could lead to a loss of accuracy due to other information possibly flowing to similar tokens.

### Open Question 2
How does the performance of SCALE vary with different chunk sizes, and what is the optimal chunk size for different types of documents? The paper discusses the effect of chunk size on SCALE's performance and time, showing that there is a sharp increase in performance and a sharp decrease in model run time up until the chunk size is 1000 tokens long.

### Open Question 3
How does SCALE's calibration performance vary across different natural language generation tasks and model sizes? The paper mentions that SCALE struggles with calibration on certain tasks and that this can even vary by model size, but it does not provide a detailed breakdown of these variations.

## Limitations

- Limited validation on document types beyond dialogues and articles, with uncertain performance on highly technical or domain-specific documents
- Potential artifacts introduced by chunk boundaries when relevant information spans across them
- Binary search tree algorithm may miss relevant source sentences that fall outside highest-scoring chunks at each division level

## Confidence

**High Confidence**: SCALE's superior accuracy on factual inconsistency detection compared to sentence-level decomposition methods; the effectiveness of large text chunks (up to 1000 tokens) for preserving context in NLI-based detection; SCALE's improved calibration performance across multiple datasets

**Medium Confidence**: The efficiency gains from binary search tree retrieval (limited ablation studies); the model's ability to handle arbitrary document lengths (theoretical claim, limited empirical validation on extremely long documents); the generalizability of SCALE's chunking strategy across different types of factual inconsistency tasks

**Low Confidence**: The exact trade-off point between chunk size and performance (no systematic study across varying document lengths); the model's behavior on documents with highly distributed relevant information (minimal discussion of edge cases); long-term scalability with respect to memory requirements for very large chunks

## Next Checks

1. **Chunk Size Sensitivity Analysis**: Conduct experiments varying chunk sizes (e.g., 200, 500, 1000, 1500 tokens) on ScreenEval to identify the optimal chunk size for different document lengths and determine the point of diminishing returns.

2. **Binary Search Tree Ablation**: Compare the binary search tree retrieval method against a full linear search baseline on a subset of the TRUE benchmark, measuring both retrieval accuracy (recall of relevant sentences) and efficiency (model calls, runtime) to quantify the trade-off.

3. **Cross-Domain Robustness Test**: Evaluate SCALE on at least two additional datasets from different domains (e.g., scientific papers, legal documents, or news articles) not included in the ScreenEval or TRUE benchmark to assess generalizability beyond the tested domains.