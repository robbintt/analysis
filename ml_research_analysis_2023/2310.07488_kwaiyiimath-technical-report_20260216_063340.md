---
ver: rpa2
title: 'KwaiYiiMath: Technical Report'
arxiv_id: '2310.07488'
source_url: https://arxiv.org/abs/2310.07488
tags:
- arxiv
- preprint
- language
- reasoning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report introduces KwaiYiiMath, a large language model fine-tuned
  for mathematical reasoning. The model is developed through supervised fine-tuning
  (SFT) and human preference alignment using reinforcement learning from human feedback
  (RLHF) and direct preference optimization (DPO).
---

# KwaiYiiMath: Technical Report

## Quick Facts
- arXiv ID: 2310.07488
- Source URL: https://arxiv.org/abs/2310.07488
- Reference count: 20
- Primary result: State-of-the-art performance among similar-sized models on mathematical reasoning benchmarks

## Executive Summary
KwaiYiiMath is a large language model fine-tuned for mathematical reasoning that achieves state-of-the-art performance among similar-sized models through supervised fine-tuning and human preference alignment. The model uses dual-evolving actions in depth and constrained-mutation evolving actions in breadth to generate diverse mathematical instructions while preserving chain-of-thought processes. It demonstrates strong performance on GSM8k, CMath, and KMath benchmarks, approaching GPT-4 performance and showing robustness to out-of-distribution samples.

## Method Summary
The method consists of three main phases: (1) Supervised fine-tuning on high-quality mathematical instruction data collected through dual-evolving actions and constrained-mutation evolving actions, (2) Human preference data collection using multiple models and diverse prompting strategies to generate chain-of-thought reasoning paths, and (3) Human preference alignment through reinforcement learning from human feedback (RLHF) using PPO and direct preference optimization (DPO). The model is built on a 13B parameter KwaiYiiBase foundation and is evaluated on mathematical reasoning benchmarks in both English and Chinese.

## Key Results
- Achieves state-of-the-art performance among similar-sized models on GSM8k, CMath, and KMath benchmarks
- Approaches GPT-4 performance levels while maintaining robustness to out-of-distribution samples
- Demonstrates strong performance across varying problem complexities from basic arithmetic to advanced calculus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-evolving actions in depth and constrained-mutation evolving actions in breadth generate diverse mathematical instructions while preserving the chain-of-thought process
- Mechanism: The method decomposes complex problems into sub-problems (depth) and then evolves new problems within constrained scopes (breadth), maintaining logical connections between steps
- Core assumption: Mathematical problems can be logically decomposed into sub-problems that maintain CoT relationships when evolved separately
- Evidence anchors:
  - [abstract]: "dual-evolving actions in depth and constrained-mutation evolving actions in breadth to generate diverse mathematical instructions while preserving the chain-of-thought process"
  - [section]: "We propose Dual-evolving actions in depth to retain the CoT process and expand instruction diversity... constrained-evolving action that is inspired by the mutation evolution"
  - [corpus]: Weak evidence - neighboring papers focus on different mathematical reasoning approaches without explicit discussion of dual-evolving actions

### Mechanism 2
- Claim: Diverse reasoning paths generated from multiple models and prompting strategies improve mathematical reasoning performance
- Mechanism: Multiple models generate different solution paths for each problem, and diverse CoT prompts create varied reasoning approaches that capture different problem-solving strategies
- Core assumption: Different models and prompting strategies capture complementary reasoning patterns that enhance overall performance
- Evidence anchors:
  - [abstract]: "high-quality mathematical data is collected with careful filtering for correct calculations and answers"
  - [section]: "we collect various available LLMs... use these models to generate multiple reasoning paths... we collect diverse CoT prompts, and use different prompting strategies"
  - [corpus]: No direct evidence - neighboring papers don't explicitly discuss multi-model reasoning path generation

### Mechanism 3
- Claim: Human preference alignment through RLHF and DPO improves answer correctness and reasoning quality beyond supervised fine-tuning alone
- Mechanism: Reward models trained on human preferences guide policy updates via PPO, while DPO directly optimizes for preference satisfaction without intermediate reward model
- Core assumption: Human preferences capture meaningful quality distinctions that standard supervised learning misses
- Evidence anchors:
  - [abstract]: "human preference alignment using reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO)"
  - [section]: "we use two scalable alignment frameworks, reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO)"
  - [corpus]: Weak evidence - neighboring papers focus on technical aspects but don't provide comparative evidence of human preference alignment effectiveness

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Mathematical problem-solving requires multi-step reasoning that benefits from explicit intermediate steps
  - Quick check question: Can you explain why showing intermediate steps helps LLMs solve complex math problems?

- Concept: Supervised fine-tuning with instruction tuning
  - Why needed here: SFT adapts the base model to mathematical tasks using high-quality instruction data
  - Quick check question: What's the difference between instruction tuning and regular supervised fine-tuning?

- Concept: Reinforcement learning from human feedback
  - Why needed here: RLHF aligns model outputs with human preferences beyond what supervised learning achieves
  - Quick check question: How does RLHF differ from standard supervised fine-tuning in terms of learning objective?

## Architecture Onboarding

- Component map: SFT → Human Preference Data Collection → RLHF/DPO Training → Evaluation
- Critical path: SFT → Human Preference Data Collection → RLHF/DPO Training → Evaluation
- Design tradeoffs: Larger model size provides better performance but increases computational cost; diverse data collection improves robustness but requires more annotation effort; human preference alignment improves quality but adds complexity
- Failure signatures: Poor performance on KMath indicates Chinese math reasoning issues; low GSM8k scores suggest English reasoning problems; inconsistent results across benchmarks may indicate overfitting
- First 3 experiments:
  1. Compare SFT-only vs SFT+RLHF performance on GSM8k to measure alignment benefit
  2. Test different prompting strategies on CMath to find optimal CoT approach
  3. Evaluate model robustness using GSM8k Robust dataset to check for overfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Dual-evolving action approach compare to Reinforcement Learning from Evol-Instruct Feedback (RL-EIF) in terms of instruction diversity and mathematical reasoning performance?
- Basis in paper: [explicit] The paper introduces Dual-evolving actions in depth and constrained-mutation evolving actions in breadth as an alternative to RL-EIF, which was developed concurrently
- Why unresolved: The paper does not provide a direct comparison between Dual-evolving actions and RL-EIF in terms of their effectiveness in generating diverse instructions and improving mathematical reasoning performance
- What evidence would resolve it: A controlled experiment comparing the performance of models trained with Dual-evolving actions and RL-EIF on the same mathematical reasoning tasks and benchmarks

### Open Question 2
- Question: What is the impact of instruction diversity on the mathematical reasoning performance of large language models, and how does it compare to the impact of data quality?
- Basis in paper: [explicit] The paper emphasizes the importance of both instruction diversity and data quality in improving the mathematical reasoning abilities of KwaiYiiMath
- Why unresolved: The paper does not provide a quantitative analysis of the relative contributions of instruction diversity and data quality to the overall performance improvement
- What evidence would resolve it: A study that systematically varies the levels of instruction diversity and data quality in the training data and measures their individual and combined effects on mathematical reasoning performance

### Open Question 3
- Question: How does the robustness of KwaiYiiMath to distractors in mathematical problems compare to its robustness to other forms of noise, such as typos or grammatical errors in the problem statements?
- Basis in paper: [explicit] The paper demonstrates that KwaiYiiMath's performance degrades when presented with problems containing distractors, but does not explore its robustness to other types of noise
- Why unresolved: The paper does not investigate the model's ability to handle other common forms of noise that may be present in real-world mathematical problems
- What evidence would resolve it: An experiment that evaluates KwaiYiiMath's performance on a dataset of mathematical problems with various types of noise, such as typos, grammatical errors, or ambiguous phrasing

## Limitations

- Lack of specific implementation details for the dual-evolving action methodology
- No ablation studies showing individual contributions of SFT, RLHF, and DPO
- Claims about approaching GPT-4 performance lack direct comparative evidence

## Confidence

- **High Confidence**: The model architecture and training pipeline (SFT → RLHF/DPO) are clearly specified and follow established practices
- **Medium Confidence**: Performance improvements over baseline models are demonstrated, but the specific contributions of the dual-evolving methodology are not empirically validated
- **Low Confidence**: Claims about approaching GPT-4 performance lack direct comparative evidence and may overstate the model's capabilities

## Next Checks

1. **Ablation Study**: Conduct controlled experiments to quantify the individual contributions of SFT, RLHF, and DPO to overall performance improvements
2. **Dual-Evolving Validation**: Implement and test the dual-evolving actions methodology independently to verify its effectiveness in generating diverse mathematical instructions
3. **Direct GPT-4 Comparison**: Perform head-to-head evaluation against GPT-4 on the same benchmarks using identical evaluation protocols to validate the "approaching GPT-4" performance claim