---
ver: rpa2
title: A Benchmark Study on Calibration
arxiv_id: '2308.11838'
source_url: https://arxiv.org/abs/2308.11838
tags:
- calibration
- bins
- cwce
- accuracy
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a large-scale benchmark dataset for evaluating
  the calibration of neural networks across diverse architectures and datasets. Leveraging
  the extensive NATS-Bench search space, the dataset assesses 102 calibration metrics
  on 117,702 unique neural network architectures, providing a comprehensive resource
  for understanding calibration properties.
---

# A Benchmark Study on Calibration

## Quick Facts
- arXiv ID: 2308.11838
- Source URL: https://arxiv.org/abs/2308.11838
- Reference count: 40
- One-line primary result: This study presents a large-scale benchmark dataset for evaluating the calibration of neural networks across diverse architectures and datasets, revealing that calibration performance is not generalizable across datasets and that robustness metrics correlate with calibration only for high-accuracy models.

## Executive Summary
This study presents a comprehensive benchmark dataset for evaluating the calibration of neural networks across diverse architectures and datasets. Leveraging the extensive NATS-Bench search space, the dataset assesses 102 calibration metrics on 117,702 unique neural network architectures, providing a large-scale resource for understanding calibration properties. The study reveals several key findings: calibration performance is not generalizable across datasets, robustness metrics correlate with calibration only for high-accuracy models, and post-hoc calibration methods affect models non-uniformly. The research also challenges the assumption that larger models are inherently poorly calibrated and identifies architectural design preferences, such as skip connections, that enhance calibration.

## Method Summary
The study uses NATS-Bench to evaluate 102 calibration metrics on 117,702 unique neural network architectures across three datasets: CIFAR-10, CIFAR-100, and ImageNet16-120. Architectures were pretrained for 200 epochs, and temperature scaling was applied as a post-hoc calibration method. The analysis employed Kendall ranking correlation to assess relationships between calibration rankings across datasets, metrics, and model populations. The Harmonic Calibration Score (HCS) was used to identify architectural design preferences that enhance calibration.

## Key Results
- Calibration performance is not generalizable across datasets (e.g., CIFAR-10 vs. CIFAR-100).
- Robustness metrics correlate with calibration only for high-accuracy models.
- Post-hoc calibration methods (e.g., temperature scaling) affect models non-uniformly and can disrupt ranking.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Calibration is not generalizable across different tasks/datasets.
- **Mechanism**: The ranking of model calibration performance under one dataset (e.g., CIFAR-10) does not correlate with its ranking under another dataset (e.g., CIFAR-100 or ImageNet). This is because calibration depends heavily on the specific characteristics of the data distribution, such as class prior and complexity.
- **Core assumption**: The statistical properties of different datasets are sufficiently distinct that a model's calibration behavior on one does not predict its behavior on another.
- **Evidence anchors**:
  - [abstract] "Key findings include: calibration performance is not generalizable across datasets..."
  - [section 4.1] "Upon examining the correlation across datasets, we observed a substantial variation in the ranking of calibration metrics... the calibration performance on CIFAR-10 exhibited little correlation with the calibration performance on CIFAR-100... Furthermore, the calibration performance on CIFAR-10 exhibited no correlation with the performance on ImageNet..."
- **Break condition**: If two datasets have very similar statistical properties (e.g., same classes, similar class priors, same data distribution), the calibration ranking may correlate more strongly.

### Mechanism 2
- **Claim**: Robustness metrics can serve as calibration measures only for high-accuracy models.
- **Mechanism**: For models with high prediction accuracy, robustness to adversarial attacks and data corruption correlates with calibration performance (ECE). However, for lower-performing models, this correlation disappears, indicating that robustness and calibration are not universally interchangeable.
- **Core assumption**: There exists a subset of the model population where prediction accuracy and calibration are sufficiently aligned that robustness proxies become informative.
- **Evidence anchors**:
  - [abstract] "robustness metrics correlate with calibration only for high-accuracy models"
  - [section 4.2] "Upon analyzing the top 1000 ranked models, we found that all robustness metrics... exhibited a strong correlation with ECE... However, as we included worse-performing models... the correlation was not observed for most metrics..."
- **Break condition**: If robustness and calibration are both poor, or if the model's accuracy is low, the correlation between robustness and calibration metrics breaks down.

### Mechanism 3
- **Claim**: Post-hoc calibration methods (e.g., temperature scaling) affect models non-uniformly and can disrupt ranking.
- **Mechanism**: Applying temperature scaling changes the calibration error (ECE) in a way that is not uniformly distributed across models. Well-calibrated models may not maintain their relative ranking after post-hoc calibration, and the choice of bin size significantly influences post-hoc calibration measurements.
- **Core assumption**: Temperature scaling is a linear transformation that does not preserve the relative calibration quality ordering across models.
- **Evidence anchors**:
  - [abstract] "post-hoc calibration methods affect models non-uniformly"
  - [section 4.4] "The correlation coefficient between pre-ECE and post-ECE is nearly zero, suggesting that a well-calibrated model may not maintain its ranking of calibration performance after undergoing post-hoc calibration methods... On the other hand, a less well-calibrated model may improve its calibration performance after such methods."
  - [section 4.6] "Our findings suggest that bin size has a more substantial impact on post-ECE, with a negligible correlation between post-ECE measurements at varying bin sizes."
- **Break condition**: If the post-hoc calibration method is highly model-specific (e.g., per-model temperature scaling) or if the calibration error is already very low, the effect on ranking may be less pronounced.

## Foundational Learning

- **Concept**: Bin-based vs. non-bin-based calibration metrics
  - Why needed here: The study evaluates 102 different calibration metrics, including both bin-based (e.g., ECE, ECEem) and non-bin-based (e.g., KSCE, MMCE, KDECE). Understanding the distinction is crucial for interpreting results, especially the finding that classwise equal-mass binning (cwCEem) may be unreliable.
  - Quick check question: What is the main difference between ECE (equal-width binning) and ECEem (equal-mass binning), and why might one be preferred over the other?

- **Concept**: Neural Architecture Search (NAS) and search spaces
  - Why needed here: The study uses NATS-Bench, which provides a large search space of neural architectures. Understanding NAS and how search spaces are defined is essential for grasping the scale and scope of the calibration dataset.
  - Quick check question: What are the two main search spaces in NATS-Bench (TSS and SSS), and how do they differ in terms of the architectures they cover?

- **Concept**: Kendall ranking correlation coefficient
  - Why needed here: The study uses Kendall correlation to measure the relationship between calibration rankings across datasets, metrics, and model populations. This is a non-parametric statistic that assesses the degree of correspondence between two rankings.
  - Quick check question: What does a Kendall correlation coefficient of +1, -1, or 0 indicate about the relationship between two rankings?

## Architecture Onboarding

- **Component map**: NATS-Bench search space (117,702 architectures) -> CIFAR-10, CIFAR-100, ImageNet16-120 datasets -> 102 calibration metrics (ECE, MCE, Brier score, Log-likelihood, robustness metrics) -> temperature scaling -> bin sizes (5, 10, ..., 500) -> Kendall correlation analysis -> visualizations (box plots, scatter plots)

- **Critical path**:
  1. Load pretrained weights for each architecture.
  2. Evaluate all 102 calibration metrics for each architecture on each dataset.
  3. Compute Kendall correlation matrices to assess relationships between datasets, metrics, and model populations.
  4. Generate visualizations (box plots, scatter plots) to illustrate trends.
  5. Analyze architectural design preferences using the Harmonic Calibration Score (HCS).

- **Design tradeoffs**:
  - Using a large search space (NATS-Bench) provides comprehensive coverage but increases computational cost.
  - Evaluating many metrics provides a thorough assessment but introduces redundancy and potential noise.
  - Temperature scaling as a post-hoc method is simple but may not be optimal for all models.

- **Failure signatures**:
  - High variance in ECE across bin sizes (especially post-temperature scaling) indicates sensitivity to bin choice.
  - Low Kendall correlation between calibration rankings on different datasets indicates poor generalizability.
  - Negative correlation between robustness and calibration for low-accuracy models indicates breakdown of robustness as a proxy.

- **First 3 experiments**:
  1. Reproduce the Kendall correlation matrix between ECE on CIFAR-10, CIFAR-100, and ImageNet for top 1000 models.
  2. Plot ECE vs. Accuracy for models with accuracy >90% and all models on CIFAR-10 to observe the correlation pattern.
  3. Generate box plots of ECE across different bin sizes (5, 10, 15, 20, 25, 50, 100, 200, 500) for models trained for 12 and 200 epochs on CIFAR-10.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can model calibration be generalized across different datasets, and if not, what are the specific dataset characteristics that influence calibration?
- Basis in paper: [explicit] The study found that calibration performance on CIFAR-10 exhibited little correlation with performance on CIFAR-100 and ImageNet, suggesting weak or non-existent relationships between calibration performance and evaluation dataset.
- Why unresolved: The paper does not identify specific dataset characteristics (e.g., number of classes, image complexity, domain similarity) that determine whether calibration generalizes across tasks.
- What evidence would resolve it: Controlled experiments varying dataset characteristics (e.g., class count, image resolution, domain shift) while measuring calibration performance would reveal which factors influence generalization.

### Open Question 2
- Question: How does the choice of bin size in calibration metrics affect the reliability and stability of calibration measurements, particularly for underfitting models?
- Basis in paper: [explicit] The study found that bin size has a more substantial impact on post-ECE measurements than pre-ECE, with underfitting models being particularly sensitive to bin size choice.
- Why unresolved: The paper does not establish optimal bin size ranges for different model training stages or provide guidelines for selecting bin sizes based on model characteristics.
- What evidence would resolve it: Systematic studies varying bin sizes across different model architectures, training stages, and dataset complexities would establish reliable guidelines for bin size selection.

### Open Question 3
- Question: Which architectural designs and configurations consistently lead to better-calibrated models across different datasets and tasks?
- Basis in paper: [explicit] The study identified architectural preferences (e.g., 1x1 and 3x3 convolutions, skip connections) in top-performing models but did not establish generalizable design principles.
- Why unresolved: The analysis was limited to specific search spaces and datasets, and the study did not investigate the underlying mechanisms by which architectural choices affect calibration.
- What evidence would resolve it: Extensive architectural search across diverse datasets and tasks, combined with interpretability studies linking architectural features to calibration properties, would reveal generalizable design principles.

## Limitations
- The study relies on NATS-Bench, which may not fully represent the diversity of real-world neural architectures, potentially limiting generalizability to other search spaces or architectures.
- The analysis focuses on image classification tasks (CIFAR-10, CIFAR-100, ImageNet16-120), and findings may not extend to other domains like NLP or speech.
- The use of temperature scaling as the sole post-hoc calibration method may not capture the full spectrum of calibration techniques available.

## Confidence
- **High Confidence**: Calibration is not generalizable across datasets; post-hoc calibration methods affect models non-uniformly; calibration is more sensitive to bin size in underfitting models.
- **Medium Confidence**: Robustness metrics correlate with calibration only for high-accuracy models; larger models are not inherently poorly calibrated.
- **Low Confidence**: Architectural design preferences (e.g., skip connections) that enhance calibration, as this analysis is based on a single search space and may not generalize.

## Next Checks
1. Replicate the study using a different neural architecture search space (e.g., DARTS or ENAS) to verify if the calibration trends hold across diverse architectures.
2. Extend the analysis to non-image datasets (e.g., text classification or speech recognition) to assess the generalizability of calibration findings to other domains.
3. Evaluate additional post-hoc calibration methods (e.g., Platt scaling, Bayesian binning) to determine if the non-uniform effects observed with temperature scaling persist.