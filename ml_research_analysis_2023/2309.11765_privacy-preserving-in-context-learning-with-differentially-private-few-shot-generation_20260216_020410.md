---
ver: rpa2
title: Privacy-Preserving In-Context Learning with Differentially Private Few-Shot
  Generation
arxiv_id: '2309.11765'
source_url: https://arxiv.org/abs/2309.11765
tags:
- private
- privacy
- algorithm
- demonstrations
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of in-context learning (ICL) with
  large language models (LLMs) on private datasets while preserving privacy. The authors
  propose a novel algorithm that generates synthetic few-shot demonstrations from
  the private dataset with formal differential privacy (DP) guarantees.
---

# Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation

## Quick Facts
- arXiv ID: 2309.11765
- Source URL: https://arxiv.org/abs/2309.11765
- Reference count: 40
- Primary result: Achieves 50.7% accuracy on TREC with GPT-3 Babbage and ϵ=1, competitive with non-private 50.6% baseline

## Executive Summary
This paper introduces a novel algorithm for privacy-preserving in-context learning (ICL) with large language models (LLMs) on private datasets. The method generates synthetic few-shot demonstrations from private data with formal differential privacy (DP) guarantees by privately aggregating the generation process across disjoint subsets of the private data, similar to the PATE framework. The key innovation enables effective ICL while protecting private information, achieving competitive performance with strong privacy levels (ϵ=1-8) across multiple datasets.

## Method Summary
The algorithm generates DP synthetic few-shot demonstrations from private data by sampling M×N examples, partitioning them into M disjoint subsets, and generating next-token probabilities from each subset. These probabilities are then privately aggregated using Gaussian or Exponential DP mechanisms. The synthetic demonstrations are used for ICL inference, which benefits from the post-processing property of DP to allow unbounded queries without additional privacy cost. The method optionally reduces vocabulary to top-K tokens from public instructions before applying DP noise to improve generation quality.

## Key Results
- Achieves 50.7% accuracy on TREC dataset with GPT-3 Babbage and ϵ=1, matching non-private baseline of 50.6%
- Significantly improves upon fully private 0-shot performance (10.5% → 50.7% on TREC)
- Maintains strong privacy guarantees while enabling effective ICL across AGNews, TREC, DBPedia, and MIT Movies datasets
- Demonstrates competitive performance with ϵ=1-8 across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1: Private Aggregation via Disjoint Subsets
- Claim: Achieves differential privacy by aggregating generation probabilities from M disjoint subsets of private data
- Mechanism: Samples M×N examples, partitions into M disjoint subsets, generates next-token probabilities from each, and privately aggregates using DP mechanisms
- Core assumption: Privacy loss can be bounded through subsampling and composition theorems across token generations
- Evidence anchors: [abstract] formal DP guarantees; [section 4.2] detailed privacy analysis; [corpus] weak evidence from related work
- Break condition: If disjoint subsets aren't truly independent or sampling introduces bias, privacy amplification may not hold

### Mechanism 2: Vocabulary Reduction for Noise Reduction
- Claim: Limiting vocabulary to top-K tokens from public instruction reduces noise impact on generation
- Mechanism: Reduces vocabulary to top-K tokens from public instruction, sets others to zero, renormalizes before DP noise addition
- Core assumption: Top-K tokens from public instruction likely contain correct next token
- Evidence anchors: [section 4.1] vocabulary reduction description; [section 5.2] RVP effectiveness; [corpus] missing evidence in related work
- Break condition: If correct token consistently falls outside top-K, mechanism systematically excludes correct tokens

### Mechanism 3: Private Synthetic Demonstrations Enable Unbounded Queries
- Claim: Offline DP synthetic generation allows unlimited ICL queries without additional privacy cost
- Mechanism: First generates DP synthetic examples, then uses them for ICL inference benefiting from DP post-processing property
- Core assumption: Post-processing property of DP holds for ICL inference step
- Evidence anchors: [abstract] PATE-based approach; [section 4] post-processing property ensures (ϵ, δ)-DP for infinite queries; [corpus] strong evidence from PATE framework
- Break condition: If ICL inference amplifies privacy loss through adaptive queries, post-processing may not suffice

## Foundational Learning

- Differential Privacy: Understanding (ϵ, δ)-DP definitions, composition theorems, and privacy amplification by subsampling
  - Why needed here: Entire privacy guarantee relies on proper DP application
  - Quick check question: If an algorithm satisfies (ϵ, δ)-DP, what can we say about privacy of any function applied to its output?

- Language Model Token Generation: Understanding how LLMs generate text token-by-token using next-token probability distributions
  - Why needed here: Algorithm relies on LLM's ability to generate similar examples from demonstrations
  - Quick check question: In autoregressive text generation, how does the model generate the second token given the first token?

- Privacy Composition: Understanding numerical composition of privacy curves and how privacy loss accumulates across operations
  - Why needed here: Algorithm composes privacy loss across Tmax iterations requiring careful analysis
  - Quick check question: If each of k operations satisfies (ϵi, δi)-DP, what composition theorem bounds sequential composition privacy loss?

## Architecture Onboarding

- Component map: Private dataset Dpriv -> DP few-shot generation (Algorithm 1) -> Synthetic demonstrations -> ICL inference -> Query response

- Critical path: Private dataset → DP few-shot generation → Synthetic demonstrations → ICL inference → Query response

- Design tradeoffs:
  - Privacy vs utility: Higher privacy (smaller ϵ) requires more noise, reducing generation quality
  - M vs N: Larger M provides better privacy amplification but increases computation; larger N provides better signal but reduces privacy amplification
  - Vocabulary reduction: Improves utility but may exclude correct tokens if K is too small
  - Token generation strategy: Arg max (simpler, used in Exponential) vs sampling (more natural, used in Gaussian)

- Failure signatures:
  - Poor ICL performance: Could indicate insufficient privacy budget, wrong hyperparameters, or model mismatch
  - High variance across runs: May suggest unstable sampling or insufficient privacy amplification
  - Privacy budget exhaustion: Would manifest if composition analysis is incorrect or queries not properly accounted for

- First 3 experiments:
  1. Verify privacy guarantee: Run Algorithm 1 with synthetic data and check if privacy parameters match theoretical analysis using privacy accounting tools
  2. Test vocabulary reduction: Compare generation quality with and without vocabulary reduction (K=100) on small dataset to quantify utility improvement
  3. Validate composition: Measure actual privacy loss across multiple token generations versus theoretical bounds to ensure composition analysis is correct

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance scale with number of shots beyond 12?
- Basis in paper: [inferred] Authors note 8-shot and 12-shot cases perform better with random sampling from private dataset, suggesting potential improvements for larger n-shot scenarios
- Why unresolved: Paper only tests up to 12 shots without exploring scaling behavior for larger numbers
- What evidence would resolve it: Experiments with varying shots (16, 32, 64) comparing DP algorithm with non-private approaches

### Open Question 2
- Question: Can advanced sampling techniques beyond arg max improve DP few-shot generation performance?
- Basis in paper: [explicit] Gaussian mechanism benefits from post-processing property and is suitable for advanced sampling beyond arg max
- Why unresolved: Paper only uses basic arg max sampling without exploring other techniques
- What evidence would resolve it: Implementing and comparing performance using different sampling techniques (top-k, nucleus sampling) and evaluating impact on generated samples' quality

### Open Question 3
- Question: How does algorithm perform on tasks beyond text classification and information extraction?
- Basis in paper: [explicit] Method readily extends beyond text classification as demonstrated by experiments
- Why unresolved: Algorithm only evaluated on text classification and information extraction tasks
- What evidence would resolve it: Applying algorithm to different tasks (text summarization, question answering) and comparing with non-private approaches

### Open Question 4
- Question: Can algorithm be extended to handle multi-modal data (text+images, text+audio)?
- Basis in paper: [inferred] Framework could potentially be adapted to handle multi-modal data
- Why unresolved: Paper doesn't explore extension to multi-modal data and potential challenges
- What evidence would resolve it: Developing multi-modal version and evaluating performance on tasks involving text and other modalities

## Limitations

- Lack of detailed implementation specifications for critical components like disjoint subset sampling and DP noise mechanisms
- Vocabulary reduction mechanism lacks rigorous theoretical justification for why top-K tokens from public instructions reliably contain correct next tokens
- Exponential mechanism variant using arg max rather than sampling mentioned but not extensively evaluated
- Composition analysis assumes independence that may not hold due to autoregressive nature of LLM generation

## Confidence

- **High Confidence**: Core privacy guarantee that Algorithm 1 satisfies (ϵ, δ)-DP with proper composition across iterations
- **Medium Confidence**: Effectiveness of vocabulary reduction (RVP) in improving utility while maintaining privacy
- **Medium Confidence**: Competitive ICL performance with strong privacy (ϵ=1) compared to non-private baselines

## Next Checks

1. **Privacy Amplification Verification**: Implement Algorithm 1 with synthetic data and use privacy accounting tool (Privacy on Beam or zCDP) to verify actual privacy loss matches theoretical bounds across multiple token generations

2. **Vocabulary Reduction Robustness**: Conduct ablation studies systematically varying K (top-K tokens) from 10 to 1000 on multiple datasets to quantify trade-off between utility improvement and risk of excluding correct tokens

3. **Composition Analysis Validation**: Measure empirical privacy loss across sequential token generations versus theoretical predictions to identify deviations caused by dependencies in autoregressive generation process