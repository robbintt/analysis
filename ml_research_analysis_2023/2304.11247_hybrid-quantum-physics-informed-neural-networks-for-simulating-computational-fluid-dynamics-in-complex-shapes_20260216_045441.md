---
ver: rpa2
title: Hybrid quantum physics-informed neural networks for simulating computational
  fluid dynamics in complex shapes
arxiv_id: '2304.11247'
source_url: https://arxiv.org/abs/2304.11247
tags:
- quantum
- arxiv
- pinn
- neural
- classical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hybrid quantum physics-informed neural
  network (PINN) to simulate laminar fluid flows in 3D Y-shaped mixers, addressing
  the computational expense and inflexibility of traditional CFD solvers like OpenFOAM
  and Ansys when handling complex geometries and parameterized domains. The proposed
  approach combines the expressive power of quantum circuits with the flexibility
  of PINNs, embedding physical laws and boundary conditions into the loss function
  to solve the Navier-Stokes equations.
---

# Hybrid quantum physics-informed neural networks for simulating computational fluid dynamics in complex shapes

## Quick Facts
- arXiv ID: 2304.11247
- Source URL: https://arxiv.org/abs/2304.11247
- Reference count: 0
- Key outcome: Hybrid quantum PINN outperforms classical PINN by 21% in accuracy for 3D Y-shaped mixer simulations

## Executive Summary
This paper introduces a hybrid quantum physics-informed neural network (PINN) to simulate laminar fluid flows in 3D Y-shaped mixers, addressing the computational expense and inflexibility of traditional CFD solvers like OpenFOAM and Ansys when handling complex geometries and parameterized domains. The proposed approach combines the expressive power of quantum circuits with the flexibility of PINNs, embedding physical laws and boundary conditions into the loss function to solve the Navier-Stokes equations. The hybrid model outperforms a purely classical neural network by 21% in accuracy. Transfer learning is also explored, showing that PINNs can generalize solutions across slightly modified geometries without full re-simulation.

## Method Summary
The method combines a classical multilayer perceptron with a variational quantum circuit layer, trained to solve the steady Navier-Stokes equations for incompressible flow in a 3D Y-shaped mixer geometry. The hybrid architecture uses a 5-layer classical network (64 neurons each) with SiLU activation, followed by a quantum encoding layer, variational quantum circuit, and measurement layer. The physics-informed loss function combines PDE residuals and boundary condition errors, optimized using mini-batch Adam for 100 epochs. Transfer learning experiments used pre-trained models as initialization for slightly modified geometries, trained for an additional 100 epochs with L-BFGS.

## Key Results
- Hybrid quantum PINN achieves 21% higher accuracy compared to classical PINN
- Transfer learning enables solution generalization across modified geometries without full re-simulation
- The approach addresses limitations of traditional CFD solvers for complex geometries and shape optimization tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hybrid quantum layer improves model expressiveness for complex fluid dynamics.
- Mechanism: Quantum circuits provide richer functional representations than classical layers alone, enabling better approximation of the highly nonlinear Navier-Stokes equations.
- Core assumption: Variational quantum circuits can encode complex functions with fewer parameters than classical networks while maintaining differentiability for gradient-based optimization.
- Evidence anchors:
  - [abstract] "Our approach combines the expressive power of a quantum model with the flexibility of a PINN, resulting in a 21% higher accuracy compared to a purely classical neural network."
  - [section] "Fortunately, expressivity is a known strength of quantum computers [42]."
  - [corpus] Weak evidence - no directly comparable quantum-PINN studies found in neighboring papers.

### Mechanism 2
- Claim: Physics-informed loss functions constrain the neural network to satisfy governing equations.
- Mechanism: The loss function combines PDE residuals and boundary condition errors, forcing the network to learn solutions that are physically valid rather than just data-fitting.
- Core assumption: Proper weighting and formulation of physics loss terms ensures convergence to correct solutions.
- Evidence anchors:
  - [abstract] "embedding physical laws and boundary conditions into the loss function to solve the Navier-Stokes equations."
  - [section] "The loss,L, that the PINN tries to minimize is defined as L = LPDE + LBC."
  - [corpus] Moderate evidence - PINN methodology is well-established in neighboring papers.

### Mechanism 3
- Claim: Transfer learning enables generalization across similar geometries without full retraining.
- Mechanism: Pre-trained weights provide a good initialization for slightly modified geometries, reducing training time and improving convergence.
- Core assumption: Small geometric changes don't drastically alter the underlying physics, so solutions are transferable.
- Evidence anchors:
  - [abstract] "Transfer learning is also explored, showing that PINNs can generalize solutions across slightly modified geometries without full re-simulation."
  - [section] "For transfer learning, we used a model from the previous section as a base... Each iteration is trained for 100 epochs with L-BFGS."
  - [corpus] Weak evidence - limited examples of PINN transfer learning in corpus.

## Foundational Learning

- Concept: Partial Differential Equations and Navier-Stokes formulation
  - Why needed here: The PINN must embed the Navier-Stokes equations and continuity equation into its loss function
  - Quick check question: Can you write the steady Navier-Stokes equations for incompressible flow and identify the role of each term?

- Concept: Automatic differentiation and backpropagation through quantum circuits
  - Why needed here: The training requires computing gradients of both classical and quantum components with respect to all parameters
  - Quick check question: How does the adjoint differentiation method enable gradient computation for variational quantum circuits?

- Concept: Variational quantum circuits and expressivity
  - Why needed here: Understanding why quantum circuits might provide better function approximation than classical networks
  - Quick check question: What properties of quantum circuits contribute to their higher expressivity compared to classical neural networks?

## Architecture Onboarding

- Component map: Input (x,y,z) coordinates → Classical MLP layers (3-64-64-64-16 neurons) → Quantum circuit encoding layer → Variational quantum circuit → Measurement → Fully connected layer (16→4) → Filter/constraint splitter → Loss calculation

- Critical path: Forward pass through all layers → Constraint filtering → Loss calculation → Backward pass through quantum and classical components → Parameter update

- Design tradeoffs:
  - Quantum circuit depth vs. noise tolerance on real hardware
  - Number of classical layers vs. quantum circuit expressivity
  - Batch size (full vs. mini-batch) affecting training stability and speed
  - Optimizer choice (Adam vs. L-BFGS) for different training phases

- Failure signatures:
  - Loss plateaus or diverges → Check gradient flow through quantum layer
  - Solution collapses to zero → Examine physics loss term scaling or sampling strategy
  - Asymmetry in symmetric geometries → Verify quantum circuit encoding preserves symmetry
  - Poor transfer learning performance → Ensure geometric modifications are within valid transfer range

- First 3 experiments:
  1. Replace quantum layer with classical layer, keeping all other architecture identical; compare training curves and final accuracy
  2. Train classical PINN on the Y-mixer problem, analyze where solution degenerates to zero, and adjust physics loss weighting
  3. Implement transfer learning from α=30° to α=31°, then to α=32°, measuring improvement in convergence speed and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different data encoding strategies affect the expressivity and performance of quantum layers in PINNs?
- Basis in paper: [inferred] The paper mentions that different encoding strategies could be explored but does not investigate them systematically.
- Why unresolved: The paper only briefly mentions encoding strategies without conducting comparative analysis or theoretical investigation.
- What evidence would resolve it: Systematic comparison of different encoding schemes (amplitude, angle, basis encoding) on the same problem set with performance metrics.

### Open Question 2
- Question: What is the optimal architecture and depth of quantum circuits for PINNs in fluid dynamics problems?
- Basis in paper: [explicit] The paper mentions that using too many qubits and layer repetitions leads to the barren plateau problem, but doesn't explore optimal architectures.
- Why unresolved: The paper uses a specific VQC design inspired by previous work but doesn't explore the design space or provide guidelines for optimal architecture selection.
- What evidence would resolve it: Empirical studies varying circuit depth, qubit count, and layer structures across multiple fluid dynamics problems.

### Open Question 3
- Question: How do quantum noise and decoherence affect the accuracy of quantum PINNs on real quantum hardware?
- Basis in paper: [explicit] The paper tested on real QPUs (Lucy and Rigetti's Aspen-M-3) and found results greatly differed from simulator ones, but didn't investigate mitigation strategies.
- Why unresolved: The paper only reports that results differ from simulators without exploring noise mitigation techniques or error correction approaches.
- What evidence would resolve it: Comparative studies of quantum PINNs performance with and without noise mitigation techniques on real quantum hardware.

## Limitations
- Quantum circuit specifications are not fully disclosed, making exact reproduction difficult
- Performance claims based on a single benchmark geometry (Y-mixer), limiting generalizability
- Study doesn't address quantum noise effects or compare against other classical alternatives

## Confidence
- **High confidence**: Physics-informed neural network methodology and Navier-Stokes equation formulation are well-established and correctly implemented
- **Medium confidence**: Hybrid quantum-classical architecture design is reasonable, but performance claims depend on specific quantum circuit implementation details not fully disclosed
- **Low confidence**: Transfer learning results are promising but based on very limited geometric variations; the claimed generalization capability needs validation across a broader range of shape modifications

## Next Checks
1. Implement ablation studies comparing different quantum circuit architectures (depth, width, entangling strategy) while keeping classical components fixed to isolate quantum contribution to performance
2. Test transfer learning across a parameterized family of Y-mixer geometries with varying bifurcation angles (e.g., α=30°, 35°, 40°, 45°) to quantify the limits of successful knowledge transfer
3. Benchmark against classical attention-based PINNs on the same Y-mixer problem to determine whether quantum advantages persist when comparing against other state-of-the-art neural architectures