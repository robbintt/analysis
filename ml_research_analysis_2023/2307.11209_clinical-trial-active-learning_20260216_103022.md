---
ver: rpa2
title: Clinical Trial Active Learning
arxiv_id: '2307.11209'
source_url: https://arxiv.org/abs/2307.11209
tags:
- learning
- active
- data
- clinical
- prospective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces prospective active learning for clinical
  trials, addressing the non-i.i.d. nature of time-dependent medical data.
---

# Clinical Trial Active Learning

## Quick Facts
- arXiv ID: 2307.11209
- Source URL: https://arxiv.org/abs/2307.11209
- Reference count: 36
- Primary result: Prospective active learning achieves >90% accuracy by round 9 on OCT-based diabetic retinopathy detection

## Executive Summary
This paper addresses the challenge of applying active learning to clinical trial data, which exhibits non-i.i.d. temporal dependencies. Traditional retrospective active learning methods violate temporal independence by allowing samples from future visits to influence the model during training. The authors introduce prospective active learning, which conditions sample selection on visit time to enforce sequential querying that reflects real clinical data collection workflows. Applied to optical coherence tomography (OCT) images from diabetic retinopathy (DR) and diabetic macular edema (DME) clinical trials, the method outperforms traditional approaches across multiple query strategies while providing more realistic evaluation through dynamic test sets.

## Method Summary
The authors propose prospective active learning for clinical trials by conditioning sample selection on visit time, ensuring that models only query data from current or past visits to maintain temporal independence. The framework uses a Resnet18 architecture trained on the OLIVES dataset containing 78,189 OCT images from DR and DME clinical trials. The method employs 20 training rounds with 256 samples queried per round, using various query strategies (Entropy, Margin, Coreset, BADGE) and evaluating performance on both fixed test sets (2,000 images from excluded eyes) and dynamic test sets (80 images per round from excluded eyes). The approach addresses catastrophic forgetting through sequential learning and provides more clinically realistic evaluation by matching the prospective trial workflow.

## Key Results
- Prospective active learning achieves over 90% accuracy on fixed test set by round 9
- The method demonstrates improved stability and reduced generalization error compared to retrospective approaches
- Dynamic test set evaluation shows consistent performance on visit-specific data, validating clinical realism
- Prospective active learning outperforms retrospective methods across all tested query strategies (Entropy, Margin, Coreset, BADGE)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prospective active learning enforces the i.i.d. assumption by conditioning sample selection on visit time.
- Mechanism: By restricting the query strategy to select samples only from the current or past visits, the model avoids sampling future data that would violate temporal independence.
- Core assumption: The treatment effect creates a causal dependency between consecutive visits, so only sequential sampling reflects the true data distribution.
- Evidence anchors:
  - [abstract] "This paper introduces prospective active learning for clinical trials, addressing the non-i.i.d. nature of time-dependent medical data."
  - [section] "We propose prospective active learning to overcome the limitations present in traditional active learning methods and apply it to disease detection in optical coherence tomography (OCT) images, where we condition on the time an image was collected to enforce the i.i.d. assumption."
  - [corpus] Weak evidence - no direct mention of conditioning on visit time in related papers.
- Break condition: If treatment effects are negligible or visits are independent, conditioning on time provides no benefit.

### Mechanism 2
- Claim: Sequential querying improves generalization by learning the intervention function across visits.
- Mechanism: The model learns to distinguish between treatment types (DR vs DME) by observing the progression of disease states over time, capturing the intervention's effect.
- Core assumption: Each patient's disease state at visit d depends on the treatment received at visit d-1, creating a temporal dependency that the model can exploit.
- Evidence anchors:
  - [section] "We argue that if the data is observed in a sequential manner, the intervention function ùêº1 or ùêº2 will be able to be properly estimated."
  - [section] "We show that with this framework, we attain increased performance on a non-i.i.d. clinical trial dataset by exploiting the relationship between data at each time step."
  - [corpus] No direct evidence in related papers about intervention function learning.
- Break condition: If disease progression is random or treatment effects are minimal, sequential learning provides no advantage.

### Mechanism 3
- Claim: Dynamic test sets provide more realistic evaluation by matching the prospective clinical trial workflow.
- Mechanism: Evaluating on data from current and past visits mimics how clinicians assess treatment effects in real clinical trials, providing a more accurate measure of model performance.
- Core assumption: In prospective clinical trials, treatment effects are evaluated on current visit data, not on future data that hasn't been collected yet.
- Evidence anchors:
  - [abstract] "Dynamic test set results show consistent performance on visit-specific data, validating the method's clinical realism and effectiveness in handling temporal dependencies."
  - [section] "The dynamic test set ensures that both active learning setups are able to generalize to the current visit, while not losing important information collected about previous visits."
  - [corpus] Weak evidence - no related papers discuss dynamic test set evaluation in clinical trials.
- Break condition: If the evaluation metric prioritizes overall accuracy over temporal generalization, fixed test sets may be sufficient.

## Foundational Learning

- Concept: Non-i.i.d. data structures in time-series clinical data
  - Why needed here: Clinical trial data has temporal dependencies that violate the i.i.d. assumption required by most active learning algorithms.
  - Quick check question: Why can't we treat each OCT image as an independent sample in this clinical trial setting?

- Concept: Intervention function estimation in sequential data
  - Why needed here: The model needs to learn how treatments affect disease progression over time to make accurate classifications.
  - Quick check question: How does conditioning on visit time help the model learn the treatment intervention function?

- Concept: Catastrophic forgetting in incremental learning
  - Why needed here: As new data is added each round, the model must retain knowledge from previous visits while learning new patterns.
  - Quick check question: What metric does the paper use to measure catastrophic forgetting, and how does it correlate with test accuracy?

## Architecture Onboarding

- Component map: Data preprocessing ‚Üí Resnet18 model ‚Üí Active learning query strategy ‚Üí Training loop ‚Üí Evaluation (fixed and dynamic test sets)
- Critical path: Query strategy selection ‚Üí Model training ‚Üí Evaluation ‚Üí Budget check
- Design tradeoffs: Sequential vs. random visit querying, fixed vs. dynamic test sets, query size vs. labeling budget
- Failure signatures: High negative flip rate, inconsistent accuracy across rounds, poor performance on dynamic test set
- First 3 experiments:
  1. Implement prospective active learning with random query strategy and compare to retrospective baseline
  2. Test different query strategies (entropy, margin, coresest) in prospective setting
  3. Evaluate impact of query size on convergence and accuracy using the fixed test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different model architectures (beyond ResNet18) impact the performance of prospective active learning in clinical trials?
- Basis in paper: [explicit] The authors mention "Future directions for this type of research include...incorporating a healthy patient population into the datasets may provide additional insights into the performance of prospective active learning" and "the effect of different model architectures and query sizes can be further investigated."
- Why unresolved: The paper only evaluates ResNet18, leaving uncertainty about whether other architectures (e.g., Vision Transformers, EfficientNet) would yield different results in the prospective active learning setting.
- What evidence would resolve it: Systematic experiments comparing prospective active learning performance across multiple model architectures on the same clinical trial dataset.

### Open Question 2
- Question: How does the inclusion of healthy patient data affect the performance gap between prospective and retrospective active learning?
- Basis in paper: [explicit] The authors state "incorporating a healthy patient population into the datasets may provide additional insights into the performance of prospective active learning, where our current understanding expects prospective and retrospective active learning to provide similar results if minor changes are observed in the healthy cohort across visits."
- Why unresolved: The OLIVES dataset only contains diseased patients (DR/DME), so the authors cannot test whether healthy patients with minimal disease progression would reduce the advantage of prospective active learning.
- What evidence would resolve it: Experiments on a clinical trial dataset that includes both diseased and healthy patients, measuring whether the performance difference between prospective and retrospective active learning diminishes for healthy cohorts.

### Open Question 3
- Question: What is the optimal query size for prospective active learning in clinical trials given labeling budget constraints?
- Basis in paper: [inferred] The authors note "Our current query size selection takes into account a medical expert's limited labeling budget while still providing enough training data at each round such that the model is able to learn the fine-grained differences between DR and DME," but also suggest "the effect of different...query sizes can be further investigated."
- Why unresolved: The paper uses a fixed query size of 256 images per round, but the optimal size likely depends on factors like dataset size, labeling budget, and disease complexity that weren't systematically explored.
- What evidence would resolve it: Experiments varying query sizes across multiple clinical trial datasets to identify the size that maximizes performance while minimizing labeling burden.

### Open Question 4
- Question: How does prospective active learning perform on clinical trials with different temporal patterns (e.g., irregular visit intervals, long-term follow-up)?
- Basis in paper: [inferred] The authors analyze performance on OCT data with relatively regular visit patterns (average 16.6 visits over 114 weeks), but don't address how the method would handle clinical trials with irregular visit schedules or very long-term follow-up.
- Why unresolved: The OLIVES dataset has relatively consistent visit timing, so the impact of temporal irregularity on prospective active learning performance remains unknown.
- What evidence would resolve it: Application of prospective active learning to clinical trials with varying temporal structures, including irregular visit intervals and extended follow-up periods.

## Limitations
- Study relies on a single clinical dataset (OLIVES), limiting generalizability to other time-dependent medical domains
- Exact implementation details of how prospective differs from retrospective conditioning on visit time remain underspecified
- Fixed test set evaluation may overestimate performance since it includes data from all visits, unlike the more realistic dynamic test set

## Confidence

- **High confidence**: Claims about improved accuracy on fixed test set using prospective active learning, as these are directly supported by empirical results showing 90%+ accuracy by round 9.
- **Medium confidence**: Claims about dynamic test set superiority, as results show consistent performance but sample sizes per visit (80 images) may be too small for robust statistical conclusions.
- **Low confidence**: Claims about intervention function learning, as the paper provides theoretical justification but limited empirical evidence showing the model actually learns treatment effects rather than just temporal patterns.

## Next Checks

1. **Cross-domain validation**: Test prospective active learning on a different time-dependent medical dataset (e.g., longitudinal MRI scans) to verify generalizability beyond OCT images.

2. **Implementation verification**: Conduct ablation studies comparing exact implementations of prospective vs retrospective conditioning on visit time to quantify the specific contribution of temporal constraints.

3. **Statistical power analysis**: Perform significance testing on dynamic test set results with 80-sample rounds to determine if observed performance differences are statistically meaningful given the small sample sizes.