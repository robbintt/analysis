---
ver: rpa2
title: Matching Patients to Clinical Trials with Large Language Models
arxiv_id: '2307.15051'
source_url: https://arxiv.org/abs/2307.15051
tags:
- patient
- clinical
- trialgpt
- criteria
- trials
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TrialGPT uses large language models to match patients to clinical
  trials by predicting criterion-level eligibility with explanations and aggregating
  these predictions into trial-level scores. Evaluated on 184 patients and 18,238
  clinical trials, TrialGPT achieved 87.3% accuracy in criterion-level predictions
  with faithful explanations.
---

# Matching Patients to Clinical Trials with Large Language Models

## Quick Facts
- arXiv ID: 2307.15051
- Source URL: https://arxiv.org/abs/2307.15051
- Reference count: 0
- TrialGPT achieves 87.3% accuracy in criterion-level predictions with faithful explanations and reduces screening time by 42.6%

## Executive Summary
TrialGPT is an LLM-based system that matches patients to clinical trials by predicting eligibility at the criterion level with natural language explanations, then aggregating these predictions into trial-level scores. The system uses GPT-3.5-turbo with one-shot in-context learning, achieving 87.3% accuracy in criterion-level predictions and outperforming state-of-the-art models by 43.8% in trial ranking and exclusion. A user study demonstrated 42.6% reduction in screening time while maintaining high accuracy.

## Method Summary
TrialGPT employs GPT-3.5-turbo with in-context learning using exemplar annotations to predict patient eligibility for individual trial criteria with explanations. The system processes patient notes and trial eligibility criteria through prompts that include task descriptions and exemplars. Criterion-level predictions are aggregated using both linear methods and LLM-based scoring to generate trial-level relevance and eligibility scores. The model was evaluated on three public datasets (SIGIR 2016, TREC 2021 CT, TREC 2022 CT) containing 184 patients and 18,238 trials, using metrics including NDCG@10, P@10, and AUROC.

## Key Results
- 87.3% accuracy in criterion-level eligibility predictions with 84.6% correct explanations
- LLM-aggregated trial-level scores achieved NDCG@10 of 0.7421 and P@10 of 0.6705
- 42.6% reduction in screening time compared to manual methods in user study

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can accurately predict criterion-level patient eligibility with faithful explanations.
- Mechanism: LLMs use in-context learning with exemplar annotations to generate natural language explanations, identify relevant sentences in patient notes, and classify eligibility for each criterion.
- Core assumption: A single exemplar provides sufficient context for the LLM to generalize to new patient-trial pairs.
- Evidence anchors:
  - TrialGPT achieved 87.3% accuracy in criterion-level predictions with faithful explanations
  - Manual evaluations on 415 criterion-level predictions show high accuracy in explanations (84.6% correct), sentence location (94.9% precision), and eligibility prediction (0.86-0.84 accuracy)
- Break condition: If the exemplar is not representative of the patient-trial pairs or if the LLM lacks sufficient medical domain knowledge.

### Mechanism 2
- Claim: Aggregating criterion-level predictions using LLMs yields trial-level scores that correlate with expert annotations and outperform linear aggregation methods.
- Mechanism: LLMs process all criterion-level predictions for a trial and generate a single relevance score (0-100) and eligibility score (-100 to 100) that capture the overall relationship between patient and trial.
- Core assumption: LLM-aggregated scores capture complex relationships between criteria that simple linear aggregation cannot.
- Evidence anchors:
  - Trial-level scores showed high correlation with expert annotations and outperformed state-of-the-art models by 43.8% in ranking and excluding trials
  - LLM-aggregated relevance score achieved NDCG@10 of 0.7421 and P@10 of 0.6705, outperforming linear aggregations
- Break condition: If the LLM fails to properly weigh the importance of different criteria or if the linear aggregations are more appropriate for the specific trial population.

### Mechanism 3
- Claim: TrialGPT reduces screening time by 42.6% compared to manual methods while maintaining high accuracy.
- Mechanism: The explainable nature of LLM predictions allows human recruiters to quickly verify eligibility decisions rather than starting from scratch.
- Core assumption: The explanations provided by TrialGPT are sufficiently clear and accurate to reduce the cognitive load on human reviewers.
- Evidence anchors:
  - TrialGPT reduced screening time by 42.6% in a user study
  - The explanatory capabilities of LLMs are highlighted as valuable for integrating AI assistance into workflow
- Break condition: If the explanations are not clear enough or if the LLM makes too many errors that require extensive human verification.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: TrialGPT uses one-shot learning with exemplar annotations rather than requiring large training datasets
  - Quick check question: What is the difference between fine-tuning and in-context learning for LLMs?

- Concept: Natural Language Inference (NLI)
  - Why needed here: The eligibility prediction task maps to NLI by determining if patient information entails, contradicts, or is neutral with respect to trial criteria
  - Quick check question: How do the three eligibility labels (included, not included, no relevant information) map to NLI labels?

- Concept: Evaluation metrics for ranking systems
  - Why needed here: TrialGPT performance is measured using NDCG@10, P@10, and AUROC to assess ranking and exclusion capabilities
  - Quick check question: What is the difference between NDCG and P@K metrics in evaluating ranked lists?

## Architecture Onboarding

- Component map: Backbone LLM (GPT-3.5-turbo) -> Criterion-level prediction module -> Trial-level aggregation module -> Ranking/Exclusion output
- Critical path: Patient note and trial criteria -> LLM prompt with exemplar -> Criterion-level predictions -> Aggregated trial scores -> Ranked trial list
- Design tradeoffs: Single LLM vs. specialized models for each task; simple linear aggregation vs. LLM aggregation; general LLM vs. domain-specific model
- Failure signatures: Low criterion-level accuracy suggests exemplar or prompt issues; poor trial-level correlation suggests aggregation method problems; high screening time suggests explanation quality issues
- First 3 experiments:
  1. Test criterion-level predictions with different exemplars to find optimal in-context learning setup
  2. Compare LLM aggregation vs. linear aggregation methods on a held-out validation set
  3. Evaluate the impact of explanation quality on human screening time in a user study

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be improved to better handle medical domain-specific context dependency in clinical trial matching?
- Basis in paper: The authors note that TrialGPT makes errors due to limited medical context understanding capabilities, such as failing to capture implicit information from patient notes.
- Why unresolved: Current LLMs like GPT-3.5 lack the medical knowledge and context understanding needed for accurate clinical trial matching.
- What evidence would resolve it: Developing and testing LLMs with enhanced medical knowledge or integrating them with external medical knowledge bases to improve context understanding in clinical trial matching scenarios.

### Open Question 2
- Question: What is the optimal way to aggregate criterion-level predictions into trial-level scores for ranking and excluding clinical trials?
- Basis in paper: The authors explore different linear and LLM aggregation methods to combine criterion-level predictions, but the optimal combination strategy remains unclear.
- Why unresolved: While the authors find that combining linear and LLM aggregations yields the highest performance, the optimal feature combination for ranking and excluding trials is not definitively established.
- What evidence would resolve it: Conducting extensive experiments to determine the most effective aggregation method and feature combination for different clinical trial matching tasks.

### Open Question 3
- Question: How can LLMs be adapted to handle longer contexts, structured data, and multi-modal inputs for real-world clinical trial matching?
- Basis in paper: The authors note that their evaluation uses simplified patient-trial matching datasets and suggest that real-world scenarios would require handling longer contexts, structured data, and multi-modal inputs.
- Why unresolved: Current LLMs and evaluation methods may not be sufficient for the complexity of real-world clinical trial matching involving comprehensive patient information.
- What evidence would resolve it: Developing and testing LLM-based systems capable of processing longer contexts, structured data, and multi-modal inputs in real-world clinical trial matching settings.

## Limitations

- Model generalization to broader clinical populations and different trial types remains uncertain
- Explanatory quality verification relies on manual evaluation of a subset rather than scalable approaches
- Medical knowledge boundaries may limit accuracy as medical knowledge evolves over time

## Confidence

**High Confidence**:
- Criterion-level prediction accuracy of 87.3% is well-supported by manual evaluations
- LLM aggregation outperforming linear methods by 43.8% is demonstrated with statistical significance
- 42.6% reduction in screening time is validated through user study

**Medium Confidence**:
- Correlation between LLM-aggregated scores and expert annotations assumes experts represent ground truth
- Performance improvements may be dataset-specific given limited evaluation scope
- The generalizability of results to real-world clinical settings requires further validation

**Low Confidence**:
- Scalability to millions of trials and diverse patient populations is asserted but not demonstrated
- Long-term maintenance and updating of medical knowledge in the model is not addressed
- Integration into existing clinical workflows beyond the controlled user study remains unproven

## Next Checks

1. **Cross-Dataset Validation**: Test TrialGPT on additional clinical trial datasets not used in training or development to assess true generalization capabilities across different medical domains and trial types.

2. **Expert Clinician Validation**: Conduct a larger-scale user study with practicing clinicians across multiple institutions to evaluate real-world performance, workflow integration, and potential biases in diverse clinical settings.

3. **Temporal Knowledge Update Assessment**: Implement a framework to measure how well the model maintains accuracy as medical knowledge evolves, testing performance degradation over time and the effectiveness of knowledge updates.