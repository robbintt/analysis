---
ver: rpa2
title: Large Language Models Are Not Robust Multiple Choice Selectors
arxiv_id: '2309.03882'
source_url: https://arxiv.org/abs/2309.03882
tags:
- option
- bias
- selection
- pride
- debiasing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work reveals that modern LLMs exhibit an inherent selection
  bias in MCQs, making their performance not robust to option position changes. We
  identify option numbering (ID symbols like A/B/C/D) as a primary cause of selection
  bias.
---

# Large Language Models Are Not Robust Multiple Choice Selectors

## Quick Facts
- arXiv ID: 2309.03882
- Source URL: https://arxiv.org/abs/2309.03882
- Reference count: 24
- Modern LLMs exhibit inherent selection bias in MCQs due to option numbering, making performance not robust to option position changes.

## Executive Summary
This work reveals that modern LLMs exhibit an inherent selection bias in multiple-choice questions (MCQs), making their performance not robust to option position changes. The study identifies option numbering (ID symbols like A/B/C/D) as a primary cause of this bias. To address this issue, the authors propose PriDe, a label-free, inference-time debiasing method that estimates the model's prior preference for option IDs using a small number of test samples and applies this prior to debias subsequent samples. Experiments demonstrate that PriDe achieves superior debiasing effectiveness compared to strong baselines while maintaining negligible computational overhead and good cross-domain generalization.

## Method Summary
The PriDe method estimates a global prior over option IDs from a small subset of test samples through permutation-based probability decomposition. For K% of test samples, PriDe applies cyclic permutations to estimate per-sample priors using softmax over log probabilities. These priors are then averaged to obtain a global prior, which is applied to debias the remaining samples without labels during inference. The method is evaluated across 20 LLMs on three MCQ benchmarks (MMLU, ARC, CSQA) with both 0-shot and 5-shot settings, comparing against baselines like removing option IDs and using cyclic/full permutations.

## Key Results
- PriDe effectively reduces selection bias across 20 different LLMs, achieving superior debiasing effectiveness compared to strong baselines
- The method maintains or improves accuracy while reducing standard deviation of recalls (RStd) across option positions
- Estimated priors generalize well across different domains, demonstrating good cross-domain debiasing capabilities
- PriDe primarily corrects low-confidence predictions affected by selection bias, never changing a wrong prediction into another wrong prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit inherent selection bias due to token-level preferences for specific option ID symbols (A/B/C/D).
- Mechanism: The model assigns higher prior probability mass to certain ID tokens regardless of the actual content of the options.
- Core assumption: The bias originates from training data patterns where certain option IDs appear more frequently in correct answers.
- Evidence anchors:
  - [abstract]: "This work reveals that modern LLMs exhibit an inherent selection bias in MCQs, making their performance not robust to option position changes. We identify option numbering (ID symbols like A/B/C/D) as a primary cause of selection bias."
  - [section 2.4]: "We propose two hypotheses for the causes of selection bias: option numbering and option ordering. Option numbering indicates that the LLM leans towards the options numbered with specific ID symbols (such as A or C)."
  - [corpus]: Weak - only general mentions of selection bias without specific token-level evidence.
- Break condition: If the model's token distribution over ID symbols is uniform or content-dependent rather than inherently biased.

### Mechanism 2
- Claim: PriDe separates intrinsic prediction over option contents from prior distribution over option IDs through probability decomposition.
- Mechanism: PriDe estimates the prior preference for option IDs by permuting option contents on a small number of test samples, then uses this estimated prior to debias subsequent samples.
- Core assumption: The model's prior preference for option IDs is an inherent property that generalizes across different samples and domains.
- Evidence anchors:
  - [abstract]: "To mitigate selection bias, we propose PriDe, a label-free, inference-time debiasing method. PriDe estimates the model's prior preference for option IDs with a small number of test samples and uses the estimated prior to debias subsequent samples."
  - [section 3.3]: "Equation 8 then becomes: Pprior(di|q) = softmax(1/|I| ΣI∈I log Pobserved(di|q, xI)), ∀i ∈ {1, 2, ..., n}."
  - [corpus]: Weak - no direct evidence about the decomposition mechanism in related work.
- Break condition: If the prior preference does not generalize across domains or requires too many samples for accurate estimation.

### Mechanism 3
- Claim: Debiasing improves model performance by correcting low-confidence predictions affected by selection bias.
- Mechanism: PriDe primarily affects predictions where the model has moderate or low confidence, shifting wrong predictions toward correct ones.
- Core assumption: Selection bias disproportionately impacts low-confidence predictions, making them more susceptible to correction.
- Evidence anchors:
  - [section 4.2]: "PriDe debiasing never changes a wrong prediction into another wrong prediction. The ratios of changes from wrong to correct always surpass those from correct to wrong."
  - [section 4.2]: "On those samples with predictions changed by debiasing, the model usually holds moderate or low confidence about its original predictions."
  - [corpus]: Weak - no evidence about performance improvement through debiasing in related work.
- Break condition: If the model's confidence distribution is uniform across all predictions or if debiasing consistently harms high-confidence predictions.

## Foundational Learning

- Concept: Probability decomposition and prior estimation
  - Why needed here: Understanding how PriDe separates intrinsic prediction from prior bias requires knowledge of probability theory and Bayesian inference.
  - Quick check question: Can you explain how PriDe uses permutation to estimate the prior distribution over option IDs?

- Concept: Cross-domain generalization
  - Why needed here: The paper claims that estimated priors generalize across different domains, which requires understanding of domain adaptation and transfer learning.
  - Quick check question: Why would selection bias patterns be similar across different domains within the same LLM?

- Concept: Evaluation metrics for bias
  - Why needed here: Understanding why recall balance is used instead of simple counting requires knowledge of evaluation methodology and bias measurement.
  - Quick check question: How does measuring recall balance help avoid false judgments of bias due to label imbalance?

## Architecture Onboarding

- Component map: Input processing -> Probability estimation -> Permutation engine -> Prior estimation -> Debiasing module -> Output layer

- Critical path:
  1. Parse input MCQ
  2. Generate permutations for K estimation samples
  3. Calculate observed probabilities for each permutation
  4. Estimate priors using softmax over log probabilities
  5. Average priors across estimation samples
  6. Apply priors to remaining samples
  7. Output final predictions

- Design tradeoffs:
  - Estimation budget (K) vs. debiasing effectiveness
  - Full permutation vs. cyclic permutation for computational efficiency
  - Sample size for prior estimation vs. inference time overhead
  - Generalization across domains vs. domain-specific priors

- Failure signatures:
  - Poor debiasing effectiveness: Priors don't generalize or model doesn't exhibit selection bias
  - Performance degradation: Debiasing harms correct predictions or introduces new biases
  - High computational cost: K is too large or full permutations are used unnecessarily
  - Unstable priors: High variance in estimated priors across different estimation runs

- First 3 experiments:
  1. Verify selection bias exists: Measure recall balance across option positions for various models
  2. Test permutation effectiveness: Compare debiasing results using full vs. cyclic permutations
  3. Validate cross-domain generalization: Estimate priors on one domain and test on others

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the inherent selection bias in LLMs arise primarily from the training data distribution, model architecture, or the fine-tuning process?
- Basis in paper: [explicit] The paper states that selection bias is prevalent across various LLMs and varies with model families and sizes, suggesting complex interactions among training data, model capacity, and other potential factors.
- Why unresolved: The paper identifies that scaling up model size reduces selection bias for foundation models but not for fine-tuned models, indicating that the source of bias is not straightforward.
- What evidence would resolve it: Systematic ablation studies controlling for training data, architecture, and fine-tuning could isolate the primary contributor to selection bias.

### Open Question 2
- Question: How does the calibration of LLMs affect the alignment between estimated prior probabilities and empirical selection bias?
- Basis in paper: [inferred] The paper notes that the estimated prior probabilities may not perfectly align with the frequency-based recall balance, suggesting a relationship with model calibration.
- Why unresolved: The paper conjectures that ideal alignment requires well-calibrated models but does not empirically investigate this relationship.
- What evidence would resolve it: Experiments measuring LLM calibration and comparing it to the alignment of estimated priors with observed selection bias would clarify this relationship.

### Open Question 3
- Question: What is the impact of different prompt formats and in-context examples on selection bias in LLMs?
- Basis in paper: [explicit] The paper mentions that in-context examples can alleviate but may also alter selection bias, and that removing option IDs impairs performance under the 0-shot setting.
- Why unresolved: The paper does not thoroughly explore how different prompt formats or in-context example strategies influence selection bias.
- What evidence would resolve it: Controlled experiments varying prompt formats and in-context examples while measuring selection bias would reveal their impact.

## Limitations

- The paper focuses primarily on option ID symbols as the source of bias, without thoroughly investigating other contributing factors like option ordering or content patterns.
- PriDe requires access to model output probabilities, which may not be available for all LLM APIs, particularly closed-source models.
- Cross-domain generalization claims are based on limited domain pairs and may not hold for more diverse or specialized domains.

## Confidence

- **High Confidence**: The existence of selection bias in LLMs (measured through recall balance) - well-supported by extensive experiments across 20 models and three benchmarks.
- **Medium Confidence**: PriDe's effectiveness as a debiasing method - strong empirical results but relies on the assumption that priors generalize across domains.
- **Medium Confidence**: Option numbering as the primary cause of selection bias - plausible mechanism but not definitively proven against alternative explanations.

## Next Checks

1. **Mechanism Validation**: Conduct ablation studies to isolate whether option numbering is indeed the primary source of bias by testing models on MCQs with and without option ID symbols while controlling for other variables.

2. **Prior Estimation Robustness**: Evaluate PriDe's performance when estimated priors are based on different sample sizes and distributions to determine the minimum effective estimation budget and test stability across different estimation runs.

3. **Cross-Domain Generalization**: Test PriDe's debiasing effectiveness when priors are estimated on one domain and applied to significantly different domains (e.g., estimating on science questions and applying to humanities questions) to validate the claimed generalization capability.