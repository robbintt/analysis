---
ver: rpa2
title: End-to-End Speech Recognition Contextualization with Large Language Models
arxiv_id: '2309.10917'
source_url: https://arxiv.org/abs/2309.10917
tags:
- speech
- context
- recognition
- contextual
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Speech LLaMA, a novel decoder-only architecture
  for speech recognition that incorporates large language models (LLMs) to leverage
  contextual information. The approach treats speech recognition as a mixed-modal
  language modeling task, where audio features and optional text tokens for context
  are provided to the pretrained LLM to complete transcriptions in a decoder-only
  fashion.
---

# End-to-End Speech Recognition Contextualization with Large Language Models

## Quick Facts
- arXiv ID: 2309.10917
- Source URL: https://arxiv.org/abs/2309.10917
- Reference count: 0
- Key outcome: 7.5% overall WER improvement and 17% Rare WER improvement over baseline RNN-T system

## Executive Summary
This paper introduces Speech LLaMA, a novel decoder-only architecture for speech recognition that leverages large language models (LLMs) to incorporate contextual information. The approach treats speech recognition as a mixed-modal language modeling task where audio features and optional text tokens for context are provided to a pretrained LLM to complete transcriptions. By freezing LLM weights and fine-tuning only the audio encoder and LoRa adapters, the method achieves significant improvements in word error rate (WER) when additional textual context is provided, particularly for rare words.

## Method Summary
Speech LLaMA processes 80-dimensional log Mel features through a conformer-based audio encoder, then concatenates the resulting audio tokens with contextual text tokens (up to 50 tokens from video titles and descriptions) and passes them through a frozen 7B LLaMA decoder. The model uses causal masking only on transcription tokens while allowing full attention for context and audio tokens. Training involves 200k updates with Adam optimizer, mixed precision, and SpecAugment, with cross-entropy loss computed only on transcription tokens. The architecture adds approximately 30 million trainable parameters via LoRa adapters while keeping the remaining 6.7 billion LLM parameters frozen.

## Key Results
- 6% WER reduction when additional textual context is provided
- 7.5% WER improvement overall against baseline RNN-T system
- 17% WER improvement on rare words compared to baseline
- Achieves better performance than RNN-T trained on 4M hours despite training on only 200k hours

## Why This Works (Mechanism)

### Mechanism 1
The model learns to copy rare words from contextual prompts because the cross-entropy loss is only computed on spoken tokens, not contextual ones. During training, the model sees full textual context plus audio tokens, but the loss is masked for context tokens. This incentivizes the model to treat context as a reliable hint rather than being penalized for copying words from context. If context contains correct rare words, the model can learn to copy them without punishment.

### Mechanism 2
Using a frozen pretrained LLM decoder allows the model to leverage pre-learned linguistic knowledge, reducing the need for large-scale training data. The pretrained LLaMA LLM has already learned rich linguistic patterns from vast text data. By freezing its weights and only fine-tuning the audio encoder and LoRa adapters, the model can apply this linguistic knowledge to decode speech into text, effectively transferring language understanding from text to speech.

### Mechanism 3
The decoder-only architecture with causal masking on transcription tokens allows the model to use full context for reasoning while preventing information leakage during generation. The model processes audio and context tokens with full attention (no causal masking), allowing it to fully contextualize the input. However, when generating transcription tokens, causal masking is applied so each prediction only depends on previous tokens. This setup enables the model to use all available information for reasoning but generates text sequentially.

## Foundational Learning

- **Pretrained Language Models and Transfer Learning**
  - Why needed here: Understanding how a pretrained LLM can be adapted for speech recognition by freezing most weights and only fine-tuning a small portion
  - Quick check question: Why is it beneficial to keep the LLM weights frozen and only fine-tune adapters during training?

- **Attention Mechanisms and Masking**
  - Why needed here: The model uses self-attention with causal masking for transcription tokens to prevent future information leakage, while allowing full attention for context and audio tokens
  - Quick check question: What is the purpose of applying causal masking only to transcription tokens and not to context or audio tokens?

- **Automatic Speech Recognition (ASR) Metrics**
  - Why needed here: The paper evaluates performance using Word Error Rate (WER) and Rare WER, which require understanding how ASR systems are assessed
  - Quick check question: How does Rare WER differ from overall WER, and why is it important for evaluating contextual biasing?

## Architecture Onboarding

- **Component map**: Audio features -> Audio Encoder -> Audio tokens -> [concat with Context tokens] -> LLM Decoder -> Transcription tokens

- **Critical path**: 1) Audio features → Audio Encoder → Audio tokens 2) Context text → Tokenizer → Context tokens 3) Concatenate audio and context tokens with <bos> 4) LLM decoder processes tokens with causal masking on transcription 5) Loss computed only on transcription tokens

- **Design tradeoffs**: Freezing LLM weights reduces trainable parameters but relies on LLM's linguistic knowledge being useful for ASR; using LoRa adapters instead of full fine-tuning limits adaptation but keeps most parameters frozen; limiting context to 50 tokens reduces computational cost but may exclude relevant information

- **Failure signatures**: High WER when context is irrelevant or incorrect (model copies wrong words); degraded performance if audio encoder is not well-pretrained or adapted; scalability issues with long contexts due to quadratic attention

- **First 3 experiments**: 1) Train without context to establish baseline WER 2) Train with context but evaluate without context to check generalization 3) Perturb context (random words, ground truth, respellings) during evaluation to study context sensitivity

## Open Questions the Paper Calls Out
1. How does the Speech LLaMA model perform when the context size exceeds 50 tokens, and what is the optimal context length for maximum WER improvement?

2. Can the Speech LLaMA model maintain its performance when the contextual information is unrelated or noisy, and how does it handle such scenarios?

3. How does the Speech LLaMA model's performance compare to other state-of-the-art speech recognition models that incorporate contextual information, such as Whisper?

## Limitations
- Uses in-house dataset from Facebook and Instagram videos, making independent verification difficult
- Comparison with RNN-T trained on 4M hours (25x more data) raises fairness questions despite acknowledging the difference
- Model's robustness to noisy or adversarial contexts remains unclear beyond controlled perturbations

## Confidence
- **High Confidence**: Technical implementation details are well-specified and reproducible; core contribution of treating ASR as mixed-modal language modeling is sound
- **Medium Confidence**: Empirical results showing WER improvements are promising but require independent verification; efficiency claims need more context about data quality
- **Low Confidence**: Claims about "unlocking contextualized speech recognition" for pretrained LLMs while maintaining text-only functionality are somewhat overstated

## Next Checks
1. Evaluate model's performance with various types of context perturbations including irrelevant context, contradictory information, and missing context to assess robustness

2. Test the model on publicly available datasets (like LibriSpeech or Common Voice) with appropriate contextual information to verify generalization to different domains

3. Conduct detailed analysis of computational requirements comparing Speech LLaMA with baseline RNN-T system, including inference latency, memory usage, and training efficiency