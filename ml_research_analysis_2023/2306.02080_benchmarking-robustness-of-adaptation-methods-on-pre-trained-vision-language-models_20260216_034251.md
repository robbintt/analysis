---
ver: rpa2
title: Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language
  Models
arxiv_id: '2306.02080'
source_url: https://arxiv.org/abs/2306.02080
tags:
- robustness
- adaptation
- methods
- arxiv
- corruptions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a large-scale robustness benchmark for adaptation
  methods on pre-trained vision-language models, which is critical for safety-critical
  applications. The benchmark includes 96 visual and 87 textual corruptions applied
  to 4 VL datasets (VQAv2, GQA, NLVR2, MSCOCO Caption).
---

# Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models

## Quick Facts
- **arXiv ID**: 2306.02080
- **Source URL**: https://arxiv.org/abs/2306.02080
- **Reference count**: 40
- **Primary result**: Adapter-based methods like Hyperformer achieve better robustness with comparable clean performance on pre-trained VL models

## Executive Summary
This paper introduces a large-scale robustness benchmark for adaptation methods on pre-trained vision-language models, evaluating 11 adaptation methods across 4 VL datasets under 96 visual and 87 textual corruptions. The study reveals that adaptation methods are more sensitive to text corruptions than visual corruptions, full fine-tuning does not consistently provide the highest robustness, and increasing adaptation data or parameters can actually decrease robustness. Adapter-based methods like Hyperformer can achieve better robustness while maintaining comparable clean performance. The benchmark, code, and dataset are publicly available.

## Method Summary
The study evaluates 11 adaptation methods on pre-trained CLIP-BART (T5) models across four VL datasets (VQAv2, GQA, NLVR2, MSCOCO Caption). Methods include full fine-tuning, LoRA, soft prompt, and adapter-based approaches (Adapter, Hyperformer, Compacter). The evaluation uses 96 visual and 87 textual corruptions from ImageNet-C and NLP robustness toolkits. Relative robustness (RR) is measured as performance decrease on corrupted data compared to clean data, using accuracy for VQA/Visual Reasoning tasks and CIDEr for image captioning.

## Key Results
- Adaptation methods are more sensitive to text corruptions than visual corruptions
- Full fine-tuning does not consistently provide the highest robustness; adapter-based methods can achieve better robustness with comparable clean performance
- Increasing adaptation data or parameters does not guarantee enhanced robustness and can result in lower robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapter-based methods like Hyperformer can achieve better robustness with comparable clean performance.
- Mechanism: Adapters insert small trainable modules into intermediate layers of pre-trained models, modifying internal representation without updating the full model. This targeted modification preserves general pre-trained knowledge while adapting to task-specific patterns, potentially reducing overfitting to clean data and improving generalization under corruptions.
- Core assumption: The original pre-trained weights contain robust, generalizable features maintained when freezing them and only updating adapter parameters.
- Evidence anchors:
  - [abstract] "Full fine-tuning does not consistently provide the highest robustness; instead, adapters can achieve better robustness with comparable clean performance."
  - [section] "Adapter [29] consists of a pair of downsampling and upsampling layers as well as a residual connection."
- Break condition: If pre-trained weights are not robust themselves or corruptions affect frozen layers significantly, the adapter cannot compensate.

### Mechanism 2
- Claim: Increasing adaptation data or parameters does not guarantee enhanced robustness; instead it results in even lower robustness.
- Mechanism: More adaptation data and parameters can lead to overfitting on the clean training distribution, making the model less able to generalize under distributional shifts. The adaptation process becomes too specialized to the training domain, reducing robustness.
- Core assumption: Robustness is harmed when adaptation shifts the model too far from the original pre-trained distribution.
- Evidence anchors:
  - [abstract] "our findings indicate that increasing the number of adaptation data and parameters does not guarantee enhanced robustness; instead it results in even lower robustness."
  - [section] "We also investigate the robustness of other adaptation methods... All lines present a steady declining tendency, which indicates that increasing the size of the adaptation data does not consistently enhance relative robustness."
- Break condition: If adaptation data is carefully curated to be diverse and representative of expected distribution shifts, more data could improve robustness.

### Mechanism 3
- Claim: Adaptation methods are more sensitive to text corruptions than visual corruptions.
- Mechanism: VL models rely heavily on language representations for many tasks. Corruptions at the character or word level directly affect semantic processing and understanding, which is more disruptive than image-level visual distortions that may still preserve high-level features recognizable by the vision encoder.
- Core assumption: Language understanding is more brittle to small perturbations than visual feature extraction in this architecture.
- Evidence anchors:
  - [abstract] "Adaptation methods are more sensitive to text corruptions than visual corruptions."
  - [section] "Our experimental findings suggest a potential vulnerability of adaptation methods on multimodal VL models to text corruptions, particularly those at the character level."
- Break condition: If visual corruptions are severe enough to destroy the image content, or if the model relies more on visual cues than language, sensitivity may shift.

## Foundational Learning

- Concept: Vision-language pre-training and adaptation
  - Why needed here: The paper evaluates robustness of different adaptation methods on pre-trained VL models. Understanding how these models work and how adaptation modifies them is essential.
  - Quick check question: What is the main difference between full fine-tuning and adapter-based adaptation in terms of parameters updated?

- Concept: Distribution shifts and robustness evaluation
  - Why needed here: The benchmark measures how adaptation methods perform under corrupted data versus clean data. Knowing how robustness is defined and measured is critical to interpreting results.
  - Quick check question: How is relative robustness (RR) computed in this study?

- Concept: Multimodal corruption types and their effects
  - Why needed here: The study applies both visual and textual corruptions. Understanding the types of corruptions and their expected impact on VL models helps in reasoning about the results.
  - Quick check question: Why might character-level text corruptions be more harmful than sentence-level corruptions?

## Architecture Onboarding

- Component map: Pre-trained VL model (CLIP-BART) → Adaptation method (e.g., Adapter, LoRA, Prompt) → Downstream task dataset (VQAv2, GQA, NLVR2, MSCOCO Caption) → Corruption application (visual and textual) → Evaluation (accuracy/CIDEr and relative robustness)
- Critical path: Pre-trained model → Apply adaptation method on clean data → Evaluate on corrupted data → Compare robustness across methods
- Design tradeoffs: Full fine-tuning offers maximum flexibility but is computationally expensive and may overfit; adapter-based methods are efficient but may be limited by the fixed pre-trained backbone; prompt-tuning is very lightweight but may underperform in complex tasks
- Failure signatures: Low relative robustness despite high clean performance suggests overfitting; poor clean performance indicates adaptation is ineffective; large variance across corruption types suggests sensitivity to specific modalities
- First 3 experiments:
  1. Run full fine-tuning on VQAv2 with clean data and evaluate accuracy
  2. Apply the same full fine-tuning model to corrupted VQAv2 data and compute RR
  3. Repeat steps 1-2 with a single adapter method and compare RR and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does increasing the number of adaptation examples sometimes decrease robustness in vision-language models?
- Basis in paper: [explicit] The paper found that increasing adaptation data size does not guarantee enhanced robustness and may lead to decreased robustness
- Why unresolved: The paper observes this counterintuitive phenomenon but does not provide a mechanistic explanation for why more training data could harm robustness
- What evidence would resolve it: Experiments isolating the effects of data distribution shifts between training and adaptation sets, or analysis of how different adaptation methods handle out-of-distribution examples differently

### Open Question 2
- Question: What are the fundamental differences in how text corruptions versus visual corruptions affect the robustness of adaptation methods?
- Basis in paper: [explicit] The paper found that adaptation methods are more sensitive to text corruptions than visual corruptions
- Why unresolved: The paper identifies the asymmetry but doesn't explore the underlying reasons why language processing components are more vulnerable to perturbations than vision components
- What evidence would resolve it: Comparative analysis of attention patterns, gradient magnitudes, or feature representations under different corruption types

### Open Question 3
- Question: Why do certain vision-language tasks (like GQA) show fundamentally different robustness patterns compared to others when using the same adaptation methods?
- Basis in paper: [explicit] The paper found that GQA shows unique robustness characteristics where single adapters perform worse than full fine-tuning, unlike other tasks
- Why unresolved: The paper notes this discrepancy but doesn't explain the task-specific factors that make GQA more sensitive to adaptation method choices
- What evidence would resolve it: Detailed analysis of the reasoning complexity, input-output relationships, or information requirements that distinguish GQA from other VL tasks

## Limitations

- The study focuses on a single pre-trained VL architecture (CLIP-BART), limiting generalizability to other architectures like Flamingo or LLaVA
- Robustness evaluation uses synthetically generated corruptions which may not fully represent real-world distribution shifts in safety-critical applications
- The study does not investigate mechanisms by which corruptions affect frozen pre-trained layers versus adapter parameters

## Confidence

- **High confidence**: Adaptation methods are more sensitive to text corruptions than visual corruptions (supported by systematic experiments across multiple datasets and corruption types)
- **Medium confidence**: Adapter-based methods like Hyperformer can achieve better robustness with comparable clean performance (consistent results across datasets, but dependent on specific implementation details)
- **Medium confidence**: Increasing adaptation data or parameters does not guarantee enhanced robustness (clear trend in experimental results, though the exact relationship may depend on dataset/task complexity)

## Next Checks

1. Replicate the robustness comparison using a different VL architecture (e.g., CLIP-ViT) to verify that adapter-based methods consistently outperform full fine-tuning across architectures

2. Test the robustness of these adaptation methods on real-world corrupted data from deployed applications rather than synthetic ImageNet-C style corruptions

3. Conduct ablation studies freezing different subsets of the pre-trained model layers to determine which components contribute most to robustness degradation under corruptions