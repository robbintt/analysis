---
ver: rpa2
title: 'QuantProb: Generalizing Probabilities along with Predictions for a Pre-trained
  Classifier'
arxiv_id: '2304.12766'
source_url: https://arxiv.org/abs/2304.12766
tags:
- quantile
- representations
- classi
- data
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the unreliability of class probabilities in
  deep learning models, particularly under distribution shifts. The authors propose
  Quantile Probabilities (QuantProb), a method that generates robust probabilities
  by leveraging quantile representations.
---

# QuantProb: Generalizing Probabilities along with Predictions for a Pre-trained Classifier

## Quick Facts
- arXiv ID: 2304.12766
- Source URL: https://arxiv.org/abs/2304.12766
- Reference count: 40
- Key outcome: QuantProb maintains constant calibration errors across distortions by leveraging quantile representations, outperforming traditional post-hoc calibration methods.

## Executive Summary
This paper addresses the critical issue of unreliable class probabilities in deep learning models under distribution shifts. The authors propose Quantile Probabilities (QuantProb), a method that generates robust probabilities by decoupling quantile representation construction from the loss function. This allows computing quantile probabilities from any pre-trained classifier by establishing a duality property between quantiles and probabilities. QuantProb demonstrates superior performance in preserving calibration errors across distortions and improves out-of-distribution detection compared to traditional calibration techniques.

## Method Summary
QuantProb works by constructing quantile representations from any pre-trained classifier without retraining. The method leverages a duality property between quantiles and probabilities, allowing the base classifier's output probabilities to be interpreted as the median quantile representation. Quantile representations at different levels are generated by thresholding the base classifier's predictions to create modified labels. These representations are then used to compute quantile probabilities that generalize better across small distortions. The method is validated on CIFAR10 and SVHN datasets for both calibration error estimation and out-of-distribution detection tasks.

## Key Results
- QuantProb maintains constant calibration errors across distortions, unlike traditional post-hoc calibration methods which increase errors under shifts
- Outperforms traditional calibration techniques in preserving calibration errors across CIFAR10C distortions
- Demonstrates improved OOD detection performance using quantile representations with Local Outlier Factor

## Why This Works (Mechanism)

### Mechanism 1
Quantile representations generalize better across small distortions than softmax probabilities because the duality property between quantiles and probabilities allows constructing quantile representations for any pre-trained classifier. This enables probabilities to remain stable under distributional shifts. The core assumption is that the base classifier's predictions can be thresholded to create modified labels at different quantile levels, preserving essential information for classification.

### Mechanism 2
Decoupling quantile representation construction from the loss function allows application to any pre-trained classifier by leveraging the duality between quantiles and probabilities. This enables assigning an arbitrary classifier at the median quantile and generating the full spectrum of quantile representations without retraining the base model. The core assumption is that the base classifier's output probabilities can be interpreted as the median quantile representation, enabling construction of other quantiles through thresholding.

### Mechanism 3
Quantile representations capture more relevant information for classification than raw features or single-classifier probabilities by encoding "aspects of the feature space that the classifier uses for classification." This information is more informative than probabilities but less than the entire feature space. The core assumption is that the base classifier's decision boundaries at different quantile levels capture discriminative aspects of the data distribution relevant to classification.

## Foundational Learning

- Concept: Quantile regression and its loss function (pinball loss)
  - Why needed here: The paper builds on quantile regression to create robust probability estimates
  - Quick check question: What is the pinball loss, and how does it differ from mean squared error?

- Concept: Calibration of probabilistic classifiers
  - Why needed here: The paper addresses calibration errors and proposes a method that maintains calibration across distortions
  - Quick check question: What is the Expected Calibration Error (ECE), and how is it computed?

- Concept: Out-of-distribution (OOD) detection methods
  - Why needed here: The paper evaluates its method on OOD detection tasks
  - Quick check question: How does Local Outlier Factor (LOF) work for OOD detection?

## Architecture Onboarding

- Component map: Base classifier -> Quantile representation generator -> OOD detector/Calibration evaluator
- Critical path: 1) Train base classifier on in-distribution data; 2) Generate quantile representations for the base classifier; 3) Use quantile representations for OOD detection or calibration evaluation
- Design tradeoffs:
  - Computational cost: Generating quantile representations for many quantiles can be expensive; interpolation can reduce this
  - Representation quality: Using logits instead of probabilities can improve numerical stability but may affect interpretability
  - Multi-class handling: One-vs-rest approach increases complexity but allows application to multi-class problems
- Failure signatures:
  - OOD detection fails: Quantile representations do not capture discriminative information or are not robust to noise
  - Calibration does not improve: Base classifier is too noisy or the quantile representation construction is flawed
  - High computational cost: Too many quantiles or inefficient implementation
- First 3 experiments:
  1. Verify that quantile representations preserve calibration errors across distortions by comparing ECE on clean and corrupted datasets
  2. Test OOD detection performance using quantile representations versus base classifier outputs on CIFAR10 + SVHN with LSUN/iSUN as OOD datasets
  3. Check that cross-correlations between features computed from quantile representations match those from raw features

## Open Questions the Paper Calls Out

### Open Question 1
Can quantile representations accurately estimate the calibration error of a model under various types of distribution shifts beyond those tested in the paper? The paper only tested calibration error robustness on the CIFAR10C dataset with 15 types of common corruptions. Testing quantile representations on a wider range of distribution shift types, including adversarial attacks, semantic shifts, and real-world dataset shifts, would determine the general applicability of the method.

### Open Question 2
Is it possible to design a loss function that, when used to train a classifier, inherently produces calibrated probabilities without requiring post-hoc correction methods? The paper proposes quantile representations as a post-hoc solution but does not explore the possibility of designing a loss function that directly optimizes for calibrated probabilities during training.

### Open Question 3
Can quantile representations be effectively used to detect and correct for adversarial examples in addition to out-of-distribution samples? While the paper demonstrates use for OOD detection, it does not investigate whether quantile representations can detect adversarial examples, which are designed to be misclassified by the model.

## Limitations
- Scalability concerns for large-scale datasets and complex base classifiers beyond Resnet34 and Densenet
- Limited validation of performance under large distribution shifts or adversarial attacks
- Computational overhead of generating and storing quantile representations for many quantiles may limit practical applicability

## Confidence
- Medium: The theoretical framework linking quantiles to probabilities is sound, and experimental results on CIFAR10 and SVHN support the claims of improved calibration and OOD detection. However, the lack of ablation studies on the impact of the number of quantiles and the absence of comparisons with state-of-the-art OOD detection methods reduce confidence in the method's general superiority.

## Next Checks
1. **Scalability Test**: Evaluate QuantProb on larger datasets (e.g., ImageNet) to assess computational feasibility and performance retention
2. **Adversarial Robustness**: Test the method's calibration and OOD detection performance under adversarial perturbations to verify robustness claims
3. **Hyperparameter Sensitivity**: Conduct ablation studies to determine the optimal number of quantiles and assess the impact of interpolation and weighting schemes on performance