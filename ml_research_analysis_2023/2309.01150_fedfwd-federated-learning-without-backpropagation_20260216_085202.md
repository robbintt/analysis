---
ver: rpa2
title: 'FedFwd: Federated Learning without Backpropagation'
arxiv_id: '2309.01150'
source_url: https://arxiv.org/abs/2309.01150
tags:
- fedfwd
- learning
- layer
- fedavg
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents FedFwd, a federated learning algorithm that
  replaces the backpropagation procedure with the Forward-Forward algorithm by Hinton
  (2022). This change aims to reduce computational burden on clients by eliminating
  the need to store intermediate activations and performing layer-wise local updates.
---

# FedFwd: Federated Learning without Backpropagation

## Quick Facts
- arXiv ID: 2309.01150
- Source URL: https://arxiv.org/abs/2309.01150
- Reference count: 40
- Key outcome: FedFwd achieves comparable test accuracy to FedAvg on MNIST under i.i.d. settings (96.78% vs 98.25%) but shows lower performance under non-i.i.d. settings (92.57% vs 97.46%)

## Executive Summary
This paper introduces FedFwd, a federated learning algorithm that replaces the traditional backpropagation procedure with the Forward-Forward algorithm by Hinton (2022). The approach aims to reduce computational burden on clients by eliminating the need to store intermediate activations and performing layer-wise local updates. The authors evaluate FedFwd on MNIST and CIFAR-10 datasets using multi-layer perceptron architectures and compare it to the standard FedAvg algorithm. FedFwd achieves comparable test accuracy to FedAvg on MNIST under i.i.d. settings but shows lower performance under non-i.i.d. settings.

## Method Summary
FedFwd replaces the backpropagation procedure in federated learning with the Forward-Forward algorithm, which uses two forward passes instead of a forward-backward pass. This eliminates the need to store intermediate activations during local training. The algorithm performs layer-wise local updates where each layer is optimized independently using a goodness function. The goodness function seeks to maximize positive samples exceeding a threshold while minimizing negative samples beneath this threshold. The server-side aggregation remains the same as FedAvg, with clients sending updated models for averaging.

## Key Results
- FedFwd achieves 96.78% test accuracy on MNIST under i.i.d. settings compared to FedAvg's 98.25%
- Under non-i.i.d. settings, FedFwd shows lower performance at 92.57% vs FedAvg's 97.46% on MNIST
- Training time of FedFwd is initially slower than FedAvg with small batch sizes but becomes comparable as batch size increases
- A modification to the objective function improves training stability and convergence speed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FedFwd reduces client-side memory usage by eliminating the need to store intermediate activations during local training.
- Mechanism: The Forward-Forward algorithm uses two forward passes instead of a forward-backward pass, avoiding the need to retain activations for gradient computation.
- Core assumption: Each layer can be updated independently using only the current input and output without relying on downstream gradients.
- Evidence anchors:
  - [abstract] "FedFwd can reduce a significant amount of computations for updating parameters by performing layer-wise local updates, and therefore, there is no need to store all intermediate activation values during training."
  - [section] "The Forward-Forward algorithm, a greedy layer-wise learning technique, adopts an alternative approach to the traditional backpropagation’s one forward and one backward pass by employing two forward passes."
  - [corpus] Weak evidence—no direct citations on activation storage reduction in FedFwd context.
- Break condition: If layer dependencies or skip connections require intermediate states for correct gradient flow, the independence assumption fails.

### Mechanism 2
- Claim: Layer-wise local updates in FedFwd enable more granular, potentially faster convergence on individual clients.
- Mechanism: Each layer is optimized greedily based on a local "goodness" metric, allowing isolated layer training without waiting for full network backpropagation.
- Core assumption: Layer-level optimization with goodness functions can approximate full-network gradient descent when aggregated across rounds.
- Evidence anchors:
  - [section] "The Forward-Forward algorithm, a greedy layer-wise learning technique, adopts an alternative approach... by employing two forward passes... Each layer seeks to maximize the goodness of positive samples exceeding a threshold, θ, while minimizing the goodness of negative samples beneath this threshold."
  - [abstract] "performing layer-wise local updates"
  - [corpus] No direct evidence—corpus neighbors focus on heterogeneity and personalization, not layer-wise training.
- Break condition: If goodness-based layer optimization diverges from global loss minimization, overall model performance degrades.

### Mechanism 3
- Claim: FedFwd's two-forward-pass structure reduces per-round training time for large batch sizes compared to backpropagation.
- Mechanism: Eliminating backward pass computation and activation storage reduces computational overhead per batch, especially as batch size grows.
- Core assumption: The cost of two forward passes plus layer-wise updates is less than one forward pass plus one backward pass for large batches.
- Evidence anchors:
  - [section] "Table 2 and Figure 3... as the batch size increases, this difference decrease significantly."
  - [abstract] "The training time of FedFwd is initially slower than FedAvg with small batch sizes but becomes comparable as batch size increases."
  - [corpus] Weak evidence—no direct timing benchmarks from corpus neighbors.
- Break condition: If layer-wise updates become the bottleneck for large batches, the speed advantage disappears.

## Foundational Learning

- Concept: Backpropagation and gradient flow
  - Why needed here: FedFwd replaces backpropagation, so understanding gradient computation is essential to grasp what is being changed.
  - Quick check question: What is stored during the forward pass in standard backpropagation that FedFwd eliminates?

- Concept: Federated averaging (FedAvg) protocol
  - Why needed here: FedFwd builds on FedAvg's client selection and model aggregation steps, only changing local training.
  - Quick check question: In FedAvg, what happens after clients complete local SGD updates?

- Concept: Goodness function and threshold-based layer training
  - Why needed here: FedFwd's objective function differs fundamentally from cross-entropy loss, so understanding it is key to reproducing the algorithm.
  - Quick check question: How does the goodness function decide whether to increase or decrease a layer's weights?

## Architecture Onboarding

- Component map:
  - Client-side: Layer-wise Forward-Forward training, no activation storage, goodness computation, local model update
  - Server-side: Same as FedAvg—model averaging, client selection, global aggregation

- Critical path:
  1. Client receives global model
  2. Client performs layer-wise Forward-Forward updates using two forward passes
  3. Client sends updated model to server
  4. Server averages received models

- Design tradeoffs:
  - Memory vs. accuracy: Eliminating activation storage saves memory but may reduce gradient quality
  - Speed vs. convergence: Layer-wise updates may be faster per batch but risk slower overall convergence on non-i.i.d. data
  - Complexity vs. compatibility: FedFwd requires model architecture changes (goodness function, layer normalization) but remains compatible with FedAvg aggregation

- Failure signatures:
  - Divergence on non-i.i.d. data (as shown in MNIST results)
  - Slower initial training with small batch sizes
  - Potential instability if goodness thresholds are poorly tuned

- First 3 experiments:
  1. Compare memory usage of FedFwd vs FedAvg on a fixed MLP architecture
  2. Benchmark training time vs batch size scaling for both algorithms
  3. Test convergence sensitivity to goodness threshold θ on a synthetic dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Forward-Forward algorithm perform in federated learning settings compared to backpropagation, particularly in terms of accuracy and convergence speed?
- Basis in paper: [explicit] The paper presents FedFwd, a federated learning algorithm using the Forward-Forward algorithm, and compares its performance with the standard FedAvg algorithm on MNIST and CIFAR-10 datasets.
- Why unresolved: While the paper provides initial results, the comparison is limited to specific datasets and model architectures. The generalizability of these results to other datasets and architectures is not explored.
- What evidence would resolve it: Conducting experiments with a wider range of datasets and model architectures to compare the performance of the Forward-Forward algorithm and backpropagation in federated learning settings.

### Open Question 2
- Question: Can the Forward-Forward algorithm be further optimized to improve its performance in federated learning settings?
- Basis in paper: [explicit] The paper mentions a modification to the objective function to improve training stability and convergence speed.
- Why unresolved: The paper does not explore other potential optimizations for the Forward-Forward algorithm in federated learning settings, such as adjusting the learning rate, batch size, or other hyperparameters.
- What evidence would resolve it: Conducting a comprehensive hyperparameter tuning study and exploring other potential optimizations to improve the performance of the Forward-Forward algorithm in federated learning settings.

### Open Question 3
- Question: How does the Forward-Forward algorithm perform in federated learning settings with non-IID data distributions?
- Basis in paper: [explicit] The paper compares the performance of the Forward-Forward algorithm and backpropagation in federated learning settings with both IID and non-IID data distributions on MNIST and CIFAR-10 datasets.
- Why unresolved: The paper provides initial results, but the comparison is limited to specific datasets and model architectures. The generalizability of these results to other datasets and architectures with non-IID data distributions is not explored.
- What evidence would resolve it: Conducting experiments with a wider range of datasets and model architectures to compare the performance of the Forward-Forward algorithm and backpropagation in federated learning settings with non-IID data distributions.

## Limitations

- Experimental scope limited to MNIST and CIFAR-10 with MLP architectures, leaving scalability to deeper CNNs and larger datasets unknown
- Claims about reduced memory usage are supported by algorithmic design but lack direct empirical measurement
- Performance degradation under non-i.i.d. settings is demonstrated but not explained mechanistically

## Confidence

- Mechanism 1: Medium - Algorithmic design supports memory reduction claim, but lacks direct empirical measurement
- Mechanism 2: Medium - Layer-wise updates are well-defined but lack ablation studies isolating their effects
- Mechanism 3: Medium - Speedup observation for large batches needs rigorous benchmarking against activation storage costs

## Next Checks

1. Measure peak memory usage per client during FedFwd vs FedAvg training on the same hardware
2. Run FedFwd with varying goodness thresholds θ to map sensitivity and identify optimal ranges
3. Test FedFwd on a CNN architecture (e.g., LeNet) to assess scalability beyond MLPs