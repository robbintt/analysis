---
ver: rpa2
title: Spoken Question Answering and Speech Continuation Using Spectrogram-Powered
  LLM
arxiv_id: '2305.15255'
source_url: https://arxiv.org/abs/2305.15255
tags:
- speech
- language
- continuation
- text
- spectrogram
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SPECTRON, a novel approach for adapting pre-trained
  large language models (LLMs) to perform spoken question answering (QA) and speech
  continuation. SPECTRON leverages a pre-trained speech encoder to process spectrograms
  as both input and output, enabling the LLM to take speech inputs and generate speech
  outputs.
---

# Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM

## Quick Facts
- **arXiv ID**: 2305.15255
- **Source URL**: https://arxiv.org/abs/2305.15255
- **Reference count**: 40
- **Primary result**: SPECTRON, a spectrogram-powered LLM for spoken QA and speech continuation, surpasses existing models in speaker preservation and semantic coherence

## Executive Summary
This paper introduces SPECTRON, a novel approach for adapting pre-trained large language models (LLMs) to perform spoken question answering and speech continuation tasks. The key innovation is using a pre-trained speech encoder to process spectrograms as both input and output, enabling the LLM to handle speech inputs and generate speech outputs directly. The model is trained end-to-end using only paired speech-text pairs, with a joint training objective that supervises speech recognition, text continuation, and speech synthesis simultaneously. Experimental results demonstrate that SPECTRON achieves superior performance in speaker preservation and semantic coherence compared to existing spoken language models, while maintaining the knowledge of the original LLM.

## Method Summary
SPECTRON adapts pre-trained LLMs by integrating a pre-trained Conformer speech encoder to process 128-dimensional log-mel spectrograms (16kHz, 10ms frame-rate) as both input and output. The model fine-tunes a 350M-parameter decoder language model with the speech encoder using a joint loss function combining cross-entropy loss (for speech recognition and transcript continuation) and a reconstruction loss (for speech continuation). Training uses pseudo-labeled data from Libri-Light (60k hours) generated through noisy self-training with a Conformer ASR system. The model generates intermediate text predictions that serve as reasoning before producing spectrogram continuations, operating within a single decoding pass.

## Key Results
- SPECTRON surpasses existing spoken language models in speaker preservation and semantic coherence
- The model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets
- Achieves these results while operating directly on spectrograms without requiring discrete speech tokenization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SPECTRON leverages a pre-trained speech encoder to process spectrograms as both input and output, enabling the LLM to take speech inputs and generate speech outputs.
- **Mechanism**: The speech encoder converts raw spectrograms into continuous linguistic features that serve as prefix conditioning for the language model. The language model then autoregressively generates both text and spectrogram continuations in a single decoding pass.
- **Core assumption**: The continuous spectrogram representation preserves sufficient acoustic information for speech generation without requiring discrete tokenization.
- **Evidence anchors**: [abstract] "By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs." [section] "The speech encoder E is a 600M-parameter Conformer encoder [33] pre-trained on web-scale data (12M hours) [34]."
- **Break condition**: If the spectrogram representation loses critical prosodic or speaker information during encoding, speech quality will degrade.

### Mechanism 2
- **Claim**: A joint training objective enables cross-modal chain-of-thought reasoning within a single decoding pass.
- **Mechanism**: The language model predicts text transcription and continuation alongside spectrogram continuation using teacher forcing. The text serves as intermediate reasoning that conditions speech generation.
- **Core assumption**: The language model's pre-training on text provides sufficient linguistic reasoning capability to benefit speech synthesis.
- **Evidence anchors**: [abstract] "A key aspect of SPECTRON is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a 'cross-modal' chain-of-thought within a single decoding pass." [section] "By having the same architecture decode the intermediate text and the spectrograms, we gain two benefits... the predicted text serves as intermediate reasoning, enhancing the quality of the synthesized speech."
- **Break condition**: If the intermediate text predictions are poor, speech generation quality will suffer despite the cross-modal approach.

### Mechanism 3
- **Claim**: SPECTRON surpasses existing spoken language models in speaker preservation and semantic coherence.
- **Mechanism**: Direct spectrogram modeling preserves continuous acoustic features better than discrete tokenization approaches. The pre-trained LM provides semantic coherence through its language understanding capabilities.
- **Core assumption**: Continuous spectrogram representations retain more speaker and prosodic information than quantized units.
- **Evidence anchors**: [abstract] "Our method surpasses existing spoken language models in speaker preservation and semantic coherence." [section] "Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets."
- **Break condition**: If speaker embedding preservation is not maintained through the spectrogram encoding/decoding pipeline, speaker similarity metrics will degrade.

## Foundational Learning

- **Concept**: Spectrogram processing and representation
  - **Why needed here**: SPECTRON operates directly on spectrograms rather than discrete speech tokens, requiring understanding of audio signal processing and spectrogram characteristics
  - **Quick check question**: What are the key differences between raw audio waveforms and spectrograms in terms of information representation?

- **Concept**: Chain-of-thought reasoning in language models
  - **Why needed here**: SPECTRON uses text generation as intermediate reasoning for speech synthesis, leveraging CoT principles
  - **Quick check question**: How does intermediate reasoning improve generation quality in language models, and how might this transfer to speech generation?

- **Concept**: Teacher forcing and autoregressive generation
  - **Why needed here**: The model uses teacher forcing during training and autoregressive generation during inference for both text and spectrogram continuations
  - **Quick check question**: What are the benefits and limitations of teacher forcing versus scheduled sampling in sequence generation tasks?

## Architecture Onboarding

- **Component map**: Speech Encoder (Conformer) → Projection Layer → Language Model (Transformer Decoder) → Pre-net/Post-net → Vocoder
- **Critical path**: Speech input → Speech Encoder → Language Model → Spectrogram output → Vocoder → Speech output
- **Design tradeoffs**: Continuous spectrogram modeling vs discrete tokenization (better fidelity but higher computational cost), joint training vs cascaded approaches (simpler but requires careful loss balancing)
- **Failure signatures**: Poor speaker similarity indicates spectrogram representation issues, low semantic coherence suggests language model degradation, high perplexity indicates generation quality problems
- **First 3 experiments**:
  1. Validate spectrogram reconstruction with frozen LM and encoder components
  2. Test text generation quality with speech prompts to verify cross-modal reasoning
  3. Measure speaker similarity between input and generated speech to validate acoustic preservation

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of SPECTRON compare when trained on languages other than English, given that its speech encoder and language model are multilingual?
  - **Basis in paper**: [explicit] The paper mentions that "our speech encoder and LM are multilingual and thus amenable to a broader set of speech data."
  - **Why unresolved**: The authors explicitly state they used the same dataset (Libri-Light) and evaluated in English to compare with previous works, but do not explore multilingual capabilities.
  - **What evidence would resolve it**: Training and evaluating SPECTRON on multilingual datasets and comparing performance across different languages would provide evidence.

- **Open Question 2**: Would parameter-efficient training methods, such as adapter modules, be sufficient to achieve comparable performance to full fine-tuning when adapting SPECTRON to new tasks or domains?
  - **Basis in paper**: [explicit] The authors mention that "to maximize performance gains, we update all parameters" but note this "may be compute-prohibitive when starting with large pre-trained LMs" and suggest "parameter-efficient modules can lead to integrations of acceptable quality with less compute."
  - **Why unresolved**: The paper uses full fine-tuning for their experiments but does not explore parameter-efficient alternatives.
  - **What evidence would resolve it**: Conducting experiments comparing full fine-tuning versus parameter-efficient methods (e.g., adapters, LoRA) on the same tasks would provide evidence.

## Limitations

- The model's reliance on pseudo-labeled data from Libri-Light introduces uncertainty about performance on more diverse, naturalistic speech patterns
- The cross-modal chain-of-thought mechanism lacks empirical validation of whether intermediate text predictions actually improve speech quality beyond direct spectrogram modeling
- The 350M parameter size may limit the model's capacity for complex reasoning compared to larger LLMs

## Confidence

- **High Confidence**: The architectural design of using spectrograms as continuous representations is well-grounded in existing signal processing literature. The joint training objective formulation is technically sound and follows established practices in multi-task learning.
- **Medium Confidence**: The experimental results showing improved speaker preservation and semantic coherence are internally consistent, but the evaluation lacks comparison with more recent discrete-token approaches. The claim about retaining original LLM knowledge is supported by QA benchmarks but could benefit from more comprehensive probing.
- **Low Confidence**: The assertion that cross-modal chain-of-thought reasoning significantly improves speech generation quality is based on indirect evidence. The paper does not provide ablation studies isolating the contribution of intermediate text generation to overall performance.

## Next Checks

1. **Ablation Study on Chain-of-Thought Mechanism**: Conduct controlled experiments comparing SPECTRON with and without intermediate text generation (using direct spectrogram prediction) on the same test sets to quantify the actual contribution of the cross-modal reasoning component.

2. **Generalization Testing on Diverse Speech Data**: Evaluate SPECTRON on spontaneous speech datasets (e.g., AMI meeting corpus, Switchboard) and accented speech (e.g., Common Voice with non-native speakers) to assess robustness beyond read speech from audiobooks.

3. **Speaker Embedding Stability Analysis**: Perform detailed analysis of speaker embedding drift across long generations, including cosine similarity trajectories over time and perceptual evaluation of speaker consistency in multi-turn dialogues.