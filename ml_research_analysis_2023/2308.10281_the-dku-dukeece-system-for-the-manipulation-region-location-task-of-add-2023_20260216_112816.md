---
ver: rpa2
title: The DKU-DUKEECE System for the Manipulation Region Location Task of ADD 2023
arxiv_id: '2308.10281'
source_url: https://arxiv.org/abs/2308.10281
tags:
- detection
- fake
- audio
- utterances
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The DKU-DUKEECE system addresses the challenge of detecting and
  locating manipulated regions in audio splicing forgeries for the ADD 2023 challenge.
  The core approach integrates three models: a boundary detection system to identify
  waveform concatenation points, a frame-level anti-spoofing model to classify audio
  segments as genuine or fake, and a VAE-based outlier detector trained on genuine
  data.'
---

# The DKU-DUKEECE System for the Manipulation Region Location Task of ADD 2023

## Quick Facts
- arXiv ID: 2308.10281
- Source URL: https://arxiv.org/abs/2308.10281
- Reference count: 25
- Primary result: First place in ADD 2023 Track 2 manipulation region detection

## Executive Summary
The DKU-DUKEECE system addresses the challenge of detecting and locating manipulated regions in audio splicing forgeries for the ADD 2023 challenge. The core approach integrates three models: a boundary detection system to identify waveform concatenation points, a frame-level anti-spoofing model to classify audio segments as genuine or fake, and a VAE-based outlier detector trained on genuine data. By fusing these components with a scoring strategy, the system achieves a final ADD score of 0.6713, ranking first in Track 2. Key performance metrics include 82.23% sentence accuracy and an F1 score of 60.66%. The solution demonstrates robust detection capabilities on both in-domain and out-of-domain datasets.

## Method Summary
The system employs a multi-model fusion approach for detecting and locating manipulated regions in audio splicing forgeries. Three specialized models work in sequence: a boundary detection model identifies waveform concatenation points, an anti-spoofing model classifies frame-level audio segments, and a VAE-based outlier detector serves as supplementary verification. The system uses WavLM and Wav2Vec pre-trained models for feature extraction, followed by ResNet-1D and Transformer Encoder architectures for classification. The outputs are combined using a scoring strategy that leverages the complementary strengths of each component. The approach is evaluated on ADD-Train, ADD-Dev, ADD-Test, and ADD-Eval datasets, demonstrating strong performance in both in-domain and out-of-domain scenarios.

## Key Results
- Achieved first place in ADD 2023 Track 2 manipulation region detection with a final ADD score of 0.6713
- 82.23% sentence accuracy and 60.66% F1 score on the ADD challenge
- WavLM-based boundary detection achieved EER of 0.09% on ADD-Dev and 0.67% on ADD-Eval
- Outperformed baseline systems in both in-domain and out-of-domain evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-model fusion improves detection robustness by compensating for individual model weaknesses.
- Mechanism: The system combines a boundary detection model, an anti-spoofing model, and a VAE outlier detector. The boundary model locates waveform concatenation points, the anti-spoofing model classifies each segment as genuine or fake, and the VAE model identifies segments that deviate significantly from genuine data. These models operate in sequence and their outputs are combined using a scoring strategy that leverages the strengths of each component.
- Core assumption: Different models capture complementary aspects of audio manipulation, and their errors are not perfectly correlated.
- Evidence anchors:
  - [abstract] "Through the fusion of these three systems, our top-performing solution for the ADD challenge achieves an impressive 82.23% sentence accuracy and an ùêπ1 score of 60.66%."
  - [section] "By combining these systems, we achieve a final score of 0.6713 in the ADD challenge."
- Break condition: If all three models share the same blind spots or if their error patterns are highly correlated, fusion will not provide significant benefit.

### Mechanism 2
- Claim: WavLM-based features provide superior performance for both boundary detection and anti-spoofing tasks compared to Wav2Vec.
- Mechanism: The WavLM model is used as the feature extractor for both the boundary detection and anti-spoofing models. WavLM's large-scale self-supervised pretraining on diverse speech data allows it to capture more robust and discriminative features for detecting audio manipulation.
- Core assumption: The additional pretraining data and objectives in WavLM lead to better generalization on unseen manipulation scenarios.
- Evidence anchors:
  - [section] "Similar trends are observed in the anti-spoofing system. However, when evaluating the out-of-domain dataset, ADD-Eval, the Wav2Vec-based model outperforms other models with an ùêπ1 score of 0.4628."
  - [section] "The WavLM-based model achieves the best performance in the boundary detection task, achieving an EER of 0.09% on the ADD-Dev set and 0.67% on the ADD-Eval set."
- Break condition: If the manipulation techniques in the test set differ significantly from the pretraining data distribution, WavLM's advantage may diminish.

### Mechanism 3
- Claim: VAE-based outlier detection using reconstruction probability is effective for identifying manipulated audio segments.
- Mechanism: The VAE is trained exclusively on genuine audio samples and learns to reconstruct normal audio patterns. When presented with manipulated segments, the reconstruction probability is expected to be lower, indicating deviation from genuine data. This serves as an additional verification step for segments that are difficult to classify with the other models.
- Core assumption: Manipulated audio segments have statistical properties that differ significantly from genuine audio, making them identifiable through reconstruction probability.
- Evidence anchors:
  - [section] "We utilize the open-source module pyod to implement it. The encoder and decoder of the VAE model are constructed with three hidden layers each, with dimensions of 128, 64, and 32, respectively."
  - [section] "During the inference stage, we begin by dividing the audio signal into overlapping audio clips. Each clip has the same length as the training samples, which is 1.28 seconds, with a step size of 0.64 seconds. After obtaining the probabilities for each frame within the audio clips, we merge the results of all the clips by averaging the overlapping regions."
- Break condition: If the manipulation techniques are sophisticated enough to preserve the statistical properties of genuine audio, the VAE may fail to detect them.

## Foundational Learning

- Concept: Audio splicing forgery detection
  - Why needed here: The system is designed to detect and locate manipulated regions in audio splicing forgeries, which is the core task of Track 2 in ADD 2023.
  - Quick check question: What is the difference between audio splicing forgery and other forms of audio deepfakes like voice conversion or text-to-speech synthesis?

- Concept: Frame-level audio classification
  - Why needed here: The boundary detection and anti-spoofing models operate at the frame level, classifying each frame as genuine or fake, or as being near a boundary.
  - Quick check question: How does frame-level classification differ from utterance-level classification in terms of the information available and the challenges involved?

- Concept: Self-supervised learning for speech representations
  - Why needed here: Wav2Vec and WavLM are used as feature extractors, and their ability to learn meaningful representations from unlabeled data is crucial for the system's performance.
  - Quick check question: What are the key differences between Wav2Vec and WavLM in terms of their pretraining objectives and the resulting representations?

## Architecture Onboarding

- Component map: Raw audio ‚Üí WavLM features ‚Üí ResNet-1D ‚Üí Backend classifier ‚Üí Frame-level predictions ‚Üí Scoring strategy ‚Üí Final output

- Critical path: Raw audio ‚Üí WavLM features ‚Üí ResNet-1D ‚Üí Backend classifier ‚Üí Frame-level predictions ‚Üí Scoring strategy ‚Üí Final output

- Design tradeoffs:
  - Using WavLM vs Wav2Vec: WavLM provides better performance but is larger and slower.
  - Frame length (1.28s) vs overlap (0.64s): Longer frames capture more context but reduce temporal resolution.
  - Number of models in fusion: More models can improve robustness but increase complexity and inference time.

- Failure signatures:
  - High EER or low ùêπ1 on validation set: Indicates issues with the individual models.
  - Degradation in performance on out-of-domain data: Suggests overfitting to the training distribution.
  - Inconsistent predictions across models: May require adjustment of the scoring strategy.

- First 3 experiments:
  1. Train and evaluate the WavLM-based boundary detection model on ADD-Dev to establish a baseline EER.
  2. Train and evaluate the WavLM-based anti-spoofing model on ADD-Dev to establish a baseline ùêπ1 score.
  3. Implement the VAE model and evaluate its ability to distinguish genuine from manipulated audio segments using reconstruction probability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the fusion of multiple detection models impact the robustness of the system against unseen synthesis techniques?
- Basis in paper: [explicit] The paper mentions that most deep learning models trained on specific systems and datasets show diminished discriminatory ability when confronted with unseen scenarios and mismatched domains.
- Why unresolved: The paper does not provide experimental results or analysis on the system's performance against synthesis techniques not encountered in the training data.
- What evidence would resolve it: Comparative analysis of the system's performance on datasets containing novel synthesis techniques, demonstrating its robustness or lack thereof.

### Open Question 2
- Question: What are the limitations of using a VAE model trained exclusively on genuine data for outlier detection in manipulated audio regions?
- Basis in paper: [explicit] The paper introduces a VAE-based model as a supplementary verification system to assess the authenticity of audio clips, trained exclusively on genuine audio samples.
- Why unresolved: The paper does not discuss the potential limitations or challenges of relying solely on genuine data for training the VAE model, such as its ability to generalize to manipulated audio.
- What evidence would resolve it: Detailed analysis of the VAE model's performance on manipulated audio samples, highlighting any weaknesses or areas for improvement.

### Open Question 3
- Question: How does the choice of self-supervised pre-training models (e.g., Wav2Vec, WavLM) affect the overall performance of the frame-level detection models?
- Basis in paper: [explicit] The paper mentions the use of large-scale self-supervised pre-training models like Wav2Vec and WavLM for extracting frame-level acoustic representations.
- Why unresolved: The paper does not provide a comparative analysis of the performance differences between models using different pre-training approaches or discuss the impact of these choices on the system's effectiveness.
- What evidence would resolve it: Experimental results comparing the performance of frame-level detection models using different pre-training models, along with a discussion on the implications of these findings.

## Limitations

- Performance degradation on out-of-domain data (ADD-Eval) suggests potential overfitting to training distribution
- Exact scoring algorithm for fusing model outputs is inadequately specified, making exact reproduction challenging
- Lack of detailed ablation studies to quantify individual contributions of each model component

## Confidence

- **High Confidence**: The overall system architecture and integration of three models (boundary detection, anti-spoofing, and VAE outlier detection) is well-documented and reproducible. The framework's ability to achieve first place in Track 2 of ADD 2023 is a concrete achievement.
- **Medium Confidence**: The specific implementation details of the WavLM-based feature extraction and ResNet-1D module are sufficiently described for reproduction, though exact hyperparameters are unspecified. The claim that WavLM outperforms Wav2Vec for boundary detection is supported by comparative results.
- **Low Confidence**: The exact scoring algorithm used to fuse model outputs is inadequately specified, and the paper lacks detailed error analysis for different manipulation types. The generalization capability to unseen manipulation techniques remains uncertain.

## Next Checks

1. **Ablation Study**: Systematically evaluate the system's performance with each component removed (boundary detection only, anti-spoofing only, VAE only, and all pairwise combinations) to quantify individual contributions and identify potential redundancy.

2. **Cross-Validation on Manipulation Types**: Analyze the system's performance breakdown across different splicing techniques (e.g., waveform concatenation points vs. segment replacement) to identify specific weaknesses and guide targeted improvements.

3. **Out-of-Domain Robustness Test**: Evaluate the complete system on multiple unseen datasets with varying manipulation characteristics to assess generalization beyond the ADD-Eval dataset and identify failure patterns.