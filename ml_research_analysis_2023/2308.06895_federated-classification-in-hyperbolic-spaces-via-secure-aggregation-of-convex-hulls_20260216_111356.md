---
ver: rpa2
title: Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex
  Hulls
arxiv_id: '2308.06895'
source_url: https://arxiv.org/abs/2308.06895
tags:
- convex
- data
- points
- poincar
- hyperbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first federated learning method for hyperbolic
  space classification, addressing the need to handle hierarchical data with privacy
  constraints. The approach uses convex hulls of data clusters, Bh sequences for label
  resolution, and secure Reed-Solomon-like encoding to enable efficient one-round
  communication.
---

# Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls

## Quick Facts
- **arXiv ID**: 2308.06895
- **Source URL**: https://arxiv.org/abs/2308.06895
- **Reference count**: 40
- **Primary result**: Up to 11% higher accuracy than Euclidean federated learning baselines on hierarchical datasets

## Executive Summary
This paper introduces the first federated learning method for hyperbolic space classification, addressing the need to handle hierarchical data with privacy constraints. The approach uses convex hulls of data clusters, Bh sequences for label resolution, and secure Reed-Solomon-like encoding to enable efficient one-round communication. A key innovation is the analysis of convex hull complexity in hyperbolic spaces and a novel quantization scheme for Poincaré discs. The method was tested on synthetic and biological datasets, achieving up to 11% higher accuracy than Euclidean federated learning baselines. Results demonstrate the effectiveness of hyperbolic space embeddings and secure aggregation for privacy-preserving classification in distributed hierarchical data settings.

## Method Summary
The framework operates by having clients locally compute convex hulls of their data clusters in the Poincaré disc, quantize these hulls to control communication cost, encode cluster labels using Bh sequences for secure aggregation, and transmit to the server. The server decodes the labels, aggregates convex hulls using balanced graph partitioning, and trains a hyperbolic SVM. The approach combines Poincaré quantization, Bh sequence-based label encoding, and secure Reed-Solomon-like transmission to achieve privacy-preserving federated classification in hyperbolic spaces.

## Key Results
- Achieved up to 11% higher accuracy than Euclidean federated learning baselines
- Demonstrated effectiveness of hyperbolic space embeddings for hierarchical data
- Introduced first method for federated classification in hyperbolic spaces
- Validated on synthetic and biological datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Poincaré quantization bounds pairwise hyperbolic distances within bins, ensuring bounded communication error.
- **Mechanism**: The method partitions the Poincaré disc radially and angularly so that hyperbolic bin diameters are bounded by ϵ/2, and uses bin centroids for point representation.
- **Core assumption**: Radial and angular partition parameters can be chosen so that the hyperbolic length along both directions is ≤ ϵ/2.
- **Evidence anchors**:
  - [abstract] "a new quantization method for the Poincaré disc coupled with Reed-Solomon-like encoding"
  - [section 4.2] "we partition the Poincaré disc in terms of concentric circles...angular bins...radial bins...hyperbolic length...upper-bounded by ϵ/2"
- **Break condition**: If the data distribution is highly skewed toward the disc boundary, the assumed uniform area/angle partition may produce bins whose hyperbolic diameter exceeds ϵ/2.

### Mechanism 2
- **Claim**: Bh sequences enable unique label resolution without revealing client identities.
- **Mechanism**: Each client assigns Bh labels (integers with unique h-sum property) to its local clusters; aggregated sums uniquely decode individual labels across clients.
- **Core assumption**: The number of clients sharing a bin never exceeds h, so that the unique sum property holds.
- **Evidence anchors**:
  - [abstract] "introduce a number-theoretic approach for label recovery based on the so-called integer Bh sequences"
  - [section 4.4] "Bh sequences are sequences of positive integers with the defining property that any sum of ≤h possibly repeated terms of the sequence is unique"
- **Break condition**: If more than h clients contribute to the same bin, the sum is no longer unique and decoding fails.

### Mechanism 3
- **Claim**: Balanced graph partitioning aggregates local convex hulls into global clusters without leaking client identities.
- **Mechanism**: Construct a complete weighted graph on convex hulls with edge weights inversely proportional to average hyperbolic distance; find a balanced cut to group hulls into global classes.
- **Core assumption**: The true global classes will form tight clusters in the distance-based graph, so the balanced cut will separate them correctly.
- **Evidence anchors**:
  - [abstract] "aggregating convex hulls of the clients based on balanced graph partitioning"
  - [section 4.6] "construct a weighted complete graph...minimum balanced cut...two groups of aggregated local convex hulls"
- **Break condition**: If quantization distortion causes overlaps between true global classes, the graph may not separate them cleanly.

## Foundational Learning

- **Concept**: Poincaré disc model of hyperbolic geometry and Möbius addition.
  - **Why needed here**: All data points, distances, and convex hulls are defined in this space; mis-understanding the metric leads to wrong quantization and classification.
  - **Quick check question**: Given two points x, y in B²ₖ, write the formula for their hyperbolic distance dk(x,y).

- **Concept**: Minimal convex hull in hyperbolic space.
  - **Why needed here**: The algorithm must transmit only extreme points; understanding the minimal hull ensures correct client-side computation.
  - **Quick check question**: Why does removing non-extreme points from a convex hull not change the classification boundary?

- **Concept**: Bh (Sidon) sequences and unique-sum property.
  - **Why needed here**: Label switching resolution relies on the uniqueness of sums of at most h elements.
  - **Quick check question**: For h=2, give an example Bh sequence of length 4 and verify that all pairwise sums are distinct.

## Architecture Onboarding

- **Component map**: Client preprocessing → Poincaré embedding → convex hull (Graham scan) → quantization → Bh labeling → SCMA encoding → transmission → Server aggregation → Bh decoding → balanced graph partitioning → global hull formation → SVM training
- **Critical path**: Quantization → Bh labeling → SCMA secure transmission; any delay or failure here stalls label decoding and global training.
- **Design tradeoffs**:
  - Smaller ϵ → lower distortion but more bins → higher communication cost.
  - Larger h → more robust to collisions but requires larger label alphabet → more SCMA field size.
  - Number of quantization bins B vs SCMA field size q: must ensure q > B to encode bin indices.
- **Failure signatures**:
  - Decoded labels contain duplicates → h too small for number of colliding clients.
  - Graph cut separates true same-class hulls → quantization too coarse.
  - SVM training diverges → convex hull complexity too high, leaking too much info.
- **First 3 experiments**:
  1. Verify Poincaré quantization: generate uniform points, apply PQ(·), check max intra-bin hyperbolic distance ≤ ϵ.
  2. Test Bh decoding: simulate 3 clients with overlapping bins, check unique sum recovery for h=2 and h=3.
  3. Run end-to-end on a tiny synthetic dataset: 2 clients, 2 classes, measure classification accuracy vs ϵ.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed framework perform under label switching scenarios with more than two labels?
- **Basis in paper**: [explicit] The paper mentions extending to multi-label classification but does not evaluate label switching in multi-label settings.
- **Why unresolved**: The experiments only tested label switching in binary classification scenarios.
- **What evidence would resolve it**: Experimental results on multi-label classification tasks with varying degrees of label switching across clients.

### Open Question 2
- **Question**: What is the theoretical bound on the error introduced by quantization in the Poincaré disc?
- **Basis in paper**: [inferred] The paper analyzes convex hull complexity after quantization but doesn't provide theoretical error bounds on the classification accuracy.
- **Why unresolved**: The analysis focuses on convex hull complexity rather than the impact on the learned classifier's accuracy.
- **What evidence would resolve it**: A theorem or empirical study showing the relationship between quantization parameter ϵ and classification error rate.

### Open Question 3
- **Question**: How does the framework handle data points that are not uniformly distributed in the Poincaré disc?
- **Basis in paper**: [explicit] The convex hull complexity analysis assumes uniform distribution, but the paper doesn't explore non-uniform distributions.
- **Why unresolved**: The analysis relies on uniform distribution assumptions which may not hold in real-world data.
- **What evidence would resolve it**: Analysis of convex hull complexity and classification accuracy for different non-uniform data distributions.

## Limitations
- Effectiveness depends critically on three assumptions that lack systematic validation across diverse datasets
- Performance under skewed data distributions remains unverified
- SCMA encoding parameters and their impact on communication efficiency versus privacy tradeoff are not thoroughly explored

## Confidence

**High Confidence**: The theoretical foundation for Bh sequences and their unique-sum property is well-established in number theory literature. The quantization method's mathematical framework is sound given uniform data assumptions.

**Medium Confidence**: The balanced graph partitioning approach for hull aggregation is plausible but untested on datasets where quantization distortion causes class overlaps. The overall accuracy improvements (up to 11%) are promising but may not generalize to non-hierarchical data.

**Low Confidence**: The claim about convex hull complexity serving as a privacy metric lacks empirical validation. The impact of varying h and quantization parameters on the privacy-accuracy tradeoff is not quantified.

## Next Checks

1. **Stress Test Bh Sequences**: Generate synthetic datasets with controlled client overlap per bin (1×, 2×, 3×, h× clients). Verify label decoding success rates and identify the exact failure threshold where unique-sum property breaks.

2. **Quantization Robustness Analysis**: Apply the method to exponentially distributed data (skewed toward disc boundary) and measure: (a) actual vs. theoretical maximum intra-bin hyperbolic distances, (b) classification accuracy degradation, and (c) convex hull complexity increase.

3. **Privacy-A Accuracy Tradeoff**: Systematically vary (a) quantization parameter ϵ, (b) maximum clients per bin h, and (c) number of bins B. For each configuration, measure: (1) classification accuracy, (2) convex hull complexity, and (3) reconstruction error if an adversary intercepts encoded messages.