---
ver: rpa2
title: Mitigating Degree Biases in Message Passing Mechanism by Utilizing Community
  Structures
arxiv_id: '2312.16788'
source_url: https://arxiv.org/abs/2312.16788
tags:
- nodes
- graph
- node
- learning
- degree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of degree biases in message-passing
  GNNs, where low-degree nodes receive fewer messages than high-degree nodes, leading
  to poor performance on these nodes. The authors propose Community-aware Graph Transformers
  (CGT) that use learnable graph augmentations and graph transformers to learn degree-unbiased
  representations.
---

# Mitigating Degree Biases in Message Passing Mechanism by Utilizing Community Structures

## Quick Facts
- arXiv ID: 2312.16788
- Source URL: https://arxiv.org/abs/2312.16788
- Reference count: 34
- Primary result: CGT significantly improves performance on low-degree nodes, reducing misclassification rate gap between low and high-degree nodes.

## Executive Summary
This paper addresses degree biases in message-passing graph neural networks, where low-degree nodes receive fewer messages than high-degree nodes, leading to poor performance. The authors propose Community-aware Graph Transformers (CGT) that use learnable graph augmentations and improved self-attention to learn degree-unbiased representations. By generating more within-community edges connecting low-degree nodes and capturing high-order proximity and node roles, CGT outperforms state-of-the-art baselines on node classification and clustering tasks while significantly improving low-degree node performance.

## Method Summary
The CGT framework consists of three key components: learnable graph augmentation that generates more within-community edges connecting low-degree nodes through edge perturbation, improved self-attention that captures high-order proximity and node roles within communities, and a self-supervised learning task that preserves graph structure and regularizes augmentations. The method clusters nodes into communities using K-means, samples context nodes within communities and k-hop distance, ranks them by inverse degree score, and generates augmented adjacency via edge perturbation. The augmented graph is then fed to a transformer with improved attention, and the model is trained with SSL loss combining transition probability preservation and edge BCE.

## Key Results
- CGT outperforms state-of-the-art baselines on node classification and clustering tasks.
- Significantly improves performance on low-degree nodes, reducing the misclassification rate gap between low and high-degree nodes.
- Experiments on benchmark datasets (Cora, Citeseer, Pubmed, WikiCS, Amazon Computers, Amazon Photo) demonstrate effectiveness.

## Why This Works (Mechanism)

### Mechanism 1
- Learnable graph augmentation generates more within-community edges connecting low-degree nodes, improving their message reception.
- Core assumption: Nodes within the same community and k-hop distance are more likely to share similar features, and low-degree nodes benefit more from connections to other low-degree nodes than from high-degree hubs.
- Evidence: Weak - no direct mention of community-based augmentation in neighbors.

### Mechanism 2
- Improved self-attention captures high-order proximity and node roles within communities, reducing bias.
- Core assumption: Within-community nodes with similar degrees (roles) share more relevant structural information, and encoding this into attention improves representation learning.
- Evidence: Weak - no direct mention of role-based attention in neighbors.

### Mechanism 3
- Self-supervised learning preserves graph connectivity and regularizes augmentations, preventing collapse.
- Core assumption: Preserving transition probabilities and original features constrains the augmentation to maintain useful graph structure rather than generating fully connected or overly sparse graphs.
- Evidence: Weak - no direct mention of SSL regularization in neighbors.

## Foundational Learning

- Concept: Graph Neural Networks and Message Passing
  - Why needed here: The paper addresses degree biases in MP GNNs, so understanding how messages flow from neighbors to nodes is foundational.
  - Quick check question: In a 2-layer MP GNN, how many hops away can a node receive information from?

- Concept: Community Detection and Structure
  - Why needed here: The augmentation and attention mechanisms rely on nodes being grouped into communities to improve low-degree node performance.
  - Quick check question: What is the main assumption behind using community structure to improve low-degree node representation?

- Concept: Self-Attention and Transformer Mechanisms
  - Why needed here: The improved self-attention is a core component for capturing high-order proximity and node roles.
  - Quick check question: In standard self-attention, what is the formula for computing the attention score between two nodes?

## Architecture Onboarding

- Component map: Input -> Community Clustering -> Learnable Augmentation -> Improved Self-Attention -> SSL Loss -> Node Representations

- Critical path:
  1. Cluster nodes into communities
  2. For each node, sample context nodes within community and k-hop
  3. Rank context nodes by inverse degree score
  4. Generate augmented adjacency via edge perturbation
  5. Feed augmented graph to transformer with improved attention
  6. Apply SSL loss to preserve structure and regularize augmentations
  7. Extract representations for downstream tasks

- Design tradeoffs:
  - Learnable vs heuristic augmentation: Learnable is more flexible but computationally heavier; heuristic is faster but may be less effective.
  - Full vs sparse attention: Full attention captures all pairwise interactions but is O(N²); sparse attention is faster but may miss important long-range dependencies.

- Failure signatures:
  - Poor performance on low-degree nodes: Augmentation module not generating useful edges or attention not capturing role similarity.
  - Overfitting to training data: SSL loss not strong enough to regularize augmentations.
  - High computational cost: Full attention on large graphs becomes prohibitive.

- First 3 experiments:
  1. Run CGT on a small dataset (e.g., Cora) with default hyperparameters; check if low-degree node accuracy improves vs GT.
  2. Disable the community clustering step; observe if performance degrades, confirming the importance of community structure.
  3. Remove the degree-based distance term ϕf(i,j) from attention; check if low-degree node performance drops, confirming its role.

## Open Questions the Paper Calls Out
- How does the proposed CGT framework perform on graphs with sparse community structures or limited number of nodes within communities?
- What is the impact of different community detection algorithms on the performance of CGT?
- How does the performance of CGT vary with different graph sizes and structures?

## Limitations
- The paper's effectiveness depends heavily on accurate community detection and the ability of the model to learn meaningful augmentations.
- Specific implementation details of the improved self-attention and SSL loss are somewhat unclear.
- Claims about significant improvements on low-degree nodes are difficult to verify without access to exact code and hyperparameters.

## Confidence
- High confidence: The core problem of degree bias in MP GNNs is well-established, and the general approach of using community structure and learnable augmentations is sound.
- Medium confidence: The specific implementation details of the improved self-attention and SSL loss are somewhat unclear, but the overall framework is plausible.
- Low confidence: The paper's claims about significant improvements on low-degree nodes are difficult to verify without access to the exact code and hyperparameters.

## Next Checks
1. Verify that the model's performance gains on low-degree nodes persist when community detection is disabled, confirming the importance of community structure.
2. Test the model's robustness to noisy or incomplete community assignments by varying the quality of the initial clustering.
3. Analyze the learned attention weights to confirm that the model is indeed capturing meaningful structural patterns within communities, rather than simply memorizing degree-based heuristics.