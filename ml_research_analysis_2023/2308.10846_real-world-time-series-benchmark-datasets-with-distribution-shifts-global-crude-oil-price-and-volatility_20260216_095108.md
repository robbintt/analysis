---
ver: rpa2
title: 'Real World Time Series Benchmark Datasets with Distribution Shifts: Global
  Crude Oil Price and Volatility'
arxiv_id: '2308.10846'
source_url: https://arxiv.org/abs/2308.10846
tags:
- data
- crude
- datasets
- learning
- brent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces three new time-series benchmark datasets
  for the crude oil market (WTI, Brent, and Dubai Crude), covering over 30 years of
  data with significant distribution shifts. The authors transform asset price data
  into volatility proxies and use an expectation-maximization (EM) algorithm to generate
  task labels based on these distribution shifts.
---

# Real World Time Series Benchmark Datasets with Distribution Shifts: Global Crude Oil Price and Volatility

## Quick Facts
- arXiv ID: 2308.10846
- Source URL: https://arxiv.org/abs/2308.10846
- Reference count: 15
- Key outcome: Three new time-series benchmark datasets for crude oil markets with distribution shifts, showing universal improvement when task labels are provided to continual learning algorithms

## Executive Summary
This paper introduces three benchmark datasets for crude oil markets (WTI, Brent, and Dubai Crude) spanning over 30 years of data with significant distribution shifts. The authors transform asset price data into volatility proxies and use an expectation-maximization algorithm to generate task labels based on these distribution shifts. They demonstrate that including these task labels universally improves the performance of four continual learning algorithms across multiple forecasting horizons. The benchmark datasets and task labeling algorithm are made publicly available to accelerate research in handling distribution shifts in real-world financial data.

## Method Summary
The authors create benchmark datasets by transforming historical crude oil price data into volatility proxies (weekly percent changes for WTI/Brent, monthly for Dubai). They apply an EM algorithm (Kim et al., 1998) to fit Markov regime-switching models to the volatility data, generating task labels that capture distribution shifts. The number of tasks k=3 is selected by minimizing the sum of AIC, BIC, and HQIC. These datasets are then used to evaluate four continual learning algorithms (MOLe, MoB, MAML k-shot, MAML-continuous) on forecasting tasks, comparing performance with and without task labels across multiple horizons.

## Key Results
- EM algorithm generates meaningful task labels that align with real-world economic events
- Task labels universally improve MSE forecasting accuracy across four continual learning algorithms
- Weekly resampling of WTI/Brent data produces more realistic tasks than daily or bi-daily resampling
- The benchmark datasets capture significant distribution shifts in global crude oil markets over 30+ years

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EM algorithm optimally partitions time series into tasks by maximizing likelihood and minimizing information criteria
- Mechanism: EM iteratively estimates latent task states and updates model parameters until convergence, selecting k to minimize AIC+BIC+HQIC sum
- Core assumption: Data contains finite regimes capturable by Markov switching model
- Evidence anchors: Abstract states task labels "align with real-world events"; section shows k=3 chosen via AIC+BIC+HQIC minimization
- Break condition: Markov switching assumptions don't hold, EM converges to poor local optima

### Mechanism 2
- Claim: Volatility proxies make distribution shifts more detectable by EM
- Mechanism: Percent changes capture magnitude/direction of price movements, emphasizing high volatility periods corresponding to regime changes
- Core assumption: Volatility is more stable signal for regime detection than raw prices
- Evidence anchors: Section notes percent changes are "important in evaluating value at risk (VaR)"; weekly resampling produces "more realistic results"
- Break condition: Resampling frequency inappropriate for data, volatility proxy doesn't reflect true regime changes

### Mechanism 3
- Claim: Task labels universally improve continual learning performance by providing explicit context about distribution shifts
- Mechanism: Task labels allow model to condition predictions on current regime, reducing catastrophic forgetting
- Core assumption: EM-generated task labels accurately reflect underlying distribution shifts
- Evidence anchors: Abstract states "universal improvement over each benchmark, time-horizon, and model considered"
- Break condition: Task labels are noisy or misaligned with actual distribution shifts

## Foundational Learning

- Concept: Expectation-Maximization (EM) algorithm for parameter estimation in models with latent variables
  - Why needed here: EM fits Markov switching model to time series data and infers latent task states
  - Quick check question: What are the two steps of the EM algorithm, and how do they alternate to find maximum likelihood estimate?

- Concept: Information criteria (AIC, BIC, HQIC) for model selection
  - Why needed here: Used to select optimal number of tasks k by balancing model fit and complexity
  - Quick check question: How do AIC, BIC, and HQIC differ in penalties for model complexity, and what is intuition behind each?

- Concept: Volatility as proxy for risk and distribution shifts in financial time series
  - Why needed here: Volatility proxies (percent changes) used instead of raw prices to make distribution shifts more detectable
  - Quick check question: Why might volatility be more stable signal for regime detection than raw prices in financial time series?

## Architecture Onboarding

- Component map: Raw price data -> Volatility proxy (percent changes) -> Resampling (weekly/daily) -> EM algorithm with Markov switching model -> Smoothed regime probabilities -> Task labels (argmax) -> Benchmark creation with task labels -> Continual learning evaluation
- Critical path: Data preprocessing -> Task generation -> Benchmark creation -> Evaluation
- Design tradeoffs:
  - Resampling frequency: Weekly vs daily vs other intervals; weekly produces most realistic tasks
  - Number of tasks k: Chosen to minimize information criteria sum while ensuring each task present; k=3 optimal
  - Volatility proxy: Percent changes vs other measures; chosen for alignment with VaR and risk metrics
- Failure signatures:
  - EM fails to converge or produces unrealistic tasks (oscillating between tasks too frequently)
  - Task labels don't align with known real-world events or recessions
  - Including task labels degrades model performance (suggesting labels are noisy or misaligned)
- First 3 experiments:
  1. Verify EM produces meaningful tasks by visualizing smoothed probabilities and comparing to known events (e.g., recessions)
  2. Evaluate impact of resampling frequency on task quality by comparing tasks from daily, bi-daily, and weekly resampled data
  3. Test universal improvement claim by running each continual learning algorithm with/without task labels on WTI dataset, 1-week horizon

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would benchmark performance change if different resampling frequencies were used for WTI and Brent crude oil data?
- Basis in paper: [explicit] Paper discusses resampling by every two days and weekly, finding weekly resampling produced more realistic results for task generation
- Why unresolved: Paper only evaluates weekly resampling, leaving impact of other resampling frequencies unexplored
- What evidence would resolve it: Conduct experiments using different resampling frequencies (daily, every two days, bi-weekly) and compare continual learning algorithm performance

### Open Question 2
- Question: Can the task labeling algorithm be adapted to handle other types of non-stationary data beyond crude oil prices?
- Basis in paper: [inferred] Paper demonstrates algorithm's effectiveness on crude oil data but doesn't explore applicability to other domains
- Why unresolved: Algorithm's generalizability to other non-stationary datasets is not tested or discussed
- What evidence would resolve it: Apply task labeling algorithm to datasets from different domains (stock prices, weather data) and evaluate performance in generating meaningful task labels

### Open Question 3
- Question: What are the long-term effects of using task labels on model performance in continual learning scenarios?
- Basis in paper: [explicit] Paper shows including task labels improves performance across multiple forecasting horizons but doesn't address long-term effects
- Why unresolved: Paper focuses on short to medium-term forecasting horizons, leaving long-term impact unexplored
- What evidence would resolve it: Conduct longitudinal studies evaluating model performance over extended periods with and without task labels to assess long-term benefits

## Limitations

- Universal improvement claim across all four continual learning algorithms has weak empirical validation - only aggregate result reported without individual algorithm performance breakdowns
- Optimal task label generation depends heavily on Markov switching model assumptions, which may not hold for all financial time series
- Choice of k=3 tasks is heuristic, selected via information criteria but not validated against known regime change events

## Confidence

- Mechanism 1 (EM task generation): Medium - methodologically sound but assumes Markov regime structure
- Mechanism 2 (volatility proxy): Medium - reasonable choice for financial data but frequency selection is somewhat arbitrary
- Mechanism 3 (universal improvement): Low - aggregate claim lacks individual algorithm validation

## Next Checks

1. Validate individual algorithm performance by testing each continual learning method separately with and without task labels, reporting per-algorithm MSE improvements
2. Cross-validate task label quality by comparing generated regimes to known economic events (e.g., 2008 financial crisis, 2020 pandemic) across all three datasets
3. Test sensitivity to resampling frequency by generating tasks using daily, bi-daily, and weekly resampled data, comparing task quality and downstream forecasting performance