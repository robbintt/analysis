---
ver: rpa2
title: Getting MoRE out of Mixture of Language Model Reasoning Experts
arxiv_id: '2305.14628'
source_url: https://arxiv.org/abs/2305.14628
tags:
- question
- expert
- specialized
- mope
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of generalizing large language
  models (LLMs) across diverse question types requiring distinct reasoning abilities.
  The authors propose a Mixture-of-Prompt-Experts (MoPE) framework that ensembles
  specialized LLMs, each optimized for a specific reasoning category (factual, multihop,
  mathematical, and commonsense).
---

# Getting MoRE out of Mixture of Language Model Reasoning Experts

## Quick Facts
- arXiv ID: 2305.14628
- Source URL: https://arxiv.org/abs/2305.14628
- Reference count: 6
- Primary result: MoPE achieves 57.6% macro-average accuracy vs 49.6% best single expert

## Executive Summary
This paper introduces Mixture-of-Prompt-Experts (MoPE), a framework that ensembles specialized large language models to achieve generalizable and interpretable question answering across diverse reasoning types. The system uses four specialized experts optimized for factual, multihop, mathematical, and commonsense reasoning, with a random forest classifier selecting the best answer based on multiple features including inter-expert agreement. Experiments on 12 QA datasets demonstrate that MoPE significantly outperforms single expert models while improving selective QA through interpretable calibration mechanisms.

## Method Summary
MoPE implements four specialized expert models using Codex with different prompting strategies: factual expert with retrieval-augmentation, multihop expert with Chain-of-Thought, math expert with Chain-of-Thought, and commonsense expert with generated knowledge prompting. A random forest classifier scores each expert's prediction using features including expert type, question characteristics, answer characteristics, context features, and inter-expert agreement. The framework is evaluated on 12 QA datasets covering four reasoning types, with 400 test questions and 100 training questions sampled from each dataset for router training.

## Key Results
- MoPE achieves 57.6% macro-average accuracy across 12 datasets, outperforming the best single expert at 49.6%
- Incorporating inter-expert agreement features improves selective QA performance as measured by AUC, coverage at accuracy, and effective reliability metrics
- Human studies show that presenting expert predictions and answer selection process helps annotators better calibrate trust in system output
- The interpretable design enables analysis of which expert performs best on different reasoning types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inter-expert agreement signals improve selective QA calibration
- Mechanism: When multiple specialized models disagree on an answer, the system can detect low agreement and abstain from answering, reducing error rates
- Core assumption: Specialized models trained on different reasoning types will produce complementary but sometimes conflicting predictions
- Evidence anchors: The paper shows improved selective QA metrics when incorporating inter-expert agreement features, though direct validation of agreement-calibration relationship is limited

### Mechanism 2
- Claim: Prompt specialization creates complementary expertise across reasoning types
- Mechanism: Each expert model uses prompts optimized for a specific reasoning type (factual, multihop, math, commonsense), allowing the ensemble to handle diverse question types
- Core assumption: Different reasoning types require distinct prompting strategies to achieve optimal performance
- Evidence anchors: Performance gap analysis shows experts perform worse on reasoning types outside their specialization, supporting the specialization hypothesis

### Mechanism 3
- Claim: Routing based on multiple features improves answer selection accuracy
- Mechanism: A random forest classifier uses features including question characteristics, answer characteristics, and inter-expert agreement to select the best answer from specialized models
- Core assumption: The combination of multiple informative features can reliably predict which specialized model will perform best on a given question
- Evidence anchors: Overall performance gains demonstrate routing effectiveness, but feature importance analysis is not provided

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Provides theoretical foundation for combining multiple specialized models
  - Quick check question: What is the key difference between MoE at the token level vs. MoE at the answer level as used in this paper?

- Concept: Prompt engineering for LLMs
  - Why needed here: Enables specialization of base models for different reasoning types
  - Quick check question: How does Chain-of-Thought prompting differ from retrieval-augmented prompting in terms of their impact on reasoning performance?

- Concept: Selective classification and calibration
  - Why needed here: Allows the system to abstain from answering when confidence is low, improving reliability
  - Quick check question: What metrics are used to evaluate selective QA performance and how do they differ from standard accuracy metrics?

## Architecture Onboarding

- Component map: Input question -> 4 specialized experts (factual, multihop, math, commonsense) -> feature extraction module -> random forest classifier -> answer selection -> output
- Critical path: 1) Question input -> specialized experts generate predictions 2) Feature extraction from question and predictions 3) Random forest classifier scores each prediction 4) Highest-scoring prediction selected as output 5) (Optional) Calibration module determines if system should abstain
- Design tradeoffs: More compute vs. better accuracy (running all 4 experts vs. question-only routing), Interpretability vs. performance (feature-based vs. end-to-end learned routing), Coverage vs. reliability (selective vs. always answering)
- Failure signatures: High disagreement among experts with no clear winner, Random forest classifier unable to distinguish good from bad predictions, Calibration module unable to find effective threshold, Human annotators disagree significantly on answer correctness
- First 3 experiments: 1) Compare single expert performance vs. MoPE on each dataset to verify specialization benefits 2) Ablate inter-expert agreement features to measure their impact on selective QA 3) Test question-only routing vs. full MoPE to measure cost-accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MoRE framework perform on question types not explicitly tested in the paper (e.g., questions with multiple answers, ambiguous questions, questions with false presuppositions)?
- Basis in paper: The paper states that the list of reasoning types tested is not exhaustive and that the system can be easily extended to other reasoning types, but it does not provide empirical evidence for such extensions
- Why unresolved: The paper only evaluates the MoRE framework on four specific reasoning types and does not explore its performance on other potential question types that may arise in real-life applications
- What evidence would resolve it: Conducting experiments on additional question types not covered in the paper, such as those mentioned above, and comparing the performance of MoRE with other state-of-the-art question answering systems on these datasets

### Open Question 2
- Question: How does the MoRE framework compare to other ensemble methods, such as Puerto et al. (2021), in terms of performance and interpretability?
- Basis in paper: The paper mentions that Puerto et al. (2021) also ensembles multiple QA agents via a selector but does not design experts that specialize in distinct reasoning types and does not study selective QA
- Why unresolved: The paper does not provide a direct comparison between MoRE and other ensemble methods, making it difficult to assess the relative strengths and weaknesses of each approach
- What evidence would resolve it: Conducting experiments comparing MoRE with other ensemble methods, such as Puerto et al. (2021), on the same datasets and evaluation metrics used in the paper, and analyzing the trade-offs between performance and interpretability

### Open Question 3
- Question: How does the MoRE framework perform when using different backbone language models or QA models, especially open-source ones?
- Basis in paper: The paper states that it only focused on the Codex model due to its strong performance on QA tasks and suggests that it would be interesting to verify the framework on different language models or QA models
- Why unresolved: The paper does not explore the performance of MoRE when using different backbone models, limiting the generalizability of the findings to other language models or QA models
- What evidence would resolve it: Conducting experiments using different backbone language models or QA models, such as BERT, RoBERTa, or open-source alternatives, and comparing their performance with MoRE when using the Codex model on the same datasets and evaluation metrics

## Limitations

- The framework requires running all four specialized models for each question, creating significant computational overhead compared to question-only routing approaches
- Human studies on trust calibration are limited to a single dataset (MuSiQue) and don't measure long-term user trust or calibration stability across different populations
- The paper doesn't provide runtime comparisons or cost-benefit analysis to quantify the efficiency tradeoff of the MoPE approach

## Confidence

- **Low confidence**: The claim that inter-expert agreement specifically drives calibration improvements lacks direct experimental validation. While the paper reports improved selective QA metrics, the ablation studies don't isolate agreement features from other correlated features.
- **Medium confidence**: The specialization benefits across reasoning types are well-supported by experimental results showing MoPE outperforming individual experts on macro-average accuracy. However, the paper doesn't analyze whether this improvement comes from better coverage or genuine expertise gains.
- **Medium confidence**: The routing mechanism's effectiveness is demonstrated through overall performance gains, but the feature importance analysis for the random forest classifier is not provided.

## Next Checks

1. **Feature Ablation Study**: Run ablation experiments removing individual feature groups (expert type, question characteristics, answer characteristics, contexts, inter-expert agreement) to quantify each component's contribution to overall performance.

2. **Agreement Pattern Analysis**: Analyze specific cases where inter-expert agreement successfully predicted calibration needs versus cases where it failed. Compare agreement patterns across different reasoning types to identify systematic strengths and weaknesses.

3. **Runtime and Cost Analysis**: Measure the wall-clock time and computational resources required for MoPE versus question-only routing baselines across the 12 datasets. Calculate the accuracy improvement per unit of additional compute to quantify the efficiency tradeoff.