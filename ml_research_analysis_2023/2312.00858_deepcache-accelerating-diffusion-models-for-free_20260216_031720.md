---
ver: rpa2
title: 'DeepCache: Accelerating Diffusion Models for Free'
arxiv_id: '2312.00858'
source_url: https://arxiv.org/abs/2312.00858
tags:
- diffusion
- steps
- arxiv
- features
- deepcache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DeepCache, a training-free method to accelerate
  diffusion models by exploiting temporal redundancy in high-level features across
  denoising steps. The approach caches high-level features from previous steps and
  reuses them, reducing redundant computations while updating low-level features.
---

# DeepCache: Accelerating Diffusion Models for Free

## Quick Facts
- arXiv ID: 2312.00858
- Source URL: https://arxiv.org/abs/2312.00858
- Reference count: 40
- Primary result: Training-free 2.3× speedup on Stable Diffusion v1.5 with 0.05 CLIP Score drop

## Executive Summary
DeepCache introduces a training-free method to accelerate diffusion models by exploiting temporal redundancy in high-level features across denoising steps. The approach caches and reuses features from previous steps while updating only low-level features, achieving significant speedups without quality degradation. Tested on Stable Diffusion v1.5 and LDM-4-G, DeepCache demonstrates 2.3× and 4.1× speedups respectively with minimal impact on image quality metrics.

## Method Summary
DeepCache leverages temporal redundancy in high-level features across denoising steps of diffusion models. By caching features from previous steps and reusing them in subsequent steps while recalculating only low-level features, the method reduces redundant computations. The approach exploits U-Net skip connections to separate cacheable high-level features from updatable low-level features, enabling selective caching without breaking the denoising process. DeepCache also introduces a non-uniform 1:N inference strategy to optimize caching intervals based on feature similarity patterns.

## Key Results
- 2.3× speedup on Stable Diffusion v1.5 with only 0.05 drop in CLIP Score
- 4.1× speedup on LDM-4-G with 0.22 decrease in FID on ImageNet
- Outperforms pruning and distillation approaches requiring retraining
- Compatible with existing sampling techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeepCache exploits temporal redundancy in high-level features across denoising steps.
- Mechanism: Caches high-level features from previous denoising steps and reuses them in subsequent steps, while recalculating only low-level features.
- Core assumption: High-level features change slowly across adjacent denoising steps, making them cacheable without significant quality loss.
- Evidence anchors:
  - [abstract]: "DeepCache capitalizes on the inherent temporal redundancy observed in the sequential denoising steps of diffusion models, which caches and retrieves features across adjacent denoising stages"
  - [section 3.2]: "Adjacent steps in the denoising process exhibit significant temporal similarity in high-level features" and "at least 10% of the adjacent timesteps exhibit a high similarity (>0.95) to the current step"
  - [corpus]: Weak evidence - corpus contains related caching acceleration methods but not specific to diffusion model high-level feature caching
- Break condition: If high-level features change significantly between adjacent steps (similarity < threshold), caching will degrade image quality.

### Mechanism 2
- Claim: DeepCache leverages U-Net skip connections to separate cacheable high-level features from updatable low-level features.
- Mechanism: Uses the U-Net architecture where skip connections allow high-level features from the main branch to be cached while low-level features from the skip branch are updated at each step.
- Core assumption: U-Net's architectural property of combining high-level (main branch) and low-level (skip branch) features at each layer enables selective caching without breaking the denoising process.
- Evidence anchors:
  - [section 3.1]: "U-Net is constructed on stacked downsampling and upsampling blocks... Those skip paths directly forward the rich and relatively more low-level information from Di to Ui"
  - [section 3.3]: "Utilizing the property of the U-Net, we reuse the high-level features while updating the low-level features in a very cheap way"
  - [corpus]: Weak evidence - corpus contains related U-Net acceleration methods but not specific to diffusion model skip connection caching
- Break condition: If U-Net architecture changes significantly (e.g., different skip connection patterns), the caching mechanism may not work.

### Mechanism 3
- Claim: DeepCache enables non-uniform 1:N inference strategy to optimize caching intervals based on feature similarity patterns.
- Mechanism: Instead of uniform caching intervals, samples more frequently on steps with smaller feature similarities to maintain quality while maximizing speedup.
- Core assumption: Feature similarity patterns across denoising steps are non-uniform, allowing adaptive caching intervals that balance speed and quality.
- Evidence anchors:
  - [section 3.3]: "For the non-uniform 1:N inference, we tend to sample more on those steps with relatively small similarities to the adjacent steps"
  - [section 4.5]: "as the parameters p and c are incremented, there is an initial improvement in the generated image quality followed by a subsequent decline"
  - [corpus]: Weak evidence - corpus contains related non-uniform sampling methods but not specific to diffusion model feature similarity-based caching
- Break condition: If feature similarity patterns are uniform across all steps, non-uniform strategy provides no benefit over uniform caching.

## Foundational Learning

- Concept: Temporal feature similarity in diffusion models
  - Why needed here: Understanding that high-level features exhibit temporal consistency across denoising steps is fundamental to why DeepCache works
  - Quick check question: Why can we cache high-level features from previous denoising steps without recalculating them?

- Concept: U-Net skip connection architecture
  - Why needed here: DeepCache relies on U-Net's dual-pathway structure to separate cacheable high-level features from updatable low-level features
  - Quick check question: How do U-Net skip connections enable selective caching of features?

- Concept: Diffusion model denoising process
  - Why needed here: Understanding the sequential nature of diffusion model denoising is essential to appreciate the computational bottleneck DeepCache addresses
  - Quick check question: What makes diffusion model inference inherently sequential and computationally expensive?

## Architecture Onboarding

- Component map:
  - U-Net model with downsampling blocks {Di}d i=1, upsampling blocks {Ui}d i=1, and middle blocks M
  - Feature cache storage for high-level features from previous steps
  - Cache management system for storing/retrieving cached features
  - Control logic for deciding when to use cached features vs full inference

- Critical path:
  1. Initial denoising step: Full U-Net inference to generate xt and cache high-level features
  2. Subsequent steps: Partial inference using cached high-level features + updated low-level features
  3. Cache update: Periodically recalculate and update cached features based on 1:N or non-uniform strategy

- Design tradeoffs:
  - Caching interval N vs image quality: Larger N provides more speedup but may degrade quality
  - Skip branch selection: Different U-Net layers offer different speed-quality tradeoffs
  - Uniform vs non-uniform caching: Non-uniform can optimize for feature similarity patterns but adds complexity

- Failure signatures:
  - Image quality degradation: Indicates caching interval too large or inappropriate skip branch selection
  - Cache thrashing: Frequent cache updates defeating the speedup purpose
  - Memory overflow: Cache size exceeding available memory

- First 3 experiments:
  1. Baseline: Run standard diffusion model inference to establish reference performance metrics
  2. Simple caching: Implement uniform 1:2 caching with fixed skip branch to validate basic mechanism
  3. Interval optimization: Test different caching intervals (N=2,3,5,10) to find optimal speed-quality tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of DeepCache vary across different U-Net architectures beyond the tested Stable Diffusion, LDM, and DDPM models?
- Basis in paper: [explicit] The paper mentions that "Our approach would benefit from U-Net structures that have a larger number of skip branches, facilitating finer divisions of models, and giving us more choices for trade-off the speed and quality."
- Why unresolved: The paper only provides empirical evidence for three specific diffusion models. The generalizability of DeepCache's effectiveness to other U-Net architectures remains unexplored.
- What evidence would resolve it: Testing DeepCache on a wider variety of U-Net architectures with different numbers of skip branches and varying computational distributions across layers would provide insights into its broader applicability.

### Open Question 2
- Question: What is the optimal strategy for selecting the non-uniform 1:N caching intervals across different datasets and model types?
- Basis in paper: [explicit] The paper states that "The optimal hyper-parameter values employed in our experiments are detailed in Table 10" but also notes that "A noticeable trend is observed, indicating that the majority of optimal parameters tend to center around the 15th timestep, accompanied by a power value of approximately 1.4."
- Why unresolved: While the paper provides some guidance on hyper-parameter selection, it does not offer a universal strategy for determining optimal caching intervals that would work across all datasets and model types.
- What evidence would resolve it: Developing a systematic approach or algorithm for determining optimal non-uniform 1:N caching intervals based on dataset characteristics and model architecture would address this open question.

### Open Question 3
- Question: How does the choice of sampling method (e.g., DDIM, PLMS) interact with DeepCache's performance, and can this interaction be optimized?
- Basis in paper: [explicit] The paper mentions that "Our method is categorized under an objective to minimize the average inference time per step" and "Our method is compatible with existing fast samplers."
- Why unresolved: While the paper demonstrates compatibility with various sampling methods, it does not explore how the choice of sampler affects DeepCache's performance or whether there's an optimal pairing.
- What evidence would resolve it: Conducting a comprehensive study comparing DeepCache's performance with different sampling methods across various datasets and models would provide insights into optimal sampler-DeepCache combinations.

## Limitations

- Limited to diffusion models with U-Net architectures featuring skip connections
- Performance depends on temporal feature similarity patterns that may vary across architectures
- No analysis of memory overhead for extensive caching across large models
- Limited empirical validation beyond 2D image generation (no video/3D diffusion testing)

## Confidence

- Core mechanism (temporal feature caching via U-Net skip connections): **High**
- Non-uniform 1:N strategy effectiveness: **Medium**
- Scalability to larger models and longer sequences: **Low**

## Next Checks

1. **Architecture Generalization Test**: Apply DeepCache to video diffusion transformers and 3D diffusion models to verify the method's effectiveness beyond 2D image generation, particularly examining whether temporal redundancy assumptions hold in higher-dimensional spaces.

2. **Memory Overhead Analysis**: Conduct systematic experiments measuring memory usage as a function of caching interval N and skip branch depth, establishing the memory-speed tradeoff curve and identifying the maximum practical cache size before diminishing returns.

3. **Feature Similarity Pattern Analysis**: Perform detailed analysis of high-level feature similarity patterns across different diffusion model families (DDPM, DDIM, DPM-Solver) and training objectives to determine which architectural choices maximize temporal consistency for DeepCache effectiveness.