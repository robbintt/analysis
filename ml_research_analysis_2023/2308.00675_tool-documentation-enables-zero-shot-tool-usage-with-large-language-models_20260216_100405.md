---
ver: rpa2
title: Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models
arxiv_id: '2308.00675'
source_url: https://arxiv.org/abs/2308.00675
tags:
- arxiv
- tool
- tools
- documentation
- demos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether large language models (LLMs) can
  use new tools effectively without demonstrations by relying solely on tool documentation.
  The key finding is that zero-shot prompts with only tool documentation can achieve
  performance on par with or better than few-shot demonstrations across six tasks
  spanning language and vision modalities.
---

# Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models
## Quick Facts
- arXiv ID: 2308.00675
- Source URL: https://arxiv.org/abs/2308.00675
- Authors: 
- Reference count: 40
- Primary result: Zero-shot prompts with tool documentation can achieve performance on par with or better than few-shot demonstrations across six tasks spanning language and vision modalities.

## Executive Summary
This work investigates whether large language models (LLMs) can use new tools effectively without demonstrations by relying solely on tool documentation. The key finding is that zero-shot prompts with only tool documentation can achieve performance on par with or better than few-shot demonstrations across six tasks spanning language and vision modalities. On a newly collected dataset of hundreds of API tools, zero-shot documentation significantly outperformed few-shot prompts without documentation. The approach also enabled LLMs to seamlessly incorporate new tools like GroundingDINO, SAM, and XMem for tasks like image editing and video tracking without further demonstrations.

## Method Summary
The method involves using zero-shot prompts with only tool documentation to elicit proper tool usage from LLMs, and comparing this approach with few-shot demonstrations. The study uses a newly collected realistic tool-use dataset with hundreds of API tools and evaluates performance on six tasks spanning language and vision modalities. The approach tests the ability of LLMs to incorporate new tools and re-invent functionalities of recent models solely from their documentation.

## Key Results
- Zero-shot prompts with tool documentation achieved performance on par with or better than few-shot demonstrations
- Zero-shot documentation significantly outperformed few-shot prompts without documentation on a dataset of hundreds of API tools
- LLMs could seamlessly incorporate new tools like GroundingDINO, SAM, and XMem for tasks like image editing and video tracking without further demonstrations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tool documentation provides sufficient guidance for LLMs to use new tools effectively without demonstrations
- Mechanism: The documentation serves as a neutral instruction set that describes tool functionality and usage patterns, allowing the LLM to infer how to invoke tools correctly through pattern matching and reasoning
- Core assumption: LLMs can effectively parse and understand documentation text to extract actionable tool usage information
- Evidence anchors:
  - [abstract]: "zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage"
  - [section 4.1]: "when provided with tool docs, the model is able to maintain stable performance as we strip away the number of demos used"
  - [corpus]: Weak - the corpus papers focus on tool usage but don't specifically validate documentation-based approaches

### Mechanism 2
- Claim: Documentation reduces performance sensitivity to demonstration selection
- Mechanism: By providing comprehensive tool descriptions rather than specific usage examples, documentation eliminates the need for careful demonstration curation and reduces the risk of bias from suboptimal examples
- Core assumption: The LLM can generalize from documentation descriptions to appropriate usage patterns across diverse scenarios
- Evidence anchors:
  - [abstract]: "tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation"
  - [section 4.1]: "without tool docs, the model performance is very sensitive to the number of demos used"
  - [corpus]: Weak - the corpus contains related tool usage papers but lacks specific evidence about documentation's role in reducing sensitivity to demonstration selection

### Mechanism 3
- Claim: Documentation enables efficient scaling to large tool sets
- Mechanism: Unlike demonstrations which require curating examples for each tool combination, documentation provides a scalable way to introduce multiple tools without combinatorial explosion in demonstration requirements
- Core assumption: LLMs can effectively manage and recall documentation for large numbers of tools when needed
- Evidence anchors:
  - [abstract]: "on a newly collected realistic tool-use dataset with hundreds of available tool APIs, we show that tool documentation is significantly more valuable than demonstrations"
  - [section 4.2]: "Due to the length constraints of the LLM we use, we cannot fit documentation of 200 tools in a single prompt. Therefore, we employ a simple TF-IDF search using the questions as queries to retrieve the most relevant documentations"
  - [corpus]: Weak - the corpus papers discuss tool usage but don't specifically address scaling challenges with documentation

## Foundational Learning

- Concept: In-context learning through documentation
  - Why needed here: This work relies on the LLM's ability to learn tool usage patterns from documentation rather than through fine-tuning or demonstrations
  - Quick check question: Can you explain how an LLM might use documentation to infer the correct parameters for a tool function without seeing any examples?

- Concept: Tool composition and planning
  - Why needed here: The LLM must not only understand individual tools but also plan sequences of tool usage to accomplish complex tasks
  - Quick check question: How would an LLM determine the order of tool execution when solving a multi-step problem using only documentation?

- Concept: Retrieval-augmented reasoning
  - Why needed here: With large tool sets, the LLM needs to retrieve relevant documentation dynamically rather than having all documentation in context
  - Quick check question: What retrieval strategy would you use to find the most relevant tool documentation for a given task description?

## Architecture Onboarding

- Component map: LLM Planner -> Documentation Repository -> Retrieval System -> Execution Environment -> Evaluation Framework
- Critical path: Task instruction → Documentation retrieval → LLM planning with documentation → Tool execution plan generation → Execution and evaluation
- Design tradeoffs:
  - Documentation length vs. LLM context window: Longer documentation provides more detail but may exceed context limits
  - Retrieval precision vs. recall: More precise retrieval may miss relevant tools, while broader retrieval may include irrelevant documentation
  - Zero-shot vs. few-shot balance: Documentation-only approaches may miss nuanced usage patterns that demonstrations capture
- Failure signatures:
  - Poor tool selection: LLM chooses inappropriate tools for the task
  - Incorrect parameter usage: LLM uses wrong parameters or syntax when invoking tools
  - Missing steps: LLM fails to include necessary intermediate steps in the execution plan
  - Hallucination: LLM invents tools or parameters that don't exist
- First 3 experiments:
  1. Compare zero-shot documentation performance against zero-shot without documentation on a simple task with known tools
  2. Test documentation retrieval effectiveness by measuring how often the correct tool documentation is retrieved for given tasks
  3. Evaluate the impact of documentation quality by comparing performance with complete vs. truncated documentation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of tool documentation affect LLM performance in zero-shot tool usage?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that documentation quality impacts performance but does not provide a detailed analysis of how different documentation qualities (e.g., length, clarity, completeness) specifically affect LLM performance. It only notes that longer documentation up to a certain point improves performance.
- What evidence would resolve it: A systematic study varying the quality and length of documentation and measuring the corresponding changes in LLM performance would clarify the impact.

### Open Question 2
- Question: Can the findings on zero-shot tool usage with documentation be generalized to other types of AI models beyond LLMs, such as small language models or specialized models?
- Basis in paper: Inferred
- Why unresolved: The paper focuses on large language models (LLMs) like ChatGPT and does not explore whether the benefits of tool documentation apply to other AI models or smaller language models.
- What evidence would resolve it: Conducting similar experiments with different types of AI models and comparing their performance with and without documentation would determine generalizability.

### Open Question 3
- Question: What are the limitations of tool documentation in enabling LLMs to use tools effectively, and how can these limitations be addressed?
- Basis in paper: Explicit
- Why unresolved: While the paper highlights the effectiveness of tool documentation, it does not extensively discuss the limitations or potential drawbacks, such as the challenges with lengthy documents or the need for human effort in creating documentation.
- What evidence would resolve it: Identifying specific scenarios where documentation fails or is less effective and proposing solutions or improvements to address these limitations would provide a clearer understanding.

## Limitations
- The evaluation relies on a newly collected dataset of API tools without detailed description of its diversity, scale, or potential biases
- The approach's effectiveness for highly complex or novel tools that lack clear documentation patterns remains untested
- The retrieval-augmented approach for large tool sets (200+ tools) introduces additional complexity and potential failure modes not fully explored in the evaluation

## Confidence

**High confidence**: The claim that zero-shot prompts with tool documentation can achieve comparable or superior performance to few-shot demonstrations is well-supported by systematic experiments across six tasks. The comparison methodology is sound and the results are consistent across different evaluation scenarios.

**Medium confidence**: The assertion that tool documentation is "significantly more valuable" than demonstrations is supported but may be sensitive to the specific tasks and tools evaluated. The performance advantage observed could vary with different tool domains or documentation quality.

**Low confidence**: The claim about automatic knowledge discovery through documentation-based tool usage is largely anecdotal and based on the LLM's ability to "re-invent" recent model functionalities. This requires more rigorous validation to confirm genuine novel capability discovery rather than pattern matching.

## Next Checks

1. **Documentation quality sensitivity test**: Systematically vary the completeness and clarity of tool documentation to determine the minimum documentation quality threshold required for effective zero-shot tool usage, measuring performance degradation as documentation quality decreases.

2. **Cross-domain generalization validation**: Evaluate the approach on tool sets from completely different domains (e.g., scientific computing, financial analysis) to test whether the zero-shot documentation advantage generalizes beyond the evaluated vision and language tasks.

3. **Hallucination detection and prevention**: Implement automated validation to detect when the LLM invents non-existent tools or parameters, measuring the frequency of such hallucinations and testing whether specific prompt engineering or retrieval strategies can reduce this failure mode.