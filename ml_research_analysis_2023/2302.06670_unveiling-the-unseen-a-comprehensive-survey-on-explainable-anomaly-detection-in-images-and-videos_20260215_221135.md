---
ver: rpa2
title: 'Unveiling the Unseen: A Comprehensive Survey on Explainable Anomaly Detection
  in Images and Videos'
arxiv_id: '2302.06670'
source_url: https://arxiv.org/abs/2302.06670
tags:
- anomaly
- detection
- methods
- explainable
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides the first comprehensive overview of explainable
  visual anomaly detection (X-VAD), covering both image (IAD) and video (VAD) modalities.
  The paper categorizes existing explainable methods into four groups: attention-based,
  generative model-based, reasoning-based, and foundation model-based.'
---

# Unveiling the Unseen: A Comprehensive Survey on Explainable Anomaly Detection in Images and Videos

## Quick Facts
- arXiv ID: 2302.06670
- Source URL: https://arxiv.org/abs/2302.06670
- Reference count: 7
- Key outcome: First comprehensive survey of explainable visual anomaly detection (X-VAD) covering both image and video modalities, categorizing methods into four groups and highlighting the need for quantitative evaluation of explanation quality

## Executive Summary
This survey provides the first comprehensive overview of explainable visual anomaly detection (X-VAD) for both images and videos. The paper categorizes existing explainable methods into four groups: attention-based, generative model-based, reasoning-based, and foundation model-based. While many methods provide explanations through attention maps, error maps, or high-level semantic reasoning, most lack quantitative evaluation of explanation quality. The survey identifies key challenges including developing benchmarks with ground-truth explanations, quantifying explanation effectiveness using metrics like IoU, and explaining self-supervised learning-based anomaly detection methods. This work addresses the critical need for interpretable AI in safety-critical applications like medical imaging and surveillance.

## Method Summary
The survey systematically categorizes explainable visual anomaly detection methods into four main approaches: attention-based regularization that forces models to focus on normal regions, input-perturbation techniques like ODIN and Mahalanobis that exploit anomaly sensitivity to perturbations, generative models (GANs, VAEs, diffusion models) that detect anomalies through reconstruction error, and reasoning-based methods using scene graphs and causal inference. For videos, methods extend to attention-based localization, temporal reasoning through object attributes, and intrinsically interpretable approaches. The survey highlights that while these methods provide various forms of explanations, most lack standardized quantitative metrics for evaluating explanation quality.

## Key Results
- Most explainable VAD methods lack quantitative evaluation of explanation quality
- Attention-based regularization can enforce models to encode normal patterns thoroughly
- Input-perturbation methods exploit the inherent sensitivity of anomalies to perturbations
- Generative models detect anomalies through poor reconstruction of abnormal data
- Future directions include developing benchmarks with ground-truth explanations and quantifying explanation effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention-based methods improve anomaly detection by forcing models to focus on normal regions during training, making anomalies more detectable through sparse attention in those areas.
- Mechanism: The model uses gradient-based attention maps to regularize training. By enforcing attention to cover all normal regions (complementary guided attention loss), the network is compelled to encode normal patterns thoroughly. During testing, anomalous regions receive sparse attention, highlighting them.
- Core assumption: Normal patterns are spatially coherent and can be systematically covered by attention regularization, while anomalies are inherently sparse and diverse.
- Evidence anchors:
  - [abstract] "attention-based regularization, input-perturbation techniques (like ODIN and Mahalanobis), and generative models (GANs, VAEs, diffusion models)"
  - [section] "the gradient-based attention to regularize the normal data training of the proposed convolutional adversarial V AE by proposing a loss called attention expansion to enforce the attention map to concentrate on all the normal parts"
  - [corpus] No direct evidence in corpus neighbors about attention-based regularization for anomaly detection.

### Mechanism 2
- Claim: Input-perturbation-based methods exploit the nature of anomalies being minority and diverse by making anomalies more sensitive to perturbations than normal data.
- Mechanism: Perturbations (either adversarial or opposite) are added to inputs, and the resulting score changes are measured. Since anomalies are less robust to perturbations, the magnitude of score changes differs between normal and anomalous samples, enabling detection.
- Core assumption: Anomalies are inherently less robust to input perturbations than normal data due to their minority status and diverse modes.
- Evidence anchors:
  - [abstract] "input-perturbation techniques (like ODIN and Mahalanobis)"
  - [section] "when applying the same amount of perturbation, the score alternations of them will be different from those of the normal data"
  - [corpus] No direct evidence in corpus neighbors about input-perturbation methods for anomaly detection.

### Mechanism 3
- Claim: Generative-model-based methods detect anomalies by measuring reconstruction error or likelihood, leveraging the fact that models trained only on normal data will poorly reconstruct anomalies.
- Mechanism: Models like GANs, VAEs, or diffusion models are trained on normal data. During inference, high reconstruction error or low likelihood indicates anomalies because the model has not learned to represent them.
- Core assumption: Models trained exclusively on normal data will inherently fail to reconstruct or assign high likelihood to anomalies.
- Evidence anchors:
  - [abstract] "generative model-based, reasoning-based, and foundation model-based"
  - [section] "the GAN fail to reconstruct the anomalies well and the discriminator will regard the anomalies as fake since they only encounter normal ones in the training phase"
  - [corpus] No direct evidence in corpus neighbors about generative models for anomaly detection.

## Foundational Learning

- Concept: Self-supervised learning
  - Why needed here: Many state-of-the-art anomaly detection methods use self-supervised learning to create pretext tasks that help models learn normal patterns without labels.
  - Quick check question: How does creating pretext tasks in self-supervised learning help in distinguishing normal from anomalous data?

- Concept: Attention mechanisms
  - Why needed here: Attention-based methods rely on highlighting important regions in images or videos, which is crucial for explaining why certain areas are flagged as anomalous.
  - Quick check question: What is the difference between post-hoc attention (calculated after training) and learned attention (integrated into the model during training)?

- Concept: Generative models
  - Why needed here: Generative models like GANs, VAEs, and diffusion models are used to model normal data distribution, and anomalies are detected based on reconstruction error or likelihood.
  - Quick check question: Why do generative models trained only on normal data tend to assign low likelihood to anomalies?

## Architecture Onboarding

- Component map: Data preprocessing -> Model architecture (attention/generative/reasoning) -> Explanation generation (attention maps/error maps/semantic reasoning) -> Evaluation metrics (IoU/faithfulness/stability)
- Critical path: Train model on normal data only -> Generate explanations during inference -> Evaluate explanation quality quantitatively
- Design tradeoffs:
  - Attention-based: High interpretability but may require careful regularization
  - Generative-based: Good reconstruction but may overfit normal data
  - Reasoning-based: High-level explanations but requires complex object recognition
- Failure signatures:
  - Attention maps not highlighting anomalies
  - Generative models reconstructing anomalies well
  - Explanations not aligning with human intuition
- First 3 experiments:
  1. Implement a simple VAE on MVTec-AD and visualize reconstruction errors
  2. Add attention regularization to an existing autoencoder and compare attention maps
  3. Test input-perturbation methods (ODIN/Mahalanobis) on a small OOD dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we quantify the effectiveness of explanation quality in explainable visual anomaly detection methods?
- Basis in paper: [explicit] The paper explicitly states that "none of them has quantified their explanation performance" and suggests using metrics like Intersection Over Union (IoU) for attention-based methods.
- Why unresolved: Current methods lack standardized quantitative metrics for evaluating explanation quality, relying instead on qualitative examples or indirect measures like anomaly detection accuracy.
- What evidence would resolve it: Development and validation of standardized benchmarks with ground-truth explanation annotations, coupled with quantitative metrics specifically designed to evaluate explanation quality in visual anomaly detection.

### Open Question 2
- Question: What are the key challenges in explaining self-supervised learning-based anomaly detection methods?
- Basis in paper: [explicit] The paper identifies this as a future direction, noting that "there exist few work clearly explaining why creating pretext tasks can improve anomaly detection performance."
- Why unresolved: Self-supervised methods learn through auxiliary tasks that don't directly optimize for anomaly detection, making it difficult to interpret how these tasks help distinguish normal from abnormal data.
- What evidence would resolve it: Theoretical analysis linking pretext task performance to anomaly detection capability, empirical studies showing how normal/abnormal samples behave differently on auxiliary tasks, and visualization techniques to trace the learning process.

### Open Question 3
- Question: How can explainable visual anomaly detection methods handle background-dependent anomalies in real-world scenarios?
- Basis in paper: [explicit] The paper identifies the need for new benchmarks that can handle "deceptive cases" where normal objects/behaviors in one context might be abnormal in another (e.g., boxing in a classroom vs. a ring).
- Why unresolved: Current datasets and methods are too simplistic and context-independent, failing to capture the complex relationship between background, objects, and their normal/abnormal status.
- What evidence would resolve it: Development of more realistic datasets with context-dependent anomalies, methods that can perform reasoning about object-background relationships, and evaluation protocols that test contextual understanding.

## Limitations

- Most methods lack quantitative evaluation of explanation quality
- Ground truth explanations for benchmark datasets are not widely available
- Many cited methods lack specific implementation details for faithful reproduction

## Confidence

- **High confidence**: The existence of the problem (need for explainable anomaly detection in safety-critical applications) and the general categorization of methods
- **Medium confidence**: The description of method categories and their general mechanisms
- **Low confidence**: Quantitative claims about method performance and explanation quality due to lack of standardized benchmarks

## Next Checks

1. **Benchmark creation**: Create a small benchmark dataset with ground truth anomaly explanations (e.g., pixel-level masks showing why regions are anomalous) on existing datasets like MVTec-AD to enable quantitative evaluation of explanation quality.

2. **Cross-method comparison**: Implement 2-3 representative methods from different categories (e.g., attention-based Grad-CAM, input-perturbation ODIN, and generative VAE) on the same dataset and compare both detection performance and explanation quality using IoU metrics where possible.

3. **Explanation faithfulness study**: Conduct a user study where humans evaluate whether the generated explanations (attention maps, error maps) correctly identify the anomalous regions compared to their visual inspection, establishing baseline faithfulness metrics for future work.