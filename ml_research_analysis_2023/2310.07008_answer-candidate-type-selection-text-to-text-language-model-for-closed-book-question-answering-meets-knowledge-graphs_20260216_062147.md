---
ver: rpa2
title: 'Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book
  Question Answering Meets Knowledge Graphs'
arxiv_id: '2310.07008'
source_url: https://arxiv.org/abs/2310.07008
tags:
- answer
- text-to-text
- question
- candidate
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses limitations of Text-to-Text language models
  for Knowledge Graph Question Answering (KGQA), specifically their poor performance
  on questions with less popular entities. The proposed solution, Answer Candidate
  Type (ACT) Selection, leverages the types of generated answers to filter and re-rank
  candidates using Wikidata's "instanceof" property.
---

# Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs

## Quick Facts
- arXiv ID: 2310.07008
- Source URL: https://arxiv.org/abs/2310.07008
- Reference count: 16
- Primary result: ACT Selection improves KGQA performance, achieving Hit@1 scores comparable to specialized methods and ChatGPT

## Executive Summary
This work addresses the limitation of Text-to-Text language models in Knowledge Graph Question Answering (KGQA), particularly their poor performance on questions involving less popular entities. The proposed Answer Candidate Type (ACT) Selection method leverages the semantic types of generated answer candidates to filter and re-rank them using Wikidata's "instance_of" property. By combining type overlap, entity proximity, Text-to-Text confidence, and question-property similarity scores, ACT Selection consistently improves performance across three one-hop datasets and achieves results comparable to specialized KGQA methods and ChatGPT in zero-shot settings.

## Method Summary
The method generates diverse answer candidates using a Text-to-Text model with Diverse Beam Search, extracts their types from Wikidata's instance_of property, and employs a scoring system incorporating type overlap, entity proximity, Text-to-Text confidence, and question-property similarity to select the final answer. The pipeline involves fine-tuning Text-to-Text models on KGQA datasets, generating candidates, extracting and aggregating types, performing entity linking to obtain neighbors, computing the four scores, and selecting the highest-scoring candidate.

## Key Results
- ACT Selection consistently improves KGQA performance across SQWD, RuBQ, and Mintaka datasets
- On SQWD, T5-large-ssm with ACT Selection achieves Hit@1 of 47.42, significantly outperforming the base model (23.66)
- Results approach the performance of larger models like T5-11b-ssm-nq (38.51) and specialized KGQA methods

## Why This Works (Mechanism)

### Mechanism 1
Text-to-Text models often generate plausible but incorrect answers that share the correct answer's type. Even when the model outputs the wrong entity, the answer often belongs to the correct semantic category, allowing the model to act as a type classifier. The model's parametric knowledge includes general entity type information, even if it lacks specific entity facts.

### Mechanism 2
Aggregating instance_of types from diverse beam search candidates yields a robust answer type prediction. By running Diverse Beam Search and collecting types from multiple candidates, the method identifies the most frequent and representative types. This aggregation acts as a type clustering step, with the correct answer type appearing frequently among the diverse candidate set due to distributional similarity in outputs.

### Mechanism 3
Combining type overlap, entity proximity, Text-to-Text confidence, and question-property similarity scores yields effective candidate ranking. A weighted sum of four scores ranks candidates, leveraging both semantic and structural KG cues. Each score captures a different signal about answer correctness, and their combination is more discriminative than any single score alone.

## Foundational Learning

- **Knowledge Graph Question Answering (KGQA)**
  - Why needed here: The method bridges Text-to-Text QA and KGQA by leveraging KG structure after generating text-based candidates
  - Quick check question: What is the difference between a simple QA system and a KGQA system?

- **Entity Typing and Instance-of Relations**
  - Why needed here: The method relies on Wikidata's instance_of property to determine entity types for filtering
  - Quick check question: How does the instance_of property differ from subclass_of in knowledge graphs?

- **Diverse Beam Search**
  - Why needed here: It generates a set of diverse answer candidates, increasing the chance of capturing the correct type
  - Quick check question: Why might diverse beam search produce more useful candidates than standard beam search in this context?

## Architecture Onboarding

- **Component map**: Text-to-Text model (e.g., T5, BART) → Diverse Beam Search → Answer Candidate Typing (type extraction + aggregation) → Entity Linking (neighbor enrichment) → Candidate Scorer (4 scores) → Final Answer Selection
- **Critical path**: Generate candidates → Extract and aggregate types → Score and rank candidates → Select top candidate
- **Design tradeoffs**: Larger beam sizes increase diversity but cost more computation; using instance_of vs. more complex typing schemas trades simplicity for potential coverage; relying on neighbors assumes one-hop connectivity; multi-hop would require subgraph expansion
- **Failure signatures**: Low type overlap score indicates the model's outputs are not type-consistent; low neighbor score suggests missing entity linking or sparse graph; poor final score despite high Text-to-Text confidence indicates the scoring weights need tuning
- **First 3 experiments**:
  1. Run Diverse Beam Search with 200 beams on a sample question and inspect type distribution
  2. Test the type extraction and aggregation module with a small set of known entities
  3. Evaluate the impact of each score component via ablation on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does the ACT Selection method perform on multi-hop questions?
- Basis in paper: The paper mentions that the method was only tested on one-hop questions and suggests that it may be adaptable to multi-hop questions, but further testing is needed
- Why unresolved: The authors explicitly state that the current study only tested the method on one-hop questions and that the approach's effectiveness on multi-hop questions needs to be evaluated
- What evidence would resolve it: Testing the ACT Selection method on a dataset of multi-hop questions and comparing its performance to other KGQA methods

### Open Question 2
- Question: Does the ACT Selection method improve performance on less popular entities?
- Basis in paper: The paper states that Text-to-Text models struggle with less popular entities and that the ACT Selection method aims to address this issue
- Why unresolved: While the paper presents evidence that the method improves performance on one-hop questions, it does not explicitly show that it specifically benefits less popular entities
- What evidence would resolve it: Analyzing the performance of the ACT Selection method on questions with less popular entities compared to questions with popular entities

### Open Question 3
- Question: How does the computational cost of the ACT Selection method compare to other KGQA methods?
- Basis in paper: The paper mentions that diverse beam search, used in the ACT Selection method, is computationally more expensive than other methods
- Why unresolved: The paper does not provide a direct comparison of the computational cost of the ACT Selection method to other KGQA methods
- What evidence would resolve it: Conducting experiments to measure the runtime and resource usage of the ACT Selection method compared to other KGQA methods on the same datasets

## Limitations

- The method's effectiveness for multi-hop questions is not evaluated, as experiments focus only on one-hop datasets
- The reliance on Wikidata's instance_of property may limit applicability to KGs with different typing schemas
- The overhead of entity linking and type extraction is not quantified relative to computational cost

## Confidence

- **High Confidence**: ACT Selection consistently improves KGQA performance across multiple datasets (SQWD, RuBQ, Mintaka) compared to baseline Text-to-Text models
- **Medium Confidence**: The mechanism of leveraging type overlap from diverse beam search candidates is effective, but the exact contribution of each score component in the weighted sum is not fully isolated
- **Low Confidence**: Claims about computational efficiency relative to specialized KGQA methods lack quantitative validation

## Next Checks

1. **Score Weight Sensitivity Analysis**: Systematically vary the weighting coefficients for the four scores (type overlap, neighbor proximity, Text-to-Text confidence, question-property similarity) and evaluate their impact on final performance to determine optimal weighting
2. **Multi-Hop Question Evaluation**: Extend the ACT Selection pipeline to handle multi-hop questions by incorporating subgraph expansion and evaluate performance on datasets like WebQSP or ComplexWebQuestions
3. **Typing Schema Generalization**: Replace Wikidata's instance_of property with alternative typing schemas (e.g., YAGO, DBpedia) and assess whether ACT Selection remains effective, testing the method's robustness to different KG structures