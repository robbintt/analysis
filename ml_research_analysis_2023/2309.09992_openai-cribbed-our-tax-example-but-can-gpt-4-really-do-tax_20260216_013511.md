---
ver: rpa2
title: OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?
arxiv_id: '2309.09992'
source_url: https://arxiv.org/abs/2309.09992
tags:
- gpt-4
- sara
- livestream
- statutes
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OpenAI used a tax example from the authors' academic dataset (SARA)
  in its GPT-4 livestream, leading to incorrect tax calculations due to edited tax
  code sections. The authors tested GPT-4 on all 376 SARA cases and found it correctly
  calculated tax liabilities within $1 for only 31% of cases.
---

# OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?

## Quick Facts
- arXiv ID: 2309.09992
- Source URL: https://arxiv.org/abs/2309.09992
- Reference count: 0
- Primary result: GPT-4 correctly calculated tax liabilities within $1 for only 31% of test cases

## Executive Summary
OpenAI used a tax example from the authors' academic dataset (SARA) in its GPT-4 livestream, leading to incorrect tax calculations due to edited tax code sections. The authors tested GPT-4 on all 376 SARA cases and found it correctly calculated tax liabilities within $1 for only 31% of cases. GPT-4 achieved 67% accuracy on true/false statutory reasoning questions and made errors primarily from misreading statutes rather than mathematical mistakes. The model often confused personal exemption rules and standard deduction calculations. While GPT-4 represents progress in AI statutory reasoning, it remains unreliable for tax preparation without human oversight.

## Method Summary
The authors tested GPT-4 (model gpt-4-0314) on the complete SARA v2 dataset containing 376 tax calculation cases. They prompted GPT-4 with simplified IRC statutes plus case facts and questions, then compared the model's answers to SARA's correct answers. The evaluation measured accuracy for both tax liability calculations (within $1 tolerance) and true/false statutory reasoning questions. All tests used temperature=0 to ensure reproducibility.

## Key Results
- GPT-4 correctly calculated tax liabilities within $1 for only 31% of 376 SARA cases
- GPT-4 achieved 67% accuracy on true/false statutory reasoning questions
- All calculation errors resulted from misreading statutes rather than mathematical mistakes
- GPT-4 refused to answer only 2 of 100 tax calculation cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 fails to correctly apply tax law due to misreading statutory text rather than computational errors
- Mechanism: The model lacks precise semantic comprehension of legal provisions, leading to incorrect application of exemptions and deductions despite having access to simplified statutory text
- Core assumption: GPT-4's reasoning over legal text operates at a level insufficient for accurate tax calculation
- Evidence anchors:
  - GPT-4 correctly calculated tax liabilities within $1 for only 31% of cases
  - Not one of the errors was mathematical; all involved misreading the statutes
  - GPT-4 can make mistakes in reading statutes
- Break condition: The model achieves near-perfect accuracy on statutory reasoning tasks or demonstrates consistent correct application of tax law provisions

### Mechanism 2
- Claim: The use of heavily edited, simplified statutes leads to incorrect tax calculations when applied to real-world scenarios
- Mechanism: The simplified statutes in SARA lack critical provisions (like TCJA rates) that GPT-4 needs to make correct calculations, and the model doesn't recognize when real-world statutes differ from the simplified versions
- Core assumption: GPT-4 cannot distinguish between simplified academic examples and actual tax law requirements
- Evidence anchors:
  - We edited out section 1(j), which contains the reduced Tax Cuts and Jobs Act rates for 2018-2025
  - OpenAI incorrectly thought GPT-4 had correctly calculated the tax liability because its answer matched the SARA answer
  - GPT-4 consistently answers $10,597.68, just as during the livestream
- Break condition: GPT-4 demonstrates awareness of statutory differences between simplified and actual versions and applies appropriate rules

### Mechanism 3
- Claim: GPT-4's performance on tax reasoning tasks is significantly better than random guessing but still unreliable for practical use
- Mechanism: The model achieves 67% accuracy on true/false statutory reasoning questions, showing some capability for legal reasoning, but its error rate remains too high for tax preparation applications
- Core assumption: A 67% accuracy rate represents meaningful progress but insufficient reliability for tax preparation
- Evidence anchors:
  - GPT-4 got 186 correct, amounting to 67 percent accuracy
  - A simple coin flip on true/false questions would result in 50 percent accuracy on average
  - GPT-4 is impressive... able to take raw tax statutes and facts and correctly calculate the tax liability around one-third of the time
- Break condition: The model achieves accuracy rates comparable to or exceeding human tax professionals

## Foundational Learning

- Concept: Statutory reasoning
  - Why needed here: Understanding how AI models interpret and apply legal text is crucial for evaluating their reliability in tax calculations
  - Quick check question: Can you explain why GPT-4 misread section 151 to provide for no personal exemption for years between 2010 and 2017?

- Concept: Tax law structure and provisions
  - Why needed here: Knowledge of tax law components (exemptions, deductions, rates) helps identify where GPT-4 makes errors
  - Quick check question: What is the difference between a personal exemption and a standard deduction in tax law?

- Concept: AI language model limitations
  - Why needed here: Understanding the inherent limitations of LLMs in legal reasoning helps set appropriate expectations for their use
  - Quick check question: Why might an AI model be more likely to make errors when applying edited versus complete statutory text?

## Architecture Onboarding

- Component map: Input statutes -> GPT-4 model -> Reasoning engine -> Tax liability calculation -> Output answer
- Critical path: Receive tax facts and statutes → Parse statutory text → Apply provisions to facts → Calculate tax liability → Generate response
- Design tradeoffs:
  - Simplified statutes vs. comprehensive statutes (easier processing vs. accuracy)
  - Temperature setting (creativity vs. reproducibility)
  - Model size and capability vs. computational cost
- Failure signatures:
  - Incorrect tax calculations due to misreading statutory provisions
  - Consistent errors on specific tax law concepts (exemptions, deductions)
  - Failure to recognize when simplified statutes differ from actual law
- First 3 experiments:
  1. Test GPT-4 on the full SARA dataset with actual IRC sections instead of simplified versions
  2. Vary temperature settings to assess impact on reproducibility and accuracy
  3. Compare GPT-4's performance on synthetic statutes (like in Blair-Stanek et al.) versus actual tax law

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific improvements in statutory reasoning would enable AI models to reliably calculate tax liabilities for real-world scenarios?
- Basis in paper: The authors note GPT-4 gets tax liabilities exactly right only about one-third of the time and misreads statutes in over 10% of cases
- Why unresolved: Current models like GPT-4 make systematic errors in interpreting statutory language and fail to account for real-world complexity
- What evidence would resolve it: A model achieving 95%+ accuracy on both SARA cases and real IRC scenarios while demonstrating consistent correct interpretation of statutory provisions

### Open Question 2
- Question: How can AI systems be designed to identify when they lack sufficient information to answer tax questions reliably?
- Basis in paper: GPT-4 refused to answer only 2 of 100 tax calculation cases, suggesting limited self-awareness of its limitations
- Why unresolved: Current models tend to provide answers even when information is insufficient, as evidenced by GPT-4's 98% answer rate
- What evidence would resolve it: A model that consistently identifies its knowledge gaps and declines to answer when missing critical information, with accuracy rates comparable to human tax professionals

### Open Question 3
- Question: What architectural or training approaches would enable AI to better distinguish between relevant and irrelevant statutory provisions in tax calculations?
- Basis in paper: GPT-4 ignored relevant FUTA tax provisions in 2 cases despite clear facts indicating their applicability
- Why unresolved: Models struggle with selective attention to relevant statutes, often missing or misapplying provisions
- What evidence would resolve it: A model demonstrating 95%+ accuracy in identifying and correctly applying only the relevant statutory provisions for any given tax scenario

## Limitations

- Evaluation relies on a single academic dataset (SARA) with simplified statutes that may not generalize to real-world tax code complexity
- Only tested one model configuration (gpt-4-0314 with temperature=0), leaving uncertainty about performance under different settings
- Authors' use of their own dataset raises potential concerns about selection bias or overfitting to known problem patterns

## Confidence

**High Confidence**: GPT-4 correctly calculated tax liabilities within $1 for only 31% of cases, and all errors involved misreading statutes rather than mathematical mistakes. These findings are directly supported by systematic testing of the entire SARA dataset.

**Medium Confidence**: GPT-4 achieved 67% accuracy on true/false statutory reasoning questions, which represents meaningful progress but insufficient reliability for practical tax preparation. The accuracy rate is based on comprehensive testing but may not generalize to other legal domains or more complex tax scenarios.

**Low Confidence**: Claims about why OpenAI used this specific example (whether due to memorization or random selection) remain speculative. The authors acknowledge uncertainty about whether OpenAI's choice reflected model memorization or simple coincidence.

## Next Checks

1. **Real Code Validation**: Test GPT-4 on actual IRC provisions rather than the simplified SARA statutes to determine if performance improves with complete tax code context.

2. **Temperature Sensitivity**: Evaluate how varying temperature settings (0, 0.5, 1.0) affects GPT-4's accuracy and reproducibility on the same SARA cases to assess the impact of stochasticity on tax calculations.

3. **Generalization Testing**: Apply GPT-4 to tax scenarios outside the SARA dataset, including cases involving different tax concepts (estate tax, corporate tax) and more complex fact patterns to assess the model's broader reasoning capabilities.