---
ver: rpa2
title: Evaluating Large Language Model Creativity from a Literary Perspective
arxiv_id: '2312.03746'
source_url: https://arxiv.org/abs/2312.03746
tags:
- cupboard
- world
- user
- like
- creativity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study develops interactive and multi-voice prompting strategies
  to generate literary text with large language models (LLMs). By interleaving scene
  setting, instructions, style samples, and critical discussion, the authors produce
  sophisticated fragments of a time-travel novel.
---

# Evaluating Large Language Model Creativity from a Literary Perspective

## Quick Facts
- arXiv ID: 2312.03746
- Source URL: https://arxiv.org/abs/2312.03746
- Reference count: 23
- One-line primary result: LLM-generated text can be novel, valuable, and autonomous, with sophistication mirroring the sophistication of prompting

## Executive Summary
This study explores the creative writing capabilities of large language models (LLMs) through interactive and multi-voice prompting strategies. By developing prompts that interleave scene setting, instructions, style samples, and critical discussion, the authors generate sophisticated fragments of a time-travel novel. Qualitative literary analysis reveals the LLM-generated text to be novel, valuable, and autonomous, with unexpected sophistication in self-critique. The results demonstrate that the sophistication of LLM outputs mirrors the sophistication of the prompting, supporting the potential for LLMs to supplement and enhance human creativity in writing.

## Method Summary
The study employs interactive and multi-voice prompting strategies to generate literary text with GPT-4. In the creative dialogue experiment, the LLM is used as a creative writing assistant, iteratively refining the opening paragraph of a novel. In the multi-voice generation experiment, the LLM generates both the narrative and the critical commentary, simulating a dialogue between an author and a mentor. The temperature parameter is varied to influence the creativity of the generated text. The study focuses on a single case study of a time-travel novel involving characters Effie and George.

## Key Results
- LLM-generated text can be novel, valuable, and autonomous, with unexpected sophistication in self-critique.
- The sophistication of LLM outputs mirrors the sophistication of the prompting.
- LLMs can engage in sophisticated self-critique and creative collaboration when prompted to adopt multiple roles.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sophistication of LLM outputs directly mirrors the sophistication of the prompting.
- Mechanism: Complex, multi-layered prompts that interleave scene-setting, instructions, style samples, and critical discussion guide the LLM to produce text with comparable complexity and nuance.
- Core assumption: LLMs can parse and respond to sophisticated, multi-modal prompts that mix narrative elements, critical commentary, and stylistic guidance.
- Evidence anchors:
  - [abstract] "Our findings lend support to the view that the sophistication of the results that can be achieved with an LLM mirrors the sophistication of the prompting."
  - [section] The creative dialogue experiment shows ChatGPT responding to increasingly sophisticated prompts with text that incorporates imagery, metaphor, and character perspective.
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.372, average citations=0.0. Top related titles include "Evaluating the Creativity of LLMs in Persian Literary Text Generation" and "Optimising ChatGPT for creativity in literary translation."
- Break condition: If the model fails to parse the layered prompt structure or responds with generic or irrelevant content, the mirroring effect breaks down.

### Mechanism 2
- Claim: Raising the temperature parameter in LLMs leads to more experimental and potentially creative outputs.
- Mechanism: Higher temperature settings increase the randomness of word selection, allowing the model to generate novel word combinations and unexpected imagery.
- Core assumption: The increased randomness introduced by higher temperature settings will produce semantically coherent and contextually relevant text, not just gibberish.
- Evidence anchors:
  - [section] "Raising the temperature parameter caused it to produce neologisms that, far from being nonsensical, could be interpreted as semantically coherent and contextually relevant."
  - [section] The high-temperature responses include creative uses of vocabulary and unconventional grammar that, while not always immediately comprehensible, contribute to the overall texture and imagery of the passage.
  - [corpus] "CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation" suggests evaluating creativity in LLM outputs.
- Break condition: If the output becomes too incoherent or loses all semantic meaning, the temperature effect becomes counterproductive.

### Mechanism 3
- Claim: LLMs can engage in sophisticated self-critique and creative collaboration.
- Mechanism: By prompting the model to adopt multiple roles (author and mentor), it can generate both creative text and meaningful feedback, even introducing new elements to the narrative.
- Core assumption: The LLM can maintain distinct voices and perspectives when role-playing, and can provide substantive feedback that goes beyond generic praise or criticism.
- Evidence anchors:
  - [section] "In the multi-voice mode, it spontaneously introduced an entirely new character, although nowhere in the prompting was there any hint of such a possibility."
  - [section] The model-generated mentor comments on the author's text in ways that are "meaningful" and sometimes "evidence-based."
  - [corpus] "Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction" suggests evaluating LLM outputs for literary merit.
- Break condition: If the model's self-critique becomes repetitive, superficial, or fails to engage meaningfully with the text, the collaborative potential is limited.

## Foundational Learning

- Concept: Understanding of literary criticism and its application to evaluating creative text.
  - Why needed here: The study relies on qualitative literary analysis to assess the creativity and sophistication of LLM-generated text.
  - Quick check question: Can you identify examples of imagery, metaphor, and narrative perspective in a given text passage?

- Concept: Familiarity with large language models and their basic functionality.
  - Why needed here: The study involves using LLMs for creative writing, requiring an understanding of how they generate text and how parameters like temperature affect their output.
  - Quick check question: What is the difference between a dialogue agent and accessing a model directly via API?

- Concept: Knowledge of the four-Ps framework for assessing creativity in AI systems.
  - Why needed here: The discussion section situates the study within the broader context of computational creativity using this framework.
  - Quick check question: How do the Person, Process, Product, and Press dimensions apply to the use of LLMs for creative writing?

## Architecture Onboarding

- Component map: User (human writer) -> Prompt structure (scene-setting, instructions, style samples, critical discussion) -> LLM (GPT-4) -> Generated text
- Critical path: User composes prompt → LLM generates text → User evaluates output → User refines prompt or adjusts parameters → Repeat
- Design tradeoffs: Using a proprietary model (GPT-4) offers high capability but limits transparency about the training data and model architecture. The study focuses on a single case study, which provides depth but limits generalizability.
- Failure signatures: Output that is generic, repetitive, or fails to engage with the prompt's complexity. Incoherent text when temperature is too high. Superficial or irrelevant feedback in the multi-voice mode.
- First 3 experiments:
  1. Creative Dialogue: Use ChatGPT as a creative writing assistant, iteratively refining the opening paragraph of a novel.
  2. Raising the Temperature: Vary the temperature parameter when generating the second paragraph, observing the effect on style and imagery.
  3. Multi-Voice Generation: Turn the dialogue from the first experiment into a transcript and prompt GPT-4 to adopt both author and mentor roles, generating self-critique.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sophistication of LLM outputs scale with the complexity of the prompts, and is there a point of diminishing returns?
- Basis in paper: [explicit] The authors note that "the sophistication of the results that can be achieved with an LLM mirrors the sophistication of the prompting."
- Why unresolved: The paper provides a qualitative case study with complex prompts but doesn't quantify the relationship between prompt complexity and output sophistication or identify a point where additional complexity yields minimal improvement.
- What evidence would resolve it: A systematic study varying prompt complexity (e.g., number of instructions, use of literary devices, character development) and measuring output sophistication (e.g., through literary analysis, reader comprehension, creativity metrics) would establish this relationship.

### Open Question 2
- Question: Can LLM-generated text be effectively used as a standalone creative work, or is it primarily valuable as a collaborative tool in the writing process?
- Basis in paper: [explicit] The authors discuss using LLM outputs as "creative starting points and provocations" and a "tool for playfully investigating regions of their imagination," suggesting a collaborative role rather than standalone use.
- Why unresolved: The paper doesn't explore whether LLM-generated text, particularly at higher temperature settings, could be adapted into complete, publishable works without significant human intervention.
- What evidence would resolve it: A study where writers attempt to develop LLM-generated fragments into complete stories or novels, with and without human intervention, would determine the standalone viability of LLM outputs.

### Open Question 3
- Question: How do the ethical considerations surrounding LLMs, such as copyright and potential displacement of human creativity, impact their use in creative writing?
- Basis in paper: [explicit] The authors acknowledge "a raft of ethical issues" including copyright concerns and the potential impact on creative industries, stating that their findings "do not endorse the substitution of LLMs for human creativity."
- Why unresolved: The paper doesn't explore how these ethical concerns influence the adoption and perception of LLMs in creative writing or propose potential solutions or guidelines for responsible use.
- What evidence would resolve it: Surveys of writers, publishers, and readers about their views on LLM use in creative writing, coupled with analysis of copyright laws and industry practices, would provide insights into the ethical landscape and potential paths forward.

## Limitations

- The study focuses on a single case study of a time-travel novel, limiting broader conclusions about LLM creativity across genres and styles.
- The use of GPT-4, a proprietary model with undisclosed training data, introduces uncertainty about the source of the generated text's creativity.
- The qualitative assessment of creativity relies on the authors' interpretation, which may not be universally applicable or representative of broader literary critical perspectives.

## Confidence

- Confidence: Low in the generalizability of the findings beyond this single case study.
- Confidence: Medium in the qualitative assessment of creativity.
- Confidence: Medium in the mechanism linking prompt sophistication to output quality.

## Next Checks

1. **Replication with different genres and styles**: Repeat the experiments using prompts for different literary genres (e.g., poetry, drama, non-fiction) and styles (e.g., minimalist, stream-of-consciousness, magical realism) to assess the robustness of the findings.

2. **Blind evaluation by literary experts**: Have literary scholars and creative writing professionals evaluate the LLM-generated text without knowledge of its origin, comparing it to human-authored works in the same genre. This would provide an external, unbiased assessment of the text's creativity and literary merit.

3. **A/B testing of prompt structures**: Systematically vary the complexity, length, and composition of the prompts (e.g., scene-setting vs. critical discussion vs. style samples) to isolate their individual and combined effects on the LLM's output. This would provide insights into the most effective prompt design strategies for eliciting creative text.