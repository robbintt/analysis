---
ver: rpa2
title: 'DEPHN: Different Expression Parallel Heterogeneous Network using virtual gradient
  optimization for Multi-task Learning'
arxiv_id: '2307.12519'
source_url: https://arxiv.org/abs/2307.12519
tags:
- task
- information
- different
- gating
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DEPHN improves multi-task recommendation models by combining heterogeneous
  feature interaction experts, explicit activation mapping, and task-correlation-aware
  gating. It enhances shared information representation while maintaining task-specific
  differentiation, validated on synthetic and real datasets.
---

# DEPHN: Different Expression Parallel Heterogeneous Network using virtual gradient optimization for Multi-task Learning

## Quick Facts
- arXiv ID: 2307.12519
- Source URL: https://arxiv.org/abs/2307.12519
- Reference count: 22
- Key outcome: DEPHN achieves logloss of 0.162253 and AUC of 0.663543 for CVR task on Ali-CCP dataset, outperforming state-of-the-art multi-task baselines.

## Executive Summary
DEPHN introduces a novel multi-task learning framework for recommendation systems that combines heterogeneous feature interaction experts, explicit activation mapping, and task-correlation-aware gating. The model constructs experts using different feature interaction methods (Cross Interaction, Field Interaction, DNN) in parallel to capture both task-specific and shared information patterns. By explicitly mapping activation values and introducing virtual gradient coefficients based on task correlations, DEPHN improves the representation of shared information flow while maintaining task-specific differentiation. The approach demonstrates superior performance on both synthetic and real-world datasets, achieving state-of-the-art results on the Ali-CCP competition dataset.

## Method Summary
DEPHN implements a multi-task learning architecture that uses heterogeneous feature interaction experts in the shared bottom layer, explicit activation mapping to enhance information representation, and virtual gradient coefficients to adjust gating values based on task relationships. The model processes input through embedding layers, applies soft selection gating, then routes features through parallel heterogeneous layers containing different feature interaction methods. Explicit mapping operations transform expert outputs using nonlinear functions, while virtual gradient coefficients modify backpropagation to focus on task similarity and differences. The architecture ends with task-specific towers that produce final predictions for each target behavior.

## Key Results
- DEPHN achieves logloss of 0.162253 and AUC of 0.663543 for CVR task on Ali-CCP dataset
- Outperforms state-of-the-art multi-task baselines including ESSM, MMoE, and PLE on multiple datasets
- Demonstrates improved interpretability for task relationships through explicit mapping and correlation-aware gating

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DEPHN improves generalization by using heterogeneous feature interaction experts in the shared bottom layer.
- Mechanism: Different feature interaction modules (Cross Interaction Layer, Field Interaction Layer, DNN) are combined in parallel to capture both task-specific and task-shared information patterns.
- Core assumption: Different feature interaction methods have complementary strengths in capturing task-specific vs. shared information.
- Evidence anchors:
  - [abstract] "DEPHN constructs the experts at the bottom of the model by using different feature interaction methods to improve the generalization ability of the shared information flow."
  - [section III.A.2] "In these methods, DNN is used as the task-sharing part for its capacity to extract task-independent semantic information by implicitly conducting high-order feature interaction."
  - [corpus] Weak - corpus papers discuss heterogeneous graph learning and parallel networks but not specifically this MTL expert construction approach.
- Break condition: If task-specific and shared information cannot be effectively separated, or if interaction modules interfere rather than complement each other.

### Mechanism 2
- Claim: Explicit mapping of activation values improves the representation of shared information flow.
- Mechanism: Activation values from each expert are explicitly mapped using nonlinear functions (raw mapping and trigonometric mapping) to strengthen the representation of shared information flow and provide more information integration schemes for subsequent gated units.
- Core assumption: The functional relationship between activation values across tasks can be approximated through explicit mapping.
- Evidence anchors:
  - [section III.B] "explicit mapping operations is carried out on the activation values output by expert k... From the perspective of function expansion, the explicit mapping module uses nonlinear functions... to fit the trend relationship and periodic relationship between the activation values of different tasks."
  - [section IV.C] "DEPHN explicitly maps public information to improve the expression dimension of shared information flow."
  - [corpus] Weak - corpus papers discuss mapping and transformation but not this specific activation value mapping approach.
- Break condition: If the explicit mapping introduces noise or if the assumed functional relationships don't exist in the data.

### Mechanism 3
- Claim: Virtual gradient coefficients improve task correlation handling by adjusting gating values based on task relationships.
- Mechanism: During backpropagation, a "virtual gradient coefficient" is applied to gated units based on task correlation measures and gating value differences, allowing the model to focus on task similarity and differences.
- Core assumption: The relationship between tasks can be measured and used to adjust gradients for better information flow control.
- Evidence anchors:
  - [section III.C] "Virtual Gradient is added in the training process, and the correlation measurement between tasks is transmitted back to the corresponding gated unit as a gradient, so that it can focus on the fitting of task similarity and differences."
  - [section IV.C] "In the training process of DEPHN, we introduced a 'virtual gradient coefficient' for the gated unit on the premise of not affecting the forward propagation value."
  - [corpus] Weak - corpus papers discuss gradient methods but not this specific virtual gradient approach for task correlation.
- Break condition: If task correlation measures are inaccurate or if the virtual gradient adjustment destabilizes training.

## Foundational Learning

- Concept: Multi-Task Learning (MTL) in recommendation systems
  - Why needed here: DEPHN is specifically designed for MTL scenarios in recommendation systems where multiple user behaviors need to be predicted simultaneously.
  - Quick check question: What are the two main considerations in MTL for recommendation systems according to the paper?

- Concept: Feature interaction methods in CTR prediction
  - Why needed here: DEPHN uses different feature interaction methods (CrossNet, Field Interaction, DNN) as heterogeneous experts.
  - Quick check question: What are the three types of feature interaction methods used in DEPHN's Parallel Heterogeneous Layers?

- Concept: Soft gating mechanisms
  - Why needed here: DEPHN uses a Soft Selection Gating (SSG) module to enhance embedding features for different structures and control information flow.
  - Quick check question: What is the purpose of the SSG module in DEPHN's architecture?

## Architecture Onboarding

- Component map: Input → Embedding → Soft Selection Gating → Parallel Heterogeneous Layers (experts) → Explicit Mapping → Virtual Gradient Gating → Task Towers → Output
- Critical path: The path from input features through the heterogeneous experts to the task-specific towers, with explicit mapping and virtual gradient adjustments.
- Design tradeoffs: Complexity vs. performance (more experts and mappings increase complexity but may improve results), interpretability vs. accuracy (explicit mapping provides interpretability but may limit flexibility).
- Failure signatures: Poor performance on one or more tasks, unstable training, inability to capture task correlations, overfitting on the training data.
- First 3 experiments:
  1. Replace the heterogeneous experts with a single shared DNN and measure performance degradation.
  2. Remove the explicit mapping and compare the model's ability to capture task relationships.
  3. Disable the virtual gradient and observe the impact on task correlation handling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DEPHN compare to other state-of-the-art multi-task learning models in real-world recommendation systems?
- Basis in paper: [inferred] The paper states that DEPHN achieves better performance than baseline models on artificial and real-world datasets, but does not provide a direct comparison with other state-of-the-art models.
- Why unresolved: The paper does not provide a direct comparison of DEPHN's performance with other state-of-the-art multi-task learning models in real-world recommendation systems.
- What evidence would resolve it: Conducting experiments comparing DEPHN's performance with other state-of-the-art multi-task learning models on real-world recommendation system datasets would provide evidence to resolve this question.

### Open Question 2
- Question: How does the choice of explicit mapping functions in DEPHN affect its performance in different recommendation scenarios?
- Basis in paper: [explicit] The paper discusses the use of explicit mapping functions in DEPHN to strengthen the representation of experts, but does not explore the impact of different explicit mapping functions on performance in different recommendation scenarios.
- Why unresolved: The paper does not provide an analysis of how the choice of explicit mapping functions affects DEPHN's performance in different recommendation scenarios.
- What evidence would resolve it: Conducting experiments with different explicit mapping functions in DEPHN and evaluating their performance in various recommendation scenarios would provide evidence to resolve this question.

### Open Question 3
- Question: How does the introduction of virtual gradient coefficients in DEPHN impact the convergence and stability of the model during training?
- Basis in paper: [explicit] The paper introduces virtual gradient coefficients in DEPHN to adjust the activation value based on task relevance and gated value differences, but does not provide an analysis of their impact on convergence and stability during training.
- Why unresolved: The paper does not provide an analysis of how the introduction of virtual gradient coefficients affects the convergence and stability of DEPHN during training.
- What evidence would resolve it: Conducting experiments to analyze the impact of virtual gradient coefficients on the convergence and stability of DEPHN during training would provide evidence to resolve this question.

## Limitations

- Limited empirical validation with only a small set of baseline comparisons, reducing confidence in superiority claims
- Architectural complexity with multiple novel components may make practical implementation challenging
- Reliance on task correlation measurement that may not generalize well across different datasets

## Confidence

- **High confidence** in the theoretical framework: The architectural components (heterogeneous experts, explicit mapping, soft gating) are well-motivated by existing literature and the mathematical formulations appear sound.
- **Medium confidence** in performance claims: The reported results on benchmark datasets are promising, but the limited baseline comparisons and lack of extensive ablation studies reduce confidence in the claimed improvements.
- **Low confidence** in practical implementation: The complexity of the model and the novelty of components like virtual gradient gating suggest significant implementation challenges that aren't fully addressed in the paper.

## Next Checks

1. **Ablation study**: Remove each key component (heterogeneous experts, explicit mapping, virtual gradient) individually to quantify their individual contributions to overall performance.

2. **Robustness testing**: Evaluate DEPHN's performance across datasets with varying task correlations and data distributions to assess generalization capabilities.

3. **Training stability analysis**: Conduct experiments to measure the impact of virtual gradient coefficients on training convergence and identify optimal hyperparameter ranges for practical deployment.