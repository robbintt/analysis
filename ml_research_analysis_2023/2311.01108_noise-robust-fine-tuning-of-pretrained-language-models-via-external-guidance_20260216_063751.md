---
ver: rpa2
title: Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance
arxiv_id: '2311.01108'
source_url: https://arxiv.org/abs/2311.01108
tags:
- noisy
- labels
- samples
- noise
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of fine-tuning PLMs with noisy
  labels, a common issue in real-world data annotation. The authors propose LAFT,
  a novel framework that leverages external guidance from LLMs like ChatGPT to distinguish
  clean and noisy samples and provide supplementary information beyond the noisy labels.
---

# Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance

## Quick Facts
- arXiv ID: 2311.01108
- Source URL: https://arxiv.org/abs/2311.01108
- Reference count: 15
- Key outcome: LAFT framework improves PLM fine-tuning on noisy datasets by 3.2-6.6% accuracy over state-of-the-art methods

## Executive Summary
This paper addresses the challenge of fine-tuning pretrained language models (PLMs) with noisy labels, a common issue in real-world data annotation. The authors propose LAFT, a novel framework that leverages external guidance from large language models (LLMs) like ChatGPT to distinguish clean and noisy samples and provide supplementary information beyond the noisy labels. LAFT performs a two-step sample separation process based on both LLM-generated and PLM-generated confidences, resulting in three subsets: Easy Clean, Hard Clean, and True Noisy Sets. Each subset is subjected to specific noise-robust fine-tuning strategies. Extensive experiments on synthetic and real-world noisy datasets demonstrate LAFT's superiority over state-of-the-art baselines, achieving significant improvements in accuracy, especially for datasets with higher noise ratios and a greater number of classes.

## Method Summary
The LAFT framework addresses noisy label fine-tuning by using LLM-generated confidences to guide sample separation into three subsets: Easy Clean (EC), Hard Clean (HC), and True Noisy (TN) sets. The process involves a coarse-grained separation using LLM confidence matching, followed by fine-grained separation using adaptive thresholds based on both LLM and PLM confidences. Each subset is then fine-tuned with specific strategies: EC samples use standard cross-entropy loss, HC samples use adaptive loss weighting, and TN samples use LLM-generated confidences as soft labels. The framework is evaluated on synthetic datasets (20Ng, AGNews) with various noise types and real-world datasets (SemEval, TREC, Hausa), demonstrating significant accuracy improvements over baseline methods.

## Key Results
- LAFT achieves 3.2-6.6% accuracy improvements over state-of-the-art baselines across synthetic and real-world noisy datasets
- Performance gains are most pronounced on datasets with higher noise ratios and a greater number of classes
- The framework demonstrates robustness to different noise types including symmetric, asymmetric, and instance-dependent noise
- On real-world noisy datasets, LAFT improves accuracy by 1.2-3.8% compared to the best baseline method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using LLM-generated confidences enables more accurate separation of clean and noisy samples than relying solely on PLM-generated confidences.
- Mechanism: LLM-generated confidences are not affected by label noise during fine-tuning, providing an external, noise-independent signal to identify clean samples.
- Core assumption: LLM-generated labels are highly accurate for samples whose assigned labels match the LLM-generated labels (Assumption 1).
- Evidence anchors:
  - [abstract] "This guidance assists in accurately distinguishing between clean and noisy samples"
  - [section] "LLM-generated confidences are not affected by label noise, they can provide potentially relevant labels that are useful even if they are not entirely accurate."
  - [corpus] Weak - no direct corpus evidence found for this specific claim.
- Break condition: If LLM-generated labels have low accuracy, the separation based on matching assigned labels becomes unreliable.

### Mechanism 2
- Claim: The two-step separation process (coarse-grained then fine-grained) improves sample separation accuracy compared to single-step approaches.
- Mechanism: First step uses LLM confidence matching to identify easy clean samples. Second step uses adaptive thresholds on both LLM and PLM confidences to distinguish hard clean from true noisy samples.
- Core assumption: Hard clean samples have lower confidences than noisy samples early in fine-tuning, allowing adaptive threshold separation.
- Evidence anchors:
  - [abstract] "We propose a novel framework LAFT that can effectively separate clean and noisy samples"
  - [section] "The intuition is that within the disagreed set D, the LLM-generated labels can be incorrect for specific hard samples with correct assigned labels"
  - [corpus] Weak - no direct corpus evidence found for this specific claim.
- Break condition: If PLM-generated confidences for hard clean and noisy samples overlap significantly, adaptive separation fails.

### Mechanism 3
- Claim: Using LLM-generated confidences as supervision for noisy samples provides more robust learning than pseudo-labeling with PLM predictions.
- Mechanism: LLM confidences provide soft labels that capture useful information even when not perfectly correct, avoiding confirmation bias from noisy PLM predictions.
- Core assumption: LLM-generated confidences on true noisy set preserve similar accuracy as on the whole dataset (Remark 1).
- Evidence anchors:
  - [abstract] "provides supplementary information beyond the noisy labels, thereby boosting the learning process"
  - [section] "we leverage the confidences generated by LLMs to learn from potentially correct labels"
  - [corpus] Weak - no direct corpus evidence found for this specific claim.
- Break condition: If LLM-generated confidences are too inaccurate, they provide misleading supervision.

## Foundational Learning

- Concept: Cross-entropy loss with soft labels
  - Why needed here: The framework uses weighted cross-entropy where labels can be soft (LLM confidences) rather than hard one-hot vectors.
  - Quick check question: How does the loss function change when using probability distributions instead of hard labels?

- Concept: Adaptive thresholding based on training dynamics
  - Why needed here: The fine-grained separation uses an adaptive threshold that changes as training progresses, requiring understanding of how model confidence evolves.
  - Quick check question: Why would hard clean samples have lower confidence than noisy samples early in fine-tuning?

- Concept: Curriculum learning through sample separation
  - Why needed here: The framework separates samples into subsets that are trained with different strategies, similar to curriculum learning where easier samples are presented first.
  - Quick check question: How does separating samples into easy/hard/noisy sets relate to curriculum learning principles?

## Architecture Onboarding

- Component map: Input text → LLM confidence generator → Coarse separation (EC vs D) → Fine separation (H vs N) → Three loss functions (LE, LH, LN) → PLM fine-tuning → Output predictions
- Critical path: LLM query → Coarse separation → Fine-grained separation → Loss computation → PLM update
- Design tradeoffs:
  - Using LLM confidences adds computational cost but improves separation accuracy
  - Adaptive thresholds require hyperparameter tuning but adapt to training dynamics
  - Three separate loss functions add complexity but allow targeted training strategies
- Failure signatures:
  - Poor separation: High accuracy on EC set but low overall accuracy indicates separation failed
  - Overfitting: EC set accuracy much higher than validation accuracy indicates overfitting to clean samples
  - Noisy learning: TN set performance worse than random indicates poor utilization of LLM confidences
- First 3 experiments:
  1. Test coarse-grained separation only on synthetic dataset with known noise labels to verify Assumption 1
  2. Evaluate adaptive threshold behavior by plotting PLM confidence distributions over training epochs
  3. Compare TN set loss performance with and without LLM confidence supervision on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LAFT change when using different types of LLMs (e.g., GPT-3.5, GPT-4, or other open-source alternatives) as the external guidance source?
- Basis in paper: [explicit] The paper mentions using ChatGPT based on GPT4 for experiments, but doesn't explore the impact of different LLM choices.
- Why unresolved: The paper focuses on demonstrating the effectiveness of LAFT with GPT4, but doesn't investigate the sensitivity to different LLM models or their potential impact on performance.
- What evidence would resolve it: Conducting experiments with different LLM models and comparing their impact on LAFT's performance across various datasets and noise types would provide insights into the importance of LLM choice and potential alternatives.

### Open Question 2
- Question: How does LAFT perform when dealing with label noise in datasets with a larger number of classes or more complex label structures (e.g., hierarchical labels)?
- Basis in paper: [inferred] The paper mentions that LAFT's performance improvement is more pronounced on datasets with a greater number of classes (e.g., 20Ng with 20 classes vs. AGNews with 4 classes). However, it doesn't explore scenarios with extremely large class sets or complex label structures.
- Why unresolved: The experiments are limited to datasets with a moderate number of classes, and the paper doesn't discuss the potential challenges or limitations of LAFT when scaling to more complex label spaces.
- What evidence would resolve it: Conducting experiments on datasets with a significantly larger number of classes or more complex label structures (e.g., hierarchical labels) and analyzing LAFT's performance would provide insights into its scalability and robustness to different label complexities.

### Open Question 3
- Question: Can LAFT be extended to handle other types of noisy labels beyond the three types explored in the paper (symmetric, asymmetric, and instance-dependent noise)?
- Basis in paper: [explicit] The paper focuses on three specific types of label noise and doesn't discuss the potential for extending LAFT to handle other noise types.
- Why unresolved: The paper demonstrates LAFT's effectiveness on the explored noise types but doesn't investigate its adaptability to other potential noise scenarios that might occur in real-world datasets.
- What evidence would resolve it: Conducting experiments with other types of label noise (e.g., random noise, class-dependent noise) and analyzing LAFT's performance would provide insights into its generalizability and potential need for adaptation to different noise patterns.

## Limitations

- Heavy reliance on LLM-generated confidences without quantitative validation of LLM accuracy on target datasets
- Computational overhead from querying LLM for every training sample at multiple epochs may be prohibitive for large-scale applications
- Limited evaluation to specific noise types (symmetric, asymmetric, instance-dependent) without testing on other realistic noise patterns
- Adaptive threshold sensitivity to initialization and hyperparameters not thoroughly explored

## Confidence

- High Confidence: Overall effectiveness of LAFT framework in improving accuracy on noisy datasets
- Medium Confidence: Two-step separation process provides theoretical benefits
- Low Confidence: LLM-generated confidences remain accurate throughout fine-tuning and provide meaningful supervision

## Next Checks

1. Measure the accuracy of LLM-generated labels on a held-out clean validation set before fine-tuning to quantify the reliability of the separation process.

2. Systematically vary the adaptive threshold parameter across multiple runs to assess the robustness of the fine-grained separation to hyperparameter choices.

3. Apply LAFT to a different domain (e.g., medical text or code) with noisy labels to test whether the LLM-guided separation generalizes beyond the evaluated datasets.