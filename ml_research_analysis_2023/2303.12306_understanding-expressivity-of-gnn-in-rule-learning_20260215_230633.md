---
ver: rpa2
title: Understanding Expressivity of GNN in Rule Learning
arxiv_id: '2303.12306'
source_url: https://arxiv.org/abs/2303.12306
tags:
- logical
- reasoning
- graph
- rule
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the logical expressiveness of Graph Neural
  Networks (GNNs) for knowledge graph (KG) reasoning. The authors propose to bridge
  the gap between GNNs and logic by theoretically analyzing the logical expressiveness
  of GNNs.
---

# Understanding Expressivity of GNN in Rule Learning

## Quick Facts
- arXiv ID: 2303.12306
- Source URL: https://arxiv.org/abs/2303.12306
- Reference count: 16
- Primary result: GNNs can implicitly encode graded modal logic rules, and query labeling improves capture of dependent logical rules

## Executive Summary
This paper bridges the gap between Graph Neural Networks (GNNs) and logical reasoning by analyzing what logical rules GNNs can capture for knowledge graph (KG) reasoning. The authors establish that GNNs can encode graded modal logic (CML) formulas, which represent a subset of logical rules. They introduce a query labeling technique that transforms difficult logical rules into capturable forms, and propose an entity labeling method to improve GNN performance on complex reasoning tasks. Experimental results on synthetic and real datasets validate the theoretical findings and demonstrate the effectiveness of the proposed approach.

## Method Summary
The paper analyzes GNN expressiveness for KG reasoning through three main approaches: theoretical analysis of logical expressiveness, query labeling mechanism (QL-GNN), and entity labeling method (EL-GNN). The theoretical framework establishes equivalence between GNN representations and CML formulas. QL-GNN improves performance by assigning constants to query entities, transforming rules into capturable CML formulas. EL-GNN extends this by labeling high-degree entities based on an out-degree threshold d. The methods are evaluated on synthetic datasets with three logical rule types and real KG datasets including Family, Kinship, UMLS, and WN18RR.

## Key Results
- ERA-GNN cannot capture logical rules where head and tail entities are connected in rule structures (relations I and U)
- QL-GNN successfully captures relation I but fails on relation U due to CML's limited expressiveness
- EL-GNN with entity labeling improves performance on difficult logical rules by transforming them into capturable CML formulas
- The proposed entity labeling method achieves significant improvements on synthetic and real KG reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GNNs can implicitly encode logical rules from graded modal logic (CML) into node representations.
- Mechanism: GNNs use message-passing to propagate logical sub-formulas across the graph structure, with each layer capturing increasingly complex logical expressions. The theorem proves that GNN representations are exactly equivalent to CML formulas.
- Core assumption: The aggregation and combination functions in GNNs can implement the logical operations needed for CML (conjunction, negation, counting quantifiers).
- Evidence anchors:
  - [abstract] "We theoretically analyze GNN from logical expressiveness and find out what kind of logical rules can be captured from KG."
  - [section] "Theorem 4.1. In a knowledge graph, a logical rule ϕ(x) is captured by the representations of GNN in Eq.(1) on the knowledge graph if and only if the logical rule ϕ(x) is a formula in CML of the knowledge graph."
  - [corpus] "On the Correspondence Between Monotonic Max-Sum GNNs and Datalog" - shows connections between GNNs and logical frameworks

### Mechanism 2
- Claim: Query labeling in QL-GNN makes it easier to capture logical rules with dependent head and tail entities.
- Mechanism: By assigning a constant (label) to the query entity h, QL-GNN transforms the logical rule R(x,y) into R(h,x), which becomes a formula in CML[K,h]. This allows GNNs to distinguish graph structures that would otherwise be indistinguishable.
- Core assumption: The constant assignment to query entity provides sufficient discriminative power to capture the logical structure.
- Evidence anchors:
  - [abstract] "Our theory shows that the query labeling trick in QL-GNN makes it easier to capture logical rules"
  - [section] "Theorem 5.2 (Logical expressiveness of QL-GNN). For KG reasoning, given query (h,R,?), a logical rule R(h,x) is captured by QL-GNN if and only if the logical rule R(h,x) is a formula in CML[K,h]."
  - [corpus] "MAG-GNN: Reinforcement Learning Boosted Graph Neural Network" - uses labeling strategies for improved expressivity

### Mechanism 3
- Claim: Entity labeling method can transform difficult logical rules into capturable CML formulas.
- Mechanism: By assigning constants to entities with out-degree larger than a threshold d, the method transforms complex logical rules into formulas in CML[K,h,c1,...,ck], making them capturable by QL-GNN.
- Core assumption: Adding constants to high-degree nodes provides enough structural information to distinguish previously indistinguishable patterns.
- Evidence anchors:
  - [abstract] "Our theory provides a way for transforming difficult logical rules to be capturable. Based on our theory, we propose an entity labeling method to improve the performance of learning difficult logical rules."
  - [section] "Theorem 6.1. Assume R(h,x) in FOC[K,h] describes a single-connected rule structure G in K. If we assign constants c1,c2,...,ck to entities with out-degree larger than 1 in K, the rule structure G can be described with R'(h,x) in CML[K,h,c1,...,ck]."
  - [corpus] Weak evidence - no direct corpus support for this specific transformation mechanism

## Foundational Learning

- Concept: Graded Modal Logic (CML) and its relationship to first-order logic
  - Why needed here: The entire theoretical framework is built on understanding what logical expressions GNNs can capture, which is precisely defined by CML
  - Quick check question: What is the key difference between CML and full first-order logic that makes CML capturable by GNNs?

- Concept: Message-passing mechanisms in Graph Neural Networks
  - Why needed here: The paper analyzes how GNNs propagate information through graphs, which directly relates to their ability to capture logical rules
  - Quick check question: How does the aggregation function in GNNs relate to the logical operations needed for CML?

- Concept: Graph isomorphism and counting bisimulation
  - Why needed here: The paper uses these concepts to understand the limitations of GNNs and why certain logical rules cannot be distinguished
  - Quick check question: What is counting bisimulation and why does it limit the expressiveness of GNNs?

## Architecture Onboarding

- Component map: Logical Expressiveness Analysis -> Query Labeling Mechanism -> Entity Labeling Method -> Experimental Validation

- Critical path: Logical Expressiveness Analysis → Query Labeling Insight → Entity Labeling Method → Experimental Validation

- Design tradeoffs:
  - ERA-GNN vs QL-GNN: ERA-GNN has simpler architecture but limited logical expressiveness; QL-GNN is more expressive but requires query-specific modifications
  - Entity labeling threshold d: Lower d captures more complex rules but reduces generalization; higher d improves generalization but may miss difficult rules
  - Constant assignment strategy: Assigning to all high-degree nodes vs. selective assignment based on rule structure

- Failure signatures:
  - Poor performance on synthetic datasets that test specific logical rules
  - Inability to distinguish counting bisimilar structures
  - Overfitting when too many constants are assigned
  - Underfitting when insufficient constants are assigned to capture complex rules

- First 3 experiments:
  1. Test ERA-GNN on synthetic dataset with chain-like logical rules (relation C) - should fail
  2. Test QL-GNN on synthetic dataset with dependent head-tail relationships (relations I and U) - should succeed on I, fail on U
  3. Test EL-GNN with varying degree thresholds d on difficult logical rules - should show optimal performance at intermediate d values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GNNs capture all formulas in graded modal logic (GML) of knowledge graphs, or are there specific limitations?
- Basis in paper: The paper states that GNNs can capture logical rules from GML, but it also mentions that CML (a subset of GML) has limited expressiveness for describing graphs.
- Why unresolved: The paper provides a theoretical analysis showing that GNNs can capture CML, but it does not explicitly state whether this is the complete set of logical rules that GNNs can capture.
- What evidence would resolve it: Further theoretical analysis or experimental results demonstrating the expressiveness of GNNs beyond CML.

### Open Question 2
- Question: How does the degree threshold d in the entity labeling method affect the performance of EL-GNN?
- Basis in paper: The paper mentions that a smaller d makes GNN learn the logical rules with many constants with bad generalization, while a larger d may not be able to transform difficult logical rules into formulas in CML.
- Why unresolved: The paper does not provide a systematic study of the impact of the degree threshold d on the performance of EL-GNN.
- What evidence would resolve it: Experimental results showing the performance of EL-GNN with different values of d on various datasets.

### Open Question 3
- Question: Can the labeling method proposed in the paper be extended to other types of GNNs beyond QL-GNNs?
- Basis in paper: The paper focuses on QL-GNNs and proposes a labeling method to improve their performance in learning difficult logical rules.
- Why unresolved: The paper does not discuss the applicability of the labeling method to other types of GNNs, such as ERA-GNNs or LT-GNNs with general labeling tricks.
- What evidence would resolve it: Theoretical analysis or experimental results demonstrating the effectiveness of the labeling method on other types of GNNs.

## Limitations

- The theoretical analysis relies heavily on the equivalence between GNN representations and graded modal logic (CML) formulas, which may not fully capture all aspects of practical GNN behavior
- The proposed entity labeling method's effectiveness depends on choosing an appropriate degree threshold d, which may not generalize well across different KG domains
- Empirical validation on diverse KG reasoning tasks is limited, with positive results demonstrated on a relatively small set of KG reasoning tasks

## Confidence

- **High confidence**: The theoretical framework connecting GNNs to CML is well-established and rigorously proven
- **Medium confidence**: The empirical improvements shown on synthetic and real datasets, while positive, are demonstrated on a limited set of KG reasoning tasks
- **Medium confidence**: The claim that ERA-GNN cannot capture certain logical rules (relations I and U) is theoretically sound, but practical implementations may have variations that could partially overcome these limitations

## Next Checks

1. **Synthetic rule generalization**: Test EL-GNN on a broader range of synthetic KG reasoning tasks with more complex logical rules to verify the method's scalability and identify failure modes.

2. **Cross-domain performance**: Evaluate EL-GNN on diverse KG datasets from different domains (e.g., biomedical, social networks, commonsense knowledge) to assess the generalization of the entity labeling approach across varying degree distributions.

3. **Threshold sensitivity analysis**: Conduct a systematic study of how the degree threshold d affects performance on different types of logical rules to provide guidance on selecting appropriate thresholds for specific KG reasoning tasks.