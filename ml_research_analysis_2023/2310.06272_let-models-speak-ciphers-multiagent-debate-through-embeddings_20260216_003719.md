---
ver: rpa2
title: 'Let Models Speak Ciphers: Multiagent Debate through Embeddings'
arxiv_id: '2310.06272'
source_url: https://arxiv.org/abs/2310.06272
tags:
- answer
- cipher
- solution
- debate
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CIPHER (Communicative Inter-Model Protocol
  Through Embedding Representation), a novel communication protocol for multi-agent
  debates among Large Language Models (LLMs). The key idea is to bypass token sampling
  and let LLMs communicate directly through the expectation of raw transformer output
  embeddings, which encodes richer information than natural language.
---

# Let Models Speak Ciphers: Multiagent Debate through Embeddings

## Quick Facts
- **arXiv ID:** 2310.06272
- **Source URL:** https://arxiv.org/abs/2310.06272
- **Reference count:** 40
- **Primary result:** CIPHER outperforms natural language debate by 0.5-5.0% across five reasoning tasks

## Executive Summary
This paper introduces CIPHER, a novel communication protocol for multi-agent debates among Large Language Models (LLMs). The key innovation is bypassing token sampling and allowing LLMs to communicate directly through the expectation of raw transformer output embeddings. This approach encodes richer information than natural language by preserving the full probability distribution over vocabulary tokens. CIPHER demonstrates superior performance compared to state-of-the-art LLM debate methods using natural language, achieving consistent improvements across multiple reasoning tasks and model sizes.

## Method Summary
CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) works by computing the weighted average of all token embeddings using the LLM's output probability distribution, rather than sampling a single token. This preserves uncertainty and information across the entire vocabulary. In multi-agent debates, models communicate through these embedding vectors across debate rounds, with final answers generated via nearest-neighbor search in the embedding space. The method requires no modification to model weights and works across different model sizes as long as they share the same tokenizer.

## Key Results
- CIPHER outperforms natural language debate by 0.5-5.0% across five reasoning tasks
- Consistent performance improvements across multiple open-source LLMs of varying sizes
- Demonstrates robustness across GSM8K, Arithmetic, MMLU Formal Logic, MMLU High School Math, and MMLU Professional Psychology datasets
- Shows effectiveness with LLaMA2-70B and other models without requiring architectural modifications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CIPHER encodes richer information than natural language by retaining the full probability distribution over vocabulary tokens
- **Mechanism:** Instead of sampling a single token, CIPHER generates a weighted average of all token embeddings using the LLM's output probabilities, preserving uncertainty and information across the entire vocabulary
- **Core assumption:** The LLM's output distribution contains meaningful information beyond the highest probability token, and averaging embeddings weighted by these probabilities yields a useful representation
- **Evidence anchors:** [abstract] "we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings"
- **Break condition:** If the model's probability distribution is too peaked or uninformative, the weighted average may collapse to the dominant token embedding, negating the benefit

### Mechanism 2
- **Claim:** CIPHER enables more effective multiagent debate by allowing models to share uncertainty and belief information directly
- **Mechanism:** During debate rounds, each model receives embedding vectors that represent not just a single answer but the model's full distribution over possible answers, allowing models to better understand disagreement sources
- **Core assumption:** Models can interpret and use embedding vectors from other models effectively, even when these vectors don't correspond to natural language tokens
- **Evidence anchors:** [abstract] "by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information"
- **Break condition:** If models cannot effectively process embedding vectors from other models, or if the embedding space doesn't capture useful disagreement information

### Mechanism 3
- **Claim:** CIPHER works across different model sizes and architectures because it relies on shared tokenizer embeddings rather than model-specific features
- **Mechanism:** As long as models share the same tokenizer, they can communicate through embeddings regardless of their underlying architecture or size differences
- **Core assumption:** Different models using the same tokenizer will have compatible embedding spaces, allowing meaningful communication through embedding vectors
- **Evidence anchors:** [abstract] "CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights"
- **Break condition:** If different models map tokens to embeddings differently, or if embedding spaces are not compatible across architectures

## Foundational Learning

- **Concept:** Probability distributions and softmax functions
  - Why needed here: Understanding how LLMs generate token probabilities and why sampling loses information
  - Quick check question: What happens to information when you sample a single token from a probability distribution?

- **Concept:** Embeddings and vector spaces
  - Why needed here: CIPHER operates in the embedding space rather than token space, requiring understanding of how tokens map to vectors
  - Quick check question: How do token embeddings represent semantic meaning in vector space?

- **Concept:** Multiagent systems and debate protocols
  - Why needed here: Understanding how multiple models can communicate and refine responses through iterative debate
  - Quick check question: What makes multiagent debate different from single-model self-consistency?

## Architecture Onboarding

- **Component map:** Tokenizer -> LLM -> CIPHER layer -> Debate manager -> Final aggregation
- **Critical path:**
  1. Initial prompt encoding to embeddings
  2. LLM probability distribution generation
  3. CIPHER weighted average computation
  4. Embedding-to-embedding communication
  5. Final aggregation and nearest-neighbor search
- **Design tradeoffs:**
  - Information richness vs. interpretability
  - Embedding space compatibility across models
  - Computational overhead of weighted averaging
  - Debate round effectiveness vs. convergence speed
- **Failure signatures:**
  - Models produce similar embeddings regardless of input differences
  - Debate rounds don't lead to response refinement
  - Embedding vectors don't map back to meaningful tokens
  - Temperature settings prevent effective communication
- **First 3 experiments:**
  1. Compare single-token sampling vs. CIPHER weighted averaging on a simple classification task
  2. Test embedding communication between identical models at different temperatures
  3. Validate embedding-to-text conversion accuracy using nearest-neighbor search

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can CIPHER be extended to support multi-agent debates with more than two agents while maintaining efficiency and accuracy?
- **Basis in paper:** [explicit] The paper mentions investigating debates in extended scales with more agents and rounds but does not provide conclusive results
- **Why unresolved:** The paper only provides preliminary results on debates with 2-4 agents and does not explore the scalability or efficiency of CIPHER in larger multi-agent settings
- **What evidence would resolve it:** Experiments comparing CIPHER performance and efficiency with 3+ agents on diverse datasets, measuring accuracy gains versus computational cost

### Open Question 2
- **Question:** Can CIPHER be adapted to work across LLMs with different tokenizers or vocabulary sets?
- **Basis in paper:** [explicit] The paper acknowledges this as a limitation, stating CIPHER is currently restricted to LLMs sharing a common vocabulary
- **Why unresolved:** The paper does not propose solutions for aligning embedding-vocabulary mappings across different tokenizers
- **What evidence would resolve it:** Demonstration of CIPHER working between LLMs with different tokenizers, or a proposed method for mapping embeddings between vocabularies

### Open Question 3
- **Question:** What is the optimal temperature difference between CIPHER debaters for maximizing debate performance?
- **Basis in paper:** [explicit] The paper shows that optimal performance is often achieved when debaters operate at different temperatures, but does not quantify the ideal temperature gap
- **Why unresolved:** While the paper observes that more diverse temperatures are generally better, it does not determine the specific temperature difference that maximizes gains
- **What evidence would resolve it:** Systematic experiments varying temperature differences between debaters to find the optimal gap for different tasks and model pairs

## Limitations
- Limited validation of cross-model embedding compatibility across different architectures
- Lack of direct measurement of information content preserved in embedding space versus natural language
- Uncertainty about scalability to more than two agents in debate settings

## Confidence

**High Confidence:** The experimental results showing CIPHER outperforming natural language debate baselines across multiple datasets and model sizes are reproducible and statistically significant.

**Medium Confidence:** The claim that CIPHER encodes "richer information" is supported by the mechanism but lacks direct measurement of information content.

**Low Confidence:** The scalability claims to other reasoning tasks and model architectures haven't been validated beyond the tested combinations.

## Next Checks

1. **Information Content Measurement:** Implement an experiment comparing the entropy of the full probability distribution (used in CIPHER) versus single-token sampling, measuring how much information is preserved through the weighted averaging process using mutual information metrics.

2. **Cross-Model Compatibility Test:** Design a controlled experiment where identical models at different temperatures communicate through embeddings, then test whether models with different architectures but shared tokenizers produce meaningfully different embeddings for the same input probability distributions.

3. **Debate Mechanism Ablation:** Create a modified version of CIPHER that uses random vectors instead of embedding communication to isolate whether the debate improvement comes from the embedding communication specifically or other factors like temperature diversity and multiple response generation.