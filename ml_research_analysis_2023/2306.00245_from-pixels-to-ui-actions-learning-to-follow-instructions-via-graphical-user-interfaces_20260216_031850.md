---
ver: rpa2
title: 'From Pixels to UI Actions: Learning to Follow Instructions via Graphical User
  Interfaces'
arxiv_id: '2306.00245'
source_url: https://arxiv.org/abs/2306.00245
tags:
- tasks
- miniwob
- actions
- search
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PIX2ACT, an agent that learns to follow instructions
  via graphical user interfaces using only pixel-based observations and a generic
  action space of mouse and keyboard actions. The method builds on a pretrained image-to-text
  model (PIX2STRUCT) and fine-tunes it using human demonstrations and environment
  interactions, with tree search for policy improvement.
---

# From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces

## Quick Facts
- arXiv ID: 2306.00245
- Source URL: https://arxiv.org/abs/2306.00245
- Reference count: 16
- Key outcome: PIX2ACT achieves 96.2 mean score on MiniWob++, outperforming human crowdworkers (94.3) and prior pixel-only methods (66.5)

## Executive Summary
This paper introduces PIX2ACT, an agent that learns to follow instructions via graphical user interfaces using only pixel-based observations and a generic action space of mouse and keyboard actions. The method builds on a pretrained image-to-text model (PIX2STRUCT) and fine-tunes it using human demonstrations and environment interactions, with tree search for policy improvement. On the MiniWob++ benchmark, PIX2ACT outperforms human crowdworkers and significantly improves over prior work, achieving a mean score of 96.2 compared to 94.3 for humans and 66.5 for the best previous pixel-only method. The results demonstrate that pixel-based pretraining and tree search are effective for GUI-based instruction following. The approach also establishes the first baseline on the adapted WebShop benchmark.

## Method Summary
PIX2ACT fine-tunes the PIX2STRUCT image-to-text model using behavioral cloning on human demonstrations and environment interactions. The agent operates in browser-based GUI environments using Selenium for pixel-based screenshot capture and generic mouse/keyboard actions. During training, tree search generates additional trajectories for policy improvement. The model is fine-tuned on 160×210 resolution images for MiniWob++ and 800×600 for WebShop, with input text preprocessed to remove filler words and special characters. Tree search uses Monte Carlo Tree Search with a learned value network to guide exploration and improve the policy beyond greedy action selection.

## Key Results
- PIX2ACT achieves 96.2 mean score on MiniWob++ benchmark, surpassing human crowdworkers (94.3) and previous pixel-only methods (66.5)
- Ablation studies confirm pretraining benefits: random initialization performs worse than PIX2STRUCT pretraining
- Tree search improves performance over greedy policy, demonstrating effectiveness of planning in deterministic environments
- PIX2ACT establishes first baseline on adapted WebShop benchmark with Task Score of 7.8, though lower than HTML-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pixel-based pretraining enables strong GUI understanding without requiring structured DOM inputs.
- Mechanism: PIX2STRUCT pretraining learns to map screenshots to structured HTML representations, implicitly learning to recognize GUI layouts, widgets, and their visual relationships. This learned visual-linguistic understanding transfers effectively to instruction-following tasks when fine-tuned on demonstrations.
- Core assumption: The visual features learned from screenshot parsing are generalizable enough to support understanding of new GUI tasks without requiring explicit DOM structure.
- Evidence anchors:
  - [abstract] "Building upon recent progress in pixel-based pretraining, we show, for the first time, that it is possible for such agents to outperform human crowdworkers on the MiniWob++ benchmark"
  - [section 3] "we show that PIX2STRUCT's pre-training via screenshot parsing is effective for GUI-based instruction following with pixel-based inputs"
  - [corpus] Weak - corpus neighbors focus on recent GUI agent developments but don't provide evidence about pretraining effectiveness
- Break condition: If pretraining data doesn't cover the visual patterns in target GUI tasks, or if the tasks require understanding of GUI elements that weren't prevalent in pretraining data.

### Mechanism 2
- Claim: Tree search with learned value functions provides effective policy improvement beyond greedy action selection.
- Mechanism: Monte Carlo Tree Search uses the learned policy network to guide exploration while the value network provides leaf state estimates. This combination allows the agent to discover better action sequences than greedy selection alone, even when the underlying policy isn't perfect.
- Core assumption: The deterministic nature of MiniWob++ environments enables effective tree search, and the value network can provide reasonable estimates for leaf states to guide search.
- Evidence anchors:
  - [section 3.1] "tree search leverages the deterministic nature of the environment to look ahead at the consequences of possible actions to determine a more optimal policy"
  - [section 5.3] "Table 2 shows the improvement in MiniWob++ scores by training on episodes generated by tree search"
  - [corpus] Weak - corpus doesn't provide evidence about tree search effectiveness specifically
- Break condition: If the environment becomes non-deterministic, or if the value network provides poor estimates for states encountered during search.

### Mechanism 3
- Claim: General low-level action spaces enable transfer learning across different GUI tasks and environments.
- Mechanism: By using basic mouse and keyboard actions instead of task-specific actions, the agent learns general interaction skills that transfer across tasks. The model learns to map instructions to sequences of low-level actions rather than memorizing task-specific action patterns.
- Core assumption: Low-level actions are sufficient to complete diverse GUI tasks, and the mapping from instructions to action sequences can be learned rather than being hand-engineered.
- Evidence anchors:
  - [section 1] "This paper focuses on creating agents that interact with the digital world using the same conceptual interface that humans commonly use — via pixel-based screenshots and a generic action space"
  - [section 4.1] "We were able to perform this mapping with a reasonable degree of success for 59 tasks"
  - [corpus] Weak - corpus doesn't provide evidence about transfer benefits of generic action spaces
- Break condition: If certain GUI tasks require actions that cannot be expressed through basic mouse and keyboard operations, or if the instruction-to-action mapping becomes too complex to learn.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation
  - Why needed here: The paper models GUI interaction as an MDP where the agent receives observations and selects actions, enabling the use of reinforcement learning and planning techniques.
  - Quick check question: What are the four components of an MDP (states, actions, transition function, reward function) and how does each apply to GUI interaction?

- Concept: Behavioral Cloning vs. Reinforcement Learning
  - Why needed here: The paper explores both supervised learning from demonstrations (BC) and reinforcement learning/tree search approaches, requiring understanding when each is appropriate.
  - Quick check question: What are the key differences between behavioral cloning and reinforcement learning in terms of data requirements and exploration capabilities?

- Concept: Pretraining and Fine-tuning
  - Why needed here: The approach relies on pretraining PIX2STRUCT on screenshot parsing before fine-tuning on GUI instruction tasks, requiring understanding of transfer learning principles.
  - Quick check question: How does pretraining on a related task (screenshot parsing) help improve performance on the target task (instruction following)?

## Architecture Onboarding

- Component map:
  - PIX2STRUCT model (282M parameters) with Vision Transformer encoder and T5 decoder
  - Environment framework using Selenium for browser interaction
  - Tree search implementation for policy improvement
  - Data preprocessing pipeline for converting demonstrations

- Critical path:
  1. Environment provides screenshot observation
  2. PIX2STRUCT processes image and generates action sequence
  3. Action executed in environment
  4. Episode continues until completion or max steps
  5. For training: collect transitions for BC or use tree search to generate trajectories

- Design tradeoffs:
  - Using low-level actions provides generality but increases complexity of instruction mapping
  - Pixel-only inputs avoid dependency on DOM but require more sophisticated visual understanding
  - Tree search improves performance but adds computational cost at inference time

- Failure signatures:
  - Poor performance on tasks requiring precise coordinate selection (may indicate insufficient coordinate binning)
  - Inability to complete tasks requiring text entry or complex interactions (may indicate action space limitations)
  - Performance degradation on held-out tasks (may indicate overfitting to training task distribution)

- First 3 experiments:
  1. Verify basic functionality by running agent on a simple task (e.g., click-button) and confirming it can complete the task
  2. Test pretraining ablation by training with random initialization on MiniWob++ and comparing performance
  3. Evaluate tree search impact by comparing greedy policy performance vs. tree search policy on a subset of tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would PIX2ACT's performance compare to human crowdworkers if evaluated on all 104 MiniWob++ tasks, including those involving scrolling, copying/pasting, and real-time interactions?
- Basis in paper: [explicit] The paper notes that 59 out of 104 MiniWob++ tasks were supported due to limitations in the Selenium-based environment, excluding tasks with scrolling, copying/pasting, and animations.
- Why unresolved: The paper only evaluated on a subset of tasks, and extending the environment to support the excluded tasks would require significant engineering effort.
- What evidence would resolve it: Evaluating PIX2ACT on all 104 MiniWob++ tasks after extending the environment to support the excluded interactions.

### Open Question 2
- Question: How would PIX2ACT's performance change if trained with a larger number of human demonstrations or using reinforcement learning on WebShop?
- Basis in paper: [inferred] The paper notes that PIX2ACT's performance on WebShop is lower than the best models using HTML inputs and custom actions, and that the authors did not explore RL on WebShop due to limited improvements shown in prior work.
- Why unresolved: The paper only explored behavioral cloning on WebShop, and it is unclear how additional training data or RL would impact performance.
- What evidence would resolve it: Training PIX2ACT on WebShop with a larger number of demonstrations and/or using RL, and comparing performance to the best HTML-based models.

### Open Question 3
- Question: How would PIX2ACT's performance on held-out MiniWob++ tasks compare to a model that has access to DOM information?
- Basis in paper: [explicit] The paper shows that PIX2ACT can generalize to held-out tasks with pretraining, but does not compare to models with DOM access.
- Why unresolved: The paper only compares PIX2ACT to models with DOM access on the full set of tasks, not on held-out tasks.
- What evidence would resolve it: Training a model with DOM access on the same held-out tasks as PIX2ACT and comparing performance.

## Limitations

- The approach relies heavily on deterministic environments, with tree search performance potentially degrading in stochastic settings
- Pixel-based pretraining may struggle with novel GUI designs or layouts not well-represented in pretraining data
- The use of generic low-level actions creates significant mapping challenges between natural language instructions and action sequences

## Confidence

- **High Confidence**: The core claim that PIX2ACT outperforms human crowdworkers on MiniWob++ is well-supported by the reported results (96.2 vs 94.3 mean score) and ablation studies showing pretraining benefits.
- **Medium Confidence**: The effectiveness of tree search for policy improvement is demonstrated but relies on the deterministic nature of environments; generalization to stochastic settings remains untested.
- **Medium Confidence**: The pretraining mechanism's effectiveness is supported by ablations, but the specific contribution of PIX2STRUCT's architectural choices versus pretraining data remains unclear.

## Next Checks

1. **Stochastic Environment Testing**: Evaluate PIX2ACT performance on environments with stochastic elements or delays to assess robustness beyond the deterministic MiniWob++ benchmark.
2. **Pretraining Data Coverage Analysis**: Analyze the distribution of GUI elements and layouts in PIX2STRUCT's pretraining data versus those encountered in MiniWob++ and WebShop to quantify potential domain gaps.
3. **Cross-Task Generalization**: Test the agent on entirely new GUI tasks not seen during training or fine-tuning to evaluate true generalization capabilities beyond the adapted action space.