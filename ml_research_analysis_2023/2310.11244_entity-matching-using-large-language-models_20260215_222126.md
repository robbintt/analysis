---
ver: rpa2
title: Entity Matching using Large Language Models
arxiv_id: '2310.11244'
source_url: https://arxiv.org/abs/2310.11244
tags:
- matching
- entity
- prompt
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using large language models (LLMs) for
  entity matching, a central task in data integration. The authors compare hosted
  LLMs (GPT3.5, GPT4) and open-source LLMs (SOLAR, StableBeluga2) in zero-shot and
  fine-tuning scenarios, evaluating different prompt designs and in-context learning
  techniques.
---

# Entity Matching using Large Language Models

## Quick Facts
- arXiv ID: 2310.11244
- Source URL: https://arxiv.org/abs/2310.11244
- Authors: 
- Reference count: 40
- Key outcome: GPT4 achieves strong zero-shot performance, outperforming fine-tuned PLMs on three of five benchmark datasets with F1 scores around 90%.

## Executive Summary
This paper investigates the use of large language models (LLMs) for entity matching, a central task in data integration. The authors compare hosted LLMs (GPT3.5, GPT4) and open-source LLMs (SOLAR, StableBeluga2) in zero-shot and fine-tuning scenarios across five benchmark datasets. GPT4 demonstrates strong zero-shot performance, outperforming fine-tuned pre-trained language models (PLMs) on most datasets without requiring task-specific training data. Fine-tuning GPT3.5 further improves performance in some cases, while LLMs show better generalization to out-of-distribution entities compared to PLMs.

## Method Summary
The study evaluates LLM-based entity matching using zero-shot prompting, in-context learning with demonstrations, learning matching rules, and fine-tuning GPT3.5. Entity pairs are serialized into prompts with task descriptions and output format specifications. Five benchmark datasets (WDC Products, Abt-Buy, Walmart-Amazon, Amazon-Google, DBLP-Scholar) are used for evaluation. Performance is measured using F1 scores, with comparisons to RoBERTa and Ditto PLM baselines. Different prompt designs are tested to assess sensitivity, and generalization is evaluated by testing on out-of-distribution entities.

## Key Results
- GPT4 achieves strong zero-shot performance, outperforming fine-tuned PLMs on three of five benchmark datasets with F1 scores around 90%
- Fine-tuning GPT3.5 further improves performance, exceeding GPT4 in some cases
- LLMs show better generalization to out-of-distribution entities compared to PLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot GPT4 outperforms fine-tuned PLMs on most entity matching datasets without requiring task-specific training data.
- Mechanism: LLMs leverage massive pre-training on diverse text, enabling strong performance on new tasks via prompt engineering alone.
- Core assumption: Pre-training generalizes well enough for structured entity matching tasks.
- Evidence anchors:
  - [abstract] "GPT4 achieves strong zero-shot performance, outperforming fine-tuned pre-trained language models (PLMs) on three out of five benchmark datasets, reaching F1 scores around 90%."
  - [section] "GPT4 without any task-specific training data outperforms the fine-tuned PLMs on three out of five datasets by approximately 4% F1."
- Break condition: If entity matching relies heavily on domain-specific knowledge not well-represented in pre-training data.

### Mechanism 2
- Claim: Fine-tuning GPT3.5 further improves performance, exceeding GPT4 in some cases.
- Mechanism: Task-specific fine-tuning adapts general language understanding to domain-specific entity matching patterns.
- Core assumption: Limited fine-tuning data is sufficient to specialize the model without overfitting.
- Evidence anchors:
  - [abstract] "Fine-tuning GPT3.5 further improves performance, exceeding GPT4 in some cases."
  - [section] "Fine-tuning GPT3.5 leads to a 3% higher performance than the best performing GPT4 model/prompt combination on three out of five datasets with comparable performance on the remaining two."
- Break condition: If fine-tuning data is too small or unrepresentative of real-world entity matching scenarios.

### Mechanism 3
- Claim: LLMs show better generalization to out-of-distribution entities compared to PLMs.
- Mechanism: Larger model capacity and diverse pre-training enable better handling of unseen entity types.
- Core assumption: Pre-training data diversity translates to better generalization.
- Evidence anchors:
  - [abstract] "LLMs show better generalization to out-of-distribution entities compared to PLMs."
  - [section] "Compared to fine-tuning directly on the WDC Products development set (84.90% F1 for Ditto), the transfer of fine-tuned models leads to large drops in performance ranging from 36 to 53% F1 for Ditto and 22 to 47% F1 for RoBERTa. StableBeluga2... still performs better by 8% F1 when compared to the best transferred PLM-based matcher and GPT4 outperforms it by 34%."
- Break condition: If out-of-distribution entities differ too radically from pre-training distribution.

## Foundational Learning

- Concept: Prompt engineering and in-context learning
  - Why needed here: Different prompt designs significantly impact LLM performance on entity matching tasks.
  - Quick check question: What prompt design yielded the best results for GPT4 on the WDC Products dataset?

- Concept: Contrastive learning and PLM fine-tuning
  - Why needed here: Understanding PLM baselines helps evaluate LLM improvements and identify LLM advantages.
  - Quick check question: What was the F1 score of the fine-tuned RoBERTa model on the DBLP-Scholar dataset?

- Concept: Domain adaptation and transfer learning
  - Why needed here: Analyzing how models generalize across different entity matching datasets reveals strengths and limitations.
  - Quick check question: Did fine-tuning on one product dataset improve performance on other product datasets?

## Architecture Onboarding

- Component map: Entity pairs -> Serialization -> Prompt templates -> LLM models -> F1 score calculation
- Critical path:
  1. Serialize entity pairs
  2. Construct prompt with chosen design
  3. Send prompt to LLM
  4. Parse LLM response for "Yes"/"No"
  5. Calculate F1 score
- Design tradeoffs:
  - Zero-shot vs. fine-tuning: Performance vs. data requirements
  - Hosted vs. open-source LLMs: Cost/complexity vs. control/reproducibility
  - Prompt complexity: Performance vs. cost (token usage)
- Failure signatures:
  - Low F1 scores across multiple models: Likely data quality or serialization issues
  - Inconsistent results across prompt variations: Model sensitivity or prompt engineering problems
  - High costs with minimal performance gains: Inefficient prompt design or model selection
- First 3 experiments:
  1. Test all models with simple prompt design on a small subset of data
  2. Compare zero-shot performance of GPT4 vs. fine-tuned PLM baseline
  3. Evaluate impact of in-context demonstrations on top-performing model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-based entity matching systems change when applied to different languages or multilingual datasets?
- Basis in paper: [inferred] The paper focuses on English benchmark datasets and does not explore multilingual capabilities.
- Why unresolved: The study only evaluates models on English datasets, leaving the multilingual generalization of LLMs for entity matching unexplored.
- What evidence would resolve it: Experiments comparing LLM-based entity matching performance across multiple languages and multilingual datasets would provide insights into their generalizability.

### Open Question 2
- Question: What is the impact of using different prompt engineering techniques, such as chain-of-thought or tree-of-thought, on the performance of LLM-based entity matching systems?
- Basis in paper: [inferred] The paper explores basic prompt designs but does not investigate more advanced prompt engineering techniques.
- Why unresolved: The study focuses on simple prompt designs and does not explore the potential benefits of more sophisticated prompt engineering methods.
- What evidence would resolve it: Experiments comparing the performance of LLM-based entity matching systems using various prompt engineering techniques would reveal their impact on matching accuracy.

### Open Question 3
- Question: How do LLM-based entity matching systems perform when dealing with noisy or incomplete entity descriptions, and what strategies can be employed to improve their robustness?
- Basis in paper: [inferred] The paper evaluates models on clean benchmark datasets and does not address their performance on noisy or incomplete data.
- Why unresolved: The study does not investigate the robustness of LLM-based entity matching systems when faced with real-world data imperfections.
- What evidence would resolve it: Experiments testing LLM-based entity matching systems on datasets with varying levels of noise and incompleteness, along with evaluations of different strategies to improve robustness, would provide insights into their practical applicability.

## Limitations
- Study relies on commercial LLM APIs that may exhibit different behavior across versions, making exact reproduction difficult
- Prompt engineering shows high sensitivity, with F1 scores varying up to 30% based on prompt design choices
- Evaluation only considers five datasets, limiting generalizability to other entity matching domains

## Confidence
- **High confidence**: GPT4 zero-shot performance exceeding fine-tuned PLMs on most datasets. This claim is directly supported by F1 score comparisons and represents the core finding with clear numerical evidence.
- **Medium confidence**: Fine-tuned GPT3.5 exceeding GPT4 performance in some cases. While stated in the abstract, the specific conditions and dataset distributions for these improvements are not fully detailed in the available text.
- **Medium confidence**: Superior out-of-distribution generalization of LLMs versus PLMs. The comparison provides directional evidence but doesn't establish the magnitude of generalization benefits across diverse entity types.

## Next Checks
1. **Reproduce prompt sensitivity analysis**: Systematically test the same model across 10+ diverse prompt designs on a single dataset to quantify the stability range and identify which prompt components drive the most variation in performance.

2. **Cross-dataset transfer validation**: Fine-tune the same LLM model on one product entity matching dataset, then evaluate on held-out datasets to measure true out-of-distribution generalization versus in-distribution performance gains.

3. **Cost-performance tradeoff analysis**: Calculate per-entity matching costs (tokens Ã— price) for the top-performing prompt designs across models, then benchmark against PLM inference costs to determine economic viability at scale.