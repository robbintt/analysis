---
ver: rpa2
title: Duality of Bures and Shape Distances with Implications for Comparing Neural
  Representations
arxiv_id: '2311.11436'
source_url: https://arxiv.org/abs/2311.11436
tags:
- similarity
- neural
- distance
- shape
- matrices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a mathematical connection between two broad
  classes of methods for comparing neural network representations. One class learns
  explicit alignment mappings between neural units (e.g., shape distances, canonical
  correlation analysis), while the other compares stimulus-by-stimulus similarity
  matrices (e.g., representational similarity analysis, centered kernel alignment,
  normalized Bures similarity).
---

# Duality of Bures and Shape Distances with Implications for Comparing Neural Representations

## Quick Facts
- arXiv ID: 2311.11436
- Source URL: https://arxiv.org/abs/2311.11436
- Reference count: 40
- One-line primary result: Establishes mathematical duality between shape distances and normalized Bures similarity for comparing neural representations

## Executive Summary
This paper establishes a fundamental mathematical connection between two broad classes of methods for comparing neural network representations. One class learns explicit alignment mappings between neural units (shape distances, canonical correlation analysis), while the other compares stimulus-by-stimulus similarity matrices (representational similarity analysis, centered kernel alignment, normalized Bures similarity). The key finding is that the cosine of the Riemannian shape distance equals the normalized Bures similarity, creating a duality that links these two perspectives. This connection allows insights from optimal transport and quantum information theory to inform the analysis of shape distances, and enables immediate asymptotic analyses of both measures.

## Method Summary
The paper presents theoretical analysis establishing the duality between Procrustes shape distance and Bures distance, and between Riemannian shape distance and normalized Bures similarity (NBS). It conducts asymptotic analysis showing convergence properties of both measures as the number of stimuli or neurons approaches infinity. The work also compares NBS to centered kernel alignment (CKA) through both numerical experiments and analytical bounds, demonstrating that despite superficial similarity, these measures utilize fundamentally different geometries and can produce discrepant quantitative outcomes.

## Key Results
- Proves that cosine of Riemannian shape distance equals normalized Bures similarity (NBS)
- Establishes asymptotic convergence properties for both shape and Bures distances as data size increases
- Demonstrates that CKA and NBS utilize fundamentally different geometries with loose relationships
- Provides theoretical bounds showing CKA is bounded by NBS within an envelope determined by matrix ranks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The cosine of Riemannian shape distance equals normalized Bures similarity due to a fundamental duality between optimal transport and alignment-based geometric measures
- Mechanism: When comparing neural representations, the Procrustes distance between centered data matrices equals the Bures distance between their corresponding kernel matrices. This equivalence arises because both measures capture the same underlying geometric structure - one through explicit rotational alignment and the other through optimal transport between probability distributions
- Core assumption: The neural representations are centered and the comparison is based on the statistical structure of stimulus-response relationships
- Evidence anchors:
  - [abstract] "the cosine of the Riemannian shape distance is equal to the normalized Bures similarity (NBS)"
  - [section 3] "Theorem 1. Let KX and KY be centered linear kernel matrices... Then, B(KX, KY) = P(X, Y) and furthermore, NBS(KX, KY) = cos θ*(X, Y)"
  - [corpus] Weak evidence - corpus contains related papers but none directly proving this duality
- Break condition: If the data is not properly centered or if the neural representations have fundamentally different statistical properties that cannot be captured by covariance structure

### Mechanism 2
- Claim: The duality allows immediate asymptotic analysis of representational similarity measures as data size increases
- Mechanism: As the number of stimuli (M) or neurons (N) approaches infinity, both shape and Bures distances converge to meaningful limits due to the law of large numbers. The duality allows leveraging well-established results from optimal transport theory to characterize these limits without re-deriving them from scratch
- Core assumption: The underlying data distribution is stationary and the neural representations capture consistent statistical structure
- Evidence anchors:
  - [section 4] "Asymptotic Analysis of Shape/Bures Distances... both of these regimes are of interest to researchers in neuroscience and deep learning, and the duality established in theorem 1 enables immediate insights"
  - [abstract] "Asymptotic analyses show that both shape and Bures distances converge to reasonable values as the number of stimuli or neurons goes to infinity"
  - [corpus] Weak evidence - no direct citations supporting asymptotic convergence claims
- Break condition: If the underlying data distribution changes with sample size or if neural representations are fundamentally non-stationary

### Mechanism 3
- Claim: CKA and NBS, despite superficial similarity, utilize fundamentally different geometries and can produce discrepant quantitative outcomes
- Mechanism: CKA measures similarity using Euclidean geometry on PSD matrices, while NBS uses Bures geometry. This geometric difference means that while CKA is bounded by NBS within an envelope determined by matrix ranks, the actual similarity values can differ substantially in practice
- Core assumption: The geometric properties of PSD matrices significantly impact similarity measurement outcomes
- Evidence anchors:
  - [section 5] "We show numerically and analytically that the relationship between these quantities is rather loose, and we therefore do not expect them to be interchangeable in practical applications"
  - [abstract] "CKA and NBS utilize fundamentally different geometries and can produce discrepant quantitative outcomes"
  - [corpus] Weak evidence - corpus contains related papers but none directly comparing CKA and NBS geometries
- Break condition: If the matrix ranks are very low or if the data structure happens to align perfectly with Euclidean geometry assumptions

## Foundational Learning

- Concept: Covariance matrix estimation and properties
  - Why needed here: The entire duality rests on understanding how covariance matrices capture the essential structure of neural representations and how their properties translate between different geometric frameworks
  - Quick check question: What is the relationship between the singular values of X⊤Y and the eigenvalues of X⊤YY⊤X?

- Concept: Optimal transport and Wasserstein distance
  - Why needed here: Bures distance is equivalent to 2-Wasserstein distance for normal distributions, and understanding this connection is crucial for interpreting the geometric meaning of the similarity measures
  - Quick check question: How does the Bures distance relate to the cost of transporting mass between two probability distributions?

- Concept: Procrustes analysis and shape theory
  - Why needed here: Shape distances provide the alignment-based perspective that is dual to the optimal transport perspective, and understanding Procrustes analysis is essential for interpreting the geometric intuition
  - Quick check question: What nuisance transformations are removed when computing the Riemannian shape distance?

## Architecture Onboarding

- Component map: Data → Centering → Kernel/Covariance Computation → Similarity Measure → Interpretation
- Critical path: Data → Centering → Kernel/Covariance Computation → Similarity Measure → Interpretation. The critical performance bottleneck is typically kernel matrix computation for large datasets.
- Design tradeoffs: Shape distances require explicit alignment computation which can be expensive for high-dimensional data, while Bures distances require matrix square roots and fidelity computations. The choice depends on whether interpretability (shape) or computational efficiency (Bures) is prioritized.
- Failure signatures: If the duality doesn't hold, it typically manifests as inconsistent similarity scores between shape and Bures implementations, or convergence failures in asymptotic analysis. Check data centering and matrix properties first.
- First 3 experiments:
  1. Implement both shape and Bures distance computations on synthetic data with known ground truth similarity to verify the duality numerically
  2. Test asymptotic convergence by varying M and N on real neural data to observe the convergence behavior predicted by the theory
  3. Compare CKA vs NBS on benchmark datasets to quantify the discrepancy envelope and validate the theoretical bounds from section 5.2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of the Riemannian shape distance and normalized Bures similarity make them uniquely suitable for comparing neural network representations compared to other measures?
- Basis in paper: [explicit] The paper states that shape distances and NBS enjoy a special duality, connecting two perspectives on representational similarity, which other measures typically do not share.
- Why unresolved: While the paper demonstrates the duality and discusses advantages, it does not definitively establish that these measures are uniquely superior for all applications.
- What evidence would resolve it: Comparative studies showing shape distances and NBS outperform other measures across a wide range of neural network comparison tasks and scenarios.

### Open Question 2
- Question: How does the duality between shape distances and NBS impact the interpretation and analysis of neural representations in practical applications?
- Basis in paper: [explicit] The paper discusses how the duality allows leveraging insights from optimal transport and quantum information theory to analyze shape distances, and how it simplifies asymptotic analyses.
- Why unresolved: While the paper provides examples of how the duality can be beneficial, it does not fully explore all potential practical implications and applications.
- What evidence would resolve it: Case studies and empirical results demonstrating the advantages of using the duality in real-world neural network analysis and comparison tasks.

### Open Question 3
- Question: Can the duality between shape distances and NBS be extended to other types of neural network representations or dissimilarity measures?
- Basis in paper: [inferred] The paper focuses on comparing linear kernel matrices and covariance matrices, but does not explore whether the duality holds for other types of representations or dissimilarity measures.
- Why unresolved: The paper only establishes the duality for specific cases and does not provide a general framework for extending it to other scenarios.
- What evidence would resolve it: Mathematical proofs or empirical studies demonstrating the extension of the duality to other types of neural network representations or dissimilarity measures.

## Limitations

- The duality relies on the assumption that neural representations can be adequately captured through centered covariance structures, which may not hold for non-linear relationships
- Asymptotic analyses depend on specific data-generating assumptions that may not generalize to all neural datasets
- The comparison with CKA does not fully explore practical scenarios where geometric differences might lead to conflicting scientific conclusions

## Confidence

- High Confidence: The mathematical proof of the duality between Procrustes shape distance and Bures distance - the derivation follows standard linear algebra techniques
- Medium Confidence: The asymptotic convergence results - theoretically justified but require empirical validation across diverse datasets
- Medium Confidence: The geometric interpretation distinguishing CKA from NBS - mathematically sound but practical significance needs more systematic investigation

## Next Checks

1. Test the duality on non-centered neural representations and examine how centering affects the similarity measures across different preprocessing pipelines

2. Conduct extensive empirical validation of asymptotic convergence across diverse neural datasets with varying sample sizes and dimensionalities to establish practical convergence bounds

3. Design benchmark experiments comparing CKA and NBS on datasets with known ground truth relationships to quantify their disagreement in realistic scenarios and identify conditions where one measure might be preferred over the other