---
ver: rpa2
title: 'Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion
  Image Editing'
arxiv_id: '2304.02051'
source_url: https://arxiv.org/abs/2304.02051
tags:
- dress
- image
- multimodal
- sketch
- garment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce a novel multimodal-conditioned fashion image
  editing task that uses text, pose maps, and garment sketches to guide the generation
  of human-centric fashion images while preserving model identity and body shape.
  They propose Multimodal Garment Designer (MGD), a latent diffusion architecture
  that conditions Stable Diffusion with multimodal inputs and pose consistency, enabling
  controlled fashion image synthesis.
---

# Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing

## Quick Facts
- arXiv ID: 2304.02051
- Source URL: https://arxiv.org/abs/2304.02051
- Reference count: 40
- Key outcome: MGD outperforms baselines in realism (FID/KID) and multimodal coherence (CLIP-S, PD, SD) for fashion image editing

## Executive Summary
The paper introduces a novel multimodal-conditioned fashion image editing task that uses text, pose maps, and garment sketches to guide the generation of human-centric fashion images while preserving model identity and body shape. The authors propose Multimodal Garment Designer (MGD), a latent diffusion architecture that conditions Stable Diffusion with multimodal inputs and pose consistency, enabling controlled fashion image synthesis. To support the task, they extend two virtual try-on datasets (Dress Code and VITON-HD) with semi-automatically collected multimodal annotations. Experiments show MGD outperforms baselines in realism (FID/KID) and multimodal coherence (CLIP-S, PD, SD), and is validated by user studies.

## Method Summary
The approach extends Stable Diffusion's latent diffusion framework by modifying the denoising U-Net to accept additional conditioning inputs: 18 channels for human keypoint pose maps and 1 channel for garment sketches. The model uses a fast multi-condition classifier-free guidance variant to handle multiple modalities simultaneously. The training procedure involves 150k steps with batch size 16, learning rate 1e-5, and AdamW optimizer. Inference uses DDIM sampling with 50 steps and guidance scale 7.5. The method is evaluated on extended versions of Dress Code and VITON-HD datasets with multimodal annotations.

## Key Results
- MGD achieves lower FID and KID scores than baseline methods (Stable Diffusion, FICE, SDedit) on both Dress Code and VITON-HD datasets
- Multimodal coherence metrics (CLIP-S, PD, SD) show MGD better preserves text and sketch conditioning while maintaining pose consistency
- User study confirms MGD generates more realistic and controllable fashion images compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning latent diffusion with pose maps preserves human body structure during garment editing.
- Mechanism: The denoising network's first convolutional layer is extended with 18 channels, one for each human keypoint, and the pose map is resized to match the latent space dimensions. This allows spatial preservation of body information in the masked region.
- Core assumption: Convolutional operations preserve spatial structure in latent space, and additional keypoint channels can be integrated without retraining from scratch.
- Evidence anchors:
  - [section 3.2]: "we modify the first convolution layer of the network by adding 18 additional channels, one for each keypoint"
  - [section 3.2]: "Thanks to the fully convolutional nature of the encoder E and the decoder D, this LDMs-based architecture can preserve the spatial information in the latent space"
- Break condition: If the pose map resolution doesn't match latent space dimensions, or if the keypoint detector fails to identify body parts in the masked region.

### Mechanism 2
- Claim: Adding garment sketches as conditioning improves spatial detail control beyond what text alone can provide.
- Mechanism: Sketches are concatenated with the latent input in early denoising steps, providing fine-grained shape information that complements the text description. This is implemented by extending the denoising network input with an additional channel for the sketch.
- Core assumption: Sketches provide spatial information that text descriptions cannot fully capture, and conditioning on sketches in early diffusion steps is most effective.
- Evidence anchors:
  - [section 3.2]: "we propose to leverage garment sketches to enrich the textual input with additional spatial fine-grained details"
  - [section 3.2]: "we only condition the early steps of the denoising process as the final steps have little influence on the shapes"
- Break condition: If the sketch extraction fails to capture garment edges accurately, or if the sketch resolution is too low to provide meaningful spatial guidance.

### Mechanism 3
- Claim: Classifier-free guidance with multi-condition adaptation enables effective multimodal control without requiring multiple forward passes.
- Mechanism: The model is trained to work both with and without conditions by randomly masking inputs during training. At inference, a single direction vector is computed for all conditions rather than per-condition guidance, reducing computational cost while maintaining control.
- Core assumption: Training with masked conditions allows the model to learn unconditional generation capability, and multi-condition guidance can be computed efficiently.
- Evidence anchors:
  - [section 3.3]: "we use the fast variant multi-condition classifier-free guidance proposed in [1]"
  - [section 3.3]: "Instead of performing the classifier-free guidance according to each condition probability, it computes the direction of the joint probability of all the conditions"
- Break condition: If the guidance scale is set too high, it may cause mode collapse or loss of conditioning signal fidelity.

## Foundational Learning

- Concept: Latent diffusion models operate in compressed latent space rather than pixel space.
  - Why needed here: Understanding that the model works with h×w×4 latent representations (where h=H/8, w=W/8) is crucial for grasping how spatial information is preserved and how conditioning signals are integrated.
  - Quick check question: If an image is 1024×768, what are the dimensions of its latent representation in Stable Diffusion?

- Concept: Multimodal conditioning requires careful integration of different signal types (text, pose, sketch) into the denoising process.
  - Why needed here: The paper combines three distinct modalities, each with different characteristics (textual, spatial, structural), requiring understanding of how they're encoded and concatenated.
  - Quick check question: How are pose maps and sketches resized to match the latent space dimensions before being concatenated with the denoising network input?

- Concept: Classifier-free guidance enables conditional generation without explicit classifier models.
  - Why needed here: The paper uses a variant that handles multiple conditions simultaneously, which is key to understanding the inference process and the guidance scale parameter.
  - Quick check question: What is the difference between standard classifier-free guidance and the multi-condition variant used in this work?

## Architecture Onboarding

- Component map: Stable Diffusion autoencoder (E/D) → Extended U-Net denoiser with 18 keypoint channels + 1 sketch channel → Multi-condition classifier-free guidance → DDIM sampling
- Critical path: Input image → Encoder E → Latent z → Masking + conditioning (text, pose, sketch) → Denoising U-Net → Decoder D → Output image
- Design tradeoffs: Adding keypoint channels enables pose preservation but requires careful initialization; sketch conditioning improves detail but adds complexity; multi-condition guidance reduces compute but may have less fine-grained control than per-condition guidance
- Failure signatures: Poor pose preservation (incorrect keypoint integration), unrealistic garments (insufficient sketch conditioning), loss of text adherence (improper guidance scale)
- First 3 experiments:
  1. Test pose preservation by generating images with and without pose conditioning, measuring keypoint distance
  2. Evaluate sketch effectiveness by comparing generations with different sketch conditioning step fractions
  3. Assess guidance scale impact by varying α and measuring realism vs. adherence trade-offs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do variations in the fraction of unconditional training steps affect the model's ability to generate coherent multimodal outputs?
- Basis in paper: [explicit] The paper mentions that tuning the fraction of unconditional training steps can affect the quality of the final results.
- Why unresolved: The paper only evaluates one setting (0.2) and does not explore the full range of possible values or their impact on different metrics.
- What evidence would resolve it: A systematic ablation study varying the fraction of unconditional training steps and reporting metrics like FID, CLIP-S, PD, and SD for each setting.

### Open Question 2
- Question: Does the model's performance degrade when generating garments with complex or unusual shapes that deviate significantly from the input sketch?
- Basis in paper: [inferred] The paper mentions that the sketch distance metric correlates with the input sketch, suggesting the model follows the sketch closely, but does not test edge cases.
- Why unresolved: The experiments use standard garment sketches, and there is no analysis of performance on atypical or highly stylized sketches.
- What evidence would resolve it: Testing the model on a diverse set of sketches including highly unconventional shapes and measuring the output quality and adherence.

### Open Question 3
- Question: How does the model handle multimodal inputs with conflicting information (e.g., text describing a different garment than the sketch)?
- Basis in paper: [explicit] The paper notes that the model can work with each modality independently, but does not test conflicting inputs.
- Why unresolved: The experiments only use consistent multimodal inputs, and there is no study of model behavior under conflicting conditions.
- What evidence would resolve it: Generating outputs using conflicting text and sketch inputs and analyzing which modality the model prioritizes or how it blends the inputs.

## Limitations

- The paper relies heavily on semi-automated dataset annotation, which may introduce labeling inconsistencies that affect model performance.
- The effectiveness of the 18-channel pose conditioning mechanism depends on accurate keypoint detection, which may fail in complex poses or occlusions.
- The fast multi-condition classifier-free guidance variant's implementation details are not fully specified, making it difficult to verify whether the reported efficiency gains are reproducible.

## Confidence

- High confidence: The core architectural approach of extending Stable Diffusion with multimodal conditioning is well-grounded in existing literature and the implementation details are largely specified.
- Medium confidence: The quantitative improvements over baselines (FID/KID scores) are reported, but the choice of baselines and their implementation details could affect the validity of comparisons.
- Medium confidence: The qualitative results and user study findings are promising, but the sample sizes and methodology details are limited.

## Next Checks

1. Implement a controlled experiment comparing pose preservation with and without the 18-channel conditioning to verify the spatial structure preservation claim.
2. Conduct ablation studies on the sketch conditioning effectiveness by varying the fraction of denoising steps that receive sketch input, as suggested in the mechanism description.
3. Test the model's robustness to keypoint detection failures by systematically occluding body parts in test images and measuring the impact on generation quality.