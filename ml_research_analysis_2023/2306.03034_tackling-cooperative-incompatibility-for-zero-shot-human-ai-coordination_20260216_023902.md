---
ver: rpa2
title: Tackling Cooperative Incompatibility for Zero-Shot Human-AI Coordination
arxiv_id: '2306.03034'
source_url: https://arxiv.org/abs/2306.03034
tags:
- cooperative
- strategies
- learning
- game
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of cooperative incompatibility
  in zero-shot human-AI coordination, where AI agents fail to adapt to previously
  unknown partners. The authors propose the Cooperative Open-ended LEarning (COLE)
  framework, which uses graphic-form games and preference graphs to evaluate and identify
  cooperative incompatibility.
---

# Tackling Cooperative Incompatibility for Zero-Shot Human-AI Coordination

## Quick Facts
- arXiv ID: 2306.03034
- Source URL: https://arxiv.org/abs/2306.03034
- Reference count: 9
- Key outcome: Proposed COLE framework overcomes cooperative incompatibility in zero-shot human-AI coordination, outperforming state-of-the-art methods in Overcooked environment

## Executive Summary
This paper addresses the challenge of cooperative incompatibility in zero-shot human-AI coordination, where AI agents fail to adapt to previously unknown partners. The authors propose the Cooperative Open-ended LEarning (COLE) framework, which uses graphic-form games and preference graphs to evaluate and identify cooperative incompatibility. COLE iteratively generates new strategies that approximate the best responses to empirical gamescapes within preference graphs. The authors also develop COLESV, a practical algorithm that combines the Shapley Value solution with their framework. Extensive experiments in Overcooked environment and with 130 human participants demonstrate that COLESV significantly outperforms state-of-the-art methods in both objective and subjective evaluations when coordinating with previously unencountered AI agents and human proxy models.

## Method Summary
The COLE framework addresses cooperative incompatibility by representing strategies as nodes in graphic-form games (GFGs), where edge weights encode cooperation outcomes. Preference graphs (P-GFGs) identify each strategy's most favorable partner, with in-degree centrality indicating cooperative ability. The Graphic Shapley Value solver evaluates strategies using weighted PageRank, identifying problematic strategies through cooperative incompatible distributions. COLESV combines individual and cooperative objectives to train strategies that work well with incompatible partners. The framework iteratively generates new strategies, evaluates them through the solver, and trains against both individual performance and cooperation with incompatible partners.

## Key Results
- COLESV outperforms state-of-the-art methods (self-play, PBT, FCP, MEP) in Overcooked environment
- Significant improvement in both objective rewards and subjective human evaluations (teamwork, contribution, understanding)
- Effectively overcomes cooperative incompatibility by generating strategies that coordinate well with previously unencountered partners

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graphic-Form Games (GFG) representation allows modeling cooperative relationships as a graph, enabling quantitative evaluation of cooperative incompatibility
- Mechanism: Each node represents a strategy, and edge weights encode expected cooperation outcomes. Preference graph (P-GFG) extracts most favorable teammate for each node, creating directed graph where in-degree reflects preference
- Core assumption: Edge weights accurately represent cooperation quality between strategies
- Evidence anchors:
  - [abstract]: "we introduce Graphic-Form Games (GFGs) to reframe cooperative tasks from the perspective of graph theory to evaluate and pinpoint the cooperative capacity of each strategy"
  - [section]: "The semantics of the preference graph is that a strategy or node i prefers to play with the tailed node to achieve the highest results"
- Break condition: If edge weights fail to capture meaningful cooperation quality, centrality metric becomes invalid

### Mechanism 2
- Claim: COLE framework iteratively generates strategies that approximate best responses to cooperative incompatible partners
- Mechanism: COLE uses solver to derive cooperative incompatible distribution based on Shapley Value and graph centrality, then optimizes for both individual performance and cooperation with incompatible partners
- Core assumption: Cooperative incompatible distribution accurately identifies partners causing coordination failure
- Evidence anchors:
  - [abstract]: "COLE iteratively generates new strategies that approximate the best responses to empirical gamescapes within preference graphs"
  - [section]: "We also show that COLE could effectively overcome the cooperative incompatibility from theoretical and empirical analysis"
- Break condition: If cooperative incompatible distribution fails to identify truly problematic partners

### Mechanism 3
- Claim: Shapley Value-based Graphic Shapley Value solver effectively evaluates and identifies failed-to-collaborate strategies
- Mechanism: Solver uses characteristic function combining Shapley Value with weighted PageRank to evaluate how badly a node performs on its game graph
- Core assumption: Characteristic function accurately captures value of coalitions and identifies strategies that fail to cooperate
- Evidence anchors:
  - [section]: "We incorporate weighted PageRank (WPG) from graph theory into the Shapley Value to evaluate adaptability, particularly with failed-to-collaborate strategies"
  - [section]: "The solver evaluates and identifies failed-to-collaborate strategies by calculating the incompatible cooperative distribution"
- Break condition: If characteristic function fails to capture true coalition value

## Foundational Learning

- Concept: Normal-form games and cooperative game theory
  - Why needed here: Framework builds on game-theoretic foundations to model cooperation and incompatibility as strategic interactions
  - Quick check question: What is the difference between a normal-form game and a characteristic function game?

- Concept: Graph theory and centrality measures
  - Why needed here: GFG and P-GFG representations rely on graph structures and centrality metrics to evaluate cooperative ability
  - Quick check question: How does in-degree centrality in a preference graph relate to cooperative ability?

- Concept: Shapley Value and cooperative solution concepts
  - Why needed here: Graphic Shapley Value solver uses this concept to fairly distribute cooperative value and identify incompatible strategies
  - Quick check question: What does the Shapley Value measure in a cooperative game?

## Architecture Onboarding

- Component map: Simulator -> Solver -> Trainer -> Population Manager -> Evaluation Platform
- Critical path: 1. Initialize population of strategies 2. Complete payoff matrix using simulator 3. Compute cooperative incompatible distribution using solver 4. Train new strategy optimizing individual + cooperative objectives 5. Add new strategy to population 6. Repeat until convergence
- Design tradeoffs:
  - Population size vs. computational cost: Larger populations provide better coverage but increase complexity exponentially
  - Exploration vs. exploitation: Balancing between trying new strategies and refining known good ones
  - Individual vs. cooperative objectives: Finding optimal balance between self-improvement and partner compatibility
- Failure signatures:
  - Cooperative incompatibility persists despite iterations (centrality matrix shows no improvement)
  - Population converges to single strategy with no diversity
  - Human evaluation scores plateau or decrease over iterations
  - Computational resources exhausted before convergence
- First 3 experiments:
  1. Run COLESV with only individual objective (α=1) and observe performance with unseen partners
  2. Run COLESV with only cooperative incompatible objective (α=0) and measure diversity and convergence
  3. Compare COLESV performance against baseline methods (PBT, MEP) in controlled Overcooked layouts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of COLE vary with different values of hyperparameter k controlling ranking threshold for best-preferred strategy?
- Basis in paper: [explicit] Paper states convergence requires satisfying assumption that preference centrality of newly generated strategy is ranked in top k
- Why unresolved: Paper does not provide systematic study of how different k values affect convergence rate and performance
- What evidence would resolve it: Experiments varying k and measuring convergence rate, final performance, and robustness to changes in k

### Open Question 2
- Question: How does computational complexity of Graphic Shapley Value solver scale with size of strategy population and complexity of game graph?
- Basis in paper: [explicit] Paper mentions computational complexity is high and limits population size to 50 due to computational resources
- Why unresolved: Paper does not provide detailed analysis of computational complexity or discuss potential optimizations
- What evidence would resolve it: Theoretical analysis of computational complexity and empirical measurements of computation time for varying population sizes

### Open Question 3
- Question: How does performance of COLE compare to other open-ended learning methods in cooperative multi-agent reinforcement learning, such as MAZE or Pipeline PSRO?
- Basis in paper: [explicit] Paper mentions most open-ended learning methods focus on zero-sum games but does not provide direct comparison with these methods in cooperative setting
- Why unresolved: Paper only compares COLE to traditional self-play, population-based training, and other zero-shot coordination methods
- What evidence would resolve it: Experiments comparing COLE to open-ended learning methods like MAZE or Pipeline PSRO in cooperative multi-agent environments

## Limitations

- Theoretical guarantees are not rigorously proven, relationship between preference centrality and actual coordination success needs stronger empirical validation
- Framework evaluated primarily in Overcooked environment with human proxy model; effectiveness with truly novel human partners in other domains remains uncertain
- Computational complexity of maintaining and training multiple strategy populations could become prohibitive as strategy space grows

## Confidence

**High Confidence**: Basic mechanism of using graphic-form games to represent cooperative relationships and preference graphs to evaluate coordination quality is sound and well-supported by experimental results

**Medium Confidence**: Claim that COLE effectively overcomes cooperative incompatibility is supported by experiments but would benefit from additional validation on truly novel human partners and in different domains

**Low Confidence**: Theoretical analysis of why COLE works and relationship between Shapley Value-based evaluation and actual coordination success are not fully established

## Next Checks

1. **Human Partner Validation**: Conduct experiments with diverse set of real human participants to validate that COLE maintains coordination advantage with truly novel partners across different skill levels and playstyles

2. **Cross-Domain Transfer**: Test framework in different cooperative environment (e.g., multi-agent navigation or resource gathering) to assess whether GFG and P-GFG representations generalize beyond Overcooked

3. **Computational Efficiency Analysis**: Measure computational cost as function of strategy population size and compare against simpler baselines to determine practical scalability limits