---
ver: rpa2
title: Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4
arxiv_id: '2312.16171'
source_url: https://arxiv.org/abs/2312.16171
tags:
- prompts
- kibble
- language
- prompt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces 26 guiding principles for crafting effective
  prompts for large language models (LLMs), aiming to improve response quality, accuracy,
  and alignment with user intent. The principles are organized into five categories:
  Prompt Structure and Clarity, Specificity and Information, User Interaction and
  Engagement, Content and Language Style, and Complex Tasks and Coding Prompts.'
---

# Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4

## Quick Facts
- arXiv ID: 2312.16171
- Source URL: https://arxiv.org/abs/2312.16171
- Reference count: 32
- One-line primary result: Structured, principled prompts improve LLM response quality and correctness across multiple model scales

## Executive Summary
This paper introduces 26 guiding principles for crafting effective prompts for large language models (LLMs), organized into five categories covering prompt structure, specificity, user interaction, content style, and complex tasks. The principles significantly enhance both the quality and correctness of LLM responses, with average improvements of 57.7% in quality and 67.3% in correctness for GPT-4. The improvements scale with model size, demonstrating that larger models benefit more from principled prompting. The authors validate their approach across LLaMA-1/2 (7B, 13B, 70B), GPT-3.5, and GPT-4 using a manually curated ATLAS benchmark.

## Method Summary
The authors systematically tested 26 prompt engineering principles on multiple LLM architectures by applying them to a benchmark of 20 questions per principle. They compared original prompts against principled prompts through human evaluation, measuring both quality improvements ("Boosting") and correctness. The study used LLaMA-1/2 models (7B, 13B, 70B parameters) and GPT models (3.5 and 4), with human evaluators scoring responses. The methodology relies on manual prompt crafting and human judgment rather than automated metrics.

## Key Results
- Average quality improvement of 57.7% for GPT-4 responses using principled prompts
- Average correctness improvement of 67.3% for GPT-4 responses
- Improvements scale with model size: larger models (70B) show greater gains than smaller ones (7B)
- Consistent improvements observed across LLaMA-1/2, GPT-3.5, and GPT-4 model families

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Principled prompts act as structural anchors that shape LLM inference paths.
- Mechanism: The 26 principles constrain the model's latent reasoning space by embedding explicit formatting, role assignment, and task decomposition cues, thereby reducing ambiguity in token prediction.
- Core assumption: LLMs interpret structured textual cues (e.g., "###Instruction###") as task boundaries that guide reasoning decomposition.
- Evidence anchors:
  - [abstract] "26 guiding principles designed to streamline the process of querying and prompting large language models"
  - [section 3.3] "Break down complex tasks into a sequence of simpler prompts in an interactive conversation"
  - [corpus] Weak: No direct corpus evidence linking prompt structure to attention or reasoning pathways.
- Break condition: If the model's attention mechanism ignores structural delimiters (e.g., "###" tokens), the intended task segmentation fails.

### Mechanism 2
- Claim: Specificity in prompts reduces model reliance on training data memorization.
- Mechanism: By explicitly defining audience, task requirements, and output constraints, prompts force the model to adapt its response generation rather than retrieve memorized patterns.
- Core assumption: LLMs have sufficient capacity to generate novel outputs when sufficiently constrained by explicit instructions.
- Evidence anchors:
  - [abstract] "the more precise the task or directive provided, the more effectively the model performs"
  - [section 3.3] "The prompt must provide relevant context that helps the model understand the background and domain of the task"
  - [corpus] No direct evidence linking specificity to reduced memorization in this paper.
- Break condition: If the model's training data already contains near-identical task-response pairs, specificity may not improve generalization.

### Mechanism 3
- Claim: Role assignment and user interaction principles activate the model's simulation capabilities.
- Mechanism: Assigning roles (e.g., "You are an expert in the field") and eliciting requirements through questioning engages the model's ability to simulate expert behavior and refine task understanding iteratively.
- Core assumption: LLMs can simulate personas and adapt responses based on interactive dialogue cues.
- Evidence anchors:
  - [abstract] "It proves beneficial to assign a specific role to LLMs as a means to elicit outputs that better match our intended results"
  - [section 3.3] "Allow the model to elicit precise details and requirements from you by asking you questions until he has enough information"
  - [corpus] Weak: No corpus evidence demonstrating role-based simulation or interactive refinement mechanisms.
- Break condition: If the model lacks robust persona simulation capabilities, role assignment may have minimal impact.

## Foundational Learning

- Concept: Attention mechanisms in transformer models
  - Why needed here: Understanding how attention mechanisms interpret structured prompts is crucial for explaining why principled instructions improve output quality.
  - Quick check question: How do positional encodings and attention heads interact when processing delimiters like "###Instruction###"?

- Concept: Few-shot and zero-shot learning
  - Why needed here: Many principles rely on example-driven prompting, which leverages the model's ability to generalize from few examples without explicit fine-tuning.
  - Quick check question: What distinguishes few-shot prompting from zero-shot prompting in terms of model behavior and prompt design?

- Concept: Bias and fairness in language models
  - Why needed here: Several principles explicitly address bias mitigation, requiring understanding of how biases are encoded in training data and activated during inference.
  - Quick check question: How do specific phrases like "Ensure that your answer is unbiased" influence the model's token generation probabilities?

## Architecture Onboarding

- Component map: Prompt generation → Structured instruction application → LLM inference → Human evaluation → Performance metrics (quality, correctness)
- Critical path: Design principled prompts → Apply to baseline model → Evaluate improvements in quality and correctness
- Design tradeoffs: Balancing prompt specificity with generality; structured delimiters vs. natural language flow; role assignment vs. model bias
- Failure signatures: No improvement in quality/correctness metrics; degradation in response relevance; increased model confusion due to overly complex instructions
- First 3 experiments:
  1. Apply a single principle (e.g., role assignment) to a small-scale model and measure quality improvement.
  2. Combine two complementary principles (e.g., specificity + few-shot examples) and evaluate correctness gains.
  3. Test the same principles on multiple model scales (7B, 13B, 70B) to observe scaling effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed 26 principles perform on language models with architectures significantly different from those tested (e.g., non-transformer models)?
- Basis in paper: [explicit] The authors acknowledge this as a limitation, stating "models with architectures different from those tested might respond in different ways to these principles."
- Why unresolved: The study only evaluated the principles on transformer-based LLMs (LLaMA-1/2 and GPT series). No testing was done on other architectures like recurrent neural networks, convolutional models, or hybrid approaches.
- What evidence would resolve it: Systematic testing of the 26 principles on a diverse set of language models with different architectures, comparing their effectiveness across model types.

### Open Question 2
- Question: How does the effectiveness of the principles scale with the complexity and specificity of questions?
- Basis in paper: [explicit] The authors mention "the effectiveness of these principles may diminish when dealing with questions that are very complex or highly specialized."
- Why unresolved: While the authors tested on different scales of models, they didn't systematically vary question complexity or specialization to map the boundaries of principle effectiveness.
- What evidence would resolve it: A comprehensive study varying question complexity (simple factual to complex reasoning) and specialization (general knowledge to domain-specific expertise) while measuring principle effectiveness across different model scales.

### Open Question 3
- Question: What is the optimal combination of principles for different task types and model scales?
- Basis in paper: [inferred] The paper presents individual principle effects but doesn't explore interactions between principles or identify optimal combinations for specific use cases.
- Why unresolved: The study evaluated principles individually but didn't investigate whether certain combinations work better together or if some principles are redundant when others are applied.
- What evidence would resolve it: Empirical testing of principle combinations across various task types (question answering, code generation, creative writing) and model scales to identify optimal sets for different scenarios.

## Limitations
- Human evaluation introduces subjectivity and lacks detailed inter-rater reliability analysis
- No quantitative metrics beyond human judgment for validation and reproducibility
- Limited testing on non-transformer architectures and highly complex/specialized questions

## Confidence

- **High Confidence**: The general finding that structured, principled prompts improve LLM performance is well-supported by the experimental results across multiple model scales (7B, 13B, 70B). The consistent improvements across different model families (LLaMA-1/2, GPT-3.5/4) strengthen this claim.

- **Medium Confidence**: The mechanism by which specific principles like role assignment and task decomposition improve responses is plausible but not rigorously proven. While the authors provide logical arguments and examples, the underlying neural mechanisms remain speculative.

- **Low Confidence**: The claim that specificity reduces model reliance on training data memorization lacks direct evidence. The paper does not provide analysis of how principles affect the model's internal reasoning processes or token generation probabilities.

## Next Checks

1. **Inter-rater Reliability Analysis**: Conduct a detailed analysis of human evaluator agreement rates, including Cohen's kappa scores, to assess the consistency and reliability of the quality and correctness evaluations.

2. **Quantitative Metric Correlation**: Develop automated metrics (e.g., ROUGE, BLEU, perplexity) to correlate with human evaluations and assess whether the improvements observed by human judges can be validated by objective measures.

3. **Ablation Studies on Individual Principles**: Systematically remove each of the 26 principles from the prompts and measure the marginal contribution of each principle to quality and correctness improvements, identifying which principles provide the most significant benefits.