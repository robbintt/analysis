---
ver: rpa2
title: On consequences of finetuning on data with highly discriminative features
arxiv_id: '2310.19537'
source_url: https://arxiv.org/abs/2310.19537
tags:
- learning
- task
- representations
- training
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study identifies a novel form of catastrophic forgetting in
  neural networks during transfer learning, termed "feature erosion," where models
  abandon previously learned generalizable features in favor of simpler, more discriminative
  patterns introduced during fine-tuning. Through experiments with VGG-19, ResNet-18,
  and ResNet-50 on CIFAR-10 and ImageNet, the research demonstrates that even when
  models achieve perfect accuracy on original tasks, the introduction of highly discriminative
  features (e.g., colored squares) causes abrupt performance collapse on the original
  dataset while the model exclusively focuses on the new patterns.
---

# On consequences of finetuning on data with highly discriminative features

## Quick Facts
- arXiv ID: 2310.19537
- Source URL: https://arxiv.org/abs/2310.19537
- Reference count: 4
- One-line primary result: Novel form of catastrophic forgetting identified where neural networks abandon generalizable features for simpler discriminative patterns during transfer learning

## Executive Summary
This study identifies "feature erosion" as a novel form of catastrophic forgetting in neural networks during transfer learning. The phenomenon occurs when models, fine-tuned on data containing highly discriminative features (e.g., colored squares), abandon previously learned generalizable features in favor of simpler patterns. Experiments with VGG-19, ResNet-18, and ResNet-50 on CIFAR-10 and ImageNet demonstrate that even perfect accuracy on original tasks can be lost after fine-tuning on oversimplified data. The research uses Centered Kernel Alignment (CKA) to reveal significant representation changes and rank collapse across layers, while also showing impaired plasticity for relearning previous tasks.

## Method Summary
The researchers train models to 100% accuracy on original tasks (CIFAR-10, ImageNet), then fine-tune them on modified datasets containing highly discriminative features (colored squares). They measure performance degradation on original tasks, analyze representation changes using CKA similarity across layers, and assess rank collapse in hidden representations. To evaluate plasticity loss, they train models on sequential tasks (original → simplified → original) and measure relearning performance. The study uses standard architectures including VGG-19, ResNet-18, and ResNet-50, with controlled introduction of discriminative patterns.

## Key Results
- Feature erosion causes abrupt performance collapse on original tasks despite perfect initial accuracy
- CKA analysis reveals significant representation changes and rank collapse across layers after fine-tuning
- Models show impaired plasticity, learning new tasks more slowly than training from scratch
- Even modest ratios of oversimplified discriminative data can induce forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on highly discriminative features causes the model to abandon previously learned generalizable features in favor of simpler, more discriminative patterns.
- Mechanism: During transfer learning, the model's optimization process prioritizes features that provide immediate predictive power, even if they are simpler and less generalizable. When presented with highly discriminative features (e.g., colored squares), the model favors these features over previously learned complex representations, leading to catastrophic forgetting of the original task.
- Core assumption: The model's optimization process is greedy and prioritizes immediate performance gains over preserving generalizable knowledge.
- Evidence anchors:
  - [abstract] "networks tend to prioritize basic data patterns, forsaking valuable pre-learned features"
  - [section] "Despite the model's perfect accuracy, finetuning it on the oversimplified task causes an abrupt performance loss...and pushes the model to focus solely on the novel pattern"
  - [corpus] Weak evidence - no direct citations in the corpus papers support this specific mechanism

### Mechanism 2
- Claim: Feature erosion causes rank collapse in the network's hidden representations, reducing the model's ability to learn new tasks.
- Mechanism: When the model focuses on highly discriminative features during fine-tuning, the hidden representations collapse into lower-dimensional subspaces. This rank collapse limits the model's representational capacity and plasticity, making it harder to relearn previous tasks or adapt to new ones.
- Core assumption: The rank of hidden representations is directly correlated with the model's learning capacity and plasticity.
- Evidence anchors:
  - [section] "the numerical rank of representations experiences a substantial decline at each layer following fine-tuning"
  - [section] "The model is trained on the sequence of 3 tasks...However, our results...indicate a deviation from this expected behavior...the network not only learns more slowly compared to training from scratch"
  - [corpus] No direct evidence in corpus papers - this is a novel finding from the study

### Mechanism 3
- Claim: The presence of even modest ratios of oversimplified discriminative data can induce feature erosion, leading to catastrophic forgetting.
- Mechanism: The model's susceptibility to feature erosion is not proportional to the amount of oversimplified data. Even small amounts of highly discriminative features can cause the model to abandon generalizable features, suggesting that the phenomenon is robust and not easily mitigated by data curation.
- Core assumption: The model's optimization process is sensitive to the presence of any highly discriminative features, regardless of their proportion in the dataset.
- Evidence anchors:
  - [section] "we observe a non-linear relationship between the number of oversimplified samples...and the extent of forgetting. However, even modest ratios of oversimplified data are enough to induce forgetting in the model"
  - [section] "Real-world applications that continually receive new data with limited human involvement might come across samples containing highly predictive patterns"
  - [corpus] No direct evidence in corpus papers - this finding is specific to this study

## Foundational Learning

- Concept: Catastrophic Forgetting
  - Why needed here: Understanding catastrophic forgetting is crucial to grasp the phenomenon of feature erosion, where models lose previously learned knowledge during fine-tuning.
  - Quick check question: What is the difference between catastrophic forgetting and graceful forgetting in neural networks?

- Concept: Transfer Learning
  - Why needed here: Feature erosion is a specific case of transfer learning failure, where the model fails to leverage previously learned knowledge effectively.
  - Quick check question: How does transfer learning typically benefit model performance, and why does feature erosion represent a failure of this process?

- Concept: Simplicity Bias
  - Why needed here: The study suggests that feature erosion is related to the model's tendency to prioritize simpler, more discriminative features over complex, generalizable ones.
  - Quick check question: What is simplicity bias in neural networks, and how might it contribute to the phenomenon of feature erosion?

## Architecture Onboarding

- Component map: Pre-training phase -> Introduction of discriminative features -> Fine-tuning phase -> Performance evaluation and representation analysis
- Critical path: The fine-tuning phase where the introduction of highly discriminative features leads to feature erosion and catastrophic forgetting of the original task
- Design tradeoffs: The tradeoff is between leveraging highly discriminative features for immediate performance gains and preserving generalizable knowledge from previous tasks. The study suggests that prioritizing discriminative features can lead to loss of plasticity and difficulty in relearning previous tasks.
- Failure signatures: Abrupt performance collapse on the original task, significant changes in hidden representations (measured by CKA similarity), and rank collapse of representations across layers
- First 3 experiments:
  1. Replicate the basic feature erosion experiment using VGG-19 on CIFAR-10, introducing colored squares to the dataset and measuring performance on the original task.
  2. Analyze the impact of feature erosion on hidden representations using CKA similarity and rank analysis across different layers.
  3. Investigate the loss of plasticity by training the model on a sequence of tasks (original, oversimplified, original) and measuring the ability to relearn the original task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum ratio of highly discriminative features needed to induce feature erosion in a model?
- Basis in paper: [explicit] The study observes that even modest ratios of oversimplified data are enough to induce forgetting, but does not quantify the exact threshold.
- Why unresolved: The paper only mentions that "modest ratios" are sufficient without providing specific numbers or conducting a systematic analysis to determine the minimum effective ratio.
- What evidence would resolve it: Conducting experiments with varying ratios of samples containing highly discriminative features and measuring the point at which feature erosion consistently occurs.

### Open Question 2
- Question: Can feature erosion be prevented or mitigated through architectural modifications or training techniques?
- Basis in paper: [inferred] The study notes that simplicity bias is resilient to approaches like ensembles or adversarial training, but does not explore other potential solutions.
- Why unresolved: The paper focuses on identifying and analyzing feature erosion rather than developing countermeasures, leaving open the question of how to prevent or mitigate this phenomenon.
- What evidence would resolve it: Experiments testing various architectural modifications (e.g., different network structures, regularization techniques) or training strategies to determine their effectiveness in preventing feature erosion.

### Open Question 3
- Question: How does feature erosion affect the model's ability to learn entirely new tasks unrelated to the original or simplified tasks?
- Basis in paper: [inferred] While the study examines loss of plasticity in relearning the original task, it does not investigate the impact on learning completely new, unrelated tasks.
- Why unresolved: The experiments focus on sequential tasks that are related, leaving the effect of feature erosion on learning entirely new tasks unexplored.
- What evidence would resolve it: Conducting experiments where models with feature erosion are trained on completely new tasks to assess their learning performance compared to models without feature erosion.

## Limitations

- Limited architecture diversity: Experiments primarily use VGG-19, ResNet-18, and ResNet-50, which may not capture the full range of scenarios where feature erosion occurs
- Dataset specificity: Findings are based on CIFAR-10 and ImageNet subsets, limiting generalizability to other domains
- No mitigation strategies explored: The study focuses on identifying and analyzing feature erosion rather than developing countermeasures

## Confidence

- High Confidence: The empirical demonstration of feature erosion using CKA similarity and rank analysis is well-supported by the experimental results
- Medium Confidence: The mechanism explanation (simplicity bias leading to feature erosion) is plausible but could benefit from additional theoretical grounding
- Low Confidence: The generalizability of findings across different architectures and datasets is limited by the scope of experiments

## Next Checks

1. **Architecture Diversity Test**: Replicate the experiments with additional architectures (e.g., Vision Transformers, EfficientNets) to assess whether feature erosion is a universal phenomenon across different neural network designs

2. **Dataset Generalization**: Test the feature erosion phenomenon on diverse datasets beyond CIFAR-10 and ImageNet, including medical imaging or other specialized domains, to evaluate robustness across different data distributions

3. **Intervention Effectiveness**: Experiment with various regularization techniques (e.g., elastic weight consolidation, knowledge distillation) during fine-tuning to determine if feature erosion can be mitigated while maintaining the benefits of transfer learning