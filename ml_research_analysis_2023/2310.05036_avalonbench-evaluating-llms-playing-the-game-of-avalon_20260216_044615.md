---
ver: rpa2
title: 'AvalonBench: Evaluating LLMs Playing the Game of Avalon'
arxiv_id: '2310.05036'
source_url: https://arxiv.org/abs/2310.05036
tags:
- player
- players
- team
- evil
- good
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AvalonBench, a comprehensive game environment
  for evaluating multi-agent LLM agents in the strategic social deduction game Resistance
  Avalon. The benchmark includes a game environment, rule-based bots as baseline opponents,
  and ReAct-style LLM agents with tailored prompts.
---

# AvalonBench: Evaluating LLMs Playing the Game of Avalon
## Quick Facts
- arXiv ID: 2310.05036
- Source URL: https://arxiv.org/abs/2310.05036
- Reference count: 31
- ChatGPT playing good role achieves 22.2% win rate against rule-based bots

## Executive Summary
This paper introduces AvalonBench, a comprehensive game environment for evaluating multi-agent LLM agents in the strategic social deduction game Resistance Avalon. The benchmark includes a game environment, rule-based bots as baseline opponents, and ReAct-style LLM agents with tailored prompts. Experiments show a significant capability gap: ChatGPT playing the good role achieves a win rate of 22.2% against rule-based bots playing evil, while the good-role bot achieves 38.2% in the same setting. The paper demonstrates that current LLM agents lack the reasoning, persuasion, negotiation, and deception capabilities needed to play Avalon effectively, highlighting the potential for exploring advanced decision-making techniques integrated with LLMs.

## Method Summary
The paper presents AvalonBench as a game environment for evaluating LLM agents in the social deduction game Resistance Avalon. The framework uses ReAct-style prompting with separate parsing to improve action reliability, recursive summarization to handle context limitations, and compares LLM agents against rule-based bots in various configurations. The evaluation measures win rates for good vs evil teams, assassination accuracy for Merlin, and deduction accuracy for servants across different experimental settings.

## Key Results
- ChatGPT playing good role achieves 22.2% win rate vs rule-based bots playing evil
- Rule-based bot playing good role achieves 38.2% win rate in same setting
- LLM agents show significant gaps in reasoning, persuasion, negotiation, and deception capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReAct-style prompting with separate parsing improves action reliability compared to direct LLM output
- Mechanism: LLM generates natural language reasoning and action description, then a separate parsing LLM converts that description into structured game commands
- Core assumption: LLM output format is inconsistent but semantically meaningful; parsing LLM can extract valid actions from natural language
- Evidence anchors:
  - [abstract] "we ask the LLM generate both the (1) actions, (2) dialogue, and (3) summary of the game history"
  - [section 3.1] "we found that using a separate PARSER improves the ability of the LLM-player to produce the correct actions (with success rate 100% in our pilot experiments)"

### Mechanism 2
- Claim: Recursive summarization reduces context window limitations and improves reasoning quality
- Mechanism: LLM maintains a compressed summary of game history, updated each round by incorporating discussion minutes and mission outcomes
- Core assumption: Critical information can be preserved in compressed form while filtering noise; summary is sufficient for decision-making
- Evidence anchors:
  - [section 3.2] "Large amounts of discussion and game data can be generated... our baseline naive strategies only use the outcomes of missions for their strategies"
  - [section 3.2] "we also ask the LLM to summarize their history recursively by feeding them the previous history Summaryt−1, the minutes of the discussions this round Minutest"

### Mechanism 3
- Claim: The Avalon game environment provides a natural benchmark for multi-agent LLM reasoning and deception capabilities
- Mechanism: Game requires players to deduce identities, coordinate, and deceive through language, creating a challenging test-bed that isolates LLM capabilities
- Core assumption: The complexity of Avalon (deduction, negotiation, deception) requires advanced language and reasoning beyond current LLM capabilities
- Evidence anchors:
  - [abstract] "Players in Avalon are challenged not only to make informed decisions based on dynamically evolving game phases, but also to engage in discussions where they must deceive, deduce, and negotiate with other players"
  - [section 1] "Based on these features of Resistance Avalon, we believe it is a good test-bed for evaluating and studying the language understanding and reasoning capability of AI Agents"

## Foundational Learning

- Concept: Social deduction game mechanics (hidden identities, asymmetric information)
  - Why needed here: Understanding Avalon's rules is essential to grasp why LLM performance gaps exist and what capabilities need improvement
  - Quick check question: In Avalon, which players know each other's identities and which do not?

- Concept: ReAct prompting framework (reason-then-act paradigm)
  - Why needed here: The paper's LLM agents use ReAct for decision-making, so understanding this framework is crucial for implementation
  - Quick check question: What are the two main components of ReAct prompting as applied in this paper?

- Concept: Chain-of-thought reasoning in LLMs
  - Why needed here: The paper uses zero-shot CoT for decision-making, which affects how LLMs process game states
  - Quick check question: How does zero-shot CoT prompting differ from few-shot prompting in the context of game decision-making?

## Architecture Onboarding

- Component map: Game engine (state management, rule enforcement) → LLM agent (actions, discussion, summarization) → Parser (action extraction) → Baseline bots (deterministic strategies)
- Critical path: Game state update → LLM decision generation → Parser action extraction → Game state update
- Design tradeoffs: Language-based reasoning vs. rule-based efficiency; summary compression vs. information retention; parsing reliability vs. LLM autonomy
- Failure signatures: LLM generates invalid actions → Parser fails → LLM reasoning contradicts game state → LLM reveals own identity in discussion
- First 3 experiments:
  1. Run a single game with one LLM agent vs. baseline bots, verify all components communicate correctly
  2. Test LLM action parsing with edge cases (ambiguous language, multiple valid actions)
  3. Compare LLM win rate vs. baseline in controlled scenarios (e.g., fixed team compositions)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of private information (Merlin knowing all evil players' identities) impact the effectiveness of LLM agents compared to rule-based bots in Avalon?
- Basis in paper: [explicit] The paper discusses that Merlin has additional information and needs to guide good players discreetly, while evil players try to sniff out Merlin.
- Why unresolved: The experiments show that LLM agents struggle with deception and persuasion, but it's unclear how much the private information advantage affects this.
- What evidence would resolve it: Comparative experiments with and without private information for LLM agents, measuring win rates and deduction accuracy in both scenarios.

### Open Question 2
- Question: Would allowing LLMs to use voting history in addition to mission outcomes significantly improve their performance in Avalon?
- Basis in paper: [inferred] The paper notes that baseline strategies only use mission outcomes, not voting history, and this is done to ensure fair comparison.
- Why unresolved: The paper doesn't explore whether incorporating voting history would help LLMs perform better, as it's excluded to maintain fairness with baselines.
- What evidence would resolve it: Experiments comparing LLM performance with and without access to voting history, measuring win rates and deduction accuracy.

### Open Question 3
- Question: How would implementing a more sophisticated deception strategy for LLM agents (e.g., randomizing voting patterns) affect their win rates in Avalon?
- Basis in paper: [explicit] The paper mentions that naive Merlin reveals identity too easily through voting patterns and suggests adding voting randomness to improve.
- Why unresolved: The paper only uses naive strategies and doesn't explore more advanced deception techniques for LLM agents.
- What evidence would resolve it: Experiments implementing various deception strategies (e.g., randomized voting, strategic team selection) for LLM agents and measuring their impact on win rates and Merlin assassination accuracy.

## Limitations
- Exact prompt templates and LLM parameter settings are not specified
- Temperature and sampling parameters for LLM inference are missing
- Specific model versions and configurations are not disclosed

## Confidence
- High confidence: Fundamental observation that current LLM agents struggle with Avalon's social deduction mechanics
- Medium confidence: Effectiveness of ReAct prompting with parsing mechanism
- Low confidence: Generalizability of AvalonBench framework to other social deduction games

## Next Checks
1. Reconstruct prompt templates by testing variations with complete rules, role-specific instructions, action format requirements, and summary format guidelines
2. Systematically vary temperature settings (0.0, 0.3, 0.7) and max_tokens during LLM inference to determine optimal parameters
3. Test AvalonBench framework with different LLM models (GPT-4, Claude, Mistral) to assess performance differences