---
ver: rpa2
title: 'My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and
  Evaluation Benchmarks'
arxiv_id: '2306.14030'
source_url: https://arxiv.org/abs/2306.14030
tags:
- marathi
- code-mixed
- tweets
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of code-mixed Marathi-English datasets
  and pretrained models for NLP research. The authors introduce L3Cube-MeCorpus, a
  5-million-sentence code-mixed corpus for pretraining, and three supervised evaluation
  datasets (MeHate, MeSent, MeLID) with ~12,000 annotated tweets each.
---

# My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks

## Quick Facts
- arXiv ID: 2306.14030
- Source URL: https://arxiv.org/abs/2306.14030
- Authors: 
- Reference count: 10
- Key outcome: Introduces L3Cube-MeCorpus (5M sentences), three supervised datasets, and MeBERT/MeRoBERTa models for code-mixed Marathi-English NLP with significant performance improvements

## Executive Summary
This paper addresses the critical lack of resources for code-mixed Marathi-English NLP research by introducing a comprehensive suite of datasets and pretrained models. The authors present L3Cube-MeCorpus, a 5-million-sentence code-mixed corpus for pretraining, and three supervised evaluation datasets (MeHate, MeSent, MeLID) with approximately 12,000 annotated tweets each. They train and release MeBERT and MeRoBERTa models on this corpus, achieving state-of-the-art results on downstream tasks including hate speech detection (78.07% F1), sentiment analysis (67.27%), and language identification (88.41%).

## Method Summary
The approach involves three main components: first, collecting and preprocessing a large code-mixed Marathi-English corpus from social media sources; second, pretraining BERT and RoBERTa models using masked language modeling on this corpus; and third, fine-tuning the pretrained models on three supervised datasets for specific downstream tasks. The pretraining uses 5 million code-mixed sentences, followed by task-specific fine-tuning on hate speech detection, sentiment analysis, and language identification datasets, each containing approximately 12,000 annotated tweets.

## Key Results
- MeRoBERTa achieves 78.07% F1 on code-mixed Marathi-English hate speech detection
- MeBERT-based models significantly outperform existing Marathi BERT models on all tasks
- MeRoBERTa reaches 88.41% F1 on language identification and 67.27% on sentiment analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on a large code-mixed corpus improves model performance on downstream code-mixed tasks.
- Mechanism: The model learns language patterns, vocabulary, and context specific to code-mixed Marathi-English through masked language modeling on 5 million sentences, enabling better generalization to tasks like hate speech detection, sentiment analysis, and language identification.
- Core assumption: The pretraining corpus adequately represents the distribution of code-mixed Marathi-English data encountered in real-world applications.
- Evidence anchors:
  - [abstract]: "We present L3Cube-MeCorpus, a large code-mixed Marathi-English (Mr-En) corpus with 10 million social media sentences for pretraining."
  - [section]: "We introduce MeCorpus, a new pre-training corpus of 5 million code-mixed Marathi sentences."
  - [corpus]: Limited - the paper doesn't provide detailed statistics on the distribution of Marathi vs English words or the diversity of mixing patterns in the corpus.
- Break condition: If the pretraining corpus contains biased or unrepresentative samples, or if the mixing patterns differ significantly from the downstream tasks, performance gains may not materialize.

### Mechanism 2
- Claim: Fine-tuning on task-specific supervised datasets improves model performance on those tasks.
- Mechanism: The pre-trained models are adapted to the specific task objectives (hate speech detection, sentiment analysis, language identification) by training on labeled examples, learning task-specific decision boundaries.
- Core assumption: The supervised datasets are large enough and representative of the task's complexity to enable effective fine-tuning.
- Evidence anchors:
  - [abstract]: "Furthermore, for benchmarking, we present three supervised datasets MeHate, MeSent, and MeLID for downstream tasks like code-mixed Mr-En hate speech detection, sentiment analysis, and language identification respectively."
  - [section]: "The supervised dataset contains labels for code-mixed Marathi-English hate classification, sentiment detection, and language identification."
  - [corpus]: Strong - Table 3 shows each dataset has ~12,000 annotated tweets, and the annotation process is described in detail.
- Break condition: If the supervised datasets are too small, contain annotation errors, or don't cover the full complexity of the task, fine-tuning may not lead to improved performance.

### Mechanism 3
- Claim: Using code-mixed models outperforms models trained on monolingual data for code-mixed tasks.
- Mechanism: Code-mixed models learn to handle the linguistic phenomena specific to code-mixing (e.g., word order changes, borrowing, code-switching points) that monolingual models may struggle with.
- Core assumption: The linguistic phenomena in code-mixed Marathi-English are distinct enough from monolingual Marathi or English to warrant specialized models.
- Evidence anchors:
  - [abstract]: "Ablations show that the models trained on this novel corpus significantly outperform the existing state-of-the-art BERT models."
  - [section]: "We train several well-known models on our novel pretraining corpora... resulting models were named similarly to the original models, prefixed with 'me', which stands for Marathi-English."
  - [corpus]: Weak - while the paper claims code-mixed models outperform monolingual ones, it doesn't provide detailed analysis of the specific linguistic phenomena that contribute to this improvement.
- Break condition: If the code-mixed data doesn't exhibit distinct linguistic phenomena, or if the monolingual models are already well-adapted to handle code-mixing, the advantage of code-mixed models may be limited.

## Foundational Learning

- Concept: Code-mixing (also known as code-switching)
  - Why needed here: The entire work focuses on code-mixed Marathi-English data and models, so understanding what code-mixing is and its challenges is crucial.
  - Quick check question: What are the key differences between code-mixing and code-switching, and how do they impact NLP model performance?

- Concept: Masked Language Modeling (MLM)
  - Why needed here: The pretraining process uses MLM to train the BERT-based models on the code-mixed corpus.
  - Quick check question: How does the MLM objective work, and why is it effective for pretraining language models?

- Concept: Transformer architecture
  - Why needed here: The models used (BERT, RoBERTa) are based on the transformer architecture, which is fundamental to their functioning.
  - Quick check question: What are the key components of the transformer architecture, and how do they contribute to the model's ability to handle long-range dependencies and parallelization?

## Architecture Onboarding

- Component map: Data Collection -> Pretraining -> Fine-tuning -> Evaluation
- Critical path: Data Collection → Pretraining → Fine-tuning → Evaluation
  - The quality and quantity of the pretraining corpus directly impacts the model's ability to learn code-mixed patterns.
  - The size and quality of the supervised datasets determine the effectiveness of fine-tuning.
  - The evaluation process validates the overall effectiveness of the approach.

- Design tradeoffs:
  - Using a larger pretraining corpus may improve generalization but increases computational cost.
  - Focusing on specific dialects or topics in data collection may improve task-specific performance but reduce overall robustness.
  - Using more complex model architectures may improve performance but increase training time and resource requirements.

- Failure signatures:
  - Poor performance on downstream tasks despite successful pretraining may indicate issues with the supervised datasets or fine-tuning process.
  - High variance in model performance across different splits of the data may indicate overfitting or under-representation in the training data.
  - Unexpectedly low performance compared to baseline models may indicate issues with the pretraining corpus or model architecture choices.

- First 3 experiments:
  1. Train a baseline BERT model on the code-mixed corpus and evaluate on the downstream tasks to establish a reference point.
  2. Train a BERT model on a monolingual Marathi corpus and compare its performance on the code-mixed tasks to the code-mixed model.
  3. Fine-tune the code-mixed model on each supervised dataset separately to identify which tasks benefit most from the pretraining.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MeBERT and MeRoBERTa models compare to other multilingual models like multilingual-BERT and XLM-RoBERTa when trained on code-mixed Marathi data?
- Basis in paper: [explicit] The paper mentions that previous research efforts have focused on data representations that are multilingual and cross-lingual in nature, offering improvements in accuracy and latency. However, these models are pre-trained on less than a hundred thousand real code-mix texts.
- Why unresolved: The paper does not provide a direct comparison of MeBERT and MeRoBERTa models with multilingual-BERT and XLM-RoBERTa models on code-mixed Marathi data.
- What evidence would resolve it: A direct comparison of the performance of MeBERT and MeRoBERTa models with multilingual-BERT and XLM-RoBERTa models on code-mixed Marathi data would resolve this question.

### Open Question 2
- Question: What are the specific challenges in processing and analyzing code-mixed data, and how can these challenges be addressed in future research?
- Basis in paper: [explicit] The paper mentions that code-mixed data is inherently difficult to process and analyze due to its linguistic complexity, variance in spelling and grammar, and long-tailed distribution of uncommon terms and phrases, which are often specific to the geography and demographic of the source location.
- Why unresolved: The paper does not provide a detailed analysis of the specific challenges in processing and analyzing code-mixed data or propose solutions to address these challenges.
- What evidence would resolve it: A comprehensive analysis of the specific challenges in processing and analyzing code-mixed data, along with proposed solutions and their effectiveness, would resolve this question.

### Open Question 3
- Question: How can the MeBERT and MeRoBERTa models be further improved to handle the diverse spelling variations present in real-life code-mixed texts?
- Basis in paper: [explicit] The paper mentions that a major problem while dealing with Romanized Marathi is the lack of a singular correct spelling of words, and developing efficient approaches to tackle this issue will lead to a significant increase in performance on NLP tasks dealing with code-mixed languages.
- Why unresolved: The paper does not provide specific methods or approaches to improve the MeBERT and MeRoBERTa models to handle the diverse spelling variations in code-mixed texts.
- What evidence would resolve it: Research demonstrating the effectiveness of specific methods or approaches to improve the MeBERT and MeRoBERTa models in handling diverse spelling variations in code-mixed texts would resolve this question.

## Limitations
- Limited corpus statistics and analysis of code-mixing patterns in the pretraining data
- Evaluation restricted to Twitter domain without testing on other text sources
- Insufficient comparison with other multilingual models and monolingual baselines

## Confidence
**High Confidence Claims**
- The MeBERT and MeRoBERTa models outperform existing Marathi BERT models on the three supervised datasets
- The supervised datasets (MeHate, MeSent, MeLID) are well-constructed with approximately 12,000 annotated tweets each
- The pretraining process using masked language modeling on the code-mixed corpus is effective for learning code-mixed patterns

**Medium Confidence Claims**
- Code-mixed models generally outperform monolingual models for code-mixed tasks (limited empirical comparison provided)
- The pretraining corpus adequately represents real-world code-mixed Marathi-English data (insufficient statistical analysis of corpus composition)
- The improvements in F1 scores translate to meaningful practical improvements in downstream applications

**Low Confidence Claims**
- The specific linguistic phenomena that contribute to code-mixed model advantages are identified and characterized
- The models will generalize well to code-mixed Marathi-English beyond the Twitter domain
- The current model sizes and architectures are optimal for code-mixed Marathi-English tasks

## Next Checks
1. **Corpus Composition Analysis**: Conduct a detailed statistical analysis of the MeCorpus to quantify the distribution of Marathi vs English content, identify common code-switching patterns, and assess whether the corpus represents the diversity of code-mixed Marathi-English usage across different contexts and registers.

2. **Domain Generalization Study**: Evaluate the MeBERT and MeRoBERTa models on code-mixed Marathi-English data from sources other than Twitter (e.g., news articles, social media platforms, conversational data) to assess their ability to generalize beyond the training domain and identify potential domain-specific limitations.

3. **Error Analysis and Ablation**: Perform detailed error analysis on the three supervised tasks to identify specific types of errors the models make, conduct ablation studies to determine which components of the model architecture and training process contribute most to performance improvements, and compare performance against simplified baselines to better understand the value added by the code-mixed pretraining approach.