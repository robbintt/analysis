---
ver: rpa2
title: 'CiwaGAN: Articulatory information exchange'
arxiv_id: '2309.07861'
source_url: https://arxiv.org/abs/2309.07861
tags:
- articulatory
- learning
- information
- speech
- generator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CiwaGAN, a model that combines unsupervised
  articulatory modeling with unsupervised information exchange through the auditory
  modality, aiming to create a more realistic approximation of human spoken language
  acquisition using deep learning. The model uses a Generative Adversarial Network
  (GAN) architecture where an Articulatory Generator creates articulatory movements
  and a Q-network decodes information from the generated sounds.
---

# CiwaGAN: Articulatory information exchange

## Quick Facts
- arXiv ID: 2309.07861
- Source URL: https://arxiv.org/abs/2309.07861
- Reference count: 0
- Key outcome: Combines unsupervised articulatory modeling with auditory information exchange using GAN architecture to achieve near-categorical lexical learning performance

## Executive Summary
CiwaGAN is a novel model that combines unsupervised articulatory modeling with information exchange through the auditory modality to create a more realistic approximation of human spoken language acquisition. The model uses a Generative Adversarial Network (GAN) architecture where an Articulatory Generator creates articulatory movements that are converted into waveforms via a pre-trained physical model. A Q-network then decodes information from the generated sounds, enabling bidirectional information flow between articulatory and auditory representations. The model achieves near-categorical performance on lexical learning, with specific words consistently produced when latent code variables are set to extreme values.

## Method Summary
CiwaGAN combines an Articulatory Generator that produces EMA (Electromagnetic Articulography) channels and voicing signals with an ema2wav model that converts these articulatory movements into waveforms. The model is trained on 9 words from the TIMIT dataset (water, like, carry, greasy, ask, year, suit, dark, wash) and MNGU0 EMA recordings from one British English speaker. A Q-network learns to recover the latent code from generated audio, enabling information exchange between modalities. The model uses extreme value analysis (setting code values to 20) to achieve near-categorical lexical performance and employs DTW correlation to evaluate articulatory learning quality.

## Key Results
- Near-categorical lexical learning performance with most words showing single pronounced peaks for specific codes when set to extreme values
- "Suit" word achieved 94% accuracy (94/100 samples) when code was set to [0,0,0,0,0,0,0,0,20]
- Generated articulatory movements closely matched real EMA data for the word "suit"
- Achieved 22.5% WER after 250 steps of fine-tuning Whisper ASR on generated speech

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CiwaGAN achieves near-categorical lexical learning by setting latent code variables to extreme values.
- Mechanism: When individual code variables are set to extreme values (e.g., 20 instead of 1), the model produces a single pronounced lexical item consistently, allowing the underlying representation of each latent code to be uncovered.
- Core assumption: The Articulatory Generator learns to encode information into articulatory movements in a way that the Q-network can decode it from the auditory output, even though the modalities differ.
- Evidence anchors:
  - [abstract] "The model achieves near-categorical performance on lexical learning, with most words showing a single pronounced peak corresponding to a specific code when its value is set to extreme values."
  - [section] "By applying the extreme value technique to the Articulatory Generator, we are testing the technique on a new frontier... we can uncover the underlying representation of each unit and get a near categorical performance on lexical learning."

### Mechanism 2
- Claim: The Q-network learns to decode the unique code from the Generator's latent space by reconstructing the posterior distribution Q(c|x).
- Mechanism: The Q-network takes the waveform output of the ema2wav model and attempts to recover the latent code c that was passed into the Articulatory Generator, optimizing against the Q-loss to approximate the posterior distribution.
- Core assumption: The Generator learns to encode information into the generated data in a way that is recoverable by the Q-network, despite the Generator never directly accessing the training data.
- Evidence anchors:
  - [section] "Along with the Articulatory Generator, this network is optimized against the following 'Q-loss' in an attempt to approximate the posterior distribution Q(c|x)."

### Mechanism 3
- Claim: The improved ema2wav model with increased audio quality and new hyperparameters reduces noise and jitter in the EMA channels, leading to more interpretable articulatory gestures.
- Mechanism: The updated ema2wav model initializes weights with those of a neural spectrum-to-waveform vocoder, improving the fidelity of the trained model. Decreased stride and filter size in the Articulatory Generator reduce noise and jitter in the EMA channels.
- Core assumption: Higher audio quality and reduced noise in the EMA channels lead to more accurate and interpretable articulatory gestures that correspond to specific lexical items.
- Evidence anchors:
  - [section] "We also introduce new hyperparameter choices that improve training: decreased stride and filter size which reduce noise and jitter in the EMA channels (Figure 4)."

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: CiwaGAN uses a GAN architecture where the Articulatory Generator learns to produce realistic articulatory movements and the Discriminator evaluates their authenticity.
  - Quick check question: What is the role of the Discriminator in a GAN, and how does it contribute to the learning process of the Generator?

- Concept: Unsupervised Learning
  - Why needed here: CiwaGAN learns to encode and decode information without direct access to labeled data, mimicking the unsupervised nature of human language acquisition.
  - Quick check question: How does CiwaGAN learn to associate specific articulatory gestures with lexical items without explicit supervision?

- Concept: Information Exchange
  - Why needed here: The model aims to simulate human spoken language acquisition by learning to encode information into sounds and decode it from them, as humans do during language acquisition.
  - Quick check question: How does the Q-network in CiwaGAN contribute to the information exchange process, and what is its role in decoding the latent code from the generated audio?

## Architecture Onboarding

- Component map: Articulatory Generator (latent code + z) -> EMA channels + voicing -> ema2wav model -> waveforms -> Discriminator + Q-network
- Critical path:
  1. Articulatory Generator produces articulatory movements based on latent code variables
  2. ema2wav model converts the articulatory movements into waveforms
  3. Discriminator evaluates the authenticity of the generated audio
  4. Q-network attempts to decode the latent code from the generated audio

- Design tradeoffs:
  - Using a GAN architecture allows for unsupervised learning and realistic articulatory modeling but introduces challenges in training stability and information exchange between modalities
  - The use of a pre-trained ema2wav model simplifies the audio generation process but may limit the model's ability to adapt to different speakers or accents

- Failure signatures:
  - If the Discriminator fails to distinguish between real and generated audio, the Articulatory Generator may not learn realistic articulatory representations
  - If the Q-network cannot accurately recover the latent code from the generated audio, the information exchange process may be compromised

- First 3 experiments:
  1. Train the model with a small set of lexical items and evaluate the near-categorical performance of the Articulatory Generator using the extreme value technique
  2. Analyze the quality of the generated articulatory gestures by comparing them to real EMA data and calculating correlation coefficients
  3. Test the model's ability to generalize to unseen lexical items by evaluating the Q-network's performance on generated audio from new codes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the CiwaGAN model generalize to new speakers beyond the training data, particularly given that the ema2wav model is trained on a single speaker of British English while the training data contains approximately 600 speakers of Standardized American English?
- Basis in paper: [explicit] The paper states: "The ema2wav model is trained on a single speaker of British English, while the training data contains approximately 600 speakers of Standardized American English. Our model offers an advantage from the cognitive modeling perspective: humans learn to utilize their individual articulators based on auditory feedback from various individuals. However, this setting significantly increases the complexity of the training objectives."
- Why unresolved: The paper does not provide results or analysis on how well the model generalizes to new speakers or how the single-speaker ema2wav model affects performance on multi-speaker data.
- What evidence would resolve it: Experiments showing CiwaGAN's performance on speakers not present in the training data, comparisons between ema2wav models trained on single vs. multiple speakers, and analysis of speaker-specific vs. speaker-independent features in the generated articulatory data.

### Open Question 2
- Question: How does the extreme value technique (setting individual latent variables to values far outside the training range) affect the interpretability and performance of other GAN-based models for articulatory learning?
- Basis in paper: [explicit] The paper states: "By setting unique code values to extremes, we can induce the output of specific words. Consequently, CiwaGAN provides a framework for analyzing both lexical and articulatory learning."
- Why unresolved: The paper applies this technique specifically to CiwaGAN but does not explore its applicability or effectiveness in other GAN-based articulatory models.
- What evidence would resolve it: Applying the extreme value technique to other GAN-based articulatory models and comparing the resulting interpretability and performance with CiwaGAN, including quantitative metrics and qualitative analysis of generated articulatory data.

### Open Question 3
- Question: What are the limitations of using a fine-tuned Whisper ASR model for evaluating the quality of generated speech, and how might these limitations affect the conclusions about lexical learning in CiwaGAN?
- Basis in paper: [explicit] The paper states: "The performance of CiwaGAN is sufficiently high so that the outputs can be automatically evaluated (as opposed to being evaluated by a trained phonetician). We evaluate the outputs with a fine-tuned Whisper ASR model."
- Why unresolved: The paper does not discuss potential biases, errors, or limitations of using Whisper for evaluating generated speech, nor does it consider how these might impact the assessment of lexical learning.
- What evidence would resolve it: A detailed analysis of Whisper's performance on CiwaGAN-generated speech, including error patterns, comparisons with human phonetician evaluations, and potential improvements or alternatives for automatic evaluation of generated speech quality and lexical learning.

## Limitations
- Limited to 9 training words, with no evaluation of generalization to new vocabulary
- Relies on EMA data from a single British English speaker, limiting cross-speaker generalization
- Extreme value technique (setting codes to 20) represents artificial experimental condition rather than natural operating mode

## Confidence
- High Confidence: Technical architecture and training procedure are well-specified and reproducible
- Medium Confidence: Demonstration of lexical learning through extreme value analysis is methodologically sound but limited in generalizability
- Low Confidence: Claims about realistic approximation of human language acquisition lack empirical validation against human benchmarks

## Next Checks
1. Evaluate model's ability to generate and correctly identify previously unseen lexical items beyond the 9 training words
2. Test the model with EMA data from multiple speakers to evaluate robustness to individual articulatory differences
3. Conduct perceptual studies where human listeners evaluate the naturalness and intelligibility of generated speech