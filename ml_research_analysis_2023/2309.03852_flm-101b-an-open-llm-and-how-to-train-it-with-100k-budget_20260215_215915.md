---
ver: rpa2
title: 'FLM-101B: An Open LLM and How to Train It with $100K Budget'
arxiv_id: '2309.03852'
source_url: https://arxiv.org/abs/2309.03852
tags:
- flm-101b
- training
- language
- data
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FLM-101B, a 101B-parameter large language
  model trained on a $100K budget using a progressive growth strategy. The model is
  built on the FreeLM architecture and incorporates function-preserving growth operators
  inspired by MSG.
---

# FLM-101B: An Open LLM and How to Train It with $100K Budget

## Quick Facts
- arXiv ID: 2309.03852
- Source URL: https://arxiv.org/abs/2309.03852
- Authors: 
- Reference count: 40
- Key outcome: 101B-parameter LLM trained for $100K using progressive growth, achieving 80% of baseline performance with 10% of FLOPs

## Executive Summary
FLM-101B introduces a cost-effective approach to training extremely large language models by using a progressive growth strategy that trains smaller models first and grows them sequentially to larger sizes. The authors demonstrate that by training 16B, 51B, and finally 101B parameter models with decreasing data amounts, they can achieve competitive performance while significantly reducing training costs. The model incorporates improvements to training stability through unified objectives and leverages hyperparameter transfer techniques to maintain performance across growth stages.

## Method Summary
The paper presents a progressive growth training strategy where three models (16B, 51B, 101B) are trained sequentially using function-preserving growth operators inspired by MSG. Each subsequent model receives less data than its predecessor, with the total training consuming 245.37B tokens for 16B, 39.64B tokens for 51B, and 26.54B tokens for 101B models. The approach uses a unified training objective combining language modeling and teacher signal objectives, mixed precision training with Bfloat16, and a 3D parallel strategy (tensor, pipeline, and data parallelism) across 24 DGX-A800 GPU servers. The total training time was 21.54 days, achieving a 2.2x speedup compared to direct 101B training.

## Key Results
- Trained 101B-parameter model for $100K budget using progressive growth strategy
- Achieved 80% of baseline performance using only 10% of floating-point operations
- Demonstrated 54.8% time savings (2.2x speedup) compared to training 101B model from scratch
- Released model checkpoint at https://huggingface.co/CofeAI/FLM-101B

## Why This Works (Mechanism)

### Mechanism 1
Progressive growth strategy reduces training cost while maintaining performance. By training sequentially from 16B to 51B to 101B parameters, smaller models consume data faster and larger models improve loss more per step, achieving 80% of baseline performance with only 10% of FLOPs. Core assumption: Function-preserving growth operators ensure knowledge inheritance across model sizes.

### Mechanism 2
Unified training objectives improve stability for large models. Combining language modeling and teacher signal objectives using masking strategy and special tokens eliminates training instability beyond 16B parameters. Core assumption: Unified objective handles both signal types without additional classification modules.

### Mechanism 3
μP transfer enables stable training of large models with small model insights. μScaling uses loss prediction from smaller proxy models to identify optimal hyperparameters, ensuring convergence when transferred to larger models. Core assumption: Universal relations across training dynamics of models with width tending to infinity hold for practical model sizes.

## Foundational Learning

- **Concept: Progressive growth in neural networks**
  - Why needed here: Enables training of very large models by starting with smaller, faster-to-train versions that inherit knowledge
  - Quick check question: What are the two key advantages of starting with a smaller model in progressive growth?

- **Concept: Function-preserving transformations**
  - Why needed here: Ensures knowledge from smaller models is not lost when growing to larger sizes
  - Quick check question: What mathematical property must growth operators satisfy to ensure function preservation?

- **Concept: μP (maximal update parameterization)**
  - Why needed here: Enables stable transfer of hyperparameters from small to large models
  - Quick check question: How does μP differ from standard parameterization in terms of scaling behavior?

## Architecture Onboarding

- **Component map**: FreeLM backbone (GPT-based with Pre-LayerNorm) -> xPos (Extrapolatable Position Embedding) -> Unified training objectives (language + teacher signals) -> Progressive growth operators (MSG-inspired) -> 3D parallelism (tensor + pipeline + data parallelism)
- **Critical path**: Model initialization → 16B training → growth to 51B → growth to 101B → evaluation
- **Design tradeoffs**: Mixed precision (Bfloat16) vs. full precision: saves memory but may affect stability; Unified objectives vs. separate objectives: improves stability but may lose some specialization; Progressive growth vs. direct training: saves cost but adds complexity
- **Failure signatures**: Loss divergence during growth stages; Performance degradation compared to direct training; Memory overflow during mixed precision training
- **First 3 experiments**: 1) Train 16B model with unified objectives and μP-based hyperparameters; 2) Apply growth operators to 16B checkpoint and train as 51B model; 3) Apply growth operators to 51B checkpoint and train as 101B model

## Open Questions the Paper Calls Out

- **Open Question 1**: How can we further reduce the training cost of large language models beyond the growth strategy approach described in this paper? The paper demonstrates significant cost reduction but acknowledges this is an ongoing research area with potential for further improvements.

- **Open Question 2**: What are the long-term effects of the growth strategy on model performance and knowledge retention when scaling to even larger models (e.g., 1T+ parameters)? The paper only tests three model sizes and doesn't examine whether benefits persist at extreme scales.

- **Open Question 3**: How can we develop more comprehensive and objective evaluation frameworks for measuring artificial intelligence that go beyond knowledge-oriented benchmarks? The paper identifies this critical gap and proposes an IQ evaluation framework as a starting point.

## Limitations
- The specific GPU allocation details and cluster configuration are not fully specified, making it difficult to independently verify the claimed cost savings
- The unified training objective lacks detailed ablation studies comparing it to separate objectives for language modeling and teacher signals
- The novel IQ evaluation framework and its correlation with real-world language understanding capabilities remain unproven

## Confidence

**High Confidence**: The basic training methodology using progressive growth and the FreeLM architecture is technically sound and follows established practices. The sequential training approach is well-documented.

**Medium Confidence**: Performance claims on standard benchmarks appear reasonable given the model size and training approach, though comparisons to baselines could be more comprehensive.

**Low Confidence**: The novel IQ evaluation framework and its implications for model intelligence assessment lack sufficient validation. The claim that FLM-101B "excels" in symbolic mapping and pattern mining tasks needs more rigorous comparison.

## Next Checks
1. **Independent Cost Verification**: Replicate the training setup using the provided checkpoint and measure actual GPU hours and associated costs to verify the $100K budget claim and the 54.8% time-saving compared to direct training.

2. **Ablation Study of Unified Objectives**: Conduct controlled experiments comparing the unified training objective with separate objectives for language modeling and teacher signals across different model sizes to quantify the stability improvements and potential performance trade-offs.

3. **IQ Evaluation Benchmark Validation**: Compare FLM-101B's performance on the proposed IQ tasks against established benchmarks (MMLU, BIG-bench) and other open-source models of similar size to establish the correlation between symbolic task performance and general language understanding capabilities.