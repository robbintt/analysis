---
ver: rpa2
title: 'Models of reference production: How do they withstand the test of time?'
arxiv_id: '2307.14817'
source_url: https://arxiv.org/abs/2307.14817
tags:
- grec
- corpora
- different
- corpus
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study revisits the GREC shared tasks in reference generation
  by testing models on corpora beyond the original Wikipedia domain and using more
  advanced evaluation methods. It finds that the original GREC conclusions are no
  longer reliable, as model performance is highly sensitive to corpus choice and evaluation
  metrics.
---

# Models of reference production: How do they withstand the test of time?

## Quick Facts
- arXiv ID: 2307.14817
- Source URL: https://arxiv.org/abs/2307.14817
- Reference count: 19
- Key outcome: Original GREC conclusions are unreliable; model performance is highly sensitive to corpus choice and evaluation metrics, with PLMs showing better cross-corpus robustness

## Executive Summary
This study revisits the GREC shared tasks in reference generation by testing models on corpora beyond the original Wikipedia domain and using more advanced evaluation methods. It finds that the original GREC conclusions are no longer reliable, as model performance is highly sensitive to corpus choice and evaluation metrics. PLM-based models show greater robustness across corpora, while classic ML models perform inconsistently. Per-class analysis reveals that ML models struggle with low-frequency classes like descriptions in non-Wikipedia data. Feature importance varies by corpus, suggesting corpus-specific effects. The study concludes that future work should use multiple corpora and metrics, and that PLMs offer better generalizability for reference generation tasks.

## Method Summary
The study evaluates reference generation models on three corpora: MSR (GREC original), NEG (news articles), and WSJ (financial news). It tests classic ML models (UDel, ICSI, CNTS, OSU, IS-G) and PLM-based models (BERT, RoBERTa) using three evaluation metrics: accuracy, macro-F1, and weighted macro-F1. The research conducts per-class analysis, correlation studies between corpora, and feature importance analysis using XGBoost and permutation importance to understand model behavior across different domains.

## Key Results
- Classic ML models show high sensitivity to corpus choice, with performance rankings varying significantly across domains
- PLM-based models (BERT, RoBERTa) demonstrate superior robustness and consistent performance across all three corpora
- Macro-F1 emerges as the most reliable evaluation metric, particularly for detecting poor performance on low-frequency classes like descriptions
- Feature importance rankings vary substantially across corpora, indicating corpus-specific linguistic effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PLM-based models outperform classic ML models in cross-corpus generalizability.
- Mechanism: PLMs are trained on diverse large-scale text corpora, capturing robust linguistic patterns that generalize across domains. Fine-tuning these models on reference generation tasks allows them to adapt to different corpora without losing generalization.
- Core assumption: Pre-trained representations capture universal linguistic features relevant to referential form selection.
- Evidence anchors:
  - [abstract] "Our results also suggest that pre-trained language models are less dependent on the choice of corpus than classic Machine Learning models, and therefore make more robust class predictions."
  - [section 6.3] "In six out of nine rankings, BERT and RoBERTa are ranked as the top two models."
- Break condition: If PLMs are fine-tuned on corpora with highly domain-specific or artificial language, their generalization advantage may diminish.

### Mechanism 2
- Claim: Classic ML models are highly sensitive to corpus choice due to their reliance on handcrafted features.
- Mechanism: Classic ML models depend on corpus-specific feature engineering. When applied to a different genre, the feature distributions shift, leading to poor performance as the models cannot adapt without retraining on new features.
- Core assumption: Corpus-specific linguistic features (e.g., recency, grammatical role) have different distributions across genres.
- Evidence anchors:
  - [section 6.3] "In contrast to the robust performance of the PLM models, the performance of the classic ML models is more corpus-dependent."
  - [section 7.3] "The lack of correlation between the results on MSR/WSJ and those on NEG/WSJ suggests that using a corpus of a different genre could greatly influence the ranking of the models."
- Break condition: If feature engineering is made corpus-agnostic or includes robust cross-domain features, this sensitivity may reduce.

### Mechanism 3
- Claim: Macro-F1 is a more reliable evaluation metric for reference generation tasks than accuracy or weighted F1.
- Mechanism: Macro-F1 treats all classes equally, preventing over-generation of frequent classes from masking poor performance on rare classes. This is crucial in REG where minority classes (e.g., descriptions) are important for natural-sounding text.
- Core assumption: In reference generation, balanced performance across all referential forms is more important than overall accuracy.
- Evidence anchors:
  - [section 7.3] "Also, we may conclude that macro-averaged F1 is a more reliable evaluation metric (see the discussions in Section 6, Section 7.1, and Section 7.2)."
  - [section 7.2] "The poor prediction of the class description by the classic ML NEG models is likely due to an insufficient number of instances in the training dataset."
- Break condition: If the dataset is perfectly balanced or all classes are equally important, macro-F1 may not provide additional insights over other metrics.

## Foundational Learning

- Concept: Cross-corpus evaluation
  - Why needed here: The study investigates whether models generalize across different genres (Wikipedia vs. WSJ), requiring understanding of how to split, preprocess, and evaluate models on multiple corpora.
  - Quick check question: What is the difference between document-wise and instance-wise splitting, and why is document-wise splitting preferred for REG tasks?

- Concept: Pre-trained language models (PLMs)
  - Why needed here: PLMs are central to the study's comparison of model robustness. Understanding how to fine-tune PLMs for classification tasks is essential.
  - Quick check question: What is the difference between BERT and RoBERTa, and why might one be preferred over the other for a given task?

- Concept: Feature importance analysis
  - Why needed here: The study uses XGBoost and permutation importance to analyze how linguistic features contribute to model predictions across corpora.
  - Quick check question: What is permutation importance, and how does it differ from model-specific importance measures like those from decision trees?

## Architecture Onboarding

- Component map: MSR, NEG, WSJ corpora -> Preprocess (RE annotation parsing) -> Classic ML models (UDel, ICSI, CNTS, OSU, IS-G) and PLM models (BERT, RoBERTa) -> Evaluation (Accuracy, macro-F1, weighted macro-F1) -> Analysis (Bayes Factor, per-class evaluation, correlation, feature importance)

- Critical path:
  1. Load and preprocess MSR, NEG, and WSJ corpora
  2. Train classic ML models with corpus-specific features
  3. Fine-tune PLM models on each corpus
  4. Evaluate all models using three metrics
  5. Perform Bayes Factor and per-class analysis
  6. Compute correlation between corpora
  7. Conduct feature importance analysis

- Design tradeoffs:
  - Classic ML: Requires extensive feature engineering, sensitive to corpus choice, interpretable
  - PLM: Requires less feature engineering, more robust to corpus shifts, less interpretable
  - Evaluation: Accuracy favors majority classes, macro-F1 balances class importance, weighted macro-F1 balances frequency and importance

- Failure signatures:
  - Classic ML models fail on low-frequency classes in new corpora (e.g., descriptions in NEG)
  - PLMs may overfit if fine-tuned too aggressively on small corpora
  - Inconsistent results across metrics indicate corpus-specific overfitting

- First 3 experiments:
  1. Train and evaluate UDel (classic ML) on MSR and WSJ; compare per-class F1 to detect corpus sensitivity
  2. Fine-tune BERT on MSR, evaluate on NEG and WSJ; measure drop in macro-F1 to assess generalization
  3. Compute Spearman correlation between MSR and WSJ evaluation rankings using macro-F1 to quantify corpus impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of ML-based models improve with more training data for low-frequency classes like descriptions?
- Basis in paper: [explicit] The paper notes that ML models struggle with low-frequency classes like descriptions in non-Wikipedia data and suggests this may be due to insufficient instances in the training dataset.
- Why unresolved: The study used fixed datasets and did not experiment with varying amounts of training data for these classes.
- What evidence would resolve it: Controlled experiments varying the number of training instances for low-frequency classes while measuring model performance would clarify if data quantity is the limiting factor.

### Open Question 2
- Question: How do PLM-based models generalize to entirely new domains beyond the WSJ corpus?
- Basis in paper: [explicit] The paper shows PLM-based models are less dependent on corpus choice than ML models but only tested on WSJ as a non-Wikipedia domain.
- Why unresolved: Testing was limited to one additional domain, leaving questions about performance in other genres or domains.
- What evidence would resolve it: Testing PLM-based models on a diverse set of new domains (e.g., scientific papers, social media, legal texts) would reveal their true generalizability limits.

### Open Question 3
- Question: What specific linguistic factors most influence referential form selection across different domains?
- Basis in paper: [explicit] The feature importance analysis showed that the ranking of linguistic factors varies across corpora, suggesting corpus-specific effects.
- Why unresolved: The study identified that rankings differ but did not deeply investigate which factors are universally important versus domain-specific.
- What evidence would resolve it: Systematic feature ablation studies across multiple diverse corpora would identify which factors consistently matter versus those that are domain-dependent.

## Limitations
- The study focuses on a specific reference generation task (pronoun, description, proper name classification) that may not capture the full complexity of real-world reference production
- Feature importance analysis examined a limited set of linguistic features, potentially missing other important factors
- The generalizability of findings to domains beyond the tested WSJ corpus remains uncertain

## Confidence
- High: PLMs show robust cross-corpus performance compared to classic ML models
- Medium: Feature importance findings reveal corpus-specific effects but with limited feature scope
- Medium: Macro-F1 is identified as optimal metric, though this depends on balanced class importance

## Next Checks
1. Test model robustness on additional corpora from different domains (e.g., news articles, social media, scientific literature) to verify corpus-generalizability findings

2. Conduct ablation studies on feature engineering for classic ML models to determine which features contribute most to corpus sensitivity

3. Evaluate model performance on a balanced reference generation dataset to assess whether macro-F1 remains the optimal evaluation metric when class distributions are equalized