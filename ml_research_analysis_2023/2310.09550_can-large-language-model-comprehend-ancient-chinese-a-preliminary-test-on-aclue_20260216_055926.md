---
ver: rpa2
title: Can Large Language Model Comprehend Ancient Chinese? A Preliminary Test on
  ACLUE
arxiv_id: '2310.09550'
source_url: https://arxiv.org/abs/2310.09550
tags:
- chinese
- ancient
- language
- tasks
- aclue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We introduce ACLUE, a benchmark for evaluating large language\
  \ models\u2019 ability to understand ancient Chinese. It contains 15 tasks covering\
  \ lexical, syntactic, semantic, inference, and knowledge skills, with questions\
  \ ranging from the Xia dynasty to the Ming dynasty."
---

# Can Large Language Model Comprehend Ancient Chinese? A Preliminary Test on ACLUE

## Quick Facts
- arXiv ID: 2310.09550
- Source URL: https://arxiv.org/abs/2310.09550
- Reference count: 14
- Key outcome: Large language models achieve only 37.4% average accuracy on ancient Chinese comprehension tasks, significantly underperforming on ancient versus modern Chinese

## Executive Summary
This paper introduces ACLUE, a benchmark designed to evaluate large language models' ability to understand ancient Chinese texts spanning from the Xia dynasty to the Ming dynasty. The benchmark contains 15 tasks covering lexical, syntactic, semantic, inference, and knowledge skills. When tested on 8 state-of-the-art models including ChatGPT, LLaMA, and ChatGLM2, the models showed strong performance on modern Chinese but struggled significantly with ancient Chinese, achieving an average accuracy of only 37.4%. The best-performing model, ChatGLM2, still only achieved modest scores, indicating substantial room for improvement in LLMs' ancient Chinese comprehension capabilities.

## Method Summary
The ACLUE benchmark evaluates 8 state-of-the-art LLMs using zero-shot and in-context five-shot evaluation on 15 tasks covering ancient Chinese comprehension. Tasks were derived from a mix of manually curated questions from publicly available resources and automatically generated questions from classical Chinese language corpora. All tasks are formatted as multiple-choice questions with four options to ensure fair comparison across models. The benchmark spans various ancient Chinese periods and tests different skills including lexical understanding, syntax, semantics, inference, and knowledge. Models are evaluated on their ability to answer questions about ancient Chinese texts, with accuracy measured as the primary metric.

## Key Results
- Large language models achieve only 37.4% average accuracy on ancient Chinese comprehension tasks
- ChatGLM2 performs best among evaluated models but still shows limited ancient Chinese understanding
- Models show strong performance on modern Chinese but struggle significantly with ancient Chinese texts
- Automatic question generation provides scalable benchmark expansion but may introduce quality variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models perform worse on ancient Chinese than on modern Chinese due to lack of exposure during pretraining
- Mechanism: The pretraining corpus of most LLMs is heavily biased toward modern text, leaving them undertrained on the distinct syntax, vocabulary, and cultural references of ancient Chinese
- Core assumption: Pretraining data distributions determine downstream language understanding capability
- Evidence anchors:
  - [abstract] "While these models perform well on modern Chinese, they struggle with ancient Chinese, achieving an average accuracy of only 37.4% on ACLUE"
  - [section] "However, due to the lack of ancient language benchmarks, the abilities of LLMs in handling ancient language remains largely unexplored"
  - [corpus] Found 25 related papers with average neighbor FMR=0.368, suggesting limited related work and benchmark availability
- Break condition: If pretraining data is augmented with ancient Chinese or if models undergo task-specific fine-tuning

### Mechanism 2
- Claim: Multiple-choice format with distractors of similar difficulty allows fairer comparison across models with different prompting strategies
- Mechanism: By standardizing all tasks to four-option multiple-choice questions, the evaluation reduces variance from differences in output decoding or prompt formatting
- Core assumption: Multiple-choice questions with one correct answer and three plausible distractors provide a controlled evaluation setup
- Evidence anchors:
  - [abstract] "Similar to the well-established LLM benchmarks such as ARC (Clark et al., 2018) and MMLU (Hendrycks et al., 2021), ACLUE adopts multiple-choice question format for all tasks"
  - [section] "To ensure fair comparison among different models trained with varying approaches, all tasks are formatted into multiple-choice questions with four choices, of which only one is correct"
  - [corpus] Limited related benchmarks mentioned, supporting novelty of standardized evaluation format
- Break condition: If models begin exploiting subtle biases in distractor construction or if question ambiguity increases

### Mechanism 3
- Claim: Automatic generation of questions from corpora provides a scalable way to expand benchmark coverage, though with slightly lower quality than manually curated questions
- Mechanism: The ACLUE benchmark uses both automatically generated questions from annotated corpora and manually collected questions from tests and other works to balance scale and quality
- Core assumption: Automated question generation can approximate the difficulty of real test questions when grounded in high-quality annotations
- Evidence anchors:
  - [abstract] "The tasks were derived from a mix of manually curated and automatically generated questions"
  - [section] "Among the 15 tasks, 8 were automatically generated using existing corpora or datasets, 5 were collected from freely available standard tests, and 2 were directly sourced from other work"
  - [corpus] Evidence suggests limited prior work in ancient Chinese benchmarks, making automatic generation valuable
- Break condition: If automatically generated questions become too predictable or fail to cover the full range of required skills

## Foundational Learning

- Concept: Understanding of pretraining data distributions and their impact on model performance
  - Why needed here: The paper's results hinge on the idea that models are undertrained on ancient Chinese due to pretraining corpus bias
  - Quick check question: If a model is pretrained only on modern text, what performance would you expect on ancient Chinese comprehension tasks without fine-tuning?

- Concept: Benchmark design principles, particularly the use of standardized evaluation formats
  - Why needed here: ACLUE uses multiple-choice format to ensure fair comparison, which requires understanding why this approach works
  - Quick check question: Why might a multiple-choice format with distractors be preferable to open-ended generation for cross-model comparison?

- Concept: Automated data generation techniques for NLP benchmarks
  - Why needed here: A significant portion of ACLUE tasks were automatically generated, requiring knowledge of how this process works
  - Quick check question: What are potential advantages and limitations of using automatically generated questions versus manually curated ones in a benchmark?

## Architecture Onboarding

- Component map: Benchmark loader → Task selection → Prompt formatting → Model inference → Answer extraction → Accuracy calculation
- Critical path: Task selection → Prompt formatting (with or without examples) → Model inference → Answer extraction → Accuracy calculation
- Design tradeoffs: Using multiple-choice questions simplifies evaluation but may not capture the full complexity of ancient Chinese understanding. The mix of automatically generated and manually curated questions balances scale with quality but introduces potential inconsistency in difficulty.
- Failure signatures: Low variance in model performance across tasks may indicate that questions are too easy or too difficult. Disproportionate performance on automatically generated versus manually collected questions could suggest exposure bias or quality differences.
- First 3 experiments:
  1. Run ACLUE evaluation on a model with known modern Chinese proficiency to establish baseline performance gap
  2. Compare model performance on automatically generated questions versus manually collected questions to assess quality differences
  3. Test model performance with and without few-shot examples to determine if in-context learning helps with ancient Chinese comprehension

## Open Questions the Paper Calls Out

- Question: What specific architectural or training modifications could improve LLMs' performance on ancient Chinese tasks?
  - Basis in paper: [explicit] The paper notes that existing LLMs struggle with ancient Chinese despite their success in modern Chinese, suggesting room for improvement
  - Why unresolved: The paper only evaluates existing models without exploring modifications
  - What evidence would resolve it: Experiments testing different model architectures, training datasets, or fine-tuning strategies specifically for ancient Chinese comprehension

- Question: How does the performance gap between modern and ancient Chinese comprehension vary across different types of ancient Chinese texts?
  - Basis in paper: [explicit] The paper observes a performance disparity between modern and ancient Chinese but doesn't analyze variations across different text types
  - Why unresolved: The paper provides overall performance metrics but lacks detailed analysis by text genre
  - What evidence would resolve it: Comparative analysis of model performance across poetry, prose, historical texts, and other ancient Chinese genres

- Question: To what extent does exposure to specific ancient Chinese corpora during pre-training influence model performance on ACLUE tasks?
  - Basis in paper: [explicit] The paper notes that ChatGLM2's strong performance on certain tasks might be due to exposure to original texts used in ACLUE
  - Why unresolved: The paper doesn't investigate the relationship between training data and task performance
  - What evidence would resolve it: Detailed analysis of model training data and its correlation with performance on different ACLUE tasks

## Limitations
- Benchmark quality may vary between automatically generated and manually curated questions
- Pretraining corpus distributions of evaluated models are not disclosed
- Multiple-choice format may not fully capture ancient Chinese comprehension complexity

## Confidence
- **High**: Models perform significantly worse on ancient Chinese than modern Chinese (37.4% average accuracy)
- **Medium**: Performance gaps are primarily due to pretraining corpus bias against ancient Chinese
- **Medium**: Multiple-choice format provides fair cross-model comparison

## Next Checks
1. Conduct ablation studies comparing model performance on automatically generated versus manually curated ACLUE questions to quantify quality differences
2. Analyze pretraining corpus composition of evaluated models to establish correlation between ancient Chinese exposure and performance
3. Test whether fine-tuning on ancient Chinese texts significantly improves ACLUE scores, distinguishing between pretraining data effects and architectural limitations