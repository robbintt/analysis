---
ver: rpa2
title: Re-evaluating Retrosynthesis Algorithms with Syntheseus
arxiv_id: '2310.19796'
source_url: https://arxiv.org/abs/2310.19796
tags:
- search
- single-step
- accuracy
- retrosynthesis
- reactions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Automated synthesis planning, also known as retrosynthesis, is
  a growing focus in the intersection of chemistry and machine learning. Despite the
  appearance of steady progress, we argue that imperfect benchmarks and inconsistent
  comparisons mask systematic shortcomings of existing techniques.
---

# Re-evaluating Retrosynthesis Algorithms with Syntheseus

## Quick Facts
- arXiv ID: 2310.19796
- Source URL: https://arxiv.org/abs/2310.19796
- Reference count: 29
- Key outcome: Automated synthesis planning evaluation framework revealing systematic shortcomings in existing benchmarks

## Executive Summary
This paper presents syntheseus, a synthesis planning library with an extensive benchmarking framework designed to promote best practices in retrosynthesis evaluation. The authors argue that existing benchmarks suffer from inconsistent evaluation protocols, leading to overstated performance claims and incomparable results across studies. By standardizing evaluation procedures including molecule validation, deduplication, and search algorithm implementation, syntheseus enables meaningful comparisons between single-step models and multi-step planning algorithms. The framework is demonstrated through re-evaluation of several state-of-the-art retrosynthesis algorithms, revealing that model rankings can change significantly under controlled evaluation conditions.

## Method Summary
The syntheseus library provides a standardized interface for evaluating retrosynthesis algorithms through three main components: single-step model wrappers, multi-step search algorithm implementations, and evaluation metrics collection. The framework standardizes preprocessing steps including invalid molecule filtering, stereochemistry canonicalization, and deduplication across all models. For multi-step evaluation, it implements caching mechanisms to prevent redundant computation during search, ensuring that time limits reflect algorithmic performance rather than computational overhead. The library supports various search algorithms including MCTS and Retro*, and provides visualization tools for analyzing planning results. All evaluations are performed under identical conditions using pre-trained models wrapped through a consistent interface.

## Key Results
- GLN's published top-5 accuracy on USPTO-50K can be increased by 5.8% through deduplication
- Model rankings change significantly when evaluated under consistent conditions
- Caching single-step model outputs is essential for fair multi-step search evaluation
- Top-k accuracy measures recall rather than precision, highlighting limitations of current metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework corrects overstated performance claims by controlling for model post-processing
- Mechanism: By standardizing how invalid molecules, duplicates, and stereochemistry are handled, the library eliminates inconsistent evaluation practices that artificially inflate reported accuracy
- Core assumption: Previous works used inconsistent post-processing steps that directly affected the reported metrics
- Evidence anchors:
  - "Some prior works include invalid molecules in the top-k, whereas other works filter them and consider the top-k valid molecules"
  - "we found that the published top-5 accuracy of GLN (Dai et al., 2019) on USPTO-50K can be increased by as much as 5.8% by applying simple deduplication"
- Break condition: If the underlying data contains systematic errors that affect all evaluations equally, the relative rankings might still be preserved

### Mechanism 2
- Claim: Consistent benchmarking enables meaningful model comparisons across different single-step models
- Mechanism: The library enforces a minimal interface for all single-step models, ensuring that each model is evaluated under identical conditions
- Core assumption: Different implementations of the same evaluation protocol can produce inconsistent results
- Evidence anchors:
  - "This enables users to build their models separately from SYNTHESEUS and integrate them by writing a thin wrapper, allowing SYNTHESEUS to evaluate and use all single-step models in a consistent way"
  - "Crucially, results in this paper were produced by our evaluation framework with no numbers copied from previous work, which ensures a fair comparison immune to many issues discussed in Section 2"
- Break condition: If the wrapper introduces implementation errors or if models fundamentally rely on different input representations that cannot be standardized

### Mechanism 3
- Claim: Proper multi-step search evaluation requires caching to avoid resource misallocation
- Mechanism: The library automatically caches single-step model outputs during search, preventing redundant computation and ensuring that time limits reflect actual algorithmic performance
- Core assumption: Without caching, the same molecule may be expanded multiple times in different parts of the search tree, wasting computational resources
- Evidence anchors:
  - "As calling the reaction model is expensive, a well-engineered CASP system would clearly cache the outputs of the reaction model to avoid duplicate computation"
  - "often large sub-trees can occur in multiple places during search; without a cache, expanding each occurrence of these subtrees will count against an algorithm's time budget, whereas with a cache these expansions are effectively free"
- Break condition: If the search algorithm fundamentally relies on re-evaluating molecules with different contexts, caching might hide important performance differences

## Foundational Learning

- Concept: Single-step retrosynthesis accuracy metrics
  - Why needed here: Understanding the limitations of top-k accuracy and MRR is crucial for interpreting model performance and avoiding misleading conclusions
  - Quick check question: What is the key difference between measuring recall and precision in retrosynthesis evaluation?

- Concept: Multi-step search algorithm components
  - Why needed here: Distinguishing between the single-step model, search graph, and heuristics is essential for understanding how to properly evaluate and compare different planning algorithms
  - Quick check question: How does caching single-step model outputs affect the fairness of multi-step search comparisons?

- Concept: Chemical reaction representation and processing
  - Why needed here: Knowledge of SMILES strings, stereochemistry, and atom mappings is necessary for understanding the preprocessing steps and evaluation criteria
  - Quick check question: Why is it important to remove atom mapping information before evaluating single-step models on USPTO-50K?

## Architecture Onboarding

- Component map: Model wrapper → single-step prediction → caching layer → search algorithm → metric collection → result aggregation
- Critical path: For evaluation, the critical path is: model wrapper → single-step prediction → caching layer → search algorithm → metric collection → result aggregation
- Design tradeoffs: The framework prioritizes consistency and extensibility over raw performance, accepting some computational overhead to ensure fair comparisons
- Failure signatures: Inconsistent results across runs likely indicate issues with random seeds or non-deterministic model behavior; missing cache hits suggest problems with molecule canonicalization
- First 3 experiments:
  1. Wrap a simple template-based model and verify that top-1 accuracy matches published results on USPTO-50K
  2. Run a basic MCTS search with a single-step model and verify that the caching mechanism reduces redundant calls
  3. Compare the effect of deduplication on published GLN results to reproduce the 5.8% improvement finding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the best way to evaluate the precision of single-step retrosynthesis models beyond simple top-k accuracy metrics?
- Basis in paper: [explicit] The paper argues that top-k accuracy measures recall rather than precision, and suggests that precision (fraction of top k reactions that are feasible) is equally or more important for multi-step search
- Why unresolved: Without experimental validation or a high-quality feasibility model, it's difficult to measure the precision of single-step models
- What evidence would resolve it: Development of a reliable feasibility model or experimental validation data that can be used to measure the precision of single-step models across a wide range of reactions

### Open Question 2
- Question: How can we train a high-quality reaction feasibility model to estimate whether reactions will succeed?
- Basis in paper: [inferred] The paper suggests that developing a reaction feasibility model could resolve several issues with current evaluation practices
- Why unresolved: Training a high-quality feasibility model is described as an open research question
- What evidence would resolve it: Successful development and validation of a reaction feasibility model that can accurately predict the success of reactions

### Open Question 3
- Question: What is the optimal balance between beam search and test-time data augmentation for models like RootAligned?
- Basis in paper: [explicit] The paper mentions that RootAligned uses a combination of beam search and test-time data augmentation, and that finding the right balance requires careful tuning
- Why unresolved: The paper only used default settings for RootAligned and did not explore the optimal balance between these techniques
- What evidence would resolve it: Systematic experiments varying both the number of augmentations and beams for RootAligned and other similar models

## Limitations

- The framework's effectiveness depends on the quality and consistency of input data preprocessing
- Results are limited to the specific datasets and models included in the study and may not generalize to all retrosynthesis scenarios
- The framework addresses benchmarking consistency but not fundamental limitations in retrosynthesis model architectures

## Confidence

- High Confidence: Claims about the impact of inconsistent post-processing (e.g., deduplication) on reported accuracy metrics
- Medium Confidence: Claims about the relative ranking of models changing under controlled conditions
- Medium Confidence: Claims about the importance of caching in multi-step search evaluation

## Next Checks

1. Apply syntheseus to evaluate models on additional retrosynthesis datasets beyond USPTO-50K to verify generalization across different chemical spaces
2. Systematically compare models with different architectural approaches (template-based, transformer-based, graph-based) using syntheseus to determine if relative rankings are consistent across model families
3. Engage with the retrosynthesis research community to adopt syntheseus as a standard evaluation tool and collect feedback on additional benchmarking challenges