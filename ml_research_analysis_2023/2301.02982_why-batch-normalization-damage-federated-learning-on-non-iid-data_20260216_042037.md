---
ver: rpa2
title: Why Batch Normalization Damage Federated Learning on Non-IID Data?
arxiv_id: '2301.02982'
source_url: https://arxiv.org/abs/2301.02982
tags:
- local
- data
- parameters
- global
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the first convergence analysis of FedAvg with
  batch normalization (BN) under non-i.i.d. data, showing that BN mismatches in local
  vs global statistical parameters and gradients cause gradient deviation, slowing
  and biasing FL convergence.
---

# Why Batch Normalization Damage Federated Learning on Non-IID Data?

## Quick Facts
- arXiv ID: 2301.02982
- Source URL: https://arxiv.org/abs/2301.02982
- Reference count: 40
- Key outcome: First convergence analysis of FedAvg with BN under non-i.i.d. data, showing gradient deviation from BN parameter mismatches slows and biases FL convergence. FedTAN is proposed to solve this via layer-wise aggregation of statistical parameters and gradients.

## Executive Summary
This paper provides the first theoretical and empirical analysis of how batch normalization (BN) damages federated learning (FL) convergence under non-i.i.d. data. The core finding is that mismatches between local and global BN statistical parameters and their gradients cause gradient deviation, which slows and biases FL convergence. To address this, the authors propose FedTAN, which performs layer-wise aggregation of BN parameters and gradients to ensure consistency. Experiments on CIFAR-10 with ResNet-20 demonstrate that FedTAN outperforms existing FL methods under various non-i.i.d. data distributions while maintaining fast convergence and negligible communication overhead.

## Method Summary
The paper analyzes FedAvg with BN under non-i.i.d. data, proving that gradient deviation occurs when local and global BN statistical parameters (batch mean, batch variance) mismatch. FedTAN is proposed to eliminate this issue through layer-wise aggregation of both BN parameters and their gradients during forward and backward propagation. The method is implemented on CIFAR-10 with ResNet-20 across 5 clients, comparing against FedAvg+BN, FedAvg+Algorithm 2, FedBN, SiloBN, and FedAvg+GN baselines. Training uses mini-batch SGD with batch size 128, learning rate starting at 0.5 then 0.05 after 6000 iterations, with E=5 local steps.

## Key Results
- FedTAN achieves superior performance under non-i.i.d. data compared to FedAvg+BN, FedBN, SiloBN, and FedAvg+Algorithm 2
- Gradient deviation from BN parameter mismatches slows and biases FL convergence under non-i.i.d. data
- Layer-wise aggregation of both BN parameters and gradients is necessary for robust FL performance
- FedTAN maintains fast convergence and negligible communication overhead while handling various data distributions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Batch normalization (BN) causes gradient deviation in federated learning (FL) under non-i.i.d. data.
- **Mechanism**: The mismatch between local and global statistical parameters (batch mean, batch variance) in BN causes the gradient deviation between local and global models. This gradient deviation slows down and biases FL convergence.
- **Core assumption**: The inconsistency between local and global statistical parameters and their gradients leads to gradient deviation.
- **Evidence anchors**:
  - [abstract]: "under the non-i.i.d. data, the mismatch between the local and global statistical parameters in BN causes the gradient deviation between the local and global models"
  - [section 3.1]: "under the non-i.i.d. data, the mismatch between the local and global statistical parameters in BN causes the gradient deviation between the local and global models"
- **Break condition**: If data becomes i.i.d., then the local and global statistical parameters are the same, eliminating gradient deviation.

### Mechanism 2
- **Claim**: Ensuring consistency in both statistical parameters and their gradients eliminates gradient deviation.
- **Mechanism**: FedTAN performs layer-wise aggregation of statistical parameters and their gradients to ensure consistency, thereby eliminating gradient deviation and achieving robust FL performance.
- **Core assumption**: Consistency in both statistical parameters and their gradients is necessary for FL convergence.
- **Evidence anchors**:
  - [abstract]: "it is insufficient to aggregate only part of the DNN model parameters at the server while keeping some BN parameters updated in clients"
  - [section 4.3]: "S r,1 Di = S r,1 D alone is insufficient and ∆S r,1 Di = ∆S r,1 D is necessary"
- **Break condition**: If layer-wise aggregation is not performed, gradient deviation persists.

### Mechanism 3
- **Claim**: Non-i.i.d. data causes different local and global statistical parameters and their gradients.
- **Mechanism**: Under non-i.i.d. data, the local dataset and global dataset cause different statistical parameters and their gradients, leading to gradient deviation.
- **Core assumption**: Non-i.i.d. data inherently causes statistical parameter mismatch.
- **Evidence anchors**:
  - [section 4.2]: "With Deﬁnition 1, we have the following property for the non-i.i.d. data case. Property 2 With the same initial gradient parameters ¯wr−1 of a DNN model, in the non-i.i.d. case, the statistical parameters and their gradients obtained by each client in FL and by centralized learning are generally different"
- **Break condition**: If data distribution becomes i.i.d., statistical parameters match.

## Foundational Learning

- **Concept**: Batch Normalization (BN)
  - **Why needed here**: BN is a crucial component in deep neural networks that accelerates training and improves generalization. Understanding its mechanism is essential to analyze why it damages FL under non-i.i.d. data.
  - **Quick check question**: What are the two key parameters in BN that need to be tracked and why?

- **Concept**: Federated Learning (FL)
  - **Why needed here**: FL is the distributed learning paradigm being analyzed. Understanding its mechanics, especially FedAvg, is crucial for understanding how BN affects its convergence.
  - **Quick check question**: What is the key assumption for FedAvg convergence that BN violates under non-i.i.d. data?

- **Concept**: Gradient Deviation
  - **Why needed here**: Gradient deviation is the core problem caused by BN under non-i.i.d. data. Understanding its definition and consequences is essential for grasping the paper's contribution.
  - **Quick check question**: How does gradient deviation affect the convergence of FL with BN under non-i.i.d. data?

## Architecture Onboarding

- **Component map**: ResNet-20 with BN layers -> FedTAN algorithm with layer-wise aggregation -> Statistical parameters (batch mean, batch variance) -> Gradients w.r.t. statistical parameters

- **Critical path**:
  1. Initialize global model
  2. Clients receive global model and update local models
  3. Layer-wise aggregation of statistical parameters and gradients
  4. Global model aggregation
  5. Repeat until convergence

- **Design tradeoffs**:
  - FedTAN requires extra communication for layer-wise aggregation but achieves better performance under non-i.i.d. data
  - FedAvg+BN is simpler but performs poorly under non-i.i.d. data
  - FedBN and SiloBN keep some BN parameters local but still underperform

- **Failure signatures**:
  - Slow convergence under non-i.i.d. data
  - Poor generalization to global test data
  - Large gap between training and testing accuracy

- **First 3 experiments**:
  1. Compare FedTAN with FedAvg+BN under i.i.d. and non-i.i.d. data distributions on CIFAR-10
  2. Measure communication overhead of FedTAN vs other FL schemes
  3. Test FedTAN's robustness to different levels of non-i.i.d. data heterogeneity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do BN mismatches specifically affect convergence speed versus convergence quality (e.g., final accuracy) in FL?
- **Basis in paper**: [explicit] Theorem 1 shows that gradient deviation from BN mismatches slows and biases convergence, but does not quantify separate effects on speed vs final accuracy.
- **Why unresolved**: The theoretical analysis only provides bounds on convergence rate, not distinct effects on speed vs quality.
- **What evidence would resolve it**: Empirical studies comparing convergence speed and final accuracy of FL with/without BN under various non-i.i.d. data distributions.

### Open Question 2
- **Question**: Are there alternative normalization techniques that can match BN performance in FL without the convergence issues?
- **Basis in paper**: [explicit] The paper notes that GN and LN have been proposed as alternatives to BN but have limitations like sensitivity to noise or assuming uniform neuron contributions.
- **Why unresolved**: The paper only briefly mentions limitations of GN and LN, without exploring other potential normalization techniques or their viability.
- **What evidence would resolve it**: Empirical comparisons of FL performance using various normalization techniques beyond BN, GN, and LN under different data distributions.

### Open Question 3
- **Question**: How does the number of communication rounds in FedTAN scale with the number of BN layers in the DNN?
- **Basis in paper**: [explicit] The paper notes that FedTAN requires 3L+1 communication rounds per iteration, where L is the number of BN layers, and mentions reducing rounds as a future direction.
- **Why unresolved**: The paper only provides a general formula for communication rounds, without specific analysis of how it scales with L.
- **What evidence would resolve it**: Empirical studies measuring communication rounds and training time in FedTAN as a function of the number of BN layers.

## Limitations
- Experiments are limited to ResNet-20 on CIFAR-10, raising questions about generalization to other tasks and model architectures
- Real-world federated learning often involves heterogeneous client hardware, communication constraints, and more complex data distributions not fully explored
- The theoretical analysis relies on assumptions about gradient deviation that may not hold for all DNN architectures

## Confidence
- **Theoretical framework**: High confidence in the theoretical framework for understanding BN-related gradient deviation in FL
- **FedTAN algorithm**: Medium confidence in FedTAN's practical effectiveness across different non-i.i.d. scenarios, given limited experimental scope
- **Generalizability**: Low confidence in generalizability to architectures beyond ResNet-20 and datasets beyond CIFAR-10

## Next Checks
1. Test FedTAN on diverse DNN architectures (e.g., transformers, LSTMs) and datasets (e.g., ImageNet, medical imaging) to assess generalizability
2. Evaluate FedTAN's performance under extreme non-i.i.d. scenarios (e.g., each client having data from only 1-2 classes)
3. Conduct ablation studies to isolate the impact of layer-wise aggregation of statistical parameters versus gradient aggregation on convergence and accuracy