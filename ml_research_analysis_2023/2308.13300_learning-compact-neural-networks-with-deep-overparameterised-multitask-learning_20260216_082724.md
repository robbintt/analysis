---
ver: rpa2
title: Learning Compact Neural Networks with Deep Overparameterised Multitask Learning
arxiv_id: '2308.13300'
source_url: https://arxiv.org/abs/2308.13300
tags:
- training
- tasks
- learning
- neural
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach for efficient multi-task learning
  (MTL) by overparameterizing the model architecture during training and sharing the
  overparameterized parameters more effectively across tasks. The method uses tensor
  decomposition to expand fully-connected and convolutional layers into multiple matrices,
  which are then shared across tasks, with task-specific diagonal matrices learned
  for each task.
---

# Learning Compact Neural Networks with Deep Overparameterised Multitask Learning

## Quick Facts
- arXiv ID: 2308.13300
- Source URL: https://arxiv.org/abs/2308.13300
- Reference count: 7
- Primary result: Outperforms state-of-the-art overparameterization methods on NYUv2 and COCO datasets for multi-task learning

## Executive Summary
This paper proposes a novel approach for efficient multi-task learning (MTL) by overparameterizing the model architecture during training and sharing the overparameterized parameters more effectively across tasks. The method uses tensor decomposition to expand fully-connected and convolutional layers into multiple matrices, which are then shared across tasks, with task-specific diagonal matrices learned for each task. An iterative training strategy is employed to separately train shared and task-specific parameters. Experiments on NYUv2 and COCO datasets demonstrate the effectiveness of the proposed method across various convolutional networks and parameter sizes, outperforming state-of-the-art overparameterization methods and achieving comparable or superior performance to baseline models.

## Method Summary
The proposed method involves overparameterizing neural network layers using tensor decomposition, sharing the overparameterised parameters across tasks, and learning task-specific diagonal matrices. The overparameterized layers are trained iteratively, with shared parameters and task-specific parameters updated separately. During inference, the overparameterised matrices are contracted back into the original layer size to maintain a compact model. The method is evaluated on NYUv2 and COCO datasets for various tasks, including semantic segmentation, depth estimation, surface normal estimation, and instance segmentation.

## Key Results
- Outperforms state-of-the-art overparameterization methods on NYUv2 and COCO datasets
- Achieves comparable or superior performance to baseline models, even when the backbone structure experiences changes due to overparameterization
- Demonstrates effectiveness across various convolutional networks and parameter sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Overparameterization via tensor decomposition improves generalization by creating multiple pathways for optimization
- Mechanism: The method replaces standard weight matrices with products of multiple matrices (U, M, V) following spatial SVD. During training, all tasks share U and V matrices while each task learns its own diagonal M matrix. This creates an overparameterized space where optimization can find better minima without increasing model expressiveness.
- Core assumption: The optimization landscape becomes smoother and more convex-like when parameters are decomposed into multiple matrices
- Evidence anchors:
  - [abstract] "overparameterizing the model architecture in training and sharing the overparameterised model parameters more effectively across tasks, for better optimisation and generalisation"
  - [section 2.1] "tensor decomposition is used for model expansion instead of model compression during training"
  - [corpus] Weak evidence - no direct citations found supporting this specific decomposition mechanism
- Break condition: If the optimization landscape becomes too flat or if the decomposition creates redundant pathways that don't contribute to learning

### Mechanism 2
- Claim: Task-specific diagonal matrices allow adaptive scaling of shared features for each task
- Mechanism: Each task learns its own diagonal matrix M that scales the shared U and V matrices differently. This allows the same shared feature space to be adapted to different task objectives through learned scaling factors.
- Core assumption: Different tasks benefit from different scalings of the same feature representations
- Evidence anchors:
  - [section 2.2] "M (a), M (b) are assigned as task-specific parameters for the corresponding tasks. The task-specific parameters M (a) and M (b) are learned as scaling factors in changing the scales of shared parameters U and V according to each individual task"
  - [corpus] No direct corpus evidence found for this specific scaling mechanism
- Break condition: If task objectives are too dissimilar that shared features become meaningless, or if the diagonal scaling becomes too extreme

### Mechanism 3
- Claim: Iterative training strategy prevents interference between shared and task-specific parameters
- Mechanism: The training alternates between (1) updating only task-specific M matrices using task-specific losses, then (2) updating shared parameters U, V and other layers using multi-task losses. This separation prevents gradient conflicts between different parameter types.
- Core assumption: Joint optimization of all parameters simultaneously would create interference that hurts performance
- Evidence anchors:
  - [section 2.3] "In order to train shared and task-specific parameters separately, we propose an iterative training strategy which consists of two training processes for each epoch"
  - [abstract] "We implement an iterative training strategy for the proposed design that is effective and efficient"
- Break condition: If the alternating schedule creates too much instability or if the warm-up phase for M matrices is insufficient

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: The method relies on decomposing weight matrices into U, M, V components following SVD structure
  - Quick check question: What are the dimensions of U, M, V when decomposing an m×n matrix using SVD with rank r?

- Concept: Tensor factorization for convolutional layers
  - Why needed here: Convolutional layers are factorized using spatial SVD to maintain spatial relationships while creating overparameterization
  - Quick check question: How does the tensor factorization change the dimensions of a convolutional layer with kernel size k×k, co output channels, and ci input channels?

- Concept: Hadamard product for diagonal matrices
  - Why needed here: Multiple task-specific diagonal matrices are combined using Hadamard product to create the final scaling matrix
  - Quick check question: Why is the Hadamard product equivalent to standard matrix multiplication for diagonal matrices?

## Architecture Onboarding

- Component map: Input -> Shared convolutional/fully-connected layers (overparameterized) -> Task-specific scaling (M matrices) -> Task-specific heads -> Output
- Critical path: Input -> Shared convolutional/fully-connected layers (overparameterized) -> Task-specific scaling (M matrices) -> Task-specific heads -> Output
- Design tradeoffs: 
  - More parameter sharing reduces memory but may hurt task-specific performance
  - Higher rank r increases overparameterization but also computational cost
  - Iterative training adds complexity but prevents interference
- Failure signatures:
  - Performance degrades when r is too small (insufficient overparameterization)
  - Tasks interfere when rank is too large (overfitting to shared features)
  - Training instability when alternating schedule is too aggressive
- First 3 experiments:
  1. Compare single-task performance with/without overparameterization on NYUv2 semantic segmentation
  2. Test different rank values (r) on multi-task performance to find sweet spot
  3. Evaluate iterative vs joint training strategies on convergence speed and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed overparameterization method perform compared to other state-of-the-art overparameterization methods for single-task learning?
- Basis in paper: [explicit] The paper compares the proposed method with RepVGG and ExpandNet for single-task semantic segmentation on NYUv2 dataset.
- Why unresolved: The paper only provides results for a single dataset and model (SegNet), and does not compare the proposed method with other overparameterization techniques for different tasks or models.
- What evidence would resolve it: Comparative experiments on different datasets, tasks, and models would help to evaluate the generalizability of the proposed method compared to other overparameterization techniques.

### Open Question 2
- Question: How does the proposed iterative training strategy affect the performance of the overparameterized model?
- Basis in paper: [explicit] The paper introduces an iterative training strategy for the proposed overparameterized MTL model and conducts ablation studies to compare the proposed method with a variant without iterative training.
- Why unresolved: The paper does not provide a detailed analysis of the impact of the iterative training strategy on the model's performance, such as convergence speed or sensitivity to hyperparameters.
- What evidence would resolve it: Additional experiments and analysis on the impact of the iterative training strategy on model performance, convergence speed, and hyperparameter sensitivity would help to better understand its effectiveness.

### Open Question 3
- Question: How does the proposed method handle resource competition among multiple tasks in MTL?
- Basis in paper: [explicit] The paper states that the proposed method shares overparameterized parameters among different tasks and learns task-specific diagonal matrices to better align with the singular vectors of the training data.
- Why unresolved: The paper does not provide a detailed analysis of how the proposed method addresses resource competition among tasks or how it compares to other MTL methods in terms of resource allocation.
- What evidence would resolve it: Further analysis and comparison of the proposed method with other MTL methods in terms of resource allocation and task performance would help to better understand its effectiveness in handling resource competition.

## Limitations
- Performance gains appear sensitive to hyperparameter choices, particularly the rank parameter r
- Limited systematic exploration of sensitivity to rank parameter
- Method assumes tasks share sufficient representational structure, which may not hold for highly divergent tasks

## Confidence
- High confidence: The mathematical formulation of the overparameterization mechanism and iterative training strategy
- Medium confidence: The empirical performance claims across datasets and comparison to baselines
- Low confidence: Generalization to tasks outside the tested domains and scalability to very large models

## Next Checks
1. Ablation on rank parameter r: Systematically test different rank values (e.g., r ∈ {1, 2, 4, 8, 16}) across multiple task combinations to establish sensitivity and identify optimal ranges.

2. Comparison with alternative decomposition methods: Benchmark against other tensor decomposition approaches (e.g., CP decomposition, Tucker decomposition) to validate the specific SVD-based choice.

3. Task dissimilarity stress test: Evaluate performance degradation when adding increasingly dissimilar tasks to the MTL setup to determine the method's limits for task diversity.