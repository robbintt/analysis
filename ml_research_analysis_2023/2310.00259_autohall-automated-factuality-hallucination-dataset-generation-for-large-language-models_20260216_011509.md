---
ver: rpa2
title: 'AutoHall: Automated Factuality Hallucination Dataset Generation for Large
  Language Models'
arxiv_id: '2310.00259'
source_url: https://arxiv.org/abs/2310.00259
tags:
- hallucination
- claim
- arxiv
- reference
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AutoHall, an automated approach for generating
  hallucination datasets for large language models (LLMs). The key idea is to leverage
  existing fact-checking datasets to automatically collect hallucinatory references
  by prompting LLMs to generate references for claims and using ground truth labels
  to assess the hallucinatory nature of the generated references.
---

# AutoHall: Automated Factuality Hallucination Dataset Generation for Large Language Models

## Quick Facts
- **arXiv ID:** 2310.00259
- **Source URL:** https://arxiv.org/abs/2310.00259
- **Reference count:** 30
- **Key outcome:** AutoHall is an automated approach for generating hallucination datasets for LLMs using fact-checking datasets and a zero-resource detection method based on self-contradiction, achieving 20-30% hallucination rates across models

## Executive Summary
This paper introduces AutoHall, a novel automated method for generating datasets of hallucinatory references in large language models. The key innovation is leveraging existing fact-checking datasets to automatically collect hallucinatory references by prompting LLMs to generate references for claims and using ground truth labels to assess the hallucinatory nature of the generated references. The authors also propose a zero-resource hallucination detection method based on self-contradiction, which checks for contradictions among multiple references generated by the LLM for the same claim. Experiments demonstrate that this approach can effectively detect hallucinations, with rates of 20-30% across different models and topics.

## Method Summary
AutoHall generates hallucination datasets by first collecting claims from fact-checking datasets (Climate-fever, Pubhealth, WICE) along with their ground truth labels. For each claim, an LLM is prompted to generate references, which are then used to classify the claim's truthfulness. References that lead to classification errors (i.e., the LLM's classification differs from the ground truth) are labeled as hallucinatory. For hallucination detection, the method generates multiple references for a claim and checks for self-contradictions among them. If contradictions exist, the original reference likely contains hallucination. The approach is evaluated on ChatGPT and Llama-2 models using accuracy and F1 score as metrics.

## Key Results
- AutoHall achieves superior hallucination detection performance compared to baselines like Chain-of-Thought checking and SelfCheckGPT
- LLMs generate hallucinations at a rate of 20-30% across different topics and models
- Self-contradiction detection method demonstrates effectiveness in identifying hallucinatory references
- Detection performance varies across different LLM architectures (ChatGPT vs. Llama-2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hallucinations can be automatically collected by generating references for claims and checking if generated references contradict ground truth labels.
- Mechanism: Use fact-checking datasets with claims and ground truth labels. Prompt LLM to generate references for each claim. Compare classification results (true/false) from LLM with ground truth labels. References that lead to classification errors are labeled as hallucinations.
- Core assumption: LLM classification of claim truthfulness based on generated reference is reliable enough to serve as a proxy for hallucination detection.
- Evidence anchors:
  - [abstract]: "we can determine whether hallucination has occurred by generating references to the claims and exploring whether the references can infer the correct labels for the claims."
  - [section]: "We expect correct classification to each claim, while wrong classification may be taken as a sign of the existence of hallucination in the generated reference"
- Break condition: If LLM classification accuracy is too low, then classification errors may not reliably indicate hallucinations.

### Mechanism 2
- Claim: Self-contradiction among multiple references generated for the same claim can detect hallucinations.
- Mechanism: Prompt LLM to generate K references for a claim. For each reference, check if it contradicts any other reference. If contradictions exist, the original reference likely contains hallucination.
- Core assumption: A well-informed LLM should generate consistent references for a claim, so contradictions indicate hallucination.
- Evidence anchors:
  - [abstract]: "Given an LLM accurately understands one claim, its randomly sampled references are less likely to contain contradictions."
  - [section]: "if the LLM knows one claim well, even when we query it to provide multiple references, self-contradictions among them should be absent otherwise hallucination information must exist in one reference"
- Break condition: If LLM generates inherently contradictory responses even for factual claims, then self-contradiction may not reliably indicate hallucination.

### Mechanism 3
- Claim: Hallucination detection based on self-contradiction outperforms baselines that don't use external databases.
- Mechanism: Compare detection accuracy of self-contradiction method against baselines like Chain-of-Thought checking and SelfCheckGPT. Show superior F1 and accuracy scores.
- Core assumption: Self-contradiction detection is more effective than alternative zero-resource methods for hallucination detection.
- Evidence anchors:
  - [abstract]: "achieving superior hallucination detection performance compared to extant baselines"
  - [section]: "our method has the highest F1 score and accuracy among the baselines"
- Break condition: If self-contradiction method fails to outperform baselines in rigorous testing, then claimed superiority may not hold.

## Foundational Learning

- Concept: Fact-checking datasets
  - Why needed here: Provide claims with ground truth labels needed to automatically identify hallucinations
  - Quick check question: What key information do fact-checking datasets contain that enables automated hallucination dataset generation?

- Concept: Self-consistency in LLM responses
  - Why needed here: Underlies the mechanism for detecting hallucinations through contradictions among multiple generated references
  - Quick check question: What does it indicate when an LLM generates contradictory references for the same claim?

- Concept: Temperature in LLM generation
  - Why needed here: Different temperature settings affect the diversity and potential contradictions in generated references
  - Quick check question: How does increasing temperature typically affect the potential for self-contradictions in generated references?

## Architecture Onboarding

- Component map: Fact-checking dataset -> Reference generation module -> Classification module -> Contradiction detection module
- Critical path: For dataset generation - claim → reference generation → classification → label as hallucination/factual. For detection - claim → multiple reference generation → pairwise contradiction checking → hallucination verdict.
- Design tradeoffs: Using LLM for both reference generation and classification reduces need for external resources but may inherit LLM biases. Using fact-checking datasets limits scope to verifiable claims.
- Failure signatures: High rate of invalid references indicates LLM lacks knowledge in certain domains. Poor detection accuracy suggests self-contradiction method isn't reliable for that model. Significant differences in hallucination rates across models suggests model-specific issues.
- First 3 experiments:
  1. Generate dataset with ChatGPT at different temperatures, measure hallucination rate and classification accuracy
  2. Compare self-contradiction detection accuracy against Chain-of-Thought and SelfCheckGPT baselines
  3. Analyze topic distribution of hallucinations to identify which domains LLMs struggle with most

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different temperature settings during reference generation impact the hallucination rate and types of hallucinations produced by LLMs?
- Basis in paper: [explicit] The paper mentions that different temperatures (0.1, 0.5, 0.9) are used to construct hallucination datasets for each LLM and observes that the hallucination rate remains at 20-30% across temperatures.
- Why unresolved: While the paper notes the stability of hallucination rates across temperatures, it does not delve into how temperature variations specifically influence the nature or types of hallucinations produced. Understanding this could provide insights into the role of randomness in generation and its effect on hallucination properties.
- What evidence would resolve it: A detailed analysis comparing the types and characteristics of hallucinations generated at each temperature setting, potentially using qualitative analysis or clustering techniques to categorize hallucination types.

### Open Question 2
- Question: What is the impact of model size and architecture on the propensity for hallucination in LLMs?
- Basis in paper: [inferred] The paper compares hallucination detection performance between ChatGPT and Llama-2 models (7B and 13B parameters) and notes differences in performance, suggesting potential differences in hallucination generation.
- Why unresolved: The paper hints at differences in hallucination detection performance between models of different sizes but does not explicitly explore how model architecture and size directly correlate with hallucination propensity or the ability to detect them.
- What evidence would resolve it: Comparative studies involving a wider range of models of varying sizes and architectures, analyzing hallucination rates and detection performance, possibly supplemented with ablation studies focusing on specific architectural components.

### Open Question 3
- Question: How do domain-specific factors influence the occurrence and detection of hallucinations in LLMs?
- Basis in paper: [explicit] The paper uses datasets from different domains (Climate-fever, Pubhealth, WICE) and notes variations in hallucination rates and types among different models, indicating potential domain-specific influences.
- Why unresolved: While the paper observes variations in hallucination across domains, it does not thoroughly investigate how specific domain characteristics (e.g., complexity, novelty, specificity of claims) affect hallucination occurrence and detection efficacy.
- What evidence would resolve it: A systematic study examining hallucination rates and detection performance across a broader range of domains, coupled with an analysis of domain-specific features that may contribute to hallucination susceptibility.

## Limitations

- The approach relies heavily on the LLM's ability to accurately classify claims based on generated references, which may not always hold true
- The self-contradiction method assumes that a well-informed LLM should generate consistent references, but this may not account for the inherent stochasticity in LLM outputs
- Claims about superiority over baselines are based on limited comparisons and need more extensive benchmarking

## Confidence

- **High confidence**: The dataset generation methodology is well-defined and reproducible using fact-checking datasets and LLM prompting. The core concept of using ground truth labels to identify hallucinations in generated references is sound.
- **Medium confidence**: The self-contradiction detection method shows promise in experiments, but its effectiveness may vary across different LLM architectures and domains. The assumption that contradictions indicate hallucinations needs further validation.
- **Low confidence**: Claims about the superiority of the proposed method over baselines like Chain-of-Thought and SelfCheckGPT are based on limited comparisons. More extensive benchmarking against diverse hallucination detection methods would strengthen these claims.

## Next Checks

1. **Classification Reliability Test**: Evaluate the accuracy of LLM-based classification on a held-out validation set of claims and references. Calculate precision and recall of correctly identifying factual vs. hallucinatory references to establish the reliability of using classification errors as hallucination indicators.

2. **Temperature Sensitivity Analysis**: Systematically vary the temperature parameter in reference generation and measure its impact on: a) the rate of self-contradictions, b) the accuracy of hallucination detection, and c) the overall quality of generated references. This would help determine optimal temperature settings for the approach.

3. **Cross-Model Generalization**: Test the self-contradiction detection method across multiple LLM architectures (beyond ChatGPT and Llama-2) including smaller models and different training paradigms. Compare detection accuracy and hallucination rates to assess whether the method generalizes or is model-specific.