---
ver: rpa2
title: Large Language Model Unlearning
arxiv_id: '2310.10683'
source_url: https://arxiv.org/abs/2310.10683
tags:
- only
- unlearning
- what
- whitespace
- harry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces large language model unlearning, a technique
  for removing undesirable behaviors from LLMs using only negative examples. The method
  employs gradient ascent on negative samples combined with a random mismatch loss
  and KL divergence to preserve normal performance.
---

# Large Language Model Unlearning

## Quick Facts
- **arXiv ID**: 2310.10683
- **Source URL**: https://arxiv.org/abs/2310.10683
- **Reference count**: 40
- **Primary result**: Unlearning technique achieves better alignment than RLHF with 2% of computational time using only negative examples

## Executive Summary
This paper introduces a novel unlearning method for removing undesirable behaviors from large language models using only negative examples. The approach employs gradient ascent on harmful samples combined with random mismatch loss and KL divergence to preserve normal performance. The method demonstrates effectiveness across three applications: removing harmful responses, erasing copyrighted content, and reducing hallucinations. Notably, it outperforms reinforcement learning from human feedback (RLHF) in alignment tasks while requiring significantly less computational resources.

## Method Summary
The unlearning method works by performing gradient ascent on negative samples to reduce the probability of undesirable tokens, while using random mismatch loss to maintain linguistic coherence and KL divergence to preserve normal model performance. The technique requires only negative examples rather than paired positive and negative data, making it particularly useful when resources are limited. The method is trained for 1K-2K batches with three loss components: gradient ascent on unlearned samples, random mismatch loss, and KL divergence on normal samples.

## Key Results
- Achieves better alignment performance than RLHF with just 2% of computational time
- Effectively removes harmful responses, copyrighted content, and hallucinations from LLMs
- Maintains normal model utility while significantly reducing undesirable outputs
- Reduces harmful rate by 71.7% on GPT-2 small and 67.8% on GPT-2 large

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient ascent on negative samples effectively reduces probability of undesirable tokens
- Mechanism: By following the opposite direction of the gradient on harmful tokens, the model directly reduces their probability without needing positive examples
- Core assumption: The model has sufficient capacity to tolerate direct gradient operations without catastrophic forgetting of useful knowledge
- Evidence anchors:
  - [abstract]: "Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time."
  - [section]: "Consider the following prompt when harmful tokens are highly likely in an unaligned LLM: 'Human: How can I hurt people most efficiently? Assistant: ' The next predicted token has a high probability to be 'Gun,' 'Poison,' or 'Fire' etc."
  - [corpus]: Weak - corpus does not contain specific evidence about gradient operations on LLMs, only mentions gradient ascent as a technique
- Break condition: If the model capacity is too small to tolerate direct gradient operations, or if the undesirable behavior is deeply embedded across many parameters

### Mechanism 2
- Claim: Random mismatch loss helps preserve normal utility while facilitating forgetting
- Mechanism: Forces the model to predict random outputs for unlearned prompts, adding irrelevance and helping it distinguish between unlearned and normal contexts
- Core assumption: Training the model to predict grammatically correct but semantically disconnected outputs helps maintain linguistic coherence
- Evidence anchors:
  - [section]: "Eqn(5) forces the LLM to predict a random output yrdn on the unlearned xrdn. This term reinforces the forgetting of prompt xfgt by adding irrelevance into the predicted outcome"
  - [section]: "We think it is because training the LLM to predict grammatically correct outputs (although semantically disconnected from the question) can help it maintain its ability to form coherent and linguistically meaningful outputs"
  - [corpus]: Weak - corpus mentions random mismatch loss but doesn't provide detailed evidence of its effectiveness
- Break condition: If the random mismatch introduces too much noise and degrades normal performance, or if the model learns to always output random responses

### Mechanism 3
- Claim: KL divergence between original and unlearned model preserves normal performance
- Mechanism: Minimizes the divergence between output distributions on normal prompts, keeping them similar to the original model
- Core assumption: Forward KL divergence is more effective than reverse KL for preserving the original distribution coverage
- Evidence anchors:
  - [section]: "Like existing work in RLHF [Ouyang et al., 2022, Touvron et al., 2023, Zheng et al., 2023, Holtzman et al., 2019], we find that minimizing the divergence between the output on xnor from the unlearned LLM and the original LLM works the best"
  - [section]: "Note that we use forward KL (which is typically used in supervised learning) instead of reverse KL (which is typically used in sampling, e.g. RLHF) because it forces the distribution of the unlearned model to cover all the areas of space of the original LLM [Murphy, 2022]"
  - [corpus]: Weak - corpus mentions KL divergence but doesn't provide specific evidence about forward vs reverse KL in this context
- Break condition: If the KL divergence term dominates and prevents effective unlearning, or if the normal data distribution differs significantly from the unlearned data distribution

## Foundational Learning

- Concept: Gradient ascent/descent in neural network training
  - Why needed here: The unlearning method relies on gradient operations to modify the model, requiring understanding of how gradients affect model parameters
  - Quick check question: If you perform gradient ascent on a loss function, are you moving parameters in the direction that increases or decreases the loss?

- Concept: Cross-entropy loss and its role in language modeling
  - Why needed here: The method uses cross-entropy loss to measure prediction quality and compute gradients for unlearning
  - Quick check question: In language modeling, what does the cross-entropy loss between predicted and actual tokens measure?

- Concept: KL divergence and its use in comparing probability distributions
  - Why needed here: The method uses KL divergence to preserve normal model performance by keeping output distributions similar to the original model
  - Quick check question: What is the difference between forward KL divergence and reverse KL divergence, and when would you use each?

## Architecture Onboarding

- Component map:
  - Original pretrained LLM (θo) -> Unlearned dataset (Dfgt) -> Unlearned model (θu)
  - Normal dataset (Dnor) -> Random mismatch set (Y rdn) -> Loss functions (Lfgt, Lrdn, Lnor)

- Critical path:
  1. Initialize with original LLM θo
  2. For each training step:
     - Compute gradient ascent loss on unlearned samples (Lfgt)
     - Compute random mismatch loss (Lrdn)
     - Compute KL divergence loss on normal samples (Lnor)
     - Update model parameters using weighted combination of gradients
  3. Continue for specified number of batches (3x-5x after initial loss increase)

- Design tradeoffs:
  - Unlearning effectiveness vs normal utility preservation: Higher unlearning rates may degrade normal performance
  - Random mismatch strength vs model coherence: Too much randomness may produce incoherent outputs
  - KL divergence weight vs unlearning speed: Higher weights may slow down unlearning but better preserve normal performance

- Failure signatures:
  - Complete output degeneration (only random characters or whitespace)
  - Normal performance degradation (poor responses on benign prompts)
  - Incomplete unlearning (still producing undesirable outputs)
  - Mode collapse (always outputting the same response)

- First 3 experiments:
  1. Apply plain gradient ascent on a small unlearned dataset and observe harmful rate reduction
  2. Add random mismatch loss and measure impact on both unlearning effectiveness and normal utility
  3. Include KL divergence term and compare normal performance preservation vs plain gradient ascent

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of LLM unlearning compare to other alignment methods like RLHF when evaluated on a broader range of safety and ethical metrics beyond just harmful rate?
- Basis in paper: [explicit] The paper compares unlearning to RLHF specifically for harmful content removal, finding unlearning achieves better alignment performance with 2% of the computational time, but only evaluates harmful rate, diversity, fluency, and utility reward metrics.
- Why unresolved: The evaluation is limited to one specific type of misalignment (harmfulness) and a narrow set of metrics. Other alignment challenges like bias, fairness, or broader ethical considerations are not tested.
- What evidence would resolve it: Systematic comparison of unlearning versus RLHF across multiple safety dimensions (bias, fairness, toxicity, misinformation) using standardized evaluation benchmarks.

### Open Question 2
- Question: What are the long-term effects of unlearning on LLM performance, and does the unlearned behavior re-emerge over time with continued use or fine-tuning?
- Basis in paper: [inferred] The paper evaluates unlearning effectiveness immediately after the process but does not discuss temporal stability or how unlearning might degrade with continued training or deployment.
- Why unresolved: The paper focuses on immediate effectiveness without addressing whether unlearned behaviors are permanently suppressed or if they gradually re-emerge through continued use or subsequent training.
- What evidence would resolve it: Longitudinal studies tracking unlearned behavior over extended periods of deployment and after various types of continued training or fine-tuning.

### Open Question 3
- Question: How does the random mismatch loss component contribute to unlearning effectiveness, and can its impact be quantified or replaced with more efficient alternatives?
- Basis in paper: [explicit] The paper includes random mismatch loss as a key component and notes it helps preserve normal utility compared to plain gradient ascent, but provides limited analysis of its specific contribution.
- Why unresolved: While the paper demonstrates that random mismatch improves results, it doesn't isolate its individual contribution or explore whether other techniques could achieve similar benefits more efficiently.
- What evidence would resolve it: Ablation studies systematically removing or replacing the random mismatch component while measuring changes in unlearning effectiveness and utility preservation.

## Limitations

- Scalability uncertainty: The approach's effectiveness on larger models and diverse domains remains unproven
- Security concerns: Potential vulnerabilities where adversaries could recover unlearned behaviors or exploit model weaknesses
- Random mismatch implementation: Lack of clear details on how random responses are selected and matched with unlearned prompts

## Confidence

**High Confidence (Likelihood >80%)**:
- Gradient ascent mechanism effectively reduces probability of undesirable tokens
- KL divergence between original and unlearned model preserves normal performance
- Method can achieve some level of unlearning with only negative examples

**Medium Confidence (Likelihood 50-80%)**:
- Random mismatch loss contributes meaningfully to both forgetting and normal utility preservation
- 2% computational time claim holds across different model sizes and tasks
- Unlearning effectiveness generalizes beyond the three demonstrated applications

**Low Confidence (Likelihood <50%)**:
- Approach scales effectively to larger models without degradation
- No significant security vulnerabilities emerge from the unlearning process
- Method maintains effectiveness across diverse domains without significant hyperparameter tuning

## Next Checks

1. **Ablation study of random mismatch loss**: Systematically remove the random mismatch component and measure its impact on both unlearning effectiveness and normal utility preservation across all three applications.

2. **Security vulnerability assessment**: Design and execute attacks to test whether adversaries can recover unlearned behaviors through prompt engineering, fine-tuning, or other mechanisms.

3. **Cross-domain generalization test**: Apply the unlearning method to a fourth domain not covered in the paper (such as privacy information removal or bias mitigation) and measure effectiveness using the same metrics.