---
ver: rpa2
title: Experiential Co-Learning of Software-Developing Agents
arxiv_id: '2312.17025'
source_url: https://arxiv.org/abs/2312.17025
tags:
- agents
- software
- task
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of autonomous LLM agents making
  repeated mistakes and inefficient attempts when solving multi-step tasks, particularly
  in software development, due to their inability to learn from past experiences.
  The proposed Experiential Co-Learning framework enables instructor and assistant
  agents to collaboratively gather and utilize shortcut-oriented experiences from
  historical task trajectories.
---

# Experiential Co-Learning of Software-Developing Agents

## Quick Facts
- arXiv ID: 2312.17025
- Source URL: https://arxiv.org/abs/2312.17025
- Reference count: 14
- Primary result: Improves LLM agent autonomy from 0.3340 to 0.7100 and completeness from 0.6131 to 0.9497 in software development tasks

## Executive Summary
This paper addresses the problem of autonomous LLM agents making repeated mistakes and inefficient attempts when solving multi-step software development tasks. The proposed Experiential Co-Learning framework enables instructor and assistant agents to collaboratively gather and utilize shortcut-oriented experiences from historical task trajectories. By extracting and reusing valuable patterns from past experiences, the framework significantly improves task-solving autonomy, completeness, and executability while reducing the number of agent iterations needed.

## Method Summary
The Experiential Co-Learning framework consists of three key modules: co-tracking creates procedural trajectories by recording agent interactions, co-memorizing extracts "shortcuts" from historical trajectories using external feedback signals and stores them in experience pools, and co-reasoning applies these experiences to guide current task execution through semantic similarity retrieval. The system uses a two-agent collaborative structure where instructor and assistant agents maintain complementary memory pools and iteratively improve their performance through experience-augmented reasoning.

## Key Results
- Autonomy improved from 0.3340 to 0.7100 compared to baseline ChatDev
- Completeness increased from 0.6131 to 0.9497
- Executability improved from 0.6890 to 0.9400
- Reduced number of agent iterations needed for task completion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Experiential Co-Learning enables agents to avoid repetitive mistakes by extracting and reusing "shortcuts" from past task trajectories.
- Mechanism: The framework converts task execution chains into graphs, identifies non-adjacent nodes that represent faster paths to solutions, and stores these as experience pairs in memory pools.
- Core assumption: The quality of past experiences can be reliably assessed using external feedback (compilation success, similarity to task description).
- Evidence anchors:
  - [abstract] "The proposed Experiential Co-Learning framework enables instructor and assistant agents to collaboratively gather and utilize shortcut-oriented experiences from historical task trajectories."
  - [section 3.2] "The co-memorizing module heuristically mines 'shortcuts' from historical trajectories using external environment feedback, which are then preserved in their experience pools in an interleaved manner."
  - [corpus] Weak evidence - no direct corpus support for the shortcut extraction mechanism.
- Break condition: If external feedback signals become unreliable or inconsistent across different types of tasks, the quality assessment of experiences would fail.

### Mechanism 2
- Claim: The co-reasoning module improves task-solving efficiency by retrieving relevant past experiences to guide current instructions and responses.
- Mechanism: At each interaction step, the instructor retrieves similar past responses to inform its current instruction, while the assistant retrieves similar past instructions to inform its current response, creating a feedback loop of experience-augmented reasoning.
- Core assumption: Past experiences are semantically similar enough to be useful for unseen tasks.
- Evidence anchors:
  - [abstract] "The framework includes three key modules: co-tracking for creating procedural trajectories, co-memorizing for extracting and storing shortcuts from past experiences, and co-reasoning for applying these experiences to unseen tasks."
  - [section 3.3] "The co-reasoning module combines the collective experience pools of agents, enabling advanced interaction through instructions and responses."
  - [corpus] Weak evidence - no direct corpus support for the co-reasoning mechanism's effectiveness.
- Break condition: If the semantic similarity retrieval fails to find relevant experiences, the reasoning loop would break down and provide no benefit.

### Mechanism 3
- Claim: The collaborative structure between instructor and assistant agents creates emergent learning that exceeds what either agent could achieve independently.
- Mechanism: The two-agent system creates a feedback loop where the instructor's instructions are informed by past responses, and the assistant's responses are informed by past instructions, enabling mutual improvement.
- Core assumption: The division of roles (instructor vs assistant) creates complementary capabilities that enhance learning.
- Evidence anchors:
  - [abstract] "The proposed Experiential Co-Learning framework enables instructor and assistant agents to collaboratively gather and utilize shortcut-oriented experiences from historical task trajectories."
  - [section 3.3] "This iterative process empowers the language agents to collectively enhance their task-solving capabilities by building upon prior states and accumulated experiences."
  - [corpus] Weak evidence - no direct corpus support for the two-agent collaborative learning benefits.
- Break condition: If the role division becomes unclear or the agents' capabilities are too similar, the collaborative advantage would diminish.

## Foundational Learning

- Concept: Procedural trajectory construction and analysis
  - Why needed here: The framework relies on tracking the sequence of interactions between agents to identify patterns and shortcuts that can be reused.
  - Quick check question: How would you represent the sequence of agent interactions as a graph structure that can be analyzed for shortcuts?

- Concept: Semantic similarity and embedding-based retrieval
  - Why needed here: The co-reasoning module depends on retrieving relevant past experiences based on semantic similarity between current states and stored experiences.
  - Quick check question: What embedding model would you choose for comparing code fragments and natural language instructions?

- Concept: External feedback integration for quality assessment
  - Why needed here: The framework uses compilation success and similarity metrics to evaluate the quality of experiences and determine which shortcuts are valuable.
  - Quick check question: How would you design a heuristic reward function that balances task relevance, solution quality, and compilation success?

## Architecture Onboarding

- Component map: co-tracking -> graph construction -> co-memorizing -> shortcut extraction -> experience pool storage -> co-reasoning -> semantic retrieval -> guided task execution
- Critical path: Task execution → Graph construction → Shortcut extraction → Memory storage → Experience retrieval → Guided task execution
- Design tradeoffs: The framework trades increased computational overhead (graph analysis, retrieval operations) for improved task completion rates and reduced iterations.
- Failure signatures: Poor performance could stem from inadequate external feedback, semantic similarity retrieval failures, or insufficient diversity in training tasks.
- First 3 experiments:
  1. Implement co-tracking with a simple two-agent interaction on a basic coding task and visualize the resulting execution graph.
  2. Add the co-memorizing module and test whether extracted shortcuts improve performance on a similar but unseen task.
  3. Integrate the co-reasoning module and measure improvements in autonomy metrics compared to the baseline ChatDev approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Experiential Co-Learning framework be extended to more than two agents?
- Basis in paper: [inferred] The paper focuses on a two-agent system (instructor and assistant) but discusses autonomous agents in general and mentions "multi-agent" frameworks.
- Why unresolved: The paper only demonstrates the effectiveness of the framework with two agents, but many real-world applications may involve more complex multi-agent interactions.
- What evidence would resolve it: Experiments showing the framework's effectiveness with 3+ agents in software development tasks, comparing performance metrics like autonomy and efficiency.

### Open Question 2
- Question: What is the optimal threshold (ϵ) for identifying valuable shortcuts in the co-memorizing module?
- Basis in paper: [explicit] The paper mentions using an information gain threshold of 0.90 to filter experiences but doesn't explore how different threshold values affect performance.
- Why unresolved: The threshold value likely impacts the quality and quantity of shortcuts discovered, affecting overall performance, but optimal values may vary depending on task complexity and domain.
- What evidence would resolve it: A systematic study varying the threshold value and measuring its impact on key performance metrics (autonomy, efficiency) across different types of software development tasks.

### Open Question 3
- Question: How does the framework perform on non-software development tasks?
- Basis in paper: [explicit] The paper focuses specifically on software development tasks and datasets.
- Why unresolved: While the framework is designed for autonomous agents in general, its effectiveness on other complex, multi-step tasks (e.g., mathematical reasoning, game playing, scientific research) remains unknown.
- What evidence would resolve it: Experiments applying the framework to at least two other domains mentioned in the related work (e.g., mathematical reasoning, game playing) and comparing performance to domain-specific baselines.

## Limitations

- Narrow experimental scope limited to a single curated dataset of 1,200 software tasks
- Reliance on compilation-based feedback that may not generalize to non-compilable software development activities
- Computational overhead from graph analysis and semantic similarity retrieval not quantified

## Confidence

**High Confidence**: The basic architecture of the three-module framework is well-defined and logically sound; quantitative performance improvements over baseline systems are clearly demonstrated; task execution graph representation and shortcut identification methodology is rigorously specified.

**Medium Confidence**: The semantic similarity-based experience retrieval mechanism appears sound but lacks detailed validation of retrieval accuracy; collaborative learning benefits between instructor and assistant agents are claimed but not extensively validated through ablation studies; scalability to more complex tasks remains unproven.

**Low Confidence**: Generalizability to non-compilable software development tasks is not demonstrated; long-term learning capabilities and potential catastrophic forgetting issues are not addressed; computational efficiency claims lack quantitative validation.

## Next Checks

1. **Cross-Domain Generalization Test**: Apply the trained Experiential Co-Learning model to a completely different software development dataset (e.g., GitHub issues or Stack Overflow code snippets) to evaluate whether the extracted shortcuts and learned behaviors transfer effectively outside the NLDD domain. Measure performance degradation and identify which types of shortcuts are most/least transferable.

2. **Ablation Study on Feedback Mechanisms**: Systematically replace the compilation-based feedback with alternative quality assessment methods (e.g., static code analysis, test coverage, or peer review simulation) to determine the sensitivity of the framework to different external feedback signals and identify the minimum viable feedback requirements.

3. **Computational Overhead Benchmarking**: Implement comprehensive profiling of the framework's runtime performance, including graph construction time, shortcut extraction computational cost, and semantic similarity retrieval latency. Compare these overheads against the actual time savings from reduced agent iterations to calculate the net efficiency gain across different task complexities.