---
ver: rpa2
title: An In-depth Look at Gemini's Language Abilities
arxiv_id: '2312.11444'
source_url: https://arxiv.org/abs/2312.11444
tags:
- gemini
- turbo
- tasks
- accuracy
- gpt-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides an in-depth third-party evaluation of Google's
  Gemini Pro model, comparing it to OpenAI's GPT-3.5 Turbo and GPT-4 Turbo models
  across 10 diverse language tasks including knowledge-based question answering, reasoning,
  mathematics, code generation, machine translation, and web agents. The authors find
  that Gemini Pro achieves accuracy slightly inferior to GPT-3.5 Turbo on average
  across all tasks, with specific weaknesses in mathematical reasoning with many digits,
  sensitivity to multiple-choice answer ordering, and aggressive content filtering.
---

# An In-depth Look at Gemini's Language Abilities

## Quick Facts
- arXiv ID: 2312.11444
- Source URL: https://arxiv.org/abs/2312.11444
- Authors: 
- Reference count: 17
- Key outcome: Gemini Pro achieves accuracy slightly inferior to GPT-3.5 Turbo on average across 10 diverse language tasks, with specific weaknesses in mathematical reasoning, content filtering, and multiple-choice answer ordering.

## Executive Summary
This paper provides a third-party evaluation of Google's Gemini Pro model against OpenAI's GPT-3.5 Turbo and GPT-4 Turbo across 10 diverse language tasks. The evaluation finds that Gemini Pro performs slightly worse than GPT-3.5 Turbo on average, with notable weaknesses in mathematical reasoning involving many digits, sensitivity to multiple-choice answer ordering, and aggressive content filtering that blocks responses on sensitive topics. The paper offers detailed analysis of these performance differences and provides reproducible code and data for verification.

## Method Summary
The evaluation compares Gemini Pro, GPT-3.5 Turbo, and GPT-4 Turbo across 10 datasets covering knowledge-based QA, reasoning, mathematics, code generation, machine translation, and web agents. Standardized prompts and evaluation protocols are used across all models, with results measured using accuracy metrics. The evaluation framework includes a model querying interface (LiteLLM), prompt management system, and evaluation harness that processes outputs against ground truth answers. Code and data are available at https://github.com/neulab/gemini-benchmark for reproducibility.

## Key Results
- Gemini Pro underperforms GPT-3.5 Turbo on average across all tasks
- Significant content filtering blocks responses on sensitive topics (28% for human_sexuality, 85% for moral_scenarios)
- Gemini Pro shows bias toward selecting the final multiple-choice option "D" unlike GPT models which maintain balanced distributions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Gemini Pro underperforms on knowledge-based QA tasks due to aggressive content filtering and multiple-choice answer ordering bias.
- **Mechanism**: Content filtering blocks responses on sensitive topics, while the model shows bias toward selecting the final option "D" in multiple-choice questions.
- **Core assumption**: Performance is directly impacted by blocked responses and inherent answer selection biases.
- **Evidence anchors**:
  - [abstract] "failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering"
  - [section] "two had notably low response rates: moral_scenarios at 85% and human_sexuality at 28%"
  - [section] "Gemini has a very skewed label distribution, biased towards selecting the final choice of 'D'"

### Mechanism 2
- **Claim**: Gemini Pro's performance degrades more on longer and more complex questions compared to GPT models.
- **Mechanism**: The model struggles with maintaining state across longer reasoning chains, leading to decreased accuracy on complex tasks.
- **Core assumption**: Ability to handle longer reasoning chains is key to performance on general-purpose reasoning tasks.
- **Evidence anchors**:
  - [abstract] "handling longer and more complex reasoning chains" as an area where Gemini demonstrates high performance, but results show the opposite
  - [section] "Gemini Pro underperformed on longer, more complex questions while the GPT models were more robust to this"
  - [section] "Gemini Pro is particularly bad at the 'tracking_shuffled_objects' tasks"

### Mechanism 3
- **Claim**: Gemini Pro's code generation capabilities are inferior to GPT-3.5 Turbo due to difficulties in API usage and implementing complex intents.
- **Mechanism**: The model makes mistakes in API usage and struggles with implementing code that correctly matches complex problem intents.
- **Core assumption**: Correct API usage and complex code logic implementation are crucial for code generation performance.
- **Evidence anchors**:
  - [abstract] "under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others"
  - [section] "Gemini Pro had a higher proportion of mistakes where the implemented code was syntactically correct but did not correctly match with a more complex intent"
  - [section] "Gemini Pro tends to make mistakes in API usage, such as type mismatches"

## Foundational Learning

- **Concept**: Content filtering in language models
  - Why needed here: Understanding how content filtering impacts model performance, particularly on sensitive topics.
  - Quick check question: What percentage of responses were blocked by Gemini Pro on the human_sexuality task?
    - Answer: 28%

- **Concept**: Multiple-choice answer ordering bias
  - Why needed here: Recognizing how models can develop biases towards certain answer options based on their training data.
  - Quick check question: Which multiple-choice option did Gemini Pro show a bias towards selecting?
    - Answer: Option "D"

- **Concept**: Chain-of-thought prompting
  - Why needed here: Understanding how different prompting strategies can impact model performance on reasoning tasks.
  - Quick check question: Did chain-of-thought prompting significantly improve Gemini Pro's performance on MMLU tasks?
    - Answer: No, there was little difference in performance using chain-of-thought prompting.

## Architecture Onboarding

- **Component map**: Model querying interface (LiteLLM) -> Prompt management system -> Evaluation harness -> Ground truth comparison
- **Critical path**: Generate prompts -> Query models -> Process outputs -> Evaluate results against ground truth
- **Design tradeoffs**: Standardized prompts ensure fairness but may not capture each model's full potential; reproducibility prioritized over optimization
- **Failure signatures**: Model response blocking due to content filtering, incorrect answer selection in multiple-choice tasks, inability to handle complex reasoning chains or API usage in code generation
- **First 3 experiments**:
  1. Test content filtering rates across all models on sensitive topics to quantify impact on performance
  2. Evaluate models on MMLU tasks with varying question complexity to assess robustness to longer reasoning chains
  3. Assess models' ability to correctly use APIs and implement complex code logic on diverse code generation tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes Gemini Pro's sensitivity to multiple-choice answer ordering, particularly its bias toward selecting the final choice "D"?
- Basis in paper: The paper states Gemini has a skewed label distribution biased toward "D", contrasting with GPT models' balanced distribution, possibly indicating insufficient instruction-tuning for multiple-choice questions.
- Why unresolved: The paper mentions this bias but doesn't analyze its underlying causes or potential solutions.
- What evidence would resolve it: Investigation into Gemini Pro's training data and fine-tuning process to understand the bias origin, and testing whether prompt or parameter adjustments can mitigate it.

### Open Question 2
- Question: What causes Gemini Pro's aggressive content filtering that blocks significant responses on certain tasks?
- Basis in paper: The paper notes that Gemini Pro blocks questions on potentially illegal or sensitive material, treating blocked responses as incorrect.
- Why unresolved: While mentioned as a limitation, the paper doesn't explore reasons for aggressive filtering or ways to reduce it.
- What evidence would resolve it: Analysis of Gemini Pro's content filtering policies and thresholds, and testing whether prompt or model setting adjustments can reduce blocked responses without compromising safety.

### Open Question 3
- Question: How does Gemini Ultra compare to GPT-4 in language abilities, given that only Gemini Pro was tested?
- Basis in paper: The paper states Gemini Ultra is not yet publicly available and thus not tested.
- Why unresolved: The paper only evaluates Gemini Pro without insights into the larger Gemini Ultra model's performance.
- What evidence would resolve it: Testing and benchmarking Gemini Ultra against GPT-4 on the same tasks to compare language abilities.

### Open Question 4
- Question: How does performance change with different prompts, hyperparameters, or sampling strategies like self-consistency?
- Basis in paper: The paper mentions that with further prompt engineering or multiple samples and self-consistency, results could change significantly.
- Why unresolved: The paper uses standardized prompts and greedy decoding without exploring how different strategies might affect performance.
- What evidence would resolve it: Experiments with various prompts, hyperparameters, and sampling strategies to identify most effective settings for each model and task.

### Open Question 5
- Question: To what extent does data leakage from training data affect results, and how can this be mitigated?
- Basis in paper: The paper acknowledges data leakage plagues current LLM evaluation but doesn't explicitly measure its impact.
- Why unresolved: While attempting to mitigate data leakage through diverse tasks, the paper doesn't explicitly address its potential impact on results.
- What evidence would resolve it: Analysis of training data for overlaps with test sets and development of strategies to minimize data leakage impact on benchmark results.

## Limitations

- Content filtering impact: Gemini Pro's aggressive content filtering significantly impacts performance on sensitive topics, potentially confounding capability comparisons
- Prompt sensitivity: Results may vary with different prompting strategies, though standardized prompts were used for fairness
- Model transparency: Exact specifications of Gemini Pro are not publicly disclosed, limiting understanding of architectural factors

## Confidence

**High Confidence**: Gemini Pro underperforms GPT-3.5 Turbo on average across all tasks, with specific failure modes clearly demonstrated quantitatively.

**Medium Confidence**: The claim that Gemini Pro performs better on longer, more complex reasoning tasks is contradicted by results showing underperformance on BIG-Bench Hard tasks.

**Low Confidence**: The conclusion about inferior code generation capabilities is based on limited task diversity and may not represent the full range of code generation challenges.

## Next Checks

1. **Content Filtering Analysis**: Conduct systematic study of content filtering rates across all models on sensitive topics to quantify extent to which filtering policies versus capabilities drive performance differences.

2. **Prompt Sensitivity Study**: Evaluate all models using optimized prompts tailored to each model's strengths to establish upper bounds on performance and understand true capability gaps.

3. **Extended Code Generation Evaluation**: Expand code generation evaluation to include more diverse programming tasks involving complex API usage, error handling, and real-world software engineering scenarios.