---
ver: rpa2
title: 'SplitOut: Out-of-the-Box Training-Hijacking Detection in Split Learning via
  Outlier Detection'
arxiv_id: '2302.08618'
source_url: https://arxiv.org/abs/2302.08618
tags:
- server
- data
- training
- clients
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SplitOut, a training-hijacking detection method
  for SplitNN that leverages out-of-the-box outlier detection. The core idea is that
  gradients received during a training-hijacking attack will appear as outliers compared
  to gradients from honest training, enabling reliable detection via density-based
  outlier detection (LOF).
---

# SplitOut: Out-of-the-Box Training-Hijacking Detection in Split Learning via Outlier Detection

## Quick Facts
- arXiv ID: 2302.08618
- Source URL: https://arxiv.org/abs/2302.08618
- Reference count: 27
- This paper presents SplitOut, a training-hijacking detection method for SplitNN that leverages out-of-the-box outlier detection

## Executive Summary
This paper addresses the critical security challenge of training-hijacking attacks in Split Learning, where malicious servers can manipulate client-side gradients to compromise privacy and model integrity. The authors propose SplitOut, a detection method that uses Local Outlier Factor (LOF) to identify anomalous gradients during training. By detecting these outliers, SplitOut can effectively identify when a server is conducting a training-hijacking attack with near-zero false positive rates across multiple benchmark datasets.

## Method Summary
SplitOut leverages density-based outlier detection (specifically Local Outlier Factor) to identify training-hijacking attacks in SplitNN. The method assumes that gradients received during malicious training will appear as outliers compared to honest gradients. During initialization, the client trains locally on a small subset of data to establish baseline gradient patterns. During regular training, incoming gradients are evaluated using LOF, and if a majority of recent gradients are classified as outliers, an attack is detected. The approach requires minimal hyperparameter tuning and modest client compute capabilities while maintaining privacy by not requiring label sharing with the server.

## Key Results
- Achieves near-zero false positive rates across MNIST, Fashion-MNIST, CIFAR-10/100 datasets
- Detects training-hijacking attacks early in training with minimal computational overhead
- Outperforms earlier heuristic-based detection methods in both accuracy and simplicity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradients during training-hijacking attacks appear as outliers compared to honest gradients
- Mechanism: The server manipulates gradients to optimize for an adversarial task (e.g., making outputs easily invertible), causing the gradient distribution to shift from the expected distribution of honest training
- Core assumption: The malicious gradients are sufficiently different from honest gradients that they can be detected by outlier detection methods
- Break condition: If the malicious gradients happen to align with the distribution of honest gradients (e.g., through sophisticated optimization techniques), the outlier detection will fail

### Mechanism 2
- Claim: During honest training, gradients from batches with randomized labels are distinguishable from gradients from regular batches
- Mechanism: When the model is learning the intended task, parameter updates for batches with randomized labels cause larger deviations from optimal parameters compared to regular batches, creating a detectable pattern
- Core assumption: The angle and magnitude differences between fake and regular gradients are statistically significant when the model is learning the intended task
- Break condition: If the server uses multitasking (combining both the original task and the attack), the gradient patterns may become indistinguishable

### Mechanism 3
- Claim: Local Outlier Factor (LOF) can reliably distinguish malicious gradients from honest gradients
- Mechanism: LOF computes local density-based scores where outliers have scores significantly different from 1, allowing detection of anomalous gradients without distributional assumptions
- Core assumption: The local density of malicious gradients differs sufficiently from honest gradients that LOF scores will consistently indicate outliers
- Break condition: If the malicious gradients are carefully crafted to match the local density of honest gradients, LOF will fail to detect them

## Foundational Learning

- Concept: Split Learning architecture
  - Why needed here: Understanding how the client and server split computation is essential to understanding the attack vector and detection mechanisms
  - Quick check question: In SplitNN, which party computes the loss and initiates backpropagation when labels are not shared with the server?

- Concept: Local Outlier Factor algorithm
  - Why needed here: LOF is the core detection mechanism used by SplitOut, so understanding its computation is crucial
  - Quick check question: What is the key difference between LOF and traditional outlier detection methods that assume a global distribution?

- Concept: Training-hijacking attack vector
  - Why needed here: Understanding how the server can manipulate client gradients is essential to understanding what needs to be detected
  - Quick check question: What is the primary objective of a training-hijacking attack in SplitNN?

## Architecture Onboarding

- Component map: Client-side neural network layers -> Gradient computation -> LOF algorithm -> SplitGuard logic -> Communication channel -> Server-side neural network layers

- Critical path:
  1. Client receives gradients from server
  2. Gradients are input to LOF algorithm
  3. LOF computes outlier scores
  4. Decision logic determines if attack is detected
  5. If attack detected, training is halted

- Design tradeoffs:
  - Sensitivity vs. false positives: Higher sensitivity may increase false positive rate
  - Compute overhead: LOF computation adds client-side overhead
  - Data privacy: SplitGuard maintains privacy by not requiring label sharing

- Failure signatures:
  - High LOF scores on honest gradients indicate false positives
  - Consistently low SG-LC scores may indicate either attack or model convergence
  - Sudden changes in gradient patterns may indicate attack detection

- First 3 experiments:
  1. Test LOF detection on MNIST with varying k values to find optimal neighbor count
  2. Compare SG-LC scores between honest and malicious servers on Fashion-MNIST
  3. Measure detection time and false positive rate for CIFAR10/100 datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SplitOut maintain its near-zero false positive rate against more sophisticated training-hijacking attacks that incorporate label information into their objective functions?
- Basis in paper: The paper discusses how the Feature-Space Hijacking Attack (FSHA) currently does not incorporate label information into its objective, but notes that future attacks might adapt to subvert detection methods like SplitOut.
- Why unresolved: The current implementation of SplitOut relies on detecting discrepancies between gradients when labels are randomized, but if future attacks incorporate label information, this detection mechanism might be less effective.
- What evidence would resolve it: Testing SplitOut against variants of FSHA that include label information in their loss functions across multiple datasets and attack scenarios would demonstrate its robustness against such attacks.

### Open Question 2
- Question: What is the optimal balance between detection accuracy and computational overhead when using different proportions of client data for LOF training?
- Basis in paper: The paper shows that using 1% of client data for LOF training achieves near-zero false positive rates, but notes that this still incurs a significant overhead (35-55% additional training time) compared to label-changing methods.
- Why unresolved: While the paper provides detection accuracy results for various data proportions, it doesn't fully explore the trade-off between detection performance and computational cost, particularly for larger models or more complex datasets.
- What evidence would resolve it: Systematic evaluation of detection accuracy, false positive rates, and computational overhead across varying data proportions (0.1%, 0.5%, 1%, 5%, 10%) on multiple datasets and model architectures would identify the optimal balance.

### Open Question 3
- Question: Can SplitOut be extended to detect training-hijacking attacks in multi-client split learning scenarios where clients take turns training with the server?
- Basis in paper: The paper focuses on the two-party split learning scenario and doesn't address the round-robin training protocol used when multiple clients participate.
- Why unresolved: The current detection mechanisms assume a single client-server interaction, but real-world split learning often involves multiple clients sharing the same server, which could affect gradient patterns and detection reliability.
- What evidence would resolve it: Implementation and evaluation of SplitOut in multi-client split learning environments with varying numbers of clients (2, 5, 10, 20) and different data distributions would demonstrate its effectiveness in these more complex scenarios.

## Limitations

- The detection performance may degrade against sophisticated attacks that carefully craft gradients to blend with honest ones
- The method's effectiveness on more complex datasets and real-world scenarios remains to be thoroughly validated
- The computational overhead, while modest, may still be prohibitive for resource-constrained client devices

## Confidence

- High Confidence: The core claim that training-hijacking attacks produce distinguishable gradient patterns that can be detected by outlier detection methods is well-supported by experimental results across multiple datasets.
- Medium Confidence: The assertion that SplitOut requires minimal hyperparameter tuning is somewhat subjective and depends on the specific implementation details not fully disclosed in the paper.
- Medium Confidence: The claim of minimal overhead is based on "modest client compute capabilities" but lacks specific benchmarks across different hardware configurations.

## Next Checks

1. Test SplitOut's detection performance on more complex datasets like ImageNet to evaluate scalability and robustness to diverse data distributions.
2. Evaluate the method's performance against adaptive attacks where malicious gradients are specifically designed to evade outlier detection.
3. Measure and compare the computational overhead of SplitOut across different client hardware profiles (mobile devices, IoT sensors, edge servers) to quantify real-world performance impact.