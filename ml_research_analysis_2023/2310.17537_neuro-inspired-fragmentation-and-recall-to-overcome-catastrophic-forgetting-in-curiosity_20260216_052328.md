---
ver: rpa2
title: Neuro-Inspired Fragmentation and Recall to Overcome Catastrophic Forgetting
  in Curiosity
arxiv_id: '2310.17537'
source_url: https://arxiv.org/abs/2310.17537
tags:
- intrinsic
- reward
- agent
- forgetting
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses catastrophic forgetting in prediction-based
  curiosity methods for reinforcement learning, which leads to misguiding exploration
  by increasing intrinsic rewards at previously visited states. The authors propose
  FARCuriosity, a neuro-inspired approach that fragments the environment into subspaces
  based on surprisal (intrinsic reward) and recalls previously stored curiosity modules
  from long-term memory when re-entering similar states.
---

# Neuro-Inspired Fragmentation and Recall to Overcome Catastrophic Forgetting in Curiosity

## Quick Facts
- arXiv ID: 2310.17537
- Source URL: https://arxiv.org/abs/2310.17537
- Reference count: 40
- Key outcome: FARCuriosity achieves better extrinsic rewards in heterogeneous Atari games while preventing catastrophic forgetting in prediction-based curiosity methods

## Executive Summary
This work addresses catastrophic forgetting in prediction-based curiosity methods for reinforcement learning, which leads to misguiding exploration by increasing intrinsic rewards at previously visited states. The authors propose FARCuriosity, a neuro-inspired approach that fragments the environment into subspaces based on surprisal (intrinsic reward) and recalls previously stored curiosity modules from long-term memory when re-entering similar states. This prevents modules from being trained on the entire environment and reduces forgetting. FARCuriosity outperforms standard RND in heterogeneous Atari environments, achieving better extrinsic rewards in games like Jamesbond and River Raid while maintaining similar performance in homogeneous games.

## Method Summary
FARCuriosity modifies prediction-based curiosity by introducing a fragmentation-and-recall mechanism. When the surprisal (prediction error) of the current curiosity module exceeds a threshold, the module is stored in long-term memory and either a new module is initialized or a similar one is recalled based on cosine similarity with stored observations. This creates specialized modules for different environmental regions, preventing catastrophic forgetting by limiting each module's training to local subspaces. The method is evaluated against standard RND on Atari games using PPO as the policy optimization algorithm.

## Key Results
- FARCuriosity outperforms standard RND in heterogeneous Atari environments, achieving better extrinsic rewards in games like Jamesbond and River Raid
- The method maintains similar performance to RND in homogeneous games like Tennis and Montezuma's Revenge
- Fragmentation reduces catastrophic forgetting by preventing curiosity modules from being trained on the entire environment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prediction-based intrinsic rewards increase over time for visited states due to catastrophic forgetting.
- Mechanism: When a neural network learns to predict future states, repeated exposure to the same observation causes the model's weights to shift toward fitting other states, increasing prediction error on the original state.
- Core assumption: Non-i.i.d. property of RL environments causes repeated updates on different observations before revisiting the same one.
- Evidence anchors: Abstract states prediction-based rewards increase at visited states; Section shows intrinsic rewards increase with gradually increasing episode length in Jamesbond.

### Mechanism 2
- Claim: Fragmentation prevents catastrophic forgetting by limiting each module's training to a local subspace.
- Mechanism: When surprisal exceeds a threshold, the current module is stored and a new one is initialized, ensuring each module only trains on similar observations.
- Core assumption: Different environmental regions have sufficiently distinct observation distributions that separate modules can specialize.
- Evidence anchors: Abstract explains fragmentation prevents modules from being trained on the entire environment; Section states previous modules preserve information without forgetting from learning other spaces.

### Mechanism 3
- Claim: Recall allows the agent to reuse previously learned local curiosity modules when returning to similar states.
- Mechanism: When the current observation is similar to a stored observation (based on cosine similarity), the corresponding module is recalled from LTM instead of training a new one.
- Core assumption: Similar observations in the environment correspond to similar curiosity requirements.
- Evidence anchors: Abstract describes storing modules in LTM and recalling based on state similarity; Section explains recall occurs when current observation is sufficiently similar to stored keys.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why prediction-based curiosity fails in RL environments
  - Quick check question: What happens to a neural network's weights when it learns new information while retaining old information is not prioritized?

- Concept: Intrinsic reward functions in reinforcement learning
  - Why needed here: The paper's approach modifies how intrinsic rewards are generated to prevent forgetting
  - Quick check question: How does an intrinsic reward function differ from an extrinsic reward function in RL?

- Concept: Short-term vs. long-term memory in computational models
  - Why needed here: The fragmentation-and-recall approach uses STM for current learning and LTM for storing modules
  - Quick check question: What is the computational advantage of storing learned information in LTM rather than keeping it in STM?

## Architecture Onboarding

- Component map: Observation → Feature extraction → Curiosity prediction → Surprisal calculation → Fragmentation/Recall decision → Module selection → Policy action

- Critical path: The agent receives observations, extracts features, predicts curiosity, calculates surprisal, decides whether to fragment or recall, selects appropriate module, and executes policy actions based on combined rewards

- Design tradeoffs: Memory vs. performance (more fragments = more memory but potentially better specialization), fragmentation threshold (too low = excessive fragmentation, too high = insufficient prevention of forgetting)

- Failure signatures: 
  - Performance similar to baseline RND (fragmentation not occurring)
  - Degraded performance (excessive fragmentation or poor recall)
  - Memory overflow (too many fragments stored)

- First 3 experiments:
  1. Run FARCuriosity on a simple grid world with increasing episode length to verify fragmentation occurs
  2. Test recall mechanism by creating a circular environment where the agent must revisit similar states
  3. Compare performance on homogeneous vs. heterogeneous Atari environments to validate the method's assumptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which catastrophic forgetting occurs in prediction-based intrinsic reward functions during reinforcement learning?
- Basis in paper: [explicit] The paper demonstrates that intrinsic rewards from RND increase over training in Jamesbond, indicating forgetting, and shows similar effects in a grid world experiment with gradually increasing episode length.
- Why unresolved: While the paper identifies forgetting as a problem, it doesn't fully characterize the exact neural network mechanisms or conditions that cause it to happen during standard RL with resets.
- What evidence would resolve it: Detailed analysis of how network weights change over time when exposed to the same state repeatedly across episodes, and under what training conditions the forgetting is most severe.

### Open Question 2
- Question: How does the performance of FARCuriosity compare to other exploration methods that don't suffer from catastrophic forgetting, such as count-based exploration with infinite memory?
- Basis in paper: [inferred] The paper compares FARCuriosity to RND and PPO, showing improvement, but doesn't compare to other memory-augmented or non-forgetting exploration methods.
- Why unresolved: The relative advantages of FARCuriosity's fragmentation approach versus other approaches to avoiding forgetting are not established.
- What evidence would resolve it: Direct comparison of FARCuriosity against count-based methods with unlimited memory and other exploration strategies on the same heterogeneous environments.

### Open Question 3
- Question: What is the relationship between the heterogeneity of an environment and the optimal number of fragments needed for FARCuriosity to perform well?
- Basis in paper: [explicit] The paper shows that FARCuriosity performs better in heterogeneous environments and measures environment heterogeneity, but finds no clear relationship between the number of fragments and performance improvement.
- Why unresolved: The paper suggests that fragmentation should help in heterogeneous environments but doesn't establish how to determine the optimal number of fragments for a given environment.
- What evidence would resolve it: Systematic experiments varying both environment heterogeneity and fragment count to establish optimal fragment numbers for different levels of environmental diversity.

## Limitations
- The paper lacks detailed implementation specifications for the fragmentation and recall mechanisms, particularly the surprisal threshold calculation and cosine similarity matching criteria
- Experimental validation is limited to Atari games, leaving open questions about generalizability to continuous control tasks or other RL domains
- The memory overhead of storing multiple curiosity modules is not quantified, which could be prohibitive in complex environments

## Confidence
- **High confidence**: The core mechanism of preventing catastrophic forgetting through modular learning is well-grounded in continual learning literature
- **Medium confidence**: The empirical results on Atari games show clear improvements in heterogeneous environments
- **Medium confidence**: The theoretical motivation for why prediction-based curiosity suffers from forgetting is sound but lacks rigorous mathematical proof

## Next Checks
1. **Memory complexity analysis**: Measure the exact memory overhead of storing multiple curiosity modules across different fragmentation frequencies and report the trade-off with performance gains

2. **Generalization test**: Implement FARCuriosity on continuous control benchmarks (e.g., DM Control Suite) to validate performance beyond discrete Atari environments

3. **Ablation study**: Systematically vary the surprisal threshold and cosine similarity matching criteria to determine optimal parameters and understand their impact on performance and memory usage