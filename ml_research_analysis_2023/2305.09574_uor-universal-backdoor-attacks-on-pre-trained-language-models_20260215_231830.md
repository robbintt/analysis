---
ver: rpa2
title: 'UOR: Universal Backdoor Attacks on Pre-trained Language Models'
arxiv_id: '2305.09574'
source_url: https://arxiv.org/abs/2305.09574
tags:
- plms
- backdoor
- trigger
- tasks
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a new backdoor attack method for pre-trained
  language models (PLMs) called UOR. It addresses the limitation of existing targeted
  backdoor attacks on PLMs which use manually predefined triggers and output representations,
  limiting their effectiveness and generalizability.
---

# UOR: Universal Backdoor Attacks on Pre-trained Language Models

## Quick Facts
- arXiv ID: 2305.09574
- Source URL: https://arxiv.org/abs/2305.09574
- Authors: 
- Reference count: 40
- This paper presents a new backdoor attack method for pre-trained language models (PLMs) called UOR that achieves better attack performance on various text classification tasks compared to manual methods.

## Executive Summary
This paper introduces UOR (Universal Output Representations), a novel backdoor attack method for pre-trained language models that addresses limitations of existing targeted attacks. Traditional backdoor attacks use manually predefined triggers and output representations, limiting their effectiveness and generalizability across different PLMs and tasks. UOR automatically learns more uniform and universal output representations of triggers using poisoned supervised contrastive learning, while also employing gradient search to select appropriate trigger words adaptive to different PLMs and vocabularies. Experiments demonstrate that UOR achieves superior attack performance on various text classification tasks compared to manual methods and shows strong universality across different PLM architectures and fine-tuning paradigms.

## Method Summary
UOR introduces poisoned supervised contrastive learning (PSCL) that treats poisoned datasets as separate classes, forcing trigger output representations to be centralized and uniformly distributed in feature space. The method also employs gradient search using the PSCL loss as supervision to iteratively update trigger words by minimizing the first-order Taylor approximation of the loss. Additionally, UOR introduces feature alignment between backdoored and clean PLMs using mean square error loss on clean data representations to maintain clean task accuracy while preserving backdoor effectiveness.

## Key Results
- UOR achieves better attack performance on various text classification tasks compared to manual backdoor methods
- The attack demonstrates universality across different PLM architectures, usage paradigms, and more difficult tasks
- Gradient search for trigger word selection improves attack effectiveness compared to manual trigger selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Poisoned Supervised Contrastive Learning (PSCL) automatically learns more uniform and universal output representations of triggers across different PLMs.
- Mechanism: PSCL treats poisoned datasets as separate classes in supervised contrastive learning, forcing the output representations of each class to be centralized and uniformly distributed in the feature space. This uniformity increases the probability that these representations will hit different labels after fine-tuning.
- Core assumption: Uniform distribution of output representations in the feature space leads to better coverage of downstream task labels.
- Evidence anchors:
  - [abstract]: "define poisoned supervised contrastive learning which can automatically learn the more uniform and universal output representations of triggers for various PLMs"
  - [section]: "We define poisoned supervised contrastive learning (PSCL)...The output representations of the trigger words, i.e., UORs, are automatically learned while establishing the strong links."
  - [corpus]: Weak evidence - the related papers mention backdoor attacks but do not specifically discuss contrastive learning mechanisms for uniformity.
- Break condition: If the downstream task's feature space is highly non-linear or if the classification layer creates complex decision boundaries that cannot be effectively covered by uniform representations.

### Mechanism 2
- Claim: Gradient search selects trigger words that are adaptive to different PLMs and vocabularies, improving attack effectiveness.
- Mechanism: Uses the PSCL loss as a supervision signal to iteratively update trigger words by minimizing the first-order Taylor approximation of the loss. This finds words that maximize the contrastive loss, making them more effective triggers.
- Core assumption: The gradient of the PSCL loss with respect to trigger word embeddings provides meaningful signal for finding better trigger words.
- Evidence anchors:
  - [abstract]: "we use gradient search to select appropriate trigger words which can be adaptive to different PLMs and vocabularies"
  - [section]: "Inspired by UAT [37], we further use gradient search to get more suitable trigger words. Specifically, we use the poisoned supervised contrastive learning loss Lp described in the following subsection as the supervised signal."
  - [corpus]: No direct evidence - related papers do not discuss gradient-based trigger word selection methods.
- Break condition: If the vocabulary search space is too large relative to the available compute, making the beam search computationally infeasible.

### Mechanism 3
- Claim: Feature alignment between backdoored and clean PLMs maintains clean task accuracy while preserving backdoor effectiveness.
- Mechanism: Uses mean square error loss between the output representations of clean data from backdoored and clean PLMs, ensuring that the backdoored model's clean data features remain in their original positions in feature space.
- Core assumption: Maintaining feature representation positions for clean data prevents accuracy degradation while allowing backdoor representations to be learned independently.
- Evidence anchors:
  - [abstract]: "we introduce a clean PLM and align the feature representations of clean data from backdoored PLM and clean PLM by mean square error loss"
  - [section]: "While injecting backdoors, we need to maintain the accuracy of PLMs on clean data...we use feature alignment to maintain the clean task accuracy."
  - [corpus]: No direct evidence - related papers do not discuss feature alignment as a method for maintaining clean accuracy during backdoor injection.
- Break condition: If the feature alignment is too strong, it might also constrain the learning of backdoor representations, reducing attack effectiveness.

## Foundational Learning

- Concept: Contrastive learning and its alignment/uniformity properties
  - Why needed here: Understanding how contrastive learning creates uniform distributions in feature space is crucial for grasping why PSCL works
  - Quick check question: How does supervised contrastive learning differ from unsupervised contrastive learning in terms of the positive/negative sample construction?

- Concept: Gradient-based discrete optimization (e.g., using first-order Taylor approximation for discrete tokens)
  - Why needed here: The trigger word selection uses gradient search despite tokens being discrete, requiring understanding of this optimization technique
  - Quick check question: Why can't we directly backpropagate through discrete token embeddings, and how does the first-order Taylor approximation help?

- Concept: PLM fine-tuning paradigms (full fine-tuning vs. prompt-based approaches)
  - Why needed here: The attack must work across different usage paradigms, requiring understanding of how each paradigm affects the model's feature space
  - Quick check question: How does the choice of fine-tuning paradigm affect which layers or representations are most vulnerable to backdoor attacks?

## Architecture Onboarding

- Component map: Trigger word selection (gradient search) -> Poisoned dataset creation -> PSCL backdoor injection with feature alignment -> Downstream fine-tuning -> Attack evaluation
- Critical path: Trigger word selection → poisoned dataset creation → PSCL backdoor injection with feature alignment → downstream fine-tuning → attack evaluation
- Design tradeoffs: Using rare words as initial triggers balances stealth with effectiveness, but may limit the search space; gradient search improves trigger selection but increases computational cost; feature alignment maintains accuracy but requires a clean PLM
- Failure signatures: Low T-ASR indicates poor trigger word selection or PSCL effectiveness; high clean task accuracy drop suggests feature alignment is too weak; inconsistent L-ASR across tasks suggests poor uniformity of UORs
- First 3 experiments:
  1. Implement PSCL with manually selected triggers on a simple binary classification task like SST-2 to verify the basic backdoor injection mechanism works
  2. Add gradient search to trigger word selection and compare T-ASR improvement against manual selection
  3. Test the attack across different PLM architectures (BERT, RoBERTa, BART) to verify universality claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of trigger words affect the effectiveness and stealthiness of backdoor attacks on PLMs?
- Basis in paper: [explicit] The paper discusses the use of rare words and gradient search to select appropriate trigger words that can be adaptive to different PLMs and vocabularies.
- Why unresolved: The paper does not provide a comprehensive analysis of how different types of trigger words (e.g., rare words, common words, contextually relevant words) impact the attack's success rate, transferability, and stealthiness across various downstream tasks and PLM architectures.
- What evidence would resolve it: A systematic study comparing the performance of backdoor attacks using different trigger word selection strategies (e.g., rare words, common words, contextually relevant words) across various downstream tasks and PLM architectures.

### Open Question 2
- Question: How does the poisoned supervised contrastive learning (PSCL) loss function contribute to the uniformity and universality of output representations of triggers?
- Basis in paper: [explicit] The paper introduces PSCL to automatically learn the more uniform and universal output representations of triggers for various PLMs.
- Why unresolved: The paper does not provide a detailed analysis of how the PSCL loss function specifically contributes to the uniformity and universality of the output representations, and how it compares to other contrastive learning approaches in terms of effectiveness and generality.
- What evidence would resolve it: An ablation study comparing the performance of backdoor attacks using different contrastive learning approaches (e.g., PSCL, standard contrastive learning) and analyzing the learned output representations in terms of uniformity and universality.

### Open Question 3
- Question: How do backdoor attacks on PLMs perform against more advanced defense mechanisms?
- Basis in paper: [explicit] The paper mentions testing the attack method on three defense methods: Onion, Re-init, and Pruning, but the results are not provided due to space limitations.
- Why unresolved: The paper does not provide a comprehensive evaluation of the attack method's performance against various defense mechanisms, such as adversarial training, input preprocessing, and model verification techniques.
- What evidence would resolve it: A thorough evaluation of the attack method's performance against a wide range of defense mechanisms, including adversarial training, input preprocessing, and model verification techniques, across various downstream tasks and PLM architectures.

## Limitations
- The computational cost of gradient search for trigger word selection may be prohibitive for large vocabularies
- The experiments are conducted in white-box settings, and real-world black-box scenarios could significantly impact attack effectiveness
- The paper does not extensively discuss ethical implications or potential misuse scenarios

## Confidence
- **High Confidence**: The core mechanism of poisoned supervised contrastive learning (PSCL) for creating uniform output representations is well-supported by theoretical foundations in contrastive learning literature
- **Medium Confidence**: The gradient search approach for trigger word selection is reasonable but relies on first-order Taylor approximations that may not always find globally optimal solutions
- **Medium Confidence**: The universality claims across different PLM architectures and fine-tuning paradigms are supported by experimental results, but the sample size of tested models and tasks is relatively limited

## Next Checks
1. Conduct an ablation study removing the feature alignment component to quantify its exact contribution to maintaining clean task accuracy while preserving backdoor effectiveness
2. Evaluate the attack's effectiveness in a black-box setting where the attacker only has access to the PLM's outputs without internal gradients or representations
3. Systematically test whether the attack remains effective when starting with common words rather than rare words as initial triggers, to understand the sensitivity to this assumption