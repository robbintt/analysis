---
ver: rpa2
title: Questioning the Survey Responses of Large Language Models
arxiv_id: '2306.07951'
source_url: https://arxiv.org/abs/2306.07951
tags:
- survey
- language
- responses
- questions
- census
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether the survey responses of large language
  models resemble those of human populations, focusing on the American Community Survey
  by the U.S. Census Bureau.
---

# Questioning the Survey Responses of Large Language Models

## Quick Facts
- arXiv ID: 2306.07951
- Source URL: https://arxiv.org/abs/2306.07951
- Reference count: 40
- Primary result: Language models' survey responses are governed by labeling biases and trend towards uniformly random distributions when corrected for these biases, lacking the statistical patterns found in human populations.

## Executive Summary
This paper investigates whether large language models' survey responses resemble those of human populations by surveying 43 different language models using the American Community Survey format. The authors identify two dominant patterns: models exhibit strong ordering and labeling biases (particularly an "A-bias"), and when these biases are corrected through randomized answer ordering, models' responses converge toward uniformly random distributions regardless of model size or training data. The study concludes that survey-derived alignment measures often have simple explanations rooted in these systematic biases rather than genuine alignment with human population statistics.

## Method Summary
The authors survey 43 language models of varying sizes (110M to 175B parameters) using de-facto standard prompting methodologies with the American Community Survey questions. They employ two experimental conditions: independent prompting where each question is asked separately, and sequential prompting where questions are asked in order with previous answers included as context. To address labeling bias, they randomize answer order for each prompt. They measure response distributions using entropy and KL divergence compared to both U.S. census data and uniform distributions. Additionally, they conduct a "signal test" by generating 100,000 sequential responses per model and attempting to predict income from other answers using various classifiers.

## Key Results
- Language models exhibit strong "A-bias," consistently over-picking the first answer choice labeled "A"
- After correcting for labeling bias through randomized ordering, models' responses trend toward uniformly random distributions regardless of model size
- No classifier can predict one answer from others better than random, indicating model responses lack the statistical correlations found in human survey data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Language models exhibit systematic ordering and labeling biases when answering survey questions, most notably an "A-bias" that drives responses toward the first option presented.
- **Mechanism:** Smaller models over-index on the lexical form of answer labels (e.g., "A", "B") rather than their semantic content. When prompted with standard survey formatting, the token corresponding to "A" appears early in the sequence, so the model assigns it higher probability due to positional bias in its next-token prediction.
- **Core assumption:** The model's probability distribution is heavily influenced by token position and labeling conventions in the prompt, rather than the underlying meaning of the choices.
- **Evidence anchors:**
  - [abstract] "...models' responses are governed by ordering and labeling biases, for example, towards survey responses labeled with the letter 'A'."
  - [section 3.1] "We find that models in the order of a few billion parameters or fewer consistently exhibit a strong tendency to over-pick the 'A' choice consistently."
- **Break condition:** If answer choices are randomized or relabeled with non-alphabetic tokens, the A-bias disappears or reverses, as shown in the randomization experiments.

### Mechanism 2
- **Claim:** After correcting for labeling bias through randomized answer ordering, language models' aggregate responses converge toward uniformly random distributions, regardless of model size or instruction-tuning.
- **Mechanism:** Once the spurious lexical signal (e.g., "A") is removed, the remaining distribution over answer choices lacks the structured variation present in human populations. The model's next-token probabilities over the remaining choices are effectively flat, yielding entropy near the theoretical maximum.
- **Core assumption:** Without strong prompting signals or structured training data for each specific survey question, the model defaults to a near-uniform output distribution.
- **Evidence anchors:**
  - [abstract] "...when adjusting for these systematic biases through randomized answer ordering, models across the board trend towards uniformly random survey responses, irrespective of model size or pre-training data."
  - [section 4.1] "When randomizing choice order, language models' survey responses, irrespective of model size or question asked, have a normalized entropy of approximately 1."
- **Break condition:** If the prompt contains additional contextual or demographic cues, or if the model is fine-tuned on survey-style data, the distribution may shift away from uniform.

### Mechanism 3
- **Claim:** Sequential survey responses from language models lack the statistical correlations (signals) found in human survey data; no classifier can predict one answer from others better than random.
- **Mechanism:** In human populations, survey answers are correlated due to shared underlying factors (e.g., age, income). Language models, when prompted sequentially, generate each answer independently conditioned only on the prompt, not on latent demographic structure. This independence results in no exploitable statistical signal.
- **Core assumption:** The model's generation process for each question does not implicitly model the joint distribution of demographic variables as they co-vary in real populations.
- **Evidence anchors:**
  - [abstract] "We find that no predictor performs significantly better than the constant predictor... data generated by surveying large language models lack the statistical patterns found in data collected by surveying human populations."
  - [section 5.2] "On the model generated data, no classifier is able to predict income with a significantly higher accuracy than a constant classifier, irrespective of model family, model size, or whether the classifier is instruction-tuned or not."
- **Break condition:** If the model is trained or fine-tuned on paired demographic datasets that capture real-world correlations, or if the prompt explicitly encodes demographic context, the signal may reappear.

## Foundational Learning

- **Concept: Entropy as a measure of distribution randomness**
  - Why needed here: Entropy quantifies how far a model's answer distribution deviates from uniform; it's central to detecting A-bias and comparing to census data.
  - Quick check question: If a model outputs each of k choices with probability 1/k, what is its normalized entropy?

- **Concept: KL divergence for comparing categorical distributions**
  - Why needed here: KL divergence measures similarity between model responses and reference populations (census or uniform); it's used to assess alignment.
  - Quick check question: If a model's distribution exactly matches the census distribution for a question, what is the KL divergence between them?

- **Concept: Randomized controlled trials in experimental design**
  - Why needed here: Randomizing answer order isolates the effect of labeling bias from semantic content, enabling causal inference.
  - Quick check question: If you randomize choice order and observe no change in output distribution, what does that imply about the model's bias?

## Architecture Onboarding

- **Component map:** Data pipeline -> Prompt construction -> Model API query -> Token probability extraction -> Aggregation -> Statistical analysis
- **Critical path:** Generate prompt -> query model -> normalize probabilities -> record distribution -> repeat for all questions/models -> compute metrics -> compare to census/uniform baselines
- **Design tradeoffs:** Using top-k probabilities vs. full distribution (cost vs. accuracy); randomizing order per question vs. per respondent (bias mitigation vs. realism); sequential vs. independent prompting (context richness vs. independence)
- **Failure signatures:** Uniform high entropy across all questions (suggests A-bias removal or lack of signal); KL divergence consistently near zero for all subgroups (suggests model collapse to uniform); no improvement in classifier accuracy over constant predictor (suggests missing population structure)
- **First 3 experiments:**
  1. Prompt GPT2-110M with non-randomized ordering; compute entropy and KL divergence; confirm A-bias.
  2. Repeat with randomized ordering; verify entropy approaches 1 and KL divergence to census increases.
  3. Generate 100k sequential responses from GPT NeoX 20B; run signal test on income prediction; confirm no signal above constant predictor.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do survey responses from large language models compare to those of human populations across different types of surveys beyond the American Community Survey?
- Basis in paper: [inferred] The paper focuses on the American Community Survey but notes that it remains to be seen how findings transfer to other surveys.
- Why unresolved: The study is limited to one specific survey, leaving uncertainty about generalizability to other survey types.
- What evidence would resolve it: Conducting similar experiments with various other surveys (e.g., political polls, consumer behavior surveys) and comparing results to human population data.

### Open Question 2
- Question: To what extent do instruction-tuned models exhibit different survey response patterns compared to base models, and can this difference be explained by specific characteristics of the fine-tuning data?
- Basis in paper: [explicit] The paper investigates instruction-tuning effects but finds no clear pattern in how responses align with census data.
- Why unresolved: The analysis shows reduced entropy but doesn't identify underlying mechanisms or data characteristics causing this change.
- What evidence would resolve it: Detailed analysis of instruction-tuning datasets and their relationship to observed response patterns in fine-tuned models.

### Open Question 3
- Question: What is the relationship between model size and the strength of labeling biases (like A-bias) in survey responses, and at what scale do these biases become negligible?
- Basis in paper: [explicit] The paper observes A-bias diminishing with model size but doesn't specify at what scale it becomes negligible.
- Why unresolved: The study doesn't investigate extremely large models where bias might approach zero.
- What evidence would resolve it: Testing models at scales beyond those in the study (e.g., 100B+ parameters) to identify the point where labeling biases become statistically insignificant.

## Limitations

- **Survey question fidelity:** The paper modifies several ACS questions for model compatibility, but exact formulations are not provided, limiting reproducibility and cross-question comparability.
- **Token-level bias generalization:** The A-bias mechanism is inferred from alphabetic labeling; it's unclear if similar biases exist for non-alphabetic or non-labeled response formats.
- **Signal test sensitivity:** The "signal test" uses a fixed 100k sample size and standard classifiers; it's unclear if this sample size or method is sufficient to detect subtle population signals.

## Confidence

- **A-bias existence and magnitude:** High - Directly observed in experiments across multiple small models with clear quantitative metrics (entropy, KL divergence).
- **Uniformity of responses after bias correction:** High - Consistently observed across model families and sizes with robust statistical measures.
- **Absence of population signals in model responses:** Medium - Based on a single signal test methodology; external validation is limited.
- **Generalizability to non-alphabetic labeling schemes:** Low - Not tested; inferred only from alphabetic case.

## Next Checks

1. **Cross-labeling bias test:** Repeat the entropy and KL divergence analysis using non-alphabetic response labels (e.g., numbers, symbols, or semantic tokens) to verify that the observed uniformity is not an artifact of alphabetic labeling conventions.

2. **Signal test robustness check:** Apply the signal test to model responses conditioned on explicit demographic prompts (e.g., "You are a 35-year-old male with income $50k") to determine if population signals re-emerge when demographic context is provided.

3. **Fine-tuning intervention test:** Fine-tune a subset of models on a small corpus of real survey responses, then repeat the full experimental pipeline (entropy, KL divergence, signal test) to assess whether exposure to human survey data induces population-like statistical patterns.