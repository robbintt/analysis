---
ver: rpa2
title: Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization
  of Stochastic Gradient Descent using Armijo Line Search
arxiv_id: '2307.13831'
source_url: https://arxiv.org/abs/2307.13831
tags:
- batch
- size
- learning
- critical
- armijo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a convergence analysis of stochastic gradient
  descent (SGD) with Armijo line search learning rates for nonconvex optimization.
  The analysis shows that the upper bound of the expectation of the squared norm of
  the full gradient becomes small when the number of steps and batch size are large.
---

# Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search

## Quick Facts
- arXiv ID: 2307.13831
- Source URL: https://arxiv.org/abs/2307.13831
- Reference count: 40
- Key outcome: Shows that SGD with Armijo line search has a critical batch size minimizing SFO complexity, with steps needed decreasing monotonically and convexly in batch size

## Executive Summary
This paper analyzes the convergence of stochastic gradient descent (SGD) with Armijo line search for nonconvex optimization, proving that the number of steps needed to reach a desired accuracy decreases monotonically and convexly with increasing batch size. The analysis establishes that there exists a critical batch size that minimizes the stochastic first-order oracle (SFO) complexity, which can be estimated from theoretical parameters. Numerical experiments on training deep neural networks validate these theoretical findings, demonstrating the practical relevance of the analysis for large-scale machine learning.

## Method Summary
The method implements SGD with Armijo line search using a backtracking approach to determine learning rates that satisfy the Armijo condition for sufficient decrease. The algorithm uses parameters c=0.05, δ=0.9, γ=2, and α=10, and is applied to train ResNet-34 on CIFAR-10/CIFAR-100 and MLP on MNIST datasets. The experiments measure the number of steps required to reach target accuracies (0.99 for CIFAR, 0.97 for MNIST) across different batch sizes, then analyze the relationship between batch size, steps needed, and SFO complexity to identify critical batch sizes.

## Key Results
- The number of steps K needed for nonconvex optimization is a monotone decreasing convex function of batch size b
- There exists a critical batch size b* that minimizes the SFO complexity N = Kb
- Theoretical upper bounds on critical batch size can be estimated from measured parameters σ² and L
- Numerical experiments show increasing batch size decreases steps needed, with critical batch sizes observed around 16-32 for tested architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Armijo line search learning rate guarantees a lower bound on the step size, which enables convergence of SGD for nonconvex optimization without requiring the strong growth condition.
- Mechanism: The backtracking Armijo line search (Algorithm 2) ensures that the learning rate αk is bounded below by α := 2δ(1-c)/L, which allows the descent lemma to be applied iteratively. This lower bound is critical because it prevents αk from becoming arbitrarily small, which would otherwise stall convergence.
- Core assumption: The learning rate computed by Algorithm 2 satisfies αk ≥ α > 0 for all k.
- Evidence anchors:
  - [section] Lemma 2.1(ii) guarantees a lower bound on the learning rate from the backtracking method.
  - [section] The descent lemma is applied using this lower bound in the proof of Theorem 3.1.
  - [corpus] Related work on SGD with Armijo line search (Vaswani et al., 2019) assumes similar conditions.
- Break condition: If the backtracking fails to find a suitable αk (e.g., due to ill-conditioned loss landscape), the lower bound may not hold, breaking the convergence guarantee.

### Mechanism 2
- Claim: Increasing the batch size reduces the number of steps K needed for nonconvex optimization, and the relationship is monotone decreasing and convex.
- Mechanism: The variance term in the upper bound of E[∥∇f(θk)∥²] is inversely proportional to the batch size b (V(σ²,b) ∝ 1/b). As b increases, this variance decreases, allowing fewer steps to achieve the same precision ϵ. The function K(b) = C₁b/(ϵ²b - C₂) is derived from this bound and is mathematically proven to be monotone decreasing and convex in b.
- Core assumption: The upper bound on the gradient norm accurately reflects the true convergence behavior, and the variance scales as 1/b.
- Evidence anchors:
  - [section] Theorem 3.2 proves that K(b) is monotone decreasing and convex.
  - [abstract] The paper states that "the number of steps needed for nonconvex optimization is a monotone decreasing convex function of the batch size."
  - [corpus] Shallue et al. (2019) and Zhang et al. (2019) observed similar scaling in deep learning practice.
- Break condition: If the variance does not scale as 1/b (e.g., due to gradient correlation across samples), the K(b) relationship may not hold.

### Mechanism 3
- Claim: There exists a critical batch size b* that minimizes the SFO complexity N = Kb, and this can be estimated from theoretical parameters.
- Mechanism: The SFO complexity N(b) = K(b)b = C₁b²/(ϵ²b - C₂) is a convex function of b. Its minimum occurs at b* = 2C₂/ϵ², which balances the trade-off between the number of steps and the cost per step. The critical batch size can be estimated using the upper bound b* ≤ 2δ(1-c)ασ²/[(α - δ(1-c)α)ϵ²].
- Core assumption: The SFO complexity is accurately captured by N = Kb, and the convexity of N(b) ensures a unique minimizer.
- Evidence anchors:
  - [section] Theorem 3.3 proves the existence and properties of the critical batch size.
  - [abstract] The paper states that "there exists a critical batch size that minimizes the SFO complexity."
  - [section] Section 4.2 provides a numerical example estimating b* from measured values.
- Break condition: If the relationship between K and b deviates from the theoretical model (e.g., due to hardware effects or adaptive optimizers), the estimated b* may not match the observed minimum.

## Foundational Learning

- Concept: Smoothness and Lipschitz continuity of the loss function.
  - Why needed here: Smoothness is used in the descent lemma and to bound the gradient differences, which are essential for proving convergence.
  - Quick check question: Why is the Lipschitz constant L important for setting the learning rate in SGD with constant step size?

- Concept: Stochastic gradient variance and its scaling with batch size.
  - Why needed here: The variance term V(σ²,b) ∝ 1/b appears in the convergence bound and explains why larger batches reduce the number of steps.
  - Quick check question: How does the variance of the stochastic gradient estimator change when you double the batch size?

- Concept: Armijo condition and backtracking line search.
  - Why needed here: The Armijo condition ensures sufficient decrease in the loss, and the backtracking method provides a lower bound on the learning rate, which is critical for the convergence proof.
  - Quick check question: What is the purpose of the Armijo condition in line search methods?

## Architecture Onboarding

- Component map: Main loop -> Batch sampling -> Gradient computation -> Backtracking line search -> Parameter update
- Critical path: For each iteration: sample batch → compute stochastic gradient → run backtracking line search → update parameters. The backtracking step is the most computationally intensive part.
- Design tradeoffs: Larger batch sizes reduce the number of steps but increase per-step cost; the critical batch size balances this. The Armijo parameters (c, δ) affect the aggressiveness of the line search and the lower bound on the learning rate.
- Failure signatures: If the learning rate becomes too small, training stalls; if the batch size is too large, the SFO complexity increases; if the Armijo condition is too strict, the line search may take many iterations.
- First 3 experiments:
  1. Verify that K(b) decreases as b increases for a simple convex problem.
  2. Measure the actual SFO complexity N(b) and locate the critical batch size empirically.
  3. Test the effect of different Armijo parameters (c, δ) on convergence speed and critical batch size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of SGD with Armijo line search compare to other adaptive methods like Adam or AdamW in terms of wall-clock time on real-world deep learning tasks?
- Basis in paper: [explicit] The paper compares convergence in terms of steps and SFO complexity but does not analyze wall-clock time, which depends on implementation details and hardware.
- Why unresolved: Wall-clock time depends on factors not considered in the theoretical analysis, such as computational overhead of line search and parallelization efficiency.
- What evidence would resolve it: Empirical comparison of training time for the same models and datasets using SGD with Armijo line search vs. Adam/AdamW on identical hardware.

### Open Question 2
- Question: Does the theoretical upper bound on the critical batch size (3.6) accurately predict the practical critical batch size across different deep learning architectures and datasets?
- Basis in paper: [explicit] The paper shows the theoretical upper bound can be used to estimate critical batch sizes and compares with measured values on ResNet-34 for CIFAR-10/100 and MLP for MNIST.
- Why unresolved: The comparison is limited to specific architectures and datasets; generalization to other architectures (e.g., transformers) and domains is unclear.
- What evidence would resolve it: Empirical validation of the critical batch size estimation on a diverse set of architectures (CNNs, transformers, etc.) and tasks (NLP, vision, RL).

### Open Question 3
- Question: How does the performance of SGD with Armijo line search degrade when the interpolation property does not hold, as is typical in deep learning?
- Basis in paper: [explicit] The paper highlights that previous convergence analyses required the interpolation property, which is unrealistic for deep learning, and provides a new analysis without this assumption.
- Why unresolved: While the paper removes the interpolation assumption, it does not empirically investigate the impact of violating this property on convergence.
- What evidence would resolve it: Empirical study comparing convergence of SGD with Armijo line search on datasets with and without the interpolation property (e.g., separable vs. non-separable data).

## Limitations
- The convergence bound relies on an upper bound of the full gradient norm, which may not tightly reflect actual convergence behavior in practice
- The analysis assumes the variance scales as 1/b, which may not hold for correlated gradients or specific loss landscapes
- The critical batch size estimation depends on theoretical parameters (L, σ²) that are often unknown or difficult to measure accurately

## Confidence
- High confidence: The monotonicity and convexity of K(b) as a function of batch size (Theorem 3.2)
- Medium confidence: The existence and estimation of critical batch size b* (Theorem 3.3), as this depends on accurate parameter estimation
- Medium confidence: The experimental validation, as it shows trends consistent with theory but uses a limited set of models and datasets

## Next Checks
1. **Empirical verification of K(b) scaling**: Systematically measure the number of steps needed to reach a fixed accuracy threshold across a wider range of batch sizes (including very large batches) on multiple architectures to validate the theoretical K(b) relationship.
2. **Critical batch size estimation accuracy**: Compare the theoretically estimated critical batch sizes with the empirically observed minima of SFO complexity across different datasets, models, and hyperparameter settings.
3. **Variance scaling validation**: Measure the actual variance of stochastic gradients at different batch sizes to verify whether it scales as 1/b, and identify conditions where this assumption breaks down.