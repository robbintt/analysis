---
ver: rpa2
title: 'How to Prune Your Language Model: Recovering Accuracy on the "Sparsity May
  Cry'''' Benchmark'
arxiv_id: '2312.13547'
source_url: https://arxiv.org/abs/2312.13547
tags:
- pruning
- sparsity
- benchmark
- embeddings
- obert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of accurate BERT-family language
  model pruning, especially in the context of the recently proposed "Sparsity May
  Cry" (SMC) benchmark, which highlighted the failure of existing pruning methods
  at moderate sparsities. The authors propose a set of general guidelines for successful
  pruning, including: adapting the length of the post-pruning training period and
  sparsification schedules relative to the desired target sparsity; avoiding pruning
  of embeddings and classification heads, which have outsized impact on accuracy;
  and using properly-tuned Knowledge Distillation.'
---

# How to Prune Your Language Model: Recovering Accuracy on the "Sparsity May Cry'' Benchmark

## Quick Facts
- arXiv ID: 2312.13547
- Source URL: https://arxiv.org/abs/2312.13547
- Reference count: 32
- Key outcome: This work addresses the challenge of accurate BERT-family language model pruning, especially in the context of the recently proposed "Sparsity May Cry" (SMC) benchmark, which highlighted the failure of existing pruning methods at moderate sparsities. The authors propose a set of general guidelines for successful pruning, including: adapting the length of the post-pruning training period and sparsification schedules relative to the desired target sparsity; avoiding pruning of embeddings and classification heads, which have outsized impact on accuracy; and using properly-tuned Knowledge Distillation. They validate these guidelines on standard BERT-pruning benchmarks and the SMC benchmark, achieving state-of-the-art results. Notably, even classic gradual magnitude pruning (GMP) can yield competitive results when applied with the right approach. On the SMC benchmark, the authors achieve high sparsities (80-90%) accurately, contradicting the strongly negative claims initially made by this benchmark.

## Executive Summary
This paper addresses the challenge of accurately pruning BERT-family language models, particularly in light of the "Sparsity May Cry" (SMC) benchmark that exposed failures of existing methods at moderate sparsities. The authors demonstrate that standard pruning techniques can achieve high accuracy at high sparsities when applied with proper methodology. Through systematic analysis, they identify three key guidelines: avoiding pruning of embeddings and classification heads, scaling sparsity schedules and learning rates according to target sparsity, and using properly-tuned knowledge distillation. These guidelines enable state-of-the-art results on the SMC benchmark, achieving 80-90% sparsity with maintained accuracy.

## Method Summary
The authors propose a set of general guidelines for successful pruning of BERT-family language models. They implement gradual magnitude pruning (GMP) and oBERT pruner with knowledge distillation (KD), following these guidelines: adapting training schedules based on target sparsity, avoiding pruning of embeddings and classification heads, and tuning KD hyperparameters (hardness=1.0, temperature=5.5). The approach is validated on RoBERTa-large models across downstream datasets like CommonsenseQA and WinoGrande, demonstrating state-of-the-art results at high sparsities (80-90%).

## Key Results
- Achieved state-of-the-art results on the SMC benchmark at 80-90% sparsity
- Demonstrated that classic GMP can achieve competitive results when properly applied
- Showed that avoiding pruning of embeddings and classification heads preserves accuracy
- Validated that scaled learning rate schedules relative to target sparsity improve accuracy recovery
- Proved that higher temperature (T=5.5) in knowledge distillation significantly improves sparse model accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning only the encoder (not embeddings or classification head) preserves accuracy while maintaining inference speed.
- Mechanism: The encoder contains the majority of parameters and FLOPs (99.9% of compute in BERT), while embeddings and classification head have negligible computational impact even when dense. Removing them from pruning prevents accuracy collapse from destroying token representations or position information.
- Core assumption: The model's representational power resides primarily in the encoder layers, and pruning other components provides no practical speed benefit due to their implementation as lookup tables.
- Evidence anchors:
  - [abstract] "We perform a cost-vs-benefits analysis of pruning model components, such as the embeddings and the classification head"
  - [section 3.1] "In summary, this analysis shows that, in this context, pruning the embeddings and classification head does not yield practical gains during inference, while negatively impacting model's accuracy"
  - [corpus] Weak - no direct corpus support for this specific claim

### Mechanism 2
- Claim: Proper Knowledge Distillation with higher temperature (T=5.5) significantly improves sparse model accuracy.
- Mechanism: Higher temperature softens the teacher's output distribution, making the knowledge distillation signal more useful for the student model rather than just reinforcing the teacher's confident predictions. This provides better regularization during pruning.
- Core assumption: The standard T=1.0 or T=2.0 temperatures make teacher predictions too confident, reducing the effectiveness of knowledge distillation as a regularization signal.
- Evidence anchors:
  - [section 3.2] "We visualize teacher's output distributions for randomly picked samples from the SQuADv1.1 task softened with three values of the temperature. As can be seen, teacher's high confidence in predicting the correct class at the commonly used temperatures T âˆˆ {1.0, 2.0} makes the knowledge distillation almost obsolete."
  - [section 3.2] "Given the results, we adopt the temperature T = 5.5"
  - [corpus] Weak - no direct corpus support for this specific temperature choice

### Mechanism 3
- Claim: Scaling pruning schedules (both sparsity and learning rate) according to target sparsity enables accurate recovery at high sparsities.
- Mechanism: The accelerated sparsity scheduler starts with a large initial pruning step (70% sparsity) rather than gradual pruning from 0%, allowing more fine-tuning time at higher sparsities where accuracy recovery is most challenging. Learning rate schedules are scaled proportionally to sparsity targets.
- Core assumption: Language models are heavily over-parameterized for downstream tasks, so aggressive initial pruning is possible without catastrophic accuracy loss if followed by sufficient fine-tuning.
- Evidence anchors:
  - [section 3.3] "Instead of starting to prune the model from zero-sparsity, in the first pruning step the model should be pruned to a much higher target (e.g. 50% or 70%)"
  - [section 3.3] "This leaves more time to distribute pruning to high sparsity targets over a longer fine-tuning range, and thus enables better accuracy recovery"
  - [corpus] Weak - no direct corpus support for this specific scheduling approach

## Foundational Learning

- Concept: Parameter magnitude-based pruning
  - Why needed here: The paper uses gradual magnitude pruning (GMP) as a baseline, which removes weights with smallest magnitudes during training
  - Quick check question: What determines which weights get pruned in magnitude-based pruning, and why might this be suboptimal for some neural network architectures?

- Concept: Knowledge Distillation (KD)
  - Why needed here: KD is used to transfer knowledge from dense to sparse models, with specific temperature and hardness parameter tuning shown to be critical
  - Quick check question: How does the temperature parameter in KD affect the softness of the teacher's output distribution, and why does this matter for pruning?

- Concept: Scheduled learning rate decay
  - Why needed here: The paper shows that learning rate schedules must be scaled relative to target sparsity to achieve good accuracy recovery
  - Quick check question: Why might a fixed learning rate schedule fail when pruning to different sparsity targets, and how does scaling address this issue?

## Architecture Onboarding

- Component map: Embeddings (token, segment, position) -> Encoder (transformer layers) -> Classification head
- Critical path: The encoder contains ~85M parameters out of 110M total, accounting for 99.9% of FLOPs. This is where pruning must focus for computational gains.
- Design tradeoffs: Avoiding pruning of embeddings and classification head preserves accuracy but maintains parameter count; aggressive initial pruning enables high sparsity but requires careful fine-tuning scheduling.
- Failure signatures: Accuracy collapse at moderate sparsities (40-60%) when embeddings/classification head are pruned; poor recovery when learning rate schedules aren't scaled to sparsity targets.
- First 3 experiments:
  1. Implement GMP with standard fine-tuning schedule on BERT-base/SQuADv1.1 and observe accuracy drop at 40-50% sparsity
  2. Apply the proposed accelerated sparsity scheduler (initial 70% pruning step) and scaled learning rate schedule, measure accuracy recovery
  3. Add knowledge distillation with T=5.5 and hardness=1.0, compare against standard T=1.0 KD implementation

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas warrant further investigation based on the findings:

## Limitations
- The analysis focuses primarily on unstructured pruning of RoBERTa-large models, leaving open questions about structured pruning approaches
- The optimal hyperparameters (temperature=5.5, hardness=1.0) appear somewhat heuristic without theoretical justification
- The accelerated pruning schedule with 70% initial pruning step lacks rigorous ablation studies showing why this specific value was chosen
- The findings may not generalize to other model architectures beyond BERT-family models

## Confidence

**High Confidence**: The finding that pruning embeddings and classification heads provides negligible inference speed benefits while harming accuracy is well-supported by the computational analysis showing these components represent minimal FLOPs (0.1% of total).

**Medium Confidence**: The temperature=5.5 setting for knowledge distillation shows strong empirical results, but the justification based on teacher confidence distributions is somewhat heuristic.

**Low Confidence**: The claim that "classic GMP can achieve competitive results when applied properly" is somewhat overstated, as the comparison to modern methods is limited.

## Next Checks
1. **Ablation study on accelerated pruning schedule**: Systematically vary the initial pruning percentage (50%, 60%, 70%, 80%) while keeping other factors constant to determine the optimal starting point and test the sensitivity of the approach.

2. **Cross-architecture validation**: Apply the proposed guidelines to prune other transformer architectures (T5, GPT-2, or LLaMA) on diverse tasks to assess whether the findings generalize beyond BERT-family models.

3. **Structured vs unstructured pruning comparison**: Implement the same guidelines using structured pruning approaches (pruning entire attention heads or layers) to evaluate whether the accuracy-sparsity tradeoffs differ from unstructured pruning.