---
ver: rpa2
title: Gaussian processes based data augmentation and expected signature for time
  series classification
arxiv_id: '2310.10836'
source_url: https://arxiv.org/abs/2310.10836
tags:
- signature
- time
- series
- expected
- normalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel time series classification model that
  leverages the expected signature as a feature extraction mechanism. The model employs
  a Gaussian process-based data augmentation technique to generate a diverse ensemble
  of time series, which are then used to compute the expected signature.
---

# Gaussian processes based data augmentation and expected signature for time series classification

## Quick Facts
- arXiv ID: 2310.10836
- Source URL: https://arxiv.org/abs/2310.10836
- Reference count: 5
- Primary result: Model achieves up to 15% accuracy improvement on real-world datasets

## Executive Summary
This paper introduces a novel time series classification model that combines Gaussian process-based data augmentation with expected signature computation. The approach generates diverse time series ensembles through GP regression, computes signatures for each sample, and averages them to obtain robust features. The model demonstrates superior performance compared to benchmark methods, achieving significant accuracy improvements on multiple real-world datasets. Key advantages include optimal feature extraction through supervised training and compatibility with backpropagation for seamless neural network integration.

## Method Summary
The proposed model uses Gaussian process regression to augment time series data, creating a diverse ensemble of trajectories that preserve statistical coherence. These augmented series are then transformed using the signature method, with tensor normalization applied to stabilize training. The expected signature is computed by averaging normalized signatures from the augmented samples, capturing the statistical properties of the original data. This feature extraction mechanism is integrated into a neural network architecture and trained using backpropagation with cross-validation.

## Key Results
- Achieves up to 15% accuracy improvement over benchmark models on real-world datasets
- Outperforms multiple augmentation methods including FFT and cubic spline interpolation
- Demonstrates robustness across diverse datasets including ECG200, PowerCons, and Epilepsy classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gaussian process-based data augmentation generates a diverse ensemble of time series that enriches the feature space.
- Mechanism: The GP regression layer learns mean and covariance functions from the input time series, then samples K new series conditioned on the observed data, producing trajectories that preserve statistical coherence.
- Core assumption: The learned mean and covariance functions accurately capture the underlying process structure so that samples remain informative.
- Evidence anchors:
  - [section] "The outcome of this part provides a much richer ensemble of time series than the original input, that nevertheless is coherent with the original time series."
  - [section] "Indeed, the possible values assumed by an interpolating function at unknown time instants... are described by the conditional law, that is the Gaussian law with mean and covariance given respectively by..."
- Break condition: If the learned GP mean/covariance deviates significantly from the true process, samples may be uninformative or introduce noise that degrades classification.

### Mechanism 2
- Claim: Computing the expected signature over the augmented ensemble yields a robust, distribution-characterizing feature vector.
- Mechanism: The signature transform applied to each augmented path captures nonlinear dependencies; averaging normalized signatures approximates the expected signature, which theoretically characterizes the law of the stochastic process.
- Core assumption: A sufficiently large K makes the empirical mean of signatures converge to the true expected signature.
- Evidence anchors:
  - [abstract] "The expected signature, properly normalized, captures the statistical properties of the original time series and enables effective classification."
  - [section] "Proposition A.14 shows that the empirical mean of the K signatures obtained by the K enlarged time series is a good approximation of the expected signature as long as K is sufficiently large."
- Break condition: Insufficient K leads to high variance in the expected signature estimate, hurting model stability.

### Mechanism 3
- Claim: Tensor normalization of the signature stabilizes training and preserves theoretical properties.
- Mechanism: The normalization constant λ scales signature components so their norm matches a target function ψ, ensuring both computational stability and the ability of the expected signature to characterize the law of the process.
- Core assumption: The chosen ψ function satisfies theoretical constraints (bounded, Lipschitz, ψ(1)=1) so that λ is well-defined and stable.
- Evidence anchors:
  - [section] "Indeed, we will see that a loose normalization can make the training unstable, see Section 3.2.1 for experimental evidence."
  - [section] "At last, we report a concrete example where the normalization is crucial... These two stochastic processes have the same expected signature."
- Break condition: An ill-chosen ψ (e.g., too loose or too strict) causes training instability or loss of discriminative power.

## Foundational Learning

- Concept: Gaussian Process regression (GP regression)
  - Why needed here: Provides a principled way to sample plausible interpolations of time series, enriching the dataset for expected signature computation.
  - Quick check question: What are the mean and covariance functions in a GP, and how do they determine the distribution over functions?

- Concept: Signature transform of paths
  - Why needed here: Converts time series into high-dimensional tensors that encode nonlinear interactions; truncation still retains most information.
  - Quick check question: How does the signature uniquely characterize a path up to time reparametrization, and why is truncation acceptable?

- Concept: Expected signature and normalization
  - Why needed here: Averaging normalized signatures yields a distribution-characterizing feature; normalization is necessary for theoretical identifiability.
  - Quick check question: What role does the normalization constant λ play, and why is it critical for the expected signature to characterize the law of the process?

## Architecture Onboarding

- Component map: Input time series → GP augmentation layer → Dimensional augmentation → Signature computation → Normalization → Expected signature → Classification head
- Critical path: Input → GP augmentation → Signature computation → Normalization → Expected signature → Classification
- Design tradeoffs:
  - K (sample size) vs. training speed: larger K gives better expected signature estimate but slower training
  - Signature truncation level L vs. feature richness: higher L captures more detail but increases dimensionality and risk of overfitting
  - Tensor normalization ψ shape parameter C: must balance between strict normalization (stability) and loose normalization (risk of instability)
- Failure signatures:
  - Training instability or exploding gradients → check normalization tightness (C too large)
  - Low accuracy despite high K → verify GP learned mean/covariance capture the process structure
  - High variance in output across multiple forward passes → increase K
- First 3 experiments:
  1. Vary K from 10 to 100 and measure output variance to find a threshold where variance stabilizes
  2. Test C values (e.g., 1.1, 1.5, 2.0) and observe training stability and convergence speed
  3. Compare performance with and without the GP augmentation layer on a small synthetic dataset to isolate its contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Gaussian process-based data augmentation technique compare to other augmentation methods (e.g., FFT, cubic spline interpolation) in terms of preserving the statistical properties of the original time series?
- Basis in paper: [explicit] The paper states that "the stochastic augmentation can strongly increase the inference capability with respect to the model that uses the signature instead of the expected signature" and compares it to other augmentation methods in the experiments.
- Why unresolved: While the paper shows that the proposed method outperforms other augmentation techniques, it does not provide a detailed analysis of why the Gaussian process-based augmentation is more effective in preserving the statistical properties of the original time series.
- What evidence would resolve it: A detailed analysis of the statistical properties of the augmented time series generated by different augmentation methods, comparing them to the original time series.

### Open Question 2
- Question: What is the impact of the shape parameter C in the tensor normalization on the computational stability and performance of the proposed model?
- Basis in paper: [explicit] The paper mentions that "the shape parameter C plays a relevant role from an experimental point of view" and shows that a loose normalization can make the training process unstable.
- Why unresolved: The paper does not provide a theoretical explanation for the impact of the shape parameter C on the computational stability and performance of the model.
- What evidence would resolve it: A theoretical analysis of the impact of the shape parameter C on the computational stability and performance of the model, supported by experimental results.

### Open Question 3
- Question: How does the number of augmented time series K affect the accuracy and stability of the expected signature estimation?
- Basis in paper: [explicit] The paper states that "the empirical mean of the K signatures obtained by the K enlarged time series is a good approximation of the expected signature as long as K is sufficiently large" and shows that increasing K improves the stability of the model.
- Why unresolved: The paper does not provide a quantitative analysis of the relationship between the number of augmented time series K and the accuracy and stability of the expected signature estimation.
- What evidence would resolve it: A quantitative analysis of the relationship between the number of augmented time series K and the accuracy and stability of the expected signature estimation, supported by experimental results.

## Limitations
- The paper demonstrates strong empirical performance but lacks comprehensive theoretical justification for why the combination of GP augmentation and expected signatures outperforms other methods
- Computational complexity is not thoroughly analyzed, particularly the impact of increasing signature truncation level L and sample size K on training time and memory usage
- The choice of tensor normalization function ψ and its shape parameter C appears critical but is not well-justified theoretically

## Confidence
- **High confidence** in the mathematical formulation of expected signatures and their properties
- **Medium confidence** in the GP-based data augmentation mechanism, though implementation details are sparse
- **Medium confidence** in the empirical results, as they are demonstrated on multiple datasets but with limited hyperparameter sensitivity analysis

## Next Checks
1. **Ablation study**: Remove the GP augmentation layer and compare performance to isolate its contribution to the model's success
2. **Hyperparameter sensitivity analysis**: Systematically vary K (sample size) and C (shape parameter) to identify optimal ranges and understand their impact on stability and accuracy
3. **Computational scaling study**: Measure training time and memory usage as L and K increase to characterize the computational burden and identify practical limits