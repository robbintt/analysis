---
ver: rpa2
title: What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data
  Selection in Instruction Tuning
arxiv_id: '2312.15685'
source_url: https://arxiv.org/abs/2312.15685
tags:
- data
- response
- prompt
- given
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates what makes good data for instruction tuning\
  \ and alignment. The authors analyze data from three dimensions\u2014complexity,\
  \ quality, and diversity\u2014and propose novel metrics for each."
---

# What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning

## Quick Facts
- arXiv ID: 2312.15685
- Source URL: https://arxiv.org/abs/2312.15685
- Reference count: 40
- Primary result: DEITA models achieve state-of-the-art performance on MT-Bench and AlpacaEval using only 6K SFT samples, outperforming models trained on 10x more data

## Executive Summary
This work investigates what makes good data for instruction tuning and alignment by analyzing data across three dimensions: complexity, quality, and diversity. The authors propose novel evolution-based metrics for each dimension and introduce a simple data selection strategy that prioritizes high complexity and quality while maintaining diversity. Using this approach, they train DEITA models from LLaMA and Mistral architectures with only 6K SFT samples, achieving strong performance on MT-Bench and AlpacaEval benchmarks—outperforming or matching state-of-the-art open-source alignment models trained on 10x more data. When further fine-tuned with direct preference optimization (DPO), DEITA-Mistral-7B + DPO reaches 7.55 MT-Bench and 90.06% AlpacaEval scores using only 6K SFT and 10K DPO samples.

## Method Summary
The paper proposes evolution-based complexity and quality metrics that evolve single data points through multiple iterations to produce examples varying in complexity or quality. These evolved samples are ranked by ChatGPT to train complexity and quality scorers. A score-first, diversity-aware selection strategy combines these scores with a representation filter using embedding distances to maintain diversity. The authors fine-tune LLaMA and Mistral models using the selected data and evaluate on MT-Bench and AlpacaEval benchmarks.

## Key Results
- DEITA-LLaMA1-13B6K achieves 6.76 MT-Bench and 87.4% AlpacaEval scores using only 6K SFT samples
- DEITA-Mistral-7B + DPO reaches 7.55 MT-Bench and 90.06% AlpacaEval scores with 6K SFT + 10K DPO samples
- Both models outperform or match state-of-the-art open-source alignment models trained on 10x more data
- The approach demonstrates that careful data selection can dramatically reduce the amount of data needed for effective alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EVOL COMPLEXITY improves alignment by systematically increasing data sample complexity through multi-stage evolution and fine-grained ranking.
- Mechanism: Data samples are evolved through five iterations using constraints, deepening, concretization, and reasoning steps. ChatGPT ranks all evolved versions of the same sample to produce granular complexity scores. These scores train a scorer that identifies complex samples for instruction tuning.
- Core assumption: Increased complexity in data samples leads to better alignment performance when instruction tuning.
- Evidence anchors:
  - [abstract]: "we propose EVOL COMPLEXITY and EVOL QUALITY, which evolve a single data point to produce a series of examples varying in complexity or quality"
  - [section]: "we ask ChatGPT to rank and score the variants of the same data sample for a small seed dataset, and we train our own complexity and quality scorers based on these scores"
  - [corpus]: "Found 25 related papers... RICo: Refined In-Context Contribution for Automatic Instruction-Tuning Data Selection"
- Break condition: If evolution process produces samples that are too complex to be useful or if ranking fails to differentiate complexity levels meaningfully.

### Mechanism 2
- Claim: EVOL QUALITY enhances alignment by evolving responses to be more helpful, relevant, deep, creative, and detailed.
- Mechanism: For each instruction-response pair, ChatGPT iteratively improves response quality through five dimensions. The evolved responses are ranked and scored, creating quality scores that train a predictor. High-quality samples are selected for instruction tuning.
- Core assumption: Higher quality responses in training data lead to better instruction-following ability in fine-tuned models.
- Evidence anchors:
  - [abstract]: "we propose EVOL QUALITY to augment the discernment of quality measurement"
  - [section]: "we prompt ChatGPT to elevate the quality of the response in an evolved way (please refer to Appendix E.4 for the prompt)"
  - [corpus]: "Accelerate Scaling of LLM Finetuning via Quantifying the Coverage and Depth of Instruction Set"
- Break condition: If quality improvements don't translate to better alignment performance or if scoring becomes inconsistent.

### Mechanism 3
- Claim: The score-first, diversity-aware selection strategy optimizes the combination of complexity, quality, and diversity for efficient alignment.
- Mechanism: Samples are sorted by the product of complexity and quality scores (evol score), then selected iteratively using a representation filter that maintains diversity by checking embedding distances. This ensures complex, high-quality, and diverse training data.
- Core assumption: Combining complexity and quality while maintaining diversity yields better alignment than focusing on any single dimension.
- Evidence anchors:
  - [abstract]: "we design a simple strategy to select the most effective data examples from a large data pool"
  - [section]: "we propose a score-first, diverse-aware data selection strategy, denoted as πDEITA"
  - [corpus]: "InsBank: Evolving Instruction Subset for Ongoing Alignment"
- Break condition: If diversity filtering removes too many high-scoring samples or if the product score doesn't correlate with alignment performance.

## Foundational Learning

- Concept: Data selection in instruction tuning
  - Why needed here: Understanding how to select effective training data is fundamental to achieving data-efficient alignment
  - Quick check question: What distinguishes data selection for instruction tuning from traditional task-specific fine-tuning?

- Concept: Multi-turn dialogue scoring
  - Why needed here: The paper scores each turn of multi-turn dialogues separately and sums them, requiring understanding of how to handle conversational data
  - Quick check question: How does the paper handle complexity/quality scoring for multi-turn dialogues?

- Concept: Embedding-based diversity filtering
  - Why needed here: The representation filter uses cosine distances between embeddings to maintain diversity, requiring knowledge of how embeddings capture semantic similarity
  - Quick check question: What threshold is used to determine if a sample adds diversity to the selected set?

## Architecture Onboarding

- Component map:
  - Data pool (Xsota/Xbase) → Complexity scorer → Quality scorer → Combined evol score → Diversity filter → Selected subset → Instruction tuning → Evaluation
  - Seed dataset (2K Alpaca samples) → Evolution prompts → ChatGPT ranking → Scorer training (LLaMA-1 7B)

- Critical path: Evolution → Ranking → Scorer training → Data selection → Instruction tuning
- Design tradeoffs: Simple product of complexity and quality scores vs. more sophisticated weighting; model-based embeddings vs. semantic-based embeddings for diversity
- Failure signatures: Random selection baseline performs well; perplexity-based complexity selection fails; direct ChatGPT scoring is expensive and less effective
- First 3 experiments:
  1. Run EVOL COMPLEXITY on a small seed dataset and verify the scorer produces meaningful complexity differentiation
  2. Test the representation filter with different thresholds (0.8, 0.85, 0.9) on a small dataset
  3. Train DEITA-LLaMO1-13B6K and evaluate on MT-Bench to confirm baseline performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of sentence representation method (model-based vs. semantic-based) impact the performance of the Repr Filter diversity selection strategy?
- Basis in paper: [explicit]
- Why unresolved: The paper only briefly mentions that the model-based approach (using LLaMA-1 13B embeddings) performs better than the semantic-based approach (using E5-Large-V2 embeddings) but doesn't provide a detailed analysis of why this is the case or explore other potential representation methods.
- What evidence would resolve it: A more comprehensive study comparing different sentence representation methods (e.g., other embedding models, fine-tuned models) and their impact on Repr Filter performance would help understand the optimal choice for this task.

### Open Question 2
- Question: What is the optimal threshold value (τ) for the Repr Filter, and how does it affect the trade-off between diversity and quality in the selected dataset?
- Basis in paper: [explicit]
- Why unresolved: The paper mentions that τ is set to 0.9 in all experiments but doesn't explore the impact of different threshold values on the final performance or provide a method for determining the optimal threshold.
- What evidence would resolve it: Experiments with varying threshold values and their corresponding performance on alignment tasks would help identify the optimal threshold and understand the trade-offs involved.

### Open Question 3
- Question: How does the complexity, quality, and diversity of the selected data affect the performance of the aligned model on different downstream tasks?
- Basis in paper: [inferred]
- Why unresolved: While the paper focuses on the overall performance of DEITA models on MT-Bench and AlpacaEval, it doesn't provide a detailed analysis of how the characteristics of the selected data (complexity, quality, diversity) influence the model's performance on specific downstream tasks or domains.
- What evidence would resolve it: A systematic analysis of DEITA models' performance on various downstream tasks (e.g., coding, reasoning, math) based on the complexity, quality, and diversity of the training data would help understand the relationship between data characteristics and task-specific performance.

## Limitations

- The approach relies heavily on ChatGPT's ability to rank and score evolved data samples, introducing potential subjectivity and consistency concerns
- The evolution process details are not fully specified, making it difficult to reproduce exactly
- The diversity filtering using embedding distances may not capture all relevant semantic diversity dimensions
- The evaluation focuses on English language benchmarks, limiting generalizability to other languages

## Confidence

- **High Confidence**: The claim that careful data selection enables data-efficient alignment is well-supported by strong empirical results showing state-of-the-art performance with 10x less data
- **Medium Confidence**: The mechanism of evolution-based complexity and quality scoring is plausible and methodologically sound, though the exact implementation details could affect reproducibility
- **Medium Confidence**: The diversity-aware selection strategy is reasonable but may have limitations in capturing all relevant dimensions of diversity, particularly for specialized domains

## Next Checks

1. **Replicate the evolution process**: Run the evolution pipeline on a small seed dataset and verify that the resulting scorer produces meaningful differentiation between complexity levels across multiple runs

2. **Test diversity threshold sensitivity**: Evaluate model performance using different diversity thresholds (0.8, 0.85, 0.9) to determine the optimal balance between diversity and data efficiency

3. **Cross-architecture validation**: Test whether the evolved complexity and quality scorers trained on one architecture (e.g., LLaMA) transfer effectively to another (e.g., Mistral) without retraining