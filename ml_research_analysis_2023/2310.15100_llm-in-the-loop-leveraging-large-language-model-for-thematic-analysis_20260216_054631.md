---
ver: rpa2
title: 'LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis'
arxiv_id: '2310.15100'
source_url: https://arxiv.org/abs/2310.15100
tags:
- data
- codes
- code
- coder
- codebook
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a human-LLM collaboration framework for thematic
  analysis, addressing the labor-intensive nature of traditional methods. The framework
  uses in-context learning to guide a large language model (e.g., GPT-3.5) in generating
  initial codes and refining them through iterative discussions with a human coder.
---

# LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis

## Quick Facts
- arXiv ID: 2310.15100
- Source URL: https://arxiv.org/abs/2310.15100
- Reference count: 9
- The framework achieves comparable coding quality to human coders while reducing labor and time demands in thematic analysis

## Executive Summary
This study introduces a human-LLM collaboration framework that addresses the labor-intensive nature of traditional thematic analysis by leveraging large language models for code generation and refinement. The framework uses in-context learning to guide LLMs in generating initial codes from qualitative data, followed by an iterative discussion loop where human coders review and refine the codes until consensus is reached. Two case studies on music listening experiences and password manager usage demonstrate that this approach achieves high inter-annotator agreement (κ = 0.87 and 0.81) and cosine similarity (0.8864 and 0.8895) compared to human-coded gold standards, while significantly reducing the time and effort required for codebook development.

## Method Summary
The framework implements a three-phase process: (1) human coders familiarize themselves with the data and create 4-8 exemplars for initial code extraction, (2) the LLM generates initial codes using in-context learning based on these exemplars and groups similar codes into themes, and (3) an iterative discussion loop occurs where the human coder reviews and edits codes with rationales, and the LLM responds with agreement or disagreement until consensus is reached. To address LLM input size limitations, the researchers randomly sampled subsets of data for codebook development, which was then applied to the full dataset. The process produces a final codebook that is used to code all responses, with inter-annotator agreement metrics calculated to evaluate quality.

## Key Results
- Achieved high inter-annotator agreement (κ = 0.87 for music listening, κ = 0.81 for password manager datasets) comparable to human-human coding
- Generated cosine similarity scores of 0.8864 and 0.8895 between LLM-generated codes and human-coded gold standards
- Successfully addressed LLM input size limitations by using partial data for codebook development while maintaining coding quality
- Demonstrated significant reduction in labor and time demands compared to traditional thematic analysis methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning (ICL) with carefully designed exemplars enables the LLM to generate initial codes that align with human coding standards
- Mechanism: The LLM uses the provided exemplars as templates to understand the expected format and granularity of codes, then applies this pattern to new responses through few-shot learning
- Core assumption: The exemplars provided are representative enough of the data distribution and coding standards to enable generalization
- Evidence anchors: [section] "We used in-context learning (ICL) to design the paradigm of prompt for the initial code extraction... Each HC produces four to eight exemplars... Each exemplar includes a response for an open-ended question and the associated actions"

### Mechanism 2
- Claim: The iterative discussion loop between human coder and LLM refines codes to achieve high inter-annotator agreement
- Mechanism: Human coder reviews LLM-generated codes, makes modifications with justifications, LLM responds with agreement/disagreement and explanations, process repeats until consensus is reached
- Core assumption: Both human and LLM can reach meaningful consensus through iterative refinement
- Evidence anchors: [section] "Step 3 in Figure 1 illustrates a cycle of the discussion process. First an HC reviews the codes and edits the codes if necessary. If the HC modifies any code, he/she states the changes and provides the rationales behind them."

### Mechanism 3
- Claim: Using partial data for codebook development addresses LLM input size limitations while maintaining coding quality
- Mechanism: Randomly sampled subset of responses is used to develop the codebook, which is then applied to the full dataset
- Core assumption: A representative subset can capture the essential themes and patterns needed for the full dataset
- Evidence anchors: [section] "Considering the LLM input size limitations, we randomly divided the responses of PM into two separate pools and applied one pool for codebook development"

## Foundational Learning

- Concept: Thematic Analysis (TA) methodology
  - Why needed here: The framework is designed to automate and enhance a specific qualitative research method
  - Quick check question: Can you explain the six phases of thematic analysis as proposed by Braun and Clarke?

- Concept: In-context learning (ICL) and few-shot prompting
  - Why needed here: The LLM's ability to learn from exemplars is central to the initial code generation step
  - Quick check question: What is the difference between zero-shot, one-shot, and few-shot learning in the context of LLMs?

- Concept: Inter-annotator agreement metrics (Cohen's kappa)
  - Why needed here: Used to evaluate the quality of the LLM-human collaboration by measuring coding consistency
  - Quick check question: What does a Cohen's kappa value of 0.87 indicate about the agreement between two coders?

## Architecture Onboarding

- Component map:
  - Human Coder (HC) -> LLM (MC) -> Discussion Loop -> Final Codebook
  - Data Pipeline -> Subset Selection -> Codebook Development -> Full Dataset Coding
  - Evaluation Module -> IAA Calculation -> Cosine Similarity

- Critical path:
  1. HC familiarizes with data and creates exemplars
  2. LLM generates initial codes using ICL
  3. Iterative discussion loop refines codes into codebook
  4. HC and LLM apply codebook to full dataset
  5. Calculate IAA and similarity metrics

- Design tradeoffs:
  - Using partial data vs. full data for codebook development (efficiency vs. comprehensiveness)
  - Temperature setting of 0 for reproducibility vs. potential loss of creative code generation
  - Manual exemplar creation vs. automated exemplar generation

- Failure signatures:
  - Low IAA values (<0.7) indicating poor alignment between HC and LLM
  - High cosine similarity but low IAA suggesting semantic alignment but coding disagreement
  - LLM consistently rejecting HC modifications without valid reasoning

- First 3 experiments:
  1. Run framework with single response to validate exemplar-based code generation works as expected
  2. Test discussion loop with a small set of codes to ensure consensus can be reached
  3. Compare IAA and cosine similarity when using different percentages of data for codebook development (10%, 50%, 100%)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the human-LLM collaboration framework handle cases where the LLM's initial codes are significantly different from the human coder's expectations?
- Basis in paper: [explicit] The paper mentions that the framework involves iterative discussions between the human coder and LLM to refine codes, but doesn't specify how major discrepancies are handled.
- Why unresolved: The paper doesn't provide details on the mechanism for resolving major disagreements between the human coder and LLM.
- What evidence would resolve it: Detailed description of the conflict resolution process when the LLM's initial codes differ substantially from the human coder's expectations.

### Open Question 2
- Question: What is the impact of different prompt formulations on the quality of codes generated by the LLM?
- Basis in paper: [inferred] The paper mentions using in-context learning and specific prompts but doesn't explore the effect of different prompt formulations on the output quality.
- Why unresolved: The study doesn't compare results using different prompt formulations to determine if certain prompts lead to better code generation.
- What evidence would resolve it: Comparative analysis of code quality using different prompt formulations or prompt engineering techniques.

### Open Question 3
- Question: How does the framework perform when applied to datasets with different characteristics (e.g., length of responses, complexity of topics, cultural context)?
- Basis in paper: [inferred] The study only uses two specific datasets (music listening experiences and password manager usage) without exploring performance across diverse dataset characteristics.
- Why unresolved: The generalizability of the framework to different types of qualitative data is not established.
- What evidence would resolve it: Application of the framework to multiple datasets with varying characteristics and comparison of performance metrics.

## Limitations
- Manual exemplar creation by human coders introduces subjectivity and may not scale well for larger datasets or different domains
- Iterative discussion mechanism lacks detailed specification of consensus criteria and may not generalize across coders with different backgrounds
- Partial data approach for codebook development raises questions about representativeness and potential bias in the sampled subset

## Confidence
- High confidence in the basic mechanism: Strong quantitative evidence from IAA values (κ = 0.87 and 0.81) and cosine similarities (0.8864 and 0.8895)
- Medium confidence in generalizability: Limited to two case studies with specific response volumes (35 and 40 responses for final coding)
- Low confidence in scalability: Reliance on manual exemplar creation and iterative discussion may not work efficiently for very large datasets

## Next Checks
1. **Cross-domain replication test**: Apply the framework to a third, substantially different domain (e.g., healthcare patient experiences or educational feedback) with at least 50 responses to assess generalizability beyond the two demonstrated case studies.

2. **Consensus mechanism stress test**: Conduct a controlled experiment where two different human coders use the framework independently with the same dataset to determine if the discussion loop consistently converges to similar codebooks, or if coder background significantly affects outcomes.

3. **Exemplar scaling experiment**: Systematically vary the number and diversity of exemplars (2, 4, 8, 16) in the in-context learning prompts to quantify the relationship between exemplar quantity/quality and coding performance, establishing guidelines for optimal exemplar selection.