---
ver: rpa2
title: 'PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese'
arxiv_id: '2307.08247'
source_url: https://arxiv.org/abs/2307.08247
tags:
- features
- attention
- visual
- dataset
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for Visual Question Answering
  (VQA) in Vietnamese, a task that requires a machine to answer a given question based
  on visual information from a given image. The authors introduce the Parallel Attention
  Transformer (PAT) which consists of a Hierarchical Linguistic Feature Extractor,
  an Image Embedding module, a Parallel Attention module, and an Answer Selector.
---

# PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese

## Quick Facts
- arXiv ID: 2307.08247
- Source URL: https://arxiv.org/abs/2307.08247
- Reference count: 40
- Key outcome: PAT achieves best accuracy on ViVQA dataset, improving ~6% over SAAA and ~3% over MCAN

## Executive Summary
This paper introduces PAT (Parallel Attention Transformer), a novel method for Visual Question Answering in Vietnamese. The approach leverages Vietnamese language characteristics through a Hierarchical Linguistic Feature Extractor and uses a Parallel Attention module for effective multimodal fusion. PAT achieves state-of-the-art performance on the ViVQA dataset, demonstrating the effectiveness of exploiting Vietnamese grammar and context advantages in VQA tasks.

## Method Summary
PAT consists of four main components: a Hierarchical Linguistic Feature Extractor that processes Vietnamese questions using FastText embeddings and 1D CNNs to capture unigram, bigram, trigram, and 4-gram features; an Image Embedding module using pre-trained VinVL model for region features; a Parallel Attention module that performs cross-and-parallel attention with four attention components; and an Answer Selector that uses attribute reduction to combine visual and linguistic features before classification. The model is trained with Adam optimizer (learning rate 0.01, batch size 64) and evaluated using Exact Match accuracy on the ViVQA dataset.

## Key Results
- PAT achieves 0.6055 accuracy on ViVQA, outperforming all baselines
- 6% improvement over SAAA baseline
- 3% improvement over MCAN baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical Linguistic Feature Extractor leverages Vietnamese grammar and context better than a simple LSTM
- Mechanism: Extracts unigram features via FastText, then applies 1D CNNs with kernel sizes 1, 2, 3, and 4 to create unigram, bigram, trigram, and 4-gram features, which are summed to produce hierarchical features
- Core assumption: Vietnamese grammar and context benefits from capturing n-gram dependencies beyond unigrams
- Evidence anchors:
  - [abstract]: "to take into account the advantages of grammar and context in Vietnamese, we propose the Hierarchical Linguistic Features Extractor instead of using an LSTM network"
  - [section III-A]: Describes the CNN-based n-gram extraction and summing process
  - [corpus]: No direct evidence in corpus neighbors; this appears to be novel to the paper
- Break condition: If Vietnamese does not benefit from n-gram features or if the CNN projection layer doesn't improve latent space alignment

### Mechanism 2
- Claim: Parallel Attention module performs cross-and-parallel attention for effective multimodal fusion
- Mechanism: Uses four attention components - vision-to-language attention, language-to-vision attention, self-attention for vision, and self-attention for language, all using multi-head attention
- Core assumption: Separate cross-attention directions and self-attention improve information fusion compared to single-hop attention
- Evidence anchors:
  - [abstract]: "Parallel Attention module performs cross-and-parallel attention to fuse information from images and questions"
  - [section III-C]: Details the four attention components and their roles
  - [corpus]: No direct evidence in corpus neighbors; this appears to be novel to the paper
- Break condition: If the parallel structure doesn't improve over simpler attention mechanisms or if training becomes unstable

### Mechanism 3
- Claim: Answer Selector uses attribute reduction to denoise and combine visual and linguistic features before final classification
- Mechanism: Applies MLP with softmax to re-weight visual and linguistic features, then element-wise multiplies and sums to produce fused features
- Core assumption: Attribute reduction helps filter noise and combine complementary information from both modalities
- Evidence anchors:
  - [section III-D]: Describes the attribute reduction and candidate selection phases with equations
  - [section IV-E]: Shows PAT achieving 0.6055 accuracy vs 0.5711 for MCAN, suggesting the selector design contributes to performance
  - [corpus]: No direct evidence in corpus neighbors; this appears to be adapted from MCAN but with novel integration
- Break condition: If the attribute reduction doesn't improve accuracy or if it overfits on small datasets

## Foundational Learning

- Concept: Vietnamese language characteristics and morphology
  - Why needed here: The model exploits Vietnamese grammar and context advantages; understanding these helps justify the hierarchical extractor design
  - Quick check question: Why might Vietnamese benefit more from n-gram features than English?

- Concept: Transformer attention mechanisms and multi-head attention
  - Why needed here: The core of PAT is the Parallel Attention module which uses multi-head attention; understanding this is essential for implementation
  - Quick check question: What is the difference between cross-attention and self-attention in transformer architectures?

- Concept: Visual feature extraction with region-based models
  - Why needed here: PAT uses VinVL pre-trained model for region features; understanding region-based vs grid features helps with feature engineering choices
  - Quick check question: What advantages do region features have over grid features for VQA tasks?

## Architecture Onboarding

- Component map: FastText embeddings → Hierarchical Linguistic Feature Extractor → VinVL region features → Parallel Attention module (4 layers) → Answer Selector → Output
- Critical path: Vietnamese question processing → Visual feature extraction → Parallel attention fusion → Answer selection
- Design tradeoffs: Hierarchical extractor adds complexity but leverages language structure; parallel attention adds parameters but may improve fusion; attribute reduction adds denoising but may lose information
- Failure signatures: If accuracy drops significantly without LSTM or hierarchical extractor (as shown in ablation), those components are critical; if parallel attention doesn't improve over single-hop, the design may be over-engineered
- First 3 experiments:
  1. Compare PAT with and without hierarchical linguistic feature extractor on ViVQA dataset
  2. Compare PAT with and without the 1-size kernel CNN projection layer
  3. Compare PAT against SAAA and MCAN on ViVQA to verify claimed improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of PAT change if Large Language Models (LLMs) were incorporated for Vietnamese linguistic feature extraction instead of FastText embeddings?
- Basis in paper: [explicit] The paper states that future work includes investigating the impact of using LLMs on the results of VQA methods.
- Why unresolved: The paper does not experiment with LLM-based embeddings and only uses FastText for word embeddings.
- What evidence would resolve it: Experimental results comparing PAT performance using FastText vs. various LLM embeddings (e.g., PhoBERT, ViBERT) on the ViVQA dataset.

### Open Question 2
- Question: What is the impact of different pre-trained image models (beyond VinVL) on PAT's performance in the Vietnamese VQA setting?
- Basis in paper: [explicit] The paper mentions that future work includes finding the most effective multimodal structure for the ViVQA dataset.
- Why unresolved: The paper only uses VinVL for image feature extraction without comparing it to other models like CLIP or OSCAR.
- What evidence would resolve it: Comparative experiments using PAT with different pre-trained image models on the ViVQA dataset.

### Open Question 3
- Question: How does PAT perform on open-ended Vietnamese VQA datasets compared to answer selection datasets?
- Basis in paper: [explicit] The paper notes that the proposed method can be evaluated on EVJVQA and OpenViVQA datasets, which are open-ended.
- Why unresolved: The paper only evaluates PAT on the answer selection ViVQA dataset and does not test it on open-ended datasets.
- What evidence would resolve it: Experimental results of PAT on EVJVQA and OpenViVQA datasets with performance metrics for open-ended answers.

## Limitations
- Lack of ablation studies to isolate contribution of each component
- Specific architecture details of Answer Selector not fully specified
- Dataset size (ViVQA) may limit generalization claims

## Confidence
- High confidence: Overall architecture design and training methodology
- Medium confidence: Claimed performance improvements due to lack of detailed ablation studies
- Low confidence: Specific implementation details of Answer Selector module and exact CNN parameters

## Next Checks
1. Perform ablation studies removing each key component (hierarchical extractor, parallel attention module, attribute reduction) to quantify individual contributions
2. Test PAT on multilingual VQA datasets beyond ViVQA to assess generalization across languages
3. Compare PAT against transformer-based VQA models using English datasets to evaluate if improvements are language-specific or architecture-driven