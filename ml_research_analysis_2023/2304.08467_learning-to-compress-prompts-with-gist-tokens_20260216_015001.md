---
ver: rpa2
title: Learning to Compress Prompts with Gist Tokens
arxiv_id: '2304.08467'
source_url: https://arxiv.org/abs/2304.08467
tags:
- gist
- tokens
- prompt
- grave
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Gisting is a method to compress prompts into shorter "gist" tokens,
  reducing compute and memory costs during inference. It works by training a language
  model to predict these gist tokens using modified attention masks that prevent later
  tokens from attending to earlier ones.
---

# Learning to Compress Prompts with Gist Tokens

## Quick Facts
- arXiv ID: 2304.08467
- Source URL: https://arxiv.org/abs/2304.08467
- Reference count: 26
- Primary result: Up to 26x prompt compression with 40% FLOPs reduction and 4.2% wall time speedup while maintaining output quality

## Executive Summary
Gisting is a method to compress prompts into shorter "gist" tokens, reducing compute and memory costs during inference. It works by training a language model to predict these gist tokens using modified attention masks that prevent later tokens from attending to earlier ones. This allows the model to compress prompts and cache the gist representations for reuse. On decoder (LLaMA-7B) and encoder-decoder (FLAN-T5-XXL) models, gisting achieves significant efficiency gains while maintaining output quality similar to the original models.

## Method Summary
Gisting compresses arbitrary prompts into a smaller set of virtual "gist" tokens by modifying Transformer attention masks during instruction finetuning. The model learns to predict gist tokens that capture essential information from the prompt, then caches these gist representations for efficient reuse during inference. This approach works by preventing tokens after gist tokens from attending to tokens before gist tokens, forcing the model to compress prompt information into the gist tokens. The method can be trained with no additional cost over standard instruction finetuning by simply modifying the attention masks to enforce prompt compression.

## Key Results
- Up to 26x compression of prompts on LLaMA-7B and FLAN-T5-XXL models
- 40% FLOPs reductions and 4.2% wall time speedups
- Storage savings through gist token caching
- Maintained output quality similar to original models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gisting works by training the model to compress prompt information into fewer gist tokens, reducing computational cost during inference.
- Mechanism: Modified attention masks prevent tokens after gist tokens from attending to tokens before gist tokens, forcing the model to compress prompt information into the gist tokens.
- Core assumption: The model can learn to compress arbitrary prompts into gist tokens during instruction finetuning without additional training cost.
- Evidence anchors:
  - [abstract] "Gist models can be trained with no additional cost over standard instruction finetuning by simply modifying Transformer attention masks to encourage prompt compression."
  - [section 2] "using the LM itself as the gist predictor G...allows us to learn gisting by simply doing standard instruction finetuning while modifying the Transformer attention masks to enforce prompt compression."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.435, average citations=0.0. Top related titles include "Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression" which supports the core premise of prompt compression.

### Mechanism 2
- Claim: Gisting generalizes to unseen prompts by amortizing the cost of learning task-specific prefixes across a distribution of tasks.
- Mechanism: The gist model learns to predict gist tokens zero-shot given only the prompt, using a meta-learning approach rather than learning prefixes via gradient descent for each task.
- Core assumption: The model can generalize gist representations to novel tasks without additional training.
- Evidence anchors:
  - [abstract] "Since gist tokens are much shorter than the full prompt, gisting allows arbitrary prompts to be compressed, cached, and reused for compute efficiency...we expect G to generalize to unseen tasks."
  - [section 2.1] "Instead, we propose gisting...which compresses arbitrary prompts into a smaller set of virtual 'gist' tokens...Gisting is a different approach that amortizes both (1) the inference-time cost of conditioning pLM on t and (2) the train-time cost of learning a new ptLM for each t."
  - [corpus] The presence of papers like "Long Context In-Context Compression by Getting to the Gist of Gisting" suggests ongoing research into generalization capabilities.

### Mechanism 3
- Claim: Gisting provides efficiency gains through reduced FLOPs and storage costs during inference.
- Mechanism: By compressing prompts into fewer gist tokens, the model reduces the number of tokens that need to be processed during self-attention, leading to computational savings.
- Core assumption: The computational savings from reduced token count outweigh any overhead from generating gist tokens.
- Evidence anchors:
  - [abstract] "On decoder (LLaMA-7B) and encoder-decoder (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings."
  - [section 6] "Gist caching improves significantly over unoptimized models, with 40% FLOPs savings and 4-7% lower wall clock time across both models."
  - [corpus] Limited corpus evidence specifically on efficiency gains, but related work on "Discrete Prompt Compression with Reinforcement Learning" suggests this is an active area of research.

## Foundational Learning

- Concept: Transformer attention mechanisms and their computational complexity
  - Why needed here: Understanding how attention masks can be modified to force prompt compression is crucial for implementing gisting
  - Quick check question: How does the computational complexity of self-attention scale with input length, and how might this be affected by prompt compression?

- Concept: Instruction finetuning and its role in adapting language models to follow instructions
  - Why needed here: Gisting is implemented as part of instruction finetuning, so understanding this process is essential
  - Quick check question: What are the key differences between standard pretraining and instruction finetuning, and how might these affect the ability to learn prompt compression?

- Concept: Context distillation and its relationship to prompt compression
  - Why needed here: Gisting can be viewed as a form of meta-context distillation, so understanding this concept helps in grasping the broader context
  - Quick check question: How does context distillation differ from standard knowledge distillation, and what are the implications for learning to compress prompts?

## Architecture Onboarding

- Component map:
  Input processing: Task instruction (t), optional input (x), and gist tokens (g1...gk) -> Modified attention mechanism with gist tokens -> Autoregressive decoding from compressed prompt representation -> Caching of gist token activations

- Critical path:
  1. Tokenize and concatenate (t, g1...gk, x)
  2. Apply modified attention mask
  3. Generate gist tokens during training
  4. Cache gist token activations for inference
  5. Decode output using gist representation

- Design tradeoffs:
  - Number of gist tokens (k): More tokens allow better compression but increase computational cost
  - Attention mask modifications: Stricter masking may improve compression but could hurt performance
  - Training data: Broader task distribution improves generalization but increases training cost

- Failure signatures:
  - Poor compression: Model fails to capture essential information in gist tokens, leading to degraded performance
  - Overfitting: Model learns task-specific gist representations that don't generalize to unseen prompts
  - Computational overhead: Generating gist tokens becomes a bottleneck, negating efficiency gains

- First 3 experiments:
  1. Train gist model with single gist token and compare to positive control on seen tasks
  2. Evaluate generalization to unseen tasks within training distribution
  3. Test performance on out-of-distribution human-crafted prompts to assess true generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of gist models vary with different instruction fine-tuning datasets, such as FLAN or P3, compared to the Alpaca+ dataset used in the paper?
- Basis in paper: The paper mentions that Alpaca+ is a noisy and imperfect dataset but shows comparable performance to the original models. It also notes that other academic datasets like FLAN and P3 are narrower in their task distributions compared to Alpaca+.
- Why unresolved: The paper does not provide a direct comparison of gist model performance using different instruction fine-tuning datasets. It would be valuable to understand how the choice of dataset affects the model's ability to compress and generalize prompts.
- What evidence would resolve it: Conducting experiments using gist models trained on different datasets (e.g., FLAN, P3) and comparing their performance on the same evaluation metrics would provide insights into the impact of dataset choice on gist model effectiveness.

### Open Question 2
- Question: What are the potential benefits and limitations of using parameter-efficient gisting methods, such as retrofitting an existing frozen LM, compared to training the entire LM with gist tokens?
- Basis in paper: The paper discusses the possibility of parameter-efficient gisting by training a smaller compression model or retrofitting an existing, frozen LM to generate gist tokens.
- Why unresolved: The paper does not explore the practical implementation and performance of parameter-efficient gisting methods. Understanding the trade-offs between these methods and full training would be valuable for real-world applications.
- What evidence would resolve it: Implementing and evaluating parameter-efficient gisting methods on various tasks and comparing their performance, efficiency, and generalization capabilities to full training would provide insights into their potential benefits and limitations.

### Open Question 3
- Question: How does the performance of gist models change when compressing longer prompts, such as entire documents or k-shot prompts for large k, that may not fit into a single context window?
- Basis in paper: The paper mentions that the largest compute and efficiency savings from gisting will result from compressing very long prompts, but it is limited by the size and breadth of current instruction following datasets.
- Why unresolved: The paper does not explore the performance of gist models on longer prompts or investigate the potential for recursive compression of gist tokens.
- What evidence would resolve it: Conducting experiments with gist models on longer prompts and evaluating their performance, efficiency, and generalization capabilities would provide insights into the potential of gist models for handling longer sequences.

### Open Question 4
- Question: How does gist pretraining, which involves learning to compress arbitrary spans of natural language before learning prompt compression, affect the performance of gist models?
- Basis in paper: The paper suggests that gist pretraining could improve gist compression performance by first learning to compress arbitrary spans of natural language before then learning prompt compression.
- Why unresolved: The paper does not provide any experimental results or evidence on the effectiveness of gist pretraining. Understanding the impact of gist pretraining on model performance would be valuable for improving gist model training.
- What evidence would resolve it: Implementing gist pretraining objectives and evaluating the performance of gist models trained with and without gist pretraining on various tasks would provide insights into the potential benefits of gist pretraining.

### Open Question 5
- Question: How does the choice of the number of gist tokens (k) affect the performance of gist models, and is there an optimal value of k that balances prompt compression and output quality?
- Basis in paper: The paper mentions that models were generally insensitive to the number of gist tokens k, and that having too many gist tokens might hurt performance due to overfitting.
- Why unresolved: The paper does not provide a detailed analysis of the impact of the number of gist tokens on model performance or identify an optimal value of k.
- What evidence would resolve it: Conducting experiments with gist models using different values of k and evaluating their performance on various tasks would provide insights into the optimal number of gist tokens for balancing prompt compression and output quality.

## Limitations
- Generalization Performance: Significant performance degradation on truly out-of-distribution prompts, with only 11% win rate against positive control for unseen tasks
- Efficiency Claims: Modest 4.2% wall time speedup relative to 40% FLOPs reduction suggests computational savings may be partially offset by other factors
- Encoder-Decoder Architecture: Less effective for encoder-decoder models, with 50 gist tokens performing worse than 1 gist token

## Confidence

**High Confidence**: The core mechanism of prompt compression through modified attention masks is well-supported by both theoretical reasoning and empirical results. The experimental setup is clearly described, and the efficiency metrics (FLOPs reduction, storage savings) are straightforward to measure and validate.

**Medium Confidence**: The generalization claims to unseen tasks are supported by experimental evidence but show significant performance degradation. The results on out-of-distribution prompts raise questions about the robustness of the approach across diverse task distributions.

**Low Confidence**: The relationship between prompt compression and output quality preservation across all possible prompt types is not fully established. The paper focuses on instruction-following tasks, and it's unclear how gisting would perform on other prompt types or domains.

## Next Checks

1. **Generalization Stress Test**: Evaluate gist models on a broader set of truly out-of-distribution prompts, including prompts from different domains (creative writing, code generation, reasoning tasks) to assess the robustness of the compression mechanism across diverse task types.

2. **Efficiency Break-even Analysis**: Profile the complete inference pipeline to identify where the 4.2% wall time speedup breaks down. Measure the contribution of gist token generation time, KV cache access patterns, and memory bandwidth to isolate bottlenecks that prevent achieving the full 40% FLOPs reduction in practice.

3. **Attention Mask Ablation Study**: Systematically vary the strictness of the attention mask modifications (e.g., allowing limited attention to earlier tokens, using different masking patterns) to identify the optimal tradeoff between compression effectiveness and information retention, particularly for encoder-decoder architectures.