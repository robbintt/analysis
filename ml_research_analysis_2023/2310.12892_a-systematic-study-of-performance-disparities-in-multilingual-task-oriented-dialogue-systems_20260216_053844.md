---
ver: rpa2
title: A Systematic Study of Performance Disparities in Multilingual Task-Oriented
  Dialogue Systems
arxiv_id: '2310.12892'
source_url: https://arxiv.org/abs/2310.12892
tags:
- data
- performance
- training
- language
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically analyzes performance disparities in multilingual
  task-oriented dialogue (TOD) systems. It introduces new measures of absolute and
  relative equivalence in system performance to quantify disparities across and within
  languages.
---

# A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems

## Quick Facts
- arXiv ID: 2310.12892
- Source URL: https://arxiv.org/abs/2310.12892
- Authors: 
- Reference count: 40
- Primary result: Introduces absolute and relative equivalence metrics to quantify multilingual TOD performance disparities, showing complex tasks require more in-language data and strategic data collection improves performance.

## Executive Summary
This paper presents a systematic analysis of performance disparities in multilingual task-oriented dialogue (TOD) systems across four languages: Arabic, English, French, and Turkish. The study introduces new measures of absolute and relative equivalence in system performance to quantify disparities both across languages and within languages. Through controlled experiments on the MULTI3WOZ dataset, the research demonstrates that performance disparities exist even with equal amounts of parallel training data, primarily due to adaptation bias (lack of in-language TOD data) and intrinsic bias (mPLM underrepresentation of target languages). The study also identifies that task complexity significantly impacts data requirements, with simpler tasks like intent detection achieving near-English performance with less data, while complex tasks like dialogue state tracking and natural language generation require substantially more in-language data. Additionally, the research shows that strategic data collection maximizing n-gram diversity can improve performance without additional annotation costs.

## Method Summary
The study fine-tunes multilingual pretrained language models (XLM-R, mT5, Flan-T5) on four TOD tasks (intent detection, slot labeling, dialogue state tracking, and natural language generation) using the MULTI3WOZ dataset. The experimental design varies training data amounts (1% to 100%) for each language and task, comparing zero-shot, translate-train, and fully supervised scenarios. Models are evaluated using standard metrics (Accuracy, F1, JGA, Turn Accuracy, BLEU, ROUGE, METEOR) and the newly proposed absolute and relative θ-equivalence measures. The study also tests data selection strategies, particularly the Max N-gram approach that maximizes trigram and lexical coverage. Experiments are conducted on A100 GPUs with 80GB memory using HuggingFace Transformers implementations.

## Key Results
- Performance disparities persist even with equal parallel training data due to adaptation bias and intrinsic bias in mPLMs
- Simpler TOD tasks (intent detection, slot labeling) reach near-English performance with less than 20% of training data
- More complex tasks (dialogue state tracking, natural language generation) require approximately 50-80% of training data for relative equivalence
- Strategic data collection maximizing n-gram diversity improves performance without additional annotation costs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Performance disparities exist even with equal parallel data due to adaptation bias and intrinsic bias.
- **Mechanism**: mPLMs trained on web-scale data may have insufficient exposure to low-resource language dialogue patterns, so fine-tuning with limited in-language TOD data cannot fully bridge the gap to high-resource languages like English.
- **Core assumption**: Parallel datasets are truly parallel in structure, flow, and quality, but still underrepresent linguistic patterns unique to each language.
- **Evidence anchors**:
  - [abstract] "We empirically prove the existence of the adaptation and intrinsic biases in current TOD systems: e.g., TOD systems trained for Arabic or Turkish using annotated TOD data fully parallel to English TOD data still exhibit diminished TOD task performance."
  - [section 3.2] Defines absolute and relative equivalence in performance.
  - [corpus] Neighbor paper "Transfer-Free Data-Efficient Multilingual Slot Labeling" also discusses data scarcity and bias in multilingual TOD.
- **Break condition**: If the mPLM training corpus includes robust, balanced dialogue data for all target languages, or if fine-tuning uses sufficiently large in-language TOD data.

### Mechanism 2
- **Claim**: Simpler TOD tasks reach near-English performance with less data; complex tasks need more.
- **Mechanism**: Task complexity determines the amount of in-language data required for relative θ-equivalence. Simple tasks have fewer output classes and less dependence on language-specific nuances, while DST and NLG require richer linguistic context and domain knowledge.
- **Core assumption**: Task complexity correlates with the number of distinct output labels and the degree of language-specific phrasing required.
- **Evidence anchors**:
  - [section 5] "We find that simpler tasks like ID and SL reach relative equivalence with less than 20% of the training dataset... More complex tasks such as DST and NLG require more data, with relative equivalence being achieved at approximately 50% and 80% of the training data, respectively."
  - [corpus] Neighbor paper "Spec-TOD: A Specialized Instruction-Tuned LLM Framework" discusses task complexity in low-resource scenarios.
- **Break condition**: If task definitions or output ontologies change, or if mPLMs are adapted specifically to reduce complexity in complex tasks.

### Mechanism 3
- **Claim**: Strategic data collection maximizing n-gram diversity improves performance without extra annotation cost.
- **Mechanism**: Selecting dialogues that maximize trigram coverage increases lexical and syntactic diversity, providing better generalization across language patterns.
- **Core assumption**: N-gram diversity correlates with coverage of dialogue patterns and reduces overfitting to repetitive or narrow patterns.
- **Evidence anchors**:
  - [section 5] "Particularly, the Max N-gram strategy, which involves creating a target dataset D TGT f ew from a subset D SRC f ew that maximises both trigram and lexical coverage, leads to improved performance compared to the random baseline."
  - [corpus] Neighbor paper "Transfer-Free Data-Efficient Multilingual Slot Labeling" also highlights the importance of data diversity for slot labeling.
- **Break condition**: If the relationship between n-gram diversity and task performance does not hold for a specific language or domain, or if the cost of measuring diversity exceeds the benefit.

## Foundational Learning

- **Concept**: Multilingual pretrained language models (mPLMs) and their bias profiles.
  - Why needed here: Understanding how mPLMs encode and transfer linguistic knowledge is essential to diagnose why performance disparities occur.
  - Quick check question: What are the main sources of bias in mPLMs, and how do they differ between high- and low-resource languages?

- **Concept**: Task-oriented dialogue (TOD) system architecture and evaluation metrics.
  - Why needed here: Knowing the standard TOD pipeline (NLU, DST, NLG) and evaluation metrics (JGA, F1, BLEU) is required to interpret performance disparities.
  - Quick check question: How do evaluation metrics like JGA, F1, and BLEU capture different aspects of system performance, and why might they diverge across languages?

- **Concept**: Cross-lingual transfer and few-shot learning in dialogue systems.
  - Why needed here: Understanding how models leverage source language data to improve target language performance is central to the study's experimental design.
  - Quick check question: What are the limitations of zero-shot and few-shot cross-lingual transfer in dialogue tasks, and when does in-language data become necessary?

## Architecture Onboarding

- **Component map**: MULTI3WOZ dataset -> mPLM backbone (XLM-R, mT5, Flan-T5) -> Task-specific heads (intent detection, slot labeling, DST, NLG) -> Evaluation pipeline (automatic metrics per task)

- **Critical path**: Load parallel dialogues → Encode with mPLM → Apply task head → Evaluate with metric → Compare across languages

- **Design tradeoffs**:
  - Model size vs. training time vs. cross-lingual transfer effectiveness
  - Data diversity vs. annotation cost in few-shot scenarios
  - Choice of evaluation metric vs. alignment with user experience

- **Failure signatures**:
  - Large gap between source and target language performance despite parallel data → adaptation or intrinsic bias
  - Performance plateau with increasing data → task complexity ceiling or model capacity limit
  - Inconsistent results across metrics → misalignment between metric and task goals

- **First 3 experiments**:
  1. Train ID and SL models on parallel data for each language and compare accuracy/F1 to English baseline
  2. Vary training data size (1%, 5%, 10%, ..., 100%) for each language and plot relative θ-equivalence curves
  3. Apply Max N-gram data selection strategy and compare performance to random sampling baseline for SL and DST

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance disparity between languages change when using monolingual PLMs instead of multilingual ones?
- **Basis in paper**: [explicit] The paper mentions that mPLMs still lag behind their monolingual counterparts, citing an example where substituting a multilingual model with a monolingual Arabic-BERTlarge model improved Arabic SL performance by 9.7 F1 points.
- **Why unresolved**: The paper only provides one specific example of a monolingual vs multilingual comparison. A systematic study comparing all languages and tasks with monolingual models would provide more comprehensive insights.
- **What evidence would resolve it**: A comprehensive experimental study comparing all languages and tasks using both monolingual and multilingual PLMs, measuring performance disparities in each case.

### Open Question 2
- **Question**: What is the optimal strategy for creating validation sets in few-shot learning scenarios?
- **Basis in paper**: [explicit] The paper compares proportional vs full validation set strategies in few-shot setups, finding that full validation sets yield only marginal improvements. However, it doesn't explore other potential strategies.
- **Why unresolved**: The paper only compares two validation set strategies and doesn't explore the full space of possibilities or provide definitive guidelines.
- **What evidence would resolve it**: Experimental comparison of various validation set creation strategies (e.g., stratified sampling, curriculum-based selection) across different few-shot scenarios and tasks.

### Open Question 3
- **Question**: How do performance disparities change when considering spoken vs written input modalities?
- **Basis in paper**: [explicit] The paper acknowledges that its analysis is limited to text input only, noting that fully inclusive dialogue systems should consider other modalities like spoken and sign languages.
- **Why unresolved**: The study only analyzes text-based TOD systems and doesn't address potential disparities that might arise from different input modalities.
- **What evidence would resolve it**: Comparative analysis of performance disparities across languages when using spoken vs written input, potentially incorporating ASR systems and measuring their impact on overall system performance.

## Limitations
- The study assumes parallel dialogues are truly parallel across languages, but cultural or linguistic differences in dialogue flow may not be fully captured
- Intrinsic bias in mPLMs is inferred from observed performance gaps, but the exact contribution of training corpus imbalances versus fine-tuning data scarcity is not directly quantified
- The N-gram diversity strategy's effectiveness is demonstrated empirically, but its relationship to task-specific linguistic patterns or semantic coverage is not rigorously analyzed

## Confidence
- **High**: Simpler tasks (ID, SL) require less in-language data for equivalence; more complex tasks (DST, NLG) need more
- **High**: Strategic data collection maximizing n-gram diversity improves performance without extra annotation cost
- **Medium**: Performance disparities persist even with equal parallel data due to adaptation and intrinsic bias
- **Low**: The exact proportion of performance gap attributable to mPLM training data bias versus fine-tuning data scarcity

## Next Checks
1. Conduct ablation studies to isolate the effect of mPLM training corpus bias by fine-tuning with in-domain dialogue data for each language and measuring the residual performance gap
2. Analyze the linguistic and semantic coverage of the Max N-gram selection strategy by comparing its dialogue content to random baselines using topic modeling or semantic similarity metrics
3. Extend the study to additional languages and domains to verify whether observed performance disparities and data efficiency trends generalize beyond the four languages in MULTI3WOZ