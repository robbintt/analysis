---
ver: rpa2
title: Scaling Riemannian Diffusion Models
arxiv_id: '2310.20030'
source_url: https://arxiv.org/abs/2310.20030
tags:
- riemannian
- heat
- kernel
- matching
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of scaling Riemannian diffusion
  models to high-dimensional manifolds by improving the computational efficiency and
  accuracy of the heat kernel, which is central to the denoising score matching objective.
  The authors propose leveraging the symmetric space structure of common manifolds
  to derive more efficient heat kernel estimators, including eigenfunction expansions
  and improved small-time approximations like the Schwinger-Dewitt and sum over paths
  formulas.
---

# Scaling Riemannian Diffusion Models

## Quick Facts
- arXiv ID: 2310.20030
- Source URL: https://arxiv.org/abs/2310.20030
- Reference count: 40
- Primary result: The paper enables training Riemannian diffusion models on high-dimensional tasks like QCD densities on SU(3) lattices (128 dimensions) and hyperspherical embeddings from contrastive learning (127 dimensions) through improved heat kernel computation.

## Executive Summary
This paper addresses the computational bottleneck in Riemannian diffusion models by leveraging the symmetric space structure of common manifolds. The authors develop efficient heat kernel computation methods that dramatically reduce the computational cost and numerical instability that previously limited these models to low dimensions. By combining eigenfunction expansions for large time steps with improved small-time approximations (Schwinger-Dewitt and sum over paths formulas), they enable training on previously intractable high-dimensional tasks. The method achieves significant improvements on standard benchmarks while scaling to real-world applications in physics and contrastive learning.

## Method Summary
The authors improve Riemannian diffusion models by exploiting the symmetric space structure of manifolds to compute the heat kernel more efficiently. They develop a unified estimator that uses eigenfunction expansions for large time steps and refined small-time approximations (Schwinger-Dewitt and sum over paths formulas) for small time steps. For sampling, they introduce a manifold ODE-based approach using the probability flow ODE on the maximal torus, avoiding the numerical drift and computational cost of geodesic random walks. The method enables denoising score matching training with exact heat kernel computation and sampling, allowing the models to scale to high-dimensional tasks previously infeasible.

## Key Results
- Achieves improved performance over sliced score matching on low-dimensional Earth science datasets (negative log likelihood reduction)
- Successfully trains Riemannian diffusion models on QCD densities on SU(3) lattices (128 dimensions)
- Models hyperspherical embeddings from contrastive learning on CIFAR-100 (127 dimensions) with improved out-of-distribution detection
- Demonstrates significant computational speedups through heat kernel approximations, reducing eigenfunction evaluations by orders of magnitude

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The symmetric space structure allows efficient computation of the heat kernel via eigenfunction expansions restricted to the maximal torus.
- Mechanism: By leveraging the algebraic structure of Riemannian symmetric spaces, the high-dimensional heat kernel computation is reduced to a lower-dimensional problem on the maximal torus. This reduces the dimensionality from n to O(1) or O(n) and enables the use of stable eigenfunction expansions.
- Core assumption: The manifold of interest is (or is diffeomorphic to) a Riemannian symmetric space, and the maximal torus structure is well-defined and computationally tractable.
- Evidence anchors:
  - [abstract]: "Our key observation is that most relevant manifolds are symmetric spaces, which are much more amenable to computation."
  - [section]: "By restricting our analysis to Riemannian symmetric spaces [...] we can make substantial improvements."
  - [corpus]: Weak—no direct citation, but the claim aligns with the paper's core contribution.
- Break condition: If the manifold is not symmetric or lacks a tractable maximal torus, the eigenfunction expansion method fails.

### Mechanism 2
- Claim: Improved small-time approximations (Schwinger-Dewitt and sum over paths) control numerical instability and reduce eigenfunction evaluations.
- Mechanism: For small time steps, the heat kernel is approximated using curvature-aware formulas (Schwinger-Dewitt) or exact path-based formulas (sum over paths), avoiding the need for thousands of eigenfunctions and mitigating numerical blowup.
- Core assumption: The manifold has a well-defined curvature structure and root system, enabling closed-form approximations of the heat kernel for small t.
- Evidence anchors:
  - [abstract]: "By leveraging and combining various ansätze, we can quickly compute relevant quantities to high precision."
  - [section]: "To address this problem, we examine several refined versions of the Varadhan approximation that use the fact that the manifold is symmetric."
  - [corpus]: Weak—corroborates general improvement claims but lacks specific mechanism details.
- Break condition: If t is not sufficiently small or the manifold lacks the required geometric structure, the approximations lose accuracy.

### Mechanism 3
- Claim: Exact sampling from the heat kernel via manifold ODEs and rejection sampling avoids the numerical drift and computational cost of geodesic random walks.
- Mechanism: The probability flow ODE is used to sample from the heat kernel by solving a manifold ODE on the maximal torus, which is numerically stable and efficient. For low dimensions, rejection sampling with a tractable prior distribution is also feasible.
- Core assumption: The manifold ODE solver is adaptive and stable, and the rejection sampling prior distribution tightly bounds the heat kernel.
- Evidence anchors:
  - [abstract]: "Furthermore, we develop a sampling method based on the probability flow ODE and show that this can be quickly computed with standard ODE solvers on the maximal torus of the manifold."
  - [section]: "As an alternative, we notice that we can apply the probability flow ODE to sample from the heat kernel exactly."
  - [corpus]: Weak—no direct support for the manifold ODE sampling claim.
- Break condition: If the ODE solver is unstable or the rejection sampling prior is poorly chosen, sampling becomes inaccurate or inefficient.

## Foundational Learning

- Concept: Riemannian symmetric spaces and their maximal torus structure.
  - Why needed here: The entire computational speedup relies on reducing the heat kernel problem to the maximal torus, which is only possible for symmetric spaces.
  - Quick check question: What is the maximal torus of SU(n) and how does it simplify heat kernel computation?
- Concept: Heat kernel approximations (eigenfunction expansion, Schwinger-Dewitt, sum over paths).
  - Why needed here: These approximations are the core tools for efficient and accurate computation of the heat kernel, which is central to the denoising score matching objective.
  - Quick check question: How does the Schwinger-Dewitt approximation improve upon Varadhan's approximation for small t?
- Concept: Manifold ODEs and geodesic random walks.
  - Why needed here: Sampling from the heat kernel traditionally requires simulating Brownian motion via geodesic random walks, which is slow and prone to numerical drift. Manifold ODEs provide an exact alternative.
  - Quick check question: How does the probability flow ODE on the maximal torus enable exact sampling from the heat kernel?

## Architecture Onboarding

- Component map:
  - Heat kernel estimator: Combines eigenfunction expansion (large t) and improved small-time approximations (small t)
  - Manifold ODE sampler: Uses the probability flow ODE on the maximal torus for exact sampling
  - Score network: Riemannian diffusion model trained with denoising score matching using the accurate heat kernel
  - Training loop: Samples from the heat kernel, computes the score matching loss, and updates the score network
- Critical path:
  1. Compute the heat kernel using the unified estimator (eigenfunction expansion or small-time approximation)
  2. Sample from the heat kernel using the manifold ODE sampler or rejection sampling
  3. Compute the denoising score matching loss
  4. Update the score network parameters
- Design tradeoffs:
  - Accuracy vs. speed: Eigenfunction expansions are accurate but slow for small t; small-time approximations are fast but less accurate for large t
  - Sampling method: Manifold ODE sampling is exact but requires solving ODEs; rejection sampling is fast but only feasible for low dimensions
  - Manifold choice: Restricting to symmetric spaces enables efficient computation but limits applicability to non-symmetric manifolds
- Failure signatures:
  - Numerical instability: Heat kernel evaluations blow up or return NaNs, especially for small t or high dimensions
  - Poor sampling: Manifold ODE solver fails to converge or rejection sampling has low acceptance rate
  - Suboptimal training: Score matching loss is inaccurate due to poor heat kernel approximations
- First 3 experiments:
  1. Verify heat kernel accuracy on S2 and S127 using the unified estimator and compare with ground truth
  2. Test manifold ODE sampling on S1 and SO(3) and compare with geodesic random walk sampling
  3. Train a Riemannian diffusion model on a simple checkerboard dataset on the torus and visualize the learned density

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the heat kernel approximation methods be extended to non-symmetric Riemannian manifolds while maintaining computational efficiency?
- Basis in paper: [inferred] The paper focuses on symmetric spaces and mentions noncompact symmetric spaces briefly in Appendix A.2, suggesting potential for extension.
- Why unresolved: The paper explicitly restricts analysis to Riemannian symmetric spaces due to their computational advantages, leaving the question of efficiency on general manifolds unanswered.
- What evidence would resolve it: Empirical comparison of heat kernel approximation methods on both symmetric and non-symmetric manifolds, demonstrating computational costs and accuracy trade-offs.

### Open Question 2
- Question: How does the performance of Riemannian Diffusion Models compare to other generative models on high-dimensional symmetric spaces when computational resources are equal?
- Basis in paper: [explicit] The paper demonstrates scaling to high-dimensional tasks but doesn't directly compare resource usage with other methods.
- Why unresolved: The paper focuses on demonstrating feasibility and improvement over previous Riemannian Diffusion Models, but doesn't benchmark against other generative models under equal resource constraints.
- What evidence would resolve it: Controlled experiments comparing Riemannian Diffusion Models with VAEs, GANs, and normalizing flows on identical symmetric space tasks with matched computational budgets.

### Open Question 3
- Question: Can the sampling methods based on the probability flow ODE be further optimized for speed without sacrificing accuracy?
- Basis in paper: [explicit] The paper introduces ODE-based sampling as an alternative to geodesic random walks but notes it's solvable using existing manifold ODE solvers.
- Why unresolved: While the paper presents ODE-based sampling as an improvement, it doesn't explore optimization techniques specific to this sampling method or compare its efficiency to other potential approaches.
- What evidence would resolve it: Comparative analysis of different ODE solvers, adaptive time-stepping strategies, and potential hybrid sampling methods, measuring both sampling speed and accuracy.

## Limitations

- The method relies on symmetric space assumptions, limiting applicability to non-symmetric manifolds common in real-world applications
- Empirical validation on high-dimensional tasks (QCD lattices, hyperspherical embeddings) lacks detailed baseline comparisons with other generative models
- Claims about sampling efficiency gains over geodesic random walks need more rigorous benchmarking, particularly regarding ODE solver impact

## Confidence

- **High confidence**: The mathematical framework for symmetric space heat kernel computation is sound and the computational improvements over naive eigenfunction expansion are demonstrable
- **Medium confidence**: The empirical improvements on low-dimensional datasets are convincing, but the high-dimensional applications lack sufficient detail about baseline comparisons
- **Low confidence**: The claims about sampling efficiency gains over geodesic random walks need more rigorous benchmarking, particularly regarding the impact of ODE solver choice and timestep selection

## Next Checks

1. **Robustness testing**: Implement the method on non-symmetric manifolds (e.g., generic Lie groups or warped product spaces) to verify whether the performance degrades as expected when symmetric structure assumptions break down
2. **Baseline completeness**: Include recent Riemannian diffusion methods (e.g., Mori et al.'s mixture models, Azmoudeh's approaches) in the comparison suite for the high-dimensional applications
3. **Scaling analysis**: Systematically vary dimensionality and time-step parameters to quantify the computational complexity scaling, particularly for the transition region where neither eigenfunction expansion nor small-time approximations dominate