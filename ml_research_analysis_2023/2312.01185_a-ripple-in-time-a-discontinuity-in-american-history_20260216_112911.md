---
ver: rpa2
title: 'A ripple in time: a discontinuity in American history'
arxiv_id: '2312.01185'
source_url: https://arxiv.org/abs/2312.01185
tags:
- umap
- sotu
- embedding
- figure
- distilbert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a novel approach to analyzing temporal patterns
  and authorship in historical text datasets, using State of the Union addresses from
  42 US presidents as a case study. The researchers employed BERT (DistilBERT) and
  GPT-2 for vector embeddings, combined with nonlinear dimension reduction techniques
  (UMAP, TriMAP, and PaCMAP) to uncover temporal and authorial patterns.
---

# A ripple in time: a discontinuity in American history

## Quick Facts
- arXiv ID: 2312.01185
- Source URL: https://arxiv.org/abs/2312.01185
- Reference count: 9
- Key outcome: GPT-2 embeddings with UMAP reveal temporal break in State of the Union addresses around 1928-1932, achieving 93% authorship attribution accuracy

## Executive Summary
This study presents a novel approach to analyzing temporal patterns and authorship in historical text datasets, using State of the Union addresses from 42 US presidents as a case study. The researchers employed BERT (DistilBERT) and GPT-2 for vector embeddings, combined with nonlinear dimension reduction techniques (UMAP, TriMAP, and PaCMAP) to uncover temporal and authorial patterns. While BERT is traditionally favored for NLP classification, the study found that GPT-2 paired with UMAP produced superior clustering results, revealing a clear temporal break between pre-1928 and post-1932 addresses. Additionally, fine-tuning a DistilBERT model for authorship attribution achieved an accuracy of 93%, significantly higher than the 80% accuracy obtained with pre-trained embeddings alone.

## Method Summary
The researchers processed State of the Union addresses using GPT-2 and DistilBERT for text embeddings, then applied nonlinear dimension reduction (UMAP, TriMAP, PaCMAP) to visualize temporal patterns. For authorship attribution, they fine-tuned DistilBERT using 512-token sliding windows with 128-token overlap, treating each chunk as a training sample. The model aggregated logits across all chunks to predict the author of each address. The dataset was split 75%/25% train/test per president.

## Key Results
- GPT-2 embeddings with UMAP revealed a clear temporal discontinuity between pre-1928 and post-1932 SOTU addresses
- Fine-tuned DistilBERT achieved 93% authorship attribution accuracy versus 80% with pre-trained embeddings
- The temporal break aligns with historical shifts including the transition from personal authorship to speechwriter usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-2 embeddings capture more granular stylistic and topical information than DistilBERT, enabling UMAP to produce cleaner temporal clustering.
- Mechanism: GPT-2, being a generative model, learns richer token-level representations that encode both content and style. UMAP then leverages these fine-grained distinctions to reveal underlying structure.
- Core assumption: The pre-trained GPT-2 model without fine-tuning contains sufficient domain knowledge to represent historical texts effectively.
- Evidence anchors:
  - [abstract] "we find out that GPT–2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering"
  - [section] "GPT–2 is a generative model, we can also speculate that its embedding captures more granular details of the dataset"

### Mechanism 2
- Claim: Fine-tuning DistilBERT on authorship attribution dramatically improves accuracy by adapting the model to the specific vocabulary and patterns in SOTU addresses.
- Mechanism: The fine-tuned model learns president-specific stylistic signatures from chunked text, and aggregating logits over all chunks yields a robust document-level prediction.
- Core assumption: Stylistic features are consistent within each president's corpus and distinguishable from others.
- Evidence anchors:
  - [abstract] "fine-tuning a DistilBERT model for authorship attribution achieved an accuracy of 93%, significantly higher than the 80% accuracy obtained with pre-trained embeddings alone"
  - [section] "we chunked each texts with a sliding window of 512 token having an overlap of 128 tokens, and treated each chunk as a training sample"

### Mechanism 3
- Claim: The observed temporal discontinuity around 1928–1932 reflects a genuine shift in rhetorical style and content due to historical changes (e.g., shift from personal authorship to speechwriters, post-WWII geopolitical role).
- Mechanism: Non-linear dimension reduction techniques (UMAP/TriMAP/PaCMAP) preserve local neighborhoods in embedding space, making stylistic shifts visually separable along the temporal axis.
- Core assumption: The discontinuity is not an artifact of embedding or dimension reduction but reflects real changes in the dataset.
- Evidence anchors:
  - [section] "there is a large break, as demonstrated by the UMAP visualization... between addresses written before 1927 and addresses written after 1932"
  - [section] "The first is that before the time of Franklin Roosevelt, it was extremely uncommon for presidents to use speech writers, while Roosevelt used their help a lot"

## Foundational Learning

- Concept: Text embedding methods (BERT, GPT-2)
  - Why needed here: They transform raw text into fixed-dimensional vectors suitable for machine learning.
  - Quick check question: What is the main difference between BERT and GPT-2 embeddings in terms of what they capture?

- Concept: Non-linear dimension reduction (UMAP, TriMAP, PaCMAP)
  - Why needed here: They project high-dimensional embeddings into 2D/3D for visualization and reveal cluster structure.
  - Quick check question: How does UMAP differ from PCA in preserving data structure?

- Concept: Fine-tuning vs. pre-trained embeddings
  - Why needed here: Fine-tuning adapts a general-purpose model to a specific task (authorship attribution) for better performance.
  - Quick check question: Why did chunking the text improve authorship attribution accuracy?

## Architecture Onboarding

- Component map: Data ingestion -> Text preprocessing -> Embedding (GPT-2/DistilBERT) -> Dimension reduction (UMAP/TriMAP/PaCMAP) -> Clustering/Visualization -> Optional fine-tuning (DistilBERT) -> Classification -> Evaluation

- Critical path:
  - Embedding -> Dimension reduction -> Clustering (for temporal analysis)
  - Embedding + Chunking -> Fine-tuning -> Aggregation -> Classification (for authorship)

- Design tradeoffs:
  - GPT-2 gives better clustering but is slower; DistilBERT is faster but may need fine-tuning for high accuracy.
  - Chunking increases training samples but may lose long-range context.

- Failure signatures:
  - Poor clustering: embeddings not capturing relevant features, dimension reduction parameters suboptimal.
  - Low authorship accuracy: chunks too short to capture style, model overfitting or underfitting.

- First 3 experiments:
  1. Run GPT-2 -> UMAP on the full dataset, visualize and verify the 1928-1932 split.
  2. Run DistilBERT -> TriMAP, compare clustering quality and temporal breaks to experiment 1.
  3. Fine-tune DistilBERT on chunked data, evaluate chunk-level vs. document-level accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic or thematic features distinguish SOTU addresses written before 1928 from those written after 1932?
- Basis in paper: [explicit] The paper notes a clear temporal break between pre-1928 and post-1932 addresses, but does not specify the exact linguistic or thematic differences
- Why unresolved: The study focuses on detecting the temporal discontinuity using vector embeddings and dimension reduction, but does not perform detailed linguistic or thematic analysis of the content differences
- What evidence would resolve it: Detailed linguistic analysis of pre-1928 vs post-1932 SOTU addresses, focusing on vocabulary, syntax, and thematic content differences

### Open Question 2
- Question: How do the GPT-2 embeddings capture more granular details of the dataset compared to DistilBERT embeddings, as suggested by the study?
- Basis in paper: [explicit] The authors posit that GPT-2's generative nature allows it to capture more granular details, but do not provide specific examples or analysis
- Why unresolved: The study demonstrates that GPT-2 + UMAP produces better clustering, but does not explain the mechanism behind this improved performance
- What evidence would resolve it: Comparative analysis of the feature importance and attention patterns in GPT-2 vs DistilBERT embeddings for SOTU addresses

### Open Question 3
- Question: What is the relationship between the temporal clustering of SOTU addresses and major historical events or political changes in the United States?
- Basis in paper: [inferred] The authors mention potential explanations for the temporal discontinuity, including changes in speechwriting practices and the shift in US global role after WWII, but do not conduct a detailed historical analysis
- Why unresolved: The study focuses on detecting temporal patterns using computational methods, but does not correlate these patterns with specific historical events or political changes
- What evidence would resolve it: Detailed historical analysis correlating SOTU content changes with major political, economic, or social events in US history

### Open Question 4
- Question: How does the performance of GPT-2 + UMAP compare to other combinations of embeddings and dimension reduction techniques for temporal clustering of text data?
- Basis in paper: [explicit] The authors compare GPT-2 + UMAP to DistilBERT + various dimension reduction techniques, but do not explore other combinations or benchmark against different text datasets
- Why unresolved: The study focuses on a specific combination of models for a particular dataset, without exploring the broader landscape of text embedding and dimension reduction techniques
- What evidence would resolve it: Systematic comparison of various embedding and dimension reduction combinations across multiple text datasets with known temporal patterns

## Limitations

- Limited external validation: Findings based on single dataset (State of the Union addresses) without external validation
- Potential overfitting: Fine-tuning achieved 93% accuracy but lacks cross-validation details
- Unclear embedding mechanism: GPT-2 superiority not empirically validated through ablation studies

## Confidence

- High confidence: Temporal discontinuity between pre-1928 and post-1932 SOTU addresses
- Medium confidence: GPT-2 superiority for clustering and fine-tuning effectiveness for authorship
- Low confidence: Exact mechanism by which GPT-2 embeddings outperform DistilBERT

## Next Checks

1. Apply GPT-2 + UMAP pipeline to another historical text dataset to verify generalizability
2. Conduct ablation study comparing GPT-2 embeddings with and without fine-tuning
3. Systematically vary sliding window size and overlap in fine-tuning process to optimize authorship attribution accuracy