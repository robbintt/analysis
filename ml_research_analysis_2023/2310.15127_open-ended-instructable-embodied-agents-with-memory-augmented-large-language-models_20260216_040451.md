---
ver: rpa2
title: Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language
  Models
arxiv_id: '2310.15127'
source_url: https://arxiv.org/abs/2310.15127
tags:
- target
- object
- tomato
- arxiv
- commander
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HELPER introduces a memory-augmented LLM approach for open-ended
  instructable embodied agents. It uses retrieval-augmented LLM prompting with an
  external memory of language-program pairs to parse free-form dialogue and instructions
  into executable action programs.
---

# Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models

## Quick Facts
- arXiv ID: 2310.15127
- Source URL: https://arxiv.org/abs/2310.15127
- Authors: 
- Reference count: 40
- Key outcome: HELPER achieves new state-of-the-art on TEACh benchmark with 1.7x task success and 2.1x goal-condition success improvement over prior work

## Executive Summary
HELPER introduces a memory-augmented LLM approach for open-ended instructable embodied agents that can parse free-form dialogue and instructions into executable action programs. The system maintains an external memory of language-program pairs that expands during deployment with successful user-specific routines for personalization. HELPER integrates retrieval-augmented LLM prompting, visually-grounded plan correction using vision-language models, and LLM-based common sense object search to achieve state-of-the-art performance on the TEACh benchmark.

## Method Summary
HELPER uses a modular architecture with a Planner that employs retrieval-augmented LLM prompting to convert dialogue and instructions into executable programs. The system maintains an external memory storing successful language-program pairs that grows during deployment to capture user-specific routines. When programs fail, a pre-trained vision-language model diagnoses the failure and retrieves similar error correction examples from memory to generate corrective plans. The Executor performs scene perception, pre-condition checks, and action execution, while the Locator module uses LLM-based common sense reasoning to find undetected objects by suggesting relevant search locations.

## Key Results
- Achieves 1.7x improvement in task success rate and 2.1x improvement in goal-condition success rate on TEACh benchmark
- Demonstrates effectiveness of memory augmentation for personalizing to user-specific routines and language
- Shows successful integration of user feedback to improve task performance over time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memory-augmented LLM prompting enables adaptation to user-specific routines and language
- Mechanism: External memory of language-program pairs expands during deployment with successful user routines, retrieved during inference as in-context examples
- Core assumption: LLM effectively leverages retrieved examples to parse free-form dialogue into executable programs
- Evidence anchors: Memory expansion during deployment mentioned in abstract; weak corpus support for effectiveness
- Break condition: LLM fails to leverage examples or memory expansion doesn't capture user routines effectively

### Mechanism 2
- Claim: VLM-guided visually-grounded plan correction enables failure recovery
- Mechanism: VLM matches visual observations to textual failure cases, retrieves similar error correction examples from memory to generate corrective programs
- Core assumption: VLM accurately matches observations to failure cases and Planner effectively leverages retrieved corrections
- Evidence anchors: VLM failure diagnosis mentioned in abstract; weak corpus support for correction effectiveness
- Break condition: VLM fails to accurately match failures or Planner fails to leverage corrections effectively

### Mechanism 3
- Claim: LLM-based common sense object search enables efficient object localization
- Mechanism: LLM suggests potential search locations based on previous instructions and common sense knowledge when objects aren't detected in maps
- Core assumption: LLM effectively suggests relevant locations based on input language and common sense
- Evidence anchors: Common sense object search mentioned in abstract; weak corpus support for effectiveness
- Break condition: LLM fails to suggest relevant search locations

## Foundational Learning

- Concept: Retrieval-augmented LLM prompting
  - Why needed here: Enables adaptation to user-specific routines and language not known at prompt engineering time
  - Quick check question: How does HELPER retrieve relevant examples from memory for in-context examples?

- Concept: Vision-language models for failure diagnosis
  - Why needed here: Enables recovery from failures by diagnosing plan failures and retrieving similar failure cases with solutions
  - Quick check question: How does HELPER use VLM to match visual observation to textual failure case?

- Concept: LLM-based common sense reasoning
  - Why needed here: Enables efficient object localization by suggesting search locations based on common sense knowledge
  - Quick check question: How does HELPER prompt LLM to suggest object search locations from input language?

## Architecture Onboarding

- Component map: Input dialogue → Planner → Program → Executor → Action
- Critical path: Input dialogue → Planner → Program → Executor → Action
- Design tradeoffs:
  - Memory size vs. retrieval efficiency: Larger memory allows more examples but increases retrieval time
  - VLM accuracy vs. failure diagnosis granularity: More accurate VLMs enable finer-grained diagnosis but increase computational cost
  - LLM prompt size vs. plan generation quality: Larger prompts may improve quality but increase computational cost
- Failure signatures:
  - Planner failures: Incorrect/incomplete programs from dialogue
  - VLM failures: Inaccurate failure diagnosis leading to ineffective corrections
  - Executor failures: Actions fail due to perception errors, pre-condition violations, or object search failures
- First 3 experiments:
  1. Test HELPER's ability to parse simple instruction into program using fixed prompt (w/o Mem Aug)
  2. Test HELPER's ability to recover from simulated failure using VLM and memory-augmented prompting (w/ Correction)
  3. Test HELPER's ability to locate undetected object using Locator and LLM-based common sense reasoning (w/ Locator)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HELPER's performance scale with larger memory sizes and longer dialogue contexts?
- Basis in paper: [inferred] Paper mentions memory expansion but doesn't explore scaling limits
- Why unresolved: Uses fixed K=3 for retrieval, doesn't report experiments varying memory size or context length
- What evidence would resolve it: Experiments showing performance as function of memory size and context length

### Open Question 2
- Question: How robust is HELPER to ambiguous or contradictory instructions from different users?
- Basis in paper: [explicit] Mentions personalization to user routines but doesn't evaluate conflicting instructions
- Why unresolved: User personalization experiments only test modifications to existing plans, not conflicting new instructions
- What evidence would resolve it: Experiments with contradictory instructions from different users or over time

### Open Question 3
- Question: How does HELPER's approach compare to end-to-end learning methods for embodied instruction following?
- Basis in paper: [explicit] Compares to baselines but doesn't directly compare to end-to-end trained models
- Why unresolved: Comparison limited to other method families, not direct ablation comparing modular approach to end-to-end alternative
- What evidence would resolve it: Training end-to-end model on same data and comparing performance to HELPER

## Limitations

- Limited mechanistic understanding of which memory examples drive success versus adding noise
- Insufficient validation of VLM accuracy in classifying visual observations to failure categories
- Lack of comparative analysis showing common sense reasoning outperforms simpler heuristics for object localization

## Confidence

- Memory-augmented prompting for personalization: **Medium** - Strong empirical results but limited mechanistic understanding
- VLM-guided failure correction: **Medium-Low** - Novel approach but insufficient validation of accuracy and effectiveness
- LLM-based common sense object search: **Medium** - Shows practical benefit but lacks comparative analysis

## Next Checks

1. **Memory example analysis**: Conduct ablation studies systematically removing different types of memory examples to identify which examples most contribute to success versus those that add noise or redundancy.

2. **VLM failure detection accuracy**: Implement quantitative evaluation of the VLM's ability to correctly classify visual observations into failure categories, measuring precision and recall against ground truth failure types.

3. **Common sense versus heuristic comparison**: Compare the LLM-based object search against simpler heuristic approaches (e.g., searching near related objects, searching in typical locations) to isolate the contribution of common sense reasoning.