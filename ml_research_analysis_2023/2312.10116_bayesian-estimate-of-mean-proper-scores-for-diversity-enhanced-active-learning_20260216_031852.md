---
ver: rpa2
title: Bayesian Estimate of Mean Proper Scores for Diversity-Enhanced Active Learning
arxiv_id: '2312.10116'
source_url: https://arxiv.org/abs/2312.10116
tags:
- learning
- batch
- data
- active
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Bayesian Estimate of Mean Proper Scores (BEMPS),
  a novel active learning framework that estimates the increase in strictly proper
  scores (e.g., log probability, negative mean squared error) within a Bayesian framework.
  BEMPS addresses the limitations of existing uncertainty-based and diversity-based
  active learning methods by considering both aspects.
---

# Bayesian Estimate of Mean Proper Scores for Diversity-Enhanced Active Learning

## Quick Facts
- **arXiv ID**: 2312.10116
- **Source URL**: https://arxiv.org/abs/2312.10116
- **Reference count**: 40
- **Key outcome**: Novel active learning framework that estimates increases in strictly proper scores within Bayesian framework, outperforming existing methods on text and image classification tasks

## Executive Summary
This paper introduces Bayesian Estimate of Mean Proper Scores (BEMPS), a novel active learning framework that leverages strictly proper scoring rules to estimate the value of acquiring new labels. BEMPS addresses key limitations of existing uncertainty-based and diversity-based methods by directly measuring epistemic uncertainty through expected changes in proper scores like Brier score and log probability. The framework combines deep ensembles or Monte Carlo dropout with dynamic validation set construction and a complementary batch diversity enhancement mechanism. Extensive experiments demonstrate consistent improvements over state-of-the-art methods across multiple text and image classification tasks.

## Method Summary
BEMPS computes the expected change in strictly proper scores when acquiring a new label, focusing on epistemic uncertainty rather than aleatoric uncertainty. For each unlabeled sample, it calculates a vector of expected score changes across an estimation pool, then uses k-means clustering on these vectors to select diverse batches. The method employs deep ensembles or Monte Carlo dropout to approximate intractable integrals over model parameters, and uses dynamic validation set construction from the labeled pool to improve calibration. Two acquisition functions are proposed: CoreMSE based on Brier score and CoreLog based on log probability.

## Key Results
- BEMPS consistently outperforms Max-Entropy, BALD, MOCU, WMOCU, BADGE, and ALPS across multiple datasets
- Dynamic validation set construction significantly improves model calibration compared to fixed validation sets
- Batch diversity enhancement through score change vector clustering yields better performance than random batch selection
- The framework achieves strong performance on both text (AG NEWS, PUBMED, IMDB, SST5) and image (MNIST, CIFAR10) classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BEMPS uses strictly proper scoring rules to estimate epistemic uncertainty, not aleatoric uncertainty, which directly improves model performance.
- Mechanism: By using strictly proper scoring rules like Brier score and log probability, BEMPS measures the expected change in score when a new label is acquired. This focuses on model uncertainty about the data point, rather than inherent label noise.
- Core assumption: The model is identifiable and fully conditional, allowing the scoring rules to converge to the true model parameters.
- Evidence anchors:
  - [abstract] "We propose Bayesian Estimate of Mean Proper Scores (BEMPS) to estimate the increase in strictly proper scores such as log probability or negative mean square error within this framework."
  - [section] "We further prove that our acquisition function measures epistemic uncertainty, not aleatoric uncertainty."
  - [corpus] Weak - no direct evidence found in corpus.

### Mechanism 2
- Claim: BEMPS achieves diversity in batch acquisition by clustering based on expected score changes, leading to better generalization.
- Mechanism: BEMPS represents each unlabeled sample as a vector of expected score changes across an estimation pool. K-means clustering on these vectors selects diverse samples that are likely to reduce uncertainty in different regions of the feature space.
- Core assumption: Samples with similar expected score change vectors will affect the learning similarly, so co-occurrence in a batch is suboptimal.
- Evidence anchors:
  - [abstract] "To facilitate better experimentation with the new acquisition functions, we develop a complementary batch AL algorithm that encourages diversity in the vector of expected changes in scores for unlabeled data."
  - [section] "While the gradient embedding used in [11] represents a data point's impact on the model, our vector represents a data point's direct impact on the mean proper score."
  - [corpus] Weak - no direct evidence found in corpus.

### Mechanism 3
- Claim: Dynamic validation set construction in deep ensembles improves model calibration and performance without requiring a separate large validation set.
- Mechanism: Instead of using a fixed validation set, BEMPS dynamically generates training-validation splits from the labeled pool after each acquisition iteration. This provides variability in the ensemble models and improves generalization.
- Core assumption: The dynamic splits provide sufficient validation data to prevent overfitting while maintaining model diversity.
- Evidence anchors:
  - [abstract] "To allow high-performance classifiers, we combine deep ensembles, and dynamic validation set construction on pretrained models, and further speed up the ensemble process with the idea of Monte Carlo Dropout."
  - [section] "We instead use a dynamic approach to generate alternative validation sets from the ever-increasing labeled pool after each iteration."
  - [corpus] Weak - no direct evidence found in corpus.

## Foundational Learning

- Concept: Strictly proper scoring rules
  - Why needed here: They provide a theoretically sound way to measure the quality of probabilistic predictions, which is crucial for estimating the value of acquiring new labels.
  - Quick check question: What is the key property that distinguishes strictly proper scoring rules from other scoring functions?

- Concept: Bayesian decision theory and expected value of sample information
  - Why needed here: BEMPS is built on the framework of expected value of sample information, which quantifies the expected reduction in cost (measured by proper scores) when acquiring a new sample.
  - Quick check question: How does the expected value of sample information relate to the concept of information gain in active learning?

- Concept: Ensemble methods for uncertainty estimation
  - Why needed here: Deep ensembles and Monte Carlo dropout are used to approximate the intractable integrals over model parameters in BEMPS, providing more reliable uncertainty estimates.
  - Quick check question: What is the key difference between deep ensembles and Monte Carlo dropout in terms of how they approximate model uncertainty?

## Architecture Onboarding

- Component map: Core BEMPS algorithm -> Deep ensemble/MC dropout -> Dynamic validation set generator -> Batch diversity enhancer -> Classification backbone
- Critical path: 1. Initialize with small labeled pool and large unlabeled pool 2. Train ensemble models with dynamic validation sets 3. Compute expected score changes for all unlabeled samples 4. If batch size > 1, cluster samples based on expected score change vectors 5. Acquire top-scoring samples and add to labeled pool 6. Repeat steps 2-5 until annotation budget exhausted
- Design tradeoffs: Deep ensembles vs MC dropout (accuracy vs computational cost), batch size (diversity vs number of iterations), dynamic vs fixed validation sets (model diversity vs stability)
- Failure signatures: Poor performance on imbalanced datasets (insufficient diversity), high expected calibration error (scoring rule issues), slow convergence (ensemble approximation problems)
- First 3 experiments: 1. Compare BEMPS with random acquisition on IMDB with DistilBERT 2. Test different batch sizes (1, 5, 10) on IMDB 3. Compare deep ensembles with MC dropout on AG NEWS

## Open Questions the Paper Calls Out

- Question: How does the BEMPS framework perform when integrated with semi-supervised learning methods that utilize unlabeled data?
  - Basis in paper: [explicit] The authors explicitly state that a limitation of BEMPS and its theory is that it does not support the use of unlabeled data within the learning algorithm, which is common in state-of-the-art semi-supervised learning methods.
  - Why unresolved: The paper focuses on the performance of BEMPS with fully supervised active learning and does not explore its integration with semi-supervised learning techniques.
  - What evidence would resolve it: Empirical studies comparing BEMPS-based active learning with and without semi-supervised learning components on benchmark datasets, measuring performance metrics like accuracy and F1-score.

- Question: How does the choice of strictly proper scoring rule impact the performance of BEMPS in different classification tasks?
  - Basis in paper: [explicit] The authors mention that strictly proper scoring rules can be tailored for different inference tasks, and they instantiate BEMPS with the Brier score and logarithmic score. However, they do not explore other scoring rules.
  - Why unresolved: The paper only uses two scoring rules (Brier score and logarithmic score) and does not investigate the impact of other strictly proper scoring rules on BEMPS performance.
  - What evidence would resolve it: Experiments using different strictly proper scoring rules (e.g., spherical score, zero-one score) in BEMPS and comparing their performance across various classification tasks and datasets.

- Question: How does the performance of BEMPS-based active learning scale with increasing dataset size and model complexity?
  - Basis in paper: [inferred] The paper demonstrates the effectiveness of BEMPS on relatively small to medium-sized text and image datasets. However, it does not explore the scalability of BEMPS with larger datasets or more complex models.
  - Why unresolved: The experiments conducted in the paper focus on specific benchmark datasets and model architectures, leaving the scalability of BEMPS unexplored.
  - What evidence would resolve it: Extensive experiments using larger datasets (e.g., ImageNet, large-scale text corpora) and more complex models (e.g., deep transformer architectures) to evaluate the scalability and performance of BEMPS-based active learning.

## Limitations

- Dynamic validation set construction lacks precise implementation details, making exact reproduction difficult
- K-means clustering parameters for batch diversity (number of clusters, fraction of samples) are not fully specified
- Computational overhead of deep ensembles versus MC dropout trade-off is not quantitatively characterized

## Confidence

- **High**: The theoretical foundation using strictly proper scoring rules to measure epistemic uncertainty
- **Medium**: The effectiveness of batch diversity through score change vector clustering
- **Medium**: The calibration improvements from dynamic validation sets

## Next Checks

1. Reproduce single-dataset baseline: Implement CoreMSE on IMDB with DistilBERT using fixed validation sets first, then add dynamic validation to isolate its effect
2. Ablation study: Test CoreMSE with and without batch diversity clustering on MNIST to measure the specific contribution of the diversity mechanism
3. Computational profiling: Measure wall-clock time differences between deep ensembles and MC dropout implementations on CIFAR10 to quantify the practical trade-off