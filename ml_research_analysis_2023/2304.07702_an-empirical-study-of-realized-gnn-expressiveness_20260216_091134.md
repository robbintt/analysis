---
ver: rpa2
title: An Empirical Study of Realized GNN Expressiveness
arxiv_id: '2304.07702'
source_url: https://arxiv.org/abs/2304.07702
tags:
- graphs
- graph
- regular
- expressiveness
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BREC, a new dataset for measuring the expressiveness
  of Graph Neural Networks (GNNs). Previous expressiveness datasets face issues with
  difficulty, granularity, and scale.
---

# An Empirical Study of Realized GNN Expressiveness

## Quick Facts
- arXiv ID: 2304.07702
- Source URL: https://arxiv.org/abs/2304.07702
- Reference count: 35
- Key outcome: New BREC dataset with RPC evaluation method enables first comprehensive empirical comparison of 16 GNN models' expressiveness

## Executive Summary
This paper addresses the gap between theoretical expressiveness bounds and empirical GNN performance by introducing BREC, a novel dataset of 400 non-isomorphic graph pairs spanning 4 categories and difficulty levels up to 4-WL. The authors propose Reliable Paired Comparison (RPC), a statistical evaluation method that directly compares model outputs on graph pairs while accounting for numerical precision errors. When evaluating 16 state-of-the-art GNNs on BREC, the authors find significant variation in expressiveness, with accuracy ranging from 41.5% to 70.8% and I2GNN achieving the highest performance. The work provides both a new benchmark for expressiveness studies and empirical evidence about which GNN architectures can realize their theoretical potential.

## Method Summary
The paper introduces BREC, a dataset of 400 graph pairs across 4 categories (Basic, Regular, Tree, CFI) designed to test GNN expressiveness from 1-WL to 4-WL difficulty levels. For evaluation, the authors propose RPC, which uses a Siamese network architecture with contrastive loss to train models to distinguish graph pairs, then applies Hotelling's T-square statistical test to determine if output differences are significant. The training framework generates multiple copies of each graph with different node orderings, computes embeddings using the GNN, and evaluates whether the model can reliably distinguish non-isomorphic pairs based on statistical significance thresholds.

## Key Results
- BREC enables fine-grained comparison between models that previous datasets could only distinguish at 0% or 100% accuracy levels
- I2GNN achieves highest expressiveness accuracy at 70.8%, significantly outperforming other evaluated models
- RPC evaluation aligns well with theoretical expressiveness bounds, providing a more reliable assessment than threshold-based methods
- Subgraph-based GNNs show strong performance on BREC, with subgraph radius = 6 generally optimal across most methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reliable Paired Comparison (RPC) aligns model evaluation with theoretical expressiveness bounds.
- Mechanism: RPC directly compares GNN outputs on graph pairs, using statistical tests to account for numerical precision errors. This bypasses limitations of threshold-based evaluation and aligns results with theoretical expectations.
- Core assumption: Model outputs for graph pairs follow Gaussian distributions with mean difference µ=0 when models cannot distinguish the graphs.
- Evidence anchors:
  - [abstract] "RPC aligns well with theoretical results."
  - [section] "we propose a new principled evaluation procedure based on directly comparing the differences between model outputs."
- Break condition: If the Gaussian distribution assumption fails or if numerical precision errors overwhelm the statistical test, RPC may produce unreliable results.

### Mechanism 2
- Claim: BREC's diverse graph categories enable fine-grained expressiveness comparison between models.
- Mechanism: By including graphs from Basic to CFI categories, each with different difficulty levels (1-WL to 4-WL), BREC allows for nuanced comparisons of model performance across a spectrum of graph structures. This granularity addresses the coarse performance levels (0% or 100%) seen in previous datasets.
- Evidence anchors:
  - [abstract] "finer granularity (able to compare models between 1-WL and 3-WL)"
  - [section] "BREC has a greater difficulty (up to 4-WL), finer granularity (able to compare models between 1-WL and 3-WL)"
- Break condition: If the dataset's graph diversity is insufficient or if models achieve near-perfect accuracy across all categories, the fine-grained comparison benefit diminishes.

### Mechanism 3
- Claim: Training framework with Siamese network design and contrastive loss enhances model expressiveness.
- Mechanism: The Siamese network structure with identical weights and contrastive loss pushes embeddings of non-isomorphic graphs apart, potentially helping models reach their theoretical expressiveness limits. Training also aids in achieving properties like injectivity and universal approximation.
- Evidence anchors:
  - [section] "we follow the siamese network design approach [Koch et al., 2015] to train the model for each pair of graphs to distinguish."
  - [section] "Training can realize searching in the function space, leading to a better practical expressiveness."
- Break condition: If the training process overfits to the training pairs or if the contrastive loss is not properly tuned, the framework might not effectively enhance expressiveness.

## Foundational Learning

- Concept: Weisfeiler-Lehman (WL) graph isomorphism test
  - Why needed here: Understanding WL is crucial as it defines the expressiveness hierarchy for GNNs. BREC tests models on graphs that are indistinguishable by 1-WL, 3-WL, and up to 4-WL.
  - Quick check question: What is the main limitation of 1-WL in distinguishing graph structures, and how do higher-order WL tests address this?

- Concept: Graph Neural Network (GNN) expressiveness
  - Why needed here: The paper's core focus is on evaluating and comparing the expressive power of different GNN models. Knowing what factors contribute to GNN expressiveness (e.g., subgraph extraction, distance encoding) is essential for interpreting the results.
  - Quick check question: How do subgraph-based GNNs differ in their approach to enhancing expressiveness compared to k-WL hierarchy-based GNNs?

- Concept: Statistical hypothesis testing (Hotelling's T-square test)
  - Why needed here: RPC relies on statistical tests to determine if a model can distinguish graph pairs. Understanding the mechanics of these tests (e.g., T-square statistic, degrees of freedom) is crucial for implementing and interpreting RPC.
  - Quick check question: What is the null hypothesis in the Hotelling's T-square test used by RPC, and how is the decision to reject it made?

## Architecture Onboarding

- Component map: BREC dataset (Basic -> Regular -> Tree -> CFI categories) -> Siamese network architecture -> Contrastive loss training -> Hotelling's T-square test -> Expressiveness evaluation
- Critical path: For evaluating a GNN on BREC, the critical path is: 1) Generate graph copies for each pair, 2) Compute embeddings using the GNN, 3) Perform paired comparison test (T-square statistic), 4) Conduct reliability check, 5) Determine if the model can distinguish the pair based on the results.
- Design tradeoffs: Using statistical tests in RPC adds reliability but increases computational complexity. Including diverse graph categories in BREC enhances granularity but requires careful generation to ensure meaningful comparisons. The Siamese training framework can enhance expressiveness but may lead to overfitting if not properly regularized.
- Failure signatures: If RPC consistently fails to reject the null hypothesis across many pairs, it might indicate the GNN has limited expressiveness. If the reliability check fails frequently, it could suggest numerical precision issues. If training doesn't improve model performance, it might point to problems with the contrastive loss or model architecture.
- First 3 experiments:
  1. Evaluate a simple 1-WL GNN on BREC to establish a baseline for models that can't distinguish any of the non-isomorphic pairs.
  2. Test a k-WL hierarchy GNN (e.g., PPGN) on the CFI graphs to see if it can distinguish the most challenging pairs.
  3. Compare the performance of a subgraph-based GNN with different subgraph radii on the Regular graphs to observe the impact of radius on expressiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal subgraph radius for subgraph-based GNNs to balance expressiveness and information noise?
- Basis in paper: [explicit] The paper discusses that increasing subgraph radius can capture more structural information but may also include more invalid information, making it harder to reach theoretical expressiveness. It mentions that radius = 6 is generally the best selection for most methods, with exceptions for NGNN, NGNN+DE, and KPGNN.
- Why unresolved: The optimal radius likely depends on the specific graph dataset and task, requiring empirical testing. The paper only provides results for the BREC dataset.
- What evidence would resolve it: Systematic experiments testing different subgraph radii on various graph datasets and tasks to determine the optimal radius in each case.

### Open Question 2
- Question: How does the performance of non-GNN methods compare to GNNs on expressiveness datasets beyond BREC?
- Basis in paper: [explicit] The paper includes 4 types of non-GNN baselines and compares their performance to 10 state-of-the-art GNNs on the BREC dataset. It notes that these non-GNN methods generally have more theoretical meaning than practical meaning.
- Why unresolved: The paper only evaluates these methods on the BREC dataset. Their performance may vary on other expressiveness datasets with different graph types and difficulties.
- What evidence would resolve it: Evaluating the same set of non-GNN and GNN methods on multiple expressiveness datasets with varying graph types and difficulties to compare their relative performance.

### Open Question 3
- Question: Can the RPC evaluation method be extended to other graph tasks beyond expressiveness evaluation?
- Basis in paper: [inferred] The RPC method is designed to reliably compare model outputs on graph pairs by considering both external differences and internal fluctuations. While the paper focuses on expressiveness evaluation, the core idea of comparing model outputs could be applicable to other graph tasks.
- Why unresolved: The paper does not explore the application of RPC to other graph tasks. Adapting the method to different tasks may require modifications to account for task-specific considerations.
- What evidence would resolve it: Applying the RPC method to other graph tasks such as node classification, link prediction, or graph generation, and evaluating its effectiveness compared to existing evaluation methods for those tasks.

## Limitations

- The statistical assumptions underlying RPC (Gaussian distributions with mean difference µ=0) may not hold for all model architectures, potentially affecting reliability assessments
- The generalizability of results to larger-scale graphs remains untested, as BREC focuses on graphs with 12-40 nodes
- The 16 GNN models evaluated represent only a subset of existing approaches, potentially missing other important expressiveness mechanisms

## Confidence

- **High Confidence:** The dataset construction methodology and RPC evaluation framework are well-specified and reproducible
- **Medium Confidence:** The expressiveness rankings of evaluated models are reliable for the BREC graph types but may not generalize to all graph domains
- **Medium Confidence:** Training framework benefits for expressiveness, though mechanism is plausible, requires more ablation studies

## Next Checks

1. Test RPC's reliability assumptions on a broader range of GNN architectures, including those with non-standard activation functions or attention mechanisms
2. Evaluate model performance on larger-scale graphs (100+ nodes) to assess scalability of expressiveness results
3. Conduct ablation studies removing the Siamese training framework to quantify its contribution to expressiveness improvements