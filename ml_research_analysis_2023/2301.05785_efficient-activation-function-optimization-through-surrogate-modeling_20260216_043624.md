---
ver: rpa2
title: Efficient Activation Function Optimization through Surrogate Modeling
arxiv_id: '2301.05785'
source_url: https://arxiv.org/abs/2301.05785
tags:
- activation
- functions
- function
- eigenvalues
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AQuaSurF, a method for efficient activation
  function optimization through surrogate modeling. The approach combines Fisher Information
  Matrix (FIM) eigenvalues and activation function output distributions as predictive
  features, visualized via UMAP dimensionality reduction.
---

# Efficient Activation Function Optimization through Surrogate Modeling

## Quick Facts
- arXiv ID: 2301.05785
- Source URL: https://arxiv.org/abs/2301.05785
- Reference count: 26
- Primary result: Surrogate modeling using FIM eigenvalues and output distributions discovers better activation functions 100x more efficiently than random search

## Executive Summary
This paper introduces AQuaSurF, a method for efficient activation function optimization that uses Fisher Information Matrix (FIM) eigenvalues and activation function output distributions as predictive features. The approach combines these features through UMAP dimensionality reduction to create a surrogate space where regression algorithms can efficiently predict activation function performance. Experiments show the method discovers novel activation functions that outperform baseline functions on CIFAR-100 and ImageNet tasks while requiring only tens of evaluations versus hundreds or thousands for random search. Surprisingly, a sigmoidal design discovered through this method outperforms all other activation functions, challenging the dominance of rectifier nonlinearities in deep learning.

## Method Summary
AQuaSurF combines FIM eigenvalues at initialization with activation function output distributions to create predictive features for surrogate modeling. The method calculates log-scaled eigenvalue histograms of the Fisher Information Matrix and samples activation function outputs from a standard normal distribution. These features are combined using UMAP dimensionality reduction to create a low-dimensional embedding where similar activation functions cluster together. Regression algorithms (KNR, RFR, SVR) are trained on this embedding to predict performance, enabling efficient search through the space of activation functions. The approach starts with 8 baseline functions and iteratively evaluates top predicted functions, updating the regression model with new results.

## Key Results
- Discovered activation functions outperform baseline functions (ELU, ReLU, Swish, etc.) on CIFAR-100 and ImageNet tasks
- Required only tens of evaluations versus hundreds or thousands for random search
- Sigmoidal activation function design outperformed all other activation functions, challenging rectifier dominance
- Achieved state-of-the-art results while being orders of magnitude more efficient than previous methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FIM eigenvalues at initialization predict activation function performance because they capture the geometry of the loss landscape and the network's learning capacity before training begins.
- Mechanism: Different activation functions induce different Fisher Information Matrix eigenvalue distributions when applied to a given architecture. These eigenvalue distributions encode information about how the network will learn and generalize, making them predictive of final performance.
- Core assumption: The Fisher Information Matrix at initialization is representative of the training dynamics and final performance of the network.
- Evidence anchors:
  - [abstract]: "the spectrum of the Fisher information matrix associated with the model's predictive distribution at initialization...were found to be highly predictive of performance"
  - [section]: "Different activation functions induce different FIM eigenvalues for a given neural network. They can be calculated at initialization and do not require training; they can thus serve as a low-dimensional feature-vector representation of the activation function."
  - [corpus]: Weak evidence - corpus neighbors discuss activation functions but don't specifically address FIM eigenvalues as predictive features.
- Break condition: If the initialization distribution or network architecture changes significantly, the FIM eigenvalues may no longer be predictive of performance.

### Mechanism 2
- Claim: Combining FIM eigenvalues with activation function output distributions creates a more complete and robust surrogate representation of activation function quality.
- Mechanism: FIM eigenvalues capture network-level learning dynamics while output distributions capture the intrinsic shape properties of activation functions. Together they provide complementary information that is more predictive than either feature alone.
- Core assumption: The combination of global (FIM) and local (output distribution) features provides better prediction than either feature alone.
- Evidence anchors:
  - [abstract]: "Both sets of features contribute unique information. They are both predictive of performance on their own, but they are most powerful when used in tandem."
  - [section]: "The FIM eigenvalues and activation function outputs are thus important in predicting performance of activation functions...regression algorithms trained on both FIM eigenvalues and activation function outputs outperform algorithms trained on just eigenvalues or outputs alone."
  - [corpus]: Weak evidence - corpus neighbors discuss activation functions but don't specifically address the combination of FIM eigenvalues and output distributions.
- Break condition: If one feature type becomes redundant or noisy, the combination may not provide additional benefit.

### Mechanism 3
- Claim: UMAP dimensionality reduction creates an effective surrogate space where regression algorithms can efficiently predict activation function performance.
- Mechanism: UMAP preserves both local and global structure in the high-dimensional feature space, creating a low-dimensional embedding where similar activation functions are close together and performance is smoothly varying.
- Core assumption: The UMAP embedding preserves meaningful relationships between activation functions that correlate with performance.
- Evidence anchors:
  - [abstract]: "These features were combined to create a metric space where a low-dimensional representation of the activation functions could be learned."
  - [section]: "The UMAP algorithm uses an intermediate fuzzy topological representation to represent relationships between data points...This property makes it possible to combine multiple sources of data by taking intersections or unions of the representations."
  - [corpus]: Weak evidence - corpus neighbors don't specifically discuss UMAP for activation function optimization.
- Break condition: If the original feature space doesn't have meaningful structure, UMAP cannot create a useful embedding.

## Foundational Learning

- Concept: Fisher Information Matrix (FIM) and its relationship to neural network learning dynamics
  - Why needed here: Understanding how FIM eigenvalues capture information about learning capacity and generalization is crucial for appreciating why they're useful features for predicting activation function performance.
  - Quick check question: What does the Fisher Information Matrix represent in the context of neural network training, and why would its eigenvalues at initialization be predictive of final performance?

- Concept: Dimensionality reduction techniques, specifically UMAP
  - Why needed here: Understanding how UMAP preserves local and global structure in high-dimensional spaces is important for understanding why it creates effective surrogate spaces for activation function optimization.
  - Quick check question: How does UMAP differ from other dimensionality reduction techniques like t-SNE, and why is this difference important for creating useful surrogate spaces?

- Concept: Surrogate modeling and its application to optimization problems
  - Why needed here: Understanding the concept of using surrogate models to approximate expensive objective functions is fundamental to understanding the overall approach of using FIM eigenvalues and UMAP embeddings as a proxy for activation function performance.
  - Quick check question: What are the key advantages of using surrogate models in optimization, and how does the approach in this paper leverage these advantages?

## Architecture Onboarding

- Component map: FIM eigenvalue calculator -> Output distribution calculator -> UMAP embedder -> Regression model -> Search algorithm
- Critical path:
  1. Compute FIM eigenvalues and output distributions for initial set of activation functions
  2. Embed these features using UMAP
  3. Train regression model on embedded features and known performance
  4. Use regression model to predict performance of unevaluated functions
  5. Evaluate top predicted functions and update model

- Design tradeoffs:
  - Feature choice: FIM eigenvalues vs. output distributions vs. combination
  - Dimensionality: 2D UMAP projection vs. higher dimensions
  - Regression algorithm: KNR vs. RFR vs. SVR
  - Search strategy: Exploitation vs. exploration balance

- Failure signatures:
  - Poor prediction accuracy: May indicate issues with feature choice or UMAP embedding
  - Slow convergence: May suggest need for different regression algorithm or search strategy
  - Invalid FIM eigenvalues: Indicates problematic activation functions that should be filtered out

- First 3 experiments:
  1. Verify FIM eigenvalue calculation by comparing with known activation functions (ReLU, ELU, etc.)
  2. Test UMAP embedding by checking if similar activation functions cluster together
  3. Validate regression predictions by comparing predicted vs. actual performance on a small test set

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications arise from the research:

- How do the FIM eigenvalues and activation function outputs interact when searching for activation functions in more complex architectures like transformers or graph neural networks?
- Can the FIM eigenvalues and activation function outputs be used to optimize other aspects of neural network design, such as normalization layers, loss functions, or data augmentation strategies?
- How does the choice of search space impact the effectiveness of the FIM eigenvalues and activation function outputs in discovering improved activation functions?

## Limitations
- The method relies on FIM eigenvalues at initialization, which may not generalize to different initialization schemes or architectures
- The surprising finding that sigmoidal designs outperform rectifiers needs additional investigation across different problem domains
- The search space and baseline comparisons may not fully represent the complexity of real-world optimization scenarios

## Confidence

- **High confidence**: The methodology for combining FIM eigenvalues with output distributions and using UMAP embeddings for surrogate modeling is well-specified and technically sound.
- **Medium confidence**: The experimental results showing improved efficiency over random search are compelling, but the generality of these findings to other architectures and tasks needs validation.
- **Medium confidence**: The claim that sigmoidal activation functions outperform rectifiers is surprising and warrants additional investigation across different problem domains.

## Next Checks

1. **Cross-architecture validation**: Test the FIM eigenvalue predictive power on architectures not included in the original benchmark (e.g., Vision Transformers, RNNs) to assess generalizability.

2. **Long-term stability analysis**: Evaluate whether activation functions discovered through this method maintain their performance advantage after extended training and under distribution shift conditions.

3. **Ablation study on feature importance**: Systematically test the contribution of FIM eigenvalues versus output distributions by removing each feature type and measuring the impact on prediction accuracy and search efficiency.