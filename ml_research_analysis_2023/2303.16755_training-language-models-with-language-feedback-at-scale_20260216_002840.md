---
ver: rpa2
title: Training Language Models with Language Feedback at Scale
arxiv_id: '2303.16755'
source_url: https://arxiv.org/abs/2303.16755
tags:
- feedback
- summary
- summaries
- language
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Imitation Learning from Language Feedback
  (ILF), a method for training language models to generate text that better aligns
  with human preferences. ILF leverages language feedback, which provides richer information
  than binary comparisons, by generating multiple refinements of a model's output
  and fine-tuning the model to maximize the likelihood of the best refinement.
---

# Training Language Models with Language Feedback at Scale

## Quick Facts
- arXiv ID: 2303.16755
- Source URL: https://arxiv.org/abs/2303.16755
- Reference count: 40
- Key outcome: ILF outperforms fine-tuning on human summaries and achieves human-level summarization performance when combined with binary feedback (50.8% win rate)

## Executive Summary
This paper introduces Imitation Learning from Language Feedback (ILF), a method for training language models to generate text that better aligns with human preferences. ILF leverages language feedback, which provides richer information than binary comparisons, by generating multiple refinements of a model's output and fine-tuning the model to maximize the likelihood of the best refinement. Theoretically, ILF is shown to be equivalent to Bayesian inference. Empirically, ILF is evaluated on a synthetic task and a summarization task, demonstrating that large language models (175B parameters) can accurately incorporate feedback. On summarization, ILF outperforms fine-tuning on human summaries and achieves human-level performance when combined with learning from binary feedback, achieving a win rate of 50.8% against human-written summaries.

## Method Summary
ILF is an iterative refinement and finetuning process with three steps: (1) generate multiple refinements of the initial output based on language feedback using a large language model, (2) select the best refinement using an instruction-fine-tuned language model (InstructRM Ensemble), and (3) finetune the original model on the chosen refinement. The method is evaluated on a synthetic word-removal task and a summarization task using Reddit posts with human feedback and summaries. The performance is measured by win rates against human-written summaries and log-likelihood of generated summaries.

## Key Results
- Large language models (175B parameters) accurately incorporate feedback, with 57.4% success rate in incorporating the most important feedback point
- ILF outperforms finetuning on human summaries and achieves human-level summarization performance when combined with binary feedback (50.8% win rate)
- ILF finetuning scales well with dataset size and maintains KL divergence within 10% of the original model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ILF treats language feedback as evidence for Bayesian inference, analogous to RL with Human Feedback (RLHF).
- **Mechanism:** The algorithm frames the problem as maximizing the log probability of high-quality outputs given a context. It introduces a proposal distribution that samples initial outputs, collects human feedback, and generates refinements. The refinement selection and finetuning steps approximate Bayesian updating based on this evidence.
- **Core assumption:** Human feedback can be modeled as a reward function that indicates whether a refinement improves upon the initial output.
- **Evidence anchors:**
  - [abstract]: "We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback."
  - [section 2]: "Our objective of approximating the ground truth distribution p* (x1), which is proportional to the reward R has clear connections to maximizing reward in RL."
  - [corpus]: Weak - the corpus does not provide direct evidence for this mechanism.
- **Break condition:** If human feedback cannot be reliably converted into a reward signal, or if the feedback is too sparse or inconsistent to guide the proposal distribution.

### Mechanism 2
- **Claim:** Large language models (175B parameters) can effectively incorporate language feedback to generate improved refinements.
- **Mechanism:** The refinement step conditions the language model on the initial output, feedback, and context to generate multiple refinements. The best refinement is selected based on a scoring function (InstructRM Ensemble) that evaluates whether the feedback is incorporated.
- **Core assumption:** The model has sufficient capacity and instruction-following ability to understand and act on feedback.
- **Evidence anchors:**
  - [section 3]: "Our experiments demonstrate that large language models accurately incorporate feedback and that finetuning with ILF scales well with the dataset size, even outperforming finetuning on human summaries."
  - [section 4.5]: "Reﬁnement with Feedback + Best of N incorporates the most important point in the feedback most frequently (57.4± 2.2% often)."
  - [corpus]: Weak - the corpus does not provide direct evidence for this mechanism.
- **Break condition:** If the model lacks the capacity or instruction-following ability to understand and act on feedback, or if the feedback is too complex or ambiguous.

### Mechanism 3
- **Claim:** Combining ILF with learning from binary feedback (via a reward model) achieves human-level summarization performance.
- **Mechanism:** After finetuning on refinements (ILF), a reward model trained on binary comparisons is used to perform best-of-N sampling to select the highest-quality summary.
- **Core assumption:** Both ILF and binary feedback capture complementary aspects of human preferences, and their combination leads to improved performance.
- **Evidence anchors:**
  - [abstract]: "Learning from both language and comparison feedback outperforms learning from each alone, achieving human-level summarization performance."
  - [section 4.3.3]: "Combining ILF and learning from binary feedback (ILF + OPT-RM (best-of-64)) achieves human-level summarization performance with a win rate of 50.8± 1.9% using 5K samples for training."
  - [corpus]: Weak - the corpus does not provide direct evidence for this mechanism.
- **Break condition:** If the reward model is not well-calibrated or if the best-of-N sampling does not effectively select the highest-quality summaries.

## Foundational Learning

- **Concept:** Bayesian Inference
  - Why needed here: ILF is theoretically grounded in Bayesian inference, where the model updates its beliefs based on evidence (language feedback).
  - Quick check question: Can you explain how the refinement selection and finetuning steps approximate Bayesian updating?

- **Concept:** Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: ILF is analogous to RLHF, but instead of binary comparisons, it uses language feedback. Understanding RLHF provides context for ILF.
  - Quick check question: What are the key differences between ILF and RLHF in terms of the type of feedback used and the theoretical framework?

- **Concept:** Large Language Model Finetuning
  - Why needed here: ILF involves finetuning a large language model on refinements generated with language feedback. Understanding finetuning techniques is crucial for implementing ILF.
  - Quick check question: What are the key hyperparameters to consider when finetuning a large language model on refinements, and how do they affect the model's performance?

## Architecture Onboarding

- **Component map:**
  Initial LM -> Feedback Collection -> Refinement LM -> Scoring Function -> Finetuned LM -> Reward Model (optional)

- **Critical path:**
  1. Generate initial summaries with the Initial LM.
  2. Collect language feedback from human annotators.
  3. Generate multiple refinements with the Refinement LM.
  4. Select the best refinement using the Scoring Function.
  5. Finetune the Initial LM on the selected refinements.
  6. (Optional) Train a Reward Model on binary comparisons.
  7. Use the Reward Model to perform best-of-N sampling on the finetuned LM.

- **Design tradeoffs:**
  - Model size vs. feedback incorporation: Larger models (175B parameters) are better at incorporating feedback.
  - Feedback type: Language feedback provides richer information than binary comparisons but requires more complex processing.
  - Refinement selection: InstructRM Ensemble is more general than embedding similarity but may be less efficient.

- **Failure signatures:**
  - Poor feedback incorporation: Refinements do not address the feedback, leading to minimal improvement.
  - Overfitting: The model memorizes specific refinements instead of generalizing to new inputs.
  - Reward model miscalibration: The reward model does not accurately reflect human preferences, leading to suboptimal summary selection.

- **First 3 experiments:**
  1. Validate that large language models can incorporate feedback by testing on a synthetic word-removal task.
  2. Evaluate the effectiveness of different refinement ranking methods (e.g., InstructRM Ensemble vs. embedding similarity) on a development set.
  3. Compare ILF to finetuning on human summaries and initial summaries on a summarization task to assess performance gains.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- Theoretical grounding in Bayesian inference relies on approximations (Laplace approximation and KL divergence minimization) that may not hold precisely in practice.
- Evaluation focuses on summarization tasks, leaving uncertainty about performance on other text generation domains.
- Human evaluation methodology may be subject to individual evaluator bias and inconsistency.

## Confidence
- **High confidence**: The empirical demonstration that large language models (175B parameters) can effectively incorporate language feedback, as shown by the 57.4% success rate in incorporating the most important feedback point.
- **Medium confidence**: The claim that ILF outperforms finetuning on human summaries and achieves human-level performance when combined with binary feedback, based on the 50.8% win rate against human summaries, though this is evaluated on a specific dataset and task.
- **Medium confidence**: The theoretical equivalence to Bayesian inference, which is supported by mathematical formulation but requires approximation in practical implementation.

## Next Checks
1. Test ILF on diverse text generation tasks beyond summarization (e.g., dialogue generation, code generation) to assess generalizability.
2. Systematically evaluate ILF performance across different model sizes to quantify the relationship between model capacity and feedback incorporation ability.
3. Evaluate whether ILF finetuning introduces catastrophic forgetting of the base model's capabilities or causes distribution drift over extended use.