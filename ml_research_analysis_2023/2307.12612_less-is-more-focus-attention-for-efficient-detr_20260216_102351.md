---
ver: rpa2
title: 'Less is More: Focus Attention for Efficient DETR'
arxiv_id: '2307.12612'
source_url: https://arxiv.org/abs/2307.12612
tags:
- detr
- tokens
- sparse
- foreground
- focus-detr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Focus-DETR, a method that focuses attention
  on more informative tokens to improve the trade-off between computation efficiency
  and model accuracy in DETR-like models. The core idea is to use a scoring mechanism
  that considers both localization and category semantic information from multi-scale
  feature maps to determine the semantic level of tokens.
---

# Less is More: Focus Attention for Efficient DETR

## Quick Facts
- **arXiv ID**: 2307.12612
- **Source URL**: https://arxiv.org/abs/2307.12612
- **Reference count**: 40
- **Primary result**: Focus-DETR achieves 50.4AP (+2.2) on COCO compared to Sparse DETR with similar computation cost

## Executive Summary
This paper addresses the computational inefficiency of DETR-like models by proposing Focus-DETR, which selectively focuses attention on more informative tokens. The method introduces a scoring mechanism that combines localization and category semantic information from multi-scale feature maps to identify foreground tokens. By employing dual attention in the encoder—self-attention on fine-grained object tokens followed by deformable attention on all tokens—Focus-DETR enhances semantic representation while maintaining low computational cost. The approach achieves state-of-the-art performance with improved efficiency on the COCO dataset.

## Method Summary
Focus-DETR improves DETR efficiency by using a foreground token selector (FTS) that scores tokens based on both localization and category semantic information from multi-scale feature maps. The method employs a top-down score modulation mechanism where high-level feature semantics provide guidance to lower levels through upsampling. Fine-grained object tokens are selected and enhanced through self-attention before being scattered back into the foreground tokens. The encoder uses dual attention: self-attention on the enhanced fine-grained tokens followed by deformable attention between foreground tokens and all tokens. A cascade structure progressively reduces token count across encoder layers to improve fault tolerance.

## Key Results
- Focus-DETR achieves 50.4AP on COCO validation set, outperforming Sparse DETR by +2.2AP
- The method maintains similar computational cost (GFLOPs) to Sparse DETR while providing significant accuracy improvements
- Focus-DETR equipped with top-down score modulation achieves +0.4AP improvement over baseline
- The cascade structure provides an additional +0.5AP improvement

## Why This Works (Mechanism)

### Mechanism 1
The dual attention mechanism reduces computational cost while improving semantic richness by selectively enhancing fine-grained object tokens. Tokens are scored using foreground probability and category probability, then self-attention is applied among top-scoring fine-grained tokens to enrich their semantic representation before scattering them back into foreground tokens. This works under the assumption that fine-grained object tokens contain complementary semantic information that enhances overall representation without requiring full attention over all tokens. Break condition: if scoring fails to identify truly informative tokens, dual attention enhancement becomes ineffective.

### Mechanism 2
Top-down score modulation improves multi-scale feature utilization by propagating semantic information from higher to lower levels. High-level features with richer semantics provide score information that modulates adjacent lower-level feature maps through upsampling and element-wise multiplication. This creates a cascade of semantic refinement based on the assumption that higher-level features contain more reliable semantic information. Break condition: if semantic hierarchy assumption is invalid or upsampling loses critical spatial information, modulation becomes counterproductive.

### Mechanism 3
Cascade token selection increases fault tolerance by progressively refining token sets layer by layer. Instead of keeping fixed token numbers throughout all encoder layers, the method reduces foreground tokens at each subsequent layer, allowing error correction and more focused computation. This works under the assumption that token selection errors at early layers can be compensated by more refined selection in later layers. Break condition: if cascade removes too many tokens too early, important information may be permanently lost.

## Foundational Learning

- **Transformer attention mechanisms and computational complexity**: Understanding why quadratic complexity is problematic and how sparse attention methods work. Quick check: Why does traditional self-attention have O(N²) complexity, and what are the main strategies to reduce it?
- **Multi-scale feature representation in object detection**: The method relies on leveraging features from different scales with appropriate semantic content. Quick check: How do different levels of feature maps typically capture different object scales, and why is this important for detection?
- **Object detection label assignment strategies**: The method uses a custom label assignment protocol for multi-scale features. Quick check: What are the challenges in assigning labels to tokens from multi-scale feature maps, and how does the proposed strategy address them?

## Architecture Onboarding

- **Component map**: Backbone (ResNet/Swin) → Multi-scale feature extraction → Foreground Token Selector (FTS) with top-down score modulation → Multi-category Score Predictor → Encoder with dual attention (self-attention on fine-grained tokens + deformable attention on all tokens) → Decoder (standard DETR-like structure)
- **Critical path**: 1) Extract multi-scale features from backbone, 2) Apply FTS to identify foreground tokens with score modulation, 3) Use multi-category predictor to identify fine-grained object tokens, 4) Apply dual attention in encoder (self-attention on fine-grained, deformable on all), 5) Decoder processes enhanced tokens for final predictions
- **Design tradeoffs**: Token reduction vs. information retention (more aggressive pruning reduces computation but risks losing important tokens), cascade structure adds complexity but improves fault tolerance, dual attention adds minimal computation but requires reliable scoring mechanism
- **Failure signatures**: Performance drops if scoring mechanism fails to identify informative tokens, over-aggressive token pruning leads to loss of detection capability, incorrect label assignment in FTS causes poor foreground selection, top-down modulation fails if high-level semantic information is unreliable
- **First 3 experiments**: 1) Validate scoring mechanism by comparing foreground selection accuracy with ground truth, 2) Test cascade structure by varying token reduction ratios across layers, 3) Evaluate dual attention contribution by comparing with and without fine-grained token enhancement

## Open Questions the Paper Calls Out

### Open Question 1
How does the dual attention mechanism perform when applied to other DETR-like models beyond Sparse DETR, Deformable DETR, and DN-Deformable-DETR? The paper mentions dual attention can be independently embedded into other models but only tests a few with mixed results, indicating performance may vary depending on model architecture. Testing dual attention on a diverse set of DETR-like models with different query construction strategies would provide clearer understanding of its generalizability.

### Open Question 2
What is the impact of the overlap in scale intervals between adjacent feature maps on overall detection performance? The paper discusses overlap in scale intervals and claims it improves detection accuracy, but doesn't provide detailed analysis of this impact. Conducting experiments with different degrees of overlap in scale intervals and measuring corresponding changes in detection performance would help determine optimal overlap and its impact on accuracy.

### Open Question 3
How does the proposed scoring mechanism compare to other token selection strategies in terms of computational efficiency and detection accuracy? The paper introduces a scoring mechanism considering both localization and category semantic information but doesn't benchmark it against other state-of-the-art token selection strategies. Comparing the proposed scoring mechanism to other strategies based on entropy, gradient-based methods, or reinforcement learning would provide clearer picture of relative strengths and weaknesses.

## Limitations
- Implementation details remain unspecified, particularly the exact architecture of MLPs in FTS and multi-category score predictor modules
- Performance gains may be partially attributed to implementation-specific choices rather than the proposed method alone
- Computational complexity analysis focuses on theoretical GFLOPs without empirical runtime measurements across different hardware configurations
- Method's generalization to other detection tasks (instance segmentation, keypoint detection) remains untested

## Confidence

- **High Confidence**: Core concept of using semantic scores to filter tokens is well-founded and aligns with established principles in attention-based models. Computational complexity analysis is methodologically sound.
- **Medium Confidence**: Dual attention mechanism is theoretically plausible but effectiveness depends heavily on quality of scoring mechanism. Reported performance improvements are significant but may be partially implementation-dependent.
- **Low Confidence**: Top-down score modulation mechanism's contribution to performance is not conclusively demonstrated through ablation studies. Cascade structure's benefits could be dataset-specific.

## Next Checks
1. **Ablation Study Validation**: Conduct controlled experiments to isolate individual contributions of top-down score modulation, cascade structure, and dual attention by training versions with each component disabled while keeping all other factors constant.

2. **Scoring Mechanism Quality Assessment**: Implement quantitative metrics to evaluate correlation between proposed scoring mechanism and actual object importance. Compare foreground token selection accuracy against ground truth annotations across different object scales and categories.

3. **Computational Efficiency Verification**: Measure actual inference latency and memory usage on different hardware platforms (GPU and CPU) to validate theoretical GFLOPs calculations. Compare against other efficient DETR variants under identical hardware conditions.