---
ver: rpa2
title: A Confidence-based Acquisition Model for Self-supervised Active Learning and
  Label Correction
arxiv_id: '2310.08944'
source_url: https://arxiv.org/abs/2310.08944
tags:
- learning
- label
- labels
- confidence
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAMEL, a pool-based active learning framework
  tailored for sequential multi-output tasks. CAMEL reduces annotation effort by having
  experts label only a subset of time-steps and output categories, while the model
  self-labels the rest using confidence estimates.
---

# A Confidence-based Acquisition Model for Self-supervised Active Learning and Label Correction

## Quick Facts
- **arXiv ID**: 2310.08944
- **Source URL**: https://arxiv.org/abs/2310.08944
- **Reference count**: 23
- **Primary result**: Achieves 95% of full-training performance using only 16% of expert labels on dialogue belief tracking task

## Executive Summary
This paper introduces CAMEL, a pool-based active learning framework tailored for sequential multi-output tasks. CAMEL reduces annotation effort by having experts label only a subset of time-steps and output categories, while the model self-labels the rest using confidence estimates. It also includes a label validation mechanism to filter out potentially incorrect human labels. Evaluated on dialogue belief tracking, CAMEL achieves 95% of full-training performance using only 16% of expert labels. Experiments also show that CAMEL's automatic label corrections improve dataset quality and model performance.

## Method Summary
CAMEL is a four-stage iterative active learning framework that combines data selection based on prediction confidence, expert labeling, label validation to filter noisy human annotations, and semi-supervised learning with self-labeled data. The method uses ensemble-based uncertainty estimation to identify low-confidence predictions for expert annotation while automatically labeling high-confidence instances. A separate label confidence estimator evaluates the quality of human labels using a noisy dataset, discarding labels below a confidence threshold. The framework is evaluated on dialogue belief tracking (MultiWOZ 2.1) and machine translation (WMT17 DE-EN) tasks.

## Key Results
- CAMEL achieves 95% of full-training performance using only 16% of expert labels on dialogue belief tracking task
- CAMEL outperforms existing active learning approaches in terms of robustness and data efficiency
- Automatic label correction improves dataset quality and model performance on MultiWOZ 2.1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model reduces labeling effort by selectively querying only uncertain predictions and self-labeling the rest with high confidence.
- Mechanism: At each iteration, the model estimates prediction confidence for each time-step and category, selects instances where confidence falls below a threshold αsel, and labels the rest automatically using its own predictions.
- Core assumption: The model's confidence estimates are well-calibrated and accurately reflect prediction reliability.
- Evidence anchors:
  - [abstract] "CAMEL possesses two core features: (1) it requires expert annotators to label only a fraction of a chosen sequence, and (2) it facilitates self-supervision for the remainder of the sequence."
  - [section] "Stage 1: Data selection... Instances in which the model displays low confidence (confidence below the αsel threshold) are selected."
  - [corpus] Weak: Corpus contains papers on active learning but no direct evidence about CAMEL's specific selection-threshold mechanism.
- Break condition: If the confidence estimator is poorly calibrated, the model may select uninformative instances or incorrectly self-label critical ones, leading to performance degradation.

### Mechanism 2
- Claim: The label validation mechanism prevents noisy human labels from contaminating the training set.
- Mechanism: After expert labeling, each label is assigned a confidence score by a separate label confidence estimator. Labels with confidence below αval are discarded, and only reliable labels plus self-labels are retained.
- Core assumption: The label confidence estimator can distinguish correct from incorrect human annotations, even when the original model was uncertain.
- Evidence anchors:
  - [abstract] "It employs a label validation mechanism to prevent erroneous labels from contaminating the dataset and harming model performance."
  - [section] "Stage 3: Label validation... We can consider the expert labels... below a threshold αval to be potentially incorrect."
  - [corpus] Weak: Corpus has related work on label correction but no evidence specific to CAMEL's dual-uncertainty model.
- Break condition: If the noisy dataset generation is too noisy or the estimator overfits, many correct labels may be discarded, reducing training data quality.

### Mechanism 3
- Claim: Inter-category encoder allows the model to leverage correlations between labels, improving confidence estimation accuracy.
- Mechanism: Confidence estimation incorporates features from both intra-category uncertainty (per-label) and inter-category uncertainty (across all labels at a time-step), enabling the model to capture dependencies.
- Core assumption: Labels at the same time-step are not independent; their joint distribution carries useful information for confidence calibration.
- Evidence anchors:
  - [section] "Both the inter- and intra-category encoders are linear fully connected layers... The inter-category encoder allows the model to take advantage of any correlations between categories, which was not done by Xie et al. (2018)."
  - [corpus] Weak: No corpus evidence directly supporting the benefit of inter-category features in confidence estimation.
- Break condition: If label categories are truly independent or correlations are spurious, adding inter-category features may introduce noise and degrade performance.

## Foundational Learning

- **Concept**: Confidence calibration in neural networks
  - Why needed here: CAMEL's active learning and label validation rely on accurate confidence estimates; uncalibrated probabilities would lead to poor selection and validation decisions.
  - Quick check question: If a model assigns 90% confidence to 100 predictions, how many should actually be correct for the model to be perfectly calibrated?

- **Concept**: Semi-supervised learning with pseudo-labels
  - Why needed here: CAMEL uses self-supervision by trusting its own high-confidence predictions as pseudo-labels; understanding when this is safe is critical.
  - Quick check question: Under what conditions is it safe to use a model's own predictions as training labels for the next iteration?

- **Concept**: Active learning acquisition functions
  - Why needed here: CAMEL's data selection is based on uncertainty; understanding how different acquisition functions trade off exploration and exploitation is key to tuning αsel.
  - Quick check question: How does an uncertainty-based acquisition function differ from a diversity-based one in terms of data selection behavior?

## Architecture Onboarding

- **Component map**: Learning Model (CE-SetSUMBT or T5) -> Prediction Confidence Estimator -> Selection Module -> Expert Labeling -> Label Confidence Estimator -> Validation Module -> Self-Labeling Module -> Retraining Loop

- **Critical path**:
  1. Initial seed dataset → Train learning model.
  2. Learning model → Prediction confidence scores.
  3. Selection module → Choose sequences for labeling.
  4. Expert labeling + self-labeling → Combined labeled data.
  5. Label confidence estimator → Filter unreliable human labels.
  6. Retrain learning model with cleaned data.
  7. Repeat until performance threshold or full labeling.

- **Design tradeoffs**:
  - Ensemble size vs. calibration accuracy: Larger ensembles improve uncertainty quality but increase compute cost.
  - αsel tightness vs. labeling efficiency: Tighter thresholds reduce labeling effort but may miss informative examples.
  - αval strictness vs. data retention: Stricter thresholds improve data quality but reduce training set size.

- **Failure signatures**:
  - Performance plateaus despite more labeling → Confidence estimators are miscalibrated or αsel too tight.
  - Training loss increases → Too many correct labels discarded by validation or too much noise in self-labels.
  - Overfitting to small seed → Initial model is too weak to generate reliable confidence scores.

- **First 3 experiments**:
  1. Train baseline learning model on seed dataset; measure calibration error (ECE) on validation set.
  2. Run one CAMEL iteration with αsel=0.5; inspect selected vs. self-labeled examples and their confidence scores.
  3. Run label validation on a small manually labeled subset; compute precision/recall of filtering incorrect labels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of uncertainty estimation method affect the performance of CAMEL across different sequential multi-output tasks?
- Basis in paper: [explicit] The paper discusses the importance of well-calibrated uncertainty estimates for CAMEL's success and mentions that CAMEL outperforms baselines in terms of robustness and data efficiency.
- Why unresolved: While the paper demonstrates CAMEL's effectiveness, it doesn't explore the impact of different uncertainty estimation methods on its performance across various tasks.
- What evidence would resolve it: Experiments comparing CAMEL's performance using different uncertainty estimation methods (e.g., Monte Carlo dropout, deep ensembles, deterministic uncertainty quantification) across multiple sequential multi-output tasks would provide insights into the optimal approach for different scenarios.

### Open Question 2
- Question: Can CAMEL be effectively applied to non-dialogue sequential multi-output tasks, such as object tracking or pose detection, and what are the specific challenges and adaptations required?
- Basis in paper: [explicit] The paper mentions that CAMEL has potential for broad applicability across various sequential multi-output tasks, including object tracking and pose detection, but doesn't provide specific examples or experiments in these domains.
- Why unresolved: The paper focuses on dialogue belief tracking and machine translation, leaving the question of CAMEL's effectiveness in other sequential multi-output tasks unanswered.
- What evidence would resolve it: Applying CAMEL to tasks like object tracking or pose detection, along with a detailed analysis of the challenges and adaptations required for each task, would demonstrate its versatility and limitations.

### Open Question 3
- Question: How does the noise level in the dataset impact CAMEL's performance, and is there a threshold beyond which CAMEL's label correction mechanism becomes ineffective?
- Basis in paper: [explicit] The paper highlights CAMEL's robustness to annotation errors and its ability to correct noisy labels, but doesn't explore the impact of varying noise levels on its performance.
- Why unresolved: While the paper demonstrates CAMEL's effectiveness in correcting label noise, it doesn't investigate the relationship between noise level and performance or identify potential limitations.
- What evidence would resolve it: Experiments systematically varying the noise level in datasets and evaluating CAMEL's performance would reveal its sensitivity to noise and potential thresholds for effectiveness.

## Limitations
- Confidence estimation reliability is critical but not explicitly validated with calibration metrics like Expected Calibration Error
- Label validation mechanism's effectiveness is demonstrated on only one dataset with limited ablation studies
- Claims of broad applicability across sequential multi-output tasks are based on only two dataset types

## Confidence
- **High confidence**: Core claim that CAMEL reduces annotation effort while maintaining performance (95% at 16% labels) is well-supported by dialogue belief tracking experiments
- **Medium confidence**: Label correction mechanism's effectiveness is demonstrated on MultiWOZ 2.1 but only briefly mentioned for WMT17
- **Low confidence**: Generality claim across "sequential multi-output tasks" is based on only two datasets

## Next Checks
1. Compute and report Expected Calibration Error (ECE) for confidence estimators on validation sets across all datasets
2. Run ablation study removing inter-category encoder to quantify its contribution to performance
3. Manually inspect a random sample of labels filtered by αval across different confidence ranges to estimate precision and recall of validation mechanism