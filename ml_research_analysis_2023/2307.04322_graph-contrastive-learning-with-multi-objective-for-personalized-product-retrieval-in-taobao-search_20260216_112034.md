---
ver: rpa2
title: Graph Contrastive Learning with Multi-Objective for Personalized Product Retrieval
  in Taobao Search
arxiv_id: '2307.04322'
source_url: https://arxiv.org/abs/2307.04322
tags:
- items
- learning
- graph
- search
- gcl-mo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GCL-MO is a Graph Contrastive Learning with Multi-Objective model
  for personalized product retrieval in e-commerce search, proposed to address weak
  relevance and incomplete personalization issues in collaborative filtering. The
  method constructs a two-level graph structure and optimizes four objectives (relevance,
  exposure, click, purchase) using a modified contrastive loss to improve long-tail
  item representation learning.
---

# Graph Contrastive Learning with Multi-Objective for Personalized Product Retrieval in Taobao Search

## Quick Facts
- arXiv ID: 2307.04322
- Source URL: https://arxiv.org/abs/2307.04322
- Reference count: 40
- Primary result: +0.36% GMV improvement in Taobao search online A/B testing

## Executive Summary
This paper proposes GCL-MO, a Graph Contrastive Learning with Multi-Objective model for personalized product retrieval in e-commerce search. The method addresses two key challenges: weak relevance in collaborative filtering approaches and incomplete personalization from content-based methods. By constructing a two-level graph structure and optimizing four objectives (relevance, exposure, click, purchase) through a modified contrastive loss, the model significantly improves both offline metrics and online performance metrics including GMV, good rate, and click/purchase rates.

## Method Summary
GCL-MO constructs a two-level compositional graph where the objective-level captures direct relationships between triggers and exposed items, while the neighbor-level captures co-occurrence patterns within categories. The model learns item embeddings through multi-objective optimization of relevance, exposure, click, and purchase behaviors, using a modified contrastive loss that removes mutual suppression among positive samples. Graph data augmentation through node dropping and edge perturbation creates multiple views for contrastive learning, enabling robust representation learning particularly for long-tail items.

## Key Results
- +0.36% GMV, +0.78% good rate, +1.53% click rate, and +0.98% purchase rate improvements in online A/B testing
- 4.03% and 4.16% improvement in Recall@K and Recall_p@K respectively in offline evaluation
- 3.38% improvement in long-tail impression rate, demonstrating effectiveness for long-tail item retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-objective optimization prevents weak relevance by incorporating relevance constraints into item similarity learning
- Mechanism: The model optimizes four objectives simultaneously using a modified contrastive loss, where relevance labels are derived from an online relevance estimation model. This ensures retrieved items satisfy both collaborative signal and query relevance.
- Core assumption: Query-relevance constraints can be effectively modeled through multi-objective optimization without overwhelming the collaborative signal.
- Evidence anchors:
  - [abstract]: "However, existing Graph-based methods ignore user's multiple behaviours, such as click/purchase and the relevance constraint between user behaviours and items."
  - [section 3.3.1]: "Relevance Label: This label defines that two items are relevant if they are both relevant to the same query. Whether the item is relevant to the query, we use an online relevance estimation model to predict."
- Break condition: If the relevance estimation model is inaccurate or biased, the relevance constraint may degrade rather than improve overall retrieval quality.

### Mechanism 2
- Claim: Two-level graph structure enables both direct objective relationships and neighbor-based generalization
- Mechanism: Objective-level edges connect triggers to exposed items with four relationship types, while neighbor-level edges capture co-occurrence patterns within categories. This dual structure allows direct optimization of trigger-candidate relationships while leveraging neighborhood information for long-tail item learning.
- Core assumption: The two-level graph structure can effectively separate and optimize different types of relationships without interference.
- Evidence anchors:
  - [section 3.2]: "We adopt a two-level compositional approach to characterize the connections between items in the graph structure. The structure of the first level represents the objective learning relationship between items, and the structure of the second level represents the neighbor co-occurrence relationship between items."
  - [section 3.2.1]: "At the objective-level, triggers are on the left and all exposed items are on the right."
- Break condition: If neighbor-level relationships become too sparse for long-tail items, the neighbor-level structure may not provide sufficient signal for effective learning.

### Mechanism 3
- Claim: Contrastive learning with modified loss function improves long-tail item representation robustness
- Mechanism: Graph data augmentation through node dropping and edge perturbation creates multiple views of the same items. The modified contrastive loss removes mutual suppression among positive samples by excluding other positive samples from the denominator, preventing long-tail positives from being dominated by head item signals.
- Core assumption: Removing other positive samples from the contrastive loss denominator prevents score inflation without losing discrimination power.
- Evidence anchors:
  - [section 3.4.2]: "We propose a new contrastive loss function that can eliminate the mutual suppression of multiple positive examples."
  - [section 4.4.3]: "Especially when removing other positive samples in AP GCL-MO loss function, our GCL-MO method further improved ùëÉùëô, which means our proposed contrastive loss can retrieve more long-tail items."
- Break condition: If the temperature parameter œÑ is not properly tuned, the smoothing effect may either be too weak to help or too strong to maintain discrimination.

## Foundational Learning

- Concept: Graph neural networks and message passing
  - Why needed here: The model aggregates neighbor information through attention-based aggregators to learn item representations that capture collaborative patterns.
  - Quick check question: How does the attention-based aggregator differ from simple mean aggregation in terms of learning item representations?

- Concept: Multi-objective optimization and loss balancing
  - Why needed here: The model must balance four different objectives (relevance, exposure, click, purchase) with potentially conflicting gradients.
  - Quick check question: What happens to the model's behavior if one objective's weight is set much higher than others?

- Concept: Contrastive learning and data augmentation
  - Why needed here: Contrastive learning helps the model learn robust representations by comparing augmented views of items, particularly important for long-tail item generalization.
  - Quick check question: Why does removing other positive samples from the contrastive loss denominator help long-tail items?

## Architecture Onboarding

- Component map: Trigger processing ‚Üí Two-level graph construction ‚Üí Multi-objective learning ‚Üí Contrastive learning ‚Üí Embedding output ‚Üí Inverted index construction
- Critical path: User query ‚Üí Trigger selection ‚Üí Graph-based similarity computation ‚Üí Top-K retrieval from inverted index
- Design tradeoffs: Direct trigger-candidate optimization vs. indirect neighbor-based learning; single-level vs. two-level graph structure; standard vs. modified contrastive loss
- Failure signatures: Poor relevance metrics suggest relevance estimation issues; low long-tail recall suggests contrastive learning parameters need tuning; overall performance degradation may indicate loss weight imbalance
- First 3 experiments:
  1. Test the impact of removing each objective loss (relevance, exposure, click, purchase) individually to understand their contributions
  2. Compare performance with and without the contrastive learning component to measure its impact on long-tail items
  3. Evaluate different temperature parameters œÑ in the contrastive loss to find optimal smoothing level

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed contrastive loss function perform in domains beyond e-commerce product retrieval, such as general classification tasks with multiple positive samples?
- Basis in paper: [explicit] The authors explicitly state that their newly proposed contrastive loss function can be applied to other domains, such as classification tasks with multiple positive samples.
- Why unresolved: The paper only provides theoretical justification and does not present empirical results in other domains.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of the contrastive loss function in other domains like image classification or natural language processing tasks.

### Open Question 2
- Question: What is the optimal strategy for balancing the four objectives (relevance, exposure, click, purchase) in the multi-objective optimization framework, and how sensitive is the model performance to these weight parameters?
- Basis in paper: [explicit] The authors introduce a loss weight for each objective (w_o) but do not provide a systematic method for determining optimal weights or sensitivity analysis.
- Why unresolved: The paper does not discuss hyperparameter tuning for the objective weights or analyze their impact on model performance.
- What evidence would resolve it: A sensitivity analysis showing how different weight combinations affect the four metrics, or an automated method for determining optimal weights.

### Open Question 3
- Question: How does the two-level graph structure compare to alternative graph architectures (e.g., three-level or heterogeneous graphs) for modeling item relationships in e-commerce search?
- Basis in paper: [inferred] The authors propose a two-level graph structure but do not compare it with alternative architectures or provide justification for choosing exactly two levels.
- Why unresolved: The paper does not explore or compare other possible graph architectures that could potentially capture item relationships more effectively.
- What evidence would resolve it: Experimental results comparing the two-level structure with alternative graph architectures, including quantitative metrics and ablation studies.

## Limitations
- The paper lacks detailed specifications of the aggregator architecture and neighbor sampling probability distribution, which are critical for faithful reproduction.
- The temperature parameter œÑ and loss weight configurations are not explicitly defined, leaving significant hyperparameter tuning uncertainty.
- The specific contribution of the two-level graph structure is not independently validated through controlled ablation studies.

## Confidence
- Multi-objective optimization effectiveness: **High** - Strong empirical evidence from offline and online experiments with clear performance improvements across multiple metrics
- Two-level graph structure contribution: **Medium** - Theoretical justification is sound, but ablation studies specifically isolating the two-level structure impact are limited
- Contrastive loss modification benefits: **Medium** - Online results show improvements, but the specific contribution of removing positive samples from the denominator is not independently validated through controlled experiments

## Next Checks
1. Conduct ablation study isolating the two-level graph structure impact by comparing with single-level alternatives while keeping all other components constant
2. Test the modified contrastive loss against standard contrastive loss with identical augmentation strategies to quantify the specific contribution of removing positive samples from the denominator
3. Validate the robustness of the model by testing on datasets with different popularity distributions and evaluating whether the long-tail improvements generalize beyond the Taobao search environment