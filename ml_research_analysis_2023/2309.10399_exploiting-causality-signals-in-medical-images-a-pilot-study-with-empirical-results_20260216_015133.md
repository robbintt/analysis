---
ver: rpa2
title: 'Exploiting Causality Signals in Medical Images: A Pilot Study with Empirical
  Results'
arxiv_id: '2309.10399'
source_url: https://arxiv.org/abs/2309.10399
tags:
- causality
- feature
- images
- image
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method to discover and exploit weak
  causal signals within images for classification purposes. The method uses a convolutional
  neural network backbone and a causality-factors extractor module, which computes
  weights to enhance each feature map according to its causal influence in the scene.
---

# Exploiting Causality Signals in Medical Images: A Pilot Study with Empirical Results

## Quick Facts
- arXiv ID: 2309.10399
- Source URL: https://arxiv.org/abs/2309.10399
- Reference count: 3
- Primary result: Proposed method achieves 71.82% accuracy on prostate MRI dataset, 3.44% higher than baseline

## Executive Summary
This paper introduces a novel method to discover and exploit weak causal signals within images for classification purposes. The method uses a convolutional neural network backbone and a causality-factors extractor module, which computes weights to enhance each feature map according to its causal influence in the scene. The authors develop different architecture variants and empirically evaluate all the models on two public datasets of prostate MRI images and breast histopathology slides for cancer diagnosis. Results show the proposed method improves overall classification accuracy and produces more robust predictions that focus on relevant image parts.

## Method Summary
The proposed method computes pairwise conditional probabilities between feature maps from the last convolutional layer to identify causal relationships. These relationships are encoded as weights that multiply the feature maps before classification. The architecture uses a ResNet18 backbone with the AdaptiveAvgPool2D layer replaced by an identity layer to preserve 2D structure. The causality-factors extractor module processes the causality map row-wise and column-wise to count how often each feature causes or is caused by others, creating weights that either preserve magnitude (full) or binarize to 0/1 (bool). The method is evaluated using both Max and Lehmer methods for computing joint probabilities, with different architecture variants (Cat, Mulcat-full, Mulcat-bool) for exploiting causality information.

## Key Results
- The proposed method achieves 71.82% accuracy on the prostate MRI dataset, outperforming the baseline model by 3.44%
- The Mulcat-full-causes variant shows the best performance across all tested configurations
- Causality-driven models produce more robust predictions that focus on relevant parts of the image, as demonstrated by Grad-CAM visualizations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weak causality signals between image features can be extracted via asymmetric conditional probabilities and used to enhance CNN feature maps.
- Mechanism: The method computes pairwise conditional probabilities between feature maps using either Max or Lehmer methods, then uses asymmetries (ğ‘ƒ(ğ¹ğ‘–|ğ¹ğ‘—) â‰  ğ‘ƒ(ğ¹ğ‘—|ğ¹ğ‘–)) to determine causal relationships. These relationships are encoded as weights that multiply the feature maps before classification.
- Core assumption: Feature maps from the last convolutional layer can be interpreted as probability distributions of feature presence, and their pairwise conditional relationships encode meaningful causal information.
- Evidence anchors:
  - [abstract]: "This way, we model how the presence of a feature in one part of the image affects the appearance of another feature in a different part of the image."
  - [section]: "We interpret asymmetries in such probability estimates as weak causality signals between features, as they provide some information on the cause-effect of the appearance of a feature in one place of the image, given the presence of another feature within some other places of the image."

### Mechanism 2
- Claim: Causality direction (causes vs effects) and weighing mode (full vs bool) control how feature maps are weighted based on their causal importance.
- Mechanism: The causality factors extractor processes the causality map row-wise to count how often each feature causes others, and column-wise to count how often each feature is caused by others. The difference between these counts (with sign reversal based on direction) creates weights that either preserve magnitude (full) or binarize to 0/1 (bool).
- Core assumption: The frequency of causal relationships in the causality map correlates with the importance of features for classification.
- Evidence anchors:
  - [section]: "Accordingly, a feature may be deemed to be the reason for another feature when ğ‘ƒ(ğ¹ğ‘–|ğ¹ğ‘—) > ğ‘ƒ(ğ¹ğ‘—|ğ¹ğ‘–), that is (ğ¹ğ‘– â†’ ğ¹ğ‘—), and vice versa."
  - [section]: "When ğ‘‘ = causes, the vector of causes (obtained row-wise) is not altered, while the sign is changed to the elements of the effects vector (obtained column-wise)."

### Mechanism 3
- Claim: Embedding causality information through feature map multiplication (mulcat) is more effective than simple concatenation (cat) for improving classification accuracy.
- Mechanism: Instead of just concatenating flattened causality maps to feature maps, the mulcat approach multiplies feature maps by learned weights derived from causality factors, then concatenates the weighted maps. This creates a 2Ã—ğ‘›Ã—ğ‘›Ã—ğ‘˜ input to the classifier.
- Core assumption: Directly emphasizing features based on their causal importance provides more discriminative power than simply providing additional causality information as features.
- Evidence anchors:
  - [section]: "Alternatively, one could enhance or penalize parts of the existing information according to the newly gained one. Our proposition here is a new way to exploit the causality map: this time, it is used to compute a vector of causality factors that multiply (i.e., weighs) the feature maps."
  - [table]: "Mulcat-full-causes achieved 71.82 accuracy vs Cat baseline at 70.07."

## Foundational Learning

- Concept: Conditional probability and joint probability relationship
  - Why needed here: The method relies on computing ğ‘ƒ(ğ¹ğ‘–|ğ¹ğ‘—) = ğ‘ƒ(ğ¹ğ‘–,ğ¹ğ‘—)/ğ‘ƒ(ğ¹ğ‘—) to estimate causal relationships between feature maps.
  - Quick check question: If ğ‘ƒ(ğ¹ğ‘–,ğ¹ğ‘—) = 0.3 and ğ‘ƒ(ğ¹ğ‘—) = 0.5, what is ğ‘ƒ(ğ¹ğ‘–|ğ¹ğ‘—)?

- Concept: Lehmer mean and its parameter p
  - Why needed here: The Lehmer method for computing joint probabilities uses the generalized Lehmer mean with parameter p to interpolate between different types of means.
  - Quick check question: What Lehmer mean value (p=-2, -1, 0, 1) corresponds to the arithmetic mean?

- Concept: Class activation maps (CAM) and Grad-CAM
  - Why needed here: The paper uses Grad-CAM to visualize which image regions the network focuses on for classification, comparing baseline vs causality-driven models.
  - Quick check question: What layer is typically targeted for CAM computation in a CNN classification network?

## Architecture Onboarding

- Component map: ResNet18 backbone â†’ Identity layer (replacing AdaptiveAvgPool2D) â†’ Causality factors extractor (optional) â†’ Classifier
- Critical path: Input image â†’ Feature extraction â†’ Causality map computation â†’ Weight computation â†’ Feature map enhancement â†’ Classification
- Design tradeoffs: Replacing AdaptiveAvgPool2D with identity preserves 2D structure needed for causality detection but may require handling variable input sizes; mulcat adds parameters but provides better performance than cat
- Failure signatures: If causality maps show uniform values or random patterns, the model degrades to baseline performance; if weights are all zeros, feature maps are suppressed
- First 3 experiments:
  1. Implement baseline ResNet18 with identity layer replacement and verify it matches original performance
  2. Add causality map computation using Max method and cat option, compare with baseline
  3. Implement mulcat-full-causes variant with Lehmer method (p=-100) and compare performance on validation set

## Open Questions the Paper Calls Out
- How would the proposed causality-driven CNN architecture perform on other medical imaging datasets beyond prostate MRI, such as CT scans or ultrasound images?
- Can the causality map computation be improved by allowing the network to learn the parameter p in the Lehmer mean calculation, rather than using a fixed value?
- How would the proposed method perform in few-shot learning scenarios, where only a small number of labeled examples are available?

## Limitations
- Only evaluated on a single prostate MRI dataset and one backbone architecture
- Computationally expensive causality map computation may limit scalability to larger images or 3D volumes
- Empirical validation of causal interpretation is limited to performance improvements rather than causal verification

## Confidence
- Mechanism 1 (Conditional probability causality): Medium - mathematically sound but empirical validation of causal interpretation is limited
- Mechanism 2 (Causality direction weighting): Medium - intuitive but depends heavily on quality of causality map estimation  
- Mechanism 3 (Feature map multiplication): High - demonstrated performance improvement with clear implementation path

## Next Checks
1. Test the method on a completely different medical imaging dataset (e.g., chest X-rays for pneumonia detection) to assess generalizability
2. Conduct ablation studies comparing causality-driven weighting against alternative feature importance methods (attention mechanisms, saliency maps)
3. Implement statistical tests to verify that observed accuracy improvements are significant and not due to random variation in the experimental setup