---
ver: rpa2
title: 'DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding
  Long and Specialized Documents'
arxiv_id: '2311.09805'
source_url: https://arxiv.org/abs/2311.09805
tags:
- llms
- reasoning
- numerical
- performance
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DOCMATH-EVAL, a benchmark designed to evaluate
  large language models' numerical reasoning capabilities in financial document analysis.
  The benchmark includes four evaluation sets with varying difficulty levels, focusing
  on problems requiring complex numerical reasoning over long documents containing
  both text and tables.
---

# DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Long and Specialized Documents

## Quick Facts
- arXiv ID: 2311.09805
- Source URL: https://arxiv.org/abs/2311.09805
- Reference count: 10
- Key outcome: GPT-4 achieves 38.8% accuracy on most challenging financial reasoning tasks, significantly lagging behind human experts at 76%

## Executive Summary
This paper introduces DOCMATH-EVAL, a benchmark designed to evaluate large language models' numerical reasoning capabilities on financial documents containing both text and tables. The benchmark includes four evaluation sets with varying difficulty levels, from simple short documents to complex long documents requiring integration of information across multiple tables. The authors evaluate 19 LLMs using Chain-of-Thought and Program-of-Thought prompting methods, revealing significant performance gaps between current LLMs and human experts on the most challenging tasks.

## Method Summary
The authors create DOCMATH-EVAL, a benchmark with four evaluation sets varying in document length and problem complexity. They evaluate 19 LLMs using two prompting methods: Chain-of-Thought (CoT) for intermediate reasoning steps, and Program-of-Thought (PoT) for generating executable Python code. The evaluation uses retrieval-augmented generation for long documents, extracting top-10 relevant evidence passages before applying LLM reasoning. Performance is measured against ground truth answers and compared to human expert performance.

## Key Results
- GPT-4 achieves 38.8% accuracy on the most challenging DMCompLong set using PoT prompting
- Human experts achieve 76% accuracy on the same challenging set, highlighting significant LLM limitations
- PoT prompting consistently outperforms CoT for GPT models and code-based LLMs
- Despite finance specialization, Pixiu underperforms general-purpose Llama-2 models
- Simple problems (DMSimpShort) are solved with higher accuracy across all models compared to complex problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's superior performance stems from its ability to generate executable Python programs that combine numerical reasoning with financial domain knowledge.
- Mechanism: The Program-of-Thoughts prompting method enables GPT-4 to produce structured Python code that systematically processes tabular data and applies mathematical operations, leveraging its training on code datasets.
- Core assumption: The Python code generation capability directly translates to better numerical reasoning performance on financial documents.
- Evidence anchors:
  - [abstract]: "We conduct an extensive evaluation encompassing a wide range of LLMs, including those specialized in coding and finance."
  - [section]: "We observe that the PoT prompting method consistently improves performance over the CoT method in GPT-* models and code-based LLMs."
- Break condition: If the generated Python code becomes too complex or contains execution errors, the advantage diminishes significantly.

### Mechanism 2
- Claim: The difficulty gap between simple and complex financial problems reveals limitations in LLMs' ability to handle long-range dependencies and context integration.
- Mechanism: Simple problems (DMSimpShort) require only basic arithmetic on short documents, while complex problems (DMCompLong) demand integration of information across multiple tables and sections, which exceeds current context window capabilities.
- Core assumption: The performance drop is primarily due to document length and complexity rather than fundamental reasoning limitations.
- Evidence anchors:
  - [abstract]: "GPT-4 significantly outperforms other open-source LLMs, achieving an accuracy of 38.8% on the most challenging evaluation set (i.e.,DMCompLong) when applying PoT prompting. However, it still lags far behind human expert performance, which stands at 76%."
  - [section]: "This performance gap highlights the limitations of current LLMs in real-world numerical reasoning applications within expert domains."
- Break condition: If retrieval methods improve to provide more relevant context, the performance gap may narrow without architectural changes.

### Mechanism 3
- Claim: Financial domain specialization alone does not guarantee superior performance on numerical reasoning tasks.
- Mechanism: Despite Pixiu being specifically trained for finance, it underperforms general-purpose Llama-2 models, suggesting that broad reasoning capabilities may be more important than domain-specific knowledge for this task.
- Core assumption: Domain specialization is less effective than general reasoning and programming abilities for this type of numerical reasoning.
- Evidence anchors:
  - [abstract]: "Furthermore, we provide bonuses for questions requiring financial expertise or involving complex mathematical operations."
  - [section]: "Despite its specialization in the finance domain, Pixiu does not demonstrate competitive performance in DOCMATH-EVAL."
- Break condition: If financial problems become more dependent on specialized terminology and domain-specific concepts rather than numerical reasoning.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CoT helps LLMs break down complex numerical reasoning problems into intermediate reasoning steps, improving performance on multi-step calculations.
  - Quick check question: How does CoT differ from standard prompting in terms of output structure?

- Concept: Program-of-Thoughts (PoT) prompting
  - Why needed here: PoT leverages LLMs' code generation capabilities to produce executable Python solutions, combining natural language reasoning with formal computational steps.
  - Quick check question: What advantages does PoT have over CoT when dealing with tabular data and mathematical operations?

- Concept: Document retrieval for long contexts
  - Why needed here: Financial documents often exceed LLM context windows, requiring efficient retrieval of relevant evidence from long documents before reasoning can occur.
  - Quick check question: Why is retrieval performance critical for DMCompLong but less so for DMSimpShort?

## Architecture Onboarding

- Component map: Retriever -> Context Processor -> LLM (with CoT/PoT) -> Answer Extractor -> Python Executor
- Critical path: Question -> Retrieval (top-10 evidence) -> LLM Prompting -> Code Generation -> Execution -> Answer
- Design tradeoffs: Balance between retrieval quality and computational cost; trade-off between code complexity and execution reliability
- Failure signatures: Low execution rate of generated Python code; retrieval misses critical evidence; context window limitations causing incomplete information
- First 3 experiments:
  1. Compare CoT vs PoT prompting on DMSimpShort with GPT-4 to establish baseline performance difference
  2. Test different retriever models (BM25 vs Ada vs Contriever) on DMCompLong to measure impact on final accuracy
  3. Vary the number of retrieved evidence passages (top-5 vs top-10 vs top-20) to find optimal retrieval size for accuracy vs efficiency

## Open Questions the Paper Calls Out

1. How do LLMs perform on numerical reasoning tasks in other expert domains beyond finance, such as medicine or law? The paper mentions that future research could explore LLMs' capabilities in other expert domains like medicine or science.

2. What is the optimal balance between the length of retrieved document context and the accuracy of numerical reasoning for long documents? The paper uses a fixed top-10 retrieval setting but doesn't explore how varying the amount of retrieved context affects performance.

3. Why do some general LLMs like Mistral and WizardLM show degraded performance with Program-of-Thoughts prompting compared to Chain-of-Thoughts? The paper observes this performance degradation but doesn't explain why these specific models struggle to generate executable Python code.

## Limitations
- The benchmark focuses specifically on financial documents, limiting generalizability to other domains requiring numerical reasoning
- The evaluation does not explicitly account for the reliability and correctness of generated Python programs
- The human expert baseline represents a single group's performance rather than a broader distribution of expertise

## Confidence
- Medium: Claims about GPT-4's superior performance using Program-of-Thought prompting
- Medium: Claims about the effectiveness of Chain-of-Thought vs Program-of-Thought methods
- Low: Claims about the specific causes of performance gaps on complex problems

## Next Checks
1. Measure and report the success rate of Python code execution for both code-specialized and general-purpose models across all benchmark sets to determine if execution failures contribute significantly to performance differences.

2. Systematically vary the Program-of-Thought prompt structure and test its impact on performance, particularly for complex problems, to isolate the contribution of prompt engineering from model capabilities.

3. Apply the DOCMATH-EVAL benchmark to non-financial documents containing numerical data (e.g., scientific papers, technical reports) to assess the generalizability of observed model limitations beyond the financial domain.