---
ver: rpa2
title: Complementary Random Masking for RGB-Thermal Semantic Segmentation
arxiv_id: '2303.17386'
source_url: https://arxiv.org/abs/2303.17386
tags:
- segmentation
- semantic
- masking
- modality
- rgb-t
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of RGB-thermal semantic segmentation,
  which is challenging due to modality over-reliance in existing fusion networks.
  To address this, the authors propose a complementary random masking strategy that
  randomly masks one modality and masks the other in a complementary manner, preventing
  the network from relying too heavily on a single modality.
---

# Complementary Random Masking for RGB-Thermal Semantic Segmentation

## Quick Facts
- arXiv ID: 2303.17386
- Source URL: https://arxiv.org/abs/2303.17386
- Reference count: 40
- Primary result: Achieves 61.2 mIoU on MF dataset, 1.7% improvement over previous best

## Executive Summary
This paper addresses the challenge of modality over-reliance in RGB-thermal semantic segmentation by proposing a complementary random masking strategy combined with self-distillation losses. The method randomly masks patches in one modality while masking complementary patches in the other, forcing the network to learn representations from at least one valid modality in every spatial location. This approach, combined with self-distillation and modality-wise supervision, achieves state-of-the-art performance across three benchmark datasets, demonstrating improved accuracy and robustness in challenging scenarios.

## Method Summary
The proposed method enhances RGB-thermal semantic segmentation by combining complementary random masking with self-distillation losses. The masking strategy randomly occludes patches in one modality while occluding complementary patches in the other, ensuring every spatial location has at least one valid input channel. A self-distillation loss encourages consistent predictions between clean and masked inputs, while modality-wise supervision provides separate losses for RGB-only, thermal-only, and combined inputs. The method uses a Mask2Former architecture with modality-specific Swin Transformer backbones and winner-take-all max fusion.

## Key Results
- Achieves 61.2 mIoU on MF dataset, 1.7% improvement over previous best
- Outperforms previous methods by significant margins on all three benchmark datasets (MF, PST900, KP)
- Demonstrates robustness in challenging scenarios such as low-light and noisy conditions
- Complementary random masking improves performance by 0.9% mIoU over individual random masking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Complementary random masking prevents over-reliance on a single modality by forcing the network to learn representations from at least one valid modality in every spatial location.
- Mechanism: The masking strategy randomly occludes patches in one modality while occluding the complementary patches in the other modality, ensuring that every spatial location has at least one valid input channel.
- Core assumption: The network can leverage information from the available modality to compensate for missing information in the other modality.
- Evidence anchors:
  - [abstract]: "The proposed masking strategy prevents over-reliance on a single modality. It also improves the accuracy and robustness of the neural network by forcing the network to segment and classify objects even when one modality is partially available."
  - [section 3.2]: "We push the network in a situation where one modality is partially unavailable but allows the missing information can be complemented by the other modality."
- Break condition: If both modalities are simultaneously occluded in the same spatial location, the network cannot learn from that region.

### Mechanism 2
- Claim: Self-distillation loss between clean and masked inputs encourages the network to extract consistent and complementary representations across modalities.
- Mechanism: The L_SDC loss enforces class prediction consistency between clean RGB-thermal pairs and their masked counterparts, while L_SDN enforces consistency between clean inputs and single-masked modalities to learn non-local representations.
- Core assumption: Consistent predictions across different input augmentations indicate robust and complementary feature learning.
- Evidence anchors:
  - [section 3.3]: "The proposed self-distillation loss encourages the network to extract complementary and meaningful representations from a single modality or complementary masked modalities."
  - [section 4.4.1]: "The self-distillation losses for complementary and non-local representation (L_SDC and L_SDN) are getting +0.3% and +0.2% improvement, respectively."
- Break condition: If the distillation loss is too strong, it may force the network to ignore useful modality-specific information.

### Mechanism 3
- Claim: Modality-wise supervision improves performance by allowing the network to learn specialized representations for each input type.
- Mechanism: The network is trained with separate supervised losses for RGB-only, thermal-only, and RGB-thermal inputs, rather than only using the combined input.
- Core assumption: Different input modalities provide complementary information that can be better captured with modality-specific supervision.
- Evidence anchors:
  - [section 3.4]: "Also, we empirically found that the supervised loss for each prediction result of multiple input modalities can perform better than a single supervised loss for RGB-thermal pair."
  - [section 4.4.1]: "Our empirical finding demonstrates that modality-wise supervision loss L_MWS, which provides supervision for each prediction result from multiple input modalities, yields +0.8% performance gain compared to a single supervised loss for RGB-thermal pairs."
- Break condition: If the modality-specific tasks are too different, the network may struggle to balance learning across modalities.

## Foundational Learning

- Concept: Multi-modal fusion and complementary information
  - Why needed here: The paper addresses the problem of modality over-reliance in RGB-thermal segmentation, which requires understanding how different modalities provide complementary information.
  - Quick check question: Why is it problematic if a network becomes over-reliant on a single modality in RGB-thermal segmentation?

- Concept: Self-distillation and consistency regularization
  - Why needed here: The method uses self-distillation losses to encourage consistent predictions across clean and masked inputs, which is a form of consistency regularization.
  - Quick check question: How does enforcing consistency between predictions from different input augmentations help the network learn more robust features?

- Concept: Masking strategies in vision transformers
  - Why needed here: The method uses patch-based masking compatible with Swin Transformer architecture, requiring understanding of how masking affects transformer-based models.
  - Quick check question: What is the difference between random square masking and random patch masking, and why might patch masking be preferable for transformer-based models?

## Architecture Onboarding

- Component map: RGB/thermal → modality-specific backbone → max fusion → pixel decoder → transformer decoder → class/mask predictions → losses
- Critical path: RGB/thermal → modality-specific backbone → max fusion → pixel decoder → transformer decoder → class/mask predictions → losses
- Design tradeoffs:
  - Simple max fusion vs. more complex attention-based fusion modules
  - Complementary masking vs. individual random masking
  - Self-distillation vs. additional modality-specific supervision
- Failure signatures:
  - Poor performance on modality-dropped scenarios indicates over-reliance on single modality
  - Degraded performance with high masking ratios suggests insufficient capacity to learn from incomplete information
  - Inconsistent predictions between clean and masked inputs indicates problems with self-distillation
- First 3 experiments:
  1. Train baseline Mask2Former with simple max fusion on MF dataset to establish performance without proposed method
  2. Add complementary random masking with varying patch sizes (8, 16, 32, 64) to find optimal masking strategy
  3. Add self-distillation losses incrementally (L_SDC, then L_SDN) to measure contribution of each component

## Open Questions the Paper Calls Out
No open questions explicitly called out in the paper.

## Limitations
- The method requires training from scratch rather than allowing pretraining on existing datasets, limiting practical adoption
- Self-distillation loss introduces additional hyperparameters that may be difficult to tune
- Evaluation focuses primarily on daytime scenes, leaving nighttime performance less thoroughly explored

## Confidence

- **High Confidence**: The core mechanism of complementary random masking preventing over-reliance on single modalities is well-supported by experimental results showing consistent performance improvements across all three benchmark datasets.
- **Medium Confidence**: The self-distillation loss contribution is validated through ablation studies (+0.5% mIoU improvement), but the optimal balance between consistency and learning complementary features requires further investigation.
- **Medium Confidence**: The modality-wise supervision claim is supported by empirical findings (+0.8% improvement), but the paper doesn't fully explore why this approach outperforms single supervised loss for RGB-thermal pairs.

## Next Checks

1. **Cross-dataset generalization test**: Evaluate the model trained on one dataset (e.g., MF) on the other datasets (PST900, KP) to assess robustness across different environmental conditions and sensor characteristics.

2. **Computational overhead analysis**: Measure the additional computational cost introduced by the self-distillation loss and complementary masking during both training and inference to quantify the practical trade-offs.

3. **Extreme masking scenario evaluation**: Systematically test the model with progressively higher masking ratios (up to 50%) to determine the breaking point where the network can no longer learn effectively from incomplete information.