---
ver: rpa2
title: Pairwise Similarity Learning is SimPLE
arxiv_id: '2310.09449'
source_url: https://arxiv.org/abs/2310.09449
tags:
- similarity
- simple
- learning
- pairs
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies pairwise similarity learning (PSL), a general
  problem that subsumes tasks like open-set face recognition, speaker verification,
  and image retrieval. The goal is to learn a pairwise similarity function that assigns
  higher scores to positive pairs (same label) than to negative pairs (different labels).
---

# Pairwise Similarity Learning is SimPLE

## Quick Facts
- arXiv ID: 2310.09449
- Source URL: https://arxiv.org/abs/2310.09449
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance on large-scale open-set recognition benchmarks by removing angular similarity and margin requirements

## Executive Summary
This paper introduces SimPLE, a simple yet effective framework for pairwise similarity learning (PSL) that achieves state-of-the-art performance across multiple open-set recognition tasks. The authors identify that existing PSL methods using angular similarity and margin are unnecessarily complex, and demonstrate that a simpler approach using inner product with angular bias can achieve superior results. SimPLE employs a novel loss function with reversed hard pair mining directions and a queue-based sampling strategy to effectively learn pairwise similarities without requiring feature normalization or angular margins.

## Method Summary
SimPLE is a pair-based proxy-free framework that uses inner product similarity with an angular bias term as the similarity score function. The method employs a first-in-first-out queue storing encoded samples from previous mini-batches, using a moving-averaged encoder to maintain stable pair coverage. The loss function implements reversed hard pair mining where positive pairs use 1/r scaling and negative pairs use r scaling, creating complementary gradients that avoid cancellation effects. This design removes the need for angular similarity and margin while maintaining strong generalization in open-set scenarios.

## Key Results
- Achieves state-of-the-art performance on VGGFace2, CUB-200-2011, and VoxCeleb2 benchmarks
- Outperforms existing methods by significant margins across multiple open-set recognition tasks
- Demonstrates that angular similarity and margin are not necessary for achieving strong PSL performance
- Shows robust performance across face verification, image retrieval, and speaker verification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing angular similarity and margin while keeping pair-based proxy-free learning aligns training with PSL's desideratum of perfect positive/negative pair separation.
- Mechanism: By parameterizing similarity as S(fθ(xi), fθ(xj)) with an angular bias term bθ, the method avoids degenerate solutions that arise when magnitude alone determines similarity. This creates a universal decision boundary S(·,·)+b=0 that directly separates intra-class and inter-class pairs without needing proxy-based margins.
- Core assumption: The angular bias bθ can be learned from data and remains constant during inference, providing a data-dependent threshold that generalizes to open-set scenarios.
- Evidence anchors:
  - [abstract] "which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition"
  - [section] "we remove such an assumption by adding an angular bias: S( ˜x1, ˜x2) = ∥ ˜x1∥ · ∥ ˜x2∥ · (cos(θ ˜x1, ˜x2 ) − bθ)"
- Break condition: If bθ cannot generalize from training pairs to unseen open-set pairs, the universal threshold fails and performance degrades.

### Mechanism 2
- Claim: Hard pair mining in reverse directions for positive and negative pairs avoids the cancellation effect that occurs with naive scaling.
- Mechanism: The loss function multiplies 1/r to positive pair similarity and r to negative pair similarity. This simultaneously emphasizes easy positives and hard negatives, creating a balanced gradient that pushes intra-class similarity up and inter-class similarity down.
- Core assumption: The reverse mining directions create complementary gradients rather than competing ones, allowing effective hard sample focus without needing angular normalization.
- Evidence anchors:
  - [section] "we propose a simple yet novel remedy – perform hard pair mining in a reverse direction for positive and negative pairs"
  - [section] "The core idea is that as long as the mining directions are reversed for positive and negative pairs, then their effect will no longer cancel out each other"
- Break condition: If r is too large, the gradient becomes unstable and may cause divergence or poor convergence.

### Mechanism 3
- Claim: Using a moving-averaged encoder for queue samples improves pair coverage and stability compared to using the current encoder.
- Mechanism: The queue maintains encoded samples from previous mini-batches using a slowly updated moving average of model parameters. This provides diverse pairs across the training trajectory and prevents overfitting to the current batch distribution.
- Core assumption: The moving-averaged encoder produces stable, representative features for pair construction while the current encoder focuses on optimization.
- Evidence anchors:
  - [section] "We use a first-in-first-out queue where the oldest mini-batch is dequeued as the current mini-batch is enqueued... The moving-averaged encoder is updated by θq ← ηθq + (1 − η)θ"
- Break condition: If η is too small, the queue becomes stale; if too large, it loses the stability benefit and may introduce noise.

## Foundational Learning

- Concept: Pair-based vs triplet-based learning distinction
  - Why needed here: Understanding why pair-based learning better aligns with PSL's universal threshold requirement versus triplet-based methods that compare anchor-positive to anchor-negative
  - Quick check question: What is the key difference between the decision boundaries used in contrastive loss versus triplet loss?

- Concept: Angular similarity and its relationship to feature normalization
  - Why needed here: Recognizing why angular similarity (cosine) became standard in open-set recognition and what problem it solves (degenerate solutions in softmax)
  - Quick check question: Why does softmax cross-entropy have degenerate solutions when features can grow arbitrarily large?

- Concept: Proxy-based vs proxy-free learning trade-offs
  - Why needed here: Understanding the computational and generalization differences between using class proxies versus direct sample-to-sample comparisons
  - Quick check question: What is the computational complexity difference between proxy-based and proxy-free pairwise similarity computation for N samples?

## Architecture Onboarding

- Component map: Feature encoder fθ -> Queue -> Pair Construction -> Loss -> Gradient -> Encoder update
- Critical path: Encoder → Queue → Pair Construction → Loss → Gradient → Encoder update
- Design tradeoffs:
  - Angular bias vs no bias: Bias improves performance but adds hyperparameter
  - Queue size vs memory: Larger queues improve pair coverage but increase memory usage
  - Mining strength r vs stability: Higher r focuses on harder pairs but may cause instability
- Failure signatures:
  - Poor validation performance with r=1 suggests need for hard mining
  - Degenerate feature norms indicate angular similarity may be needed
  - Memory overflow indicates queue size too large
- First 3 experiments:
  1. Validate baseline performance with r=1, bθ=0.3, α=0.001 on validation set
  2. Test sensitivity to r parameter (1, 2, 3) while keeping other hyperparameters fixed
  3. Compare cosine similarity vs generalized inner product (with bias) on the same validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can SimPLE be extended to multi-modal data pairs (e.g., image-text pairs) for tasks like image-text retrieval or cross-modal retrieval?
- Basis in paper: [explicit] The paper mentions that PSL is a general problem and learning a common embedding space for multi-modal data pairs can also be viewed as a PSL problem.
- Why unresolved: The paper focuses on open-set recognition tasks like face recognition, image retrieval, and speaker verification. It does not explore how SimPLE can be adapted to multi-modal settings.
- What evidence would resolve it: Experiments demonstrating SimPLE's performance on multi-modal retrieval benchmarks (e.g., Flickr30k, MS-COCO) compared to existing methods like CLIP or ALIGN would provide evidence.

### Open Question 2
- Question: What is the optimal similarity score function for pair-based proxy-free learning beyond inner product with angular bias?
- Basis in paper: [inferred] The paper discusses that the optimal similarity score is yet to be designed and how to effectively incorporate hard pair mining without the use of angular similarity remains an open problem.
- Why unresolved: The paper proposes using inner product with an angular bias as the similarity score, but acknowledges that the optimal similarity score is still an open question.
- What evidence would resolve it: Experiments comparing SimPLE's performance using different similarity score functions (e.g., Mahalanobis distance, learned similarity metrics) on various benchmarks would provide evidence.

### Open Question 3
- Question: How can pair sampling be improved in SimPLE to better approximate the desideratum of PSL, especially with large training sets?
- Basis in paper: [explicit] The paper states that the naive pair construction strategy in SimPLE cannot cover all representative pairs, especially with large training sets, and advanced pair sampling methods could be an important future direction.
- Why unresolved: The paper uses a simple pair construction strategy based on a queue of samples, but acknowledges its limitations with large datasets.
- What evidence would resolve it: Experiments comparing SimPLE's performance using different pair sampling strategies (e.g., curriculum learning, active learning) on large-scale benchmarks would provide evidence.

## Limitations

- The effectiveness of the angular bias term bθ across diverse domains remains to be validated beyond the tested face recognition and image retrieval tasks.
- The choice of mining strength parameter r=3 appears empirically determined but lacks theoretical justification for why this specific value works optimally across different datasets and tasks.
- The memory overhead of maintaining a queue with m·q encoded samples (where q=65,536) could be prohibitive for very large-scale applications or resource-constrained environments.

## Confidence

- **High Confidence**: The experimental results showing SimPLE's state-of-the-art performance across multiple benchmarks (VGGFace2, CUB-200-2011, VoxCeleb2) with significant improvements over baselines.
- **Medium Confidence**: The theoretical justification that angular similarity and margin are unnecessary for strong PSL performance, as this relies on empirical observations rather than formal proofs.
- **Low Confidence**: The universal applicability claim that angular normalization and margin can be completely removed from all pairwise similarity learning scenarios without performance degradation.

## Next Checks

1. **Cross-domain generalization test**: Evaluate SimPLE on a significantly different PSL task (e.g., cross-modal retrieval or protein structure similarity) to verify the claimed generality of removing angular normalization.

2. **Ablation study on angular bias**: Systematically test SimPLE performance across a range of bθ values (including bθ=0) on each dataset to quantify the contribution of the angular bias term.

3. **Memory efficiency analysis**: Measure the actual memory consumption of the queue-based approach across different queue sizes and evaluate whether the performance gains justify the computational overhead compared to simpler methods.