---
ver: rpa2
title: 'Instruct and Extract: Instruction Tuning for On-Demand Information Extraction'
arxiv_id: '2310.16040'
source_url: https://arxiv.org/abs/2310.16040
tags:
- instruction
- table
- text
- information
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task, On-Demand Information Extraction,
  designed to extract information based on user instructions and present it in a structured
  table format. The authors construct a benchmark dataset, INSTRUCT IE, consisting
  of automatically generated training data and human-annotated test data.
---

# Instruct and Extract: Instruction Tuning for On-Demand Information Extraction

## Quick Facts
- arXiv ID: 2310.16040
- Source URL: https://arxiv.org/abs/2310.16040
- Reference count: 34
- Key outcome: ODIE achieves F1 scores of 83.59% (headers) and 48.01% (ROUGE-L for content) on the INSTRUCT IE benchmark

## Executive Summary
This paper introduces the On-Demand Information Extraction task, where models extract information based on user instructions and present it in structured tables. The authors construct the INSTRUCT IE benchmark with automatically generated training data and human-annotated test data. They develop ODIE by fine-tuning LLaMA-7B with LoRA, achieving substantial improvements over baseline models in both table header generation and content extraction. The work demonstrates that instruction tuning with carefully curated synthetic data can significantly enhance LLMs' ability to follow complex extraction instructions.

## Method Summary
The authors propose a three-stage approach: (1) automatic data generation using ChatGPT to create instruction-table pairs from text documents, (2) multi-faceted filtering using NLI-based validation across four dimensions (validity, informativeness, consistency, faithfulness), and (3) LoRA-based fine-tuning of LLaMA-7B on the filtered dataset. Two inference methods are explored: Direct (immediate table generation) and CoT (Chain-of-Thought prompting before extraction). The resulting ODIE model is evaluated on the INSTRUCT IE benchmark using ROUGE-L for content and semantic similarity metrics for headers.

## Key Results
- ODIE achieves 83.59% F1 score for table header generation vs. 77.78% for baseline
- ODIE achieves 48.01% ROUGE-L F1 score for table content extraction vs. 43.69% for baseline
- CoT prompting improves open instruction performance by 2.5% over Direct method

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-facet filtering ensures high-quality training data for on-demand information extraction.
- **Mechanism:** The automatic pipeline generates synthetic data which is then validated across four dimensions: validity (format compliance), informativeness (sufficient rows/columns), consistency with instruction (NLI-based alignment), and faithfulness to text (NLI-based content verification).
- **Core assumption:** NLI-based scoring reliably captures semantic alignment and content faithfulness better than simple string matching.
- **Evidence anchors:**
  - [abstract] "We propose multi-faceted validation methods to filter out low-quality samples, ensuring that the synthetic data is curated from four perspectives: validity, informativeness, consistency with the instruction, and faithfulness to the text."
  - [section] "We center our attention on the generated table to carefully craft a filtering mechanism across four dimensions."
- **Break condition:** If NLI models have poor generalization to domain-specific language, filtering may remove valid examples or retain noisy ones.

### Mechanism 2
- **Claim:** CoT prompting improves open instruction extraction by enabling dynamic, adaptable reasoning.
- **Mechanism:** Adding Chain-of-Thought before table generation encourages the model to reason step-by-step, which is particularly beneficial for open instructions that lack explicit header definitions.
- **Core assumption:** Explicit reasoning steps help the model handle ambiguity in open instructions better than direct extraction.
- **Evidence anchors:**
  - [abstract] "We further incorporate a Chain-of-Thought prompting approach...enabling us to investigate the effects of elaborating the 'thought process' before extracting tables for on-demand IE use cases."
  - [section] "ODIE -COT demonstrates an even better performance in the context of open instructions, surpassing ODIE -DIRECT by 2.5%."
- **Break condition:** For fixed instructions, CoT may introduce unnecessary complexity or over-generalization, hurting performance.

### Mechanism 3
- **Claim:** Instruction tuning with LoRA enables efficient adaptation of LLMs to on-demand IE without full fine-tuning.
- **Mechanism:** LoRA adds low-rank adapter matrices to the LLM, allowing parameter-efficient fine-tuning on the instruction-specific dataset while preserving the base model's capabilities.
- **Core assumption:** The low-rank decomposition captures task-specific adaptations without catastrophic forgetting.
- **Evidence anchors:**
  - [section] "We finetune LLaMA-7B with LoRA...to obtain ODIE."
  - [section] "ODIE brings substantial improvements in the accuracy of the extracted headers and contents."
- **Break condition:** If the rank is too low, the adapter may not capture sufficient task-specific knowledge; if too high, efficiency gains diminish.

## Foundational Learning

- **Concept: NLI-based validation**
  - Why needed here: To assess semantic alignment between instructions and extracted headers, and between text and extracted content, beyond exact matching.
  - Quick check question: What would happen if we used exact string matching instead of NLI for consistency checks?

- **Concept: Instruction tuning paradigm**
  - Why needed here: On-demand IE requires the model to generalize across diverse, user-defined extraction tasks, not just predefined ones.
  - Quick check question: How does instruction tuning differ from standard supervised fine-tuning in terms of input-output format?

- **Concept: Chain-of-Thought reasoning**
  - Why needed here: Open instructions require the model to infer table structure dynamically, which benefits from explicit reasoning steps.
  - Quick check question: Why might CoT hurt performance on fixed instructions but help on open instructions?

## Architecture Onboarding

- **Component map:** ChatGPT-based data generation → Multi-facet filtering → LoRA fine-tuning (LLaMA-7B) → Inference with templated prompts
- **Critical path:** Data generation → Filtering → Model training → Evaluation (header + content metrics)
- **Design tradeoffs:** Synthetic data vs. human annotations for training; Direct vs. CoT generation methods; Exact match vs. semantic similarity metrics
- **Failure signatures:** Low ROUGE-L scores indicate poor content extraction; Low semantic similarity indicates poor instruction following; High filtering rate may indicate overly strict validation
- **First 3 experiments:**
  1. Train ODIE with and without filtering to quantify filtering impact.
  2. Compare Direct vs. CoT generation on fixed vs. open instructions separately.
  3. Evaluate LoRA rank sensitivity on extraction accuracy and efficiency.

## Open Questions the Paper Calls Out

Open Question 1
- Question: How does model size affect performance on the on-demand information extraction task?
- Basis in paper: [inferred] The paper focuses on 7B models due to compute constraints and does not explore larger models.
- Why unresolved: The paper does not experiment with larger model sizes to see how they impact performance.
- What evidence would resolve it: Experimenting with models of various sizes (e.g. 13B, 30B, 65B) and comparing their performance on the task.

Open Question 2
- Question: How does the combination of direct and CoT methods affect performance compared to using them separately?
- Basis in paper: [inferred] The paper evaluates direct and CoT methods separately but does not explore combining them.
- Why unresolved: The paper does not investigate potential synergistic effects of combining direct and CoT approaches.
- What evidence would resolve it: Training and evaluating models using both direct and CoT methods together and comparing results to using each method individually.

Open Question 3
- Question: What evaluation metrics best assess the accuracy and quality of table construction and information organization?
- Basis in paper: [explicit] The paper discusses limitations of current evaluation metrics and the need for more fine-grained assessment.
- Why unresolved: The paper relies on existing metrics like exact match and ROUGE-L, which may not fully capture table structure quality.
- What evidence would resolve it: Developing and validating new metrics that better evaluate table construction and information organization in a fine-grained manner.

## Limitations
- The automatic data generation pipeline relies heavily on ChatGPT, introducing uncertainty about synthetic data quality and diversity
- Evaluation metrics may not fully capture real-world utility - high scores don't guarantee practical usefulness
- The test set size (150 samples) is relatively small for establishing robust performance claims

## Confidence

- **High confidence**: The fundamental approach of instruction tuning LLaMA-7B with LoRA on a carefully curated dataset is sound and well-established in the literature.
- **Medium confidence**: The performance improvements over baseline models are statistically significant based on reported metrics, but absolute performance levels suggest substantial room for improvement.
- **Low confidence**: The generalizability of the filtering mechanism and robustness of CoT prompting across different domains remain unproven.

## Next Checks

1. **Ablation study of filtering criteria**: Remove each filtering dimension individually to quantify their individual contributions to final performance.

2. **Cross-domain generalization test**: Evaluate ODIE on instructions and documents from domains not represented in the training data to assess generalizability.

3. **Human evaluation of extracted utility**: Conduct user studies where domain experts assess whether extracted information is actually useful, complete, and accurate for practical applications.