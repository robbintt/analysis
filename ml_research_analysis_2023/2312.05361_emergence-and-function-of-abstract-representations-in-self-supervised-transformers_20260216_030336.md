---
ver: rpa2
title: Emergence and Function of Abstract Representations in Self-Supervised Transformers
arxiv_id: '2312.05361'
source_url: https://arxiv.org/abs/2312.05361
tags:
- object
- abstractions
- token
- network
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether self-supervised transformers develop
  abstract representations capturing latent data structures. The authors analyze small
  transformers trained on hierarchical visual datasets with compositional objects,
  showing that embeddings for semantically related tokens converge at specific computational
  stages.
---

# Emergence and Function of Abstract Representations in Self-Supervised Transformers

## Quick Facts
- arXiv ID: 2312.05361
- Source URL: https://arxiv.org/abs/2312.05361
- Reference count: 40
- Key outcome: Self-supervised transformers develop abstract representations that capture compositional structure in visual data and can be extracted as interpretable linguistic patterns

## Executive Summary
This paper investigates whether self-supervised transformers develop abstract representations that capture latent data structures in compositional visual datasets. Through analysis of small transformers trained on hierarchical object datasets, the authors demonstrate that embeddings for semantically related tokens converge at specific computational stages, forming low-dimensional manifolds that encode object membership and spatial relationships. These abstractions are shown to be causally necessary for network inference through precise manipulation experiments. The research further reveals that abstractions exhibit compositional structure with contextual independence and part-whole hierarchies mirroring the dataset's compositional nature. To extract these abstractions as interpretable linguistic patterns, the authors introduce a Language-Enhanced Architecture (LEA) that enables user control over the network's decision-making process.

## Method Summary
The research analyzes small transformers trained on a Hierarchical Object Dataset (HOD) featuring compositional visual tokens. The methodology involves training transformer encoders on reconstruction tasks, analyzing intermediate representations for convergence patterns using cosine similarity and PCA, training linear probes to detect specific abstractions, and conducting gain-of-function manipulation experiments to establish causality. A Language-Enhanced Architecture (LEA) is introduced to extract abstractions as linguistic patterns through auxiliary language networks and vector quantization. The study systematically examines whether identified abstractions are causal (necessary for inference) and meaningful (encode semantic features), while also investigating their compositional properties including contextual independence and part-whole hierarchies.

## Key Results
- Transformers develop abstract representations that converge at specific computational stages for semantically related tokens
- Identified abstractions are causally necessary for network inference, demonstrated through predictable changes from manipulation experiments
- Abstractions exhibit compositional structure with contextual independence and part-whole hierarchies mirroring dataset compositionality
- Language-Enhanced Architecture successfully extracts abstractions as interpretable linguistic patterns for user control

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Abstractions form low-dimensional attractors in representational space
- Mechanism: Distinct tokens sharing semantic features transiently converge to similar representations at specific computational stages
- Core assumption: Semantic similarity drives representational convergence
- Evidence anchors: Abstract mentions "low-dimensional manifolds where the embeddings of semantically related tokens transiently converge"; section discusses analyzing embeddings at different granularity levels

### Mechanism 2
- Claim: Abstractions are causally necessary for network inference
- Mechanism: Gain-of-function manipulation experiments show that swapping abstractions predictably alters network predictions
- Core assumption: Manipulating abstractions changes network behavior in predictable ways
- Evidence anchors: Abstract states manipulations demonstrate abstractions are "central to the network's decision-making process"; section emphasizes manipulation experiments are essential for establishing causality

### Mechanism 3
- Claim: Abstractions are compositionally structured
- Mechanism: Abstractions for composite objects are constructed from abstractions of constituent objects rather than token information directly
- Core assumption: Hierarchical composition of abstractions mirrors dataset compositional structure
- Evidence anchors: Abstract mentions abstractions exhibit "part-whole relationships that mirror the compositional nature of the dataset"; section explicitly asks whether part-whole hierarchy is reflected at abstraction level

## Foundational Learning

- Concept: Principal Component Analysis (PCA)
  - Why needed here: Used to identify major sources of variance in token embeddings and visualize representational convergence
  - Quick check question: How would you use PCA to identify whether embeddings cluster by semantic features?

- Concept: Linear probes
  - Why needed here: Used to identify representations that correlate with specific semantic features
  - Quick check question: How would you train a linear probe to detect object membership from token embeddings?

- Concept: Interchange intervention experiments
  - Why needed here: Used to establish causal relationships between representations and network behavior
  - Quick check question: How would you design an interchange intervention to test whether a representation is causal?

## Architecture Onboarding

- Component map: Masked board input → Transformer blocks → Abstract representations → Logits for reconstruction
- Critical path: Token embeddings with positional encodings → Multi-head attention and feed-forward layers → Intermediate activations → Output logits
- Design tradeoffs: Smaller networks easier to analyze but may lack representational capacity; factorized representations simplify interpretation but may limit expressivity
- Failure signatures: Poor reconstruction accuracy, lack of representational convergence, failure of manipulation experiments to produce predictable changes
- First 3 experiments:
  1. Train vanilla transformer on simple HOD dataset and verify reconstruction accuracy
  2. Use PCA to identify representational convergence by semantic features
  3. Conduct interchange intervention experiments to establish causal relationships

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do abstractions change as transformers scale in size and complexity?
- Basis in paper: [inferred] The paper analyzes small-scale transformers and suggests that studying larger models could reveal more about abstraction formation and function.
- Why unresolved: The study focuses on small transformers, leaving open how abstraction formation and function scale with model size.
- What evidence would resolve it: Analyzing abstraction emergence, convergence patterns, and causal roles in large-scale transformers and foundation models.

### Open Question 2
- Question: Can abstractions be intentionally engineered or controlled in self-supervised transformers?
- Basis in paper: [explicit] The paper introduces a language-enhanced architecture (LEA) to extract and manipulate abstractions, suggesting the possibility of controlled abstraction engineering.
- Why unresolved: While LEA demonstrates abstraction extraction and manipulation, it remains unclear how to systematically engineer specific abstractions or control their emergence during training.
- What evidence would resolve it: Developing methods to intentionally shape abstraction formation through architectural modifications, training objectives, or intervention strategies.

### Open Question 3
- Question: How do abstractions relate to other forms of representation in transformers, such as features or concepts?
- Basis in paper: [explicit] The paper discusses abstractions as low-dimensional manifolds within representational space and compares them to other forms of representation like features and concepts.
- Why unresolved: The relationship between abstractions and other forms of representation, such as features, concepts, or latent variables, is not fully elucidated.
- What evidence would resolve it: Investigating the connections between abstractions and other representational forms, such as analyzing their shared properties, interactions, and roles in transformer computations.

## Limitations

- The analysis relies heavily on cosine similarity and PCA projections to identify representational convergence, which may miss more subtle or distributed representations
- The Language-Enhanced Architecture's methodology for mapping representations to linguistic patterns is not fully specified, raising questions about whether extracted patterns reflect genuine semantic understanding
- The study focuses on small transformers and a specific compositional dataset, limiting generalizability to larger models and more complex visual domains

## Confidence

**High Confidence**: The basic finding that transformers trained on compositional datasets develop abstract representations for objects and spatial relationships is well-supported by multiple experimental approaches.

**Medium Confidence**: The claim about compositionality and hierarchical structure of abstractions is supported but requires careful interpretation regarding the exact nature of compositional relationships.

**Low Confidence**: The Language-Enhanced Architecture's ability to extract interpretable linguistic patterns from abstract representations is the most speculative claim due to unspecified methodology.

## Next Checks

1. Test whether the same abstraction mechanisms emerge when training on datasets with different compositional structures (varying object sizes, shapes, and spatial relationships) to validate that observed patterns are not specific to the particular HOD design.

2. Systematically remove or modify components of the Language-Enhanced Architecture (vector quantization parameters, reconstruction loss weights, linguistic probe architecture) to determine which aspects are essential for extracting meaningful linguistic patterns versus creating artifacts.

3. Apply additional analysis methods beyond cosine similarity and PCA (such as centered kernel alignment or canonical correlation analysis) to verify that representational convergence patterns are robust across different similarity metrics and not artifacts of the chosen analysis approach.