---
ver: rpa2
title: Imperceptible Adversarial Attack on Deep Neural Networks from Image Boundary
arxiv_id: '2308.15344'
source_url: https://arxiv.org/abs/2308.15344
tags:
- adversarial
- attack
- boundary
- example
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel imperceptible adversarial attack method
  that systemically attacks the input image boundaries to find adversarial examples.
  The key idea is to start with a one-pixel boundary attack and progressively increase
  the boundary width until an adversarial example is found, while keeping the remaining
  image area intact.
---

# Imperceptible Adversarial Attack on Deep Neural Networks from Image Boundary

## Quick Facts
- arXiv ID: 2308.15344
- Source URL: https://arxiv.org/abs/2308.15344
- Reference count: 40
- Primary result: Achieves 95.2% success rate with PSNR 41.37 dB by modifying only 32% of input content from image boundaries

## Executive Summary
This paper introduces a novel imperceptible adversarial attack method that systematically targets image boundaries to generate adversarial examples. The approach uses a progressive boundary width expansion strategy combined with iterative FGSM optimization to craft perturbations that remain visually undetectable. The attack demonstrates high effectiveness across six CNN models and a Vision Transformer, achieving strong success rates while maintaining excellent imperceptibility metrics.

## Method Summary
The method employs a two-level iterative approach where an outer loop progressively increases boundary width from 1 to 40 pixels while keeping the image interior untouched. For each boundary width, an inner FGSM-style optimization computes gradients with respect to the boundary region only, applying perturbations scaled by a decreasing epsilon value. The process terminates when misclassification is achieved with PSNR above 40 dB or when maximum boundary width is reached.

## Key Results
- Average success rate of 95.2% across six CNN models and Vision Transformer
- Average PSNR of 41.37 dB indicating high imperceptibility
- Achieves effective attacks by modifying only 32% of input image content from boundaries
- Demonstrates correlation between boundary width and success rate
- Shows adversarial boundaries can redirect model attention away from central object features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive boundary width expansion enables stealthy adversarial example discovery
- Mechanism: Outer loop starts with 1-pixel boundary and incrementally expands width, applying FGSM-style gradient ascent only to boundary regions with decreasing epsilon
- Core assumption: DNNs treat border pixels as less informative than central content, allowing boundary alterations to induce misclassification without obvious artifacts
- Evidence anchors: [abstract] "systemically attacks the input image boundary for finding the AEs... using only 32% of the input image content"; [section 3.1] "The loop increases the boundary's width to be attacked in each iteration... starts with one-pixel boundary... and keeps the remaining area untouched"
- Break condition: Attack fails if PSNR drops below 40 dB or epsilon reaches minimum before misclassification

### Mechanism 2
- Claim: Iterative FGSM with decreasing epsilon maintains high PSNR while systematically searching for adversarial perturbations
- Mechanism: Inner loop runs up to 15 FGSM iterations per boundary width, masking interior gradients and scaling perturbations by current epsilon
- Core assumption: Small, progressive changes in boundary pixels accumulate to shift the model's decision boundary without introducing detectable noise
- Evidence anchors: [section 3.2] "step3 is the adversarial perturbation where the sign is multiplied by the epsilon value"; [section 4.2] "The PSNR is used to show the ratio of the possible highest power of a signal to the power of that noise that corrupts that signal"
- Break condition: Loop exits if misclassification achieved with PSNR > 40 or epsilon reaches minimum

### Mechanism 3
- Claim: Boundary attacks shift model attention from central salient regions to adversarial boundary features
- Mechanism: Grad-CAM heatmaps show boundary perturbations redirect model focus away from true object features to perturbed borders
- Core assumption: DNNs allocate attention based on learned feature importance, which can be exploited by boundary perturbations mimicking high-importance edge cues
- Evidence anchors: [section 4.4] "we analyze how the adversarial perturbations at the boundaries change the model's attention"; [abstract] "correlation analyses are conducted... how the adversarial boundary changes the DNN model's attention"
- Break condition: Attention shift considered effective if boundary perturbations dominate Grad-CAM heatmaps and align with misclassification

## Foundational Learning

- Concept: Gradient-based adversarial attacks (FGSM, I-FGSM)
  - Why needed here: The attack relies on computing gradients of loss with respect to boundary pixels to craft perturbations that maximize misclassification
  - Quick check question: How does the sign of the gradient determine the direction of perturbation in FGSM?

- Concept: Image boundary masking and region selection
  - Why needed here: The attack must zero out interior gradients to restrict perturbations to borders only; understanding mask shapes and indexing is critical
  - Quick check question: In a (W,H,3) image, what slice notation masks the center while leaving a w-pixel border untouched?

- Concept: Peak Signal-to-Noise Ratio (PSNR) as imperceptibility metric
  - Why needed here: PSNR quantifies visual similarity; the attack uses a 40 dB threshold to ensure perturbations remain invisible to humans
  - Quick check question: If clean and adversarial images differ by MSE=5, what PSNR does that correspond to in an 8-bit image?

## Architecture Onboarding

- Component map: Input image → Boundary mask → Gradient computation → Boundary FGSM update → PSNR check → (loop: width++) → Output adversarial example
- Critical path: Image → Gradient w.r.t label → Mask interior → Apply boundary perturbation → Check misclassification & PSNR → Break or continue
- Design tradeoffs: Wider boundaries increase attack success but lower PSNR; more inner-loop iterations increase success but risk perceptibility; smaller epsilon yields higher PSNR but may fail to find adversarial examples
- Failure signatures: Low PSNR (<40 dB), misclassification not achieved before epsilon minimum, boundary width >40 without success, Grad-CAM shows no attention shift
- First 3 experiments:
  1. Validate masking logic: Apply boundary FGSM to a synthetic image and confirm only border pixels change
  2. Test PSNR threshold: Generate perturbations with varying epsilon and plot PSNR vs. misclassification rate
  3. Measure attention shift: Run Grad-CAM on clean and boundary-attacked images to confirm attention rerouting

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Heuristic selection of epsilon values (10 for CNNs, 0.02 for ViTs) and minimum bounds lacks formal justification, potentially limiting reproducibility
- Correlation analysis between boundary width and success rate shows strong trends but lacks statistical significance testing
- Grad-CAM-based attention shift analysis provides qualitative evidence but does not establish causation between boundary perturbations and model misclassification

## Confidence
- High confidence: The boundary attack mechanism (progressive width expansion with masked FGSM) is well-specified and produces measurable results (PSNR, SR, MSE)
- Medium confidence: The imperceptibility claims (PSNR > 40 dB) are supported by quantitative metrics but depend on the specific epsilon scheduling heuristic
- Low confidence: The causal relationship between boundary perturbations and attention shift, as well as the generalizability to unseen architectures, requires further validation

## Next Checks
1. Implement ablation studies varying epsilon decay rates (0.5, 0.75, 0.9) to assess sensitivity of PSNR and SR to this parameter
2. Conduct statistical tests (χ² or ANOVA) on boundary width vs. success rate data to quantify significance of observed correlations
3. Apply the attack to a held-out architecture (e.g., MobileNet) to evaluate transfer beyond the six models tested