---
ver: rpa2
title: 'Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler
  Subnetworks'
arxiv_id: '2306.04251'
source_url: https://arxiv.org/abs/2306.04251
tags:
- learning
- invariant
- stochastic
- dynamics
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes a strong implicit bias of stochastic gradient\
  \ descent (SGD) where gradient noise attracts the dynamics towards simpler subnetworks,\
  \ reducing the number of independent parameters and improving generalization. The\
  \ authors identify invariant sets\u2014parameter subspaces unmodified by SGD\u2014\
  and focus on two key types: sign invariant sets (vanishing neurons) and permutation\
  \ invariant sets (identical neurons)."
---

# Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks

## Quick Facts
- arXiv ID: 2306.04251
- Source URL: https://arxiv.org/abs/2306.04251
- Reference count: 40
- Key outcome: Gradient noise in SGD creates an implicit bias toward simpler subnetworks by attracting dynamics toward invariant parameter subspaces, improving generalization.

## Executive Summary
This paper analyzes a fundamental property of stochastic gradient descent where gradient noise creates an implicit bias toward simpler subnetworks by attracting the dynamics toward invariant parameter subspaces. The authors identify two key types of invariant sets—sign invariant sets (vanishing neurons) and permutation invariant sets (identical neurons)—and establish conditions under which SGD stochastically attracts toward these simpler configurations. The theoretical framework uses stochastic differential equations to characterize how noise intensity relative to loss curvature determines the strength of this attraction. Empirical results demonstrate that SGD naturally drives parameters toward these invariant sets during training, with stronger effects at higher learning rates. The authors prove that this stochastic collapse acts as regularization by preventing overfitting to low-signal singular modes, explaining why early training with large learning rates benefits subsequent generalization.

## Method Summary
The authors analyze SGD dynamics through the lens of stochastic differential equations, identifying invariant parameter subspaces that remain unmodified by optimization. They establish theoretical conditions for stochastic attractivity based on the balance between gradient noise intensity and loss curvature around these invariant sets. The methodology involves characterizing two main types of invariant sets (sign and permutation) in deep networks, proving their stochastic attractivity under specific conditions, and validating these theoretical predictions through empirical studies on VGG-16 and ResNet-18 architectures trained on CIFAR datasets. Additionally, they analyze a linear teacher-student model to demonstrate how stochastic collapse prevents overfitting to low-signal modes and improves generalization.

## Key Results
- SGD exhibits stochastic attractivity toward invariant sets corresponding to simpler subnetworks (vanishing or identical neurons)
- Higher learning rates and smaller batch sizes strengthen the stochastic collapse effect
- Stochastic collapse acts as implicit regularization by preventing overfitting to low-signal singular modes
- Early training with large learning rates for extended periods improves subsequent generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient noise creates a bias toward invariant sets corresponding to simpler subnetworks.
- Mechanism: The noise introduces an additional drift term in the stochastic differential equation that attracts the dynamics toward invariant sets. This attraction is stronger when the noise intensity is high relative to the curvature of the loss landscape near the invariant set.
- Core assumption: The SGD trajectory can be approximated by a stochastic differential equation (SDE) where the noise term is proportional to the gradient noise from minibatches.
- Evidence anchors:
  - [abstract] "SGD exhibits a property of stochastic attractivity towards these simpler invariant sets"
  - [section] "The drift term of this SDE is the negative full-batch gradient −∇L(θt), while the diffusion is determined by a spatially-dependent diffusion matrix D(θt) = η 2β Σ(θt)Σ(θt)⊺"
  - [corpus] "Noise Balance and Stationary Distribution of Stochastic Gradient Descent" - no direct evidence found
- Break condition: If the noise intensity is too low relative to the curvature around the invariant set, the attraction condition fails and the dynamics do not collapse toward the simpler subnetwork.

### Mechanism 2
- Claim: Sign invariant sets (vanishing neurons) are stochastically attractive when the activation function is origin-passing.
- Mechanism: For origin-passing activation functions, if a neuron's incoming and outgoing weights are zero, the gradient with respect to those weights is also zero, making it an invariant set. The stochastic attractivity condition requires the noise intensity to be sufficiently large compared to the signal strength.
- Core assumption: The activation function satisfies σ(0) = 0 and is smooth.
- Evidence anchors:
  - [abstract] "we characterize two such sets that correspond to simpler subnetworks: one for vanishing neurons"
  - [section] "If the non-linearity σ is origin-passing (i.e. σ(0) = 0), then the coordinate plane A = {θ|w(l) in,p = 0, b(l) p = 0, w(l+1) out,p = 0} is an invariant set"
  - [corpus] No direct evidence found for this specific mechanism
- Break condition: If the activation function is not origin-passing (e.g., ReLU with additional symmetries) or if the noise-to-signal ratio is too small.

### Mechanism 3
- Claim: Permutation invariant sets (identical neurons) are stochastically attractive, reducing model complexity.
- Mechanism: When two neurons in the same layer have identical incoming and outgoing weights, they form an invariant set. Stochastic gradient noise can drive the weights of different neurons toward this configuration, effectively reducing the number of independent parameters.
- Core assumption: The network architecture allows for permutation invariance among neurons in the same layer.
- Evidence anchors:
  - [abstract] "the other for identical neurons"
  - [section] "The affine space A = {θ|w(l) in,p = w(l) in,q, b(l) p = b(l) q , w(l+1) out,p = w(l+1) out,q } is an invariant set"
  - [corpus] No direct evidence found for this specific mechanism
- Break condition: If the noise intensity is insufficient or if residual connections prevent the formation of permutation invariant sets.

## Foundational Learning

- Concept: Stochastic differential equations (SDEs) as approximations of SGD dynamics
  - Why needed here: The paper uses SDEs to analyze how gradient noise affects the optimization trajectory and creates implicit biases
  - Quick check question: What is the relationship between the drift and diffusion terms in the SDE approximation of SGD?

- Concept: Invariant sets in dynamical systems
  - Why needed here: The paper identifies invariant sets as parameter subspaces that remain unmodified by SGD, which are key to understanding the implicit bias toward simpler subnetworks
  - Quick check question: What makes a set an invariant set of SGD according to the paper's definition?

- Concept: Stochastic attractivity
  - Why needed here: This property describes how SGD dynamics are attracted toward invariant sets, which is central to the paper's main contribution
  - Quick check question: What is the sufficient condition for an invariant set to be stochastically attractive according to Theorem 4.2?

## Architecture Onboarding

- Component map: Input -> Hidden layers (with origin-passing activations) -> Output
- Parameter space: Weights and biases organized by layers and neurons
- Optimization: SGD with minibatch noise

- Critical path:
  1. Initialize network parameters
  2. Compute gradients with minibatch noise
  3. Update parameters via SGD
  4. Monitor for convergence toward invariant sets
  5. Evaluate generalization performance

- Design tradeoffs:
  - Learning rate vs. noise intensity: Higher learning rates increase noise, strengthening attractivity but potentially causing instability
  - Batch size: Smaller batches increase noise, enhancing stochastic collapse but potentially slowing convergence
  - Activation function choice: Origin-passing functions enable sign invariant sets; ReLU introduces additional symmetries

- Failure signatures:
  - No convergence toward simpler subnetworks despite long training
  - Training loss decreases but test performance degrades
  - Weight norms remain large without sign of collapse

- First 3 experiments:
  1. Train a simple network with varying learning rates and batch sizes, monitoring the fraction of independent neurons over time
  2. Replace ReLU with GELU and compare the emergence of invariant sets
  3. Implement a linear teacher-student setup to verify the theoretical predictions about stochastic collapse and generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the necessary conditions for stochastic attractivity of general affine invariant sets beyond the sufficient condition provided in Theorem 4.2?
- Basis in paper: [explicit] The paper states that while a sufficient condition for stochastic attractivity is established, deriving necessary and sufficient conditions in high dimensions is challenging due to complex dynamics near the boundary of invariant sets.
- Why unresolved: The current analysis focuses on sufficient conditions based on local properties around the invariant set, but the behavior of the system away from the set or at the boundary remains unclear.
- What evidence would resolve it: A rigorous mathematical proof establishing necessary conditions for stochastic attractivity, potentially involving global properties of the stochastic process or alternative analytical frameworks beyond the current geometric approach.

### Open Question 2
- Question: How does the stochastic collapse mechanism interact with the additional symmetries present in ReLU networks compared to GELU networks?
- Basis in paper: [inferred] The authors note that ReLU networks have additional symmetries that create potentially larger invariant sets, but they chose GELU for their experiments to simplify analysis of the two main invariant sets.
- Why unresolved: The paper deliberately avoids ReLU to focus on clearer examples, leaving the more complex interaction between stochastic collapse and ReLU symmetries unexplored.
- What evidence would resolve it: Empirical studies comparing stochastic collapse patterns in ReLU vs GELU networks, potentially revealing new types of invariant sets or modified collapse dynamics unique to ReLU's piecewise linearity.

### Open Question 3
- Question: What is the exact temporal dynamics of the expectation E[ˆsi(t)] for the student network singular values beyond the conjectured form in Conjecture G.1?
- Basis in paper: [explicit] The paper conjectures an exact expression for E[ˆsi(t)] but does not prove it, noting that the conjecture captures the phase transition and shrinkage behavior.
- Why unresolved: While the conjecture is consistent with known limits and aligns with L2-regularized gradient flow, a rigorous derivation or counterexample remains lacking.
- What evidence would resolve it: A formal proof using stochastic calculus techniques or numerical validation across diverse parameter regimes that confirms or refutes the conjectured form with high precision.

## Limitations
- Theoretical analysis relies on SDE approximations that may not capture all discrete-time effects
- Stochastic attractivity conditions are somewhat restrictive, requiring careful balance between noise intensity and loss curvature
- Empirical validation shows correlation between stochastic collapse and generalization, but direct causal links remain unproven

## Confidence
- **High**: Existence of invariant sets and their characterization (sign/permutation invariant sets)
- **Medium**: Theoretical conditions for stochastic attractivity and their empirical validation
- **Low**: Direct causal relationship between stochastic collapse and generalization improvements in real-world datasets

## Next Checks
1. **Activation Function Dependence**: Systematically test networks with ReLU, GELU, and Swish activations to quantify how activation smoothness affects the rate and extent of stochastic collapse toward invariant sets.

2. **Noise-to-Signal Ratio Thresholds**: Conduct controlled experiments varying batch sizes and learning rates to empirically determine the critical noise-to-signal ratios required for stochastic attractivity across different network architectures and loss landscapes.

3. **Generalization Attribution Study**: Design ablation experiments that artificially constrain or prevent parameter collapse during training, then measure the resulting impact on test performance to establish direct causal links between stochastic collapse and generalization.