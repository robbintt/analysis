---
ver: rpa2
title: 'Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications'
arxiv_id: '2310.19942'
source_url: https://arxiv.org/abs/2310.19942
tags:
- entity
- span
- table
- detection
- splitner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a two-step NER approach, SplitNER, that divides
  the task into span detection and span classification, each formulated as a question-answering
  problem. This design allows fine-tuning BERT models separately for each sub-task,
  improving efficiency and performance.
---

# Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications

## Quick Facts
- arXiv ID: 2310.19942
- Source URL: https://arxiv.org/abs/2310.19942
- Authors: 
- Reference count: 14
- Primary result: Two-step QA-based NER approach outperforms single-model baselines on 3 of 4 datasets while reducing training and inference times

## Executive Summary
This paper introduces SplitNER, a two-step approach to Named Entity Recognition that divides the task into span detection and span classification, each formulated as a question-answering problem. By using separate BERT-based models for each sub-task, SplitNER achieves on-par or superior performance compared to single-model baselines while significantly reducing computational overhead. The framework leverages character and orthographic pattern features for span detection and uses dice loss for handling class imbalance in span classification. Experiments on four cross-domain datasets demonstrate the effectiveness of this approach, particularly in domain-specific NER tasks.

## Method Summary
SplitNER divides NER into two sequential QA-based sub-tasks: span detection and span classification. The span detection model identifies entity mention boundaries using token-level BIOE classification with additional character and orthographic features. The span classification model then classifies each detected span into its entity type. Both models use BERT-base architecture but are optimized separately with different loss functions (cross-entropy for detection, dice loss for classification). The approach reduces computational complexity from quadratic to linear in sequence length while maintaining or improving performance.

## Key Results
- SplitNER outperforms single-model baselines on BioNLP13CG, OntoNotes5.0, and CTIReports datasets
- Training and inference times are significantly reduced compared to QA-based methods
- Character and orthographic features improve span detection accuracy for domain-specific terms
- Dice loss handles class imbalance better than cross-entropy in span classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Splitting NER into two QA sub-tasks improves efficiency by reducing computational complexity.
- Mechanism: Span detection uses token-level classification (linear in sequence length) instead of enumerating all possible spans (quadratic). Span classification queries each sentence once per detected span instead of once per entity type.
- Core assumption: The two sub-tasks can be optimized separately without significant error propagation between them.
- Evidence anchors:
  - [abstract] "we formulate both sub-tasks as question-answering (QA) problems and produce two leaner models which can be optimized separately for each sub-task"
  - [section 2.1] "Unlike other span-based NER techniques which are quadratic in terms of sequence length, our Span Detection process is linear"
  - [corpus] Weak - no direct evidence found in corpus about computational complexity reduction
- Break condition: If error propagation between the two stages becomes severe enough to outweigh the efficiency gains, or if the additional model parameters negate the computational benefits.

### Mechanism 2
- Claim: Adding character and orthographic pattern features improves span detection for domain-specific terms that lack good semantic representations in BERT.
- Mechanism: Character-level CNNs capture sub-word patterns (e.g., chemical formulas), while orthographic features encode word shapes (uppercase, lowercase, digits). These are concatenated with BERT embeddings before classification.
- Core assumption: Domain-specific terms share patterns at the character and orthographic level that can be captured by these additional features.
- Evidence anchors:
  - [section 2.1] "We observed that such boundary detection errors happen mostly for domain-specific terms which occur rarely and do not have a good semantic representation in the underlying BERT model. However, these domain specific terms often share patterns at character-level"
  - [section 3.3] "Adding character and pattern features improves the accuracy of Span Detection Model"
  - [corpus] Weak - no direct corpus evidence linking character patterns to domain-specific term performance
- Break condition: If the additional feature computation time outweighs the accuracy gains, or if the features introduce noise that degrades performance on non-domain-specific terms.

### Mechanism 3
- Claim: Using dice loss instead of cross-entropy loss improves span classification performance on imbalanced datasets.
- Mechanism: Dice loss directly optimizes for the Dice coefficient (F1 score), which is more appropriate for imbalanced classification than cross-entropy loss.
- Core assumption: The span classification task has imbalanced class distributions across entity types.
- Evidence anchors:
  - [section 2.2] "For model optimization, we use cross entropy loss for span detection and dice loss for span classification"
  - [section A] "Next, we compare dice loss and cross-entropy loss for their effectiveness in handling the class imbalance issue in span classification"
  - [corpus] Weak - no corpus evidence quantifying the class imbalance in the datasets
- Break condition: If the class distribution becomes more balanced, or if cross-entropy loss with appropriate class weighting achieves similar performance with lower computational cost.

## Foundational Learning

- Concept: Question Answering (QA) formulation for NER
  - Why needed here: The paper reformulates both span detection and classification as QA tasks, requiring understanding of how QA models process input and generate outputs
  - Quick check question: How does a QA model determine the start and end positions of an answer span given a question and context passage?

- Concept: BIOE (Beginning-Inside-Outside-End) tagging scheme
  - Why needed here: Span detection uses token-level BIOE classification to identify entity boundaries
  - Quick check question: What is the difference between BIO and BIOE schemes, and when would you use each?

- Concept: Feature engineering with character-level CNNs and orthographic patterns
  - Why needed here: These additional features are critical for handling domain-specific terms and boundary detection
  - Quick check question: How do character-level CNNs differ from word-level embeddings in capturing morphological information?

## Architecture Onboarding

- Component map: Input sentence → Span Detection Model (BERT + character + pattern features → BIOE classification) → detected spans → Span Classification Model (BERT + pooled output → entity type classification) → final entity type predictions
- Critical path: Input sentence → Span Detection Model → detected spans → Span Classification Model → final entity type predictions
- Design tradeoffs: Splitting into two models increases total parameters but allows specialized optimization; character/pattern features improve domain term detection but add computation; dice loss handles imbalance but may converge slower than cross-entropy.
- Failure signatures: Span Detection errors (missed entities, wrong boundaries) propagate to Span Classification; character/pattern features may overfit to specific domains; dice loss may struggle with very rare entity types.
- First 3 experiments:
  1. Compare training times of Single(QA) vs SplitNER(QA-QA) on a small dataset with few entity types
  2. Evaluate Span Detection performance with and without character/pattern features on BioNLP13CG
  3. Test span classification with dice loss vs cross-entropy loss on an imbalanced subset of OntoNotes5.0

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SplitNER framework generalize to other NLP tasks beyond NER that involve complex span-based predictions?
- Basis in paper: [explicit] The authors conclude by suggesting that the two-step QA framework could be applied to break down other complex NLP tasks into smaller sub-tasks.
- Why unresolved: The paper only demonstrates SplitNER on NER, so its applicability to other tasks remains untested.
- What evidence would resolve it: Empirical studies applying SplitNER's two-step QA approach to tasks like relation extraction, coreference resolution, or event extraction.

### Open Question 2
- Question: What is the impact of using different pretrained language models (e.g., GPT, T5) as the backbone for SplitNER compared to BERT/RoBERTa?
- Basis in paper: [inferred] The authors mention that SplitNER can work with any BERT-based pretrained backbone, but only experiment with BERT, RoBERTa, and SciBERT.
- Why unresolved: The performance implications of using alternative pretrained models are not explored.
- What evidence would resolve it: Comparative experiments training SplitNER with different pretrained models on the same datasets.

### Open Question 3
- Question: How does the performance of SplitNER vary with different character and orthographic pattern feature configurations?
- Basis in paper: [explicit] The authors present ablation studies showing that character and pattern features improve span detection, but do not explore alternative feature configurations.
- Why unresolved: The optimal configuration of character and pattern features is not determined.
- What evidence would resolve it: Systematic experiments varying the number of CNN layers, filter sizes, and feature combinations to find the optimal configuration.

### Open Question 4
- Question: Can the Span Detection Model be improved by incorporating hierarchical entity type information during training?
- Basis in paper: [explicit] The authors suggest that grouping all entity mentions into a single class is a limitation and that hierarchical classification could improve performance.
- Why unresolved: The paper does not implement or evaluate a hierarchical extension of SplitNER.
- What evidence would resolve it: Experiments comparing the current flat classification approach with a hierarchical one that first detects coarse categories and then refines them.

## Limitations

- The paper lacks direct empirical validation of claimed computational efficiency gains from the two-stage approach
- Effectiveness of character and orthographic features is based primarily on author observations rather than systematic ablation studies
- The actual class distribution in datasets is not characterized, making it unclear whether dice loss is necessary for handling imbalance

## Confidence

High confidence: The basic two-stage pipeline architecture and its implementation details are well-specified. The claim that SplitNER achieves competitive or superior performance on three of four datasets is directly supported by experimental results.

Medium confidence: The efficiency improvements from the two-stage approach are theoretically sound but lack empirical validation. The specific contributions of character/pattern features and dice loss are plausible but not rigorously demonstrated.

Low confidence: The generalizability of these improvements across diverse domains and entity types is questionable, given that the cybersecurity dataset results are not fully disclosed and the WNUT17 dataset shows no improvement.

## Next Checks

1. **Computational Complexity Validation**: Implement a timing benchmark comparing Single(QA) vs SplitNER(QA-QA) on a small dataset with 5 entity types, measuring both training time and inference latency across 5 different sequence lengths (50, 100, 200, 500, 1000 tokens). This would directly test the claimed efficiency advantage.

2. **Feature Contribution Analysis**: Perform systematic ablation studies on the BioNLP13CG dataset by training models with: (a) BERT only, (b) BERT + character features, (c) BERT + pattern features, (d) BERT + both features. Measure the marginal improvement from each feature type to determine their individual contributions.

3. **Loss Function Comparison on Imbalanced Data**: Create an artificially imbalanced subset of OntoNotes5.0 by downsampling rare entity types, then train span classification models using: (a) cross-entropy loss with class weights, (b) dice loss, (c) focal loss. Compare F1 scores and convergence behavior to determine whether dice loss actually outperforms alternatives for this specific task.