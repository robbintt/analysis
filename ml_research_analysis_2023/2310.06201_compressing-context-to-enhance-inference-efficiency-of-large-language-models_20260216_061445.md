---
ver: rpa2
title: Compressing Context to Enhance Inference Efficiency of Large Language Models
arxiv_id: '2310.06201'
source_url: https://arxiv.org/abs/2310.06201
tags:
- context
- selective
- tasks
- llms
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a method called Selective Context that enhances
  the inference efficiency of LLMs by identifying and pruning redundancy in the input
  context to make the input more compact. The proposed method is motivated by the
  potential redundancy and repetition in human language, which has two main sources:
  the inherent redundancy of natural language and the overlap with training material.'
---

# Compressing Context to Enhance Inference Efficiency of Large Language Models

## Quick Facts
- arXiv ID: 2310.06201
- Source URL: https://arxiv.org/abs/2310.06201
- Reference count: 5
- Primary result: Achieves 50% reduction in context cost with 36% reduction in inference memory and 32% reduction in inference time while maintaining comparable performance

## Executive Summary
This paper introduces Selective Context, a method to enhance LLM inference efficiency by identifying and pruning redundant content from input context. The approach leverages self-information computed by a base causal language model to evaluate the informativeness of lexical units (tokens, phrases, or sentences). By selectively retaining high-information content, the method provides a more compact context representation that significantly reduces memory cost and generation latency. Experimental results demonstrate that Selective Context maintains strong performance across four downstream applications while achieving substantial efficiency gains, particularly at a 50% compression ratio.

## Method Summary
Selective Context identifies and prunes redundancy in LLM input by computing self-information for each lexical unit using a base causal language model. The method evaluates tokens, phrases, or sentences based on their self-information values, then retains only those above a percentile threshold. This adaptive filtering approach eliminates low-information content while preserving semantic meaning. The method is applied across multiple model scales (7B, 13B, 30B parameters) and tested on various downstream tasks including summarization, question answering, and conversation.

## Key Results
- Achieves 50% reduction in context cost with 36% reduction in inference memory usage
- Reduces inference time by 32% while maintaining performance
- Maintains BERTScore-F1 above 0.9 at 50% compression ratio
- Shows only minor performance drops (.023 in BERTscore, .038 in faithfulness) across four downstream applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective Context reduces redundancy by computing self-information for each token using a base causal language model, then retaining only high-information tokens or lexical units.
- Mechanism: Self-information (I(x) = -log₂P(x|x₀, x₁, ..., xₜ₋₁)) quantifies how "surprising" a token is given prior context. Rare tokens have high self-information; frequent ones have low. By filtering out low self-information tokens, the method keeps only the most informative content, reducing context length while preserving semantic content.
- Core assumption: Tokens with low self-information are redundant because they can be inferred from the context or are already known to the LLM from pre-training.
- Evidence anchors:
  - [abstract] "Selective Context evaluates informativeness of lexical units (i.e., tokens, phrases, or sentences) with self-information (Shannon, 1948) computed by a base causal language model."
  - [section 2] "Self-information can be used to assess the informativeness of lexical units... Lexical units with lower self-information are less informative and thus are more likely to be inferred from the context."
  - [corpus] Weak. No direct corpus support for self-information filtering efficacy; only mentions related compression methods.
- Break condition: If the base model's probability estimates are poorly calibrated, low self-information tokens may still carry critical semantic content, leading to loss of meaning.

### Mechanism 2
- Claim: Phrase-level and sentence-level lexical units improve context coherence compared to token-level pruning.
- Mechanism: After computing self-information per token, tokens are merged into phrases or sentences using NLTK and spaCy. This avoids creating disjoint fragments that could confuse the LLM and helps preserve local semantic coherence.
- Core assumption: Semantic units larger than single tokens (e.g., noun phrases, sentences) retain more contextual meaning when filtered together.
- Evidence anchors:
  - [section 3.2] "If the content filtering of selective context is directly performed on the token level, it might lead to very disjoint context. Therefore apart from token level filtering, we also conduct the filtering procedure on phrase and sentence level."
  - [section 5.5] "Employing phrase as the basic lexical units in Selective Context is the optimal approach, consistently outperforming the other two variants."
  - [corpus] Weak. No direct corpus evidence that phrase-level filtering is superior; only mentions ZipCache and token reduction as related work.
- Break condition: If phrase boundary detection is inaccurate (e.g., missing verb phrases), important semantic relations may be split, harming performance.

### Mechanism 3
- Claim: Percentile-based filtering adaptively selects informative content without needing fixed thresholds.
- Mechanism: Lexical units are ranked by self-information and the p-th percentile value is computed. Units with self-information ≥ percentile are kept. This adapts to varying context distributions.
- Core assumption: Self-information values in a given context follow a distribution that can be meaningfully summarized by a percentile cutoff.
- Evidence anchors:
  - [section 3.3] "Instead of using a fixed threshold or retaining a fixed number of top k lexical units, we design a percentile-based filtering approach to adaptively select the most informative content."
  - [section 5] "Selective Context maintains BERTScore-F1 above 0.9 when the compression ratio is 0.5 or lower."
  - [corpus] Weak. No corpus data on percentile threshold effectiveness; only mentions other compression strategies.
- Break condition: If the distribution of self-information is highly skewed, percentile filtering may retain too much or too little content.

## Foundational Learning

- Concept: Self-information and entropy in information theory
  - Why needed here: The core metric for filtering is self-information; understanding its properties (additivity, relation to probability) is essential to reason about the method.
  - Quick check question: Given P(x) = 0.1, what is I(x) in bits? (Answer: -log₂(0.1) ≈ 3.32 bits)

- Concept: Token-level vs phrase-level semantic coherence
  - Why needed here: The method merges tokens into phrases to avoid disjoint contexts; knowing why this matters requires understanding NLP tokenization and phrase structure.
  - Quick check question: Why might filtering at the token level create disjointed context? (Answer: Tokens may be split from their syntactic/semantic group, losing meaning)

- Concept: Attention-based model context windows and KV cache
  - Why needed here: LLMs store KV cache for all context tokens; reducing context length directly reduces memory and latency.
  - Quick check question: What grows quadratically with context length in transformer attention? (Answer: Memory and compute for the attention matrix)

## Architecture Onboarding

- Component map: Input -> Base LM (self-information computation) -> Tokenizer/phrase merger -> Percentile filter -> Compact context -> Target LLM

- Critical path:
  1. Tokenize input with base LM
  2. Compute self-information per token
  3. Merge tokens into phrases/sentences
  4. Rank by self-information, apply percentile filter
  5. Concatenate kept units → compressed context
  6. Feed to target LLM for downstream task

- Design tradeoffs:
  - Token vs phrase vs sentence granularity: finer granularity preserves more info but risks disjointedness; coarser preserves coherence but may over-prune.
  - Percentile vs fixed threshold: percentile adapts to context but may be unstable if distribution is skewed.
  - Base LM size: smaller base LM is faster but may give less accurate self-information estimates.

- Failure signatures:
  - Severe performance drop when compression ratio > 0.5: likely over-pruning informative tokens.
  - Unfaithful outputs: possible over-filtering of evidence tokens needed for QA.
  - Longer generation times: base LM self-information computation bottleneck.

- First 3 experiments:
  1. Run Selective Context with 0.2 compression ratio on arXiv intro paragraphs; measure BLEU vs full context.
  2. Compare token-level vs phrase-level filtering on BBC news; check coherence via manual inspection.
  3. Test different percentile cutoffs (10%, 30%, 50%) on ShareGPT conversations; measure faithfulness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal compression ratio for different NLP tasks when using Selective Context?
- Basis in paper: [explicit] The paper mentions different compression ratios (0.2, 0.35, 0.5, 0.65, 0.8) were tested but doesn't identify an optimal ratio for each task type.
- Why unresolved: The experiments show performance varies with compression ratio, but don't establish clear optimal thresholds for different task categories.
- What evidence would resolve it: Systematic experiments showing task-specific optimal compression ratios with detailed performance breakdowns for each task type.

### Open Question 2
- Question: How does Selective Context perform on languages other than English?
- Basis in paper: [inferred] The paper mentions ShareGPT dataset includes conversations in various languages, but doesn't report performance on non-English content.
- Why unresolved: The experiments are primarily focused on English language processing, leaving multilingual effectiveness unexplored.
- What evidence would resolve it: Comparative experiments on non-English datasets showing performance metrics across different languages.

### Open Question 3
- Question: What is the relationship between model scale and effectiveness of context compression?
- Basis in paper: [explicit] The paper mentions experiments with different model scales (7B, 13B, 30B parameters) but doesn't establish clear scaling laws for context compression effectiveness.
- Why unresolved: While the paper shows that larger models don't necessarily demonstrate stronger robustness, the relationship between model scale and optimal compression ratio remains unclear.
- What evidence would resolve it: Systematic scaling experiments showing how optimal compression ratios and performance metrics vary with model size across multiple model families.

## Limitations

- The method's effectiveness on non-English languages and specialized technical domains remains untested
- Computational overhead of the base model for self-information calculation is not fully characterized across different context lengths
- No detailed analysis of what specific types of semantic content are lost during compression at different ratios

## Confidence

*High Confidence:* The demonstrated memory and latency reductions are well-supported by the experimental data. The 36% reduction in inference memory usage and 32% reduction in inference time at 50% compression are directly measured and reproducible results. The basic premise that removing redundant tokens can reduce context size while maintaining performance is also well-established.

*Medium Confidence:* The claim that Selective Context strikes a good balance between efficiency and performance is supported by the aggregate metrics (BERTscore, faithfulness) but lacks granular analysis. The assertion that phrase-level filtering outperforms other granularities is based on limited comparisons. The method's performance stability across different compression ratios is demonstrated but not extensively stress-tested.

*Low Confidence:* Claims about the method's effectiveness across "various applications" are based on only four downstream tasks. The assertion that the approach is "effective in various scenarios" lacks validation on out-of-domain data. The scalability to extremely long contexts (>16K tokens) is mentioned but not empirically verified.

## Next Checks

1. **Semantic Integrity Analysis**: Conduct a detailed ablation study to identify what types of semantic content (factual statements, causal relationships, named entities) are most vulnerable to pruning at different compression ratios. Compare token-level retention rates against semantic preservation scores to quantify the tradeoff between compression and information loss.

2. **Base Model Overhead Characterization**: Measure the wall-clock time and memory usage of the base model self-information computation across varying context lengths (1K, 4K, 8K, 16K tokens) and different base model sizes. Determine the breakeven point where compression benefits are offset by base model computation costs.

3. **Cross-Domain Generalization Test**: Evaluate Selective Context on three additional domains not represented in the original datasets: medical literature, legal documents, and code repositories. Compare performance degradation patterns to identify domain-specific vulnerabilities and calibrate compression ratio recommendations accordingly.