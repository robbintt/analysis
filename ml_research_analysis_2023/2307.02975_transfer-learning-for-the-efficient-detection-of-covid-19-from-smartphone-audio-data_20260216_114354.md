---
ver: rpa2
title: Transfer Learning for the Efficient Detection of COVID-19 from Smartphone Audio
  Data
arxiv_id: '2307.02975'
source_url: https://arxiv.org/abs/2307.02975
tags:
- audio
- classi
- deep
- learning
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper compares three approaches for detecting COVID-19 from
  smartphone audio data: handcrafted acoustic features, deep audio embeddings with
  feature extraction, and fine-tuning pre-trained deep models. Four crowdsourced datasets
  (13,447 samples) were used, with user-independent 5-fold nested cross-validation.'
---

# Transfer Learning for the Efficient Detection of COVID-19 from Smartphone Audio Data

## Quick Facts
- arXiv ID: 2307.02975
- Source URL: https://arxiv.org/abs/2307.02975
- Reference count: 40
- OpenL3 embeddings outperform VGGish and YAMNET by 12.3% PR-AUC in feature extraction for COVID-19 detection

## Executive Summary
This paper systematically compares three approaches for detecting COVID-19 from smartphone audio data: handcrafted acoustic features, deep audio embeddings with feature extraction, and fine-tuning pre-trained deep models. Using four crowdsourced datasets (13,447 samples), the authors evaluate four deep audio models (VGGish, YAMNET, OpenL3) in both feature extraction and fine-tuning settings. OpenL3 embeddings consistently outperform the other approaches, achieving a 12.3% improvement in PR-AUC over YAMNET and 7.2% over VGGish. The study reveals that fine-tuning only the fully-connected layers generally underperforms feature extraction by 6.6% PR-AUC, and provides memory footprint analysis showing VGGish as the largest model at 18 MB with fine-tuning adding 7.4-57 MB overhead.

## Method Summary
The paper evaluates COVID-19 detection from respiratory sounds (cough and breath) using four crowdsourced datasets with 13,447 samples. Audio is preprocessed to 16kHz and balanced using random under-sampling. Three feature extraction approaches are compared: handcrafted acoustic features (477 features), pre-trained deep audio embeddings (VGGish, YAMNET, OpenL3), and fine-tuning of pre-trained models. Feature extraction uses PCA for dimensionality reduction and shallow classifiers (SVM, LR, RF, AB) with Bayesian optimization. Fine-tuning employs Hyperband optimization for classification layers. User-independent 5-fold nested cross-validation evaluates performance using PR-AUC as the primary metric.

## Key Results
- OpenL3 embeddings achieved 12.3% PR-AUC improvement over YAMNET and 7.2% over VGGish in feature extraction
- Fine-tuning only fully-connected layers resulted in 6.6% average PR-AUC drop compared to feature extraction
- VGGish has the largest memory footprint at 18 MB, with fine-tuning adding 7.4-57 MB overhead
- Handcrafted features performed competitively with deep embeddings on Cambridge npj dataset (0.62-0.70 PR-AUC)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OpenL3 embeddings provide superior feature extraction for COVID-19 detection from respiratory sounds compared to VGGish and YAMNET.
- Mechanism: OpenL3 is trained on both environmental sounds and music, creating embeddings that better capture the acoustic variability present in respiratory sounds than models trained primarily on speech or music.
- Core assumption: The acoustic characteristics of respiratory sounds are more similar to the diverse audio training data used for OpenL3 than to the YouTube-8M dataset used for VGGish or the AudioSet environmental sounds used for YAMNET.
- Evidence anchors:
  - [abstract] "Results clearly show the advantages of L 3-Net in all the experimental settings as it overcomes the other solutions by 12.3% in terms of Precision-Recall AUC as features extractor, and by 10% when the model is fine-tuned."
  - [section 4.3] "OpenL3 is able to obtain the best results in all the experimental settings. Compared with the other deep models, the embeddings of OpenL3 allows to achieve an overall improvement of 12.3% with respect to the deep features generated by YAMNET, and of 7.2% more than those extracted by VGGish."

### Mechanism 2
- Claim: Fine-tuning only the fully-connected layers of pre-trained models generally performs worse than feature extraction for COVID-19 detection.
- Mechanism: The convolutional layers that extract audio features are already well-optimized for general audio classification, and respiratory sounds differ enough from the original training data that only adjusting the classification layer is insufficient.
- Core assumption: The feature extraction capabilities of the convolutional layers are not transferable to respiratory sounds without also adapting the convolutional layers themselves.
- Evidence anchors:
  - [abstract] "Moreover, we note that to fine-tune only the fully-connected layers of the pre-trained models generally leads to worse performances, with an average drop of 6.6% with respect to feature extraction."
  - [section 4.4] "the fine-tuned models have lost, on average, 4%, 6.9%, and 8.8% of PR-AUC, respectively for YAMNET, VGGish, and OpenL3" compared to feature extraction results.

### Mechanism 3
- Claim: Handcrafted acoustic features can perform competitively with deep embeddings when the dataset has specific characteristics.
- Mechanism: Certain handcrafted features (duration, RMS energy, spectral centroid, MFCCs) may capture COVID-19 specific acoustic signatures that are not well-represented in the deep embeddings trained on general audio data.
- Core assumption: COVID-19 affects respiratory sounds in ways that manifest in measurable acoustic properties that handcrafted features can detect.
- Evidence anchors:
  - [section 4.3] "Surprisingly, the lowest improvement obtained by OpenL3 is the one compared with HC (6.7%), which perform better than YAMNET and VGGish in all the experimental settings, except for Cambridge npj (B) and Cambridge npj (CB), where the handcrafted features achieve the same or slightly worse results than the two deep embedding models."

## Foundational Learning

- Concept: Transfer Learning
  - Why needed here: The paper addresses COVID-19 detection from respiratory sounds with limited labeled data, making transfer learning from models pre-trained on larger audio datasets essential.
  - Quick check question: What are the two main approaches to transfer learning used in this paper, and how do they differ in terms of which parts of the pre-trained model are updated?

- Concept: Precision-Recall AUC
  - Why needed here: The paper uses PR-AUC as the primary evaluation metric because the COVID-19 detection task involves imbalanced datasets where the positive class (COVID-19) is much smaller than the negative class.
  - Quick check question: Why is PR-AUC preferred over ROC-AUC for imbalanced datasets in medical diagnosis applications?

- Concept: User-independent cross-validation
  - Why needed here: The paper employs user-independent 5-fold nested cross-validation to ensure that recordings from the same person don't appear in both training and test sets, preventing overfitting to individual voice patterns.
  - Quick check question: What problem does user-independent cross-validation solve that standard cross-validation might miss in audio classification tasks?

## Architecture Onboarding

- Component map: Raw audio -> Preprocessing (16kHz resampling) -> Feature extraction (handcrafted or deep embeddings) -> Dimensionality reduction (PCA) -> Classification (shallow ML model or fine-tuned deep model) -> Evaluation (PR-AUC)
- Critical path: The most critical path is from raw audio to feature extraction, as the quality of features directly determines classification performance. For deep models, this includes the pre-trained model selection and configuration.
- Design tradeoffs: Using larger pre-trained models like VGGish provides potentially richer features but at the cost of higher memory footprint and computational requirements. OpenL3 offers better performance but with 12 different configurations to choose from. Fine-tuning provides potential for adaptation but requires more data and computational resources.
- Failure signatures: Poor performance on user-independent splits suggests overfitting to specific users. Low PR-AUC scores indicate the model cannot distinguish COVID-19 cases effectively. Memory constraints may prevent deployment on mobile devices.
- First 3 experiments:
  1. Run feature extraction with OpenL3 (best configuration from literature) vs VGGish on a single dataset/modality combination to verify the 12.3% improvement claim.
  2. Compare memory footprint of feature extraction approach vs fine-tuning approach on the same dataset to validate the 6.6% average performance drop.
  3. Test handcrafted features vs deep embeddings on Cambridge npj dataset to understand why handcrafted features perform competitively there.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning the convolutional layers (not just fully-connected layers) of pre-trained audio models improve COVID-19 detection performance compared to feature extraction?
- Basis in paper: [explicit] The paper notes that fine-tuning only fully-connected layers performs worse than feature extraction, with an average 6.6% drop in PR-AUC, and suggests this may be because the respiratory sounds differ significantly from the original training data.
- Why unresolved: The paper only tested fine-tuning the final classification layers, leaving open whether updating earlier convolutional layers could better adapt the feature extraction to the COVID-19 domain.
- What evidence would resolve it: Direct experimental comparison of fine-tuning only fully-connected layers versus fine-tuning both convolutional and fully-connected layers on the same datasets, measuring performance differences.

### Open Question 2
- Question: How does model quantization impact both accuracy and memory footprint for mobile COVID-19 detection, and what is the optimal precision trade-off?
- Basis in paper: [explicit] The paper discusses memory constraints and suggests quantization as a solution to reduce model size, but does not empirically evaluate its impact on accuracy or test different precision levels.
- Why unresolved: While the paper identifies quantization as a potential optimization, it lacks quantitative analysis of how different quantization levels (e.g., 16-bit vs 8-bit) affect both model size and detection performance.
- What evidence would resolve it: Systematic experiments applying various quantization schemes to the models, reporting both memory reduction and accuracy changes across the four datasets.

### Open Question 3
- Question: Can XAI techniques reliably identify which acoustic features drive COVID-19 detection decisions, and do these explanations improve clinician trust in the system?
- Basis in paper: [explicit] The paper proposes exploring XAI methods to explain model decisions, noting this is critical for adoption, but does not implement or validate any XAI approaches.
- Why unresolved: While the paper identifies the need for explainability, it does not demonstrate which features (e.g., specific frequency ranges) are most important for classification or whether such explanations would be meaningful to healthcare providers.
- What evidence would resolve it: Implementation of XAI methods (e.g., saliency maps, feature importance scores) on the COVID-19 detection models, followed by validation studies with medical professionals to assess the usefulness and trustworthiness of the explanations.

## Limitations
- The crowdsourced datasets may contain noise and inconsistent recording conditions
- Limited dataset sizes constrain the effectiveness of fine-tuning approaches
- Memory footprint analysis doesn't account for runtime inference speed or energy consumption on actual mobile devices

## Confidence

- **High Confidence**: OpenL3 outperforms VGGish and YAMNET in feature extraction for COVID-19 detection
- **Medium Confidence**: Fine-tuning only fully-connected layers performs worse than feature extraction
- **Medium Confidence**: Handcrafted features can compete with deep embeddings on certain datasets

## Next Checks

1. Test the feature extraction pipeline on a held-out test set from the same datasets to verify the 12.3% PR-AUC improvement claim for OpenL3
2. Implement full fine-tuning (including convolutional layers) to determine if the performance gap is due to limited fine-tuning or fundamental differences in respiratory sound characteristics
3. Evaluate model performance across different smartphone hardware to validate the memory footprint analysis and ensure practical deployability