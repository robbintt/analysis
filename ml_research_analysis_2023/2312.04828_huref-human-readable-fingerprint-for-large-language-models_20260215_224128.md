---
ver: rpa2
title: 'HuRef: HUman-REadable Fingerprint for Large Language Models'
arxiv_id: '2312.04828'
source_url: https://arxiv.org/abs/2312.04828
tags:
- arxiv
- training
- terms
- preprint
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HuRef, a human-readable fingerprint for large
  language models (LLMs) that can uniquely identify the base model without interfering
  with training or exposing model parameters. The key idea is to leverage the stability
  of LLM parameter vector direction after pretraining, which remains invariant across
  subsequent fine-tuning, RLHF, or even continued pretraining with new languages.
---

# HuRef: HUman-REadable Fingerprint for Large Language Models

## Quick Facts
- **arXiv ID**: 2312.04828
- **Source URL**: https://arxiv.org/abs/2312.04828
- **Reference count**: 12
- **Key outcome**: Introduces HuRef, a human-readable fingerprint for LLMs that uniquely identifies base models without interfering with training or exposing parameters, leveraging stable parameter vector direction and robust invariant terms.

## Executive Summary
HuRef is a novel human-readable fingerprinting method for large language models that enables unique identification of base models while preserving privacy and training efficiency. The method exploits the stability of parameter vector direction after pretraining, which remains invariant across fine-tuning, RLHF, and continued pretraining. By defining three robust invariant terms that withstand weight rearrangement attacks and mapping them through a convolutional encoder and StyleGAN2 to natural dog images, HuRef provides a practical solution for LLM ownership verification without compromising model parameters or performance.

## Method Summary
The HuRef method extracts invariant terms from LLM parameters using matrix products that cancel out permutation and linear transformation attacks. These terms are then mapped to a Gaussian vector via a convolutional encoder trained with contrastive learning and GAN objectives. Finally, StyleGAN2 converts the Gaussian vector into a natural dog image, creating a human-readable fingerprint. The method leverages the stability of parameter vector direction post-pretraining and the robustness of carefully constructed invariant terms to provide accurate base model identification while maintaining human interpretability.

## Key Results
- Parameter vector direction remains stable (cosine similarity >0.99) across various training steps including fine-tuning and RLHF
- Invariant terms are robust to permutation, linear transformation, and sparsity mask weight rearrangement attacks
- Generated dog images successfully distinguish between different base models while remaining consistent for models sharing the same base

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameter vector direction after pretraining remains stable across subsequent training steps (SFT, RLHF, continued pretraining).
- Mechanism: The direction of the concatenated parameter vector (flattened weights and biases) acts as a stable fingerprint because updates from fine-tuning or RLHF introduce negligible perturbations to the overall vector direction.
- Core assumption: Pretraining convergence produces a stable parameter vector direction that subsequent training cannot significantly alter.
- Evidence anchors:
  - [abstract] "vector direction of LLM parameters remains stable after the model has converged during pretraining, with negligible perturbations through subsequent training steps"
  - [section] "we compute the cosine similarities between a base model LLaMA-7B and various of its offspring models... all of these models show almost full scores in cosine similarity, largely preserving the base model's parameter vector direction"
- Break condition: If subsequent training includes a loss term specifically designed to minimize cosine similarity with the base model, performance degrades (as shown in experiments).

### Mechanism 2
- Claim: Three invariant terms derived from Transformer structure remain stable under weight rearrangement attacks.
- Mechanism: By constructing invariant terms from self-attention outputs and feed-forward outputs using matrix products that cancel out permutation and linear transformation attacks, the fingerprint remains robust to weight camouflage.
- Core assumption: The mathematical structure of the invariant terms cancels out all possible weight rearrangements that don't affect model performance.
- Evidence anchors:
  - [abstract] "leveraging the Transformer structure, we systematically analyze potential attacks and define three invariant terms that identify an LLM's base model"
  - [section] "we construct three invariant terms... robust to these weight rearrangements by systematically analyzing possible rearrangements and leveraging the Transformer structure"
- Break condition: If an attack is discovered that changes the invariant terms without affecting model performance.

### Mechanism 3
- Claim: Convolutional encoder + StyleGAN2 generator creates human-readable fingerprints that preserve locality and model discriminability.
- Mechanism: The convolutional encoder learns a locality-preserving mapping from invariant terms to Gaussian vectors, while StyleGAN2 (with path length regularization) maps these to natural images where similar models produce similar-looking dogs.
- Core assumption: The encoder-discriminator GAN training ensures Gaussian output and locality preservation, while StyleGAN2's PPL metric ensures smooth latent-to-image mapping.
- Evidence anchors:
  - [abstract] "we map them to a Gaussian vector using an encoder, then convert it into a natural image using StyleGAN2"
  - [section] "the encoder discriminates between invariants from different base models and ensures Gaussian output through adversarial training, while StyleGAN2 transforms Gaussian vectors into dog images"
- Break condition: If the encoder fails to preserve locality or the image generator fails to maintain model discriminability.

## Foundational Learning

- Concept: Cosine similarity as a measure of vector direction alignment
  - Why needed here: The method relies on comparing parameter vector directions using cosine similarity to determine if models share a base model
  - Quick check question: If two vectors have cosine similarity of 0.99, are they pointing in nearly the same direction?

- Concept: Transformer architecture and parameter organization
  - Why needed here: The method leverages the specific structure of Transformer layers to construct invariant terms that are robust to weight rearrangements
  - Quick check question: What are the main components of a Transformer layer that the invariant terms are derived from?

- Concept: Generative adversarial networks (GANs) and image generation
  - Why needed here: The method uses a GAN-based approach to map invariant terms to human-readable images via StyleGAN2
  - Quick check question: How does the discriminator in a GAN help ensure the generated images are realistic and the encoder output is Gaussian?

## Architecture Onboarding

- Component map: Base LLM → Invariant term extraction (using last r layers) → Convolutional encoder → StyleGAN2 generator → Dog image fingerprint
- Critical path: The encoder must preserve locality between invariant terms and Gaussian vectors, and the image generator must preserve locality between Gaussian vectors and images
- Design tradeoffs: Using dog images for human-readability vs. other domains; choosing K=4096 for invariant terms vs. other sizes; using only last 2 layers vs. more layers
- Failure signatures: If fingerprints don't match for models known to share a base model, or if fingerprints are too similar across different base models
- First 3 experiments:
  1. Compute invariant terms for a known base model and its offspring, verify high cosine similarity
  2. Apply weight rearrangement attacks to a model, verify invariant terms remain stable
  3. Train the convolutional encoder with contrastive loss and GAN loss, verify it produces Gaussian output and preserves locality

## Open Questions the Paper Calls Out

- **Question**: How does the stability of the parameter vector direction change for LLMs of significantly different sizes (e.g., comparing LLaMA-7B to LLaMA-65B) during pretraining and subsequent fine-tuning?
  - Basis in paper: [inferred] The paper mentions that the parameter vector direction stabilizes during pretraining but does not explore the relationship between model size and stability.
  - Why unresolved: The paper does not provide data or analysis comparing the stability of parameter vector direction across different model sizes.
  - What evidence would resolve it: Experiments measuring the cosine similarity of parameter vectors between checkpoints during pretraining and subsequent fine-tuning for LLMs of varying sizes.

- **Question**: How robust is HuRef to adversarial attacks that specifically target the invariant terms, such as gradient-based attacks designed to maximize the perturbation of these terms while minimizing the impact on model performance?
  - Basis in paper: [inferred] The paper defines invariant terms that are robust to weight rearrangements but does not explore the robustness against more sophisticated adversarial attacks.
  - Why unresolved: The paper does not provide analysis or experiments testing the resilience of HuRef against adversarial attacks targeting the invariant terms.
  - What evidence would resolve it: Experiments applying gradient-based attacks to manipulate the invariant terms and measuring the impact on HuRef's ability to correctly identify the base model.

- **Question**: What is the impact of HuRef on the performance of the LLM when the invariant terms are computed during training, and how does this impact scale with the number of layers involved?
  - Basis in paper: [explicit] The paper states that the fingerprinting steps are conducted internally by the LLM owners and do not interfere with training, but does not provide details on the computational overhead.
  - Why unresolved: The paper does not provide information on the computational cost of computing the invariant terms during training or the impact on model performance.
  - What evidence would resolve it: Experiments measuring the computational overhead and performance impact of computing the invariant terms for different numbers of layers during training.

## Limitations
- Limited attack coverage: Only tests three specific attack types (permutation, linear transformation, sparsity mask)
- Architecture specificity: Primarily tested on LLaMA and LLaMA-2 models
- Human-readability validation: Limited quantitative validation of the human-readability claim

## Confidence
- Base model identification accuracy: High
- Invariance across training steps: Medium
- Robustness to weight rearrangement attacks: Medium
- Human-readability of fingerprints: Low

## Next Checks
1. **Attack Space Expansion**: Systematically test HuRef against a broader range of weight rearrangement attacks, including adversarial weight initialization and gradient-based attacks designed to minimize cosine similarity between base and offspring models.

2. **Cross-Architecture Testing**: Evaluate HuRef's performance on diverse LLM architectures (GPT, BERT, OPT, etc.) to verify the generality of the vector direction stability claim across different pretraining objectives and model designs.

3. **Long-term Stability Analysis**: Track the evolution of fingerprint stability over extended training periods (thousands of steps) with various fine-tuning objectives to quantify the degradation rate and establish practical limits on the method's applicability.