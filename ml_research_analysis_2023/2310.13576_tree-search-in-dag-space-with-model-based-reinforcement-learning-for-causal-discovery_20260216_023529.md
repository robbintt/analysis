---
ver: rpa2
title: Tree Search in DAG Space with Model-based Reinforcement Learning for Causal
  Discovery
arxiv_id: '2310.13576'
source_url: https://arxiv.org/abs/2310.13576
tags:
- search
- causal
- learning
- which
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CD-UCT, a reinforcement learning method for
  causal discovery that uses tree search to build directed acyclic graphs (DAGs) incrementally.
  The key innovation is an efficient algorithm to track cycle-inducing edges, enabling
  deeper discrete search and sampling in DAG space.
---

# Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery

## Quick Facts
- arXiv ID: 2310.13576
- Source URL: https://arxiv.org/abs/2310.13576
- Reference count: 7
- Primary result: CD-UCT substantially outperforms state-of-the-art model-free RL and greedy search methods in causal discovery on real-world datasets.

## Executive Summary
This paper introduces CD-UCT, a reinforcement learning method for causal discovery that incrementally builds directed acyclic graphs (DAGs) using tree search. The key innovation is an efficient algorithm to track cycle-inducing edges, enabling deeper discrete search in DAG space. Unlike prior model-free approaches, CD-UCT operates in a model-based manner, leveraging Upper Confidence Bound for Trees (UCT) to navigate the search space. Experiments on real-world datasets show CD-UCT substantially outperforms state-of-the-art model-free RL techniques and greedy search methods in both score function optimization and causal relationship identification, while scaling to larger graphs with up to 50 nodes.

## Method Summary
CD-UCT frames causal discovery as a Markov Decision Process where actions correspond to adding edges to an initially empty DAG. The algorithm uses Monte Carlo Tree Search with UCT to explore the search space, guided by a model-based approach that caches per-node score contributions using the decomposability of BIC. A key innovation is an incremental algorithm for detecting cycle-inducing edges that eliminates expensive cycle checks at each step by tracking which candidate edges would introduce cycles. The method employs a two-step construct-then-prune paradigm, first generating plausible structures through UCT search, then filtering non-significant edges using sparse regression (CAM).

## Key Results
- CD-UCT achieves TPR/FDR scores of 0.95/0.03 on the Sachs dataset compared to 0.55/0.29 for RL-BIC
- Runtime improves by more than an order of magnitude compared to naive cycle checking on graphs with 50 nodes
- CD-UCT outperforms Random Sampling, Random Search, and Greedy Search baselines across multiple metrics on both real and synthetic datasets

## Why This Works (Mechanism)

### Mechanism 1
The incremental algorithm for detecting cycle-inducing edges allows the search tree to grow deeper by eliminating the need for expensive cycle checks at each step. Instead of checking for cycles after every edge addition using DFS, the algorithm tracks which candidate edges would introduce cycles by leveraging the fact that descendants of a node's endpoint cannot connect to ancestors of the node's starting point without forming a cycle.

### Mechanism 2
Model-based reinforcement learning with UCT improves exploration efficiency compared to model-free methods by using a known transition model and caching per-node score contributions. CD-UCT uses the deterministic transition dynamics of the MDP and the decomposability of BIC to avoid redundant score evaluations, focusing exploration on promising branches of the search tree.

### Mechanism 3
The two-step construct-then-prune paradigm improves causal graph accuracy by first generating plausible structures and then filtering non-significant edges. The pruning step uses sparse regression (CAM) to remove edges whose statistical significance does not meet a threshold, reducing false discoveries while preserving true causal relationships.

## Foundational Learning

- Concept: Directed Acyclic Graph (DAG) properties and acyclicity constraints
  - Why needed here: The entire method relies on maintaining DAG structure during incremental construction; misunderstanding acyclicity leads to invalid graphs.
  - Quick check question: Given a DAG with edges A→B and B→C, is the edge C→A cycle-inducing? Why or why not?

- Concept: Score-based causal discovery and decomposable score functions
  - Why needed here: The algorithm caches per-node score contributions to avoid redundant evaluations; without understanding decomposability, the optimization logic is unclear.
  - Quick check question: If BIC is decomposable, can you compute the total score by summing individual node scores without recomputing from scratch?

- Concept: Monte Carlo Tree Search (MCTS) and Upper Confidence Bound for Trees (UCT)
  - Why needed here: CD-UCT is built on UCT; understanding the balance between exploration and exploitation is critical for tuning and debugging.
  - Quick check question: In UCT, what happens if the exploration parameter C is set too high or too low?

## Architecture Onboarding

- Component map: MDP environment -> CD-UCT agent -> Pruning module -> Score function
- Critical path: 1. Initialize empty DAG and cycle tracker 2. Compute valid actions using incremental cycle detection 3. Run UCT simulations 4. Select best action and update DAG 5. Apply pruning to final graph
- Design tradeoffs: Model-based vs model-free (efficiency vs flexibility), full vs reduced search horizon (accuracy vs computational cost), simulation budget (accuracy vs runtime)
- Failure signatures: Cycles in output graph (incremental cycle detection bug), poor reward despite high simulation budget (UCT parameters need tuning), runtime explosion on larger graphs (optimization needed)
- First 3 experiments: 1. Verify acyclicity maintenance on small synthetic graph (d=5, m=5) 2. Compare CD-UCT vs naive cycle detection on medium graph (d=20) 3. Test pruning effectiveness on real dataset (Sachs)

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CD-UCT compare to other model-based RL methods for causal discovery that use different tree search algorithms? The paper only compares CD-UCT to RL-BIC and greedy search methods, not exploring how different tree search algorithms might affect performance.

### Open Question 2
Can the incremental algorithm for detecting cycle-inducing edges be extended to handle directed graphs with cycles, such as those found in recurrent neural networks or Markov decision processes? The paper focuses on DAGs and does not explore potential extensions to graphs with cycles.

### Open Question 3
How does the performance of CD-UCT vary with the choice of the score function, and are there any score functions that are particularly well-suited for CD-UCT? The paper only evaluates CD-UCT with BIC and does not investigate how other score functions might affect its performance.

## Limitations

- Performance claims rely heavily on synthetic data for scalability, with limited testing on diverse real-world datasets
- The incremental cycle detection algorithm's correctness proof is stated but not fully detailed in the paper
- The pruning step using CAM assumes sparse regression assumptions hold, which may not generalize to all real-world causal structures

## Confidence

- High confidence: The core mechanism of using incremental cycle detection to enable deeper search is well-supported by theoretical reasoning and runtime comparisons
- Medium confidence: Claims about model-based RL advantages over model-free approaches are supported by general RL literature but lack direct comparative ablation studies
- Low confidence: The claim that CD-UCT "substantially outperforms" all baselines on real-world datasets is based on limited experimental conditions and specific hyperparameter settings

## Next Checks

1. Verify the incremental cycle detection algorithm's correctness on edge cases where multiple simultaneous edge additions could create cycles
2. Conduct ablation studies comparing CD-UCT with and without the pruning step to isolate its contribution to performance gains
3. Test CD-UCT on datasets with known non-linear causal relationships to validate the GP regression assumption for residual modeling