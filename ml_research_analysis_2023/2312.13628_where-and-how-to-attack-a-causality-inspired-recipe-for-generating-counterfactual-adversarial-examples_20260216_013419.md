---
ver: rpa2
title: Where and How to Attack? A Causality-Inspired Recipe for Generating Counterfactual
  Adversarial Examples
arxiv_id: '2312.13628'
source_url: https://arxiv.org/abs/2312.13628
tags:
- cade
- adversarial
- causal
- examples
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a causality-inspired approach to generating
  adversarial examples that better reflect real-world scenarios. The key insight is
  that traditional adversarial attacks ignore the causal relationships between features,
  leading to unrealistic examples (e.g., changing income without adjusting debt-to-income
  ratio).
---

# Where and How to Attack? A Causality-Inspired Recipe for Generating Counterfactual Adversarial Examples

## Quick Facts
- arXiv ID: 2312.13628
- Source URL: https://arxiv.org/abs/2312.13628
- Reference count: 34
- Key outcome: Introduces CADE framework that generates realistic adversarial examples by leveraging causal relationships, achieving up to 99.4% attack success rate on standard models and maintaining effectiveness on defense models.

## Executive Summary
This paper introduces CADE, a causality-inspired framework for generating adversarial examples that address the unrealistic nature of traditional attacks. The key insight is that standard adversarial attacks ignore causal relationships between features, leading to examples that violate real-world constraints (like changing income without adjusting debt-to-income ratio). CADE identifies valid attack points by targeting children and co-parents of the target variable in a causal graph, then uses causal generative models to create realistic counterfactual examples that preserve the current state of non-intervened variables. Experiments on Pendulum, CelebA, and SynMeasurement datasets demonstrate that CADE achieves competitive results on white-box, transfer-based, and even random black-box attacks.

## Method Summary
CADE operates through two main phases: first, it theoretically characterizes where to attack based on causal relationships, identifying children and co-parents of the target variable as valid intervention points. Second, it uses a causal generative model to produce counterfactual examples that maintain realistic relationships between features. The framework leverages the abduction-action-prediction counterfactual reasoning framework, intervening on latent variables in the causal graph and predicting their consequences while preserving non-intervened variables. The attack optimization is performed in latent space using gradient-based methods, with the intervention budget controlled by parameter ϵ. CADE is evaluated against traditional attacks (FGSM, PGD, C&W) and other counterfactual methods (SAE, ACE) across multiple attack scenarios.

## Key Results
- Achieves up to 99.4% attack success rate on standard models in white-box scenarios
- Maintains strong transfer performance with 89.8% ASR on black-box attacks
- Outperforms traditional attacks and other counterfactual methods across multiple datasets
- Creates more realistic adversarial examples that better preserve feature relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CADE improves adversarial attack effectiveness by targeting causal children and co-parents of the target variable rather than arbitrary features.
- Mechanism: Traditional adversarial attacks modify features independently without considering their causal relationships, creating unrealistic examples. CADE leverages causal graph information to identify valid attack points - specifically the children and co-parents of the target variable. Intervening on children directly affects the target prediction, while co-parent interventions create distributional shifts that models fail to generalize to.
- Core assumption: The data generating process follows a known causal structure, and DNNs are vulnerable to distributional shifts that preserve the target variable.
- Evidence anchors:
  - [abstract]: "first, we pinpoint the source of the vulnerability of DNNs via the lens of causality, then give theoretical results to answer where to attack"
  - [section 3]: "Proposition 3.2...when children are intervened, and the equivalence between pM'(y|x) and pM(y|x) when co-parents but not children are intervened"
  - [corpus]: No direct corpus evidence for this specific mechanism

### Mechanism 2
- Claim: CADE generates more realistic adversarial examples by preserving the current state of non-intervened variables through counterfactual generation.
- Mechanism: When intervening on a variable, its descendants in the causal graph also change. CADE uses a counterfactual generation framework that recovers the exogenous variables representing the current state, then predicts the consequences of interventions while preserving non-intervened variables. This creates examples that maintain realistic relationships between features.
- Core assumption: The causal generative process can be parameterized and inverted to generate counterfactual examples.
- Evidence anchors:
  - [abstract]: "considering the consequences of the attack interventions on the current state of the examples to generate more realistic adversarial examples"
  - [section 3]: "To generate counterfactuals, we resort to the generation framework proposed in (Pearl 2009), which requires the causal generating process incorporated in"
  - [corpus]: No direct corpus evidence for this specific mechanism

### Mechanism 3
- Claim: CADE achieves transferability to black-box models by creating distributional shifts that affect the underlying causal mechanisms rather than just surface features.
- Mechanism: By intervening on causally meaningful variables, CADE creates adversarial examples that exploit fundamental vulnerabilities in how models learn from data distributions. These distributional shifts affect the Markov blanket of the target variable in ways that are not captured by standard training, making the attacks effective across different model architectures.
- Core assumption: Models learn similar causal structures from data, so distributional shifts affecting these structures transfer across models.
- Evidence anchors:
  - [abstract]: "CADE achieves competitive results on white-box and transfer-based black-box attacks"
  - [section 4]: "CADE(Mb) achieve the highest transfer-based ASR among all competitors"
  - [corpus]: No direct corpus evidence for this specific mechanism

## Foundational Learning

- Concept: Causal inference and structural causal models (SCM)
  - Why needed here: The entire CADE framework relies on understanding causal relationships between variables to identify valid attack points and generate realistic counterfactuals
  - Quick check question: What are the three components of a structural causal model (M(x, f, u))?

- Concept: Counterfactual reasoning and abduction-action-prediction framework
  - Why needed here: CADE uses this framework to generate adversarial examples by predicting the consequences of hypothetical interventions
  - Quick check question: What are the three steps in Pearl's counterfactual reasoning framework?

- Concept: Adversarial machine learning and attack transferability
  - Why needed here: CADE needs to understand how adversarial examples can transfer between models to evaluate effectiveness against black-box targets
  - Quick check question: What is the key difference between white-box and black-box adversarial attacks?

## Architecture Onboarding

- Component map: Image → Encoder → Latent variables → Intervention → Counterfactual prediction → Decoder → Adversarial image → Model evaluation

- Critical path: The causal generative model (DEAR) encodes images into latent causal representations, interventions are applied to targeted latent variables, counterfactual predictions are made using the SCM, and the decoder reconstructs adversarial images for model evaluation.

- Design tradeoffs:
  - Using latent space vs pixel space: Latent space allows more meaningful interventions but requires accurate causal models
  - Intervention vs perturbation: Intervention preserves causal relationships but is more complex; perturbation is simpler but less realistic
  - Transferability vs specificity: Targeting causal mechanisms improves transferability but may reduce attack strength against specific models

- Failure signatures:
  - Low attack success rate despite correct causal structure → model may be robust to distributional shifts
  - Generated examples look unrealistic → causal generative model is inaccurate
  - Transferability fails → models learn different causal representations
  - Optimization gets stuck → step size or intervention budget needs adjustment

- First 3 experiments:
  1. Validate the causal generative model on Pendulum dataset by checking if latent interventions produce expected shadow changes
  2. Test white-box attacks on a simple MLP regression model using SynMeasurement dataset with known causal structure
  3. Compare attack success rates between CADE and FGSM on CelebA dataset using a pre-trained Res-50 model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CADE change when using an incomplete causal graph versus a complete one for image datasets?
- Basis in paper: [explicit] The paper states that CADE does not achieve the best performance on white-box attacks for CelebA because the causal graph is incomplete, suggesting limited capacity to flip predictions when drawn from interventions on such incomplete SCMs.
- Why unresolved: The paper only provides qualitative observations about incomplete causal graphs but does not quantitatively compare the performance differences between complete and incomplete causal graphs on image datasets.
- What evidence would resolve it: A controlled experiment comparing CADE's performance using both complete and incomplete causal graphs on image datasets, with detailed metrics on attack success rates.

### Open Question 2
- Question: Can CADE be effectively adapted to physical-world scenarios where the full causal generating process is not obtainable?
- Basis in paper: [explicit] The paper discusses the challenge of obtaining the required causal generating process and mentions the need to adapt CADE to more realistic settings where only partial causal knowledge can be obtained.
- Why unresolved: The paper suggests future work but does not provide experimental results or methodologies for adapting CADE to physical-world scenarios with limited causal knowledge.
- What evidence would resolve it: Experiments demonstrating CADE's effectiveness in physical-world settings, such as using real-world data with partial causal knowledge, and comparing performance against traditional adversarial attack methods.

### Open Question 3
- Question: How does the intervention budget ϵ affect the imperceptibility and effectiveness of adversarial examples generated by CADE?
- Basis in paper: [explicit] The paper investigates the sensitivity of CADE to different intervention budgets ϵ and observes increasing attack success rates with larger budgets, indicating more extensive search spaces.
- Why unresolved: While the paper shows a trend in attack success rates with varying ϵ, it does not explore the trade-off between imperceptibility and effectiveness or provide a comprehensive analysis of optimal budget settings.
- What evidence would resolve it: A detailed study analyzing the relationship between ε, the imperceptibility of adversarial examples, and their effectiveness, including metrics like human perception tests and transferability across models.

## Limitations
- Limited causal structure validation: Practical implementation relies on pre-defined causal graphs that may not capture real-world complexities
- Generative model fidelity concerns: Effectiveness depends heavily on the accuracy of the causal generative model (DEAR)
- Transferability generalization: Evaluation limited to standard model architectures, needs validation across more diverse model families

## Confidence
- High confidence in the theoretical framework: The causal analysis and formal definitions are mathematically sound
- Medium confidence in practical effectiveness: Empirical results show competitive performance but rely on synthetic datasets
- Medium confidence in transferability claims: Transfer rates are impressive but mechanism explanation needs further validation

## Next Checks
1. **Ablation study on causal structure accuracy**: Systematically vary the correctness of the causal graph specification and measure the impact on attack success rates and example realism.
2. **Cross-architecture transferability analysis**: Test CADE's transfer effectiveness on more diverse model architectures (attention-based models, small architectures, specialized CNNs).
3. **Counterfactual realism evaluation**: Conduct human studies or use automated realism metrics to verify that CADE-generated examples truly preserve realistic relationships between non-intervened variables.