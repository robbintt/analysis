---
ver: rpa2
title: 'ArcMMLU: A Library and Information Science Benchmark for Large Language Models'
arxiv_id: '2311.18658'
source_url: https://arxiv.org/abs/2311.18658
tags:
- science
- data
- llms
- information
- arcmmlu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ArcMMLU, a Chinese benchmark for evaluating\
  \ large language models in the Library & Information Science domain. ArcMMLU covers\
  \ four subdomains\u2014Archival Science, Data Science, Library Science, and Information\
  \ Science\u2014with over 6,000 curated single-choice questions."
---

# ArcMMLU: A Library and Information Science Benchmark for Large Language Models
## Quick Facts
- arXiv ID: 2311.18658
- Source URL: https://arxiv.org/abs/2311.18658
- Reference count: 7
- This paper introduces ArcMMLU, a Chinese benchmark for evaluating large language models in the Library & Information Science domain.

## Executive Summary
This paper introduces ArcMMLU, a Chinese benchmark for evaluating large language models in the Library & Information Science domain. ArcMMLU covers four subdomains—Archival Science, Data Science, Library Science, and Information Science—with over 6,000 curated single-choice questions. The benchmark reveals that mainstream models achieve an average accuracy above 50%, yet still show notable performance gaps, indicating room for improvement. Analysis shows that larger models generally perform better, but some exhibit performance drops when few-shot examples are introduced, possibly due to test data leakage or weak in-context generalization. Error analysis highlights deficiencies in domain-specific knowledge, especially in Archival and Information Science, where more specialized data is needed during training. ArcMMLU provides a robust, domain-focused evaluation tool to guide future development of LIS-tailored LLMs.

## Method Summary
ArcMMLU is constructed through systematic data collection from authoritative LIS sources, followed by expert annotation and quality control. The dataset contains over 6,000 single-choice questions across four subdomains: Archival Science, Data Science, Library Science, and Information Science. Questions are split into development and test sets, with regular expressions used to extract model answers during evaluation. The benchmark supports both zero-shot and few-shot (1-5 examples) evaluation settings, following established MMLU/CMMLU evaluation protocols.

## Key Results
- Mainstream LLMs achieve average accuracy above 50% on ArcMMLU, significantly outperforming random guessing (~25%).
- Models perform better in Library Science and Data Science (60.87% and 60.56% accuracy) compared to Archival Science and Information Science (56.31% and 55.15%).
- Some models show performance degradation when few-shot examples are introduced, possibly due to test data leakage or weak in-context generalization.

## Why This Works (Mechanism)
### Mechanism 1
- Claim: The benchmark's domain-specific focus on LIS subdomains improves evaluation precision over general benchmarks.
- Mechanism: By tailoring questions to four LIS subdomains—Archival Science, Data Science, Library Science, and Information Science—ArcMMLU isolates domain-specific knowledge that general benchmarks miss, leading to more targeted performance gaps and insights.
- Core assumption: LIS knowledge is sufficiently distinct and underrepresented in general LLMs that a specialized benchmark will reveal meaningful performance differences.
- Evidence anchors:
  - [abstract] "covers four subdomains—Archival Science, Data Science, Library Science, and Information Science—with over 6,000 curated single-choice questions."
  - [section] "Unlike more general fields, LIS encompasses a distinct and relatively self-contained set of disciplinary knowledge, often not adequately covered by comprehensive benchmarks like CMMLU."
  - [corpus] Weak evidence; corpus neighbors are related to domain-specific LLMs but not LIS specifically.
- Break condition: If LIS subdomains overlap extensively with general knowledge areas, the benchmark would not provide unique differentiation.

### Mechanism 2
- Claim: Performance variation across subdomains indicates uneven model exposure during pre-training.
- Mechanism: Better performance in Data Science and Library Science suggests these areas benefit from interdisciplinary overlap with widely represented domains (e.g., computer science), while Archival Science and Information Science suffer from less exposure.
- Core assumption: Model pre-training data distribution strongly influences performance on domain-specific benchmarks.
- Evidence anchors:
  - [abstract] "models generally perform better in library science and data science, with averaged accuracies reaching 60.87% and 60.56%, respectively. In contrast, performance in archival science and information science is notably lower, at 56.31% and 55.15%, respectively."
  - [section] "This phenomenon may be caused by the interdisciplinary nature of data science and library science, which extensively overlap with other fields such as computer science and management."
  - [corpus] Weak evidence; corpus does not include performance distribution analysis.
- Break condition: If performance gaps are due to inherent difficulty rather than training data imbalance, the mechanism would fail.

### Mechanism 3
- Claim: Few-shot examples can degrade performance due to test data leakage or weak in-context generalization.
- Mechanism: Models like ChatGLM-6B show performance drops when few-shot examples are introduced, possibly because their training included similar data or because they cannot generalize well from in-context examples.
- Core assumption: Test data leakage or inadequate in-context learning can cause negative transfer when few-shot examples are provided.
- Evidence anchors:
  - [abstract] "some exhibit performance drops when few-shot examples are introduced, possibly due to test data leakage or weak in-context generalization."
  - [section] "Some models, like ChatGLM-6B (dropping from 44.19% to 39.67%) and ChatGLM2-6B (from 52.93% to 51.84%), exhibit a decline in performance."
  - [corpus] Weak evidence; corpus does not directly address few-shot performance dynamics.
- Break condition: If performance degradation is due to prompt formatting rather than data leakage, the mechanism would not hold.

## Foundational Learning
- Concept: Domain-specific benchmarking
  - Why needed here: To accurately assess LLMs in specialized LIS tasks that general benchmarks cannot evaluate.
  - Quick check question: What are the four LIS subdomains covered by ArcMMLU?
- Concept: In-context learning and few-shot evaluation
  - Why needed here: To understand how models adapt to new tasks with limited examples and why performance may drop.
  - Quick check question: Why might some models perform worse with few-shot examples?
- Concept: Data leakage in model evaluation
  - Why needed here: To recognize when models overfit to test-like data, undermining fair evaluation.
  - Quick check question: What is one possible reason for performance drops when few-shot examples are added?

## Architecture Onboarding
- Component map: Data collection → preprocessing → annotation → dev/test split → evaluation framework → analysis pipeline
- Critical path: Data collection → preprocessing → annotation → benchmark construction → model evaluation → error analysis
- Design tradeoffs: High-quality, domain-specific data vs. scale; few-shot vs. zero-shot evaluation trade-offs; balancing subdomain coverage vs. depth
- Failure signatures: Data leakage leading to overfitting; poor annotation consistency; imbalanced subdomain representation; inadequate few-shot generalization
- First 3 experiments:
  1. Evaluate a baseline model on zero-shot ArcMMLU to establish performance floor
  2. Test the same model with increasing few-shot examples (1-5) to observe performance trends
  3. Conduct error analysis on consistently failed questions to identify knowledge gaps

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of open-source models vary when more in-domain training data is incorporated during pre-training?
- Basis in paper: [inferred] The paper notes that some models perform worse in few-shot settings, possibly due to lack of domain-specific knowledge, suggesting that more LIS-related data could help.
- Why unresolved: The study does not conduct experiments varying the amount or specificity of domain data during pre-training.
- What evidence would resolve it: Training multiple versions of models with varying amounts of LIS data and comparing their performance on ArcMMLU.

### Open Question 2
- Question: What are the specific types of knowledge gaps that cause open-source models to underperform on the ArcMMLU-Hard subset?
- Basis in paper: [explicit] The paper mentions that GPT-4's correct answers on hard questions demonstrate strong reasoning and knowledge, implying other models lack this.
- Why unresolved: The paper does not provide a detailed error analysis categorizing the types of mistakes made on hard questions.
- What evidence would resolve it: Manual analysis of incorrect answers on hard questions to categorize knowledge gaps (e.g., domain-specific facts, reasoning skills).

### Open Question 3
- Question: How do different few-shot example selection strategies impact model performance on ArcMMLU?
- Basis in paper: [inferred] The paper observes performance variations with few-shot examples but does not explore how example selection affects results.
- Why unresolved: The study uses a fixed set of few-shot examples without testing alternative selection methods.
- What evidence would resolve it: Comparing model performance using different strategies for selecting few-shot examples (e.g., random, difficulty-based, domain-specific).

## Limitations
- The benchmark's Chinese language focus limits its applicability to English-language models and international LIS contexts.
- The curated dataset, while substantial at over 6,000 questions, may still contain inherent biases based on the selection criteria and annotation process.
- The performance analysis primarily focuses on accuracy metrics without deeper investigation into reasoning quality or practical task completion capabilities.

## Confidence
- Confidence in the core findings is Medium-High. The benchmark construction methodology is sound and follows established practices in the field, with clear documentation of data collection and evaluation procedures.
- Confidence is lower regarding the interpretation of few-shot performance degradation, as the paper acknowledges uncertainty about whether this stems from test data leakage or weak in-context generalization.
- The error analysis revealing subdomain-specific weaknesses is compelling but requires further validation through additional experiments with different model architectures and training approaches.

## Next Checks
1. Conduct cross-linguistic validation by translating a subset of ArcMMLU questions into English and evaluating both Chinese and English LLMs to assess language dependency of the benchmark.
2. Perform ablation studies on the few-shot examples to determine whether specific example types cause performance degradation, distinguishing between data leakage and genuine in-context learning limitations.
3. Expand error analysis to include qualitative assessment of model responses, categorizing failure types beyond simple accuracy to understand whether models are guessing randomly or exhibiting systematic misconceptions in specific LIS subdomains.