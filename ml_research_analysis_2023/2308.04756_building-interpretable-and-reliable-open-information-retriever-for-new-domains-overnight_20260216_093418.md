---
ver: rpa2
title: Building Interpretable and Reliable Open Information Retriever for New Domains
  Overnight
arxiv_id: '2308.04756'
source_url: https://arxiv.org/abs/2308.04756
tags:
- information
- pipeline
- retrieval
- query
- linking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new information retrieval (IR) pipeline
  for open-domain question answering that leverages entity/event linking and query
  decomposition to improve both interpretability and cross-domain performance. Unlike
  existing dense retrieval models that use single vector representations, this approach
  explicitly identifies relevant information units through symbolic reasoning processes.
---

# Building Interpretable and Reliable Open Information Retriever for New Domains Overnight

## Quick Facts
- arXiv ID: 2308.04756
- Source URL: https://arxiv.org/abs/2308.04756
- Reference count: 6
- Primary result: 6-10% improvement over state-of-the-art unsupervised and cross-domain supervised retrievers

## Executive Summary
This paper introduces a novel information retrieval pipeline for open-domain question answering that improves both interpretability and cross-domain performance. The approach leverages entity and event linking combined with query decomposition to explicitly identify relevant information units through symbolic reasoning processes. Unlike traditional dense retrieval models that use single vector representations, this method decomposes queries into hypothetical supporting events, links them to real-world knowledge, and uses cross-domain supervised ranking for fine-grained passage selection.

## Method Summary
The pipeline consists of entity linking, event linking, query decomposition, coarse BM25 filtering, and cross-domain supervised ranking. It generates hypothetical supporting events from queries, links these events to real-world knowledge using Wikipedia-based models, performs coarse filtering to select top passages, and applies fine-grained ranking using a T5-large model trained on existing QA datasets. The method requires no domain-specific training or parameter tuning and achieves 6-10% improvements across five QA benchmarks.

## Key Results
- 6-10% improvement over state-of-the-art unsupervised and cross-domain supervised retrievers
- Achieves superior passage coverage (recall@5, recall@20) across NQ, TriviaQA, and HotpotQA
- Demonstrates strong accuracy on BoolQA and StrategyQA datasets
- No domain-specific training or parameter tuning required

## Why This Works (Mechanism)

### Mechanism 1
- Decomposing queries into hypothetical supporting events enables better retrieval of implicit information
- Core assumption: Query decompositions can be accurately generated and linked to relevant real-world events that support answering the question
- Break condition: If the decomposition model generates irrelevant or non-existent events, or if the event linking model cannot find corresponding real-world events

### Mechanism 2
- Cross-domain supervised ranking improves fine-grained passage selection over unsupervised methods
- Core assumption: Evidence annotations from existing QA datasets can generalize to improve passage selection across different domains
- Break condition: If the training datasets are too domain-specific or the evidence annotations don't generalize

### Mechanism 3
- Entity and event linking provide interpretable intermediate steps that improve reliability
- Core assumption: The entity and event linking models can accurately identify relevant knowledge base entries without in-domain supervision
- Break condition: If the linking models make incorrect associations, the entire retrieval pipeline will follow faulty reasoning paths

## Foundational Learning

- **Dense retrieval vs sparse retrieval**
  - Why needed here: Understanding the fundamental difference between vector-based semantic similarity and lexical matching is crucial for appreciating why the pipeline's approach is novel
  - Quick check question: Why might a single dense vector representation be insufficient for queries requiring multi-step reasoning?

- **Query decomposition and event linking**
  - Why needed here: These are the core innovations that distinguish this approach from standard dense retrievers
  - Quick check question: How does decomposing a query into hypothetical events help retrieve implicit information that a single vector might miss?

- **Cross-domain transfer learning**
  - Why needed here: The pipeline's effectiveness relies on the ability to transfer supervision from existing QA datasets to new domains without fine-tuning
  - Quick check question: What challenges might arise when using evidence annotations from one dataset to rank passages for a different dataset?

## Architecture Onboarding

- **Component map**: Query → Entity Linking → Event Linking → Query Decomposition → Event Linking (again) → Coarse BM25 Filtering → Cross-domain Supervised Ranking → Top-K Passages
- **Critical path**: The event linking and query decomposition components are most critical, as errors here propagate through the entire pipeline
- **Design tradeoffs**: The pipeline sacrifices some computational efficiency for interpretability and cross-domain generalization by using multiple explicit reasoning steps instead of a single dense vector
- **Failure signatures**: Poor recall on questions requiring implicit reasoning, inconsistent performance across different datasets, or inability to handle queries with complex event dependencies
- **First 3 experiments**:
  1. Test entity linking accuracy on a small set of queries with known entity references
  2. Evaluate query decomposition quality by comparing generated events to human-annotated reasoning steps
  3. Measure cross-domain ranking effectiveness by training on one dataset and testing on another with known answer passages

## Open Questions the Paper Calls Out

### Open Question 1
- How does the performance of the proposed pipeline compare when using different query decomposition models or architectures?
- Basis in paper: The paper mentions using T5 for query decomposition but does not explore alternative decomposition models
- Why unresolved: The paper focuses on demonstrating the effectiveness of the overall pipeline architecture rather than comparing different decomposition approaches
- What evidence would resolve it: Comparative experiments using different query decomposition models while keeping the rest of the pipeline constant

### Open Question 2
- What is the impact of varying the number of generated titles from entity linking, event linking, and decomposition on retrieval performance?
- Basis in paper: The paper mentions using 10 titles from entity linking, 5 from event linking, and 5 sets of decompositions but states "Five is a magic number"
- Why unresolved: The paper does not systematically explore how changing these numbers affects retrieval performance
- What evidence would resolve it: Systematic experiments varying the number of titles generated at each step and measuring their impact on retrieval metrics

### Open Question 3
- How does the proposed pipeline perform on datasets with different characteristics, such as those requiring multi-hop reasoning versus single-hop retrieval?
- Basis in paper: The paper evaluates on five QA datasets but does not analyze performance differences based on reasoning complexity
- Why unresolved: While the paper shows overall performance improvements, it does not provide insight into whether the pipeline's advantages are consistent across different types of reasoning tasks
- What evidence would resolve it: Detailed analysis of performance on datasets with varying reasoning complexity or manual categorization of queries by their reasoning requirements

## Limitations

- Limited evidence for true zero-shot domain transfer beyond the five benchmark datasets used
- No direct comparisons with state-of-the-art in-domain fine-tuned models
- Lack of systematic evaluation of whether intermediate representations are actually useful for debugging or improving the pipeline in practice

## Confidence

- **High confidence**: The core mechanism of using entity/event linking followed by query decomposition is technically sound and well-supported by ablation studies
- **Medium confidence**: Claims about interpretability improvements are reasonable but lack systematic user studies or qualitative analysis of intermediate outputs
- **Low confidence**: The claim that no parameter tuning is needed depends on unspecified hyperparameters for the fine-grained ranking model

## Next Checks

1. **Cross-domain robustness test**: Evaluate the pipeline on a truly held-out domain (e.g., scientific literature, legal documents) not represented in any of the five benchmark datasets to verify cross-domain generalization capability.

2. **Interpretability audit**: Conduct a qualitative analysis where human evaluators examine the intermediate outputs (linked entities, decomposed events) to assess whether they provide meaningful insights into the retrieval process.

3. **Computational efficiency comparison**: Measure the inference time and memory requirements of the full pipeline versus a standard dense retriever to quantify the performance-interpretability tradeoff.