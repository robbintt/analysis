---
ver: rpa2
title: Tiny and Efficient Model for the Edge Detection Generalization
arxiv_id: '2308.06468'
source_url: https://arxiv.org/abs/2308.06468
tags:
- edge
- detection
- teed
- image
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TEED is a lightweight edge detector (58K parameters) designed for
  efficiency and generalization. Unlike SOTA models that grow in complexity, TEED
  uses a compact CNN backbone with smish activation and skip connections, paired with
  a new double fusion module (dfuse) and double loss (dloss).
---

# Tiny and Efficient Model for the Edge Detection Generalization

## Quick Facts
- arXiv ID: 2308.06468
- Source URL: https://arxiv.org/abs/2308.06468
- Reference count: 40
- Primary result: 58K parameter edge detector achieving ODS=0.828, OIS=0.842 on UDED

## Executive Summary
TEED is a lightweight edge detector designed for efficiency and generalization. Unlike state-of-the-art models that grow in complexity, TEED uses a compact CNN backbone with smish activation and skip connections, paired with a new double fusion module (dfuse) and double loss (dloss). It is trained from scratch on BIPED, avoiding transfer learning. TEED outperforms heavier models on UDED, a new generalization dataset, achieving ODS=0.828, OIS=0.842, and best PSNR. It also excels in sketch-based retrieval, proving strong real-world generalization. Training takes <30 minutes for 6 epochs, making it highly efficient for edge detection tasks.

## Method Summary
TEED uses a 58K parameter CNN backbone with 3 convolutional blocks and skip connections. Edge detection occurs through 3 parallel USNets producing multi-scale predictions. These are combined using a novel double fusion module (dfuse) with depthwise convolutions, then refined with tracing loss. The model trains from scratch on BIPED using a double loss function combining weighted cross-entropy and tracing loss. No transfer learning is used, and training completes in under 30 minutes for 6 epochs.

## Key Results
- Achieves ODS=0.828, OIS=0.842 on UDED generalization dataset
- Outperforms heavier models (CATS with 63M parameters) on PSNR metric
- Excels in sketch-based image retrieval tasks
- Training time under 30 minutes for 6 epochs on BIPED

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Double fusion module (dfuse) efficiently combines multi-scale edge predictions with reduced parameter count compared to coFusion.
- Mechanism: dfuse uses two depthwise convolutions to generate pixel-level weight maps, then applies element-wise addition twice—once to combine depthwise outputs, and once to fuse with original predictions—achieving richer feature fusion with fewer parameters.
- Core assumption: Depthwise convolutions can sufficiently capture spatial relationships for edge fusion without sacrificing quality.
- Evidence anchors:
  - [abstract] "dfuse: a new, efficient fusion module inspired from Co-Fusion in CATS [17] (while coFusion has around 40K parameters, dfuse has less than 1K parameters)"
  - [section] "This module does not use Softmax activation nor group normalization; instead, it employs the Smish activation function [45] (ζ) to regularize the weight maps during training."
- Break condition: If depthwise convolutions cannot capture sufficient spatial context for accurate edge fusion, dfuse performance will degrade.

### Mechanism 2
- Claim: Double loss (dloss) balances edge detection accuracy with cleaner edge maps by combining weighted cross-entropy (Lwce) and tracing loss (Ltrcg).
- Mechanism: Lwce optimizes for edge detection completeness by monitoring USNet outputs, while Ltrcg refines the final dfuse output for crisp edges, preventing artifacts from conflicting annotations.
- Core assumption: The combination of Lwce for backbone and Ltrcg for fusion can simultaneously maximize edge detection and minimize false positives.
- Evidence anchors:
  - [abstract] "a dloss function for efficient and rapid training convergence"
  - [section] "TEED employs Lwce for comparing the ground-truth y with the outputs of USNets {ˆyi}3 1 as well as Ltrcg for ˆyd f use."
- Break condition: If Ltrcg overly suppresses true edges, the model may miss important edge features despite cleaner outputs.

### Mechanism 3
- Claim: Training from scratch on BIPED avoids transfer learning pitfalls and achieves strong generalization to unseen datasets.
- Mechanism: By training exclusively on BIPED, TEED learns edge detection patterns without bias from BSDS-based datasets, enabling better performance on UDED and downstream tasks.
- Core assumption: BIPED's edge annotations are more representative of general edge detection needs than BSDS.
- Evidence anchors:
  - [abstract] "Training on the BIPED dataset takes less than 30 minutes, with each epoch requiring less than 5 minutes"
  - [section] "According to [16, 24, 40], the standard datasets for edge detection, like BSDS [1], are originally introduced for image segmentation; although they have edge level annotations some of their ground truth comes with wrong annotations [16, 41]."
- Break condition: If BIPED lacks diversity in edge types, TEED may fail to generalize to certain edge detection scenarios.

## Foundational Learning

- Concept: Depthwise separable convolutions
  - Why needed here: Enables efficient spatial feature extraction in dfuse with fewer parameters
  - Quick check question: How does a depthwise convolution differ from a standard convolution in terms of computational complexity?

- Concept: Weighted cross-entropy loss for imbalanced data
  - Why needed here: Addresses the imbalance between edge and non-edge pixels in training data
  - Quick check question: Why does weighted cross-entropy help when edge pixels are much less frequent than non-edge pixels?

- Concept: Multi-scale feature fusion in CNNs
  - Why needed here: Combines edge predictions from different receptive fields for more complete edge detection
  - Quick check question: What information does each scale level contribute to edge detection, and why is combining them beneficial?

## Architecture Onboarding

- Component map: Input -> Backbone (3 conv blocks with skip connections) -> USNet branches (3 scales) -> dfuse (double fusion) -> Output edge map
- Critical path: Input → Backbone → USNet branches → dfuse → Output edge map
- Design tradeoffs:
  - Parameter efficiency vs. accuracy: TEED sacrifices some capacity compared to larger models but achieves better efficiency
  - Training time vs. convergence: Double loss enables faster convergence at the cost of more complex training logic
  - Generalization vs. dataset bias: Training from scratch on BIPED avoids BSDS bias but may miss some edge patterns
- Failure signatures:
  - Noisy or blurry edges: Likely issues with dfuse or Ltrcg component
  - Missing thin edges: Possible problems with Lwce or backbone receptive field
  - Slow convergence: Check learning rate or loss function balance
- First 3 experiments:
  1. Train with only Lwce loss (no Ltrcg) to isolate its impact on edge quality
  2. Replace dfuse with simple concatenation and 1x1 convolution to measure fusion module contribution
  3. Train on BSDS instead of BIPED to compare transfer learning vs. from-scratch performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the generalization performance of TEED compare to other SOTA edge detectors when trained on datasets other than BIPED?
- Basis in paper: [explicit] The paper mentions that TEED is trained from scratch on BIPED and evaluated on UDED, but does not explore training on other datasets.
- Why unresolved: The paper only evaluates TEED's generalization when trained on BIPED, leaving open the question of how it would perform with different training data.
- What evidence would resolve it: Comparative experiments training TEED on various datasets and evaluating generalization performance on UDED or similar datasets.

### Open Question 2
- Question: Can the double fusion module (dfuse) be effectively adapted for other computer vision tasks beyond edge detection?
- Basis in paper: [explicit] The paper introduces dfuse as a novel fusion module for TEED but does not explore its applicability to other tasks.
- Why unresolved: The paper focuses solely on edge detection and does not investigate the broader applicability of dfuse.
- What evidence would resolve it: Empirical studies applying dfuse to tasks like object detection, segmentation, or image enhancement, and comparing performance to existing fusion methods.

### Open Question 3
- Question: How does the computational efficiency of TEED scale with increasing image resolution and batch size?
- Basis in paper: [explicit] The paper reports FPS and training times for TEED but does not analyze scaling behavior with different resolutions or batch sizes.
- Why unresolved: The paper provides baseline efficiency metrics but lacks a systematic analysis of how performance changes under different computational loads.
- What evidence would resolve it: Benchmarking TEED across a range of image resolutions and batch sizes, measuring FPS, memory usage, and training/inference times.

## Limitations
- The paper's claims about training from scratch on BIPED are compelling, but the extent to which BIPED's edge annotations are superior to BSDS is not empirically validated within the paper.
- The claim that BIPED avoids "wrong annotations" is asserted but not demonstrated with concrete examples or quantitative comparison of annotation quality.
- The assertion that training from scratch on BIPED is fundamentally better than transfer learning from BSDS lacks ablation studies showing what specific edge patterns BIPED captures that BSDS misses.

## Confidence
- **High Confidence**: The architectural design choices (58K parameters, smish activation, double fusion) and their stated efficiency benefits are well-specified and reproducible.
- **Medium Confidence**: The generalization claims on UDED are supported by ODS/OIS metrics, but the comparison with larger models (CATS with 63M parameters) could be strengthened with runtime comparisons and memory usage analysis.
- **Low Confidence**: The assertion that training from scratch on BIPED is fundamentally better than transfer learning from BSDS lacks ablation studies showing what specific edge patterns BIPED captures that BSDS misses.

## Next Checks
1. Implement an ablation study comparing TEED trained on BIPED vs. fine-tuned from a BSDS-pretrained model to quantify the claimed advantage of training from scratch.
2. Add runtime and memory profiling for TEED vs. CATS to validate the efficiency claims beyond parameter count.
3. Conduct cross-dataset validation by testing TEED on BSDS500 and other standard edge detection benchmarks to assess true generalization beyond UDED.