---
ver: rpa2
title: 'Bad Values but Good Behavior: Learning Highly Misspecified Bandits and MDPs'
arxiv_id: '2310.09358'
source_url: https://arxiv.org/abs/2310.09358
tags:
- robust
- region
- regret
- bandit
- observation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies when linear bandit and contextual bandit algorithms\
  \ like \u03B5-greedy and LinUCB can achieve sublinear regret even when the true\
  \ reward function is not in the model class. The key insight is to identify a \u201C\
  robust observation region\u201D for each optimal arm such that any instance in that\
  \ region yields no regret under greedy play."
---

# Bad Values but Good Behavior: Learning Highly Misspecified Bandits and MDPs

## Quick Facts
- arXiv ID: 2310.09358
- Source URL: https://arxiv.org/abs/2310.09358
- Reference count: 40
- Primary result: ε-greedy and LinUCB achieve O(√T) regret on misspecified linear bandits when true rewards lie in the "robust observation region"

## Executive Summary
This paper challenges the conventional wisdom that linear bandit algorithms require accurate reward models. The authors show that standard algorithms like ε-greedy and LinUCB can achieve sublinear regret even when the true reward function is not in the linear model class. The key insight is identifying a "robust observation region" - a set of bandit instances where any sampling distribution leads to the optimal arm under greedy play. This structural property ensures that even with model misspecification, the algorithms continue to select optimal actions.

## Method Summary
The paper analyzes ε-greedy with exploration rate 1/√t and LinUCB for misspecified linear bandits and contextual bandits. The core approach is to characterize a robust observation region where the true reward vector guarantees optimal arm selection despite model error. For ε-greedy, the algorithm achieves O(√T) regret when the true rewards are interior to this region. For LinUCB, the analysis leverages high-confidence ellipsoids to maintain optimism about the optimal arm. The authors provide a closed-form characterization of this region for linear models and validate their theoretical results with synthetic experiments.

## Key Results
- ε-greedy with rate 1/√t achieves O(√T) regret for misspecified linear bandits when true rewards lie in the robust observation region
- LinUCB achieves O(√T) regret under the same conditions, using high-confidence ellipsoids
- The robust observation region can be characterized analytically for linear models
- Many misspecified instances empirically fall within the robust observation region

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ε-greedy with 1/√t achieves O(√T) regret for misspecified linear bandits when true rewards are in the robust observation region
- Mechanism: The robust observation region ensures that for any sampling distribution, projecting true rewards onto the linear model preserves the optimal arm under greedy play
- Core assumption: True reward vector must be an interior point of the robust observation region
- Evidence anchors: Abstract confirms O(√T) regret; Section 2.1 provides formal theorem
- Break condition: Fails if rewards outside robust region or exploration rate too low

### Mechanism 2
- Claim: LinUCB achieves O(√T) regret for misspecified linear bandits when true rewards are in the robust observation region
- Mechanism: High-confidence ellipsoids maintain optimism about optimal arm when true rewards remain in greedy region under any sampling
- Core assumption: True reward vector interior to robust region; sub-Gaussian noise
- Evidence anchors: Abstract confirms O(√T) regret; Section 2.1 provides formal theorem
- Break condition: Fails if rewards outside robust region or noise non-sub-Gaussian

### Mechanism 3
- Claim: Robust observation region can be characterized analytically for linear models
- Mechanism: Region defined by projection properties - for any full-rank submatrix, projected rewards remain in greedy region for optimal arm
- Core assumption: Feature matrix has full column rank; unique optimal arm
- Evidence anchors: Theorem 2.13 provides closed-form characterization; Section 2.1 details evaluation
- Break condition: Fails with rank-deficient features or non-unique optimal arms

## Foundational Learning

- Concept: Linear bandits and contextual bandits
  - Why needed here: The paper studies misspecified linear bandits and contextual bandits, so understanding the basic setup is crucial
  - Quick check question: In a linear bandit with K arms and d-dimensional features, what is the form of the estimated reward for arm i given parameter θ?

- Concept: Robustness to misspecification
  - Why needed here: The key insight is that some misspecified instances are still robust to model error, achieving sublinear regret
  - Quick check question: What is the robust observation region, and why does it ensure sublinear regret for ε-greedy and LinUCB?

- Concept: High-confidence ellipsoids in LinUCB
  - Why needed here: LinUCB uses high-confidence ellipsoids to estimate the parameter space, and understanding how they work is crucial for the analysis
  - Quick check question: How does LinUCB use high-confidence ellipsoids to select arms, and what role do they play in ensuring robustness to misspecification?

## Architecture Onboarding

- Component map: Bandit setup (arms, features, rewards) -> Linear model class -> Robust observation region -> Algorithms (ε-greedy, LinUCB) -> Optimal arm selection
- Critical path: (1) Define bandit setup and linear model class; (2) Characterize robust observation region; (3) Analyze ε-greedy and LinUCB under robust region assumption; (4) Prove O(√T) regret
- Design tradeoffs: Model complexity vs robustness to misspecification - more complex models may fit better but be more sensitive to error
- Failure signatures: Linear regret growth when true rewards outside robust region; insufficient exploration; non-sub-Gaussian noise for LinUCB
- First 3 experiments:
  1. Implement ε-greedy and LinUCB for simple linear bandit (2 arms, 1D features), verify O(√T) regret when rewards in robust region
  2. Modify true rewards to be outside robust region, verify failure to achieve sublinear regret
  3. Implement algorithms for contextual bandit (2 contexts, 3 arms each), verify O(√T) regret when rewards in robust region

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the robust observation region characterization be extended to non-linear function classes beyond linear models, and if so, what structural properties of the function class would be necessary?
- Basis in paper: [explicit] The paper mentions in Section G that the definitions extend to general function classes, but characterizing the robust regions for non-linear classes is left as an open problem
- Why unresolved: The paper only provides a general framework for non-linear function classes without providing concrete characterizations of the robust regions for specific non-linear models
- What evidence would resolve it: A proof or counterexample showing whether the robust region characterization can be extended to a specific non-linear function class (e.g., neural networks, kernel methods) would resolve this question

### Open Question 2
- Question: Is it possible to design a misspecification-aware algorithm that achieves better regret bounds than the standard ε-greedy and LinUCB algorithms for instances outside the robust observation region?
- Basis in paper: [explicit] The paper focuses on analyzing the performance of standard algorithms when the instance is in the robust region, but does not explore whether modified algorithms could perform better on instances outside this region
- Why unresolved: The paper proves that standard algorithms achieve sublinear regret for instances in the robust region, but does not investigate whether modifications could improve performance on instances outside the region
- What evidence would resolve it: A proof showing that a modified version of ε-greedy or LinUCB (or a completely new algorithm) can achieve sublinear regret on instances outside the robust region would resolve this question

### Open Question 3
- Question: How does the size of the robust observation region scale with the dimensionality of the feature space and the number of arms in the bandit problem?
- Basis in paper: [inferred] The paper provides examples and theoretical results for specific feature matrices and bandit instances, but does not provide a general characterization of how the size of the robust region scales with problem parameters
- Why unresolved: While the paper proves that the robust region exists and provides some examples, it does not provide a general characterization of how the size scales with dimensionality (d) and number of arms (K)
- What evidence would resolve it: A theoretical analysis showing how the volume or measure of the robust region scales with d and K would resolve this question

## Limitations

- The analysis assumes the true reward vector lies strictly in the interior of the robust observation region, with unclear implications for boundary cases
- The extension from linear bandits to contextual bandits assumes context arrivals follow a known distribution
- The characterization of the robust observation region for general non-linear models remains an open problem

## Confidence

**High Confidence**: O(√T) regret bounds for both ε-greedy and LinUCB when true rewards lie in the robust observation region, supported by analytical characterization and mathematical proofs in Sections 2.1 and 2.2

**Medium Confidence**: Empirical validation showing many misspecified instances fall within the robust observation region, as experiments use synthetic data with specific parameter settings

**Low Confidence**: Characterization of the robust observation region for general non-linear models, as this remains an open problem with only insights provided for linear cases

## Next Checks

1. **Boundary Analysis**: Test algorithms on instances near the boundary of the robust observation region to determine if the interior assumption is necessary or if results extend to boundary cases

2. **Noise Sensitivity**: Evaluate algorithms under heavy-tailed noise distributions to verify the sub-Gaussian assumption in the LinUCB analysis

3. **Real-World Deployment**: Implement algorithms on a real-world recommendation system with known misspecification to validate theoretical predictions in practice