---
ver: rpa2
title: 'Medi-CAT: Contrastive Adversarial Training for Medical Image Classification'
arxiv_id: '2311.00154'
source_url: https://arxiv.org/abs/2311.00154
tags:
- image
- training
- datasets
- vision
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel training strategy to overcome the problems
  of underfitting and overfitting in medical image classification. The method employs
  large pre-trained vision transformers, adversarial training, and contrastive learning
  techniques.
---

# Medi-CAT: Contrastive Adversarial Training for Medical Image Classification

## Quick Facts
- arXiv ID: 2311.00154
- Source URL: https://arxiv.org/abs/2311.00154
- Reference count: 32
- Primary result: Improves accuracy up to 2% on three MedMNIST datasets and up to 4.1% over baseline methods

## Executive Summary
This paper proposes Medi-CAT, a novel training strategy that combines pre-trained vision transformers with adversarial and contrastive learning to address underfitting and overfitting in medical image classification. The method employs FGSM-based adversarial training and Barlow Twins contrastive learning to improve model robustness and representation quality. Evaluated on four MedMNIST datasets, Medi-CAT achieves significant accuracy improvements compared to existing approaches, with up to 2% gains on three datasets and 4.1% improvement over baseline methods.

## Method Summary
Medi-CAT uses a pre-trained ViT-Large as the image encoder, combining adversarial training (FGSM perturbations) and Barlow Twins contrastive learning. The total loss function combines classification loss on both clean and adversarial examples with a contrastive loss term weighted by parameter α. The method is trained for 50 epochs with batch size 48 and AdamW optimizer. Images are resized to 224x224 pixels and normalized using dataset-specific parameters.

## Key Results
- Improves accuracy up to 2% on three MedMNIST datasets
- Achieves up to 4.1% improvement over baseline methods
- Demonstrates effectiveness through ablation studies

## Why This Works (Mechanism)

### Mechanism 1
- Adversarial training mitigates overfitting by introducing controlled perturbations during training using FGSM
- Core assumption: Perturbations are small enough to preserve semantic content while regularizing the model
- Break condition: If epsilon is too large, perturbed images become semantically different

### Mechanism 2
- Contrastive learning improves representation quality by pulling clean and adversarial versions closer while pushing different images apart
- Core assumption: Clean and adversarial versions of the same image share the same semantic label
- Break condition: If batch size is too small, contrastive loss may fail to create meaningful negative pairs

### Mechanism 3
- Pre-trained large ViTs overcome underfitting by leveraging rich feature representations from large-scale datasets
- Core assumption: Features learned from natural images transfer effectively to medical images
- Break condition: If medical dataset is too small or domain shift is too large, pre-training benefits may degrade

## Foundational Learning

- Concept: Gradient-based adversarial example generation (FGSM)
  - Why needed here: FGSM produces perturbed images that regularize the model during training
  - Quick check question: In FGSM, what does the sign of the gradient determine, and why is it multiplied by epsilon?

- Concept: Self-attention in vision transformers
  - Why needed here: ViT uses self-attention to capture long-range dependencies across image patches
  - Quick check question: How does splitting an image into patches and treating them as tokens enable transformer-based vision models?

- Concept: Contrastive representation learning (Barlow Twins)
  - Why needed here: Contrastive loss encourages similar embeddings for semantically equivalent views
  - Quick check question: What is the role of the redundancy reduction term in Barlow Twins loss?

## Architecture Onboarding

- Component map: Input pipeline -> Image encoder (ViT-Large) -> Adversarial module (FGSM) -> Contrastive module (Barlow Twins) -> Loss aggregator
- Critical path: Clean image forward pass → classification loss LCE1 → FGSM generates adversarial example → forward pass → LCE2 → compute contrastive loss LCTR → aggregate losses → backward pass
- Design tradeoffs: Larger ViT models improve accuracy but increase compute; higher α emphasizes contrastive learning which may hurt accuracy; smaller epsilon reduces perturbation strength
- Failure signatures: Underfitting (low training and validation accuracy), overfitting (high training accuracy, much lower validation accuracy), contrastive loss divergence (loss explodes or NaN values)
- First 3 experiments: 1) Train baseline ViT on clean images only; 2) Add adversarial training (FGSM only); 3) Add contrastive loss with varying α values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method compare to state-of-the-art transformer-based models on larger medical image datasets beyond the MedMNIST collection?
- Basis in paper: The paper evaluates on four MedMNIST datasets but does not test on larger datasets
- Why unresolved: Study is limited to MedMNIST datasets which may not represent real-world medical imaging complexity
- What evidence would resolve it: Testing on larger, more diverse medical image datasets and comparing with state-of-the-art transformer models

### Open Question 2
- Question: What is the impact of different adversarial attack methods (beyond FGSM) on the model's robustness and performance in medical image classification?
- Basis in paper: The paper uses FGSM but does not explore other attack methods like PGD or Carlini-Wagner
- Why unresolved: Choice of FGSM may limit understanding of how different adversarial techniques affect model robustness
- What evidence would resolve it: Experimenting with various adversarial attack methods across different medical image datasets

### Open Question 3
- Question: How does the proposed method perform in multi-modal medical image classification tasks, where images from different modalities are combined?
- Basis in paper: The paper focuses on single-modality image classification and does not address multi-modal scenarios
- Why unresolved: Medical diagnosis often involves integrating information from multiple imaging modalities
- What evidence would resolve it: Evaluating the method on multi-modal medical image datasets and comparing with existing multi-modal classification approaches

### Open Question 4
- Question: What is the computational overhead introduced by the proposed training strategy, and how does it scale with larger datasets and models?
- Basis in paper: The paper mentions additional training costs but does not provide detailed analysis of computational overhead
- Why unresolved: Understanding trade-off between performance gains and computational costs is crucial for practical deployment
- What evidence would resolve it: Comprehensive analysis of training time, memory usage, and scalability compared to baseline approaches

## Limitations
- Limited ablation studies isolating individual component contributions
- Does not compare against other contrastive methods or robust training alternatives
- Results primarily validated on small MedMNIST datasets, limiting generalizability

## Confidence

- **High confidence**: Reported accuracy improvements on tested MedMNIST datasets
- **Medium confidence**: Claimed mechanisms (adversarial + contrastive) contributing to performance gains
- **Low confidence**: Generalizability of results to other medical imaging domains or larger datasets

## Next Checks

1. **Ablation study**: Remove adversarial training and contrastive loss separately to quantify their individual impact on accuracy and overfitting
2. **Comparison with alternatives**: Replace Barlow Twins with SimCLR or MoCo to test if similar gains can be achieved with other contrastive methods
3. **Transfer robustness test**: Evaluate the model on a different medical dataset (e.g., from a different imaging modality) to assess generalization beyond MedMNIST