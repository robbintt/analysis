---
ver: rpa2
title: 'Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language
  Models'
arxiv_id: '2308.11521'
source_url: https://arxiv.org/abs/2308.11521
tags:
- jailbreak
- prompt
- semantic
- prompts
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a "self-deception" attack method to bypass
  the semantic firewall of large language models (LLMs). Inspired by reverse tunneling
  in traditional firewalls, the attack guides the LLM to generate jailbreak prompts
  that lead to increasingly violative content, eventually bypassing the firewall's
  content restrictions.
---

# Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models

## Quick Facts
- arXiv ID: 2308.11521
- Source URL: https://arxiv.org/abs/2308.11521
- Reference count: 40
- 2,520 attack payloads across 7 scenarios, 3 violation types, and 6 languages achieved jailbreak success rates of 86.2% and 67% on GPT-3.5-Turbo and GPT-4 respectively

## Executive Summary
This paper introduces a "self-deception" attack method that bypasses the semantic firewall of large language models by reverse tunneling through the firewall's asymmetric inspection mechanism. The attack guides LLMs to generate jailbreak prompts that progressively escalate toward violative content, exploiting the fact that firewalls only inspect input prompts, not model outputs. Experiments demonstrate high effectiveness across multiple languages and violation types, with success rates exceeding 85% for certain model-violation combinations.

## Method Summary
The self-deception attack uses a three-step conversation template where initial prompts with low semantic similarity to violation rules pass the firewall, then the model's own outputs are used as subsequent prompts to progressively escalate toward full violation. The method works by exploiting the asymmetry between input and output inspection in semantic firewalls, allowing attackers to iteratively build up to violative content that would be blocked if submitted directly.

## Key Results
- GPT-3.5-Turbo: 86.2% jailbreak success rate, 4.7% failure rate
- GPT-4: 67% jailbreak success rate, 2.2% failure rate
- Cross-language effectiveness maintained across 6 languages (English, Russian, French, Spanish, Chinese, Arabic)
- Unified violation boundaries observed: breaking one type (e.g., violence) weakened defenses across all violation types

## Why This Works (Mechanism)

### Mechanism 1
The semantic firewall only inspects input prompts, not model outputs, creating an exploitable asymmetry. Attackers start with slightly violative prompts that pass inspection, then use generated outputs as new inputs to progressively escalate toward full violation.

### Mechanism 2
Hidden prompt prefixes act as one-way instruction layers that can be overridden by conversational context. Initial safety constraints can be bypassed when subsequent user inputs inject context that overwrites these constraints through conversation continuation.

### Mechanism 3
Fine-tuned reinforcement learning creates correlated violation boundaries where breaking one type weakens defenses across all types. The model learns a holistic "harmfulness" concept during alignment training, making violation boundaries permeable once one type is breached.

## Foundational Learning

- **Semantic similarity measurement**: Needed to understand how prompts are evaluated against violation rules; Quick check: What similarity threshold would block "peaceful discussion" against violence violation rule?
- **Prompt injection and context overriding**: Needed to understand how subsequent prompts can override initial safety instructions; Quick check: How does prompt order affect behavior with conflicting instructions?
- **Reinforcement learning from human feedback (RLHF) alignment**: Needed to understand how unified safety boundaries are learned; Quick check: What happens to safety responses when fine-tuning data contains mixed violation types?

## Architecture Onboarding

- **Component map**: User Interface → Semantic Firewall → LLM → Response
- **Critical path**: User submits prompt → Firewall evaluates input similarity → If below threshold, prompt passes to LLM → LLM generates response (no inspection) → Response returned to user
- **Design tradeoffs**: Safety vs. creativity (strict filtering limits flexibility), Performance vs. security (real-time analysis adds latency), Modularity vs. effectiveness (separate review models add complexity)
- **Failure signatures**: High failure when prompts exceed similarity thresholds, uncertain cases with ambiguous responses, language-dependent variations
- **First 3 experiments**: 1) Single-step violation with minimal similarity to rules, 2) Multi-step escalation using generated content as subsequent prompts, 3) Cross-language effectiveness using translated templates

## Open Questions the Paper Calls Out

### Open Question 1
What is the precise mechanism by which the self-deception attack bypasses the semantic firewall in LLMs? The paper describes the attack method and its effectiveness but lacks a detailed technical explanation of the underlying mechanism.

### Open Question 2
How do different LLM models compare in their vulnerability to self-deception attacks? The paper only tests GPT-3.5-Turbo and GPT-4, leaving vulnerability of other models unknown.

### Open Question 3
What are the potential defenses against self-deception attacks, and how effective are they? The paper mentions possible defenses but doesn't provide specific mechanisms or effectiveness evaluations.

## Limitations
- Architectural assumptions about firewall implementations remain unverified without access to real API implementations
- Effectiveness may not generalize across different model architectures and training regimes
- Language-specific variations may reflect training data imbalances rather than true safety differences

## Confidence

**High Confidence**: The self-deception attack methodology is technically coherent and implementable; The three-step conversation template is clearly specified; Attack successfully bypassed GPT-3.5-Turbo and GPT-4 safety mechanisms

**Medium Confidence**: Asymmetric inspection is the primary attack vector; Fine-tuning creates correlated violation boundaries; Hidden prompt prefixes can be overridden through context

**Low Confidence**: Real-world firewall implementations match assumed architecture; Attack effectiveness generalizes to all LLM variants; Language variations reflect true safety differences

## Next Checks

1. **Implement Bidirectional Firewall Testing**: Create environment applying semantic checking to both inputs and outputs to validate asymmetric inspection assumption

2. **Modular Safety Classifier Analysis**: Test whether violation boundaries are unified by jailbreaking one type while measuring changes in resistance to other types, comparing against models with independent classifiers

3. **Cross-Platform Generalization Study**: Apply attack to three different LLM APIs with varying safety implementations (open-source, commercial, different training approaches) to identify which architectural factors matter most