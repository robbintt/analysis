---
ver: rpa2
title: 'GATGPT: A Pre-trained Large Language Model with Graph Attention Network for
  Spatiotemporal Imputation'
arxiv_id: '2311.14332'
source_url: https://arxiv.org/abs/2311.14332
tags:
- imputation
- data
- time
- series
- spatiotemporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GATGPT, a novel framework for spatiotemporal
  imputation that leverages pre-trained large language models (LLMs) combined with
  a graph attention mechanism. The key innovation is the integration of spatial dependency
  learning through graph attention with temporal learning from LLMs, enabling effective
  imputation of missing values in multivariate time series data.
---

# GATGPT: A Pre-trained Large Language Model with Graph Attention Network for Spatiotemporal Imputation

## Quick Facts
- arXiv ID: 2311.14332
- Source URL: https://arxiv.org/abs/2311.14332
- Authors: 
- Reference count: 35
- Primary result: GATGPT achieves state-of-the-art performance for spatiotemporal imputation on AQI-36 dataset with MAE 10.28 and MSE 341.26 for point missing data

## Executive Summary
This paper introduces GATGPT, a novel framework for spatiotemporal imputation that leverages pre-trained large language models (LLMs) combined with a graph attention mechanism. The key innovation is the integration of spatial dependency learning through graph attention with temporal learning from LLMs, enabling effective imputation of missing values in multivariate time series data. The method preserves most LLM parameters while fine-tuning upper layers and incorporates positional embeddings to capture temporal order. Evaluated on three real-world datasets (AQI-36, PEMS-BAY, METR-LA), GATGPT achieves performance comparable to state-of-the-art deep learning baselines, with best results on AQI-36 (MAE 10.28, MSE 341.26 for point missing data) and strong performance on traffic datasets.

## Method Summary
GATGPT combines a pre-trained LLM (GPT-2) with a graph attention network for spatiotemporal imputation. The model maintains most LLM parameters frozen to preserve pre-trained temporal knowledge while fine-tuning upper layers for the specific imputation task. Spatial dependencies are captured through a graph attention module that aggregates neighbor information weighted by learned attention scores, with the adjacency matrix constructed using thresholded Gaussian kernel similarity based on geographical distances. Positional embeddings are added to the token embeddings to enable the LLM to understand chronological order. The model is trained using artificial missing patterns (25% random point missing and 5% base plus 1-4 hour blocks at 0.15% probability) and evaluated using MAE and MSE metrics against baselines including MEAN, DA, kNN, KF, MICE, VAR, TRMF, BATF, BRITS, GRIN, GP-VAE, and rGAIN.

## Key Results
- GATGPT achieves best performance on AQI-36 dataset with MAE 10.28 and MSE 341.26 for point missing data
- Strong performance on traffic datasets (PEMS-BAY and METR-LA) comparable to state-of-the-art deep learning baselines
- Ablation study shows graph attention module significantly improves spatial dependency learning
- Method demonstrates robustness across different missing patterns (point vs. block missing)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The graph attention module captures spatial dependencies by aggregating neighbor information weighted by learned attention scores.
- Mechanism: The GAT module computes attention weights α_ij between nodes i and j based on their embedded representations, then aggregates neighbor embeddings H_i' = ELU(Σ_j α_ij H_j). This allows the model to dynamically learn which neighboring nodes are most relevant for imputing missing values at each location.
- Core assumption: Spatial dependencies in the data can be represented as a graph where edge weights indicate similarity or proximity between nodes.
- Evidence anchors:
  - [section]: "The graph attention component enhances the LLM's ability to understand spatial relationships."
  - [section]: "We construct the adjacency matrix by employing a thresholded Gaussian kernel similarity method, which is based on geographical distance information."
- Break condition: If spatial dependencies are not well-represented by the constructed adjacency matrix, or if the attention mechanism fails to learn meaningful relationships between nodes.

### Mechanism 2
- Claim: Freezing most LLM parameters preserves pre-trained knowledge while fine-tuning upper layers adapts the model to spatiotemporal imputation.
- Mechanism: By keeping the self-attention blocks and feed-forward layers frozen, the model retains its learned temporal patterns from pre-training. Fine-tuning only the addition and normalization layers allows adaptation to the specific spatiotemporal imputation task without destroying the pre-existing knowledge.
- Core assumption: The pre-trained LLM has learned general temporal patterns that are useful for spatiotemporal imputation, and only minor adjustments are needed for task-specific adaptation.
- Evidence anchors:
  - [abstract]: "We maintain most of the LLM parameters unchanged to leverage existing knowledge for learning temporal patterns, while fine-tuning the upper layers tailored to various applications."
  - [section]: "Recognizing that the self-attention and feed-forward layers are repositories of a substantial portion of the knowledge accumulated in pre-trained language models, we choose to freeze the self-attention blocks."
- Break condition: If the pre-trained temporal knowledge is not relevant to the spatiotemporal imputation task, or if significant architectural changes are needed for optimal performance.

### Mechanism 3
- Claim: Positional embeddings enable the LLM to understand the chronological order of time series data.
- Mechanism: Positional embeddings are added to the token embeddings before input to the LLM, providing information about the temporal position of each data point in the sequence. This allows the LLM to learn patterns that depend on the order of observations.
- Core assumption: The chronological order of observations contains important information for spatiotemporal imputation.
- Evidence anchors:
  - [section]: "Furthermore, to bolster the model's capacity for discerning temporal relationships, we have implemented positional embedding into the original input."
  - [section]: "This addition is crucial for enabling the LLM to recognize the chronological sequence inherent in our spatiotemporal data."
- Break condition: If the temporal order of observations is not important for the imputation task, or if other methods of encoding temporal information are more effective.

## Foundational Learning

- Concept: Graph Attention Networks (GAT)
  - Why needed here: GAT is used to capture spatial dependencies between different locations in the spatiotemporal data.
  - Quick check question: How does the attention mechanism in GAT differ from traditional graph convolution?

- Concept: Transformer Architecture
  - Why needed here: The LLM backbone is based on transformer architecture, which is used to learn temporal patterns in the data.
  - Quick check question: What is the role of self-attention in transformer models?

- Concept: Fine-tuning vs. Full Training
  - Why needed here: The model uses a fine-tuning approach to adapt the pre-trained LLM to the specific task of spatiotemporal imputation.
  - Quick check question: What are the advantages and disadvantages of fine-tuning compared to full training from scratch?

## Architecture Onboarding

- Component map: Input embedding layer (token + positional embeddings) → Graph attention module → Frozen LLM blocks (self-attention + feed-forward) → Fine-tuned addition + normalization layers → Output linear layer
- Critical path: Graph attention aggregation → LLM temporal modeling → Output prediction
- Design tradeoffs: Freezing LLM parameters vs. full fine-tuning (computation vs. adaptation), simple adjacency matrix vs. learned graph structure (simplicity vs. flexibility)
- Failure signatures: Poor spatial imputation indicates graph attention issues; poor temporal imputation indicates LLM adaptation issues; overall poor performance could indicate input embedding problems
- First 3 experiments:
  1. Ablation study: Remove graph attention module and compare performance
  2. Parameter sensitivity: Vary number of fine-tuned layers and observe impact
  3. Data augmentation: Test model on different missing patterns (point vs. block) to assess robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GATGPT compare to other specialized spatiotemporal imputation models when applied to datasets with significantly different characteristics than those tested?
- Basis in paper: [explicit] The paper states that GATGPT achieves performance comparable to state-of-the-art deep learning baselines but does not provide comprehensive comparisons across diverse datasets.
- Why unresolved: The evaluation is limited to three specific datasets, which may not represent the full range of possible spatiotemporal data characteristics.
- What evidence would resolve it: Testing GATGPT on a wider variety of datasets with different characteristics (e.g., varying levels of missing data, different spatial and temporal patterns) and comparing its performance to other specialized models.

### Open Question 2
- Question: What is the impact of using different pre-trained Large Language Models (LLMs) other than GPT-2 on the performance of GATGPT?
- Basis in paper: [explicit] The paper mentions that the architecture is flexible and can incorporate other LLMs such as Llama, but does not provide comparative results.
- Why unresolved: The study focuses on using GPT-2 as the backbone and does not explore the performance variations with other LLMs.
- What evidence would resolve it: Conducting experiments with various pre-trained LLMs (e.g., Llama, BERT) as the backbone and comparing the imputation performance and computational efficiency.

### Open Question 3
- Question: How does the graph attention mechanism in GATGPT contribute to the model's ability to capture spatial dependencies compared to other spatial encoding methods?
- Basis in paper: [explicit] The paper introduces a graph attention module to enhance spatial dependency learning but does not compare it with alternative spatial encoding methods.
- Why unresolved: The effectiveness of the graph attention mechanism is demonstrated, but a direct comparison with other spatial encoding techniques is lacking.
- What evidence would resolve it: Implementing alternative spatial encoding methods (e.g., graph convolutional networks, spatial transformers) and comparing their performance with the graph attention mechanism in GATGPT on the same datasets.

### Open Question 4
- Question: What are the limitations of GATGPT in handling extremely high-dimensional spatiotemporal data, and how can these be addressed?
- Basis in paper: [inferred] The paper does not explicitly discuss the limitations of GATGPT with high-dimensional data, but such data often pose challenges for deep learning models.
- Why unresolved: The datasets used in the study may not be sufficiently high-dimensional to reveal potential limitations of GATGPT.
- What evidence would resolve it: Testing GATGPT on high-dimensional spatiotemporal datasets and analyzing its performance, scalability, and computational efficiency. Identifying bottlenecks and proposing modifications to improve handling of high-dimensional data.

## Limitations

- The model achieves comparable performance to state-of-the-art deep learning baselines but does not demonstrate clear superiority, suggesting the benefits of combining LLMs with graph attention may be incremental rather than transformative for this task.
- The evaluation relies on synthetic missing data patterns (random point missing and block missing) rather than real-world missingness, which may not capture the complexity and irregularity of actual data gaps in spatiotemporal applications.
- The study focuses on three datasets in specific domains (air quality and traffic), limiting generalizability to other spatiotemporal contexts such as financial time series, weather forecasting, or sensor networks in industrial settings.

## Confidence

- **High confidence**: The core methodology of combining frozen pre-trained LLM layers with fine-tuned upper layers for spatiotemporal imputation is well-supported by the experimental results showing consistent performance across datasets.
- **Medium confidence**: The claim that graph attention significantly enhances spatial dependency learning is supported by ablation studies, but the specific contribution relative to other spatial modeling approaches remains unclear.
- **Low confidence**: The assertion that this approach represents a general framework for spatiotemporal imputation is not fully supported, as the evaluation is limited to specific datasets and missing patterns without extensive ablation or comparison to alternative architectures.

## Next Checks

1. **Ablation Study Extension**: Conduct comprehensive ablation experiments removing individual components (graph attention, positional embeddings, LLM freezing strategy) to quantify the contribution of each element to overall performance, particularly focusing on whether the graph attention module provides benefits beyond standard graph neural networks.

2. **Cross-Domain Evaluation**: Test GATGPT on diverse spatiotemporal datasets outside the air quality and traffic domains (e.g., financial time series, weather data, IoT sensor networks) to assess the generalizability of the approach and identify domain-specific limitations.

3. **Real-World Missing Data Testing**: Evaluate the model on datasets with naturally occurring missing values rather than artificially injected gaps to determine if the performance gains observed with synthetic missingness translate to real-world scenarios where missing patterns may be non-random and correlated with other variables.