---
ver: rpa2
title: 'CodeGen2: Lessons for Training LLMs on Programming and Natural Languages'
arxiv_id: '2305.02309'
source_url: https://arxiv.org/abs/2305.02309
tags:
- language
- tasks
- training
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents CodeGen2, a set of lessons and open-source
  implementations for training large language models (LLMs) on programming and natural
  languages. The study aims to make LLM training for program synthesis more efficient
  by unifying four key components: model architectures, learning methods, infill sampling,
  and data distributions.'
---

# CodeGen2: Lessons for Training LLMs on Programming and Natural Languages

## Quick Facts
- arXiv ID: 2305.02309
- Source URL: https://arxiv.org/abs/2305.02309
- Reference count: 8
- This paper presents lessons for training LLMs on programming and natural languages, finding that a simple mixture of causal language modeling and span corruption is sufficient, while prefix-LM and infill sampling do not provide the expected benefits.

## Executive Summary
This paper presents CodeGen2, a set of lessons and open-source implementations for training large language models (LLMs) on programming and natural languages. The study aims to make LLM training for program synthesis more efficient by unifying four key components: model architectures, learning methods, infill sampling, and data distributions. Through extensive empirical experiments on 1B LLMs, the authors distill four key lessons: (1) the Prefix-LM architecture does not yield measurable improvements on their set of tasks, (2) training a model with infill sampling is not a free lunch, (3) a simple mixture of causal language modeling and span corruption is sufficient, and (4) a mixture distribution of programming and natural languages looks promising.

## Method Summary
The method involves training causal decoder models with a mixture of causal language modeling (CLM) and span corruption objectives, limited to within-file spans. The authors explore different model architectures (standard causal decoder vs. prefix-LM), learning methods (pure CLM vs. mixture of CLM and span corruption), and data distributions (programming-only vs. mixture of natural and programming languages). Training is performed on permissive datasets including Stack (programming) and Pile (natural language) with a 50-50 mixture ratio. The models are evaluated on benchmarks including HumanEval for code generation, HumanEval-Infill for infill capability, XSum for few-shot generation, and SuperGLUE and CodeXGLUE for understanding tasks.

## Key Results
- Prefix-LM architecture does not yield measurable improvements over standard causal decoder baseline on evaluated tasks
- A simple mixture of 50% causal language modeling and 50% span corruption is sufficient for competitive performance
- Training with infill sampling is not a "free lunch" - it degrades zero-shot generation performance by approximately 1 point on HumanEval
- A 50-50 mixture of programming and natural language data shows promise but does not outperform domain-matched models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Prefix-LM's bidirectional attention over the prefix improves representation learning for few-shot tasks without degrading zero-shot generation performance.
- **Mechanism**: The prefix portion uses bidirectional attention while the suffix uses causal masking, unifying encoder-style bidirectional representation with decoder-style causal generation in a single architecture.
- **Core assumption**: The benefits of bidirectional attention in the prefix outweigh any potential negative effects from masking the prefix tokens during training.
- **Evidence anchors**: [abstract] "for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM"
- **Break condition**: If the prefix length is too long relative to the sequence length, the effective gradient updates may become insufficient, leading to degraded learning.

### Mechanism 2
- **Claim**: A simple mixture of causal language modeling and span corruption objectives is sufficient for achieving competitive performance in both zero-shot generation and understanding tasks.
- **Mechanism**: The model is trained with a 50/50 mixture of (1) causal language modeling on non-corrupted sequences and (2) span corruption where spans of tokens are masked and the model must predict the original tokens.
- **Core assumption**: The task-specific prior information should be minimized to avoid overfitting, hence uniform distributions over mixture ratio, prefix lengths, and span lengths.
- **Evidence anchors**: [abstract] "we propose a simple mixture objective that combines causal language modeling and span corruption, limited to within-file spans"
- **Break condition**: If the mixture ratio deviates significantly from 50/50 or if span corruption is applied outside file boundaries, performance may degrade.

### Mechanism 3
- **Claim**: Mixing natural and programming languages in the data distribution can improve performance on tasks in both domains without affecting performance within a single modality.
- **Mechanism**: The model is trained on an equal mixture of natural language data (Pile) and programming language data (Stack), exposing it to both modalities during training.
- **Core assumption**: The supervised learning signal from comments and documentation in code, combined with code snippets in natural language data, provides sufficient weak supervision for the model to learn cross-modal capabilities.
- **Evidence anchors**: [abstract] "a mixture distribution of programming and natural languages on model performance is explored"
- **Break condition**: If the natural language proportion is too high, the model may not learn sufficient programming-specific representations; if too low, the cross-modal benefits may not manifest.

## Foundational Learning

- **Concept**: Transformer architecture and self-attention mechanism
  - Why needed here: The entire model architecture is built on transformer blocks with self-attention circuits
  - Quick check question: How does the attention mask differ between encoder, decoder, and prefix-LM architectures?

- **Concept**: Causal language modeling vs. masked language modeling
  - Why needed here: The paper explores mixing these two learning objectives to achieve both generation and understanding capabilities
  - Quick check question: What is the key difference in the prediction task between causal language modeling and span corruption?

- **Concept**: Neural scaling laws and the relationship between model size, data, and performance
  - Why needed here: The paper discusses how performance is dictated by neural scaling laws as a function of model parameters and observations
  - Quick check question: According to neural scaling laws, what happens to model performance as you increase model size while keeping data constant?

## Architecture Onboarding

- **Component map**: Transformer blocks with prefix-LM attention scheme -> Input sequence divided into prefix (bidirectional attention) and context (causal attention) -> Mixture of causal language modeling and span corruption objectives

- **Critical path**: The attention mechanism that switches between bidirectional and causal modes based on position in the sequence determines how information flows during both training and inference.

- **Design tradeoffs**: The prefix-LM architecture trades off between the benefits of bidirectional attention for understanding tasks and the necessity of causal masking for generation tasks. The mixture objective trades off between the strengths of causal modeling for generation and span corruption for understanding.

- **Failure signatures**: Poor performance on HumanEval may indicate issues with the prefix-LM architecture or the mixture objective. Degraded performance on understanding tasks may indicate insufficient bidirectional attention or suboptimal span corruption.

- **First 3 experiments**:
  1. Train a baseline causal decoder on BigPython and evaluate on HumanEval to establish performance baseline
  2. Train a prefix-LM with the same data and objective, varying prefix length ratios to observe effects on performance
  3. Implement and test the mixture objective (50% causal, 50% span corruption) on the causal decoder architecture to evaluate if it matches or exceeds prefix-LM performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "free lunch" hypothesis for infill sampling hold true under different training configurations or datasets?
- Basis in paper: [explicit] The paper attempts to verify Bavarian et al.'s (2022) "free lunch" hypothesis but fails to provide evidence, observing a performance drop of about 1 point in pass@1 on HumanEval.
- Why unresolved: The paper only tests one configuration (using a causal decoder with a mixture of CLM and PSM infill objective). Different architectures, datasets, or infill strategies might yield different results.
- What evidence would resolve it: Systematic ablation studies varying model architecture, dataset composition, infill sampling strategies, and training procedures to determine under what conditions (if any) infill training does not degrade zero-shot generation performance.

### Open Question 2
- Question: Can prefix-LM architecture achieve competitive performance on both natural language understanding and code generation tasks with optimized training strategies?
- Basis in paper: [explicit] The paper finds that prefix-LM does not yield measurable improvements over causal decoder baseline on their set of tasks, despite its potential for bidirectional attention.
- Why unresolved: The study may not have explored all possible configurations, such as different prefix lengths, task mixtures, or fine-tuning strategies that could unlock the potential of prefix-LM.
- What evidence would resolve it: Extensive experimentation with various prefix-LM configurations, including different prefix-to-context ratios, training objectives, and fine-tuning approaches, evaluated on a comprehensive set of understanding and generation benchmarks.

### Open Question 3
- Question: What is the optimal ratio and composition of natural language to programming language data for training a universal LLM that performs well on both domains?
- Basis in paper: [explicit] The paper explores a simple 50-50 mixture of natural and programming languages but finds that the mixed model performs closely to domain-matched models without outperforming them.
- Why unresolved: The study uses a fixed 50-50 ratio and does not explore other ratios, data quality differences, or domain-specific fine-tuning strategies.
- What evidence would resolve it: Systematic investigation of different data mixture ratios, inclusion of high-quality natural language data, domain-specific pre-training phases, and evaluation on diverse tasks in both domains to determine the optimal data composition strategy.

### Open Question 4
- Question: How does span corruption at the file level versus document level affect model performance on code generation and understanding tasks?
- Basis in paper: [explicit] The paper introduces file-level corruption to avoid masking document boundaries but does not compare its effectiveness against document-level corruption.
- Why unresolved: The study implements file-level corruption but lacks a comparative analysis with document-level corruption or other corruption strategies.
- What evidence would resolve it: Comparative experiments using file-level and document-level corruption strategies, along with other corruption methods, evaluated on code generation benchmarks like HumanEval and understanding tasks like CodeXGLUE, to determine the impact on model performance.

## Limitations
- Experiments are conducted exclusively on 1B-parameter models, so scaling behavior to larger models remains unknown
- Focus on permissive datasets (Stack and Pile) may not represent the full diversity of real-world programming and natural language data
- Evaluation is limited to specific benchmarks, potentially missing other relevant task domains

## Confidence

**High Confidence Claims**:
- The Prefix-LM architecture does not yield measurable improvements over standard causal decoders on the evaluated tasks
- The simple mixture of causal language modeling and span corruption is sufficient for competitive performance

**Medium Confidence Claims**:
- Mixing natural and programming languages in the data distribution looks promising
- Training with infill sampling is not a free lunch

**Low Confidence Claims**:
- The scaling behavior of the proposed methods to larger models
- The generalization of findings to non-permissive datasets or different programming languages beyond Python

## Next Checks

1. **Scale-up validation**: Complete training of the proposed methods on 7B and 16B parameter models and evaluate whether the 1B-parameter findings hold at scale, particularly for the Prefix-LM architecture and mixture objectives.

2. **Cross-dataset robustness**: Replicate the experiments using non-permissive datasets (e.g., GitHub code with various licenses) to verify whether the findings about data mixture and model architecture generalize beyond the Stack and Pile datasets.

3. **Task coverage expansion**: Evaluate the trained models on additional benchmarks not covered in the current study, such as mathematical reasoning tasks (GSM8K), code translation tasks, and multilingual programming tasks to assess the breadth of the model's capabilities.